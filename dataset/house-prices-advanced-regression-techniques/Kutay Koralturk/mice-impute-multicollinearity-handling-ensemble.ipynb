{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder, OrdinalEncoder\nfrom sklearn.compose import make_column_transformer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom fancyimpute import IterativeImputer\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow import keras\nfrom sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"#Lets start-off by loading data\ntrain = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Getting Started\n\nFor future processings, I will concatinate two sets together. So that training set and test set will have the same number of features in the same format. This will provide greater ease to preprocess data and result in datas coming from same distributions\n\nIn order to preserve the boarderline between training set and test set, I am recording the initial number of rows each has. Although the number of columns and the data wihtin those columns will change in future, number of rows will remain in tact. "},{"metadata":{"trusted":false},"cell_type":"code","source":"n_train = train.shape[0]\nn_test = test.shape[0]\n\ntest_id = test[\"Id\"]\n\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n   ## 1. Missing Data"},{"metadata":{},"cell_type":"markdown","source":"As a first step, I would like to get a look at the data, explore the data types in each columns and missing values. \n\nShowing the ratio between missing values and total values in given column will help me get an understanding on the severity of missing values. \n    "},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"AllData\")\n(all_data_rows, all_data_columns) = all_data.shape\nprint(\" Number of rows: {} \\n Number of columns: {}\".format(all_data_rows, all_data_columns))\nprint(train.sample(3))\n\ndef display_missing(df):\n    for col in df.columns.tolist():\n        if df[col].isnull().sum()>0:\n            print(\"{} column missing values: {} / {}\".format(col, df[col].isnull().sum(),df.shape[0]))\n\ndisplay_missing(all_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n\nf, ax = plt.subplots(figsize=(15, 20))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above analysis provided info about missing values, now we can make an intiution on what is our threshold going to be for missing values. \n\nThe columns with missing values surpassing the threshold will be dropped.  "},{"metadata":{},"cell_type":"markdown","source":"From intuition, 20% is a good threshold for now. "},{"metadata":{"trusted":false},"cell_type":"code","source":"def filterProblematicColumns(df,threshold):\n    listOfColumnNames = []\n    for col in df.columns.tolist():\n        if df[col].isnull().sum()> threshold:\n            listOfColumnNames.append(col)\n            print(col)\n    \n    return listOfColumnNames\n\nportion = 0.2\nthreshold = all_data.shape[0] * portion\n\n\ncolumnsToDrop = filterProblematicColumns(all_data, threshold)\n\nall_data = all_data.drop(columns=columnsToDrop)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that problematic values dropped, we can work on filling missing values that lie below the threshold.\n\nWe can start our work by analyzing the remaining columns with missing values. "},{"metadata":{"trusted":false},"cell_type":"code","source":"columns_with_missing_values = all_data.loc[:, all_data.isnull().any()]\nmissing_columns = columns_with_missing_values.columns.tolist()\n\nprint(\"Columns with Missing Values: \",\"\\n\", \"\\n\", missing_columns, \"\\n\")\nprint(columns_with_missing_values.describe())\nprint(all_data.shape)\nprint(\"\\n\", \"--------------\", \"\\n\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"There are several approaches for handling missing data. \n\nThe standard approach is to fill missing values with basic statistical approach. This corresponds to analyzing the range, deviation and mean of the data. If standard deviation is high and range of data is wide, choose median to reduce the impact of outliers and if the deviation is low, choose to work with mean value to fill missing values. \n\nIn this project, I would like to work with more advanced techniques of imputation. Advanced imputation techniques encapsulates, using KNN or Multiple Imputation by Chained Equations(MICE) to impute missing values. \n\nSince MICE is more time efficient than KNN imputation, I would like to work with that in this exercise. You can use the link below to look for different imputation techniques. \n\n- This is the link: (https://medium.com/ibm-data-science-experience/missing-data-conundrum-exploration-and-imputation-techniques-9f40abe0fd87)\n"},{"metadata":{},"cell_type":"markdown","source":"Checking for correlation with a heatmap is a good idea to visualize relationships. Unfortunately, here there are two drawbacks. \n\n    1. There are too many features to get a sense of just by looking at correlation values or at a heatmap\n    \n    2. There are non-numeric columns within the dataset. Correlations only work between numeric data. "},{"metadata":{"trusted":false},"cell_type":"code","source":"numcols = all_data.select_dtypes(include = np.number).columns\n\n#Lets start by plotting a heatmap to determine if any variables are correlated\nplt.figure(figsize = (12,8))\nsns.heatmap(data= all_data[numcols].corr())\nplt.show()\nplt.gcf().clear()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there are many features, it can be hard to understand correlations from the heatmap. The function below prints out numerical values of correlation pair of features."},{"metadata":{"trusted":false},"cell_type":"code","source":"def corr_missing_values(df, columns): \n    for column in columns:\n        df_corr = df.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n        df_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\n        print(df_corr[df_corr['Feature 1'] == column])\n        print(\"\")\n\n#corr_missing_values(all_data, [x for x in missing_columns if x in numcols])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.Encoding values"},{"metadata":{},"cell_type":"markdown","source":"There are two reasons for encoding non-numeric values: \n\n    1- We have decided to deal with missing values using MICE. MICE can only fill missing values if other values are numeric. The regression can only be implemented for imputation, if the values are numbers. \n    \n    2- The predictive models will be requiring numeric values for the modeling process. \n    \n\nThere are different consideration for encoding values. Ordinal values are values given in some sort of order in relation with eachother. Nominal values are values where such ordering does not exist. \n\nFor ordinal values, I will be using OrdinalEncoder and for nominal values, I will be using dummies to encode. "},{"metadata":{},"cell_type":"markdown","source":"   ### 2a. Ordinal Values"},{"metadata":{"trusted":false},"cell_type":"code","source":"numeric_columns = all_data.select_dtypes(include = np.number).columns.tolist()\nnominal_columns = [\"MSZoning\",\"Street\",\"LandContour\",\"LotConfig\",\"Neighborhood\",\"Condition1\",\"Condition2\",\"BldgType\",\"HouseStyle\",\"RoofStyle\",\"RoofMatl\",\"Exterior1st\",\"Exterior2nd\",\"MasVnrType\",\"Foundation\",\"Heating\",\"CentralAir\",\"Electrical\",\"GarageType\",\"SaleCondition\"]\nordinal_columns = [\"LotShape\",\"Utilities\",\"LandSlope\",\"ExterQual\",\"ExterCond\",\"BsmtQual\",\"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\"HeatingQC\",\"KitchenQual\",\"Functional\",\"GarageFinish\",\"GarageQual\",\"GarageCond\",\"PavedDrive\",\"SaleType\"]\n\n#Check if numbers match, to make sure no columns are left out\nprint(all_data.shape[1])\nprint(len(numeric_columns), len(nominal_columns), len(ordinal_columns))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While encoding, it is vital that we skip missing values. Because ordinal encoder does not work with missing values. We will impute those missing values, once everydata is turned into numeric. Then MICE will handle the missing values. "},{"metadata":{"trusted":false},"cell_type":"code","source":"## Ordinal Encoding (by skipping null values)\n\nordinal_enc_dict = {}\nfor col_name in ordinal_columns:\n    ordinal_enc_dict[col_name] = OrdinalEncoder()\n\n    col = all_data[col_name]\n    col_not_null = col[col.notnull()]\n    reshaped_vals = col_not_null.values.reshape(-1,1)\n\n    encoded_vals = ordinal_enc_dict[col_name].fit_transform(reshaped_vals)\n    all_data.loc[col.notnull(), col_name] = np.squeeze(encoded_vals)\n\n#Check if the values are encoded and no column has been skipped.   \nprint(all_data[ordinal_columns].head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###   2b. Nominal Values"},{"metadata":{},"cell_type":"markdown","source":"I had hard time to find a solution to encode nominal values with missing data. I am open to suggestions here. Please comment below if you have any advice. \n\nSince dummies do not work with missing values. I choosed a more standard approach to continue. I have investigated nominal columns with missing data. I have discovered the amount of missing data is relatively low. I have choosed to fill those missing values on the basis of occurence. "},{"metadata":{"trusted":false},"cell_type":"code","source":"print(display_missing(all_data[nominal_columns]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"## Imputation with mode\nnom_cols_withnull = all_data[nominal_columns].columns[all_data[nominal_columns].isnull().any()].tolist()\n\nmost_common_imputed = all_data[nom_cols_withnull].apply(lambda x: x.fillna(x.value_counts().index[0]))\n\nfor col_name in most_common_imputed.columns:\n    all_data[col_name] = most_common_imputed[col_name]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Encoding nominal values using dummies"},{"metadata":{"trusted":false},"cell_type":"code","source":"nom_df = pd.get_dummies(all_data[nominal_columns], prefix=nominal_columns)\n\nfor col_name in nom_df.columns:\n    all_data[col_name] = nom_df[col_name]\n\nall_data = all_data.drop(columns= nominal_columns)\n\nprint(all_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Imputation with MICE\n\nFor detailed info about MICE, you can check the paper with link:\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/"},{"metadata":{"trusted":false},"cell_type":"code","source":"MICE_imputer = IterativeImputer()\nordinal_mice = all_data.copy(deep = True)\n\nordinal_mice.iloc[:,:] = np.round(MICE_imputer.fit_transform(ordinal_mice))\n\nfor col_name in ordinal_columns:\n    all_data[col_name] = ordinal_mice[col_name]\n\nfor col_name in numeric_columns:\n    all_data[col_name] = ordinal_mice[col_name]\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Skewness"},{"metadata":{},"cell_type":"markdown","source":"Since, this is a regression problem. Skewed data may pose threats for modeling of data. Graphing the y would help us investigate the skewness of data. If such situation exists, we will log transform the data to improve the performance of regression models. "},{"metadata":{"trusted":false},"cell_type":"code","source":"### Skewness in SalePrice\n\nfig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, sharey= False, figsize= (8,4))\n\nsns.distplot(train['SalePrice'], ax = ax0)\nsns.distplot(np.log1p(train['SalePrice']), ax = ax1)\n\nax0.set(title= \"Sale Price Distribution\")\nax1.set(title= \"Sale Price Distribution in Logarithmic Scale\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, there is a left skewed data. Using logarithmic transformation the skewness can be fixed and normal distribution can be attained."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Transforming y to avoid skewness\n\ny = np.log1p(train[\"SalePrice\"] )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Visualizations"},{"metadata":{},"cell_type":"markdown","source":"The visualization of several numeric features against Sale Price will better outlay the relations. "},{"metadata":{"trusted":false},"cell_type":"code","source":"x_axis_features = [\"LotArea\", \"TotRmsAbvGrd\", \"GrLivArea\", \"OverallQual\",\"OverallCond\",\"YearBuilt\",\"YrSold\",\"MoSold\"]\n\n\n\ndef subplots_vs_saleprice(df,x_features, no_cols, fig_size=(20,15)):\n    \n    no_features = len(x_features)\n    number_of_rows = no_features // no_cols +1\n    \n    \n    fig, axs = plt.subplots(number_of_rows, no_cols, figsize= fig_size)\n    \n    feature_index = 0\n    \n    for nrow in range(number_of_rows):\n        for ncol in range(no_cols):  \n            \n            axs[nrow, ncol].scatter(df[x_features[feature_index]].values, np.log1p(train[\"SalePrice\"].values))\n            axs[nrow, ncol].set_title('{} vs SalePrice'.format(x_features[feature_index]))\n            feature_index += 1\n            \n            if feature_index == no_features:\n                break\n          \n            \n\n    for ax in axs.flat:\n        ax.set(xlabel='', ylabel='SalePrice (logScale)')\n\n    # Hide x labels and tick labels for top plots and y ticks for right plots.\n    for ax in axs.flat:\n        ax.label_outer()\n\n        \nsubplots_vs_saleprice(train, x_axis_features, 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Splitting the Concatinated DataSet and Feature Scaling"},{"metadata":{},"cell_type":"markdown","source":"After all the processes, we can divide the training and test set from each other for model testing. \n\nFeature Scaling is a crutial step here to regularize the weight of each parameter to the outcome. Otherwise, values with greater magnitude will overweight in the model. "},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\n\nX = all_data.loc[:n_train-1,:]\ntest = all_data.loc[n_train:,:]\n\nX_copy = X.copy(deep = False)\ntest_copy = test.copy(deep = False)\n\nX = scaler.fit_transform(X)\ntest = scaler.fit_transform(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Model Selection with Hyperparameter tuning and GridSearchCV"},{"metadata":{},"cell_type":"markdown","source":"In hyperparameter tuning, the main purpose is to find parameters that give out best predictions for validation set. This way, we avoid overfitting and get more generalized result for real tests. \n\nGridSearchCV is a decent tool to try different parameters while crossvalidating trials on the validation set. The best parameters of each estimator will be utilized later to build an ensemble estimator. \n"},{"metadata":{},"cell_type":"markdown","source":"### Brief Explanation on Regression Models \n\n#### SVR \n\nWhat we are trying to do here is basically trying to decide a decision boundary at ‘e’ distance from the original hyper plane such that data points closest to the hyper plane or the support vectors are within that boundary line.\n\nhttps://medium.com/coinmonks/support-vector-regression-or-svr-8eb3acf6d0ff\nhttps://www.saedsayad.com/support_vector_machine_reg.htm\n\n#### Ridge \n\nIn the Ridge Regression, the cost function is altered by adding a penalty equivalent to square of the magnitude of the coefficients. \n\nhttps://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b\n\n\n\n#### Lasso \n\nThe only difference of Lasso from Ridge is instead of taking the square of the coefficients, magnitudes are taken into account. This type of regularization (L1) can lead to zero coefficients i.e. some of the features are completely neglected for the evaluation of output. So Lasso regression not only helps in reducing over-fitting but it can help us in feature selection. \n\nhttps://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b\n\nI will continue putting up short briefings about the model principles..."},{"metadata":{},"cell_type":"markdown","source":"### Model 1 - Without Multicollinearity  check"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import Ridge, Lasso, LinearRegression  \nfrom sklearn.kernel_ridge import KernelRidge  \nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np  \nfrom sklearn import metrics  \nfrom sklearn.metrics import mean_squared_error  \nimport xgboost as xgb  \nfrom sklearn.linear_model import ElasticNet  \nfrom sklearn.svm import SVR\n\n\n\n\nprint(\"-----------Stats for SVR-----------------\", \"\\n\")\n\nsvr = SVR(epsilon = 0.01)\nparameters = [{'kernel': ['rbf'], 'gamma': [1e-4, 1e-3, 0.01, 0.1, 0.2, 0.5, 0.6, 0.9],'C': [1, 10, 100, 1000, 10000]}]\ngrid_svr = GridSearchCV(svr, parameters, cv = 10, scoring=\"neg_mean_squared_error\", verbose=0,n_jobs = -1)\ngrid_svr.fit(X, y)\nprint(pd.DataFrame(grid_svr.cv_results_))\n\n\nprint(\"-----------Stats for ElasticNet-----------------\", \"\\n\")\n\nelastic_net = ElasticNet(selection=\"random\")\nelastic_params = {\"max_iter\": [1, 5, 10],\n                      \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n                      \"l1_ratio\": np.arange(0.0, 1.0, 0.1)}\n\ngrid_elastic= GridSearchCV(estimator = elastic_net, param_grid= elastic_params, scoring=\"neg_mean_squared_error\", cv=5, verbose=0,n_jobs = -1)\ngrid_elastic.fit(X,y)\nprint(pd.DataFrame(grid_elastic.cv_results_))\n\n\n\nprint(\"-----------Stats for XGB-----------------\", \"\\n\")\n\nxg_boost = xgb.XGBRegressor(objective='reg:squarederror')\nhouse_price_dmatrix = xgb.DMatrix(data = X, label=y)\nxgb_params = {\"learning_rate\":[0.01,0.1,0.5,0.9],\"n_estimators\":[200],\"subsample\": [0.3,0.5,0.9]}\ngrid_xgb= GridSearchCV(estimator = xg_boost, param_grid= xgb_params, scoring=\"neg_mean_squared_error\", cv=5, verbose=0,n_jobs = -1)\ngrid_xgb.fit(X,y)\nprint(pd.DataFrame(grid_xgb.cv_results_))\n\n\n\nprint(\"-----------Stats for KernelRidge-----------------\", \"\\n\")\n\nkernel_ridge = KernelRidge()\n\nparam_grid_kr = {'alpha': [0.001,0.003,0.01,0.3,0.1,0.3,1,3,10,30,100,300,1000,3000,100000],\n              'kernel':['polynomial'], \n              'degree':[2,3,4,5,6,7,8],\n              'coef0':[0,1,1.5,2,2.5,3,3.5,10]}\nkernel_ridge = GridSearchCV(kernel_ridge, \n                 param_grid = param_grid_kr, \n                 scoring = \"neg_mean_squared_error\", \n                 cv = 5,\n                 n_jobs = -1,\n                 verbose = 0)\n\nkernel_ridge.fit(X,y)\nprint(pd.DataFrame(kernel_ridge.cv_results_))\nk_best = kernel_ridge.best_estimator_\nkernel_ridge.best_score_\n\nparam_grid = {\"alpha\": [0.001,0.003,0.01,0.3,0.1,0.3,1,3,10,30,100,300,1000,3000,100000]}\n\nprint(\"-----------Stats for Ridge-----------------\", \"\\n\")\nridge = Ridge()  \ngrid_ridge = GridSearchCV(ridge, param_grid = param_grid, cv = 10, scoring = \"neg_mean_squared_error\", n_jobs=-1, verbose = 0) \ngrid_ridge.fit(X, y)\nprint(pd.DataFrame(grid_ridge.cv_results_))\n\nprint(\"-----------Stats for Lasso-----------------\", \"\\n\")\nlasso = Lasso()  \ngrid_lasso = GridSearchCV(lasso, param_grid = param_grid, cv = 10, scoring = \"neg_mean_squared_error\", n_jobs=-1, verbose=0) \ngrid_lasso.fit(X, y)\nprint(pd.DataFrame(grid_lasso.cv_results_))\n\nprint(\"-----------Scoreboard for Kernel Ridge-----------------\", \"\\n\")\nprint(kernel_ridge.best_score_)\nprint(kernel_ridge.best_estimator_.alpha)\n\nprint(\"-----------Scoreboard for Ridge-----------------\", \"\\n\")\nprint(grid_ridge.best_score_)\nprint(grid_ridge.best_estimator_.alpha)\n\nprint(\"-----------Scoreboard for Lasso-----------------\", \"\\n\")\n\nprint(grid_lasso.best_score_)\nprint(grid_lasso.best_estimator_.alpha)\n\nprint(\"-----------Scoreboard for XGB-----------------\", \"\\n\")\nprint(grid_xgb.best_score_)\nprint(grid_xgb.best_params_)\n\nprint(\"-----------Scoreboard for Elastic-----------------\", \"\\n\")\nprint(grid_elastic.best_score_)\nprint(grid_elastic.best_params_)\n\nprint(\"-----------Scoreboard for SVR-----------------\", \"\\n\")\nprint(grid_svr.best_score_)\nprint(grid_svr.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stack Model-1\n\nThe stack model relies on the phenomenon called \"wisdom of the crowds\". The group of models' decision outperforms the individual performance of the models.  "},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import StackingRegressor\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_val_score\n\nestimators = [('ridge', grid_ridge.best_estimator_) ,\n              ('lasso', grid_lasso.best_estimator_),\n              (\"elastic\", grid_elastic.best_estimator_),\n              ('kernel_ridge', kernel_ridge.best_estimator_),\n              (\"svr\", grid_svr.best_estimator_)\n             ]\n\nstack = StackingRegressor(estimators=estimators, final_estimator=grid_xgb.best_estimator_)\n\nprint(\"\\n ---Score for Stack--- \")\nprint(cross_val_score(stack, X, y, cv=10, scoring=\"neg_mean_squared_error\", n_jobs=-1).mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importance \n\nSince the data is normalized, meaning that each feature has a value between 0 and 1, the weights in the Lasso Regression can give us insight into importance of features in our data. Moreover, we can remove the features that does not contribute to the model which can be called as noise. "},{"metadata":{"trusted":false},"cell_type":"code","source":"lasso=grid_lasso.best_estimator_\nlasso.fit(X,y)\nfeature_importance_lasso = pd.DataFrame({\"Feature Importance\":lasso.coef_}, index=X_copy.columns)\nfeature_importance_lasso.sort_values(\"Feature Importance\",ascending=False)\nfeature_importance_lasso[feature_importance_lasso[\"Feature Importance\"]!=0].sort_values(\"Feature Importance\").plot(kind=\"barh\",figsize=(15,25))\n\nplt.xticks(rotation=90)\nplt.show()\n\nabs_feature_importance_lasso = pd.DataFrame({\"Feature Importance\":abs(lasso.coef_)}, index=X_copy.columns)\n\nprint(abs_feature_importance_lasso[abs_feature_importance_lasso[\"Feature Importance\"]!=0].sort_values(\"Feature Importance\", ascending =False).head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the feature importance list you see above, the values are listed with their absolute value. This way we can encapsulate positive and negative impacts of the features togeter to the model. "},{"metadata":{},"cell_type":"markdown","source":"### Measures Against Multicollinearity \n\nMulticollinearity may impact our model performance. If two or more values are highly correlated, it may result in overweighting of these features in the model, therefore, increasing bias. Thus we should check is such condition exist. There are 2 ways to tackle this problem. \n\n  1. Manually removing highly correlated values. \n  2. PCA to disintregrate highly correlated features. \n  \n#### Manual Approach\n\n    - Removing features that are above certain correlation threshold with eachother.  "},{"metadata":{"trusted":false},"cell_type":"code","source":"corr_matrix = X_copy.corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\ncorr_threshold = 0.85\nto_drop = [column for column in upper.columns if any(upper[column] > corr_threshold)]\n\nX_new = X_copy.copy(deep=False)\nX_new =X_new.drop(X_new[to_drop], axis=1)\nX_new = scaler.fit_transform(X_new)\n\ntest_new = test_copy.copy(deep=False)\ntest_new =test_new.drop(test_new[to_drop], axis=1)\ntest_new = scaler.fit_transform(test_new)\n\nprint(\"-----------Stats for ElasticNet-----------------\", \"\\n\")\n\nelastic_net = ElasticNet()\nelastic_params = {\"max_iter\": [1, 5, 10],\n                      \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n                      \"l1_ratio\": np.arange(0.0, 1.0, 0.1)}\n\ngrid_elastic= GridSearchCV(estimator = elastic_net, param_grid= elastic_params, scoring=\"neg_mean_squared_error\", cv=10, verbose=0,n_jobs=-1)\ngrid_elastic.fit(X_new,y)\nprint(pd.DataFrame(grid_elastic.cv_results_))\n\n\nprint(\"-----------Stats for SVR-----------------\", \"\\n\")\nsvr = SVR(epsilon = 0.01)\nparameters = [{'kernel': ['rbf'], 'gamma': [1e-4, 1e-3, 0.01, 0.1, 0.2, 0.5, 0.6, 0.9],'C': [1, 10, 100, 1000, 10000]}]\ngrid_svr = GridSearchCV(svr, parameters, cv = 10,n_jobs=-1)\ngrid_svr.fit(X, y)\nprint(pd.DataFrame(grid_svr.cv_results_))\n\n\n\nprint(\"-----------Stats for XGB-----------------\", \"\\n\")\nxg_boost = xgb.XGBRegressor(objective='reg:squarederror')\nhouse_price_dmatrix = xgb.DMatrix(data = X_new, label=y)\nxgb_params = {\"learning_rate\":[0.01,0.1,0.5,0.9],\"n_estimators\":[200],\"subsample\": [0.3,0.5,0.9]}\n\ngrid_xgb= GridSearchCV(estimator = xg_boost, param_grid= xgb_params, scoring=\"neg_mean_squared_error\", cv=10, verbose=0,n_jobs=-1)\ngrid_xgb.fit(X_new,y)\nprint(pd.DataFrame(grid_xgb.cv_results_))\n\nprint(\"-----------Stats for KernelRidge-----------------\", \"\\n\")\nkernel_ridge = KernelRidge()\nparam_grid_kr = {'alpha': [0.001,0.003,0.01,0.3,0.1,0.3,1,3,10,30,100,300,1000,3000,100000],\n              'kernel':['polynomial'], \n              'degree':[2,3,4,5,6,7,8],\n              'coef0':[0,1,1.5,2,2.5,3,3.5,10]}\nkernel_ridge = GridSearchCV(kernel_ridge, \n                 param_grid = param_grid_kr, \n                 scoring = \"neg_mean_squared_error\", \n                 cv = 10,\n                 n_jobs = -1,\n                 verbose = 0)\nkernel_ridge.fit(X_new,y)\nprint(pd.DataFrame(kernel_ridge.cv_results_))\nk_best = kernel_ridge.best_estimator_\nkernel_ridge.best_score_\n\nparam_grid = {\"alpha\": [0.001,0.003,0.01,0.3,0.1,0.3,1,3,10,30,100,300,1000,3000,100000]}\n\nprint(\"-----------Stats for Ridge-----------------\", \"\\n\")\nridge = Ridge()  \ngrid_ridge = GridSearchCV(ridge, param_grid = param_grid, cv = 10, scoring = \"neg_mean_squared_error\", n_jobs=-1, verbose = 0) \ngrid_ridge.fit(X_new, y)\nprint(pd.DataFrame(grid_ridge.cv_results_))\n\nprint(\"-----------Stats for SVR-----------------\", \"\\n\")\ngrid_ridge = GridSearchCV(ridge, param_grid = param_grid, cv = 10, scoring = \"neg_mean_squared_error\", n_jobs=-1, verbose = 0) \ngrid_ridge.fit(X_new, y)\nprint(pd.DataFrame(grid_ridge.cv_results_))\n\nprint(\"-----------Stats for Lasso-----------------\", \"\\n\")\nlasso = Lasso()  \ngrid_lasso = GridSearchCV(lasso, param_grid = param_grid, cv = 10, scoring = \"neg_mean_squared_error\", n_jobs=-1, verbose=0) \ngrid_lasso.fit(X_new, y)\nprint(pd.DataFrame(grid_lasso.cv_results_))\n\nprint(\"-----------Scoreboard for Kernel Ridge-----------------\", \"\\n\")\nprint(kernel_ridge.best_score_)\nprint(kernel_ridge.best_estimator_.alpha)\n\nprint(\"-----------Scoreboard for Ridge-----------------\", \"\\n\")\nprint(grid_ridge.best_score_)\nprint(grid_ridge.best_estimator_.alpha)\n\nprint(\"-----------Scoreboard for Lasso-----------------\", \"\\n\")\n\nprint(grid_lasso.best_score_)\nprint(grid_lasso.best_estimator_.alpha)\n\nprint(\"-----------Scoreboard for XGB-----------------\", \"\\n\")\nprint(grid_xgb.best_score_)\nprint(grid_xgb.best_params_)\n\nprint(\"-----------Scoreboard for Elastic-----------------\", \"\\n\")\nprint(grid_elastic.best_score_)\nprint(grid_elastic.best_params_)  \n\nprint(\"-----------Scoreboard for SVR-----------------\", \"\\n\")\nprint(grid_svr.best_score_)\nprint(grid_svr.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stack Model-2 After Manual Multicollinearity Check"},{"metadata":{"trusted":false},"cell_type":"code","source":"estimators = [('ridge', grid_ridge.best_estimator_) ,\n              ('lasso', grid_lasso.best_estimator_),\n              (\"elastic\", grid_elastic.best_estimator_),\n              ('kernel_ridge', kernel_ridge.best_estimator_),\n              ]\n\nstack = StackingRegressor(estimators=estimators, final_estimator=grid_xgb.best_estimator_ )\n\nprint(\"\\n ---Score for Stack--- \")\nprint(cross_val_score(stack, X_new, y, cv=10, scoring=\"neg_mean_squared_error\",n_jobs=-1).mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PCA Decomposition"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.decomposition import PCA, KernelPCA\n\npca = PCA(n_components=190)\nX_pca = pca.fit_transform(X)\ntest_pca = pca.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import Ridge, Lasso, LinearRegression  \nfrom sklearn.kernel_ridge import KernelRidge  \nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np  \nfrom sklearn import metrics  \nfrom sklearn.metrics import mean_squared_error  \nimport xgboost as xgb  \nfrom sklearn.linear_model import ElasticNet  \nfrom sklearn.svm import SVR\n\nprint(\"-----------Stats for SVR-----------------\", \"\\n\")\n\nsvr = SVR(epsilon = 0.01)\nparameters = [{'kernel': ['rbf'], 'gamma': [1e-4, 1e-3, 0.01, 0.1, 0.2, 0.5, 0.6, 0.9],'C': [1, 10, 100, 1000, 10000]}]\ngrid_svr = GridSearchCV(svr, parameters, cv = 10, scoring=\"neg_mean_squared_error\", verbose=0,n_jobs=-1)\ngrid_svr.fit(X_pca, y)\nprint(pd.DataFrame(grid_svr.cv_results_))\n\n\nprint(\"-----------Stats for ElasticNet-----------------\", \"\\n\")\n\nelastic_net = ElasticNet()\nelastic_params = {\"max_iter\": [1, 5, 10],\n                      \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n                      \"l1_ratio\": np.arange(0.0, 1.0, 0.1)}\n\ngrid_elastic= GridSearchCV(estimator = elastic_net, param_grid= elastic_params, scoring=\"neg_mean_squared_error\", cv=5, verbose=0,n_jobs=-1)\ngrid_elastic.fit(X_pca,y)\nprint(pd.DataFrame(grid_elastic.cv_results_))\n\n\n\nprint(\"-----------Stats for XGB-----------------\", \"\\n\")\n\nxg_boost = xgb.XGBRegressor(objective='reg:squarederror')\nhouse_price_dmatrix = xgb.DMatrix(data = X, label=y)\nxgb_params = {\"learning_rate\":[0.01,0.1,0.5,0.9],\"n_estimators\":[200],\"subsample\": [0.3,0.5,0.9]}\ngrid_xgb= GridSearchCV(estimator = xg_boost, param_grid= xgb_params, scoring=\"neg_mean_squared_error\", cv=5, verbose=0,n_jobs=-1)\ngrid_xgb.fit(X_pca,y)\nprint(pd.DataFrame(grid_xgb.cv_results_))\n\n\n\nprint(\"-----------Stats for KernelRidge-----------------\", \"\\n\")\n\nkernel_ridge = KernelRidge()\n\nparam_grid_kr = {'alpha': [0.001,0.003,0.01,0.3,0.1,0.3,1,3,10,30,100,300,1000,3000,100000],\n              'kernel':['polynomial'], \n              'degree':[2,3,4,5,6,7,8],\n              'coef0':[0,1,1.5,2,2.5,3,3.5,10]}\nkernel_ridge = GridSearchCV(kernel_ridge, \n                 param_grid = param_grid_kr, \n                 scoring = \"neg_mean_squared_error\", \n                 cv = 5,\n                 n_jobs = -1,\n                 verbose = 0)\n\nkernel_ridge.fit(X_pca,y)\nprint(pd.DataFrame(kernel_ridge.cv_results_))\nk_best = kernel_ridge.best_estimator_\nkernel_ridge.best_score_\n\nparam_grid = {\"alpha\": [0.001,0.003,0.01,0.3,0.1,0.3,1,3,10,30,100,300,1000,3000,100000]}\n\nprint(\"-----------Stats for Ridge-----------------\", \"\\n\")\nridge = Ridge()  \ngrid_ridge = GridSearchCV(ridge, param_grid = param_grid, cv = 10, scoring = \"neg_mean_squared_error\", n_jobs=-1, verbose = 0) \ngrid_ridge.fit(X_pca, y)\nprint(pd.DataFrame(grid_ridge.cv_results_))\n\nprint(\"-----------Stats for Lasso-----------------\", \"\\n\")\nlasso = Lasso()  \ngrid_lasso = GridSearchCV(lasso, param_grid = param_grid, cv = 10, scoring = \"neg_mean_squared_error\", n_jobs=-1, verbose=0) \ngrid_lasso.fit(X_pca, y)\nprint(pd.DataFrame(grid_lasso.cv_results_))\n\nprint(\"-----------Scoreboard for Kernel Ridge-----------------\", \"\\n\")\nprint(kernel_ridge.best_score_)\nprint(kernel_ridge.best_estimator_.alpha)\n\nprint(\"-----------Scoreboard for Ridge-----------------\", \"\\n\")\nprint(grid_ridge.best_score_)\nprint(grid_ridge.best_estimator_.alpha)\n\nprint(\"-----------Scoreboard for Lasso-----------------\", \"\\n\")\n\nprint(grid_lasso.best_score_)\nprint(grid_lasso.best_estimator_.alpha)\n\nprint(\"-----------Scoreboard for XGB-----------------\", \"\\n\")\nprint(grid_xgb.best_score_)\nprint(grid_xgb.best_params_)\n\nprint(\"-----------Scoreboard for Elastic-----------------\", \"\\n\")\nprint(grid_elastic.best_score_)\nprint(grid_elastic.best_params_)\n\nprint(\"-----------Scoreboard for SVR-----------------\", \"\\n\")\nprint(grid_svr.best_score_)\nprint(grid_svr.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stack Model 3 - Feature Disintegration with PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_val_score\n\nestimators = [('ridge', grid_ridge.best_estimator_) ,\n              ('lasso', grid_lasso.best_estimator_),\n              (\"elastic\", grid_elastic.best_estimator_),\n              ('kernel_ridge', kernel_ridge.best_estimator_),\n              (\"svr\", grid_svr.best_estimator_)\n             ]\n\nstack = StackingRegressor(estimators=estimators, final_estimator=grid_xgb.best_estimator_)\n\nprint(\"\\n ---Score for Stack--- \")\nprint(cross_val_score(stack, X_pca, y, cv=10, scoring=\"neg_mean_squared_error\",n_jobs=-1).mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The two approaches couldn't get a better score with the multicollinearity checks. Therefore, I wil choose the first model for the future production and assessments.  "},{"metadata":{},"cell_type":"markdown","source":"## 7. Submission"},{"metadata":{"trusted":false},"cell_type":"code","source":"fitted_stack = stack.fit(X,y)\npredictions = np.expm1(fitted_stack.predict(test))\n\noutput = pd.DataFrame({'Id': test_id, 'SalePrice': predictions})\noutput.to_csv('stack.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"### Things to do\n\n### Add new features with feature engineering\n### Try removing unimportant features for performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Please feel free to add comments and give recommendations on the work. I would be happy to improve myself. I hope this will help you! "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":1}