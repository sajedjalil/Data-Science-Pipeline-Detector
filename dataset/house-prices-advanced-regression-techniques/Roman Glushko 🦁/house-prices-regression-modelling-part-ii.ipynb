{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import eli5\nimport numpy as np\nimport pandas as pd\nfrom sklearn import set_config\nfrom sklearn.base import TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_absolute_error, mean_squared_log_error, make_scorer\nfrom sklearn.model_selection import GridSearchCV, cross_validate, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, FunctionTransformer","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"np.random.seed(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# House Prices: Regression Modelling (Part II)\n\n<img style=\"margin-left:0; height: 450px\" src=\"https://livability.com/sites/default/files/151SUBAME031.jpg\" />\n\nAfter we went through the previous <a href=\"https://www.kaggle.com/glushko/house-prices-domain-driven-eda-part-i\">EDA part</a>, now it's time to turn our insights into a workable regression model ðŸ§ª\n\nThis notebook will be covering all aspects of modelling process I went through in order to get to **Top 4%** (Dec 2020) with a Kaggle RMLE **0.11617**.\n\n**Feel free to upvote this notebook if you find it helpful** ðŸ’« "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest_df = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n\nfull_df = pd.concat([train_df, test_df], sort=True).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Misc Functions"},{"metadata":{"trusted":false},"cell_type":"code","source":"# define a new scoring that is used during Kaggle submission\n\ndef neg_rmsle(y_true, y_pred):\n    y_pred = np.abs(y_pred)\n    \n    return -1 * np.sqrt(mean_squared_log_error(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# function that cross-validate model performance based on RMSE, MAE, R^2 and RMSLE metrics \n\ndef score_model(model, X, Y):\n    \n    scores = cross_validate(\n        model, X, Y, \n        scoring=['r2', 'neg_mean_absolute_error', 'neg_mean_squared_error'], cv=2,\n        n_jobs=-1, verbose=0)\n\n    rmsle_score = cross_val_score(model, X, Y, cv=2, scoring=make_scorer(neg_rmsle))\n\n    mse_score = np.sqrt(-1 * scores['test_neg_mean_squared_error'].mean())\n    mse_std = np.sqrt(scores['test_neg_mean_squared_error'].std())\n\n    mae_score = -1 * scores['test_neg_mean_absolute_error'].mean()\n    mae_std = scores['test_neg_mean_absolute_error'].std()\n\n    r2_score_mean = scores['test_r2'].mean()\n    r2_std = scores['test_r2'].std()\n\n    print('[CV] RMSE: %.4f (%.4f)' % (mse_score, mse_std))\n    print('[CV] MAE: %.4f (%.4f)' % (mae_score, mae_std))\n    print('[CV] R^2: %.4f (%.4f)' % (r2_score_mean, r2_std))\n    print('[CV] RMSLE: %.6f (%.4f)' % (-1 * rmsle_score.mean(), rmsle_score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# sklearn's pipeline API is limited at this point and doesn't provide a way to get columns of transformed X array\n# This snippet will cover our back \n\ndef get_columns_from_transformer(column_transformer, input_colums):    \n    col_name = []\n\n    for transformer_in_columns in column_transformer.transformers_[:-1]: #the last transformer is ColumnTransformer's 'remainder'\n        raw_col_name = transformer_in_columns[2]\n        if isinstance(transformer_in_columns[1],Pipeline): \n            transformer = transformer_in_columns[1].steps[-1][1]\n        else:\n            transformer = transformer_in_columns[1]\n        try:\n            names = transformer.get_feature_names(raw_col_name)\n        except AttributeError: # if no 'get_feature_names' function, use raw column name\n            names = raw_col_name\n        if isinstance(names,np.ndarray): # eg.\n            col_name += names.tolist()\n        elif isinstance(names,list):\n            col_name += names    \n        elif isinstance(names,str):\n            col_name.append(names)\n\n    [_, _, reminder_columns] = column_transformer.transformers_[-1]\n\n    for col_idx in reminder_columns:\n        col_name.append(input_colums[col_idx])\n\n    return col_name","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning and Preparation"},{"metadata":{"trusted":false},"cell_type":"code","source":"for feature in (\n    'PoolQC', \n    'FireplaceQu', \n    'Alley', \n    'Fence', \n    'MiscFeature', \n    'BsmtQual', \n    'BsmtCond', \n    'BsmtExposure', \n    'BsmtFinType1', \n    'BsmtFinType2',\n    'GarageType', \n    'GarageFinish', \n    'GarageQual', \n    'GarageCond',\n    'BsmtQual', \n    'BsmtCond', \n    'BsmtExposure', \n    'BsmtFinType1', \n    'BsmtFinType2',\n    'MasVnrType',\n):\n    train_df[feature] = train_df[feature].fillna('None')\n    test_df[feature] = test_df[feature].fillna('None')\n    full_df[feature] = full_df[feature].fillna('None')\n\nfor feature in (\n    'BsmtFinSF1', \n    'BsmtFinSF2', \n    'BsmtUnfSF',\n    'TotalBsmtSF', \n    'BsmtFullBath', \n    'BsmtHalfBath',\n    'MasVnrArea',\n    'GarageCars',\n    'GarageArea',\n    'GarageYrBlt',\n):\n    train_df[feature] = train_df[feature].fillna(0)\n    test_df[feature] = test_df[feature].fillna(0)\n    full_df[feature] = full_df[feature].fillna(0)\n\nfor feature in (\n    'Electrical', \n    'KitchenQual', \n    'Exterior1st',\n    'Exterior2nd', \n    'SaleType',\n    'Utilities',\n):\n    train_df[feature] = train_df[feature].fillna(train_df[feature].mode()[0])\n    test_df[feature] = test_df[feature].fillna(test_df[feature].mode()[0])\n    full_df[feature] = full_df[feature].fillna(full_df[feature].mode()[0])\n\nfor dataframe in [train_df, test_df, full_df]:\n    dataframe['MSZoning'] = dataframe.groupby(['Neighborhood'])['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n    dataframe['MSSubClass'] = dataframe.groupby(['HouseStyle'])['MSSubClass'].transform(lambda x: x.fillna(x.mode()[0]))\n    dataframe['LotFrontage'] = dataframe.groupby(['Neighborhood', 'MSSubClass'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n    dataframe['Functional'] = dataframe['Functional'].fillna('Typ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n\nFor the sake of feature engineering we would like to encode some of the ordinal features that represents quality or conditions:"},{"metadata":{"trusted":false},"cell_type":"code","source":"num_features = [f for f in train_df.columns if train_df.dtypes[f] != 'object']\nnum_features.remove('Id')\nnum_features.remove('SalePrice')\n\ncat_features = [f for f in train_df.columns if train_df.dtypes[f] == 'object']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ordinal_feature_mapping = {\n    'ExterQual': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}, \n    'ExterCond': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},\n    'BsmtQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n    'BsmtCond': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n    'BsmtFinType1': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n    'BsmtFinType2': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n    'HeatingQC': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},\n    'KitchenQual': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},\n    'FireplaceQu': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n    'GarageFinish': {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3},\n    'GarageQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n    'GarageCond': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n    'PoolQC': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n    'Fence': {'None': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4},\n    'PavedDrive': {'N': 0, 'P': 1, 'Y': 2},\n    'CentralAir': {'N': 0, 'Y': 1},\n    'Alley': {'None': 0, 'Pave': 1, 'Grvl': 2},\n    'Street': {'Pave': 0, 'Grvl': 1},\n    'BsmtExposure': {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n    'Functional': {'Sal': 0, 'Sev': 1, 'Maj2': 2, 'Maj1': 3, 'Mod': 4, 'Min2': 5, 'Min1': 6, 'Typ': 7}\n}\n\nnon_ordinal_cat_features = list(set(cat_features) - set(ordinal_feature_mapping.keys()))\n\nfor cat_feature in non_ordinal_cat_features:\n    train_df[cat_feature + 'Enc'] = LabelEncoder().fit_transform(train_df[cat_feature])\n    test_df[cat_feature + 'Enc'] = LabelEncoder().fit_transform(test_df[cat_feature])\n    full_df[cat_feature + 'Enc'] = LabelEncoder().fit_transform(full_df[cat_feature])\n\nfor ordinal_feature, feature_mapping in ordinal_feature_mapping.items():\n    train_df[ordinal_feature + 'Enc'] = train_df[ordinal_feature].map(feature_mapping)\n    test_df[ordinal_feature + 'Enc'] = test_df[ordinal_feature].map(feature_mapping)\n    full_df[ordinal_feature + 'Enc'] = full_df[ordinal_feature].map(feature_mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"neighborhoodCategories = full_df['Neighborhood'].unique()\nsaleCondCategories = full_df['SaleCondition'].unique()\ngarageTypeCategories = full_df['GarageType'].unique()\nlotShapeCategories = full_df['LotShape'].unique()\nlandSlopeCategories = full_df['LandSlope'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can engineer new features. During expiriments there were <a href=\"https://www.kaggle.com/glushko/house-prices-domain-driven-eda-part-i\">much more features</a>, but I have removed those that did not help me to improve model generalization. Here is a list of residuals that I found to be useful during modelling:"},{"metadata":{"trusted":false},"cell_type":"code","source":"for dataframe in [train_df, test_df, full_df]:\n    dataframe['AvgRoomSF'] = dataframe['GrLivArea'] / dataframe['TotRmsAbvGrd']\n\n    dataframe['OverallHouseQC'] = dataframe['OverallQual'] + dataframe['OverallCond']\n\n    dataframe['IsNeighborhoodElite'] = (dataframe['Neighborhood'].isin(['NridgHt', 'CollgeCr', 'Crawfor', 'StoreBr', 'Timber'])) * 1 \n\n    dataframe['NeighborhoodGroups'] = dataframe['Neighborhood'].map({\n        'MeadowV': 0,\n        'IDOTRR': 1,\n        'BrDale': 1,\n        'OldTown': 1,\n        'Edwards': 1,\n        'BrkSide': 1,\n        'Sawyer': 1,\n        'Blueste': 1,\n        'SWISU': 2,\n        'NAmes': 2,\n        'NPkVill': 2,\n        'Mitchel': 2,\n        'SawyerW': 2,\n        'Gilbert': 2,  \n        'NWAmes': 2,   \n        'Blmngtn': 2,  \n        'CollgCr': 2,  \n        'ClearCr': 3,  \n        'Crawfor': 3,  \n        'Veenker': 3,  \n        'Somerst': 3,  \n        'Timber': 3,   \n        'StoneBr': 4, \n        'NoRidge': 4, \n        'NridgHt': 4,\n    })\n\n    dataframe['HasFireplace'] = dataframe['Fireplaces'].apply(lambda x: int(x > 0))\n\n    dataframe['TotalBathrooms'] = (dataframe['FullBath'] + (0.5 * dataframe['HalfBath']) +\n                                dataframe['BsmtFullBath'] + (0.5 * dataframe['BsmtHalfBath']))\n\n    dataframe['IsPavedDrive'] = (dataframe['PavedDrive'] == 'Y') * 1\n\n    year_built_bins = np.linspace(1871, 2010, 10) \n    dataframe['YearBuiltBin'] = pd.cut(dataframe['YearBuilt'], bins=year_built_bins, labels=range(1, 10)) \n\n    dataframe['HouseAge'] = dataframe['YrSold'] - dataframe['YearBuilt'].astype('int')\n    dataframe['IsRecentlyBuilt'] = (full_df['YearBuilt'] == full_df['YrSold'].astype(int)) * 1\n    dataframe['IsRecentlyRemod'] = (full_df['YearRemodAdd'] == full_df['YrSold'].astype(int)) * 1 # check it with a new model\n\n    dataframe['ExterQC'] = dataframe['ExterQualEnc'] + dataframe['ExterCondEnc']\n\n    dataframe['FunctionalGroup'] = dataframe['Functional'].map({\n        'Typ': 2,\n        'Min1': 1,\n        'Min2': 1,\n        'Mod': 1,\n        'Maj1': 0,\n        'Maj2': 0,\n        'Sev': 0,\n        'Sav': 0,\n    })\n\n    dataframe['IsGasHeating'] = dataframe['Heating'].map({'GasA': 1, 'GasW': 1, 'Grav': 0, 'Wall': 0, 'OthW': 0, 'Floor': 0})\n    dataframe['IsHeatingGood'] = dataframe['HeatingQC'].map({'Po': 0, 'Fa': 0, 'TA': 0, 'Gd': 1, 'Ex': 1})\n\n    dataframe['IsNewElectrBreakers'] = dataframe['Electrical'].map({'SBrkr': 1, 'FuseF': 0, 'FuseA': 0, 'FuseP': 0, 'Mix': 0})\n\n    dataframe['IsGarageCondGood'] = dataframe['GarageCond'].map({'None': 0, 'Po': 0, 'Fa': 0, 'TA': 1, 'Gd': 1, 'Ex': 1})\n\n    dataframe['RoofMatlCost'] = dataframe['RoofMatl'].map({\n        'CompShg': 0,\n        'WdShake': 1,\n        'ClyTile': 1,\n        'WdShngl': 1,\n        'Roll': 0,\n        'Metal': 1,\n        'Membran': 0,\n        'Tar&Grv': 0,\n    })\n    \n    dataframe['IsWoodenRoof'] = dataframe['RoofMatl'].map({\n        'CompShg': 0,\n        'WdShake': 1,\n        'ClyTile': 0,\n        'WdShngl': 1,\n        'Roll': 0,\n        'Metal': 0,\n        'Membran': 0,\n        'Tar&Grv': 0,\n    })\n\n    dataframe['IsAdjArterialStreat'] = ((dataframe['Condition1'] == 'Artery') | (dataframe['Condition2'] == 'Artery')) * 1\n    dataframe['IsAdjFeederStreat'] = ((dataframe['Condition1'] == 'Feedr') | (dataframe['Condition2'] == 'Feedr')) * 1\n    dataframe['IsNormalCondition'] = ((dataframe['Condition1'] == 'Norm') | (dataframe['Condition2'] == 'Norm')) * 1\n    dataframe['IsAjdOffSiteFeature'] = ((dataframe['Condition1'] == 'PosA') | (dataframe['Condition2'] == 'PosA')) * 1\n    dataframe['IsNearOffSiteFeature'] = ((dataframe['Condition1'] == 'PosN') | (dataframe['Condition2'] == 'PosN')) * 1\n    dataframe['IsNearRailroad'] = ((dataframe['Condition1'].isin(['RRNn', 'RRNe'])) | (dataframe['Condition2'].isin(['RRNn', 'RRNe']))) * 1\n    dataframe['IsAdjRailroad'] = ((dataframe['Condition1'].isin(['RRAn', 'RRAe'])) | (dataframe['Condition2'].isin(['RRAn', 'RRAe']))) * 1\n\n    dataframe['WoodDeckGroups'] = pd.cut(dataframe['WoodDeckSF'], bins=[-1, 1, 200, 500, 2000], labels=[0, 1, 2, 3])\n    dataframe['HasEnclosedPorch'] = (dataframe['EnclosedPorch'] > 0) * 1\n    dataframe['HasScreenPorch'] = (dataframe['ScreenPorch'] > 0) * 1\n    dataframe['HasEnclosedPorch'] = (dataframe['EnclosedPorch'] > 0) * 1\n\n    dataframe['Shed'] = (dataframe['MiscFeature'] == 'Shed') * 1 * full_df['MiscVal']\n\n    dataframe['Season'] = dataframe['MoSold'].map({\n        12: 0, 1: 0, 2: 0,\n        3: 1, 4: 1, 5: 1, \n        6: 2, 7: 2, 8: 2, \n        9: 3, 10: 3, 11: 3,\n    })\n\n    dataframe['InflationFactor'] = dataframe['YrSold'] - 2006","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I had to keep feature engineering and preceding steps outside of sklearn pipelines. This is due to the current limitations of the pipeline API which would made the code less readable and clean.\n\nI have not used any automatic technics for feature engineering and selection, but manually selected features based on my understanding of the domain and EDA:"},{"metadata":{"trusted":false},"cell_type":"code","source":"features = [\n    'GrLivArea',\n    '1stFlrSF',\n    '2ndFlrSF',\n    'LotArea',\n    'BsmtFinSF1', \n    'BsmtFinSF2',\n    'BsmtUnfSF', \n    'BsmtFinType1Enc',\n    'BsmtFinType2Enc',\n    'GarageCars',\n    'OverallCond', \n    'Neighborhood',\n    'LotShape',\n    'LandSlope',\n    'BsmtCondEnc',\n    'BsmtQualEnc', \n    'SaleCondition',\n    'CentralAirEnc',\n    'IsAdjArterialStreat',\n    'IsAdjFeederStreat',\n    'IsNormalCondition',\n    'IsNearOffSiteFeature',\n    'IsAjdOffSiteFeature',\n    'IsAdjRailroad',\n    'TotalBathrooms',\n    'GarageFinishEnc',\n    'KitchenQualEnc',\n    'BedroomAbvGr',\n    'MSZoning',\n    'IsRecentlyBuilt',\n    'LandContour',\n    'HasFireplace',\n    'FunctionalGroup',\n    'HouseAge',\n    'FenceEnc',\n    'IsGasHeating',\n    'IsHeatingGood',\n    'IsNewElectrBreakers',\n    'IsGarageCondGood',\n    'IsWoodenRoof',\n    'RoofMatlCost',\n    'BldgType',\n    'HouseStyle',\n    'MasVnrType',\n    'TotRmsAbvGrd',\n    'WoodDeckGroups',\n    'HasEnclosedPorch',\n    'YearBuiltBin',\n    'HasScreenPorch',\n    'AvgRoomSF',\n    'BsmtExposureEnc',\n    'Shed',\n    'Season',\n    'NeighborhoodGroups',\n    'OverallHouseQC',\n    'ExterQC',\n    'InflationFactor',\n    'IsPavedDrive',\n]\n\nX = train_df[features]\nY = train_df['SalePrice']\n\nx_test = test_df[features]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{},"cell_type":"markdown","source":"During experiments, I tried different models: RandomForest, Lasso, Ridge, GradientBoosting, HistGradientBoosting, VotingRegressor, StackRegressor. So far XGBoost have showed the best results for my feature set:"},{"metadata":{"trusted":false},"cell_type":"code","source":"logTransformer = FunctionTransformer(func=np.log1p, inverse_func=np.expm1)\n\nfeatureTransformer = ColumnTransformer([\n        ('log_scaling', logTransformer, ['GrLivArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'LotArea', 'AvgRoomSF', 'Shed', 'TotRmsAbvGrd']),\n        ('neighborhood_onehot', OneHotEncoder(categories=[neighborhoodCategories]), ['Neighborhood']),\n        ('neighborhood_grp_onehot', OneHotEncoder(), ['NeighborhoodGroups']),\n        ('lot_shape_onehot', OneHotEncoder(categories=[lotShapeCategories]), ['LotShape']),\n        ('land_slope_onehot', OneHotEncoder(categories=[landSlopeCategories]), ['LandSlope']),\n        ('sale_condition_onehot', OneHotEncoder(categories=[saleCondCategories]), ['SaleCondition']),\n        ('land_contour_onehot', OneHotEncoder(), ['LandContour']),\n        ('zoning_onehot', OneHotEncoder(), ['MSZoning']),\n        ('bldg_type_onehot', OneHotEncoder(), ['BldgType']),\n        ('masvrn_type_onehot', OneHotEncoder(), ['MasVnrType']),\n        ('house_style_onehot', OneHotEncoder(), ['HouseStyle']),\n        ('season_onehot', OneHotEncoder(), ['Season']),\n    ],\n    remainder='passthrough'\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\nfrom xgboost import XGBRegressor\n\nxgb_model = XGBRegressor(\n    max_depth=6,\n    n_estimators=8000,\n    learning_rate=0.01,\n    min_child_weight=1.5,\n    subsample=0.2,\n    gamma=0.01,\n    reg_alpha=1,\n    reg_lambda=0.325,\n    objective='reg:gamma',\n    booster='gbtree'\n)\n\nxgb_pipeline = Pipeline([\n    ('preprocessing', featureTransformer),\n    ('xgb_regressor', xgb_model),\n])\n\nprint('XGB Regressor:')\nscore_model(xgb_pipeline, X, Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ames dataset is pretty small for the amount of features it contains. I have used 2-folds cross-validation to evaluate the model and find hyperparams. Two folds should help to ensure that we have enough samples in validation fold to get more-less meaningfull feedback about model changes."},{"metadata":{},"cell_type":"markdown","source":"## Debugging\n\nTo inspect model results and affect of different adjustments, I have collected information about model feature importance and used ELI5 permutation importance as an additional source of information:"},{"metadata":{"trusted":false},"cell_type":"code","source":"xgb_pipeline.fit(X, Y)\nX_columns = get_columns_from_transformer(xgb_pipeline.named_steps['preprocessing'], list(X.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"features_list = sorted(zip(xgb_pipeline.named_steps['xgb_regressor'].feature_importances_, X_columns), reverse=True)\nfeatures_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from eli5.sklearn import PermutationImportance\n\ntransformed_X = xgb_pipeline.named_steps['preprocessing'].transform(X)\n\npermutation_importance = PermutationImportance(\n    xgb_model, \n    scoring=make_scorer(neg_rmsle),\n    cv=2,\n    random_state=42,\n).fit(transformed_X, Y)\n\neli5.show_weights(permutation_importance, feature_names=X_columns, top=125)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hypetuning\n\nParameter fine-tuning is quite time-consuming process. I have used GrigSearch approach to find parameter baseline set and than kind of manually optimized them from that point. My best shot is used in the XGB params above."},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\nparameters = {\n    'xgb_regressor__objective': ['reg:gamma'], # 'reg:squarederror', 'reg:squaredlogerror'\n    'xgb_regressor__learning_rate': [0.01],\n    'xgb_regressor__n_estimators': [7900, 8000, 8100],\n    'xgb_regressor__max_depth': [11, 12, 13],\n    'xgb_regressor__booster': ['gbtree'],\n    'xgb_regressor__min_child_weight': [1.5],\n    'xgb_regressor__gamma': [0],\n    'xgb_regressor__subsample': [0.2],\n    'xgb_regressor__reg_alpha': [0, 0.9, 1],\n    'xgb_regressor__reg_lambda': [1, 0.3],\n}\n\nparamSearch = GridSearchCV(\n   estimator=xgb_pipeline,\n   scoring=make_scorer(neg_rmsle),\n   param_grid=parameters,\n   cv=2,\n   n_jobs=-1, \n   verbose=3\n)\n\n#paramSearch.fit(X, Y)\n#paramSearch.best_params_, paramSearch.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict Submissions\n\nFinally, it's time to check model generalization with Kaggle submissions. I was able to get to **Top 4%** (Dec 2020) with a Kaggle RMLE **0.11617**:"},{"metadata":{"trusted":false},"cell_type":"code","source":"xgb_pipeline.fit(X, Y)\n\ny_test_predicted = xgb_pipeline.predict(x_test)\ny_test_predicted = np.rint(y_test_predicted).astype(int)\n\nsubmission_df = pd.DataFrame({\n    'Id': test_df['Id'],\n    'SalePrice': y_test_predicted,\n})\n\nsubmission_df.to_csv('./submission_xgb.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary\n\nI have spent a few weeks on this task as part of my learn by doing plan and it was definetly worth it.\n\nIt's easy to create a baseline model and it's hard to go from that point to model that would be presice enough to be useful. For this task, XGBoost model was super helpul and allowed me to cross Top 25%. The rest of the improvements was brought by extensive feature engineering (and some by hypetuning).\n\n**If you find this notebook helpfull, feel free to upvote** ðŸ’«"},{"metadata":{},"cell_type":"markdown","source":"## Reference:\n- https://www.kaggle.com/glushko/house-prices-domain-driven-eda-part-i\n- https://www.kaggle.com/humananalog/xgboost-lasso\n- https://www.kaggle.com/cerberus4229/voting-regressor-with-pipelines"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}