{"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center> A Neural Network Model for House Prices","metadata":{"_uuid":"ca3198b1525214a53f33a933ddf0f22ac7963468","_cell_guid":"c9d041f0-a7fe-46ea-8c84-821572ccd9a3"}},{"cell_type":"markdown","source":"The purpose of this notebook is to build a model (Deep Neural Network) with Keras over Tensorflow. We will see the differents steps to do that. This notebook is split in several parts:\n\n- I.    Importation & Devices Available\n- II.   Outliers\n- III.  Preprocessing\n- IV.   KerasRegressor for Contiunuous features\n- V.    Predictions\n","metadata":{"_uuid":"5608ad498d10e44e2126587e0b654dbbef918b42","_cell_guid":"0adcac33-8276-4f86-a647-f2199e9d5ccf"}},{"cell_type":"markdown","source":"# <center> I. Importation & Devices Available","metadata":{"_uuid":"bd03c1594867b43b3f3494591b788d8eb9eab920","_cell_guid":"78d12eeb-72ea-43b0-b503-c8b9842e7d7b"}},{"cell_type":"code","execution_count":null,"source":"import os\nimport keras","metadata":{"_uuid":"ff972448f235077543f3a0f6b9391bcea18329b6","_cell_guid":"90b67cbc-419a-4fc8-9a91-8d8dc7f34396"},"outputs":[]},{"cell_type":"markdown","source":"In this tutorial our data is composed to 1460 row with 81 features. 38 continuous features and 43 categorical features. As exposed in the introduction we will use onlly the continuous features to build our first model.\n\nHere the objective is to predict the House Prices. In this case we have a regression model to build.\nSo our first data we will contain 37 features to explain the 'SalePrice'. We can see the list of features that we will use to build our first model.","metadata":{"_uuid":"348514ad519851c2dc7e51dc04acdbe7d391d6fc","_cell_guid":"cdb671d1-2803-4290-8e8b-a54a00d21d44"}},{"cell_type":"code","execution_count":null,"source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\nimport matplotlib\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\ntrain = pd.read_csv('../input/train.csv')\nprint('Shape of the train data with all features:', train.shape)\ntrain = train.select_dtypes(exclude=['object'])\nprint(\"\")\nprint('Shape of the train data with numerical features:', train.shape)\ntrain.drop('Id',axis = 1, inplace = True)\ntrain.fillna(0,inplace=True)\n\ntest = pd.read_csv('../input/test.csv')\ntest = test.select_dtypes(exclude=['object'])\nID = test.Id\ntest.fillna(0,inplace=True)\ntest.drop('Id',axis = 1, inplace = True)\n\nprint(\"\")\nprint(\"List of features contained our dataset:\",list(train.columns))","metadata":{"_uuid":"16485c4c885b6416be79eade3cfff6345861a8bd","_cell_guid":"9e15ad43-8a3a-4840-9c0d-afa2ba2bd148"},"outputs":[]},{"cell_type":"markdown","source":"# <center> II. Outliers","metadata":{"_uuid":"6b90c9ce6240afc8a36106661afe100170ae738a","_cell_guid":"c675ba7f-3d0c-4641-b133-e80711a19cf1"}},{"cell_type":"markdown","source":"In this small part we will isolate the outliers with an IsolationForest (http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html). I tried with and without this step and I had a better performance removing these rows.\n\nI haven't analysed the test set but I suppose that our train set looks like more at our data test without these outliers.\n","metadata":{"_uuid":"ae94ba5f35444c2b46bd5bec0acd4a45e084bfc8","_cell_guid":"7b049d56-52b0-4b6c-8c35-490a57eaea87"}},{"cell_type":"code","execution_count":null,"source":"from sklearn.ensemble import IsolationForest\n\nclf = IsolationForest(max_samples = 100, random_state = 42)\nclf.fit(train)\ny_noano = clf.predict(train)\ny_noano = pd.DataFrame(y_noano, columns = ['Top'])\ny_noano[y_noano['Top'] == 1].index.values\n\ntrain = train.iloc[y_noano[y_noano['Top'] == 1].index.values]\ntrain.reset_index(drop = True, inplace = True)\nprint(\"Number of Outliers:\", y_noano[y_noano['Top'] == -1].shape[0])\nprint(\"Number of rows without outliers:\", train.shape[0])","metadata":{"_uuid":"08498903317073ce7632c3fbff677ae842ca8389","_cell_guid":"599c148c-bee5-424e-80cc-afc1d7b1e3fb"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"train.head(10)","metadata":{"_uuid":"d80cb31a8c00f704879e1c4771f701dd5d8031d0","_cell_guid":"b6056d39-23cd-4821-989a-e6769ad298d8"},"outputs":[]},{"cell_type":"markdown","source":"# <center> III. Preprocessing","metadata":{"_uuid":"546316d0afa2fad5717bb15a11186d2229d38cd7","_cell_guid":"96ba460b-8ca5-4479-b9a8-94c04bd08b53"}},{"cell_type":"markdown","source":"To rescale our data we will use the fonction MinMaxScaler of Scikit-learn. I am wondering if it is not interesting to use the same MinMaxScaler for Train and Test !","metadata":{"_uuid":"4b5d09fd48475d5707699f1725810c01811b61a8","_cell_guid":"445cfd64-2d2f-4218-afb6-b5870d849fb1"}},{"cell_type":"code","execution_count":null,"source":"import warnings\nwarnings.filterwarnings('ignore')\n\ncol_train = list(train.columns)\ncol_train_bis = list(train.columns)\n\ncol_train_bis.remove('SalePrice')\n\nmat_train = np.matrix(train)\nmat_test  = np.matrix(test)\nmat_new = np.matrix(train.drop('SalePrice',axis = 1))\nmat_y = np.array(train.SalePrice).reshape((1314,1))\n\nprepro_y = MinMaxScaler()\nprepro_y.fit(mat_y)\n\nprepro = MinMaxScaler()\nprepro.fit(mat_train)\n\nprepro_test = MinMaxScaler()\nprepro_test.fit(mat_new)\n\ntrain = pd.DataFrame(prepro.transform(mat_train),columns = col_train)\ntest  = pd.DataFrame(prepro_test.transform(mat_test),columns = col_train_bis)\n\ntrain.head()","metadata":{"_uuid":"bdbe7b6ceb041aace9b18fe3849e568f3ad37f99","_cell_guid":"941c0846-55bf-45bb-a49d-be8084b9c056"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"# List of features\nCOLUMNS = col_train\nFEATURES = col_train_bis\nLABEL = \"SalePrice\"\n\n# Columns\nfeature_cols = FEATURES\n\n# Training set and Prediction set with the features to predict\ntraining_set = train[COLUMNS]\nprediction_set = train.SalePrice\n\n# Train and Test \nx_train, x_test, y_train, y_test = train_test_split(training_set[FEATURES] , prediction_set, test_size=0.33, random_state=42)\ny_train = pd.DataFrame(y_train, columns = [LABEL])\ntraining_set = pd.DataFrame(x_train, columns = FEATURES).merge(y_train, left_index = True, right_index = True)\ntraining_set.head()\n\n# Training for submission\ntraining_sub = training_set[col_train]","metadata":{"collapsed":true,"_uuid":"091aee81e454d14b95d61cae7e35a90e4a376ec2","_cell_guid":"0b40bdb6-f813-4b94-94c6-599880f59445"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"# Same thing but for the test set\ny_test = pd.DataFrame(y_test, columns = [LABEL])\ntesting_set = pd.DataFrame(x_test, columns = FEATURES).merge(y_test, left_index = True, right_index = True)\ntesting_set.head()","metadata":{"_uuid":"9985b1ca4bfd4999989af752951676311c4f213e","_cell_guid":"0cd9a504-0fd9-47ca-a247-69a920911e35"},"outputs":[]},{"cell_type":"markdown","source":"# <center> IV. Deep Neural Network for continuous features","metadata":{"_uuid":"44e0b00b9d8718876bbe814f68044918207faeab","_cell_guid":"047f4ae6-7618-42d4-8ad0-4a19fd30ee81"}},{"cell_type":"markdown","source":"With tf.contrib.learn it is very easy to implement a Deep Neural Network. In our example we will have 5 hidden layers with repsectly 200, 100, 50, 25 and 12 units and the function of activation will be Relu.\n\nThe optimizer used in our case is an AdaDelta  optimizer.","metadata":{"_uuid":"b166945d4a4af37865003549116816cf5bd71062","_cell_guid":"0158e8c3-20e7-4a99-ab3a-b470ad4d0f9d"}},{"cell_type":"code","execution_count":null,"source":"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\nseed = 7\nnp.random.seed(seed)\n\n# Model\nmodel = Sequential()\nmodel.add(Dense(200, input_dim=36, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(100, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(50, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(25, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(1, kernel_initializer='normal'))\n# Compile model\nmodel.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adadelta())\n\nfeature_cols = training_set[FEATURES]\nlabels = training_set[LABEL].values\n\nmodel.fit(np.array(feature_cols), np.array(labels), epochs=100, batch_size=10)","metadata":{"_uuid":"6d2f589a8d2be2dd515929f9b9bdcf84736ac96e","_cell_guid":"ed573d14-ffb1-4ae5-9f28-97057c20c6ce"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"# Evaluation on the test set created by train_test_split\nmodel.evaluate(np.array(feature_cols), np.array(labels))","metadata":{"_uuid":"3e21c5eaba39aa7ad09df92c051ef22c5c95fd0d","_cell_guid":"eb9cad74-6898-4bb5-a563-953a060f5448"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"# Predictions\nfeature_cols_test = testing_set[FEATURES]\nlabels_test = testing_set[LABEL].values\n\ny = model.predict(np.array(feature_cols_test))\npredictions = list(itertools.islice(y, testing_set.shape[0]))\n","metadata":{"collapsed":true,"_uuid":"6ba45c1abe834c716c5445dde3b794af05e1aeff","scrolled":true,"_cell_guid":"c5c517b3-6011-4b05-9b34-ded41d82f8e4"},"outputs":[]},{"cell_type":"markdown","source":"# <center> V. Predictions and submission","metadata":{"_uuid":"d75c2f98f4524273133765d146f131624ae7569c","_cell_guid":"87746ee4-64f8-49ba-9a23-85d21261dc6b"}},{"cell_type":"markdown","source":"Let's go to prepare our first submission ! Data Preprocessed: checked ! Outlier excluded: checked ! Model built: : checked! Next step: Used our model to make the predictions with the data set Test. And add one graphic to see the difference between the reality and the predictions.","metadata":{"_uuid":"0cebdd59654ca804beddc4f1a590877e59aa4e8a","_cell_guid":"1a08cbad-7b61-4c37-8686-5cc533d496ac"}},{"cell_type":"code","execution_count":null,"source":"predictions = prepro_y.inverse_transform(np.array(predictions).reshape(434,1))","metadata":{"collapsed":true,"_uuid":"bf2281f14b4bcf56aac14ad691b1729e50cff5d8","_cell_guid":"0fb8ed1b-c0df-41b9-a764-55d675e8aa7c"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"reality = pd.DataFrame(prepro.inverse_transform(testing_set), columns = [COLUMNS]).SalePrice","metadata":{"collapsed":true,"_uuid":"aa904a7f562129ac5f362e39089e6923f78d1acc","_cell_guid":"e4e48b49-f405-4e57-b6cb-587607253aab"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"matplotlib.rc('xtick', labelsize=30) \nmatplotlib.rc('ytick', labelsize=30) \n\nfig, ax = plt.subplots(figsize=(50, 40))\n\nplt.style.use('ggplot')\nplt.plot(predictions, reality, 'ro')\nplt.xlabel('Predictions', fontsize = 30)\nplt.ylabel('Reality', fontsize = 30)\nplt.title('Predictions x Reality on dataset Test', fontsize = 30)\nax.plot([reality.min(), reality.max()], [reality.min(), reality.max()], 'k--', lw=4)\nplt.show()","metadata":{"_uuid":"a11028c04b0a6981169196e2c6fe84c245799b8e","_cell_guid":"450559b2-bff3-427f-8dec-569191bd8c94"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"y_predict = model.predict(np.array(test))\n\ndef to_submit(pred_y,name_out):\n    y_predict = list(itertools.islice(pred_y, test.shape[0]))\n    y_predict = pd.DataFrame(prepro_y.inverse_transform(np.array(y_predict).reshape(len(y_predict),1)), columns = ['SalePrice'])\n    y_predict = y_predict.join(ID)\n    y_predict.to_csv(name_out + '.csv',index=False)\n    \nto_submit(y_predict, \"submission_continuous\")","metadata":{"collapsed":true,"_uuid":"3be76f4c812b3676a6816d8eb9353f049948ac49","_kg_hide-output":false,"_cell_guid":"6008e5e1-29d7-42ac-b81a-d9e761a7e956"},"outputs":[]}],"metadata":{"language_info":{"version":"3.6.3","pygments_lexer":"ipython3","file_extension":".py","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","name":"python"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat_minor":1}