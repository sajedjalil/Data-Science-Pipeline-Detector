{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### Greetings!!!\n#### The Aim of this kernel is to cover Visualization and Data processing."},{"metadata":{},"cell_type":"markdown","source":"> #####     We'll see how to interpret different set of plots for different use case , impute the missing data, find and remove outliers. This helps in producing better model and high accuracy. Let's dive in.\n    \n##### Please upvote if you like this Kernel. Thanks in advance."},{"metadata":{},"cell_type":"markdown","source":"#### Competition description:\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\n"},{"metadata":{},"cell_type":"markdown","source":"## These are the steps We'll focus on this kernel. Let's start !!!\n#### 1. Univariate Analysis\n#### 2. Bivariate Analysis\n#### 3. Imputing missing values\n#### 4. Fix Skewed features\n#### 5. Feature Creation\n#### 6. Feature Transformation\n#### 7. Model Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing necessary models and libraries\n\n#Math tools\nfrom scipy import stats\nfrom scipy.stats import skew,norm  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nimport scipy.stats as stats\n\n\n#Visualizing tools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.patches import Rectangle\nsns.set(style=\"ticks\")\n\n#preprocessing tools\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import LabelEncoder\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\ntrain_size = train.shape[0]\nsubmission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1. Univariate Analysis\n\nOur first step in Machine Learning should always be analyzing the target variable. Saleprice is our given target/dependent variable. \n\nLet's analyse its distribution\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#skewness and kurtosis\nprint(\"Skewness: {}\".format(train['SalePrice'].skew()))\nprint(\"Kurtosis: {}\".format(train['SalePrice'].kurt()))\nprint(\"--------------------------------------\")\nprint(train['SalePrice'].describe())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### skew data makes a model difficult to find a proper pattern in the data that's why we convert skew data into normal(Gaussian) distribution "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Customizing the layout\ndef multi_plot(feature):\n    fig = plt.figure(constrained_layout=True, figsize=(15,10))\n    grid = gridspec.GridSpec(ncols=4, nrows=2, figure=fig)\n\n    ax1 = fig.add_subplot(grid[0, :2])\n    ax1.set_title('Distplot')\n    sns.distplot(train.loc[:,feature], fit=norm,color=\"mediumseagreen\", ax = ax1)\n\n    ax2 = fig.add_subplot(grid[1, :2])\n    ax2.set_title('QQ_plot')\n    stats.probplot(train.loc[:,feature], plot = ax2)\n\n    ax3= fig.add_subplot(grid[0, 2:])\n    ax3.set_title('Scatter Plot')\n    sns.scatterplot(range(train.shape[0]), train[feature].values,color='orangered')\n\n    ax4 = fig.add_subplot(grid[1, 2:])\n    ax4.set_title('Box Plot')\n    sns.boxplot(train.loc[:,feature], orient='v', ax = ax4 , color='darkorange');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multi_plot('SalePrice')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observations:\n- Target variable is not normally distributed.\n- It is rightly skewed. \n- Average sell price is 180921 USD which pulled towards outliers values at the upper end.\n- Median 163000 USD which is lower than Mean value.\n- It has couple of outliers at the upper end."},{"metadata":{},"cell_type":"markdown","source":"#### 2. Bivariate Analysis\n\nLet's analyse how the other independent/predictor/input variable correleated with the Target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(1,2,figsize=(16,4))\nsns.boxplot(train['GrLivArea'],ax=ax[0],color=\"darkorange\")\nplt.scatter(train['GrLivArea'],train['SalePrice'],color='#9b59b6')\n#outlier detection\nplt.axvline(x=4600,color='r')\nplt.xlabel('GrLiveArea')\nplt.ylabel('SalePrice')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### observations :\nWe can see that SalePrice increases along with GrLiveArea but after 4500sq ft, the SalePrice seems below 200000USD which will surely affect our model Predictions. So, We remove it."},{"metadata":{"trusted":true},"cell_type":"code","source":"#outlier removal\ntrain.drop(train[train['GrLivArea']>4500].index,axis=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(1,2,figsize=(16,4))\nsns.boxplot(train['GrLivArea'],ax=ax[0],color=\"springgreen\")\nplt.scatter(train['GrLivArea'],train['SalePrice'],color='limegreen')\nplt.xlabel('GrLiveArea')\nplt.ylabel('SalePrice')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How to plot relationship between numerical data and categorical dataa ?"},{"metadata":{},"cell_type":"markdown","source":"#### Swarmplot :\nJust like their name. A swarm of points plotted for each category with a little dispersion on the y-axis to make them easier to see\n\n#### Rectangle :\nRectangle is a shape available in matplotlib which is used to highlight a point or set of points."},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(1,1,figsize=(16,4))\n# Uncomment the below line and see why Swarmplot was suggested instead of Scatterplot,\n# when using on discrete or categorical variable.\n# sns.scatterplot('OverallQual','SalePrice', data = train)\nsns.swarmplot('OverallQual','SalePrice', data = train , palette=\"Set2\")\n\n\n#outlier detection\nax.add_patch(Rectangle((2.5,200000),1,100000 ,linewidth=5,edgecolor='b',facecolor='none'))\nplt.xlabel('OverallQual')\nplt.ylabel('SalePrice')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observation :\nDistribution shows that,\nLess number of people who bought the house when the OverallQual is less than 4. So they are not interested for lower quality. Even price looks cheaper people prefer Quality.\n\nMost of the people bought the house between the OverallQual 4 to 8. There is a slight increase in price When quality increase. So money and quality balanced here.\n\nLess number of people who bought the the house when the OverallQual are at top 9 and 10. There is drastic change in price when the quality is at 10. People shows less interest because Price seems too high."},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_index = train[(train['OverallQual'] == 4) & (train['SalePrice'] > 200000)].index\n\n# *outlier\n# We can see OverallQual increases along with SalePrice and the pattern shows that each qual level covers the previous levels completely \n# but on index 457(highlighted rectangle) looks different. so we will remove it .\n\n# outlier removal\ntrain.drop(outlier_index,axis=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding numeric features\nnumeric_cols = train.select_dtypes(exclude='object').columns\nnumeric_cols_length = len(numeric_cols)  \n\nfig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 120))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\n\n# skiped Id and saleprice feature\nfor i in range(1,numeric_cols_length-1):\n    feature = numeric_cols[i]\n    plt.subplot(numeric_cols_length, 3, i)\n    sns.scatterplot(x=feature, y='SalePrice', data=train,color='crimson')\n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n           \nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observations:\n\n- MSSubClass,MoSold,YrSold - patterns shows it as a category and description meant the same\n- OverallQual, OverallCond - Ordered value (like ratings)\n- BsmtFullBath,BsmtHalfBath,FullBath,HalfBath,Fireplaces,BedroomAbvGr,KitchenAvbGr - discrete value(no. of bathrooms)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = train.select_dtypes(include='number').corr()\nplt.figure(figsize=(16,6))\ncorr_saleprice = corr['SalePrice'].sort_values(ascending=False)[1:]\nax = sns.barplot(corr_saleprice.index,corr_saleprice.values)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Zoomed Heat Map\nZoomed heat map is nothing but a heat map with n number of features. This will help in finding the top correlated features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Correlation of top 10 feature with saleprice\ncorWithSalePrice = train.corr().nlargest(10,'SalePrice')['SalePrice'].index\nf , ax = plt.subplots(figsize = (18,12))\ncorr = train[corWithSalePrice].corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    ax = sns.heatmap(corr, mask=mask, vmax=0.8,square=True,annot=True,cmap=\"YlGnBu\")\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observations\n\n- OverallQual, GrLivArea , GarageCars , GarageArea and TotalBsmtSF are strongly correlated with the SalePrice.\n- GarageCas and GarageArea are strongly correlated this is because of parking the Garage cars at the GarageArea.\n- TotRmsAbvGrd and GrLivArea are strongly correleated. When the Ground area increases count of rooms also increases.\n- TotalBsmtSF and 1stFlrSF are strongly correleated."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot('TotalBsmtSF','1stFlrSF',data=train,height=7, aspect=1.6, palette=\"Set1\" , \n           line_kws={'color': 'salmon'},scatter_kws={'color': 'turquoise'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observations\n- TotalBsmtSF and 1stFlrSF are strongly correleated."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nsns.boxplot(x='OverallQual', y='TotalBsmtSF',data=train,palette='Set2')\nsns.swarmplot(x='OverallQual', y='TotalBsmtSF',data=train,palette='Set1')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Stripplot :\nStripplot is basically a scatter plot where the x axis represents a categorical variable. It's function is similar to Swarmplot(). Unlike Swarmplot, you can see the points overlap.\n###### This will give clear picture since there is a separation between each point"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(1,2,figsize=(16,4))\nsns.stripplot('TotRmsAbvGrd','GrLivArea',data=train,ax=ax[0])\nsns.stripplot('GarageCars','GarageArea',data=train,ax = ax[1], palette=\"Set2\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observations : \n* When the GrLiveArea increase obviously Builder builds more rooms to sell it in high price.\n* When the GarageArea increases People can able to park more cars.\n* When there is no GarageArea then people can't park their cars on Garage."},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(1,1,figsize=(16,4))\nsns.scatterplot(x='LotArea', y='SalePrice',data=train,color='mediumspringgreen')\n\n#outlier detection\nax.add_patch(Rectangle((200000,320000),25000,100000 ,linewidth=5,edgecolor='orangered',facecolor='none'))\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observations:\n* When LotArea increases Saleprice also increases exponentially. But after 100000sq area, price seems average, this might be bacause of building age. Let's analyse its Age."},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_index = train[(train['LotArea'] > 200000) & (train['SalePrice'] > 300000)]\nyear_diff = outlier_index['YearBuilt'] - outlier_index['YrSold']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(1,1,figsize=(16,4))\nsns.scatterplot(x='LotArea', y='SalePrice',data=train[(train['YrSold'] - train['YearBuilt']) > 40 ],color='mediumseagreen')\nax.add_patch(Rectangle((150000,200000),70000,190000 ,linewidth=5,edgecolor='orangered',facecolor='none'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# *outlier\n# Surprisingly, We got two more outlier. \n# Filtered data are more than 40 years older buildings.\n# Most of the values plotted within 100000 area. So, it is safe to remove these outliers.\n# outlier removal\noutlier_index = train[(train['LotArea'] > 150000) & (train['SalePrice']>200000) & (train['SalePrice'] < 400000) ].index\ntrain.drop(outlier_index,axis=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(\"TotalBsmtSF\", \"SalePrice\", data=train,height=8,color='rebeccapurple')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations : \n*     90% of the houses has less than 2500 TotalBsmtSf and price less than 400000 USD\n*     This shows linear relationship"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot('TotalBsmtSF','SalePrice',data=train,color='dodgerblue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(1,3,figsize=(16,4))\nsns.pointplot(x=train[\"Alley\"], y=train[\"SalePrice\"],jitter=True,ax=ax[0],color='orchid');\nsns.boxplot(x='Alley', y='SalePrice',data=train,ax=ax[1],palette='Set2')\nsns.stripplot(x='Alley', y='SalePrice',data=train,ax=ax[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pie chart\nmszoning = train['MSZoning'].value_counts()\nlabels = mszoning.index\nsizes = mszoning.values\nexplode = (0.1, 0, 0, 0,0)\ncolors = ['yellowgreen', 'gold', 'lightskyblue', 'lightcoral','red']\npatches, texts = plt.pie(sizes, colors=colors,explode=explode, shadow=True, startangle=90)\nplt.legend(patches, labels, loc=\"best\")\nplt.axis('equal')\nplt.tight_layout()\nplt.show()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations:\n* RL occupies more 75% of the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Donut chart\nPavedDrive = train['PavedDrive'].value_counts()\nlabels = PavedDrive.index\nsizes = PavedDrive.values\nexplode = (0.05,0.05,0.05)\nfig1, ax1 = plt.subplots()\npatches = ax1.pie(sizes, pctdistance=0.8,explode = explode, colors = colors, labels=labels, autopct='%1.1f%%', startangle=90)\n#draw circle\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n# Equal aspect ratio ensures that pie is drawn as a circle\nax1.axis('equal')  \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations :\n* So 91% of the houses was build with paved"},{"metadata":{"trusted":true},"cell_type":"code","source":"# After removing the some of outliers, you can see that skewness is reduced. Still Saleprice is not normally distributed.\nmulti_plot('SalePrice')\n#skewness and kurtosis\nprint(\"Skewness: {}\".format(train['SalePrice'].skew()))\nprint(\"Kurtosis: {}\".format(train['SalePrice'].kurt()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Log - transformation\ntrain['SalePrice_log'] = np.log1p(train['SalePrice'])\nmulti_plot('SalePrice_log')\n#skewness and kurtosis\nprint(\"Skewness: {}\".format(train['SalePrice_log'].skew()))\nprint(\"Kurtosis: {}\".format(train['SalePrice_log'].kurt()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this will remove the overfitted features\n\ndef remove_overfit_features(df,weight):\n    overfit = []\n    for i in df.columns:\n        counts = df[i].value_counts()\n        zeros = counts.iloc[0]\n        if zeros / len(df) * 100 > weight:\n            overfit.append(i)\n    overfit = list(overfit)\n    return overfit\n\n\noverfitted_features = remove_overfit_features(train,99)\ntrain.drop(overfitted_features,inplace=True,axis=1)\ntest.drop(overfitted_features,inplace=True,axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = train['SalePrice_log']\ntrain_features = train.drop(['SalePrice','SalePrice_log'], axis=1)\ntest_features = test\n\n# Combine train and test features in order to apply the feature transformation pipeline to the entire dataset\nall_features = pd.concat([train_features, test_features]).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features.drop('Id',inplace=True,axis=1)\nall_features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3. Imputing missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualize missing data\nmissing_value = all_features.isnull().sum().sort_values(ascending=False) / len(all_features) * 100\nmissing_value = missing_value[missing_value != 0]\nmissing_value = pd.DataFrame({'Missing value' :missing_value,'Type':missing_value.index.map(lambda x:all_features[x].dtype)})\nmissing_value.plot(kind='bar',figsize=(16,4))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total No. of missing value {} before Imputation\".format(sum(all_features.isnull().sum())))\ndef fill_missing_values():\n \n    fillSaleType = all_features[all_features['SaleCondition'] == 'Normal']['SaleType'].mode()[0]\n    all_features['SaleType'].fillna(fillSaleType,inplace=True)\n\n    fillElectrical = all_features[all_features['Neighborhood']=='Timber']['Electrical'].mode()[0]\n    all_features['Electrical'].fillna(fillElectrical,inplace=True)\n\n    exterior1_neighbor = all_features[all_features['Exterior1st'].isnull()]['Neighborhood'].values[0]\n    fillExterior1 = all_features[all_features['Neighborhood'] == exterior1_neighbor]['Exterior1st'].mode()[0]\n    all_features['Exterior1st'].fillna(fillExterior1,inplace=True)\n\n    exterior2_neighbor = all_features[all_features['Exterior2nd'].isnull()]['Neighborhood'].values[0]\n    fillExterior2 = all_features[all_features['Neighborhood'] == exterior1_neighbor]['Exterior1st'].mode()[0]\n    all_features['Exterior2nd'].fillna(fillExterior2,inplace=True)\n\n    bsmtNeigh = all_features[all_features['BsmtFinSF1'].isnull()]['Neighborhood'].values[0]\n    fillBsmtFinSf1 = all_features[all_features['Neighborhood'] == bsmtNeigh]['BsmtFinSF1'].mode()[0]\n    all_features['BsmtFinSF1'].fillna(fillBsmtFinSf1,inplace=True)\n\n    kitchen_grade = all_features[all_features['KitchenQual'].isnull()]['KitchenAbvGr'].values[0]\n    fillKitchenQual = all_features[all_features['KitchenAbvGr'] == kitchen_grade]['KitchenQual'].mode()[0]\n    all_features['KitchenQual'].fillna(fillKitchenQual,inplace=True)\n        \n    all_features['MSZoning'] = all_features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n       \n    all_features['LotFrontage'] = all_features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n    \n    for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','BsmtQual', 'BsmtCond', 'BsmtExposure',\n                'BsmtFinType1', 'BsmtFinType2','PoolQC']:\n        all_features[col] = all_features[col].fillna('None')\n    \n    categorical_cols =  all_features.select_dtypes(include='object').columns\n    all_features[categorical_cols] = all_features[categorical_cols].fillna('None')\n    \n    numeric_cols = all_features.select_dtypes(include='number').columns\n    all_features[numeric_cols] = all_features[numeric_cols].fillna(0)\n    \n    all_features['Shed'] = np.where(all_features['MiscFeature']=='Shed', 1, 0)\n    \n    #GarageYrBlt -  missing values there for the building which has no Garage, imputing 0 makes huge difference with other buildings,\n    #imputing mean doesn't make sense since there is no Garage. So we'll drop it\n    all_features.drop(['GarageYrBlt','MiscFeature'],inplace=True,axis=1)\n    \n    all_features['QualitySF'] = all_features['GrLivArea'] * all_features['OverallQual']\n\nfill_missing_values()\n\nprint(\"Total No. of missing value {} after Imputation\".format(sum(all_features.isnull().sum())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features = all_features.drop(['PoolQC',], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4. Fix Skewed features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting some numeric features to string\nall_features['MSSubClass'] = all_features['MSSubClass'].apply(str)\nall_features['YrSold'] = all_features['YrSold'].astype(str)\nall_features['MoSold'] = all_features['MoSold'].astype(str)\n\n\n# Filter the skewed features\nnumeric = all_features.select_dtypes(include='number').columns\nskew_features = all_features[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize skewed features using boxcox\nfor i in skew_index:\n    all_features[i] = boxcox1p(all_features[i], boxcox_normmax(all_features[i] + 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5. Feature Creation"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features['YearsSinceRemodel'] = all_features['YrSold'].astype(int) - all_features['YearRemodAdd'].astype(int)\nall_features['Total_Home_Quality'] = all_features['OverallQual'] + all_features['OverallCond']\n\nall_features['TotalSF'] = all_features['TotalBsmtSF'] + all_features['1stFlrSF'] + all_features['2ndFlrSF']\nall_features['YrBltAndRemod'] = all_features['YearBuilt'] + all_features['YearRemodAdd']\nall_features['BsmtFinType1_Unf'] = 1*(all_features['BsmtFinType1'] == 'Unf')\nall_features['Total_sqr_footage'] = (all_features['BsmtFinSF1'] + all_features['BsmtFinSF2'] +\n                                 all_features['1stFlrSF'] + all_features['2ndFlrSF'])\nall_features['Total_Bathrooms'] = (all_features['FullBath'] + (0.5 * all_features['HalfBath']) +\n                               all_features['BsmtFullBath'] + (0.5 * all_features['BsmtHalfBath']))\nall_features['Total_porch_sf'] = (all_features['OpenPorchSF'] + all_features['3SsnPorch'] +\n                              all_features['EnclosedPorch'] + all_features['ScreenPorch'] +\n                              all_features['WoodDeckSF'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def booleanFeatures(columns):\n    for col in columns:\n        all_features[col+\"_bool\"] = all_features[col].apply(lambda x: 1 if x > 0 else 0)\nbooleanFeatures(['GarageArea','TotalBsmtSF','2ndFlrSF','Fireplaces','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch'])  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def logs(columns):\n    for col in columns:\n        all_features[col+\"_log\"] = np.log(1.01+all_features[col])  \n\nlog_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                 'TotalBsmtSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                 'TotRmsAbvGrd','Fireplaces','GarageCars','WoodDeckSF','OpenPorchSF',\n                 'EnclosedPorch','3SsnPorch','ScreenPorch','MiscVal','YearRemodAdd','TotalSF']\n\nlogs(log_features)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def squares(columns):\n    for col in columns:\n        all_features[col+\"_sq\"] =  all_features[col] * all_features[col]\n\nsquared_features = ['GarageCars_log','YearRemodAdd', 'LotFrontage_log', 'TotalBsmtSF_log', '2ndFlrSF_log', 'GrLivArea_log' ]\n\nsquares(squared_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6. Feature Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# There is a natural order in their values for few categories, so converting them to numbers gives more meaning\nquality_map = {'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5}\nquality_cols = ['BsmtQual', 'BsmtCond','ExterQual', 'ExterCond','FireplaceQu','GarageQual', 'GarageCond','KitchenQual','HeatingQC']\nfor col in quality_cols:\n    all_features[col] = all_features[col].replace(quality_map)\n\nall_features['BsmtExposure'] = all_features['BsmtExposure'].replace({\"No\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3})\n\nall_features[\"PavedDrive\"] =all_features[\"PavedDrive\"].replace({\"N\" : 0, \"P\" : 1, \"Y\" : 2})\n\nbsmt_ratings = {\"None\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \"ALQ\" : 5, \"GLQ\" : 6}\nbsmt_col = ['BsmtFinType1','BsmtFinType2']\nfor col in bsmt_col:\n    all_features[col] = all_features[col].replace(bsmt_ratings)\n\n    \nall_features[\"OverallScore\"]   = all_features[\"OverallQual\"] * all_features[\"OverallCond\"]\nall_features[\"GarageScore\"]    = all_features[\"GarageQual\"] * all_features[\"GarageCond\"]\nall_features[\"ExterScore\"]     = all_features[\"ExterQual\"] * all_features[\"ExterCond\"]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features = pd.get_dummies(all_features).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = all_features.iloc[:len(train_labels), :]\nX_test = all_features.iloc[len(train_labels):, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"overfitted_features = remove_overfit_features(X,99)\nX = X.drop(overfitted_features, axis=1)\nX_test = X_test.drop(overfitted_features, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### 7. Model Prediction"},{"metadata":{},"cell_type":"markdown","source":"I have written a kernel for complete analysis on House pricing prediction, that also has model training and prediction. Please check that notebook https://www.kaggle.com/johnwill225/extensive-exploratory-data-analysis and leave your feedback.\nPlease upvote if you like this Kernel. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}