{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://www.cursio.com.au/wp-content/uploads/2016/04/house-prices-blog-post-financial-advice-melbourne.jpg\" class=\"center\" width=\"700px\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# House prices: Voting, Stacking and Belnding","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Sumary\n\n<ul>\n  <li>EDA</li>\n  <li>Data Engineering</li>\n  <li>ML</li>\n  <li>Vote and Stack</li>\n  <li>Blending</li>  \n</ul>\n\n<b> Upvote if you enjoy it =)</b>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Importing Packages ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing on packages\n\n!pip install pyod\n# Unbeatable pandas for data\nimport pandas as pd\n\n# Some packages for visualisation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Calculus\nimport numpy as np\nfrom scipy.stats import norm, skew\nimport math\n\n# ML for the win (y)\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nimport xgboost as xgb\nfrom mlxtend.regressor import StackingRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\n\n# Outlier detector\nfrom pyod.models.knn import KNN\n\n# scalers\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.preprocessing import RobustScaler\n\n# Utilities for fun\nimport operator\nfrom sklearn.manifold import TSNE\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Face up with Dataframe. show us all what u got\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read scv files\ndf_train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ndf_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\ndf_output = pd.read_csv('../input/house-prices-advanced-regression-techniques/sample_submission.csv')\ndf_train['SalePrice'] = df_train['SalePrice'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA\n\n<b>Relationship with categorical features</b>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nfig1, axs1 = plt.subplots(ncols=2, nrows = 6, figsize=(30,30))\nfor ax in fig1.axes:\n    plt.sca(ax)\n    plt.xticks(rotation=40)\nfig1.subplots_adjust(wspace=0.2, hspace=0.5)\nsns.barplot(x='Neighborhood', y='SalePrice', data=df_train, ax=axs1[0][0]) \nsns.barplot(x='MSSubClass', y='SalePrice', data=df_train, ax=axs1[0][1])\nsns.barplot(x='MSZoning',y='SalePrice', data=df_train, ax=axs1[1][0])\nsns.barplot(x='Condition1', y='SalePrice', data=df_train, ax=axs1[1][1])\nsns.barplot(x='Condition2', y='SalePrice', data=df_train, ax=axs1[2][0])\nsns.barplot(x='Exterior1st',y='SalePrice', data=df_train, ax=axs1[2][1])\nsns.barplot(x='Exterior2nd', y='SalePrice', data=df_train, ax=axs1[3][0])\nsns.barplot(x='Heating', y='SalePrice', data=df_train, ax=axs1[3][1])\nsns.barplot(x='GarageType',y='SalePrice', data=df_train, ax=axs1[4][0])\nsns.barplot(x='MiscFeature', y='SalePrice', data=df_train, ax=axs1[4][1])\nsns.barplot(x='SaleType', y='SalePrice', data=df_train, ax=axs1[5][0])\nsns.barplot(x='SaleCondition',y='SalePrice', data=df_train, ax=axs1[5][1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Let's see</b>\n<ul>\n<li> Well the Neighborhoods Noridge, Nridght and StoneBr are the most expensive they surely have a starbucks and a yoga club.</li>\n\n<li>The partial payment is more expensive because it is a partial payment that it is expensive or it is the houses that are expensive then the buyers are obliged to pay partially? Hmm ... you know what, I will wait for the sherlock's notebook I will have answers.</li>\n\n<li>The Floating Village Residential and Residential Low Density are the most expensive, people love peace and be at the lake.</li>\n</ul>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<b>Relationship with numerical variables</b>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n# sns.set(font_scale=2)\n# sns.set_context(\"paper\", rc={\"font.size\":10,\"axes.titlesize\":10,\"axes.labelsize\":10}) \nfig2, axs2 = plt.subplots(ncols=1, nrows = 3, figsize=(12,10))\nfig2.tight_layout()\n# axs.tick_params(labelrotation=45)\nsns.scatterplot(x='GrLivArea',y='SalePrice', data=df_train, ax=axs2[0], s=30)\nsns.scatterplot(x='TotalBsmtSF', y='SalePrice', data=df_train, ax=axs2[1], s=30)\nsns.scatterplot(x='YearBuilt', y='SalePrice', data=df_train, ax=axs2[2], s=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a positive linear relation between living area size, basement area size and Original construction date. \nNormal i want to say, the bigger the house is and the more nine are price increases.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<b>Correlation between values and SalePrice</b>","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"tmp = pd.get_dummies(df_train)\nscaler = StandardScaler()\ndummie_df = pd.DataFrame(scaler.fit_transform(tmp), columns = tmp.columns)\ncorr = dummie_df.corr()['SalePrice']\ndf_corr = pd.DataFrame(corr.sort_values().iloc[np.r_[0:20, -20:0]].drop('SalePrice'))\ncustom_palette = {}\nfor q , a in df_corr.iterrows():\n    if a.values < 0:\n        custom_palette[q] = 'r'\n    else:\n        custom_palette[q] = 'b'\n\nfig3, ax3 = plt.subplots(figsize=(7, 7))\nfig3.tight_layout()\nsns.barplot(x=\"SalePrice\",y = df_corr.index, data=df_corr,\n            label=\"Correlation\", palette=custom_palette, ax=ax3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a strong negative correlation between the selling price and Average quality of the material on the exterior, Kitchen quality and the height of the basement.\n\nAnd there is a strong positive relationship between the selling price and Rates the overall material and finish of the house, size of living area and size of garage in car capacity.\n\nWhy does an average quality influence the price negatively more than a mediocre quality?\n\nWe can rely on the positive correlation but on the negative one.\n\nMaybe the number of houses with average quality is more important than others? lets check.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"ExterQual\", data=df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"KitchenQual\", data=df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"BsmtQual\", data=df_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Everything is explained, the TA values are more numerous than the other values, the union is the strength.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<b>Missing values</b>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(df_train.isna().sum(), columns = ['Df_train NaN Values']).sort_values(by='Df_train NaN Values', ascending = False).head(20).plot(kind = 'bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(df_test.isna().sum(), columns = ['Df_test NaN Values']).sort_values(by='Df_test NaN Values', ascending = False).head(20).plot(kind = 'bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are many NaN values that I have to deal with later","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<b>Correlation matrix (heatmap style)</b>","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"corr = df_train.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A red square that interests me is the positive correlation between total square feet of basement area and size of garage in car capacity.\nI can deduce that the garage can be part of the basement because the bigger it is, the larger the area of the basement is important.\n\nAnother red square that may be interesting is the positive correlation between Year garage was built and Size of garage in square feet.\nAe can say that over time people have more cars and cars have become bigger, by conceiving the size of the garage is more important and greenhouse gas production too and it's not good for nature Well, that's not the subject. protect nature =).\n\nAoncerning the blue squares there are two that have the darkest color.\n\nThe strong negative correlation between the original construction date, the people are disinterested porches in their houses and the new architchtures of houses do not include porches, it is old.\n\nConcerning the blue squares there are two that have the darkest color.\n\nThe strong negative correlation between the original construction date, the people are disinterested porches in their houses and the new architchtures of houses do not include porches, it is old.\n\nAnother strong negative correlation between Unfinished square feet of basement area and Type 1 finished square feet, it is very logical the two features are oposed and contradictory.\n\nThere is still much to say but if talk about everything it will not be a notebook but a parchment.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<b>Resizing and viewing</b>","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"tmp2 = pd.get_dummies(df_train)\nscaler2 = StandardScaler()\ndummie_df2 = pd.DataFrame(scaler2.fit_transform(tmp2), columns = tmp2.columns)\nX = dummie_df2.loc[:, dummie_df2.columns != 'SalePrice']\nY = dummie_df2.loc[:, dummie_df2.columns == 'SalePrice']\nX['LotFrontage'].fillna(60.0, inplace = True)\nX['GarageYrBlt'].fillna(1890.0, inplace = True)\nX['MasVnrArea'].fillna(0, inplace = True)\ntsne = TSNE(n_components = 2, perplexity = 30).fit_transform(X)\n\nresult_f = pd.DataFrame(tsne)\ncolor = pd.qcut(Y['SalePrice'],5,labels=False).map({0 : 'blue' , 1 : 'violet' , 2: 'deeppink', 3 : 'crimson', 4: 'darkred'})\n# color= ['red' if row == 'test' else 'blue' for row in output]\nplt.figure(figsize=(10,10))\nplt.title('TSNE 2D Resizing')\nplt.scatter(result_f[0],result_f[1], s=4, c=color)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The scatter plot represents a resizing of the test data in two dimentions with a color nuanced compared to the sale price to see the attribution of the information.\n\nThrough graph we can see that there are 3 clusters of points, it may be interesting for clustring but it is a regression = /.\n\nWe can also see that the arrangement of colors differs, where there is blue there is not much darkred and where there is pink there is not much purple etc. As if there is an intensification of the blue color towards the darkred in a specific direction.\n\nThis is good news because it means that there is something that sets the prices high and low.\n\nWe can distinguish divergent points, it means that there are outliers that I have to deal with later","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Data Engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets prepar data fo ML.\n\nThe data preprocess that I have done is based on my own logic, a case that I will mention with a comment.\n\nIn this part I will deal with the missing values and transform the categorical values and add some features that I found useful.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['SalePrice'] = 'test'\ndf = pd.concat([df_train, df_test])\ndf.drop(['Id'], axis=1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['PoolQC'].fillna('No Pool', inplace = True)\ndf['MiscFeature'].fillna('None', inplace = True)\ndf['Alley'].fillna('No alley access', inplace = True)\ndf['Fence'].fillna('No Fence', inplace = True)\ndf['FireplaceQu'].fillna('No Fireplace', inplace = True)\ndf['LotFrontage'].fillna(60.0, inplace = True)\ndf['GarageCond'].fillna('No Garage', inplace = True)\ndf['GarageType'].fillna('No Garage', inplace = True)\ndf['GarageYrBlt'].fillna(1890, inplace = True)\ndf['GarageFinish'].fillna('No Garage', inplace = True)\ndf['GarageQual'].fillna('No Garage', inplace = True)\ndf['BsmtExposure'].fillna('No Basement', inplace = True)\ndf['BsmtFinType2'].fillna('No Basement', inplace = True)\ndf['BsmtFinType1'].fillna('No Basement', inplace = True)\ndf['BsmtCond'].fillna('No Basement', inplace = True)\ndf['BsmtQual'].fillna('No Basement', inplace = True)\ndf['MasVnrArea'].fillna(0, inplace = True)\ndf['MasVnrType'].fillna('None', inplace = True)\n# df['Exterior2nd'].fillna('None', inplace = True)\n# df['Exterior1st'].fillna('None', inplace = True)\ndf['BsmtFullBath'].fillna(0, inplace = True) # df[df['BsmtFullBath'].isna()][['BsmtFullBath','BsmtFinType1','BsmtFinType2']]\ndf['BsmtHalfBath'].fillna(0, inplace = True) # df[df['BsmtFullBath'].isna()][['BsmtFullBath','BsmtFinType1','BsmtFinType2']]\n# df['KitchenQual'].fillna(0, inplace = True)\n  # df[(df['Neighborhood']== 'IDOTRR') | (df['Neighborhood']== 'Mitchel')]['MSZoning']\ndf.loc[(df['MSZoning'].isna()) & (df['Neighborhood'] == 'IDOTRR'), 'MSZoning'] = 'RM'\ndf.loc[(df['MSZoning'].isna()) & (df['Neighborhood'] == 'Mitchel'), 'MSZoning'] = 'RL'\ndf['Utilities'].fillna('AllPub', inplace = True) #la majorité\n# df['BsmtHalfBath'].fillna(0.0, inplace = True) #la majorité\n# df['BsmtFullBath'].fillna(0.0, inplace = True)\ndf['Functional'].fillna('Typ', inplace = True)\ndf['Exterior1st'].fillna('VinylSd', inplace = True)\ndf['Exterior2nd'].fillna('VinylSd', inplace = True)\ndf['TotalBsmtSF'].fillna(0, inplace = True)\ndf['BsmtUnfSF'].fillna(0, inplace = True)\ndf['BsmtFinSF2'].fillna(0, inplace = True)\ndf['GarageArea'].fillna(0, inplace = True)\ndf['GarageCars'].fillna(0, inplace = True)\ndf['KitchenQual'].fillna('TA', inplace = True)\ndf['BsmtFinSF1'].fillna(0, inplace = True)\ndf['SaleType'].fillna('WD', inplace = True)\ndf['Electrical'].fillna('SBrkr', inplace = True)\ndf['MSSubClass'] = df['MSSubClass'].astype(str)\ndf['OverallCond'] = df['OverallCond'].astype(str)\n\ndf['BsmtExposure'] = df['BsmtExposure'].map({'No Basement' : 0 ,'No' : 1, 'Mn' : 2, 'Av' : 3, 'Gd' : 4})\ndf['KitchenQual'] = df['KitchenQual'].map({'Po' : 0 ,'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4})\n\ndf['GarageYrBlt_Years'] =  abs(1890.0 - df['GarageYrBlt'])\ndf['YearBuilt_Years'] = abs(1872.0 - df['YearBuilt'])\ndf['GarageYrBlt'] = df['GarageYrBlt'].astype(str)\ndf['YearBuilt'] = df['YearBuilt'].astype(str)\n\ntmp = df['MoSold'].map({ 1: 0.0833, 2: 0.1666, 3: 0.25, 4: 0.3333, 5: 0.4166, 6: 0.50, 7: 0.5833, 8: 0.6666, 9: 0.75, 10: 0.8333\n      , 11: 0.9166\n      , 12: 0.9999})\ndf['YearSold'] = abs(2006.0 - (df['YrSold'] + tmp)).astype(float)\ndf['MoSold'] = df['MoSold'].astype(str)\ndf['YrSold'] = df['YrSold'].astype(str)\n\ncriteria = [df['YearRemodAdd'] == df['YearBuilt'], df['YearRemodAdd'] != df['YearBuilt']]\nvalues = [0, 1]\n\ndf['Remod'] = np.select(criteria, values, 0)\ndf['YearRemod'] = (df['YearRemodAdd'].astype(int) - df['YearBuilt'].astype(int)).astype(int)\ndf['YearRemodAdd'] = df['YearRemodAdd'].astype(str)\ndf['YearBuilt'] = df['YearBuilt'].astype(str)\n\ndf['Street'] = df['Street'].map({'Grvl': 0, 'Pave' : 1})\ndf['Alley'] = df['Alley'].map({'Grvl': 1, 'Pave' : 2 , 'No alley access' : 0 })\ndf['LotShape'] = df['LotShape'].map({'IR3': 0, 'IR2' : 1 , 'IR1' : 2 , 'Reg' : 3})\ndf['LandContour'] = df['LandContour'].map({'Low': 0, 'HLS' : 1 , 'Bnk' : 2 , 'Lvl' : 3})\ndf['Utilities'] = df['Utilities'].map({'ELO': 0, 'NoSeWa' : 1 , 'NoSewr' : 2 , 'AllPub' : 3})\n# df['LotConfig'] = df['LotConfig'].map({'ELO': 0, 'NoSeWa' : 1 , 'NoSewr' : 2 , 'AllPub' : 3})\ndf['LandSlope'] = df['LandSlope'].map({'Gtl': 2, 'Mod' : 1 , 'Sev' : 0})\n\ncriteria = [\n#AA\n((df['Condition1'] == 'Artery') | (df['Condition1'] == 'RRAn') | (df['Condition1'] == 'PosA') | (df['Condition1'] == 'RRAe'))\n&\n((df['Condition2'] == 'Artery') | (df['Condition2'] == 'RRAn') | (df['Condition2'] == 'PosA') | (df['Condition2'] == 'RRAe'))\n,\n#AB\n((df['Condition1'] == 'Artery') | (df['Condition1'] == 'RRAn') | (df['Condition1'] == 'PosA') | (df['Condition1'] == 'RRAe'))\n&\n((df['Condition2'] == 'Feedr') | (df['Condition2'] == 'RRNn') | (df['Condition2'] == 'PosN') | (df['Condition2'] == 'RRNe'))\n,\n#BA\n((df['Condition2'] == 'Artery') | (df['Condition2'] == 'RRAn') | (df['Condition2'] == 'PosA') | (df['Condition2'] == 'RRAe'))\n&\n((df['Condition1'] == 'Feedr') | (df['Condition1'] == 'RRNn') | (df['Condition1'] == 'PosN') | (df['Condition1'] == 'RRNe'))\n,\n#BB\n((df['Condition1'] == 'Feedr') | (df['Condition1'] == 'RRNn') | (df['Condition1'] == 'PosN') | (df['Condition1'] == 'RRNe'))\n&\n((df['Condition2'] == 'Feedr') | (df['Condition2'] == 'RRNn') | (df['Condition2'] == 'PosN') | (df['Condition2'] == 'RRNe'))\n,\n#A0\n((df['Condition1'] == 'Artery') | (df['Condition1'] == 'RRAn') | (df['Condition1'] == 'PosA') | (df['Condition1'] == 'RRAe'))\n&\n(df['Condition2'] == 'Norm')\n,\n#0A\n((df['Condition2'] == 'Artery') | (df['Condition2'] == 'RRAn') | (df['Condition2'] == 'PosA') | (df['Condition2'] == 'RRAe'))\n&\n(df['Condition1'] == 'Norm' )\n,\n#B0\n((df['Condition1'] == 'Feedr') | (df['Condition1'] == 'RRNn') | (df['Condition1'] == 'PosN') | (df['Condition1'] == 'RRNe'))\n&\n(df['Condition2'] == 'Norm' )\n,\n#0B\n((df['Condition2'] == 'Feedr') | (df['Condition2'] == 'RRNn') | (df['Condition2'] == 'PosN') | (df['Condition2'] == 'RRNe'))\n&\n(df['Condition1'] == 'Norm')\n,\n#00\n(df['Condition1'] == 'Norm' )\n&\n(df['Condition2'] == 'Norm' )\n]\n\nvalues = [4,3,3,2,2,2,1,1,0]\ndf['conditions'] = np.select(criteria, values, 10)\n\ndf['Exterior2nd'] = df['Exterior2nd'].map({ 'AsbShng': 'AsbShng', 'AsphShn': 'AsphShn', 'Brk Cmn': 'BrkComm', 'BrkFace': 'BrkFace', 'CBlock': 'CBlock', 'CmentBd': 'CemntBd',\n                       'HdBoard': 'HdBoard', 'ImStucc': 'ImStucc', 'MetalSd': 'MetalSd', 'Plywood': 'Plywood'\n                      , 'Stone': 'Stone'\n                      , 'Stucco': 'Stucco'\n                      ,'VinylSd': 'VinylSd'\n                      ,'Wd Sdng': 'Wd Sdng'\n                      ,'Wd Shng': 'WdShing'\n                      , 'None'  : 'None'\n                      , 'Other' :  'Other'})\n\ncriteria = [\n# Only 1\n((df['Exterior1st'] == df['Exterior2nd']) | (df['Exterior2nd'] !=  'Other'))\n,\n# No One\n(df['Exterior1st'] == 'None') &  (df['Exterior2nd'] == 'None')\n,\n# 2\n(df['Exterior1st'] !=  df['Exterior2nd']) \n]\n\nvalues = [1,0,2]\ndf['Exterior'] = np.select(criteria, values, 10)\n\ncriteria = [\n# Have veneer \n(df['MasVnrType'] != 'None')\n,\n# Havnt veneer    \n(df['MasVnrType'] == 'None')\n]\n\nvalues = [1,0]\ndf['veneer'] = np.select(criteria, values, 10)\n\ndf['ExterQual'] = df['ExterQual'].map({'Po' : 0 ,'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4})\n\ndf['ExterCond'] = df['ExterCond'].map({'Po' : 0 ,'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4})\n\ndf['BsmtQual'] = df['BsmtQual'].map({'No Basement' : 0 ,'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5})\n\ndf['BsmtFinType1'] = df['BsmtFinType1'].map({'No Basement' : 0 ,'Unf' : 1, 'LwQ' : 2, 'Rec' : 3, 'BLQ' : 4, 'ALQ' : 5, 'GLQ': 6})\n\ndf['BsmtFinType2'] = df['BsmtFinType2'].map({'No Basement' : 0 ,'Unf' : 1, 'LwQ' : 2, 'Rec' : 3, 'BLQ' : 4, 'ALQ' : 5, 'GLQ': 6})\n\ncriteria = [\n# No Bsmt\n(df['BsmtFinType1'] == 0) & (df['BsmtFinType2'] == 0)\n,\n# 1 Bsmt   \n((df['BsmtFinType1'] != 0) & (df['BsmtFinType2'] != 0) & (df['BsmtFinType1'] == df['BsmtFinType2']))\n|\n((df['BsmtFinType1'] != 0) & (df['BsmtFinType2'] == 0))\n|\n((df['BsmtFinType1'] == 0) & (df['BsmtFinType2'] != 0)) \n,\n# 2 bsmnt\n(df['BsmtFinType1'] != 0) & (df['BsmtFinType2'] != 0) & (df['BsmtFinType1'] != df['BsmtFinType2'])\n]\n\nvalues = [0,1,2]\ndf['Bsmt'] = np.select(criteria, values, 10)\n\ndf['HeatingQC'] = df['HeatingQC'].map({'Po' : 0 ,'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4})\n\ndf['CentralAir'] = df['CentralAir'].map({'N' : 0 ,'Y' : 1})\n\ndf['FireplaceQu'] = df['FireplaceQu'].map({'No Fireplace' : 0 ,'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4 , 'Ex' : 5})\n\ndf['Functional'] = df['Functional'].map({'Sal' : 0 ,'Sev' : 1, 'Maj2' : 2, 'Maj1' : 3, 'Mod' : 4 , 'Min2' : 5, 'Min1' : 6, 'Typ' : 7})\n\ncriteria = [\n# No garage\ndf['GarageType'] == 'No Garage'\n,\n#2 garages\ndf['GarageType'] == '2Types'\n,\n# only one garage\n(df['GarageType'] != '2Types') & (df['GarageType'] != 'No Garage')\n]\n\nvalues = [0,2,1]\ndf['Garage'] = np.select(criteria, values, 10)\n\ndf['GarageFinish'] = df['GarageFinish'].map({'No Garage' : 0 ,'Unf' : 1, 'RFn' : 2, 'Fin' : 3})\n\ndf['GarageQual'] = df['GarageQual'].map({'No Garage' : 0 ,'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4 , 'Ex' :5})\n\ndf['GarageCond'] = df['GarageCond'].map({'No Garage' : 0 ,'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4 , 'Ex' :5})\n\ndf['PavedDrive'] = df['PavedDrive'].map({'N' : 0 ,'P' : 1, 'Y' : 2})\n\ndf['PoolQC'] = df['PoolQC'].map({'No Pool' : 0 ,'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4})\n\ndf['Fence'] = df['Fence'].map({'No Fence' : 0 ,'MnWw' : 1, 'GdWo' : 2, 'MnPrv' : 3, 'GdPrv' : 4})\n\ncriteria = [\n# No Feature\ndf['MiscFeature'] == 'None'\n,\n# with Feature\ndf['MiscFeature'] != 'None'\n]\nvalues = [0,1]\ndf['Feature'] = np.select(criteria, values, 10)\ndf['TotalArea'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF'] + df['GrLivArea'] +df['GarageArea']\ndf['Bathrooms'] = df['FullBath'] + df['HalfBath']*0.5 \ndf['Year average']= (df['YearRemodAdd'].astype(int)+df['YearBuilt'].astype(int))/2\n\ndf['HasBsmt'] = pd.Series(len(df['TotalBsmtSF']), index=df_train.index)\ndf['HasBsmt'] = 0 \ndf.loc[df['TotalBsmtSF']>0,'HasBsmt'] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this part I keep the common columns between the test and the train and I delete the outliers in the train data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df[df['SalePrice'] != 'test']\ndf_test = df[df['SalePrice'] == 'test']\ndf_train['SalePrice'] = df_train['SalePrice'].astype(float)\n\n# to reduce the skewness of the saleprice it is necessary to apply the logarithm function on the saleprice. \n# More details are on this notebook: https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python\ndf_train['SalePrice'] =  np.log1p(df_train['SalePrice'])\ndf_train['GrLivArea'] = np.log1p(df_train['GrLivArea'])\ndf_train.loc[df_train['HasBsmt']==1,'TotalBsmtSF'] = np.log1p(df_train['TotalBsmtSF'])\n\ndf_test['GrLivArea'] = np.log1p(df_test['GrLivArea'])\ndf_test.loc[df_test['HasBsmt']==1,'TotalBsmtSF'] = np.log1p(df_test['TotalBsmtSF'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler1 = StandardScaler()\ndf_test.drop(['SalePrice'], axis = 1, inplace = True)\ndum =  pd.get_dummies(df_train, drop_first  = True)\ndum_test =  pd.get_dummies(df_test, drop_first  = True)\n\ndum1 = dum.copy()\ndum_test1 = dum_test.copy()\ncol = []\nfor i in dum.columns:\n    if i not in dum_test.columns :\n        col.append(i)\n        \nfor i in dum_test.columns:\n    if i not in dum.columns :\n        col.append(i)\n        \nfor i in dum1.columns:\n    if i in col :\n        dum1.drop(i, axis = 1, inplace = True)\nfor i in dum_test1.columns:\n    if i in col :\n        dum_test1.drop(i, axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = scaler1.fit_transform(dum1)\n\nknn = KNN()\noutliers = knn.fit_predict(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dum1['outliers_flag'] = outliers\ndum1['SalePrice'] = df_train['SalePrice']\n\nfinal_outlier = dum1[dum1.outliers_flag != 1]\nX = final_outlier.loc[:, final_outlier.columns != 'outliers_flag']\nX = X.loc[:, X.columns != 'SalePrice']\ny = final_outlier['SalePrice']\n\npca = PCA(n_components=2)\nX['pca1'] =  pd.DataFrame(pca.fit_transform(pd.DataFrame(X)))[0].tolist()\nX['pca2'] =  pd.DataFrame(pca.fit_transform(pd.DataFrame(X)))[1].tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Machine Learning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I took several classification algorithm and I compared their RMSE on the train data in cross validation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X)\n    rmse= np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\nmodel_xgb = make_pipeline(RobustScaler(),xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1))\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nKRR = make_pipeline(RobustScaler(),KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5))\nGBoost = make_pipeline(RobustScaler(),GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5))\nRFR = make_pipeline(RobustScaler(),RandomForestRegressor(random_state = 1))\nRidge = make_pipeline(RobustScaler(),Ridge(alpha = 17))\nlr = make_pipeline(RobustScaler(),LinearRegression())\nex_reg=make_pipeline(RobustScaler(),ExtraTreesRegressor(n_estimators=2000, max_depth=20))\nsvr = make_pipeline(RobustScaler(),SVR(kernel='rbf',C=20000,gamma=0.045))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = {}\nscore = rmsle_cv(model_xgb)\nscores['Xgboost'] = score.mean()\nscore = rmsle_cv(lasso)\nscores['Lasso'] = score.mean()\nscore = rmsle_cv(ENet)\nscores['ElasticNet'] = score.mean()\nscore = rmsle_cv(KRR)\nscores['Kernel Ridge'] = score.mean()\nscore = rmsle_cv(GBoost)\nscores['Gradient Boosting'] = score.mean()\nscore = rmsle_cv(RFR)\nscores['Random Forest'] = score.mean()\nscore = rmsle_cv(Ridge)\nscores['Ridge'] = score.mean()\nscore = rmsle_cv(lr)\nscores['lr'] = score.mean()\nscore = rmsle_cv(ex_reg)\nscores['svr'] = score.mean()\nscore = rmsle_cv(svr)\nscores['svr'] = score.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(scores.items(), key=operator.itemgetter(1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lasso, ElasticNet, Ridge and Xgboost gave the best score, lets test voting on them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vote_mod = make_pipeline(RobustScaler(),VotingRegressor([('Ridge', Ridge), ('Lasso', lasso), ('Elastic', ENet), ('Xgboost', model_xgb)]))\n\nscore = rmsle_cv(vote_mod)\nprint(\"vote_mod score: {:.4f} \\n\".format(score.mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stregr = make_pipeline(RobustScaler(),StackingRegressor(regressors=[ENet,Ridge, lasso, vote_mod], \n                           meta_regressor=model_xgb, use_features_in_secondary=True))\n\nscore = rmsle_cv(stregr)\nprint(\"stregr score: {:.4f}\\n\".format(score.mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Voting and stacking have almost the same score, use blending between them and applying some adjustment to the result of the blending.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stregr.fit(X, y)\nvote_mod.fit(X, y)\n\npca = PCA(n_components=2)\ndum_test1['pca1'] =  pd.DataFrame(pca.fit_transform(pd.DataFrame(dum_test1)))[0].tolist()\ndum_test1['pca2'] =  pd.DataFrame(pca.fit_transform(pd.DataFrame(dum_test1)))[1].tolist()\n\nstack_predict  =  stregr.predict(dum_test1)\nvote_pred  =  vote_mod.predict(dum_test1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_output['SalePrice'] = (2/3) * np.expm1(vote_pred) + (1/3) * np.expm1(stack_predict)\n\n# source : https://www.kaggle.com/jesucristo/1-house-prices-solution-top-1\n\nq1 = df_output['SalePrice'].quantile(0.0045)\nq2 = df_output['SalePrice'].quantile(0.99)\n\ndf_output['SalePrice'] = df_output['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\ndf_output['SalePrice'] = df_output['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\n\nfilename = 'submission.csv'\n\ndf_output.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Done !\n\nI hope you enjoyed. If so, put an <b>Upvote!</b> =)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}