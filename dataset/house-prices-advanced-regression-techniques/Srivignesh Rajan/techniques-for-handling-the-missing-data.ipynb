{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Why handling missing data is important?\nThe problem of missing data is relatively common in almost all research and can have a significant effect on the conclusions that can be drawn from the data.\nMissing data present various problems. \n* First, the absence of data reduces statistical power. \n* Second, the lost data can cause bias in the estimation of parameters. \n* Third, it may complicate the analysis of the study. \n* Fourth, many machine learning packages in python does not accept missing data.\n\nEach of these distortions may threaten the validity of the trials and can lead to invalid conclusions which reduces the reliability on the model.\n\n[**Reference**](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3668100/)\n\n# Missing data mechanisms\n\n* **Missing Completely At Random (MCAR):**\nValues in a data set are Missing Completely At Random (MCAR) if the events that lead to any particular data-item being missing are independent both of observed data and of missing data.\n\n* **Missing At Random (MAR):**\nMissing At Random (MAR) is when the missing data is related to the observed data but not the missing data.\n\n* **Missing Not At Random (MNAR):**\nMissing Not At Random (MNAR) is data that is neither MAR nor MCAR. The missing values on the variable are related to that of both the observed and missing variables. \n\n[**Reference**](https://en.wikipedia.org/wiki/Missing_data#Missing_not_at_random)\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Handling Missing data\n1. **Dropping variables**\n2. **Partial Deletion**\n    * 2.1 **Listwise Deletion**\n3. **Data Imputation**\n    * 3.1 **Single Imputation**\n        * 3.1.1 Single Imputation for Numeric columns\n            * 3.1.1.1 Mean Imputation\n            * 3.1.1.2 Regression Imputation\n        * 3.1.2 Single Imputation for Categoric columns\n            * 3.1.2.1 Mode Imputation\n    * 3.2 **Multiple Imputation**\n        * 3.2.1 MICE Imputation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Dropping Variables\nDelete the column if it consists of more than **70% of missing values** otherwise Data Imputation is the most preferred method than deleting it. Greater the information to the model, the greater is the reliability of the model's results.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np \nimport pandas as pd \nimport xgboost\nfrom sklearn.experimental import enable_iterative_imputer \nfrom sklearn.impute import IterativeImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.ensemble import (GradientBoostingRegressor, GradientBoostingClassifier)\npd.set_option('max.columns',100)\npd.set_option('max.rows',500)\n\n'''Load the data'''\ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n\ndef find_missing_percent(data):\n    \"\"\"\n    Returns dataframe containing the total missing values and percentage of total\n    missing values of a column.\n    \"\"\"\n    miss_df = pd.DataFrame({'ColumnName':[],'TotalMissingVals':[],'PercentMissing':[]})\n    for col in data.columns:\n        sum_miss_val = data[col].isnull().sum()\n        percent_miss_val = round((sum_miss_val/data.shape[0])*100,2)\n        miss_df = miss_df.append(dict(zip(miss_df.columns,[col,sum_miss_val,percent_miss_val])),ignore_index=True)\n    return miss_df\n\nmiss_df = find_missing_percent(train)\n'''Columns with missing values'''\nprint(f\"Number of columns with missing values: {str(miss_df[miss_df['PercentMissing']>0.0].shape[0])}\")\ndisplay(miss_df[miss_df['PercentMissing']>0.0])\n\n'''Drop the columns with more than 70% of missing values'''\ndrop_cols = miss_df[miss_df['PercentMissing'] >70.0].ColumnName.tolist()\ntrain = train.drop(drop_cols,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Partial Deletion\n### 2.1 Listwise Deletion\n**Listwise deletion** is a technique in which the rows that contain missing values are deleted.\n\n**Disadvantages:**\nListwise deletion affects statistical power of the tests conducted. Statistical power relies in part on high sample size. Because listwise deletion excludes data with missing values, it reduces the sample which is being statistically analysed.\n\n* Listwise deletion is also problematic when the data is **Missing Not At Random (MNAR)** (i.e., questions aiming to extract sensitive information). Many of the subjects in the sample may not answer due to the intrusive nature of the questions, but may answer all other items. \n* Listwise deletion will exclude these respondents from analysis. This may create a **bias** in the dataset.\n\n**[Reference](https://en.wikipedia.org/wiki/Listwise_deletion)** ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def listwise_deletion(train):\n    for col in train.columns:\n        miss_ind = train[col][train[col].isnull()].index\n        train = train.drop(miss_ind, axis = 0)\n    return train\n\ntrain_lwd = listwise_deletion(train)\n'''Samples remaining after deletion'''\nprint(f\"Train data shape:{train_lwd.shape}\")\ntrain_lwd.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Data Imputation\n### 3.1 Single Imputation\nSingle Imputation attempts to impute the missing data by a single value as opposed to Multiple Imputation which replaces the missing data with multiple values.\n* 3.1.1 Single Imputation for Numeric columns\n    * 3.1.1.1 Mean Imputation\n    * 3.1.1.2 Regression Imputation\n* 3.1.2 Single Imputation for Categoric columns\n    * 3.1.2.1 Mode Imputation\n   ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 3.1.1 Single Imputation for Numeric columns\n**3.1.1.1 Mean Imputation**\n* Mean Imputation is the process of imputing the missing data by the mean of the variable and can be done only to numeric columns.\n\n**Disadvantage:** Mean Imputation is more likely to introduce bias in the model.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Segregate numeric and categoric columns'''\nnumeric_cols = train.select_dtypes(['float','int']).columns\ncategoric_cols = train.select_dtypes('object').columns\n\ntrain_numeric = train[numeric_cols]\ntrain_categoric = train[categoric_cols]\n\ndef mean_imputation(train_numeric):\n    \"\"\"\n    Mean Imputation\n    \"\"\"\n    for col in train_numeric.columns:\n        mean = train_numeric[col].mean()\n        train_numeric[col] = train_numeric[col].fillna(mean)\n    return train_numeric\n\ntrain_mean_imp = mean_imputation(train_numeric)\ntrain_mean_imp.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3.1.1.2 Regression Imputation**\n* A Regression model is fitted where the predictors are the features without missing values and the targets are the features with missing values. The missing values are then replaced with the predictions. Regression imputation is less likely to introduce bias in the model.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Select all the numeric columns for regression imputation'''\ntrain_numeric_regr = train[numeric_cols]\n'''Numeric columns with missing values which acts as target in training'''\ntarget_cols = ['LotFrontage','GarageYrBlt']\n'''Predictors for regression imputation'''\npredictors = train_numeric_regr.drop(target_cols, axis =1)\n\ndef find_missing_index(train_numeric_regr, target_cols):\n    \"\"\"\n    Returns the index of the missing values in the columns.\n    \"\"\"\n    miss_index_dict = {}\n    for tcol in target_cols:\n        index = train_numeric_regr[tcol][train_numeric_regr[tcol].isnull()].index\n        miss_index_dict[tcol] = index\n    return miss_index_dict\n\ndef regression_imputation(train_numeric_regr, target_cols, miss_index_dict):\n    \"\"\"\n    Fits XGBoost Regressor and replaces the missing values with\n    the prediction.\n    \"\"\"\n    for tcol in target_cols:\n        y = train_numeric_regr[tcol]\n        '''Initially impute the column with mean'''\n        y = y.fillna(y.mean())\n        xgb = xgboost.XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n        '''Fit the model where y is the target column which is to be imputed'''\n        xgb.fit(predictors, y)\n        predictions = pd.Series(xgb.predict(predictors),index= y.index)    \n        index = miss_index_dict[tcol]\n        '''Replace the missing values with the predictions'''\n        train_numeric_regr[tcol].loc[index] = predictions.loc[index]\n    return train_numeric_regr\n\nmiss_index_dict = find_missing_index(train_numeric_regr, target_cols)\ntrain_numeric_regr = regression_imputation(train_numeric_regr, target_cols, miss_index_dict)\ntrain_numeric_regr.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.1.2 Single Imputation for categoric columns\n**3.1.2.1 Mode Imputation**\n* Mode Imputation is the process of imputing the missing data by the mode of the variable and can be done only to categoric columns.\n\n**Disadvantage:** More likely to introduce bias in the model. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def mode_imputation(train_categoric):\n    \"\"\"\n    Mode Imputation\n    \"\"\"\n    for col in train_categoric.columns:\n        mode = train_categoric[col].mode().iloc[0]\n        train_categoric[col] = train_categoric[col].fillna(mode)\n    return train_categoric\n\ntrain_mode_imp = mode_imputation(train_categoric)\n'''Concatenate the mean and mode imputed columns'''\ntrain_imputed = pd.concat([train_mean_imp, train_mode_imp], axis = 1)\ntrain_imputed.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Multiple Imputation\n* In multiple imputation, the missing data is imputed with multiple values.\n\n#### 3.2.1 MICE (Multiple Imputation by Chained Equation)\n\n    \n#### MICE Algorithm:\nThe chained equation process can be broken down into four general steps:\n\n* **Step 1:** A simple imputation, such as imputing the mean, is performed for every missing value in the dataset. These mean imputations can be thought of as “place holders.”\n* **Step 2:** The “place holder” mean imputations for one variable (“var”) are set back to missing.\n* **Step 3:** The observed values from the variable “var” in Step 2 are regressed(can use any other regressors like Gradient Boosting Regressor or XGBoost Regressor for numeric data) on the other variables in the imputation model, which may or may not consist of all of the variables in the dataset. In other words, “var” is the dependent variable in a regression model and all the other variables are independent variables in the regression model. These regression models operate under the same assumptions that one would make when performing linear, logistic, or Poison regression models outside of the context of imputing missing data.\n* **Step 4:** The missing values for “var” are then replaced with predictions (imputations) from the regression model. When “var” is subsequently used as an independent variable in the regression models for other variables, both the observed and these imputed values will be used.\n* **Step 5:** Steps 2–4 are then repeated for each variable that has missing data. The cycling through each of the variables constitutes one iteration or “cycle.” At the end of one cycle all of the missing values have been replaced with predictions from regressions that reflect the relationships observed in the data.\n* **Step 6:** Steps 2–4 are repeated for a number of cycles, with the imputations being updated at each cycle.\n\n[**Reference** ](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/)\n\n#### MICE Algorithm for Categorical data:\nBefore going through the steps 1 to 6 in MICE algorithm the following steps must be done in order to impute categorical data.\n* **Step 1:** Ordinal Encode the non-null values\n* **Step 2:** Use MICE imputation with Gradient Boosting Classifier to impute the ordinal encoded data\n* **Step 3:** Convert back from ordinal values to categorical values.\n* **Step 4:** Follow steps 1 to 6 in MICE Algorithm. Instead of using Mean imputation for initial strategy use **Mode imputation**.\n\n[**Reference**](https://projector-video-pdf-converter.datacamp.com/17404/chapter4.pdf) \n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def mice_imputation_numeric(train_numeric):\n    \"\"\"\n    Impute numeric data using MICE imputation with Gradient Boosting Regressor.\n    (we can use any other regressors to impute the data)\n    \"\"\"\n    iter_imp_numeric = IterativeImputer(GradientBoostingRegressor())\n    imputed_train = iter_imp_numeric.fit_transform(train_numeric)\n    train_numeric_imp = pd.DataFrame(imputed_train, columns = train_numeric.columns, index= train_numeric.index)\n    return train_numeric_imp\n\ndef mice_imputation_categoric(train_categoric):\n    \"\"\"\n    Impute categoric data using MICE imputation with Gradient Boosting Classifier.\n    Steps:\n    1. Ordinal Encode the non-null values\n    2. Use MICE imputation with Gradient Boosting Classifier to impute the ordinal encoded data\n    (we can use any other classifier to impute the data)\n    3. Inverse transform the ordinal encoded data.\n    \"\"\"\n    ordinal_dict={}\n    for col in train_categoric:\n        '''Ordinal encode train data'''\n        ordinal_dict[col] = OrdinalEncoder()\n        nn_vals = np.array(train_categoric[col][train_categoric[col].notnull()]).reshape(-1,1)\n        nn_vals_arr = np.array(ordinal_dict[col].fit_transform(nn_vals)).reshape(-1,)\n        train_categoric[col].loc[train_categoric[col].notnull()] = nn_vals_arr\n\n    '''Impute the data using MICE with Gradient Boosting Classifier'''\n    iter_imp_categoric = IterativeImputer(GradientBoostingClassifier(), max_iter =5, initial_strategy='most_frequent')\n    imputed_train = iter_imp_categoric.fit_transform(train_categoric)\n    train_categoric_imp = pd.DataFrame(imputed_train, columns =train_categoric.columns,index = train_categoric.index).astype(int)\n    \n    '''Inverse Transform'''\n    for col in train_categoric_imp.columns:\n        oe = ordinal_dict[col]\n        train_arr= np.array(train_categoric_imp[col]).reshape(-1,1)\n        train_categoric_imp[col] = oe.inverse_transform(train_arr)\n        \n    return train_categoric_imp\n\ntrain_numeric_imp  = mice_imputation_numeric(train_numeric)\ntrain_categoric_imp = mice_imputation_categoric(train_categoric)\n\n'''Concatenate Numeric and Categoric Training and Test set data '''\ntrain_mice_imp = pd.concat([train_numeric_imp, train_categoric_imp, train['SalePrice']], axis = 1)\ntrain_mice_imp.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}