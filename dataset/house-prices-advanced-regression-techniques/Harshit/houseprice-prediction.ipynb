{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport pandas_profiling \n\nimport seaborn as sns\nimport matplotlib.style as style\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy.stats import boxcox_normmax\nfrom scipy.special import boxcox1p\nfrom scipy.stats import norm, skew\nimport scipy.stats as stats\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import LocalOutlierFactor\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"house_train= pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\nhouse_test= pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\nprint(\"Dataset shape:\",'house_train', house_train.shape, 'house_test', house_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#house_train.profile_report()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SalesPrice correlation with all the feature\nplt.figure(figsize=(8, 12))\nhouse_train.corr()['SalePrice'].sort_values().plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Heatmap for top 10 Sales Price-features correlation\nk = 10\ncols = house_train.corr().nlargest(k, 'SalePrice')['SalePrice'].index\nk_corr_matrix = house_train[cols].corr()\nplt.figure(figsize=(12, 8))\nsns.heatmap(k_corr_matrix, annot=True, cmap=plt.cm.RdBu_r)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Correlation** assumes data should be related linearly"},{"metadata":{"trusted":true},"cell_type":"code","source":"#scatterplot to verify linear relationship\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(house_train[cols], size = 2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Is Target variable Normal?\ntarget = house_train['SalePrice']\nf, axes = plt.subplots(1, 3, figsize=(15, 4))\nsns.distplot(target, kde=False, fit=stats.johnsonsu, ax=axes[0])\nsns.distplot(target, kde=False, fit=stats.norm, ax=axes[1])\nsns.distplot(target, kde=False, fit=stats.lognorm, ax=axes[ 2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is apparent that SalePrice ***doesn't follow normal distribution***, so before performing regression it has to be transformed"},{"metadata":{"trusted":true},"cell_type":"code","source":"# applying log transformation\nhouse_train['SalePrice'] = np.log1p(house_train['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# distribution histogram and normal probability plot\n(mu, sigma) = norm.fit(house_train['SalePrice'])\nsns.distplot(house_train['SalePrice'], fit=norm)\nplt.legend(['Normal dist ($\\mu=${:.2f}, $\\sigma=${:.2f})'.format(mu, sigma)])\n\nfig = plt.figure()\nstats.probplot(house_train['SalePrice'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Finding Outliers in Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect_outliers(x, y, top=5, plot=True):\n    lof = LocalOutlierFactor(n_neighbors=40, contamination=0.1)\n    x_ =np.array(x).reshape(-1,1)\n    preds = lof.fit_predict(x_)\n    lof_scr = lof.negative_outlier_factor_\n    out_idx = pd.Series(lof_scr).sort_values()[:top].index\n    if plot:\n        f, ax = plt.subplots(figsize=(9, 6))\n        plt.scatter(x=x, y=y, c=np.exp(lof_scr), cmap='RdBu')\n    return out_idx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Standardize the data and see if there're any outlier points"},{"metadata":{"trusted":true},"cell_type":"code","source":"#GrLivArea-SalePrice outlier detection\nouts = detect_outliers(house_train['GrLivArea'], house_train['SalePrice'],top=5) \nouts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Separating qualitative(categorical) and quantitative(continuous) featues\nquantitative = [feature for feature in house_train.columns if house_train.dtypes[feature] != 'object']\nquantitative.remove('SalePrice')\nquantitative.remove('Id')\nqualitative = [feature for feature in house_train.columns if house_train.dtypes[feature] == 'object']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Outliers for all quantitative features\nfrom collections import Counter\nall_outliers=[]\n\nfor feature in quantitative:\n    try:\n        outs = detect_outliers(house_train[feature], house_train['SalePrice'],top=5, plot=False)\n    except:\n        continue\n    all_outliers.extend(outs)\n\nprint(Counter(all_outliers).most_common())\n\noutliers = [30, 88, 462, 523, 632, 1298, 1324] #\nfor i in outliers:\n    if i in all_outliers:\n        print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#delete outliers from training dataset\nhouse_train = house_train.drop(house_train.index[outliers])\nhouse_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"house_train.reset_index(drop=True, inplace=True)\ny_train = house_train['SalePrice']\nX_train = house_train.drop(['SalePrice'], axis=1)\nX_test = house_test\n\nprint(\"Dataset shape:\",'X_train', X_train.shape, 'y_train', y_train.shape, 'X_test', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling NA's of the quantitative features  \nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics_train = []; numerics_test = []\nfor i in house_train.columns: \n    if house_train[i].dtype in numeric_dtypes:\n        numerics_train.append(i)\nhouse_train.update(house_train[numerics_train].fillna(0)) #Filling NA's of training dataset\n\nfor i in house_test.columns:\n    if house_test[i].dtype in numeric_dtypes:\n        numerics_test.append(i)\nhouse_test.update(house_test[numerics_test].fillna(0)) #Filling NA's of test dataset\n#house_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Skewness check and correction using boxcop for quantitative/continuous features\nskew_train = house_train[quantitative].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew_train = skew_train[skew_train > 0.5] #skewness value\nfor i in high_skew_train.index:\n    house_train[i] = boxcox1p(house_train[i], boxcox_normmax(house_train[i] + 1))\n\nskew_test = house_test[quantitative].apply(lambda x: skew(x)).sort_values(ascending=False)    \nhigh_skew_test = skew_test[skew_train > 0.5]\nfor i in high_skew_test.index:\n    house_test[i] = boxcox1p(house_test[i], boxcox_normmax(house_test[i] + 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Standard scaling to verify boxcox transformation\nsale_price_scaled = StandardScaler().fit_transform(house_train['SalePrice'][:, np.newaxis])\n\nsns.distplot(sale_price_scaled, fit=norm)\n\nlow_range = sale_price_scaled[sale_price_scaled[:, 0].argsort()[:5]]\nhigh_range = sale_price_scaled[sale_price_scaled[:, 0].argsort()[-5:]]\nprint(f'outer range (low) of the distribution: \\n{low_range}')\nprint(f'outer range (high) of the distribution: \\n{high_range}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combining train and test datasets\nall_data = pd.concat([X_train, house_test], axis=0, sort=False)\nall_data.drop(['Id'], axis=1, inplace=True)\nall_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some of the non-numeric predictors are stored as numbers; we convert them into strings \nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating missing data\nna_total = all_data.isnull().sum().sort_values(ascending=False)\nna_ratio = (all_data.isnull().sum() / all_data.shape[0]).sort_values(ascending=False)\nmissing_data = pd.concat([na_total, na_ratio], axis=1, keys=['Total', 'Ratio'])\nmissing_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most value of these 4 features are missing and they have no pattern , just delete them\nall_data.drop(['PoolQC', 'Utilities', 'Street', 'MiscFeature', ], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filling NA with None for categorical features\nfor col in ('Alley','Fence','FireplaceQu','GarageQual','GarageFinish','GarageCond','GarageType','BsmtExposure',\n          'BsmtCond','BsmtQual','BsmtFinType2','BsmtFinType1'):\n     all_data[col] = all_data[col].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(all_data[all_data['GarageCars'].isnull()][['GarageArea', 'GarageCars', 'GarageType', 'GarageYrBlt', 'GarageQual']])\nall_data['GarageArea'].fillna(0, inplace=True)\nall_data['GarageCars'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(all_data[all_data['TotalBsmtSF'].isnull()][\n    ['TotalBsmtSF', 'BsmtFinSF2', 'BsmtUnfSF', 'BsmtFinSF1', 'BsmtFullBath','BsmtHalfBath']])\nfor col in ('TotalBsmtSF', 'BsmtFinSF2', 'BsmtUnfSF', 'BsmtFinSF1', 'BsmtFullBath','BsmtHalfBath'):\n     all_data[col] = all_data[col].fillna(0)\n# all_data['TotalBsmtSF'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['MasVnrType'].fillna('None', inplace=True)\nall_data['HasMasVnr'] = all_data['MasVnrType'].apply(lambda x: 0 if x == 'None' else 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = all_data.iloc[:len(y_train), :]\nX_test = all_data.iloc[len(y_train):, :]\nprint(\"Dataset shape:\",'X_train', X_train.shape, 'y_train', y_train.shape, 'X_test', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill the NA with the mode, which means most categorical type of the feature-train &test\nX_train['MSZoning'] = X_train.groupby(['MSSubClass'])['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\nX_test['MSZoning'] = X_test.groupby(['MSSubClass'])['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\nfor col in ('Functional','Exterior1st','Electrical','KitchenQual','SaleType','Exterior2nd'):\n    X_train[col] = X_train[col].fillna(X_train[col].mode()[0])\n    X_test[col] = X_test[col].fillna(X_test[col].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['LotFrontage'] = X_train.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\nX_train['GarageYrBlt'] = (X_train['YearBuilt'] + X_train['YearRemodAdd']) /2\nX_train['MasVnrArea'] = X_train.groupby(['MasVnrType'])['MasVnrArea'].transform(lambda x: x.fillna(x.median()))\n\nX_test['LotFrontage'] = X_test.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\nX_test['GarageYrBlt'] = (X_test['YearBuilt'] + X_test['YearRemodAdd']) /2\nX_test['MasVnrArea'] = X_test.groupby(['MasVnrType'])['MasVnrArea'].transform(lambda x: x.fillna(x.median()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['YrBltAndRemod']=X_train['YearBuilt']+X_train['YearRemodAdd']\nX_train['TotalSF']=X_train['TotalBsmtSF'] + X_train['1stFlrSF'] + X_train['2ndFlrSF']\nX_train['TotalSqrFootage'] = (X_train['BsmtFinSF1'] + X_train['BsmtFinSF2'] +\n                                 X_train['1stFlrSF'] + X_train['2ndFlrSF'])\nX_train['TotalBathrooms'] = (X_train['FullBath'] + (0.5 * X_train['HalfBath']) +\n                               X_train['BsmtFullBath'] + (0.5 * X_train['BsmtHalfBath']))\nX_train['TotalPorchSF'] = (X_train['OpenPorchSF'] + X_train['3SsnPorch'] +\n                              X_train['EnclosedPorch'] + X_train['ScreenPorch'] +\n                              X_train['WoodDeckSF'])\n\nX_test['YrBltAndRemod']=X_test['YearBuilt']+X_test['YearRemodAdd']\nX_test['TotalSF']=X_test['TotalBsmtSF'] + X_test['1stFlrSF'] + X_test['2ndFlrSF']\nX_test['TotalSqrFootage'] = (X_test['BsmtFinSF1'] + X_test['BsmtFinSF2'] +\n                                 X_test['1stFlrSF'] + X_test['2ndFlrSF'])\nX_test['TotalBathrooms'] = (X_test['FullBath'] + (0.5 * X_test['HalfBath']) +\n                               X_test['BsmtFullBath'] + (0.5 * X_test['BsmtHalfBath']))\nX_test['TotalPorchSF'] = (X_test['OpenPorchSF'] + X_test['3SsnPorch'] +\n                              X_test['EnclosedPorch'] + X_test['ScreenPorch'] +\n                              X_test['WoodDeckSF'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['has2ndfloor'] = X_train['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nX_train['hasgarage'] = X_train['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nX_train['hasbsmt'] = X_train['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nX_train['hasfireplace'] = X_train['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nX_test['has2ndfloor'] = X_test['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nX_test['hasgarage'] = X_test['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nX_test['hasbsmt'] = X_test['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nX_test['hasfireplace'] = X_test['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Dataset shape:\",'X_train', X_train.shape, 'y_train', y_train.shape, 'X_test', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import category_encoders as ce\nohe = ce.OneHotEncoder(handle_unknown='ignore', use_cat_names=True)\nencoded_data=ohe.fit_transform(pd.concat([X_train,X_test], axis=0, sort=False)).reset_index(drop=True)\nX_train =  encoded_data.iloc[:len(y_train), :]\nX_test = encoded_data.iloc[len(y_train):, :]\nprint(\"Dataset shape:\",'X_train', X_train.shape, 'y_train', y_train.shape, 'X_test', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train2= pd.get_dummies(X_train).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pd.concat([pd.DataFrame(X_train.columns),pd.DataFrame(X_test.columns)]).drop_duplicates(keep=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.isnull().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removes colums where the threshold of zero's is (> 99.95), means has only zero values \noverfit = []\nlen_X_train =len(X_train)\n\nfor i in X_train.columns:\n    counts = X_train[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros / len_X_train * 100 > 99.94 :\n        overfit.append(i)\n\noverfit = list(overfit)\noverfit.append('MSZoning_C (all)')\n\n#Converting numpy array to handle XGB feature mismatch error -https://github.com/dmlc/xgboost/issues/2334\nX_train = np.array(X_train.drop(overfit, axis=1).copy())\ny_train = np.array(y_train)\nX_test = np.array(X_test.drop(overfit, axis=1).copy())\n\nprint(\"Dataset shape:\",'X_train', X_train.shape, 'y_train', y_train.shape, 'X_test', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.linear_model import ElasticNet, Lasso, Ridge, ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.svm import SVR\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom mlxtend.regressor import StackingCVRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cross_val_score to get the root mean square error, which is the score method for current regression problem\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mse(y, y_pred))\n\ndef cv_rmse(model, X_train=X_train):\n    rmse = np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#parameters(for grid search)\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ridge\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\n\n#lasso\nlasso = make_pipeline(\n    RobustScaler(),\n    LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds))\n\n#elastic net\nelasticnet = make_pipeline(\n    RobustScaler(),\n    ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))\n\n#svm\nsvr = make_pipeline(RobustScaler(), SVR(\n    C=20,\n    epsilon=0.009,\n    gamma=0.0003,\n))\n\n#GradientBoosting\ngbr = GradientBoostingRegressor(n_estimators=3000,\n                                learning_rate=0.05,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)\n\n#lightgbm\nlightgbm = LGBMRegressor(\n    objective='regression',\n    num_leaves=4,\n    learning_rate=0.01,\n    n_estimators=5000,\n    max_bin=200,\n    bagging_fraction=0.75,\n    bagging_freq=5,\n    bagging_seed=7,\n    feature_fraction=0.2,\n    feature_fraction_seed=7,\n    verbose=-1,\n    #min_data_in_leaf=2,\n    #min_sum_hessian_in_leaf=11\n)\n\n#xgboost reg:squarederror replacing reg:linear\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=3460,\n                       max_depth=5,\n                       min_child_weight=0,\n                       gamma=0,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:squarederror',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#StackingCVRegressorï¼šA 'Stacking Cross-Validation' regressor for scikit-learn estimators.\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('TEST score on CV')\n\nscore = cv_rmse(ridge) #cross_val_score(RidgeCV(alphas),X, y)\nprint(\"Ridge score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\nscore = cv_rmse(lasso)\nprint(\"Lasso score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\nscore = cv_rmse(elasticnet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\nscore = cv_rmse(svr)\nprint(\"SVR score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\nscore = cv_rmse(lightgbm)\nprint(\"Lightgbm score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\nscore = cv_rmse(gbr)\nprint(\"GradientBoosting score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\nscore = cv_rmse(xgboost)\nprint(\"Xgboost score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train the stacking model\n#1.1 learn first-level model\n#1.2 construct a training set for second-level model\n#2. train the second-level model\n#3. re-learn first-level model on the entire train set\nprint('Training Model')\nstack_gen_model = stack_gen.fit(X_train, y_train) #Fit ensemble regressors and the meta-regressor\nprint('Model Trained')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submit prediction result\nprint('Predict submission')\nresult = np.floor(np.expm1(stack_gen_model.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission=pd.DataFrame()\nsubmission['Id'] = house_test['Id']\nsubmission['SalePrice']= result\nsubmission.head()\nsubmission.to_csv(\"houseprice_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #The Shapiro-Wilk test tests the null hypothesis that the data was drawn from a normal distribution.\n# test_normality = lambda x: stats.shapiro(x.fillna(0))[1] < 0.01\n# normal = pd.DataFrame(house_train[quantitative])\n# normal = normal.apply(test_normality)\n# print(not normal.any())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def encode(frame, feature):\n#     ordering = pd.DataFrame()\n#     ordering['val'] = frame[feature].unique()\n#     ordering.index = ordering.val\n#     ordering['spmean'] = frame[[feature, 'SalePrice']].groupby(feature).mean()['SalePrice']\n#     ordering = ordering.sort_values('spmean')\n#     ordering['ordering'] = range(1, ordering.shape[0]+1)\n#     ordering = ordering['ordering'].to_dict()\n    \n#     for cat, o in ordering.items():\n#         frame.loc[frame[feature] == cat, feature+'_E'] = o\n    \n# qual_encoded = []\n# for q in qualitative:  \n#     encode(train, q)\n#     qual_encoded.append(q+'_E')\n# print(qual_encoded)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}