{"cells":[{"metadata":{"_cell_guid":"e81ee64d-e474-4662-9036-ce23df615199","_uuid":"b6269c0e8f417f82daf093dda8fa0da6d2c57d86"},"cell_type":"markdown","source":"This is the basis of my submission to the [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition. This notebook was started based on the lessons on machine learning curated on [Kaggle Learn](https://www.kaggle.com/learn/machine-learning) then refined further to achieve more accuracy and additional visualization.\n\n<br/>\nI also was able to learn some new techniques from the following kernels:\n* [Laurenstc's kernel](https://www.kaggle.com/laurenstc/top-2-of-leaderboard-advanced-fe)\n* [Aleksandrs Gehsbargs's kernel](https://www.kaggle.com/agehsbarg/top-10-0-10943-stacking-mice-and-brutal-force)\n* [Serigne's kernel](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard)\n* [Massquantity's kernel](https://www.kaggle.com/massquantity/all-you-need-is-pca-lb-0-11421-top-4)\n* [Fkstepz's kernel](https://www.kaggle.com/fkstepz/step-by-step-house-prices-prediction)\n\n<br/>\nThe notebook is arranged into the following sections:<br/>\n1. Prep<br/>\n2. Exploratory Visualization and Data Cleansing<br/>\n3. Feature Engineering<br/>\n4. Modeling<br/>\n5. Submit results"},{"metadata":{"_kg_hide-input":true,"_cell_guid":"f633c6d4-4a7c-4c65-a2c5-2d32e8770c6f","_uuid":"a475ed366d9b2935c233868bd6847dafe382564d","trusted":true},"cell_type":"code","source":"from IPython.display import Image\nurl = 'http://ep60qmdjq8-flywheel.netdna-ssl.com/wp-content/uploads/2009/08/commercial-real-estate.jpg'\nImage(url=url,width=800, height=600)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"009915c5-3584-4f5b-b3d2-1fa66dec98c0","_uuid":"d8eaa4c62fc0efe1a4bb55a6eff21dc86fc718d8"},"cell_type":"markdown","source":"# 1. Prep\nImport all libraries, define functions, and load data."},{"metadata":{"_cell_guid":"1b2fee56-4989-4887-9911-1b0c514d94ee","_uuid":"4b67545521f40f58f41bf9ec91cc56f5d0855c4d"},"cell_type":"markdown","source":"**Import all libraries and define functions**"},{"metadata":{"scrolled":false,"_cell_guid":"86b26423-563a-4fa1-a595-89e25ff93089","_uuid":"1c728098629e1301643443b1341556a15c089b2b","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom xgboost import XGBRegressor\nimport warnings\n\n# Turn off the nagging warnings from sklearn and seaborn\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn\n\n# Identify numeric columns\ndef numeric_cols(data_frame):\n    numeric_cols = [cname for cname in data_frame.columns if \n                                data_frame[cname].dtype in ['int64', 'float64']]\n    return(numeric_cols)\n\n# Identify categorical columns with low cardinality (a small number of distinct values)\ndef low_cardinality_cols(data_frame):\n    low_cardinality_cols = [cname for cname in data_frame.columns if \n                                data_frame[cname].nunique() < 50 and\n                                data_frame[cname].dtype == \"object\"]\n    return(low_cardinality_cols)\n\n# Identify columns with missing data\ndef cols_with_missing(data_frame):\n    cols_with_missing = [cname for cname in data_frame.columns \n                                 if data_frame[cname].isnull().any()]\n    return(cols_with_missing)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"58d28dad-9326-443a-88de-6568cc8f2fd2","_uuid":"ef4fce65ddb6d15a05d2f93ca2e86f487dd48a8b"},"cell_type":"markdown","source":"**Load the data**"},{"metadata":{"_cell_guid":"f3401f07-4c4c-4469-aaac-a75bf01672e6","_uuid":"ded85e5015324ca9c284fd09e4a57f4b9a424c14","trusted":true},"cell_type":"code","source":"# Read core training and test data\ntrain_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')\n\nprint(\"Train set size:\", train_data.shape)\nprint(\"Test set size:\", test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"40044b18-7158-4ac1-b675-5f01be41384c","_uuid":"5ff40589f5ced344bc70fd9e7a06b679d35535b8"},"cell_type":"markdown","source":"# 2. Exploratory Visualization and Data Cleansing"},{"metadata":{"_cell_guid":"1214610b-b8bd-4e51-8c63-e65d805df06d","_uuid":"64c7f047a740cbe185304eb0bd1d1da4d0c716d5"},"cell_type":"markdown","source":"**Confirm column overlap between train and test**\n\nWhile it can be assumed that all of the columns match in these data sets, I still wanted to get the code in here to confirm that SalePrice is the only column difference between the two."},{"metadata":{"_cell_guid":"b1f37d35-1429-4084-829f-d2e0bc7711e4","_uuid":"b3110c7d080ee7fa60cfe54619a934791f6d6d0a","trusted":true},"cell_type":"code","source":"print(\"Columns in training data but not in testing data\")\nprint([x for x in train_data.columns if x not in test_data.columns])\nprint(\"Columns in testing data but not in training data\")\nprint([x for x in test_data.columns if x not in train_data.columns])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"253e53ea-1c75-43f3-85d4-fa9ecdb75319","_uuid":"93f4e3140eeb0415a6f537d224e32e7b20388b0b"},"cell_type":"markdown","source":"**View statistical properties of the data**"},{"metadata":{"_cell_guid":"e6e47dc8-7a22-4249-9c5e-d1bacd455534","_uuid":"8fa9d31ff437378d97e976464c28b694a685b581","trusted":true},"cell_type":"code","source":"# view the data and get the statistical properties\ntrain_data.describe(include = 'all')\n#train_data.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7f70c82f-fd90-4fe5-b9d3-de5ae046c324","_uuid":"7a609f6baa0c926921865486b6291d8b85b7f54a"},"cell_type":"markdown","source":"**Drop the Id column since it isn't worth keeping**\n\nBy determining that all values of this column are unique I feel comfortable dropping it"},{"metadata":{"_cell_guid":"d65b7997-a743-4f65-9f41-76c4ea8268a4","_uuid":"7ca47d9b9bac7faa5f7741f304f2a320dde20f24","trusted":true},"cell_type":"code","source":"if train_data['Id'].nunique() == train_data['Id'].size:\n    train_data.drop(['Id'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b7637285-f7cd-4003-90e1-b7f94334b1b1","_uuid":"f92476ef8baec1fb9bdd87ca0ebc5909a40bf4f5"},"cell_type":"markdown","source":"**View correlation of data**\n\nView the correlation map of data with clean colors and elimination of the duplicate display on the upper right hand corner of the chart that comes with it by default."},{"metadata":{"_cell_guid":"16f7f382-a3be-441d-be20-737a9966935d","_uuid":"415e464df99ab71129445904edd881ab77a2b709","trusted":true},"cell_type":"code","source":"corrmat = train_data.corr()\nplt.subplots(figsize=(12,9))\nmask = np.zeros_like(corrmat, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corrmat, mask=mask, vmax=0.9, cmap=\"YlGnBu\",\n            square=True, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ef3f4229-5da1-4b05-b65c-b561fa2ca274","_uuid":"97613378965e9085edbcd091fbabf0a7c0f6b669"},"cell_type":"markdown","source":"**Get the data into  better working form**"},{"metadata":{"_cell_guid":"64ee7c57-44fa-44a8-b610-dbc049e18460","_uuid":"7aaaaab8b89c7b7eda732769258608cc10a81eb0","trusted":true},"cell_type":"code","source":"# Drop houses where the target is missing\ntrain_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n\n# pull all data data into target (y) and predictors (X)\ntrain_y = train_data.SalePrice\n\n# only keep data that is either numeric or has a low cardinality for categorical data\n# in this case it keeps most all of the columns\ntargeted_train_X_cols = low_cardinality_cols(train_data) + numeric_cols(train_data)\ntrain_X = train_data[targeted_train_X_cols]\ntrain_X = train_X.drop(['SalePrice'], axis=1)\ntrain_X = pd.get_dummies(train_X)\n\n# Treat the test data in the same way as training data. In this case, pull same columns.\ntargeted_test_X_cols = low_cardinality_cols(test_data) + numeric_cols(test_data)\ntest_X = test_data[targeted_test_X_cols]\ntest_X = pd.get_dummies(test_X)\n\n# inner join the data to ensure the exact columns included are aligned\ntrain_X, test_X = train_X.align(test_X,\n                                join='inner', \n                                axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"97f299a3-5296-4405-89ea-582e10b87eae","_uuid":"37b92cddc30cb10d4586eccfb953fd08b33e8d3b"},"cell_type":"markdown","source":"**Plot all significant features along with a regression line**"},{"metadata":{"_cell_guid":"83cac469-11cd-4659-b86f-c577d96e2930","collapsed":true,"_uuid":"4e4c573e038f4364e3c6670ed5b95f106b92a4ca","trusted":false},"cell_type":"code","source":"# Get columns that are most correlated to SalePrice to scatter plot them\ntesting_columns = corrmat.loc[(corrmat.index != 'SalePrice'), (corrmat.SalePrice > 0)].index\n\n# Count subplots, total columns of plot, and total rows of plots\nsubplot_count = int(len(testing_columns))\ncol_count = 2\nrow_count = int((subplot_count/col_count)+1)\n\n# Initiate the subplots\nfig, axes = plt.subplots(nrows=row_count, \n                         ncols=col_count, \n                         sharey=True, \n                         figsize=(12,row_count*2), \n                         squeeze=False)\naxes_list = [item for sublist in axes for item in sublist] \nplt.subplots_adjust(top = 0.99, bottom=0.01, hspace=1.5, wspace=0.4)\niter_col_count = 0\niter_row_count = 0\n\n# Loop through all columns to create a subplot for each\nfor i in testing_columns:\n    ax = axes_list.pop(0)\n    ax = sns.regplot(train_X[i],\n                     train_y,\n                     ax=axes[iter_row_count][iter_col_count],\n                     color='blue'\n                    )\n    if iter_col_count == col_count - 1:\n        iter_col_count = 0\n        iter_row_count = iter_row_count + 1\n    else:\n        iter_col_count = iter_col_count + 1\n    ax.set_title(i)\n    ax.tick_params(which='both', bottom='off', left='off',\n                   right='off', top='off'\n                  )\n    ax.spines['left'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n\n# delete anything we didn't use\nfor ax in axes_list:\n    ax.remove()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"49846191-a439-4110-92e2-b186329b8abb","_uuid":"07543345fa7f7e2c475d3a507c0837732eaec207"},"cell_type":"markdown","source":"**Plot sales price against Living Area sliced by Overall Quality**"},{"metadata":{"scrolled":false,"_cell_guid":"67bb68ef-55d4-433d-9207-aee12b2fbe47","_uuid":"766559b868866820d3ebda92e0432abdba9643b5","trusted":true},"cell_type":"code","source":"# Check out \nexplore_data = train_X.copy()\nexplore_data['SalePrice'] = train_data.SalePrice\n\nsns.lmplot(x='GrLivArea', y='SalePrice', hue='OverallQual',\n           #markers=['o', 'x', '*'], \n           data=explore_data, fit_reg=False)\n\nsns.lmplot(x='GrLivArea', y='SalePrice', hue='OverallQual',\n           #markers=['o', 'x', '*'], \n           data=explore_data.loc[explore_data['OverallQual'] == 10], fit_reg=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b15b3638-a041-4b33-96fd-b6a2cbb23aab","_uuid":"6e1bcbd8981028881f7dbd9f7f425f33b5ab86e7"},"cell_type":"markdown","source":"**Run outlier detection on Overall Quality of 10**\n\nFrom the chart above we can clearly see that there are a few outliers for quality 10 so we'll run the Local Outlier Factor to detect these outliers. (anything with a -1 is an outlier)"},{"metadata":{"_cell_guid":"cd114148-b9f9-4b2a-8039-9acc8727c7ec","_uuid":"b7958039e2759c0fec3b5a6e6e28b2fe05bb1455","trusted":true},"cell_type":"code","source":"# detect outliers\noutlier_detect = explore_data.loc[explore_data['OverallQual'] == 10]\noutlier_detect = outlier_detect[['SalePrice','GrLivArea']]\noutlier_detect['outlier'] = LocalOutlierFactor(n_neighbors=20).fit_predict(outlier_detect)\nprint(outlier_detect)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"efb3f34d-0be3-4c18-979f-baca63f61147","_uuid":"dd246a639c087d8246679e69d07d6fe301816a75"},"cell_type":"markdown","source":"**Remove the outliers from our data set**"},{"metadata":{"_cell_guid":"819032eb-2348-4e21-97f4-34ac16a2ac32","_uuid":"17ff2b0952609ca102c3ac4540257ddd32c8f66c","trusted":true},"cell_type":"code","source":"# remove outliers from primary training data sets\noutliers = outlier_detect.loc[outlier_detect['outlier']==-1].index.values\nprint('Shape of data prior to removal')\nprint(train_X.shape)\nprint(train_y.shape)\ntrain_X.drop(outliers, inplace=True)\ntrain_y.drop(outliers, inplace=True)\nprint('Shape of data after removal')\nprint(train_X.shape)\nprint(train_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"297ceee9-098c-48a3-ac15-af0f3f773bc2","_uuid":"946dd5d784b98a58caaa14bca6cdc470a3f75901"},"cell_type":"markdown","source":"**Check for skew in the sales price vs transform with log**"},{"metadata":{"_cell_guid":"4eee5221-2a15-4749-b84b-6efb8fb6f7ad","_uuid":"ef277294b2e310311e047757805aa25996dd0ab3","trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(12,9))\nplt.subplot(1, 2, 1)\nsns.distplot(train_y)\n\nplt.subplot(1, 2, 2)\nsns.distplot(np.log1p(train_y))\nplt.xlabel('Log SalePrice')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"457dd469-4bc6-403e-8483-5aeaff97ad85","collapsed":true,"_uuid":"f1b94ead9a41fe8e7b7fcfc3b3866a0b021c22ac","trusted":false},"cell_type":"code","source":"# perform log transformation to reduce skew\n#train_y = np.log1p(train_y)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0c882fed-daa5-4d2a-ac8b-13da13262612","_uuid":"ad79e6f554b292c3e4f3e874da2868d17c436b50"},"cell_type":"markdown","source":"# 3. Feature engineering"},{"metadata":{"_cell_guid":"54f90dea-6ce0-4c4f-8c00-6462590958d3","_uuid":"d6332dcb6fbda7a2862e2fcbce8904946921edc6"},"cell_type":"markdown","source":"**Sum multiple features together to get overall totals**"},{"metadata":{"_cell_guid":"f968c403-77ba-476f-a412-7a2b2ef09ada","_uuid":"f4e7dd0cf28d74c919d535d6241fbe1576b2e75a","trusted":true},"cell_type":"code","source":"# Add Total SF\nfeatures_to_sum = ['TotalBsmtSF','1stFlrSF','2ndFlrSF']\ntrain_X['newTotalSF'] = train_X[features_to_sum].sum(axis=1)\ntest_X['newTotalSF'] = test_X[features_to_sum].sum(axis=1)\n\n# Add Total Finished SF\n#features_to_sum = ['BsmtFinSF1','BsmtFinSF2','1stFlrSF','2ndFlrSF']\n#train_X['newFinTotalSF'] = train_X[features_to_sum].sum(axis=1)\n#test_X['newFinTotalSF'] = test_X[features_to_sum].sum(axis=1)\n\n# Add Total Bathrooms\n#features_to_sum = ['FullBath','HalfBath','BsmtFullBath','BsmtHalfBath']\n#train_X['newTotalBath'] = train_X[features_to_sum].sum(axis=1)\n#test_X['newTotalBath'] = test_X[features_to_sum].sum(axis=1)\n\n# Add Total Porch SF\n#features_to_sum = ['OpenPorchSF','3SsnPorch','EnclosedPorch','ScreenPorch','WoodDeckSF']\n#train_X['newTotalPorchSF'] = train_X[features_to_sum].sum(axis=1)\n#test_X['newTotalPorchSF'] = test_X[features_to_sum].sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"202c7ffa-1111-4b13-97dc-8119fb701250","_uuid":"bec92d903e3f32b29d63655276f91e2e4bef7228"},"cell_type":"markdown","source":"# 4. Modeling\nBuild best model and predict final results"},{"metadata":{"_cell_guid":"783c1c18-6c6b-4e7e-9b21-0e9f74af4091","_uuid":"7d701c9678ebe63334b94c46fade40c29d23242e"},"cell_type":"markdown","source":"**Create pipeline with models**"},{"metadata":{"_cell_guid":"231078a8-1940-45af-bd4a-5efba91c5732","_uuid":"80f5ca3117417ee60941ab3d408688817ed826ca","trusted":true},"cell_type":"code","source":"# create pipeline\nmy_pipeline = make_pipeline(Imputer(),\n                            XGBRegressor(n_estimators=1000, \n                                         learning_rate=0.05)\n                           )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d3204ddb-e70a-48c0-8a0d-e8f9433170fe","_uuid":"8be1f1708a3e709e236bf8291fdffd0300defd53"},"cell_type":"markdown","source":"**Score pipeline**"},{"metadata":{"_cell_guid":"fd41a07a-7a35-4315-9351-3897a31b1c87","_uuid":"7df2855ed1eb4e0ae183b616a9720a1176df69ee","trusted":true},"cell_type":"code","source":"# Scoring - Root Mean Squared Error\ndef rmse_score(model,X,y):\n    return np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5))\n\n# Scoring - Mean Absolute Error\ndef mae_score(model,X,y):\n    return -cross_val_score(model, X, y, scoring=\"neg_mean_absolute_error\", cv=5)\n\n# score the pipeline\n#mae = mae_score(my_pipeline,train_X,train_y)\n#print('Mean Absolute Error: {}'.format(mae.mean()))\n#rmse = rmse_score(my_pipeline,train_X,train_y)\n#print('Root Mean Squared Error: {}'.format(rmse.mean()))\nrmse_log = rmse_score(my_pipeline,train_X,np.log1p(train_y))\nprint('Root Mean Squared Error with log tranform: {}'.format(rmse_log.mean()))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ffa78778-5262-4309-be2a-407ecb3d5a90","_uuid":"f80fc6cc072d1891ea1149f1bbc7627d4a47009a"},"cell_type":"markdown","source":"**Train the model**"},{"metadata":{"_cell_guid":"88fd3206-0c3d-4385-8a32-64351b57c79a","_uuid":"65e89dd098bcd438c08c7a3fc5d8788688ff2238","trusted":true},"cell_type":"code","source":"# train the model\nmodel = my_pipeline.fit(train_X, train_y)\n\n# Use the model to make predictions\nfinal_predicted_prices = model.predict(test_X)\n\n# transform log values for sale price back into regular sale price\n#final_predicted_prices = np.expm1(final_predicted_prices)\n\n# round final prices\nfinal_predicted_prices = [round(x,2) for x in final_predicted_prices]\n\n# look at the predicted prices to ensure we have something sensible.\nprint(final_predicted_prices[:5])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ad6aff50bf2d386bf0b58bf06e6f78eb8cff028"},"cell_type":"markdown","source":"**Permutation Importance - Importance of various features**"},{"metadata":{"trusted":true,"_uuid":"ed0bb9ee39fd15b5e20850abd2ed16fcfdb7d0f4"},"cell_type":"code","source":"#TO DO - if I want this to work it seems I'll need to concentrate on only numeric values\n\n#import eli5\n#from eli5.sklearn import PermutationImportance\n\n# I'm using training data here for now since I didn't take the time to split out validation data \n#perm = PermutationImportance(model, random_state=1).fit(train_X, train_y)\n\n#eli5.show_weights(perm, feature_names = train_X.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6b5cf662bf4b038359f125e859f81a02ac59256"},"cell_type":"markdown","source":"**Partial Dependence Plot (PDP)**"},{"metadata":{"trusted":true,"_uuid":"f896798b7e493eca31757b613c83690296c7082c"},"cell_type":"code","source":"from pdpbox import pdp, get_dataset, info_plots\n\ncompare_cols = [cname for cname in test_X.columns]\n\nfeatures_to_review = ['OverallQual','YearBuilt','MoSold','Fireplaces']\n\nfor feat_name in features_to_review:\n    pdp_dist = pdp.pdp_isolate(model=model, dataset=test_X, model_features=compare_cols, feature=feat_name, num_grid_points=100)\n    pdp.pdp_plot(pdp_dist, feat_name)\n    plt.show()\n\n#help(pdp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d25458f4930e4136c774a620af4cc57051d65371"},"cell_type":"code","source":"features_to_review2 = ['OverallQual','YearBuilt']\n\npdp_inter1 = pdp.pdp_interact(model=model, dataset=test_X, model_features=compare_cols, features=features_to_review2)\npdp.pdp_interact_plot(pdp_interact_out=pdp_inter1, feature_names=features_to_review2, plot_type='contour')\nplt.show()    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1266d2682d733f95c76bbc10964626e8ced05ffc"},"cell_type":"markdown","source":"**SHAP Values for one house**"},{"metadata":{"trusted":true,"_uuid":"23ae7f5f5e90ad4a02afb05837de2860642a85a2"},"cell_type":"code","source":"import shap  # package used to calculate Shap values\n\ndef prediction_factors(model, predict_data, core_data):\n    # Had to use KernelExplainer instead of TreeExplainer due to pipeline\n    explainer = shap.KernelExplainer(model, core_data)\n    shap_values = explainer.shap_values(predict_data)\n    shap.initjs()\n    return shap.force_plot(explainer.expected_value[1], shap_values[1], predict_data)\n\nsample_data_for_prediction = test_X.iloc[0]\nprediction_factors(my_pipeline, sample_data_for_prediction, test_X)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"09e5bfde-db24-4432-bd48-24298ebb0f05","_uuid":"63a5064af463adb14380fb8c4eed4a14e9433e02"},"cell_type":"markdown","source":"# 5. Submit results"},{"metadata":{"_cell_guid":"2f5d4f77-e594-4cd2-a9e3-c8b1d1cc9152","collapsed":true,"_uuid":"dceebe60bfa69f1926c18881c7e57715b9d33535","trusted":false},"cell_type":"code","source":"# submit results\nmy_submission = pd.DataFrame({'Id': test_data.Id, 'SalePrice': final_predicted_prices})\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}