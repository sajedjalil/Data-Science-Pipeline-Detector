{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n!pip install --upgrade seaborn\nimport seaborn as sns\n\nnp.random.seed(42)\n\nplt.rcParams.update({'figure.max_open_warning': 0})\npd.options.mode.chained_assignment = None\npd.set_option('display.max_rows', 250)\npd.set_option('display.max_columns', 250)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold, cross_val_score\nfrom skopt.space import Real, Integer\nfrom skopt import BayesSearchCV\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.feature_selection import VarianceThreshold\n!pip install --upgrade lightgbm\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nimport lightgbm as lgb\nimport xgboost as xgb","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. General Infos About Data\nLoading, describing, getting general infos about data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\n\ntrain_id = train.Id\ntest_id = test.Id\n\nprint(\"Train set: \", train.shape)\nprint(\"Test set: \", test.shape)\n\ntrain.info()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe().round(3)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe(include=[\"O\"])","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([train, test])\ndf","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Handling Missing Values\nInstead of mode or median encoding, I fill missing values with similarities between the observations. It is manual process, takes more time than statistical imputing methods, but we get more insghts about data. Also it creates a chance to correctting some infos like garage features or totalbasement etc","metadata":{}},{"cell_type":"code","source":"def col_types(df):\n    num_cols = df.loc[:, df.dtypes != \"object\"].columns.tolist()\n    cat_cols = df.loc[:, df.dtypes == \"object\"].columns.tolist()\n    ord_cols = []\n    for col in num_cols:\n        if df[col].value_counts().size < 20:\n            ord_cols.append(col)\n\n    num_cols = [x for x in num_cols if x not in ord_cols + [\"Id\", \"SalePrice\"]]\n    \n    return num_cols, cat_cols, ord_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def missing(df):\n    miss = pd.DataFrame({\"no_missing_values\": df.isnull().sum(),\n                         \"missing_value_ratio\": (df.isnull().sum() / df.shape[0]).round(4),\n                         \"missing_in_train\": df[df.SalePrice.notnull()].isnull().sum(),\n                         \"missing_in_test\": df[df.SalePrice.isnull()].isnull().sum()})\n    return miss[miss.no_missing_values > 0].sort_values(\"no_missing_values\", ascending=False)\n\nmissing(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[(df.GarageFinish.isnull()) & (df.GarageType.notnull())]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[(df.GarageFinish.isnull()) & (df.GarageType.notnull()), \"GarageFinish\"] = \"Fin\"\ndf.loc[(df.GarageCars.isnull()) & (df.GarageType.notnull()), \"GarageCars\"] = 1\ndf.loc[(df.GarageQual.isnull()) & (df.GarageType.notnull()), \"GarageQual\"] = \"TA\"\ndf.loc[(df.GarageCond.isnull()) & (df.GarageType.notnull()), \"GarageCond\"] = \"TA\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df.GarageYrBlt == df.YearBuilt].shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[(df.GarageYrBlt.isnull()) & (df.GarageType.notnull()), \"GarageYrBlt\"] = df.loc[(df.GarageYrBlt.isnull()) & (df.GarageType.notnull())].YearBuilt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[(df.GarageType == \"Detchd\") & (df.YearBuilt < 1930) & (df.YearRemodAdd < 2000) & (df.YearRemodAdd > 1980) & (df.GarageCars == 1)].GarageArea.median()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[(df.GarageArea.isnull()) & (df.GarageType.notnull()), \"GarageArea\"] = 234","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_cols, cat_cols, ord_cols = col_types(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"none_cols = [\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\", \"GarageCond\", \"GarageFinish\", \"GarageQual\", \n             \"GarageType\", \"BsmtExposure\", \"BsmtCond\", \"BsmtQual\", \"BsmtFinType1\", \"MasVnrType\", \"BsmtFinType2\"]\n\nfor col in none_cols: \n    df[col].fillna(\"None\", inplace=True)\n    \nmissing(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[(df.MasVnrArea.isnull()) & (df.MasVnrType == \"None\"), \"MasVnrArea\"] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in [\"BsmtFullBath\", \"BsmtHalfBath\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"GarageYrBlt\"]:\n    df[col].fillna(0, inplace = True)\n    \ndf[\"TotalBsmtSF\"].fillna(df[\"BsmtFinSF1\"] + df[\"BsmtFinSF2\"] + df[\"BsmtUnfSF\"], inplace = True)\n\nmissing(df)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.MSZoning.value_counts())\n\ndf[df.MSZoning.isnull()]","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby(\"Neighborhood\").MSZoning.value_counts()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[(df.MSZoning.isnull()) & (df.Neighborhood == \"IDOTRR\"), \"MSZoning\"] = \"C (all)\"\ndf.loc[(df.MSZoning.isnull()) & (df.Neighborhood == \"Mitchel\"), \"MSZoning\"] = \"RL\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.KitchenQual.value_counts()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.groupby([\"OverallQual\", \"KitchenAbvGr\"]).KitchenQual.value_counts())\ndf.loc[(df.KitchenQual.isnull()) & (df.OverallQual == 5 ) & (df.KitchenAbvGr == 1), \"KitchenQual\"] = \"TA\"","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.SaleType.value_counts())\n\ndf[df.SaleType.isnull()]","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.groupby([\"Neighborhood\", \"SaleCondition\"]).SaleType.value_counts())\ndf.loc[(df.SaleType.isnull()) & (df.Neighborhood == \"Sawyer\" ) & (df.SaleCondition == \"Normal\"), \"SaleType\"] = \"WD\"","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.Electrical.value_counts())\n\ndf[df.Electrical.isnull()]","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[(df.YearBuilt > 2005)].Electrical.value_counts()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Electrical.fillna(\"SBrkr\", inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.Exterior1st.value_counts())\n\ndf[df.Exterior1st.isnull()]","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df[(df.RoofMatl == \"Tar&Grv\")].Exterior1st.value_counts())\ndf.Exterior1st.fillna(\"Plywood\", inplace=True)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df[(df.RoofMatl == \"Tar&Grv\")].Exterior2nd.value_counts())\ndf.Exterior2nd.fillna(\"Plywood\", inplace=True)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.Functional.value_counts())\n\ndf[df.Functional.isnull()]","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[(df.Neighborhood == \"IDOTRR\") &  (df.OverallQual < 5) & (df.YearRemodAdd < 1960) & (df.ExterQual == \"Fa\")].Functional.value_counts()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Functional.fillna(\"Mod\", inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"LotFrontage\"] = df[\"LotFrontage\"].fillna(df.groupby([\"Neighborhood\", \"LotShape\", \"LotConfig\"])[\"LotFrontage\"].transform(\"median\"))\ndf[\"LotFrontage\"] = df[\"LotFrontage\"].fillna(df.groupby([\"Neighborhood\", \"LotShape\"])[\"LotFrontage\"].transform(\"median\"))\ndf[\"LotFrontage\"] = df[\"LotFrontage\"].fillna(df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\"median\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Utilities.fillna(df.Utilities.mode()[0], inplace=True)\n\nmissing(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"MSSubClass\"] = df[\"MSSubClass\"].astype(\"str\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[(df.GarageYrBlt == 2207), \"GarageYrBlt\"] = 2007\n\ndf.loc[(df.Exterior2nd == \"CmentBd\"), \"Exterior2nd\"] = \"CemntBd\"\ndf.loc[(df.Exterior2nd == \"Wd Shng\"), \"Exterior2nd\"] = \"WdShing\"\ndf.loc[(df.Exterior2nd == \"Brk Cmn\"), \"Exterior2nd\"] = \"BrkComm\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Comprehensive Eda & Feature Engineering\n\nYou can take a look for my notebook to eda for classification and regression. https://www.kaggle.com/mustafacicek/eda-for-classification-regression-notestomyself\n\n\"bar_box\" --> includes countplot for train data, countplot for test data and boxplot for target to each category. it helps us to understand distribution of categories and distribution of target over categories\n\n\"plot_scatter\" --> includes scatter plot for target and feature. it shows the correlation coefficient between them and coloring for correlation's degree. it help us to understand relationship between continuous numerical features and target.\n\n\"feature_distribution\" --> includes kdeplot, boxplot and probplot for continuous numerical features.\n\nDefining these functions helps us because machine learning is an iterative process. You need to try different things over and over.","metadata":{}},{"cell_type":"code","source":"dff = df.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_cols, cat_cols, ord_cols = col_types(dff)\n\nfor col in dff.columns:\n    print(\"For column: \", col + \"\\n\")\n    print(dff[col].value_counts(), \"\\n\")","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bar_box(df, col, target = \"SalePrice\"):\n    \n    sns.set_style(\"darkgrid\")\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharex = True)\n    \n    order = sorted(df[col].unique())\n    \n    sns.countplot(data = df[df[target].notnull()], x = col, ax = axes[0], order = order)    \n    sns.countplot(data = df[df[target].isnull()], x = col, ax = axes[1], order = order)    \n    sns.boxplot(data = df, x = col, ax = axes[2], y = target, order = order)\n    \n    fig.suptitle(\"For Feature:  \" + col)\n    axes[0].set_title(\"in Training Set \")\n    axes[1].set_title(\"in Test Set \")\n    axes[2].set_title(col + \" --- \" + target)\n    \n    for ax in fig.axes:\n        plt.sca(ax)\n        plt.xticks(rotation=90)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_scatter(df, col, target = \"SalePrice\"):\n    sns.set_style(\"darkgrid\")\n    \n    corr = df[[col, target]].corr()[col][1]    \n    c = [\"red\"] if corr >= 0.7 else ([\"brown\"] if corr >= 0.3 else\\\n                                    ([\"lightcoral\"] if corr >= 0 else\\\n                                    ([\"blue\"] if corr <= -0.7 else\\\n                                    ([\"royalblue\"] if corr <= -0.3 else [\"lightskyblue\"]))))    \n\n    fig, ax = plt.subplots(figsize = (5, 5))\n    \n    sns.scatterplot(x = col, y = target, data = df, c = c, ax = ax)        \n    ax.set_title(\"Correlation between \" + col + \" and \" + target + \" is: \" + str(corr.round(4)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_distribution(df, col, target = \"SalePrice\", test = True):\n    sns.set_style(\"darkgrid\")\n    if test == True:\n        fig, axes = plt.subplots(1, 5, figsize=(25, 5))\n\n        sns.kdeplot(data = df[df[target].notnull()], x = col, fill=True, label = \"Train\", ax = axes[0], color = \"orangered\")\n        sns.kdeplot(data = df[df[target].isnull()], x = col, fill=True, label = \"Test\", ax = axes[0], color = \"royalblue\")\n        axes[0].set_title(\"Distribution\")\n        axes[0].legend(loc = \"best\")\n        \n        sns.boxplot(data = df[df[target].notnull()], y = col, ax = axes[1], color = \"orangered\")\n        sns.boxplot(data = df[df[target].isnull()], y = col, ax = axes[2], color = \"royalblue\")\n        axes[2].set_ylim(axes[1].get_ylim())        \n        axes[1].set_title(\"Boxplot For Train Data\")\n        axes[2].set_title(\"Boxplot For Test Data\")\n        \n\n        stats.probplot(df[df[target].notnull()][col], plot = axes[3])\n        stats.probplot(df[df[target].isnull()][col], plot = axes[4])\n        axes[4].set_ylim(axes[3].get_ylim())        \n        axes[3].set_title(\"Probability Plot For Train data\")\n        axes[4].set_title(\"Probability Plot For Test data\")\n        \n        fig.suptitle(\"For Feature:  \" + col)\n    else:\n        fig, axes = plt.subplots(1, 3, figsize = (18, 6))\n        \n        sns.kdeplot(data = df, x = col, fill = True, ax = axes[0], color = \"orangered\")\n        sns.boxplot(data = df, y = col, ax = axes[1], color = \"orangered\")\n        stats.probplot(df[col], plot = axes[2])\n        \n        axes[0].set_title(\"Distribution\")\n        axes[1].set_title(\"Boxplot\")\n        axes[2].set_title(\"Probability Plot\")\n        fig.suptitle(\"For Feature:  \" + col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.1 For Categorical Features\n\nConverting some features to ordinal, extracting infos from features, combining bins that have same characteristics","metadata":{}},{"cell_type":"code","source":"for col in cat_cols:\n    bar_box(dff, col)","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dff[cat_cols]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dff[\"Older1945\"] = dff[\"MSSubClass\"].apply(lambda x: 1 if x in [\"30\", \"70\"] else 0)\n\ndff[\"Newer1946\"] = dff[\"MSSubClass\"].apply(lambda x: 1 if x in [\"20\", \"60\", \"120\", \"160\"] else 0)\n\ndff[\"AllStyles\"] = dff[\"MSSubClass\"].apply(lambda x: 1 if x in [\"20\", \"90\", \"190\"] else 0)\n\ndff[\"AllAges\"] = dff[\"MSSubClass\"].apply(lambda x: 1 if x in [\"40\", \"45\", \"50\", \"75\", \"90\", \"150\", \"190\"] else 0)\n\ndff[\"Pud\"] = dff[\"MSSubClass\"].apply(lambda x: 1 if x in [\"120\", \"150\", \"160\", \"180\"] else 0)\n\ndff[\"Split\"] = dff[\"MSSubClass\"].apply(lambda x: 1 if x in [\"80\", \"85\"\"180\"] else 0)\n\ndff[\"MSSubClass\"] = dff[\"MSSubClass\"].apply(lambda x: \"180\" if x == \"150\" else x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dff[\"Density\"] = dff[\"MSZoning\"].apply(lambda x: 1 if x in [\"RL\", \"RP\"] else (2 if x in [\"RM\", \"RH\"] else 0))\n\ndff[\"MSZoning\"] = dff[\"MSZoning\"].apply(lambda x: \"R\" if x.startswith(\"R\") else x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**These are just an example of using external data on feature engineering. If you can use external data on feature engineering you should always try it.**","metadata":{}},{"cell_type":"code","source":"dff[\"North\"] = dff[\"Neighborhood\"].apply(lambda x: 1 if x in [\"Blmngtn\", \"BrDale\", \"ClearCr\", \"Gilbert\",  \"Names\", \"NoRidge\", \n                                                              \"NPkVill\", \"NWAmes\", \"NoRidge\", \"NridgHt\", \"Sawyer\", \"Somerst\", \n                                                              \"StoneBr\", \"Veenker\", \"NridgHt\"] else 0)\n\ndff[\"South\"] = dff[\"Neighborhood\"].apply(lambda x: 1 if x in [\"Blueste\", \"Edwards\", \"Mitchel\", \"MeadowV\", \n                                                              \"SWISU\", \"IDOTRR\", \"Timber\"] else 0)\n\ndff[\"Downtown\"] = dff[\"Neighborhood\"].apply(lambda x: 1 if x in [\"BrkSide\", \"Crawfor\", \"OldTown\", \"CollgCr\"] else 0)\n\ndff[\"East\"] = dff[\"Neighborhood\"].apply(lambda x: 1 if x in [\"IDOTRR\", \"Mitchel\"] else 0)\n\ndff[\"West\"] = dff[\"Neighborhood\"].apply(lambda x: 1 if x in [\"Edwards\", \"NWAmes\", \"SWISU\", \"Sawyer\", \"SawyerW\"] else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dff.loc[(dff[\"Condition1\"] == \"Feedr\") | (dff[\"Condition2\"] == \"Feedr\"), \"StreetDegree\"] = 1\ndff.loc[(dff[\"Condition1\"] == \"Artery\") | (dff[\"Condition2\"] == \"Artery\"), \"StreetDegree\"] = 2\ndff[\"StreetDegree\"].fillna(0, inplace = True)\n\ndff.loc[(dff[\"Condition1\"].isin([\"RRNn\", \"RRNe\"])) | (dff[\"Condition2\"].isin([\"RRNn\", \"RRNe\"])), \"RailroadDegree\"] = 1\ndff.loc[(dff[\"Condition1\"].isin([\"RRAn\", \"RRAe\"])) | (dff[\"Condition2\"].isin([\"RRAn\", \"RRAe\"])), \"RailroadDegree\"] = 2\ndff[\"RailroadDegree\"].fillna(0, inplace = True)\n\ndff.loc[(dff[\"Condition1\"] == \"PosN\") | (dff[\"Condition2\"] == \"PosN\"), \"OffsiteFeature\"] = 1\ndff.loc[(dff[\"Condition1\"] == \"PosA\") | (dff[\"Condition2\"] == \"PosA\"), \"OffsiteFeature\"] = 2\ndff[\"OffsiteFeature\"].fillna(0, inplace = True)\n\ndff[\"Norm1\"] = dff[\"Condition1\"].apply(lambda x: 1 if x == \"Norm\" else 0)\ndff[\"Norm2\"] = dff[\"Condition2\"].apply(lambda x: 1 if x == \"Norm\" else 0)\ndff[\"Norm\"] = dff[\"Norm1\"] + dff[\"Norm2\"]\ndff.drop([\"Norm1\", \"Norm2\"], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lotshape = {\"IR3\": 1, \"IR2\": 2, \"IR1\": 3, \"Reg\": 4}\nlandcontour = {\"Low\":1, \"HLS\": 2, \"Bnk\":3, \"Lvl\": 4}\nutilities = {\"ELO\": 1, \"NoSeWa\": 2, \"NoSewr\": 3, \"AllPub\": 4}\nlandslope = {\"Sev\": 1, \"Mod\": 2, \"Gtl\": 3}\n\ngeneral = {\"None\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n\nbsmtexposure = {\"None\": 0, \"No\": 0, \"Mn\": 1, \"Av\": 2, \"Gd\": 3}\nbsmtfintype = {\"None\": 0, \"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5, \"GLQ\": 6}\nelectrical = {\"Mix\": 1, \"FuseP\": 2, \"FuseF\": 3, \"FuseA\": 4, \"SBrkr\": 5}\nfunctional = {\"Typ\": 1, \"Min1\": 2, \"Min2\": 3, \"Mod\": 4, \"Maj1\": 5, \"Maj2\": 6, \"Sev\": 7, \"Sal\": 8}\ngaragefinish = {\"None\": 0, \"Unf\": 1, \"RFn\": 2, \"Fin\": 3}\nfence = {\"None\": 0, \"MnWw\": 1, \"GdWo\": 2, \"MnPrv\": 3, \"GdPrv\": 4}\n\ndff.replace({\"LotShape\": lotshape, \"LandContour\": landcontour, \"Utilities\": utilities, \"LandSlope\": landslope, \n             \"BsmtExposure\": bsmtexposure, \"BsmtFinType1\": bsmtfintype, \"BsmtFinType2\":bsmtfintype, \"Electrical\": electrical, \n             \"Functional\": functional, \"GarageFinish\": garagefinish, \"Fence\": fence}, \n             inplace = True)\n\nfor col in [\"ExterQual\", \"ExterCond\", \"BsmtQual\", \"BsmtCond\", \"HeatingQC\", \"KitchenQual\", \n            \"FireplaceQu\", \"GarageQual\", \"GarageCond\", \"PoolQC\"]:\n    dff[col] = dff[col].replace(general)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For categorical features, some categories have less observations. We get a lot of dummy variables if we use one-hot encoding and that causes increase on data and maybe some infos get lost. To prevent this, combine categories that have same characteristics.","metadata":{}},{"cell_type":"code","source":"dff[\"BldgType\"] = dff[\"BldgType\"].apply(lambda x: \"2Fam\" if x in [\"2fmCon\", \"Duplex\"] else x)\n\ndff[\"SaleType\"] = dff[\"SaleType\"].apply(lambda x: \"WD\" if x.endswith(\"WD\") else x)\ndff[\"SaleType\"] = dff[\"SaleType\"].apply(lambda x: \"Contract\" if x.startswith(\"Con\") else x)\ndff[\"SaleType\"] = dff[\"SaleType\"].apply(lambda x: \"Oth\" if x == \"COD\" else x)\n\ndff[\"SaleCondition\"] = dff[\"SaleCondition\"].apply(lambda x: \"Abnormal_Adjland\" if x in [\"Abnorml\", \"AdjLand\"] else x)\ndff[\"SaleCondition\"] = dff[\"SaleCondition\"].apply(lambda x: \"Alloca_Family\" if x in [\"Alloca\", \"Family\"] else x)\ndff[\"SaleCondition\"] = dff[\"SaleCondition\"].apply(lambda x: \"Other\" if x in [\"Abnormal_Adjland\", \"Alloca_Family\"] else x)\n\ndff[\"GarageType\"] = dff[\"GarageType\"].apply(lambda x: \"Carport_None\" if x in [\"CarPort\", \"None\"] else x)\ndff[\"GarageType\"] = dff[\"GarageType\"].apply(lambda x: \"Basement_2Types\" if x in [\"Basment\", \"2Types\"] else x)\n\ndff[\"LotConfig\"] = dff[\"LotConfig\"].apply(lambda x: \"CulDSac_FR3\" if x in [\"CulDSac\", \"FR3\"] else x)\n\ndff[\"RoofStyle\"] = dff[\"RoofStyle\"].apply(lambda x: \"Other\" if x not in [\"Gable\"] else x)\ndff[\"RoofMatl\"] = dff[\"RoofMatl\"].apply(lambda x: \"Other\" if x != \"CompShg\" else x)\ndff[\"MasVnrType\"] = dff[\"MasVnrType\"].apply(lambda x: \"None_BrkCmn\" if x in [\"None\", \"BrkCmn\"] else x)\n\ndff[\"Foundation\"] = dff[\"Foundation\"].apply(lambda x: \"BrkTil_Stone\" if x in [\"BrkTil\", \"Stone\"] else x)\ndff[\"Foundation\"] = dff[\"Foundation\"].apply(lambda x: \"BrkTil_Stone_Slab\" if x in [\"BrkTil_Stone\", \"Slab\"] else x)\ndff[\"Foundation\"] = dff[\"Foundation\"].apply(lambda x: \"PConc_Wood\" if x in [\"PConc\", \"Wood\"] else x)\n\ndff[\"Heating\"] = dff[\"Heating\"].apply(lambda x: \"Other\" if x != \"GasA\" else x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.2 For Numerical Features\nCreating features with using feature interactions, creating binary features, new features with using ordinal ones","metadata":{}},{"cell_type":"code","source":"for col in num_cols:\n    feature_distribution(dff, col)","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in num_cols:\n    plot_scatter(dff, col)","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dff[num_cols]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dff[\"FrontageRatio\"] = (dff[\"LotFrontage\"] / dff[\"LotArea\"])\ndff[\"HQFloor\"] = dff[\"1stFlrSF\"] + dff[\"2ndFlrSF\"]\ndff[\"FloorAreaRatio\"] = dff[\"GrLivArea\"] / dff[\"LotArea\"]\n\ndff[\"TotalArea\"] = dff[\"TotalBsmtSF\"] + dff[\"GrLivArea\"]\ndff[\"TotalPorch\"] = dff[\"WoodDeckSF\"] + dff[\"OpenPorchSF\"] + dff[\"EnclosedPorch\"] + dff[\"3SsnPorch\"] + dff[\"ScreenPorch\"]\n\ndff[\"WeightedBsmtFinSF1\"] = dff[\"BsmtFinSF1\"] * dff[\"BsmtFinType1\"]\ndff[\"WeightedBsmtFinSF2\"] = dff[\"BsmtFinSF2\"] * dff[\"BsmtFinType2\"]\ndff[\"WeightedTotalBasement\"] =  dff[\"WeightedBsmtFinSF1\"] + dff[\"BsmtFinSF2\"] * dff[\"BsmtFinType2\"] +  dff[\"BsmtUnfSF\"]\n\ndff[\"TotalFullBath\"] = dff[\"BsmtFullBath\"] + dff[\"FullBath\"]\ndff[\"TotalHalfBath\"] = dff[\"BsmtHalfBath\"] + dff[\"HalfBath\"]\n\ndff[\"TotalBsmtBath\"] = dff[\"BsmtFullBath\"] + 0.5 * dff[\"BsmtHalfBath\"]\ndff[\"TotalBath\"] = dff[\"TotalFullBath\"] + 0.5 * (dff[\"BsmtHalfBath\"] + dff[\"HalfBath\"]) + dff[\"BsmtFullBath\"] + 0.5 * dff[\"BsmtHalfBath\"]\n\ndff[\"HasPool\"] = dff[\"PoolArea\"].apply(lambda x: 0 if x == 0 else 1)\ndff[\"Has2ndFlr\"] = dff[\"2ndFlrSF\"].apply(lambda x: 0 if x == 0 else 1)\ndff[\"HasBsmt\"] = dff[\"TotalBsmtSF\"].apply(lambda x: 0 if x == 0 else 1)\ndff[\"HasFireplace\"] = dff[\"Fireplaces\"].apply(lambda x: 0 if x == 0 else 1)\ndff[\"HasGarage\"] = dff[\"GarageArea\"].apply(lambda x: 0 if x == 0 else 1)\ndff[\"HasLowQual\"] = dff[\"LowQualFinSF\"].apply(lambda x: 0 if x == 0 else 1)\ndff[\"HasPorch\"] = dff[\"TotalPorch\"].apply(lambda x: 0 if x == 0 else 1)\ndff[\"HasMiscVal\"] = dff[\"MiscVal\"].apply(lambda x: 0 if x == 0 else 1)\ndff[\"HasWoodDeck\"] = dff[\"WoodDeckSF\"].apply(lambda x: 0 if x == 0 else 1)\ndff[\"HasOpenPorch\"] = dff[\"OpenPorchSF\"].apply(lambda x: 0 if x == 0 else 1)\ndff[\"HasEnclosedPorch\"] = dff[\"EnclosedPorch\"].apply(lambda x: 0 if x == 0 else 1)\ndff[\"Has3SsnPorch\"] = dff[\"3SsnPorch\"].apply(lambda x: 0 if x == 0 else 1)\ndff[\"HasScreenPorch\"] = dff[\"ScreenPorch\"].apply(lambda x: 0 if x == 0 else 1)\n\ndff[\"TotalPorchType\"] = dff[\"HasWoodDeck\"] + dff[\"HasOpenPorch\"] + dff[\"HasEnclosedPorch\"] + dff[\"Has3SsnPorch\"] + dff[\"HasScreenPorch\"]\ndff[\"TotalPorchType\"] = dff[\"TotalPorchType\"].apply(lambda x: 3 if x >=3 else x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dff[\"RestorationAge\"] = dff[\"YearRemodAdd\"] - dff[\"YearBuilt\"]\ndff[\"RestorationAge\"] = dff[\"RestorationAge\"].apply(lambda x: 0 if x < 0 else x)\ndff[\"HasRestoration\"] = dff[\"RestorationAge\"].apply(lambda x: 0 if x == 0 else 1)\n\ndff[\"YearAfterRestoration\"] = dff[\"YrSold\"] - dff[\"YearRemodAdd\"]\ndff[\"YearAfterRestoration\"] = dff[\"YearAfterRestoration\"].apply(lambda x: 0 if x < 0 else x)\n\ndff[\"BuildAge\"] = dff[\"YrSold\"] - dff[\"YearBuilt\"]\ndff[\"BuildAge\"] = dff[\"BuildAge\"].apply(lambda x: 0 if x < 0 else x)\ndff[\"IsNewHouse\"] = dff[\"BuildAge\"].apply(lambda x: 1 if x == 0 else 0)\n\ndef year_map(year):\n    # 1: GildedAge, 2: ProgressiveEra, 3: WorldWar1, 4: RoaringTwenties, 5: GreatDepression, \n    # 6: WorlWar2, 7: Post-warEra, 8: CivilRightsEra, 9: ReaganEra, 10: Post-ColdWarEra, 11: ModernEra\n    year = 1 if year <= 1895 else\\\n    (2 if year <= 1916 else\\\n     (3 if year <= 1919 else\\\n      (4 if year <= 1929 else\\\n       (5 if year <= 1941 else\\\n        (6 if year <= 1945 else\\\n         (7 if year <= 1964 else\\\n          (8 if year <= 1980 else\\\n           (9 if year <= 1991 else\\\n            (10 if year < 2008 else 11))))))))) \n    \n    return year\n\ndff[\"YearBuilt_bins\"] = dff[\"YearBuilt\"].apply(lambda year: year_map(year))\ndff[\"YearRemodAdd_bins\"] = dff[\"YearRemodAdd\"].apply(lambda year: year_map(year))\ndff[\"GarageYrBlt_bins\"] = dff[\"GarageYrBlt\"].apply(lambda year: year_map(year))\n\ndff[\"YrSold\"] = dff[\"YrSold\"].astype(str)\ndff[\"MoSold\"] = dff[\"MoSold\"].astype(str)\ndff[\"Season\"] = dff[\"MoSold\"].apply(lambda x: \"Winter\" if x in [\"12\", \"1\", \"2\"] else\\\n                                   (\"Spring\" if x in [\"3\", \"4\", \"5\"] else\\\n                                   (\"Summer\" if x in [\"6\", \"7\", \"8\"] else \"Fall\")))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dff[\"OverallValue\"] = dff[\"OverallQual\"] * dff[\"OverallCond\"]\ndff[\"ExterValue\"] = dff[\"ExterQual\"] * dff[\"ExterCond\"]\ndff[\"BsmtValue\"] = ((dff[\"BsmtQual\"] + dff[\"BsmtFinType1\"] + dff[\"BsmtFinType2\"]) * dff[\"BsmtCond\"]) / 2\ndff[\"KitchenValue\"] = dff[\"KitchenAbvGr\"] * dff[\"KitchenQual\"]\ndff[\"FireplaceValue\"] = dff[\"Fireplaces\"] * dff[\"FireplaceQu\"]\ndff[\"GarageValue\"] = dff[\"GarageQual\"] * dff[\"GarageCond\"]\n\ndff[\"TotalValue\"] = dff[\"OverallValue\"] + dff[\"ExterValue\"] + dff[\"BsmtValue\"] + dff[\"KitchenValue\"] + dff[\"FireplaceValue\"] + dff[\"GarageValue\"] +\\\ndff[\"HeatingQC\"] + dff[\"Utilities\"] + dff[\"Electrical\"] - dff[\"Functional\"]  + dff[\"PoolQC\"]\n\ndff[\"TotalQual\"] = dff[\"OverallQual\"] + dff[\"ExterQual\"] + dff[\"BsmtQual\"] + dff[\"KitchenQual\"] + dff[\"FireplaceQu\"] + dff[\"GarageQual\"] +\\\ndff[\"HeatingQC\"] + dff[\"PoolQC\"]\n\ndff[\"TotalCond\"] = dff[\"OverallCond\"] + dff[\"ExterCond\"] + dff[\"BsmtCond\"] + dff[\"GarageCond\"]\ndff[\"TotalQualCond\"] = dff[\"TotalQual\"] + dff[\"TotalCond\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dff[\"BsmtSFxValue\"] = dff[\"TotalBsmtSF\"] * dff[\"BsmtValue\"]\ndff[\"BsmtSFxQual\"] = dff[\"TotalBsmtSF\"] * dff[\"BsmtQual\"]\n\ndff[\"TotalAreaXOverallValue\"] = dff[\"TotalArea\"] * dff[\"OverallValue\"]\ndff[\"TotalAreaXOverallQual\"] = dff[\"TotalArea\"] * dff[\"OverallQual\"]\n\ndff[\"GarageAreaXGarageValue\"] = dff[\"GarageArea\"] * dff[\"GarageValue\"]\ndff[\"GarageAreaXGarageQual\"] = dff[\"GarageArea\"] * dff[\"GarageQual\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dff2 = dff.copy()\nnum_cols2, cat_cols2, ord_cols2 = col_types(dff2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3 For Ordinal Features\nCombining bins, it will help us to get stronger correlations. It is useful espeically for linear models.","metadata":{}},{"cell_type":"code","source":"for col in ord_cols2:\n    bar_box(dff, col)","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dff2[\"LotShape\"] = dff2[\"LotShape\"].apply(lambda x: 1 if x in [1, 2] else (2 if x == 3 else 3))\ndff2[\"LandSlope\"] = dff2[\"LandSlope\"].apply(lambda x: 1 if x in [1, 2] else (2 if x == 3 else 3))\ndff2[\"OverallCond\"] = dff2[\"OverallCond\"].apply(lambda x: 1 if x in [1, 2, 3] else x-1)\ndff2[\"OverallQual\"] = dff2[\"OverallQual\"].apply(lambda x: 1 if x in [1, 2] else x-1)\ndff2[\"ExterCond\"] = dff2[\"ExterCond\"].apply(lambda x: 1 if x in [1, 2] else (2 if x == 3 else 3))\ndff2[\"BsmtQual\"] = dff2[\"BsmtQual\"].apply(lambda x: 0 if x in [0, 1, 2] else (1 if x == 3 else (2 if x == 4 else 3)))\ndff2[\"BsmtCond\"] = dff2[\"BsmtCond\"].apply(lambda x: 0 if x in [0, 1, 2] else (1 if x == 3 else 2))\ndff2[\"BsmtFinType1\"] = dff2[\"BsmtFinType1\"].apply(lambda x: 1 if x in [1, 2, 3, 4, 5] else (2 if x == 6 else x))\ndff2[\"BsmtFinType2\"] = dff2[\"BsmtFinType2\"].apply(lambda x: 1 if x in [1, 2, 3, 4, 5] else (2 if x == 6 else x))\ndff2[\"HeatingQC\"] = dff2[\"HeatingQC\"].apply(lambda x: 1 if x in [1, 2] else (2 if x in [3, 4] else 3))\ndff2[\"Electrical\"] = dff2[\"Electrical\"].apply(lambda x: 1 if x in [1, 2] else x-3)\ndff2[\"BsmtFullBath\"] = dff2[\"BsmtFullBath\"].apply(lambda x: 2 if x >= 2 else x)\ndff2[\"FullBath\"] = dff2[\"FullBath\"].apply(lambda x: 1 if x <= 1 else (3 if x >= 3 else x))\ndff2[\"HalfBath\"] = dff2[\"HalfBath\"].apply(lambda x: 1 if x >= 1 else 0)\ndff2[\"BedroomAbvGr\"] = dff2[\"BedroomAbvGr\"].apply(lambda x: 1 if x <=1 else (5 if x >= 5 else x))\ndff2[\"KitchenAbvGr\"] = dff2[\"KitchenAbvGr\"].apply(lambda x: 1 if x <= 1 else (2 if x >= 2 else x))\ndff2[\"TotRmsAbvGrd\"] = dff2[\"TotRmsAbvGrd\"].apply(lambda x: 3 if x <= 4 else (10 if x >= 11 else x-1))\ndff2[\"Functional\"] = dff2[\"Functional\"].apply(lambda x: 1 if x == 1 else 2)\ndff2[\"Fireplaces\"] = dff2[\"Fireplaces\"].apply(lambda x: 2 if x >= 2 else x)\ndff2[\"GarageCars\"] = dff2[\"GarageCars\"].apply(lambda x: 3 if x >= 3 else x)\ndff2[\"GarageQual\"] = dff2[\"GarageQual\"].apply(lambda x: 1 if x <= 2 else (2 if x == 3 else 3))\ndff2[\"GarageCond\"] = dff2[\"GarageCond\"].apply(lambda x: 1 if x <= 2 else 2)\ndff2[\"Fence\"] = dff2[\"Fence\"].apply(lambda x: 1 if x in [1, 3] else x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dff3 = dff2.copy()\ndff3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Target Encoding For High Cardinality Categorical Features","metadata":{}},{"cell_type":"markdown","source":"If your categorical variable has lots of group, using target encoding is one of the best way to handle. \nFor example, you have a categorical variable that has 20 value to get. If you perform one-hot encoding to this feature, one-hot encoding generates 19 new column, dummy variable, to handle only one categorical feature. It causes to huge increasing on data and maybe losing some information.\n\nIn below, I just take high cardinality categorical features and I create new features with ranking median SalePrice per group.","metadata":{}},{"cell_type":"code","source":"for col in cat_cols2:\n    print(col, dff3[col].value_counts().size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_encoding = [\"MSSubClass\", \"Neighborhood\", \"Exterior1st\", \"Exterior2nd\", \"Condition1\", \"Condition2\", \"HouseStyle\"]\n\nfor col in target_encoding:\n    feature_name = col + \"Rank\"\n    dff3.loc[:, feature_name] = dff3[col].map(dff3.groupby(col).SalePrice.median())\n    dff3.loc[:, feature_name] = dff3.loc[:, feature_name].rank(method = \"dense\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dff3[\"Exterior\"] = np.where((dff3[\"Exterior1st\"] != dff3[\"Exterior2nd\"]), \"Mixed\", dff3[\"Exterior1st\"])\ndff3[\"No2ndExt\"] = dff3[\"Exterior\"].apply(lambda x: 0 if x == \"Mixed\" else 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_cols = [\"MSSubClass\", \"Neighborhood\", \"Condition1\", \"Condition2\", \"Exterior1st\", \"Exterior2nd\", \"PoolArea\", \"PoolQC\",\n             \"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\", \"LowQualFinSF\", \"MiscVal\", \"2ndFlrSF\", \n             \"HouseStyle\",\"YrSold\", \"MoSold\", \"YearBuilt\", \"YearRemodAdd\", \"GarageYrBlt\", \"Exterior\", \"Utilities\", \"Street\"]\n\n\ndff3.drop(drop_cols, axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_, cat_, ord_ = col_types(dff3)\n\ndef prep_data(df, cat_cols, target):\n    \n    dummies = pd.get_dummies(df[cat_cols], drop_first = True)\n    data = pd.concat([df, dummies], axis = 1).drop(cat_cols, axis = 1)\n    \n    train = data[data[target].notnull()]\n    test = data[data[target].isnull()]\n    \n    return train, test\n\n\ntrain, test = prep_data(dff3, cat_, \"SalePrice\")\n\ntarget = \"SalePrice\"\npredictors = [x for x in train.columns if x not in [\"Id\", \"SalePrice\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Preprocessing\n\n# 4.1 Handling Skewness","metadata":{}},{"cell_type":"markdown","source":"For most of machine learning algorithms, especially linear models, normally distributed features gives us better results. (Tree based algorithms don't need it)\n\nDetecting skewed features and normalizing with transformation.\n\nOur features have 0 values, that's why yeo johnson transformation method used.\nFor box-cox transformation, we need data that have positive values. If you want to perform box-cox transformation, you need add a smaller positive value like 0.0001.\n\nhttps://www.kaggle.com/rtatman/boxcox-transform-on-train-test-data","metadata":{}},{"cell_type":"code","source":"train_skew = []\ntest_skew = []\ncheck_cols = [x for x in num_ if not x.endswith(\"Rank\")]\n\nfor col in check_cols:\n    train_skew.append(train[col].skew())\n    test_skew.append(test[col].skew())\n    \nskew_df = pd.DataFrame({\"Feature\": check_cols, \"TrainSkewness\": train_skew, \"TestSkewness\": test_skew})\nskewed = skew_df[skew_df.TrainSkewness.abs() >= 0.5]\nskewed","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_skew_yeoj = []\ntest_skew_yeoj = []\n\nfor col in skewed.Feature.tolist():\n    train[col], fitted_lambda = stats.yeojohnson(train[col])\n    test[col] = stats.yeojohnson(test[col], fitted_lambda)\n    \n    train_skew_yeoj.append(train[col].skew())\n    test_skew_yeoj.append(test[col].skew())    \n    \nskewed[\"TrainSkewness_AfterYeoJohnson\"] = train_skew_yeoj\nskewed[\"TestSkewness_AfterYeoJohnson\"] = test_skew_yeoj\n\nskewed","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"high_skew = skewed[skewed.TrainSkewness_AfterYeoJohnson.abs() > 1].Feature.tolist()\nprint(high_skew)\n\ntrain.drop(high_skew, axis = 1, inplace = True)\ntest.drop(high_skew, axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.2 Target Distribution","metadata":{}},{"cell_type":"markdown","source":"Target distribution;\n\nNormal distributed target is also gives better results, especially linear algorithms.\n\nAlso, in this task, our evaluation metric is rmsle. Converting SalePrice to logarithmic form and applying rmse gives us the competition's metric.","metadata":{}},{"cell_type":"code","source":"feature_distribution(train, target, test = False)\ntrain[target].skew()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[target] = np.log1p(train[target])\n\nfeature_distribution(train, target, test = False)\n\ntrain[target].skew()","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = \"SalePrice\"\npredictors = [x for x in train.columns if x not in [\"Id\", \"SalePrice\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = RobustScaler()\n\ntrain[predictors] = scaler.fit_transform(train[predictors])\ntest[predictors] = scaler.transform(test[predictors])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train[predictors]\ny_train = train[target]\nX_test = test[predictors]\n\nprint(X_train.shape)\nprint(X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selector = VarianceThreshold(0.01)\n\nselector.fit(X_train)\nX_train = X_train[X_train.columns[selector.get_support()]]\n\nselector.transform(X_test)\nX_test = X_test[X_test.columns[selector.get_support()]]\n\nprint(X_train.shape)\nprint(X_test.shape)\n\nX_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_id = test[[\"Id\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from numpy import savetxt\n\n# savetxt('xtrain.csv', X_train, delimiter=',')\n# savetxt('xtest.csv', X_test, delimiter=',')\n# savetxt('ytrain.csv', y_train, delimiter=',')\n# savetxt('testid.csv', test_id, delimiter=',')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits = 10, shuffle = True, random_state = 42)\n\ndef rmse_cv(model, X = X_train, y = y_train):    \n    return np.sqrt(-cross_val_score(model, X, y, scoring = 'neg_mean_squared_error', cv = kf)).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parameter Tuning\n\nFor parameter tuning process, you can take a look at this notebook: https://www.kaggle.com/mustafacicek/lightgbm-xgboost-parameter-tuning-bayessearchcv","metadata":{}},{"cell_type":"markdown","source":"# Final: OOF Predictions & Stacking","metadata":{}},{"cell_type":"markdown","source":"Instead of using sklearn's StackingRegressor or mlxtend's StackingCVRegressor, I just use the codes in below. Because, while you use LGBMRegressor and XGBRegressor, you need to set early stopping rounds in fit params to prevent overfitting. Sklearn's StackingRegressor doesn't give the opportunity to setting an estimator's fit parameters (or it gives, I don't know), and that's why I will use the scratch in below.\n\nhttps://machinelearningmastery.com/out-of-fold-predictions-in-machine-learning/","metadata":{"trusted":true}},{"cell_type":"code","source":"lgb_model = lgb.LGBMRegressor(colsample_bytree=0.25, learning_rate=0.01,\n                              max_depth=13, min_child_samples=7, n_estimators=10000,\n                              num_leaves=20, objective='regression', random_state=42,\n                              subsample=0.9330025956033094, subsample_freq=1)\n\nxgb_model = xgb.XGBRegressor(colsample_bytree=0.25, gamma=0.0, learning_rate=0.01, max_depth=3,\n                             n_estimators=15000, n_jobs=-1, random_state=42, \n                             reg_alpha=0.24206673672530965, reg_lambda=0.40464485640717085, subsample=1.0)\n\ngbr_model = GradientBoostingRegressor(alpha=0.8979588317644014,\n                                      learning_rate=0.01, loss='huber',\n                                      max_depth=13, max_features=0.1, min_samples_split=109,\n                                      n_estimators=10000, n_iter_no_change=100, random_state=42)\n\nsvr_model = SVR(C=0.7682824405204463, coef0=0.0001, degree=2, epsilon=0.0001, gamma=0.0042151786393578635, max_iter=10000)\n\nlasso_model = Lasso(alpha=0.00012609086150256233, max_iter=5000, random_state=42)\n\nridge_model = Ridge(alpha=2.651347536470113, max_iter=5000, random_state=42)\n\nenet_model = ElasticNet(alpha=0.0002286518512853544, l1_ratio=0.6510386358323069, max_iter=5000, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I don't use Elasticnet model for stacking. \n\nI just add conditions for boosting models and a little verbosity for classical out of folds predictions template.","metadata":{}},{"cell_type":"code","source":"%%time\nmodels = {\n    \"LGBMRegressor\": lgb_model,\n    \"XGBRegressor\": xgb_model,\n    \"GradientBoostingRegressor\": gbr_model,\n    \"SVR\": svr_model,\n    \"Lasso\": lasso_model,\n    \"Ridge\": ridge_model,\n#     \"ElasticNet\": enet_model,\n         }\n\noof_df = pd.DataFrame()\npredictions_df = pd.DataFrame()\n\n\nfor name, model in models.items():\n    \n    print(\"For model \", name, \"\\n\")\n    i = 1\n    oof = np.zeros(len(X_train))\n    predictions = np.zeros(len(X_test))\n    \n    for train_ix, test_ix in kf.split(X_train.values):\n        \n        print(\"Out of fold predictions generating for fold \", i)\n        \n        train_X, train_y = X_train.values[train_ix], y_train[train_ix]\n        test_X, test_y = X_train.values[test_ix], y_train[test_ix]\n        \n        if name == \"LGBMRegressor\":\n            model.fit(train_X, train_y,\n                      eval_set = [(test_X, test_y)],\n                      eval_metric = \"rmse\",\n                      early_stopping_rounds=200,\n                      verbose=0)\n            \n        elif name == \"XGBRegressor\":\n            model.fit(train_X, train_y,\n                      eval_set = [(test_X, test_y)],\n                      eval_metric = \"rmse\",\n                      early_stopping_rounds=250,\n                      verbose=0)\n        else:\n            model.fit(train_X, train_y)\n            \n        oof[test_ix] = oof[test_ix] + model.predict(X_train.values[test_ix])\n        predictions = predictions + model.predict(X_test.values)\n        \n        i = i + 1\n        \n        oof_df[name] = oof\n        predictions_df[name] = predictions / 10\n        \n        \n    print(\"\\nDone \\n\")","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is for meta model. Generally, linear models (linear regression, lasso, ridge etc.) used as meta motel (StackingRegressor uses Ridge for example).\nI try Ridge as meta model, it gave me better results on \"stack_preds\" but non-linear model + blending outperforms it as a final prediction. ","metadata":{}},{"cell_type":"code","source":"oof = np.zeros(len(X_train))\npredictions = np.zeros(len(X_test))\ni = 1\n\nfor train_ix, test_ix in kf.split(oof_df):\n\n    print(\"Out of fold predictions generating for fold \", i)\n\n    train_X, train_y = oof_df.values[train_ix], y_train[train_ix]\n    test_X, test_y = oof_df.values[test_ix], y_train[test_ix]\n    \n    model = gbr_model\n    model.fit(train_X, train_y)\n\n#     model.fit(train_X, train_y,\n#                   eval_set = [(test_X, test_y)],\n#                   eval_metric = \"rmse\",\n#                   early_stopping_rounds=250,\n#                   verbose=0)        \n\n    oof[test_ix] = oof[test_ix] + model.predict(oof_df.values[test_ix])\n    predictions = predictions + model.predict(predictions_df)\n    \n    i = i + 1\n\n    oof_stacked = oof\n    stack_preds = predictions / 10      ","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stack_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = (4 * stack_preds +\n         predictions_df[\"LGBMRegressor\"] +\n         predictions_df[\"XGBRegressor\"] +\n         2 * predictions_df[\"GradientBoostingRegressor\"] +\n         predictions_df[\"SVR\"] +\n         predictions_df[\"Lasso\"]) / 10\n\nsub = pd.DataFrame({\"Id\": test_id.Id, \"SalePrice\": np.expm1(preds)})\nsub.to_csv(\"BlendedModel120121.csv\", index = False)\n\nsub","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}