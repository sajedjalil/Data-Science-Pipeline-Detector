{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Fist Step** - Data & Field Understanding"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# I started my work by taking tips from https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n# Importing libraries\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# bring the numbers in\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\nprint('Size of train data set is :',df_train.shape)\nprint('Size of test data set is :',df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Second Step** - Let's look deeper at the features and target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Saving Ids\ntrain_ID = df_train['Id']\ntest_ID = df_test['Id']\n\n#Dropping Ids\ndf_train.drop(\"Id\", axis = 1, inplace = True)\ndf_test.drop(\"Id\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Outliers**\n\nFirst of all, we're going to remove some outliers according to the author's suggestion.\nLet's explore these outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nax.scatter(x = df_train['GrLivArea'], y = df_train['SalePrice'], c = \"skyblue\")\nplt.ylabel('SalePrice', fontsize=6)\nplt.xlabel('GrLivArea', fontsize=6)\nplt.show()\n\ndf_train = df_train.drop(df_train[(df_train['GrLivArea']>4000)].index)\n\nfig, ax = plt.subplots()\nax.scatter(x = df_train['GrLivArea'], y = df_train['SalePrice'], c = \"skyblue\")\nplt.ylabel('SalePrice', fontsize=6)\nplt.xlabel('GrLivArea', fontsize=6)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First glance correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation matrix\ncorrmat = df_train.corr()\nmask = np.zeros_like(corrmat)\nmask[np.triu_indices_from(mask)] = True\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, mask=mask, linewidths=.5, vmax=0.7, square=True, cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's get some info about the Target Variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"#stat summary\ndf_train['SalePrice'].describe()\n\n#get distribution & QQ Plot\nsns.distplot(df_train['SalePrice'], \n             kde_kws={\"color\": \"coral\", \"lw\": 1, \"label\": \"KDE\"}, \n             hist_kws={\"histtype\": \"stepfilled\", \"linewidth\": 3, \"alpha\": 0.8, \"color\": \"skyblue\"});\n\n\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SalePrice is not normally distributed. We will make a log transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['SalePrice_Log'] = np.log(df_train['SalePrice'])\n\nsns.distplot(df_train['SalePrice_Log'], \n             kde_kws={\"color\": \"coral\", \"lw\": 1, \"label\": \"KDE\"}, \n             hist_kws={\"histtype\": \"stepfilled\", \"linewidth\": 3, \"alpha\": 1, \"color\": \"skyblue\"});\n \n# skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice_Log'].kurt())\n\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice_Log'], plot=plt)\nplt.show()\n\n# dropping SalePrice\ndf_train.drop('SalePrice', axis= 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Third step** - Missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Saving sets sizes, concat sets and dropping target variable\nsize_df_train = df_train.shape[0]\nsize_df_test = df_test.shape[0]\ntarget_variable = df_train.SalePrice_Log.values\ndata = pd.concat((df_train, df_test)).reset_index(drop=True)\ndata.drop(['SalePrice_Log'], axis=1, inplace=True)\n\n# Lets check if the ammount of null values in data\ndata.count().sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- NA for 'PoolQC' means \"No Pool\".\n- MiscFeature: NA means \"None\"\n- Alley: NA means \"No alley access\"\n- Fence: NA means \"No fence\"\n- FireplaceQu: NA means \"No fireplace\"\n- LotFrontage: fill missing values with median LotFrontage of neighborhood\n- GarageFinish: NA means \"None\"\n- GarageQual: NA means \"None\"\n- GarageCond: NA means \"None\"\n- GarageYrBlt: NA means 0\n- GarageType: NA means \"None\"\n- BsmtCond: NA means \"None\"\n- BsmtExposure: NA means \"None\"\n- BsmtQual: NA means \"None\"\n- BsmtFinType2: NA means \"None\"\n- BsmtFinType1: NA means \"None\"\n- MasVnrType: NA means \"None\"\n- MasVnrArea: NA means \"0\"\n- BsmtHalfBath: NA means \"0\"\n- BsmtFullBath: NA means \"0\"\n- BsmtFinSF1: NA means \"0\"\n- BsmtFinSF2: NA means \"0\"\n- BsmtUnfSF: NA means \"0\"\n- TotalBsmtSF: NA means \"0\"\n- GarageCars: NA means 0\n- GarageArea: NA means 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_fill_na_none = ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu',\n               'GarageQual','GarageCond','GarageFinish','GarageType',\n               'BsmtExposure','BsmtCond','BsmtQual','BsmtFinType1','BsmtFinType2',\n               'MasVnrType']\n\nfor feature_none in features_fill_na_none:\n    data[feature_none].fillna('None',inplace=True)\n    \nfeatures_fill_na_0 = ['GarageYrBlt', 'GarageArea', 'GarageCars', 'MasVnrArea',\n                      'BsmtFullBath','BsmtHalfBath', 'BsmtFinSF1', 'BsmtFinSF2', \n                      'BsmtUnfSF', 'TotalBsmtSF']\n\nfor feature_0 in features_fill_na_0:\n    data[feature_0].fillna('None',inplace=True)\n\n#LotFrontage\n#We'll fill missing values by the median of observation's Neighborhood\ndata[\"LotFrontage\"] = data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\n#MSzoning - 4 missing values\n#We'll fill missing values with most common value\ndata['MSZoning'] = data['MSZoning'].fillna(data['MSZoning'].mode()[0])\n\n# Utilities\n# All records have \"AllPub\", but 3. From those 3, 2 are NA and one is \"NoSeWa\" is \n# in the training set.\n# We may proceed to drop this column\ndata = data.drop(columns=['Utilities'],axis=1)\n\n#Functional: NA means \"Typ\"\ndata[\"Functional\"] = data[\"Functional\"].fillna(\"Typ\")\n\n# Electrical - 91% of observations have Electrical = SBrkr\n# We'll fill missing values with SBrkr\ndata['Electrical'] = data['Electrical'].fillna(\"SBrkr\")\n\n#Exterior1st and Exterior2nd, one missing value, same observation\n#We'll substitute it with the most common value\ndata['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior1st'].mode()[0])\ndata['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])\n\n#KitchenQual, ony one missing value\n#We'll subsitute it with the most common value\ndata['KitchenQual'] = data['KitchenQual'].fillna(data['KitchenQual'].mode()[0])\n\n#Saletype, ony one missing value\n#We'll subsitute it with the most common value\ndata['SaleType'] = data['SaleType'].fillna(data['SaleType'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll check again if we have filled all missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.count().sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All missing values filled\n\n**Fourth step** - A little bit of Feature Eng."},{"metadata":{"trusted":true},"cell_type":"code","source":"#getting numerical features and categorical features\nnumerical_features = data.dtypes[data.dtypes != \"object\"].index\ncategorical_features = data.dtypes[data.dtypes == \"object\"].index\n\nprint(\"We have: \", len(numerical_features), 'Numerical Features')\nprint(\"We have: \", len(categorical_features), 'Categorical Features')\n\nnumerical_features\ncategorical_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you might see, 3 numerical features are categorical\nLet's change this"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_to_transform = ['MSSubClass', 'OverallCond', 'YrSold', 'MoSold']\n\nfor feature in features_to_transform:\n    data[feature] = data[feature].apply(str)\n\n#Let's check how our features stand now\nnumerical_features = data.dtypes[data.dtypes != \"object\"].index\ncategorical_features = data.dtypes[data.dtypes == \"object\"].index\n\nprint(\"We have: \", len(numerical_features), 'Numerical Features')\nprint(\"We have: \", len(categorical_features), 'Categorical Features')\n\nnumerical_features\ncategorical_features\n\n#Let's encode categorical variables\nencode_cat_variables = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\nfor variable in encode_cat_variables:\n    lbl = LabelEncoder() \n    lbl.fit(list(data[variable].values)) \n    data[variable] = lbl.transform(list(data[variable].values))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's workout the numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Boxplot for numerical_features\nsns.set_style(\"whitegrid\")\nf, ax = plt.subplots(figsize=(20, 20))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=data[numerical_features] , orient=\"h\", palette=\"ch:2.5,-.2,dark=.5\")\nax.set(ylabel=\"Features\")\nax.set(xlabel=\"Value\")\nax.set(title=\"Distribution\")\nsns.despine(trim=True, left=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look for numerical_features that can be normalized"},{"metadata":{"trusted":true},"cell_type":"code","source":"skewed_features = data[numerical_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nnorm_target_features = skewed_features[skewed_features > 0.5]\nnorm_target_index = norm_target_features.index\nprint(\"#{} numerical features need normalization; :\".format(norm_target_features.shape[0]))\nskewness = pd.DataFrame({'Skew' :norm_target_features})\nnorm_target_features\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalizing with Box Cox Transformation\nfor i in norm_target_index:\n    data[i] = boxcox1p(data[i], boxcox_normmax(data[i] + 1))\n\n#Let's look how the transformed features are standing now\nsns.set_style(\"whitegrid\")\nf, ax = plt.subplots(figsize=(15, 15))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=data[norm_target_index] , orient=\"h\", palette=\"ch:2.5,-.2,dark=.3\")\nax.set(ylabel=\"Features\")\nax.set(xlabel=\"Value\")\nax.set(title=\"Distribution\")\nsns.despine(trim=True, left=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#back to train and test sets\ndf_train = data[:size_df_train]\ndf_test = data[size_df_train:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fifth step** - Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cross Validation\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(df_train.values)\n    rmse= np.sqrt(-cross_val_score(model, df_train.values, target_variable, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}