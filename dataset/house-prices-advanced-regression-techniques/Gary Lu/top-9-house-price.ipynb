{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Table of Contents\n\n1. Introduction\n1. Exploratory data analysis\n1. Feature engineeing\n1. Modeling and Prediction\n1. Conclusion\n"},{"metadata":{},"cell_type":"markdown","source":"## 1. Introduction\n\nHouse Pricing Prediction has been a classic challenge, as the price of houses are usually dependent on many features. The goal of our study is to set up prediction models for Boston house prices, based on 80 given features.\n\nTwo data sets are given: the train data set consist of 1460 rows each representing a distinct house transaction, and 81 columns (1 SalePrice & 80 features),\nand the test data set consist of 1459 rows, and 80 columns (features).\n\nOur study follows the process: exploratory data analysis - feature engineering - prediction model setting. Finally 7 models are used and ensembles. The result turns out that:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import all the dependencies\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport optuna\nimport xgboost as xgb\nimport lightgbm as lgb\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler, PowerTransformer\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.ensemble import StackingRegressor, RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.feature_selection import VarianceThreshold\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Exploratory data analysis\n\n### 2.1 Load the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load data from csv files\ntrain_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data = pd.concat([train_data, test_data], axis=0)\ncombined_data['Label'] =  combined_data['SalePrice'].isnull()\nmapping = {False:'Tain Data',True:'Test Data' }\ncombined_data['Label'] = combined_data['Label'].map(mapping)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Train Data Exploration"},{"metadata":{},"cell_type":"markdown","source":"For both train and test data, we explored the items as below:\n- first 5 rows\n- data shape\n- general data information\n- data type\n- null value"},{"metadata":{},"cell_type":"markdown","source":"#### 2.2.1 First Five Records - Train Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.2.2 Data Shape - Train Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.2.3 Data Information - Trian Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.2.4 Data Type - Train Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"s1 = train_data.dtypes\ns1.groupby(s1).count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.2.5 Null Value - Train Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isnull().sum().sort_values(ascending = False).head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3 Test Data Exploration"},{"metadata":{},"cell_type":"markdown","source":"#### 2.3.1 First Five Records - Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.3.2 Data Type - Test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.3.3 Data Information - Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.3.4 Data Type - Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"s2 = test_data.dtypes\ns2.groupby(s2).count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.3.5 Null Data - Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.isnull().sum().sort_values(ascending = False).head(33)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.4 Train Data & Test Data Comnparison"},{"metadata":{},"cell_type":"markdown","source":"After exploration, we noticed that data types seem not match between train and test data, so we want to compare the differences.\nIn this part, we mainly compared features below between train and test data:\n- Data Types\n- Null Values\n- Data Distribution"},{"metadata":{},"cell_type":"markdown","source":"#### 2.4.1 Data Type Comparison"},{"metadata":{"trusted":true},"cell_type":"code","source":"s1_train = s1.drop('SalePrice')\ns1_train.compare(s2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['GarageArea'].head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The inconsistent data types are int64&float64 and not considered a problem."},{"metadata":{},"cell_type":"markdown","source":"#### 2.4.2 Null Value Comparison"},{"metadata":{"trusted":true},"cell_type":"code","source":"null_train = train_data.isnull().sum()\nnull_test = test_data.isnull().sum()\nnull_train = null_train.drop('SalePrice')\nnull_train.compare(null_test).sort_values(['self','other'], ascending= [False,False])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We noticed PoolQC, MiscFeature, Alley, Fence are the features with most missing values, we will consider to drop a few such features later."},{"metadata":{},"cell_type":"markdown","source":"#### 2.4.3 Distribution Comparison - Continous Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"con_var = s1[s1.values != 'object'].index\nf, axes = plt.subplots(7,6 , figsize=(30, 30), sharex=False)\nfor i, feature in enumerate(con_var):\n    sns.histplot(data=combined_data, x = feature, hue=\"Label\",ax=axes[i%7, i//7])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution above shows that:\n- The distribution of train data and test data are similar for most continous features;\n- Some features can be reclassified as 'Categorical', such as 'MSSubClass';\n- Some features are dominated by 0/null (eg:PoolArea), thus we can consider to drop."},{"metadata":{},"cell_type":"markdown","source":"Then we also want to see the linearity between all features and the response variable (saleprice)"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(7,6 , figsize=(30, 30), sharex=False)\nfor i, feature in enumerate(con_var):\n    sns.scatterplot(data=train_data, x = feature, y= \"SalePrice\",ax=axes[i%7, i//7])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we can see that some relations seem positive but not quite linear:\n- 'SalePrice' VS.'BsmtUnfSF', \n- 'SalePrice' VS.'LotFrontage', \n- 'SalePrice' VS.'LotArea', \n- 'SalePrice' VS.'1stFlrSF', \n- 'SalePrice' VS.'GrLivArea',\n- 'SalePrice' VS.'TotalBsmtSF', \n- 'SalePrice' VS.'GarageArea',\nSo we will consider transform such features into log forms."},{"metadata":{},"cell_type":"markdown","source":"#### 2.4.4 Distribution Comparison - Catagorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_var = s1[s1.values == 'object'].index\nf, axes = plt.subplots(7,7 , figsize=(30, 30), sharex=False)\nfor i, feature in enumerate(cat_var):\n    sns.countplot(data = combined_data, x = feature, hue=\"Label\",ax=axes[i%7, i//7])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The comparison of the categorical variables showed that:\n- Train data and test data distributions are similar for most features\n- Some features have dominant items, we can consider to combine some minor items into a group, such as\n    - 'Fa' & 'Po' in 'HeatingQC', 'FireplaceQu', 'GarageQual' and 'GarageCond'"},{"metadata":{},"cell_type":"markdown","source":"We also want to confirm that the items we want to combine has similar prices."},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(7,7 , figsize=(30, 30), sharex=False)\nfor i, feature in enumerate(cat_var):\n    sort_list = sorted(train_data.groupby(feature)['SalePrice'].median().items(), key= lambda x:x[1], reverse = True)\n    order_list = [x[0] for x in sort_list ]\n    sns.boxplot(data = train_data, x = feature, y = 'SalePrice', order=order_list, ax=axes[i%7, i//7])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we could see that sale prices for 'Fa' & 'Po' in 'HeatingQC', 'FireplaceQu', 'GarageQual' and 'GarageCond' are similar, so we may consider go ahead and combine the items."},{"metadata":{},"cell_type":"markdown","source":"### 2.5 Data Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"con_data = train_data.copy()\nfor col in cat_var:\n    con_data = con_data.drop(col, axis = 1)\n\ntraining_corr = con_data.corr(method='spearman')\nmask = np.zeros_like(training_corr)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(20,10))\nsns.heatmap(training_corr, mask=mask, cmap=\"YlGnBu\", linewidths=.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see that 'OverallQual', 'GrlivArea', 'GarageCars' are highly correlated to SalePrice."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 10 correlated features vs. SalePrice\ncorrelations = con_data.corr(method='spearman')['SalePrice'].sort_values(ascending=False)\ncorrelations_abs = correlations.abs()\nprint(correlations_abs.head(11))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Feature engineeing\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pre_process(train_data, test_data, fillna_dict = {}, drop_list = [], convert_list=[], log_list=[], regroup_dict={}):\n    combined_data = pd.concat([train_data, test_data], ignore_index=True, axis=0)\n    \n    # Step 1: Fill missing values\n    for col, fill_value in fillna_dict.items():\n        combined_data[col] = combined_data[col].fillna(value=fill_value)\n        \n    # Step 2: Drop columns\n    combined_data.drop(columns=drop_list, inplace=True, errors='ignore')\n    \n    # Step 3: Convert \"numerical\" feature to categorical\n    for col in convert_list:\n        combined_data[col] = combined_data[col].astype('str')\n        \n    # Step 4: Apply PowerTransformer to columns\n    for col in log_list:\n        log = PowerTransformer()\n        log.fit(train_data[[col]]) # TODO: fit with combined_data to avoid overfitting with training data?\n        combined_data[col] = log.transform(combined_data[[col]])\n        \n    # Step 5: Regroup features\n    for col, regroup_value in regroup_dict.items():\n        mask = combined_data[col].isin(regroup_value)\n        combined_data[col][mask] = 'Other'\n        \n    # Step 6: Drop categorical features with an absolute dominatining value\n#     for i in combined_data.columns:\n#         if combined_data[i].dtype != 'object':\n#             continue\n#         counts = combined_data[i].value_counts()\n#         zeros = counts.iloc[0]\n#         if zeros / len(combined_data) > 0.995:\n#             print(f'Feature {combined_data[i].name} value {counts.index[0]} is dominating with percentage {zeros / len(combined_data)}, dropping the feature')\n#             combined_data.drop(columns=combined_data[i].name, inplace=True, errors='ignore')\n    \n    # Step 7: Add features\n#     combined_data['Total_Home_Quality'] = combined_data['OverallQual'] + combined_data['OverallCond']\n\n#     combined_data['Total_Bathrooms'] = (combined_data['FullBath'] + (0.5 * combined_data['HalfBath']) +\n#                                combined_data['BsmtFullBath'] + (0.5 * combined_data['BsmtHalfBath']))\n\n#     combined_data[\"HighQualSF\"] = combined_data[\"1stFlrSF\"] + combined_data[\"2ndFlrSF\"]\n\n#     combined_data['Total_sqr_footage'] = (combined_data['BsmtFinSF1'] + combined_data['BsmtFinSF2'] +\n#                                  combined_data['1stFlrSF'] + combined_data['2ndFlrSF'])\n    \n    # Step -1: Generate one-hot dummy columns\n    combined_data = pd.get_dummies(combined_data).reset_index(drop=True)\n    \n    new_train_data = combined_data.iloc[:len(train_data), :]\n    new_test_data = combined_data.iloc[len(train_data):, :]\n    X_train = new_train_data.drop('SalePrice', axis=1)\n    y_train = np.log1p(new_train_data['SalePrice'].values.ravel())\n    X_test = new_test_data.drop('SalePrice', axis=1)\n    return X_train, y_train, X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fillna_dict = {\n    'Alley': 'NA',\n    'PoolQC': 'NA',\n    'LotFrontage': train_data['LotFrontage'].mean(),\n    'MasVnrArea': 0.0,\n    'GarageYrBlt': 0.0,\n    'BsmtFinSF1': 0.0,\n    'BsmtFinSF2': 0.0,\n    'BsmtUnfSF': 0.0,\n    'TotalBsmtSF': 0.0,\n    'BsmtFullBath': 0.0,\n    'BsmtHalfBath': 0.0,\n    'GarageCars': 0.0,\n    'GarageArea': 0.0,\n    'MiscFeature': 'NA',\n    'Fence':'NA',\n    'FireplaceQu': 'NA',\n    'GarageFinish': 'NA',\n    'GarageQual': 'NA',\n    'GarageCond': 'NA',\n    'GarageType': 'NA',\n    'BsmtCond': 'NA',\n    'BsmtQual': 'NA',\n    'BsmtExposure':'NA',\n    'BsmtFinType1':'NA',\n    'BsmtFinType2':'NA',\n    'MasVnrType': 'None',\n    'MSZoning': train_data['MSZoning'].mode()[0]\n}\n\ndrop_list = ['Id']\n\nconvert_to_str_list = ['MSSubClass']\n\nlog_list = [\n    'BsmtUnfSF', 'LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea',\n    'TotalBsmtSF', 'GarageArea',\n]\n\nregroup_dict = {\n#     'LotConfig': ['FR2','FR3'],\n#     'LandSlope':['Mod','Sev'],\n#     'BldgType':['2FmCon','Duplex'],\n#     'RoofStyle':['Mansard','Flat','Gambrel'],\n#     'Electrical':['FuseF','FuseP','FuseA','Mix'],\n#     'SaleCondition':['Abnorml','AdjLand','Alloca','Family'],\n#     'BsmtExposure':['Min','Av'],\n#     'Functional':['Min1','Maj1','Min2','Mod','Maj2','Sev'],\n#     'LotShape':['IR2','IR3'],\n    'HeatingQC':['Fa','Po'],\n    'FireplaceQu':['Fa','Po'],\n    'GarageQual':['Fa','Po'],\n    'GarageCond':['Fa','Po'],\n}\n\nX, y, X_test = pre_process(train_data, test_data,\n                           fillna_dict=fillna_dict,\n                           drop_list=drop_list,\n                           convert_list=convert_to_str_list,\n                           log_list=log_list,\n                           regroup_dict=regroup_dict,\n                          )\n\nprint(X.shape)\n\npre_precessing_pipeline = make_pipeline(RobustScaler(), \n                                        # VarianceThreshold(0.001),\n                                       )\n\nX = pre_precessing_pipeline.fit_transform(X)\nX_test = pre_precessing_pipeline.transform(X_test)\n\nprint(X.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Modeling and Prediction"},{"metadata":{},"cell_type":"markdown","source":"In this section, we will fit the model with the data, tune the hyperparameters of them, and make prediction for the testing data.\n\nWe selected the following regressor models:\n- [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n- [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n- [Elastic Net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)\n- [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n- [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n- [Gradient Boosting Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\n- [Stacking Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html) with the other models\n\nRegarding hyperparameter tuning, we preferred the searching algorithm of [Optuna](https://github.com/optuna/optuna)."},{"metadata":{"trusted":true},"cell_type":"code","source":"RANDOM_SEED = 42\n\n# 10-fold CV\nkfolds = KFold(n_splits=10, shuffle=True, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tune(objective):\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=100)\n\n    params = study.best_params\n    best_score = study.best_value\n    print(f\"Best score: {best_score} \\nOptimized parameters: {params}\")\n    return params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ridge_objective(trial):\n\n    _alpha = trial.suggest_float(\"alpha\", 0.1, 20)\n\n    ridge = Ridge(alpha=_alpha, random_state=RANDOM_SEED)\n\n    score = cross_val_score(\n        ridge, X, y, cv=kfolds, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    return score\n\n# Best score: -0.12599963936207256 \n# ridge_params = tune(ridge_objective)\nridge_params = {'alpha': 7.491061624529043}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = Ridge(**ridge_params, random_state=RANDOM_SEED)\nridge.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lasso_objective(trial):\n\n    _alpha = trial.suggest_float(\"alpha\", 0.0001, 1)\n\n    lasso = Lasso(alpha=_alpha, random_state=RANDOM_SEED)\n\n    score = cross_val_score(\n        lasso, X, y, cv=kfolds, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    return score\n\n# Best score:-0.1237148073006272\n# lasso_params = tune(lasso_objective)\nlasso_params = {'alpha': 0.00041398687418613947}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = Lasso(**lasso_params, random_state=RANDOM_SEED)\nlasso.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def elasticnet_objective(trial):\n\n    _alpha = trial.suggest_float(\"alpha\", 0.0001, 1)\n    _l1_ratio = trial.suggest_float(\"l1_ratio\", 0.01, 1)\n\n    elastic = ElasticNet(alpha=_alpha, l1_ratio=_l1_ratio, random_state=RANDOM_SEED)\n\n    score = cross_val_score(\n        elastic, X, y, cv=kfolds, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    return score\n\n# Best score:-0.12399332824933833\n# elasticnet_params = tune(elasticnet_objective)\nelasticnet_params = {'alpha': 0.00048679709811971084, 'l1_ratio': 0.5791344072946181}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elasticnet = ElasticNet(**elasticnet_params, random_state=RANDOM_SEED)\nelasticnet.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def svr_objective(trial):\n\n    _C = trial.suggest_float(\"C\", 0.1, 10)\n    _epsilon = trial.suggest_float(\"epsilon\", 0.01, 10)\n#     _kernel = trial.suggest_categorical(\"kernel\", ['linear', 'poly', 'rbf', 'sigmoid'])\n    _coef0 = trial.suggest_float(\"coef0\", 0.01, 1)\n\n    svr = SVR(C=_C, epsilon=_epsilon, kernel='poly')\n\n    score = cross_val_score(\n        svr, X, y, cv=kfolds, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    return score\n\n\n# Best score: -0.396815691360129 \n# svr_params = tune(svr_objective)\nsvr_params = {'C': 1.5959075672900394, 'epsilon': 1.1567830632624725, 'coef0': 0.4841022473611773}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svr = SVR(kernel='poly', **svr_params)\nsvr.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def randomforest_objective(trial):\n    _n_estimators = trial.suggest_int(\"n_estimators\", 50, 200)\n    _max_depth = trial.suggest_int(\"max_depth\", 5, 20)\n    _min_samp_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n    _min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 2, 10)\n    _max_features = trial.suggest_int(\"max_features\", 10, 50)\n\n    rf = RandomForestRegressor(\n        max_depth=_max_depth,\n        min_samples_split=_min_samp_split,\n        min_samples_leaf=_min_samples_leaf,\n        max_features=_max_features,\n        n_estimators=_n_estimators,\n        n_jobs=-1,\n        random_state=RANDOM_SEED,\n    )\n\n    score = cross_val_score(\n        rf, X, y, cv=kfolds, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    return score\n\n# Best score: -0.13808317983522972\n# randomforest_params = tune(randomforest_objective)\nrandomforest_params = {'n_estimators': 180, 'max_depth': 18, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 49}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor(n_jobs=-1, random_state=RANDOM_SEED, **randomforest_params)\nrf.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gbr_objective(trial):\n    _n_estimators = trial.suggest_int(\"n_estimators\", 50, 2000)\n    _learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 1)\n    _max_depth = trial.suggest_int(\"max_depth\", 1, 20)\n    _min_samp_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n    _min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 2, 20)\n    _max_features = trial.suggest_int(\"max_features\", 10, 50)\n\n    gbr = GradientBoostingRegressor(\n        n_estimators=_n_estimators,\n        learning_rate=_learning_rate,\n        max_depth=_max_depth, \n        max_features=_max_features,\n        min_samples_leaf=_min_samples_leaf,\n        min_samples_split=_min_samp_split,\n        \n        random_state=RANDOM_SEED,\n    )\n\n    score = cross_val_score(\n        gbr, X, y, cv=kfolds, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    return score\n\n# Best score: -0.11848459248013568\n# gbr_params = tune(gbr_objective)\ngbr_params = {'n_estimators': 1808, 'learning_rate': 0.03603208066350368, 'max_depth': 3, 'min_samples_split': 12, 'min_samples_leaf': 2, 'max_features': 42}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbr = GradientBoostingRegressor(random_state=RANDOM_SEED, **gbr_params)\ngbr.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def xgb_objective(trial):\n    _n_estimators = trial.suggest_int(\"n_estimators\", 50, 2000)\n    _max_depth = trial.suggest_int(\"max_depth\", 1, 20)\n    _learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 1)\n    _gamma = trial.suggest_float(\"gamma\", 0.01, 1)\n    _min_child_weight = trial.suggest_float(\"min_child_weight\", 0.1, 10)\n    _subsample = trial.suggest_float('subsample', 0.01, 1)\n    _reg_alpha = trial.suggest_float('reg_alpha', 0.01, 10)\n    _reg_lambda = trial.suggest_float('reg_lambda', 0.01, 10)\n\n    \n    xgbr = xgb.XGBRegressor(\n        n_estimators=_n_estimators,\n        max_depth=_max_depth, \n        learning_rate=_learning_rate,\n        gamma=_gamma,\n        min_child_weight=_min_child_weight,\n        subsample=_subsample,\n        reg_alpha=_reg_alpha,\n        reg_lambda=_reg_lambda,\n        random_state=RANDOM_SEED,\n    )\n    \n\n    score = cross_val_score(\n        xgbr, X, y, cv=kfolds, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    return score\n\n# Best score: -0.1225190827846444\n# xgb_params = tune(xgb_objective)\nxgb_params = {'n_estimators': 500, 'max_depth': 8, 'learning_rate': 0.03314181092616917, 'gamma': 0.03861572735293306, 'min_child_weight': 2.5264657011723335, 'subsample': 0.69824536298609, 'reg_alpha': 0.021753223362733998, 'reg_lambda': 3.216048970671949}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbr = xgb.XGBRegressor(random_state=RANDOM_SEED, **xgb_params)\nxgbr.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\ndef lgb_objective(trial):\n    _num_leaves = trial.suggest_int(\"num_leaves\", 50, 100)\n    _max_depth = trial.suggest_int(\"max_depth\", 1, 20)\n    _learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 1)\n    _n_estimators = trial.suggest_int(\"n_estimators\", 50, 2000)\n    _min_child_weight = trial.suggest_float(\"min_child_weight\", 0.1, 10)\n    _reg_alpha = trial.suggest_float('reg_alpha', 0.01, 10)\n    _reg_lambda = trial.suggest_float('reg_lambda', 0.01, 10)\n    _subsample = trial.suggest_float('subsample', 0.01, 1)\n\n\n    \n    lgbr = lgb.LGBMRegressor(objective='regression',\n                             num_leaves=_num_leaves,\n                             max_depth=_max_depth,\n                             learning_rate=_learning_rate,\n                             n_estimators=_n_estimators,\n                             min_child_weight=_min_child_weight,\n                             subsample=_subsample,\n                             reg_alpha=_reg_alpha,\n                             reg_lambda=_reg_lambda,\n                             random_state=RANDOM_SEED,\n    )\n    \n\n    score = cross_val_score(\n        lgbr, X, y, cv=kfolds, scoring=\"neg_root_mean_squared_error\"\n    ).mean()\n    return score\n\n# Best score: -0.12497294451988177 \n# lgb_params = tune(lgb_objective)\nlgb_params = {'num_leaves': 81, 'max_depth': 2, 'learning_rate': 0.05943111506493225, 'n_estimators': 1668, 'min_child_weight': 4.6721695700874015, 'reg_alpha': 0.33400189583009254, 'reg_lambda': 1.4457484337302167, 'subsample': 0.42380175866399206}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbr = lgb.LGBMRegressor(objective='regression', random_state=RANDOM_SEED, **lgb_params)\nlgbr.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stack models\nstack = StackingRegressor(\n    estimators=[\n        ('ridge', ridge),\n        ('lasso', lasso),\n        ('elasticnet', elasticnet),\n        ('randomforest', rf),\n        ('gradientboostingregressor', gbr),\n        ('xgb', xgbr),\n        ('lgb', lgbr),\n        # ('svr', svr), # Not using this for now as its score is significantly worse than the others\n    ],\n    cv=kfolds)\nstack.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Comparison"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cv_rmse(model):\n    rmse = -cross_val_score(model, X, y,\n                            scoring=\"neg_root_mean_squared_error\",\n                            cv=kfolds)\n    return (rmse)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compare_models():\n    models = {\n        'Ridge': ridge,\n        'Lasso': lasso,\n        'Elastic Net': elasticnet,\n        'Random Forest': rf,\n        'Gradient Boosting': gbr,\n        'XGBoost': xgbr,\n        'LightGBM': lgbr,\n        'Stacking': stack, \n        # 'SVR': svr, # TODO: Investigate why SVR got such a bad result\n    }\n\n    scores = pd.DataFrame(columns=['score', 'model'])\n\n    for name, model in models.items():\n        score = cv_rmse(model)\n        print(\"{:s} score: {:.4f} ({:.4f})\\n\".format(name, score.mean(), score.std()))\n        df = pd.Series(score, name='score').to_frame()\n        df['model'] = name\n        scores = scores.append(df)\n\n    plt.figure(figsize=(20,10))\n    sns.boxplot(data = scores, x = 'model', y = 'score')\n    plt.show()\n    \ncompare_models()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Predict submission')\nsubmission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\n\nsubmission.iloc[:,1] = np.expm1(stack.predict(X_test))\n\nsubmission.to_csv('my_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Conclusion\n\n#### Next Step\n- Dive deeper into feature engineering\n- Investigate why SVR got such a bad result. (It is expected that its result is worse than the others, but not to this extent)\n- Try other hyperparameter tuning tools like [Ray](https://docs.ray.io/en/latest/tune/)\n- Try NN (maybe training data is too few for NN?)\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}