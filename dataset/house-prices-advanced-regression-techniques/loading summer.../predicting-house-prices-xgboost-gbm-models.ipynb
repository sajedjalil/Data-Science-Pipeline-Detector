{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predicting House Prices XGBoost + GBM Models\n\n\n\n**Bugra Sebati E.** - **July 2021**","metadata":{}},{"cell_type":"markdown","source":"## Introduction\n\nFor this competiton, we are given a data set of 1460 homes, each with a few dozen features of types: float, integer, and categorical. We are tasked with building a regression model to estimate a home's sale price. Total number of attributes equals 81, of which 36 is quantitative, 43 categorical + Id and SalePrice.\n\n**What you can find on this notebook?**\n\n* Understanding the data\n* Exploratory Data Analysis\n* Data Preprocessing\n* PCA Trial\n* GBM and XGBoost Models\n* Submission","metadata":{}},{"cell_type":"markdown","source":"![](http://media1.tenor.com/images/286156bd33ce64d69f6a2367557392b5/tenor.gif?itemid=10804810)\n","metadata":{}},{"cell_type":"markdown","source":"### Lets meet variables\n\n* **SalePrice** : The property's sale price in dollars. This is target variable for predict\n* **MSSubClass**: The building class\n* **MSZoning**: The general zoning classification\n* **LotFrontage**: Linear feet of street connected to property\n* **LotArea**: Lot size in square feet\n* **Street**: Type of road access\n* **Alley**: Type of alley access\n* **LotShape**: General shape of property\n* **LandContour**: Flatness of the property\n* **Utilities**: Type of utilities available\n* **LotConfig**: Lot configuration\n* **LandSlope**: Slope of property\n* **Neighborhood**: Physical locations within Ames city limits\n* **Condition1**: Proximity to main road or railroad\n* **Condition2**: Proximity to main road or railroad (if a second is present)\n* **BldgType**: Type of dwelling\n* **HouseStyle**: Style of dwelling\n* **OverallQual**: Overall material and finish quality\n* **OverallCond**: Overall condition rating\n* **YearBuilt**: Original construction date\n* **YearRemodAdd**: Remodel date\n* **RoofStyle**: Type of roof\n* **RoofMatl**: Roof material\n* **Exterior1st**: Exterior covering on house\n* **Exterior2nd**: Exterior covering on house (if more than one material)\n* **MasVnrType**: Masonry veneer type\n* **MasVnrArea**: Masonry veneer area in square feet\n* **ExterQual**: Exterior material quality\n* **ExterCond**: Present condition of the material on the exterior\n* **Foundation**: Type of foundation\n* **BsmtQual**: Height of the basement\n* **BsmtCond**: General condition of the basement\n* **BsmtExposure**: Walkout or garden level basement walls\n* **BsmtFinType1**: Quality of basement finished area\n* **BsmtFinSF1**: Type 1 finished square feet\n* **BsmtFinType2**: Quality of second finished area (if present)\n* **BsmtFinSF2**: Type 2 finished square feet\n* **BsmtUnfSF**: Unfinished square feet of basement area\n* **TotalBsmtSF**: Total square feet of basement area\n* **Heating**: Type of heating\n* **HeatingQC**: Heating quality and condition\n* **CentralAir**: Central air conditioning\n* **Electrical**: Electrical system\n* **1stFlrSF**: First Floor square feet\n* **2ndFlrSF**: Second floor square feet\n* **LowQualFinSF**: Low quality finished square feet (all floors)\n* **GrLivArea**: Above grade (ground) living area square feet\n* **BsmtFullBath**: Basement full bathrooms\n* **BsmtHalfBath**: Basement half bathrooms\n* **FullBath**: Full bathrooms above grade\n* **HalfBath**: Half baths above grade\n* **Bedroom**: Number of bedrooms above basement level\n* **Kitchen**: Number of kitchens\n* **KitchenQual**: Kitchen quality\n* **TotRmsAbvGrd**: Total rooms above grade (does not include bathrooms)\n* **Functional**: Home functionality rating\n* **Fireplaces**: Number of fireplaces\n* **FireplaceQu**: Fireplace quality\n* **GarageType**: Garage location\n* **GarageYrBlt**: Year garage was built\n* **GarageFinish**: Interior finish of the garage\n* **GarageCars**: Size of garage in car capacity\n* **GarageArea**: Size of garage in square feet\n* **GarageQual**: Garage quality\n* **GarageCond**: Garage condition\n* **PavedDrive**: Paved driveway\n* **WoodDeckSF**: Wood deck area in square feet\n* **OpenPorchSF**: Open porch area in square feet\n* **EnclosedPorch**: Enclosed porch area in square feet\n* **3SsnPorch**: Three season porch area in square feet\n* **ScreenPorch**: Screen porch area in square feet\n* **PoolArea**: Pool area in square feet\n* **PoolQC**: Pool quality\n* **Fence**: Fence quality\n* **MiscFeature**: Miscellaneous feature not covered in other categories\n* **MiscVal**: Value of miscellaneous feature\n* **MoSold**: Month Sold\n* **YrSold**: Year Sold\n* **SaleType**: Type of sale\n* **SaleCondition**: Condition of sale","metadata":{}},{"cell_type":"markdown","source":"#### Since we learn variables, we can start now...\nIf you like this notebook,dont forget to upvote :) **Thanks !**","metadata":{}},{"cell_type":"code","source":"#### IMPORT LIBRARIES\n\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:27.451647Z","iopub.execute_input":"2021-08-02T05:31:27.452086Z","iopub.status.idle":"2021-08-02T05:31:28.916122Z","shell.execute_reply.started":"2021-08-02T05:31:27.451992Z","shell.execute_reply":"2021-08-02T05:31:28.915257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\ntraindf = train.copy()\ntestdf = test.copy()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:28.920194Z","iopub.execute_input":"2021-08-02T05:31:28.922021Z","iopub.status.idle":"2021-08-02T05:31:29.011927Z","shell.execute_reply.started":"2021-08-02T05:31:28.921966Z","shell.execute_reply":"2021-08-02T05:31:29.010952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:29.01598Z","iopub.execute_input":"2021-08-02T05:31:29.016316Z","iopub.status.idle":"2021-08-02T05:31:29.06038Z","shell.execute_reply.started":"2021-08-02T05:31:29.016288Z","shell.execute_reply":"2021-08-02T05:31:29.058755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:29.06174Z","iopub.execute_input":"2021-08-02T05:31:29.062007Z","iopub.status.idle":"2021-08-02T05:31:29.092689Z","shell.execute_reply.started":"2021-08-02T05:31:29.061981Z","shell.execute_reply":"2021-08-02T05:31:29.091644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### I like colors :-9\n\ntrainshape = (\"Train Data:\",train.shape[0],\"obs, and\", train.shape[1], \"features\" )\nprint(\"\\033[95m {}\\033[00m\" .format(trainshape))\ntestshape = (\"Test Data:\",test.shape[0],\"obs, and\", test.shape[1], \"features\" )\nprint(\"\\033[95m {}\\033[00m\" .format(testshape))","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:29.094239Z","iopub.execute_input":"2021-08-02T05:31:29.094538Z","iopub.status.idle":"2021-08-02T05:31:29.101504Z","shell.execute_reply.started":"2021-08-02T05:31:29.094507Z","shell.execute_reply":"2021-08-02T05:31:29.100525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save id \ntrain_id = train[\"Id\"]\ntest_id = test[\"Id\"]\n\n# drop id\ntrain.drop(\"Id\" , axis = 1 , inplace = True)\ntest.drop(\"Id\" , axis = 1 , inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:29.102746Z","iopub.execute_input":"2021-08-02T05:31:29.103012Z","iopub.status.idle":"2021-08-02T05:31:29.119989Z","shell.execute_reply.started":"2021-08-02T05:31:29.102987Z","shell.execute_reply":"2021-08-02T05:31:29.118963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe().T","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:29.121244Z","iopub.execute_input":"2021-08-02T05:31:29.121554Z","iopub.status.idle":"2021-08-02T05:31:29.241513Z","shell.execute_reply.started":"2021-08-02T05:31:29.121525Z","shell.execute_reply":"2021-08-02T05:31:29.240427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Focus Target Variable\n\nsns.distplot(train[\"SalePrice\"] , color = \"g\", bins = 60 , hist_kws={\"alpha\": 0.4});","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:29.244957Z","iopub.execute_input":"2021-08-02T05:31:29.245528Z","iopub.status.idle":"2021-08-02T05:31:29.72911Z","shell.execute_reply.started":"2021-08-02T05:31:29.245483Z","shell.execute_reply":"2021-08-02T05:31:29.727963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see at the above, the target variable SalePrice is not distributed normally.\n\nThis can reduce the performance of the ML regression models because some of them assume normal distribution.\n\nTherfore we need to log transform.","metadata":{}},{"cell_type":"code","source":"sns.distplot(np.log1p(train[\"SalePrice\"]) , color = \"g\", bins = 60 , hist_kws={\"alpha\": 0.4});","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:29.731453Z","iopub.execute_input":"2021-08-02T05:31:29.731889Z","iopub.status.idle":"2021-08-02T05:31:30.051819Z","shell.execute_reply.started":"2021-08-02T05:31:29.731846Z","shell.execute_reply":"2021-08-02T05:31:30.050819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like better :) \n\nNow, let's look at the best 8 correlation with heatmap.","metadata":{}},{"cell_type":"code","source":"corrmatrix = train.corr()\nplt.figure(figsize = (10,6))\ncolumnss = corrmatrix.nlargest(8, \"SalePrice\")[\"SalePrice\"].index\ncm = np.corrcoef(train[columnss].values.T)\nsns.set(font_scale = 1.1)\nhm = sns.heatmap(cm, cbar = True, annot = True, square = True, cmap = \"RdPu\" ,  fmt = \".2f\", annot_kws = {\"size\": 10},\n                 yticklabels = columnss.values, xticklabels = columnss.values)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:30.053155Z","iopub.execute_input":"2021-08-02T05:31:30.053457Z","iopub.status.idle":"2021-08-02T05:31:30.742961Z","shell.execute_reply.started":"2021-08-02T05:31:30.053428Z","shell.execute_reply":"2021-08-02T05:31:30.74222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's look at the distribution of the variable with the 3 highest correlations.","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(figsize = (10, 7))\nsns.boxplot(x = \"OverallQual\", y = \"SalePrice\", data = train);","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:30.743916Z","iopub.execute_input":"2021-08-02T05:31:30.744335Z","iopub.status.idle":"2021-08-02T05:31:31.13586Z","shell.execute_reply.started":"2021-08-02T05:31:30.744307Z","shell.execute_reply":"2021-08-02T05:31:31.134852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.jointplot(x = train[\"GrLivArea\"], y = train[\"SalePrice\"], kind = \"reg\");","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:31.137139Z","iopub.execute_input":"2021-08-02T05:31:31.137431Z","iopub.status.idle":"2021-08-02T05:31:32.128195Z","shell.execute_reply.started":"2021-08-02T05:31:31.137403Z","shell.execute_reply":"2021-08-02T05:31:32.126998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x = train[\"GarageCars\"], y = train[\"SalePrice\"]);","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:32.129842Z","iopub.execute_input":"2021-08-02T05:31:32.130252Z","iopub.status.idle":"2021-08-02T05:31:32.384498Z","shell.execute_reply.started":"2021-08-02T05:31:32.130207Z","shell.execute_reply":"2021-08-02T05:31:32.383511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### - **Outliers**\n\nCan you see two points at the bottom right on GrLivArea. Yes ! It's outliers !\n\nCar garages result in less Sale Price? That doesn't make much sense.\n\nWe need to remove outliers.","metadata":{}},{"cell_type":"code","source":"train = train.drop(train[(train[\"GrLivArea\"] > 4000) \n                         & (train[\"SalePrice\"] < 200000)].index).reset_index(drop = True)\ntrain = train.drop(train[(train[\"GarageCars\"] > 3) \n                         & (train[\"SalePrice\"] < 300000)].index).reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:32.3858Z","iopub.execute_input":"2021-08-02T05:31:32.386098Z","iopub.status.idle":"2021-08-02T05:31:32.399574Z","shell.execute_reply.started":"2021-08-02T05:31:32.386069Z","shell.execute_reply":"2021-08-02T05:31:32.398419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It should look better.","metadata":{}},{"cell_type":"code","source":"sns.jointplot(x = train[\"GrLivArea\"], y = train[\"SalePrice\"], kind = \"reg\");","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:32.401191Z","iopub.execute_input":"2021-08-02T05:31:32.401501Z","iopub.status.idle":"2021-08-02T05:31:33.37696Z","shell.execute_reply.started":"2021-08-02T05:31:32.40147Z","shell.execute_reply":"2021-08-02T05:31:33.376014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x = train[\"GarageCars\"], y = train[\"SalePrice\"]);","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:33.378136Z","iopub.execute_input":"2021-08-02T05:31:33.378422Z","iopub.status.idle":"2021-08-02T05:31:33.608626Z","shell.execute_reply.started":"2021-08-02T05:31:33.378395Z","shell.execute_reply":"2021-08-02T05:31:33.607764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"They Look succesfull.\n\nNow, we need to concanete train and test data for some cleaning operations.","metadata":{}},{"cell_type":"code","source":"df = pd.concat((train, test)).reset_index(drop = True)\ndf.drop([\"SalePrice\"], axis = 1, inplace = True)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:33.609978Z","iopub.execute_input":"2021-08-02T05:31:33.610269Z","iopub.status.idle":"2021-08-02T05:31:33.64497Z","shell.execute_reply.started":"2021-08-02T05:31:33.610242Z","shell.execute_reply":"2021-08-02T05:31:33.644048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Focus missing values\n\ndf.isna().sum().nlargest(35)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:33.646163Z","iopub.execute_input":"2021-08-02T05:31:33.646546Z","iopub.status.idle":"2021-08-02T05:31:33.668092Z","shell.execute_reply.started":"2021-08-02T05:31:33.646505Z","shell.execute_reply":"2021-08-02T05:31:33.667134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nf , ax = plt.subplots(figsize = (12, 6))\nmiss = round(df.isnull().mean()*100,2)\nmiss = miss[miss > 0]\nmiss.sort_values(inplace = True)\nmiss.plot.bar(color = \"g\")\nax.set(title=\"Percent missing data by variables\");","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:33.669502Z","iopub.execute_input":"2021-08-02T05:31:33.669935Z","iopub.status.idle":"2021-08-02T05:31:34.415922Z","shell.execute_reply.started":"2021-08-02T05:31:33.669885Z","shell.execute_reply":"2021-08-02T05:31:34.415217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As can be seen, there are many missing observations in the data.\n\n#### - **Filling missing values**\n\nFor a few columns there is lots of NaN entries.\n\nHowever, reading the data description we find this is not missing data:\n\nFor PoolQC, NaN is not missing data but means no pool, likewise for Fence, FireplaceQu etc.\n\nNow, lets filling NA values :)","metadata":{}},{"cell_type":"code","source":"some_miss_columns = [\"PoolQC\",\"MiscFeature\",\"Alley\",\"Fence\",\"FireplaceQu\",\"GarageType\",\"GarageFinish\",\"GarageQual\",\"GarageCond\",\n                  \"BsmtQual\",\"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\"MasVnrType\",\"MSSubClass\"]\n\nfor i in some_miss_columns :\n        df[i].fillna(\"None\" , inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:34.416942Z","iopub.execute_input":"2021-08-02T05:31:34.417328Z","iopub.status.idle":"2021-08-02T05:31:34.433899Z","shell.execute_reply.started":"2021-08-02T05:31:34.417294Z","shell.execute_reply":"2021-08-02T05:31:34.432924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Functional\"] = df[\"Functional\"].fillna(\"Typ\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:34.435033Z","iopub.execute_input":"2021-08-02T05:31:34.435512Z","iopub.status.idle":"2021-08-02T05:31:34.446473Z","shell.execute_reply.started":"2021-08-02T05:31:34.435478Z","shell.execute_reply":"2021-08-02T05:31:34.445469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"some_miss_columns2 = [\"MSZoning\", \"BsmtFullBath\", \"BsmtHalfBath\", \"Utilities\",\"MSZoning\",\n                      \"Electrical\", \"KitchenQual\", \"SaleType\",\"Exterior1st\", \"Exterior2nd\",\"MasVnrArea\"]\nfor i in some_miss_columns2:\n    df[i].fillna(df[i].mode()[0], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:34.447846Z","iopub.execute_input":"2021-08-02T05:31:34.448212Z","iopub.status.idle":"2021-08-02T05:31:34.472547Z","shell.execute_reply.started":"2021-08-02T05:31:34.448182Z","shell.execute_reply":"2021-08-02T05:31:34.471596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"some_miss_columns3 = [\"GarageYrBlt\", \"GarageArea\", \"GarageCars\",\"BsmtFinSF1\",\"BsmtFinSF2\",\"BsmtUnfSF\",\"TotalBsmtSF\"]\nfor i in some_miss_columns3 :\n    df[i] = df[i].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:34.476233Z","iopub.execute_input":"2021-08-02T05:31:34.476542Z","iopub.status.idle":"2021-08-02T05:31:34.484604Z","shell.execute_reply.started":"2021-08-02T05:31:34.47651Z","shell.execute_reply":"2021-08-02T05:31:34.483606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"LotFrontage\"] = df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:34.486753Z","iopub.execute_input":"2021-08-02T05:31:34.487049Z","iopub.status.idle":"2021-08-02T05:31:34.511107Z","shell.execute_reply.started":"2021-08-02T05:31:34.48702Z","shell.execute_reply":"2021-08-02T05:31:34.50975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We've filled out all the missing data.\n\nLet's control.","metadata":{}},{"cell_type":"code","source":"df.isna().sum().nlargest(3)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:34.512605Z","iopub.execute_input":"2021-08-02T05:31:34.51294Z","iopub.status.idle":"2021-08-02T05:31:34.536816Z","shell.execute_reply.started":"2021-08-02T05:31:34.512908Z","shell.execute_reply":"2021-08-02T05:31:34.535827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We should transform for some variables.","metadata":{}},{"cell_type":"code","source":"Nm = [\"MSSubClass\",\"MoSold\",\"YrSold\"]\nfor col in Nm:\n    df[col] = df[col].astype(str)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:34.538496Z","iopub.execute_input":"2021-08-02T05:31:34.53884Z","iopub.status.idle":"2021-08-02T05:31:34.557374Z","shell.execute_reply.started":"2021-08-02T05:31:34.538808Z","shell.execute_reply":"2021-08-02T05:31:34.556403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### - **Label Encoder**\n\nConvert this kind of categorical text data into model-understandable numerical data, we use the Label Encoder class.","metadata":{}},{"cell_type":"code","source":"lbe = LabelEncoder()\nencodecolumns = (\"FireplaceQu\",\"BsmtQual\",\"BsmtCond\",\"ExterQual\",\"ExterCond\",\"HeatingQC\",\"GarageQual\",\n                \"GarageCond\",\"PoolQC\",\"KitchenQual\",\"BsmtFinType1\",\"BsmtFinType2\",\"Functional\",\"Fence\",\n                \"BsmtExposure\",\"GarageFinish\",\"LandSlope\",\"LotShape\",\"PavedDrive\",\"Street\",\"Alley\",\n                \"CentralAir\",\"MSSubClass\",\"OverallCond\",\"YrSold\",\"MoSold\")\nfor i in encodecolumns :\n    lbe.fit(list(df[i].values))\n    df[i] = lbe.transform(list(df[i].values))","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:34.559033Z","iopub.execute_input":"2021-08-02T05:31:34.559356Z","iopub.status.idle":"2021-08-02T05:31:34.650276Z","shell.execute_reply.started":"2021-08-02T05:31:34.559327Z","shell.execute_reply":"2021-08-02T05:31:34.649161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### - **Log Transform for SalePrice**\n\nWe must apply logarithmic transformation to our target variable.Because ML models work better with normal distribution.","metadata":{}},{"cell_type":"code","source":"train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\ny = train.SalePrice.values\ny[:5]","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:34.651615Z","iopub.execute_input":"2021-08-02T05:31:34.651962Z","iopub.status.idle":"2021-08-02T05:31:34.659818Z","shell.execute_reply.started":"2021-08-02T05:31:34.651929Z","shell.execute_reply":"2021-08-02T05:31:34.658823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### - **Fixing \"Skewed\" features**\n\nWe need to fix all of the skewed data to be more normal so that our models will be more accurate when making predictions.","metadata":{}},{"cell_type":"code","source":"numeric = df.dtypes[df.dtypes != \"object\"].index\nskewed_var = df[numeric].apply(lambda x: skew(x.dropna())).sort_values(ascending = False)\nskewness = pd.DataFrame({\"Skewed Features\" :skewed_var})\nskewness.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:34.661355Z","iopub.execute_input":"2021-08-02T05:31:34.661773Z","iopub.status.idle":"2021-08-02T05:31:34.71429Z","shell.execute_reply.started":"2021-08-02T05:31:34.661731Z","shell.execute_reply":"2021-08-02T05:31:34.713376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will apply box cox transformation to these skewed values. So what is box cox transformation?","metadata":{}},{"cell_type":"markdown","source":"#### - **Box Cox Transformation** \n\n A Box Cox transformation is a transformation of a non-normal dependent variables into a normal shape. Normality is an important assumption for many statistical techniques; if your data isnâ€™t normal, applying a Box-Cox means that you are able to run a broader number of tests.\n \nReferences : Box, G. E. P. and Cox, D. R. (1964). An analysis of transformations.\n\nLets do it.","metadata":{}},{"cell_type":"code","source":"skewness = skewness[abs(skewness) > 0.75]\nskewed_var2 = skewness.index\nfor i in skewed_var2:\n    df[i] = boxcox1p(df[i], 0.15)\n    df[i] += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:34.715416Z","iopub.execute_input":"2021-08-02T05:31:34.715688Z","iopub.status.idle":"2021-08-02T05:31:34.77566Z","shell.execute_reply.started":"2021-08-02T05:31:34.715662Z","shell.execute_reply":"2021-08-02T05:31:34.774707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### - **Dummy Variables**\n\nNext step is dummy variables ! \n\nIn statistics and econometrics, particularly in regression analysis, a dummy variable is one that takes only the value 0 or 1 to indicate the absence or presence of some categorical effect that may be expected to shift the outcome.","metadata":{}},{"cell_type":"code","source":"df = pd.get_dummies(df)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:34.776859Z","iopub.execute_input":"2021-08-02T05:31:34.777153Z","iopub.status.idle":"2021-08-02T05:31:34.834227Z","shell.execute_reply.started":"2021-08-02T05:31:34.777122Z","shell.execute_reply":"2021-08-02T05:31:34.833205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = df[:train.shape[0]]\nX_test = df[train.shape[0]:]","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:34.835554Z","iopub.execute_input":"2021-08-02T05:31:34.83587Z","iopub.status.idle":"2021-08-02T05:31:34.840102Z","shell.execute_reply.started":"2021-08-02T05:31:34.835838Z","shell.execute_reply":"2021-08-02T05:31:34.839414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we are ready to ML, but i want to try PCA. So what is the PCA ?\n","metadata":{}},{"cell_type":"markdown","source":"#### **PCA (Principal component analysis)**\nPCA is used in exploratory data analysis and for making predictive models. It is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible. The first principal component can equivalently be defined as a direction that maximizes the variance of the projected data. Lets try it.\n\n**Note** : You need to **standardize** the data before using PCA.","metadata":{}},{"cell_type":"code","source":"dff = df.copy()\n##df_standardize = StandardScaler().fit_transform(dff)\n##I didn't standardize it again because the data is already close to the standard.\npca = PCA()\npca_fit = pca.fit_transform(dff)\npca = PCA().fit(dff)\nplt.plot(np.cumsum(pca.explained_variance_ratio_));","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:34.841409Z","iopub.execute_input":"2021-08-02T05:31:34.842014Z","iopub.status.idle":"2021-08-02T05:31:35.230968Z","shell.execute_reply.started":"2021-08-02T05:31:34.841981Z","shell.execute_reply":"2021-08-02T05:31:35.230017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With about 30 variables, we can explain 90% of the variance in the dataset.How do we do that ?","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components = 30)\npca_fit = pca.fit_transform(dff)\npca_df = pd.DataFrame(data = pca_fit)\npca_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:35.234353Z","iopub.execute_input":"2021-08-02T05:31:35.23466Z","iopub.status.idle":"2021-08-02T05:31:35.36052Z","shell.execute_reply.started":"2021-08-02T05:31:35.234632Z","shell.execute_reply":"2021-08-02T05:31:35.359435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I didn't have much experience with PCA , so I just wanted to try it. Your positive and negative opinions are important to me :)\n\nNow, we will predict models ! Firstly start Cross-validation with k-folds","metadata":{}},{"cell_type":"code","source":"n_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle = True, random_state = 42).get_n_splits(X_train.values)\n    rmse = np.sqrt(-cross_val_score(model, X_train.values, y, scoring = \"neg_mean_squared_error\", cv = kf))\n    return(rmse)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:35.365846Z","iopub.execute_input":"2021-08-02T05:31:35.368559Z","iopub.status.idle":"2021-08-02T05:31:35.378304Z","shell.execute_reply.started":"2021-08-02T05:31:35.36849Z","shell.execute_reply":"2021-08-02T05:31:35.377078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_xgb = xgb.XGBRegressor(colsample_bytree = 0.2, gamma = 0.0 ,\n                             learning_rate = 0.05, max_depth = 6, \n                             min_child_weight = 1.5, n_estimators = 7200,\n                             reg_alpha = 0.9, reg_lambda = 0.6,\n                             subsample = 0.2,seed = 42,\n                             random_state = 7)\n\nmodel_gbm = GradientBoostingRegressor(n_estimators = 3000, learning_rate = 0.05,\n                                   max_depth = 4, max_features = \"sqrt\",\n                                   min_samples_leaf = 15, min_samples_split = 10, \n                                   loss = \"huber\", random_state = 5)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:35.384418Z","iopub.execute_input":"2021-08-02T05:31:35.387688Z","iopub.status.idle":"2021-08-02T05:31:35.398651Z","shell.execute_reply.started":"2021-08-02T05:31:35.387626Z","shell.execute_reply":"2021-08-02T05:31:35.397479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking performance of base models by evaluating the cross-validation RMSLE error.","metadata":{}},{"cell_type":"code","source":"score = rmsle_cv(model_xgb)\nprint(\"XGBoost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(model_gbm)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:31:35.405319Z","iopub.execute_input":"2021-08-02T05:31:35.408352Z","iopub.status.idle":"2021-08-02T05:33:09.953887Z","shell.execute_reply.started":"2021-08-02T05:31:35.408289Z","shell.execute_reply":"2021-08-02T05:33:09.952622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## we need this func\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:33:09.955392Z","iopub.execute_input":"2021-08-02T05:33:09.955834Z","iopub.status.idle":"2021-08-02T05:33:09.960681Z","shell.execute_reply.started":"2021-08-02T05:33:09.95579Z","shell.execute_reply":"2021-08-02T05:33:09.959719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### - **XGBoost**","metadata":{}},{"cell_type":"code","source":"model_xgb.fit(X_train, y)\nxgb_train_pred = model_xgb.predict(X_train)\nxgb_pred = np.expm1(model_xgb.predict(X_test))\nprint(rmsle(y, xgb_train_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:33:09.961814Z","iopub.execute_input":"2021-08-02T05:33:09.962104Z","iopub.status.idle":"2021-08-02T05:33:25.747208Z","shell.execute_reply.started":"2021-08-02T05:33:09.962068Z","shell.execute_reply":"2021-08-02T05:33:25.74635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_pred[:5]","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:33:25.748588Z","iopub.execute_input":"2021-08-02T05:33:25.749177Z","iopub.status.idle":"2021-08-02T05:33:25.755727Z","shell.execute_reply.started":"2021-08-02T05:33:25.749141Z","shell.execute_reply":"2021-08-02T05:33:25.754836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### - **GBM (Gradient Boosting Machines)**","metadata":{}},{"cell_type":"code","source":"model_gbm.fit(X_train, y)\ngbm_train_pred = model_gbm.predict(X_train)\ngbm_pred = np.expm1(model_gbm.predict(X_test.values))\nprint(rmsle(y, gbm_train_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:33:25.756939Z","iopub.execute_input":"2021-08-02T05:33:25.760399Z","iopub.status.idle":"2021-08-02T05:33:36.733046Z","shell.execute_reply.started":"2021-08-02T05:33:25.760348Z","shell.execute_reply":"2021-08-02T05:33:36.731934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbm_pred[:5]","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:33:36.734256Z","iopub.execute_input":"2021-08-02T05:33:36.734551Z","iopub.status.idle":"2021-08-02T05:33:36.741366Z","shell.execute_reply.started":"2021-08-02T05:33:36.734524Z","shell.execute_reply":"2021-08-02T05:33:36.740349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### - **SUBMISSION**","metadata":{}},{"cell_type":"code","source":"trybest = (0.5 * xgb_pred ) + (0.5 * gbm_pred)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:33:36.742691Z","iopub.execute_input":"2021-08-02T05:33:36.743189Z","iopub.status.idle":"2021-08-02T05:33:36.752395Z","shell.execute_reply.started":"2021-08-02T05:33:36.743147Z","shell.execute_reply":"2021-08-02T05:33:36.751197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({\"Id\": test_id, \"SalePrice\": trybest})\nsubmission.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:33:36.753982Z","iopub.execute_input":"2021-08-02T05:33:36.754373Z","iopub.status.idle":"2021-08-02T05:33:36.77145Z","shell.execute_reply.started":"2021-08-02T05:33:36.754337Z","shell.execute_reply":"2021-08-02T05:33:36.77023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:33:36.773501Z","iopub.execute_input":"2021-08-02T05:33:36.773829Z","iopub.status.idle":"2021-08-02T05:33:36.788038Z","shell.execute_reply.started":"2021-08-02T05:33:36.773798Z","shell.execute_reply":"2021-08-02T05:33:36.787191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](http://media.giphy.com/media/mofrKGJMwOHM4/giphy.gif)","metadata":{}},{"cell_type":"markdown","source":"You can get better scores with different models and combinations. I just wanted to try these 2 models and a combination in this notebook.\n\n**Thanks for attention ! ;)**","metadata":{}}]}