{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-13T21:59:25.367621Z","iopub.execute_input":"2022-01-13T21:59:25.368883Z","iopub.status.idle":"2022-01-13T21:59:25.37861Z","shell.execute_reply.started":"2022-01-13T21:59:25.368841Z","shell.execute_reply":"2022-01-13T21:59:25.378035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainingset = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntestset = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\no_submission = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-13T21:59:25.379728Z","iopub.execute_input":"2022-01-13T21:59:25.380293Z","iopub.status.idle":"2022-01-13T21:59:25.428375Z","shell.execute_reply.started":"2022-01-13T21:59:25.380259Z","shell.execute_reply":"2022-01-13T21:59:25.427588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I am staring out my DS journey to practice what I have leaned so far. This competetion helped me to learn how to impute missing data, how to fix skewness, how to select features using Lasso, and how to fine-tune the parameters for the simple ML models such as Lasso and Ridge. \n\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom scipy.special import boxcox1p\nimport missingno as msno\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn \n\npd.set_option(\"display.float_format\", lambda x: \"{:.3f}\".format(x)) #Limiting floats output to 3 decimal points\npd.set_option(\"display.max_columns\", None)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:25.430322Z","iopub.execute_input":"2022-01-13T21:59:25.430606Z","iopub.status.idle":"2022-01-13T21:59:25.440163Z","shell.execute_reply.started":"2022-01-13T21:59:25.430567Z","shell.execute_reply":"2022-01-13T21:59:25.439406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Basic EDA","metadata":{}},{"cell_type":"code","source":"print(trainingset.shape)\nprint(testset.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:25.441333Z","iopub.execute_input":"2022-01-13T21:59:25.441843Z","iopub.status.idle":"2022-01-13T21:59:25.462396Z","shell.execute_reply.started":"2022-01-13T21:59:25.441816Z","shell.execute_reply":"2022-01-13T21:59:25.460792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#trainingset.describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:25.463603Z","iopub.execute_input":"2022-01-13T21:59:25.464263Z","iopub.status.idle":"2022-01-13T21:59:25.473073Z","shell.execute_reply.started":"2022-01-13T21:59:25.464211Z","shell.execute_reply":"2022-01-13T21:59:25.472138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking duplication\ntrainingset.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:25.474648Z","iopub.execute_input":"2022-01-13T21:59:25.474841Z","iopub.status.idle":"2022-01-13T21:59:25.503003Z","shell.execute_reply.started":"2022-01-13T21:59:25.474812Z","shell.execute_reply":"2022-01-13T21:59:25.502488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note: Train and test data have missing values in diffent columns. Handle all missing values later in the data processing stage.","metadata":{}},{"cell_type":"code","source":"train_null = trainingset.isnull().sum().sort_values(ascending=False)\ntrain_null = train_null[train_null>0]\n\ntest_null = testset.isnull().sum().sort_values(ascending=False)\ntest_null = test_null[test_null>0]\nmissing_df = pd.concat([train_null,test_null, train_null/len(trainingset),test_null/len(testset), trainingset[train_null.index].dtypes, testset[test_null.index].dtypes], axis=1)\nmissing_df.rename({0: \"train_null\", 1: \"test_null\", 2: \"train_null raito\", 3: \"test_null raito\", 4: \"dtype_train\", 5: \"dtype_test\" },  axis='columns')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-13T21:59:25.695092Z","iopub.execute_input":"2022-01-13T21:59:25.695353Z","iopub.status.idle":"2022-01-13T21:59:25.740332Z","shell.execute_reply.started":"2022-01-13T21:59:25.695325Z","shell.execute_reply":"2022-01-13T21:59:25.73933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.1 Analyse target variable: SalePrice\n* SalePrice has positive skewness.","metadata":{}},{"cell_type":"code","source":"trainingset.SalePrice.describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:25.741754Z","iopub.execute_input":"2022-01-13T21:59:25.741968Z","iopub.status.idle":"2022-01-13T21:59:25.751099Z","shell.execute_reply.started":"2022-01-13T21:59:25.741945Z","shell.execute_reply":"2022-01-13T21:59:25.750503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(trainingset.SalePrice)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:25.752332Z","iopub.execute_input":"2022-01-13T21:59:25.752513Z","iopub.status.idle":"2022-01-13T21:59:26.013754Z","shell.execute_reply.started":"2022-01-13T21:59:25.752487Z","shell.execute_reply":"2022-01-13T21:59:26.013364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skew = trainingset.SalePrice.skew()\nkurt = trainingset.SalePrice.kurt()\nprint(\"Skew: {} /  Kurt: {}\".format(skew, kurt))","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:26.014775Z","iopub.execute_input":"2022-01-13T21:59:26.015298Z","iopub.status.idle":"2022-01-13T21:59:26.020135Z","shell.execute_reply.started":"2022-01-13T21:59:26.015268Z","shell.execute_reply":"2022-01-13T21:59:26.019458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.2 Bivariate analysis","metadata":{}},{"cell_type":"code","source":"# Replace missging Age values based on other columns\n# check correlation between different variables\nmatrix = trainingset.select_dtypes(exclude=\"object\").corr()\nmask = np.triu(np.ones_like(matrix, dtype=bool))\ncmap = sns.diverging_palette(220, 25, s=80, n=9, as_cmap=True, center=\"light\")\nplt.figure(figsize=(12, 10))\nsns.heatmap(matrix, mask = mask, annot=False, cmap=cmap, square=True, fmt='.2f',linewidth=.2, center=0, vmin=-0.15, vmax=0.55)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:26.021171Z","iopub.execute_input":"2022-01-13T21:59:26.021377Z","iopub.status.idle":"2022-01-13T21:59:26.869483Z","shell.execute_reply.started":"2022-01-13T21:59:26.021348Z","shell.execute_reply":"2022-01-13T21:59:26.868034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* SalePrice is highly corelated with \n'OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF', '1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt'","metadata":{}},{"cell_type":"code","source":"# Pick top 10 features that highly corelated to SalePrice\nk = 10 #number of variables for heatmap\ncol_names = matrix.nlargest(k, 'SalePrice')['SalePrice'].index\ncoef_matrix = np.corrcoef(trainingset[col_names].values.T)\nmask = np.triu(np.ones_like(coef_matrix, dtype=bool))\ncmap = sns.diverging_palette(220, 25, s=80, n=9, as_cmap=True, center=\"light\")\nplt.figure(figsize=(10, 10))\nsns.heatmap(coef_matrix, cbar=True, mask = mask, annot=True, cmap=cmap, square=True, fmt='.2f',linewidth=.2, center=0, vmin=-0.15, vmax=0.55, yticklabels=col_names.values, xticklabels=col_names.values)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:26.871228Z","iopub.execute_input":"2022-01-13T21:59:26.871495Z","iopub.status.idle":"2022-01-13T21:59:27.375649Z","shell.execute_reply.started":"2022-01-13T21:59:26.871457Z","shell.execute_reply":"2022-01-13T21:59:27.375015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* There are outliers in GrLivArea (bottom right corner), TotalBsmtSF, and 1stFlrSF\n* After checking these outliers, ID 1298 is the common outliers for all three features, remove this later.\n* GrLivArea also has another outlier ID 523, removing this later too.\n\n* 1stFlrSF and TotalBsmtSF are highly correlated. (0.82)\n* TotRmsAbvGrd and GrLiveArea are also highly correlated (0.83)\n* GarageArea and GarageCards are highly correlated (0.88)\n\n","metadata":{}},{"cell_type":"code","source":"# plot multiple subplots to validate correlations\nfig, axs = plt.subplots(2, 2, figsize=(10,8))\naxs[0, 0].plot(trainingset.GrLivArea, trainingset.SalePrice, marker=\"o\", linestyle=\"\", markersize=1.2, alpha=.8)\naxs[0, 0].set_title('GrLivArea vs SalePrice')\naxs[0, 1].plot(trainingset.GarageArea, trainingset.SalePrice, marker=\"o\", linestyle=\"\", markersize=1.2, alpha=.8)\naxs[0, 1].set_title('GarageArea vs SalePrice')\naxs[1, 0].plot(trainingset.TotalBsmtSF, trainingset.SalePrice, marker=\"o\", linestyle=\"\", markersize=1.2, alpha=.8)\naxs[1, 0].set_title('TotalBsmtSF vs SalePrice')\naxs[1, 1].plot(trainingset[\"1stFlrSF\"], trainingset.SalePrice, marker=\"o\", linestyle=\"\", markersize=1.2, alpha=.8)\naxs[1, 1].set_title('1stFlrSF vs SalePrice')\n\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:27.376693Z","iopub.execute_input":"2022-01-13T21:59:27.376905Z","iopub.status.idle":"2022-01-13T21:59:28.227581Z","shell.execute_reply.started":"2022-01-13T21:59:27.376876Z","shell.execute_reply":"2022-01-13T21:59:28.22682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(12, 8))\nfig.suptitle('Categorical data vs SalePrice')\n\nsns.boxplot(ax=axes[0, 0], data=trainingset, x='OverallQual', y='SalePrice')\nsns.boxplot(ax=axes[0, 1], data=trainingset, x='GarageCars', y='SalePrice')\nsns.boxplot(ax=axes[1, 0], data=trainingset, x='FullBath', y='SalePrice')\nsns.boxplot(ax=axes[1, 1], data=trainingset, x='TotRmsAbvGrd', y='SalePrice')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:28.229231Z","iopub.execute_input":"2022-01-13T21:59:28.229394Z","iopub.status.idle":"2022-01-13T21:59:28.971291Z","shell.execute_reply.started":"2022-01-13T21:59:28.229373Z","shell.execute_reply":"2022-01-13T21:59:28.970834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Processing","metadata":{}},{"cell_type":"markdown","source":"### Handling Outliers","metadata":{}},{"cell_type":"code","source":"# removing two outliers in GrLivArea\nprint(\"Before: {}\".format(trainingset.shape))\noutlier_index = trainingset[(trainingset.GrLivArea > 4000) & (trainingset.SalePrice < 200000)].index\ntrainingset = trainingset.drop(outlier_index)\nprint(\"After: {}\".format(trainingset.shape))\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:28.972406Z","iopub.execute_input":"2022-01-13T21:59:28.972561Z","iopub.status.idle":"2022-01-13T21:59:28.982409Z","shell.execute_reply.started":"2022-01-13T21:59:28.972539Z","shell.execute_reply":"2022-01-13T21:59:28.981426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Investigate 'LotFrontage'\n# After checking plots below, droped two outliers in LotArea(one row)  and LotFrontage (two rows)\n# sns.scatterplot(trainingset.LotArea, trainingset.SalePrice)\n# sns.scatterplot(trainingset.LotFrontage, trainingset.SalePrice)\n\n# removing outliers in LotFrontage and LotArea\ntrainingset = trainingset.loc[trainingset.LotArea != max(trainingset.LotArea), :] # one row\ntrainingset = trainingset.loc[trainingset.LotFrontage != max(trainingset.LotFrontage), :] # one row\nprint(\"trainingset data size: {}\".format(trainingset.shape))\nprint(\"After: {}\".format(trainingset.shape))","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:28.983705Z","iopub.execute_input":"2022-01-13T21:59:28.983882Z","iopub.status.idle":"2022-01-13T21:59:28.995364Z","shell.execute_reply.started":"2022-01-13T21:59:28.98386Z","shell.execute_reply":"2022-01-13T21:59:28.99443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handling Missing Values\n##### Notes:\n* NA values can cause errors with machine learning later down the line, so I will impute the missing values.\n* PoolQC, MiscFeature, Alley, Fence columns have more than 80% of missing values. If there are no good ways to figure out the values, I will drop these columns. The rest columns (FireplaceQu, Lotfrontage etc.) need to impute missing values.\n* Based on MSNO graph, features start with 'Garage' in the trainingset have the same number of missing data. \n* Based on MSNO graph, Features start with 'Bsmt' in the trainingset have the same number of missing data. ","metadata":{}},{"cell_type":"code","source":"nullcols_train = [col for col in trainingset.columns if trainingset[col].isnull().any()]\nnullcols_test = [col for col in testset.columns if testset[col].isnull().any()]\nprint(nullcols_train, len(nullcols_train))\nprint(nullcols_test, len(nullcols_test))","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:28.996473Z","iopub.execute_input":"2022-01-13T21:59:28.996621Z","iopub.status.idle":"2022-01-13T21:59:29.039624Z","shell.execute_reply.started":"2022-01-13T21:59:28.996601Z","shell.execute_reply":"2022-01-13T21:59:29.039151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualise missing values.\nmsno.matrix(trainingset.loc[:, nullcols_train])\n# #msno.matrix(trainingset.select_dtypes(include=\"object\")) \n# #msno.matrix(testset.loc[:, test_null.index])","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:29.040324Z","iopub.execute_input":"2022-01-13T21:59:29.040471Z","iopub.status.idle":"2022-01-13T21:59:29.556562Z","shell.execute_reply.started":"2022-01-13T21:59:29.04045Z","shell.execute_reply":"2022-01-13T21:59:29.555758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PoolQC : data description says NA means \"No Pool\". \n# MiscFeature : data description says NA means \"no misc feature\".\n# Alley : data description says NA means \"no alley access\".\n# Fence: data description says NA means \"no fence\". \n# FireplaceQu: data description says NA meand \"NoFireplace\". \n# Features start with 'Garage' in the trainingset have the same number of missing data. \n#-----GarageType, GarageFinish,GarageCond, GarageQual : data description says NA means \"No Garage\". \n#-----GarageYrBlt has NA beacause the property does not have garage.\n#-----GarageArea and GarageCars in the testset has one missing value each beacuse the property does not have a garage according to GarageQual column.\n# Features start with 'Bsmtuniquere missing same data.\n#-----BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2 : data description says NA means \"No Basement\". \n#-----'BsmtFullBath','BsmtHalfBath','BsmtUnfSF', 'TotalBsmtSF' : data is missing because there is no basement.\n# MasVnrType, MasVnrArea : Data is missing probably there is no veneer. (8 values are missing in the trainingset.)\nvalues = {\n    \"PoolQC\": \"None\",\n    \"MiscFeature\": \"None\",\n    \"Alley\": \"None\",\n    \"Fence\": \"None\",\n    \"FireplaceQu\": \"None\",\n    \"GarageType\": \"None\",\n    \"GarageFinish\": \"None\",    \n    \"GarageCond\": \"None\",\n    \"GarageQual\": \"None\",\n    \"GarageYrBlt\": 1880.0, #No garage, give an artificial year here    \n    \"GarageArea\": 0,#No garage\n    \"GarageCars\": 0,#No garage\n    \"BsmtExposure\": \"None\",#No basement\n    \"BsmtFinType2\": \"None\",#No basement\n    \"BsmtFinType1\": \"None\",#No basement\n    \"BsmtCond\": \"None\",#No basement\n    \"BsmtQual\": \"None\",#No basement\n    \"BsmtFullBath\":0, #No basement\n    \"BsmtHalfBath\":0, #No basement\n    \"BsmtUnfSF\":0, #No basement\n    \"BsmtFinSF2\":0, #No basement\n    \"BsmtFinSF1\":0, #No basement\n    \"TotalBsmtSF\":0, #No basement\n    \"MasVnrArea\": 0,#No Veneer\n    \"MasVnrType\": \"None\"\n}\n\ntrainingset.fillna(value=values, inplace=True)\ntestset.fillna(value=values, inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:29.557654Z","iopub.execute_input":"2022-01-13T21:59:29.557826Z","iopub.status.idle":"2022-01-13T21:59:29.581734Z","shell.execute_reply.started":"2022-01-13T21:59:29.557804Z","shell.execute_reply":"2022-01-13T21:59:29.581014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Electrical, MSZoning,Exterior2nd, Exterior1st, SaleType, KitchenQual: these columns have one or two missing values, \n# imputing missing values with the most common category in the corresponding column.\nmost_common_cols = [\"Electrical\", \"MSZoning\", \"Exterior1st\", \"Exterior2nd\", \"SaleType\", \"KitchenQual\"]\nfor col in most_common_cols:\n    trainingset[col] = trainingset[col].fillna(trainingset[col].mode()[0])\n    testset[col] = testset[col].fillna(testset[col].mode()[0])\n\n#data description says \"Assume typical unless deductions are warranted\"\ntestset[\"Functional\"] = testset[\"Functional\"].fillna(\"Typ\") \n\n#LotFrontage : Since the area of each street connected to the house property most likely have \n# a similar area to other houses in its neighborhood, fill in missing values by the median LotFrontage of the same neighborhood.\ntrainingset[\"LotFrontage\"] = trainingset.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\ntestset[\"LotFrontage\"] = testset.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:29.582437Z","iopub.execute_input":"2022-01-13T21:59:29.582589Z","iopub.status.idle":"2022-01-13T21:59:29.62713Z","shell.execute_reply.started":"2022-01-13T21:59:29.582568Z","shell.execute_reply":"2022-01-13T21:59:29.62627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Handle Low Variance columns","metadata":{}},{"cell_type":"code","source":"# Check low variance columns in categorical columns\nresults = {}\ndef valueCount(df):\n    df_filter = df.select_dtypes(include=\"object\")\n    for col in df_filter.columns:\n        result = df_filter[col].value_counts(normalize=True) \n        results[\"------\" + col + \"------\"] = result\n    return results\n\nvalueCount(trainingset)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-13T21:59:29.62926Z","iopub.execute_input":"2022-01-13T21:59:29.629448Z","iopub.status.idle":"2022-01-13T21:59:29.676802Z","shell.execute_reply.started":"2022-01-13T21:59:29.629422Z","shell.execute_reply":"2022-01-13T21:59:29.676019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Select low variance columns from the value_counts above and remove them as these columns are dominated by one single value and not informative to our target at all.\n* For instance, 99% of Street has only one value \"Pave\" ","metadata":{}},{"cell_type":"code","source":"# 99% of Street has only one value \"Pave\" \nlow_variance_cols = ['Street','Utilities', 'Heating','RoofMatl','Condition2', 'PoolQC', 'MiscFeature'] \nfor col in low_variance_cols:\n    print(trainingset[col].value_counts(normalize=True)) \ntrainingset = trainingset.drop(low_variance_cols,axis=1) # drop 7 columns\ntestset = testset.drop(low_variance_cols,axis=1) # drop 7 columns","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:29.678064Z","iopub.execute_input":"2022-01-13T21:59:29.678264Z","iopub.status.idle":"2022-01-13T21:59:29.693665Z","shell.execute_reply.started":"2022-01-13T21:59:29.678232Z","shell.execute_reply":"2022-01-13T21:59:29.69302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check if all missing data are imputed.\nprint(trainingset.isnull().sum().any())\nprint(testset.isnull().sum().any())\n# Check data shapes\nprint(trainingset.shape)\nprint(testset.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:29.694712Z","iopub.execute_input":"2022-01-13T21:59:29.694878Z","iopub.status.idle":"2022-01-13T21:59:29.716466Z","shell.execute_reply.started":"2022-01-13T21:59:29.694854Z","shell.execute_reply":"2022-01-13T21:59:29.71582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Handling Skewness in data\nIn skewed data, the tail region may act as an outlier for the statistical model and we know that outliers adversely affect the modelâ€™s performance especially regression-based models. There are statistical model that are robust to outlier like a Tree-based models but it will limit the possibility to try other models. So there is a necessity to transform the skewed data to close enough to a Gaussian distribution or Normal distribution. This will allow us to try more number of statistical model.\n\n\n* Before fixing skewness in data, tree-based models performed better.\n* After fixing skewness in data, linear models performed better.\n\nThe rule of thumb seems to be:\n* If the skewness is between -0.5 and 0.5, the data are fairly symmetrical.\n* If the skewness is between -1 and -0.5(negatively skewed) or between 0.5 and 1(positively skewed), the data are moderately skewed.\n* If the skewness is less than -1(negatively skewed) or greater than 1(positively skewed), the data are highly skewed.","metadata":{}},{"cell_type":"code","source":"#Before handling skewness in SalePrice\n#histogram and normal probability plot\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8,4))\nsns.distplot(ax=axs[0], x=trainingset['SalePrice'], fit=norm);\nstats.probplot(x= trainingset['SalePrice'], plot=plt)\nplt.show()\n\n# After handling skewness\n# Applying log transformation on columns without zero values\ntrainingset[\"SalePrice\"] = np.log(trainingset[\"SalePrice\"])\n\n# Transformed histogram and normal probability plot\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8,4))\nsns.distplot(ax=axs[0], x=trainingset['SalePrice'], fit=norm);\nstats.probplot(x= trainingset['SalePrice'], plot=plt)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:29.717363Z","iopub.execute_input":"2022-01-13T21:59:29.71758Z","iopub.status.idle":"2022-01-13T21:59:30.240485Z","shell.execute_reply.started":"2022-01-13T21:59:29.717548Z","shell.execute_reply":"2022-01-13T21:59:30.239342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_cols = trainingset.select_dtypes(exclude=\"object\").columns\nnumerical_cols = numerical_cols.drop(\"Id\")\nhighly_skewed_cols = [col for col in numerical_cols if np.abs(trainingset[col].skew()) > 0.75] #20 columns\n\n# Using boxcox from scipy library to fix skewness\nlam = 0.15\nfor col in highly_skewed_cols:\n    trainingset[col] = boxcox1p(trainingset[col], lam)\n    testset[col] = boxcox1p(testset[col], lam)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:30.243619Z","iopub.execute_input":"2022-01-13T21:59:30.243829Z","iopub.status.idle":"2022-01-13T21:59:30.27668Z","shell.execute_reply.started":"2022-01-13T21:59:30.243802Z","shell.execute_reply":"2022-01-13T21:59:30.275699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainingset[highly_skewed_cols].hist(figsize=(12,12))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:30.280206Z","iopub.execute_input":"2022-01-13T21:59:30.280401Z","iopub.status.idle":"2022-01-13T21:59:32.107933Z","shell.execute_reply.started":"2022-01-13T21:59:30.280375Z","shell.execute_reply":"2022-01-13T21:59:32.107284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check homoscedasticity","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(trainingset.GrLivArea, trainingset.SalePrice)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:32.108908Z","iopub.execute_input":"2022-01-13T21:59:32.109117Z","iopub.status.idle":"2022-01-13T21:59:32.29952Z","shell.execute_reply.started":"2022-01-13T21:59:32.109073Z","shell.execute_reply":"2022-01-13T21:59:32.298706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(trainingset[\"1stFlrSF\"], trainingset.SalePrice)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:32.300689Z","iopub.execute_input":"2022-01-13T21:59:32.300877Z","iopub.status.idle":"2022-01-13T21:59:32.492568Z","shell.execute_reply.started":"2022-01-13T21:59:32.300853Z","shell.execute_reply":"2022-01-13T21:59:32.49175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Handle oridinal columns","metadata":{}},{"cell_type":"code","source":"ordinal_cols = [\"ExterQual\", \"ExterCond\", \"BsmtCond\", \"BsmtQual\",\"HeatingQC\", \"KitchenQual\", \n                \"FireplaceQu\",\"GarageQual\", \"GarageCond\", \"BsmtFinType1\", \"BsmtFinType2\"]\nbin_map  = {'Ex':4, 'Gd':3,'TA':2, 'Fa':1,'Po':1,  'None':0, \n            \"GLQ\" : 6, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3,\"BLQ\" : 4, \"ALQ\" : 5\n            }\nfor col in ordinal_cols:\n    trainingset[col] = trainingset[col].map(bin_map)\n    testset[col] = testset[col].map(bin_map)\n\n    \nBsmtExposure = {\"Gd\" : 4, \"Av\": 3, \"Mn\" : 2, 'No': 1, \"None\":0}\ntrainingset['BsmtExposure'] = trainingset['BsmtExposure'].map(BsmtExposure)\ntestset['BsmtExposure'] = testset['BsmtExposure'].map(BsmtExposure)\n\nPavedDrive =   {\"Y\" : 2, \"N\" : 0, \"P\" : 1}\ntrainingset['PavedDrive'] = trainingset['PavedDrive'].map(PavedDrive)\ntestset['PavedDrive'] = testset['PavedDrive'].map(PavedDrive)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:32.495066Z","iopub.execute_input":"2022-01-13T21:59:32.495263Z","iopub.status.idle":"2022-01-13T21:59:32.532599Z","shell.execute_reply.started":"2022-01-13T21:59:32.495241Z","shell.execute_reply":"2022-01-13T21:59:32.531778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(trainingset.shape)\nprint(testset.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:32.533738Z","iopub.execute_input":"2022-01-13T21:59:32.533904Z","iopub.status.idle":"2022-01-13T21:59:32.53801Z","shell.execute_reply.started":"2022-01-13T21:59:32.533882Z","shell.execute_reply":"2022-01-13T21:59:32.537397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(trainingset.isnull().sum().any())\nprint(testset.isnull().sum().any())","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:32.538906Z","iopub.execute_input":"2022-01-13T21:59:32.539129Z","iopub.status.idle":"2022-01-13T21:59:32.563447Z","shell.execute_reply.started":"2022-01-13T21:59:32.5391Z","shell.execute_reply":"2022-01-13T21:59:32.562532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature selection with Lasso","metadata":{}},{"cell_type":"markdown","source":"I ran the following code to select important features first and then fit various models only with the selected features.","metadata":{}},{"cell_type":"code","source":"# print(\"{}{}\".format(trainingset.shape, testset.shape))\n\n# # Spliting Data\n# X = trainingset.drop([\"Id\",\"SalePrice\"], axis=1)\n# y = trainingset.SalePrice\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# print(\"{}{}{}{}\".format(X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n\n# result = {}\n# def evaluation(y, y_pred):\n#     r_squared = r2_score(y, y_pred),\n#     mae = mean_absolute_error(y, y_pred),\n#     mse = mean_squared_error(y, y_pred),\n#     rmse = np.sqrt(mean_squared_error(y, y_pred))\n#     return r_squared, mae, mse, rmse\n\n# low_cardinality_cols = [col for col in X_train.columns if (X_train[col].dtype==\"object\") & (X_train[col].nunique()<26)]\n# #print(len(low_cardinality_cols), low_cardinality_cols)\n\n# column_trans = make_column_transformer(\n#     (OneHotEncoder(handle_unknown=\"ignore\", sparse=False),low_cardinality_cols),\n#     remainder='passthrough'\n# )\n# sc = StandardScaler()\n\n\n# model = Lasso() \n# pipe = make_pipeline(column_trans, sc, model) \n# params = {\"lasso__alpha\": np.arange(0.001,0.005,0.001)} #print(pipe.get_params().keys())\n# cv_lasso = GridSearchCV(pipe, param_grid=params)\n# cv_lasso.fit(X_train, y_train)\n# preds_lasso = cv_lasso.predict(X_test)\n# preds_lasso = np.exp(preds_lasso) #inverse logrithm on predicted SalePrice.\n\n\n# lasso_r2, lasso_mae, lasso_mse, lasso_rmse = evaluation(y_test, preds_lasso) \n# print(lasso_r2,lasso_mae, lasso_mse, lasso_rmse)\n# print(cv_lasso.best_params_, cv_lasso.best_score_)\n\n# ############################\n# # Plot feature graph\n\n# temp_df = pd.DataFrame(column_trans.fit_transform(X_train), columns=column_trans.get_feature_names())\n# cols = temp_df.columns \n# temp_df = pd.DataFrame(sc.fit_transform(temp_df), columns=cols)\n# names = cols\n# lasso = Lasso(alpha=0.003)\n# lasso_coef = lasso.fit(temp_df, y_train).coef_\n\n# # pick important features\n# names_n = np.array(names)[np.abs(lasso_coef)>0.005]\n# lasso_coef_n = lasso_coef[np.abs(lasso_coef)>0.005]\n# print(len(names_n),len(lasso_coef_n))\n\n# _ = plt.figure(figsize=(8,8))\n# _ = plt.plot(range(len(names_n)), lasso_coef_n,  'bo-')\n# _ = plt.xticks(range(len(names_n)), names_n, rotation=90)\n# _ = plt.ylabel('Coefficients')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:32.564603Z","iopub.execute_input":"2022-01-13T21:59:32.564753Z","iopub.status.idle":"2022-01-13T21:59:32.571366Z","shell.execute_reply.started":"2022-01-13T21:59:32.564732Z","shell.execute_reply":"2022-01-13T21:59:32.569891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tried with different threshold of lasso coef and finally selected the best performing threshold.\n* np.abs(lasso_coef)>0.005, which gave me 36 important features.","metadata":{}},{"cell_type":"code","source":"# cat_names = ['MSZoning', 'Alley', 'LotShape','LandContour', 'LotConfig', 'LandSlope',\n#                    'Neighborhood', 'Condition1', 'BldgType','HouseStyle', \n#                    'RoofStyle', 'Exterior1st','Exterior2nd', 'MasVnrType', 'Foundation',\n#                    'CentralAir', 'Electrical', 'Functional',\n#                    'GarageType', 'GarageFinish', 'Fence','SaleType', 'SaleCondition']\n# for i, v in enumerate(cat_names):\n#     print(i, \":\", v)\n\n# #names_n\n\n############# Selected features after fixing the skewness ############# \n# # # # Threshold: abs(Lasso coef)> 0 (after fixed skewness)\n# features_53 = ['MSZoning','Alley','LotShape', 'LandContour','LotConfig', 'LandSlope', 'Neighborhood',\n# 'Condition1','BldgType', 'HouseStyle','RoofStyle', 'Exterior1st', 'Exterior2nd','MasVnrType','Foundation',\n# 'CentralAir', 'Functional', 'GarageType', 'GarageFinish','Fence',  'SaleType','SaleCondition',\n# 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond','YearBuilt', 'YearRemodAdd', 'ExterQual', 'BsmtQual',\n# 'BsmtExposure', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'HeatingQC', '1stFlrSF', 'GrLivArea',\n# 'BsmtFullBath', 'FullBath', 'HalfBath', 'KitchenAbvGr','KitchenQual', 'TotRmsAbvGrd', 'Fireplaces', \n#  'FireplaceQu','GarageCars', 'GarageArea', 'GarageQual', 'WoodDeckSF','OpenPorchSF', 'ScreenPorch', 'PoolArea']\n\n# # find highly correlated features\n# corr_matrix = trainingset[features_53].corr().abs()\n# # Select upper triangle of correlation matrix\n# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n# # Find features with correlation greater than 0.8\n# to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n# print(to_drop)\n# feature_selection = set(features_53) - set(to_drop)\n# print(len(feature_selection))\n\n\n############# Selected features before fixing the skewness ############# \n# # Threshold: abs(Lasso coef)> 0\n# selected_features_64 = ['MSZoning', 'Alley', 'LotShape','LandContour', 'LotConfig', 'LandSlope', \n#                         'Neighborhood', 'Condition1', 'BldgType','HouseStyle', 'RoofStyle', \n#                         'Exterior1st','Exterior2nd', 'MasVnrType', 'Foundation',\n#                         'Functional','GarageType', 'GarageFinish', 'Fence','SaleType', 'SaleCondition',\n#                         'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual','OverallCond', \n#                         'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'ExterQual', 'ExterCond', \n#                         'BsmtQual', 'BsmtCond', 'BsmtExposure','BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', \n#                         'TotalBsmtSF','HeatingQC', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', \n#                         'HalfBath', 'BedroomAbvGr','KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', \n#                         'Fireplaces', 'FireplaceQu', 'GarageYrBlt', 'GarageCars', 'GarageArea',\n#                         'GarageQual', 'GarageCond', 'PavedDrive', 'WoodDeckSF','OpenPorchSF', \n#                         '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MoSold', 'YrSold']\n\n# # Threshold: abs(Lasso coef)> 1000\n# selected_features_48 = ['MSZoning', 'LotShape', 'LotConfig', 'Neighborhood', 'Condition1', 'BldgType','HouseStyle', 'Exterior1st',\n#  'Exterior2nd', 'MasVnrType', 'Foundation', 'Functional','GarageType',  'Fence','SaleCondition', 'MSSubClass', 'LotArea','OverallQual', 'OverallCond', 'YearBuilt', 'MasVnrArea','ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n# 'BsmtFinType1', 'BsmtFinSF1', 'TotalBsmtSF', '2ndFlrSF','LowQualFinSF', 'GrLivArea', \n# 'BsmtFullBath', 'HalfBath','BedroomAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Fireplaces',\n# 'FireplaceQu', 'GarageCars', 'GarageArea', 'GarageQual','WoodDeckSF', 'OpenPorchSF', \n# '3SsnPorch', 'ScreenPorch','PoolArea', 'MoSold']\n\n\n# # Threshold: abs(Lasso coef)> 2000\n# selected_features_31 = ['LandSlope', 'Neighborhood','BldgType','HouseStyle', \n#                         'Exterior1st','MasVnrType', 'Functional','GarageFinish','SaleCondition',\n#                         'MSSubClass', 'LotArea','OverallQual', 'OverallCond', 'YearBuilt', \n#                         'MasVnrArea','ExterQual', 'ExterCond', 'BsmtQual', 'BsmtExposure', 'BsmtFinSF1',\n#                         'TotalBsmtSF', '2ndFlrSF', 'GrLivArea', 'BedroomAbvGr','KitchenQual', \n#                         'TotRmsAbvGrd', 'GarageCars', 'GarageArea','GarageQual', 'OpenPorchSF', 'PoolArea']\n\n# # Threshold: abs(Lasso coef)> 7000\n# selected_features_5 = ['SaleType', 'OverallQual', 'YearBuilt', 'BsmtFinSF1','GrLivArea']\n\n\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-13T21:59:32.572477Z","iopub.execute_input":"2022-01-13T21:59:32.572646Z","iopub.status.idle":"2022-01-13T21:59:32.586623Z","shell.execute_reply.started":"2022-01-13T21:59:32.572622Z","shell.execute_reply":"2022-01-13T21:59:32.585936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Threshold: abs(Lasso coef)> 0.005 (after fixed skewness)\n# features_36 = ['MSZoning', 'LotConfig','Neighborhood', 'Condition1', 'BldgType', 'Exterior1st','MasVnrType', \n#  'Foundation','CentralAir', 'Functional','SaleType', 'SaleCondition', 'LotArea', 'OverallQual',\n#  'OverallCond', 'YearBuilt', 'YearRemodAdd', 'ExterQual','BsmtQual', 'BsmtExposure', \n#  'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', 'HeatingQC', '1stFlrSF', 'GrLivArea', 'BsmtFullBath',\n#  'HalfBath', 'KitchenAbvGr', 'KitchenQual','Fireplaces', 'GarageCars', 'GarageArea', 'GarageQual',\n#  'WoodDeckSF', 'ScreenPorch']\n\n############# Remove redundant features #################\n# removed two highly correlated features: 1stFlrSF and GarageCards\n# 1stFlrSF and TotalBsmtSF are highly correlated. (0.82)\n# TotRmsAbvGrd and GrLiveArea are also highly correlated (0.83)\n# GarageArea and GarageCards are highly correlated (0.88)\nfeature_selection = ['MSZoning', 'LotConfig','Neighborhood', 'Condition1', 'BldgType', 'Exterior1st','MasVnrType', \n 'Foundation','CentralAir', 'Functional','SaleType', 'SaleCondition', 'LotArea', 'OverallQual',\n 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'ExterQual','BsmtQual', 'BsmtExposure', \n 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', 'HeatingQC',  'GrLivArea', 'BsmtFullBath',\n 'HalfBath', 'KitchenAbvGr', 'KitchenQual','Fireplaces', 'GarageArea', 'GarageQual',\n 'WoodDeckSF', 'ScreenPorch']\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:32.587587Z","iopub.execute_input":"2022-01-13T21:59:32.587751Z","iopub.status.idle":"2022-01-13T21:59:32.605655Z","shell.execute_reply.started":"2022-01-13T21:59:32.587729Z","shell.execute_reply":"2022-01-13T21:59:32.604945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Models with selected features","metadata":{}},{"cell_type":"code","source":"print(\"{}{}\".format(trainingset.shape, testset.shape))\n\n# Spliting Data\nX = trainingset[feature_selection]\ny = trainingset.SalePrice\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(\"{}{}{}{}\".format(X_train.shape, y_train.shape, X_test.shape, y_test.shape))","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:32.606783Z","iopub.execute_input":"2022-01-13T21:59:32.606966Z","iopub.status.idle":"2022-01-13T21:59:32.625929Z","shell.execute_reply.started":"2022-01-13T21:59:32.606942Z","shell.execute_reply":"2022-01-13T21:59:32.625445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = {}\ndef evaluation(y, y_pred):\n    r_squared = r2_score(y, y_pred),\n    mae = mean_absolute_error(y, y_pred),\n    mse = mean_squared_error(y, y_pred),\n    rmse = np.sqrt(mean_squared_error(y, y_pred))\n    return r_squared, mae, mse, rmse","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:32.627317Z","iopub.execute_input":"2022-01-13T21:59:32.62765Z","iopub.status.idle":"2022-01-13T21:59:32.636931Z","shell.execute_reply.started":"2022-01-13T21:59:32.627619Z","shell.execute_reply":"2022-01-13T21:59:32.635062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"low_cardinality_cols = [col for col in X_train.columns if (X_train[col].dtype==\"object\") & (X_train[col].nunique()<26)]\n#print(len(low_cardinality_cols), low_cardinality_cols)\n\ncolumn_trans = make_column_transformer(\n    (OneHotEncoder(handle_unknown=\"ignore\", sparse=False),low_cardinality_cols),\n    remainder='passthrough'\n)\nsc = StandardScaler()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:32.63864Z","iopub.execute_input":"2022-01-13T21:59:32.63883Z","iopub.status.idle":"2022-01-13T21:59:32.655237Z","shell.execute_reply.started":"2022-01-13T21:59:32.638807Z","shell.execute_reply":"2022-01-13T21:59:32.654247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Linear Regression","metadata":{}},{"cell_type":"code","source":"model = LinearRegression() \npipe = make_pipeline(column_trans, model) \npipe.fit(X_train, y_train)\npreds_lm = pipe.predict(X_test)\npreds_lm = np.exp(preds_lm) #inverse logrithm on predicted SalePrice.\nprint(pipe.score(X_test, y_test))\npd.DataFrame(preds_lm.reshape(len(preds_lm),1)).describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:32.656344Z","iopub.execute_input":"2022-01-13T21:59:32.656549Z","iopub.status.idle":"2022-01-13T21:59:32.718263Z","shell.execute_reply.started":"2022-01-13T21:59:32.656521Z","shell.execute_reply":"2022-01-13T21:59:32.717496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Lasso","metadata":{}},{"cell_type":"code","source":"model = Lasso() \npipe = make_pipeline(column_trans, sc, model) \nparams = {\"lasso__alpha\": np.arange(0.001,0.002,0.0001)} #print(pipe.get_params().keys())\ncv_lasso = GridSearchCV(pipe, param_grid=params)\ncv_lasso.fit(X_train, y_train)\npreds_lasso = cv_lasso.predict(X_test)\npreds_lasso = np.exp(preds_lasso) #inverse logrithm on predicted SalePrice.\nlasso_r2, lasso_mae, lasso_mse, lasso_rmse = evaluation(y_test, preds_lasso) \nprint(cv_lasso.best_params_, cv_lasso.best_score_)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:32.719267Z","iopub.execute_input":"2022-01-13T21:59:32.719701Z","iopub.status.idle":"2022-01-13T21:59:34.083042Z","shell.execute_reply.started":"2022-01-13T21:59:32.719672Z","shell.execute_reply":"2022-01-13T21:59:34.082535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Ridge","metadata":{}},{"cell_type":"code","source":"model = Ridge() \npipe = make_pipeline(column_trans, sc, model) \nparams = {\"ridge__alpha\": np.arange(1,100,10)} #print(pipe.get_params().keys())\ncv_ridge = GridSearchCV(pipe, param_grid=params)\ncv_ridge.fit(X_train, y_train)\npreds_ridge = cv_ridge.predict(X_test)\npreds_ridge = np.exp(preds_ridge) #inverse logrithm on predicted SalePrice.\nprint(cv_ridge.best_params_, cv_ridge.best_score_)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:34.084044Z","iopub.execute_input":"2022-01-13T21:59:34.084699Z","iopub.status.idle":"2022-01-13T21:59:35.160643Z","shell.execute_reply.started":"2022-01-13T21:59:34.084669Z","shell.execute_reply":"2022-01-13T21:59:35.160152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ElasticNet","metadata":{}},{"cell_type":"code","source":"model = ElasticNet()\npipe = make_pipeline(column_trans, sc, model) \n#print(pipe.get_params().keys())\nparams = {\"elasticnet__alpha\": np.arange(0,0.2,0.01)} \ncv_elastic = GridSearchCV(pipe, param_grid=params)\ncv_elastic.fit(X_train, y_train)\npreds_elastic = cv_elastic.predict(X_test)\npreds_elastic = np.exp(preds_elastic) #inverse logrithm on predicted SalePrice.\nprint(cv_elastic.best_params_, cv_elastic.best_score_)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:35.161642Z","iopub.execute_input":"2022-01-13T21:59:35.162312Z","iopub.status.idle":"2022-01-13T21:59:37.931208Z","shell.execute_reply.started":"2022-01-13T21:59:35.16228Z","shell.execute_reply":"2022-01-13T21:59:37.930726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### SVM","metadata":{}},{"cell_type":"code","source":"model = SVR()\npipe = make_pipeline(column_trans, sc, model) \n#print(pipe.get_params().keys())\nparams = {\"svr__C\": np.arange(1,1.6,0.1)} \ncv_svr = GridSearchCV(pipe, param_grid=params)\ncv_svr.fit(X_train, y_train)\npreds_svr = cv_svr.predict(X_test)\npreds_svr = np.exp(preds_svr) #inverse logrithm on predicted SalePrice.\nprint(cv_svr.best_params_, cv_svr.best_score_)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:37.932174Z","iopub.execute_input":"2022-01-13T21:59:37.932741Z","iopub.status.idle":"2022-01-13T21:59:41.849542Z","shell.execute_reply.started":"2022-01-13T21:59:37.932706Z","shell.execute_reply":"2022-01-13T21:59:41.848813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Stochstic Gradient Descent","metadata":{}},{"cell_type":"code","source":"model = SGDRegressor()\npipe = make_pipeline(column_trans, sc, model) \n#print(pipe.get_params().keys())\nparams = {\"sgdregressor__tol\": [1e-3], #stopping criteria\n          \"sgdregressor__n_iter_no_change\": [200],\n          \"sgdregressor__eta0\": [0.006, 0.009, 0.01, 0.03], \n          \"sgdregressor__alpha\": np.arange(0.005,0.1,0.005),\n          \"sgdregressor__random_state\": [42]\n         }\ncv_sgd = GridSearchCV(pipe, param_grid=params)\ncv_sgd.fit(X_train, y_train)\npreds_sgd = cv_sgd.predict(X_test)\npreds_sgd = np.exp(preds_sgd) #inverse logrithm on predicted SalePrice.\nprint(cv_sgd.best_params_, cv_sgd.best_score_)\n\n\npd.DataFrame(preds_sgd.reshape(len(preds_sgd),1)).describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T21:59:41.852011Z","iopub.execute_input":"2022-01-13T21:59:41.852281Z","iopub.status.idle":"2022-01-13T22:01:37.750564Z","shell.execute_reply.started":"2022-01-13T21:59:41.852258Z","shell.execute_reply":"2022-01-13T22:01:37.750031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Randome Forest Resgression","metadata":{}},{"cell_type":"code","source":"model = RandomForestRegressor()\npipe = make_pipeline(column_trans, model) \n#print(pipe.get_params().keys())\nparams = {\"randomforestregressor__n_estimators\": np.arange(50,150,50)} \ncv_rf = GridSearchCV(pipe, param_grid=params)\ncv_rf.fit(X_train, y_train)\npreds_rf = cv_rf.predict(X_test)\npreds_rf = np.exp(preds_rf) #inverse logrithm on predicted SalePrice.\npd.DataFrame(preds_rf.reshape(len(preds_rf),1)).describe()\nprint(cv_rf.best_params_, cv_rf.best_score_)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T22:01:37.753883Z","iopub.execute_input":"2022-01-13T22:01:37.755556Z","iopub.status.idle":"2022-01-13T22:01:46.404969Z","shell.execute_reply.started":"2022-01-13T22:01:37.755518Z","shell.execute_reply":"2022-01-13T22:01:46.404406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### XGBoost","metadata":{}},{"cell_type":"code","source":"model = XGBRegressor()\npipe = make_pipeline(column_trans, model) \n#print(pipe.get_params().keys())\nparams = {\"xgbregressor__n_estimators\": np.arange(100,200,50), \"xgbregressor__learning_rate\": np.arange(0.08,0.12,0.01)} \ncv_xg = GridSearchCV(pipe, param_grid=params)\ncv_xg.fit(X_train, y_train)\npreds_xg = cv_xg.predict(X_test)\npreds_xg = np.exp(preds_xg) #inverse logrithm on predicted SalePrice.\npd.DataFrame(preds_xg.reshape(len(preds_xg),1)).describe()\nprint(cv_xg.best_params_, cv_xg.best_score_)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T22:01:46.406013Z","iopub.execute_input":"2022-01-13T22:01:46.406381Z","iopub.status.idle":"2022-01-13T22:02:12.947214Z","shell.execute_reply.started":"2022-01-13T22:01:46.40635Z","shell.execute_reply":"2022-01-13T22:02:12.946635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model comparison","metadata":{}},{"cell_type":"code","source":"lm_r2, lm_mae, lm_mse, lm_rmse = evaluation(np.exp(y_test), preds_lm) \nresult[\"Liner\"] = [lm_r2, lm_mae, lm_mse, lm_rmse]\n\nlasso_r2, lasso_mae, lasso_mse, lasso_rmse = evaluation(np.exp(y_test), preds_lasso) \nresult[\"Lasso\"] = [lasso_r2,lasso_mae, lasso_mse, lasso_rmse]\n\nridge_r2, ridge_mae, ridge_mse, ridge_rmse = evaluation(np.exp(y_test), preds_ridge) \nresult[\"Ridge\"] = [ridge_r2, ridge_mae, ridge_mse, ridge_rmse]\n\nelastic_r2, elastic_mae, elastic_mse, elastic_rmse = evaluation(np.exp(y_test), preds_elastic) \nresult[\"Elastic\"] = [elastic_r2, elastic_mae, elastic_mse, elastic_rmse]\n\nsvr_r2, svr_mae, svr_mse, svr_rmse = evaluation(np.exp(y_test), preds_svr) \nresult[\"SVR\"] = [svr_r2, svr_mae, svr_mse, svr_rmse]\n\nrf_r2, rf_mae, rf_mse, rf_rmse = evaluation(np.exp(y_test), preds_rf) \nresult[\"RandomForest\"] = [rf_r2, rf_mae, rf_mse, rf_rmse]\n\nxg_r2, xg_mae, xg_mse, xg_rmse = evaluation(np.exp(y_test), preds_rf) \nresult[\"XGBoost\"] = [xg_r2, xg_mae, xg_mse, xg_rmse]\n\n\nsgd_r2, sgd_mae, sgd_mse, sgd_rmse = evaluation(np.exp(y_test), preds_sgd) \nresult[\"Stochastic\"] = [sgd_r2, sgd_mae, sgd_mse, sgd_rmse]\n\n\n","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-01-13T22:02:12.949891Z","iopub.execute_input":"2022-01-13T22:02:12.95126Z","iopub.status.idle":"2022-01-13T22:02:12.97184Z","shell.execute_reply.started":"2022-01-13T22:02:12.951229Z","shell.execute_reply":"2022-01-13T22:02:12.971061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = pd.DataFrame.from_dict(result, orient='index', columns=[\"R2 Score\",\"MAE\",\"MSE\",\"RMSE\"])\nresult.sort_values(by=\"RMSE\")\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T22:02:12.972789Z","iopub.execute_input":"2022-01-13T22:02:12.973008Z","iopub.status.idle":"2022-01-13T22:02:13.000021Z","shell.execute_reply.started":"2022-01-13T22:02:12.972957Z","shell.execute_reply":"2022-01-13T22:02:12.99892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.barplot(result.index,result.RMSE)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T22:12:06.073609Z","iopub.execute_input":"2022-01-13T22:12:06.07388Z","iopub.status.idle":"2022-01-13T22:12:06.266396Z","shell.execute_reply.started":"2022-01-13T22:12:06.07385Z","shell.execute_reply":"2022-01-13T22:12:06.265651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n### Output","metadata":{}},{"cell_type":"code","source":"test_X = testset[feature_selection]\n# model = RandomForestRegressor(n_estimators=100)\n# pipe = make_pipeline(column_trans, model) \n# pipe.fit(X_train, y_train)\n# predictions = pipe.predict(test_X)\n# pd.DataFrame(predictions.reshape(len(predictions),1)).describe()\n\n\n# model = XGBRegressor(n_estimators=150, learning_rate=0.08)\n# pipe = make_pipeline(column_trans, model) \n# pipe.fit(X_train, y_train)\n# predictions = pipe.predict(test_X)\n# predictions = np.exp(predictions) #inverse logrithm on predicted SalePrice.\n\nmodel = Lasso(alpha=0.0014) \npipe = make_pipeline(column_trans, sc, model) \npipe.fit(X_train, y_train)\npredictions = pipe.predict(test_X)\npredictions = np.exp(predictions) #inverse logrithm on predicted SalePrice.\npd.DataFrame(predictions.reshape(len(predictions),1)).describe()\n\n\n# model = Ridge(alpha=21) \n# pipe = make_pipeline(column_trans, sc, model) \n# pipe.fit(X_train, y_train)\n# predictions = pipe.predict(test_X)\n# predictions = np.exp(predictions) #inverse logrithm on predicted SalePrice.\n# pd.DataFrame(predictions.reshape(len(predictions),1)).describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T22:02:13.18836Z","iopub.execute_input":"2022-01-13T22:02:13.188623Z","iopub.status.idle":"2022-01-13T22:02:13.239221Z","shell.execute_reply.started":"2022-01-13T22:02:13.18859Z","shell.execute_reply":"2022-01-13T22:02:13.238644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_dic = {\"Id\": testset[\"Id\"], \"SalePrice\": predictions}\noutput_df = pd.DataFrame(output_dic)\noutput_df.to_csv(\"submission.csv\", index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2022-01-13T22:02:13.240128Z","iopub.execute_input":"2022-01-13T22:02:13.240379Z","iopub.status.idle":"2022-01-13T22:02:13.252651Z","shell.execute_reply.started":"2022-01-13T22:02:13.240348Z","shell.execute_reply":"2022-01-13T22:02:13.252006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Experiment: Learning Curve","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import learning_curve\n# Use learning curve to get training and test scores along with train sizes\ntrain_sizes, train_scores, test_scores = learning_curve(\n    estimator=pipe, \n    X=X_train, \n    y=y_train,\n    train_sizes=np.linspace(0.1, 1.0, 10),\n    n_jobs=1,\n    cv=5\n)\n\n#\n# Calculate training and test mean and std\n#\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n#\n# Plot the learning curve\n#\nplt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='Training Accuracy')\nplt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\nplt.plot(train_sizes, test_mean, color='green', marker='+', markersize=5, linestyle='--', label='Validation Accuracy')\nplt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\nplt.title('Learning Curve')\nplt.xlabel('Training Data Size')\nplt.ylabel('Accuracy')\nplt.grid()\nplt.legend(loc='lower right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T22:02:13.253735Z","iopub.execute_input":"2022-01-13T22:02:13.253964Z","iopub.status.idle":"2022-01-13T22:02:14.681963Z","shell.execute_reply.started":"2022-01-13T22:02:13.253933Z","shell.execute_reply":"2022-01-13T22:02:14.681446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"############# Fix Skewness ##################\n# #trainingset.loc[:,highly_skewed_cols].describe()\n# has_zero_cols = ['MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', '2ndFlrSF', 'LowQualFinSF', \n# 'KitchenAbvGr', 'BsmtHalfBath',  'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', \n#  'MiscVal']\n# non_zero_cols = ['MSSubClass', 'LotArea', '1stFlrSF', 'GrLivArea','GarageYrBlt']\n\n# for col in non_zero_cols:\n#     trainingset[col] = np.log(trainingset[col])\n\n# # Handle cols with zero values\n# for col in has_zero_cols:\n#     trainingset[col + '_flag'] = trainingset[col]>0\n#     trainingset.loc[trainingset[col + '_flag']==1,col] = np.log(trainingset[col])\n\n# # Drop columns with _flag where I created to perform log transformation on the columns contain zero values.\n# trainingset = trainingset[trainingset.columns.drop(list(trainingset.filter(regex='_flag')))]\n\n\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-13T22:02:14.682869Z","iopub.execute_input":"2022-01-13T22:02:14.683149Z","iopub.status.idle":"2022-01-13T22:02:14.687478Z","shell.execute_reply.started":"2022-01-13T22:02:14.683118Z","shell.execute_reply":"2022-01-13T22:02:14.686862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## References","metadata":{}},{"cell_type":"markdown","source":"Feature selection lecture: [UW Machine Learning: Regression](https://www.coursera.org/learn/ml-regression?specialization=machine-learning)\n    \nI've read some great notebooks in Kaggle. Regarding imputing missing values, I refered the notebook below.\n* [Handling Missing Values](https://www.kaggle.com/dansbecker/handling-missing-values ) by DANB.\n* [Using Categorical Data with One Hot Encoding](https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding) by DANB.\n* [Stacked Regressions : Top 4% on LeaderBoard](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard) by Serigne.\n\nThe note below has a stragetic approach about EDA. \n* [Comprehensive data exploration with Python](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python) by PEDRO MARCELINO\n\nTransforming skewed data (why and how)\n* [Skewed Data: A problem to your statistical model](https://towardsdatascience.com/skewed-data-a-problem-to-your-statistical-model-9a6b5bb74e37)\n* [Transforming Skewed Data for Machine Learning](https://odsc.medium.com/transforming-skewed-data-for-machine-learning-90e6cc364b0)\n* https://codeburst.io/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis-388fef94eeaa\n\n","metadata":{}}]}