{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <u>House Prices: Simple and easy 3 steps to submit</u>\nThis notebook will guide you through three simple and effective steps to submit."},{"metadata":{},"cell_type":"markdown","source":"* [1. Preparations](#1)\n    * [1.1 Import libraries](#1.1)\n    * [1.2 Load dataset](#1.2)\n    * [1.3 Check data](#1.3)\n    * [1.4 Combine train and test](#1.4)\n* [2. Feature Engineering](#2)\n    * [2.1 Transform numeric into logarithms](#2.1)\n    * [2.2 Transform categorical into one-hot vector](#2.2)\n* [3. Prediction and submission](#3)\n    * [3.1 Format data](#3.1)\n    * [3.2 Prediction](#3.2)\n    * [3.3 Create submission](#3.3)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a><h1 style='background:slateblue; border:.; color:white'><center>1. Preparations</center></h1>"},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Import libraries<a id=\"1.1\"></a>\n**Import all required libraries**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import skew\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Load dataset<a id=\"1.2\"></a>\n**Load each data as a Pandas DataFrame**"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv')\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Check data<a id=\"1.3\"></a>\n**check data shape, count and dtype of each column**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('=========== train infomation ===========')\ntrain.info()\nprint('\\n\\n=========== test infomation ===========')\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.4 Combine train and test<a id=\"1.4\"></a>\n**Combine train and test so that you can do each future operation once**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([train, test])\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a><h1 style='background:slateblue; border:.; color:white'><center>2. Feature Engineering</center></h1>"},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Transform numeric into logarithms<a id=\"2.1\"></a>\n**transform all numeric columns of the data frame into logarithms to reduce skewness**<br><br>\nApproximating to a normal distribution often improves the accuracy of machine learning.<br>Logarithmic conversion reduces the range when the feature scale is large and expands it when the feature scale is small.<br>This often allows you to get closer to a mountainous distribution as if you were crushing a long-tailed distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"# current numeric data\nnumerics = data.loc[:,data.dtypes != 'object'].drop('Id', axis=1)\nnumerics.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# numeric data after conversion to logarithm\nlog_numerics = np.log1p(numerics)\nlog_numerics.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compare skewnesses before with after of logarithmization\nskewness = pd.concat([numerics.apply(lambda x: skew(x.dropna())),\n                      log_numerics.apply(lambda x: skew(x.dropna()))],\n                     axis=1).rename(columns={0:'original', 1:'logarithmization'}).sort_values('original')\nskewness.plot.barh(figsize=(12,10), title='Comparison of skewness of original and logarithmized', width=0.8);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The skewnesses of many features are reduced by logarithmic conversion. Some skewnesses have increased, but overall it has decreased."},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Transform categorical into one-hot vector<a id=\"2.2\"></a>\n**transform all categorical columns of the data frame into one-hot vector**<br><br>\nSince GBDT treats features as numerical data, it is necessary to convert categorical data to numerical values. Label encoding is fine, but one-hot encoding often has better performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = data.loc[:,data.dtypes == 'object'].columns\ndata.loc[:,cat_cols].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# categorical data after conversion to one-hot vector\n# cat_data = pd.get_dummies(data.loc[:, cat_cols], drop_first=True, dummy_na=True)\ncat_data = pd.get_dummies(data.loc[:, cat_cols], drop_first=True)\ncat_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n<h1 style='background:slateblue; border:.; color:white'><center>3. Prediction and submission</center></h1>"},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Format data<a id=\"3.1\"></a>\n**Format data for training**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge categorical and numeric columns\noptimized_data = pd.concat([data['Id'], cat_data, log_numerics], axis=1)\noptimized_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into X_train, y_train and test\ntrain = optimized_data[:train.shape[0]]\ntest = optimized_data[train.shape[0]:].drop(['Id', 'SalePrice'], axis=1)\nX_train = train.drop(['Id', 'SalePrice'], axis=1)\ny_train = train['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Prediction<a id=\"3.2\"></a>\n**fit and predict using lightGBM**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train\nlgb_train = lgb.Dataset(X_train, y_train)\nparams = {\n        'objective' : 'regression',\n        'metric' : {'rmse'}\n}\ngbm = lgb.train(params, lgb_train)\n# predict\npred = gbm.predict(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.3 Create submission<a id=\"3.3\"></a>\n**convert prediction into exponent and export CSV file**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert logarithms into exponent\npred = np.expm1(pred)\n# create submission file\nresults = pd.Series(pred, name='SalePrice')\nsubmission = pd.concat([submission['Id'], results], axis=1)\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This notebook has a score of 0.13086 and is in the top 30%.<br>Based on this notebook, Feature engineering, Hyper-parameter tuning and ensemble will give you a better score."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}