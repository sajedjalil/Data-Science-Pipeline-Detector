{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Author: David Hurley\n\n### Blog: [Link](https://medium.com/@davidhurley_48402)\n\n### GitHub: [Link](https://github.com/david-hurley)\n\n# Notebook Objective\nThis notebook uses linear and tree based models to predict the sale price of homes in Ames, Iowa. The predicted sale prices are submitted to the Kaggle competition *House Prices: Advanced Regression Techniques*. \n\n\n## Overview\nThere are a lot of great notebooks associated with this competition, a few that I found very helpful are:\n* [Regularized Linear Models](https://www.kaggle.com/apapiu/regularized-linear-models) (Alexandru Papiu)\n* [Stacked Regressions : Top 4% on LeaderBoard](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard) (Serigne)\n\nThe biggest thing I have found missing in many of these notebooks is a streamlined pipeline for testing the models. This notebook focuses on doing basic EDA and using a preprocessing pipeline to process data, fit, and evaluate several models. With basic preprocessing and assumptions we can quickly obtain Root Mean Square Error (RMSE) scores that rank in the top 20%.\n\n### Why using pipelines to do preprocessing is important?\nPipelines are great because they package your data processing and model fitting into a single package. This makes it easy to manage complex models and do feature engineering. Pipelines are even better becuase they eliminate aspects of data leakage. \n\nData leakage is a major problem when developing a machine learning model. Typically, it causes a model to have seemingly high performance in the training and even validation stages. However, once the model is deployed in production (i.e. predicting unseen test data) it is likely to perform much worse than anticipated. A common cause of data leakage is Train-Test split contamination. This can occur, for example, when a preprocessing step is fit to both train and validation datasets (i.e. fitting a SimpleImpute before calling train-test split). With pipelines this can easily be avoided as perprocessing is applied independtly on the train and validation data. \n\n## Outline\nThe following sections are included in this notebook:\n\n### A. [Load and Parse Data](#section-one)\n\n### B. [Exploratory Data Analysis (EDA)](#section-two)\n   1. [Missing Data](#section-two-a)    \n   2. [Distribution of the Target Variable](#section-two-b)    \n   3. [Distribution of the Numeric Feature Variable](#section-two-c)\n   4. [Outliers](#section-two-d)\n   5. [Categorical Feature Cardinality](#section-two-e)\n    \n### C. [Preprocessing](#section-three)\n   1. [Initial Preprocessing](#section-three-a)\n   2. [Building a Preprocessing Pipeline](#section-three-b)\n       * [Define custom transformers](#section-three-b1)\n       * [Define helper functions](#section-three-b2)\n       * [Define Training Data](#section-three-b3)\n       * [Define datatypes and encoding](#section-three-b4)\n       * [Build the preprocessing pipeline](#section-three-b5)\n        \n### D. [Fit and Evaluate the Model](#section-four)\n   1. [Cross-Validation](#section-four-a)\n       * LassoCV\n       * RidgeCV        \n       * ElasticNetCV        \n       * Random Forest        \n       * XGBRegressor        \n       * LGBMRegressor        \n   2. [Model Stacking](#section-four-c)\n    \n### E. [Predict Test Dataset and Submit](#section-five)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# A. Load and Parse Data","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import (train_test_split, cross_val_score)\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import (OneHotEncoder, FunctionTransformer, StandardScaler, OrdinalEncoder, LabelEncoder)\nfrom sklearn.pipeline import (Pipeline, FeatureUnion)\nfrom IPython.display import display\nfrom sklearn.linear_model import (ElasticNetCV, LassoCV, RidgeCV, LinearRegression)\nfrom sklearn.ensemble import (RandomForestRegressor, StackingRegressor)\nfrom xgboost import XGBRegressor\nfrom scipy.stats import skew\nfrom lightgbm import LGBMRegressor\nfrom sklearn.base import TransformerMixin\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (12.0, 6.0) #  set defualt figure size","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#  Load train and test data\n\ndf_train = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ndf_test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Remove any duplicate target values in the training dataset\n\nif len(set(df_train.Id)) == len(df_train):\n    print('There are no duplicates of the target variable')\nelse:\n    df_train.drop_duplicates(subset=['Id'], inplace=True)\n    \n#  Create new variable test_id and remove Id from train and test data as it says nothing about sale price\n\ntest_id = df_test.Id\ndf_train.drop(columns=['Id'], inplace=True)\ndf_test.drop(columns=['Id'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n# B. Exploratory Data Analysis (EDA)\nThe purpose of EDA is to get familiar with our data, but not so familiar that we begin making assumptions about the model fit! In Kaggle competitions it can be tempting to overfit the training data in hopes of a lower test score, but this often doesn't bode well for real world applications. Typically, it's best to let the data speak for itself and allow the model the flexibility to find correlations between the target and features. Afterall, the models in todays age are very robust. \n\n### Do Preprocessing Later!\nThis is really more of a personal opinion. I find it hard to keep track of data processing done in cells throughout an EDA section. Typically, I prefer to do all the preprocessing in a single code block or even better in a pipeline. This way I know the preprocessing is being applied the same way to the train, validation, and test datasets. I use EDA as a way to identify the preprocessing steps that need to take place and potential feature engineering opportunities. \n\nRemember, it's best to do preprocessing in a pipeline!!!\n\nIn this section I will explore the following common issues:\n1. Missing Data\n2. Distribution of the Target Variable\n3. Distribution of the Feature Variables\n4. Outliers\n5. Categorical Feature Cardinality","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-two-a\"></a>\n### 1. Missing Data\n\n##### A few things to note here:\n* Most of the missing data is associated with categorical features (16 categorical features with missing data and only 3 numeric)\n* A few features have more than 50% missing data (i.e. Pool Quality)\n* Some features have almost no valid data (i.e. Pool Quality)\n\n### IMPORTANT\nUpon further investigation of the *data_description.txt* it seems that 'NA' for cateogrical data represents 'No' (i.e. no garage) and is not an indicator of missing data. When we read in the data with Pandas it automatically flags 'NA' and converts to 'NaN'. We will need to address this later on in the preprocessing pipeline by replacing missing values in categroical data with 'None' or something equivalent. Afterall, this is useful data! ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Calculate percent missing data in train dataset\nmissing = df_train.isnull().sum()\nmissing = missing[missing > 0] / len(df_train) * 100\n\n#  Sort data\nmissing.sort_values(inplace=True)\n\n#  Create bar plot\nmissing.plot.bar(zorder=2)\nplt.title('Training Dataset - Missing Values')\nplt.ylabel('Percent Missing (%)')\nplt.grid(zorder=0)\n\n#  Print number of categorical and numeric features with missing data\nnum_cols = df_train.select_dtypes(exclude='object').columns\ncat_cols = df_train.select_dtypes('object').columns\n\nnum_missing = len(df_train[num_cols].columns[df_train[num_cols].isnull().any()])\ncat_missing = len(df_train[cat_cols].columns[df_train[cat_cols].isnull().any()])\n\nprint(f\"Number of numerical features with missing data: {num_missing}\")\nprint(f\"Number of categorical features with missing data: {cat_missing}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-two-b\"></a>\n### 2. Distribution of the Target Variable\n\n##### A few things to note here:\n* Sale Price is positively skewed\n* Taking the Log of sale price we can normalize the distribution\n\nWe are going to use linear and tree based models to predict sale price. Tree based models don't assume normally distributed variables but linear models do. We see that Sale Price is not normally distributed, it's positively skewed. We should use the Log of sale price to obtain a more normal distribution. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Plot distribution of sale price in training dataset\nf, axs = plt.subplots(1,2)\naxs[0].hist(df_train.SalePrice, bins=36, zorder=2)\naxs[0].set_title('Positively Skewed')\naxs[0].set_xlabel('Sale Price ($)')\naxs[0].set_ylabel('Frequency')\naxs[0].set_xticks(ticks=[1e5, 3e5, 5e5, 7e5])\naxs[0].grid(zorder=0)\n\n#  Plot distribution of log of sale price in training dataset\naxs[1].hist(np.log(df_train.SalePrice), bins=36, zorder=2)\naxs[1].set_title('Normally Distributed')\naxs[1].set_xlabel('Log of Sale Price ($)')\naxs[1].grid(zorder=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-two-c\"></a>\n### 3. Distribution of the Numeric Feature Variables\n\n##### A few things to note here:\n* Some of the numeric features are continous (i.e. square footage features) while some are discrete (i.e. # of baths/rooms)\n* None of the features display much of a normal distribution\n* The discrete features represent metrics like *# of rooms* and *year sold*.\n\nAs with the target we should check that the numeric features are roughly normally distributed. We see that all of the features have some amount of skew and we will want to reduce this skew before fitting our models. Additionally, we see that the range of values is all over. We can make the ranges more similar by using a standard scalar to remove the mean and scaling to unit variance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Only numeric columns and features (i.e. no sale price)\nnum_cols = list(df_train.select_dtypes(exclude='object').columns)\nnum_cols.remove('SalePrice')\n\n#  Plot all feature data\ndf_train[num_cols].hist(bins=36, figsize=(12,12))\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<a id=\"section-two-d\"></a>\n### 4. Outliers\n\n##### A few things to note here:\n* There are two distinct outliers that have huge square footage but cheap prices\n* If we leave these then are linear models won't work very well\n\n\nTypically it's not great practice to remove outliers unless you have domain knowledge for whey they are being removed. We can picture the relation between Sale Price and Above Ground Square Footage pretty easily (larger house --> larger price). ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Find outliers\n\noutliers = df_train.loc[(df_train.SalePrice < 200000) & (df_train.GrLivArea > 4000)]\n\n#  Plot Sale Price against Above Ground Square Footage\n\nplt.figure(figsize=(8,8))\n\nplt.plot(df_train.GrLivArea, df_train.SalePrice,'b.')\nplt.plot(outliers.GrLivArea, outliers.SalePrice, 'ro', markerfacecolor='none', markersize=10, label='outliers')\nplt.xlabel('Above Ground Square Footage (GrLivArea)')\nplt.ylabel('Sale Price ($)')\nplt.legend()\nplt.grid()\n\n#  Drop outliers from training data\ndf_train.drop(outliers.index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-two-e\"></a>\n### 5. Categorical Feature Cardinality\n\n##### A few things to note here:\n* Most categorical features have fewer than 10 unique values\n* A few categorical features have greater than 15 unique values\n* In general we should be fine with One Hot Encoding without overloading our matrix dimensions\n\nWhen we build our preprocessing pipeline we will need to encode categorical features with OneHot Encoding.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Plot cardinality of categorical columns\n\ncat_cols = df_train.select_dtypes('object').columns\n\ndf_train[cat_cols].nunique().plot.bar(zorder=2)\nplt.ylabel('Count')\nplt.title('Cardinality of Categorical Data')\nplt.grid(zorder=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n# C. Preprocessing\n\nNow that we have done some initial EDA we should preprocess the data based on the findings. I prefer to group preprocessing steps together as I find it's easier to \"stack trace\" any problems. \n\nIn this section I will do the following:\n* Apply initial preprocessing\n* Build a preprocessing pipeline\n\nWe are not going to be very advanced and will address the following issues that we identified in EDA:\n1. Remove outliers\n2. Transform Sale Price to Log of Sale Price\n3. Replace missing values in numeric features with the *mean*\n4. Add a feature for Total Square Footage\n5. Transform numeric features to Log of numeric features\n6. Replace missing values in categorical features with 'None'\n7. OneHot Encode categorical features\n8. Standarize all features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-three-a\"></a>\n### 1. Initial Preprocessing\n\nLets remove the identified outliers from the training dataset and take the log of the target variable to obtain a more normal distribution. Notice that these steps are specific to the training dataset and won't be applied to the test dataset. Anything that is done to both the training and test datasets will be done in our preprocessing pipeline. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Remove outliers from training dataset\n\noutliers = df_train.loc[(df_train.SalePrice < 200000) & (df_train.GrLivArea > 4000)]\ndf_train.drop(outliers.index, inplace=True)\n\n#  Use log of sale price in training dataset\n\ndf_train.SalePrice = np.log(df_train.SalePrice)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-three-b\"></a>\n### 2. Building a Preprocessing Pipeline\n\nPipelines are a way to prevent train-test split contamination, keep models organized, and easily assess the performance of hyperparameter tuning and feature engineering. Additionally, if you are going to deploy models in production, such as on a cloud based server, this keeps everything organized and traceable. \n\n**In this section I will do the following:**\n1. Define custom transformer classes to be included in the pipeline\n2. Define custom helper functions to compute Root Mean Square Error (RMSE)\n3. Split the training and test data\n4. Define numeric and categorical features\n3. Build the pipeline","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-three-b1\"></a>\n### Define Custom Transformers\n\nScikit-learn has great built-in classes for data preprocessing, like imputing missing values and normalizing features. However, sometimes we want to add preprocessing steps that don't exist or modify an existing transformers behaviour (i.e. have the output of SimpleImputer be a dataframe). We can accomplish this by defining our own custom transformers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Custom transformer to extract specific features\n    \nclass ColumnExtractor(TransformerMixin):\n    \n    def __init__(self, cols):\n        self.cols = cols\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        Xcols = X[self.cols]\n        return Xcols\n    \n#  Custom transformer that inherits from SimpleImputer class and returns a dataframe\n\nclass DFSimpleImputer(SimpleImputer):\n    \n    def transform(self, X):\n        Xim = super(DFSimpleImputer, self).transform(X)\n        Xim = pd.DataFrame(Xim, index=X.index, columns=X.columns)\n        return Xim\n    \n#  Custom transformer that inherits from OneHotEncoder and return a dataframe\n    \nclass DFOneHotEncoder(OneHotEncoder):\n    \n    def transform(self, X):\n        Xoh = super(DFOneHotEncoder, self).transform(X)\n        Xoh = pd.DataFrame(Xoh, X.index)\n        return Xoh\n\n#  Custom transformer that creates a new feature TotalSquareFootage\n\nclass TotalSF(TransformerMixin):\n\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        Xadd = X.copy()\n        Xadd['TotalSF'] = Xadd.GrLivArea + Xadd.TotalBsmtSF + Xadd.GarageArea\n        return Xadd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-three-b2\"></a>\n### Define Helper Functions\n\nLets create a helper function that will use Cross-Validaton to evauluate the average Root Mean Square Error (RMSE) over X folds. This is useful as it gives us a more robust score of our model predictions on the training dataset. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Helper function that computes the average RMSE over X folds using Cross-Validation. Cross-Validation function will fit and score data.\n\ndef get_RMSE(pipeline, X, y, folds):\n\n    MSE_scores = -1 * cross_val_score(pipeline, X, y, cv=folds, scoring='neg_mean_squared_error')\n    RMSE_scores = np.sqrt(MSE_scores)\n    \n    return RMSE_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<a id=\"section-three-b3\"></a>\n### Define Training and Testing Data\n\nDefine training data to run through cross-validation and hyperparameter tuning and test data to sumbit prediction to Kaggle","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Training and testing datasets\n\nX_train = df_train.drop(columns=['SalePrice'])\ny_train = df_train.SalePrice\nX_test = df_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-three-b4\"></a>\n### Define Data Types and Encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Numerical features\n\nnumeric_columns = X_train.select_dtypes(exclude='object').columns\n\n#  Categorical features\n\ncategorical_columns = X_train.select_dtypes('object').columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-three-b5\"></a>\n### Build the Preprocessing Pipeline!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Define the preprocessing pipeline\n\npipeline = Pipeline([\n    ('features', FeatureUnion([\n        ('numeric', Pipeline([\n            ('extract', ColumnExtractor(numeric_columns)),\n            ('imputer', DFSimpleImputer()),\n            ('totalSF', TotalSF()), #  create a new feature total square footage\n            ('logger', FunctionTransformer(np.log1p)) #  take the log of all numeric features to create a normal distribution\n        ])),\n        ('categorical', Pipeline([\n            ('extract', ColumnExtractor(categorical_columns)),\n            ('imputer', DFSimpleImputer(strategy='constant', fill_value='None')), #  we determined earlier that 'NA' for categorical really means 'None'\n            ('encode', DFOneHotEncoder(handle_unknown='ignore', sparse=False))\n        ])),\n    ])),\n    ('scale', StandardScaler())  #  scale all features\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n# D. Fit and Evauluate the Models\n\nNow that we have defined a preprocessing pipeline we can fit a range of models and evaluate using our scoring metric. I am going to use Cross-Validation to get a more robust score and compare each model.\n\n**The following models are going to be fit using the default hyperparameters:**\n* LassoCV (Cross-Validation is incorporated in the algorithim so we will have nested cross-validation)\n* RidgeCV (same as LassoCV)\n* ElasticNetCV (same as LassoCV)\n* Random Forest\n* XGBoost\n* LGBMRegressor\n\n**Amazingly, with very little data preperation and using basic linear models we can quickly get a training score that would put us in the top 20% of the leaderboard.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four-a\"></a>\n### 1. Cross Validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Define the models to fit and evaluate.\n\nmodels = [\n    LassoCV(),\n    RidgeCV(),\n    ElasticNetCV(),\n    RandomForestRegressor(),\n    XGBRegressor(), \n    LGBMRegressor()\n]\n\n#  Preprocess the data for each model, fit the model, and evaluate\n\nprint('RMSE Cross-Validation Training Scores \\n')\n\nRMSE = []\nmodel_names = []\nfor i, model in enumerate(models):\n    \n    full_pipeline = Pipeline(steps=[('pipeline', pipeline),\n                                    ('model', model)])\n\n    #  Fit training data and score\n    \n    RMSE.append(get_RMSE(full_pipeline, X_train, y_train, 5))\n    \n    #  Print the scores\n    \n    model_names.append(str(model).split('(')[0])\n    print('{} Training Score: {}'.format(model_names[i], round(np.mean(RMSE[i]),4)))\n    \n#  Create a boxplot of the scores\n\nplt.figure(figsize=(18,7))\nplt.boxplot(RMSE, labels=model_names, showmeans=True)\nplt.xlabel('Models', fontsize=16)\nplt.ylabel('Root Mean Square Error (RMSE)', fontsize=16)\nplt.title('Cross-Validation Scores', fontsize=18)\nplt.tick_params(axis = 'both', which = 'major', labelsize = 14)\nplt.tick_params(axis = 'both', which = 'minor', labelsize = 14)\n\nx = '' #  Hack to stop figure vomit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<a id=\"section-four-c\"></a>\n### 2. Model Stacking\n\nMany times a model may perform well on certain aspects of a dataset and worse on others. This can make it challening to choose one model over another. Instead we can create a stacked ensemble of models that takes advantage of each models \"strengths\". \n\nNOTE: Continue to receive convergence error when stacking models. Have tested with multiple combinations of models (even using just one) and have tried adjusting the tolerance and max iterations. I imagine there is some level of overfitting happening. For a real world problem this would not be ideal and should not be ignored but for achieving the \"best\" score on Kaggle this stacked model works. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Define models that we want to stack\n\nmodels = [\n    LassoCV(),\n    RidgeCV(),\n    ElasticNetCV(),\n    RandomForestRegressor(),\n    XGBRegressor(), \n    LGBMRegressor()\n]\n\n#  Define estimators to stacked regressor\nestimator_names = ['lassoCV', 'ridgeCV', 'elasticnetCV', 'random_forest', 'xgbregressor', 'lgbmregressor']\nestimators = [(estimator_names[i], model) for i, model in enumerate(models)]\n    \n#  Define stacked model with Linear Regression as the final estimator\nStacked = StackingRegressor(estimators=estimators, final_estimator=LinearRegression(), cv=3)\n\n#  Add stacked model to models list from previous step\nmodels.append(Stacked)\n\n#  Preprocess the data for each model, fit the model, and evaluate\n\nprint('RMSE Cross-Validation Training Scores With Stacked Model \\n')\n\nRMSE = []\nmodel_names = []\nfor i, model in enumerate(models):\n    \n    full_pipeline = Pipeline(steps=[('pipeline', pipeline),\n                                    ('model', model)])\n\n    #  Fit training data and score\n    RMSE.append(get_RMSE(full_pipeline, X_train, y_train, 5))\n    \n    model_names.append(str(model).split('(')[0])\n    print('{} Training Score: {}'.format(model_names[i], round(np.mean(RMSE[i]),4)))\n    \n#  Create a boxplot of the scores\nplt.figure(figsize=(18,7))\nplt.boxplot(RMSE, labels=model_names, showmeans=True)\nplt.xlabel('Models', fontsize=16)\nplt.ylabel('Root Mean Square Error (RMSE)', fontsize=16)\nplt.title('Cross-Validation Scores With Stacked Model', fontsize=18)\nplt.tick_params(axis = 'both', which = 'major', labelsize = 14)\nplt.tick_params(axis = 'both', which = 'minor', labelsize = 14)\n\nx = '' #  Hack to stop figure vomit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<a id=\"section-five\"></a>\n# E. Predict the Test Dataset and Submit\n\nLet's submit our predictions! Before we do we should retrain using the entire training dataset so that we squeeze out a bit more performance. Look how easy this is with pipelines!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Preprocess and fit entire training dataset\n\nfull_pipeline = Pipeline(steps=[('pipeline', pipeline),\n                                ('model', Stacked)])\n\nfull_pipeline.fit(X_train, y_train) #  fit to entire training dataset and not just K folds of it\n\n#  Predict the test dataset target values\n\ny_predict = full_pipeline.predict(df_test)\ny_predict = np.expm1(y_predict)  #  Kaggle will take the log of sale price to compare\n\nmy_submission = pd.DataFrame({'Id': test_id, 'SalePrice': y_predict})\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}