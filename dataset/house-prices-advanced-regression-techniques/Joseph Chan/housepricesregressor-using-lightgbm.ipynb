{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# <center>HousePricesRegressor </center>\n<img src= \"https://miro.medium.com/max/402/1*2foyXif7hwkO8wWB5T9KtQ.png\" height=\"200\" align=\"center\"/>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"<a id=\"Table-Of-Contents\"></a>\n# Table Of Contents\n* [Table Of Contents](#Table-Of-Contents)\n* [Introduction](#Introduction)\n* [Importing Libraries](#Importing-Libraries)\n* [Task Details](#Task-Details)\n* [Feature Description](#Feature-Description)\n* [Read in Data](#Read-in-Data)\n    - [Training Data](#Training-Data)\n    - [Test Data](#Test-Data)\n* [Preprocessing Data](#Preprocessing-Data)\n    - [Label Encoding](#Label-Encoding)\n    - [Train-Test Split](#Train-Test-Split)\n* [Initial Models](#Initial-Models)\n* [LightGBM Regressor](#LightGBM-Regressor)\n    - [Bayesian Optimization](#Bayesian-Optimization)\n    - [Tuning LightGBM](#Tuning-LightGBM)\n    - [Feature Importance](#Feature-Importance)\n    - [Cross Validation](#Cross-Validation)\n* [Prediction for Test Data](#Prediction-for-Test-Data)\n* [Conclusion](#Conclusion)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Introduction\"></a>\n# Introduction \nThis notebook goes through various machine learning techniques for regression. The main focus will be hyper-tuning LightGBM using bayesian optimization. This is a beginner-level notebook but I believe you will find it still useful to read and look over. Please leave comments on where I can improve and give a like! Thank you!"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Importing-Libraries\"></a>\n# Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Imports\n\n# Basic Imports \nimport numpy as np\nimport pandas as pd\n\n# Plotting \nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n%matplotlib inline\n\n# Preprocessing\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import LabelEncoder\n\n# Metrics \nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n# ML Models\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor \nimport xgboost as xg \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import svm\n\n# Model Tuning \nfrom bayes_opt import BayesianOptimization\n\n# Feature Importance \nimport shap\n\n# Ignore Warnings \nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Task-Details\"></a>\n# Task Detail \n\n## Goal\nIt is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. \n\n## Metric\nSubmissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Feature-Description\"></a>\n# Feature Description \nHere's a brief version of what you'll find in the data description file.\n\n## Target  \nSalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.  \n\n## Features\nMSSubClass: The building class  \nMSZoning: The general zoning classification  \nLotFrontage: Linear feet of street connected to property  \nLotArea: Lot size in square feet  \nStreet: Type of road access  \nAlley: Type of alley access  \nLotShape: General shape of property  \nLandContour: Flatness of the property  \nUtilities: Type of utilities available  \nLotConfig: Lot configuration  \nLandSlope: Slope of property  \nNeighborhood: Physical locations within Ames city limits  \nCondition1: Proximity to main road or railroad  \nCondition2: Proximity to main road or railroad (if a second is present)  \nBldgType: Type of dwelling  \nHouseStyle: Style of dwelling  \nOverallQual: Overall material and finish quality  \nOverallCond: Overall condition rating  \nYearBuilt: Original construction date  \nYearRemodAdd: Remodel date  \nRoofStyle: Type of roof  \nRoofMatl: Roof material  \nExterior1st: Exterior covering on house  \nExterior2nd: Exterior covering on house (if more than one material)  \nMasVnrType: Masonry veneer type  \nMasVnrArea: Masonry veneer area in square feet  \nExterQual: Exterior material quality  \nExterCond: Present condition of the material on the exterior  \nFoundation: Type of foundation  \nBsmtQual: Height of the basement  \nBsmtCond: General condition of the basement  \nBsmtExposure: Walkout or garden level basement walls  \nBsmtFinType1: Quality of basement finished area  \nBsmtFinSF1: Type 1 finished square feet  \nBsmtFinType2: Quality of second finished area (if present)  \nBsmtFinSF2: Type 2 finished square feet  \nBsmtUnfSF: Unfinished square feet of basement area  \nTotalBsmtSF: Total square feet of basement area  \nHeating: Type of heating  \nHeatingQC: Heating quality and condition  \nCentralAir: Central air conditioning  \nElectrical: Electrical system  \n1stFlrSF: First Floor square feet  \n2ndFlrSF: Second floor square feet  \nLowQualFinSF: Low quality finished square feet (all floors)  \nGrLivArea: Above grade (ground) living area square feet  \nBsmtFullBath: Basement full bathrooms  \nBsmtHalfBath: Basement half bathrooms  \nFullBath: Full bathrooms above grade  \nHalfBath: Half baths above grade  \nBedroom: Number of bedrooms above basement level  \nKitchen: Number of kitchens  \nKitchenQual: Kitchen quality  \nTotRmsAbvGrd: Total rooms above grade (does not include bathrooms)  \nFunctional: Home functionality rating  \nFireplaces: Number of fireplaces  \nFireplaceQu: Fireplace quality  \nGarageType: Garage location  \nGarageYrBlt: Year garage was built  \nGarageFinish: Interior finish of the garage  \nGarageCars: Size of garage in car capacity  \nGarageArea: Size of garage in square feet  \nGarageQual: Garage quality  \nGarageCond: Garage condition  \nPavedDrive: Paved driveway  \nWoodDeckSF: Wood deck area in square feet  \nOpenPorchSF: Open porch area in square feet  \nEnclosedPorch: Enclosed porch area in square feet  \n3SsnPorch: Three season porch area in square feet  \nScreenPorch: Screen porch area in square feet  \nPoolArea: Pool area in square feet  \nPoolQC: Pool quality  \nFence: Fence quality  \nMiscFeature: Miscellaneous feature not covered in other categories  \nMiscVal: $Value of miscellaneous feature  \nMoSold: Month Sold  \nYrSold: Year Sold  \nSaleType: Type of sale  \nSaleCondition: Condition of sale  "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Read-in-Data\"></a>\n# Read in Data"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Training-Data\"></a>\n## Training Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Read train.csv\ntrain_csv = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n\n# Initial glance at train.csv\nprint(train_csv.info(verbose = True,null_counts=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for missing or NaN data in train.csv\npd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\nprint('Feature      Number of NaN')\nprint(train_csv.isnull().sum().sort_values(ascending=False).loc[lambda x : x!=0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Test-Data\"></a>\n## Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Read test.csv\ntest_csv = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n\n# Initial glance at data\nprint(test_csv.info(verbose = True,null_counts=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for missing or NaN data in test.csv\npd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\nprint('Feature      Number of NaN')\nprint(test_csv.isnull().sum().sort_values(ascending=False).loc[lambda x : x!=0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Preprocessing-Data\"></a>\n# Preprocessing Data\nWe see that the both train.csv and test.csv have missing data. The features ['PoolQC', 'MiscFeature', 'Alley'] should be dropped due to having more than 90% missing values. We will preprocess the dataset by imputation and removing columns with high missing values. "},{"metadata":{"trusted":true},"cell_type":"code","source":"## return columns with more than num_percent % or more missing values \ndef findMissing(df , num_percent = 0.9):\n    len_df = len(df)\n    amt_missing = len_df * num_percent\n    missing_columns = (df.isnull().sum().sort_values(ascending=False).loc[lambda x : x > amt_missing]).index.to_list()\n    missing_columns_count = [df[missing_column].isnull().sum() for missing_column in missing_columns]\n    missing_columns_percent = [f\"{round(df[missing_column].isnull().sum()/len_df,4)*100}%\" for missing_column in missing_columns]\n    missing_columns_type = [df[missing_column].dtypes for missing_column in missing_columns]\n    print(missing_columns_count)\n    print(missing_columns_percent)\n    print(missing_columns_type)\n    return missing_columns\n\nprint(findMissing(train_csv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns to be drop in train.csv and test.csv due to having 90% or more missing values \ndrop_columns = (train_csv.isnull().sum().sort_values(ascending=False).loc[lambda x : x > .90*1460]).index.to_list()\ndrop_columns.append('Id')\n\n# save the 'Id' for Train and Test \ntrain_Id = train_csv['Id'].to_list()\ntest_Id = test_csv['Id'].to_list()\n\nprint('Feature      Number of NaN')\nprint(test_csv.isnull().sum().sort_values(ascending=False).loc[lambda x : x > .90*1460])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For features/columns that have low missing values, less than 10% of each indidiual dataset, we can impute using median imputation for numerical values and mode imputation for categorical values. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# mode imputation on categorical features \n# median imputation on numeric features\ntrain_clean = train_csv.drop(drop_columns, axis = 'columns', errors = 'ignore')\ntest_clean = test_csv.drop(drop_columns, axis = 'columns', errors = 'ignore')\n\ntrain_10_percent_missing_features = train_clean.isnull().sum().sort_values(ascending=False).loc[lambda x : (x<.10*1460)  & (x != 0)].index.to_list()\ntrain_10_percent_missing_features_cat = train_clean[train_10_percent_missing_features].select_dtypes('object').columns.to_list()\ntrain_10_percent_missing_features_num = train_clean[train_10_percent_missing_features].select_dtypes('number').columns.to_list()\n\ntrain_clean[train_10_percent_missing_features_cat] = train_clean[train_10_percent_missing_features_cat].fillna(train_clean[train_10_percent_missing_features_cat].mode().iloc[0])\ntrain_clean[train_10_percent_missing_features_num] = train_clean[train_10_percent_missing_features_num].fillna(train_clean[train_10_percent_missing_features_num].median().iloc[0])\n\n\ntest_10_percent_missing_features = test_clean.isnull().sum().sort_values(ascending=False).loc[lambda x : (x<.10*1460)  & (x != 0)].index.to_list()\ntest_10_percent_missing_features_cat = test_clean[test_10_percent_missing_features].select_dtypes('object').columns.to_list()\ntest_10_percent_missing_features_num = test_clean[test_10_percent_missing_features].select_dtypes('number').columns.to_list()\n\ntest_clean[test_10_percent_missing_features_cat] = test_clean[test_10_percent_missing_features_cat].fillna(test_clean[test_10_percent_missing_features_cat].mode().iloc[0])\ntest_clean[test_10_percent_missing_features_num] = test_clean[test_10_percent_missing_features_num].fillna(test_clean[test_10_percent_missing_features_num].median().iloc[0])\n\n#LotFrontage is a numeric feature and needs to be imputed as well. We can use median imputation again. \n\ntrain_clean[\"LotFrontage\"] = train_clean[\"LotFrontage\"].fillna(train_clean[\"LotFrontage\"].median())\ntest_clean[\"LotFrontage\"] = test_clean[\"LotFrontage\"].fillna(test_clean[\"LotFrontage\"].median())\n\nprint(\"train_clean\")\nprint('Feature      Number of NaN')\nprint(train_clean.isnull().sum().sort_values(ascending=False).loc[lambda x : x!=0])\n\nprint('\\n')\n\nprint(\"test_clean\")\nprint('Feature      Number of NaN')\nprint(test_clean.isnull().sum().sort_values(ascending=False).loc[lambda x : x!=0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the rest of the features we can use just use NaN as a categorical feature. This will be done in MultiColumnLabelEncoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seperate train_clean into target and features \ny = train_clean['SalePrice']\nX_train_clean = train_clean.drop('SalePrice',axis = 'columns')\n\n# save the index for X_aug_train \nX_train_clean_index = X_train_clean.index.to_list()\n\n# row bind aug_train features with aug_test features \n# this makes it easier to apply label encoding onto the entire dataset \nX_total = X_train_clean.append(test_clean,ignore_index = True)\ndisplay(X_total.info(verbose = True,null_counts=True))\n\n# save the index for X_aug_test \nX_test_clean_index = np.setdiff1d(X_total.index.to_list() ,X_train_clean_index) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Label-Encoding\"></a>\n# Label Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% MultiColumnLabelEncoder\n# Code snipet found on Stack Exchange \n# https://stackoverflow.com/questions/24458645/label-encoding-across-multiple-columns-in-scikit-learn\nfrom sklearn.preprocessing import LabelEncoder\n\n\nclass MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                # convert float NaN --> string NaN\n                output[col] = output[col].fillna('NaN')\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)\n\n# store the catagorical features names as a list      \ncat_features = X_total.select_dtypes(['object']).columns.to_list()\n\n# use MultiColumnLabelEncoder to apply LabelEncoding on cat_features \n# uses NaN as a value , no imputation will be used for missing data\nX_total_encoded = MultiColumnLabelEncoder(columns = cat_features).fit_transform(X_total)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##% Split X_total_encoded \nX_train_clean_encoded = X_total_encoded.iloc[X_train_clean_index, :]\nX_test_clean_encoded = X_total_encoded.iloc[X_test_clean_index, :].reset_index(drop = True) \nX_train_clean_encoded.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##% Before and After LabelEncoding for train.csv \ndisplay(X_train_clean.head())\ndisplay(X_train_clean_encoded.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##% Before and After LabelEncoding for test.csv \ndisplay(test_clean.head())\ndisplay(X_test_clean_encoded.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"#Train-Test-Split\"></a>\n# Train-Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create test and train set 80-20\n#%%  train-test split using a 80-20 split\ntrain_X, valid_X, train_y, valid_y = train_test_split(X_train_clean_encoded, y, test_size=0.2, shuffle = True, random_state=0)\n\ntrain_X.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"\n# Initial Models\nWe can not apply different machine learning algorthims to test which model perform better on this dataset. I've listed below various machine learning techniques applied in this section.\n \n1. RandomForest\n2. Support Vector Machine\n3. XGBoost\n4. LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"##% evaluateRegressor\n# from sklearn.metrics import mean_squared_error, mean_absolute_error\ndef evaluateRegressor(true,predicted,message = \"Test set\"):\n    MSE = mean_squared_error(true,predicted,squared = True)\n    MAE = mean_absolute_error(true,predicted)\n    RMSE = mean_squared_error(true,predicted,squared = False)\n    LogRMSE = mean_squared_error(np.log(true),np.log(predicted),squared = False)\n    print(message)\n    print(\"MSE:\", MSE)\n    print(\"MAE:\", MAE)\n    print(\"RMSE:\", RMSE)\n    print(\"LogRMSE:\", LogRMSE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##% Plot True vs predicted values. Useful for continuous y \ndef PlotPrediction(true,predicted, title = \"Dataset: \"):\n    fig = plt.figure(figsize=(20,20))\n    ax1 = fig.add_subplot(111)\n    ax1.set_title(title + 'True vs Predicted')\n    ax1.scatter(list(range(0,len(true))),true, s=10, c='r', marker=\"o\", label='True')\n    ax1.scatter(list(range(0,len(predicted))), predicted, s=10, c='b', marker=\"o\", label='Predicted')\n    plt.legend(loc='upper right');\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##% Initial Models\n# from sklearn.ensemble import RandomForestRegressor\n# from sklearn import svm\n# import lightgbm as lgb\n# import xgboost as xg \n\nRFReg = RandomForestRegressor(random_state = 0).fit(train_X, train_y)\nSVM = svm.SVR().fit(train_X, train_y) \nXGReg = xg.XGBRegressor(objective ='reg:squarederror', seed = 0,verbosity=0).fit(train_X,train_y) \nLGBMReg = lgb.LGBMRegressor(random_state=0).fit(train_X,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##% Model Metrics\nprint(\"Random Forest Regressor\") \npredicted_train_y = RFReg.predict(train_X)\nevaluateRegressor(train_y,predicted_train_y,\"    Training Set\")\npredicted_valid_y = RFReg.predict(valid_X)\nevaluateRegressor(valid_y,predicted_valid_y,\"    Test Set\")\nprint(\"\\n\")\n    \nprint(\"Support Vector Machine\") \npredicted_train_y = SVM.predict(train_X)\nevaluateRegressor(train_y,predicted_train_y,\"    Training Set\")\npredicted_valid_y = SVM.predict(valid_X)\nevaluateRegressor(valid_y,predicted_valid_y,\"    Test Set\")\nprint(\"\\n\")\n\n\nprint(\"XGBoost Regressor\") \npredicted_train_y = XGReg.predict(train_X)\nevaluateRegressor(train_y,predicted_train_y,\"    Training Set\")\npredicted_valid_y = XGReg.predict(valid_X)\nevaluateRegressor(valid_y,predicted_valid_y,\"    Test Set\")\nprint(\"\\n\")\n\nprint(\"LightGBM Regressor\") \npredicted_train_y = LGBMReg.predict(train_X)\nevaluateRegressor(train_y,predicted_train_y,\"    Training Set\")\npredicted_valid_y = LGBMReg.predict(valid_X)\nevaluateRegressor(valid_y,predicted_valid_y,\"    Test Set\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These models perform poorly as we conduct no parameter tuning. We see lightGBM performs the best on the test set. I use Bayesian Optimization to tune the hyperparameters for LightGBM to obtain a better model. "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"LightGBM-Regressor\"></a>\n# LightGBM Regressor"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Bayesian-Optimization\"></a>\n## Bayesian Optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"##% parameter tuning for lightgbm \n# store the catagorical features names as a list      \ncat_features = X_train_clean_encoded.select_dtypes(['object']).columns.to_list()\n\n# Create the LightGBM data containers\n# Make sure that cat_features are used\ntrain_data=lgb.Dataset(train_X,label=train_y, categorical_feature = cat_features,free_raw_data=False)\nvalid_data=lgb.Dataset(valid_X,label=valid_y, categorical_feature = cat_features,free_raw_data=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://medium.com/analytics-vidhya/hyperparameters-optimization-for-lightgbm-catboost-and-xgboost-regressors-using-bayesian-6e7c495947a9\n# from lightgbm import LGBMRegressor \n# from bayes_opt import BayesianOptimization\ndef search_best_param(X,y,cat_features):\n    \n    trainXY = lgb.Dataset(data=X, label=y,categorical_feature = cat_features,free_raw_data=False)\n    # define the lightGBM cross validation\n    def lightGBM_CV(max_depth, num_leaves, n_estimators, learning_rate, subsample, colsample_bytree, \n                lambda_l1, lambda_l2, min_child_weight):\n    \n        params = {'boosting_type': 'gbdt', 'objective': 'regression', 'metric':'rmse', 'verbose': -1,\n                  'early_stopping_round':100}\n        \n        params['max_depth'] = int(round(max_depth))\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params[\"n_estimators\"] = int(round(n_estimators))\n        params['learning_rate'] = learning_rate\n        params['subsample'] = subsample\n        params['colsample_bytree'] = colsample_bytree\n        params['lambda_l1'] = max(lambda_l1, 0)\n        params['lambda_l2'] = max(lambda_l2, 0)\n        params['min_child_weight'] = min_child_weight\n    \n        score = lgb.cv(params, trainXY, nfold=5, seed=1, stratified=False, verbose_eval =False, metrics=['rmse'])\n\n        return -np.min(score['rmse-mean']) # return negative rmse to minimize rmse \n\n    # use bayesian optimization to search for the best hyper-parameter combination\n    lightGBM_Bo = BayesianOptimization(lightGBM_CV, \n                                       {\n                                          'max_depth': (5, 50),\n                                          'num_leaves': (20, 100),\n                                          'n_estimators': (50, 1000),\n                                          'learning_rate': (0.01, 0.3),\n                                          'subsample': (0.7, 0.8),\n                                          'colsample_bytree' :(0.5, 0.99),\n                                          'lambda_l1': (0, 5),\n                                          'lambda_l2': (0, 3),\n                                          'min_child_weight': (2, 50) \n                                      },\n                                       random_state = 1,\n                                       verbose = 0\n                                      )\n    np.random.seed(1)\n    \n    lightGBM_Bo.maximize(init_points=5, n_iter=25) \n    \n    params_set = lightGBM_Bo.max['params']\n    \n    # get the params of the maximum target     \n    max_target = -np.inf\n    for i in lightGBM_Bo.res: # loop thru all the residuals \n        if i['target'] > max_target:\n            params_set = i['params']\n            max_target = i['target']\n    \n    params_set.update({'verbose': -1})\n    params_set.update({'metric': 'rmse'})\n    params_set.update({'boosting_type': 'gbdt'})\n    params_set.update({'objective': 'regression'})\n    \n    params_set['max_depth'] = int(round(params_set['max_depth']))\n    params_set['num_leaves'] = int(round(params_set['num_leaves']))\n    params_set['n_estimators'] = int(round(params_set['n_estimators']))\n    params_set['seed'] = 1 #set seed\n    \n    return params_set\n\nbest_params = search_best_param(train_X,train_y,cat_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print best_params\nfor key, value in best_params.items():\n    print(key, ' : ', value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Tuning-LightGBM\"></a>\n## Tuning LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train lgbm_best using the best params found from Bayesian Optimization\nlgbm_best = lgb.train(best_params,\n                 train_data,\n                 num_boost_round = 2500,\n                 valid_sets = valid_data,\n                 early_stopping_rounds = 200,\n                 verbose_eval = 100\n                 )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"LightGBM-Model-Peformance \"></a>\n## LightGBM Model Peformance "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"LightGBM Regressor Tuned\") \npredicted_train_y = lgbm_best.predict(train_X)\nevaluateRegressor(train_y,predicted_train_y,\"    Training Set\")\nPlotPrediction(train_y,predicted_train_y,\"Training Set: \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_valid_y = lgbm_best.predict(valid_X)\nevaluateRegressor(valid_y,predicted_valid_y,\"    Test Set\")\nPlotPrediction(valid_y,predicted_valid_y,\"Test Set: \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Feature-Importance \"></a>\n## Feature Importance "},{"metadata":{"trusted":true},"cell_type":"code","source":"##% Feature Importance \n# https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\nlgb.plot_importance(lgbm_best,figsize=(25,20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##% Feature Importance using shap package \n# import shap\nlgbm_best.params['objective'] = 'regression'\nshap_values = shap.TreeExplainer(lgbm_best).shap_values(valid_X)\nshap.summary_plot(shap_values, valid_X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From both feature importance, we can see that **GrLivArea** and **LotArea** contributes a lot in prediction the price of home. The shap package is prefer when finding feature importance as it preservces consistency and accuracy. You can read more about the shap package in the links provided below:\n\n[https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/tree_based_models/Census%20income%20classification%20with%20LightGBM.html](https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/tree_based_models/Census%20income%20classification%20with%20LightGBM.html)\n\n[https://towardsdatascience.com/explain-your-model-with-the-shap-values-bc36aac4de3d](https://towardsdatascience.com/explain-your-model-with-the-shap-values-bc36aac4de3d)  \n\n[https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27](https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Cross-Validation \"></a>\n## Cross Validation "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross Validation with LightGBM\n\ndef K_Fold_LightGBM(X_train, y_train , cat_features, num_folds = 3):\n    num = 0\n    models = []\n    folds = KFold(n_splits=num_folds, shuffle=True, random_state=0)\n\n        # 5 times \n    for n_fold, (train_idx, valid_idx) in enumerate (folds.split(X_train, y_train)):\n        print(f\"     model{num}\")\n        train_X, train_y = X_train.iloc[train_idx], y_train.iloc[train_idx]\n        valid_X, valid_y = X_train.iloc[valid_idx], y_train.iloc[valid_idx]\n        \n        train_data=lgb.Dataset(train_X,label=train_y, categorical_feature = cat_features,free_raw_data=False)\n        valid_data=lgb.Dataset(valid_X,label=valid_y, categorical_feature = cat_features,free_raw_data=False)\n        \n        params_set = search_best_param(train_X,train_y,cat_features)\n        \n        CV_LGBM = lgb.train(params_set,\n                            train_data,\n                            num_boost_round = 2500,\n                            valid_sets = valid_data,\n                            early_stopping_rounds = 200,\n                            verbose_eval = 100\n                           )\n        \n        # increase early_stopping_rounds can lead to overfitting \n        models.append(CV_LGBM)\n        \n        print(\"Train set logRMSE:\", mean_squared_error(np.log(train_y),np.log(models[num].predict(train_X)),squared = False))\n        print(\" Test set logRMSE:\", mean_squared_error(np.log(valid_y),np.log(models[num].predict(valid_X)),squared = False))\n        print(\"\\n\")\n        num = num + 1\n        \n    return models\n\nlgbm_models = K_Fold_LightGBM(X_train_clean_encoded,y,cat_features,5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict y_prds using models from cross validation \ndef predict_cv(models_cv,X):\n    y_preds = np.zeros(shape = X.shape[0])\n    for model in models_cv:\n        y_preds += model.predict(X)\n        \n    return y_preds/len(models_cv)\n\n# evalute model using the entire dataset from Train.csv\nevaluateRegressor(y,predict_cv(lgbm_models,X_train_clean_encoded),\"Train.csv \")\nPlotPrediction(y,predict_cv(lgbm_models,X_train_clean_encoded),\"Train.csv: \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Prediction-for-Test-Data\"></a>\n# Prediction for Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictLGBM = lgbm_best.predict(X_test_clean_encoded)\nsubmissionLGBM = pd.DataFrame({'Id':test_Id,'SalePrice':predictLGBM})\ndisplay(submissionLGBM.head())\n\npredictLGBM_CV = predict_cv(lgbm_models,X_test_clean_encoded)\nsubmissionLGBM_CV = pd.DataFrame({'Id':test_Id,'SalePrice':predictLGBM_CV})\ndisplay(submissionLGBM_CV.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##% Submit Predictions \nsubmissionLGBM.to_csv('submissionLGBM4.csv',index=False)\nsubmissionLGBM_CV.to_csv('submissionLGBM_CV4.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Conclusion\"></a>\n# Conclusion\n\n**Conclusion**\n* LightGBM is a great ML algorithim that handles catagorical features and missing values \n* This is a great dataset to work on and lots of knowledge can be gain from withing with this dataset \n* Researching and reading other Kaggle notebooks is essential for becoming a better data scientist\n\n**Challenges**\n* This dataset had missing data so other imputation techniques could be used \n* Overfitting is an issue and might have occured\n* No Data Visualization due to lots of features \n\n**Closing Remarks**  \n* Please comment and like the notebook if it of use to you! Have a wonderful year! \n\n**Other Notebooks** \n* [https://www.kaggle.com/josephchan524/studentperformanceregressor-rmse-12-26-r2-0-26](https://www.kaggle.com/josephchan524/studentperformanceregressor-rmse-12-26-r2-0-26)\n* [https://www.kaggle.com/josephchan524/bankchurnersclassifier-recall-97-accuracy-95](https://www.kaggle.com/josephchan524/bankchurnersclassifier-recall-97-accuracy-95)\n* [https://www.kaggle.com/josephchan524/housepricesregressor-using-lightgbm](https://www.kaggle.com/josephchan524/housepricesregressor-using-lightgbm)\n* [https://www.kaggle.com/josephchan524/tabularplaygroundregressor-using-lightgbm-feb2021](https://www.kaggle.com/josephchan524/tabularplaygroundregressor-using-lightgbm-feb2021)\n\n\n2-12-2020\nJoseph Chan "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}