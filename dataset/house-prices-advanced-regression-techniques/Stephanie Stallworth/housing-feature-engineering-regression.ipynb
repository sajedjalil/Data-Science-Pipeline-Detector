{"nbformat_minor":1,"nbformat":4,"cells":[{"cell_type":"markdown","metadata":{"_uuid":"47d273a8ad6b8ddd5051a59b5360bc7b77aed62d","_cell_guid":"5a3263c7-7693-4cad-8267-1b54d7226efc"},"source":"# **EXECUTIVE SUMMARY**\n <br/>\nThe objectives of this kernel can be described in three parts:\n\n**1. Data Pre-Processing / Feature Engineering**  \n- Load and explore data shape assumptions\n- Apply advanced engineering techniques to address missing data\n- Encode categorical variables\n\n**2. Dimensionality Reduction**  \n- Feature selection with Lasso Regression (Regularized Regression) \n- Univariate feature selection methods with sklearn\n\n**3. Kaggle Submission**\n- Apply the best feature selection technique and model to test data for contest submission\n\n---\n## **TRAINING DATA PRE-PROCESSING**\nThe first step in the data science pipeline is to load the data and get a better feel for the columns it contains.  "},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"7553113b2263ade6e05e749c63e904e979c7db79","_cell_guid":"4be10316-9183-4fdc-bb03-56fb01c95a07","scrolled":true},"source":"# Data Pre-Processing\n# Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sklearn\nimport seaborn as sns\nimport matplotlib.mlab as mlab\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Read data\ntrain_data = pd.read_csv('../input/train.csv')\n\n# Settings\npd.set_option('display.height', 1000)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 2000)\n\n# Data shape\nprint('Data Shape',train_data.shape)\nprint(train_data.info())\ntrain_data.head(5)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"ff015fa97d32a66087072dfb7ae4eab76a939cab","_cell_guid":"3e45be72-ded6-4a7a-a787-dbdada2e40e7"},"source":" ## **FEATURE ENGINEERING IN TRAINING DATA**  \n ### **MISSING DATA**\nFrom the entry totals above, many of the features are missing a significant amount of data.  I will explore these columns further to determine the best approach for each."},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"b59d93c641a3205d5a32d2838df89b753a48a29e","_cell_guid":"e0b41a62-d279-4e73-9f91-319101b6dcf4"},"source":"# Missing Value Count Function\ndef show_missing():\n    missing = train_data.columns[train_data.isnull().any()].tolist()\n    return missing\n\n# Missing data counts and percentage\nprint('Missing Data Count')\nprint(train_data[show_missing()].isnull().sum().sort_values(ascending = False))\nprint('--'*40)\nprint('Missing Data Percentage')\nprint(round(train_data[show_missing()].isnull().sum().sort_values(ascending = False)/len(train_data)*100,2))","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"2fe30937c5b6060e6f34854fab29750c85bd51a9","_cell_guid":"240ecaf5-628e-4dd0-be23-8458b1c70565"},"source":"# Functions to address missing data\n\n# Explore features\ndef feat_explore(column):\n    return train_data[column].value_counts()\n\n# Function to impute missing values\ndef feat_impute(column, value):\n    train_data.loc[train_data[column].isnull(),column] = value","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"25df3755855c5f9ead80210beb9ac0eb1fc153bb","_cell_guid":"a9887d37-3646-4310-92e6-4588c1aefde0"},"source":"**Over 50% Missing**  \nPoolQC, MiscFeature, Alley, Fence will all be removed as they are missing over half of their observations.  "},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"24dc97cb0ee170217efafc61739bff94e13c7657","_cell_guid":"760772dd-b065-443f-ab09-e180a98f08a4"},"source":"# Features with over 50% of its observations missings will be removed\ntrain_data = train_data.drop(['PoolQC','MiscFeature','Alley','Fence'],axis = 1)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"edffe63af63dba9251fdf53c20630c8cdc1ac248","_cell_guid":"8cd84a8c-f3df-490a-840b-dec8431023d6"},"source":"**Fireplace Qu**  \nFireplaceQu is missing 690 observations.  However, these nulls may be attributed to homes that do not have fireplaces at all.  If this assumption proves to be true, we can impute these nulls  with '0' as they do not have a fireplace.  "},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"fcfff4bdf42bee49a6555bacdd86db9b2d0d300d","_cell_guid":"0ca23702-7424-47ab-b9df-0e8921e84217","scrolled":true},"source":"# FireplaceQu missing data\nprint('FireplaceQu Missing Before:', train_data['FireplaceQu'].isnull().sum())\nprint('--'*40)\n\n# The null values may be homes that do not have fireplaces at all. Need to check this assumption\nprint(train_data[train_data['FireplaceQu'].isnull()][['Fireplaces','FireplaceQu']])\nprint(train_data[train_data['FireplaceQu'].isnull()][['Fireplaces','FireplaceQu']].shape)\nprint('--'*40)\n\n# Impute the nulls with None \ntrain_data['FireplaceQu'] = train_data['FireplaceQu'].fillna('None')\nprint('FireplaceQu Missing After:', train_data['FireplaceQu'].isnull().sum())\n\nprint('--'*40)\n# Cross check columns\nprint('Confirm Imputation')\nprint(pd.crosstab(train_data.FireplaceQu,train_data.Fireplaces,))","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"d7733739ffb3bfa1a71b8a163810b98956ce7989","_cell_guid":"c3e2146c-e564-4797-b06c-d4d7dab77045"},"source":"**Lot Frontage**  \nLotFrontage is missing 259 observations.  First, I will check to see if there are other variables that are strongly correlated with LotFrontage I can use for imputation.  Otherwise, I will impute with the median LotFrontage value.  "},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"00f1352a05d849372ce8a006b708d0b06ffa60a8","_cell_guid":"edf283eb-bc4e-43de-b4ed-d455f9165423","scrolled":true},"source":"# Lot Frontage\nprint('LotFrontage Missing Before:', train_data['LotFrontage'].isnull().sum())\n\n# Check to see if there is a strong correlation with other variables we can use to impute\ncorr_lf = train_data.select_dtypes(include = ['float64', 'int64']).iloc[:, 1:].corr()\ncor_dict_lf = corr_lf['LotFrontage'].to_dict()\ndel cor_dict_lf['LotFrontage']\nprint(\"Numeric features by Correlation with LotFrontage:\\n\")\nfor ele in sorted(cor_dict_lf.items(), key = lambda x: -abs(x[1])):\n    print(\"{0}: \\t{1}\".format(*ele))\n\n# Nothing highly correlated to LotFrontage so will impute with the mean\ntrain_data['LotFrontage'] = train_data['LotFrontage'].fillna(train_data['LotFrontage'].median())\nprint('LotFrontage Missing After:', train_data['LotFrontage'].isnull().sum())","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"468c89bd696ed064e57fba978fb60109b7411230","_cell_guid":"e4b90361-08ac-4d15-ae85-4ab7aa4e73a2"},"source":"**Garage Features**  \nGarageYrBlt, GarageType, GarageFinish,GarageQual, and GarageCond are all missing 81 observations. These null values are assumed to be in the same rows for each column and associated with homes that do not have garages at all. If these assumptions are correct,  the nulls can be inputed with zero as these are properties without garages. \n"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"8c8b65b2f2a79eb7c8c2dc731dae38712d7b06c2","_cell_guid":"a1fa4988-aa21-4054-8161-87750d6e2921","scrolled":true},"source":"# Garage Features\nprint('Garage Features Missing Before')\nprint(train_data[['GarageYrBlt', 'GarageType', 'GarageFinish','GarageQual','GarageCond']].isnull().sum())\n\n# Assumptions check\nprint('--'*40)\nprint('Assumption Check')\nnull_garage = ['GarageYrBlt','GarageType','GarageQual','GarageCond','GarageFinish']\nprint(train_data[(train_data['GarageYrBlt'].isnull())|\n                 (train_data['GarageType'].isnull())|\n                 (train_data['GarageQual'].isnull())|\n                 (train_data['GarageCond'].isnull())|\n                 (train_data['GarageFinish'].isnull())]\n                 [['GarageCars','GarageYrBlt','GarageType','GarageQual','GarageCond','GarageFinish']])\n\n# Impute null garage features\nfor cols in null_garage:\n   if train_data[cols].dtype ==np.object:\n         feat_impute(cols, 'None')\n   else:\n         feat_impute(cols, 0)\n        \n# Garage Features After\nprint('--'*40)\nprint('Garage Features Missing After')\nprint(train_data[['GarageYrBlt', 'GarageType', 'GarageFinish','GarageQual','GarageCond']].isnull().sum())\nprint('--'*40)\n# Cross check columns\nprint('Confirm Imputation')\nfor cols in null_garage:\n    print(pd.crosstab(train_data[cols],train_data.GarageCars))","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"f8daac102af616c228ccae150c2b41c0052cecba","_cell_guid":"94119572-edf9-4f4e-91fc-d4e1f654fbc1"},"source":" **Basement Features Part 1**  \nBsmtFinType2 and 'BsmtExposure are both missing 38 observations.  It is suspected that these observations are in the same rows for both columns and associated with homes that do not have basements.  If these assumptions are true, we can impute the nulls with zero as we have for other missing values previously.\n"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"9096a9a0b4ddf190e4e990eedf3fcaf518d35cde","_cell_guid":"4ed95e38-434d-4bd7-8f71-b1f538d901b2"},"source":"# Basement Features\nprint('Basement Features Missing Before')\nprint(train_data[['BsmtFinType2', 'BsmtExposure']].isnull().sum())\nprint('--'*40)\n\n# BsmtFinType2 and BsmtExposure are both missing 38 observations\n# Check that data is missing in the same rows\n# Confirm if all nulls correspond to homes without basements\nprint('Assumption Check')\nnull_basement = ['BsmtFinType2','BsmtExposure']\nprint(train_data[train_data['BsmtFinType2'].isnull()|(train_data['BsmtExposure'].isnull())][['TotalBsmtSF','BsmtFinType2','BsmtExposure']])\nprint('entries',train_data[train_data['BsmtFinType2'].isnull()|(train_data['BsmtExposure'].isnull())][['TotalBsmtSF','BsmtFinType2','BsmtExposure']].shape)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"3dec5aa891309c98e48d14c486832cd175b04789","_cell_guid":"1c111832-b6ec-4eab-9f97-5f467f7fc14d"},"source":"**Basement Features Part 1 (Continued...)**  \nMost of the nulls are homes without basements; however, there are two exceptions that must be addressed before we can impute the nulls with zero.  \n1. **BsmTfinType2** at index 332 is null, but has a basement of 3206 sq feet.  \n2. **BsmtExposure** at index 948 is null, but has a basement of 936 sq feet.  \n\nThese two exceptions will be imputed with that column's most frequent value.  The remaning null values will be imputed with 'None' as they corresond to homes without basements.  "},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"76b38676e7c09864a9c5297ac987e237808e961e","_cell_guid":"d4fc534a-bc9a-496f-86c1-1e9256c8c7cc"},"source":"# Impute the only null BsmtFinType2 with a basement at index 332 with most frequent value\ntrain_data.iloc[332, train_data.columns.get_loc('BsmtFinType2')] = train_data['BsmtFinType2'].mode()[0]\n#train_data.set_value(332,'BsmtFinType2',train_data['BsmtFinType2'].mode()[0])\n\n# Impute the only null BsmtExposure with a basement at index 948 with most frequent value\ntrain_data.iloc[948, train_data.columns.get_loc('BsmtExposure')] = train_data['BsmtExposure'].mode()[0]\n\n# Impute the remaining nulls as None\nfor cols in null_basement:\n   if train_data[cols].dtype ==np.object:\n         feat_impute(cols, 'None')\n   else:\n         feat_impute(cols, 0)\n\n# Basement Features After\nprint('--'*40)\nprint('Basement Features Missing After')\nprint(train_data[['BsmtFinType2', 'BsmtExposure']].isnull().sum())\nprint('--'*40)\n# Cross check columns\nprint('Confirm Imputation')\nfor cols in null_basement:\n    print(pd.crosstab(train_data.TotalBsmtSF,train_data[cols]))","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"2865e9b749013f2c43c52e2e1ecae11d78246b3a","_cell_guid":"d9a4f712-6875-41f1-bcc3-9815830efce7"},"source":"**Basement Features Part 2**  \nThere is another set of basement features that are missing the same number of observations. BsmtFinType1, BsmtCond, BsmtQual all have 37 missing values.  It is assumed that these observations are in the same rows and correspond to homes that do not have basements.  If these assumptions are true, we will impute nulls with 'None', otherwise impute using the most frequent value for each feature.  "},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"d2fd2572196bb94dd3d115a20861c819914dcdfc","_cell_guid":"aad4dbcf-00eb-464c-bed2-956144689125","scrolled":true},"source":"# Basement Features Part 2\nprint('Basement Features Part 2 Missing Before')\nprint(train_data[['BsmtFinType1', 'BsmtCond','BsmtQual']].isnull().sum())\nprint('--'*40)\n\n# Check assumptions\nnull_basement2 = ['BsmtFinType1', 'BsmtCond','BsmtQual']\nprint('Assumption Check')\nprint(train_data[(train_data['BsmtFinType1'].isnull())|\n                 (train_data['BsmtCond'].isnull())|\n                 (train_data['BsmtQual'].isnull())]\n                 [['TotalBsmtSF','BsmtFinType1', 'BsmtCond','BsmtQual']])\nprint('entries',train_data[(train_data['BsmtFinType1'].isnull())|\n                 (train_data['BsmtCond'].isnull())|\n                 (train_data['BsmtQual'].isnull())]\n                 [['TotalBsmtSF','BsmtFinType1', 'BsmtCond','BsmtQual']].shape)\n\n# NA in all. NA means No basement\n# for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n#   features[col] = features[col].fillna('None')\n\n# Impute nulls to None or 0\nfor cols in null_basement2:\n    if train_data[cols].dtype ==np.object:\n        cols = feat_impute(cols, 'None')\n    else:\n        cols = feat_impute(cols, 0)    \n\nprint('--'*40)\nprint('Basement Features Part 2 Missing After')\nprint(train_data[['BsmtFinType1', 'BsmtCond','BsmtQual']].isnull().sum())\n","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"fd76f7d464adb5748815e9403ea606b4f61702e6","_cell_guid":"967df914-248f-41cd-b701-cd37f36996db"},"source":"**Masonry Features**  \nMasVnrArea and MasVnrType are both missing 8 observations.  Again, we'll check to see they are missing in the same rows and then impute with the most frequent value for each column.  \n"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"bea3007301c281c6c151dc100f8645b2fac3eddf","_cell_guid":"7ad5901f-b765-4686-93b6-d463bf7b7fd8"},"source":"# MasVnrArea and MasVnrType are each missing 8 observations\nprint('Masonry Features Missing Before')\nprint(train_data[['MasVnrArea', 'MasVnrType']].isnull().sum())\nprint('--'*40)\n\n# Confirm that the missing values in these columns are the same rows\nprint('Check Assumptions')\nprint(train_data[(train_data['MasVnrArea'].isnull())|\n                 (train_data['MasVnrType'].isnull())]\n                 [['MasVnrArea','MasVnrType']])\n\nprint(train_data[(train_data['MasVnrArea'].isnull())|\n                 (train_data['MasVnrType'].isnull())]\n                 [['MasVnrArea','MasVnrType']].shape)\n\n# Impute MasVnrArea with the most frequent values\n# feat_explore('MasVnrArea')\n# feat_impute('MasVnrArea','None')\ntrain_data['MasVnrArea'] = train_data['MasVnrArea'].fillna(train_data['MasVnrArea'].mode()[0])\n\n# Impute MasVnrType with the most frequent values\n# feat_explore('MasVnrType')\n# feat_impute('MasVnrType',0.0)\ntrain_data['MasVnrType'] = train_data['MasVnrType'].fillna(train_data['MasVnrType'].mode()[0])\n\nprint('Masonry Features Missing After')\nprint(train_data[['MasVnrArea', 'MasVnrType']].isnull().sum())\nprint('--'*40)\n\nprint('Confirm Imputation')\nprint(train_data[(train_data['MasVnrArea'].isnull())|\n                 (train_data['MasVnrType'].isnull())]\n                 [['MasVnrArea','MasVnrType']])","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"9088fd40ffab978b6cd0a54fd507972746fdaf20","_cell_guid":"1e7cda71-f8e6-4498-a8f6-cf4a15b0ecf9"},"source":"**Electrical**  \nElectrical is only missing one observation, which can be imputed with the most frequent value in the column.  "},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"75c56a9b53f53ef1cad99ebb99131bf2dec77248","_cell_guid":"7f035b63-2749-42b1-a84f-4072800d8446"},"source":"# Electrical is only missing one value\nprint('Electrical Feature Missing Before')\nprint(train_data[['Electrical']].isnull().sum())\nprint('--'*40)\n\n# Impute Electrical with the most frequent value, 'SBrkr'\n# feat_explore('Electrical')\n# feat_impute('Electrical','SBrkr')\ntrain_data['Electrical'] = train_data['Electrical'].fillna(train_data['Electrical'].mode()[0])\nprint('Electrical Feature Missing After')\nprint(train_data[['Electrical']].isnull().sum())\nprint('--'*40)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"d014e32b77c5ebbabc7094a8333fa3996d904b09","_cell_guid":"6fb637f8-ec01-402a-b7a7-1ebeb6d328b9"},"source":"# Confirm all changes\nprint('Missing Data Count')\nprint(train_data[show_missing()].isnull().sum().sort_values(ascending = False))\nprint('No Missing Values')","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"7abd83c030764219420158acf54b66fd75779694","_cell_guid":"b7961d57-144c-45be-9d42-82ef83cba08f","scrolled":true},"source":"train_data.info()","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"7302a7e5e0aee1c06fcc11a9181416f3b21cbc11","_cell_guid":"b47e90f8-40de-4ce8-973e-7ce317eb17f5"},"source":"### **ENCODING CATEGORICAL VARIABLES IN TRAINING DATA**"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"829671e5e99e9d122a533319b84e4446ceee9ff8","_cell_guid":"c4a42415-454f-403b-9b3e-ca26bbc57c83"},"source":"# Data Types\n# Categorical Features\nprint('Categorical Features:\\n ', train_data.select_dtypes(include=['object']).columns)\nprint('--'*40)\n\n# Numeric Features\nprint('Numeric Features:\\n ', train_data.select_dtypes(exclude=['object']).columns)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"41df209bb7a3e77cdb6b307f9b1c8c8c2d57f5ce","_cell_guid":"49437afd-7aa4-4afb-a8d4-9790187ae83d","scrolled":true},"source":"catcols = train_data.select_dtypes(['object'])\nfor cat in catcols:\n    print('--'*40)\n    print(cat)\n    print(train_data[cat].value_counts())","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"e46ea7d8805a9e152e53cb27d9445e8ecd82bebc","_cell_guid":"b805637a-26f6-4964-bcb7-5b5689fd5732"},"source":"# Encode ordinal data\ntrain_data['LotShape'] = train_data['LotShape'].map({'Reg':0,'IR1':1,'IR2':2,'IR3':3})\ntrain_data['LandContour'] = train_data['LandContour'].map({'Low':0,'HLS':1,'Bnk':2,'Lvl':3})\ntrain_data['Utilities'] = train_data['Utilities'].map({'NoSeWa':0,'NoSeWa':1,'AllPub':2})\ntrain_data['BldgType'] = train_data['BldgType'].map({'Twnhs':0,'TwnhsE':1,'Duplex':2,'2fmCon':3,'1Fam':4})\ntrain_data['HouseStyle'] = train_data['HouseStyle'].map({'1Story':0,'1.5Fin':1,'1.5Unf':2,'2Story':3,'2.5Fin':4,'2.5Unf':5,'SFoyer':6,'SLvl':7})\ntrain_data['BsmtFinType1'] = train_data['BsmtFinType1'].map({'None':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\ntrain_data['BsmtFinType2'] = train_data['BsmtFinType2'].map({'None':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\ntrain_data['LandSlope'] = train_data['LandSlope'].map({'Gtl':0,'Mod':1,'Sev':2})\ntrain_data['Street'] = train_data['Street'].map({'Grvl':0,'Pave':1})\ntrain_data['MasVnrType'] = train_data['MasVnrType'].map({'None':0,'BrkCmn':1,'BrkFace':2,'CBlock':3,'Stone':4})\ntrain_data['CentralAir'] = train_data['CentralAir'].map({'N':0,'Y':1})\ntrain_data['GarageFinish'] = train_data['GarageFinish'].map({'None':0,'Unf':1,'RFn':2,'Fin':3})\ntrain_data['PavedDrive'] = train_data['PavedDrive'].map({'N':0,'P':1,'Y':2})\ntrain_data['BsmtExposure'] = train_data['BsmtExposure'].map({'None':0,'No':1,'Mn':2,'Av':3,'Gd':4})\ntrain_data['ExterQual'] = train_data['ExterQual'].map({'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntrain_data['ExterCond'] = train_data['ExterCond'].map({'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntrain_data['BsmtCond'] = train_data['BsmtCond'].map({'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntrain_data['BsmtQual'] = train_data['BsmtQual'].map({'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntrain_data['HeatingQC'] = train_data['HeatingQC'].map({'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntrain_data['KitchenQual'] = train_data['KitchenQual'].map({'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntrain_data['FireplaceQu'] = train_data['FireplaceQu'].map({'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntrain_data['GarageQual'] = train_data['GarageQual'].map({'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntrain_data['GarageCond'] = train_data['GarageCond'].map({'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\n\n# Encode Categorical Variables\ntrain_data['Foundation'] = train_data['Foundation'].map({'BrkTil':0,'CBlock':1,'PConc':2,'Slab':3,'Stone':4,'Wood':5})\ntrain_data['Heating'] = train_data['Heating'].map({'Floor':0,'GasA':1,'GasW':2,'Grav':3,'OthW':4,'Wall':5})\ntrain_data['Electrical'] = train_data['Electrical'].map({'SBrkr':0,'FuseA':1,'FuseF':2,'FuseP':3,'Mix':4})\ntrain_data['Functional'] = train_data['Functional'].map({'Sal':0,'Sev':1,'Maj2':2,'Maj1':3,'Mod':4,'Min2':5,'Min1':6,'Typ':7})\ntrain_data['GarageType'] = train_data['GarageType'].map({'None':0,'Detchd':1,'CarPort':2,'BuiltIn':3,'Basment':4,'Attchd':5,'2Types':6})\ntrain_data['SaleType'] = train_data['SaleType'].map({'Oth':0,'ConLD':1,'ConLI':2,'ConLw':3,'Con':4,'COD':5,'New':6,'VWD':7,'CWD':8,'WD':9})\ntrain_data['SaleCondition'] = train_data['SaleCondition'].map({'Partial':0,'Family':1,'Alloca':2,'AdjLand':3,'Abnorml':4,'Normal':5})\ntrain_data['MSZoning'] = train_data['MSZoning'].map({'A':0,'FV':1,'RL':2,'RP':3,'RM':4,'RH':5,'C (all)':6,'I':7})\ntrain_data['LotConfig'] = train_data['LotConfig'].map({'Inside':0,'Corner':1,'CulDSac':2,'FR2':3,'FR3':4})\ntrain_data['Neighborhood'] = train_data['Neighborhood'].map({'Blmngtn':0,'Blueste':1,'BrDale':2,'BrkSide':3, 'ClearCr':4,'CollgCr':5,'Crawfor':6,'Edwards':7,'Gilbert':8,\n                                                             'IDOTRR':9,'MeadowV':10,'Mitchel':11, 'NAmes':12,'NoRidge':13,'NPkVill':14,'NridgHt':15, 'NWAmes':16,\n                                                             'OldTown':17,'SWISU':18,'Sawyer':19, 'SawyerW':20,'Somerst':21,'StoneBr':22,'Timber':23,'Veenker':24})\ntrain_data['Condition1'] = train_data['Condition1'].map({'Artery':0,'Feedr':1,'Norm':2,'RRNn':3, 'RRAn':4,'PosN':5,'PosA':6,'RRNe':7,'RRAe':8})\ntrain_data['Condition2'] = train_data['Condition2'].map({'Artery':0,'Feedr':1,'Norm':2,'RRNn':3, 'RRAn':4,'PosN':5,'PosA':6,'RRNe':7,'RRAe':8})\ntrain_data['RoofStyle'] = train_data['RoofStyle'].map({'Flat':0,'Gable':1,'Gambrel':2,'Hip':3,'Mansard':4,'Shed':5})\ntrain_data['RoofMatl'] = train_data['RoofMatl'].map({'ClyTile':0,'CompShg':1,'Membran':2,'Metal':3,'Roll':4,'Tar&Grv':5,'WdShake':6,'WdShngl':7})\ntrain_data['Exterior1st'] = train_data['Exterior1st'].map({'AsbShng':0,'AsphShn':1,'BrkComm':2,'BrkFace':3,'CBlock':4,'CemntBd':5,'HdBoard':6,'ImStucc':7,'MetalSd':8,\n                                                           'Other':9,'Plywood':10,'PreCast':11,'Stone':12,'Stucco':13,'VinylSd':14,'Wd Sdng':15,'WdShing':16})\ntrain_data['Exterior2nd'] = train_data['Exterior2nd'].map({'AsbShng':0,'AsphShn':1,'Brk Cmn':2,'BrkFace':3,'CBlock':4,'CmentBd':5,'HdBoard':6,'ImStucc':7,'MetalSd':8,\n                                                           'Other':9,'Plywood':10,'PreCast':11,'Stone':12,'Stucco':13,'VinylSd':14,'Wd Sdng':15,'Wd Shng':16})  \n\n","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"7dff3d8ce91fd179ad0e97c27296374f7de323e5","_cell_guid":"06634318-b499-4ca3-9f51-071a15d7696c","scrolled":true},"source":"# Confirm encoding\npd.options.display.float_format = '{:.2f}'.format\nnp.set_printoptions(suppress = False)\ntrain_data.describe().transpose()","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"5c902e056899bee17feee51bba01226d7c2b9787","_cell_guid":"4c7fd877-a029-4dc6-aaf0-8a1300625826"},"source":"# **Target Variable**\nThe independent variable we are trying to predict is SalePrice.  "},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"f720d64c43ef7826e7ae78562ba5ddb1a77311e7","_cell_guid":"2d4696df-4bb6-480e-a9db-d19ea169dac6"},"source":"# Statistical Summary\nprint(\"SalePrice Statistical Summary:\\n\")\nprint(train_data['SalePrice'].describe())\nprint(\"Median Sale Price:\", train_data['SalePrice'].median(axis = 0))\nprint('Skewness:',train_data['SalePrice'].skew())\nskew = train_data['SalePrice'].skew()\n\n# mean distribution\nmu = train_data['SalePrice'].mean()\n\n# std distribution\nsigma = train_data['SalePrice'].std()\nnum_bins = 40\n\n# Histogram of SalesPrice\nplt.figure(figsize=(11, 6))\nn, bins, patches = plt.hist(train_data['SalePrice'], num_bins, normed=1,edgecolor = 'black', lw = 1, alpha = .40)\n\n# Normal Distribution\ny = mlab.normpdf(bins, mu, sigma)\nplt.plot(bins, y, 'r--', linewidth=2)\nplt.xlabel('Sale Price')\nplt.ylabel('Probability density')\n\nplt.title(r'$\\mathrm{Histogram\\ of\\ SalePrice:}\\ \\mu=%.3f,\\ \\sigma=%.3f$'%(mu,sigma))\nplt.grid(True)\n#fig.tight_layout()\nplt.show()","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"5187dc438e566d17e5a256b61693ec6f8c6d42bc","_cell_guid":"d492a90a-7495-4344-bad0-81b7418d6d0c"},"source":"**Target Variable**  \nThe majority of homes are between \\$100k - \\$200k with a median home price of \\$163k.  The distribution appears to have a right skew (mean > median) as a result of outliers above \\$750k.   There are no regression assumptions that require the independent or dependent variables to be normal.  However,  I will take a log of this variable to make the distribution appear more normal (symmetric) to reduce the influence of the outliers in the right tail.  "},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"c1018734489230072452a3a77d45ae46456df9e8","_cell_guid":"cee9a55f-d331-4342-92b1-fe414139f129"},"source":"# Normalize SalePrice using log-transformation\nsale_price_norm = np.log1p(train_data['SalePrice'])\n\n# Mean distribution\nmu = sale_price_norm.mean()\n\n# Standard distribution\nsigma = sale_price_norm.std()\nnum_bins = 40\nplt.figure(figsize=(11, 6))\nn, bins, patches = plt.hist(sale_price_norm, num_bins, normed=1, edgecolor = 'black', lw = 1,alpha = .40)\n\ny = mlab.normpdf(bins, mu, sigma)\nplt.plot(bins, y, 'r--', linewidth=2)\nplt.xlabel('Sale Price')\nplt.ylabel('Probability density')\n\nplt.title(r'$\\mathrm{Histogram\\ of\\ SalePrice:}\\ \\mu=%.3f,\\ \\sigma=%.3f$'%(mu,sigma))\nplt.grid(True)\n#fig.tight_layout()\nplt.show()","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"1fd505bbaefb3d20a2f7635268c57ce001ae0d66","_cell_guid":"86a4ddb2-9e38-4c34-96a4-50fb087e9440"},"source":"---\n# **FEATURE SELECTION IN TRAINING DATA**\nThis dataset contains 76 feature variables (excluding 4 variables that were removed due to excessive missing data). Many of these features can likely be removed as they may not all provide useful information in predicting SalePrice.   \n\nAlso, the variables can be highly correlated with one another and leaving in redundant information will only slow the model down, rather than improving performance.  \n\nAs such, I will perform feature selection to distill the features that contain the most useful information while eliminating as much noice (useless information) as possible. "},{"cell_type":"markdown","metadata":{"_uuid":"4b0d4c0d86458aafe31edee5dac5f3bd4402e683","_cell_guid":"41234dfb-3d99-4362-85ca-e8a8259be28c"},"source":"### **FEATURE SELECTION (REGULARIZATION) IN REGRESSION**\n**Regularization**  \nPower of regualization is that it can automatically do feature selection for you.  ''Regularized regression' automatically penalizes extra features and for features that don't help regression results enough, can set its coefficient to zero.  \n <br/>\n**Normal Linear Regression:**  \nJust wants to minimize SSE and uses all the features made available to it and it'll assign each one a coefficient of regression  <br/><br/>\n**Lasso Regression (Regularized Regression):**   \nIn addition to minimizing SSE, also whats to minimize the number of features so  a penalty parameter is used for additional features. Lasso regression automatically takes in account this penalty parameter and in so doing, it helps identify which features have the most important effect on the regression and eliminate (or set to zero) the coefficients to the features that basically don't help.  <br/><br/>\nIt will try adding features in one at a time and if the new feature does't improve the fit enough to outweigh the penalty term of including that feature then it won't be added (coefficient is set to zero).  The gain in terms of precision/goodness of fit has to be bigger than the loss that I take as a result of having that additional feature in my regression.**"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"a6be6fd0070df6aba0224aa56364380196b123a0","_cell_guid":"fa963d2f-5a0c-4fd7-a72e-54a5e3939b41"},"source":"# Lasso Regression\n\n# Split\n# Create matrix of all x features\nX = train_data.drop(['SalePrice'], axis = 1)\n\n# Create array of target variable\ny = train_data['SalePrice']\n\n# Split training data into train and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .20,random_state = 101)\n\n# Fit\n# Import model\nfrom sklearn.linear_model import Lasso\n\n# Instantiate object\nlasso = Lasso()\n\n# Fit model to training data\nlasso = lasso.fit(X_train, y_train)\n\n# Predict\ny_pred_lasso = lasso.predict(X_test)\n\n# Score It\nfrom sklearn import metrics\nprint('Linear Regression Performance')\nprint('MAE',metrics.mean_absolute_error(y_test, y_pred_lasso))\nprint('MSE',metrics.mean_squared_error(y_test, y_pred_lasso))\nprint('RMSE',np.sqrt(metrics.mean_squared_error(y_test, y_pred_lasso)))\nprint('R^2 =',metrics.explained_variance_score(y_test,y_pred_lasso))\n\n# Lasso Coefficients\npd.set_option('display.float_format', lambda x: '%.2f' % x)\ncdf = pd.DataFrame(data = lasso.coef_,index = X_train.columns, columns = ['Lasso Coefficients'])\n# **RANDOM FOREST**\ncdf.sort_values(by = 'Lasso Coefficients', ascending = False)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"01eb0cc57e97369aecdacac6d3b1b1d3775aa27a","_cell_guid":"f250cf89-1a92-4123-8a17-bdb80854e8d8"},"source":"### **UNIVARIATE FEATURE SELECTION**\nThe linear regression model's performance is sub-optimal at best with R-squared of just 0.64.  This could be an indication that the data is not linearly separable, so I will try univariate feature selection to feed into a non-linear model: Random Forest.  \n\nInstead of using regularized regression, there are several go-to methods for automatically selecting features in sklearn. Many of them fall under the umbrella of 'univariate feature selection', which selects the variables *most* related to the target outcome through univariate statistical tests.  Each feature is treated independently to determine how much power it has in classifying or regressing.  \n\n**SelectPercentile**  \nIf we take the univariate approach of selecting variables based on its level of association with the target, the class SelectPercentile provides an automatic procedure for keeping only a certain percentage of the best, associated features.  \n\nIts metrics for association are:   \n1. **f_regression:** Used only for numeric targets and based on linear regression performance.  \n2. ** f_classif:** Used only for categorical targets, based on Analysis of Variance (ANOVA) statistical test.\n3.  **chi2:** Performs the chi-square statistic for categorical targets, which is less sensible to the nonlinear relationship between the predictive variable and its target.  \n    - When evaluating candidates for a classification problem, f_classif and chi2 tend to provide the same set of top variables. Itâ€™s still a good practice to test the selections from both the association metrics.\n"},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"005ee631330af768a99d4a46ea1546fa6b57aaf2","_cell_guid":"c0089761-bef2-41d9-b6b8-32ef47116d04","scrolled":true},"source":"# Select top 20% of features\n\n# Create matrix of x features\nX = train_data.drop(['SalePrice'],axis = 1)\n\n# Create array of target variable y\ny =train_data['SalePrice']\n\n# Feature Selector\n# Import\nfrom sklearn.feature_selection import SelectPercentile, f_regression\n\n# Instantiate object\nselector_f = SelectPercentile(f_regression, percentile=20)\n\n# Fit and transform\nx_best = selector_f.fit_transform(X, y)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"8a7ba24c808fcd843923c1da547bf2bc28100b85","_cell_guid":"cf95d258-1214-40fe-87ab-01c2ae9f9209"},"source":"**Pareto Approach**  \nIn choosing the percentile cutoff, I took a Pareto 80/20 approach and selected the top 20% best, associated features.  However, I'll review the F-score and p-values to confirm that 20% is an appropriate percentile to exclude a feature from participating in the learning process."},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"3da773c5c1c8dac367fe9d4ffb4d26d86f350a58","_cell_guid":"8311fb9d-b8f1-44df-91a6-547f204ab743"},"source":"support = np.asarray(selector_f.get_support())\n\n# Supress displaying long numbers in scientific notation\n#pd.set_option('display.float_format', lambda x: '%.4f' % x)\n\n# Enable scientific notation\npd.set_option('display.float_format', '{:.2e}'.format)\n\n# Column names of top 20%\nfeatures = np.asarray(X.columns.values)\nfeatures_with_support = features[support]\n# print('Top 20% of the best, associated features to SalePrice\\n',columns_with_support)\n# print('Number of Features:', len(columns_with_support))\n\n#f-scores of top 20%\nfscores = np.asarray(selector_f.scores_)\nfscores_with_support = fscores[support]\n\n# p-values of top 20%\npvalues = np.asarray(selector_f.pvalues_)\npvalues_with_support = pvalues[support]\n\n# Dataframe of top 20%\ntop20 = pd.DataFrame({'F-score':fscores_with_support,\n                      'p-value':pvalues_with_support},\n                     index = features_with_support)\n# top20.index.name = 'Feature'\nprint('Top 20% best associated features to SalePrice\\nNumber of features:',len(features_with_support))\nprint(top20.sort_values(by = 'p-value', ascending = 'True'))\n\n#Print All Selector_f.scores_\n# for n,s in zip(train_data.columns,Selector_f.scores_):\n#      print('F-score: %3.2ft for feature %s ' % (s,n))\n\n# Dataframe of all f-scores\n# fscores = pd.DataFrame(selector_f.scores_,X.columns,['F-score'])\n# fscores.sort_values(by = 'F-score', ascending = False)\n\n# Dataframe of all p-values\n# pscores = pd.DataFrame(selector_f.pvalues_,X.columns, ['P_Value'])\n# pscores.sort_values(by = 'P_Value', ascending = True)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"72d2b7058d0f7268434aeed675a0a3c77c5f2c38","_cell_guid":"456399a1-9ea2-4fcf-aac0-738d2f981fe9"},"source":"**Statistical Signifiance**  \nThe top 20% of features are all statistically significant as their p-values are far under .05.  This further confirms that these features will be good to use in predicting SalePrice.    \n <br/>\n**Alternate Appraoch: Correlations to SalePrice**  \nAn alternate approach would be identify variables with the highest correlation to SalePrice and select based on an specified cutoff (ie. correlations greater than .30 or less than -.30).  This approach would yield similar results but would not provide p-values to gauge each feature's statistical significance.  \n"},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"a86f1db74af5bc30073862a9b7b5e24069450ee5","_cell_guid":"57e2fa96-d3a9-4f81-840e-ed0ef6c7770c","scrolled":true},"source":"# # Correlations to SalePrice\n# corr = train_data.select_dtypes(include = ['float64', 'int64']).iloc[:, 1:].corr()\n# cor_dict = corr['SalePrice'].to_dict()\n# del cor_dict['SalePrice']\n# print(\"List the numerical features in decending order by their correlation with Sale Price:\\n\")\n# for ele in sorted(cor_dict.items(), key = lambda x: -abs(x[1])):\n#     print(\"{0}: \\t{1}\".format(*ele))\n    \n# #Correlation matrix heatmap\n# corrmat = train_data.corr()\n# plt.figure(figsize=(30, 20))\n\n# #number of variables for heatmap\n# k = 76\n# cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n# cm = np.corrcoef(train_data[cols].values.T)\n\n# #generate mask for upper triangle\n# mask = np.zeros_like(cm, dtype=np.bool)\n# mask[np.triu_indices_from(mask)] = True\n\n# sns.set(font_scale=.80)\n# sns.heatmap(cm, mask=mask, cbar=True, annot=True, square=True,\\\n#                  fmt='.2f',annot_kws={'size': 7}, yticklabels=cols.values,\\\n#                  xticklabels=cols.values, cmap = 'coolwarm',lw = .1)\n# plt.show() \n\n# Feature-to-Feature Correlation\n# corr = train_data.drop('SalePrice', axis=1).corr() # We already examined SalePrice correlations\n# plt.figure(figsize=(12, 10))\n\n# sns.heatmap(corr[(corr >= 0.5) | (corr <= -0.4)], \n#             cmap='RdYlGn', vmax=1.0, vmin=-1.0, linewidths=0.1,\n#             annot=True, annot_kws={\"size\": 8}, square=True);","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"3d9d7d6f64656785638f9e28e6517f97540072b6","_cell_guid":"f3ada0d9-0a17-45b8-83c0-97c4ed9433a2"},"source":"**Feature to Feature Correlation**  \nThere are a few variables that are highly correlated (correlation >.80) with one another . The features representing redundant information and are less correlated with SalePrice can also be removed for further dimentionality reduction.\n"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"196302d80de0cb905a95ca1d1cdae55635c675da","_cell_guid":"81ff51b4-03ba-4f0f-b83a-6123b758de4f"},"source":"best_feat = train_data[features_with_support]\ncorr =best_feat.corr() # We already examined SalePrice correlations\nplt.figure(figsize=(12, 10))\n\nsns.heatmap(corr[(corr >= 0.7) | (corr <= -0.7)], \n            cmap='coolwarm', vmax=1.0, vmin=-1.0, linewidths=0.1,\n            annot=True, annot_kws={\"size\": 8}, square=True);","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"fac441da3359ea3806931bad7d4d2716439fbf39","_cell_guid":"7336bf8e-b90a-4815-96b3-8ec01057b9a7"},"source":"# Correlations to SalePrice\nfrom scipy import stats\nprint('Correlation to SalePrice')\nprint('GrLivArea:',stats.pearsonr(best_feat['GrLivArea'],train_data['SalePrice'])[0])\nprint('TotRmsAbvGrd:',stats.pearsonr(best_feat['TotRmsAbvGrd'],train_data['SalePrice'])[0])\nprint('--'*40)\nprint('GarageCars:',stats.pearsonr(best_feat['GarageCars'],train_data['SalePrice'])[0])\nprint('GarageArea:',stats.pearsonr(best_feat['GarageArea'],train_data['SalePrice'])[0])","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"9be3dd571d924937081b4c2e363802dd9e1d9445","_cell_guid":"70f3fa5e-1615-470d-8120-9a78abcf2806"},"source":"**Removing Duplicated Features**    \n <br/>\n**GrLivArea vs TotRmsAbvGrd**  \nThese features are highly correlated with a correlation of 0.83. Of these two variables, TotRmsAbvGrd will be removed because it has a lower correlation with SalePrice.  \n\n**Garage Cars vs Garage Area**  \nGarage Cars and GarageArea are  also highly correlated  as indicated by their correlation coefficient of 0.88. GarageArea will be removed as it has a lower correlation with our target variable, SalePrice."},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"7915c588ed3acd348f19efab73417bb478e53602","_cell_guid":"1b67e48c-87ca-4481-9c2b-4cb46c319858"},"source":"# Remove redundant features\nbest_feat = best_feat.drop(['TotRmsAbvGrd','GarageArea'], axis = 1)\nbest_feat.columns","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"93cef488740df156d7a5a2780b3f4f82774553e0","_cell_guid":"ad80578f-4ee3-421b-885e-ef3774a76cfa"},"source":"## **RANDOM FOREST**\nNow that the best, most correlated features to SalesPrice have been identified.  I will feed these features into a non-linear regression model, Random Forest.  "},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"3f430a13155af5317ea483d27cba0ac17b3938db","_cell_guid":"e69c7c4e-9cce-44ac-86d6-5d7ef9f3f41e"},"source":"# Random Forest Regression with Best Features\n# Split\n# Create matrix of best x features\nX_best = train_data[['OverallQual', 'YearBuilt', 'YearRemodAdd', 'ExterQual', 'BsmtQual', 'TotalBsmtSF', \n               '1stFlrSF', 'GrLivArea', 'FullBath', 'KitchenQual', 'TotRmsAbvGrd', 'FireplaceQu', \n               'GarageFinish', 'GarageCars', 'GarageArea']]\n\n# Create array of target variable\ny = train_data['SalePrice']\n\n# Split training data into train and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_best,y, test_size = .20,random_state = 101)\n\n# Fit\nfrom sklearn.ensemble import RandomForestRegressor\nrforest = RandomForestRegressor(n_estimators = 300, random_state = 0) \nrforest.fit(X_best,y)\n\n# Predict\ny_pred_rforest = rforest.predict(X_test)\n\n# Score It\nfrom sklearn import metrics\nprint('Random Forest Regression Performance')\nprint('MAE',metrics.mean_absolute_error(y_test, y_pred_rforest))\nprint('MSE',metrics.mean_squared_error(y_test, y_pred_rforest))\nprint('RMSE',np.sqrt(metrics.mean_squared_error(y_test, y_pred_rforest)))\nprint('R^2 =',metrics.explained_variance_score(y_test,y_pred_rforest))","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"d8ae7d29ffe7a9bc112fc6dae65105cb4f5f3223","_cell_guid":"97ed72ca-b110-435e-b5ef-013b0cf8ac83"},"source":"---\n# ** KAGGLE SUBMISSION WITH TEST DATA**  \nNow that I've determined the best features and model to use for this data problem, I will apply Random Forest to the test data for the Kaggle submission.\n\n## ** PRE-PROCESSING TEST DATA**\nBut before I can use the test data, I need to perform the same pre-processing procedures and feature engineering used on the training data above. Again, I'll need to address missing data, encoding categorical variables, and dimensionality reduction.  \n"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"79756fed880251cad654be019848ea85746ba8b3","_cell_guid":"9b1c52b7-3cd1-48b6-8297-b07efd832ef9"},"source":"# Load test data\ntest_data = pd.read_csv('../input/test.csv')\n\n# Test data info\ntest_data.info()\n\n# Test data shape\nprint('shape',test_data.shape)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"c4a365fe8622cc0a1b9c5239c828c52a14d14663","_cell_guid":"bd88abe8-4100-47de-be75-04e6184ee1b6"},"source":"### **MISSING DATA**  \nFrom the entry totals above, there is quite a few missing observations in the test set.  A similar strategy used to address missing data in the training set above will be repeated for the test set."},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"1dd9277a75f11a238ed3d9605f749a54af6f353e","_cell_guid":"6a1bffcd-6ba0-44c0-9285-5769753c8c02"},"source":"# Missing Value Count Function\ndef show_missing():\n    missing = test_data.columns[test_data.isnull().any()].tolist()\n    return missing\n\n# Missing data counts and percentage\npd.set_option('display.float_format', lambda x: '%.4f' % x)\nprint('Missing Data Count')\nprint(test_data[show_missing()].isnull().sum().sort_values(ascending = False))\nprint('--'*40)\nprint('Missing Data Percentage')\nprint(round(test_data[show_missing()].isnull().sum().sort_values(ascending = False)/len(test_data)*100,2))","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"d387a7b59c1298dafddcd2f0a80eb6efd501052b","_cell_guid":"0ebc221b-a018-46fe-903b-ac5ae6024e75"},"source":"# Function to impute missing values\ndef feat_impute(column, value):\n    test_data.loc[test_data[column].isnull(),column] = value","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"5831fc07a0416bf8566939e092936d6ffd81f3b9","_cell_guid":"4cfdb16f-85aa-4181-82ea-8e5153badb1f"},"source":"**Missing Over 50%**  \nSimilar to the approach used in the training set, I will remove teatures missing more than 50% o their observations: ''PoolQC','MiscFeature','Alley','Fence' "},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"a61382c67fe243af112d88560156f3d700d3c2ef","_cell_guid":"eb5e2687-c464-46fb-b4c8-c2f6648bb033"},"source":"# Features with over 50% of its observations missings will be removed\ntest_data = test_data.drop(['PoolQC','MiscFeature','Alley','Fence'],axis = 1)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"fec43fdd2d31f777093b54bf0b6613965a942375","_cell_guid":"7d23cd4f-2898-4f9c-a53b-d9bfb5b4a766"},"source":"**FireplaceQu**    \nFireplaces are not considered standard features in a home.  Thus, it is assumed that the nulls in FireplaceQu are associated with homes that do not have fireplaces.  Like the training set, I will confirm this assumption and impute accoridingly.  "},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"145e8dc979f6fa0b6c02b930bc8fc6c1a8ceba9f","_cell_guid":"8cd1bf58-5f87-4c59-a633-c3e6ca83697d"},"source":"# FireplaceQu missing data\nprint('FireplaceQu Missing Before:', test_data['FireplaceQu'].isnull().sum())\nprint('--'*40)\n\n# The null values may be homes that do not have fireplaces at all. Need to check this assumption\nprint(test_data[test_data['FireplaceQu'].isnull()][['Fireplaces','FireplaceQu']])\nprint(test_data[test_data['FireplaceQu'].isnull()][['Fireplaces','FireplaceQu']].shape)\nprint('--'*40)\n\n# Impute the nulls with None \ntest_data['FireplaceQu'] = test_data['FireplaceQu'].fillna('None')\nprint('FireplaceQu Missing After:', test_data['FireplaceQu'].isnull().sum())\n\nprint('--'*40)\n# Cross check columns\nprint('Confirm Imputation')\nprint(pd.crosstab(test_data.FireplaceQu,test_data.Fireplaces))","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"7153352aea6cec17462a11e912ccc6e8b78bea02","_cell_guid":"f499be2a-658a-4f23-8a92-9f30c5795655"},"source":"**LotFrontage**"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"101540fc93dbadb12bb52c38d82e3205ffe88b9f","_cell_guid":"75376200-5049-4460-8fbc-d9aa323f2621"},"source":"# LotFrontage nulls\nprint('LotFrontage Missing Before:', test_data['LotFrontage'].isnull().sum())\n\n# Impute with mean\ntest_data['LotFrontage'] = test_data['LotFrontage'].fillna(test_data['LotFrontage'].median())\nprint('LotFrontage Missing After:', test_data['LotFrontage'].isnull().sum())","outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"_uuid":"2ef4219249ab1341086beb5ad87c996958a91f5d","_cell_guid":"93886033-fe01-4e8f-ac29-b3e4380ea01a"},"source":"**Garage Features**  \nGarages are considered another optional feature in a home.  Thus, the assumption is that nulls in these features correspond to homes that do not have a garage at all.  I will test this assumption and inpute accordingly.  "},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"38b79161b89d94656042ec90481aa0f954302db6","_cell_guid":"d78801d6-61e6-41db-a900-1144e0a8fddd"},"source":"# Garage features null\nprint('Garage Features Missing Before')\nprint(test_data[['GarageYrBlt','GarageCond','GarageQual', 'GarageFinish',\n                 'GarageType','GarageCars','GarageArea']].isnull().sum())\n\n# The null values may be homes that do not have Garages at all.\n# Need to check this assumption and inpute accordingly\nprint('Assumption Check')\nprint(test_data[(test_data['GarageYrBlt'].isnull())|\n                 (test_data['GarageCond'].isnull())|\n                 (test_data['GarageQual'].isnull())|\n                (test_data['GarageFinish'].isnull())|\n                (test_data['GarageType'].isnull())|\n                (test_data['GarageCars'].isnull())|\n                (test_data['GarageArea'].isnull())]\n                 [['GarageYrBlt','GarageCond','GarageQual', 'GarageFinish',\n                 'GarageType','GarageCars','GarageArea']])\n\n# Most of the nulls are associated with homes without a garage.  \n# However, there are exceptions that must be addressed before we can inpute the remaining nulls with 'None'\n# Inpute nulls at index 666 that have a garage with most common value in each column for categorical variables \ntest_data.iloc[666, test_data.columns.get_loc('GarageYrBlt')] = test_data['GarageYrBlt'].mode()[0]\ntest_data.iloc[666, test_data.columns.get_loc('GarageCond')] = test_data['GarageCond'].mode()[0]\ntest_data.iloc[666, test_data.columns.get_loc('GarageFinish')] = test_data['GarageFinish'].mode()[0]\ntest_data.iloc[666, test_data.columns.get_loc('GarageQual')] = test_data['GarageQual'].mode()[0]\ntest_data.iloc[666, test_data.columns.get_loc('GarageType')] = test_data['GarageType'].mode()[0]\n\n# Inpute nulls at index 1116 that have a garage with most common value in each column for categorical variables \ntest_data.iloc[1116, test_data.columns.get_loc('GarageYrBlt')] = test_data['GarageYrBlt'].mode()[0]\ntest_data.iloc[1116, test_data.columns.get_loc('GarageCond')] = test_data['GarageCond'].mode()[0]\ntest_data.iloc[1116, test_data.columns.get_loc('GarageFinish')] = test_data['GarageFinish'].mode()[0]\ntest_data.iloc[1116, test_data.columns.get_loc('GarageQual')] = test_data['GarageQual'].mode()[0]\ntest_data.iloc[1116, test_data.columns.get_loc('GarageType')] = test_data['GarageType'].mode()[0]\n\n# Inpute nulls at index 1116 that have a garage with median value in each column for continuous variables \ntest_data.iloc[1116, test_data.columns.get_loc('GarageCars')] = test_data['GarageCars'].median()\ntest_data.iloc[1116, test_data.columns.get_loc('GarageArea')] = test_data['GarageArea'].median()\n\n# Impute the remaining nulls as None\nnull_garage = ['GarageYrBlt','GarageCond','GarageFinish','GarageQual', \n                 'GarageType','GarageCars','GarageArea']\n\nfor cols in null_garage:\n   if test_data[cols].dtype ==np.object:\n         feat_impute(cols, 'None')\n   else:\n         feat_impute(cols, 0)\n\n# Basement Features After\nprint('--'*40)\nprint('Garage Features Missing After')\nprint(test_data[['GarageYrBlt','GarageCond','GarageQual', 'GarageFinish',\n                 'GarageType','GarageCars','GarageArea']].isnull().sum())\n\nprint('--'*40)\n# Cross check columns\n# print('Confirm Imputation')\n# for cols in null_basement:\n#     print(pd.crosstab(test_data.TotalBsmtSF,test_data[cols]))","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"c34041431329fa6d5b6cda45ad8909301d22d351","_cell_guid":"8382a99a-47a1-4f77-9216-18ad6cf480af"},"source":"**Basement Features**  \nAgain, basement is another feature that is not standard to every home.  I will check to see if the nulls here correspond with homes without a basement.  As there are many basement features missing observations, I address these nulls in two parts for readability.    \n\n**Basment Features Part 1**  \nFirst, I will address basement features that are missing a similar number of observations (43 or 44 observations): BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2'"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"b2da573101d23db863f34352e61e209e2a164c6c","_cell_guid":"ca6a7000-762a-4736-88a9-8002104795fe"},"source":"null_basement1 = ['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2']\n\nprint('Basement Features Part 1 Missing Before')\nfor cols in null_basement:\n    print (cols ,test_data[cols].isnull().sum())\n \n# The null values may be homes that do not have Garages at all.\n# Need to check this assumption against BsmtFinSF1 and inpute accordingly\nprint('Assumption Check')\nprint(test_data[(test_data['BsmtCond'].isnull())|(test_data['BsmtExposure'].isnull())|\n                 (test_data['BsmtQual'].isnull())| (test_data['BsmtFinType1'].isnull())|\n                (test_data['BsmtFinType2'].isnull())]\n                 [['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2']])\n\nprint(test_data[(test_data['BsmtCond'].isnull())|(test_data['BsmtExposure'].isnull())|\n                 (test_data['BsmtQual'].isnull())| (test_data['BsmtFinType1'].isnull())|\n                (test_data['BsmtFinType2'].isnull())]\n                 [['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2']].shape)\n\n# Most of the nulls are associated with homes without a basement.  \n# However, there are exceptions that must be addressed before we can inpute the remaining nulls with 'None'\n# Inpute nulls of BasmtExposure that have a basement with most common value \ntest_data.iloc[27, test_data.columns.get_loc('BsmtExposure')] = test_data['BsmtExposure'].mode()[0]\ntest_data.iloc[888, test_data.columns.get_loc('BsmtExposure')] = test_data['BsmtExposure'].mode()[0]\n\n# Inpute nulls of BsmtCond that have a basement with most common value \ntest_data.iloc[540, test_data.columns.get_loc('BsmtCond')] = test_data['BsmtCond'].mode()[0]\ntest_data.iloc[580, test_data.columns.get_loc('BsmtCond')] = test_data['BsmtCond'].mode()[0]\ntest_data.iloc[725, test_data.columns.get_loc('BsmtCond')] = test_data['BsmtCond'].mode()[0]\ntest_data.iloc[1064, test_data.columns.get_loc('BsmtCond')] = test_data['BsmtCond'].mode()[0]\ntest_data.iloc[1064, test_data.columns.get_loc('BsmtCond')] = test_data['BsmtCond'].mode()[0]\n\n# Inpute nulls in BsmetQualthat have a basement with most common value\ntest_data.iloc[757, test_data.columns.get_loc('BsmtQual')] = test_data['BsmtQual'].mode()[0]\ntest_data.iloc[758, test_data.columns.get_loc('BsmtQual')] = test_data['BsmtQual'].mode()[0]\n\n# Inpute nulls in basement features with 'None' for categorical variables or zero for numeric variables\nfor cols in null_basement1:\n   if test_data[cols].dtype ==np.object:\n         feat_impute(cols, 'None')\n   else:\n         feat_impute(cols, 0)\n        \nprint('Basement Features Part 1 Missing After')\nfor cols in null_basement:\n    print (cols ,test_data[cols].isnull().sum())","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"91f5786a20e276050f912d1b73cea4a21b601406","_cell_guid":"ae2f5460-5dd4-4872-904c-449749913758"},"source":"**Basement Features Part 2**  \nNext, I will address basement features that are only missing a few observations:  'BsmtFullBath','BsmtHalfBath','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF', and 'TotalBsmtSF'"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"a106c97b741192ecfa3327708b939b35899f694f","_cell_guid":"20ba3a1f-daa2-4caa-986d-e3eab8e037e9"},"source":"null_basement2= ['BsmtFullBath','BsmtHalfBath','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF']  \n\n# Need to check that nulls are homes without basements \nprint('Assumption Check')\nprint(test_data[ (test_data['BsmtFullBath'].isnull())|(test_data['BsmtHalfBath'].isnull())|\n                (test_data['BsmtFinSF1'].isnull())|(test_data['BsmtFinSF2'].isnull())|\n                 (test_data['BsmtUnfSF'].isnull())|(test_data['TotalBsmtSF'].isnull())]\n                 [['BsmtFullBath','BsmtHalfBath','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF']])\n\n# Since assumption has been confirmed, impute accordingly\nfor cols in null_basement2:\n   if test_data[cols].dtype ==np.object:\n         feat_impute(cols, 'None')\n   else:\n         feat_impute(cols, 0)\n\n# Basement Features After\nprint('--'*40)\nprint('Basement Features Part 1 Missing After')\nfor cols in null_basement2:\n    print (cols ,test_data[cols].isnull().sum())","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"ec089210f3059de89dbe9b9a76b21210b6c7f63e","_cell_guid":"a2750d55-674e-4909-85fd-6e9fab92c031"},"source":"**Masonry Features**"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"651ce3b7c3c59eda985a498b14e887cf2f3b0100","_cell_guid":"8fb0ee0b-41c5-4891-86ee-c53620e55032"},"source":"null_masonry = ['MasVnrType','MasVnrArea']\nprint('Missing Data Before')\nfor cols in null_masonry:\n    print(cols,test_data[cols].isnull().sum())\n    \n# View nulls in masonry features\nprint('--'*40,'\\nAssumption Check')\nprint(test_data[(test_data['MasVnrType'].isnull())|(test_data['MasVnrType'].isnull())|\n                (test_data['MasVnrArea'].isnull())|(test_data['MasVnrArea'].isnull())]\n                 [['MasVnrType','MasVnrArea']])\n\n# Impute exceptions to assumption that nulls correspond to homes with no exposure\ntest_data.iloc[1150, test_data.columns.get_loc('MasVnrType')] = test_data['MasVnrType'].mode()[0]\n\n# Impute the remaining nulls with 'None' or zero\nfor cols in null_masonry:\n   if test_data[cols].dtype ==np.object:\n         feat_impute(cols, 'None')\n   else:\n         feat_impute(cols, 0)\n        \nprint('--'*40,'\\nMissing Data After')\nfor cols in null_masonry:\n    print(cols,test_data[cols].isnull().sum())","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"5377a4e1aa57a2c4e49418c4968be116322b74e4","_cell_guid":"4dbc8586-2b7a-4913-a648-f72dd7822b46"},"source":"**Other Categorical  Features**  \nThe remaining categorical variables: 'MSZoning', 'Utilities','Functional','Exterior2nd','Exterior1st','SaleType','KitchenQual' are not optional features in a home, so I will impute with their most common values, respectively.  "},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"0d8dce0b823e9ca6c19adf12fe397bae285baee1","_cell_guid":"73ccd91e-49fe-40ae-8aab-d99bbaec7c5c"},"source":"# Impute other categorical features with most frequent value\nnull_others = ['MSZoning', 'Utilities','Functional','Exterior2nd','Exterior1st','SaleType','KitchenQual'] \n\nprint('Missing Data Before')\nfor cols in null_others:\n    print(cols,test_data[cols].isnull().sum())\n\n# Impute with most common value\nfor cols in null_others:\n    test_data[cols] = test_data[cols].mode()[0]\n\nprint('--'*40)\nprint('Missing Data After')\nfor cols in null_others:\n    print(cols,test_data[cols].isnull().sum())","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"c4d18ed9c38dca7953b5f9c1c70060a1a8829554","_cell_guid":"dec0341c-5343-42e1-9180-fb0a9370196c"},"source":"**Other Numeric Features**  \nThe final variable is LotFrontage.  I will impute with the median as this is a continuous variable.  "},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"920305cf71469d7b2b6ca4b652d2ebf03783a5c2","_cell_guid":"c3fb88e5-2db1-4655-b3f5-598a2c517c74"},"source":"# LotFrontage nulls\nprint('LotFrontage Missing Before:', test_data['LotFrontage'].isnull().sum())\n\n# Impute with mean\ntest_data['LotFrontage'] = test_data['LotFrontage'].fillna(test_data['LotFrontage'].median())\nprint('LotFrontage Missing After:', test_data['LotFrontage'].isnull().sum())","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"8b85c419e2885fcffb51ded9cd58cab219ad7e19","_cell_guid":"d1ca57b8-a06d-422e-96ec-848a843af831"},"source":"# Confirm Imputations in test data\ntest_data.info()","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"d3b51ffb8711c2367dff242c337a3e2c4dbb776c","_cell_guid":"a797d80e-0485-4642-9b0c-9612fad180b0"},"source":"## **ENCODING CATEGORICAL FEATURES IN TEST DATA**"},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"9b5c9a5a52506279d4073acdab703237d0d5a592","_cell_guid":"2f1ef640-cd9e-44c0-9219-21c759ff3b8a"},"source":"# Encode ordinal data\ntest_data['LandContour'] = test_data['LandContour'].map({'Low':0,'HLS':1,'Bnk':2,'Lvl':3})\ntest_data['Utilities'] = test_data['Utilities'].map({'NoSeWa':0,'NoSeWa':1,'AllPub':2})\ntest_data['BldgType'] = test_data['BldgType'].map({'Twnhs':0,'TwnhsE':1,'Duplex':2,'2fmCon':3,'1Fam':4})\ntest_data['HouseStyle'] = test_data['HouseStyle'].map({'1Story':0,'1.5Fin':1,'1.5Unf':2,'2Story':3,'2.5Fin':4,'2.5Unf':5,'SFoyer':6,'SLvl':7})\ntest_data['BsmtFinType1'] = test_data['BsmtFinType1'].map({'None':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\ntest_data['BsmtFinType2'] = test_data['BsmtFinType2'].map({'None':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\ntest_data['LandSlope'] = test_data['LandSlope'].map({'Gtl':0,'Mod':1,'Sev':2})\ntest_data['Street'] = test_data['Street'].map({'Grvl':0,'Pave':1})\ntest_data['MasVnrType'] = test_data['MasVnrType'].map({'None':0,'BrkCmn':1,'BrkFace':2,'CBlock':3,'Stone':4})\ntest_data['CentralAir'] = test_data['CentralAir'].map({'N':0,'Y':1})\ntest_data['GarageFinish'] = test_data['GarageFinish'].map({'None':0,'Unf':1,'RFn':2,'Fin':3})\ntest_data['PavedDrive'] = test_data['PavedDrive'].map({'N':0,'P':1,'Y':2})\ntest_data['BsmtExposure'] = test_data['BsmtExposure'].map({'None':0,'No':1,'Mn':2,'Av':3,'Gd':4})\ntest_data['ExterQual'] = test_data['ExterQual'].map({'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntest_data['ExterCond'] = test_data['ExterCond'].map({'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntest_data['BsmtCond'] = test_data['BsmtCond'].map({'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntest_data['BsmtQual'] = test_data['BsmtQual'].map({'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntest_data['HeatingQC'] = test_data['HeatingQC'].map({'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntest_data['KitchenQual'] = test_data['KitchenQual'].map({'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntest_data['FireplaceQu'] = test_data['FireplaceQu'].map({'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntest_data['GarageQual'] = test_data['GarageQual'].map({'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntest_data['GarageCond'] = test_data['GarageCond'].map({'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\n\n# Encode Categorical Variables\ntest_data['Foundation'] = test_data['Foundation'].map({'BrkTil':0,'CBlock':1,'PConc':2,'Slab':3,'Stone':4,'Wood':5})\ntest_data['Heating'] = test_data['Heating'].map({'Floor':0,'GasA':1,'GasW':2,'Grav':3,'OthW':4,'Wall':5})\ntest_data['Electrical'] = test_data['Electrical'].map({'SBrkr':0,'FuseA':1,'FuseF':2,'FuseP':3,'Mix':4})\ntest_data['Functional'] = test_data['Functional'].map({'Sal':0,'Sev':1,'Maj2':2,'Maj1':3,'Mod':4,'Min2':5,'Min1':6,'Typ':7})\ntest_data['GarageType'] = test_data['GarageType'].map({'None':0,'Detchd':1,'CarPort':2,'BuiltIn':3,'Basment':4,'Attchd':5,'2Types':6})\ntest_data['SaleType'] = test_data['SaleType'].map({'Oth':0,'ConLD':1,'ConLI':2,'ConLw':3,'Con':4,'COD':5,'New':6,'VWD':7,'CWD':8,'WD':9})\ntest_data['SaleCondition'] = test_data['SaleCondition'].map({'Partial':0,'Family':1,'Alloca':2,'AdjLand':3,'Abnorml':4,'Normal':5})\ntest_data['MSZoning'] = test_data['MSZoning'].map({'A':0,'FV':1,'RL':2,'RP':3,'RM':4,'RH':5,'C (all)':6,'I':7})\ntest_data['LotConfig'] = test_data['LotConfig'].map({'Inside':0,'Corner':1,'CulDSac':2,'FR2':3,'FR3':4})\ntest_data['Neighborhood'] = test_data['Neighborhood'].map({'Blmngtn':0,'Blueste':1,'BrDale':2,'BrkSide':3, 'ClearCr':4,'CollgCr':5,'Crawfor':6,'Edwards':7,'Gilbert':8,\n                                                             'IDOTRR':9,'MeadowV':10,'Mitchel':11, 'NAmes':12,'NoRidge':13,'NPkVill':14,'NridgHt':15, 'NWAmes':16,\n                                                             'OldTown':17,'SWISU':18,'Sawyer':19, 'SawyerW':20,'Somerst':21,'StoneBr':22,'Timber':23,'Veenker':24})\ntest_data['Condition1'] = test_data['Condition1'].map({'Artery':0,'Feedr':1,'Norm':2,'RRNn':3, 'RRAn':4,'PosN':5,'PosA':6,'RRNe':7,'RRAe':8})\ntest_data['Condition2'] = test_data['Condition2'].map({'Artery':0,'Feedr':1,'Norm':2,'RRNn':3, 'RRAn':4,'PosN':5,'PosA':6,'RRNe':7,'RRAe':8})\ntest_data['RoofStyle'] = test_data['RoofStyle'].map({'Flat':0,'Gable':1,'Gambrel':2,'Hip':3,'Mansard':4,'Shed':5})\ntest_data['RoofMatl'] = test_data['RoofMatl'].map({'ClyTile':0,'CompShg':1,'Membran':2,'Metal':3,'Roll':4,'Tar&Grv':5,'WdShake':6,'WdShngl':7})\ntest_data['Exterior1st'] = test_data['Exterior1st'].map({'AsbShng':0,'AsphShn':1,'BrkComm':2,'BrkFace':3,'CBlock':4,'CemntBd':5,'HdBoard':6,'ImStucc':7,'MetalSd':8,\n                                                           'Other':9,'Plywood':10,'PreCast':11,'Stone':12,'Stucco':13,'VinylSd':14,'Wd Sdng':15,'WdShing':16})\ntest_data['Exterior2nd'] = test_data['Exterior2nd'].map({'AsbShng':0,'AsphShn':1,'Brk Cmn':2,'BrkFace':3,'CBlock':4,'CmentBd':5,'HdBoard':6,'ImStucc':7,'MetalSd':8,\n                                                           'Other':9,'Plywood':10,'PreCast':11,'Stone':12,'Stucco':13,'VinylSd':14,'Wd Sdng':15,'Wd Shng':16})  ","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"d50739a237cbb0234a63ed90e6a104e2f21a5cf2","_cell_guid":"15de9068-dac2-4b3a-93f5-31eb209c454b"},"source":"test_data.describe().transpose()","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"1fc5e81529756cea9c058e307baf4943deb7e3c7","_cell_guid":"8a2d26ba-0bb5-4805-86ab-a550ff79d2dd"},"source":"# Missing data counts and percentage\npd.set_option('display.float_format', lambda x: '%.4f' % x)\nprint('Missing Data Count')\nprint(test_data[show_missing()].isnull().sum().sort_values(ascending = False))\nprint('--'*40)\nprint('Missing Data Percentage')\nprint(round(test_data[show_missing()].isnull().sum().sort_values(ascending = False)/len(test_data)*100,2))\n\nprint(test_data.info())","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"4c9c922afe296385a4b61c3955b6eacf3c6a02fb","_cell_guid":"124340c6-4aa8-45b0-af23-f963c1e1c267"},"source":"## **RANDOM FOREST ON TEST DATA**  \nNow that test data is 'model ready', I will apply a Random Forest model using the best, most associated features to SalePrice that were identified in the training data previously: 'OverallQual', 'YearBuilt', 'YearRemodAdd', 'ExterQual', 'BsmtQual', 'TotalBsmtSF',  '1stFlrSF', 'GrLivArea', 'FullBath', 'KitchenQual', 'TotRmsAbvGrd', 'FireplaceQu',  'GarageFinish', 'GarageCars', 'GarageArea'"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"8cbe4bb29d338b82c35479afb2923d75c5bc76b0","_cell_guid":"364bcb5d-ae71-4f2c-8b6c-fa8a21601e31"},"source":"# Split\n# Create matrix of x features for training data\nX_train2 = train_data[['OverallQual', 'YearBuilt', 'YearRemodAdd', 'ExterQual', 'BsmtQual', 'TotalBsmtSF', \n               '1stFlrSF', 'GrLivArea', 'FullBath', 'KitchenQual', 'TotRmsAbvGrd', 'FireplaceQu', \n               'GarageFinish', 'GarageCars', 'GarageArea']]\n\n# Create target variable array for training data\ny_train2 = train_data['SalePrice']\n\n# Create matrix of x features for test data\nX_test2 = test_data[['OverallQual', 'YearBuilt', 'YearRemodAdd', 'ExterQual', 'BsmtQual', 'TotalBsmtSF', \n               '1stFlrSF', 'GrLivArea', 'FullBath', 'KitchenQual', 'TotRmsAbvGrd', 'FireplaceQu', \n               'GarageFinish', 'GarageCars', 'GarageArea']]\n\n# There is no target variable array for test data\n\n# Confirm data shapes\nprint('Data Shapes')\nprint('x_train shape', X_train2.shape)\nprint('y_train shape',y_train2.shape)\nprint('x_test shape', X_test2.shape)\n\n# Fit Random Forest to training data\nfrom sklearn.ensemble import RandomForestRegressor\nrforest = RandomForestRegressor(n_estimators = 300, random_state = 0) \nrforest.fit(X_train2,y_train2)\n\n# Predict using test data\ny_pred_rforest2 = rforest.predict(X_test2)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"f8933f3e65ad95df8f10d447470f64fecf2412cf","_cell_guid":"b98e536c-c0a4-420c-871d-fa0a1d17cfbc"},"source":"# Create contest submission\nsubmission = pd.DataFrame({\n        \"Id\": test_data[\"Id\"],\n        \"SalePrice\": y_pred_rforest2\n    })\n\nsubmission.to_csv('HousingSubmissionbb.csv', index=False)","outputs":[]}],"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"pygments_lexer":"ipython3","mimetype":"text/x-python","file_extension":".py","version":"3.6.1","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","nbconvert_exporter":"python"}}}