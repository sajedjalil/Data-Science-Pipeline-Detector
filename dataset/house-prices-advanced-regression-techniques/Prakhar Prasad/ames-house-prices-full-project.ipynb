{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summary\n\nIf you like this kernel then please upvote. If you fork it then definitely upvote. You would spot a number of functions and class to deal with the features & usage of pipeline to accidentally get into data leakage especially during cross validation and grid/randomized search. \n\nPlease leave a comment if you have suggestions, feedback. The model can be defintely improved by smart choices of feature engineering. \n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":true},"cell_type":"code","source":"# python version\nimport sys\nassert sys.version_info > (3,5)\n\n# sklearn version\nimport sklearn\nassert sklearn.__version__ > '0.20'\n\n# common imports\nimport os\nimport pandas as pd\nimport numpy as np\n\n#visualization imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# display visuals in the notebook\n%matplotlib inline\n\n# handle internal library warnings\nimport warnings\nwarnings.filterwarnings(action='ignore',message='')\n\n# consistent plot size\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 12,5\nrcParams['xtick.labelsize'] = 12\nrcParams['ytick.labelsize'] = 12\nrcParams['axes.labelsize'] = 12\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load the data"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"house_train_full =pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\nhouse_test =pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 1: Data Exploration"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# view all the columns of the dataframe\npd.options.display.max_columns = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# inspect the first few rows\nhouse_train_full.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"#house_train_full.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*There are too many features in the dataset. Efficient approach to check for the missing values would be to define a function to return the percentage of null values in each predictor*"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# function to return the percentage of null values in a list of features\ndef percent_na(feature, df = house_train_full):\n    for val in feature:\n        return df[feature].isna().sum()/len(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Print the percentage null values of each of the features in the descending order. Below it prints the top 10 features with missing values*"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"percent_na(list(house_train_full.columns),house_train_full).sort_values(ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Four features have more than 80% missing values and FireplaceQu has close to 50% missing values. Generall quality appears to have the maximum missing values, especially for features which are not very common in all houses, e.g fence, pool.*\n* PoolQC - Pool Quality\n* MiscFeature - Contains features not covered in other categories\n* Alley - Type of alley access\n* Fence -  Fence Quality\n* FireplaceQu - Fireplace Quality\n\nLot Frontage has 17% missing values. However it could be a very important predictor for house sale price. \nRest of the features have nominal (<5%) or no missing values.\n \n"},{"metadata":{},"cell_type":"markdown","source":"*Check the distribution of the target value SalePrice in the training set*\n*The distribution of the SalePrice is fairly normal with mean around 200,000. 500,000 and above seems to be outliers at first sight. We will check this using the box plot.*\n"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"plt.hist(house_train_full['SalePrice'],bins=30)\nplt.title('Histogram of House Sale Price')\nplt.xlabel('Sale Price');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# create boxplot of SalePrice\nplt.boxplot(house_train_full['SalePrice'],vert=False)\nplt.title('Boxplot of House Sale Price');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# log transform of the sale price and check the histogram and boxplot\nplt.hist(np.log(house_train_full['SalePrice']),bins=30)\nplt.title('Histogram of House Sale Price (Log transformed)')\nplt.xlabel('Sale Price (log transformed)');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# test for normality of the log transformed sale price\nfrom statsmodels.graphics.gofplots import qqplot\n\nqqplot(np.log(house_train_full['SalePrice']),line='s')\nplt.title('Quantile-Quantile Plot Log Transformed Sale Price');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the Q-Q plot it is clear that the log transformed SalePrice follows normal distribution. Due to outliers, there is a deviation from the normal probability plot. "},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# create boxplot of the log transformed SalePrice\nplt.boxplot(np.log(house_train_full['SalePrice']),vert=False)\nplt.title('Boxplot of House Sale Price (log transformed)');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# function to return the quantile of the numerical feature\ndef quantile_num(df,num_feature,quant):\n    quantiles = []\n    for q in quant:\n        quantiles.append(np.quantile(df[num_feature],q))\n    return quantiles        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"quantile_num(df=house_train_full,num_feature='SalePrice',quant = [0.5,0.75,0.9,0.95,0.99])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*99th quantile is 442567. \n 95th quantile is 326099. \nI will next check what are the houses above these percentiles.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"house_train_full[house_train_full['SalePrice']>np.quantile(house_train_full['SalePrice'],0.99)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"house_train_full.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Above 700,000 is mentioned as Abnormal Sale condition (trade,foreclosure or short sale) and is the only one with a pool and a fence. The zone of this property is Residential Low Density. \nMore information to understand the various attributes is in the data description. So better to read it.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# uncomment to read the file content\n\n#with  open('/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt') as file:\n #   file_content = file.read()\n  #  print (file_content)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading the data description and with a bit of experience, the following categorical features could be important attributes in determining the price of the house. While at this early stage of exploration, it could be all wrong :-)\n- Neighborhood\n- Proximity to various conditions (Condition 1) ... Condition 2 is also very similar. I will check it next\n- MSZoning (zoning classification of the sale , agricultural, high rise etc. ) \n- OverallCond (overall condition of the house)\n- MSSubClass (the type of dwelling involved) "},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the abnormal sales neighborhoods\nhouse_train_full[house_train_full['SaleCondition']=='Abnorml']['Neighborhood'].value_counts().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the neighborhood of the houses in sale price above the 99 percentile\nhouse_train_full[house_train_full['SalePrice']>np.quantile(house_train_full['SalePrice'],0.99)]['Neighborhood'].value_counts().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A more understanding of the neighborhood can be understood from the map of Ames city. Certain neighborhoods are deemed to be more expensive than the other ones. "},{"metadata":{},"cell_type":"markdown","source":"It is very effort intensive to check the value count of each categorical feature one at a time. I will define a function to separate all the categorical features and numerical features and explore them when needed. "},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# function to list all the categorical features\ndef type_features(df,features):\n    cat_features = []\n    num_features = []\n    for feat in features:\n        if df[feat].dtype == 'O':\n            cat_features.append(feat)\n        else:\n            num_features.append(feat)\n    return (cat_features,num_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"categorical_features, numerical_features = type_features(house_train_full,house_train_full.columns)\nlen(categorical_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 43 categorical features in the dataset"},{"metadata":{},"cell_type":"markdown","source":"**Check the correlation of the numerical features with Sale Price of the house**|"},{"metadata":{"trusted":true},"cell_type":"code","source":"house_train_full.corr()['SalePrice'].sort_values(ascending=False).head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"house_train_full.corr()['SalePrice'].sort_values(ascending=False).tail(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\nattributes = ['SalePrice','OverallQual','GrLivArea','GarageCars','GarageArea','TotalBsmtSF']\nscatter_matrix(house_train_full[attributes],figsize=(15,8),grid=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Check the value count of each categorical features*"},{"metadata":{"trusted":true},"cell_type":"code","source":"for features in categorical_features:\n    print (house_train_full[features].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What is interesting is that many of the quality features, for instance BsmtQual (Basement quality) is a categorical feature while the overall quality is a numerical feature. Perhaps good to have all quality labels as categorical."},{"metadata":{},"cell_type":"markdown","source":"*Visualize the sale condition - Normal, Abnormal etc. *"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('SaleCondition',hue='SaleType',data=house_train_full)\nplt.legend(loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"sns.countplot('SaleCondition',data=house_train_full)\nplt.legend(loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is what we have from the sale type\nSaleType: Type of sale\n\t\t\n       WD \tWarranty Deed - Conventional\n       CWD\tWarranty Deed - Cash\n       VWD\tWarranty Deed - VA Loan\n       New\tHome just constructed and sold\n       COD\tCourt Officer Deed/Estate\n       Con\tContract 15% Down payment regular terms\n       ConLw\tContract Low Down payment and low interest\n       ConLI\tContract Low Interest\n       ConLD\tContract Low Down\n       Oth\tOther"},{"metadata":{},"cell_type":"markdown","source":"There is more clarity when I look at the description of the sale condition\nSaleCondition: Condition of sale\n\n       Normal\tNormal Sale\n       Abnorml\tAbnormal Sale -  trade, foreclosure, short sale\n       AdjLand\tAdjoining Land Purchase\n       Alloca\tAllocation - two linked properties with separate deeds, typically condo with a garage unit\t\n       Family\tSale between family members\n       Partial\tHome was not completed when last assessed (associated with New Homes)"},{"metadata":{},"cell_type":"markdown","source":"In order for the model to generalize well, the sample selection for training the model should be the near representative of the data set. Sale condition could be one of the feature to bin the data. However, lets explore the overall quality distribution in the dataset. "},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"sns.countplot('OverallQual',data=house_train_full,palette='viridis')\nplt.title('House Count per Overall Quality');\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, this looks more like a bell shaped with maximum overall quality between 4 to 8. Visualize the Sale Price vs the Overall Quality more closely than done in the scatter matrix"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"sns.swarmplot('OverallQual','SalePrice',data=house_train_full)\nplt.title('Sale Price vs Overall Quality')\nplt.legend(loc='upper right');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Things are becoming little clear now.\n\nMost expensive houses, SalePrice above 99 percentile, all have overall quality rating 8 and above. Most expensive house also has the best quality rating. However, there are also a few with 10 rating but with median price range. The least expensive have ratings 1 to 4. There are more sales of houses with the quality rating between 5 to 9. Both 3 and 4 quality rating have houses with median price range. This could be associated with the neighborhood the houses are located.\n"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"sns.swarmplot('Neighborhood','SalePrice',data=house_train_full[house_train_full['OverallQual']==10])\nplt.title('Houses with Quality Rating 10, SalePrice vs Neighborhood');\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, Edwards locality has least expensive housing with highest quality. Would be interesting to check which sale condition these houses fall into ---> new , renovated etc. \n\nWe will leave it at this point. What is clear and is expected to be true for other quality ratings as well that Neighborhood is an important predictor for Sale Price."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Boxplot SalePrice vs Neighborhood\nplt.figure(figsize=(20,10))\nsns.boxplot('Neighborhood','SalePrice',data=house_train_full,palette='viridis')\nplt.tight_layout(True)\nplt.title('SalePrice vs Neighborhood');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Clearly, neighborhood is a strong predictor of the house price"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Visualize the boxplot sorted by median SalePrice and plotted in descending order\n\n# create the sorted dataframe\ngrouped = house_train_full.groupby(['Neighborhood'])\ndf = pd.DataFrame({col:vals['SalePrice'] for col,vals in grouped})\n\nmeds = df.median()\nmeds.sort_values(ascending=False, inplace=True)\ndf = df[meds.index]\n\n# generate boxplot\nplt.figure(figsize=(20,10))\n\ndf.boxplot(grid=False)\n\nplt.tight_layout(True)\nplt.title('SalePrice vs Neighborhood (sorted based on descending order of median price)')\nplt.xlabel('Ames Neighborhoods')\nplt.ylabel('House Sale Prices');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we see the localities on the x-axis from most expensive to least expensive (left to right) based on the median house price"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Part 2: Data Preparation"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"#create a copy of the original train set\nhousing = house_train_full.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# read the categorical and the numerical features\ncategorical_features, numerical_features = type_features(housing,housing.columns)\nlen(categorical_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"percent_na(list(housing.columns),housing).sort_values(ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# drop list of columns\ndrop_list_1 = ['Id','PoolQC','MiscFeature','Alley','Fence']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# function to drop the columns\ndef drop_feature(df,drop_list):\n    for feature in drop_list:\n        df.drop(feature,axis=1,inplace=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"save_id = house_test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# drop the features from the training set and the test set\ndrop_feature(housing,drop_list_1)\ndrop_feature(house_test,drop_list_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"housing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"#update the feature list\ncategorical_features, numerical_features = type_features(housing,housing.columns)\nlen(categorical_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"OverallQual is a strong predictor of the housing price. It takes the integer values from 1 to 10. It is better to convert this feature into a categorical column."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"num_to_cat_list = ['MSSubClass','OverallQual','OverallCond','YearBuilt','YearRemodAdd','GarageYrBlt','MoSold','YrSold']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"#convert the selected numerical features to categorical\ndef to_category(df,num_list):\n    for feature in num_list:\n        df[feature] = df[feature].astype('str')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"to_category(housing,num_to_cat_list)\nto_category(house_test,num_to_cat_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# update the categorical and the numerical feature list\ncategorical_features,numerical_features = type_features(housing,housing.columns)\nlen(categorical_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"housing['OverallQual'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"In order that the model sees the houses of various quality, I would use Stratified split on the housing dataset into train and validation test set. "},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\nstrat_split = StratifiedShuffleSplit(n_splits=10,test_size=0.2,random_state=42)\nfor train_index,valid_index in strat_split.split(housing,housing['OverallQual']):\n    strat_house_train = housing.loc[train_index]\n    strat_house_valid = housing.loc[valid_index]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"len(strat_house_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"strat_house_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# separate the SalePrice, the final label to be predicted\nX_train = strat_house_train.drop('SalePrice',axis=1)\n#evaluation is based on the log transformed value of the SalePrice\ny_train = np.log(strat_house_train['SalePrice'])\n\nX_valid = strat_house_valid.drop('SalePrice',axis=1)\ny_valid = np.log(strat_house_valid['SalePrice'])\n\nX_test = house_test.copy()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Preparation step sequence\n- create a numerical feature dataset \n- impute the missing values \n- apply transformation on the selected features\n- scale the features\n"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# reapply the function to extract the categorical and the numeric features\ncategorical_features,numerical_features = type_features(X_train,X_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"X_train_num = X_train[numerical_features]\nX_train_num.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"X_train_cat = X_train[categorical_features]\nX_train_cat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"lets deal with the numerical dataset first. I would define a pipeline and apply all the steps on the validation dataset"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"from sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"imputer = KNNImputer()\nX_train_num_imp = imputer.fit_transform(X_train_num)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a custom transformer to log transform the entire numerical features\n"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"scalar = StandardScaler()\nX_train_num_prep = scalar.fit_transform(X_train_num_imp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"X_train_num_prep","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Reconstruct the numerical features dataframe\nX_train_num_prepared = pd.DataFrame(X_train_num_prep,\n                                   columns=list(X_train_num.columns),index=X_train_num.index)\nX_train_num_prepared.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a pipeline for the numerical features. The pipeline would be used to transform the validation & test set "},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\n\nnum_pipeline = Pipeline([('num_imputer',KNNImputer()),\n                         ('std_scalar',StandardScaler()),\n                         ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"hot_encoder = OneHotEncoder(sparse=False)\nord_encoder = OrdinalEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"markdown","source":"before we can impute the missing values in the categorical feature set, the non missing values should be encoded"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# define a function to apply one hot encoder and then impute the missing values\ndef encode(df):\n    # keep only the non null values \n    arr = np.array(df.dropna())\n    # reshape the data for encoding\n    arr_reshape = arr.reshape(-1,1)\n    # encode the data\n    arr_encoded = ord_encoder.fit_transform(arr_reshape)\n    # bring the encoded data back to the df\n    df.loc[df.notnull()] = np.squeeze(arr_encoded)\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"for cat_feature in categorical_features:    \n    encode(X_train_cat[cat_feature])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"X_train_cat.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can use the KNN imputer on the missing values in the categorical dataset"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"cat_imputer = KNNImputer()\nX_train_cat_imp = cat_imputer.fit_transform(X_train_cat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Construct the dataframe with the categorical features"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Reconstruct the categorical features dataframe\nX_train_cat_prepared = pd.DataFrame(X_train_cat_imp,\n                                   columns=list(X_train_cat.columns),index=X_train_cat.index)\nX_train_cat_prepared.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#np.max(X_train_cat_prepared)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Pipeline to deal with the categorical "},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"cat_pipeline = Pipeline([('cat_imputer',cat_imputer),\n                         ('cat_std_sclar',StandardScaler())])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"create a full pipeline for the numerical and categorical features combined. for ithis we will make use of the ColumnTransfomer from Sci-kit learn compose package available in version 0.2 and above"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\n\nnum_attribs = numerical_features\ncat_attribs = categorical_features\n\nfull_pipeline = ColumnTransformer([\n    ('num',num_pipeline,num_attribs),\n    ('cat',cat_pipeline,cat_attribs),\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Summary of the data preparation step applied on the training data \n- train dataset was split into two dataset containing the numerical and categorical features\n- the numerical dataset was imputed first using KNN and then standard scalar was applied \n- the categorical dataset was first encoded using the custom encode function and then imputed using KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_num_prepared.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_cat_prepared.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_prepared = pd.concat([X_train_num_prepared,X_train_cat_prepared],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_prepared.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_num = X_train[numerical_features]\nX_train_cat = X_train[categorical_features]\n\nfor cat_feature in categorical_features:\n    encode(X_train_cat[cat_feature])\n    \nwarnings.filterwarnings(action='ignore',message='')    \n\nX_train_num_prep = num_pipeline.fit_transform(X_train_num)\nX_train_num_prepared = pd.DataFrame(X_train_num_prep,\n                                   columns=list(X_train_num.columns),index=X_train_num.index)\n\nX_train_cat_imp = cat_pipeline.fit_transform(X_train_cat)\nX_train_cat_prepared = pd.DataFrame(X_train_cat_imp,\n                                    columns=list(X_train_cat.columns),index=X_train_cat.index)\n\nX_train_prepared = pd.concat([X_train_num_prepared,X_train_cat_prepared],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare the validation dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_valid_num = X_valid[numerical_features]\nX_valid_cat = X_valid[categorical_features]\n\nfor cat_feature in categorical_features:\n    encode(X_valid_cat[cat_feature])\n    \nX_valid_num_prep = num_pipeline.transform(X_valid_num)\nX_valid_num_prepared = pd.DataFrame(X_valid_num_prep,\n                                   columns=list(X_valid_num.columns),index=X_valid_num.index)\n\nX_valid_cat_imp = cat_pipeline.transform(X_valid_cat)\nX_valid_cat_prepared = pd.DataFrame(X_valid_cat_imp,\n                                    columns=list(X_valid_cat.columns),index=X_valid_cat.index)\n\nX_valid_prepared = pd.concat([X_valid_num_prepared,X_valid_cat_prepared],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_valid_prepared.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Prepare the test dataset\nX_test_num = X_test[numerical_features]\nX_test_cat = X_test[categorical_features]\n\nfor cat_feature in categorical_features:\n    encode(X_test_cat[cat_feature])\n    \nX_test_num_prep = num_pipeline.transform(X_test_num)\nX_test_num_prepared = pd.DataFrame(X_test_num_prep,\n                                   columns=list(X_test_num.columns),index=X_test_num.index)\n\nX_test_cat_imp = cat_pipeline.transform(X_test_cat)\nX_test_cat_prepared = pd.DataFrame(X_test_cat_imp,\n                                   columns=list(X_test_cat.columns),index=X_test_cat.index)\n\nX_test_prepared = pd.concat([X_test_num_prepared,X_test_cat_prepared],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_prepared.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train_prepared.shape,X_valid_prepared.shape,X_test_prepared.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(X_train_prepared.columns)[5:30]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are many columns which could negatively impact the model performance due to multi-collinearity. To deal with it feature engineering and subsequent removal of these feature is helpful. I created a custom class to add additional attributes. There is a lot of scope of improvement as far as feature engineering is concerned. Below is based on the hunch that I get by quickly looking into the feature set. This is exactly where domain experience plays a role. \n\n***This is a late realization and should have been part of the earlier pipeline. This can be done by addressing it earlier. I leave it as it is now for the moment.*** :-)\n\nMy model so far best performed with all the feature set and no removal of the observations from the dataset. However, getting rid of some of the outliers would definitely help. "},{"metadata":{"trusted":true},"cell_type":"code","source":"## Custom Attribute Class Transformer\nfrom sklearn.base import BaseEstimator,TransformerMixin\n\nBsmtFinSF1_idx, BsmtFinSF2_idx ,BsmtUnfSF_idx, TotalBsmtSF_idx = 3,4,5,6\nFstFlrSF_idx, SndFlrSF_idx, GrLivArea_idx = 7,8,10\nBsmtFullBath_idx, BsmtHalfBath_idx = 11,12\nFullBath_idx, HalfBath_idx = 13,14\nBedroomAbvGr_idx, KitchenAbvGr_idx, TotRmsAbvGrd_idx = 15,16,17\nGarageCars_idx, GarageArea_idx = 19,20\nOverallQual_idx, OverallCond_idx =41,42\nBsmtQual_idx, BsmtCond_idx = 53,54 \nGarageQual_idx, GarageCond_idx = 68,69\n\n\n\nclass CustomAttribsAdder(BaseEstimator,TransformerMixin):\n    def __init__(self,trans=True):\n        self.trans = trans\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X):\n        if self.trans:\n            \n            X[:,GrLivArea_idx] = X[:,GrLivArea_idx] * 3 \n            X[:, OverallQual_idx] = X[:, OverallQual_idx] * 3\n            \n            overall_quality = X[:, OverallQual_idx] * X[:, OverallCond_idx]\n            garage_quality = X[:,GarageQual_idx] * X[:,GarageCond_idx]\n            bsmt_quality = X[:,BsmtQual_idx] * X[:,BsmtCond_idx]\n            \n            #garage_area_per_car = X[:, GarageCars_idx] + X[:,GarageArea_idx]\n            \n            rooms_above_ground = X[:,TotRmsAbvGrd_idx] + X[:,KitchenAbvGr_idx] + X[:,BedroomAbvGr_idx]\n            \n            full_sqft = X[:,GrLivArea_idx] + X[:,FstFlrSF_idx] + X[:,SndFlrSF_idx]\n            \n            bsmt_fin = (X[:,BsmtFinSF1_idx] + X[:,BsmtFinSF2_idx]) / X[:,TotalBsmtSF_idx]\n            bsmt_unfin = X[:,BsmtUnfSF_idx] / X[:,TotalBsmtSF_idx]\n            \n            total_bath_grd = X[:,FullBath_idx] + X[:,HalfBath_idx]\n            total_bath_bsmt = X[:,BsmtFullBath_idx] + X[:,BsmtHalfBath_idx]\n            return np.c_[X,overall_quality,garage_quality,bsmt_quality,rooms_above_ground,full_sqft,\n                        bsmt_fin,bsmt_unfin,total_bath_grd,total_bath_bsmt]\n        else:\n            return X\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attr_adder =  CustomAttribsAdder(trans=True)\nX_train_attribs = attr_adder.transform(X_train_prepared.values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_valid_attribs = attr_adder.transform(X_valid_prepared.values)\nX_test_attribs = attr_adder.transform(X_test_prepared.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_prepared = pd.DataFrame(X_train_attribs,\n                                columns=list(X_train_prepared.columns)+['overall_quality','garage_quality','bsmt_quality','rooms_above_ground','full_sqft',\\\n                                                                        'bsmt_fin','bsmt_unfin','total_bath_grd','total_bath_bsmt'],index=X_train_prepared.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_valid_prepared = pd.DataFrame(X_valid_attribs,\n                                columns=list(X_valid_prepared.columns)+['overall_quality','garage_quality','bsmt_quality','rooms_above_ground','full_sqft',\\\n                                                                        'bsmt_fin','bsmt_unfin','total_bath_grd','total_bath_bsmt'],index=X_valid_prepared.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_prepared = pd.DataFrame(X_test_attribs,\n                                columns=list(X_test_prepared.columns)+['overall_quality','garage_quality','bsmt_quality','rooms_above_ground','full_sqft',\\\n                                                                        'bsmt_fin','bsmt_unfin','total_bath_grd','total_bath_bsmt'],index=X_test_prepared.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove the features - try out with various combinations \ndrop_list_2 = ['OverallCond','GarageArea','GarageCond','BsmtCond',\\\n               'BedroomAbvGr','KitchenAbvGr','TotRmsAbvGrd','1stFlrSF','2ndFlrSF',\\\n              'TotalBsmtSF','BsmtUnfSF','HalfBath','BsmtHalfBath',\\\n              'FullBath','BsmtFullBath']\n\n#drop_list_2 = ['YearBuilt', 'OverallCond','GarageCars',\\\n#              'BedroomAbvGr','KitchenAbvGr','1stFlrSF','2ndFlrSF']\n\n# call the function defined earlier to drop the columns from the dataset\ndrop_feature(X_train_prepared,drop_list_2)\ndrop_feature(X_valid_prepared,drop_list_2)\ndrop_feature(X_test_prepared,drop_list_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensemble Model Stacking"},{"metadata":{},"cell_type":"markdown","source":"The brew package has been deprecated. Still I want to try out stacking multiple models. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor,AdaBoostRegressor\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nfrom sklearn.model_selection import cross_val_score, cross_val_predict,RepeatedKFold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to prevent data leakage during cross validation and later during the grid and randomized search, it is important that the validation fold during the cross validation is as good as an unseen data. Else, the estimate of the score would be too optimistic and not reliable. One simple solution is to pass the estimator in cross validation and grid/randomized search via a pipeline. "},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_pipe = Pipeline([('RandomForest',RandomForestRegressor())])\ngb_pipe = Pipeline([('GradientBoost',GradientBoostingRegressor())])\nada_pipe = Pipeline([('AdaBoost',AdaBoostRegressor())])\nsvr_pipe = Pipeline([('SupportVector',SVR())])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    models['RF'] = rf_pipe                                        \n    models['GB'] = gb_pipe\n    models['SVR'] = svr_pipe\n    return models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate a given model using k fold cross validation\ndef evaluate_model(model,X,y):\n    cv = RepeatedKFold(n_splits=10,n_repeats=3,random_state=42)\n    scores = cross_val_score(model,X,y,scoring='neg_mean_squared_error',cv=cv,n_jobs=-1,error_score='raise')\n    return scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compare machine learning models for comparison \nmodels = get_models()\n# evaluate the models and store the results\nresults,names = list(),list()\n\nfor name,model in models.items():\n    scores = evaluate_model(model, X_train_prepared, y_train)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get a stacking ensemble of models\ndef get_stacking():\n    # define the base models\n    level0 =list()\n    level0.append(('RF',RandomForestRegressor(random_state=42)))\n    level0.append(('GB',GradientBoostingRegressor(random_state=42)))\n    level0.append(('AdaBoost',AdaBoostRegressor()))\n    #define the meta learner model \n    level1=LinearRegression()\n    #define the stacking ensemble\n    model = StackingRegressor(estimators=level0,final_estimator=level1,cv=5)\n    return model\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stack_model = get_stacking()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stack_model.fit(X_train_prepared,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_stack_model = stack_model.predict(X_valid_prepared)\nprint(f'RMSE = {np.sqrt(mean_squared_error(y_valid,predictions_stack_model))}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The performance of the stacked model is not better than the individual models. Hence, we would stick with the base models and fine tune them."},{"metadata":{},"cell_type":"markdown","source":"## Train Regression Models"},{"metadata":{},"cell_type":"markdown","source":"### Predict using the default hyper-parameters without any cross validation and fine tuning."},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_reg = RandomForestRegressor(random_state=42) \nrf_reg.fit(X_train_prepared,y_train)\npredictions_rf_reg = rf_reg.predict(X_valid_prepared)\nprint(f'RMSE = {np.sqrt(mean_squared_error(y_valid,predictions_rf_reg))}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_reg = GradientBoostingRegressor(random_state=42)\ngb_reg.fit(X_train_prepared,y_train)\npredictions_gb_reg = gb_reg.predict(X_valid_prepared)\nprint(f'RMSE = {np.sqrt(mean_squared_error(y_valid,predictions_gb_reg))}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ada_reg = AdaBoostRegressor(random_state=42) \nada_reg.fit(X_train_prepared,y_train)\npredictions_ada_reg = ada_reg.predict(X_valid_prepared)\nprint(f'RMSE = {np.sqrt(mean_squared_error(y_valid,predictions_ada_reg))}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Lets try out with Support Vector Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the support vector regressor \nfrom sklearn.svm import SVR \nnp.random.seed(42)\nsvr_reg = SVR()\nsvr_reg.fit(X_train_prepared,y_train)\nvalid_predict_svr_reg = svr_reg.predict(X_valid_prepared)\nprint(f'RMSE = {np.sqrt(mean_squared_error(y_valid,valid_predict_svr_reg))}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Fine Tuning"},{"metadata":{},"cell_type":"markdown","source":"In order to prevent data leakage during cross validation and later during the grid and randomized search, it is important that the validation fold during the cross validation is as good as an unseen data. Else, the estimate of the score would be too optimistic and not reliable. One simple solution is to pass the estimator in cross validation and grid/randomized search via a pipeline. "},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_pipe = Pipeline([('RandomForest',RandomForestRegressor())])\ngb_pipe = Pipeline([('GradientBoost',GradientBoostingRegressor())])\nada_pipe = Pipeline([('AdaBoost',AdaBoostRegressor())])\nsvr_pipe = Pipeline([('SupportVector',SVR())])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross Validation\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\n\nforest_scores = cross_val_score(rf_pipe,X_train_prepared,y_train,\n                               scoring='neg_mean_squared_error',cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\nprint('Cross Validation Random Forest RMSE = {}'.format(forest_rmse_scores))\nprint (f'Mean is {np.mean(forest_rmse_scores)} Std. Deviation = {np.std(forest_rmse_scores)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cross validation on The gradient Boosting regressor\ngb_scores = cross_val_score(gb_pipe,X_train_prepared,y_train,\n                               scoring='neg_mean_squared_error',cv=10)\ngb_rmse_scores = np.sqrt(-gb_scores)\nprint('Cross Validation Gradient Boosting RMSE = {}'.format(gb_rmse_scores))\nprint (f'Mean is {np.mean(gb_rmse_scores)} Std. Deviation = {np.std(gb_rmse_scores)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cross validation on the support vector regressor\nsvm_scores = cross_val_score(svr_pipe,X_train_prepared,y_train,scoring='neg_mean_squared_error',\n                            cv=10)\nsvm_scores = np.sqrt(-svm_scores)\nprint('Cross Validation Support Vector Regression RMSE = {}'.format(svm_scores))\nprint (f'Mean is {np.mean(svm_scores)} Std. Deviation = {np.std(svm_scores)}')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cross validation on the AdaBoost regressor\nada_scores = cross_val_score(ada_pipe,X_train_prepared,y_train,scoring='neg_mean_squared_error',\n                            cv=10)\nada_scores = np.sqrt(-ada_scores)\nprint('Cross Validation AdaBoost Regression RMSE = {}'.format(ada_scores))\nprint (f'Mean is {np.mean(ada_scores)} Std. Deviation = {np.std(ada_scores)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyper parameter tuning"},{"metadata":{},"cell_type":"markdown","source":"First we will try the Grid search for both Random Forest and Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Grid Search\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_reg = GradientBoostingRegressor()\ngb_param_grid = [{'n_estimators':[100,200,300],'max_features':[8,16,32,64],\n                 'max_depth':[3,5,7]}]\n\ngrid_search_gb = GridSearchCV(gb_reg,gb_param_grid,cv=5,scoring='neg_mean_squared_error',\n                              return_train_score=True)\ngrid_search_gb.fit(X_train_prepared,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_best_reg = sklearn.base.clone(grid_search_gb.best_estimator_)\ngb_best_reg.fit(X_train_prepared,y_train)\nvalid_pred_gb = gb_best_reg.predict(X_valid_prepared)\nprint('RMSE={}'.format(np.sqrt(mean_squared_error(y_valid,valid_pred_gb))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_reg = RandomForestRegressor(random_state=42)\nrf_param_grid = [\n    # try 12 (3×4) combinations of hyperparameters\n    {'n_estimators': [100,200,300], 'max_features': [8,16,32,64]},\n    # then try 6 (2×3) combinations with bootstrap set as False\n    {'bootstrap': [False], 'n_estimators': [100, 200], 'max_features': [2, 3, 4]},\n  ]\n\ngrid_search_rf = GridSearchCV(rf_reg,rf_param_grid,cv=5,scoring='neg_mean_squared_error',\n                              return_train_score=True)\ngrid_search_rf.fit(X_train_prepared,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# best estimator based on Grid search\ngrid_search_rf.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the test scores for each of the tried combination\ncvres = grid_search_rf.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the RMSE score on the validation data based on the grid search best estimator\nrf_reg_grid = sklearn.base.clone(grid_search_rf.best_estimator_ )\nrf_reg_grid.fit(X_train_prepared,y_train)\npred_valid_rf_grid = rf_reg_grid.predict(X_valid_prepared)\nprint(f'RMSE = {np.sqrt(mean_squared_error(y_valid,pred_valid_rf_grid))}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance = rf_reg_grid.feature_importances_\n# create a basic plot - improvement , plot tge feature names on the x-axis \nplt.plot(feature_importance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_prepared.columns[65:70]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Principal Component Analysis"},{"metadata":{},"cell_type":"markdown","source":"Dimensionality can be a curse. I tried out reducing it but only to the extent that the variance of the data should be explained by atleast 95%. This way I need not keep playing with arbitrary choice of n_components hyperparameter in PCA. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the pca package\nfrom sklearn.decomposition import PCA\npca = PCA()\npca.fit(X_train_prepared)\ncumsum = np.cumsum(pca.explained_variance_ratio_)\nd = np.argmax(cumsum >= 0.95) + 1\nprint (f'{d}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components = d)\nX_train_reduced = pca.fit_transform(X_train_prepared)\nX_valid_reduced = pca.transform(X_valid_prepared)\nX_test_reduced = pca.transform(X_test_prepared)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_pca = RandomForestRegressor(random_state=42)\nrf_pca.fit(X_train_reduced,y_train)\nvalid_pred = rf_pca.predict(X_valid_reduced)\nprint(f'RMSE on validation data = {np.sqrt(mean_squared_error(y_valid,valid_pred))}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reducing the dataset into its principal components does not seem to be improving the model performance on the validation dataset. Grid search can also be applied on PCA. However, I would leave out this option."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Randomized Grid Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_distribs = {'n_estimators':randint(100,300),\n                 'max_features':randint(16,67),\n                 'max_depth':randint(3,6)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_reg_rand =  RandomForestRegressor(random_state=42)\nrand_search = RandomizedSearchCV(rf_reg_rand,param_distributions=param_distribs,cv=10,\n                                 scoring='neg_mean_squared_error',random_state=42,n_iter=10)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_search.fit(X_train_prepared,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_reg_rand_best = sklearn.base.clone(rand_search.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_reg_rand_best.fit(X_train_prepared,y_train)\nvalid_pred = rf_reg_rand_best.predict(X_valid_prepared)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'RMSE = {np.sqrt(mean_squared_error(y_valid,valid_pred))}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_pred = rf_reg_grid.predict(X_test_prepared)\nprice_arr = np.exp(final_pred)\noutput = pd.DataFrame({\"Id\":save_id['Id'], \"SalePrice\":price_arr})\noutput.to_csv('submission_rf_001.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}