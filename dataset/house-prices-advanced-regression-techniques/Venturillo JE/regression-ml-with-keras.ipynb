{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing and Seeding\nSeeding is important if you want to make a reproducible code. Though there will still be some variations, seeding here will minimize the randomness of each rerun of the kernel."},{"metadata":{"_uuid":"b3fb09b7-aea8-4508-85db-7d9a5ff3d836","_cell_guid":"ab7e5a96-6cac-47b5-9bb6-e440b57c7140","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport time\nimport gc\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom keras.optimizers import Adam\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nseed = random.randint(10, 10000)\nseed = 2546 #From V21 of commit\nprint(\"This run's seed is:\", seed)\n\nnp.random.seed(seed)\nrandom.seed(seed)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Creation\nThe model to be used is a simple regressor model using a Sequential model from Keras. It has three hidden Dense layer of sizes 128, 64 and 16 with activators sigmoid, relu and relu respectively. I also applied Batch Normalization and Dropout to avoid overfitting.\n\nThe loss function to be used is decided to be Mean Square Logarithmic Error or MSLE so that the Y_hat or predicted_value can be minimized. The metrics to be monitored is set to be the Mean Squared Error or MSE of the function. The two are similar but one is on the Logarithmic side while the other is not."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(input_shape):\n    model = Sequential()\n    model.add(Dense(128, input_dim=input_shape, activation='sigmoid'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(64, input_dim=input_shape, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(16, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(1))\n    \n    optimizer = Adam(lr=0.005, decay=0.)\n    model.compile(optimizer=optimizer,\n             loss='msle',\n             metrics=['mse'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Expansion\n\nTo further simplify learning from the data, we would need to do some tweaks to it. One is to fill the N/A parts of the dataset. This can be done mostly by fitting Zero, Mean or Median of the column in its place. For this one, I chose the median for it's versatility in minimizing the amount of extremities.\n\nTo expand the features, I also did a simple boolean columns of form key>N where N is a number between the min and the max of the column."},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_correction_numeric(keys):\n    #We will access the global num_df for this\n    for key in keys:\n        mn, mx = abs(int(num_df[key].min())), abs(int(num_df[key].max()))\n        if mx < mn:\n            mn, mx = mx, mn\n        print(\"Min:\", mn, \"Max:\", mx)\n        try:\n            for suf in range(mn, mx, int((mx-mn)/3)):\n                num_df[key+'>'+str(suf)] = num_df[key].map(lambda x: x>suf)\n                num_df[key+'<'+str(suf)] = num_df[key].map(lambda x: x<suf)\n        except:\n            print(\"ERROR for %s\" %key)\n        x_val = num_df[key].median()\n        num_df[key] = num_df[key].fillna(x_val)\n        num_df[key] = num_df[key]-x_val\n\ndef data_correction_category(df, keys):\n    for key in keys:\n        x_val = 0#df[key].value_counts().median()\n        df[key] = df[key].fillna(x_val)\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n\nHere, we will be fitting our data correction functions to the full train and test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read the input data\ntrain = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\nprint(train.shape, test.shape)\ntrain = train.set_index(\"Id\")\ntest = test.set_index(\"Id\")\n\ntest_index = test.index\n#Clean the train and test data\ncombined = pd.concat([train, test], axis=0, sort=False)\nprint(combined.columns)\n\n#Free some memory\ndel train, test\n\n#Get the log(y) to minimize values\nY = combined[combined[\"SalePrice\"].notnull()][\"SalePrice\"].sort_index().values\nlog_Y = np.log(Y)\n\ndel Y\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_val_list = [\"OverallQual\", \"GrLivArea\", \"YearBuilt\", \"MSSubClass\", \"OverallCond\",\n                    \"GarageCars\", \"LotArea\", \"Fireplaces\", \"LotFrontage\", \"TotRmsAbvGrd\",\n                    \"KitchenAbvGr\", \"FullBath\"]\ncategorical_val_list = [\"BsmtExposure\", \"BsmtFinType1\", \"Neighborhood\", \"BsmtQual\", \"MSZoning\", \"BsmtCond\",\n                        \"Exterior1st\", \"KitchenQual\", \"Exterior2nd\", \"SaleCondition\", \"HouseStyle\",\n                        \"LotConfig\", \"GarageFinish\", \"MasVnrType\", \"RoofStyle\"]\nnum_df = combined[numeric_val_list]\ncat_df = combined[categorical_val_list]\n\n#Cleaning the data\ndata_correction_numeric(numeric_val_list)\ncat_df = data_correction_category(cat_df, categorical_val_list)\n\ncat_df = pd.get_dummies(cat_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Adjustments\n\nHere, we will be adjusting the values of the train and test data once more by fitting them to a scaler."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split Data to train and test\ntrain_c = cat_df[cat_df.index <= 1460] \ntest_c = cat_df[cat_df.index > 1460]\ntrain_n = num_df[num_df.index <= 1460]\ntest_n = num_df[num_df.index > 1460]\n\ndel num_df, cat_df\n\nscale = StandardScaler()\n\ntrain_n = scale.fit_transform(train_n)\ntest_n = scale.fit_transform(test_n)\n\ntrain = np.concatenate((train_n, train_c.values), axis=1)\ntest = np.concatenate((test_n, test_c.values), axis=1)\n\ndel train_c, train_n, test_c, test_n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will also define a plotter so that we can see the train vs validation learning values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for loss\nimport matplotlib.pyplot as plt\ndef plotter(history, n):\n    plt.plot(history.history['mse'])\n    plt.plot(history.history['val_mse'])\n    plt.title('MODEL MSE #%i' %n)\n    plt.ylabel('MSE')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper right')\n    plt.ylim(top=.1, bottom=0.01)\n    plt.savefig('history_mse_{}.png'.format(n))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training\nNow, we fit the training model. I also use some callbacks: ReduceLROnPlateau for slow cooking, and EarlyStopping for, well, early stopping of the training. The patience values are decided after much trial and error, to ensure that there's enough room for adjustment.\n\nAfter that, we train and predict the data over a ten-fold repetition of computation. This will minimize the overfitting of the data and will, hopefully, increase the accuracy of the prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Callbacks\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nlrr = ReduceLROnPlateau(monitor = 'val_mse',\n                         patience = 200,\n                         verbose = 1,\n                         factor = 0.75,\n                         min_lr = 1e-6)\n\nes = EarlyStopping(monitor='val_loss',\n                   mode='min',\n                   verbose=1,\n                   patience=1000,\n                   restore_best_weights=True)\n\nprint(\"Shape of Train:\", train.shape)\npredictions = []\n\nlast_mse = []\nfolds = 10\nfor x in range(1, folds+1):\n    #Separate train data into train and validation data\n    X_train, X_val, Y_train, Y_val = train_test_split(train, log_Y, test_size=0.2, shuffle=True, random_state=seed)\n    print(\"#\"*72)\n    print(\"Current RERUN: #%i\" %(x))\n    #Design the Model\n    model = create_model(X_train.shape[1])\n    #Start the training\n    history=model.fit(X_train, Y_train, validation_data=(X_val, Y_val),\n                  epochs=10000, batch_size=128, verbose=0,\n                 callbacks=[es, lrr])\n    #Predicting\n    predict=model.predict(test)\n    try:\n        predictions = np.concatenate([predictions, predict], axis=1)\n    except:\n        predictions = predict\n    #Show the MSE Plot\n    plotter(history, x)\n    loss, mse = model.evaluate(X_val, Y_val)\n    print(\"Loss:\", loss, \"\\tMSE:\", mse)\n    last_mse.append(mse)\n    #Clear some Memory\n    del X_train, X_val, Y_train, Y_val, model, history, predict, loss, mse\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction\nFinally, we will be saving our prediction. As we did a Ten-Fold prediction, we will be doing a weighted combination based on the MSE evaluation of the validation set for each fold of the training, which was save to the variable `last_mse`. Since the metrics we used is the `error` or the predictions, then, the larger the error, the smaller the effect, thus, I used the equation `(total - x)/((n-1)*total)` so as to reverse the relationship."},{"metadata":{"trusted":true},"cell_type":"code","source":"def ensemble(preds, metrics):\n    over = sum(metrics)\n    n = len(metrics)\n    return [sum((over - metrics[x])*preds[i,x]/((n-1)*over) for x in range(n)) for i in range(len(preds))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicting the Test data...\")\nprediction = ensemble(predictions, last_mse)\nprediction = np.exp(prediction)\nsubmission = pd.DataFrame()\nsubmission['Id'] = test_index\nsubmission['SalePrice'] = prediction\nprint(\"Saving prediction to output...\")\nsubmission.to_csv(\"prediction_regression.csv\", index=False)\nprint(\"Done.\")\n\nprint(submission)\n\nx = np.mean(last_mse)\nprint(x, x**.5)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}