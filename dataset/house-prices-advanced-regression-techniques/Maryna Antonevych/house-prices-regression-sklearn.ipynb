{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# House Prices - Advanced Regression Techniques\nHello everyone! In this notebook we are going to predict House Prices using sklearn library to solve regression task.\n\n\nFor this notebook **I would like to say thank you some authors for their notebooks that have inspired me to write own notebook**:\n1. [PEDRO MARCELINO, PHD. Comprehensive data exploration with Python ](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python)\n2. [SERIGNE. Stacked Regressions : Top 4% on LeaderBoard](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard)\n3. [PRADNESH LACHAKE. Multiple Linear Regression and Regularization](https://www.kaggle.com/pradneshlachake/multiple-linear-regression-and-regularization)\n","metadata":{}},{"cell_type":"markdown","source":"# 1. Import libraries\nFor regression task we are going to use [sklearn](https://scikit-learn.org/stable/) library.\n","metadata":{}},{"cell_type":"code","source":"import math\nimport random\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew \n\n\nimport pickle\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression, Ridge, \\\n                                 RidgeCV, Lasso, LassoCV, \\\n                                 ElasticNet, ElasticNetCV","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Read data\nHere we read train and test sets.","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv', header = 0)\ndf_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head(5) # look for train first 5 rows","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head(5) # look for test first 5 rows","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train.shape)\nprint(df_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Visualize data","metadata":{}},{"cell_type":"markdown","source":"# 2.1. the relation between target column and some other columns","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize = (18,10))\n\nfig.add_subplot(121)\nplt.scatter(x = df_train['GrLivArea'], y = df_train['SalePrice'], color = \"g\", edgecolor = 'k')\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\n\nfig.add_subplot(122)\nplt.scatter(x = df_train['TotalBsmtSF'], y = df_train['SalePrice'], color = \"m\", edgecolor = 'k')\nplt.xlabel(\"TotalBsmtSF\")\nplt.ylabel(\"SalePrice\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, here we can see the relationship of **'SalePrice'** with **'GrLivArea'** and **'TotalBsmtSF'**. The first one looks like linear dependency and the second one is like exponential (or also linear).","metadata":{}},{"cell_type":"markdown","source":"# 2.2. distribution of target 'SalePrice' variable","metadata":{}},{"cell_type":"markdown","source":"Firstly, let's apply **describe()** method to look through the statistics.","metadata":{}},{"cell_type":"code","source":"stats = df_train['SalePrice'].describe()\nstats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After that, we can plot the distribution of the target variable.","metadata":{}},{"cell_type":"code","source":"def plot_distribution(df):\n    fig = plt.figure(figsize = (20,10))\n    df['SalePrice'].plot.kde(color = 'r')\n    df['SalePrice'].plot.hist(density = True, color = 'blue', edgecolor = 'k', bins = 100)\n    plt.legend(['Normal distibution, ($\\mu =${:.2f} and $\\sigma =${:.2f})'.format(stats[1], stats[2])], loc='best')\n    plt.title(\"Frequency distribution plot\")\n    plt.xlabel(\"SalePrice\")\n    # I don't like \"1e6\" number notation, so style will be 'plain'\n    plt.ticklabel_format(style = 'plain', axis = 'y') \n    plt.ticklabel_format(style = 'plain', axis = 'x') \n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_distribution(df_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the [evaluation](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/evaluation) of the code needs to be converted to log, we'll apply it to our target variable:\n*Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally*. It is explained that the target variable is right skewed. As linear models like normally distributed data, we need to transform this variable and make it more normally distributed.","metadata":{}},{"cell_type":"code","source":"df_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])\nplot_distribution(df_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.3. correlation between variables\nHere we calculate the correlation bewween variables.","metadata":{}},{"cell_type":"code","source":"cor_matrix = df_train.corr()\ncor_matrix.style.background_gradient(cmap='coolwarm')","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And, for better unstanding, we can separate the correlation between the target variable and other variables for the better visualizing.","metadata":{}},{"cell_type":"code","source":"cor_matrix2 = cor_matrix[\"SalePrice\"]\ncor_matrix2 = cor_matrix2.to_frame()\ncor_matrix2.style.background_gradient(cmap='coolwarm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.4. pairplots","metadata":{}},{"cell_type":"code","source":"# choosing some columns for plotting pairplots\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF']\npd.plotting.scatter_matrix(df_train[cols], alpha=0.2, figsize=(25, 25), color = 'cyan', edgecolor='k')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Delete NaNs\nWe can check dataframe for missing data (NaNs).","metadata":{}},{"cell_type":"code","source":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'InPercents'])\nmissing_data.head(35).style.background_gradient(cmap='autumn')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's analyse this to understand how to handle the missing data. I'll consider that when more than 8 elements of the data is missing, I will delete the corresponding variable. According to the results above, cells that will be deleted are: 'BsmtFinType1', 'BsmtCond', ..., 'MiscFeature' and 'PoolQC'. And the point is: will we miss this data? I think no, because I don't think that these variables are vety important (in other case, I don't think that there wiuld be so much missing data in important variables).","metadata":{}},{"cell_type":"code","source":"# choosing data, where missed more, than 8 cells\nmask = (missing_data[\"Total\"] > 8)\nmissing_data = missing_data.loc[mask]\n\n# dropping these columns from original datasets\ndf_train = df_train.drop(columns = missing_data.index)\ndf_test = df_test.drop(columns = missing_data.index)\n\n# fill NaNs with \"Unknown\"\ndf_train = df_train.fillna(\"Unknown\") \ndf_test = df_test.fillna(\"Unknown\") \n\nprint(df_train.shape)\nprint(df_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And other NaNs we will replace with **mode** of the column.","metadata":{}},{"cell_type":"code","source":"# fill other NaNs with mode\nfor col in df_train: \n    df_train[col] = df_train[col].replace(\"Unknown\",df_train[col].mode()[0])\nfor col in df_test:\n    df_test[col] = df_test[col].replace(\"Unknown\",df_test[col].mode()[0])\n    \nprint(df_train.shape)\nprint(df_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And here we have the final check of the NaNs. The 'Total' column must have 0 in each cell.","metadata":{}},{"cell_type":"code","source":"# check missing data in df_train after working with missed values\ntotal = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'InPercents'])\nmissing_data.head(5).style.background_gradient(cmap='autumn')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Feature engineering\nNow it's time to prepare data before we will train our regression models. Firstly, let's convert some obvious variables (like 'MoSold') to categorical type. We will make it by convertring data to **\"str\" type.**","metadata":{}},{"cell_type":"code","source":"# convert some vars to categorical features\ndef convert_to_categorical(df):\n    df['MSSubClass'] = df['MSSubClass'].apply(str)\n    df['OverallCond'] = df['OverallCond'].astype(str)\n    # df['YrSold'] = df['YrSold'].astype(str)\n    df['MoSold'] = df['MoSold'].astype(str)\n    \nconvert_to_categorical(df_train)\nconvert_to_categorical(df_test)\n\nprint(df_train.shape)\nprint(df_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For training data, we don't need 'SalePrice', because it is the tagret variable. In addition, we don't need the 'Id' column, because it doesn't influence on the inner properties of the house (like size, color, etc), it is just the number of the house in the dataset.","metadata":{}},{"cell_type":"code","source":"y_train = df_train[\"SalePrice\"].copy()\nx_train = df_train.copy().drop(columns = [\"Id\", \"SalePrice\"])\nx_test = df_test.copy().drop(columns = [\"Id\"])\n\nprint(x_train.shape)\nprint(x_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.head() # check train data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test.head() # check test data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After that, we need to label our string data. We will apply one-hot encoding - **pd.dummies()**, to the categorical variables.\n\nP.S. categorical variables has **'np.object' type**. It is the complex type of the variable.","metadata":{}},{"cell_type":"code","source":"x_all = pd.concat([x_train, x_test])\ncategorical_cols = x_all.select_dtypes(include=np.object).columns\nx_all = pd.get_dummies(x_all, prefix=categorical_cols)\n\nx_train = x_all[:len(x_train)]\nx_test = x_all[len(x_train):]\n\nprint(x_train.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Normalize data\nWe will apply min-max normalization in range [0,1].","metadata":{}},{"cell_type":"code","source":"scaler = MinMaxScaler(feature_range = (0, 1)) # range is [0, 1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we normalize train set.","metadata":{}},{"cell_type":"code","source":"normed = scaler.fit_transform(x_train.copy())\nx_train = pd.DataFrame(data=normed, columns=x_train.columns)\nx_train.head() # check the result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And here we normalize test set.","metadata":{}},{"cell_type":"code","source":"normed = scaler.fit_transform(x_test.copy())\nx_test = pd.DataFrame(data=normed, columns=x_test.columns)\nx_test.head() # check the result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Machine learning (regression)\nHere we will apply [sklearn linear models](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) to regression task. ","metadata":{}},{"cell_type":"code","source":"all_regr_models = [\n    LinearRegression(),\n    Ridge(), \n    RidgeCV(),\n    LassoCV(max_iter=100000),\n    ElasticNetCV()  \n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# there we will store RMSEs\nall_rmse_train = {}\n\n# there we will store accuracies\nall_acc_train = {}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# learn all regressors, write accuracy and save trained models in pickle-format\nfor model in all_regr_models:\n    \n    # get the regressor name \n    model_name = model.__class__.__name__ \n    print(\"â™¦ \", model_name)\n    \n    # train model\n    model.fit(x_train, y_train)\n    \n    # calculate rmse on train set\n    y_train_pred = model.predict(x_train)\n    mse_train = mean_squared_error(y_train, y_train_pred)\n    rmse_train = math.sqrt(mse_train)\n    print(\"- rmse_train =\", round(rmse_train,7))\n    \n    # calculate model accuracy on train set\n    model_acc_train = explained_variance_score(y_train, y_train_pred)\n    print(\"- model_acc_train =\", round(model_acc_train,7))\n    \n    # save its rmse on train set\n    all_rmse_train[model_name] = rmse_train\n    all_acc_train[model_name] = model_acc_train\n    \n    # predict data on test set (result variable for competition)\n    y_test_pred_log1p = model.predict(x_test)\n    y_test_pred = np.expm1(y_test_pred_log1p)\n    \n    # save model\n    filename = model_name + '_model.pickle'\n    pickle.dump(model, open(filename, 'wb'))  \n    \n    # # load model\n    # loaded_model = pickle.load(open(filename, 'rb'))\n    # result = loaded_model.score(x_train, y_train)     \n    \n    \n    # submit prediction for competition\n    if (model_name == 'LassoCV'):\n        submission = pd.DataFrame({'Id': df_test['Id'], 'SalePrice': y_test_pred})\n        submission.to_csv('submission.csv', index=False)\n        print('! submission is successful')\n        \n    print()\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the best results were shown by LinearRegression and LassoCV. I decided to save LassoCV results as my submition (because for LinearRegression I achieved ~ 0.14 RMSE for test set in the Leaderboard and for LassoCV I achieved ~0.13 RMSE).","metadata":{}},{"cell_type":"markdown","source":"# 7. Visualize results","metadata":{}},{"cell_type":"code","source":"def visualize_results(fig, subplot_id, sort_order, all_list, color_map, title, y_label):\n    # sort from biggest to smallest\n    all_list = dict(sorted(all_list.items(), key=lambda item: item[1], reverse=sort_order))\n\n    # get keys and values as parameters to build plot\n    keys = all_list.keys()\n    values = all_list.values()\n\n    # color map for bar chart\n    color = color_map(np.linspace(0, 1, len(keys)))\n\n    # plot\n    fig.add_subplot(subplot_id)\n    plt.title(title)\n    plt.xlabel('regressors')\n    plt.ylabel(y_label)\n    plt.bar(keys, values, color=color)\n    plt.xticks(rotation = 'vertical')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(15, 5))\nvisualize_results(fig, 121, True, all_acc_train, plt.cm.jet, 'ACCURACY OF REGRESSORS', 'accuracy')\nvisualize_results(fig, 122, False, all_rmse_train, plt.cm.copper, 'RMSE OF REGRESSORS', 'RMSE')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Conclusion\nThank you for reading my new article! Hope, you liked it and it was interesting for you! There are some more my articles:\n* [Automobile Customer Clustering (K-means & PCA)](https://www.kaggle.com/maricinnamon/automobile-customer-clustering-k-means-pca)\n* [Credit Card Fraud detection sklearn](https://www.kaggle.com/maricinnamon/credit-card-fraud-detection-sklearn)\n* [Market Basket Analysis for beginners](https://www.kaggle.com/maricinnamon/market-basket-analysis-for-beginners)\n* [Neural Network for beginners with keras](https://www.kaggle.com/maricinnamon/neural-network-for-beginners-with-keras)\n* [Fetal Health Classification for beginners sklearn](https://www.kaggle.com/maricinnamon/fetal-health-classification-for-beginners-sklearn)\n* [Retail Trade Report Department Stores (LSTM)](https://www.kaggle.com/maricinnamon/retail-trade-report-department-stores-lstm)","metadata":{}}]}