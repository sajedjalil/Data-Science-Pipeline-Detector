{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n## Display all the columns of the dataframe\n\npd.pandas.set_option('display.max_columns',None)\ntrain = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets explore what we have to predict","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['SalePrice'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Skewness = \", train['SalePrice'].skew())\nprint(\"Kurtosis = \", train['SalePrice'].kurtosis())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nfrom scipy.stats import norm, skew\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.distplot(train['SalePrice'], fit=norm)\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint('\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be clearly seen that the data is right skewed. Regression model performs better for normally distributed data.\nAs it can be seen that lower values and higher values makes our data deviating.So lets make variable trandform which can diminish this difference.\nLog Transform seems viable for our purpose here","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain.loc[:, \"SalePrice\"] = np.log1p(train.loc[:, \"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be clearly seen that our data is more close to normal distribution now.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Removing Outliers as suggested by  Dean De Cook author of Ames house dataset**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" Dean De Cook author of Ames house dataset recommended the removal of some 5 outliers representing unsual sales in GrLivArea greater than 4000 square feet. Ref: www.amstat.org/publications/jse/v19n3/decock.pdf","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.iloc[np.where(train.GrLivArea > 4000)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.iloc[np.where(test.GrLivArea > 4000)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.grid()\nscatter = sns.regplot(x='GrLivArea', y='SalePrice', fit_reg =False, data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2 of the houses are outliers, large houses prized relatively low, while the 2 on top in the scatter are very large houses with commensurate sales. The 2 outliers in the train set will be removed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping the outliers in the train set\ntrain = train.drop(train[train['Id'] == 524].index)\ntrain = train.drop(train[train['Id'] == 1299].index)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Combining test and train data**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"to do one hot encoding of categorical features we are combining train and test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ntrain = train.shape[0]\nntest = test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combining train and test values\ndata = pd.concat((train, test)).reset_index(drop=True)\ntrain_y = pd.DataFrame(train.SalePrice)\nID = data['Id']\ndata.drop('Id', axis = 1, inplace = True)\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(['SalePrice'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now lets handle missing values**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# determine the threshold for missing values\ndef percent_missing(df):\n    data = pd.DataFrame(df)\n    df_cols = list(pd.DataFrame(data))\n    dict_x = {}\n    for i in range(0, len(df_cols)):\n        dict_x.update({df_cols[i]: round(data[df_cols[i]].isnull().mean()*100,2)})\n    \n    return dict_x\n\nmissing = percent_missing(data)\ndf_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\nprint('Percent of missing data')\ndf_miss[0:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Handle missing values for features where median/mean or most common value doesn't make sense\n\n# Alley : data description says NA means \"no alley access\"\ndata.loc[:, \"Alley\"] = data.loc[:, \"Alley\"].fillna(\"None\")\n# BedroomAbvGr : NA most likely means 0\ndata.loc[:, \"BedroomAbvGr\"] = data.loc[:, \"BedroomAbvGr\"].fillna(0)\n# BsmtQual etc : data description says NA for basement features is \"no basement\"\ndata.loc[:, \"BsmtQual\"] = data.loc[:, \"BsmtQual\"].fillna(\"No\")\ndata.loc[:, \"BsmtCond\"] = data.loc[:, \"BsmtCond\"].fillna(\"No\")\ndata.loc[:, \"BsmtExposure\"] = data.loc[:, \"BsmtExposure\"].fillna(\"No\")\ndata.loc[:, \"BsmtFinType1\"] = data.loc[:, \"BsmtFinType1\"].fillna(\"No\")\ndata.loc[:, \"BsmtFinType2\"] = data.loc[:, \"BsmtFinType2\"].fillna(\"No\")\ndata.loc[:, \"BsmtFullBath\"] = data.loc[:, \"BsmtFullBath\"].fillna(0)\ndata.loc[:, \"BsmtHalfBath\"] = data.loc[:, \"BsmtHalfBath\"].fillna(0)\ndata.loc[:, \"BsmtUnfSF\"] = data.loc[:, \"BsmtUnfSF\"].fillna(0)\ndata.loc[:, \"BsmtFinSF1\"] = data.loc[:, \"BsmtUnfSF\"].fillna(0)\ndata.loc[:, \"BsmtFinSF2\"] = data.loc[:, \"BsmtUnfSF\"].fillna(0)\ndata.loc[:, \"BsmtFinSF2\"] = data.loc[:, \"BsmtUnfSF\"].fillna(0)\ndata.loc[:, \"TotalBsmtSF\"] = data.loc[:, \"BsmtUnfSF\"].fillna(0)\n# CentralAir : NA most likely means No\ndata.loc[:, \"CentralAir\"] = data.loc[:, \"CentralAir\"].fillna(\"N\")\n# Condition : NA most likely means Normal\ndata.loc[:, \"Condition1\"] = data.loc[:, \"Condition1\"].fillna(\"Norm\")\ndata.loc[:, \"Condition2\"] = data.loc[:, \"Condition2\"].fillna(\"Norm\")\n# EnclosedPorch : NA most likely means no enclosed porch\ndata.loc[:, \"EnclosedPorch\"] = data.loc[:, \"EnclosedPorch\"].fillna(0)\n# External stuff : NA most likely means average\ndata.loc[:, \"ExterCond\"] = data.loc[:, \"ExterCond\"].fillna(\"TA\")\ndata.loc[:, \"ExterQual\"] = data.loc[:, \"ExterQual\"].fillna(\"TA\")\ndata.loc[:, \"Exterior1st\"] = data.loc[:, \"Exterior1st\"].fillna(\"None\")\ndata.loc[:, \"Exterior2nd\"] = data.loc[:, \"Exterior2nd\"].fillna(\"None\")\n# Fence : data description says NA means \"no fence\"\ndata.loc[:, \"Fence\"] = data.loc[:, \"Fence\"].fillna(\"No\")\n# FireplaceQu : data description says NA means \"no fireplace\"\ndata.loc[:, \"FireplaceQu\"] = data.loc[:, \"FireplaceQu\"].fillna(\"No\")\ndata.loc[:, \"Fireplaces\"] = data.loc[:, \"Fireplaces\"].fillna(0)\n# Functional : data description says NA means typical\ndata.loc[:, \"Functional\"] = data.loc[:, \"Functional\"].fillna(\"Typ\")\n# GarageType etc : data description says NA for garage features is \"no garage\"\ndata.loc[:, \"GarageType\"] = data.loc[:, \"GarageType\"].fillna(\"No\")\ndata.loc[:, \"GarageFinish\"] = data.loc[:, \"GarageFinish\"].fillna(\"No\")\ndata.loc[:, \"GarageQual\"] = data.loc[:, \"GarageQual\"].fillna(\"No\")\ndata.loc[:, \"GarageCond\"] = data.loc[:, \"GarageCond\"].fillna(\"No\")\ndata.loc[:, \"GarageYrBlt\"] = data.loc[:, \"GarageYrBlt\"].fillna(0)\ndata.loc[:, \"GarageArea\"] = data.loc[:, \"GarageArea\"].fillna(0)\ndata.loc[:, \"GarageCars\"] = data.loc[:, \"GarageCars\"].fillna(0)\n# HalfBath : NA most likely means no half baths above grade\ndata.loc[:, \"HalfBath\"] = data.loc[:, \"HalfBath\"].fillna(0)\n# HeatingQC : NA most likely means typical\ndata.loc[:, \"HeatingQC\"] = data.loc[:, \"HeatingQC\"].fillna(\"TA\")\n# KitchenAbvGr : NA most likely means 0\ndata.loc[:, \"KitchenAbvGr\"] = data.loc[:, \"KitchenAbvGr\"].fillna(0)\n# KitchenQual : NA most likely means typical\ndata.loc[:, \"KitchenQual\"] = data.loc[:, \"KitchenQual\"].fillna(\"TA\")\n# LotFrontage : NA most likely means no lot frontage\ndata.loc[:, \"LotFrontage\"] = data.loc[:, \"LotFrontage\"].fillna(0)\n# LotShape : NA most likely means regular\ndata.loc[:, \"LotShape\"] = data.loc[:, \"LotShape\"].fillna(\"Reg\")\n# MasVnrType : NA most likely means no veneer\ndata.loc[:, \"MasVnrType\"] = data.loc[:, \"MasVnrType\"].fillna(\"None\")\ndata.loc[:, \"MasVnrArea\"] = data.loc[:, \"MasVnrArea\"].fillna(0)\n# MiscFeature : data description says NA means \"no misc feature\"\ndata.loc[:, \"MiscFeature\"] = data.loc[:, \"MiscFeature\"].fillna(\"No\")\ndata.loc[:, \"MiscVal\"] = data.loc[:, \"MiscVal\"].fillna(0)\n# OpenPorchSF : NA most likely means no open porch\ndata.loc[:, \"OpenPorchSF\"] = data.loc[:, \"OpenPorchSF\"].fillna(0)\n# PavedDrive : NA most likely means not paved\ndata.loc[:, \"PavedDrive\"] = data.loc[:, \"PavedDrive\"].fillna(\"N\")\n# PoolQC : data description says NA means \"no pool\"\ndata.loc[:, \"PoolQC\"] = data.loc[:, \"PoolQC\"].fillna(\"No\")\ndata.loc[:, \"PoolArea\"] = data.loc[:, \"PoolArea\"].fillna(0)\n# SaleCondition : NA most likely means normal sale\ndata.loc[:, \"SaleCondition\"] = data.loc[:, \"SaleCondition\"].fillna(\"Normal\")\n# ScreenPorch : NA most likely means no screen porch\ndata.loc[:, \"ScreenPorch\"] = data.loc[:, \"ScreenPorch\"].fillna(0)\n# TotRmsAbvGrd : NA most likely means 0\ndata.loc[:, \"TotRmsAbvGrd\"] = data.loc[:, \"TotRmsAbvGrd\"].fillna(0)\n# Utilities : NA most likely means all public utilities\ndata.loc[:, \"Utilities\"] = data.loc[:, \"Utilities\"].fillna(\"AllPub\")\n# WoodDeckSF : NA most likely means no wood deck\ndata.loc[:, \"WoodDeckSF\"] = data.loc[:, \"WoodDeckSF\"].fillna(0)\n# Electrical: NA not explicitly assigned in the data description will be filled with the mode in the Neighborhood\ndata['Electrical'] = data.groupby(['Neighborhood','MSSubClass' ])['Electrical'].apply(lambda x: x.fillna(x.value_counts().index[0]))\n# MSZoning: NA not explicitly assigned in the data description will be filled with the mode\ndata['MSZoning'] = data['MSZoning'].fillna(data['MSZoning'].mode()[0])\n# SaleType : NA most likely means Other\ndata.loc[:, \"SaleType\"] = data.loc[:, \"SaleType\"].fillna(\"Oth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's make sure we handled all the missing values\nmissing = percent_missing(train)\ndf_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\nprint('Percent of missing data')\ndf_miss[0:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Extraction**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Feature Extraction:\n1.)Create new features from existing features. \n2.)There are 2 types of categorical variables, nominal and ordinal. The ordinal variables show some rank and will be encoded with numeric values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation:  Its the most basic way to find relation between any two quantities.\ncorr = train.corr()\n\nplt.figure(figsize=(15,15))\n\nsns.heatmap(corr,vmax=0.9,square=True)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Top co_related features to sales price are: GrLivArea(Highly correlated),OverallQual(Higly corelated), TotRmsAbvGrd, GarageYrBlt and YearBuilt, 1stFlrSF and TotalBsmtSF,GarageArea and GarageCars etc.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation\ncorr = train.corr()\n# sort in descending order\ncorr_top = corr['SalePrice'].abs().sort_values(ascending=False)[:15]#getting top 15 features\n#.abs() is necesasry to get both strong positive and strong negative correlation\ntop_features = corr_top.index[1:]\n\ncorr_top","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top features and SalePrice\nfig,ax=plt.subplots(nrows=14,ncols=1,figsize=(6,30))\nfor i in range(len(top_features)):    \n\n    ax[i].scatter(x=train[top_features[i]], y=train['SalePrice'])\n    ax[i].set_xlabel('%s'%(top_features[i]))\n    ax[i].set_ylabel('SalePrice')\n\nplt.tight_layout()\nplt.savefig('./Top_featuresvsSalePrice.jpg',dpi=300,bbox_inches='tight')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_cols = ['GrLivArea','GarageArea','TotalBsmtSF','1stFlrSF','YearBuilt']\nnominal_cols = ['OverallQual','GarageCars','FullBath','TotRmsAbvGrd']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# impute ordinal data with numeric values\ndata['KitchenQual'].replace(['Ex','Gd','TA','Fa'],[4,3,2,1],inplace=True)\ndata['FireplaceQu'].replace(['Ex','Gd','TA','Fa','Po', 'No'],[6,5,4,3,2,1],inplace=True)\ndata['GarageQual'].replace(['Ex','Gd','TA','Fa','Po','No'],[6,5,4,3,2,1],inplace=True)\ndata['GarageCond'].replace(['Ex','Gd','TA','Fa','Po','No'],[6,5,4,3,2,1],inplace=True)\ndata['PoolQC'].replace(['Ex','Gd','TA','Fa','No'],[5,4,3,2,1],inplace=True)\ndata['ExterQual'].replace(['Ex','Gd','TA','Fa'],[4,3,2,1],inplace=True)\ndata['ExterCond'].replace(['Ex','Gd','TA','Fa','Po'],[5,4,3,2,1],inplace=True)\ndata['BsmtQual'].replace(['Ex','Gd','TA','Fa','Po','No'],[6,5,4,3,2,1],inplace=True)\ndata['BsmtCond'].replace(['Ex','Gd','TA','Fa','Po','No'],[6,5,4,3,2,1],inplace=True)\ndata['BsmtExposure'].replace(['Gd','Av','Mn','No','None'],[5,4,3,2,1],inplace=True)\ndata['HeatingQC'].replace(['Ex','Gd','TA','Fa','Po'],[5,4,3,2,1],inplace=True)\n# transform discrete features to  categorical feature\ndata['MSSubClass'] = data['MSSubClass'].astype(str)\ndata['YrSold'] = data['YrSold'].astype(str)   \ndata['MoSold'] = data['MoSold'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"some features which are related to each other can be combined together","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# combinations of old features\ndata['GarageScale'] = data['GarageCars'] * data['GarageArea']\n\ndata['GarageOrdinal'] = data['GarageQual'] + data['GarageCond']\ndata['AllPorch'] = data['OpenPorchSF'] + data['EnclosedPorch'] + data['3SsnPorch'] + data['ScreenPorch']\ndata['ExterOrdinal'] = data['ExterQual'] + data['ExterCond']\ndata['KitchenCombined'] = data['KitchenQual'] * data['KitchenAbvGr']\ndata['FireplaceCombined'] = data['FireplaceQu'] * data['Fireplaces']\ndata['BsmtOrdinal'] = data['BsmtQual'] + data['BsmtCond']\ndata['BsmtFinishedAll'] = data['BsmtFinSF1'] + data['BsmtFinSF2']\ndata['AllFlrSF'] = data['1stFlrSF'] + data['2ndFlrSF']\ndata['OverallCombined'] = data['OverallQual'] + data['OverallCond']\ndata['TotalFullBath'] = data['BsmtFullBath'] +  + data[\"FullBath\"] \ndata['TotalHalfBath'] = data[\"HalfBath\"] + data['BsmtHalfBath']\ndata['TotalSF'] = data['AllFlrSF'] + data['TotalBsmtSF']\ndata['YrBltAndRemod'] = data[\"YearRemodAdd\"] + data['YearBuilt']\ndata=data.drop(['GarageCars','GarageArea','GarageQual','GarageCond','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','ExterQual','ExterCond','KitchenQual','KitchenAbvGr','FireplaceQu','Fireplaces','BsmtQual','BsmtCond','BsmtFinSF1',\n               'BsmtFinSF2','1stFlrSF','2ndFlrSF','OverallQual','OverallCond','BsmtFullBath',\"FullBath\",\"HalfBath\",'BsmtHalfBath','AllFlrSF','TotalBsmtSF',\n               \"YearRemodAdd\",'YearBuilt'],axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Skewed Features**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_feats = data.dtypes[data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Box Cox Transformation of (highly) skewed features\n\nWe use the scipy function boxcox1p which computes the Box-Cox transformation of  1+x .\n\nNote that setting  Î»=0  is equivalent to log1p used above for the target variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    data[feat] = boxcox1p(data[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Categorical encoding** Lets do one hot encoding of categorical features now","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.get_dummies(data)\nprint(data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's make sure we handled all the missing values\nmissing = percent_missing(data)\ndf_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\nprint('Percent of missing data')\ndf_miss[0:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Features with zero values that can be described as almost 100% can cause overfitting and will be dropped","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"overfit = []\nfor i in data.columns:\n    counts = data[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros / len(data) * 100 > 99.94:\n        overfit.append(i)\n\noverfit = list(overfit)\ndata = data.drop(overfit, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = pd.concat([ID[:ntrain], data[:ntrain]], axis = 1)\ntest_x = pd.concat([ID[ntrain:], data[ntrain:]], axis = 1)\nprint(\"Train:\", train_x.shape)\nprint(\"Test:\",test_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\nrs = RobustScaler()\nrs.fit(train_x)\ntrain_x = rs.transform(train_x)\n# train_x = pd.DataFrame(train_x, columns = index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import make_scorer, mean_squared_error\n\ndef mean_squared_error_(ground_truth, predictions):\n    return mean_squared_error(ground_truth, predictions) ** 0.5\nRMSE = make_scorer(mean_squared_error_, greater_is_better=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1.) LINEAR REGRESSION**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#LETS MAKE TRAINING AND VALIDATION SET","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting the dataset as training and Validation dataset\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(train_x, train_y, test_size = 0.2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#building the model\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\n#Accuracy\nprint(\"R-Squared Value for Training Set: {:.3f}\".format(linreg.score(X_train, y_train)))\nprint(\"R-Squared Value for Validation Set: {:.3f}\".format(linreg.score(X_val, y_val)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The train log RMSE is {:.4f}'.format(mean_squared_error_(linreg.predict(X_train), y_train)))\nprint('The validation Log RMSE is {:.4f}'.format(mean_squared_error_(linreg.predict(X_val), y_val)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2.) Ridge Regression**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\nridge = Ridge()\nridge.fit(X_train, y_train)\n\nprint('R-squared score (training): {:.3f}'.format(ridge.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(ridge.score(X_val, y_val)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The train log RMSE is {:.4f}'.format(mean_squared_error_(ridge.predict(X_train), y_train)))\nprint('The validation Log RMSE is {:.4f}'.format(mean_squared_error_(ridge.predict(X_val), y_val)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3.)LASSO Regression**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\nlasso = Lasso(max_iter = 1000)\nlasso.fit(X_train, y_train)\n\nprint('R-squared score (training): {:.3f}'.format(lasso.score(X_train, y_train)))\nprint('R-squared score (validation): {:.3f}'.format(lasso.score(X_val, y_val)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The train log RMSE is {:.4f}'.format(mean_squared_error_(lasso.predict(X_train), y_train)))\nprint('The validation Log RMSE is {:.4f}'.format(mean_squared_error_(lasso.predict(X_val), y_val)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4.)Random Forest Regression**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom time import time\n\nclf = RandomForestRegressor()\n\nparam_grid = {'min_samples_split': [2, 10, 50, 100],\n              'min_samples_leaf': [1, 10, 50, 100],\n              'n_estimators': [100, 500, 1000],\n              'max_depth': [1, 5, 10, None]\n             }\n\n# run grid search\ngrid_search = GridSearchCV(clf, param_grid=param_grid, cv=3, scoring= 'neg_mean_squared_error', n_jobs = 6, verbose=True)\nstart = time()\ngrid_search.fit(X_train, y_train)\n\nprint(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n      % (time() - start, len(grid_search.cv_results_['params'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_score_, grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor(max_depth = 10, min_samples_split= 2, min_samples_leaf= 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('R-squared score (training): {:.3f}'.format(rf.score(X_train, y_train)))\nprint('R-squared score (validation): {:.3f}'.format(rf.score(X_val, y_val)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The train log RMSE is {:.4f}'.format(mean_squared_error_(rf.predict(X_train), y_train)))\nprint('The validation Log RMSE is {:.4f}'.format(mean_squared_error_(rf.predict(X_val), y_val)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = rf.predict(test_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = np.expm1(y_pred)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**5.)Gradient Boost Regression**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\nclf = GradientBoostingRegressor()\n\nparam_grid = {'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [3, 7, 10],\n              'min_samples_leaf': [1, 20, 50, 100],\n              \"min_samples_split\": [2, 10, 25, 50],\n              \"n_estimators\": [1000],\n              \"subsample\": [0.6, 0.8, 1.0]\n             }\n\n# run grid search\ngrid_search = GridSearchCV(clf, param_grid=param_grid, cv=3, scoring= 'neg_mean_squared_error', n_jobs = 6, verbose=True)\nstart = time()\ngrid_search.fit(X_train, y_train['SalePrice'])\n\nprint(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n      % (time() - start, len(grid_search.cv_results_['params'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The train Log RMSE is {:.4f}'.format(mean_squared_error_(gbr.predict(train_x), train_y)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**6.) Polynomial regression**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_new=Pipeline([(\"poly\",PolynomialFeatures(degree=2)),(\"linear\",LinearRegression(fit_intercept=False))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_new.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Accuracy\nprint(\"R-Squared Value for Training Set: {:.3f}\".format(model_new.score(X_train, y_train)))\nprint(\"R-Squared Value for Validation Set: {:.3f}\".format(model_new.score(X_val, y_val)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_new = model_new.predict(test_x)\ny_pred_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission = pd.DataFrame({'Id': test_x.Id, 'SalePrice': y_pred_new[:,0]})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}