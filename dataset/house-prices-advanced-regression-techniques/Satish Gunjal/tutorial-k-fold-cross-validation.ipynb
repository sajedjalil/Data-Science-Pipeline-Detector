{"cells":[{"metadata":{},"cell_type":"markdown","source":"![KFold_Cross_Validation_Header_1000x690](https://raw.githubusercontent.com/satishgunjal/images/master/KFold_Cross_Validation_Header_1000x690.png)"},{"metadata":{},"cell_type":"markdown","source":"# Index\n* [Introduction](#1)\n* [Inner Working of Cross Validation](#2)\n* [K Fold Cross Validation](#3)\n* [Stratified K Fold Cross Validation](#4)\n* [Hyperparameter Tuning and Model Selection](#5)\n* [Advantages](#6)\n* [Disadvantages](#7)\n* [K Fold: Regression Example](#8)\n  - [Import Libraries](#9)\n  - [Load Dataset](#10)\n  - [Understanding the Data](#11)\n  - [Model Score Using KFold](#12)\n    - [Using Linear Regression](#13), [Using Decision Tree Regressor](#14), [Using Random Forest Regressor](#15)\n  - [Model Tuning using KFold](#16)\n    - [Decision Tree Regressor Tuning](#17), [Random Forest Regressor Tuning](#18)\n* [K Fold: Classification Example](#19)\n  - [Load Dataset](#20)\n  - [Understanding the Data](#21)\n  - [Model Score Using KFold](#22)\n    - [Using Logistic Regression](#23), [Using Decision Classifier](#24), [Using Random Forest Classifier](#25)\n  - [Model Tuning using KFold](#26)\n    - [Logistic Classifier Tuning](#27), [Decision Tree Classifier Tuning](#28), [Random Forest Classifier Tuning](#28), \n* [Reference](#30)"},{"metadata":{},"cell_type":"markdown","source":"# Introduction <a id =\"1\"></a>\n\nAs of now we have divided the input data into train and test datasets and use it for model training and testing respectively. This method is not very reliable as train and test data not always have same kind of variation like original data, which will affect the accuracy of the model. Cross validation solves this problem by dividing the input data into multiple groups instead of just two groups. There are multiple ways to split the data, in this article we are going to cover K Fold and Stratified K Fold cross validation techniques.\n\nIn case you are not familiar with train test split method, please refer [this](https://satishgunjal.com/train_test_split/) article.\n\n\n# Inner Working of Cross Validation <a id =\"2\"></a>\n\n* Shuffle the dataset in order to remove any kind of order\n* Split the data into K number of folds. K= 5 or 10 will work for most of the cases.\n* Now keep one fold for testing and remaining all the folds for training.\n* Train(fit) the model on train set and test(evaluate) it on test set and note down the results for that split\n* Now repeat this process for all the folds, every time choosing separate fold as test data\n* So for every iteration our model gets trained and tested on different sets of data\n* At the end sum up the scores from each split and get the mean score\n\n![Inner_Working_KFold](https://raw.githubusercontent.com/satishgunjal/images/master/Inner_Working_KFold.png)\n\n# K Fold Cross Validation <a id =\"3\"></a>\n\nIn case of K Fold cross validation input data is divided into 'K' number of folds, hence the name K Fold. Suppose we have divided data into 5 folds i.e. K=5. Now we have 5 sets of data to train and test our model. So the model will get trained and tested 5 times, but for every iteration we will use one fold as test data and rest all as training data. Note that for every iteration, data in training and test fold changes which adds to the effectiveness of this method. \n\nThis significantly reduces underfitting as we are using most of the data for training(fitting), and also significantly reduces overfitting as most of the data is also being used in validation set. K Fold cross validation helps to generalize the machine learning model, which results in better predictions on unknown data. To know more about underfitting & overfitting please refer [this](https://satishgunjal.com/underfitting_overfitting/) article.\n\nFor most of the cases 5 or 10 folds are sufficient but depending on problem you can split the data into any number of folds.\n\n![KFold_Cross_Validation](https://raw.githubusercontent.com/satishgunjal/images/master/KFold_Cross_Validation.png)\n\n\n# Stratified K Fold Cross Validation <a id =\"4\"></a>\n\nStratified K Fold used when just random shuffling and splitting the data is not sufficient, and we want to have correct distribution of data in each fold. In case of regression problem folds are selected so that the mean response value is approximately equal in all the folds. In case of classification problem folds are selected to have same proportion of class labels. Stratified K Fold is more useful in case of classification problems, where it is very important to have same percentage of labels in every fold.\n\n![Stratified_KFold_Cross_Validation](https://raw.githubusercontent.com/satishgunjal/images/master/Stratified_KFold_Cross_Validation.png)\n\n# Hyperparameter Tuning and Model Selection <a id =\"5\"></a>\n\nNow you are familiar with inner working of cross validation, lets see how we can use it to tune the parameters and select best model. \n\nFor hyperparameter tuning or to find the best model we have to run the model against multiple combination of parameters and features and record score for analysis. To do this we can use sklearns 'cross_val_score' function. This function evaluates a score by cross-validation, and depending on the scores we can finalize the hyperparameter which provides the best results. Similarly, we can try multiple model and choose the model which provides the best score.\n\nNote: In this article I will do the model parameter tuning using for loop for better understanding. There are more sophisticated ways like using GridSearchCV() to do hyperparameter tuning. To know more about it please refer [this](https://www.kaggle.com/satishgunjal/tutorial-hyperparameter-tuning/) article\n\n# Advantages <a id =\"6\"></a>\n\n* We end up using all the data for training and testing and this is very useful in case of small datasets\n* It covers the variation of input data by validating the performance of the model on multiple folds\n* Multiple folds also helps in case of unbalanced data\n* Model performance analysis for every fold gives us more insights to fine tune the model \n* Used for hyperparameter tuning\n\n# Disadvantages <a id =\"7\"></a>\n\nK Fold cross validation not really helpful in case time series data. To know more about time series data please refer [this tutorial](https://www.kaggle.com/satishgunjal/tutorial-time-series-analysis-and-forecasting)"},{"metadata":{},"cell_type":"markdown","source":"# K Fold: Regression Example <a id =\"8\"></a>\n\nWe are going to use [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition data. We will convert this dataset into toy dataset so that we can straightaway jump into model building using K Fold cross validation\n\n![House_Prices_Advanced_Regression_Techniques](https://raw.githubusercontent.com/satishgunjal/images/master/House_Prices_Advanced_Regression_Techniques.png)\n \n## Import Libraries <a id =\"9\"></a>\n\n* pandas: Used for data manipulation and analysis\n* numpy : Numpy is the core library for scientific computing in Python. It is used for working with arrays and matrices.\n* KFold: Sklearn K-Folds cross-validator\n* StratifiedKFold: Stratified K-Folds cross-validator\n* cross_val_score: Sklearn library to evaluate a score by cross-validation\n* linear_model: Sklearn library, we are using LinearRegression and LogisticRegression algorithm\n* tree: Sklearn library, we are using DecisionTreeRegressor and DecisionTreeClassifier\n* ensemble: SKlearn library, we are using RandomForestRegressor and RandomForestClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\nfrom sklearn import linear_model, tree, ensemble","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Dataset <a id =\"10\"></a>\nWe will load the dataset into pandas dataframe and convert it into a toy dataset by removing categorical columns and rows and columns with null values."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\n\n# Remove rows with missing target values\ntrain_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = train_data.SalePrice # Target variable             \ntrain_data.drop(['SalePrice'], axis=1, inplace=True) # Removing target variable from training data\n\ntrain_data.drop(['LotFrontage', 'GarageYrBlt', 'MasVnrArea'], axis=1, inplace=True) # Remove columns with null values\n\n# Select numeric columns only\nnumeric_cols = [cname for cname in train_data.columns if train_data[cname].dtype in ['int64', 'float64']]\nX = train_data[numeric_cols].copy()\n\nprint(\"Shape of input data: {} and shape of target variable: {}\".format(X.shape, y.shape))\n\nX.head() # Show first 5 training examples","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Understanding the Data <a id =\"11\"></a>\nFinal dataset contains 34 features and 1460 training examples. We have to predict the house sales price based on available training data."},{"metadata":{},"cell_type":"markdown","source":"## Model Score Using KFold <a id =\"12\"></a>\nLet's use **cross_val_score()** to evaluate a score by cross-validation. We are going to use three different models for analysis. We will find the score for every split and then take average to get the overall score. We will analyze the model performance based on Root Mean Square Error (RMSE). Since RMSE is not directly available from scoring parameter, first we find the Mean Square Error and then take the square root of it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets split the data into 5 folds.  \n# We will use this 'kf'(KFold splitting stratergy) object as input to cross_val_score() method\nkf =KFold(n_splits=5, shuffle=True, random_state=42)\n\ncnt = 1\n# split()  method generate indices to split data into training and test set.\nfor train_index, test_index in kf.split(X, y):\n    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n    cnt += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nWhy we are using '-' sign to calculate RMSE?\nANS: Classification accuracy is reward function, means something you want to maximize. Mean Square Error is loss function, \nmeans something you want to minimize. Now if we use 'cross_val_score' function then best score(high value) will give worst \nmodel in case of loss function! There are other sklearn functions which also depends on 'cross_val_score' to select best model by\nlooking for highest scores, so a design decision was made for 'cross_val_score' to negate the output of all loss function. \nSo that when other sklearn function calls 'cross_val_score' those function can always assume that highest score indicate better model.\nIn short ignore the negative sign and rate the error based on its absolute value.\n\"\"\"\ndef rmse(score):\n    rmse = np.sqrt(-score)\n    print(f'rmse= {\"{:.2f}\".format(rmse)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using Linear Regression <a id =\"13\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"score = cross_val_score(linear_model.LinearRegression(), X, y, cv= kf, scoring=\"neg_mean_squared_error\")\nprint(f'Scores for each fold: {score}')\nrmse(score.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using Decision Tree Regressor <a id =\"14\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"score = cross_val_score(tree.DecisionTreeRegressor(random_state= 42), X, y, cv=kf, scoring=\"neg_mean_squared_error\")\nprint(f'Scores for each fold: {score}')\nrmse(score.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using Random Forest Regressor <a id =\"15\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"score = cross_val_score(ensemble.RandomForestRegressor(random_state= 42), X, y, cv= kf, scoring=\"neg_mean_squared_error\")\nprint(f'Scores for each fold are: {score}')\nrmse(score.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Tuning using KFold <a id =\"16\"></a>\nWe can also use **cross_val_score()** along with KFold to evaluate the model for different hyperparameters. Here we are going to try different hyperparameter values and choose the ones for which we get the highest model score."},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree Regressor Tuning <a id =\"17\"></a>\n\nThere are multiple hyperparameters like max_depth, min_samples_split, min_samples_leaf etc which affect the model performance. Here we are going to do tuning based on 'max_depth'. We will try with max depth starting from 1 to 10 and depending on the final 'rmse' score choose the value of max_depth."},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depth = [1,2,3,4,5,6,7,8,9,10]\n\nfor val in max_depth:\n    score = cross_val_score(tree.DecisionTreeRegressor(max_depth= val, random_state= 42), X, y, cv= kf, scoring=\"neg_mean_squared_error\")\n    print(f'For max depth: {val}')\n    rmse(score.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Regressor Tuning <a id =\"18\"></a>\n\nThere are multiple hyperparameters like n_estimators, max_depth, min_samples_split etc which affect the model performance. Here we are going to do tuning based on 'n_estimators'. We will try with estimators starting from 50 to 350 and depending on the final 'rmse' score choose the value of estimator."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = [50, 100, 150, 200, 250, 300, 350]\n\nfor count in estimators:\n    score = cross_val_score(ensemble.RandomForestRegressor(n_estimators= count, random_state= 42), X, y, cv= kf, scoring=\"neg_mean_squared_error\")\n    print(f'For estimators: {count}')\n    rmse(score.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K Fold: Classification Example <a id =\"19\"></a>\n\nWe are going to use [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/overview) competition data. We will convert this dataset into toy dataset so that we can straightaway jump into model building using K Fold cross validation\n\n![Titanic_Machine_Learning_from_Disaster](https://raw.githubusercontent.com/satishgunjal/images/master/Titanic_Machine_Learning_from_Disaster.png)\n\n## Load Dataset <a id =\"20\"></a>\n\nWe will load the dataset into pandas dataframe and convert it into a toy dataset by removing categorical columns and rows and columns with null values."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/titanic/train.csv')\n\n# Remove rows with missing target values\ntrain_data.dropna(axis=0, subset=['Survived'], inplace=True)\ny = train_data.Survived # Target variable             \ntrain_data.drop(['Survived'], axis=1, inplace=True) # Removing target variable from training data\n\ntrain_data.drop(['Age'], axis=1, inplace=True) # Remove columns with null values\n\n# Select numeric columns only\nnumeric_cols = [cname for cname in train_data.columns if train_data[cname].dtype in ['int64', 'float64']]\nX = train_data[numeric_cols].copy()\n\nprint(\"Shape of input data: {} and shape of target variable: {}\".format(X.shape, y.shape))\npd.concat([X, y], axis=1).head() # Show first 5 training examples","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Understanding the Data <a id =\"21\"></a>\nFinal dataset contains 5 features and 891 training examples. We have to predict which passengers survived the Titanic shipwreck based on available training data. Features that we are going to use in this example are passenger id, ticket class, sibling/spouse aboard, parent/children aboard and ticket fare"},{"metadata":{},"cell_type":"markdown","source":"## Model Score Using KFold <a id =\"22\"></a>\n\nLet's use **cross_val_score()** to evaluate a score by cross-validation. We are going to use three different models for analysis. We are going to find the score for every fold and then take average to get the overall score. We will analyze the model performance based on accuracy score, here score value indicate how many predictions are matching with actual values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets split the data into 5 folds. \n# We will use this 'kf'(StratiFiedKFold splitting stratergy) object as input to cross_val_score() method\n# The folds are made by preserving the percentage of samples for each class.\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\ncnt = 1\n# split()  method generate indices to split data into training and test set.\nfor train_index, test_index in kf.split(X, y):\n    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n    cnt+=1\n    \n# Note that: \n# cross_val_score() parameter 'cv' will by default use StratifiedKFold spliting startergy if we just specify value of number of folds. \n# So you can bypass above step and just specify cv= 5 in cross_val_score() function","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using Logistic Regression <a id =\"23\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"score = cross_val_score(linear_model.LogisticRegression(random_state= 42), X, y, cv= kf, scoring=\"accuracy\")\nprint(f'Scores for each fold are: {score}')\nprint(f'Average score: {\"{:.2f}\".format(score.mean())}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using Decision Classifier <a id =\"24\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"score = cross_val_score(tree.DecisionTreeClassifier(random_state= 42), X, y, cv= kf, scoring=\"accuracy\")\nprint(f'Scores for each fold are: {score}')\nprint(f'Average score: {\"{:.2f}\".format(score.mean())}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using Random Forest Classifier <a id =\"25\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"score = cross_val_score(ensemble.RandomForestClassifier(random_state= 42), X, y, cv= kf, scoring=\"accuracy\")\nprint(f'Scores for each fold are: {score}')\nprint(f'Average score: {\"{:.2f}\".format(score.mean())}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Tuning using KFold <a id =\"26\"></a>\n\nWe can also use **cross_val_score()** along with StratifiedKFold to evaluate the model for different hyperparameters. Here we are going to try different hyperparameter values and choose the ones for which we get the highest model score.\n\n### Logistic Classifier Tuning <a id =\"27\"></a>\n\nWe will try different optimization algorithm to finalize the one with the highest accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"algorithms = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n\nfor algo in algorithms:\n    score = cross_val_score(linear_model.LogisticRegression(max_iter= 4000, solver= algo, random_state= 42), X, y, cv= kf, scoring=\"accuracy\")\n    print(f'Average score({algo}): {\"{:.3f}\".format(score.mean())}')\n    \n# Note, here we are using max_iter = 4000, so that all the solver gets chance to converge. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Classifier Tuning <a id =\"28\"></a>\n\nHere we are going to do tuning based on 'max_depth'. We will try with max depth starting from 1 to 10 and depending on the final 'accuracy' score choose the value of max_depth."},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depth = [1,2,3,4,5,6,7,8,9,10]\n\nfor val in max_depth:\n    score = cross_val_score(tree.DecisionTreeClassifier(max_depth= val, random_state= 42), X, y, cv= kf, scoring=\"accuracy\")\n    print(f'Average score({val}): {\"{:.3f}\".format(score.mean())}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Classifier Tuning <a id =\"29\"></a>\n\nHere we are going to do tuning based on 'n_estimators'. We will try with estimators starting from 50 to 350 and depending on the final 'rmse' score, choose the value of estimator."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = [50, 100, 150, 200, 250, 300, 350]\n\nfor val in n_estimators:\n    score = cross_val_score(ensemble.RandomForestClassifier(n_estimators= val, random_state= 42), X, y, cv= kf, scoring=\"accuracy\")\n    print(f'Average score({val}): {\"{:.3f}\".format(score.mean())}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reference <a id =\"30\"></a>\n* [5 Reasons why you should use Cross-Validation in your Data Science Projects](https://towardsdatascience.com/5-reasons-why-you-should-use-cross-validation-in-your-data-science-project-8163311a1e79)\n* [Selecting the best model in scikit-learn using cross-validation](https://youtu.be/6dbrR-WymjI)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}