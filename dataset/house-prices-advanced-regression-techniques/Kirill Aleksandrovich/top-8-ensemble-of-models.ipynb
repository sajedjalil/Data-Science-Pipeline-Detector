{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\">  DISCLAIMER </h1>\n<p>Guys, everything that you see in the kernel you can freely take, copy, modify and create your own amazing solutions!</p>\n<p>If you don't want to waste your time - just read 'briefly' section I made for you.</p>\n<p>Do not forget upvote if the kernel was useful for you.</p>\n<img src=\"https://i.gifer.com/7lDV.gif\">"},{"metadata":{},"cell_type":"markdown","source":"<h1>TODO-list</h1>\n<ul>\n    <li>Add more new features  means it's CLUSTERING</li>\n    <li>Add extra features indicating the feature value is absent</li>\n    <li>Try to stack some models or just investigate how to do it</li>\n</ul>"},{"metadata":{},"cell_type":"markdown","source":"<h1>\n    Imports\n</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nimport warnings\n\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mlxtend.regressor import StackingCVRegressor\nfrom scipy.stats import skew\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\n\n\nprint(\"Imports have been set\")\n\n# Disabling warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>\n    Input data handling\n</h1>\n<span> Briefly:\n    <ul>\n        <li> removing rows with NaN in Sale Price </li>\n        <li> logarithm SalePrice (and when model predicts I use np.expm1 function to return value)  </li>\n        <li> splitting X to target variable y and train_features </li>\n        <li> joining X_test and train_features to process all features together </li>\n    </ul>\n</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading the training/val data and the test data\nX = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv', index_col='Id')\nX_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv', index_col='Id')\n\n# Rows before:\nrows_before = X.shape[0]\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\nrows_after = X.shape[0]\nprint(\"\\nRows containing NaN in SalePrice were dropped: \" + str(rows_before - rows_after))\n\n# Logarithming target variable in order to make distribution better\nX['SalePrice'] = np.log1p(X['SalePrice'])\n\ny = X['SalePrice'].reset_index(drop=True)\ntrain_features = X.drop(['SalePrice'], axis=1)\n\n# concatenate the train and the test set as features for tranformation to avoid mismatch\nfeatures = pd.concat([train_features, X_test]).reset_index(drop=True)\nprint('\\nFeatures size:', features.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>\n    Checking for NaNs and printing them\n</h1>\n<span> Briefly:\n    <ul>\n        <li> printing NaN-containing columns names </li>\n        <li> printing NaN-containing columns values for clarity</li>\n    </ul>\n</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"nan_count_table = (features.isnull().sum())\nnan_count_table = nan_count_table[nan_count_table > 0].sort_values(ascending=False)\nprint(\"\\nColums containig NaN: \")\nprint(nan_count_table)\n\ncolumns_containig_nan = nan_count_table.index.to_list()\nprint(\"\\nWhat values they contain: \")\nprint(features[columns_containig_nan])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>\n    Feature engineering\n</h1>\n<span> Briefly:\n    <ul>\n        <li> Filling with 0 numeric columns </li>\n        <li> Filling with 'None' categoric columns where 'NA' meant 'other' value</li>\n        <li> Filling with the most frequent values categoric columns where 'NA' meant 'nothing is here'</li>\n        <li> Turning to 'str' columns which are actually categoric </li>\n        <li> Turning to 'int' columns which are actually numeric </li>\n    </ul>\n</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in columns_containig_nan:\n\n    # populating with 0\n    if column in ['GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n                  'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'TotalBsmtSF',\n                  'Fireplaces', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold',\n                  'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea']:\n        features[column] = features[column].fillna(0)\n\n    # populate with 'None'\n    if column in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', \"PoolQC\", 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n                  'BsmtFinType2', 'Neighborhood', 'BldgType', 'HouseStyle', 'MasVnrType', 'FireplaceQu', 'Fence', 'MiscFeature']:\n        features[column] = features[column].fillna('None')\n\n    # populate with most frequent value for cateforic\n    if column in ['Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'RoofStyle',\n                  'Electrical', 'Functional', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'RoofMatl', 'ExterQual', 'ExterCond',\n                  'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'PavedDrive', 'SaleType', 'SaleCondition']:\n        features[column] = features[column].fillna(features[column].mode()[0])\n\n# MSSubClass: Numeric feature. Identifies the type of dwelling involved in the sale.\n#     20  1-STORY 1946 & NEWER ALL STYLES\n#     30  1-STORY 1945 & OLDER\n#     40  1-STORY W/FINISHED ATTIC ALL AGES\n#     45  1-1/2 STORY - UNFINISHED ALL AGES\n#     50  1-1/2 STORY FINISHED ALL AGES\n#     60  2-STORY 1946 & NEWER\n#     70  2-STORY 1945 & OLDER\n#     75  2-1/2 STORY ALL AGES\n#     80  SPLIT OR MULTI-LEVEL\n#     85  SPLIT FOYER\n#     90  DUPLEX - ALL STYLES AND AGES\n#    120  1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n#    150  1-1/2 STORY PUD - ALL AGES\n#    160  2-STORY PUD - 1946 & NEWER\n#    180  PUD - MULTILEVEL - INCL SPLIT LEV/FOYER\n#    190  2 FAMILY CONVERSION - ALL STYLES AND AGES\n\n# Stored as number so converted to string.\nfeatures['MSSubClass'] = features['MSSubClass'].apply(str)\nfeatures[\"MSSubClass\"] = features[\"MSSubClass\"].fillna(\"Unknown\")\n# MSZoning: Identifies the general zoning classification of the sale.\n#    A    Agriculture\n#    C    Commercial\n#    FV   Floating Village Residential\n#    I    Industrial\n#    RH   Residential High Density\n#    RL   Residential Low Density\n#    RP   Residential Low Density Park\n#    RM   Residential Medium Density\n\n# 'RL' is by far the most common value. So we can fill in missing values with 'RL'\nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n# LotFrontage: Linear feet of street connected to property\n# Groupped by neighborhood and filled in missing value by the median LotFrontage of all the neighborhood\n# TODO may be 0 would perform better than median?\nfeatures['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n# LotArea: Lot size in square feet.\n# Stored as string so converted to int.\nfeatures['LotArea'] = features['LotArea'].astype(np.int64)\n# Alley: Type of alley access to property\n#    Grvl Gravel\n#    Pave Paved\n#    NA   No alley access\n\n# So. If 'Street' made of 'Pave', so it would be reasonable to assume that 'Alley' might be 'Pave' as well.\nfeatures['Alley'] = features['Alley'].fillna('Pave')\n# MasVnrArea: Masonry veneer area in square feet\n# Stored as string so converted to int.\nfeatures['MasVnrArea'] = features['MasVnrArea'].astype(np.int64)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>\n    Adding new features\n</h1>\n<span> Briefly:\n    <ul>\n        <li> YrBltAndRemod means overall sum of years </li>\n        <li> Separating to the other features overall squares</li>\n        <li> Separating to the other features presence/absence of a garage and so on</li>\n    </ul>\n</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"features['YrBltAndRemod'] = features['YearBuilt'] + features['YearRemodAdd']\nfeatures['TotalSF'] = features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n\nfeatures['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                                 features['1stFlrSF'] + features['2ndFlrSF'])\n\nfeatures['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n\nfeatures['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] + features['ScreenPorch'] +\n                              features['WoodDeckSF'])\n\n# If area is not 0 so creating new feature looks reasonable\nfeatures['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nprint('Features size:', features.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>\n    Let's check if we filled all the gaps\n</h1>\n<span> Briefly:\n    <ul>\n        <li> Just printing True or False if all the gaps are filled </li>\n    </ul>\n</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"nan_count_train_table = (features.isnull().sum())\nnan_count_train_table = nan_count_train_table[nan_count_train_table > 0].sort_values(ascending=False)\nprint(\"\\nAre no NaN here now: \" + str(nan_count_train_table.size == 0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>\n    Fixing skewed values\n</h1>\n<span> Briefly:\n    <ul>\n        <li> Checking skewness of all the numeric features and logarithm it if more than 0.7 </li>\n    </ul>\n</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_columns = [cname for cname in features.columns if features[cname].dtype in ['int64', 'float64']]\nprint(\"\\nColumns which are numeric: \" + str(len(numeric_columns)) + \" out of \" + str(features.shape[1]))\nprint(numeric_columns)\n\ncategoric_columns = [cname for cname in features.columns if features[cname].dtype == \"object\"]\nprint(\"\\nColumns whice are categoric: \" + str(len(categoric_columns)) + \" out of \" + str(features.shape[1]))\nprint(categoric_columns)\n\nskewness = features[numeric_columns].apply(lambda x: skew(x))\nprint(skewness.sort_values(ascending=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skewness = skewness[abs(skewness) > 0.7]\nprint(\"\\nSkewed values: \" + str(skewness.index))\nfixed_features = np.log1p(features[skewness.index])\n\nprint(\"\\nLet's see and compare skewed features\")\nfor column in skewness.index[0:5]:\n    sns.distplot(features[column])\n    plt.show()\n    sns.distplot(fixed_features[column])\n    plt.show()\n    print(\"---------------------------------------------------\")\n    \n\n\nprint(\"\\nSkewed values: \" + str(skewness.index))\nfeatures[skewness.index] = np.log1p(features[skewness.index])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>\n    Categoric features encoding and splitting to train and test data\n</h1>\n<span> Briefly:\n    <ul>\n        <li> I used pd.get_dummies(features) which returns kind of One-Hot encoded categoric features</li>\n        <li> Splitted to X and X_test by y length</li>\n    </ul>\n</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Kind of One-Hot encoding\nfinal_features = pd.get_dummies(features).reset_index(drop=True)\n\n# Spliting the data back to train(X,y) and test(X_sub)\nX = final_features.iloc[:len(y), :]\nX_test = final_features.iloc[len(X):, :]\nprint('Features size for train(X,y) and test(X_test):')\nprint('X', X.shape, 'y', y.shape, 'X_test', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>\n    ML part (models initialization)\n</h1>\n<span> Briefly:\n    <ul>\n        <li> I used pd.get_dummies(features) as encoder which returns kind of One-Hot encoded categoric features</li>\n        <li> Splitted to X and X_test by y length</li>\n    </ul>\n</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n\n# check maybe 10 kfolds would be better\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\n\n# Kernel Ridge Regression : made robust to outliers\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\n\n# LASSO Regression : made robust to outliers\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas2, random_state=14, cv=kfolds))\n\n# Elastic Net Regression : made robust to outliers\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))\n\n# Gradient Boosting for regression\ngboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10,\n                                   loss='huber', random_state=5)\n\n# LightGBM regressor.\nlgbm = lgb.LGBMRegressor(objective='regression', num_leaves=5,\n                         learning_rate=0.05, n_estimators=720,\n                         max_bin=55, bagging_fraction=0.8,\n                         bagging_freq=5, feature_fraction=0.2319,\n                         feature_fraction_seed=9, bagging_seed=9,\n                         min_data_in_leaf=6, min_sum_hessian_in_leaf=11)\n\n# optimal parameters, received from CV\nc_grid = {\"n_estimators\": [1000],\n          \"early_stopping_rounds\": [1],\n          \"learning_rate\": [0.1]}\nxgb_regressor = XGBRegressor(objective='reg:squarederror')\ncross_validation = KFold(n_splits=10, shuffle=True, random_state=2)\nxgb_r = GridSearchCV(estimator=xgb_regressor,\n                     param_grid=c_grid,\n                     cv=cross_validation)\n\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, lgbm, gboost),\n                                meta_regressor=elasticnet,\n                                use_features_in_secondary=True)\n\nsvr = make_pipeline(RobustScaler(), SVR(C=20, epsilon=0.008, gamma=0.0003))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>\n    ML part (models fitting)\n</h1>\n<span> Briefly:\n    <ul>\n        <li> One-by-one all models fitting</li>\n        <li> Printing models scores (might be commented for quicker work) </li>\n    </ul>\n</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\n\\nFitting our models ensemble: ')\n\nprint('Elasticnet is fitting now...')\nelastic_model = elasticnet.fit(X, y)\nprint('Lasso is fitting now...')\nlasso_model = lasso.fit(X, y)\nprint('Ridge is fitting now...')\nridge_model = ridge.fit(X, y)\nprint('XGB is fitting now...')\nxgb_model = xgb_r.fit(X, y)\nprint('Gradient Boosting regressor is fitting now...')\ngboost_model = gboost.fit(X, y)\nprint('LGBMRegressor is fitting now...')\nlgbm_model = lgbm.fit(X, y)\nprint('stack_gen is fitting now...')\nstack_gen_model = stack_gen.fit(X, y)\nprint('SVR is fitting now...')\nsvr_model = svr.fit(X, y)\n\n\n# model scoring and validation function\n# def cv_rmse(the_model, x):\n#     return np.sqrt(-cross_val_score(the_model, x, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n\n\n# print('\\n\\nModels evaluating: ')\n# score = cv_rmse(ridge_model, X)\n# print(\"Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n#\n# score = cv_rmse(lasso_model, X)\n# print(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n#\n# score = cv_rmse(elastic_model, X)\n# print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n#\n# score = cv_rmse(xgb_model, X)\n# print(\"xgb_r score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n#\n# score = cv_rmse(gboost_model, X)\n# print(\"Gradient boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n#\n# score = cv_rmse(lgbm_model, X)\n# print(\"LGB score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n#\n# score = cv_rmse(stack_gen_model, X)\n# print(\"Stack gen score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n#\n# score = cv_rmse(svr_model, X)\n# print(\"SVR score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>\n    ML part (models ensembling)\n</h1>\n<span> Briefly:\n    <ul>\n        <li> The weighted sum of models on the basis of which the solution is assembled</li>\n        <li> There is score in comment to each row which explains coefficient to model</li>\n    </ul>\n</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Last successful: 0.11663\n# Try to increase stack_gen and try it manually\n# [0.2, 0.1, 0.2, 0.2, 0.0, 0.2, 0.1, 0.0] - 0.11383\ndef blend_models(x):\n    return (0.20 * stack_gen_model.predict(x) +    # 0.1236 (0.0232)\n            0.10 * gboost_model.predict(x) +      # 0.1249 (0.0208)\n            0.20 * elastic_model.predict(x) +    # 0.1266 (0.0221)\n            0.20 * lasso_model.predict(x) +     # 0.1267 (0.0221)\n            0.00 * lgbm_model.predict(x) +     # 0.1272 (0.0198)\n            0.20 * svr_model.predict(x) +     # 0.1273 (0.0247)\n            0.10 * xgb_model.predict(x) +     # 0.1283 (0.0168)\n            0.00 * ridge_model.predict(x))  # 0.1301 (0.0216)\n\n\ndef rmsle(y_actual, y_pred):\n    return np.sqrt(mean_squared_error(y_actual, y_pred))\n\n\nprint('\\nRMSLE score on train data:')\nprint(rmsle(y, blend_models(X)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance = gboost_model.feature_importances_\nimportance.sort(axis=0)\n\nplt.figure(figsize=(200,100))\nplt.plot(importance)\nplt.xticks(np.arange(X.shape[1]), X.columns.tolist(), rotation=90);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>Model improvements history to see how various changes affect the performance of the model</h1>\n<span>\n\n            \n1. Initial start only with XGBoost:\n            - RMSLE: 0.13901\n            \n2. Added 'SalePrice' logarithming:\n             - RMSLE: 0.13984 ↗\n             - Review: For now it made performance worse. Reasons are unknown.\n            \n3. Added huge amount of feature engineering:\n             - RMSLE: 0.13490 ↘\n             - Review: Obviously this makes sense\n            \n4. Used Label encoder instead of get_dummies():\n             - RMSLE: 0.13630 ↗\n             - Review: get_dummies performs better by it's kind of OH-encoding\n            \n5. Aded skew-features fix:\n             - RMSLE: 0.13494 ↗\n             - Review: It is just a bit better than 0.13490 (launch after feature engineering)\n            \n6. Created models ensemble: ridge, lasso, elasticnet, gbr, xgboost:\n             - RMSLE: 0.11759 ↘\n             - Review: Sure. Models ensemble would make magic.\n            \n7. Added KernelRidge, Gradient Boosting and LGBMRegressor to ensemble and sum made weighted:\n             - RMSLE: 0.11638 ↘\n             - Review: Very nice! Keep going!\n            \n8. Added SVR and Stack Gen:\n             - RMSLE: 0.11534 ↘\n             - Review: Nice as well, but here comes question with coefficients.\n            \n9. Made coefficiential bruteforce:\n             - RMSLE: 0.11506 ↘\n             - Review: Looks promising. It's hard to imagine how lot depends on coefficients regardless of your model.            \n</span>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\nsubmission.iloc[:, 1] = np.expm1(blend_models(X_test))\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(\"Submission file is formed\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}