{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Time Complexity of Machine Learning Algorithms**\n\nTable of Contents:\n1. Hard computing vs Soft computing\n1. A Theoretical point of view\n1. Justifications\n1. A Practical point of view\n1. Algorithm Complexity \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. Hard computing vs Soft computing \nThere are two paradigms of computing - hard computing and soft computing.\n\nHard computing deals with problems that have exact solutions, and in which approximate / uncertain solutions are not acceptable. This is the conventional computing, and most algorithms courses deal with hard computing.\n\nSoft computing, on the other hand, looks at techniques to approximately solve problems that are not solvable in finite time. Most machine learning algorithms fall in this category. The quality of the solution improves as you spend more time in solving the problem. So complexity in terms of big-oh ,we can make appropriate  but ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2. A Theoretical point of view\nIt is harder than one would think to evaluate the complexity of a machine learning algorithm, especially as it may be implementation dependent, properties of the data may lead to other algorithms or the training time often depends on some parameters passed to the algorithm.\n\nLets start looking at worst case time complexity when the data is dense,\n* n -> the number of training sample\n* p -> the number of features\n* n*tress* -> the number of trees (for methods based on various trees)\n* n*sv* -> the number of support vectors\n* n*li* -> the number of neurons at layer i in a neural network\n\n\n\nwe have the following approximations,","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/ml-cplexity/ml.JPG\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Justifications\n### Decision Tree based models\n\n\n\nObviously, ensemble methods multiply the complexity of the original model by the number of “voters” in the model, and replace the training size by the size of each bag.\n\nWhen training a decision tree, a split has to be found until a maximum depth \nd\n has been reached.\n\nThe strategy for finding this split is to look for each variable (there are \np\n of them) to the different thresholds (there are up to \nn\n of them) and the information gain that is achieved (evaluation in \nO\n(\nn\n)\n)\n\nIn the Breiman implementation, and for classification, it is recommanded to use \n√\np\n predictors for each (weak) classifier.\n\n\n### Linear regressions\n\nThe problem of finding the vector of weights \nβ\n in a linear regression boils down to evaluating the following equation: \nβ=\n(\nX\n′\nX\n)\n−\n1\nX\n′\nY\n.\n\nThe most computationnaly intensive part is to evaluate the product \nX\n′\nX\n, which is done in \np\n2\nn\n operations, and then inverting it, which is done in \np\n3\n operations.\n\nThough most implementations prefer to use a gradient descent to solve the system of equations \n(\nX\n′\nX\n)\nβ=\nX\n′\nY\n, the complexity remains the same.\n\n\nSupport Vector Machine\n\nFor the training part, the classical algorithms require to evaluate the kernel matrix \nK\n, the matrix whose general term is \nK\n(\nx\ni\n,\nx\nj\n)\n where \nK\n is the specified kernel.\n\nIt is assumed that K can be evaluated with a \nO\n(\np\n)\n complexity, as it is true for common kernels (Gaussian, polynomials, sigmoid…). This assumption may be wrong for other kernels.\n\nThen, solving the constrained quadratic programm is “morally equivalent to” inverting a square matrix of size \nn\n, whose complexity is assumed to be \nO\n(\nn\n3\n)\n\n### k-Nearest Neighbours\n\nIn its simplest form, given a new data point \nx\n, the kNN algorithm looks for the k closest points to \nx\n in the training data and returns the most common label (or the averaged values of targets for a regression problem).\n\nTo achieve this, it is necessary to compare the distance between \nx\n and every point in the data set. This amounts to \nn\n operations. For the common distances (Euclide, Manhattan…) this operation is performed in a \nO\n(\np\n)\n operations. Not that kernel k Nearest Neighbours have the same complexity (provided the kernels enjoy the same property).\n\nHowever, many efforts pre-train the kNN, indexing the points with quadtrees, which enable to lower dramatically the number of comparisons to the points in the training set.\n\nLikewise, in the case of a sparse data set, with an inverted indexation of the rows, it is not necessary to compute all the pairwise distances.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4. The practical point of view\nThe assumptions will be that the complexities take the form of \nO\n(\nn\nα\np\nβ\n)\n and \nα\n and \nβ\n will be estimated using randomly generated samples with \nn\n and \np\n varying. Then, using a log-log regression, the complexities are estimated.\n\nThough this assumption is wrong, it should help to have a better idea of how the algorithms work and it will reveal some implementation details / difference between the default settings of the same algorithm that one may overlook.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Another interesting point to note are the complexities in \np\n for the random forest and extra trees, the component in \np\n varies according to the fact that we are performing a regression or a classification problem. A short look at the documentation explains it, they have different behaviors for each problem!\n \n **For the regression:**\n max_features : int, float, string or None, optional (default=”auto”)\n\nThe number of features to consider when looking for the best split:\n\n* If int, then consider max_features features at each split.\n* If float, then max_features is a percentage and int(max_features * n_features) features are considered at each split.\n* If “auto”, then max_features=n_features.\n* If “sqrt”, then max_features=sqrt(n_features).\n* If “log2”, then max_features=log2(n_features).\n* If None, then max_features=n_features.\n\nWhereas the classification default behavior is\n* If “auto”, then max_features=sqrt(n_features).\n\nTo learn more on this,\n* [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n* [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\nimport time\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ComplexityEvaluator:\n    def __init__(self , nrow_samples , ncol_samples):\n        self._nrow_samples = nrow_samples\n        self._ncol_samples = ncol_samples\n    \n    #random data\n    def _time_samples(self , model , random_data_generator):\n        row_list = []\n        # iterate with rows and columns\n        for nrow in self._nrow_samples:\n            for ncol in self._ncol_samples:\n                train , label = random_data_generator(nrow , ncol)\n                #initiate timer\n                start_time = time.time()\n                model.fit(train , label)\n                elapsed_time = time.time() - start_time\n                result = {\"N\" : nrow , \"P\" : ncol , \"Time\" : elapsed_time}\n                row_list.append(result)\n                \n        return row_list , len(row_list)\n    \n    #house pricing data\n    def _time_houseprice(self , model):\n        row_list = []\n        #initiate timer\n        train = self._nrow_samples\n        label = self._ncol_samples\n        start_time = time.time()\n        model.fit(train , label)\n        elapsed_time = time.time() - start_time\n        #print(\"time : \" , elapsed_time)\n        result = {\"N\" :len(self._nrow_samples) , \"P\" : len(self._ncol_samples), \"Time\" : elapsed_time}\n        row_list.append(result)\n                \n        return row_list , len(row_list)\n    \n    def run(self , model , random_data_generator , ds='random'):\n        import random\n        if ds == 'random':\n            row_list , length = self._time_samples(model, random_data_generator)\n        else:\n            row_list , length = self._time_houseprice(model)\n            \n        cols = list(range(0 , length))\n        data = pd.DataFrame(row_list , index =cols)\n        print(data)\n        data = data.applymap(math.log)\n        #print(\"apply math : \", data)\n        linear_model = LinearRegression(fit_intercept=True)\n        linear_model.fit(data[[\"N\" , \"P\"]] , data[[\"Time\"]])\n        #print(\"coefficients : \" , linear_model.coef_)\n        return linear_model.coef_\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestModel:\n    def __init__(self):\n        pass\n    \n    def fit(self , x, y):\n        time.sleep(x.shape[0] /1000)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_data_generator(n , p):\n    return np.random.rand(n , p) , np.random.rand(n , 1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After a small unit test, everything seems consistent.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == \"__main__\":\n    model = TestModel()\n    nrow_samples = [200, 500, 1000, 2000, 3000]\n    ncol_samples = [1,5,10]\n    complexity_evaluator = ComplexityEvaluator(nrow_samples , ncol_samples)\n    res = complexity_evaluator.run(model , random_data_generator)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So let’s enjoy the number of algorithms offered by sklearn.I have used House price data to run on different ML models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVR, SVC\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regression_models = [RandomForestRegressor(),\n                     ExtraTreesRegressor(),\n                     AdaBoostRegressor(),\n                     LinearRegression(),\n                     SVR()]\n\nclassification_models = [RandomForestClassifier(),\n                         ExtraTreesClassifier(),\n                         AdaBoostClassifier(),\n                         SVC(),\n                         LogisticRegression(),\n                         LogisticRegression(solver='sag')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = [\"RandomForestRegressor\",\n         \"ExtraTreesRegressor\",\n         \"AdaBoostRegressor\",\n         \"LinearRegression\",\n         \"SVR\",\n         \"RandomForestClassifier\",\n         \"ExtraTreesClassifier\",\n         \"AdaBoostClassifier\",\n         \"SVC\",\n         \"LogisticRegression(solver=liblinear)\",\n         \"LogisticRegression(solver=sag)\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using sample data to run on different models\nsample_data = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\nsample_data = sample_data.loc[:, sample_data.dtypes !=np.object]\nsample_data = sample_data.fillna(0)\nnrows = sample_data.iloc[:,:-1].values.tolist()\nncols = sample_data['SalePrice'].values.tolist()\ncomplexity_evaluator = ComplexityEvaluator(nrows,ncols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nfor model in regression_models:\n    res = complexity_evaluator.run(model, random_data_generator , 'houseprice')[0]\n    print(names[i] + ' | ' + str(round(res[0], 2)) +\n          ' | ' + str(round(res[1], 2)))\n    i = i + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Algorithm Complexity\n\nMachine Learning is primarily about optimization of an objective function. Often the function is so represented that the target is to reach the global minima. Solving it involves heuristics, and thereby multiple iterations. In gradient descent for instance, you need multiple iterations to reach the minima. So given an algorithm, you can at best estimate the running 'time' for a single iteration. \n\nWe are talking about finding Minima of cost functions whose complexity depend on the ‘value’ of the data and not just the ‘size’ of the data. The cost function is a function of the dataset. This is a key difference between algorithms used for ML and others.\n\nNote that this again cannot be a parameter for comparison since for different algorithms, the objective function would reach a minima in different number of iterations for different data sets.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**References:**\n* [https://mitpress.mit.edu/books/introduction-computational-learning-theory](http://https://mitpress.mit.edu/books/introduction-computational-learning-theory)\n* [https://mitpress.mit.edu/books/computational-complexity-machine-learning](http://https://mitpress.mit.edu/books/computational-complexity-machine-learning)\n* [http://www.scalaformachinelearning.com/2015/11/time-complexity-in-machine-learning.html](http://http://www.scalaformachinelearning.com/2015/11/time-complexity-in-machine-learning.html)\n* [https://scikit-learn.org/stable/modules/manifold.html](http://https://scikit-learn.org/stable/modules/manifold.html)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}