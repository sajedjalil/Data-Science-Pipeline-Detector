{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Recursive Feature Elimination (RFE) example\nRecursive feature elimination [1] is an example of *backward feature elimination* [2] in which we essentially first fit our model using *all* the features in a given set, then progressively one by one we remove the *least* significant features, each time re-fitting, until we are left with the desired number of features, which is set by the parameter `n_features_to_select`.\n\nThis simple script uses the scikit-learn *Recursive Feature Elimination* routine [sklearn.feature_selection.RFE](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html). In this example we shall use the [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) data.\nFor the regressor we shall use the [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) routine, also from scikit-learn.\n\nScikit-learn also has a variant of this routine that incorporates [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics&#41;), see: [sklearn.feature_selection.RFECV](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html).\n\n### Forward feature selection\nRFE has its counterpart in *forward feature selection*, which does the opposite: accrete features rather than eliminate them, usually via some form of [greedy algorithm](https://en.wikipedia.org/wiki/Greedy_algorithm) [3]. Scikit-learn has the routine [sklearn.feature_selection.f_regression](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html) to facilitate this. \n\nNote that both of these *wrapper* methods can be beaten if one has access to \"domain knowledge\", i.e. understanding the problem and having a good idea as to which features will be important in the model one is constructing.\n\n### The python code:","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/python3\n# coding=utf-8\n#===========================================================================\n# This is a simple script to perform recursive feature elimination on \n# the kaggle 'House Prices' data set using the scikit-learn RFE\n# Carl McBride Ellis (2.V.2020)\n#===========================================================================\n#===========================================================================\n# load up the libraries\n#===========================================================================\nimport pandas  as pd\nimport numpy as np\n\n#===========================================================================\n# read in the data and specify the target feature\n#===========================================================================\ntrain_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv',index_col=0)\ntest_data  = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv',index_col=0)\ntarget     = 'SalePrice'\n\n#===========================================================================\n#===========================================================================\nX_train = train_data.select_dtypes(include=['number']).copy()\nX_train = X_train.drop([target], axis=1)\ny_train = train_data[target]\nX_test  = test_data.select_dtypes(include=['number']).copy()\n\n#===========================================================================\n# simple preprocessing: imputation; substitute any 'NaN' with mean value\n#===========================================================================\nX_train = X_train.fillna(X_train.mean())\nX_test  = X_test.fillna(X_test.mean())\n\n#===========================================================================\n# set up our regressor. Today we shall be using the random forest\n#===========================================================================\nfrom sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators=100, max_depth=10)\n\n#===========================================================================\n# perform a scikit-learn Recursive Feature Elimination (RFE)\n#===========================================================================\nfrom sklearn.feature_selection import RFE\n# here we want only one final feature, we do this to produce a ranking\nn_features_to_select = 1\nrfe = RFE(regressor, n_features_to_select=n_features_to_select)\nrfe.fit(X_train, y_train)\n\n#===========================================================================\n# now print out the features in order of ranking\n#===========================================================================\nfrom operator import itemgetter\nfeatures = X_train.columns.to_list()\nfor x, y in (sorted(zip(rfe.ranking_ , features), key=itemgetter(0))):\n    print(x, y)\n\n#===========================================================================\n# ok, this time let's choose the top 10 featues and use them for the model\n#===========================================================================\nn_features_to_select = 10\nrfe = RFE(regressor, n_features_to_select=n_features_to_select)\nrfe.fit(X_train, y_train)\n\n#===========================================================================\n# use the model to predict the prices for the test data\n#===========================================================================\npredictions = rfe.predict(X_test)\n\n#===========================================================================\n# write out CSV submission file\n#===========================================================================\noutput = pd.DataFrame({\"Id\":test_data.index, target:predictions})\noutput.to_csv('submission.csv', index=False)","metadata":{"_uuid":"e2f3408a-218f-43bc-9df4-0ac34ce6336b","_cell_guid":"806c8458-34c5-4125-b015-bca5648d5f7a","execution":{"iopub.status.busy":"2021-08-18T05:38:20.615769Z","iopub.execute_input":"2021-08-18T05:38:20.616302Z","iopub.status.idle":"2021-08-18T05:39:24.66489Z","shell.execute_reply.started":"2021-08-18T05:38:20.616255Z","shell.execute_reply":"2021-08-18T05:39:24.663286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### References\n\n1. [Isabelle Guyon, Jason Weston, Stephen Barnhill and Vladimir Vapnik \"Gene Selection for Cancer Classification using Support Vector Machines\", Machine Learning volume 46, pages 389â€“422 (2002) (doi: 10.1023/A:1012487302797)](https://doi.org/10.1023/A:1012487302797)\n2. [Ron Kohavi and George H. John \"Wrappers for feature subset selection\", Artificial Intelligence Volume 97 pages 273-324 (1997) (doi: 10.1016/S0004-3702(97)00043-X)](https://www.sciencedirect.com/science/article/pii/S000437029700043X)\n3. [Haleh Vafaie and Ibrahim F. Imam \"Feature Selection Methods: Genetic Algorithms vs. Greedy-like Search\", Proceedings of the 3rd International Conference on Fuzzy and Intelligent Control Systems (1994)](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.8452)","metadata":{}}]}