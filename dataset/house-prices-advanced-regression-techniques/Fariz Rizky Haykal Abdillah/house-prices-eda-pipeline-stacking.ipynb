{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Home Selling Price Prediction**","metadata":{}},{"cell_type":"markdown","source":"> <center><img src=\"https://media.istockphoto.com/photos/home-for-sale-real-estate-sign-and-house-picture-id168769007?k=20&m=168769007&s=612x612&w=0&h=uPj_q8BUB6N27npzmIsZlnUu-ysnvsoR1elRfmPhwlc=\" width=\"1300px\"></center>\n\n","metadata":{}},{"cell_type":"markdown","source":"* **Problem Statement:**\n<div align='left'><font size=\"3\" color=\"#000000\"> It is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. \n</font></div>\n","metadata":{}},{"cell_type":"markdown","source":"## 1. EDA\n> Analyze and investigate data sets and summarize their main characteristics.\n\n## 2. Feature Engineering\n* Convert non numeric features to string\n> There are some of the features that are actually included as an 'object' but represent in numerical, convert them to data type 'object' and encode them with One-hot-encoder.\n* Normalize skewed feature (SalePrice)\n> Make a skew data into normal or Gaussian one by applying log-transformation to remove or reduce skewness.\n* Deal with outliers\n> Remove outliers from some of the features.\n\n## 3. Data preprocessing\n* Apply Pipeline\n> `sklearn.pipeline.Pipeline` Sequentially apply a list of transforms and a final estimator. The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters.\n* Check for null values\n> If there are null values then impute the numerical data type with mean and most frequent one for the object data type by using SimpleImputer().\n\n* Check for categorical feature columns and encode them \n>  All features with data type 'object' can be ecoded using a One-hot-encoder. <br>\n\n* Standardize\n> Standardize numerical features by removing the mean and scaling to unit variance using StandardScaler().\n\n* Building model and apply k-fold cross validation\n> Preprocess the models and evaluate a score by k-fold cross validation.\n\n\n## 4. Model Comparison\n> Compare the score from two Regressions models:\n> * XGBoost Regressor \n> * Support Vector Regressor\n\n## 5. Model Improvement\n* RandomSearchCV\n> Get the best parameters for the model to get better accuracy.\n* Stacking\n> Combining the predictions from multiple machine learning models on the same dataset.\n\n## 6. Making Final Prediction\n> Selecting the best model and train with whole training dataset to be used for predicting test data.\n\n## 7. Submission\n> Submit the selected model with the best prediction.","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\n\n# preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Stats\nfrom scipy.stats import skew, norm\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# column transformer\nfrom sklearn.compose import ColumnTransformer\n\n# cross validation\nfrom sklearn.model_selection import KFold, cross_val_score\n\n# Stacking\nfrom mlxtend.regressor import StackingCVRegressor\n\n# hyperparameter tuning\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Pipeline\nfrom sklearn.pipeline import Pipeline\n\n# model\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\n\n# Removes warning\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:08:59.029643Z","iopub.execute_input":"2021-10-19T16:08:59.030406Z","iopub.status.idle":"2021-10-19T16:09:00.539869Z","shell.execute_reply.started":"2021-10-19T16:08:59.030363Z","shell.execute_reply":"2021-10-19T16:09:00.538983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest_df = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n\nprint(\"Data is loaded\")","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:00.541609Z","iopub.execute_input":"2021-10-19T16:09:00.541977Z","iopub.status.idle":"2021-10-19T16:09:00.619978Z","shell.execute_reply.started":"2021-10-19T16:09:00.541871Z","shell.execute_reply":"2021-10-19T16:09:00.618954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. EDA","metadata":{}},{"cell_type":"code","source":"print (\"Train: \",train_df.shape[0],\"sales, and \",train_df.shape[1],\"features\")\nprint (\"Test: \",test_df.shape[0],\"sales, and \",test_df.shape[1],\"features\")","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:01.360756Z","iopub.execute_input":"2021-10-19T16:09:01.361419Z","iopub.status.idle":"2021-10-19T16:09:01.368877Z","shell.execute_reply.started":"2021-10-19T16:09:01.361373Z","shell.execute_reply":"2021-10-19T16:09:01.367766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:01.709399Z","iopub.execute_input":"2021-10-19T16:09:01.709681Z","iopub.status.idle":"2021-10-19T16:09:01.749668Z","shell.execute_reply.started":"2021-10-19T16:09:01.709653Z","shell.execute_reply":"2021-10-19T16:09:01.748643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:01.899502Z","iopub.execute_input":"2021-10-19T16:09:01.899771Z","iopub.status.idle":"2021-10-19T16:09:01.937564Z","shell.execute_reply.started":"2021-10-19T16:09:01.899742Z","shell.execute_reply":"2021-10-19T16:09:01.936513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1 SalePrice Distribution (Target Variable)","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n\n# Check the distribution \nsns.distplot(train_df['SalePrice'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:02.443751Z","iopub.execute_input":"2021-10-19T16:09:02.444712Z","iopub.status.idle":"2021-10-19T16:09:02.82904Z","shell.execute_reply.started":"2021-10-19T16:09:02.444671Z","shell.execute_reply":"2021-10-19T16:09:02.828055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graph above, it shows that the price is right skewed. Skew data makes a model difficult to find a proper pattern in the data which is the reason why we have to make a skew data into normal or Gaussian one. The log-transformation does remove or reduce skewness.","metadata":{}},{"cell_type":"code","source":"# Skew\nprint(\"Skewness: %f\" % train_df['SalePrice'].skew())","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:02.831263Z","iopub.execute_input":"2021-10-19T16:09:02.831587Z","iopub.status.idle":"2021-10-19T16:09:02.837779Z","shell.execute_reply.started":"2021-10-19T16:09:02.831537Z","shell.execute_reply":"2021-10-19T16:09:02.836731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2 Correlation","metadata":{}},{"cell_type":"markdown","source":"#### 1.2.1 Heatmap","metadata":{}},{"cell_type":"code","source":"corr = train_df.corr()\nplt.subplots(figsize=(15,12))\nsns.heatmap(corr, vmax=0.9, cmap='coolwarm', square=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:03.396052Z","iopub.execute_input":"2021-10-19T16:09:03.396325Z","iopub.status.idle":"2021-10-19T16:09:04.761657Z","shell.execute_reply.started":"2021-10-19T16:09:03.396293Z","shell.execute_reply":"2021-10-19T16:09:04.760984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.corr()['SalePrice'].sort_values()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:04.763408Z","iopub.execute_input":"2021-10-19T16:09:04.763881Z","iopub.status.idle":"2021-10-19T16:09:04.779121Z","shell.execute_reply.started":"2021-10-19T16:09:04.763845Z","shell.execute_reply":"2021-10-19T16:09:04.778311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the result above, **OverallQual** and **GrLivArea** are highly correlated with the SalePrice. From this observation we should look up at to these two features and check if there are outliers visible.","metadata":{}},{"cell_type":"markdown","source":"#### 1.2.2 GrLivArea","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(data=train_df,x='GrLivArea', y='SalePrice')\nplt.axhline(y=300000, color='r')\nplt.axvline(x=4550, color='r')","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:04.780505Z","iopub.execute_input":"2021-10-19T16:09:04.781106Z","iopub.status.idle":"2021-10-19T16:09:05.10612Z","shell.execute_reply.started":"2021-10-19T16:09:04.781064Z","shell.execute_reply":"2021-10-19T16:09:05.105179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graph above the there are outliers there are two visible outliers.","metadata":{}},{"cell_type":"code","source":"train_df[(train_df['GrLivArea']>4500) & (train_df['SalePrice']<300000)][['SalePrice', 'GrLivArea']]","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:05.10779Z","iopub.execute_input":"2021-10-19T16:09:05.108103Z","iopub.status.idle":"2021-10-19T16:09:05.119862Z","shell.execute_reply.started":"2021-10-19T16:09:05.10807Z","shell.execute_reply":"2021-10-19T16:09:05.118991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.2.3 OverallQual","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(data=train_df,x='OverallQual', y='SalePrice')\nplt.axvline(x=4.9,color='r')\nplt.axhline(y=650000,color='r')","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:05.268612Z","iopub.execute_input":"2021-10-19T16:09:05.268873Z","iopub.status.idle":"2021-10-19T16:09:05.58953Z","shell.execute_reply.started":"2021-10-19T16:09:05.268845Z","shell.execute_reply":"2021-10-19T16:09:05.587311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[(train_df['OverallQual']<5) & (train_df['SalePrice']<200000)][['SalePrice', 'OverallQual']]","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:05.650038Z","iopub.execute_input":"2021-10-19T16:09:05.650292Z","iopub.status.idle":"2021-10-19T16:09:05.663571Z","shell.execute_reply.started":"2021-10-19T16:09:05.650264Z","shell.execute_reply":"2021-10-19T16:09:05.662794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3 Check on Missing Value","metadata":{}},{"cell_type":"code","source":"# Define function to get the percentage of missing values in attributes\ndef missing_percent(train_df):\n    nan_percent= 100*(train_df.isnull().sum()/len(train_df))\n    nan_percent= nan_percent[nan_percent>0].sort_values()\n    return nan_percent","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:06.018324Z","iopub.execute_input":"2021-10-19T16:09:06.019149Z","iopub.status.idle":"2021-10-19T16:09:06.024656Z","shell.execute_reply.started":"2021-10-19T16:09:06.019074Z","shell.execute_reply":"2021-10-19T16:09:06.023708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values in attrbiutes comparison in %\nnan_percent= missing_percent(train_df)\nnan_percent","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:06.25505Z","iopub.execute_input":"2021-10-19T16:09:06.255355Z","iopub.status.idle":"2021-10-19T16:09:06.276764Z","shell.execute_reply.started":"2021-10-19T16:09:06.25532Z","shell.execute_reply":"2021-10-19T16:09:06.275674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nmissing = train_df.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:06.43929Z","iopub.execute_input":"2021-10-19T16:09:06.439988Z","iopub.status.idle":"2021-10-19T16:09:06.881804Z","shell.execute_reply.started":"2021-10-19T16:09:06.439952Z","shell.execute_reply":"2021-10-19T16:09:06.881193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Convert non-numeric features to strings","metadata":{}},{"cell_type":"code","source":"# Some of the non-numeric predictors are stored as numbers; convert them into strings \ntrain_df['MSSubClass'] = train_df['MSSubClass'].apply(str)\ntrain_df['YrSold'] = train_df['YrSold'].astype(str)\ntrain_df['MoSold'] = train_df['MoSold'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:07.010121Z","iopub.execute_input":"2021-10-19T16:09:07.010434Z","iopub.status.idle":"2021-10-19T16:09:07.022732Z","shell.execute_reply.started":"2021-10-19T16:09:07.010401Z","shell.execute_reply":"2021-10-19T16:09:07.02201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1 Normalize Skewed Feature","metadata":{}},{"cell_type":"markdown","source":"#### 2.1.1 SalePrice Distribution (Target Variable)","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n\n# Check the distribution \nsns.distplot(train_df['SalePrice'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:07.64181Z","iopub.execute_input":"2021-10-19T16:09:07.642699Z","iopub.status.idle":"2021-10-19T16:09:07.988469Z","shell.execute_reply.started":"2021-10-19T16:09:07.642649Z","shell.execute_reply":"2021-10-19T16:09:07.987726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graph above, it shows that the price is right skewed. Skew data makes a model difficult to find a proper pattern in the data which is the reason why we have to make a skew data into normal or Gaussian one. The log-transformation does remove or reduce skewness.","metadata":{}},{"cell_type":"code","source":"# Apply log-transfomation (log(1+x))\ntrain_df[\"SalePrice\"] = np.log1p(train_df[\"SalePrice\"])","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:08.001437Z","iopub.execute_input":"2021-10-19T16:09:08.001947Z","iopub.status.idle":"2021-10-19T16:09:08.008251Z","shell.execute_reply.started":"2021-10-19T16:09:08.001876Z","shell.execute_reply":"2021-10-19T16:09:08.007391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2.2 After apply Log-Transformation","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(train_df['SalePrice'] , fit=norm, color=\"b\");\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_df['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:08.378597Z","iopub.execute_input":"2021-10-19T16:09:08.378894Z","iopub.status.idle":"2021-10-19T16:09:08.965095Z","shell.execute_reply.started":"2021-10-19T16:09:08.37886Z","shell.execute_reply":"2021-10-19T16:09:08.964022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div align='left'><font size=\"3\" color=\"#000000\"> The graph now normally distributed and shows a better result as it does not have skewness present after applying log-transformation.\n</font></div>","metadata":{}},{"cell_type":"markdown","source":"### 2.3 Deal with Outliers","metadata":{}},{"cell_type":"markdown","source":"#### 2.3.1 What is an Outlier?\n<div align='left'><font size=\"3\" color=\"#000000\"> Outlier is an observation that is numerically distant from the rest of the data or in a simple word it is the value which is out of the range. letâ€™s take an example to check what happens to a data set with and data set without outliers.\n</font></div>\n\n|| | Data without outlier |  | Data with outlier | \n|--||--||--|\n|**Data**| |1,2,3,3,4,5,4 |  |1,2,3,3,4,5,**400** | \n|**Mean**| |3.142 | |**59.714** |  \n|**Median**| |3|  |3|\n|**Standard Deviation**| |1.345185| |**150.057**|\n\n<div align='left'><font size=\"3\" color=\"#000000\"> As you can see, data set with outliers has significantly different mean and standard deviation. In the first scenario, we will say that average is 3.14. But with the outlier, average soars to 59.71. This would change the estimate completely.\n</font></div>\n\n<div align='left'><font size=\"3\" color=\"#000000\"> Lets take a real world example. In a company of 50 employees, 45 people having monthly salary of Rs.6,000, 5 senior employees having monthly salary of Rs.100000 each. If you calculate the average monthly salary of employees in the company is Rs.14,500, which will give you the wrong conclusion (majority of employees have lesser than 14.5k salary). But if you take median salary, it is Rs.6000 which is more sense than the average. For this reason median is appropriate measure than mean. Here you can see the effect of outlier.\n</font></div>    \n<hr>   \n<div class=\"alert alert-info\" ><font size=\"3\"><strong> Outlier </strong> is a commonly used terminology by analysts and data scientists as it needs close attention else it can result in wildly wrong estimations. Simply speaking, Outlier is an observation that appears far away and diverges from an overall pattern in a sample.</div>","metadata":{}},{"cell_type":"markdown","source":"Source and credit to https://www.kaggle.com/nareshbhat/outlier-the-silent-killer/notebook","metadata":{}},{"cell_type":"markdown","source":"#### 2.3.2 Remove Outliers","metadata":{}},{"cell_type":"code","source":"# Remove outliers\ntrain_df.drop(train_df[(train_df['OverallQual'] < 5) & (train_df['SalePrice'] > 200000)].index, inplace=True)\ntrain_df.drop(train_df[(train_df['GrLivArea'] > 4500) & (train_df['SalePrice'] < 300000)].index, inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:10.184739Z","iopub.execute_input":"2021-10-19T16:09:10.18503Z","iopub.status.idle":"2021-10-19T16:09:10.200654Z","shell.execute_reply.started":"2021-10-19T16:09:10.184997Z","shell.execute_reply":"2021-10-19T16:09:10.199846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### 3.1 Splitting Dataset","metadata":{}},{"cell_type":"code","source":"X = train_df.drop(columns=['SalePrice'])\ny = train_df['SalePrice']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2 Generate Pipeline for Transformations","metadata":{}},{"cell_type":"code","source":"#  integer category\nint_cat_features = list(X.select_dtypes(include='int64').columns)\nint_cat_transformers = Pipeline(steps=[('imputer', SimpleImputer()),\n                                      ('scale', StandardScaler())])\n\n# string category\nstr_cat_features = list(X.select_dtypes(include='object').columns)\nstr_cat_transformers = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n                                       ('one-hot', OneHotEncoder(handle_unknown='ignore'))])\n\n# continues neumerical - floats\nfloat_cat_features = list(X.select_dtypes(include='float64').columns)\nfloat_cat_transformers = Pipeline(steps=[('imputer', SimpleImputer()),\n                                         ('scale', StandardScaler())])","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:11.696957Z","iopub.execute_input":"2021-10-19T16:09:11.697683Z","iopub.status.idle":"2021-10-19T16:09:11.710695Z","shell.execute_reply.started":"2021-10-19T16:09:11.697643Z","shell.execute_reply":"2021-10-19T16:09:11.709725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3 Building Model","metadata":{}},{"cell_type":"markdown","source":"#### 3.3 What is K-Fold Cross Validation?\n<div align='left'><font size=\"3\" color=\"#000000\"> K-Fold CV is where a given data set is split into a K number of sections/folds where each fold is used as a testing set at some point. Lets take the scenario of 5-Fold cross validation(K=5). Here, the data set is split into 5 folds. In the first iteration, the first fold is used to test the model and the rest are used to train the model. In the second iteration, 2nd fold is used as the testing set while the rest serve as the training set. This process is repeated until each fold of the 5 folds have been used as the testing set.\n</font></div>\n\n> <center><img src=\"https://miro.medium.com/max/2000/1*IjKy-Zc9zVOHFzMw2GXaQw.png\" width=\"800px\"></center>\n> <center><font size=\"3\" color=\"#000000\">5-Fold Cross Validation\n\nsource and credit to https://medium.datadriveninvestor.com/k-fold-cross-validation-6b8518070833","metadata":{}},{"cell_type":"code","source":"# Setup cross validation folds\nkf = KFold(n_splits=5, random_state=42, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:12.877639Z","iopub.execute_input":"2021-10-19T16:09:12.878742Z","iopub.status.idle":"2021-10-19T16:09:12.882781Z","shell.execute_reply.started":"2021-10-19T16:09:12.878698Z","shell.execute_reply":"2021-10-19T16:09:12.88194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model building\n\ndef model_building(model):\n    #applying transformations\n    preprocess = ColumnTransformer(transformers=[('int_cat', int_cat_transformers, int_cat_features),\n                                                 ('str_cat', str_cat_transformers, str_cat_features),\n                                                 ('float_cat', float_cat_transformers, float_cat_features)\n                                                ])\n    # preprocessing and modeling pipeline\n    pipe = Pipeline(steps=[('preprocessing', preprocess),\n                           ('modeling', model)])\n    \n    return pipe\n    \n# Cross validating\ndef cross_validate_pipeline(pipeline, X, y):\n    cv_scores = -cross_val_score(pipeline, X, y, scoring=\"neg_root_mean_squared_error\", cv=kf)\n    return cv_scores","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:13.111072Z","iopub.execute_input":"2021-10-19T16:09:13.111807Z","iopub.status.idle":"2021-10-19T16:09:13.118654Z","shell.execute_reply.started":"2021-10-19T16:09:13.111769Z","shell.execute_reply":"2021-10-19T16:09:13.117464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Model Comparison","metadata":{}},{"cell_type":"code","source":"models = [('SVR', SVR()),\n          ('XGBRegressor',XGBRegressor()),\n         ]\n\n\nfor name,model in models:\n    model_pipeline = model_building(model)\n    cv_scores = cross_validate_pipeline(model_pipeline, X, y)\n    print(f'{name :20} {cv_scores.mean()}')","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:09:13.96332Z","iopub.execute_input":"2021-10-19T16:09:13.963677Z","iopub.status.idle":"2021-10-19T16:09:18.200132Z","shell.execute_reply.started":"2021-10-19T16:09:13.963639Z","shell.execute_reply":"2021-10-19T16:09:18.199416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div align='left'><font size=\"3\" color=\"#000000\"> The results showed that XGBRegressor gave better results than SVR with an RMSE score.\n</font></div>\n\n> <center><img src=\"https://c.tenor.com/74hajejcvqwAAAAS/rock.gif\" width=\"320px\"></center>\n> <center><font size=\"3\" color=\"#000000\">Even so, can the models still be improved?\n\n<div align='left'><font size=\"3\" color=\"#000000\"> Another way to improve results is to do hyper-parameters tuning of each model using GridSearchCV or RandomSearchCV and also applying stacking. From there it can improve the model in terms of accuracy to predict.\n</font></div>","metadata":{}},{"cell_type":"markdown","source":"## 5. Model Improvement","metadata":{}},{"cell_type":"markdown","source":"### 5.1 RandomSearchCV","metadata":{}},{"cell_type":"code","source":"models = [('SVR',\n           SVR(),\n           {'modeling__C':[20,30,40],\n            'modeling__epsilon':[0.007,0.008,0.009],\n            'modeling__gamma':[0.0002,0.0003,0.0004],}),\n          \n          \n          ('XGBRegressor',\n           XGBRegressor(),\n           {'modeling__learning_rate':[0.01],\n            'modeling__max_depth':[4],\n            'modeling__n_estimators':[3000],\n            'modeling__subsample':[0.6,0.5,0.7,]})\n         ]\n#[2000,3000]\n\nfor name, model, param_grid in models:\n    pipe = model_building(model)\n    rs = RandomizedSearchCV(estimator = pipe, \n                            param_distributions = param_grid,\n                            scoring=\"neg_mean_squared_error\", \n                            cv = 5,\n                            n_iter = 5,\n                            random_state = 34)\n    rs.fit(X,y)\n    print(f'{name :20} {np.sqrt(np.negative(rs.best_score_))}')\n    print(f'{name :20} {rs.best_params_}')","metadata":{"execution":{"iopub.status.busy":"2021-10-19T16:29:37.054659Z","iopub.execute_input":"2021-10-19T16:29:37.054965Z","iopub.status.idle":"2021-10-19T16:32:06.290847Z","shell.execute_reply.started":"2021-10-19T16:29:37.054923Z","shell.execute_reply":"2021-10-19T16:32:06.289884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div align='left'><font size=\"3\" color=\"#000000\"> The results show that both model scores show much better accuracy than before, RandomSearchCV allows us to find the best parameters for the model to get the best score from various predefined parameters.\n</font></div>","metadata":{}},{"cell_type":"markdown","source":"### 5.2 Stacking","metadata":{}},{"cell_type":"markdown","source":"#### 5.2.2 What is stacking?\n<div align='left'><font size=\"3\" color=\"#000000\"> Stacking is an ensemble learning technique to combine multiple regression models via a meta-regressor. The StackingCVRegressor extends the standard stacking algorithm (implemented as StackingRegressor) using out-of-fold predictions to prepare the input data for the level-2 regressor.\n\n\n</font></div>\n\n<div align='left'><font size=\"3\" color=\"#000000\"> In the standard stacking procedure, the first-level regressors are fit to the same training set that is used prepare the inputs for the second-level regressor, which may lead to overfitting. The StackingCVRegressor, however, uses the concept of out-of-fold predictions: the dataset is split into k folds, and in k successive rounds, k-1 folds are used to fit the first level regressor. In each round, the first-level regressors are then applied to the remaining 1 subset that was not used for model fitting in each iteration. The resulting predictions are then stacked and provided -- as input data -- to the second-level regressor. After the training of the StackingCVRegressor, the first-level regressors are fit to the entire dataset for optimal predicitons.\n</font></div>\n\n> <center><img src=\"http://rasbt.github.io/mlxtend/user_guide/regressor/StackingCVRegressor_files/stacking_cv_regressor_overview.png\" width=\"500px\"></center>\n\nSource and credit to http://rasbt.github.io/mlxtend/user_guide/regressor/StackingCVRegressor/","metadata":{}},{"cell_type":"markdown","source":"#### 5.3 Hyper-Parameters Tuning","metadata":{}},{"cell_type":"code","source":"svr_model= SVR(gamma= 0.0003,\n               epsilon= 0.009,\n               C= 30)\n\nxgb_model= XGBRegressor(subsample=0.5,\n                        n_estimators=3000,\n                        max_depth=4,\n                        learning_rate=0.01)\n\n# Stack up all the models above, optimized using xgboost\nstack_reg = StackingCVRegressor(regressors=(svr_model, xgb_model),\n                                meta_regressor=svr_model,\n                                use_features_in_secondary=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-19T17:29:48.776762Z","iopub.execute_input":"2021-10-19T17:29:48.777243Z","iopub.status.idle":"2021-10-19T17:29:48.783903Z","shell.execute_reply.started":"2021-10-19T17:29:48.777195Z","shell.execute_reply":"2021-10-19T17:29:48.783128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div align='left'><font size=\"3\" color=\"#000000\"> In this implementation, previous regressions model that have already tuned will be be stack and SVR model will be apply as the MetaRegressor.\n</font></div>","metadata":{}},{"cell_type":"code","source":"stack_model = [('StackingCVRegressor', stack_reg)]\n\n\nfor name,model in stack_model:\n    model_pipeline = model_building(model)\n    cv_scores = cross_validate_pipeline(model_pipeline, X, y)\n    print(f'{name :20} {cv_scores.mean()}')","metadata":{"execution":{"iopub.status.busy":"2021-10-19T17:29:50.983866Z","iopub.execute_input":"2021-10-19T17:29:50.98418Z","iopub.status.idle":"2021-10-19T17:33:21.352357Z","shell.execute_reply.started":"2021-10-19T17:29:50.984145Z","shell.execute_reply":"2021-10-19T17:33:21.351314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div align='left'><font size=\"3\" color=\"#000000\"> The result gives a slight better result compare to two regressions model that have already tuned.\n</font></div>","metadata":{}},{"cell_type":"markdown","source":"## 6. Making Final Prediction","metadata":{}},{"cell_type":"code","source":"# modeling\nmodel = model_building(stack_reg)\n# training\nmodel.fit(X,y)\n# making predictions\npreds = model.predict(test_df)","metadata":{"execution":{"iopub.status.busy":"2021-10-19T17:53:38.226532Z","iopub.execute_input":"2021-10-19T17:53:38.226804Z","iopub.status.idle":"2021-10-19T17:54:30.492567Z","shell.execute_reply.started":"2021-10-19T17:53:38.226775Z","shell.execute_reply":"2021-10-19T17:54:30.491672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Submission","metadata":{}},{"cell_type":"code","source":"output = pd.DataFrame(data={'Id':test_df['Id'],'SalePrice':np.expm1(preds)})\noutput.to_csv('submission.csv', index=False)\nprint('Your submission was successfully saved!')","metadata":{"execution":{"iopub.status.busy":"2021-10-19T18:03:14.44867Z","iopub.execute_input":"2021-10-19T18:03:14.448977Z","iopub.status.idle":"2021-10-19T18:03:14.467325Z","shell.execute_reply.started":"2021-10-19T18:03:14.44894Z","shell.execute_reply":"2021-10-19T18:03:14.466709Z"},"trusted":true},"execution_count":null,"outputs":[]}]}