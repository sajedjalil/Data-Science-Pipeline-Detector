{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hi everybody. This is my first Data Science \"project\". I would appreciate a lot every advice or sugestion that you will do. I am here to learn so I will use everything that I will learn in the near future to improve this code. Help me to imporve faster :)\nAlso I will try to explain what I do in every piece of code, I may be wrong so feel free to correct me.\nI am not English so probably I will make some orthographic mistakes, forgive me.\n\nI want to give credit to one notebook that helped me a lot: \nhttps://www.kaggle.com/amiiiney/price-prediction-regularization-stacking#5--Outliers-detection\n\n(v2) I added Ridge and Lasso Regression","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First of all I load the data and get familiar with it","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path_train = \"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\"\npath_test = \"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\"\ntrain = pd.read_csv(path_train)\ntest = pd.read_csv(path_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train: \",train.shape)\nprint(\"Test: \",test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric = train.select_dtypes(exclude='object')\nprint(\"\\nNumber of numeric features : \",(len(numeric.axes[1])))\nprint(\"\\n\", numeric.axes[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I will study the data. For that I will split numerical and categorical features and study them a little bit","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Isolate the numeric features and check his relevance\n\nnum_corr = numeric.corr()\ntable = num_corr['SalePrice'].sort_values(ascending=False).to_frame()\ncm = sns.light_palette(\"green\", as_cmap=True)\ntb = table.style.background_gradient(cmap=cm)\ntb\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Isolate the categorical data\ncategorical = train.select_dtypes(include='object')\ncategorical.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\nNumber of categorical features : \",(len(categorical.axes[1])))\nprint(\"\\n\", categorical.axes[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have decided to delete all features with 80% missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# I will drop features with 80% missing values\ntrain_d = train.dropna(thresh=len(train)*0.8, axis=1)\ndroppedF = []\nprint(\"We dropped the next features: \")\nfor x in train.axes[1]:\n    if(x not in train_d.axes[1]):\n        droppedF.append(x)\n\nprint(droppedF)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I will drop this features also in the test dataset\ntest_d = test.drop(droppedF,axis=1)\nprint(train_d.shape, test_d.shape)\nsh_train = train_d.shape\n# I will also mix both (test and train) to do all at the same time\nc1 = pd.concat((train_d, test_d), sort=False).reset_index(drop=True)\n\nprint(\"Total size is :\",c1.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am observing which features have missing values to study what to do with them","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now I will detect and study what to do with missing values\nc1_NA = (c1.isnull().sum() / len(c1)*100)\nc1_NA = c1_NA.drop(c1_NA[c1_NA == 0].index).sort_values(ascending = False)\nplt.figure(figsize=(14,7))\nchart = sns.barplot(x=c1_NA.index , y = c1_NA)\nchart.set_xticklabels(chart.get_xticklabels(), rotation=90, horizontalalignment='right')\nplt.ylabel(\"Missing Value Percentage\")\nplt.xlabel(\"Features\")\nplt.title(\"Missing Value´s Percentage by Feature\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I wrote the next lines when I was coding in after a previous \"study\" of the features. When I wrote fill with 0 I mean 0 to numeric features and NA to cat. features(NA is not null, NA means    lack of in this dataset). I did what is written below plus some other methods that i found out they were necessary, everything is explained on the commentaries\n \n -----------------------------------------------------------------------------------------------------------------------------\n \n LotFrontage: Linear feet of street connected to property so I decided to fill the null values with the median\n I guess all or almost all this garage features that have null values is because they have no garage. I will fill this with 0( to categorical with label encoding) and create a features   has_garage.\n With basement features ocurrs the same than with garage so I will do the same\n Same with MasVnrType (it is a categorical, I will label encode it)\n MasVrnArea, fill it with 0\n For the rest I will fill it with 0 because they have not that much missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# First of all split numerical NA and categorical NA data\nNA=c1[c1_NA.index.to_list()]\ncatNA=NA.select_dtypes(include='object')\nnumNA=NA.select_dtypes(exclude='object')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start with the numerical ones\nnumNA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill with 0 and Garage Year Built fill it with the median to not disturb the data. Lot frontage also with the median\nFillw0 = ['MasVnrArea','BsmtFullBath','BsmtFinSF1','BsmtFinSF1','TotalBsmtSF','GarageCars','GarageArea','BsmtFinSF2','BsmtHalfBath','BsmtUnfSF']\nc1[Fillw0] = c1[Fillw0].fillna(0)\nc1['GarageYrBlt'] = c1.GarageYrBlt.fillna(c1.GarageYrBlt.median())\nc1['LotFrontage'] = c1.LotFrontage.fillna(c1.GarageYrBlt.median())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a look to the cat features\ncatNA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I will use the forward fill method to the ones with almost no null values.\n# I will fill the rest with NA for the same reason I filled numerical ones with 0\nf_forward = ['Electrical', 'SaleType', 'KitchenQual', 'Exterior1st',\n             'Exterior2nd', 'Functional', 'Utilities', 'MSZoning']\nfor col in c1[f_forward]:\n    c1[col] = c1[col].fillna(method='ffill')\ncatNA.drop(f_forward,axis=1)\n\nc1['has_garage'] = c1['GarageQual'].isnull().astype(int)\nc1['has_Bsmt'] = c1['BsmtCond'].isnull().astype(int)\nc1['has_MasVnr'] = c1['MasVnrType'].isnull().astype(int)\n\nfor col in catNA:\n    c1[col] = c1[col].fillna(\"NA\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we will encode the categorical features. For that I will use one hot encoding and I will also add has_bathroom, has_garage, and has_MasVnr\n\ncb=pd.get_dummies(c1)\nprint(\"the shape of the original dataset\",c1.shape)\nprint(\"the shape of the encoded dataset\",cb.shape)\nprint(\"We have \",cb.shape[1]- c1.shape[1], 'new encoded features')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split again train and test\ntrain = cb[:sh_train[0]] #sh_train is the shape of train_d\ntest = cb[sh_train[0]:]\ntest = test.drop('SalePrice',axis=1)# Remove SalePrice from the test\nprint(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I wanted to study outliars in the most relevant features and that´s what I did. I detected and eliminated them","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Detect and remove the outliers in the most significant features\nfig = plt.figure(figsize=(15,15))\nax1 = plt.subplot2grid((3,2),(0,0))\nplt.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\n\n\n\nax1 = plt.subplot2grid((3,2),(0,1))\nplt.scatter(x = train['OverallQual'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('OverallQual', fontsize=13)\n\n\n\nax1 = plt.subplot2grid((3,2),(1,0))\nplt.scatter(x = train['MasVnrArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('MasVnrArea', fontsize=13)\n\n\n\nax1 = plt.subplot2grid((3,2),(1,1))\nplt.scatter(x = train['GarageArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GarageArea', fontsize=13)\n\n\n\n\nax1 = plt.subplot2grid((3,2),(2,0))\nplt.scatter(x = train['TotalBsmtSF'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('TotalBsmtSF', fontsize=13)\n\n\n\nax1 = plt.subplot2grid((3,2),(2,1))\nplt.scatter(x = train['1stFlrSF'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('1stFlrSF', fontsize=13)\nplt.show()\n \n\n \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove the detected outliers\ntrain = train.drop(train[(train['GrLivArea'] > 4000)&(train['SalePrice'] < 250000)].index)\ntrain = train.drop(train[(train['OverallQual'] == 10)&(train['SalePrice'] < 210000)].index)\ntrain = train.drop(train[(train['MasVnrArea'] > 1400)&(train['SalePrice'] < 300000)].index)\ntrain = train.drop(train[(train['GarageArea'] > 1200)&(train['SalePrice'] < 300000)].index)\ntrain = train.drop(train[(train['TotalBsmtSF'] > 5000)&(train['SalePrice'] < 250000)].index)\ntrain = train.drop(train[(train['1stFlrSF'] > 4000)&(train['SalePrice'] < 250000)].index)\n\nfig = plt.figure(figsize=(15,15))\nax1 = plt.subplot2grid((3,2),(0,0))\nplt.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\n\n\n\nax1 = plt.subplot2grid((3,2),(0,1))\nplt.scatter(x = train['OverallQual'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('OverallQual', fontsize=13)\n\n\n\nax1 = plt.subplot2grid((3,2),(1,0))\nplt.scatter(x = train['MasVnrArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('MasVnrArea', fontsize=13)\n\n\n\nax1 = plt.subplot2grid((3,2),(1,1))\nplt.scatter(x = train['GarageArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GarageArea', fontsize=13)\n\n\n\n\nax1 = plt.subplot2grid((3,2),(2,0))\nplt.scatter(x = train['TotalBsmtSF'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('TotalBsmtSF', fontsize=13)\n\n\n\nax1 = plt.subplot2grid((3,2),(2,1))\nplt.scatter(x = train['1stFlrSF'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('1stFlrSF', fontsize=13)\nplt.show()\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I normalized the SalePrice feature because I observed that the kurtosis and the skewness were not good. For that I used the log transformation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Study and correct (if it is necessary) the skewness and kurtosis.\nprint(\"Skewness:\", train['SalePrice'].skew())\nprint(\"Kurtosis: \",train['SalePrice'].kurt())\n\nplt.hist(train.SalePrice, bins=10, color='mediumpurple',alpha=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply log1p to correct this  distribution\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\nprint(\"Skewness:\", train['SalePrice'].skew())\nprint(\"Kurtosis: \",train['SalePrice'].kurt())\n\nplt.hist(train.SalePrice, bins=10, color='mediumpurple',alpha=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I compared Linear Regression and XGBRegressor because are the only two models I worked with haha\n\nNote(v2) I have read about Lasso Regression and Ridge Regression. I will see how they perform but I am sure that they will perform better than Linear Regression.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finally we will apply Linear Regression and XGBoost\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# First plit X and y\ny = train.SalePrice\nx = train.drop('SalePrice',axis=1)\n\n\n# Split the them into train and test to evaluate both models\nX_train, X_valid, y_train, y_valid = train_test_split(x, y,test_size = 0.2, random_state=0)\n\n# Define the model\nXGB = XGBRegressor(n_estimators=10,learning_rate=0.05)\nlr = LinearRegression()\n# Fit the model\nXGB.fit(X_train,y_train) \nlr.fit(X_train,y_train) \n# Get predictions\npred_XGB = XGB.predict(X_valid)\npred_lr = lr.predict(X_valid)\n# Calculate MAE\nmae_XGB =  mean_absolute_error(pred_XGB,y_valid)\nmae_lr =  mean_absolute_error(pred_lr,y_valid)\n# Uncomment to print MAE\nprint(\"Mean Absolute Error XGB:\" , mae_XGB)\nprint(\"Mean Absolute Error lr:\" , mae_lr)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best Alpha for Ridge Regression\nimport sklearn.model_selection as ms\nimport sklearn.model_selection as GridSearchCV #Cross-Validation\nfrom sklearn.linear_model import Ridge\n\nridge=Ridge()\nparameters= {'alpha':[x for x in range(1,101)]}\n\nridge_reg=ms.GridSearchCV(ridge, param_grid=parameters, scoring='neg_mean_squared_error', cv=3)\nridge_reg.fit(X_train,y_train)\nprint(\"The best value of Alpha is: \",ridge_reg.best_params_)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ridge Error\nridge_mod=Ridge(alpha=1)\nridge_mod.fit(X_train,y_train)\npred_rr=ridge_mod.predict(X_valid)\nmae_rr =  mean_absolute_error(pred_rr,y_valid)\n\nprint(\"Mean absolute error Ridge Regression: \",mae_rr )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best alpha Lasso Regression\nfrom sklearn.linear_model import Lasso\n\nparameters= {'alpha':[0.0001,0.0009,0.001,0.002,0.003,0.01,0.1,1,10,100]}\n\nlasso=Lasso()\nlasso_reg=ms.GridSearchCV(lasso, param_grid=parameters, scoring='neg_mean_squared_error', cv=3)\nlasso_reg.fit(X_train,y_train)\n\nprint('The best value of Alpha is: ',lasso_reg.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lasso Error\nlasso_mod=Ridge(alpha=0.0001)\nlasso_mod.fit(X_train,y_train)\npred_la=lasso_mod.predict(X_valid)\nmae_la =  mean_absolute_error(pred_la,y_valid)\n\nprint(\"Mean absolute error Lasso Regression: \",mae_la)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finlly I decided to use Ridge regression because performs better on this scenario. I also undo the log transformation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Based on that we will use Linear Regression\nRidge2 = Ridge(alpha=16)\n# Fit the model\nRidge2.fit(x,y)\n# Get predictions\npred_ridge2 = ridge2.predict(test)\nfinal = np.expm1(pred_ridge2) # Undo the log1p\npath_f = \"/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\"\nf = pd.read_csv(path_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'Id': test.Id,\n                       'SalePrice': final})\noutput.to_csv('sample_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thank´s to all that have read my first notebook. Do not heasitate on comment your suggestions,advices or questions :)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}