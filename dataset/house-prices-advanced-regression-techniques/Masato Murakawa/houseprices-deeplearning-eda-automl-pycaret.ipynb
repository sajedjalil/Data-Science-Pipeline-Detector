{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---\n# [House Prices - Advanced Regression Techniques][1]\n\nGoal: To predict the sales price for each house.\n\n---\n#### **The aim of this notebook is to**\n- **1. Conduct Exploratory Data Analysis (EDA) and Feature Engineering.**\n- **2. Use AutoML (PyCaret) for the entire ML pipeline.**\n- **3. Create and train a Deep Learning model with TensorFlow.**\n- **4. Learn how to use TPU.**\n\n---\n#### **Note:**\n- You can run this notebook on CPU, GPU, and TPU without changing codes.\n-  In this notebook, training model on TPU takes more time than on GPU or CPU, because of the small batch size, small datasets, ect. Please understand that I didn't optimize the experiment parameters for TPU.\n\n\n---\n#### **References:**\n Thanks to previous great codes and notebooks.\n\n- [PyCaret Tutoricals][2]\n - [Regression Tutorial - Level Beginner][3]\n - [Regression Tutorial  - Level Intermediate][4]\n \n- [How to Use Kaggle: Tensor Processing Units (TPUs)][6]\n\n---\n#### **My Previous Notebooks:**\n- [SpaceshipTitanic: EDA + TabTransformer[TensorFlow]][5]\n\n---\n### **If you find this notebook useful, or when you copy&edit this notebook, please do give me an upvote. It helps me keep up my motivation.**\n\n---\n[1]: https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques\n[2]: https://pycaret.gitbook.io/docs/get-started/tutorials\n[3]: https://github.com/pycaret/pycaret/blob/master/tutorials/Regression%20Tutorial%20Level%20Beginner%20-%20REG101.ipynb\n[4]: https://github.com/pycaret/pycaret/blob/master/tutorials/Regression%20Tutorial%20Level%20Intermediate%20-%20REG102.ipynb\n[5]: https://www.kaggle.com/code/masatomurakawamm/spaceshiptitanic-eda-tabtransformer-tensorflow\n[6]: https://www.kaggle.com/docs/tpu","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background:#05445E; border:0; border-radius: 12px; color:#D3D3D3\"><center>0. TABLE OF CONTENTS</center></h1>\n\n<ul class=\"list-group\" style=\"list-style-type:none;\">\n    <li><a href=\"#1\" class=\"list-group-item list-group-item-action\">1. Settings</a></li>\n    <li><a href=\"#2\" class=\"list-group-item list-group-item-action\">2. Data Loading</a></li>\n    <li><a href=\"#3\" class=\"list-group-item list-group-item-action\">3. EDA and Feature Engineering</a>\n        <ul class=\"list-group\" style=\"list-style-type:none;\">\n            <li><a href=\"#3.1\" class=\"list-group-item list-group-item-action\">3.1 AutoEDA with Sweetviz</a></li>\n            <li><a href=\"#3.2\" class=\"list-group-item list-group-item-action\">3.2 Feature Selection</a></li>\n            <li><a href=\"#3.3\" class=\"list-group-item list-group-item-action\">3.3 Target Distribution</a></li>\n            <li><a href=\"#3.4\" class=\"list-group-item list-group-item-action\">3.4 Numerical Features</a></li>\n            <li><a href=\"#3.5\" class=\"list-group-item list-group-item-action\">3.5 Categorical Features</a></li>\n            <li><a href=\"#3.6\" class=\"list-group-item list-group-item-action\">3.6 Validation Split</a></li>\n        </ul>\n    </li>\n    <li><a href=\"#4\" class=\"list-group-item list-group-item-action\">4. AutoML (PyCaret)</a></li>\n    <li><a href=\"#5\" class=\"list-group-item list-group-item-action\">5. Deep Learning</a>\n        <ul class=\"list-group\" style=\"list-style-type:none;\">\n            <li><a href=\"#5.1\" class=\"list-group-item list-group-item-action\">5.1 Creating Dataset</a></li>\n            <li><a href=\"#5.2\" class=\"list-group-item list-group-item-action\">5.2 Creating Model</a></li>\n            <li><a href=\"#5.3\" class=\"list-group-item list-group-item-action\">5.3 Training Model</a></li>\n            <li><a href=\"#5.4\" class=\"list-group-item list-group-item-action\">5.4 Inference</a></li>\n        </ul>\n    </li>\n    <li><a href=\"#6\" class=\"list-group-item list-group-item-action\">6. Cross Validation and Ensebmling</a></li>\n</ul>","metadata":{}},{"cell_type":"markdown","source":"<a id =\"1\"></a><h1 style=\"background:#05445E; border:0; border-radius: 12px; color:#D3D3D3\"><center>1. Settings</center></h1>","metadata":{}},{"cell_type":"code","source":"## Parameters\ndata_config = {'train.csv': '../input/house-prices-advanced-regression-techniques/train.csv',\n               'test.csv': '../input/house-prices-advanced-regression-techniques/test.csv',\n               'sample_submission.csv': '../input/house-prices-advanced-regression-techniques/sample_submission.csv',\n              }\n\nexp_config = {'competition_name': 'house-prices-advanced-regression-techniques',\n              'n_splits': 5,\n              'normalization': 'Robust',\n              'encoding': 'one_hot',\n              'n_sample_per_TPU_core': 32,\n              'batch_size': 128,\n              'learning_rate': 1e-3,\n              'train_epochs': 100,\n              'checkpoint_filepath': './tmp/model/exp.ckpt',\n              'cross_validation': True,\n             }\n\nmodel_config = {'model_input_shape': (423, ),\n                'model_units': [512, 128],\n                'dropout_rates': [0., 0.2],\n               }\n\nprint('Parameters setted!')","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:20:48.49238Z","iopub.execute_input":"2022-06-17T04:20:48.492923Z","iopub.status.idle":"2022-06-17T04:20:48.529715Z","shell.execute_reply.started":"2022-06-17T04:20:48.492822Z","shell.execute_reply":"2022-06-17T04:20:48.528426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Import dependencies \nimport numpy as np\nimport pandas as pd\nimport scipy as sp\n\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport sklearn\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\n\nimport os\nimport pathlib\nimport gc\nimport sys\nimport re\nimport math \nimport random\nimport time \nimport datetime as dt\nfrom tqdm import tqdm \n\nprint('Import done!')","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-06-17T04:20:48.531758Z","iopub.execute_input":"2022-06-17T04:20:48.532041Z","iopub.status.idle":"2022-06-17T04:20:59.300279Z","shell.execute_reply.started":"2022-06-17T04:20:48.53201Z","shell.execute_reply":"2022-06-17T04:20:59.298853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## For reproducible results    \ndef seed_all(s):\n    random.seed(s)\n    np.random.seed(s)\n    tf.random.set_seed(s)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n    os.environ['PYTHONHASHSEED'] = str(s) \n    print('Seeds setted!')\n    \nglobal_seed = 42\nseed_all(global_seed)","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-06-17T04:20:59.302278Z","iopub.execute_input":"2022-06-17T04:20:59.304028Z","iopub.status.idle":"2022-06-17T04:20:59.311825Z","shell.execute_reply.started":"2022-06-17T04:20:59.303977Z","shell.execute_reply":"2022-06-17T04:20:59.310871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n# [TPU] Distribution Strategy #\n\nA TPU has eight different *cores* and each of these cores acts as its own accelerator. (A TPU is sort of like having eight GPUs in one machine.) We tell TensorFlow how to make use of all these cores at once through a **distribution strategy**. Run the following cell to create the distribution strategy that we'll later apply to our model. We'll use the distribution strategy when we create our neural network model. Then, TensorFlow will distribute the training among the eight TPU cores by creating eight different replicas of the model, one for each core.","metadata":{}},{"cell_type":"code","source":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-17T04:20:59.313436Z","iopub.execute_input":"2022-06-17T04:20:59.313921Z","iopub.status.idle":"2022-06-17T04:20:59.34754Z","shell.execute_reply.started":"2022-06-17T04:20:59.313877Z","shell.execute_reply":"2022-06-17T04:20:59.346402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<a id =\"2\"></a><h1 style=\"background:#05445E; border:0; border-radius: 12px; color:#D3D3D3\"><center>2. Data Loading</center></h1>","metadata":{}},{"cell_type":"markdown","source":"---\n## [TPU] Loading the Competition Data ##\n\nWhen used with TPUs, datasets need to be stored in a [Google Cloud Storage bucket](https://cloud.google.com/storage/). You can use data from any public GCS bucket by giving its path just like you would data from `'/kaggle/input'`. The following will retrieve the GCS path for this competition's dataset.","metadata":{}},{"cell_type":"code","source":"competition_name = exp_config['competition_name']\n\n## Get GCS Path\nfrom kaggle_datasets import KaggleDatasets\n\nif tpu:\n    DATA_DIR = KaggleDatasets().get_gcs_path(competition_name) \n    \n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n    load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n    \n    for file_path in tf.io.gfile.glob(os.path.join(DATA_DIR, \"*\")):\n        file_name = file_path.split('/')[-1]\n        data_config[file_name] = file_path\n    \nelse:\n    DATA_DIR = '/kaggle/input/' + competition_name\n    save_locally = None\n    load_locally = None\n\nprint(f\"Data Directory Path: {DATA_DIR}\\n\")\nprint(\"Contents of Data Directory:\")\nfor file in tf.io.gfile.glob(os.path.join(DATA_DIR, \"*\")):\n    print(f\"\\t{file}\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-17T04:20:59.350771Z","iopub.execute_input":"2022-06-17T04:20:59.351394Z","iopub.status.idle":"2022-06-17T04:20:59.366826Z","shell.execute_reply.started":"2022-06-17T04:20:59.351346Z","shell.execute_reply":"2022-06-17T04:20:59.365846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After Loading data, we can conduct EDA or Feature Engineering just as like on CPU/GPU.\n\n---","metadata":{}},{"cell_type":"markdown","source":"### [File and Data Field Descriptions](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data)\n\n- **train.csv** - the training set\n- **test.csv** - the test set\n- **data_description.txt** - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here\n- **sample_submission.csv** - a benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms.\n\n\n---\n### [Submission & Evaluation](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview/evaluation)\n\n- Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)","metadata":{}},{"cell_type":"code","source":"## Data Loading\ntrain_df = pd.read_csv(data_config['train.csv'])\ntest_df = pd.read_csv(data_config['test.csv'])\nsubmission_df = pd.read_csv(data_config['sample_submission.csv'])\n\nprint(f'train_length: {len(train_df)}')\nprint(f'test_lenght: {len(test_df)}')\nprint(f'submission_length: {len(submission_df)}')","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-06-17T04:20:59.367952Z","iopub.execute_input":"2022-06-17T04:20:59.368255Z","iopub.status.idle":"2022-06-17T04:20:59.472372Z","shell.execute_reply.started":"2022-06-17T04:20:59.368211Z","shell.execute_reply":"2022-06-17T04:20:59.471285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Null Value Check\nprint('train_df.info()'); print(train_df.info(), '\\n')\nprint('test_df.info()'); print(test_df.info(), '\\n')\n\n## train_df Check\ntrain_df.head()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-17T04:20:59.473944Z","iopub.execute_input":"2022-06-17T04:20:59.474306Z","iopub.status.idle":"2022-06-17T04:20:59.593036Z","shell.execute_reply.started":"2022-06-17T04:20:59.47426Z","shell.execute_reply":"2022-06-17T04:20:59.592065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"3\"></a><h1 style=\"background:#05445E; border:0; border-radius: 12px; color:#D3D3D3\"><center>3. EDA and Feature Engineering</center></h1>","metadata":{}},{"cell_type":"markdown","source":"<a id =\"3.1\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>3.1 AutoEDA with Sweetviz</center></h2>","metadata":{}},{"cell_type":"code","source":"## Import dependencies\n!pip install -U -q sweetviz \nimport sweetviz","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-17T04:20:59.594398Z","iopub.execute_input":"2022-06-17T04:20:59.594644Z","iopub.status.idle":"2022-06-17T04:21:14.899297Z","shell.execute_reply.started":"2022-06-17T04:20:59.594613Z","shell.execute_reply":"2022-06-17T04:21:14.898184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#my_report = sweetviz.analyze(train_df, \"SalePrice\")\nmy_report = sweetviz.compare([train_df, \"Train\"], [test_df, \"Test\"], \"SalePrice\")\nmy_report.show_notebook()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:21:14.90134Z","iopub.execute_input":"2022-06-17T04:21:14.901591Z","iopub.status.idle":"2022-06-17T04:22:46.583982Z","shell.execute_reply.started":"2022-06-17T04:21:14.901558Z","shell.execute_reply":"2022-06-17T04:22:46.581152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"3.2\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>3.2 Feature Selection</center></h2>","metadata":{}},{"cell_type":"code","source":"## Drop the columns which contain null values more than 500.\ndef feature_selection(dataframe):\n    df = dataframe.copy()\n    for column in df.columns:\n        n_null = train_df[column].isnull().sum()\n        if n_null > 500:\n            df = df.drop([column], axis=1)\n    return df\n\n## Feature selection on train data\ntrain = feature_selection(train_df)\nprint(len(train_df.columns), len(train.columns))","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:22:46.588704Z","iopub.execute_input":"2022-06-17T04:22:46.589204Z","iopub.status.idle":"2022-06-17T04:22:46.639761Z","shell.execute_reply.started":"2022-06-17T04:22:46.589146Z","shell.execute_reply":"2022-06-17T04:22:46.638614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Feature selection on test data\nfeature_list = list(train.columns)\nfeature_list.remove('SalePrice')\ntest = test_df[feature_list]","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:22:46.642974Z","iopub.execute_input":"2022-06-17T04:22:46.643234Z","iopub.status.idle":"2022-06-17T04:22:46.650526Z","shell.execute_reply.started":"2022-06-17T04:22:46.643204Z","shell.execute_reply":"2022-06-17T04:22:46.64941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Select numerical and categorical features\ndef get_numerical_categorical(df, feature_list, n_unique=False):\n    numerical_features = []\n    categorical_features = []\n    for column in feature_list:\n        if n_unique:\n            if df[column].nunique() > n_unique:\n                numerical_features.append(column)\n            else:\n                categorical_features.append(column)\n        else:\n            if df[column].dtypes == 'object': \n                categorical_features.append(column)\n            else:\n                numerical_features.append(column)\n    return numerical_features, categorical_features\n\ntarget = 'SalePrice'\n\n## Features which has more than 30 unique values as numerical\nnumerical_features, categorical_features = get_numerical_categorical(train,\n                                                                     feature_list,\n                                                                     n_unique=30)\nnumerical_features.remove('Id')\nprint(len(numerical_features), len(categorical_features))","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:22:46.652561Z","iopub.execute_input":"2022-06-17T04:22:46.653213Z","iopub.status.idle":"2022-06-17T04:22:46.685279Z","shell.execute_reply.started":"2022-06-17T04:22:46.653163Z","shell.execute_reply":"2022-06-17T04:22:46.684126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Numerical features' dtype check \nfor n in range(len(numerical_features)):\n    print(numerical_features[n])\n    print(train[numerical_features[n]].dtypes)\n    print()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-17T04:22:46.686875Z","iopub.execute_input":"2022-06-17T04:22:46.687126Z","iopub.status.idle":"2022-06-17T04:22:46.700904Z","shell.execute_reply.started":"2022-06-17T04:22:46.687072Z","shell.execute_reply":"2022-06-17T04:22:46.700063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Categoical features' unique values check\nfor n in range(len(categorical_features)):\n    print(categorical_features[n])\n    print(train[categorical_features[n]].unique())\n    print()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-17T04:22:46.70231Z","iopub.execute_input":"2022-06-17T04:22:46.702616Z","iopub.status.idle":"2022-06-17T04:22:46.751168Z","shell.execute_reply.started":"2022-06-17T04:22:46.702579Z","shell.execute_reply":"2022-06-17T04:22:46.749902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"3.3\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>3.3 Target Distribution</center></h2>","metadata":{}},{"cell_type":"code","source":"sns.histplot(x=target, data=train, kde=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:22:46.752959Z","iopub.execute_input":"2022-06-17T04:22:46.75367Z","iopub.status.idle":"2022-06-17T04:22:47.141073Z","shell.execute_reply.started":"2022-06-17T04:22:46.753602Z","shell.execute_reply":"2022-06-17T04:22:47.139861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### [Box-Cox Transformation](https://qiita.com/dyamaguc/items/b468ae66f9ce6ee89724) on 'SalePrice'","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(10, 8))\n\nlist_lambda = [-2, -1, -0.5, 0, 0.5, 1, 2]\nfor i, lmbda in enumerate(list_lambda):\n    boxcox = sp.stats.boxcox(train[target], lmbda=lmbda)\n    ax = fig.add_subplot(4, 2, i+1)\n    sns.histplot(data=boxcox, kde=True, ax=ax)\n    plt.title('lambda='+str(list_lambda[i]))\n    plt.xlabel('SalePrice')\n    \nauto_boxcox, best_lambda = sp.stats.boxcox(train[target], lmbda=None)\nax = fig.add_subplot(4, 2, 8)\nsns.histplot(data=auto_boxcox, kde=True, ax=ax)\nplt.title('lambda=' + str(round(best_lambda, 2)))\nplt.xlabel('SalePrice')\n    \nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:22:47.142742Z","iopub.execute_input":"2022-06-17T04:22:47.142998Z","iopub.status.idle":"2022-06-17T04:22:49.673928Z","shell.execute_reply.started":"2022-06-17T04:22:47.142967Z","shell.execute_reply":"2022-06-17T04:22:49.672681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Box-Cox Summary\nfig = plt.figure(figsize=(12, 5))\n\n## Box-Cox Transformation\nauto_boxcox, best_lambda = sp.stats.boxcox(train[target], lmbda=None)\nax = fig.add_subplot(1, 2, 1)\nsns.histplot(data=auto_boxcox, kde=True, ax=ax)\nplt.title('Transformed')\nplt.xlabel('SalePrice')\n\n## Reverse Transformation\nx = sp.special.inv_boxcox(auto_boxcox, best_lambda)\nax = fig.add_subplot(1, 2, 2)\nsns.histplot(x=x, data=train, kde=True, ax=ax)\nplt.title('Reverse Transformed')\nplt.xlabel('SalePrice')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:22:49.675401Z","iopub.execute_input":"2022-06-17T04:22:49.675656Z","iopub.status.idle":"2022-06-17T04:22:50.263544Z","shell.execute_reply.started":"2022-06-17T04:22:49.675626Z","shell.execute_reply":"2022-06-17T04:22:50.262313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_target = pd.DataFrame()\n\n## Box-Cox Transformation on target ('SalePrice')\nauto_boxcox, best_lambda = sp.stats.boxcox(train[target], lmbda=None)\ntrain_target['target_transformed'] = auto_boxcox\n\n## Reverse Transformation\n#x = sp.special.inv_boxcox(auto_boxcox, best_lambda)\n\n## Box-Cox + Standardization\nbox_cox_mean = train_target['target_transformed'].mean()\nbox_cox_std = train_target['target_transformed'].std()\ntrain_target['target_transformed_standardized'] = (train_target['target_transformed'] - box_cox_mean) / box_cox_std","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:22:50.265269Z","iopub.execute_input":"2022-06-17T04:22:50.265609Z","iopub.status.idle":"2022-06-17T04:22:50.282341Z","shell.execute_reply.started":"2022-06-17T04:22:50.265567Z","shell.execute_reply":"2022-06-17T04:22:50.280935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"3.4\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>3.4 Numerical Features</center></h2>","metadata":{}},{"cell_type":"code","source":"## Distributions of numerical features\nfig = plt.figure(figsize=(10, 18))\nfor i, nf in enumerate(numerical_features):\n    ax = fig.add_subplot(6, 3, i+1)\n    sns.histplot(train[nf], kde=True, ax=ax)\n    plt.title(nf)\n    plt.xlabel(None)\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:22:50.284155Z","iopub.execute_input":"2022-06-17T04:22:50.284593Z","iopub.status.idle":"2022-06-17T04:22:55.384041Z","shell.execute_reply.started":"2022-06-17T04:22:50.284551Z","shell.execute_reply":"2022-06-17T04:22:55.383189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Heatmap of correlation matrix\nnumerical_columns = numerical_features + ['SalePrice']\ntrain_numerical = train[numerical_columns]\n\nfig = px.imshow(train_numerical.corr(),\n                color_continuous_scale='RdBu_r',\n                color_continuous_midpoint=0, \n                aspect='auto')\nfig.update_layout(height=600, \n                  width=600,\n                  title = \"Heatmap\",                  \n                  showlegend=False)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:22:55.38551Z","iopub.execute_input":"2022-06-17T04:22:55.386621Z","iopub.status.idle":"2022-06-17T04:22:56.504155Z","shell.execute_reply.started":"2022-06-17T04:22:55.386561Z","shell.execute_reply":"2022-06-17T04:22:56.503129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Statistics of training data\ntrain[numerical_features].describe().T.style.bar(subset=['mean'],)\\\n                        .background_gradient(subset=['std'], cmap='coolwarm')\\\n                        .background_gradient(subset=['50%'], cmap='coolwarm')\\\n                        .background_gradient(subset=['max'], cmap='coolwarm')","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:22:56.50575Z","iopub.execute_input":"2022-06-17T04:22:56.506021Z","iopub.status.idle":"2022-06-17T04:22:56.644989Z","shell.execute_reply.started":"2022-06-17T04:22:56.50599Z","shell.execute_reply":"2022-06-17T04:22:56.643937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Statistics of test data\ntest[numerical_features].describe().T.style.bar(subset=['mean'],)\\\n                        .background_gradient(subset=['std'], cmap='coolwarm')\\\n                        .background_gradient(subset=['50%'], cmap='coolwarm')\\\n                        .background_gradient(subset=['max'], cmap='coolwarm')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-17T04:22:56.646177Z","iopub.execute_input":"2022-06-17T04:22:56.646395Z","iopub.status.idle":"2022-06-17T04:22:56.721776Z","shell.execute_reply.started":"2022-06-17T04:22:56.646369Z","shell.execute_reply":"2022-06-17T04:22:56.720835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Feature Engineering on interval scales\ntarget_features = ['LotArea', 'YearBuilt', 'YearRemodAdd', 'GarageYrBlt']\ntarget_min = train[target_features].min()\ntest[target_features] = test[target_features] - target_min\ntrain[target_features] = train[target_features] - target_min\n\ntrain[numerical_features].describe().T.style.bar(subset=['mean'],)\\\n                        .background_gradient(subset=['std'], cmap='coolwarm')\\\n                        .background_gradient(subset=['50%'], cmap='coolwarm')\\\n                        .background_gradient(subset=['max'], cmap='coolwarm')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-17T04:22:56.72307Z","iopub.execute_input":"2022-06-17T04:22:56.723462Z","iopub.status.idle":"2022-06-17T04:22:56.802387Z","shell.execute_reply.started":"2022-06-17T04:22:56.723419Z","shell.execute_reply":"2022-06-17T04:22:56.801823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data Scaling","metadata":{}},{"cell_type":"code","source":"def data_scaling(dataframe,\n                 numerical_features,\n                 scaling_type='log_transform'):\n    df = dataframe.copy()\n    numerical_df = df[numerical_features]\n    mean_df = numerical_df.mean()\n    #var_df = numerical_df.var()\n    std_df = numerical_df.std()\n    min_df = numerical_df.min()\n    max_df = numerical_df.max()\n    quantiles = numerical_df.quantile([0.05, 0.95])\n    minus_flg_df = pd.DataFrame(np.where(numerical_df.values < 0, -1, 1), columns=numerical_df.columns)\n    \n    for nf in numerical_features:\n        numerical = numerical_df[nf].values\n        \n        ## Clipping on (mean Â± std * 3)\n        if scaling_type == 'clip_std':\n            nf_mean = mean_df[nf]\n            nf_std = std_df[nf]\n            threshold_1 = nf_mean - nf_std * 3\n            threshold_2 = nf_mean + nf_std * 3\n            numerical = np.where(numerical < threshold_1, threshold_1, numerical)\n            nuemrical = np.where(numerical > threshold_2, threshold_2, numerical)\n            \n        ## Clipping on Quantile\n        elif scaling_type == 'clip_quantile':\n            threshold_1 = quantiles[nf].values[0]\n            threshold_2 = quantiles[nf].values[1]\n            numerical = np.where(numerical < threshold_1, threshold_1, numerical)\n            numerical = np.where(numerical > threshold_2, threshold_2, numerical)\n            \n        ## Min-Max Scaling\n        elif scaling_type == 'min_max_scaling':\n            nf_min = min_df[nf]\n            nf_max = max_df[nf]\n            numerical = (numerical - nf_min) / (nf_max - nf_min)\n            \n        ## Logarithmic transformation\n        elif scaling_type == 'log_transform':\n            numerical = np.log(1 + np.abs(numerical))\n            numerical *= minus_flg_df[nf].values\n            \n        df[nf] = numerical\n    \n    return df\n\ntrain = data_scaling(train, numerical_features, scaling_type='log_transform')\ntest = data_scaling(test, numerical_features, scaling_type='log_transform')","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:22:56.803658Z","iopub.execute_input":"2022-06-17T04:22:56.804092Z","iopub.status.idle":"2022-06-17T04:22:56.84579Z","shell.execute_reply.started":"2022-06-17T04:22:56.804049Z","shell.execute_reply":"2022-06-17T04:22:56.844612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Statistics of training data after data scaling\ntrain[numerical_features].describe().T.style.bar(subset=['mean'],)\\\n                        .background_gradient(subset=['std'], cmap='coolwarm')\\\n                        .background_gradient(subset=['50%'], cmap='coolwarm')\\\n                        .background_gradient(subset=['max'], cmap='coolwarm')","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-06-17T04:22:56.847285Z","iopub.execute_input":"2022-06-17T04:22:56.847562Z","iopub.status.idle":"2022-06-17T04:22:56.921517Z","shell.execute_reply.started":"2022-06-17T04:22:56.847533Z","shell.execute_reply":"2022-06-17T04:22:56.920753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Statistics of test data after data scaling\ntest[numerical_features].describe().T.style.bar(subset=['mean'],)\\\n                        .background_gradient(subset=['std'], cmap='coolwarm')\\\n                        .background_gradient(subset=['50%'], cmap='coolwarm')\\\n                        .background_gradient(subset=['max'], cmap='coolwarm')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-17T04:22:56.922894Z","iopub.execute_input":"2022-06-17T04:22:56.923928Z","iopub.status.idle":"2022-06-17T04:22:57.002523Z","shell.execute_reply.started":"2022-06-17T04:22:56.923881Z","shell.execute_reply":"2022-06-17T04:22:57.001148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Handling Null Values and Data Normalization","metadata":{}},{"cell_type":"code","source":"## Fill NaN in numerical columns with its median\ntrain[numerical_features] = train[numerical_features].fillna(train[numerical_features].median()) \ntest[numerical_features] = test[numerical_features].fillna(train[numerical_features].median()) ","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:22:57.00452Z","iopub.execute_input":"2022-06-17T04:22:57.004834Z","iopub.status.idle":"2022-06-17T04:22:57.034898Z","shell.execute_reply.started":"2022-06-17T04:22:57.004802Z","shell.execute_reply":"2022-06-17T04:22:57.033728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def df_normalize(numerical_features, train_df, test_df, valid_df=None, normalization='Standard'):\n    \n    if normalization == 'MinMax':\n        sc = preprocessing.MinMaxScaler()\n    elif normalization == 'Robust':\n        sc = preprocessing.RobustScaler(quantile_range=(25.0, 75.0))\n    elif normalization == 'Standard':\n        sc = preprocessing.StandardScaler()\n        \n    #train = train_df.copy()\n    #train_numerical = sc.fit_transform(train[numerical_features])\n    sc.fit(train_df[numerical_features])\n    train_numerical = pd.DataFrame(sc.transform(train_df[numerical_features]),\n                                   columns=numerical_features)\n    test_numerical = pd.DataFrame(sc.transform(test_df[numerical_features]),\n                                  columns=numerical_features)\n    if valid_df is not None:\n        valid_numerical = pd.DataFrame(sc.transform(valid_df[numerical_features]),\n                                       columns=numerical_features)\n        return train_numerical, valid_numerical, test_numerical\n    return train_numerical, test_numerical\n\n\n## Robust normalization\nnormalization = exp_config['normalization']\ntrain_numerical, test_numerical = df_normalize(numerical_features,\n                                               train,\n                                               test,\n                                               normalization=normalization)\n\nprint(train_numerical.shape, test_numerical.shape)","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-06-17T04:22:57.036644Z","iopub.execute_input":"2022-06-17T04:22:57.037286Z","iopub.status.idle":"2022-06-17T04:22:57.06517Z","shell.execute_reply.started":"2022-06-17T04:22:57.037247Z","shell.execute_reply":"2022-06-17T04:22:57.063888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"3.5\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>3.5 Categorical Features</center></h2>","metadata":{}},{"cell_type":"code","source":"## Distributions of categorical features\nfig = plt.figure(figsize=(10, 50))\nfor i, cf in enumerate(categorical_features):\n    ax = fig.add_subplot(19, 3, i+1)\n    sns.histplot(train[cf], kde=False, ax=ax)\n    plt.title(cf)\n    plt.xlabel(None)\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:22:57.070697Z","iopub.execute_input":"2022-06-17T04:22:57.071317Z","iopub.status.idle":"2022-06-17T04:23:07.5836Z","shell.execute_reply.started":"2022-06-17T04:22:57.07126Z","shell.execute_reply":"2022-06-17T04:23:07.582883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Handling Null Values and Data Encoding","metadata":{}},{"cell_type":"code","source":"## Fill NaN in categorical columns with its mode\ntrain[categorical_features] = train[categorical_features].fillna(train[categorical_features].mode().iloc[0])  \ntest[categorical_features] = test[categorical_features].fillna(train[categorical_features].mode().iloc[0])  ","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:23:07.584873Z","iopub.execute_input":"2022-06-17T04:23:07.585319Z","iopub.status.idle":"2022-06-17T04:23:07.699116Z","shell.execute_reply.started":"2022-06-17T04:23:07.585281Z","shell.execute_reply":"2022-06-17T04:23:07.698145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def df_encode(categorical_features,\n              train_df,\n              test_df,\n              valid_df=None,\n              encoding='one_hot',\n              encoder=None,\n              return_encoder=False):\n    \n    if encoder is not None:\n        enc = encoder\n    else:\n        if encoding == 'one_hot':\n            enc = preprocessing.OneHotEncoder(handle_unknown='ignore',\n                                              sparse=False,\n                                              dtype=np.int32)\n            \n        elif encoding == 'label':\n            enc = preprocessing.OrdinalEncoder(handle_unknown='use_encoded_value',\n                                               unknown_value=-1,\n                                               dtype=np.int32)   \n        #train = train_df.copy()\n        #train_categorical = ohe.fit_transform(train[categorical_features])\n        enc.fit(train_df[categorical_features])\n        \n    train_categorical = pd.DataFrame(enc.transform(train_df[categorical_features]),\n                                     #columns=enc.get_feature_names_out(),\n                                     columns=enc.get_feature_names(),\n                                     )\n    test_categorical = pd.DataFrame(enc.transform(test_df[categorical_features]),\n                                    #columns=enc.get_feature_names_out(),\n                                    columns=enc.get_feature_names(),\n                                    )\n    if valid_df is not None:\n        valid_categorical = pd.DataFrame(enc.transform(valid_df[categorical_features]),\n                                         #columns=enc.get_feature_names_out(),\n                                         columns=enc.get_feature_names(),\n                                         )\n        if return_encoder:\n            return train_categorical, valid_categorical, test_categorical, enc\n        else:\n            return train_categorical, valid_categorical, test_categorical\n        \n    if return_encoder:\n        return train_categorical, test_categorical, enc\n    else:\n        return train_categorical, test_categorical","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:23:07.700959Z","iopub.execute_input":"2022-06-17T04:23:07.701371Z","iopub.status.idle":"2022-06-17T04:23:07.715409Z","shell.execute_reply.started":"2022-06-17T04:23:07.701325Z","shell.execute_reply":"2022-06-17T04:23:07.714477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## One-Hot Encoding \nencoding = exp_config['encoding']\n\n_, _, enc =  df_encode(categorical_features,\n                       train,\n                       test,\n                       encoding=encoding,\n                       return_encoder=True)\n    \ntrain_categorical, test_categorical = df_encode(categorical_features,\n                                                train,\n                                                test,\n                                                encoding=encoding,\n                                                encoder=enc)\n\nprint(train_categorical.shape, test_categorical.shape)","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-06-17T04:23:07.716724Z","iopub.execute_input":"2022-06-17T04:23:07.716975Z","iopub.status.idle":"2022-06-17T04:23:07.890252Z","shell.execute_reply.started":"2022-06-17T04:23:07.716944Z","shell.execute_reply":"2022-06-17T04:23:07.889485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"3.6\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>3.6 Validation Split</center></h2>","metadata":{}},{"cell_type":"code","source":"## K-Fold validation split\nn_splits = exp_config['n_splits']\nkf = KFold(n_splits=n_splits)\n\ntrain['k_folds'] = -1\ntrain_target['k_folds'] = -1\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(train)):\n    train['k_folds'][valid_idx] = fold\n    train_target['k_folds'][valid_idx] = fold\n    \nfor i in range(n_splits):\n    print(f\"fold {i}: {len(train.query('k_folds==@i'))} samples\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-17T04:23:07.891683Z","iopub.execute_input":"2022-06-17T04:23:07.892431Z","iopub.status.idle":"2022-06-17T04:23:07.958399Z","shell.execute_reply.started":"2022-06-17T04:23:07.892395Z","shell.execute_reply":"2022-06-17T04:23:07.957183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Hold-out validation split\nvalid_fold = train.query(f'k_folds == 0').reset_index(drop=True)\ntrain_fold = train.query(f'k_folds != 0').reset_index(drop=True)\n\nvalid_fold_target = train_target.query(f'k_folds == 0').reset_index(drop=True)\ntrain_fold_target = train_target.query(f'k_folds != 0').reset_index(drop=True)\n\nprint(len(train_fold), len(valid_fold))","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-06-17T04:23:07.959951Z","iopub.execute_input":"2022-06-17T04:23:07.960194Z","iopub.status.idle":"2022-06-17T04:23:07.98695Z","shell.execute_reply.started":"2022-06-17T04:23:07.960163Z","shell.execute_reply":"2022-06-17T04:23:07.986247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Normalization of numerical features\nnormalization = exp_config['normalization']\ntrain_numerical, valid_numerical, test_numerical = df_normalize(numerical_features,\n                                                                train_fold,\n                                                                test,\n                                                                valid_df=valid_fold,\n                                                                normalization=normalization)\n\n## Encoding of categorical features\nencoding = exp_config['encoding']\n_, _, enc =  df_encode(categorical_features,\n                       train,\n                       test,\n                       encoding=encoding,\n                       return_encoder=True)\n\ntrain_categorical, valid_categorical, test_categorical = df_encode(categorical_features,\n                                                                   train_fold,\n                                                                   test,\n                                                                   valid_df=valid_fold,\n                                                                   encoding=encoding,\n                                                                   encoder=enc,\n                                                                   return_encoder=False)\n\nprint(train_categorical.shape, test_categorical.shape)","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-06-17T04:23:07.988268Z","iopub.execute_input":"2022-06-17T04:23:07.989131Z","iopub.status.idle":"2022-06-17T04:23:08.180687Z","shell.execute_reply.started":"2022-06-17T04:23:07.989071Z","shell.execute_reply":"2022-06-17T04:23:08.179806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"4\"></a><h1 style=\"background:#05445E; border:0; border-radius: 12px; color:#D3D3D3\"><center>4. AutoML (PyCaret)</center></h1>","metadata":{}},{"cell_type":"markdown","source":"#### [PyCaret](https://pycaret.org/) is an open-source, low-code machine learning library in Python that automates machine learning workflows. \n\nUnfortunately, some PyCaret's codes doesn't work on Kaggle kernel when using GPU or TPU because of the differences between the library versions. Thus, I removed the cells operating PyCaret. If you are interested in AutoML or PyCaret, please check the previous version of this notebook (version 7 or more), and you can run them on CPU.","metadata":{}},{"cell_type":"markdown","source":"<a id =\"5\"></a><h1 style=\"background:#05445E; border:0; border-radius: 12px; color:#D3D3D3\"><center>5. Deep Learning</center></h1>","metadata":{}},{"cell_type":"markdown","source":"<a id =\"5.1\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>5.1 Creating Dataset</center></h2>","metadata":{}},{"cell_type":"markdown","source":"---\n## [TPU] Batch size ##\n\nTo go fast on a TPU, increase the batch size. The rule of thumb is to use batches of 128 elements per core (ex: batch size of 128*8=1024 for a TPU with 8 cores). At this size, the 128x128 hardware matrix multipliers of the TPU (see hardware section below) are most likely to be kept busy. You start seeing interesting speedups from a batch size of 8 per core though. In the sample above, the batch size is scaled with the core count through this line of code:","metadata":{}},{"cell_type":"code","source":"if tpu:\n    n_sample_per_TPU_core = exp_config['n_sample_per_TPU_core']\n    batch_size = n_sample_per_TPU_core * strategy.num_replicas_in_sync\nelse:\n    batch_size = exp_config['batch_size']","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:23:08.182038Z","iopub.execute_input":"2022-06-17T04:23:08.182312Z","iopub.status.idle":"2022-06-17T04:23:08.188655Z","shell.execute_reply.started":"2022-06-17T04:23:08.18228Z","shell.execute_reply":"2022-06-17T04:23:08.187419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"def df_to_dataset(numerical_dataframe, \n                  categorical_dataframe,\n                  target_dataframe=None,\n                  target_column='target_transformed',\n                  shuffle=False, repeat=False,\n                  batch_size=5, drop_remainder=False):\n    \n    numerical = numerical_dataframe.values\n    categorical = categorical_dataframe.values\n    features = np.concatenate([numerical, categorical], axis=1)\n    \n    if target_dataframe is not None:\n        target = target_dataframe[target_column]\n        ds = tf.data.Dataset.from_tensor_slices((features, target))\n    else:\n        ds = tf.data.Dataset.from_tensor_slices(features)\n        \n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(numerical))\n    if repeat:\n        ds = ds.repeat()\n    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n    ds = ds.prefetch(batch_size)\n    \n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:23:08.190795Z","iopub.execute_input":"2022-06-17T04:23:08.191118Z","iopub.status.idle":"2022-06-17T04:23:08.200614Z","shell.execute_reply.started":"2022-06-17T04:23:08.19102Z","shell.execute_reply":"2022-06-17T04:23:08.19941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create datasets\nbatch_size = exp_config['batch_size']\ntarget = 'target_transformed_standardized'\n\ntrain_ds = df_to_dataset(train_numerical,\n                         train_categorical,\n                         target_dataframe=train_fold_target,\n                         target_column=target,\n                         shuffle=True,\n                         repeat=False,\n                         batch_size=batch_size,\n                         drop_remainder=False,)\n    \nvalid_ds = df_to_dataset(valid_numerical,\n                         valid_categorical,\n                         target_dataframe=valid_fold_target,\n                         target_column=target,\n                         shuffle=False,\n                         repeat=False,\n                         batch_size=batch_size,\n                         drop_remainder=False,)\n\nf, t = next(iter(train_ds))\nprint(f.shape, t.shape)","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-06-17T04:23:08.202342Z","iopub.execute_input":"2022-06-17T04:23:08.202582Z","iopub.status.idle":"2022-06-17T04:23:08.354677Z","shell.execute_reply.started":"2022-06-17T04:23:08.202553Z","shell.execute_reply":"2022-06-17T04:23:08.353969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"5.2\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>5.2 Creating Model</center></h2>","metadata":{}},{"cell_type":"code","source":"def create_training_model(input_shape, model_units=[128,], dropout_rates=[0.2]):\n    \n    model_inputs = layers.Input(shape=input_shape)\n    x = model_inputs\n    \n    for units, dropout_rate in zip(model_units, dropout_rates):\n        feedforward = keras.Sequential([\n            layers.Dense(units, use_bias=False),\n            layers.BatchNormalization(),\n            layers.ReLU(),\n            layers.Dropout(dropout_rate),\n        ])\n        x = feedforward(x)\n        \n    final_layer = layers.Dense(units=1, activation=None)\n    model_outputs = final_layer(x)\n    \n    training_model = tf.keras.Model(inputs=model_inputs,\n                                    outputs=model_outputs)\n    return training_model","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:23:08.356055Z","iopub.execute_input":"2022-06-17T04:23:08.356478Z","iopub.status.idle":"2022-06-17T04:23:08.364055Z","shell.execute_reply.started":"2022-06-17T04:23:08.356445Z","shell.execute_reply":"2022-06-17T04:23:08.363178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"--- \n## [TPU] Model on TPUs ##\n\nThe strategy scope instructs Tensorflow to instantiate all the variables of the model in the memory of the TPU. The TPUClusterResolver.connect() call automatically enters the TPU device scope which instructs Tensorflow to run Tensorflow operations on the TPU. ","metadata":{}},{"cell_type":"code","source":"## Create training model\ninput_shape = model_config['model_input_shape']\nmodel_units = model_config['model_units']\ndropout_rates = model_config['dropout_rates']\n\nif tpu:\n    with strategy.scope():\n        training_model = create_training_model(input_shape=input_shape,\n                                               model_units=model_units, \n                                               dropout_rates=dropout_rates)\nelse:\n    training_model = create_training_model(input_shape=input_shape,\n                                           model_units=model_units, \n                                           dropout_rates=dropout_rates)\n\n## Model compile and build\nlr = exp_config['learning_rate']\noptimizer = keras.optimizers.Adam(learning_rate=lr)\nloss_fn = keras.losses.MeanSquaredError()\n\ntraining_model.compile(optimizer=optimizer,\n                       loss=loss_fn,\n                       metrics=[keras.metrics.mean_squared_error,])\n\ntraining_model.summary()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-17T04:23:08.365849Z","iopub.execute_input":"2022-06-17T04:23:08.366178Z","iopub.status.idle":"2022-06-17T04:23:08.550012Z","shell.execute_reply.started":"2022-06-17T04:23:08.366134Z","shell.execute_reply":"2022-06-17T04:23:08.54889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<a id =\"5.3\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>5.3 Training Model</center></h2>","metadata":{}},{"cell_type":"markdown","source":"---\n## [TPU] Model saving/loading on TPUs ##\nWhen loading and saving models TPU models from/to the local disk, the `experimental_io_device `option must be used. It can be omitted if writing to GCS because TPUs have direct access to GCS. This option does nothing on GPUs.\n\nTPU users will remember that in order to train a model on TPU, you have to instantiate the model in a TPUStrategy scope. The strategy scope instructs Tensorflow to instantiate all the variables of the model in the memory of the TPU. The TPUClusterResolver.connect() call automatically enters the TPU device scope which instructs Tensorflow to run Tensorflow operations on the TPU. Now if you call model.save('./model') when you are connected to a TPU, Tensorflow will try to run the save operations on the TPU and since the TPU is a network-connected accelerator that has no access to your local disk, the operation will fail. Notice that saving to GCS will work though. The TPU does have access to GCS. If you want to save a TPU model to your local disk, you need to run the saving operation on your local machine and that is what the `experimental_io_device='/job:localhost'` flag does.","metadata":{}},{"cell_type":"code","source":"## Settings for Training\nepochs = exp_config['train_epochs']\nbatch_size = exp_config['batch_size']\nsteps_per_epoch = len(train_fold)//batch_size \n\n## For saving the best model\ncheckpoint_filepath = exp_config['checkpoint_filepath']\nif tpu:\n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n        filepath=checkpoint_filepath, \n        save_weights_only=False, \n        monitor='val_loss', \n        mode='min', \n        save_best_only=True,\n        options=save_locally)\nelse:\n    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n        filepath=checkpoint_filepath, \n        save_weights_only=True, \n        monitor='val_loss', \n        mode='min', \n        save_best_only=True)\n\n## For the adjustment of learning rate\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=3,\n    cooldown=10,\n    min_lr=1e-5,\n    verbose=1)\n\n## Model training\nhistory = training_model.fit(train_ds,\n                             epochs=epochs,\n                             shuffle=True,\n                             validation_data=valid_ds,\n                             callbacks=[model_checkpoint_callback,\n                                        reduce_lr,\n                                        ]\n                             )\n\n## Load the best parameters\nif tpu:\n    with strategy.scope():\n        load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n        training_model = tf.keras.models.load_model(checkpoint_filepath,\n                                                    options=load_locally)\nelse:\n    training_model.load_weights(checkpoint_filepath)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-17T04:23:08.551304Z","iopub.execute_input":"2022-06-17T04:23:08.55153Z","iopub.status.idle":"2022-06-17T04:23:22.459445Z","shell.execute_reply.started":"2022-06-17T04:23:08.551503Z","shell.execute_reply":"2022-06-17T04:23:22.458373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"## Plot the train and valid losses\ndef plot_history(hist, title=None, valid=True):\n    plt.figure(figsize=(7, 5))\n    plt.plot(np.array(hist.index), hist['loss'], label='Train Loss')\n    if valid:\n        plt.plot(np.array(hist.index), hist['val_loss'], label='Valid Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title(title)\n    plt.show()\n    \nhist = pd.DataFrame(history.history)\nplot_history(hist)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:23:22.460875Z","iopub.execute_input":"2022-06-17T04:23:22.461174Z","iopub.status.idle":"2022-06-17T04:23:22.701221Z","shell.execute_reply.started":"2022-06-17T04:23:22.461138Z","shell.execute_reply":"2022-06-17T04:23:22.700096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"5.4\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>5.4 Inference</center></h2>","metadata":{}},{"cell_type":"code","source":"## Create test dataset\ntest_ds = df_to_dataset(test_numerical,\n                        test_categorical,\n                        target_dataframe=None,\n                        shuffle=False,\n                        repeat=False,\n                        batch_size=batch_size,\n                        drop_remainder=False,)\n\nf= next(iter(test_ds))\nprint(f.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:23:22.702663Z","iopub.execute_input":"2022-06-17T04:23:22.702886Z","iopub.status.idle":"2022-06-17T04:23:22.721785Z","shell.execute_reply.started":"2022-06-17T04:23:22.702858Z","shell.execute_reply":"2022-06-17T04:23:22.720877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = training_model.predict(test_ds)\npreds = np.squeeze(preds)\n\ndef reverse_transformation(preds,\n                           mean=box_cox_mean,\n                           std=box_cox_std,\n                           box_cox_lambda=best_lambda):\n    x = (preds * std) + mean\n    x = sp.special.inv_boxcox(x, box_cox_lambda)\n    return x \n\nprice_inference = reverse_transformation(preds)\nsubmission_df['SalePrice'] = price_inference\nsubmission_df.to_csv('submission_dnn.csv', index=False)\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:23:22.723307Z","iopub.execute_input":"2022-06-17T04:23:22.723579Z","iopub.status.idle":"2022-06-17T04:23:22.921666Z","shell.execute_reply.started":"2022-06-17T04:23:22.723539Z","shell.execute_reply":"2022-06-17T04:23:22.920363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"6\"></a><h1 style=\"background:#05445E; border:0; border-radius: 12px; color:#D3D3D3\"><center>6. Cross Validation and Ensebmling</center></h1>","metadata":{}},{"cell_type":"code","source":"if exp_config['cross_validation']:\n    ## Settings for Training\n    epochs = exp_config['train_epochs']\n    batch_size = exp_config['batch_size']\n    steps_per_epoch = len(train_fold)//batch_size \n    \n    cv_results = submission_df.drop('SalePrice', axis=1)\n    cv_results['preds_mean'] = 0.\n    \n    ## Create test dataset\n    test_ds = df_to_dataset(test_numerical,\n                            test_categorical,\n                            target_dataframe=None,\n                            shuffle=False,\n                            repeat=False,\n                            batch_size=batch_size,\n                            drop_remainder=False,)\n    \n    ## Create cross validation samples\n    for fold in range(exp_config['n_splits']):\n        valid_fold = train.query(f'k_folds == {fold}').reset_index(drop=True)\n        train_fold = train.query(f'k_folds != {fold}').reset_index(drop=True)\n        \n        train_fold = train_fold.drop(['k_folds'], axis=1)\n        valid_fold = valid_fold.drop(['k_folds'], axis=1)\n        \n        valid_fold_target = train_target.query(f'k_folds == {fold}').reset_index(drop=True)\n        train_fold_target = train_target.query(f'k_folds != {fold}').reset_index(drop=True)\n        \n        \n        ## Robust normalization\n        normalization = exp_config['normalization']\n        train_numerical, valid_numerical, test_numerical = df_normalize(numerical_features,\n                                                                train_fold,\n                                                                test,\n                                                                valid_df=valid_fold,\n                                                                normalization=normalization)\n        \n        ## One-Hot Encoding \n        train_categorical, valid_categorical, test_categorical = df_encode(categorical_features,\n                                                                   train_fold,\n                                                                   test,\n                                                                   valid_df=valid_fold,\n                                                                   encoding=encoding,\n                                                                   encoder=enc,\n                                                                   return_encoder=False)\n        \n        ## Create datasets\n        train_ds = df_to_dataset(train_numerical,\n                                 train_categorical,\n                                 target_dataframe=train_fold_target,\n                                 target_column=target,\n                                 shuffle=True,\n                                 repeat=False,\n                                 batch_size=batch_size,\n                                 drop_remainder=False,)\n        \n        valid_ds = df_to_dataset(valid_numerical,\n                                 valid_categorical,\n                                 target_dataframe=valid_fold_target,\n                                 target_column=target,\n                                 shuffle=False,\n                                 repeat=False,\n                                 batch_size=batch_size,\n                                 drop_remainder=False,)\n        \n        ## Create training model\n        input_shape = model_config['model_input_shape']\n        model_units = model_config['model_units']\n        dropout_rates = model_config['dropout_rates']\n        \n        if tpu:\n            with strategy.scope():\n                training_model = create_training_model(input_shape=input_shape,\n                                                       model_units=model_units, \n                                                       dropout_rates=dropout_rates)\n                \n        else:\n            training_model = create_training_model(input_shape=input_shape,\n                                                   model_units=model_units, \n                                                   dropout_rates=dropout_rates)\n            \n        ## Model compile and build\n        lr = exp_config['learning_rate']\n        optimizer = keras.optimizers.Adam(learning_rate=lr)\n        loss_fn = keras.losses.MeanSquaredError()\n        training_model.compile(optimizer=optimizer,\n                               loss=loss_fn,\n                               metrics=[keras.metrics.mean_squared_error,])\n        \n        ## For saving the best model\n        checkpoint_filepath = exp_config['checkpoint_filepath'] ## './tmp/model/exp.ckpt'\n        if fold is not None:\n            l = checkpoint_filepath.split('/')  ## ['.', 'tmp', 'model', 'exp.ckpt']\n            l[2] = l[2] + '_' + str(fold)\n            checkpoint_filepath = '/'.join(l) ## f'./tmp/model_{fold}/exp.ckpt'\n            \n        if tpu:\n            save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n            model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n                filepath=checkpoint_filepath, \n                save_weights_only=False, \n                monitor='val_loss', \n                mode='min', \n                save_best_only=True,\n                options=save_locally)  \n        else:\n            model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n                filepath=checkpoint_filepath, \n                save_weights_only=True, \n                monitor='val_loss', \n                mode='min', \n                save_best_only=True)\n            \n        ## For the adjustment of learning rate\n        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=3,\n            cooldown=10,\n            min_lr=1e-5,\n            verbose=0)\n        \n        ## Model training\n        history = training_model.fit(train_ds,\n                                     epochs=epochs,\n                                     shuffle=True,\n                                     validation_data=valid_ds,\n                                     callbacks=[model_checkpoint_callback,\n                                                reduce_lr],\n                                     verbose=0,\n                                    )\n        \n        ## Load the best parameters\n        if tpu:\n            with strategy.scope():\n                load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n                training_model = tf.keras.models.load_model(checkpoint_filepath,\n                                                            options=load_locally)\n        else:\n            training_model.load_weights(checkpoint_filepath)\n            \n        ## Plot the train and valid losses\n        hist = pd.DataFrame(history.history)\n        plot_history(hist, title=f'fold: {fold}')\n        \n        ## Inference\n        preds = training_model.predict(test_ds)\n        preds = np.squeeze(preds)\n        price_inference = reverse_transformation(preds)\n        \n        cv_results[f'preds_{fold}'] = price_inference\n        cv_results['preds_mean'] += price_inference\n        \n    ## Ensebmle the inferences of cross-validations\n    cv_results['preds_mean'] /= exp_config['n_splits']\n    preds_mean = cv_results['preds_mean'].values\n    submission_df['SalePrice'] = preds_mean\n    \n    submission_df.to_csv('submission_cv.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:23:22.923615Z","iopub.execute_input":"2022-06-17T04:23:22.923909Z","iopub.status.idle":"2022-06-17T04:24:21.976122Z","shell.execute_reply.started":"2022-06-17T04:23:22.923876Z","shell.execute_reply":"2022-06-17T04:24:21.974926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if exp_config['cross_validation']:\n    submission_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:24:21.977727Z","iopub.execute_input":"2022-06-17T04:24:21.97801Z","iopub.status.idle":"2022-06-17T04:24:21.987799Z","shell.execute_reply.started":"2022-06-17T04:24:21.977958Z","shell.execute_reply":"2022-06-17T04:24:21.987179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if exp_config['cross_validation']:\n    cv_results.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T04:24:21.989126Z","iopub.execute_input":"2022-06-17T04:24:21.989568Z","iopub.status.idle":"2022-06-17T04:24:22.012179Z","shell.execute_reply.started":"2022-06-17T04:24:21.989536Z","shell.execute_reply":"2022-06-17T04:24:22.011194Z"},"trusted":true},"execution_count":null,"outputs":[]}]}