{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <b>1 <span style='color:lightseagreen'>|</span> Introduction</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>1.1 | House Prices Prediction Competition</b></p>\n</div>\n\nAsk a home buyer to describe their **<span style='color:lightseagreen'>dream house</span>**, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this **<span style='color:lightseagreen'>playground competition's dataset</span>** proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\n![](https://storage.googleapis.com/kaggle-competitions/kaggle/5407/media/housesbanner.png)\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home. Practice skills: \n\n1. Creative feature engineering \n2. Advanced regression techniques like random forest and gradient boosting\n\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>1.2 | Acknowledgements</b></p>\n</div>\n\n* Kaggle's [Feature Engineering](https://www.kaggle.com/learn/feature-engineering).","metadata":{"_uuid":"56cb9382-64b1-4962-aedc-4abec286897e","_cell_guid":"4304522e-dec8-4969-975b-185d21683547","trusted":true}},{"cell_type":"code","source":"!pip install pycaret\nimport os\nimport warnings\nfrom pathlib import Path\nfrom IPython.display import clear_output\nfrom IPython.display import display\nfrom pandas.api.types import CategoricalDtype\n\n# Basic libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\nimport seaborn as sns\n\n# Clustering\nfrom sklearn.cluster import KMeans\n\n# Principal Component Analysis (PCA)\nfrom sklearn.decomposition import PCA\n\n#Mutual Information\nfrom sklearn.feature_selection import mutual_info_regression\n\n# Cross Validation\nfrom sklearn.model_selection import KFold, cross_val_score, StratifiedKFold, learning_curve, train_test_split\n\n# Encoders\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom category_encoders import MEstimateEncoder\n\n# Algorithms\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor\n\n# Optuna - Bayesian Optimization \nimport optuna\nfrom optuna.samplers import TPESampler\n\n# Plotly\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport plotly.offline as offline\nimport plotly.graph_objs as go\n\n# Metric\nfrom sklearn.metrics import mean_absolute_error as mae\n\n# PyCaret\nfrom pycaret.regression import *\n\n# Permutation Importance\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nwarnings.filterwarnings('ignore')\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n \n    return df\n\ndef load_data():\n    data_dir = Path(\"../input/house-prices-advanced-regression-techniques/\")\n    df_train = pd.read_csv(data_dir / \"train.csv\", index_col=\"Id\")\n    df_test = pd.read_csv(data_dir / \"test.csv\", index_col=\"Id\")\n    # Merge the splits so we can process them together\n    df = pd.concat([df_train, df_test])\n    return df\ndf_data = load_data()\nclear_output()","metadata":{"_uuid":"77f348b5-8945-4539-957d-a65387542f65","_cell_guid":"572ba60e-86a2-4cad-a24c-5fd2084c46da","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:03:51.587061Z","iopub.execute_input":"2022-03-25T16:03:51.587471Z","iopub.status.idle":"2022-03-25T16:04:02.446696Z","shell.execute_reply.started":"2022-03-25T16:03:51.587431Z","shell.execute_reply":"2022-03-25T16:04:02.445052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pp.ProfileReport(df_data)","metadata":{"_uuid":"141a9462-6ba2-4d4a-9694-9843c731412e","_cell_guid":"a0890301-89a3-421b-9995-e1a2f17aeafb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:02.449781Z","iopub.execute_input":"2022-03-25T16:04:02.450371Z","iopub.status.idle":"2022-03-25T16:04:02.455644Z","shell.execute_reply.started":"2022-03-25T16:04:02.45032Z","shell.execute_reply":"2022-03-25T16:04:02.454331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>2 <span style='color:lightseagreen'>|</span> Exploratory Data Analysis</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.1 | Target Analysis</b></p>\n</div>\n\n### 2.1.1 | Numerical Features","metadata":{}},{"cell_type":"code","source":"# Defining all our palette colours.\nprimary_blue = \"#496595\"\nprimary_blue2 = \"#85a1c1\"\nprimary_blue3 = \"#3f4d63\"\nprimary_grey = \"#c6ccd8\"\nprimary_black = \"#202022\"\nprimary_bgcolor = \"#f4f0ea\"\n\n# \"coffee\" pallette turqoise-gold.\nf1 = \"#a2885e\"\nf2 = \"#e9cf87\"\nf3 = \"#f1efd9\"\nf4 = \"#8eb3aa\"\nf5 = \"#235f83\"\nf6 = \"#b4cde3\"\n\ndef plot_box(fig, feature, r, c):\n    fig.add_trace(go.Box(x=df_data[feature].astype(object), y=df_data.SalePrice, marker = dict(color= px.colors.sequential.Viridis_r[5])), row =r, col = c)\n    fig.update_xaxes(showgrid = False, showline = True, linecolor = 'gray', linewidth = 2, zeroline = False,row = r, col = c)\n    fig.update_yaxes(showgrid = False, gridcolor = 'gray', gridwidth = 0.5, showline = True, linecolor = 'gray', linewidth = 2, row = r, col = c)\n    \ndef plot_scatter(fig, feature, r, c):\n    fig.add_trace(go.Scatter(x=df_data[feature], y=df_data.SalePrice, mode='markers', marker = dict(color=np.random.randn(10000), colorscale = px.colors.sequential.Viridis)), row = r, col = c)\n    fig.update_xaxes(showgrid = False, showline = True, linecolor = 'gray', linewidth = 2, zeroline = False, row = r, col = c)\n    fig.update_yaxes(showgrid = False, gridcolor = 'gray', gridwidth = 0.5, showline = True, linecolor = 'gray', linewidth = 2, row = r, col = c)\n    \n\ndef plot_hist(fig, feature, r, c):\n    fig.add_trace(go.Histogram(x=df_data['SalePrice'], name='Sale Price Distribution', marker = dict(color = px.colors.sequential.Viridis_r[5])), row = r, col = c)\n    fig.update_xaxes(showgrid = False, showline = True, linecolor = 'gray', linewidth = 2, row = r, col = c)\n    fig.update_yaxes(showgrid = False, gridcolor = 'gray', gridwidth = 0.5, showline = True, linecolor = 'gray', linewidth = 2, row = r, col = c)\n    \n# chart\nfig = make_subplots(rows=6, cols=3, \n                    specs=[[{\"type\": \"scatter\"}, {\"type\":\"histogram\"}, {'type':'scatter'}], [{'type':'scatter'}, {'type':'scatter'}, \n                    {'type':'scatter'}], [{'type':'box'}, {'type':'scatter'}, {'type':'scatter'}], [{\"type\": \"scatter\"}, {\"type\":\"box\"}, \n                    {'type':'scatter'}],[{\"type\": \"box\"}, {\"type\":\"scatter\"}, {'type':'box'}],[{\"type\": \"scatter\"}, {\"type\":\"scatter\"}, \n                    {'type':'scatter'}],], column_widths=[0.34, 0.33, 0.33], \n                    vertical_spacing=0.05, horizontal_spacing=0.1, subplot_titles=('Target vs LotFrontage',\"Target Distribution\",\n                    'Target vs MasVnrArea','Target vs BsmtFinSF1','Target vs BsmtFinSF2','Target vs TotalBsmtSF','Target vs BsmtFullBath',\n                    'Target vs GarageArea','Target vs GarageCars','Target vs LotArea','Target vs MSSubClass','Target vs YearBuilt','Target vs OverallQual',\n                    'Target vs YearRemodAdd','Target vs OverallCond','Target vs GrLivArea','Target vs 1stFlrSF','Target vs 2ndFlrSF'))\n\n# Charts\nplot_scatter(fig, 'LotFrontage',1,1)\nplot_hist(fig, 'SalePrice',1,2)\nplot_scatter(fig, 'MasVnrArea',1,3)\nplot_scatter(fig, 'BsmtFinSF1',2,1)\nplot_scatter(fig, 'BsmtFinSF2',2,2)\nplot_scatter(fig, 'TotalBsmtSF',2,3)\nplot_box(fig, 'BsmtFullBath', 3, 1)\nplot_scatter(fig, 'GarageArea',3,2)\nplot_box(fig, 'GarageCars', 3, 3)\nplot_scatter(fig, 'LotArea',4,1)\nplot_box(fig, 'MSSubClass',4,2)\nplot_scatter(fig, 'YearBuilt',4,3)\nplot_box(fig, 'OverallQual',5,1)\nplot_scatter(fig, 'YearRemodAdd',5,2)\nplot_box(fig, 'OverallCond',5,3)\nplot_scatter(fig, 'GrLivArea',6,1)\nplot_scatter(fig, '1stFlrSF', 6, 2)\nplot_scatter(fig, '2ndFlrSF', 6, 3)\n\n# General Styling\nfig.update_layout(height=1500, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>Sales Prices Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-25T16:04:02.461546Z","iopub.execute_input":"2022-03-25T16:04:02.461938Z","iopub.status.idle":"2022-03-25T16:04:03.112301Z","shell.execute_reply.started":"2022-03-25T16:04:02.461888Z","shell.execute_reply":"2022-03-25T16:04:03.111063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1.2 | Categorical Features","metadata":{}},{"cell_type":"code","source":"categorical_col = df_data.select_dtypes(['object']).columns[0:21]\n\ndef plot_box(fig, feature, r, c):\n    fig.add_trace(go.Box(x=df_data[feature].astype(object), y=df_data.SalePrice, marker = dict(color= px.colors.sequential.Viridis_r[5])), row =r, col = c)\n    fig.update_xaxes(showgrid = False, showline = True, linecolor = 'gray', linewidth = 2, zeroline = False,row = r, col = c)\n    fig.update_yaxes(showgrid = False, gridcolor = 'gray', gridwidth = 0.5, showline = True, linecolor = 'gray', linewidth = 2, row = r, col = c)\n    \ndef plot_scatter(fig, feature, r, c):\n    fig.add_trace(go.Scatter(x=df_data[feature], y=df_data.SalePrice, mode='markers', marker = dict(color=np.random.randn(10000), colorscale = px.colors.sequential.Viridis)), row = r, col = c)\n    fig.update_xaxes(showgrid = False, showline = True, linecolor = 'gray', linewidth = 2, zeroline = False, row = r, col = c)\n    fig.update_yaxes(showgrid = False, gridcolor = 'gray', gridwidth = 0.5, showline = True, linecolor = 'gray', linewidth = 2, row = r, col = c)\n    \n\ndef plot_hist(fig, feature, r, c):\n    fig.add_trace(go.Histogram(x=df_data['SalePrice'], name='Sale Price Distribution', marker = dict(color = px.colors.sequential.Viridis_r[5])), row = r, col = c)\n    fig.update_xaxes(showgrid = False, showline = True, linecolor = 'gray', linewidth = 2, row = r, col = c)\n    fig.update_yaxes(showgrid = False, gridcolor = 'gray', gridwidth = 0.5, showline = True, linecolor = 'gray', linewidth = 2, row = r, col = c)\n    \n# chart\nfig = make_subplots(rows=7, cols=3, \n                    column_widths=[0.34, 0.33, 0.33], \n                    vertical_spacing=0.05, horizontal_spacing=0.1, subplot_titles=('Target vs MSZoning',\"Target vs Street\",\n                    'Target vs Alley','Target vs LotShape','Target vs LandContour','Target vs Utilities','Target vs LotConfig',\n                    'Target vs LandSlope','Target vs Neighborhood','Target vs Condition1','Target vs Condition2','Target vs BldgType','Target vs HouseStyle',\n                    'Target vs RoofStyle','Target vs RoofMatl','Target vs Exterior1st','Target vs Exterior2nd','Target vs MasVnrType',\n                    'Target vs ExterQual','Target vs ExterCond','Target vs Foundation'))\n\nfor i in range(1,22):\n        if i%3 != 0:\n            plot_box(fig, categorical_col[i-1], int(i/3)+1, int(i%3))\n        else:\n            plot_box(fig, categorical_col[i-1], int(i/3), 3)\n\n# General Styling\nfig.update_layout(height=2000, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>Sales Prices Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-25T16:04:03.114656Z","iopub.execute_input":"2022-03-25T16:04:03.115213Z","iopub.status.idle":"2022-03-25T16:04:03.914975Z","shell.execute_reply.started":"2022-03-25T16:04:03.115169Z","shell.execute_reply":"2022-03-25T16:04:03.914151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_col = df_data.select_dtypes(['object']).columns[21:42]\n\ndef plot_box(fig, feature, r, c):\n    fig.add_trace(go.Box(x=df_data[feature].astype(object), y=df_data.SalePrice, marker = dict(color= px.colors.sequential.Viridis_r[5])), row =r, col = c)\n    fig.update_xaxes(showgrid = False, showline = True, linecolor = 'gray', linewidth = 2, zeroline = False,row = r, col = c)\n    fig.update_yaxes(showgrid = False, gridcolor = 'gray', gridwidth = 0.5, showline = True, linecolor = 'gray', linewidth = 2, row = r, col = c)\n    \ndef plot_scatter(fig, feature, r, c):\n    fig.add_trace(go.Scatter(x=df_data[feature], y=df_data.SalePrice, mode='markers', marker = dict(color=np.random.randn(10000), colorscale = px.colors.sequential.Viridis)), row = r, col = c)\n    fig.update_xaxes(showgrid = False, showline = True, linecolor = 'gray', linewidth = 2, zeroline = False, row = r, col = c)\n    fig.update_yaxes(showgrid = False, gridcolor = 'gray', gridwidth = 0.5, showline = True, linecolor = 'gray', linewidth = 2, row = r, col = c)\n    \n\ndef plot_hist(fig, feature, r, c):\n    fig.add_trace(go.Histogram(x=df_data['SalePrice'], name='Sale Price Distribution', marker = dict(color = px.colors.sequential.Viridis_r[5])), row = r, col = c)\n    fig.update_xaxes(showgrid = False, showline = True, linecolor = 'gray', linewidth = 2, row = r, col = c)\n    fig.update_yaxes(showgrid = False, gridcolor = 'gray', gridwidth = 0.5, showline = True, linecolor = 'gray', linewidth = 2, row = r, col = c)\n    \n# chart\nfig = make_subplots(rows=7, cols=3, \n                    column_widths=[0.34, 0.33, 0.33], \n                    vertical_spacing=0.05, horizontal_spacing=0.1, subplot_titles=('Target vs BsmtQual',\"Target vs BsmtCond\",\n                    'Target vs BsmtExposure','Target vs BsmtFinType1','Target vs BsmtFinType2','Target vs Heating','Target vs HeatingQC',\n                    'Target vs CentralAir','Target vs Electrical','Target vs KitchenQual','Target vs Functional','Target vs FireplaceQu','Target vs GarageType',\n                    'Target vs GarageFinish','Target vs GarageQual','Target vs GarageCond','Target vs PavedDrive','Target vs PoolQC', \n                     'Target vs Fence', 'Target vs MiscFeature','Saletype'))\n\nfor i in range(1,22):\n        if i%3 != 0:\n            plot_box(fig, categorical_col[i-1], int(i/3)+1, int(i%3))\n        else:\n            plot_box(fig, categorical_col[i-1], int(i/3), 3)\n\n# General Styling\nfig.update_layout(height=2000, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>Sales Prices Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-25T16:04:03.917862Z","iopub.execute_input":"2022-03-25T16:04:03.918123Z","iopub.status.idle":"2022-03-25T16:04:04.708803Z","shell.execute_reply.started":"2022-03-25T16:04:03.918092Z","shell.execute_reply":"2022-03-25T16:04:04.707698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1.3 | Heatmap - Most Correlated Features with Target","metadata":{}},{"cell_type":"code","source":"corr = df_data[df_data.select_dtypes(['float','int']).columns].corr()\nhighest_corr_features = corr.index[abs(corr[\"SalePrice\"])>0.5]\n\nfig = px.imshow(df_data[highest_corr_features].corr(), color_continuous_scale='RdBu_r', origin='lower', text_auto=True, aspect='auto')\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False)\nfig.update_yaxes(showgrid = True, gridcolor='gray',gridwidth=0.5, linecolor='gray',linewidth=2, zeroline = False)\n\n# General Styling\nfig.update_layout(height=500, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100, t=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>Heatmap - Most Correlated Features with Target</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-25T16:04:04.710096Z","iopub.execute_input":"2022-03-25T16:04:04.710314Z","iopub.status.idle":"2022-03-25T16:04:04.797681Z","shell.execute_reply.started":"2022-03-25T16:04:04.710287Z","shell.execute_reply":"2022-03-25T16:04:04.796881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.2 | OverallQual Analysis</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"df_graph = df_data[df_data['SalePrice'].isnull() == False]\n\n# chart\nfig = make_subplots(rows=1, cols=3, \n                    specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}, {'type':'bar'}]],\n                    column_widths=[0.33, 0.34, 0.33], vertical_spacing=0.1, horizontal_spacing=0.1,\n                    subplot_titles=(\"Mean Price per OverallQual\", \"Sold Houses per OverallQual\", \"TotalBsmtSF per OverallQual\"))\n\n# Left chart\ndf_qual = df_graph.groupby(['OverallQual']).agg({\"SalePrice\" : \"mean\"})\nvalues = list(range(10))\nfig.add_trace(go.Bar(x=df_qual.index, y=df_qual['SalePrice'], marker = dict(color=values, colorscale=\"Blugrn\"), name='Mean Price'), row=1, col=1)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=1)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=1)\n\n# Middle Chart\ndf_qual = df_graph.groupby(['OverallQual']).agg({\"SalePrice\" : \"count\"})\nvalues = list(range(10))\nfig.add_trace(go.Pie(values=df_qual['SalePrice'], labels=df_qual.index, name='Count',\n                     marker=dict(colors=px.colors.sequential.Blugrn)), row=1, col=2)\n\n# Right Chart\ndf_qual = df_graph.groupby(['OverallQual']).agg({\"TotalBsmtSF\" : \"mean\"})\nvalues = list(range(10))\nfig.add_trace(go.Bar(x=df_qual.index, y=df_qual['TotalBsmtSF'], marker = dict(color=values, colorscale=\"Blugrn\"), name='Count'), row=1, col=3)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=3)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=3)\n\n# General Styling\nfig.update_layout(height=400, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>OverallQual Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-25T16:04:04.799293Z","iopub.execute_input":"2022-03-25T16:04:04.799971Z","iopub.status.idle":"2022-03-25T16:04:04.884365Z","shell.execute_reply.started":"2022-03-25T16:04:04.799931Z","shell.execute_reply":"2022-03-25T16:04:04.883545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>2 <span style='color:lightseagreen'>|</span> Data Cleaning</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.1 | Garage, Basement, Frontage, Alley, Utilities, Fireplace and Masonry veneer</b></p>\n</div>\n\nWe'll start by loading the data and **<span style='color:lightseagreen'>concatenate</span>** train and test datasets, in order to **<span style='color:lightseagreen'>preprocess</span>** it, and then divide them again. We are going to start with some features that not every home has. We have some missing values for Garage and Basement. It is easily interpretable that they are due to its **<span style='color:lightseagreen'>absence</span>** in those homes, as not all houses have for example a Fire Place. Thus, we are going to replace missing values as follows: \n\n* For numerical features, we will replace NaN for 0.0\n* For object features, we will replace NaN for None.","metadata":{"_uuid":"d99874ba-0c2c-4fca-88ff-a304779a6ebd","_cell_guid":"e6603491-f6ce-45c3-a2c8-642f2ea4e717","trusted":true}},{"cell_type":"code","source":"missing_col = [col for col in df_data.columns if df_data[col].isnull().sum() >0]\nprint('{}\\n'.format(missing_col))","metadata":{"_uuid":"52ddc597-999a-460b-87a1-2393de162a46","_cell_guid":"4b8feb8e-70c9-44a0-8509-4641cf014656","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:04.885769Z","iopub.execute_input":"2022-03-25T16:04:04.886006Z","iopub.status.idle":"2022-03-25T16:04:04.923786Z","shell.execute_reply.started":"2022-03-25T16:04:04.885961Z","shell.execute_reply":"2022-03-25T16:04:04.922825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.2 | MSZoning, Exterior Covering, Kitchen Quality, Functional, SaleType</b></p>\n</div>\n\nAs you can observe, all of these features are categorical. Moreover, they have a **<span style='color:lightseagreen'>small percentage</span>** of missing values as show below. Therefore, we are going to fill these features missing values with most repeated values for each feature.","metadata":{"_uuid":"d805db92-4bb5-4e92-b53d-02556eb53268","_cell_guid":"5272c7d9-4794-494b-be5c-5bb6585eb6d2","trusted":true}},{"cell_type":"code","source":"columns = ['MSZoning','Exterior1st', 'Exterior2nd','KitchenQual', 'Functional','SaleType']\nfor col in columns: \n    print('Missing values percentage for {}: {}'.format(col, df_data[col].isnull().sum()/df_data.shape[0]))","metadata":{"_uuid":"c698552a-d0cd-4118-8b9d-6ae88f54e415","_cell_guid":"7a3d36ec-c5de-4511-91b8-d8605c5816a7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:04.925428Z","iopub.execute_input":"2022-03-25T16:04:04.926028Z","iopub.status.idle":"2022-03-25T16:04:04.940315Z","shell.execute_reply.started":"2022-03-25T16:04:04.925977Z","shell.execute_reply":"2022-03-25T16:04:04.938989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.3 | PoolQC, Fence, MiscFeature</b></p>\n</div>\n\nFinally, we are going to consider features with a **<span style='color:lightseagreen'>high percentage</span>** of missing values. Although they should be very uninformative features, we are going to keep them **<span style='color:lightseagreen'>until Feature Importance Analysis</span>**, which is next section. There, we'll decide whether to keep these features or not.","metadata":{"_uuid":"e83c8ca5-cb3c-46fc-9cf2-95a721fdaa61","_cell_guid":"0482458b-ee97-4ca5-b19f-65225c8c6833","execution":{"iopub.status.busy":"2022-02-28T14:24:10.330329Z","iopub.execute_input":"2022-02-28T14:24:10.331279Z","iopub.status.idle":"2022-02-28T14:24:10.338554Z","shell.execute_reply.started":"2022-02-28T14:24:10.331223Z","shell.execute_reply":"2022-02-28T14:24:10.337963Z"},"trusted":true}},{"cell_type":"code","source":"high_perc_columns = ['PoolQC','Fence','MiscFeature']\nfor col in high_perc_columns: \n    print('Missing values percentage for {}: {}'.format(col, df_data[col].isnull().sum()/df_data.shape[0]))","metadata":{"_uuid":"c33dce81-c33e-4af5-9a30-9f91efb26c5b","_cell_guid":"14203660-729b-4d71-9727-3a2071271d2c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:04.942418Z","iopub.execute_input":"2022-03-25T16:04:04.942802Z","iopub.status.idle":"2022-03-25T16:04:04.953015Z","shell.execute_reply.started":"2022-03-25T16:04:04.942756Z","shell.execute_reply":"2022-03-25T16:04:04.951826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.4 | Missing Values Function</b></p>\n</div>","metadata":{"_uuid":"1b29572b-2ec9-4af2-b4c0-dbe6b4cec21b","_cell_guid":"16dd36b9-007a-44a3-99b2-c65ca69ebe23","trusted":true}},{"cell_type":"code","source":"def fill_missing_values(df):\n    df_data = df.copy()\n    absent_features = ['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond',\n                  'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2',\n                     'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Electrical', 'BsmtFullBath', 'BsmtHalfBath', \n                  'LotFrontage','Alley','Utilities','FireplaceQu','MasVnrType', 'MasVnrArea']\n\n    numerical_features = [col for col in absent_features if df_data[col].dtype == float]\n    object_features = df_data.select_dtypes([\"object\"]).columns\n\n    df_data[numerical_features] = df_data[numerical_features].fillna(0.0)\n    df_data[object_features] = df_data[object_features].fillna('None')\n    \n    columns = ['MSZoning','Exterior1st', 'Exterior2nd','KitchenQual', 'Functional','SaleType']\n    for column in columns: \n        moda = df_data[column].value_counts().index[0]\n        df_data[column] = df_data[column].fillna(moda)\n        \n    high_perc_columns = ['PoolQC','Fence','MiscFeature']\n    for col in high_perc_columns: \n        if col == 'Fence' or col == 'MiscFeature':\n            df_data[col] = df_data[col].fillna('None')\n            \n    return df_data","metadata":{"_uuid":"8fb9d542-f89d-443e-9e3b-c3c8e7adf4fa","_cell_guid":"ad4d581a-5f22-4bac-88b6-96d2ab66cb61","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:04.955026Z","iopub.execute_input":"2022-03-25T16:04:04.95563Z","iopub.status.idle":"2022-03-25T16:04:04.968145Z","shell.execute_reply.started":"2022-03-25T16:04:04.955581Z","shell.execute_reply":"2022-03-25T16:04:04.967454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>3 <span style='color:lightseagreen'>|</span> Feature Engineering</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>3.1 | Heatmap - Correlation</b></p>\n</div>\n\nWe'll start by looking at what features have a **<span style='color:lightseagreen'>huge impact</span>** on sales prices. In our case, we are going to choose those features such that: **<span style='color:lightseagreen'>$|\\gamma|>0.5$</span>**, where $\\gamma$ is the correlation coefficient of each feature respect to sales price. To do that, we are going to plot a heatmap which is going to show us correlations between features and sale price.","metadata":{"_uuid":"01fd5f1d-4fff-42d9-a1a0-181aadc80692","_cell_guid":"16ce8b26-a87d-4aa8-90d5-03a1b384b56b","trusted":true}},{"cell_type":"code","source":"df_data = fill_missing_values(df_data)\ndf_train = df_data[df_data['SalePrice'].isnull() == False]\ndf_test = df_data[df_data['SalePrice'].isnull() == True]\n\ncorr = df_train.corr()\nhighest_corr_features = corr.index[abs(corr[\"SalePrice\"])>0.5]\n\nfig = px.imshow(df_train[highest_corr_features].corr(), color_continuous_scale='RdBu_r', origin='lower', text_auto=True, aspect='auto')\nfig.show()","metadata":{"_uuid":"18ee0f10-c878-454a-91bd-51a4d8a1dc5e","_cell_guid":"98a6a075-23bd-4ef9-8dff-8da9dbedd6d5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:04.969385Z","iopub.execute_input":"2022-03-25T16:04:04.97007Z","iopub.status.idle":"2022-03-25T16:04:05.096667Z","shell.execute_reply.started":"2022-03-25T16:04:04.970031Z","shell.execute_reply":"2022-03-25T16:04:05.095999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ“Œ **Interpret:** We can see that **<span style='color:lightseagreen'>OverQual</span>** is in the top of highest correlation with 0.79. It is followed by **<span style='color:lightseagreen'>GrLivArea</span>**, **<span style='color:lightseagreen'>GarageCars</span>** and **<span style='color:lightseagreen'>GarageArea</span>**. It is easily to observe that both **<span style='color:lightseagreen'>GarageCars</span>** and **<span style='color:lightseagreen'>GarageArea</span>** are quite correlated, as they have 0.88 correlation coefficient between then.","metadata":{"_uuid":"6faa7dcc-8319-4287-a1a3-3840b291b928","_cell_guid":"30bb2d01-35c9-4212-bb66-6ad372535aac","trusted":true}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>3.2 | Mutual Information</b></p>\n</div>\n\nMutual information describes **<span style='color:lightseagreen'>relationships</span>** in terms of **<span style='color:lightseagreen'>uncertainty</span>**. The mutual information (MI) between two quantities is a measure of the extent to which knowledge of one quantity reduces uncertainty about the other. If you knew the value of a feature, how much more confident would you be about the target? Scikit-learn has two mutual information **<span style='color:lightseagreen'>metrics</span>** in its feature_selection module: one for **<span style='color:lightseagreen'>real-valued targets</span>** (mutual_info_regression) and one for **<span style='color:lightseagreen'>categorical targets</span>** (mutual_info_classif). Our target, price, is real-valued. The next cell computes the MI scores for our features and wraps them up in a nice dataframe. Hereafter, we are going to drop uninformative features as they are useless.","metadata":{"_uuid":"ff7afbe7-4b29-4659-81cd-c0acfc5cb8b4","_cell_guid":"f74924d4-7019-4b86-bb92-36129707b386","trusted":true}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_regression\n\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    #discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\ndef uninformative_cols(df, mi_scores):\n    return df.loc[:, mi_scores == 0.0].columns\n\ntrain_copy = df_train.copy()\nfor col in train_copy.select_dtypes(\"object\"):\n    train_copy[col], _ = train_copy[col].factorize()\n    \ny = train_copy['SalePrice']\nx = train_copy.drop('SalePrice',axis=1)\n\nmi_scores = make_mi_scores(x, y)\ncol = uninformative_cols(x, mi_scores)\nx = x.drop(col,axis=1)\nprint('Uninformative features:\\n{}'.format(mi_scores[mi_scores == 0.0]))\nmi_scores = pd.DataFrame(mi_scores).reset_index().rename(columns={'index':'Feature'})","metadata":{"_uuid":"4937778e-a18f-4e36-967c-babfd59e76c4","_cell_guid":"8161dbc2-f7ea-431a-992b-df87defca5bf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:05.097803Z","iopub.execute_input":"2022-03-25T16:04:05.098165Z","iopub.status.idle":"2022-03-25T16:04:07.853777Z","shell.execute_reply.started":"2022-03-25T16:04:05.098135Z","shell.execute_reply":"2022-03-25T16:04:07.852833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hereafter, we are gooin to plot results obtained previously in order to regard which features are the **<span style='color:lightseagreen'>most informative</span>**, and which ones requires some more analysis. You can see that we have a number of features that are highly informative and also some that don't seem to be informative at all (at least by themselves). Top scoring features will usually pay-off the **<span style='color:lightseagreen'>most during feature development</span>**, so it could be a good idea to focus your efforts on those. On the other hand, training on uninformative features can lead to overfitting.","metadata":{"_uuid":"fbcd6198-1fba-4f0a-8d43-b900e3b3f708","_cell_guid":"222238d6-4dba-4ad0-8d4e-9a7d6e154243","trusted":true}},{"cell_type":"code","source":"fig = px.bar(mi_scores, x='MI Scores', y='Feature', color=\"MI Scores\",\n             color_continuous_scale='darkmint')\nfig.update_layout(height = 1500, title_text=\"Mutual Information Scores\",\n                  title_font=dict(size=29, family=\"Lato, sans-serif\"), xaxis={'categoryorder':'category ascending'}, margin=dict(t=80))","metadata":{"_uuid":"a3de8f75-3f52-429d-a6e8-c825feeb4409","_cell_guid":"a3f5734c-7066-496e-91e1-7e1d88225227","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:07.85764Z","iopub.execute_input":"2022-03-25T16:04:07.857914Z","iopub.status.idle":"2022-03-25T16:04:07.946654Z","shell.execute_reply.started":"2022-03-25T16:04:07.857879Z","shell.execute_reply":"2022-03-25T16:04:07.945548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.1 | Building Type\n\nWe have seen that **<span style='color:lightseagreen'>BldgType</span>** feature didn't get a very high MI score. A plot confirms that the categories in BldgType don't do a good job of distinguishing values in SalePrice as the distributions look fairly similar.","metadata":{"_uuid":"a8ffb44f-2f49-4bf6-b063-4624570583dc","_cell_guid":"7c72a2f1-7b26-4c65-a683-2d48b24d49cc","trusted":true}},{"cell_type":"code","source":"px.box(df_train, x=\"BldgType\", y=\"SalePrice\", color=\"BldgType\")","metadata":{"_uuid":"2ed98ada-8f8d-4350-871e-f73c712f91c4","_cell_guid":"13b223b9-9d76-4c91-a5f7-00bf0c92c128","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:07.948188Z","iopub.execute_input":"2022-03-25T16:04:07.948418Z","iopub.status.idle":"2022-03-25T16:04:08.045373Z","shell.execute_reply.started":"2022-03-25T16:04:07.94839Z","shell.execute_reply":"2022-03-25T16:04:08.04448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"However, the type of a dwelling seems like it should be important information. Investigating whether BldgType produces a significant interaction we find that the trend lines are significantly different from one category to the next. This indicates an interaction effect, so we are keeping this variable.","metadata":{"_uuid":"9faf08b0-f23b-47a8-b16e-076b4af53707","_cell_guid":"888fa5ed-5648-4707-aa95-36568d233344","trusted":true}},{"cell_type":"code","source":"sns.lmplot(x='GrLivArea', y=\"SalePrice\", hue=\"BldgType\", col=\"BldgType\",data=df_train, scatter_kws={\"edgecolor\": 'w'}, col_wrap=3, height=6, aspect = 1)","metadata":{"_uuid":"9a5bda92-85cf-4b03-84c9-2aa5d9edfa7e","_cell_guid":"375e82d1-0527-45c0-a042-014458dc2357","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:08.046857Z","iopub.execute_input":"2022-03-25T16:04:08.047104Z","iopub.status.idle":"2022-03-25T16:04:10.285321Z","shell.execute_reply.started":"2022-03-25T16:04:08.047073Z","shell.execute_reply":"2022-03-25T16:04:10.284278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hereafter, we are going to define a baseline score which is going to help us to know whether some set of features we've assembled has actually led to any **<span style='color:lightseagreen'>improvement</span>** or not.","metadata":{"_uuid":"2d12b9c0-0f19-4f47-9056-ba7e10ed6044","_cell_guid":"0e6943af-5535-4d60-adf5-1fe5a94025eb","trusted":true}},{"cell_type":"code","source":"def score_dataset(X, y, model=XGBRegressor()):\n    # Label encoding is good for XGBoost and RandomForest, but one-hot\n    # would be better for models like Lasso or Ridge. The `cat.codes`\n    # attribute holds the category levels.\n    for colname in X.select_dtypes([\"object\"]):\n        X[colname] = X[colname].factorize()\n    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n    log_y = np.log(y)\n    score = cross_val_score(\n        model, X, log_y, cv=5, scoring=\"neg_mean_squared_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score\n\nbaseline_score = score_dataset(x, y)\nprint(f\"Baseline score: {baseline_score:.5f} RMSLE\")","metadata":{"_uuid":"f2d99adc-201c-438e-be10-3f9652d70f16","_cell_guid":"42196381-a631-4ca4-8278-99beae818a19","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:10.286622Z","iopub.execute_input":"2022-03-25T16:04:10.28684Z","iopub.status.idle":"2022-03-25T16:04:13.44318Z","shell.execute_reply.started":"2022-03-25T16:04:10.286813Z","shell.execute_reply":"2022-03-25T16:04:13.44247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>3.3 | Label Encoding</b></p>\n</div>\n\nLet's begin by defining a **<span style='color:lightseagreen'>transformation</span>** firstly, a label encoding for the categorical features. A **<span style='color:lightseagreen'>label encoding</span>** is okay for any kind of categorical feature when we use a **<span style='color:lightseagreen'>tree-ensemble</span>** like XGBoost, even for unordered categories. If you want to try a linear regression model (also popular in this competition), you would instead want to use a one-hot encoding, especially for the features with unordered categories.","metadata":{"_uuid":"6e5c45e8-d72b-4ad4-b4e0-363cc26d879d","_cell_guid":"514c8ca2-5012-4bc3-9298-121820d652be","trusted":true}},{"cell_type":"code","source":"def label_encode(df):\n    X = df.copy()\n    for colname in X.select_dtypes([\"object\"]).columns:\n        X[colname] = LabelEncoder().fit_transform(X[colname])\n    return X","metadata":{"_uuid":"28f94d61-b0e7-4480-a65a-e1ac28e37137","_cell_guid":"5d08ba49-f7ba-47c6-a877-bded8ccf2e28","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:13.447224Z","iopub.execute_input":"2022-03-25T16:04:13.449037Z","iopub.status.idle":"2022-03-25T16:04:13.455175Z","shell.execute_reply.started":"2022-03-25T16:04:13.448993Z","shell.execute_reply":"2022-03-25T16:04:13.454547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>3.4 | Normalization</b></p>\n</div>\n\nOur algorithm works better with features **<span style='color:lightseagreen'>distributed normally</span>**. Thus, let's check whether our variables are distributed normally or not. To do so, we are going to make some plots hereafter.","metadata":{"_uuid":"e87a1afb-f139-4761-bf8d-96da30de7e0d","_cell_guid":"c42e1239-2261-4e68-ab4a-560785261ec3","trusted":true}},{"cell_type":"code","source":"quantitative = [f for f in df_train.columns if df_train.dtypes[f] == 'float']\n\nsns.set_style('darkgrid')\nf = pd.melt(df_train, value_vars=quantitative)\ng = sns.FacetGrid(f, col=\"variable\", col_wrap=3, sharex=False, sharey=False, height=3, aspect=2)\ng = g.map(sns.distplot, \"value\", color='lightseagreen')","metadata":{"_uuid":"a7fa99df-3e46-4efb-9e44-6a7e5b284829","_cell_guid":"4cd6178e-42a2-4079-99db-7f46d06fbca9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:13.456691Z","iopub.execute_input":"2022-03-25T16:04:13.457Z","iopub.status.idle":"2022-03-25T16:04:18.898241Z","shell.execute_reply.started":"2022-03-25T16:04:13.456958Z","shell.execute_reply":"2022-03-25T16:04:18.897614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ“Œ **Interpret:** We can see that **<span style='color:lightseagreen'>neither</span>** of them are normally distributed. This is one of the awesome things you can learn in statistical books: in case of positive skewness, log transformations usually works well. Hereafter, we are going to make the logaritmic transformation to **<span style='color:lightseagreen'>SalePrice</span>**. \n\nNow we are going to solve normalization issue for the other quantitative variables. In general, they present some **<span style='color:lightseagreen'>skewness</span>**. However, before making the transformation we have to face another issue. They have a significant number of observations with **<span style='color:lightseagreen'>zero value</span>**, houses without basement or garage for example. That's a big issue as zero value **<span style='color:lightseagreen'>does not allow</span>** us to do log transformaion. To apply it here, we'll create a variable that can get the effect of having or not having basement (binary variable). Then, we'll do a log transformation to all the non-zero observations, **<span style='color:lightseagreen'>ignoring</span>** those with value zero. This way we can transform data, without losing the effect of having or not basement or garage.","metadata":{"_uuid":"498bb0aa-f7c4-4fd6-b01b-b6ebe89c29d3","_cell_guid":"39b24c53-c48b-4a19-b978-46b45e3162ed","trusted":true}},{"cell_type":"code","source":"def normalize(df):\n    df['SalePrice'] = np.log1p(df[\"SalePrice\"])\n    df.loc[df['hasbsmt']==1,'TotalBsmtSF'] = np.log1p(df['TotalBsmtSF'])\n    df.loc[df['hasgarage']==1,'GarageArea'] = np.log1p(df['GarageArea'])\n    df.loc[df['hasfrontage']==1,'LotFrontage'] = np.log1p(df['LotFrontage'])\n    df['LivLotRatio'] = np.log1p(df['LivLotRatio'])\n    df['Total_SF'] = np.log1p(df['Total_SF'])\n    df['Spaciousness'] = np.log1p(df['Spaciousness'])\n    return df","metadata":{"_uuid":"5a398afc-8c1e-4a43-a2a2-6de8eeba55d1","_cell_guid":"4dd04a3e-c478-4b74-a554-8bf7c9f14dc7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:18.899389Z","iopub.execute_input":"2022-03-25T16:04:18.900046Z","iopub.status.idle":"2022-03-25T16:04:18.908045Z","shell.execute_reply.started":"2022-03-25T16:04:18.900009Z","shell.execute_reply":"2022-03-25T16:04:18.906872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#columns_with_many_zeros = ['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','2ndFlrSF','LowQualFinSF',\n #                          'GarageCars','GarageArea','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea']","metadata":{"_uuid":"93d4e039-2a98-405d-a7ad-408318b1e9cf","_cell_guid":"b80e88cf-9c2e-4f52-ab08-40643441ef50","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:18.91036Z","iopub.execute_input":"2022-03-25T16:04:18.910744Z","iopub.status.idle":"2022-03-25T16:04:18.926035Z","shell.execute_reply.started":"2022-03-25T16:04:18.910707Z","shell.execute_reply":"2022-03-25T16:04:18.925049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>3.5 | Creating New Features</b></p>\n</div>\n\n### 3.5.1 | Mathematical Transformation\n\nRelationships among numerical features are often expressed through mathematical formulas, which you'll frequently come across as part of your **<span style='color:lightseagreen'>domain research</span>**. In this case, we are going to make a few of then. \n1. A living lot ratio, which is going to refer to the **<span style='color:lightseagreen'>percentage of living area in our lot</span>**. \n2. A **<span style='color:lightseagreen'>spaciousness</span>** feature, referring to average square feet area per room above the ground. \n3. We are going to combine OverallQual with OverallCond by (previously converting them to numerical type) taking a product.\n4. A **<span style='color:lightseagreen'>total square feet area</span>** of the house. \n5. Total number of **<span style='color:lightseagreen'>bathrooms</span>** in the whole house. \n6. We are going to add years from building and remodelation, in order to take into account how **<span style='color:lightseagreen'>modern</span>** a house is\n7. Total square feet area of the **<span style='color:lightseagreen'>porch</span>**.","metadata":{"_uuid":"8ea748c8-5940-4dd3-9b02-47793684755f","_cell_guid":"b1b03f9a-15af-4bb7-9837-adc3b16ff2db","trusted":true}},{"cell_type":"code","source":"def mathematical_transforms(df):\n    X = pd.DataFrame()  # dataframe to hold new features\n    X[\"LivLotRatio\"] = df.GrLivArea / df.LotArea\n    X[\"Spaciousness\"] = (df['1stFlrSF'] + df['2ndFlrSF']) / df.TotRmsAbvGrd\n    X['QualCond'] = df.OverallCond * df.OverallQual\n    X['Total_SF'] = df['TotalBsmtSF'] +df['1stFlrSF'] + df['2ndFlrSF']\n    X['TotalBth'] = (df['FullBath'] + (0.5 * df['HalfBath']) + df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath']))\n    X['YrBltAndRemod']=df['YearBuilt']+df['YearRemodAdd']\n    X['Total_porch_sf'] = (df['OpenPorchSF'] + df['3SsnPorch'] + df['EnclosedPorch'] + df['ScreenPorch'] + df['WoodDeckSF'])\n    return X","metadata":{"_uuid":"6dc34c75-c4a4-45ea-bda5-73c7861a463a","_cell_guid":"295eb1c3-f731-4d32-8e95-53588c086781","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:18.927642Z","iopub.execute_input":"2022-03-25T16:04:18.928295Z","iopub.status.idle":"2022-03-25T16:04:18.938183Z","shell.execute_reply.started":"2022-03-25T16:04:18.928244Z","shell.execute_reply":"2022-03-25T16:04:18.937511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5.2 | Counts \n\nFeatures describing the **<span style='color:lightseagreen'>presence</span>** or **<span style='color:lightseagreen'>absence</span>** of something often come in sets, for example the set of risk factors for a disease. We can aggregate such features by creating a count. These features will be binary (1 for Present, 0 for Absent) or boolean (True or False). In Python, booleans can be added up just as if they were integers.","metadata":{"_uuid":"e80eaea2-43cc-40b7-af76-885408edbdf7","_cell_guid":"53fef766-ca37-42ca-adfa-70e04e91d683","trusted":true}},{"cell_type":"code","source":"def counts(df):\n    X = pd.DataFrame()\n    X[\"PorchTypes\"] = df[[\n        \"WoodDeckSF\",\n        \"OpenPorchSF\",\n        \"EnclosedPorch\",\n        \"3SsnPorch\",\n        \"ScreenPorch\",\n    ]].gt(0.0).sum(axis=1)\n    return X","metadata":{"_uuid":"368dec50-3a55-4589-98ec-747af2dae874","_cell_guid":"86447204-9b94-4f0c-9efa-ba396bd04d8d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:18.939114Z","iopub.execute_input":"2022-03-25T16:04:18.93966Z","iopub.status.idle":"2022-03-25T16:04:18.952007Z","shell.execute_reply.started":"2022-03-25T16:04:18.939624Z","shell.execute_reply":"2022-03-25T16:04:18.951156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5.3 | Building-Up and Breaking-Down Features\n\nWe also have complex strings that can usefully be broken into **<span style='color:lightseagreen'>simple pieces</span>**. Features like these will often have some kind of structure that you can make use of. The **<span style='color:lightseagreen'>str accessor</span>** lets you apply string methods like split directly to columns.","metadata":{"_uuid":"296f80e2-a966-4443-934a-da2771077a64","_cell_guid":"08af30a5-b7ea-493f-947a-1454d9d31dbb","trusted":true}},{"cell_type":"code","source":"def break_down(df):\n    X = pd.DataFrame()\n    X[\"MSClass\"] = df.MSSubClass.str.split(\"_\", n=1, expand=True)[0]\n    return X","metadata":{"_uuid":"5680ef9d-14b4-421b-8cbc-3819c4d51d3c","_cell_guid":"133c46e6-1e2a-4d7c-9182-fe8d4a7048b7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:18.953491Z","iopub.execute_input":"2022-03-25T16:04:18.953846Z","iopub.status.idle":"2022-03-25T16:04:18.963263Z","shell.execute_reply.started":"2022-03-25T16:04:18.953804Z","shell.execute_reply":"2022-03-25T16:04:18.962679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5.4 | Group Transforms\n\nHereafter we have group transforms, which **<span style='color:lightseagreen'>aggregate</span>** information across multiple rows grouped by some category. Using an aggregation function, a group transform combines two features: a categorical feature that provides the grouping and another feature whose values you wish to aggregate.","metadata":{"_uuid":"9b196536-e486-43bc-b047-0b1da90998a6","_cell_guid":"246da779-8bfc-4cae-8e43-7fa2ee70c40d","trusted":true}},{"cell_type":"code","source":"def group_transforms(df):\n    X = pd.DataFrame()\n    X[\"MedNhbdArea\"] = df.groupby(\"Neighborhood\")[\"GrLivArea\"].transform(\"median\")\n    return X","metadata":{"_uuid":"68276ca5-6016-4256-97f4-3aa93f427b43","_cell_guid":"dd5edca3-8263-4ebf-a270-86a00cc2ddf7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:18.964708Z","iopub.execute_input":"2022-03-25T16:04:18.965189Z","iopub.status.idle":"2022-03-25T16:04:18.973891Z","shell.execute_reply.started":"2022-03-25T16:04:18.965148Z","shell.execute_reply":"2022-03-25T16:04:18.973322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5.5 | Interactions\n\nFinally, we are going to create features for interactions proved before. BldgType produces a **<span style='color:lightseagreen'>significant interaction</span>** as we found out that the trend lines are significantly different from one category to the next, referring to house prices.","metadata":{"_uuid":"3110f088-d56a-4ef8-891d-9b64b71b8886","_cell_guid":"2cd6b464-bc00-4c7d-b238-7011635a6b45","trusted":true}},{"cell_type":"code","source":"def interactions(df):\n    X = pd.get_dummies(df.BldgType, prefix=\"Bldg\")\n    X = X.mul(df.GrLivArea, axis=0)\n    return X","metadata":{"_uuid":"e933a7fa-2f96-44ac-8966-88bcd5f14f41","_cell_guid":"2b3a805c-3024-49c7-8678-4e7a6ab530bb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:18.97523Z","iopub.execute_input":"2022-03-25T16:04:18.975718Z","iopub.status.idle":"2022-03-25T16:04:18.984749Z","shell.execute_reply.started":"2022-03-25T16:04:18.975676Z","shell.execute_reply":"2022-03-25T16:04:18.984042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5.6 | Presence of House Features\nWe are going to create some variables to know whether a house has different features, such as fireplace, pool, basement, second floor ...","metadata":{"_uuid":"f442bf4d-f5d8-445d-b4b3-f4b4d05f5f74","_cell_guid":"177a334f-26c6-4574-8b22-cd995ad813d3","trusted":true}},{"cell_type":"code","source":"def features_presence(df):\n    X = pd.DataFrame()\n    X['haspool'] = df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    X['has2ndfloor'] = df['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    X['hasgarage'] = df['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    X['hasbsmt'] = df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    X['hasfireplace'] = df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n    X['hasfrontage'] = df['LotFrontage'].apply(lambda x: 1 if x > 0 else 0)\n    return X","metadata":{"_uuid":"6465dc18-54a7-43fa-8c17-939276e9f1f3","_cell_guid":"2f00b9f6-2601-4cbb-ac3f-3e84c9d19887","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:18.986187Z","iopub.execute_input":"2022-03-25T16:04:18.9868Z","iopub.status.idle":"2022-03-25T16:04:18.996847Z","shell.execute_reply.started":"2022-03-25T16:04:18.986756Z","shell.execute_reply":"2022-03-25T16:04:18.995579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5.7 | Principal Component Analysis (PCA)\n\nLike clustering is a **<span style='color:lightseagreen'>partitioning of the dataset based on proximity</span>**, you could think of PCA as a partitioning of the variation in the data. PCA is a great tool to help you discover important relationships in the data and can also be used to create more informative features.\n\n> (Technical note: PCA is typically applied to **<span style='color:lightseagreen'>standardized</span>** data. With standardized data \"variation\" means \"correlation\". With unstandardized data \"variation\" means \"covariance\". All data in this course will be standardized before applying PCA.)","metadata":{"_uuid":"f3d08417-bbcc-4f3c-a8ad-042e2014eeb3","_cell_guid":"fd470806-8245-4b9d-83c4-eb139a9b2d53","trusted":true}},{"cell_type":"code","source":"def apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) / X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n\ndef pca_inspired(df):\n    X = pd.DataFrame()\n    X[\"Feature1\"] = df.GrLivArea + df.TotalBsmtSF\n    X[\"Feature2\"] = df.YearRemodAdd * df.TotalBsmtSF\n    return X\n\n\ndef pca_components(df, features):\n    X = df.loc[:, features]\n    _, X_pca, _ = apply_pca(X)\n    return X_pca\n\n\npca_features = [\n    \"GarageArea\",\n    \"YearRemodAdd\",\n    \"TotalBsmtSF\",\n    \"GrLivArea\",\n]","metadata":{"_uuid":"9b0c2b66-55a5-4699-b401-204fbae653bb","_cell_guid":"10851680-c41c-4b7f-9a6b-6a791a8092bd","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:18.998322Z","iopub.execute_input":"2022-03-25T16:04:18.999211Z","iopub.status.idle":"2022-03-25T16:04:19.016026Z","shell.execute_reply.started":"2022-03-25T16:04:18.999142Z","shell.execute_reply":"2022-03-25T16:04:19.015225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5.8 | Target Encoding\nThere is, however, a way you can use target encoding without having to use held-out encoding data. It's basically the same trick used in cross-validation:\n\n1. Split the data into folds, each fold having two splits of the dataset.\n2. Train the encoder on one split but transform the values of the other.\n3. Repeat for all the splits.\n\nThis way, training and transformation always take place on independent sets of data, just like when you use a holdout set but without any data going to waste. In the next hidden cell is a wrapper you can use with any target encoder:","metadata":{"_uuid":"0447b8ae-f25b-463e-907c-b2b4cf100a39","_cell_guid":"81337e1f-0c01-4465-ac94-c84d5170db68","trusted":true}},{"cell_type":"code","source":"class CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=5)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) / len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","metadata":{"_uuid":"b310b737-d064-4dbf-940f-fc0ea6079582","_cell_guid":"4ba65184-902f-4896-a003-42e6e0b53a2c","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:19.017259Z","iopub.execute_input":"2022-03-25T16:04:19.01747Z","iopub.status.idle":"2022-03-25T16:04:19.031443Z","shell.execute_reply.started":"2022-03-25T16:04:19.017445Z","shell.execute_reply":"2022-03-25T16:04:19.030764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5.9 | Final Feature Set\n\nNow let's combine everything together. Putting the transformations into separate functions makes it **<span style='color:lightseagreen'>easier</span>** to experiment with various combinations. We'll score our dataset in order to see how it has improve with all new features creations.","metadata":{"_uuid":"6e316be6-dd25-406d-842d-5ad9bb57022e","_cell_guid":"7a2b5cbf-642f-4b27-b552-067d6baf82b0","trusted":true}},{"cell_type":"code","source":"def create_features(df, df_test=None):\n    X = df.copy()\n    y = X.pop(\"SalePrice\")\n    mi_scores = make_mi_scores(X, y)\n\n    if df_test is not None:\n        X_test = df_test.copy()\n        X_test.pop(\"SalePrice\")\n        X = pd.concat([X, X_test])\n\n    # Feature Engineering Course Lesson 2 - Mutual Information\n    col = uninformative_cols(X, mi_scores)\n\n    # Feature Engineering Course Lesson 3 - Transformations\n    X = X.join(mathematical_transforms(X))\n    X = X.join(interactions(X))\n    X = X.join(counts(X))\n    #X = X.join(break_down(X))\n    X = X.join(group_transforms(X))\n    X = X.join(features_presence(X))\n    # Feature Engineering Course Lesson 5 - PCA\n    X = X.join(pca_inspired(X))\n    # X = X.join(pca_components(X, pca_features))\n    # X = X.join(indicate_outliers(X))\n\n    X = label_encode(X)\n    \n    # Reform splits\n    if df_test is not None:\n        X_test = X.loc[df_test.index, :]\n        X.drop(df_test.index, inplace=True)\n\n    # Feature Engineering Course Lesson 6 - Target Encoder\n    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n    X = X.join(encoder.fit_transform(X, y, cols=[\"MSSubClass\"]))\n    if df_test is not None:\n        X_test = X_test.join(encoder.transform(X_test))\n    \n    X = X.drop(col,axis=1)\n    X_test = X_test.drop(col,axis=1)\n    \n    if df_test is not None:\n        return X, X_test\n    else:\n        return X\n\ndf_train = df_data[df_data['SalePrice'].isnull() == False]\ndf_test = df_data[df_data['SalePrice'].isnull() == True]\nX_train, X_test = create_features(df_train, df_test)\ny_train = df_train.loc[:, \"SalePrice\"]\n\ndf_train = pd.concat([X_train,y_train],axis=1)\ndf_test = pd.concat([X_test, df_test['SalePrice']],axis=1)\n\n# Logarithmic transformation\ndf_train = normalize(df_train)\ndf_test = normalize(df_test)\n\ndf_train = reduce_mem_usage(df_train)\ndf_test = reduce_mem_usage(df_test)\n\nX_train = df_train.drop('SalePrice',axis=1)\nX_test = df_test.drop('SalePrice',axis=1)\ny_train = df_train['SalePrice']\n\nscore_dataset(X_train, y_train)","metadata":{"_uuid":"4289d8e6-9347-46e3-b5b8-c750a98e1110","_cell_guid":"8d2289f3-2f5f-4d62-833c-708be0ea8acb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:19.032675Z","iopub.execute_input":"2022-03-25T16:04:19.033066Z","iopub.status.idle":"2022-03-25T16:04:25.344868Z","shell.execute_reply.started":"2022-03-25T16:04:19.033031Z","shell.execute_reply":"2022-03-25T16:04:25.3437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us now see how the logarithmic conversion has had its effect on our variables, which are now **<span style='color:lightseagreen'>normally distributed</span>**, unlike before. We'll start with **<span style='color:lightseagreen'>Normal probability plot</span>**, where data distribution should closely follow the diagonal that represents the normal distribution.","metadata":{"_uuid":"5c7da049-6cc3-4b42-94dd-0e2d9a2802c1","_cell_guid":"2be8649a-53ee-4424-9a43-c884eeaa61c4","trusted":true}},{"cell_type":"code","source":"from scipy import stats\nres = stats.probplot(df_train['SalePrice'],  plot = plt)","metadata":{"_uuid":"b66baffb-c700-4067-bbea-de4bc3f38f5c","_cell_guid":"1fe6a699-4963-4c0a-828e-296c9284bbdb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:25.348819Z","iopub.execute_input":"2022-03-25T16:04:25.349593Z","iopub.status.idle":"2022-03-25T16:04:25.606249Z","shell.execute_reply.started":"2022-03-25T16:04:25.349552Z","shell.execute_reply":"2022-03-25T16:04:25.605311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#histogram and normal probability plot\nsns.set_style('darkgrid')\nf = pd.melt(df_train, value_vars=['GarageArea','TotalBsmtSF','LotFrontage','Total_SF','Spaciousness','LivLotRatio'])\ng = sns.FacetGrid(f, col=\"variable\", col_wrap=3, sharex=False, sharey=False, aspect = 2)\ng = g.map(sns.distplot, \"value\", color='lightseagreen')","metadata":{"_uuid":"b46589fb-ac05-4070-955a-5a4e8c4d212f","_cell_guid":"d8f47211-87d7-4647-9eb8-7535a807063a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:04:25.60862Z","iopub.execute_input":"2022-03-25T16:04:25.60894Z","iopub.status.idle":"2022-03-25T16:04:28.389082Z","shell.execute_reply.started":"2022-03-25T16:04:25.608897Z","shell.execute_reply":"2022-03-25T16:04:28.388247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df_train['SalePrice']\nx = df_train.drop('SalePrice',axis=1)\n\nmi_scores = make_mi_scores(x, y)\nprint('Uninformative features:\\n{}'.format(mi_scores[mi_scores == 0.0]))\nmi_scores = pd.DataFrame(mi_scores).reset_index().rename(columns={'index':'Feature'})\n\nX_train = df_train.drop('SalePrice',axis=1)\nX_test = df_test.drop('SalePrice',axis=1)\ny_train = df_train['SalePrice']","metadata":{"execution":{"iopub.status.busy":"2022-03-25T16:04:28.390573Z","iopub.execute_input":"2022-03-25T16:04:28.390813Z","iopub.status.idle":"2022-03-25T16:04:31.216258Z","shell.execute_reply.started":"2022-03-25T16:04:28.390783Z","shell.execute_reply":"2022-03-25T16:04:31.215143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(mi_scores, x='MI Scores', y='Feature', color=\"MI Scores\",\n             color_continuous_scale='darkmint')\nfig.update_layout(height = 1500, title_text=\"Mutual Information Scores After Feature Engineering\",\n                  title_font=dict(size=29, family=\"Lato, sans-serif\"), xaxis={'categoryorder':'category ascending'}, margin=dict(t=80))","metadata":{"execution":{"iopub.status.busy":"2022-03-25T16:04:31.217935Z","iopub.execute_input":"2022-03-25T16:04:31.21832Z","iopub.status.idle":"2022-03-25T16:04:31.297461Z","shell.execute_reply.started":"2022-03-25T16:04:31.218276Z","shell.execute_reply":"2022-03-25T16:04:31.296426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>4 <span style='color:lightseagreen'>|</span> Modeling</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.1 | Comparison with PyCaret</b></p>\n</div>\n\n[PyCaret Tutorial](https://todobi.com/pycaret-paso-a-paso/)\n\nPyCaret is an open source, low-code machine learning library in Python that allows you to go from preparing your data to deploying your model within seconds in your choice of notebook environment. PyCaret being a low-code library makes you more productive. With less time spent coding, you and your team can now focus on business problems. PyCaret is simple and easy to use machine learning library that will help you to perform end-to-end ML experiments with less lines of code. PyCaret is a business ready solution. It allows you to do prototyping quickly and efficiently from your choice of notebook environment. You can reach pycaret website and documentation from [PyCaret](https://pycaret.org)\n\n![](https://pycaret.org/wp-content/uploads/2020/03/Divi93_43.png)","metadata":{"_uuid":"abaa2911-be94-422a-a16e-12c6ea2e3e6a","_cell_guid":"ec3498c6-94cb-48c4-8200-6284c4351a6c","trusted":true}},{"cell_type":"code","source":"reg = setup(data = df_train, target = 'SalePrice', silent = True, use_gpu = True)\nclear_output()\ncompare_models()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T16:04:31.29903Z","iopub.execute_input":"2022-03-25T16:04:31.299347Z","iopub.status.idle":"2022-03-25T16:06:36.297734Z","shell.execute_reply.started":"2022-03-25T16:04:31.299303Z","shell.execute_reply":"2022-03-25T16:06:36.297057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.2 | Gradient Boosting</b></p>\n</div>\n\n### 4.2.1 | Hyperparameter Tuning - Optuna\n\nIn this case, only for Catboost, we are going to make the **<span style='color:lightseagreen'>tuning with Optuna</span>**. I will add the code for hyperparameter tuning below. However, for not **<span style='color:lightseagreen'>wasting CPU time</span>**, since I have run it once, I will simply create the model with the specific features values. I will control whether making hyperparameter tuning or not with **<span style='color:lightseagreen'>allow_optimize</span>** Finally, just say that code for tuning takes plenty of time. Due to that I enabled GPU technology. ","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    params = {\n        \"random_state\":trial.suggest_categorical(\"random_state\", [2022]),           # categorical for concrete values\n        'learning_rate' : trial.suggest_loguniform('learning_rate', 0.01, 1),   # loguniform for continuos values\n        \"n_estimators\": trial.suggest_int('n_estimators',50,2000),                 # int for discrete values. Interval between [100,2000]\n        \"max_depth\" : trial.suggest_int(\"max_depth\", 1, 20),\n        \"min_samples_split\" : trial.suggest_int(\"min_samples_split\", 2, 20),\n        \"min_samples_leaf\" : trial.suggest_int(\"min_samples_leaf\", 2, 20),\n        \"alpha\" : trial.suggest_loguniform('alpha',0.9,1),\n        \"max_features\" : trial.suggest_int(\"max_features\", 10, 50)\n    }\n\n    model = GradientBoostingRegressor(**params)\n    X_train_tmp, X_valid_tmp, y_train_tmp, y_valid_tmp = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n    model.fit(X_train_tmp, y_train_tmp)\n        \n    y_train_pred = model.predict(X_train_tmp)\n    y_valid_pred = model.predict(X_valid_tmp)\n    train_mae = mae(y_train_tmp, y_train_pred)\n    valid_mae = mae(y_valid_tmp, y_valid_pred)\n    \n    print(f'MAE of Train: {train_mae}')\n    print(f'MAE of Validation: {valid_mae}')\n    \n    return valid_mae\n\nallow_optimize = 0","metadata":{"execution":{"iopub.status.busy":"2022-03-25T16:06:36.298973Z","iopub.execute_input":"2022-03-25T16:06:36.299617Z","iopub.status.idle":"2022-03-25T16:06:36.310155Z","shell.execute_reply.started":"2022-03-25T16:06:36.299579Z","shell.execute_reply":"2022-03-25T16:06:36.30927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRIALS = 100\nTIMEOUT = 3600\n\nif allow_optimize:\n    sampler = TPESampler(seed=42)\n    study = optuna.create_study(\n        study_name = 'gbr_parameter_opt',\n        direction = 'minimize',\n        sampler = sampler,\n    )\n    study.optimize(objective, n_trials=TRIALS)\n    print(\"Best Score:\",study.best_value)\n    print(\"Best trial\",study.best_trial.params)\n    \n    best_params = study.best_params\n    \n    X_train_tmp, X_valid_tmp, y_train_tmp, y_valid_tmp = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n    model_tmp = GradientBoostingRegressor(**best_params).fit(X_train_tmp, y_train_tmp)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T16:06:36.311574Z","iopub.execute_input":"2022-03-25T16:06:36.312336Z","iopub.status.idle":"2022-03-25T16:06:36.326968Z","shell.execute_reply.started":"2022-03-25T16:06:36.312284Z","shell.execute_reply":"2022-03-25T16:06:36.325927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2.2 | Fitting - Feature Importances","metadata":{}},{"cell_type":"code","source":"if allow_optimize:\n    model = GradientBoostingRegressor(**best_params, n_estimators=model_tmp.get_best_iteration(), verbose=1000).fit(X_train, y_train)\nelse:\n    model = GradientBoostingRegressor(\n        verbose=0,\n        random_state = 2022, learning_rate = 0.023841354286362568, n_estimators = 1577, max_depth = 3, \n        min_samples_split = 16, min_samples_leaf = 2, alpha = 0.9581363403215747, max_features = 20).fit(X_train, y_train)  \n    \npermutation_gbr = PermutationImportance(model, random_state=1).fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T16:06:36.328473Z","iopub.execute_input":"2022-03-25T16:06:36.328888Z","iopub.status.idle":"2022-03-25T16:07:02.506642Z","shell.execute_reply.started":"2022-03-25T16:06:36.328852Z","shell.execute_reply":"2022-03-25T16:07:02.505744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_feature_importance(importance,names,model_type):\n    \n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n    \n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n    \n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n    fi_df = fi_df[fi_df.feature_importance > 0]\n    fig = px.bar(fi_df, x='feature_names', y='feature_importance', color=\"feature_importance\",\n             color_continuous_scale='Blugrn')\n    # General Styling\n    fig.update_layout(height=750, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100,t=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>Feature Importance Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)\n    fig.show()\n    \nplot_feature_importance(model.feature_importances_,X_train.columns,'Gradient Boosting')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-25T16:07:02.50855Z","iopub.execute_input":"2022-03-25T16:07:02.508792Z","iopub.status.idle":"2022-03-25T16:07:02.612116Z","shell.execute_reply.started":"2022-03-25T16:07:02.508763Z","shell.execute_reply":"2022-03-25T16:07:02.611296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2.3 | Making Predictions","metadata":{}},{"cell_type":"code","source":"predictions_gbr = model.predict(X_test)\npredictions_gbr = np.expm1(predictions_gbr)","metadata":{"_uuid":"a99ed223-1a6d-433d-85a5-0ac6e8c66c9a","_cell_guid":"d2736c11-6fd4-4def-bed6-1afd85a23eee","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-25T16:07:02.613667Z","iopub.execute_input":"2022-03-25T16:07:02.614136Z","iopub.status.idle":"2022-03-25T16:07:02.662516Z","shell.execute_reply.started":"2022-03-25T16:07:02.614093Z","shell.execute_reply":"2022-03-25T16:07:02.661851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.3 | Gradient Boosting</b></p>\n</div>\n\n### 4.3.1 | Hyperparameter Tuning - Optuna\n\nIn this case, only for Catboost, we are going to make the **<span style='color:lightseagreen'>tuning with Optuna</span>**. I will add the code for hyperparameter tuning below. However, for not **<span style='color:lightseagreen'>wasting CPU time</span>**, since I have run it once, I will simply create the model with the specific features values. I will control whether making hyperparameter tuning or not with **<span style='color:lightseagreen'>allow_optimize</span>** Finally, just say that code for tuning takes plenty of time. Due to that I enabled GPU technology. ","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    params = {\n        \"random_state\":trial.suggest_categorical(\"random_state\", [2022]),           \n        'learning_rate' : trial.suggest_loguniform('learning_rate', 0.01, 1),   \n        \"n_estimators\": trial.suggest_int('n_estimators',50,2000),                 \n        \"max_depth\" : trial.suggest_int(\"max_depth\", 1, 20),\n        \"_min_child_weight\" : trial.suggest_float(\"_min_child_weight\", 0.1, 10),\n        \"reg_lambda\" : trial.suggest_float(\"reg_lambda\", 0.01, 10),\n        \"reg_alpha\" : trial.suggest_float('reg_alpha',0.01,10),\n        \"num_leaves\" : trial.suggest_int(\"num_leaves\", 50, 100),\n        'subsample' : trial.suggest_float('subsample', 0.01, 1)\n    }\n\n    #model = LGBMRegressor(**params, device='GPU')\n    model = LGBMRegressor(**params)\n    X_train_tmp, X_valid_tmp, y_train_tmp, y_valid_tmp = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n    model.fit(X_train_tmp, y_train_tmp)\n        \n    y_train_pred = model.predict(X_train_tmp)\n    y_valid_pred = model.predict(X_valid_tmp)\n    train_mae = mae(y_train_tmp, y_train_pred)\n    valid_mae = mae(y_valid_tmp, y_valid_pred)\n    \n    print(f'MAE of Train: {train_mae}')\n    print(f'MAE of Validation: {valid_mae}')\n    \n    return valid_mae\n\nallow_optimize = 0","metadata":{"execution":{"iopub.status.busy":"2022-03-25T16:07:02.668613Z","iopub.execute_input":"2022-03-25T16:07:02.669041Z","iopub.status.idle":"2022-03-25T16:07:02.679711Z","shell.execute_reply.started":"2022-03-25T16:07:02.66899Z","shell.execute_reply":"2022-03-25T16:07:02.678996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRIALS = 100\nTIMEOUT = 3600\n\nif allow_optimize:\n    sampler = TPESampler(seed=42)\n    study = optuna.create_study(\n        study_name = 'lgbm_parameter_opt',\n        direction = 'minimize',\n        sampler = sampler,\n    )\n    study.optimize(objective, n_trials=TRIALS)\n    print(\"Best Score:\",study.best_value)\n    print(\"Best trial\",study.best_trial.params)\n    \n    best_params = study.best_params\n    \n    X_train_tmp, X_valid_tmp, y_train_tmp, y_valid_tmp = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n    #model_tmp = LGBMRegressor(**best_params, device='GPU').fit(X_train_tmp, y_train_tmp)\n    model_tmp = LGBMRegressor(**best_params).fit(X_train_tmp, y_train_tmp)    ","metadata":{"execution":{"iopub.status.busy":"2022-03-25T16:07:02.681268Z","iopub.execute_input":"2022-03-25T16:07:02.681822Z","iopub.status.idle":"2022-03-25T16:07:02.696928Z","shell.execute_reply.started":"2022-03-25T16:07:02.681777Z","shell.execute_reply":"2022-03-25T16:07:02.695984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3.2 | Fitting - Feature Importances","metadata":{}},{"cell_type":"code","source":"if allow_optimize:\n    model = LGBMRegressor(**best_params, device='GPU', n_estimators=model_tmp.get_best_iteration(), verbose=1000).fit(X_train, y_train)\nelse:\n    model = LGBMRegressor(        \n        random_state = 2022, learning_rate = 0.06708645071974023, n_estimator = 1014, max_depth = 2, \n        min_child_weight = 7.3813273033314495, reg_lambda = 2.4677587687488165, reg_alpha = 0.44277253423371826, \n        num_leaves = 98, subsample = 0.8813132421543259).fit(X_train, y_train)  \n    \npermutation_lgbm = PermutationImportance(model, random_state=1).fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T16:07:02.698593Z","iopub.execute_input":"2022-03-25T16:07:02.699294Z","iopub.status.idle":"2022-03-25T16:07:05.910338Z","shell.execute_reply.started":"2022-03-25T16:07:02.699262Z","shell.execute_reply":"2022-03-25T16:07:05.90958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importances = pd.DataFrame({'importance':model.feature_importances_, 'feature': X_train.columns})\nimportances = importances[importances.importance > 0]","metadata":{"execution":{"iopub.status.busy":"2022-03-25T16:07:05.912026Z","iopub.execute_input":"2022-03-25T16:07:05.912626Z","iopub.status.idle":"2022-03-25T16:07:05.91902Z","shell.execute_reply.started":"2022-03-25T16:07:05.912589Z","shell.execute_reply":"2022-03-25T16:07:05.918396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_feature_importance(model.feature_importances_,X_train.columns,'LGBM')","metadata":{"execution":{"iopub.status.busy":"2022-03-25T16:07:05.92049Z","iopub.execute_input":"2022-03-25T16:07:05.920956Z","iopub.status.idle":"2022-03-25T16:07:06.003433Z","shell.execute_reply.started":"2022-03-25T16:07:05.920922Z","shell.execute_reply":"2022-03-25T16:07:06.00272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3.3 | Predictions - Submitting","metadata":{}},{"cell_type":"code","source":"predictions_lgbm = model.predict(X_test)\npredictions_lgbm = np.expm1(predictions_lgbm)\nsubmit = pd.DataFrame({'Id': df_test.index, 'SalePrice':predictions_gbr*0.95 + predictions_lgbm * 0.05}).set_index('Id')\nsubmit.to_csv('./submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-25T16:21:35.618419Z","iopub.execute_input":"2022-03-25T16:21:35.618785Z","iopub.status.idle":"2022-03-25T16:21:35.647271Z","shell.execute_reply.started":"2022-03-25T16:21:35.618751Z","shell.execute_reply":"2022-03-25T16:21:35.6464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.4 | Permutation Importance</b></p>\n</div>\n\nOne of the most basic questions we might ask of a model is: **<span style='color:lightseagreen'>What features have the biggest impact on predictions?</span>** This concept is called feature importance. There are multiple ways to measure feature importance. Some approaches answer subtly different versions of the question above. Other approaches have documented shortcomings. In this section, we'll focus on permutation importance. Compared to most other approaches, permutation importance is:\n\n- Fast to calculate,\n- Widely used and understood, and\n- Consistent with properties we would want a feature importance measure to have.","metadata":{}},{"cell_type":"code","source":"eli5.show_weights(permutation_gbr, feature_names = X_test.columns.tolist(), top=20)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T16:07:06.031919Z","iopub.execute_input":"2022-03-25T16:07:06.032148Z","iopub.status.idle":"2022-03-25T16:07:06.042442Z","shell.execute_reply.started":"2022-03-25T16:07:06.032119Z","shell.execute_reply":"2022-03-25T16:07:06.04183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eli5.show_weights(permutation_lgbm, feature_names = X_test.columns.tolist(), top=20)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T16:07:06.044022Z","iopub.execute_input":"2022-03-25T16:07:06.044445Z","iopub.status.idle":"2022-03-25T16:07:06.056033Z","shell.execute_reply.started":"2022-03-25T16:07:06.044414Z","shell.execute_reply":"2022-03-25T16:07:06.055221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}