{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#import libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nfrom scipy.stats import skew, norm, probplot\nimport time\nfrom sklearn.preprocessing import OneHotEncoder, RobustScaler, PolynomialFeatures\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.linear_model import Ridge, HuberRegressor, LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.cluster import KMeans\nimport catboost as cb\nfrom xgboost import XGBRegressor\nfrom mlxtend.regressor import StackingCVRegressor","metadata":{"execution":{"iopub.status.busy":"2022-04-12T04:04:18.246531Z","iopub.execute_input":"2022-04-12T04:04:18.246876Z","iopub.status.idle":"2022-04-12T04:04:19.990888Z","shell.execute_reply.started":"2022-04-12T04:04:18.246785Z","shell.execute_reply":"2022-04-12T04:04:19.990073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import data\nhouse_df = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\nhouse_test = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-12T04:04:41.11857Z","iopub.execute_input":"2022-04-12T04:04:41.119329Z","iopub.status.idle":"2022-04-12T04:04:41.205513Z","shell.execute_reply.started":"2022-04-12T04:04:41.119291Z","shell.execute_reply":"2022-04-12T04:04:41.204865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#info of train dataset\nhouse_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T04:04:44.639123Z","iopub.execute_input":"2022-04-12T04:04:44.639385Z","iopub.status.idle":"2022-04-12T04:04:44.67332Z","shell.execute_reply.started":"2022-04-12T04:04:44.639354Z","shell.execute_reply":"2022-04-12T04:04:44.672515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#head of train dataset\nprint(house_df.head())","metadata":{"execution":{"iopub.status.busy":"2022-04-12T04:04:48.699737Z","iopub.execute_input":"2022-04-12T04:04:48.700305Z","iopub.status.idle":"2022-04-12T04:04:48.731738Z","shell.execute_reply.started":"2022-04-12T04:04:48.700258Z","shell.execute_reply":"2022-04-12T04:04:48.730851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#info of test dataset\nhouse_test.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T04:04:53.00527Z","iopub.execute_input":"2022-04-12T04:04:53.005703Z","iopub.status.idle":"2022-04-12T04:04:53.025604Z","shell.execute_reply.started":"2022-04-12T04:04:53.005661Z","shell.execute_reply":"2022-04-12T04:04:53.024779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#head of test dataset\nprint(house_test.head())","metadata":{"execution":{"iopub.status.busy":"2022-04-12T04:04:58.380373Z","iopub.execute_input":"2022-04-12T04:04:58.380674Z","iopub.status.idle":"2022-04-12T04:04:58.409857Z","shell.execute_reply.started":"2022-04-12T04:04:58.380639Z","shell.execute_reply":"2022-04-12T04:04:58.408835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#independent and dependent variables\ny_house = house_df['SalePrice']\nhouse_df = house_df.drop(['SalePrice'],axis=1)\nhouse_df = house_df.set_index('Id')\nhouse_test = house_test.set_index('Id')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T04:05:02.08492Z","iopub.execute_input":"2022-04-12T04:05:02.085188Z","iopub.status.idle":"2022-04-12T04:05:02.09652Z","shell.execute_reply.started":"2022-04-12T04:05:02.08516Z","shell.execute_reply":"2022-04-12T04:05:02.095811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#find out columns with missing value\nnull_list = []\nfor col in house_df.columns:\n    null = house_df[col].isnull().sum()\n    test_null = house_test[col].isnull().sum()\n    if null != 0 or test_null != 0:\n        null_list.append([col,null,test_null])\nnull_df = pd.DataFrame(null_list,columns=['Feature','Null','Test Null'])\nnull_df.set_index('Feature')\nnull_df['Total Null'] = null_df['Null'] + null_df['Test Null']\nprint(\"-------------------------\")\nprint(\"Total columns with null:\")\nprint(len(null_df))\nprint(\"-------------------------\")\nprint(\"Total null values:\")\nprint(null_df['Total Null'].sum(axis=0))\nprint(\"-------------------------\")\nsns.set_palette(sns.color_palette(\"pastel\"))\nsns.barplot(data=null_df.sort_values(by='Total Null',ascending = False).head(10), \n            x='Feature',y='Total Null')\nplt.xticks(rotation = 70)\nplt.title(\"Total Nulls in Feature\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T04:05:32.318242Z","iopub.execute_input":"2022-04-12T04:05:32.318962Z","iopub.status.idle":"2022-04-12T04:05:32.655661Z","shell.execute_reply.started":"2022-04-12T04:05:32.31892Z","shell.execute_reply":"2022-04-12T04:05:32.654707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.concat([house_df,house_test],axis=0).reset_index(drop=True)\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-12T04:09:43.167106Z","iopub.execute_input":"2022-04-12T04:09:43.167387Z","iopub.status.idle":"2022-04-12T04:09:43.1826Z","shell.execute_reply.started":"2022-04-12T04:09:43.167358Z","shell.execute_reply":"2022-04-12T04:09:43.181725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isna().mean().sort_values().plot(\nkind='bar', figsize=(20,10), title='Percentage of missing values',\nylabel='Ratio of missing values per feature')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T04:08:44.068397Z","iopub.execute_input":"2022-04-12T04:08:44.068714Z","iopub.status.idle":"2022-04-12T04:08:46.754232Z","shell.execute_reply.started":"2022-04-12T04:08:44.068679Z","shell.execute_reply":"2022-04-12T04:08:46.753408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dealing with missing values\n\nFirst, we will do some exploration on all features","metadata":{}},{"cell_type":"code","source":"#combine train and test data\ndata = pd.concat([house_df,house_test],axis=0).reset_index(drop=True)\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:47:07.705395Z","iopub.execute_input":"2022-04-11T12:47:07.705724Z","iopub.status.idle":"2022-04-11T12:47:07.730527Z","shell.execute_reply.started":"2022-04-11T12:47:07.705689Z","shell.execute_reply":"2022-04-11T12:47:07.729358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check all the object variables\ncol_1 = data.select_dtypes(include=['object']).columns\nfor col in col_1:\n    print(col, 'unique values :', data[col].sort_values().unique())","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:47:09.665452Z","iopub.execute_input":"2022-04-11T12:47:09.66579Z","iopub.status.idle":"2022-04-11T12:47:09.839928Z","shell.execute_reply.started":"2022-04-11T12:47:09.665756Z","shell.execute_reply":"2022-04-11T12:47:09.838912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some features have different value than it should (maybe those are typos). We will correct that","metadata":{}},{"cell_type":"code","source":"data['MSZoning'].replace('C (all)', 'C',inplace=True)\ndata['Neighborhood'].replace('NAmes', 'Names',inplace=True)\ndata['BldgType'].replace('2fmCon', '2FmCon',inplace=True)\ndata['BldgType'].replace('Twnhs', 'TwnhsI', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:47:12.871385Z","iopub.execute_input":"2022-04-11T12:47:12.87207Z","iopub.status.idle":"2022-04-11T12:47:12.881479Z","shell.execute_reply.started":"2022-04-11T12:47:12.872017Z","shell.execute_reply":"2022-04-11T12:47:12.880788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check the data again\ncol_1 = data.select_dtypes(include=['object']).columns\nfor col in col_1:\n    print(col, 'unique values :', data[col].sort_values().unique())","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:47:15.020741Z","iopub.execute_input":"2022-04-11T12:47:15.021229Z","iopub.status.idle":"2022-04-11T12:47:15.176673Z","shell.execute_reply.started":"2022-04-11T12:47:15.021193Z","shell.execute_reply":"2022-04-11T12:47:15.175743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. MsZoning ","metadata":{}},{"cell_type":"code","source":"#Dealing with MSZoning variables\nnull = house_test[house_test['MSZoning'].isnull()][[\"Neighborhood\",\"MSZoning\"]]\ndisplay(null)\nplot_data = pd.concat([data[data['Neighborhood'] == 'IDOTRR'],data[data['Neighborhood'] == 'Mitchel']],\n                      axis = 0)\nsns.histplot(data = plot_data, x ='MSZoning', hue ='Neighborhood',multiple=\"dodge\", shrink=.9)\nplt.title(\"Distribution of MSZoning Classification\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:47:18.296339Z","iopub.execute_input":"2022-04-11T12:47:18.296651Z","iopub.status.idle":"2022-04-11T12:47:18.570647Z","shell.execute_reply.started":"2022-04-11T12:47:18.296618Z","shell.execute_reply":"2022-04-11T12:47:18.569574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MSZning classification usually depends on the Neighborhood so we will impute the missing value by the mode in each area","metadata":{}},{"cell_type":"code","source":"house_test.loc[(house_test['Neighborhood'] == 'IDOTRR') & \n               (house_test['MSZoning'].isnull()), 'MSZoning'] = 'RM'\nhouse_test.loc[(house_test['Neighborhood'] == 'Mitchel') & \n               (house_test['MSZoning'].isnull()), 'MSZoning'] = 'RL'","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:47:24.386971Z","iopub.execute_input":"2022-04-11T12:47:24.387356Z","iopub.status.idle":"2022-04-11T12:47:24.398779Z","shell.execute_reply.started":"2022-04-11T12:47:24.387317Z","shell.execute_reply":"2022-04-11T12:47:24.397928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. LotFrontage\n\n   We could expect that LotFrontage somewhat has correlation with LotArea. Hence we will use\n   LinearRegression to impute the missing values. We also manually filter out the outliers from the \n   data.","metadata":{}},{"cell_type":"code","source":"data_1 = data[(~data['LotFrontage'].isnull()) & (data['LotFrontage'] <= 150) &\n              (data['LotArea'] <= 20000)]\nsns.lmplot(data=data_1,x=\"LotArea\",y=\"LotFrontage\", line_kws={'color': 'black'})\nplt.ylabel(\"LotFrontage\")\nplt.xlabel(\"LotArea\")\nplt.title(\" Scatterplot of LotArea vs LotFrontage\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:48:50.967299Z","iopub.execute_input":"2022-04-11T12:48:50.968286Z","iopub.status.idle":"2022-04-11T12:48:51.675786Z","shell.execute_reply.started":"2022-04-11T12:48:50.968202Z","shell.execute_reply":"2022-04-11T12:48:51.674802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We could see that there is a linear relationship between LotArea and LotFrontage. Thus we can use linear regression to impute missing value of LotFrontage","metadata":{}},{"cell_type":"code","source":"LotA_LotF = LinearRegression()\nLotA_LotF_X = data_1['LotArea'].values.reshape(-1, 1)\nLotA_LotF_y = data_1['LotFrontage'].values\nLotA_LotF.fit(LotA_LotF_X,LotA_LotF_y)\nfor table in [house_df, house_test]:\n    table['LotFrontage'].fillna(LotA_LotF.intercept_ + table['LotArea'] * LotA_LotF.coef_[0] \n                                , inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:48:56.192418Z","iopub.execute_input":"2022-04-11T12:48:56.192781Z","iopub.status.idle":"2022-04-11T12:48:56.205898Z","shell.execute_reply.started":"2022-04-11T12:48:56.192735Z","shell.execute_reply":"2022-04-11T12:48:56.20484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. Alley\n\n   Data description says NA means no alley access","metadata":{}},{"cell_type":"code","source":"for table in [house_df,house_test]:\n    table['Alley'].fillna(\"None\",inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:49:07.842263Z","iopub.execute_input":"2022-04-11T12:49:07.842611Z","iopub.status.idle":"2022-04-11T12:49:07.850605Z","shell.execute_reply.started":"2022-04-11T12:49:07.842578Z","shell.execute_reply":"2022-04-11T12:49:07.849222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. Utilities\n\n   ","metadata":{}},{"cell_type":"code","source":"house_df['Utilities'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:49:11.308737Z","iopub.execute_input":"2022-04-11T12:49:11.309132Z","iopub.status.idle":"2022-04-11T12:49:11.318758Z","shell.execute_reply.started":"2022-04-11T12:49:11.309092Z","shell.execute_reply":"2022-04-11T12:49:11.318129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"house_test['Utilities'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:49:13.378241Z","iopub.execute_input":"2022-04-11T12:49:13.378753Z","iopub.status.idle":"2022-04-11T12:49:13.385952Z","shell.execute_reply.started":"2022-04-11T12:49:13.378718Z","shell.execute_reply":"2022-04-11T12:49:13.385372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since there is only 1 data that uses NoSeWa, we will  fill the missing value in test set with AllPub.\nWe will just drop the NoSeWa row in our training dataset since it is not found in the test set and will contribute to overfitting if left alone.","metadata":{}},{"cell_type":"code","source":"#fill all utilities missing values on test dataset with AllPub\nhouse_test['Utilities'].fillna(\"AllPub\",inplace=True)\n\n#drop train dataset row where Utilitiy has NoSeWa value\nhouse_df.drop(house_df[house_df['Utilities'] == 'NoSeWa'].index, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:49:15.83663Z","iopub.execute_input":"2022-04-11T12:49:15.836997Z","iopub.status.idle":"2022-04-11T12:49:15.851003Z","shell.execute_reply.started":"2022-04-11T12:49:15.83696Z","shell.execute_reply":"2022-04-11T12:49:15.849686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5. Exterior features\n\n   Exterior1st : Exterior covering on house\n   \n   Exterior2nd : Exterior covering on house (if more than one material)\n\n   There are more than 10 types of materials used in both the metrics. We will see the barplot of \n   these features.","metadata":{}},{"cell_type":"code","source":"for metrics in ['Exterior1st','Exterior2nd']:\n    table = data[metrics].value_counts(normalize=True).head()\n    sns.barplot(x=table.index,y=table.values)\n    plt.title(\"Distribution plot of \"+metrics)\n    plt.show()\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:49:18.180872Z","iopub.execute_input":"2022-04-11T12:49:18.181212Z","iopub.status.idle":"2022-04-11T12:49:18.611128Z","shell.execute_reply.started":"2022-04-11T12:49:18.18118Z","shell.execute_reply":"2022-04-11T12:49:18.60997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For Exterior1st and Exterior2nd, the mode is VinylSd. Hence, we will replace missing values with VinylSd","metadata":{}},{"cell_type":"code","source":"house_test['Exterior1st'] = house_test['Exterior1st'].fillna(data['Exterior1st'].mode()[0])\nhouse_test['Exterior2nd'] = house_test['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:49:22.03107Z","iopub.execute_input":"2022-04-11T12:49:22.031895Z","iopub.status.idle":"2022-04-11T12:49:22.041352Z","shell.execute_reply.started":"2022-04-11T12:49:22.031847Z","shell.execute_reply":"2022-04-11T12:49:22.040216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"6. MasVnrType \n\n   Data description says NA means no Masonry veneer. However, there is one data in test set with area \n   but missing type.","metadata":{}},{"cell_type":"code","source":"house_test[(house_test['MasVnrType'].isnull()) & \n           (house_test['MasVnrArea'].notnull())][['MasVnrType','MasVnrArea']]","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:49:24.286532Z","iopub.execute_input":"2022-04-11T12:49:24.287021Z","iopub.status.idle":"2022-04-11T12:49:24.301094Z","shell.execute_reply.started":"2022-04-11T12:49:24.286977Z","shell.execute_reply":"2022-04-11T12:49:24.300081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"table = data['MasVnrType'].value_counts(normalize=True).head()\nsns.barplot(x=table.index,y=table.values)\nplt.title(\"Distribution plot of MasVnrType\")\nplt.show()\nprint(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:49:26.983206Z","iopub.execute_input":"2022-04-11T12:49:26.983509Z","iopub.status.idle":"2022-04-11T12:49:27.350779Z","shell.execute_reply.started":"2022-04-11T12:49:26.983479Z","shell.execute_reply":"2022-04-11T12:49:27.349241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since around 60% of our data do not have Masonry veneer (have None value), it will be used to fill the null value in row 2611 and also the other rows.","metadata":{}},{"cell_type":"code","source":"house_test['MasVnrType'][2611] = \"BrkFace\"\nhouse_test['MasVnrType'] = house_test['MasVnrType'].fillna(data['MasVnrType'].mode()[0])\nhouse_test['MasVnrArea'] = house_test['MasVnrArea'].fillna(0)\nhouse_df['MasVnrType'] = house_df['MasVnrType'].fillna(data['MasVnrType'].mode()[0])\nhouse_df['MasVnrArea'] = house_df['MasVnrArea'].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:49:29.978558Z","iopub.execute_input":"2022-04-11T12:49:29.978847Z","iopub.status.idle":"2022-04-11T12:49:29.9922Z","shell.execute_reply.started":"2022-04-11T12:49:29.978816Z","shell.execute_reply":"2022-04-11T12:49:29.990755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"7. Basement Metrics \n\n   Data description says BsmtFinType1 measures the Type 1 finished square feet of basement.\n   However, we can see a few data in test data set having basement metrics but \"0\" squarefeets","metadata":{}},{"cell_type":"code","source":"for basement_metrics_cols in ['BsmtExposure','BsmtCond','BsmtQual']:\n    if len(data[(data[basement_metrics_cols].isnull()) & (data['BsmtFinType1'].notnull())]) > 0 :\n        print(\"\\nPresent with BsmtFinType1 but \" +  basement_metrics_cols + \" undetected\")\n        display(data[(data[basement_metrics_cols].isnull()) & (data['BsmtFinType1'].notnull())])","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:49:32.911184Z","iopub.execute_input":"2022-04-11T12:49:32.911464Z","iopub.status.idle":"2022-04-11T12:49:33.079884Z","shell.execute_reply.started":"2022-04-11T12:49:32.911435Z","shell.execute_reply":"2022-04-11T12:49:33.078983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We assume missing basement exposure of unfinished basement is \"No\".\nhouse_df.loc[((house_df['BsmtExposure'].isnull()) & (house_df['BsmtFinType1'].notnull())),\n             'BsmtExposure'] = 'No'\nhouse_test.loc[((house_test['BsmtExposure'].isnull()) & (house_test['BsmtFinType1'].notnull())), \n               'BsmtExposure'] = 'No'\n\n# We impute missing basement condition with \"mean\" value of Typical.\nhouse_test.loc[((house_test['BsmtCond'].isnull()) & (house_test['BsmtFinType1'].notnull())), \n               'BsmtCond'] = 'TA'\n# We impute unfinished basement quality with \"mean\" value of Typical.\nhouse_test.loc[((house_test['BsmtQual'].isnull()) & (house_test['BsmtFinType1'].notnull())), \n               'BsmtQual'] = 'TA'","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:49:36.604024Z","iopub.execute_input":"2022-04-11T12:49:36.604363Z","iopub.status.idle":"2022-04-11T12:49:36.619241Z","shell.execute_reply.started":"2022-04-11T12:49:36.604331Z","shell.execute_reply":"2022-04-11T12:49:36.618093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is one test data with missing square feet values. Let's check that data","metadata":{}},{"cell_type":"code","source":"house_test[house_test['BsmtFinSF1'].isnull()]","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:50:00.854159Z","iopub.execute_input":"2022-04-11T12:50:00.854955Z","iopub.status.idle":"2022-04-11T12:50:00.92055Z","shell.execute_reply.started":"2022-04-11T12:50:00.854912Z","shell.execute_reply":"2022-04-11T12:50:00.919266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This test data do not have basement. Hence, those squarefeets metrics should be filled in with 0.","metadata":{}},{"cell_type":"code","source":"for square_feet_metrics in ['TotalBsmtSF','BsmtUnfSF','BsmtFinSF2','BsmtFinSF1']:\n    house_test[square_feet_metrics][2121] = 0","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:50:04.091354Z","iopub.execute_input":"2022-04-11T12:50:04.09164Z","iopub.status.idle":"2022-04-11T12:50:04.100606Z","shell.execute_reply.started":"2022-04-11T12:50:04.091611Z","shell.execute_reply":"2022-04-11T12:50:04.099362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is two test data with missing basement bathroom values. Let's check them out first too.","metadata":{}},{"cell_type":"code","source":"house_test[house_test['BsmtFullBath'].isnull()]","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:50:06.34232Z","iopub.execute_input":"2022-04-11T12:50:06.342621Z","iopub.status.idle":"2022-04-11T12:50:06.400037Z","shell.execute_reply.started":"2022-04-11T12:50:06.342592Z","shell.execute_reply":"2022-04-11T12:50:06.398632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The two test data do not have basement. Hence, those bathroom amount in basement should also be filled in with 0.","metadata":{}},{"cell_type":"code","source":"for bathroom_metrics in ['BsmtFullBath','BsmtHalfBath']:\n    house_test[bathroom_metrics][2121] = 0\n    house_test[bathroom_metrics][2189] = 0","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:50:08.596959Z","iopub.execute_input":"2022-04-11T12:50:08.597289Z","iopub.status.idle":"2022-04-11T12:50:08.605618Z","shell.execute_reply.started":"2022-04-11T12:50:08.597256Z","shell.execute_reply":"2022-04-11T12:50:08.604454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The other data are assumed to not have basements hence filling in None.","metadata":{}},{"cell_type":"code","source":"for table in [house_df,house_test]:\n    table[table.columns[table.columns.str.contains('Bsmt')]] = table[table.columns\n                                                                     [table.columns.str.contains('Bsmt')]].fillna(\"None\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:50:11.724709Z","iopub.execute_input":"2022-04-11T12:50:11.72504Z","iopub.status.idle":"2022-04-11T12:50:11.745302Z","shell.execute_reply.started":"2022-04-11T12:50:11.725008Z","shell.execute_reply":"2022-04-11T12:50:11.7438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"8. Electrical, Functional and Kitchen Quality \n\n   We will see the distribution of each value in these features.","metadata":{}},{"cell_type":"code","source":"for metrics in ['Electrical','Functional','KitchenQual']:\n    table = data[metrics].value_counts(normalize=True)\n    sns.barplot(x=table.index,y=table.values)\n    plt.title(\"Distribution plot of \"+ metrics)\n    plt.show()\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:50:14.007455Z","iopub.execute_input":"2022-04-11T12:50:14.007785Z","iopub.status.idle":"2022-04-11T12:50:14.654253Z","shell.execute_reply.started":"2022-04-11T12:50:14.007742Z","shell.execute_reply":"2022-04-11T12:50:14.653081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These three metrics will be filled with the mode values.","metadata":{}},{"cell_type":"code","source":"house_df['Electrical'].fillna('SBrkr',inplace=True)\nhouse_test['Functional'].fillna('Typ',inplace=True)\nhouse_test['KitchenQual'].fillna('TA',inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:55:17.31743Z","iopub.execute_input":"2022-04-11T12:55:17.317789Z","iopub.status.idle":"2022-04-11T12:55:17.329273Z","shell.execute_reply.started":"2022-04-11T12:55:17.317752Z","shell.execute_reply":"2022-04-11T12:55:17.328175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"9. Garage ","metadata":{}},{"cell_type":"code","source":"data[data['GarageCars'].isnull()]","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:55:20.625618Z","iopub.execute_input":"2022-04-11T12:55:20.62598Z","iopub.status.idle":"2022-04-11T12:55:20.683801Z","shell.execute_reply.started":"2022-04-11T12:55:20.625944Z","shell.execute_reply":"2022-04-11T12:55:20.682525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Simililarly, this test data do not have a garage, filling GarageArea and GarageCars with 0.","metadata":{}},{"cell_type":"code","source":"house_test['GarageCars'].fillna(0,inplace=True)\nhouse_test['GarageArea'].fillna(0,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:55:23.00135Z","iopub.execute_input":"2022-04-11T12:55:23.001934Z","iopub.status.idle":"2022-04-11T12:55:23.007527Z","shell.execute_reply.started":"2022-04-11T12:55:23.001873Z","shell.execute_reply":"2022-04-11T12:55:23.006787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"10. SaleType","metadata":{}},{"cell_type":"code","source":"display(data[data['SaleType'].isnull()])\ntable = data['SaleType'].value_counts(normalize=True)\nsns.barplot(x=table.index,y=table.values)\nplt.title(\"Distribution plot of SaleType\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:55:25.531086Z","iopub.execute_input":"2022-04-11T12:55:25.53147Z","iopub.status.idle":"2022-04-11T12:55:25.828033Z","shell.execute_reply.started":"2022-04-11T12:55:25.531425Z","shell.execute_reply":"2022-04-11T12:55:25.827116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the SaleType column, we will impute the missing data with the mode","metadata":{}},{"cell_type":"code","source":"house_test['SaleType'].fillna('WD',inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:55:29.262211Z","iopub.execute_input":"2022-04-11T12:55:29.2625Z","iopub.status.idle":"2022-04-11T12:55:29.269719Z","shell.execute_reply.started":"2022-04-11T12:55:29.262471Z","shell.execute_reply":"2022-04-11T12:55:29.268327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check all the missing value again","metadata":{}},{"cell_type":"code","source":"null_list = []\nfor col in house_df.columns:\n    null = house_df[col].isnull().sum()\n    test_null = house_test[col].isnull().sum()\n    if null != 0 or test_null != 0:\n        null_list.append([col,null,test_null])\nnull_df = pd.DataFrame(null_list,columns=['Feature','Null','Test Null'])\nnull_df.set_index('Feature')\nnull_df['Total Null'] = null_df['Null'] + null_df['Test Null']\nprint(\"-------------------------\")\nprint(\"Total columns with null:\")\nprint(len(null_df))\nprint(\"-------------------------\")\nprint(\"Total null values:\")\nprint(null_df['Total Null'].sum(axis=0))\nprint(\"-------------------------\")\nsns.set_palette(sns.color_palette(\"pastel\"))\nsns.barplot(data=null_df.sort_values(by='Total Null',ascending = False).head(10), x='Feature',y='Total Null')\nplt.xticks(rotation = 70)\nplt.title(\"Total Nulls in Feature\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:55:31.525955Z","iopub.execute_input":"2022-04-11T12:55:31.526303Z","iopub.status.idle":"2022-04-11T12:55:31.879494Z","shell.execute_reply.started":"2022-04-11T12:55:31.526269Z","shell.execute_reply":"2022-04-11T12:55:31.878177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We do not have anything extra to infer these missing columns. Hence, we will treat them as \"None\" which is not having those items.","metadata":{}},{"cell_type":"code","source":"house_df['GarageYrBlt'].fillna(0,inplace=True)\nhouse_test['GarageYrBlt'].fillna(0,inplace=True)\nhouse_df.fillna(\"None\", inplace=True)\nhouse_test.fillna(\"None\", inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:55:35.237724Z","iopub.execute_input":"2022-04-11T12:55:35.238002Z","iopub.status.idle":"2022-04-11T12:55:35.266418Z","shell.execute_reply.started":"2022-04-11T12:55:35.237973Z","shell.execute_reply":"2022-04-11T12:55:35.265455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check the missing values again \nprint(house_df.isnull().sum().sum() + house_test.isnull().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:55:37.734009Z","iopub.execute_input":"2022-04-11T12:55:37.734333Z","iopub.status.idle":"2022-04-11T12:55:37.77399Z","shell.execute_reply.started":"2022-04-11T12:55:37.7343Z","shell.execute_reply":"2022-04-11T12:55:37.772844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"house_df.index = house_df.index - 1","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:55:40.068863Z","iopub.execute_input":"2022-04-11T12:55:40.069244Z","iopub.status.idle":"2022-04-11T12:55:40.075387Z","shell.execute_reply.started":"2022-04-11T12:55:40.069206Z","shell.execute_reply":"2022-04-11T12:55:40.074136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature Engineering\n\nIn this section, we will look for and modify features so we can perform modelling properly","metadata":{}},{"cell_type":"code","source":"#Checking y variable\n#Distribution plot\nsns.distplot(y_house , fit=norm);\n\n(mu, sigma) = norm.fit(y_house)\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n# QQ-plot\nfig = plt.figure()\nres = probplot(y_house, plot=plt)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:55:42.772267Z","iopub.execute_input":"2022-04-11T12:55:42.772565Z","iopub.status.idle":"2022-04-11T12:55:43.595178Z","shell.execute_reply.started":"2022-04-11T12:55:42.772535Z","shell.execute_reply":"2022-04-11T12:55:43.594141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first plot is a distribution plot where we compare the distribution of our target variable with a normal distribution.\nWe can see that our data is right-skewed.\n\nThe Q-Q plot the quantiles of our target feature against the quantiles of a normal distribution.\nWe can also easily see the skewness in the target feature.\n\nWe will transform our y variable","metadata":{}},{"cell_type":"code","source":"#Transform using log\ny_house = np.log(y_house)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:55:48.066036Z","iopub.execute_input":"2022-04-11T12:55:48.066366Z","iopub.status.idle":"2022-04-11T12:55:48.071156Z","shell.execute_reply.started":"2022-04-11T12:55:48.066326Z","shell.execute_reply":"2022-04-11T12:55:48.070414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#See the distribution and QQ-plot again\nsns.distplot(y_house , fit=norm);\n(mu, sigma) = norm.fit(y_house)\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\nfig = plt.figure()\nres = probplot(y_house, plot=plt)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:55:51.708323Z","iopub.execute_input":"2022-04-11T12:55:51.708653Z","iopub.status.idle":"2022-04-11T12:55:52.361588Z","shell.execute_reply.started":"2022-04-11T12:55:51.708608Z","shell.execute_reply":"2022-04-11T12:55:52.360736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now see the distribution plot is much closer to a normal distribution.\n\nThe Q-Q plot also shows that the quantiles of our target feature and the quantiles of a normal distribution are much closer now.","metadata":{}},{"cell_type":"markdown","source":"Feature creation\n\nIn this short section we will construct some new (important) features from existing features that can be fed into our model later on. There are many ways to increase our data, one of them is through creating combinations or ratio from the most relevant variables from the raw data.\n\nI will add only a few extra features related to square-feet as I think the size of a house will be the main factor of its price.\n\nWe also transformed some features that are supposingly categorical but labelled as numerical as they are consisting of numbers.","metadata":{}},{"cell_type":"code","source":"#Find the total square feet\nhouse_df['TotalSF'] = house_df['TotalBsmtSF'] + house_df['1stFlrSF'] + house_df['2ndFlrSF']\nhouse_test['TotalSF'] = house_test['TotalBsmtSF'] + house_test['1stFlrSF'] + house_test['2ndFlrSF']","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:55:56.023522Z","iopub.execute_input":"2022-04-11T12:55:56.023818Z","iopub.status.idle":"2022-04-11T12:55:56.032262Z","shell.execute_reply.started":"2022-04-11T12:55:56.023788Z","shell.execute_reply":"2022-04-11T12:55:56.031583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#change some datatype to string\nfor table in [house_df,house_test]:\n    table['MSSubClass'] = table['MSSubClass'].apply(str)\n    table['YrSold'] = table['YrSold'].astype(str)\n    table['MoSold'] = table['MoSold'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:55:58.269789Z","iopub.execute_input":"2022-04-11T12:55:58.270299Z","iopub.status.idle":"2022-04-11T12:55:58.289826Z","shell.execute_reply.started":"2022-04-11T12:55:58.270261Z","shell.execute_reply":"2022-04-11T12:55:58.288776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature Encoding Round 1 (Ordinal)\n\nMany machine learning models prefer or can only work with numerical values. It is common practice to transform the categorical values into numerical.\n\nThere are many ways though, to transform the features, one of which is through ordinal encoding. We use this method whenever our features has order (A is better than B) so that we can retain the information regarding the order.","metadata":{}},{"cell_type":"code","source":"qual_dict = {'None': 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\nbsmt_fin_dict = {'None': 0, \"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5, \"GLQ\": 6}\n\nfor table in [house_df,house_test]:\n    table[\"ExterQual\"] = table[\"ExterQual\"].map(qual_dict)\n    table[\"ExterCond\"] = table[\"ExterCond\"].map(qual_dict)\n    table[\"BsmtQual\"] = table[\"BsmtQual\"].map(qual_dict)\n    table[\"BsmtCond\"] = table[\"BsmtCond\"].map(qual_dict)\n    table[\"PoolQC\"] = table[\"PoolQC\"].map(qual_dict)\n    table[\"HeatingQC\"] = table[\"HeatingQC\"].map(qual_dict)\n    table[\"KitchenQual\"] = table[\"KitchenQual\"].map(qual_dict)\n    table[\"FireplaceQu\"] = table[\"FireplaceQu\"].map(qual_dict)\n    table[\"GarageQual\"] = table[\"GarageQual\"].map(qual_dict)\n    table[\"GarageCond\"] = table[\"GarageCond\"].map(qual_dict)\n\n    table[\"BsmtExposure\"] = table[\"BsmtExposure\"].map(\n        {'None': 0, \"No\": 1, \"Mn\": 2, \"Av\": 3, \"Gd\": 4}) \n    table[\"BsmtFinType1\"] = table[\"BsmtFinType1\"].map(bsmt_fin_dict)\n    table[\"BsmtFinType2\"] = table[\"BsmtFinType2\"].map(bsmt_fin_dict)\n\n    table[\"Functional\"] = table[\"Functional\"].map(\n        {'None': 0, \"Sal\": 1, \"Sev\": 2, \"Maj2\": 3, \"Maj1\": 4, \n         \"Mod\": 5, \"Min2\": 6, \"Min1\": 7, \"Typ\": 8})\n\n    table[\"GarageFinish\"] = table[\"GarageFinish\"].map(\n        {'None': 0, \"Unf\": 1, \"RFn\": 2, \"Fin\": 3})\n\n    table[\"Fence\"] = table[\"Fence\"].map(\n        {'None': 0, \"MnWw\": 1, \"GdWo\": 2, \"MnPrv\": 3, \"GdPrv\": 4})\n    \n    table[\"CentralAir\"] = table[\"CentralAir\"].map(\n        {'N': 0, \"Y\": 1})\n    \n    table[\"PavedDrive\"] = table[\"PavedDrive\"].map(\n        {'N': 0, \"P\": 1, \"Y\": 2})\n\n    \n    table[\"Street\"] = table[\"Street\"].map(\n        {'Grvl': 0, \"Pave\": 1})\n    \n    table[\"Alley\"] = table[\"Alley\"].map(\n        {'None': 0, \"Grvl\": 1, \"Pave\": 2})\n    \n    table[\"LandSlope\"] = table[\"LandSlope\"].map(\n        {'Gtl': 0, \"Mod\": 1, \"Sev\": 2})\n    \n    table[\"LotShape\"] = table[\"LotShape\"].map(\n        {'Reg': 0, \"IR1\": 1, \"IR2\": 2, \"IR3\": 3})\n    \nmodified_cols = ['ExterQual','ExterCond','BsmtQual','BsmtCond','HeatingQC','KitchenQual' \\\n                    ,'FireplaceQu','GarageQual','GarageCond','BsmtExposure','BsmtFinType1' \\\n                   ,'BsmtFinType2', 'Functional','GarageFinish','Fence','Street','Alley','LandSlope'\\\n                    ,'PavedDrive' ,'CentralAir','PoolQC','OverallQual','OverallCond','LotShape']\n\n# Get list of categorical variables in holiday dataset\ns = (house_df.dtypes == 'object')\nobject_cols = list(s[s].index)\nobject_cols = [x for x in object_cols if x not in modified_cols]","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:56:00.668757Z","iopub.execute_input":"2022-04-11T12:56:00.669677Z","iopub.status.idle":"2022-04-11T12:56:00.739501Z","shell.execute_reply.started":"2022-04-11T12:56:00.66962Z","shell.execute_reply":"2022-04-11T12:56:00.738384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can still go further to simplify our features. When the feature is highly skewed, we can group some values into \"Others\" to reduce the number of columns when we use one-hot encoding later on.\n\nSo, we will plot the distributions of the features and see how we should simplify them.","metadata":{}},{"cell_type":"code","source":"data = pd.merge(left = house_df, right = y_house , left_index= True, right_index = True)\ndata['SalePrice'] = np.exp(data['SalePrice'])\n\nfor col in object_cols:\n    if data[col].nunique()> 1:\n        print(\"\\nSummary statistics and graph for \"+ col)\n        display(data.groupby(col)['SalePrice'].describe())\n        fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n        sns.countplot(data = data, x=col, ax= ax[0])\n        ax[0].title.set_text(\"Count plot of \" + col)\n        sns.swarmplot(data=data,x=col,y='SalePrice', ax= ax[1])\n        ax[1].title.set_text(\"Swarm plot of \" + col +\" versus Sale Price\")\n        if (data[col].nunique()>=15):\n            ax[0].tick_params('x',labelrotation=70)\n            ax[1].tick_params('x',labelrotation=70)\n        fig.tight_layout()\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T12:56:04.48197Z","iopub.execute_input":"2022-04-11T12:56:04.48233Z","iopub.status.idle":"2022-04-11T12:57:35.996922Z","shell.execute_reply.started":"2022-04-11T12:56:04.482298Z","shell.execute_reply":"2022-04-11T12:57:35.99616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that many of the features are highly skewed and some feature value counts are very low.\nHence, we will just group them as \"Others\". For features that only have two value, we will also just do the manual one-hot encoding here.\n\nThose that have more than two unique values will be one-hot encoded below.","metadata":{}},{"cell_type":"code","source":"cond_1_keep = ['Norm','Feedr','Artery']\nroof_style_keep = ['Gable','Hip']\nfoundation_keep = ['PConc','CBlock','BrkTil']\ngarage_keep = ['Attchd','Detchd','BuiltIn']\nsale_keep = ['WD','New','COD']\nsale_cond_keep = ['Normal','Abnorml','Partial']\npeak_months = ['5','6','7']\nlot_config_keep = ['Inside','Corner','CulDSac']\nunfinished_style = ['1.5Unf','2.5Unf']\nexter_remove = ['AsphShn','BrkComm','CBlock','ImStucc','Stone']\nfor table in [house_df,house_test]:\n    table.loc[table['LandContour']!='Lvl','LandContour'] = 0\n    table.loc[table['LandContour']!=0,'LandContour'] = 1\n    \n    table.loc[~table['Condition1'].isin(cond_1_keep),'Condition1'] = \"Others\"\n    table.loc[table['Condition2']!=\"Norm\",'Condition2'] = 0\n    table.loc[table['Condition2']!= 0,'Condition2'] = 1\n    \n    table.loc[~table['RoofStyle'].isin(roof_style_keep),'RoofStyle'] = \"Others\"\n    table.loc[table['RoofMatl']!='CompShg','RoofMatl'] = 0\n    table.loc[table['RoofMatl']!=0,'RoofMatl'] = 1\n    \n    table.loc[~table['Foundation'].isin(foundation_keep),'Foundation'] = \"Others\"\n    table.loc[table['Heating']!='GasA','Heating'] = 0\n    table.loc[table['Heating']=='GasA','Heating'] = 1\n    table.loc[table['Electrical']!='SBrkr','Electrical'] = 0\n    table.loc[table['Electrical']!=0,'Electrical'] = 1\n    \n    table.loc[~table['GarageType'].isin(garage_keep),'GarageType'] = \"Others\"\n    \n    table.loc[~table['SaleType'].isin(sale_keep),'SaleType'] = \"Others\"\n    table.loc[~table['SaleCondition'].isin(sale_cond_keep),'SaleCondition'] = \"Others\"\n    table.loc[~table['SaleCondition'].isin(sale_cond_keep),'SaleCondition'] = \"Others\"\n    \n    table.loc[table['Exterior1st'].isin(exter_remove),'Exterior1st'] = \"Others\"\n    table.loc[table['Exterior2nd'].isin(exter_remove),'Exterior2nd'] = \"Others\"\n    \n    table.loc[table['MoSold'].isin(peak_months),'PeakMonths'] = 1\n    table.loc[table['PeakMonths']!=1,'PeakMonths'] = 0\n    \n    table.loc[~table['LotConfig'].isin(lot_config_keep),'LotConfig'] = \"Others\"\n    \n    table.loc[~table['HouseStyle'].isin(unfinished_style),'Unfinished'] = 1\n    table.loc[table['Unfinished']!= 1 ,'Unfinished'] = 0\n    table.loc[table['HouseStyle'].isin(['SFoyer','SLvl']),'IsSplit'] = 1\n    table.loc[table['IsSplit']!= 1 ,'IsSplit'] = 0   \n    table[\"HouseStyle\"] = table[\"HouseStyle\"].map(\n        {'SFoyer': 0, \"SLvl\": 0, \"1Story\": 1, \"1.5Fin\": 2, \"1.5Unf\": 2, \"2Story\": 3, \n         \"2.5Fin\": 4, \"2.5Unf\": 4})\n    \n    table.drop('Utilities', axis = 1 , inplace = True)\n\n    \nmodified_cols_round_2 = ['HouseStyle','LandContour','Condition2','RoofMatl','Heating',\n                         'Electrical','Utilities']\nobject_cols = [x for x in object_cols if x not in modified_cols_round_2]","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:01:34.779197Z","iopub.execute_input":"2022-04-11T13:01:34.779956Z","iopub.status.idle":"2022-04-11T13:01:34.865459Z","shell.execute_reply.started":"2022-04-11T13:01:34.779913Z","shell.execute_reply":"2022-04-11T13:01:34.864371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature Clustering\n\nBefore we go on to one-hot encode our categorical features. We can see that some of the features still have a lot of unique values.\n\nThis will cause our final training data to have a lot of columns as each and every of the unique values will be encoded into one extra column. So we need to simmplify the features using clusters.\n\nTo do that, we will use an unsupervised learning method which is K-Means to identify suitable clusters.\n\nFor neighborhoods, I intend to group them into 5 clusters and subclasses I will group them into 4 clusters.\n\nTo do that, we try to provide K-Means with as many information regarding the feature that we want to cluster as possible. We will use .describe() to include the various statistics regarding the feature and feed it into the model.","metadata":{}},{"cell_type":"code","source":"neighborhood = data.groupby(['Neighborhood'])['SalePrice'].describe()\ndisplay(neighborhood.head())","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:01:38.820369Z","iopub.execute_input":"2022-04-11T13:01:38.820726Z","iopub.status.idle":"2022-04-11T13:01:38.887426Z","shell.execute_reply.started":"2022-04-11T13:01:38.820693Z","shell.execute_reply":"2022-04-11T13:01:38.886334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neighborhood_cluster = KMeans(n_clusters=5, random_state = 927)\nneighborhood_cluster.fit(neighborhood)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:01:41.492883Z","iopub.execute_input":"2022-04-11T13:01:41.493263Z","iopub.status.idle":"2022-04-11T13:01:41.534835Z","shell.execute_reply.started":"2022-04-11T13:01:41.493221Z","shell.execute_reply":"2022-04-11T13:01:41.533774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neigh_cluster_table = pd.DataFrame(zip(list(neighborhood.index),list(neighborhood.loc[:,'mean']),\n                      list(neighborhood_cluster.labels_)),columns = ['Neighborhood',\n                       'MeanSalePrice','Neighborhood Cluster'])\nfor i  in range(len(neigh_cluster_table.groupby('Neighborhood Cluster')['Neighborhood'].unique())):\n    print(\"Cluster \" + str(i))\n    print(neigh_cluster_table.groupby('Neighborhood Cluster')['Neighborhood'].unique()[i])\nsns.scatterplot(data = neigh_cluster_table, x='Neighborhood',y = 'MeanSalePrice', hue='Neighborhood Cluster',palette=sns.color_palette(\"Set2\",5))\nplt.xticks(rotation=70)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:01:43.507576Z","iopub.execute_input":"2022-04-11T13:01:43.507938Z","iopub.status.idle":"2022-04-11T13:01:44.348696Z","shell.execute_reply.started":"2022-04-11T13:01:43.507901Z","shell.execute_reply":"2022-04-11T13:01:44.347494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subclass = data.groupby(['MSSubClass'])['SalePrice'].describe()\ndisplay(subclass.head())","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:01:48.265508Z","iopub.execute_input":"2022-04-11T13:01:48.267073Z","iopub.status.idle":"2022-04-11T13:01:48.317923Z","shell.execute_reply.started":"2022-04-11T13:01:48.266929Z","shell.execute_reply":"2022-04-11T13:01:48.317161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subclass_cluster = KMeans(n_clusters=4, random_state = 927)\nsubclass_cluster.fit(subclass)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:01:50.84828Z","iopub.execute_input":"2022-04-11T13:01:50.848567Z","iopub.status.idle":"2022-04-11T13:01:50.879402Z","shell.execute_reply.started":"2022-04-11T13:01:50.848537Z","shell.execute_reply":"2022-04-11T13:01:50.878444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mssub_cluster_table = pd.DataFrame(zip(list(subclass.index),list(subclass.loc[:,'mean']),list(subclass_cluster.labels_)),columns = ['MSSubClass','MeanSalePrice','MSSubClass Cluster'])\nfor i  in range(len(mssub_cluster_table.groupby('MSSubClass Cluster')['MSSubClass'].unique())):\n    print(\"Cluster \" + str(i))\n    print(mssub_cluster_table.groupby('MSSubClass Cluster')['MSSubClass'].unique()[i])\nsns.scatterplot(data = mssub_cluster_table, x='MSSubClass',y = 'MeanSalePrice', hue='MSSubClass Cluster',palette=sns.color_palette(\"Set2\",4))\nplt.xticks(rotation=70)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:01:54.368925Z","iopub.execute_input":"2022-04-11T13:01:54.369219Z","iopub.status.idle":"2022-04-11T13:01:54.789627Z","shell.execute_reply.started":"2022-04-11T13:01:54.369189Z","shell.execute_reply":"2022-04-11T13:01:54.788327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mssub_cluster_table.drop('MeanSalePrice', axis = 1 ,inplace = True)\nneigh_cluster_table.drop('MeanSalePrice', axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:01:57.538461Z","iopub.execute_input":"2022-04-11T13:01:57.538744Z","iopub.status.idle":"2022-04-11T13:01:57.545653Z","shell.execute_reply.started":"2022-04-11T13:01:57.538715Z","shell.execute_reply":"2022-04-11T13:01:57.544894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"house_df = pd.merge(left = house_df.reset_index(), right = mssub_cluster_table, \n                    how='left', on ='MSSubClass').set_index('Id')\nhouse_df = pd.merge(left = house_df.reset_index(), right = neigh_cluster_table, how='left', \n              on ='Neighborhood').set_index('Id')\nhouse_df.drop('MSSubClass', axis = 1 ,inplace = True)\nhouse_df.drop('Neighborhood', axis = 1 ,inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:01:59.614322Z","iopub.execute_input":"2022-04-11T13:01:59.614847Z","iopub.status.idle":"2022-04-11T13:01:59.665719Z","shell.execute_reply.started":"2022-04-11T13:01:59.614809Z","shell.execute_reply":"2022-04-11T13:01:59.664423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"house_test = pd.merge(left = house_test.reset_index(), right = mssub_cluster_table, how='left', \n                on ='MSSubClass').set_index('Id')\nhouse_test = pd.merge(left = house_test.reset_index(), right = neigh_cluster_table, how='left', \n                on ='Neighborhood').set_index('Id')\nhouse_test.drop('MSSubClass', axis = 1 ,inplace = True)\nhouse_test.drop('Neighborhood', axis = 1 ,inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:02:01.829593Z","iopub.execute_input":"2022-04-11T13:02:01.829881Z","iopub.status.idle":"2022-04-11T13:02:01.877038Z","shell.execute_reply.started":"2022-04-11T13:02:01.829851Z","shell.execute_reply":"2022-04-11T13:02:01.876283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After merging the clusters into our training and test dataset, we check again the remaining categorical variables that we want to one-hot encode.","metadata":{}},{"cell_type":"code","source":"modified_cols.append('MSSubClass')\nmodified_cols.append('Neighborhood')","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:02:04.052926Z","iopub.execute_input":"2022-04-11T13:02:04.053866Z","iopub.status.idle":"2022-04-11T13:02:04.058438Z","shell.execute_reply.started":"2022-04-11T13:02:04.05382Z","shell.execute_reply":"2022-04-11T13:02:04.057113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"object_cols.append('MSSubClass Cluster')\nobject_cols.append('Neighborhood Cluster')\nobject_cols.remove('MSSubClass')\nobject_cols.remove('Neighborhood')","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:02:07.312825Z","iopub.execute_input":"2022-04-11T13:02:07.313181Z","iopub.status.idle":"2022-04-11T13:02:07.319695Z","shell.execute_reply.started":"2022-04-11T13:02:07.31315Z","shell.execute_reply":"2022-04-11T13:02:07.318587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we perform one-hot encoding to the remaining categorical variables","metadata":{}},{"cell_type":"code","source":"# One Hot Encoding for Other Columns\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols = pd.DataFrame(OH_encoder.fit_transform(house_df[object_cols]))\nOH_cols.index = house_df.index\nOH_cols.columns = OH_encoder.get_feature_names(object_cols)\nhouse_df = house_df.drop(object_cols, axis=1)\nhouse_df = pd.concat([house_df, OH_cols], axis=1)\n\nOH_cols = pd.DataFrame(OH_encoder.transform(house_test[object_cols]))\nOH_cols.index = house_test.index\nOH_cols.columns = OH_encoder.get_feature_names(object_cols)\nhouse_test = house_test.drop(object_cols, axis=1)\nhouse_test = pd.concat([house_test, OH_cols], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:02:09.702756Z","iopub.execute_input":"2022-04-11T13:02:09.703567Z","iopub.status.idle":"2022-04-11T13:02:09.755637Z","shell.execute_reply.started":"2022-04-11T13:02:09.703521Z","shell.execute_reply":"2022-04-11T13:02:09.754315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature Transformation (Skewed Features)\n\nWe should also take care of the skewness of the features in our dataset. We use skew() from the scipy.stats module to identify which columns are skewed.\n\nAny skewness greater than 0.5 is actually considered slightly skewed hence we will perform log-transformation for any values greather than that.","metadata":{}},{"cell_type":"code","source":"skewed = house_df[house_df.columns[~house_df.columns.isin(list(OH_cols.columns) + modified_cols +\n        object_cols)]].apply(lambda x: skew(x.dropna().astype(float)))\nskewed = skewed[skewed > 0.5]\nskewed = skewed.index\n\nhouse_df[skewed] = np.log1p(house_df[skewed])\nhouse_test[skewed] = np.log1p(house_test[skewed])","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:02:12.573263Z","iopub.execute_input":"2022-04-11T13:02:12.573567Z","iopub.status.idle":"2022-04-11T13:02:12.624703Z","shell.execute_reply.started":"2022-04-11T13:02:12.573536Z","shell.execute_reply":"2022-04-11T13:02:12.623455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature Scaling\n\nWhile log-transformation took care of the skewness in the features, we also need to scale the features to a standardize the range.\n\nOf the many scaling choices such as MinMaxScaler, StandardScaler, we picked RobustScaler.\n\nThe reasoning behind this is because we have seen that our data seems to be quite skewed and it will tend to have more outliers than a normal dataset. Using a RobustScaler can deal with that easily as it uses statistics that are insensitive to outliers to scale the data.\n\nA robust scaler minuses the median and divides it by the interquatile range. Both of which are not affected by the outliers.","metadata":{}},{"cell_type":"code","source":"for col in house_df[house_df.columns]:\n    if col not in (list(OH_cols.columns) + modified_cols + object_cols):\n        scaler = RobustScaler()\n        house_df[col] = scaler.fit_transform(house_df[[col]])\n        house_test[col] = scaler.transform(house_test[[col]])","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:02:15.453544Z","iopub.execute_input":"2022-04-11T13:02:15.453923Z","iopub.status.idle":"2022-04-11T13:02:15.715845Z","shell.execute_reply.started":"2022-04-11T13:02:15.453886Z","shell.execute_reply":"2022-04-11T13:02:15.715096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature Selection\n\nFeature selection is a simple way to reduce redundant and irrelevant data from our dataset and some of them contribute close to nothing.\nRemoving the irrelevant data actually improves learning accuracy and greatly reduces the computation time.\n\nBy removing redundant data, we can reduce the chance of our model overfitting to the data too.\n\nThere are some ways to perform features selection and some of which we surely studied before such as the Pearsons Correlation and Analysis of Variance (ANOVA). In this notebook, we will utilize the mutual info regression to estimate the dependency of the variables with our target variable.\n\nMutual information is a non-negative value and it shows the dependency between the variables. Meaning a mutual information of 0 will be saying that both of the features are completely independent. Hence, it is a safe bet for us to remove them off. Note the other name of mutual information is information gain (you may have heard it before).\n\nMutual information measures the amount of information one can obtain from one random variable given another. Source : Data Mining: Practical Machine Learning Tools and Techniques, 4th edition, 2016.","metadata":{}},{"cell_type":"code","source":"data = pd.merge(left = house_df, right = y_house , left_index= True, right_index = True)\nmi = mutual_info_regression(X = data.drop('SalePrice', axis = 1), y = data['SalePrice'])\nmi_df = pd.DataFrame(list(zip(data.columns,mi)), columns =['Feature','Mutual Info'])\nmi_df = mi_df.sort_values('Mutual Info',ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:02:23.297093Z","iopub.execute_input":"2022-04-11T13:02:23.297932Z","iopub.status.idle":"2022-04-11T13:02:24.778616Z","shell.execute_reply.started":"2022-04-11T13:02:23.297883Z","shell.execute_reply":"2022-04-11T13:02:24.777699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"low_mi_df = mi_df[abs(mi_df['Mutual Info']) == 0]\nfilter_feature = sorted(list(low_mi_df['Feature']))\nprint(\"Number of low correlated features dropped: \" + str(len(filter_feature)))\nhouse_df = house_df.drop(filter_feature,axis=1)\nhouse_test = house_test.drop(filter_feature,axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:03:03.307917Z","iopub.execute_input":"2022-04-11T13:03:03.308961Z","iopub.status.idle":"2022-04-11T13:03:03.321863Z","shell.execute_reply.started":"2022-04-11T13:03:03.308915Z","shell.execute_reply":"2022-04-11T13:03:03.32117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Polynomial and Interaction Features\n\nAnother part of feature creation ! In this part, we create new polynomial and interaction features from the high mutual information features to derive new combinations that might be useful to our model later on.\n\nPolynomial features can allow our linear models to grasp on the non-linearity of the features and we can also see if there is some new interesting relationships between the features themselves by introducing interaction features.\n\nWe can actually generate polynomial and interaction features from all of our features (quite large) and further cherry pick the good features. There may be hidden interesting relationship to be uncovered there but I am quite satisfied with only using the highly depended features.\n\nTo read more about interaction features: https://stattrek.com/multiple-regression/interaction.aspx","metadata":{}},{"cell_type":"code","source":"top_mi_list = list(mi_df.head(20)['Feature'])\ntop_mi_subset = house_df[top_mi_list]\nindex_copy = top_mi_subset.index\n\npoly = PolynomialFeatures(2, interaction_only=True)\npoly_features = pd.DataFrame(poly.fit_transform(top_mi_subset),\n                             columns=poly.get_feature_names_out(top_mi_list))\npoly_features = poly_features.iloc[:,len(top_mi_list) + 1:]\npoly_features.set_index(index_copy, inplace = True)\npoly_and_price = pd.concat([y_house,poly_features],axis=1).dropna()\ntop_20_poly = abs(poly_and_price.corr()['SalePrice']).sort_values(ascending=False)[1:21]","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:08:48.991423Z","iopub.execute_input":"2022-04-11T13:08:48.991759Z","iopub.status.idle":"2022-04-11T13:08:49.173762Z","shell.execute_reply.started":"2022-04-11T13:08:48.991728Z","shell.execute_reply":"2022-04-11T13:08:49.172732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"house_df = pd.concat([house_df,poly_features[top_20_poly.index]],axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:09:21.179198Z","iopub.execute_input":"2022-04-11T13:09:21.179557Z","iopub.status.idle":"2022-04-11T13:09:21.19543Z","shell.execute_reply.started":"2022-04-11T13:09:21.179523Z","shell.execute_reply":"2022-04-11T13:09:21.19406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_mi_subset = house_test[top_mi_list]\nindex_copy = top_mi_subset.index\npoly_features = pd.DataFrame(poly.transform(top_mi_subset),\n                             columns=poly.get_feature_names_out(top_mi_list))\npoly_features = poly_features.iloc[:,len(top_mi_list) + 1:]\npoly_features.set_index(index_copy, inplace = True)\nhouse_test = pd.concat([house_test,poly_features[top_20_poly.index]],axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:10:27.805554Z","iopub.execute_input":"2022-04-11T13:10:27.806675Z","iopub.status.idle":"2022-04-11T13:10:27.833503Z","shell.execute_reply.started":"2022-04-11T13:10:27.806616Z","shell.execute_reply":"2022-04-11T13:10:27.832118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_20_poly.index","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:10:31.727246Z","iopub.execute_input":"2022-04-11T13:10:31.72756Z","iopub.status.idle":"2022-04-11T13:10:31.735029Z","shell.execute_reply.started":"2022-04-11T13:10:31.72753Z","shell.execute_reply":"2022-04-11T13:10:31.734346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Outlier Identification\n\nOutliers, the one thing that statistic text books like to assume they are normal.\n\nToo bad they are usually not. A bad outlier case actually increases the variance in our model and further reduces the power of our model to grasp onto the data. Outliers cause regression model (especially linear ones) to learn a skewed understanding towards the outlier.\n\nIsolation Forest much like its' name, works to isolation a tree in a huge forest. It works by randomly sampling data based on randomly selected features and potray them in a binary decision tree structure. For an outlier, there are actually less splits needed in the forest to isolate them. Conversely, a datapoint that is not an outlier will require a lot more splits to be isloted. ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\niso_forest = IsolationForest(random_state=0)\nhouse_df_without_outlier = pd.Series(iso_forest.fit_predict(house_df), index = data.index)\nhouse_df = house_df.loc[house_df_without_outlier.index[house_df_without_outlier == 1],:]","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:13:20.353126Z","iopub.execute_input":"2022-04-11T13:13:20.353459Z","iopub.status.idle":"2022-04-11T13:13:20.860715Z","shell.execute_reply.started":"2022-04-11T13:13:20.353427Z","shell.execute_reply":"2022-04-11T13:13:20.859669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another way to categorize outliers is by using standardized residuals from linear models. Standardized residuals is can easily identify an abnormal residuals as they are standardized and we can observe the residuals in standard deviation units. Anything larger than 3 standard deviations are usually considered outliers.","metadata":{}},{"cell_type":"code","source":"data = pd.merge(left = house_df, right = y_house , left_index= True, right_index = True)\nlinear = LinearRegression()\nY = data['SalePrice']\nlinear.fit(data.drop(['SalePrice'],axis=1), Y)\nY_hat = linear.predict(data.drop(['SalePrice'],axis=1))\nresiduals = Y - Y_hat\ny_vs_yhat_df = pd.DataFrame(zip(Y.values,Y_hat,residuals),\n                            columns=['y','yhat','residuals'],index=data.index)\n\nr2 = r2_score(Y, Y_hat)\nprint(\"About \" + str(round(r2 * 100,2)) + \n      \"% of variation in the Sale Price can be explained by the model.\")\n\nsns.scatterplot(Y, Y_hat)\nsns.lineplot(np.linspace(10.5,13.5),np.linspace(10.5,13.5), color='black', linewidth=2.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:15:28.723603Z","iopub.execute_input":"2022-04-11T13:15:28.724332Z","iopub.status.idle":"2022-04-11T13:15:29.185376Z","shell.execute_reply.started":"2022-04-11T13:15:28.724289Z","shell.execute_reply":"2022-04-11T13:15:29.184514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"standard_residuals = (residuals - residuals.mean()) / residuals.std()\noutliers = data[abs(standard_residuals) > 3]\ny_vs_yhat_df.loc[y_vs_yhat_df.index.isin(outliers.index),'Outlier'] = 1\ny_vs_yhat_df.loc[y_vs_yhat_df['Outlier'] != 1 ,'Outlier'] = 0","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:16:03.60178Z","iopub.execute_input":"2022-04-11T13:16:03.602307Z","iopub.status.idle":"2022-04-11T13:16:03.611841Z","shell.execute_reply.started":"2022-04-11T13:16:03.602253Z","shell.execute_reply":"2022-04-11T13:16:03.611106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(data = y_vs_yhat_df, x='y', y='yhat',hue ='Outlier', palette = ['blue','red'])\nsns.lineplot(np.linspace(10.5,13.5),np.linspace(10.5,13.5), color='black', linewidth=2.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:16:25.592562Z","iopub.execute_input":"2022-04-11T13:16:25.593174Z","iopub.status.idle":"2022-04-11T13:16:25.945881Z","shell.execute_reply.started":"2022-04-11T13:16:25.593114Z","shell.execute_reply":"2022-04-11T13:16:25.944777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"house_df = house_df.loc[y_vs_yhat_df[y_vs_yhat_df['Outlier'] == 0].index,:]","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:16:45.311719Z","iopub.execute_input":"2022-04-11T13:16:45.312075Z","iopub.status.idle":"2022-04-11T13:16:45.322305Z","shell.execute_reply.started":"2022-04-11T13:16:45.312017Z","shell.execute_reply":"2022-04-11T13:16:45.321191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"house_df = house_df.drop(list(house_test.columns[house_test.nunique()== 1 ]),axis=1)\nhouse_test = house_test.drop(list(house_test.columns[house_test.nunique()== 1]),axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:17:39.13736Z","iopub.execute_input":"2022-04-11T13:17:39.137793Z","iopub.status.idle":"2022-04-11T13:17:39.187151Z","shell.execute_reply.started":"2022-04-11T13:17:39.137753Z","shell.execute_reply":"2022-04-11T13:17:39.18613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Modelling\n\nFor this part, we will be using Ridge, XGB, Catboost, SVR, Huber and a Stacked regression.\n\nThe performance of the models will later be averaged out (ensemble model) and we will also implement a stacked regressor at the same time.\n\nStacked regressor is a type of Level 1 ensemble model that generalizes the predictions made by different models to get the final output. ","metadata":{}},{"cell_type":"code","source":"data = pd.merge(left = house_df, right = y_house , left_index= True, right_index = True)\ntrain_y = data['SalePrice']\ntrain_X = data.drop(['SalePrice'],axis=1)\n\ndev_train, dev_test = train_test_split(data, test_size=0.2 ,shuffle=True)\ndev_train_y = dev_train['SalePrice']\ndev_train_X = dev_train.drop(['SalePrice'],axis=1)\ndev_test_y = dev_test['SalePrice']\ndev_test_X = dev_test.drop(['SalePrice'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:23:48.542789Z","iopub.execute_input":"2022-04-11T13:23:48.543166Z","iopub.status.idle":"2022-04-11T13:23:48.573796Z","shell.execute_reply.started":"2022-04-11T13:23:48.543126Z","shell.execute_reply":"2022-04-11T13:23:48.573064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ridgemodel = Ridge(alpha=26)\n\nxgbmodel = XGBRegressor(alpha= 3, colsample_bytree=0.5, reg_lambda=3, learning_rate= 0.01,\\\n           max_depth=3, n_estimators=10000, subsample=0.65)\n\nsvrmodel = SVR(C=8, epsilon=0.00005, gamma=0.0008)\n\nhubermodel = HuberRegressor(alpha=30,epsilon=3,fit_intercept=True,max_iter=2000)\n\ncbmodel = cb.CatBoostRegressor(loss_function='RMSE',colsample_bylevel=0.3, depth=2, \\\n          l2_leaf_reg=20, learning_rate=0.005, n_estimators=15000, subsample=0.3,verbose=False)\n\nstackmodel = StackingCVRegressor(regressors=(ridgemodel, xgbmodel, svrmodel, hubermodel, cbmodel),\n             meta_regressor=cbmodel, use_features_in_secondary=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:26:09.946135Z","iopub.execute_input":"2022-04-11T13:26:09.946492Z","iopub.status.idle":"2022-04-11T13:26:09.961107Z","shell.execute_reply.started":"2022-04-11T13:26:09.946459Z","shell.execute_reply":"2022-04-11T13:26:09.959832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will fit the models onto development train and test data sets first to have a quick overview of the model performances.","metadata":{}},{"cell_type":"code","source":"start = time.time()\nprint(\"Recording Modelling Time\")\nfor i in [ridgemodel,hubermodel,cbmodel,svrmodel,xgbmodel,stackmodel]:\n    i.fit(train_X,train_y)\n    if i == stackmodel:\n        i.fit(np.array(dev_train_X), np.array(dev_train_y))\nend = time.time()\nprint(\"Time Elapsed: \" + str(round((end - start)/60,0)) +\"minutes.\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:29:51.22198Z","iopub.execute_input":"2022-04-11T13:29:51.222349Z","iopub.status.idle":"2022-04-11T13:42:48.283033Z","shell.execute_reply.started":"2022-04-11T13:29:51.222316Z","shell.execute_reply":"2022-04-11T13:42:48.279674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"-----------------------------\")\nprint(\"Overview of model performance\")\nprint(\"-----------------------------\")\nfor i in [ridgemodel,hubermodel,cbmodel,svrmodel,xgbmodel,stackmodel]:\n    print(\"\\n\")\n    print(i)\n    print(\"RMSLE of Development train set: \")\n    print(mean_squared_error(dev_train_y,i.predict(dev_train_X), squared=False))\n    print(\"RMSLE of Development test set: \")\n    print(mean_squared_error(dev_test_y,i.predict(dev_test_X), squared=False))\n    print(\"\\n\")\nprint(\"-----------------------------\")\nprint(\"RMSLE of Development train set using ensemble model: \")\nfit = (svrmodel.predict(train_X) + xgbmodel.predict(train_X) +   stackmodel.predict(train_X) + ridgemodel.predict(train_X) + hubermodel.predict(train_X) + cbmodel.predict(train_X)) / 6\nprint(mean_squared_error(train_y,fit, squared=False))\nprint(\"-----------------------------\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:45:21.302211Z","iopub.execute_input":"2022-04-11T13:45:21.302555Z","iopub.status.idle":"2022-04-11T13:45:24.04671Z","shell.execute_reply.started":"2022-04-11T13:45:21.302523Z","shell.execute_reply":"2022-04-11T13:45:24.045435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This time we fit the models with all the data.","metadata":{}},{"cell_type":"code","source":"start = time.time()\nprint(\"Recording Modelling Time\")\nfor i in [ridgemodel,hubermodel,cbmodel,svrmodel,xgbmodel,stackmodel]:\n    i.fit(train_X,train_y)\n    if i == stackmodel:\n        i.fit(np.array(train_X), np.array(train_y))\nend = time.time()\nprint(\"Time Elapsed: \" + str(round((end - start)/60,0)) +\"minutes.\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:46:04.773352Z","iopub.execute_input":"2022-04-11T13:46:04.773661Z","iopub.status.idle":"2022-04-11T13:59:26.289732Z","shell.execute_reply.started":"2022-04-11T13:46:04.773631Z","shell.execute_reply":"2022-04-11T13:59:26.288743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"References\n\nhttps://www.kaggle.com/code/limyenwee/stacked-ensemble-models-top-3-on-leaderboard by **YEN WEE LIM**","metadata":{}}]}