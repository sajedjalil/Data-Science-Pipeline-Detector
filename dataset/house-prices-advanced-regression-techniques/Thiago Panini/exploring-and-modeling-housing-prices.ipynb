{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"top\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Content</h3>\n\n* [1. Reading the Data](#1)\n* [2. EDA: Exploring Insights](#2)\n    - [2.1 An Overview from the Data](#2.1)\n    - [2.2 Studying Space and Time Variables](#2.2)\n    - [2.3 House Building Variabels](#2.3)\n    - [2.4 Analysis of Location Features](#2.4)\n* [3. Prep: Building Pipelines](#3) \n    - [3.1 Initial Pipeline](#3.1)\n        - [3.1.1 Initial Drop](#3.1.1)\n        - [3.1.2 Categorical Grouping](#3.1.2)\n        - [3.1.3 Duplicated Data](#3.1.3)\n        - [3.1.4 Target Log Transformation](#3.1.4)\n        - [3.1.4 Training and Validation Data](#3.1.5)\n        - [3.1.5 Building an Initial Pipeline](#3.1.6)\n    - [3.2 Numerical Pipeline](#3.2)\n        - [3.2.1 Null Data](#3.2.1)\n        - [3.2.2 Log Distribution](#3.2.2)\n        - [3.2.3 Scaling](#3.2.3)\n        - [3.2.4 Building a Numerical Pipeline](#3.2.4)\n    - [3.3 Categorical Pipeline](#3.3)\n        - [3.3.1 Dummies Encoding](#3.3.1)\n        - [3.3.2 Building a Categorical Pipeline](#3.3.2)\n    - [3.4 Complete Pipelines](#3.4)\n* [4. Modeling: Training Predictive Models](#4) \n    - [4.1 Structuring Variables](#4.1)\n    - [4.2 Training Models](#4.2)\n    - [4.3 Evaluating Performance](#4.3)\n    - [4.4 Correlation Matrix](#4.4)\n    - [4.5 Feature Importance](#4.5)\n    - [4.6 Learning Curve](#4.6)\n    - [4.7 Feature Selection](#4.7)\n* [5. Predicting: Predicting House Prices](#5)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"}},{"cell_type":"markdown","source":"This notebook aims to allocate the development related to the exploratory analysis of insights related to the dataset [House Prices - Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) taken from Kaggle platform for improving skills in Data Science and Machine Learning. Also, this notebook uses the tools presented on [xplotter](https://github.com/ThiagoPanini/xplotter) and [mlcomposer](https://github.com/ThiagoPanini/mlcomposer) python packages made by myself and published on PyPI repository. This is a real good effort for coding useful functions for making the Exploratory Data Analysis and applying Machine Learning process a lot more easier for Data Scientists and Data Analysis through deliverying charts customization and matplotlib/seaborn plots with a little few lines of code. I really hope you all enjoy it!\n\n<div align=\"center\">\n    <img src=\"https://i.imgur.com/5XFP1Ha.png\" height=300 width=200 alt=\"xplotter Logo\">\n    <img src=\"https://i.imgur.com/MIcPH8g.png\" width=400 height=400 alt=\"mlcomposer logo\">\n</div>\n\n___\n**_Description and context:_**\n_Ask a home buyer to describe the home of their dreams and he probably won't start the description with \"basement ceiling height\" or \"proximity to an east-west railroad\". However, the data set of this competition proves that there are influences in the negotiation of houses in addition to the number of bedrooms or bathrooms. With approximately 80 explanatory variables describing virtually any residential aspect of homes in Ames, Iowa, this competition challenges the user to predict the final price of homes._\n\nAt the same time, this notebook will be essential for the validation of new developments linked to the package I called [xplotter](https://github.com/ThiagoPanini/xplotter), a homemade library developed in order to facilitate all steps related to data analysis by exploration involving graphic plots.","metadata":{}},{"cell_type":"code","source":"!pip install xplotter --upgrade\n!pip install mlcomposer --upgrade","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Project libraries\nimport pandas as pd\nimport os\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n# Project variables\nDATA_PATH = '../input/house-prices-advanced-regression-techniques/'\nTRAIN_FILENAME = 'train.csv'\nTEST_FILENAME = 'test.csv'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>1. Reading the Data</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"After importing the main libraries common to the project and also defining important variables for reading the data, it is possible to make the first contact with the database available for the development of the task.","metadata":{}},{"cell_type":"code","source":"# Reading training data\ndf = pd.read_csv(os.path.join(DATA_PATH, TRAIN_FILENAME))\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the reading of the data, it is possible to consult the [metadata] page (https://www.kaggle.com/c/house-prices-advanced-regression-techniques) to extract the meaning of each of the 80 columns available in base:\n\n* **_1stFlrSF_**: Total area of ​​the first floor of the house\n* **_2ndFlrSF_**: Total area of ​​the second floor of the house\n* **_3SsnPorch_**: Varande area of ​​three seasons (?)\n* **_Alley_**: Characteristic of the alley that gives access to housing\n* **_BedroomAbvGr_**: Number of beds in the house (above the basement)\n* **_BldgType_**: Type of housing\n* **_BsmtCond_**: Classifies the basement's general condition\n* **_BsmtExposure_**: Exposition of the basement of the dwelling\n* **_BsmtFinSF1_**: Area covered by type 1 finish (BsmtFinType1 attribute)\n* **_BsmtFinSF2_**: Area covered by type 2 finish (BsmtFinType2 attribute)\n* **_BsmtFinType1_**: Score of the finish of the basement-related region\n* **_BsmtFinType2_**: Score of the finish of the basement-related region (if more than one exists)\n* **_BsmtFullBath_**: Full bathrooms in the basement-related area\n* **_BsmtHalfBath_**: Incomplete bathrooms (half) of the basement-related area\n* **_BsmtQual_**: Classifies the house according to the size of the basement\n* **_BsmtUnfSF_**: Basement area without finishing\n* **_CentralAir_**: Defines whether or not there is a central air conditioner (Boolean attribute)\n* **_Condition1_**: Proximity to important points in the city\n* **_Condition2_**: Proximity to important points in the city (if there are more than one)\n* **_Electrical_**: Type of home electrical system\n* **_EnclosedPorch_**: Closed balcony area in the house\n* **_ExterCond_**: Condition of the external material on the observation date\n* **_Exterior1st_**: External coverage of the house\n* **_Exterior2nd_**: External roof of the house (if there is more than one roof)\n* **_ExterQual_**: Quality of the material used abroad\n* **_Fence_**: Quality of the enclosure present in the house\n* **_FireplaceQu_**: Quality of fireplaces\n* **_Fireplaces_**: Number of fireplaces in the house\n* **_Foundation_**: Type of foundation used in construction\n* **_FullBath_**: Number of full bathrooms in the house (above the basement)\n* **_Functional_**: Describes features of the house under warranty\n* **_GarageArea_**: Garage area in square meters\n* **_GarageCars_**: Size of the garage related to the number of possible cars\n* **_GarageCond_**: Score that defines the conditions of the garage\n* **_GarageFinish_**: Internal garage finish\n* **_GarageQual_**: Quality of the garage\n* **_GarageType_**: Type of garage in the house\n* **_GarageYrBlt_**: Year of construction of the garage\n* **_GrLivArea_**: Total living room area\n* **_HalfBath_**: Number of incomplete bathrooms (half) in the house (above the basement)\n* **_Heating_**: Type of house heating\n* **_HeatingQC_**: Heating quality\n* **_HouseStyle_**: Housing style\n* **_KitchenAbvGr_**: Number of kitchens in the house (above the basement)\n* **_KitchenQual_**: Quality of the kitchens\n* **_LandContour_**: Housing leveling\n* **_LandSlope_**: Property slope\n* **_LotArea_**: Allotment area\n* **_LotConfig_**: Allotment configuration\n* **_LotFrontage_**: Dimension of the front perimeter of the house\n* **_LotShape_**: General housing format\n* **_LowQualFinSF_**: Total area of ​​low quality finishes throughout the house\n* **_MasVnrArea_**: Area covered by masonry\n* **_MasVnrType_**: Type of masonry used\n* **_MiscFeature_**: Some features not included in other categories\n* **_MiscVal_**: Value of features not included in quantity criteria\n* **_MoSold_**: Month in which the sale of the house was made\n* **_MSSubClass_**: Identifies the type of residence\n* **_MSZoning_**: Classifies the property by zone\n* **_Neighborhood_**: Locality related to city boundaries\n* **_variable_name_**: description\n* **_OpenPorchSF_**: Open balcony area in the house\n* **_OverallCond_**: Score of the general condition of the house\n* **_OverallQual_**: Score of the material and finish of the house\n* **_PavedDrive_**: Attribute that defines the paving of the street (inside the house)\n* **_PoolArea_**: Pool area in the house\n* **_PoolQC_**: Quality of the pool\n* **_RoodMatl_**: Material used in the roof (roof)\n* **_RoofStyle_**: Type of roof of the house (roof)\n* **_SaleCondition_**: Conditions of sale\n* **_SaleType_**: Type of sale\n* **_ScreenPorch_**: Screen area on the balcony of the house\n* **_Street_**: Characteristic of the street that gives access to housing\n* **_TotalBsmtSF_**: Total hold area\n* **_TotRmsAbvGrd_**: Total number of rooms in the house (above the basement)\n* **_Utilities_**: Utilities\n* **_WoodDeckSF_**: Wooden deck area present in the house\n* **_YearBuilt_**: Year of construction of the house\n* **_YearRemodAdd_**: Year of remodeling of the house (same as YearBuilt if the house has not been remodeled)\n* **_YrSold_**: Year in which the sale of the house was made","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>2. EDA: Exploring Insights</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"At this point, there is a well-defined context of the project's objective, in addition to a database already read and transformed into a DataFrame format of the pandas. From this moment on, a true scan of the data will be proposed for the application of a detailed descriptive analysis in order to gather relevant insights for the business context.\n\nUsing the homemade package [xplotter](https://github.com/ThiagoPanini/xplotter), whose construction was motivated exactly to facilitate the work of data scientists in the pillars of insights and exploratory data analysis. The next steps will be based on the tools provided from xplotter library to make beautiful charts in order to get a deep understand of our data. ","metadata":{}},{"cell_type":"markdown","source":"<div align='center'>\n    <img src=\"https://i.imgur.com/5XFP1Ha.png\" height=300 width=200 alt=\"xplotter Logo\">\n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.1\"></a>\n<font color=\"dimgrey\" size=+2.0><b>2.1 An Overview from the Data</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"This project has a particularity: the presence of a large number of variables available for analysis. Thus, so that the project is not extremely extensive from the individual analysis of each of the variables, it is important to provide an overview of the attributes present in the data set with some relevant points that can assist future analysis decisions.\n\nFor that, the `data_overview()` function will be used, which, in turn, performs a series of useful analyzes on the base as a whole, finally returning relevant characteristics for each of the present attributes.","metadata":{}},{"cell_type":"code","source":"# Importing libraries and defining target tariable\nfrom xplotter.insights import *\n\nTARGET = 'SalePrice'\n\ndf_overview = data_overview(df=df, corr=True, target=TARGET)\nprint(f'Some of the features and its metadata')\ndf_overview.head(25)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When executing the `data_overview()` function, it is possible to return, for each of the attributes, the following information:\n\n* **qtd_null**: amount of null data present in the respective column;\n* **pct_null**: percentage of null data in the respective column;\n* **dtype**: primitive type related to the column;\n* **qtd_cat**: number of different categories (in the case of categorical variables);\n* **target_pearson_corr**: correlation with the target variable.\n\nThus, it is possible to notice some interesting particularities:\n\n* The `PoolQC` variable has 99.5% of null entries in the database. In practice, this variable indicates the quality of the pool present in the property and, null data probably indicates that the property does not have a pool;\n* The `MiscFeature`,` Alley` and `Fence` variables also have a high amount of null data. In practice, as in `PoolQC`, these variables describe specific and particular characteristics of the properties, which, in fact, may not be present in the vast majority of houses;\n* Looking at the list ordered by amount of null data, it is possible to notice that the variables `GarageYrBlt`,` MasVnrArea` and `Fireplaces` have high correlations with the target variable (` SalePrice`) and this may be a good investigative factor in the future","metadata":{}},{"cell_type":"code","source":"# Top features by categorical entries\ndf_overview.sort_values(by='qtd_cat', ascending=False).head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Top features by positive correlation with target\ndf_overview.sort_values(by='target_pearson_corr', ascending=False).head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The two sets above show the main variables by:\n1. Number of categorical entries\n2. Greater positive correlation with the target variable\n\nThese two scenarios allow analyzing in greater depth the possible steps to be considered in the Prep stage of the database. For example, it is necessary to consider that the variable `Neighborhood`, when receiving the procedure of` encoding`, will generate 26 additional columns in the final base.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.2\"></a>\n<font color=\"dimgrey\" size=+2.0><b>2.2 Studying Space and Time Variables</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"After an initial analysis of the variables present in the database as a whole, it is important to define the next steps for data exploration in view of the large number of features present. Thus, the strategy adopted is to categorize the base columns in three different contexts:\n\n* **_Space and Environment Variables_**: analyzes related to variables that describe the situation of the property in terms of space and environment;\n* **_Construction Variables_**: insights taken from variables that indicate construction characteristics of the property;\n* **_Location Variables_**: graphical analysis on variables that bring information related to the location of the property.\n\nIn this session, the variables related to \"Space and Environment\" will be analyzed, creating specific sets of features and making graphical plots in order to verify the statistical characteristics of the features and their respective relationships with the target variable.","metadata":{}},{"cell_type":"code","source":"# Splitting housing space features\nspace_cols = ['1stFlrSF', '2ndFlrSF', '3SsnPorch', 'BedroomAbvGr', 'BsmtFullBath', 'BsmtHalfBath', \n              'BsmtQual', 'EnclosedPorch', 'Fireplaces', 'FullBath', 'GarageArea', 'GarageCars', \n              'GarageType', 'GrLivArea', 'HalfBath', 'KitchenAbvGr', 'LotArea', 'LotFrontage', \n              'LowQualFinSF', 'MoSold', 'OpenPorchSF', 'PoolArea', 'SaleCondition', 'SaleType',\n              'ScreenPorch', 'TotalBsmtSF', 'TotRmsAbvGrd', 'WoodDeckSF', 'YrSold']\n\n# Numerical features with a continuous approach\nnum_space_cols = ['LotFrontage', 'LotArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n                  'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n                  'ScreenPorch', 'PoolArea']\n\n# Numerical feateures with a discreet approach\ncat_space_cols = ['BsmtQual', 'GarageType', 'SaleType', 'SaleCondition', 'BsmtFullBath', 'BsmtHalfBath',\n                  'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', \n                  'GarageCars', 'MoSold', 'YrSold']\n\n# Plottinghousing space features distribution\nplot_multiple_distplots(df=df, col_list=num_space_cols, kind='boxen')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plotting figure above, generated by the `plot_multiple_distplot ()` function of the `pycomp.viz.insights` module, consolidates distribution graphs for a specific set of continuous variables within the context of\" Space and Environment \"of the properties. In it, it is possible to visualize how some features behave within the proposed dataset and, among the possible conclusions, it is possible to score:\n\n* The variables `LotFrontage`,` TotalBsmtSF`, `1stFlrSF` and` GrLivArea` have distributions that resemble the normal Gaussian distribution, thus allowing the removal of clear visual conclusions from specific statistical parameters (such as the mean, standard deviation, among others) );\n* Some other variables, such as `LowQualFnSF`,` EnclosedPorch`, `ScreenPorch` and` PoolArea` have distributions similar to discrete variables, containing high peaks at 0 (or close to 0).\n\nTo complement this analysis and add a visualization that allows analyzing the relationship of some variables with the target variable (`SalePrice`), it is possible to execute the function` plot_multiple_dist_scatterplot () `.","metadata":{}},{"cell_type":"code","source":"normal_dist_num_space_cols = ['LotFrontage', 'LotArea', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea']\nplot_multiple_dist_scatterplot(df=df, col_list=normal_dist_num_space_cols, y_col='SalePrice')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Through the analysis of the above plot, it is possible to notice some interesting correlations between the selected variables and the respective price of the registered property. In all observed cases, there is a positive correlation, indicating that the target variable and each of the variables selected above have a directly proportional relationship. In practice, these variables indicate information related to **area** of environments (lot, basement, land, garage, among others), thus allowing a direct and generic conclusion: \"the bigger the environment, the more expensive the property\".\n\nThis visualization is extremely important because, given the behavior observed above, it is possible to state that these variables can have a good influence during the training phase of a predictive model. Another way of visualizing this relationship is from a correlation matrix, which will be analyzed in the future, after the data preparation stage.","metadata":{}},{"cell_type":"code","source":"plot_multiple_countplots(df=df, col_list=cat_space_cols, orient='v')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plotting figure above, generated by the `plot_multiple_countplots()` function of the `xplotter.insights` module, provides a general overview of the volumes linked to each of the inputs of some environment and space variables classified as categorical. Despite the large number of visions linked, it is possible to extract some relevant insights, such as:\n\n* There is a greater historical trend in sales in months 5, 6 and 7 that can be seen in the volume plot for the variable `MoSold`;\n* Most of the properties present in the base have a 2-car garage, a fact that can be verified by plotting by `GarageCars`;\n* Most houses do not have a fireplace, however a relevant portion has at least 1 fireplace (analysis by `Fireplaces`.\n\nSimilar to the one performed above with the `plot_multiple_dist_scatterplot()` function, the view below aims to graphically analyze the data of some variables and relate them to a target variable, extracting statistical indicators such as mean, median, standard deviation, among others.","metadata":{}},{"cell_type":"code","source":"plot_cat_aggreg_report(df=df, cat_col='BsmtQual', value_col=TARGET, title3='Statistical Analysis', \n                       desc_text=f'A statistical approach for {TARGET} \\nusing the data available',\n                       stat_title_mean='Mean', stat_title_median='Median', stat_title_std='Std')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above, it is possible to view the result of executing the function `plot_cat_aggreg_report()`, which is built to provide a complete analysis involving a _category_ column and a _numeric column_, analyzing the volumes involved and the main statistical parameters (such as mean, median and deviation standard).\n\nIn this way, it is possible to view the results of the categorical variable `BsmtQual`, which, in turn, brings a discreet relationship of the quality of the basement, with the variable` SalePrice`, representing the real price of the property. With this, it is possible to score:\n\n1. Most houses have `TA` and` Gd` qualities for the cellars;\n2. Properties with “Ex” quality cellars have the highest average price;\n3. The average price of properties with `Ex` and` Gd` basements are higher than the general average of the base.","metadata":{}},{"cell_type":"code","source":"plot_cat_aggreg_report(df=df, cat_col='GarageCars', value_col=TARGET, title3='Statistical Analysis', \n                       desc_text=f'A statistical approach for {TARGET} \\nusing the data available',\n                       stat_title_mean='Mean', stat_title_median='Median', stat_title_std='Std', dist_kind='box')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above, we can see a relationship between `GarageCars` (space for cars in the garage) and` SalePrice`. The report shows us that, although most properties have space for 2 cars in the garage, the highest average price is associated with those that have space for 3 cars.","metadata":{}},{"cell_type":"code","source":"plot_cat_aggreg_report(df=df, cat_col='SaleCondition', value_col=TARGET, title3='Statistical Analysis', \n                       desc_text=f'A statistical approach for {TARGET} \\nusing the data available',\n                       stat_title_mean='Mean', stat_title_median='Median', stat_title_std='Std', dist_kind='strip')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In a combination of analysis, it is possible to check the report generated for the variables `SaleCondition` and` SalePrice`. In it, it can be seen that most of the houses were sold under the \"Normal\" condition, while the highest average of real estate prices came from sales under the \"Partial\" condition.","metadata":{}},{"cell_type":"code","source":"plot_cat_aggreg_report(df=df, cat_col='Fireplaces', value_col=TARGET, title3='Statistical Analysis', \n                       desc_text=f'A statistical approach for {TARGET} \\nusing the data available',\n                       stat_title_mean='Mean', stat_title_median='Median', stat_title_std='Std', dist_kind='boxen')","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, another relevant analysis placed above is the relationship between `Fireplaces` and the price of real estate. Here you can see a clear trend between the number of fireplaces present in the house and their respective price. Despite being a minority, buildings with 5 fireplaces have the highest average base price.","metadata":{}},{"cell_type":"markdown","source":"___\n\nIt would be relevant, within the proposal to explore this set of features, to visualize a list of real estate prices over time. In the base available for analysis and, more specifically there are two columns `MoSold` and` YrSold`. At this moment, we have the possibility to build a customized column concatenating the year (`YrSold`) and the month (` MoSold`) of sale of the property, thus generating the possibility to analyze the evolution of sales over time.","metadata":{}},{"cell_type":"code","source":"plot_evolutionplot(df=df, x='YrSold', y='SalePrice', agg_type='sum', date_col=False, x_rot=0, \n                   label_aggreg='M')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the `plot_evolutionplot()` function of the `xplotter` package, it was possible to plot, just above, an evolutionary historical view of the total sum of real estate values over the **years**. Thus, it is possible to perceive the variations relevant to each season, and it is possible to conclude that 2007 was probably the best year in terms of real estate values. Additionally, it is possible to notice that the year of 2010 probably does not have a completion closing, which explains the low value registered for that year.\n\nAmong the arguments of the `plot_evolutionplot()` function, it is possible to specify new breaks or new columns for evolutionary analysis to be plotted on the x axis. Thus, we will display, below, a graph of the sum of real estate prices per month.","metadata":{}},{"cell_type":"code","source":"plot_evolutionplot(df=df, x='MoSold', y='SalePrice', agg_type='sum', date_col=False, x_rot=0, label_aggreg='M')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using the base 'MoSold` column, it is possible to view a total sum of real estate prices for each of the months registered. In this way it is possible to conclude some important and relevant points for the business context to which we are inserted:\n\n1. There is a natural growth in property sales between months 5 and 7\n2. The first and last months of the year are not the best for the real estate industry","metadata":{}},{"cell_type":"code","source":"df_tmp = df.copy()\ndf_tmp['YrMoSold'] = (df_tmp['YrSold'] * 100 + df_tmp['MoSold'])\nplot_evolutionplot(df=df_tmp, x='YrMoSold', y='SalePrice', agg_type='sum', date_col=False, label_aggreg='M')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Concluding this set of evolutionary analyzes by total sum of prices, we have a totally historical analysis that unites the `YrSold` and` MoSold` columns into one: the customized `YrMoSold` column, which, in turn, brings a unique year reference and month of sale of the property. From the historical analysis in that column, it is possible to notice all fluctuations involving the entire base period. An interesting point to be mentioned are the peak sales in 2009 and 200606. Another interesting factor is the presence of a kind of \"standard\" of the market in relation to property sales: the characteristic curve seems to be repeated year by year in terms prices and sales.\n\nStill within the `plot_evolutionplot()` function, it is possible to enrich this historical analysis using the function's `hue` argument. With it, it is possible to select different breaks to check the historical line for each categorical entry. In the graph below, we will analyze an \"average\" of house prices by the variable `SaleCondition`","metadata":{}},{"cell_type":"code","source":"plot_evolutionplot(df=df_tmp, x='YrMoSold', y='SalePrice', hue='SaleCondition', agg_type='mean', \n                   label_data=False, style='SaleCondition', palette='plasma')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The graph above shows the evolution of property sales from the condition recorded for the given property. It is interesting to mention how the evolution of properties classified as `Partial` grew over the last months of registration. Another interesting point is the peak of sales of properties classified as `Abnorml` in 2007-07, followed by a sharp decrease in the following month.\n\nBelow, we will plot a similar view of evolution, but considering a break by `Fireplaces`. Let's see how fireplaces have influenced property sales over time.","metadata":{}},{"cell_type":"code","source":"plot_evolutionplot(df=df_tmp, x='YrMoSold', y='SalePrice', hue='Fireplaces', agg_type='mean', \n                   date_col=False, label_data=False, style='Fireplaces')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The chart above shows us an average of real estate prices by the number of fireplaces present. In it, it is possible to clearly see how houses that do not have a fireplace (red line) really have a lower average sales price. On the other side of the spectrum, houses with 2 fireplaces are those with the highest average price recorded. Sporadically, we have a few months of sales of properties with 3 fireplaces, always with a high price (analysis hampered by the low amount of properties of this type).","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.3\"></a>\n<font color=\"dimgrey\" size=+2.0><b>2.3 House Building Variabels</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"After a relevant analysis of the variables classified in the group \"Environment, Space and Time\" of the properties, we will propose an in-depth study of the features of the group \"Construction\" of the properties. In this session, we will investigate the main variables present in this group and their relevant relevance in relation to the price registered in the property.","metadata":{}},{"cell_type":"code","source":"# Splitting building features\nbuilding_cols = ['MSSubClass', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', \n                 'LandSlope', 'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', \n                 'YearRemodAdd', 'RoofStyle', 'RoodMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n                 'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtCond', 'BsmtExposure',\n                 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'Heating',\n                 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu',\n                 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC',\n                 'Fence', 'MiscFeature', 'MiscVal']\n\n# Building features with numerical meaning\nnum_building_cols = ['MasVnrArea', 'MiscVal', 'BsmtFinSF2', 'MSSubClass', 'BsmtUnfSF', 'BsmtFinSF1']\n\n# Building features with categorical meaning\ncat_building_cols = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageCond', 'GarageFinish',\n                  'GarageQual', 'BsmtFinType2', 'BsmtExposure', 'BsmtCond', 'BsmtFinType1', 'MasVnrType',\n                  'Electrical', 'Functional', 'KitchenQual', 'PavedDrive', 'HeatingQC', 'LandSlope',\n                  'HouseStyle', 'BldgType', 'LotConfig', 'Utilities', 'LandContour', 'LotShape',\n                  'Street', 'CentralAir', 'Heating', 'RoofStyle', 'Foundation', 'ExterCond', 'ExterQual',\n                  'Exterior2nd', 'Exterior1st', 'OverallQual', 'OverallCond', 'YearRemodAdd',\n                  'GarageYrBlt', 'YearBuilt']\n\n# Plotting distribution of building numerical features\nplot_multiple_distplots(df=df, col_list=num_building_cols, kind='strip')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The sequence of graphs above shows a distribution view for some variables classified as real estate \"Construction\" variables. In this group, it is possible to find few numerical variables, which are related to the _area_ of the property or to some indicator linked to _quality_ of the same.\n\nIn the view below, we will propose a broader view of the _construction_ variables through a categorical plot of volumetries.","metadata":{}},{"cell_type":"code","source":"# Categorical analysis\nplot_multiple_countplots(df=df, col_list=cat_building_cols)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the categorical view of multiple columns, it is possible to analyze exactly the context related to the construction variables. Many of the features in this group indicate some criteria of _quality_ of the property, from the finish of the basement to the outside. Some of the variables have an extremely large number of categories, for example, the year of construction of the garage and the type of the outer floor of the second floor.\n\nIt is important to mention that, during the preparation phases of the base for the training of a predictive model, it will be possible to apply `encoding` processes to treat categorical entries and separate them in columns. This will probably create an extremely high number of features in the base, a fact that can be dealt with from `feature selection` procedures embedded in pipelines","metadata":{}},{"cell_type":"code","source":"plot_multiple_dist_scatterplot(df=df, col_list=['BsmtUnfSF', 'BsmtFinSF1'], y_col='SalePrice')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The figure above shows a relationship of two variables related to the basement, which are:\n\n* **_BsmtUnfSF_**: Basement area without finishing;\n* **_BsmtFinSF1_**: Area covered by type 1 finish (BsmtFinType1 attribute)\n\nCrossing the distribution analysis with the target variable (`SalePrice`), it is possible to see the effect of the` BsmtFinSF1` feature on the general price of the property. In theory, this indicates that the larger the area of application of a particular finish (defined here simply as \"type 1\", the more expensive the home tends to be.","metadata":{}},{"cell_type":"code","source":"plot_cat_aggreg_report(df=df, cat_col='ExterQual', value_col=TARGET, dist_kind='boxen')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the plot above, an analysis is made on the external quality of the property as a way to simulate its respective price. Although we have a majority of properties registered with the qualities `Gd` (good) and` TA`, properties with external quality of the type `Ex` (excellent) have the highest average value.\n\nThis type of trend is repeated for the other categorical quality variables that, in a way, discretize the level of some sectors of the property (or the property itself), as are the cases of the `KitchenQual` and` GarageQual` features, plotted below .","metadata":{}},{"cell_type":"code","source":"plot_cat_aggreg_report(df=df, cat_col='KitchenQual', value_col=TARGET, dist_kind='strip')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_cat_aggreg_report(df=df, cat_col='GarageQual', value_col=TARGET, dist_kind='box')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.4\"></a>\n<font color=\"dimgrey\" size=+2.0><b>2.4 Analysis of Location Features</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Finally, ending the exploratory session of the database, we will analyze the last block of variables related to the location of the property. Composed of only 4 features, this is an extremely small group. Its variables basically indicate some issue related to the geographical area of the property.","metadata":{}},{"cell_type":"code","source":"location_cols = ['MSZoning', 'Neighborhood', 'Condition1', 'Condition2']\n\nplot_cat_aggreg_report(df=df, cat_col='MSZoning', value_col=TARGET, dist_kind='boxen')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus, ending the exploratory analysis, we have an aggregated and statistical relationship with the variable `MSZoning`, being possible to observe how this variable impacts on the final price of the property. Next, we will start studies related to the final preparation of the base.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>3. Prep: Building Pipelines</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"After a long journey in the exploratory analysis session, we have gathered valuable insights that can make modeling work much easier and more intuitive. Thus, the next topics in this notebook will deal with an extremely important step in the training of a machine learning model: the preparation of the database.\n\nUsing the homemade package mlcomposer, whose construction was motivated exactly to facilitate the work of data scientists in the pillars of insights, prep and modeling, for this second session is expected a full understanding of the set of available data and a clear idea of the steps required to be applied in the prep and in the modeling. To facilitate this work, a very powerful tool built within the `mlcomposer` package will be used. This is the `mlcomposer.transformers` module, which consists of ready-made classes capable of performing a series of activities and transformations in a database, in addition to a complete integration with` Pipelines` of prep data.\n\n<div align=\"center\">\n    <img src=\"https://i.imgur.com/MIcPH8g.png\" width=500 height=500 alt=\"mlcomposer logo\">\n</div>\n\n\nIn this step, features of the `mlcomposer.transformers` module will be used, which, in turn, contains ready-made classes and pre-developed transformers for use in data pipelines. In this way, it is possible to use components already programmed for the orchestration of preparation flows within the context of the project.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.1\"></a>\n<font color=\"dimgrey\" size=+2.0><b>3.1 Initial Pipeline</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"At first, it will be proposed to build an initial pipeline containing classes applied throughout the database. The purpose of this pipeline is to apply common transformations to the gross base received, in addition to facilitating the initial pruning of the features present in the base.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.1.1\"></a>\n<font color=\"dimgrey\" size=+1.0><b>3.1.1 Initial Drop</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"The first step to be taken in the data preparation phase involves the feature selection process. Not all columns present in the database are relevant to the training objective of the predictive model. In this step, a column filtering process will be performed from the `ColumnSelection()` class of the `mlcomposer.transformers` module.\n\nIn terms of objective, the `Condition2` and` RoofMatl` columns of the original base will initially be eliminated because, in a way, they are columns that bring an extremely uneven amount of volume in their categorical columns. The graph below shows a volumetric view of these two columns, thus exemplifying the reasons that made us decide for their exclusions.","metadata":{}},{"cell_type":"code","source":"# Looking at features to be dropped\nTO_DROP = ['Condition2', 'RoofMatl']\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(17, 5))\nplot_countplot(df=df, col='Condition2', ax=axs[0])\nplot_countplot(df=df, col='RoofMatl', ax=axs[1])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Through the graphs above, it can be seen that the columns `Condition2` and` RoofMatl` are practically composed of unique values, that is, more than 98% of their entries belong to a single categorical value. Because they are practically constant, these variables have no value for a possible predictive model and, therefore, can be eliminated from the initial basis.","metadata":{}},{"cell_type":"code","source":"# Importing modile class\nfrom mlcomposer.transformers import ColumnSelection\n\n# Creating a copy of original DataFrame\ndf_tmp = df.copy()\n\n# Initial parameters for feature selection\nTARGET = 'SalePrice'\nTO_DROP = ['Condition2', 'RoofMatl', 'Id']\nINITIAL_FEATURES = [col for col in df_tmp.columns if col not in TO_DROP]\n\n# Applying feature selection from class object\nselector = ColumnSelection(features=INITIAL_FEATURES)\ndf_slct = selector.fit_transform(df_tmp)\n\n# Results\nprint(f'Shape before feature selection: {df_tmp.shape}')\nprint(f'Shape after feature selection: {df_slct.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By executing the `fit_transform()` method of the `ColumnSelection()` class, it is possible to notice the elimination of the two previously defined columns.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.1.2\"></a>\n<font color=\"dimgrey\" size=+1.0><b>3.1.2 Categorical Grouping</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"A common fact to datasets with a large number of columns is the presence of categorical features with several different entries. It is known that categorical variables must, at some point in the preparation pipeline, go through an encoding process, which is responsible for the distribution of categorical entries in columns.\n\nIn a scenario of variables with many inputs, it is possible to find inconsistency of features after the application of `encoding`, especially in cases of variables with some extremely unrepresentative entrances (as in the example of the` Condition2` and `RoofMatl` variables analyzed above) .\n\nFor this, we will separate some categorical variables that fit this scenario of many different entries and apply the class `CategoricalLimitter()` of the package `mlcomposer` to group minorities.","metadata":{}},{"cell_type":"code","source":"# Importing module class\nfrom mlcomposer.transformers import CategoricalLimitter\nfrom sklearn.pipeline import Pipeline\n\n# Defining categorical features to be grouped\nN_CAT = 5\nHIGH_CAT_FEATURES = data_overview(df=df_tmp).query('qtd_cat > @N_CAT + 1')['feature'].values\nHIGH_CAT_FEATURES = [col for col in HIGH_CAT_FEATURES if col not in TO_DROP]\nCAT3_FEATURES = ['Functional', 'SaleType']\nCAT8_FEATURES = ['Neighborhood']\nCAT5_FEATURES = ['HouseStyle', 'Condition1', 'Exterior2nd', 'Exterior1st']\nOTHER_TAG = 'Other'\n\n# Applying categorical grouping\ncat3_agrup = CategoricalLimitter(features=CAT3_FEATURES, n_cat=3, other_tag=OTHER_TAG)\ncat5_agrup = CategoricalLimitter(features=CAT5_FEATURES, n_cat=5, other_tag=OTHER_TAG)\ncat8_agrup = CategoricalLimitter(features=CAT8_FEATURES, n_cat=8, other_tag=OTHER_TAG)\ncat_agrup_pipeline = Pipeline([\n    ('cat3_agrup', cat3_agrup),\n    ('cat5_agrup', cat5_agrup),\n    ('cat8_agrup', cat8_agrup)\n])\n\ndf_cat_agrup = cat_agrup_pipeline.fit_transform(df_slct)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To validate the successful execution of the categorical grouping class, we will analyze, through `countplots()`, the scenario of entries different from the set of variables defined by the object `HIGH_CAT_FEATURES` **before** and **after** of class application.","metadata":{}},{"cell_type":"code","source":"# Countplot for features with a high number of categories\nplot_multiple_countplots(df=df_tmp, col_list=HIGH_CAT_FEATURES)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After the application of the class, we have:","metadata":{}},{"cell_type":"code","source":"# Countplot after categorical grouping\nplot_multiple_countplots(df=df_cat_agrup, col_list=HIGH_CAT_FEATURES)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.1.3\"></a>\n<font color=\"dimgrey\" size=+1.0><b>3.1.3 Duplicated Data</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"The handling of duplicate data is an important step in preparing the basis for model training. This is because eliminating duplicates also means eliminating redundancy at the base, allowing machine learning models a faster convergence to local / global minimums.\n\nTo accomplish this task, the class DuplicatesDimplicates of the module pycomp.ml.transformers will be used, which, in turn, is responsible for simply eliminating duplicate records from a database passed as input.","metadata":{}},{"cell_type":"code","source":"# Importing class\nfrom mlcomposer.transformers import DropDuplicates\n\n# Applying transformer\ndup_dropper = DropDuplicates()\ndf_nodup = dup_dropper.fit_transform(df_cat_agrup)\n\n# Results\nprint(f'Total of duplicates before: {df_cat_agrup.duplicated().sum()}')\nprint(f'Total of duplicates after: {df_nodup.duplicated().sum()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.1.4\"></a>\n<font color=\"dimgrey\" size=+1.0><b>3.1.4 Target Log Transformation</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"In the proposed business problem, we need to predict property prices given their characteristics. In order for us to build a good predictive model in this regression task, it is possible to apply some transformation techniques so that the models can achieve better results. One of these techniques is the logarithmic transformation in the target variable and its advantages can be seen at [link](https://gdcoder.com/when-why-to-use-log-transformation-in-regression/)\n\nIn practice, applying the logarithm to the target variable of a database works well when that variable has an asymmetric distribution on the left, indicating the presence of possible outliers. If these outliers are important for the model, without the possibility of being eliminated, the logarithmic transformation helps predictive models (mainly those based on trees) to achieve better results. Next, we will investigate the current distribution of our target variable `SalePrice` and build a transformer to apply the logarithm, if applicable.","metadata":{}},{"cell_type":"code","source":"# SalePrice distribution\nplot_distplot(df=df, col=TARGET, hist=True, title=f'{TARGET} distribution plot')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems the case where the logarithmic transformation would be of great help for predictive models. The current distribution of `SalePrice` is asymmetric on the left and, thus, the transformation would make the values (mainly the outliers) more interpretable for the models. We will use the `LogTransformation` class to transform the database and compare the two scenarios","metadata":{}},{"cell_type":"code","source":"# Importing class\nfrom mlcomposer.transformers import LogTransformation\n\n# Comparing distributions\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(17, 7))\nplot_distplot(df=df, col=TARGET, hist=True, ax=axs[0], title=f'Original {TARGET} distribution')\n\n# Creating object and applying transformation\nlog_tr = LogTransformation(cols_to_log=TARGET)\ndf_target_log = log_tr.fit_transform(df)\n\n# Plotting distribution after transformation\nplot_distplot(df=df_target_log, col=TARGET, hist=True, color='mediumseagreen', ax=axs[1],\n              title=f'{TARGET} distribution after log transformation')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.1.5\"></a>\n<font color=\"dimgrey\" size=+1.0><b>3.1.5 Training and Validation Data</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Finishing what we could call the initial pipeline of the project, we have an important step responsible for separating the database in training and testing. Thinking about a future modeling step, evaluating the result on different bases is extremely important to make decisions regarding the best practical solution to be put into production.\n\nFor this, we will use the SplitDados class also from the pycomp.ml.transformers module, which, in turn, applies this separation in the base and returns us with properly separated training and validation data.","metadata":{}},{"cell_type":"code","source":"# Importing class\nfrom mlcomposer.transformers import DataSplitter\n\n# Applying transformer\nsplitter = DataSplitter(target=TARGET)\nX_train, X_test, y_train, y_test = splitter.fit_transform(df_nodup)\n\n# Results\nprint(f'Shape of X_train: {X_train.shape}')\nprint(f'Shape of X_test: {X_test.shape}')\nprint(f'Shape of y_train: {y_train.shape}')\nprint(f'Shape of y_test: {y_test.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus, it is possible to build a `Pipeline` that consolidates all these steps in a single transformer block, which is later used to apply the transformation steps sequentially in new received databases. The block below creates and executes a pipeline that applies the feature selection, categorical grouping and data separation processes in _training_ and _testing_.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.1.5\"></a>\n<font color=\"dimgrey\" size=+1.0><b>3.1.5 Building an Initial Pipeline</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"code","source":"# Building an initial pipeline\nfrom sklearn.pipeline import Pipeline\n\ntmp_pipeline = Pipeline([\n    ('selector', ColumnSelection(features=INITIAL_FEATURES)),\n    ('cat_agrup', Pipeline([\n        ('cat3_agrup', CategoricalLimitter(features=CAT3_FEATURES, n_cat=3, other_tag=OTHER_TAG)),\n        ('cat5_agrup', CategoricalLimitter(features=CAT5_FEATURES, n_cat=5, other_tag=OTHER_TAG)),\n        ('cat8_agrup', CategoricalLimitter(features=CAT8_FEATURES, n_cat=8, other_tag=OTHER_TAG))\n    ])),\n    ('splitter', DataSplitter(target=TARGET))\n])\n\n# Applying this pipeline into the original data\nX_train, X_test, y_train, y_test = tmp_pipeline.fit_transform(df)\nprint(f'Shape of original dataset: {df.shape}')\nprint(f'Shape of X_train: {X_train.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"However, it is extremely important to note that, due to the nature of the `CategoricalLimitter()` class, the less representative entries are automatically grouped into a new entry defined by the` OTHER_TAG` variable. Thus, it is possible to realize that, considering new databases received, the same volume and proportions will not always be the same for the categorical entries of the columns defined in `HIGH_CAT_FEATURES`. In other words, according to the database passed as input, it is possible to obtain different sets of features, since the volumes vary from set to set.\n\nFor example, it is possible that, considering a fictitious \"A\" base, the entry \"TAG 1\" (also fictitious) may have a highly representative volumetry, being considered as a relevant entry by the class `CategoricalLimitter()` which, in turn, takes into account the ordered result obtained by the `value_counts()` method. On the other hand, on a fictitious \"B\" basis, that same \"TAG 1\" entry can have a very low volume and, thus, it will be automatically grouped within the entry defined by `OTHER_TAG` and, consequently, remaining out of the set end, thus causing an error when using that base in subsequent steps.\n\nTo mitigate this problem, it is possible to \"fix\" the categorical entries considered in the initial group within each feature of the base. Simply put, a dictionary is created with the list of entries for each column, which is used for an official and definitive transformation in any database, thus ensuring that the same categorical entries will be considered, in the same way as the same categorical entries. Representative variables will be grouped in the `OTHER_TAG` variable.","metadata":{}},{"cell_type":"code","source":"# Dictionary with columns and entries\nhigh_cat_dict = {}\nfor feature in HIGH_CAT_FEATURES:\n    high_cat_dict[feature] = [col for col in X_train[feature].value_counts().index if col != OTHER_TAG]\nhigh_cat_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After building a definitive dictionary of categorical entries of the variables present in `HIGH_CAT_FEATURES`, it is possible to build a new definitive pipeline considering this categorical cluster transformer instead of the transformer used previously.","metadata":{}},{"cell_type":"code","source":"# Importing class module\nfrom mlcomposer.transformers import CategoricalMapper\n\n# Building a definitive initial pipeline\ninitial_pipeline = Pipeline([\n    ('selector', ColumnSelection(features=INITIAL_FEATURES)),\n    ('cat_agrup', CategoricalMapper(cat_dict=high_cat_dict, other_tag=OTHER_TAG)),\n    ('splitter', DataSplitter(target=TARGET))\n])\n\n# Applying this pipeline into the original data\nX_train, X_test, y_train, y_test = initial_pipeline.fit_transform(df)\nprint(f'Shape of original dataset: {df.shape}')\nprint(f'Shape of X_train: {X_train.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Verifying if the result with this new pipeline is the same as we tested before","metadata":{}},{"cell_type":"code","source":"# Verificando resultados do pipeline initial definitivo\nplot_multiple_countplots(df=X_train, col_list=HIGH_CAT_FEATURES)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.2\"></a>\n<font color=\"dimgrey\" size=+2.0><b>3.2 Numerical Pipeline</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"After building an initial pipeline capable of receiving a raw database, applying a feature selection process, performing a categorical grouping procedure and, finally, separating the data in training and testing, we will use the resulting training base to build pipelines in two different ways:\n\n* **Numerical pipeline:** preparation of the numerical data contained in the database;\n* **Categorical pipeline:** preparation of categorical data contained in the database.","metadata":{}},{"cell_type":"code","source":"# Splitting features by dtype\nnum_features = [col for col, dtype in X_train.dtypes.items() if dtype != 'object']\ncat_features = [col for col, dtype in X_train.dtypes.items() if dtype == 'object']\n\n# Splitting datasets\nX_train_num = X_train[num_features]\nX_cat_num = X_train[cat_features]\n\nprint(f'Total of numerical features: {len(num_features)}')\nprint(f'Total of categorical features: {len(cat_features)}')\nprint(f'Total of features (must be sum of numerical and categorical ones): {X_train.shape[1]}')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.2.1\"></a>\n<font color=\"dimgrey\" size=+1.0><b>3.2.1 Null Data</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"We saw, throughout the exploratory analysis, that many columns of the available set had null data. This is an important factor that needs to be considered in the preparation stage. Returning to this subject, we will then see again the numeric features with null data present","metadata":{}},{"cell_type":"code","source":"df_overview.query('feature in @num_features').head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus, it is possible to notice that the variables `LotFrontage`,`GarageYrBlt` and `MasVnrArea` have null data in different quantities. The meanings of these three columns are:\n\n* **_LotFrontage_**: Dimension of the front perimeter of the house;\n* **_GarageYrBlt_**: Year of construction of the garage;\n* **_MasVnrArea_**: Area covered by masonry\n\nAs we have different meanings for these columns, it is possible to establish different strategies for the final decision in terms of handling these null data. From the description above (and also from previous analyzes), it is noted that the feature `GarageYrBlt`, in its essence, has a discreet meaning, evidencing the year of construction of one of the parts of the property. Filling this variable with `average` or` median` does not seem entirely correct. For the other features (`LotFrontage` and `MasVnrArea`), it seems to make sense to use some statistical parameter to fill in the null values. Thus, we will adopt the following approach:\n\n1. Fill in null data of the variable `GarageYrBlt` with some \"numeric wildcard\" (-999 or 0, for example);\n2. Fill in null data for the variables `LotFrontage` and` MasVnrArea` with the respective medians.\n\n___\n\n_Obs:_ after performing some tests in this approach of filling in nulls, a modification in the topic above will be proposed, thus creating a unique pipeline of data transformation by the `median` in both columns.","metadata":{}},{"cell_type":"code","source":"# Importing library\nfrom sklearn.impute import SimpleImputer\n\n# Creating object and filling null data with median\nimputer = SimpleImputer(strategy='median')\nX_train_num_filled = imputer.fit_transform(X_train_num)\n\n# Results\nprint(f'Null data before pipeline: {X_train_num.isnull().sum().sum()}')\nprint(f'Null data after pipeline: {pd.DataFrame(X_train_num_filled, columns=num_features).isnull().sum().sum()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.2.2\"></a>\n<font color=\"dimgrey\" size=+1.0><b>3.2.2 Features Log Transformation</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"In session 2 of EDA, we saw some distributions of numerical variables amid the proposed analyzes. In them, it was possible to perceive the presence of distribution with great differences for what could be considered a \"normal Gaussian distribution\". In terms of convergence of regression models, greater performance and speed in reaching the minimum cost of the function through features with distributions close to the normal distribution.\n\nThus, we will optionally propose a step in the pipeline that applies a logarithmic transformation to the numerical features present in the base. With this, we can validate whether the final performance of the model is sensitive to this type of transformation.","metadata":{}},{"cell_type":"code","source":"# Example\ncol_log = 'LotArea'\ntmp = df.copy()\ntmp[col_log] = np.log1p(tmp[col_log])\n\n# Comparing distributions\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(17, 6))\nplot_distplot(df=df, col=col_log, hist=True, ax=axs[0], title=f'Distplot {col_log} - Original')\nplot_distplot(df=tmp, col=col_log, hist=True, ax=axs[1], title=f'Distplot {col_log} - Log Transformation')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Two highly relevant statistical measures for distribution analysis are `skew` and` kurtosis`. Through the [link](https://codeburst.io/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis-388fef94eeaa) it is possible to have a clear idea on what each of these measures is and how to interpret continuous distributions through their values.\n\nAs seen in `3.1.4`, the logarithmic transformation helps to increase performance for distributions with positive skewness (asymmetric on the left). Thus, we will analyze the numerical features again and rank the main features with the opportunity for improvement through this type of transformation.","metadata":{}},{"cell_type":"code","source":"from scipy.stats import skew, kurtosis\n\ntmp_ov = df_overview.copy()\ntmp_ov['skew'] = tmp_ov.query('feature in @num_features')['feature'].apply(lambda x: skew(X_train_num[x]))\ntmp_ov['kurtosis'] = tmp_ov.query('feature in @num_features')['feature'].apply(lambda x: kurtosis(X_train_num[x]))\ntmp_ov[~tmp_ov['skew'].isnull()].sort_values(by='skew', ascending=False).loc[:, ['feature', 'skew', 'kurtosis']]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The table above shows a list of features through their skewness and kurtosis measures of symmetry. In the code block below, we will execute the `DynamicLogTransformation` class, which, in turn, has the role of applying the logarithmic transformation in a database in a preparation pipeline. The advantage of this class is the previous definition of a list of features to which the transformation will be applied, which is defined by the user.","metadata":{}},{"cell_type":"code","source":"# Importando classe\nfrom mlcomposer.transformers import DynamicLogTransformation\n\n# Definições iniciais da transformação\nSKEW_THRESH = 0.5\ncols_idx = [np.argwhere(skew(X_train_num)==sk)[0][0] for sk in skew(X_train_num) if sk > SKEW_THRESH]\ncols_to_log = list(X_train_num.iloc[:, cols_idx].columns)\n\nprint(f'First line before transformation: \\n\\n{X_train_num_filled[0]}')\n# Aplicando transformação\nlog_tr = DynamicLogTransformation(num_features=num_features, cols_to_log=cols_to_log)\nX_train_num_log = log_tr.fit_transform(X_train_num_filled)\n\n# Validando\nprint(f'\\nFirst line after transformation: \\n\\n{X_train_num_log[0]}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.2.3\"></a>\n<font color=\"dimgrey\" size=+1.0><b>3.2.3 Scaling</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Another interesting way to apply a procedure that helps a given predictive model to converge to the optimal value more quickly is given by the “normalization” of the data. For the machine learning context, it is possible to use ready-made sklearn classes, for example, `MinMaxScaler` or `StandardScaler`.\n\nThis type of standardization / normalization can optionally be applied directly to the numerical pipeline. Below, an example of how this transformation can be applied to our numerical database will be demonstrated.","metadata":{}},{"cell_type":"code","source":"# Importing class\nfrom sklearn.preprocessing import StandardScaler\nfrom mlcomposer.transformers import DynamicScaler\n\nscaler = StandardScaler()\nX_train_num_scaled = scaler.fit_transform(X_train_num)\nX_train_num_scaled = pd.DataFrame(X_train_num_scaled, columns=num_features)\nX_train_num_scaled.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some models are sensitive to the normalization of the input data and others, not so much. Thus, we will propose the construction of an additional class with the possibility of applying or not the normalization.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.2.4\"></a>\n<font color=\"dimgrey\" size=+1.0><b>3.2.4 Building a Numerical Pipeline</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"After presenting the possible steps to be used to transform the numerical variables in our database, we will build a pipeline to encapsulate all of these procedures in a single block.","metadata":{}},{"cell_type":"code","source":"# Building pipeline\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('log', DynamicLogTransformation(num_features=num_features, cols_to_log=cols_to_log)),\n    ('scaler', DynamicScaler(scaler_type='Standard'))\n])\n\n# Applying pipeline\nX_train_num_prep = num_pipeline.fit_transform(X_train_num)\n\n# Results\nprint(f'Shape before num_pipeline: {X_train_num.shape}')\nprint(f'Shape after num_pipeline:{X_train_num_prep.shape}')\nprint(f'\\nX_train_num[0]:\\n{np.array(X_train_num.iloc[0, :])}')\nprint(f'\\nX_train_num_prep[0]:\\n{X_train_num_prep[0]}')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.3\"></a>\n<font color=\"dimgrey\" size=+2.0><b>3.3 Categorical Pipeline</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Continuing with the base transformation step, we now have the mission of applying specific transformers within the categorical universe of the data set. Recalling a little about the main features existing in this world, the block below rescues some parameters extracted previously:","metadata":{}},{"cell_type":"code","source":"print(f'Total of categorical features: {len(cat_features)}')\nprint(f'Example of categorical training data:')\nX_train_cat = X_train[cat_features]\nX_train_cat.head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before planning the steps of this pipeline, let's view some parameters extracted in the `data_overview()` function in relation to the categorical variables in our database:","metadata":{}},{"cell_type":"code","source":"cat_overview = data_overview(df=X_train_cat)\ncat_overview.head(10)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that, right away, there is a big problem involving null data in our categorical database. the `PoolQC`,` MiscFeature`, `Alley` and` Fence` columns, for example, have more than 80% of null records in the context of the data. Thus, an assertive decision needs to be made to prevent this from becoming a negative bias within the training of the proposed predictive model.\n\nAn alternative for the treatment of these data is the application of the `DummiesEncoding` class present in the `mlcomposer` package, mindful of the configuration to consider null data as a different category. In this way, the meaning of the null data is not lost or generalized (it would not be desirable to fill 99% of the data in `PoolQC` with the most common entry, for example).","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.3.1\"></a>\n<font color=\"dimgrey\" size=+1.0><b>3.3.1 Dummies Encoding</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"code","source":"# Importing class\nfrom mlcomposer.transformers import DummiesEncoding\n\n# Creating object and pplying encoding\nencoder = DummiesEncoding(dummy_na=True)\nX_train_cat_encoded = encoder.fit_transform(X_train_cat)\n\n# Results\nprint(f'Shape before encoding: {X_train_cat.shape}')\nprint(f'Shape after encoding: {X_train_cat_encoded.shape}')\nX_train_cat_encoded.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the result of the application of the encoding method, it is possible to notice a significant growth in the number of features present in our database. This was due to the large number of categorical variables present, each contributing a reasonable number of entries. When applying the `DummiesEncoding` class, each categorical entry is pivoted at the base and transformed into a different new column (example: `MsZoning_C (all)`,` MSZoning_FV`, `MSZonin_RH`, among others).\n\nAt first, the application of this process solves the problem of meaning and representativeness of null data (check new columns with suffix `_nan` in the resulting base), however, the increase in features can make predictive training more difficult, promoting a longer processing delay and a likely underperforming performance. To mitigate this fact, it is possible to analyze the training results and validate the application of feature selection processes based on the importance of the features after the training.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.3.2\"></a>\n<font color=\"dimgrey\" size=+1.0><b>3.3.2 Building a Categorical Pipeline</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"In this defined business context, the only transformer present in the categorical part of the base is `DummiesEncoding` and, thus, we can then build a pipeline that consolidates this class within a macro block. This will facilitate the union of categorical and numerical pipelines in the future.","metadata":{}},{"cell_type":"code","source":"# Building pipeline\ncat_pipeline = Pipeline([\n    ('encoder', DummiesEncoding(dummy_na=True))\n])\n\n# Applying pipeline\nX_train_cat_encoded = cat_pipeline.fit_transform(X_train_cat)\n\n# Results\nprint(f'Shape before cat_pipeline: {X_train_cat.shape}')\nprint(f'Shape after cat_pipeline: {X_train_cat_encoded.shape}')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.4\"></a>\n<font color=\"dimgrey\" size=+2.0><b>3.4 Complete Pipelines</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"After the proposals defined above within the base preparation stage, it is time to resume the blocks built and consolidate the definitive pipelines for the listed transformations. Thus, our goal is to define the fixed variables in each block and, finally, to build preparation objects that include:\n\n- **initial_train_pipeline:** initial pipeline to be applied in the _training_ stage of a received raw database;\n- **intial_pred_pipeline:** initial pipeline to be applied in the step of anchoring a raw database received;\n- **prep_pipeline:** pipeline for preparing a database in order to consolidate:\n     - *num_pipeline:* numerical preparation pipeline;\n     - *cat_pipeline:* categorical preparation pipeline","metadata":{}},{"cell_type":"code","source":"# Project variables\nTARGET = 'SalePrice'\nTO_DROP = ['Condition2', 'RoofMatl', 'Id']\nINITIAL_FEATURES = [col for col in df_tmp.columns if col not in TO_DROP]\nCAT_GROUP_DICT = {'Functional': ['Typ', 'Min2', 'Min1'],\n                  'SaleType': ['WD', 'New', 'COD'],\n                  'HouseStyle': ['1Story', '2Story', '1.5Fin', 'SLvl', 'SFoyer'],\n                  'Condition1': ['Norm', 'Feedr', 'Artery', 'RRAn', 'PosN'],\n                  'Neighborhood': ['NAmes', 'CollgCr', 'OldTown', 'Edwards', 'Somerst', 'Gilbert', 'NridgHt', 'Sawyer'],\n                  'Exterior2nd': ['VinylSd', 'MetalSd', 'Wd Sdng', 'HdBoard', 'Plywood'],\n                  'Exterior1st': ['VinylSd', 'HdBoard', 'MetalSd', 'Wd Sdng', 'Plywood']}\nOTHER_TAG = 'Other'\nCOLS_TO_LOG = ['MSSubClass', 'LotArea', 'OverallCond', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n               'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath',\n               'BsmtHalfBath', 'HalfBath', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'WoodDeckSF',\n               'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\n\n# Splitting features by its dtype\nNUM_FEATURES = [col for col, dtype in X_train.dtypes.items() if dtype != 'object']\nCAT_FEATURES = [col for col, dtype in X_train.dtypes.items() if dtype == 'object']","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing libraries\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\n\n# Initial prep pipeline\ninitial_prep_pipeline = Pipeline([\n    ('col_filter', ColumnSelection(features=INITIAL_FEATURES)),\n    ('cat_agrup', CategoricalMapper(cat_dict=CAT_GROUP_DICT, other_tag=OTHER_TAG)),\n    ('log_target', LogTransformation(cols_to_log=TARGET))\n])\n\n# Numerical pipeline\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('log', DynamicLogTransformation(num_features=NUM_FEATURES, cols_to_log=COLS_TO_LOG)),\n    ('scaler', DynamicScaler(scaler_type=None))\n])\n\n# Categorical pipeline\ncat_pipeline = Pipeline([\n    ('encoder', DummiesEncoding(dummy_na=True))\n])\n\n# Preparation pipeline\nprep_pipeline = ColumnTransformer([\n    ('num', num_pipeline, NUM_FEATURES),\n    ('cat', cat_pipeline, CAT_FEATURES)\n])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading raw data\ndf_train = pd.read_csv(os.path.join(DATA_PATH, TRAIN_FILENAME))\n\n# Applying initial pipeline\ndf_train_prep = initial_prep_pipeline.fit_transform(df_train)\n\n# Creating training and validation data\nX_train, X_val, y_train, y_val = train_test_split(df_train_prep.drop(TARGET, axis=1), \n                                                  df_train_prep[TARGET].values, test_size=.20, random_state=42)\n\n# Applying prep pipeline (train)\nX_train_prep = prep_pipeline.fit_transform(X_train)\ntrain_features = prep_pipeline.named_transformers_['cat'].named_steps['encoder'].features_after_encoding\n\n# Applying prep pipeline (validation)\nX_val_prep = prep_pipeline.fit_transform(X_val)\nval_features = prep_pipeline.named_transformers_['cat'].named_steps['encoder'].features_after_encoding\n\n# Results\nprint(f'Shape of X_train_prep: {X_train_prep.shape}')\nprint(f'Shape of X_val_prep: {X_val_prep.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Warning! The total of features present in the training dataset is different from the total of features present in the test dataset. This will certainly be a problem when trying to run a future predictive model in `X_test_prep`, which will be trained in `X_train_prep`. Errors will arise.\n\nWhat could explain this difference in the number of columns? The answer lies in the process of encoding categorical variables. Basically, the main suspicion is that the `X_test` test base does not contain the same categorical entries as `X_train`, so that, when applying the encoding process, the pivot for those missing entries does not exist and, therefore, the final number of pivot columns after encoding the test data is different.\n\nProposing a solution to this problem, the variables resulting from the encoding process in each case were stored in the `train_features` and `test_features` objects. With them, it is possible to compare exactly which were the categorical entries present in `X_train` and nonexistent in `X_test` (although rarer due to the volume, the opposite situation may also exist).","metadata":{}},{"cell_type":"code","source":"# Verifying\nprint(f'Total of train_features (after encoding): {len(train_features)}')\nprint(f'Total of val_features (after encoding): {len(val_features)}')\nprint(f'Difference between train and test: {len(train_features) - len(val_features)}')\n\n# Features not included on validation data\nnot_included_val = [col for col in train_features if col not in val_features]\nprint(f'\\nCategorical entries included on X_train but not in X_val:')\nnot_included_val","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, the blocks above indicate that, after the encoding process, some variables were left out of both sets (training and testing). In this case, it is necessary to perform an alignment step of the sets before starting the modeling steps. Thinking about scoring new entry bases, it is possible to consider a final step in the pipeline to, in fact, exactly match the features of the set before consuming the model.","metadata":{}},{"cell_type":"code","source":"# Returning final set of features that will be considered on training\nX_train_prep_df = pd.DataFrame(X_train_prep, columns=num_features+train_features)\nMODEL_FEATURES = list(X_train_prep_df.columns)\n    \n# Filling non existing features on validation data with zeros (0)\nX_val_prep_df = pd.DataFrame(X_val_prep, columns=num_features+val_features)\nfor col in not_included_val:\n    X_val_prep_df[col] = 0\nX_val_prep = np.array(X_val_prep_df.loc[:, MODEL_FEATURES])\n\n# Results\nprint(f'Shape of new X_train_prep: {X_train_prep.shape}')\nprint(f'Shape of new X_val_prep: {X_val_prep.shape}')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another important factor to be carried out before the transformation of the sets is the alignment of the features in their respective positions. For the predictive model, it would be disastrous to analyze the result in the test data considering an array of the same dimensions but with features in different positions. For this, we will align and order the columns correctly, based on the training set.","metadata":{}},{"cell_type":"code","source":"# Validating data\nprint(f'Total of model features: {len(MODEL_FEATURES)}')\n\n# Ordering X_val\nX_val_prep_df = X_val_prep_df.loc[:, MODEL_FEATURES]\n\n# Looking at the last features for each set\nprint(f'\\nLast 5 features of X_train_prep: \\n{X_train_prep_df.iloc[:, -5:].columns}')\nprint(f'\\nLast 5 features of X_val_prep: \\n{X_val_prep_df.iloc[:, -5:].columns}')\n\n# Transforming into an array\nX_train_prep = np.array(X_train_prep_df)\nX_val_prep = np.array(X_val_prep_df)\n\n# Results\nprint(f'\\nShape of final X_train_prep: {X_train_prep.shape}')\nprint(f'Shape of final X_val_prep: {X_val_prep.shape}')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With this final step of aligning the data set to create a single, definitive and orderly set according to all available features, it is possible to add an additional step in the pipeline to take care of this transformation automatically. This pipeline has the objective of applying the preparatory steps defined previously and, at the end, checking if the resulting features are in accordance with the `MODEL_FEATURES` block containing the definitive list of variables to be returned after the pipeline. For each of the features that are not present in this block, new columns will be inserted and, later, ordered in order to generate sets faithful to those used in the training steps.\n\nSuch a step will be proposed after training the modeling phase. The pilot and tests will be carried out on the official test basis for this business problem.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>4: Modeling: Training Predictive Models</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"After a long journey in the stage of preparing the available data, the great time has come to collect all the results obtained and conduct training on predictive models to, in fact, build an intelligence capable of receiving some inputs or characteristics of real estate and, in the end, predict the value of houses in a generic way.\n\nIn this step, we will use the features of the `mlcomposer` package within the `mlcomposer.trainer` module so that, in an easy, fast, efficient and intuitive way, we can train and evaluate predictive models from the most diverse sources within this task **Linear Regression**.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.1\"></a>\n<font color=\"dimgrey\" size=+2.0><b>4.1 Structuring Variables</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"In this step, the objective is to import the regressors and prepare the structure that will be used within the `LinearRegressor` class of the `mlcomposer` package. Thus, the cells below will be responsible for instantiating the models and preparing the search space for the best hyperparameters of the model","metadata":{}},{"cell_type":"code","source":"# Importing regression models\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n\n# Creating objects\nlin_reg = LinearRegression()\ntree_reg = DecisionTreeRegressor()\nforest_reg = RandomForestRegressor()\nridge_reg = Ridge()\nlasso_reg = Lasso()\nelastic_reg = ElasticNet()\nlgbm_reg = LGBMRegressor(objective='regression')\nxgb_reg = XGBRegressor(objective='reg:squarederror')\n#catb_reg = CatBoostRegressor()","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining hyperparameters\nlin_reg_params = {\n    'fit_intercept': [True, False],\n    'normalize': [True, False]\n}\n\ntree_reg_params = {\n    'max_depth': [100, 200, 300, 350, 400, 500],\n    'max_features': np.arange(1, len(MODEL_FEATURES)),\n    'random_state': [42]\n}\n\nforest_reg_params = {\n    'n_estimators': [75, 90, 100, 200, 300, 400, 450, 500], \n    'max_features': np.arange(1, len(MODEL_FEATURES)),\n    'random_state': [42]\n}\n\nridge_reg_params = {\n    'alpha': np.linspace(1e-5, 20, 400),\n    'fit_intercept': [True, False],\n    'normalize': [True, False]\n}\n\nlasso_reg_params = {\n    'alpha': np.linspace(1e-5, 20, 400),\n    'fit_intercept': [True, False],\n    'normalize': [True, False]\n}\n\nelastic_reg_params = {\n    'alpha': np.linspace(1e-5, 20, 400),\n    'l1_ratio': np.linspace(0, 1, 400),\n    'fit_intercept': [True, False],\n    'normalize': [True, False]\n}\n\nlgbm_param_grid = {\n    'num_leaves': np.arange(10, 250, 1),\n    'max_depth': np.arange(10, 350, 1),\n    'n_estimators': [75, 90, 100, 200, 300, 400, 450, 500],\n    'learning_rate': np.linspace(1e-5, 20, 400),\n    'reg_alpha': np.linspace(1e-5, 20, 400),\n    'reg_lambda': np.linspace(1e-5, 20, 400)\n}\n\nxgb_param_grid = {\n    'reg_lambda': np.linspace(1e-5, 20, 400),\n    'reg_alpha': np.linspace(1e-5, 20, 400),\n    'max_depth': np.arange(10, 350, 1),\n    'n_estimators': [75, 90, 100, 200, 300, 400, 450, 500],\n    'random_state': [42]\n}","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating set of regressors to be trained and evaluated\nset_regressors = {\n    'LinearRegression': {\n        'model': lin_reg,\n        'params': lin_reg_params\n    },\n    'DecisionTreeRegressor': {\n        'model': tree_reg,\n        'params': tree_reg_params\n    },\n    'RandomForestRegressor': {\n        'model': forest_reg,\n        'params': forest_reg_params\n    },\n    'Ridge': {\n        'model': ridge_reg,\n        'params': ridge_reg_params\n    },\n    'Lasso': {\n        'model': lasso_reg,\n        'params': lasso_reg_params\n    },\n    'ElasticNet': {\n        'model': elastic_reg,\n        'params': elastic_reg_params\n    },\n    'LightGBM': {\n        'model': lgbm_reg,\n        'params': {}\n    },\n    'XGBoost': {\n        'model': xgb_reg,\n        'params': {}\n    }\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.2\"></a>\n<font color=\"dimgrey\" size=+2.0><b>4.2 Training Models</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Once the modeling structure has been prepared from specific objects, such as the `set_regressors` dictionary, it is now possible to import the `RegressorLinear` class present in the `pycomp.ml.trainer` module to carry out all training and evaluation of the candidate models.\n\nThis class was developed in order to greatly facilitate the work of the analyst / scientist in terms of implementing codes to train, evaluate and optimize predictive models of linear regression. Its methods include powerful features that perform various actions with just one call.","metadata":{}},{"cell_type":"code","source":"# Importing class\nfrom mlcomposer.trainer import LinearRegressor\n\n# Creating object and training models\ntrainer = LinearRegressor()\ntrainer.fit(set_regressors, X_train_prep, y_train, random_search=True, scoring='neg_mean_squared_error')","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `fit()` method of the `trainer` object created is responsible for training the models encapsulated in the `set_regressors` dictionary created in the initial definitions stage.\n\nBy configuring the method to also apply the process of `RandomizedSearchCV` (random search of the best hyperparameters of each algorithm), it is possible to build models optimized according to the search space passed in the dictionary` set_regressors`.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.3\"></a>\n<font color=\"dimgrey\" size=+2.0><b>4.3 Evaluating Performance</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Once the candidate models are trained through the `fit()` method, it is then possible to evaluate the performance obtained in each case, thus returning the main regression metrics capable of indicating the best direction for the given task.\n\nTo perform this process, we can use the `evaluate_performance()` or `plot_metrics()` methods of the `trainer` object. In the first case, the return is an analytical DataFrame containing the result of the evaluation of each model against the main metrics. In the second case, the return is a visual analysis of the metrics for each of the models.","metadata":{}},{"cell_type":"code","source":"# Evaluating performance\nmetrics = trainer.evaluate_performance(X_train_prep, y_train, X_val_prep, y_val)\nmetrics","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have the following metrics:\n___\n**MAE - Mean Absolute Error**\n\n$\\frac{1}{n}\\sum|y-y_{pred}|$\n___\n\n**MSE - Mean Squared Error**\n\n$\\frac{1}{n}\\sum\\left(y-y_{pred}\\right)²$\n___\n\n**RMSE - Root Mean Squared Error**\n\n$\\sqrt{\\left(\\frac{1}{n}\\sum\\left(y-y_{pred}\\right)²\\right)}$\n___\n\n**R2 Score**\n\n$1 - \\frac{\\sum_{j=1}^n\\left(y_{pred}-y_{true}\\right)²}{\\sum_{j=1}^n\\left(y_j-y_{mean}\\right)²}$\n___","metadata":{}},{"cell_type":"markdown","source":"The return table allows us to compare the candidate regression models, thus making the decision on the best model for the given task to be guided by the optimization objective considered in the context. Visually, it is possible to execute the `plot_metrics()` method of the `trainer` object and see a more interactive comparison between the models.","metadata":{}},{"cell_type":"code","source":"# Visual metrics comparison\ntrainer.plot_metrics(bar_label=True)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, it is possible to notice that, among the candidate models, the `RandomForestRegressor` presented the smallest errors for the MAE, MSE and RMSE metrics, in addition to a higher R² score. In the future, it is then possible to use this tree model for more specific optimizations or to consider different other models to verify possible better performances.\n\nOnce these metrics have been analyzed, it is then possible to propose more detailed analyzes of the data set, such as the most relevant features for the property price predictive model. Next, we will use the prepared base `X_train_prep` to plot a correlation matrix capable of showing us the variables with the greatest impact (directly or inversely proportional) in relation to the price of the properties.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.4\"></a>\n<font color=\"dimgrey\" size=+2.0><b>4.4 Correlation Matrix</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"In this session, we will use the `plot_corr_matrix()` method of the `mlcomposer.insights` module. To execute it, just provide a database (`X_train_prep`) in DataFrame format and the target column of the correlation analysis. The result is a heatmap capable of scoring the main variables with an impact on the defined target, both directly (positive correlation) and inversely (negative correlation) proportional. We will then prepare the base and perform this function for both cases.","metadata":{}},{"cell_type":"code","source":"# Preparing DataFrame\nX_df = pd.DataFrame(X_train_prep, columns=MODEL_FEATURES)\nfinal_df = X_df.copy()\nfinal_df[TARGET] = y_train\n\n# Plotting correlation matrix (positive)\nplot_corr_matrix(df=final_df, corr_col=TARGET, n_vars=20, figsize=(12, 12), cbar=False)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The matrix above shows us the 20 main variables with a positive correlation in relation to the target (`SalePrice`). Among them, it is possible to mention the great influence of `OverallQual`, `GrLivArea` and `GarageCars` in the price of real estate in a directly proportional relationship. In addition, it is possible to see a strong positive correlation point between the features `TotRmsAbvGrd` and `GrLivArea`, which can be analyzed in more detail in the future in a possible approach to eliminate redundancy of highly correlated features, allowing a possible gain in performance.\n\nNext, we will analyze the matrix of the main variables with a negative correlation in relation to the target (inversely proportional relation).","metadata":{}},{"cell_type":"code","source":"# Negative correlation\nplot_corr_matrix(df=final_df, corr_col=TARGET, corr='negative', n_vars=20, figsize=(12, 12), cbar=False)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A matriz acima nos mostra as features com maior correlação negativa em relação ao preço de imóveis. É possível observar que grande parte dessas features estão atreladas a características categóricas das casas, como a inexistência de lareira (`FireplaceQu_nan`) ou a falta de acabamento em garagens (`GarageFinish_Unf`). Assim como na matriz anterior, é possível observar algumas variáveis com correlação máxima entre si, indicando assim uma redundância na modelagem. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.5\"></a>\n<font color=\"dimgrey\" size=+2.0><b>4.5 Feature Importance</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Still within the context of evaluating the most relevant features in this modeling step, it is possible to use models already trained to extract this information. For this, we will execute the `plot_feature_importance()` method of the `trainer` object in order to graphically visualize the main features considered for each of the trained models (provided they have the` feature_importances_` method available).","metadata":{}},{"cell_type":"code","source":"# Plotting feature importances\ntrainer.plot_feature_importance(features=MODEL_FEATURES)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here it is possible to notice that the `RandomForestRegressor` and` DecisionTreeRegressor` models presented sets of features similar to the one seen in the positive correlation matrix. The other models considered do not have the `feature_importances_` method.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.6\"></a>\n<font color=\"dimgrey\" size=+2.0><b>4.6 Learning Curves</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"A great tool for identifying possible overfitting and underfitting problems is the 'learning curve'. With it, it is possible to analyze the training and validation errors obtained by a given model over the use of a larger number of training data. Thus, it is possible to verify the impact of the `m` number of records in an evolutionary error view, comparing training and validation sets. The `trainer` object has a method called `plot_learning_curve()` capable of returning this analysis automatically for all models considered in the training.","metadata":{}},{"cell_type":"code","source":"# Plotting learning curves\ntrainer.plot_learning_curve()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.7\"></a>\n<font color=\"dimgrey\" size=+2.0><b>4.7 Feature Selection</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"So far, we have trained some regression models and graphically visualized the performance obtained in each case. All training was based on a database obtained after the extensive session of preparation of the set where it was possible to generate a dataset of 265 features to be analyzed.\n\nWe saw, in the correlation matrix, that not all features actively contribute to a good performance of predictive models. Some even have redundancy among themselves and, in a way, contribute to a certain worsening in this performance. That said, we will try, in this next step, to apply a `feature selection` process to consider only the most important features for the training. In this case, we will create a new training pipeline with an additional step for the selection process, leaving the `GridSearchCV` the answer for the best combination of features that maximizes a defined performance metric.","metadata":{}},{"cell_type":"code","source":"# Importing class\nfrom mlcomposer.transformers import FeatureSelection\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Train data\nX = X_train.append(X_test)\ny = np.concatenate((y_train, y_test))\n\n# Preparing complete training data\nX_prep = prep_pipeline.fit_transform(X)\n\n# Returning features and filtering it\nencoded_features = prep_pipeline.named_transformers_['cat'].named_steps['encoder'].features_after_encoding\nX_prep_df = pd.DataFrame(X_prep, columns=num_features + encoded_features)\nX_prep = np.array(X_prep_df.loc[:, MODEL_FEATURES])\n\n# Returning the selected model\nmodel_key = 'LightGBM'\nmodel = trainer.get_estimator(model_key)\nfeat_imp = model.feature_importances_\n\n# Building a feature selection pipeline\nselector_pipe = Pipeline([\n    ('selector', FeatureSelection(feature_importance=feat_imp, k=len(MODEL_FEATURES))),\n    ('model', model)\n])\n\n# Defining search hyperparameters space\nselector_grid = [{\n    'selector__k': list(range(1, len(MODEL_FEATURES)))\n}]\n\n# Applying search on full training data already prepared\nrnd_search_prep = RandomizedSearchCV(selector_pipe, selector_grid, cv=5, \n                                     scoring='neg_mean_squared_error', verbose=-1, n_jobs=-1)\nrnd_search_prep.fit(X_prep, y)\n\n# Returning the best combination\nbest_pipe = rnd_search_prep.best_estimator_\nbest_pipe","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final rmse on training data\nfrom sklearn.model_selection import cross_val_score\n\ny_scores = cross_val_score(best_pipe, X_train_prep, y_train, cv=5, scoring='neg_mean_squared_error')\nbest_rmse = np.sqrt(-y_scores).mean()\nbest_rmse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>5 Predicting: Predicting Housing Prices</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Considering the need for a certain refinement in the training stage, we will use what we have so far to make predictions on a test basis given by the task. In this session, we will read this base, apply the necessary transformations and use the best Regression model chosen to determine property prices.","metadata":{}},{"cell_type":"code","source":"# Reading test dataset\ndf_test = pd.read_csv(os.path.join(DATA_PATH, TEST_FILENAME))\nprint(f'Shape of df_test: {df_test.shape}')\ndf_test.head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def adjust_dataset_features(df_array_pipe, pipe_resulting_features, model_features):\n    \n    # Retornando set de features não inclusas no pipeline de produção\n    not_included_features = [col for col in model_features if col not in pipe_resulting_features]\n    \n    # Transformando array resultante do pipeline em DataFrame\n    df = pd.DataFrame(df_array_pipe, columns=pipe_resulting_features)\n    \n    # Adicionando colunas não inclusas (preenchimento com 0)\n    for col in not_included_features:\n        df[col] = 0\n        \n    # Ordenando base final para manter a estrutura do array utilizado no treinamento\n    df = df.loc[:, model_features]\n    \n    # Retornando resultado em formato de array\n    return np.array(df)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initial pipeline for predicting new data\ninitial_pred_pipeline = Pipeline([\n    ('col_filter', ColumnSelection(features=[col for col in INITIAL_FEATURES if col != TARGET])),\n    ('cat_agrup', CategoricalMapper(cat_dict=CAT_GROUP_DICT, other_tag=OTHER_TAG))\n])\n\n# Building a pipeline to be applied on new data\npred_pipeline = Pipeline([\n    ('initial', initial_pred_pipeline),\n    ('prep', prep_pipeline)\n])\n\n# Applying the pipeline\ndf_pred = pred_pipeline.fit_transform(df_test)\n\n# Returning resulting features\ncat_pred_features = pred_pipeline.named_steps['prep'].named_transformers_['cat'].named_steps['encoder'].features_after_encoding\npred_features = num_features + cat_pred_features\n\n# Adjusting the final data with features considered on training\nX_pred = adjust_dataset_features(df_pred, pred_features, MODEL_FEATURES)\n\nprint(f'Shape of final X_train_prep: {X_train_prep.shape}')\nprint(f'Shape of final X_pred: {X_pred.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To maintain the equity of features considered in the training, the function `adjust_dataset_features()` was built. With it, the preparation pipelines can be applied in any input base and, in cases where the final number of features is not the same (due to the lack of categorical entries or other relevant features), the function acts to normalize this case , adding \"fake features\" to the final test base.","metadata":{}},{"cell_type":"code","source":"# Returning model\nmodel_key = 'LightGBM'\nmodel = trainer.regressors_info[model_key]['estimator']\n\n# Predicting housing prices and preparing submission\ny_pred = model.predict(X_pred)\ndf_test['SalePrice'] = np.exp(y_pred)\ndf_sub = df_test.loc[:, ['Id', 'SalePrice']]\ndf_sub.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Please tell me what do you think about `xplotter` and `mlcomposer` packages and leave here a comment or a upvote. Your opinion is really important and I'm really excited to show you new implementations on those packages.\n\n* **xplotter on Github:** https://github.com/ThiagoPanini/xplotter\n* **xplotter on PyPI:** https://pypi.org/project/xplotter/\n\n\n* **mlcomposer on Github:** https://github.com/ThiagoPanini/mlcomposer\n* **mlcomposer on PyPI:** https://pypi.org/project/mlcomposer/\n___\n\n<font size=\"+1\" color=\"black\"><b>You can also visit my other kernels by clicking on the buttons</b></font><br>\n\n<a href=\"https://www.kaggle.com/thiagopanini/presenting-xplotter-and-mlcomposer-on-tps-may21\" class=\"btn btn-primary\" style=\"color:white;\">TPS May 2021</a>\n<a href=\"https://www.kaggle.com/thiagopanini/pycomp-predicting-survival-on-titanic-disaster\" class=\"btn btn-primary\" style=\"color:white;\">Titanic EDA</a>\n<a href=\"https://www.kaggle.com/thiagopanini/pycomp-exploring-and-modeling-housing-prices\" class=\"btn btn-primary\" style=\"color:white;\">Housing Prices</a>\n<a href=\"https://www.kaggle.com/thiagopanini/predicting-restaurant-s-rate-in-bengaluru\" class=\"btn btn-primary\" style=\"color:white;\">Bengaluru's Restaurants</a>\n<a href=\"https://www.kaggle.com/thiagopanini/sentimental-analysis-on-e-commerce-reviews\" class=\"btn btn-primary\" style=\"color:white;\">Sentimental Analysis E-Commerce</a>","metadata":{}}]}