{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"##Importing necessary Libraries\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport math\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n\n#pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:24.582345Z","iopub.execute_input":"2021-08-25T19:55:24.58272Z","iopub.status.idle":"2021-08-25T19:55:24.598324Z","shell.execute_reply.started":"2021-08-25T19:55:24.582673Z","shell.execute_reply":"2021-08-25T19:55:24.597293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Loading Data\ntrain_df = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest_df = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:24.641353Z","iopub.execute_input":"2021-08-25T19:55:24.641874Z","iopub.status.idle":"2021-08-25T19:55:24.727589Z","shell.execute_reply.started":"2021-08-25T19:55:24.641842Z","shell.execute_reply":"2021-08-25T19:55:24.726862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Describe the Datasets\n#train_df.shape, test_df.shape\n#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train_df.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test_df.shape))\n\n#Save the 'Id' column\ntrain_ID = train_df['Id']\ntest_ID = test_df['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain_df.drop(\"Id\", axis = 1, inplace = True)\ntest_df.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train_df.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test_df.shape))","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:24.72878Z","iopub.execute_input":"2021-08-25T19:55:24.729181Z","iopub.status.idle":"2021-08-25T19:55:24.741208Z","shell.execute_reply.started":"2021-08-25T19:55:24.729143Z","shell.execute_reply":"2021-08-25T19:55:24.74009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:24.742958Z","iopub.execute_input":"2021-08-25T19:55:24.743258Z","iopub.status.idle":"2021-08-25T19:55:24.774752Z","shell.execute_reply.started":"2021-08-25T19:55:24.743228Z","shell.execute_reply":"2021-08-25T19:55:24.773767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:24.776032Z","iopub.execute_input":"2021-08-25T19:55:24.776369Z","iopub.status.idle":"2021-08-25T19:55:24.812607Z","shell.execute_reply.started":"2021-08-25T19:55:24.77634Z","shell.execute_reply":"2021-08-25T19:55:24.81131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train has 1460 rows and 81 columns\nTest has 1459 rows and 80 columns","metadata":{}},{"cell_type":"markdown","source":"# Outliers","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nax.scatter(x = train_df['GrLivArea'], y = train_df['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:24.814169Z","iopub.execute_input":"2021-08-25T19:55:24.814456Z","iopub.status.idle":"2021-08-25T19:55:25.035832Z","shell.execute_reply.started":"2021-08-25T19:55:24.814426Z","shell.execute_reply":"2021-08-25T19:55:25.034826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see at the rightmost two with extremely large GrLivArea that are of a low price. These values are huge oultliers. Therefore, we can safely delete them.","metadata":{}},{"cell_type":"code","source":"#Deleting outliers\ntrain_df= train_df.drop(train_df[(train_df['GrLivArea']>4000) & (train_df['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train_df['GrLivArea'], train_df['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:25.037258Z","iopub.execute_input":"2021-08-25T19:55:25.037549Z","iopub.status.idle":"2021-08-25T19:55:25.251137Z","shell.execute_reply.started":"2021-08-25T19:55:25.037521Z","shell.execute_reply":"2021-08-25T19:55:25.250028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Assumptions of Linear Regression Model:\n\nLinear regression is an analysis that assesses whether one or more predictor variables explain the dependent (criterion) variable.  \nThe regression has five key assumptions:\n\n* Linear relationship\n* Multivariate normality\n* No or little multicollinearity\n* No auto-correlation\n* Homoscedasticity","metadata":{}},{"cell_type":"markdown","source":"# First things first: Lets Analyse Saleprice which is our dependent variable","metadata":{}},{"cell_type":"code","source":"#Descriptive statistics summary\ntrain_df['SalePrice'].describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:25.253804Z","iopub.execute_input":"2021-08-25T19:55:25.254332Z","iopub.status.idle":"2021-08-25T19:55:25.265431Z","shell.execute_reply.started":"2021-08-25T19:55:25.254285Z","shell.execute_reply":"2021-08-25T19:55:25.264416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create a histogram to see if the target variable is Normally distributed. If we want to create any linear model, it is essential that the features are normally distributed.","metadata":{}},{"cell_type":"code","source":"#histogram\nsns.distplot(train_df['SalePrice'] , fit=norm);\n\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_df['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train_df['SalePrice'], plot=plt)\nplt.show()\n\n#skewness and kurtosis\nprint(\"Skewness: %f\" % train_df['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train_df['SalePrice'].kurt())\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:25.267597Z","iopub.execute_input":"2021-08-25T19:55:25.268056Z","iopub.status.idle":"2021-08-25T19:55:25.888993Z","shell.execute_reply.started":"2021-08-25T19:55:25.268011Z","shell.execute_reply":"2021-08-25T19:55:25.887807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# From the above graph, we can observe that the distribution:\n\n* Deviate from the normal distribution\n* Have positive skewness\n* Show peakedness\n\nThe target variable is right skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.\n","metadata":{}},{"cell_type":"markdown","source":"# Log-transformation of the target variable","metadata":{}},{"cell_type":"code","source":"\n#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain_df[\"SalePrice\"] = np.log1p(train_df[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train_df['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_df['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train_df['SalePrice'], plot=plt)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:25.890692Z","iopub.execute_input":"2021-08-25T19:55:25.891163Z","iopub.status.idle":"2021-08-25T19:55:26.722969Z","shell.execute_reply.started":"2021-08-25T19:55:25.891114Z","shell.execute_reply":"2021-08-25T19:55:26.721743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The skew seems now corrected and the data appears more normally distributed.","metadata":{}},{"cell_type":"code","source":"# most correlated features \ncorrmat = train_df.corr()\ntop_corr_features = corrmat.index[abs(corrmat['SalePrice'])>0.5]\nplt.figure(figsize=(10,10))\nsns.heatmap(train_df[top_corr_features].corr(),annot = True);\ntop_corr_features","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:26.724687Z","iopub.execute_input":"2021-08-25T19:55:26.725047Z","iopub.status.idle":"2021-08-25T19:55:27.924507Z","shell.execute_reply.started":"2021-08-25T19:55:26.725013Z","shell.execute_reply":"2021-08-25T19:55:27.92323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We used heatmap here, so we can get the overview of all the features relationship:\n\nIn summary, we can conclude that:\n\n'GrLivArea' and 'TotalBsmtSF' seem to be linearly related with 'SalePrice'. Both relationships are positive, which means that as one variable increases, the other also increases. \n'OverallQual' and 'YearBuilt' also seem to be related with 'SalePrice'. The relationship seems to be stronger in the case of 'OverallQual', where the heat map shows how sales prices increase with the overall quality.\nWe just analysed four variables, but there are many other that we should analyse. \n\nGarageCars and GarageArea are also some of the most strongly correlated variables.\n\nSame goes for TotalBsmtSF and 1stFloor.\n\n* Top correlated features are the ones which have more than 50% correlation with SalePrice","metadata":{}},{"cell_type":"markdown","source":"# Scatter plot between 'SalePrice' and its correlated Variables","metadata":{}},{"cell_type":"code","source":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train_df[cols], size = 3)\nplt.show();\n","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:27.926028Z","iopub.execute_input":"2021-08-25T19:55:27.926427Z","iopub.status.idle":"2021-08-25T19:55:38.027713Z","shell.execute_reply.started":"2021-08-25T19:55:27.926386Z","shell.execute_reply":"2021-08-25T19:55:38.026401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"Let us concatenate the train and test data in the same dataframe","metadata":{}},{"cell_type":"code","source":"ntrain = train_df.shape[0]\nntest = test_df.shape[0]\ny_train = train_df.SalePrice.values\nprint(\"y_train shape is : {}\".format(y_train.shape))\nall_data = pd.concat((train_df, test_df)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:38.031312Z","iopub.execute_input":"2021-08-25T19:55:38.031824Z","iopub.status.idle":"2021-08-25T19:55:38.364367Z","shell.execute_reply.started":"2021-08-25T19:55:38.031763Z","shell.execute_reply":"2021-08-25T19:55:38.362928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calculate the percentage of missing values by each feature","metadata":{}},{"cell_type":"code","source":"all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(22)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:38.366788Z","iopub.execute_input":"2021-08-25T19:55:38.367225Z","iopub.status.idle":"2021-08-25T19:55:38.399283Z","shell.execute_reply.started":"2021-08-25T19:55:38.367178Z","shell.execute_reply":"2021-08-25T19:55:38.398003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualize the missing values by histogram","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:38.400594Z","iopub.execute_input":"2021-08-25T19:55:38.40107Z","iopub.status.idle":"2021-08-25T19:55:39.01505Z","shell.execute_reply.started":"2021-08-25T19:55:38.401026Z","shell.execute_reply":"2021-08-25T19:55:39.013802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nAccording to the table, the below variables have more than 15% of the data missing:\n('PoolQC','MiscFeature','Alley','Fence','FireplaceQu','LotFrontage')\n\n\nWe can see that \"GarageX\" variables have he same number of missing data. \n\nThe same logic applies to 'BsmtX' variables. The variables 'BsmtExposure', 'BsmtFinType1','BsmtQual','BsmtCond','BsmtFinType2' have similar percentages of missing data. 'MasVnrArea' and 'MasVnrType' have strong corelation with 'YearBuilt' and 'OverallQual'.\n\nFinally 'Electrical' have only one null value.","metadata":{}},{"cell_type":"markdown","source":"# Imputing the missing values","metadata":{}},{"cell_type":"markdown","source":"We impute them by proceeding sequentially through features with missing values","metadata":{}},{"cell_type":"markdown","source":"* PoolQC : data description says NA means \"No Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general.\n* MiscFeature : data description says NA means \"no misc feature\"\n* Alley : data description says NA means \"no alley access\"\n* Fence : data description says NA means \"no fence\"\n* FireplaceQu : data description says NA means \"no fireplace\"\n* LotFrontage : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.\n* GarageType, GarageFinish, GarageQual and GarageCond : Replacing missing data with None\n* GarageYrBlt, GarageArea and GarageCars : Replacing missing data with 0 (Since No garage = no cars in such garage.)\n* BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath : missing values are likely zero for having no basement\n* BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : For all these categorical basement-related features, NaN means that there is no basement.\n* MasVnrArea and MasVnrType : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type.\n* MSZoning (The general zoning classification) : 'RL' is by far the most common value. So we can fill in missing values with 'RL'\n* Utilities : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. We can then safely remove it.\n* Functional : data description says NA means typical\n* Electrical : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\n* KitchenQual: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.\n* Exterior1st and Exterior2nd : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\n* SaleType : Fill in again with most frequent which is \"WD\"\n* MSSubClass : Na most likely means No building class. We can replace missing values with None    ","metadata":{}},{"cell_type":"code","source":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\n    \nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data = all_data.drop(['Utilities'], axis=1)\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:39.016562Z","iopub.execute_input":"2021-08-25T19:55:39.017037Z","iopub.status.idle":"2021-08-25T19:55:39.072219Z","shell.execute_reply.started":"2021-08-25T19:55:39.016994Z","shell.execute_reply":"2021-08-25T19:55:39.071175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking for remaining missing values","metadata":{}},{"cell_type":"code","source":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:39.073727Z","iopub.execute_input":"2021-08-25T19:55:39.074174Z","iopub.status.idle":"2021-08-25T19:55:39.105767Z","shell.execute_reply.started":"2021-08-25T19:55:39.074127Z","shell.execute_reply":"2021-08-25T19:55:39.10453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transforming some numerical variables that are really categorical","metadata":{}},{"cell_type":"code","source":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:39.107188Z","iopub.execute_input":"2021-08-25T19:55:39.107781Z","iopub.status.idle":"2021-08-25T19:55:39.131139Z","shell.execute_reply.started":"2021-08-25T19:55:39.10773Z","shell.execute_reply":"2021-08-25T19:55:39.1302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Label Encoding some categorical variables that may contain information in their ordering set","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:39.132667Z","iopub.execute_input":"2021-08-25T19:55:39.133008Z","iopub.status.idle":"2021-08-25T19:55:39.223472Z","shell.execute_reply.started":"2021-08-25T19:55:39.132975Z","shell.execute_reply":"2021-08-25T19:55:39.222182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since area related features are very important to determine house prices, we add one more feature which is the total area of basement, first and second floor areas of each house","metadata":{}},{"cell_type":"code","source":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:39.225225Z","iopub.execute_input":"2021-08-25T19:55:39.225691Z","iopub.status.idle":"2021-08-25T19:55:39.233976Z","shell.execute_reply.started":"2021-08-25T19:55:39.225638Z","shell.execute_reply":"2021-08-25T19:55:39.232795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fixing Skewness","metadata":{}},{"cell_type":"code","source":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:39.235848Z","iopub.execute_input":"2021-08-25T19:55:39.236526Z","iopub.status.idle":"2021-08-25T19:55:39.289236Z","shell.execute_reply.started":"2021-08-25T19:55:39.236477Z","shell.execute_reply":"2021-08-25T19:55:39.287978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Box Cox Transformation of (highly) skewed features","metadata":{}},{"cell_type":"markdown","source":"We use the scipy function boxcox1p which computes the Box-Cox transformation of  1+x .\n\nNote that setting  Î»=0  is equivalent to log1p used above for the target variable","metadata":{}},{"cell_type":"code","source":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:39.290476Z","iopub.execute_input":"2021-08-25T19:55:39.290772Z","iopub.status.idle":"2021-08-25T19:55:39.337341Z","shell.execute_reply.started":"2021-08-25T19:55:39.290743Z","shell.execute_reply":"2021-08-25T19:55:39.336305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Getting dummy categorical features","metadata":{}},{"cell_type":"code","source":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:39.338582Z","iopub.execute_input":"2021-08-25T19:55:39.33889Z","iopub.status.idle":"2021-08-25T19:55:39.375882Z","shell.execute_reply.started":"2021-08-25T19:55:39.338861Z","shell.execute_reply":"2021-08-25T19:55:39.374569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Getting the new train and test sets.","metadata":{}},{"cell_type":"code","source":"x_train = all_data[:ntrain]\nx_test = all_data[ntrain:]\nprint(\"x_train shape is : {}\".format(x_train.shape))\nprint(\"x_test shape is : {}\".format(x_test.shape))\nprint(\"y_train shape is: {}\".format(y_train.shape))","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:39.377227Z","iopub.execute_input":"2021-08-25T19:55:39.377512Z","iopub.status.idle":"2021-08-25T19:55:39.384941Z","shell.execute_reply.started":"2021-08-25T19:55:39.377484Z","shell.execute_reply":"2021-08-25T19:55:39.38382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting the data into training and test datasets","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train,test_size = .3, random_state=0)\nprint(\"x_train shape is : {}\".format(X_train.shape))\nprint(\"x_test shape is : {}\".format(X_test.shape))\nprint(\"y_train shape is: {}\".format(Y_train.shape))\nprint(\"y_test shape is: {}\".format(Y_test.shape))","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:39.386972Z","iopub.execute_input":"2021-08-25T19:55:39.387522Z","iopub.status.idle":"2021-08-25T19:55:39.404382Z","shell.execute_reply.started":"2021-08-25T19:55:39.387476Z","shell.execute_reply":"2021-08-25T19:55:39.403106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"markdown","source":"Importing the Required Libraries","metadata":{}},{"cell_type":"code","source":"# importing all the required library for modeling here we are going to use statsmodels \nimport statsmodels.api as sm\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.linear_model import LinearRegression, ElasticNet, Lasso, Ridge,  BayesianRidge, LassoLarsIC\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV\n\nimport xgboost as xgb\nimport lightgbm as lgb","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:39.406112Z","iopub.execute_input":"2021-08-25T19:55:39.406409Z","iopub.status.idle":"2021-08-25T19:55:39.414008Z","shell.execute_reply.started":"2021-08-25T19:55:39.406381Z","shell.execute_reply":"2021-08-25T19:55:39.412979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building Simple Linear Model ","metadata":{}},{"cell_type":"code","source":"## Call in the LinearRegression object\nlin_reg = LinearRegression(normalize=True, n_jobs=-1)\n## fit train and test data. \nlin_reg.fit(x_train, y_train)\n## Predict test data. \ny_train_pred = lin_reg.predict(X_train)\ny_test_pred = lin_reg.predict(X_test)\nprint(\"y_train shape is : {}\".format(y_train.shape))\nprint(\"x_train shape is : {}\".format(x_train.shape))\n\n## get average squared error(MSE) by comparing predicted values with real values. \nprint ('RMSE for Train data %.4f'%np.sqrt(mean_squared_error(Y_train, y_train_pred)))\nprint ('RMSE for Test data %.4f'%np.sqrt(mean_squared_error(Y_test, y_test_pred)))","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:39.415352Z","iopub.execute_input":"2021-08-25T19:55:39.415924Z","iopub.status.idle":"2021-08-25T19:55:39.48045Z","shell.execute_reply.started":"2021-08-25T19:55:39.415888Z","shell.execute_reply":"2021-08-25T19:55:39.479424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# parity plot  \nplt.scatter(y_train_pred,Y_train,color='blue')\nplt.title('Linear Regression')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.plot([10.5,13.5],[10.5,13.5],c='red')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:39.485731Z","iopub.execute_input":"2021-08-25T19:55:39.492112Z","iopub.status.idle":"2021-08-25T19:55:39.761937Z","shell.execute_reply.started":"2021-08-25T19:55:39.492025Z","shell.execute_reply":"2021-08-25T19:55:39.76068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Regularization Models\n\nWhat makes regression model more effective is its ability of regularizing. The term \"regularizing\" stands for models ability to structurally prevent overfitting by imposing a penalty on the coefficients.  We will also define a function that returns the cross-validation rmse error so we can evaluate our models and pick the best tuning parametr. The main tuning parameter for the regularization model is alpha - a regularization parameter that measures how flexible our model is. The higher the regularization the less prone our model will be to overfit. However it will also lose flexibility and might not capture all of the signal in the data.\n\nWe are going to look at the below two regularization techniques\n\n* Ridge\n* Lasso","metadata":{}},{"cell_type":"markdown","source":"Cross Validation Strategy\n\nWe use the cross_val_score function of Sklearn. However this function has not a shuffle attribute, we add then one line of code, in order to shuffle the dataset prior to cross-validation","metadata":{}},{"cell_type":"code","source":"#Validation function\nn_folds = 10\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(x_train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, Y_train, scoring=\"neg_mean_squared_error\", cv = 5))\n    #scores = cross_val_score(model, x_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf)\n    return(rmse)\n# rmsle\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:39.763994Z","iopub.execute_input":"2021-08-25T19:55:39.764472Z","iopub.status.idle":"2021-08-25T19:55:39.772512Z","shell.execute_reply.started":"2021-08-25T19:55:39.764413Z","shell.execute_reply":"2021-08-25T19:55:39.771318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Set up Alpha values","metadata":{}},{"cell_type":"code","source":"alphas_alt = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:39.774141Z","iopub.execute_input":"2021-08-25T19:55:39.774593Z","iopub.status.idle":"2021-08-25T19:55:39.786002Z","shell.execute_reply.started":"2021-08-25T19:55:39.774547Z","shell.execute_reply":"2021-08-25T19:55:39.784843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ridge","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n\nridge=Ridge()\nparameters= {'alpha':[x for x in alphas_alt]}\n\nridge_reg=GridSearchCV(ridge, param_grid=parameters)\nridge_reg.fit(X_train,Y_train)\nprint(\"The best value of Alpha is: \",ridge_reg.best_params_,ridge_reg.best_score_)\n\ncv_ridge_mean_list =[]\ncv_ridge_std_list =[]\nfor alpha in alphas_alt:\n    ridge_reg = rmsle_cv(Ridge(alpha = alpha))\n    print(\"The alphas is : {}\".format(alpha))\n    print(\"Lasso Score mean is {:.4f}\\n\".format(ridge_reg.mean()))\n    print(\"Lasso Score std is {:.4f}\\n\".format(ridge_reg.std()))\n    cv_ridge_mean_list.append(ridge_reg.mean())\n    cv_ridge_std_list.append(ridge_reg.std())\n\n\ncv_ridge_mean = pd.Series(cv_ridge_mean_list, index = alphas_alt)\ncv_ridge_std = pd.Series(cv_ridge_std_list, index = alphas_alt)\ncv_ridge_mean.plot(title = \"Validation\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmsle\")\nprint(\"\\nRdige score: {:.4f} ({:.4f})\\n\".format(cv_ridge_mean.min(), cv_ridge_std.min()))","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:39.787743Z","iopub.execute_input":"2021-08-25T19:55:39.788271Z","iopub.status.idle":"2021-08-25T19:55:41.867725Z","shell.execute_reply.started":"2021-08-25T19:55:39.788198Z","shell.execute_reply":"2021-08-25T19:55:41.866518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note the U-ish shaped curve above. When alpha is too large the regularization is too strong and the model cannot capture all the complexities in the data. If however we let the model be too flexible (alpha small) the model begins to overfit. A value of alpha = 10 is about right based on the plot above.\n\nSo for the Ridge regression we get a rmsle of about 0.1174\n\nLet' try out the Lasso model. We will do a slightly different approach here and use the built in Lasso CV to figure out the best alpha for us. For some reason the alphas in Lasso CV are really the inverse or the alphas in Ridge.\n\n","metadata":{}},{"cell_type":"markdown","source":"# Lasso","metadata":{}},{"cell_type":"code","source":"lasso=Lasso()\nparameters= {'alpha':[x for x in alphas2]}\n\nlasso_reg=GridSearchCV(lasso, param_grid=parameters)\nlasso_reg.fit(X_train,Y_train)\nprint(\"The best value of Alpha is: \",lasso_reg.best_params_,lasso_reg.best_score_)\n\ncv_lasso_mean_list =[]\ncv_lasso_std_list =[]\nfor alpha in alphas2:\n    lasso_reg = rmsle_cv(Lasso(alpha = alpha))\n    print(\"The alphas is : {}\".format(alpha))\n    print(\"Lasso Score mean is {:.4f}\\n\".format(lasso_reg.mean()))\n    print(\"Lasso Score std is {:.4f}\\n\".format(lasso_reg.std()))\n    cv_lasso_mean_list.append(lasso_reg.mean())\n    cv_lasso_std_list.append(lasso_reg.std())\n\ncv_lasso_mean = pd.Series(cv_lasso_mean_list, index = alphas2)\ncv_lasso_std = pd.Series(cv_lasso_std_list, index = alphas2)\ncv_lasso_mean.plot(title = \"Validation\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmsle\")\n\nprint(\"\\nLassoscore: {:.4f} ({:.4f})\\n\".format(cv_lasso_mean.min(), cv_ridge_std.min()))","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:41.869307Z","iopub.execute_input":"2021-08-25T19:55:41.869666Z","iopub.status.idle":"2021-08-25T19:55:50.871161Z","shell.execute_reply.started":"2021-08-25T19:55:41.869633Z","shell.execute_reply":"2021-08-25T19:55:50.870022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The lasso performs even better at aplha = 0.0005, so we'll just use this one to predict on the test set. Another neat thing about the Lasso is that it does feature selection for you - setting coefficients of features it deems unimportant to zero. Let's take a look at the coefficients:","metadata":{}},{"cell_type":"code","source":"Lasso_model =Lasso(alpha=0.0005)\nLasso_model.fit(x_train,y_train)\ny_pred_train=Lasso_model.predict(X_train)\ny_pred_test=Lasso_model.predict(X_test)\n\nprint('Root Mean Square Error train = ' + str(np.sqrt(mean_squared_error(Y_train, y_pred_train))))\nprint('Root Mean Square Error test = ' + str(np.sqrt(mean_squared_error(Y_test, y_pred_test))))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-25T20:00:08.853934Z","iopub.execute_input":"2021-08-25T20:00:08.85432Z","iopub.status.idle":"2021-08-25T20:00:08.947256Z","shell.execute_reply.started":"2021-08-25T20:00:08.854287Z","shell.execute_reply":"2021-08-25T20:00:08.946232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coef = pd.Series(Lasso_model.coef_, index = X_train.columns)\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:50.872876Z","iopub.execute_input":"2021-08-25T19:55:50.873285Z","iopub.status.idle":"2021-08-25T19:55:50.88126Z","shell.execute_reply.started":"2021-08-25T19:55:50.873248Z","shell.execute_reply":"2021-08-25T19:55:50.879898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Good job Lasso. One thing to note here however is that the features selected are not necessarily the \"correct\" ones - especially since there are a lot of collinear features in this dataset. One idea to try here is run Lasso a few times on boostrapped samples and see how stable the feature selection is.","metadata":{}},{"cell_type":"markdown","source":"# Coeficients","metadata":{}},{"cell_type":"code","source":"imp_coef = pd.concat([coef.sort_values().head(10),\n                     coef.sort_values().tail(10)])\nplt.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:50.882945Z","iopub.execute_input":"2021-08-25T19:55:50.883384Z","iopub.status.idle":"2021-08-25T19:55:51.264789Z","shell.execute_reply.started":"2021-08-25T19:55:50.883339Z","shell.execute_reply":"2021-08-25T19:55:51.263526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The most important positive feature is GrLivArea - the above ground area by area square feet. This definitely sense. Then a few other location and quality features contributed positively. Some of the negative features make less sense and would be worth looking into more - it seems like they might come from unbalanced categorical variables.\n\nAlso note that unlike the feature importance you'd get from a random forest these are actual coefficients in your model - so you can say precisely why the predicted price is what it is. The only issue here is that we log_transformed both the target and the numeric features so the actual magnitudes are a bit hard to interpret.","metadata":{}},{"cell_type":"code","source":"#let's look at the residuals as well:\nplt.rcParams['figure.figsize'] = (6.0, 6.0)\n\npreds = pd.DataFrame({\"preds\":Lasso_model.predict(X_train), \"true\":Y_train})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:51.266447Z","iopub.execute_input":"2021-08-25T19:55:51.266831Z","iopub.status.idle":"2021-08-25T19:55:51.577007Z","shell.execute_reply.started":"2021-08-25T19:55:51.266799Z","shell.execute_reply":"2021-08-25T19:55:51.575798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The residual plot looks pretty good.","metadata":{}},{"cell_type":"markdown","source":"Let us use the lasso model for the predcitions and look at the RMSE for train and test data and submit the predcitions","metadata":{}},{"cell_type":"code","source":"\nlasso_preds = np.expm1(Lasso_model.predict(X_test))\nlasso_train = np.expm1(Lasso_model.predict(X_train))\nprint(rmsle(lasso_preds, Y_test))\n\nprint('Root Mean Square Error train = ' + str(np.sqrt(mean_squared_error(Y_train, lasso_train))))\nprint('Root Mean Square Error test = ' + str(np.sqrt(mean_squared_error(Y_test, lasso_preds)))) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Prediction","metadata":{}},{"cell_type":"code","source":"y_test=Lasso_model.predict(x_test)\npredictions=np.expm1(y_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:51.608537Z","iopub.execute_input":"2021-08-25T19:55:51.609075Z","iopub.status.idle":"2021-08-25T19:55:51.626931Z","shell.execute_reply.started":"2021-08-25T19:55:51.609027Z","shell.execute_reply":"2021-08-25T19:55:51.62573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('../input/house-prices-advanced-regression-techniques/sample_submission.csv')\nprediction = pd.DataFrame(predictions)\nfinal_submission = pd.DataFrame({'Id':submission['Id'],'SalePrice':predictions})\n\nfinal_submission.dropna(inplace=True)\n\nfinal_submission['Id']=final_submission['Id'].astype(int)\n\nfinal_submission.to_csv('submission1.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:51.6286Z","iopub.execute_input":"2021-08-25T19:55:51.629525Z","iopub.status.idle":"2021-08-25T19:55:51.662418Z","shell.execute_reply.started":"2021-08-25T19:55:51.629477Z","shell.execute_reply":"2021-08-25T19:55:51.661158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T19:55:51.664074Z","iopub.execute_input":"2021-08-25T19:55:51.664812Z","iopub.status.idle":"2021-08-25T19:55:51.678091Z","shell.execute_reply.started":"2021-08-25T19:55:51.664762Z","shell.execute_reply":"2021-08-25T19:55:51.677034Z"},"trusted":true},"execution_count":null,"outputs":[]}]}