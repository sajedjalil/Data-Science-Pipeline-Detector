{"cells":[{"metadata":{},"cell_type":"markdown","source":"The purpose of this notebook includes:\n- Giving a look into the dataset's features through some simple EAD\n- Selecting appropriate data transformation methods, and \n- (To our best abilities) finding variables that are most predictive of the sale prices.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The output of this notebook contains a `Columns_importances.csv` (containing importance scores of features trained by our models), a `train_preprocessed.csv` and a `test_preprocessed.csv` (containing proprocessed, feature-selected train and test datasets, respectively). It should be noted that the preprocessing steps and the preprocessed data are by no means optimal for all cases and models. However, I hope that they serve as useful examples or suggestions for further preprocessing.<br>\n<br>\nPlease feel free to use the output data (they can be downloaded from [here](https://www.kaggle.com/hoangnguyen719/house-prices-preprocessed-feature-selected) for more consistency) if you find them useful (and give this notebook some credits if you do), as well as drop any comments or suggestions you have. Thank you!\n### Table of content\n...<a href='#preparation'>I. Preparation</a><br>\n......<a href='#package_data_loading'>1. Package & Data Loading</a><br>\n......<a href='#null_imputation_preprocessing'>2. Null Imputation & Preprocessing</a><br>\n...<a href='#exploratory_data_analysis'>II. Exploratory Data Analysis</a><br>\n......<a href='#basement'>1. Basement</a><br>\n.........<a href='#basement_numerical_features'>1.1. Basement - Numerical Features</a><br>\n.........<a href='#basement_categorical_features'>1.2. Basement - Categorical Features</a><br>\n......<a href='#bath'>2. Bath</a><br>\n......<a href='#garage'>3. Garage</a><br>\n.........<a href='#garage_categorical_features'>3.1. Garage - Categorical Features</a><br>\n.........<a href='#garage_numerical_features'>3.2. Garage - Numerical Features</a><br>\n......<a href='#miscellaneous_features'>4. Miscellaneous Features</a><br>\n.........<a href='#heating'>4.1. Heating</a><br>\n.........<a href='#kitchen'>4.2. Kitchen</a><br>\n.........<a href='#fireplace'>4.3. Fireplace</a><br>\n.........<a href='#masonry'>4.4. Masonry</a><br>\n.........<a href='#pool'>4.5. Pool</a><br>\n.........<a href='#miscellaneous'>4.6. Miscellaneous</a><br>\n.........<a href='#porch'>4.7. Porch</a><br>\n......<a href='#other_features'>5. Other Features</a><br>\n.........<a href='#other_categorical_features'>5.1. Other - Categorical Features</a><br>\n.........<a href='#other_numerical_features'>5.2. Other - Numerical Features</a><br>\n...<a href='#feature_engineering_testing'>III. Feature Engineering & Testing</a><br>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# I. Preparation <a id='preparation'></a>\n## 1. Package & Data Loading <a id='package_data_loading'></a>","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import os, math, re\n\nfrom copy import deepcopy\nfrom datetime import datetime as dt\nfrom itertools import product\nfrom lightgbm import LGBMRegressor\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import\\\n    GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.linear_model import RidgeCV, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.svm import LinearSVR\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom tensorflow.keras import \\\n    callbacks, layers, optimizers, regularizers, Sequential\nfrom tqdm import tqdm\nimport warnings\nfrom xgboost import XGBRegressor\n\nsns.set(style=\"white\")\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def reload(df=\"train.csv\", dropped_columns=['Id'], msg=True): \n    # set dropped_columns = [] if want to keep Id column\n    data_path = \"../input/house-prices-advanced-regression-techniques/\"\n    if df == \"all_data\":\n        train = pd.read_csv(data_path + \"train.csv\")\n        test = pd.read_csv(data_path + \"test.csv\")\n        data = train.drop(columns=\"SalePrice\").append(test)\n    else:\n        data = pd.read_csv(data_path + df)\n    if msg:\n        print(df + \" loaded successfully!\")\n    return data.reset_index(drop=True).drop(columns=dropped_columns)\ntrain = reload()\ntest = reload(\"test.csv\")\nall_data = reload(\"all_data\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Null Imputation & Preprocessing <a id='null_imputation_preprocessing'></a>\nNull imputing and other preprocessing steps are considered, perfomred and tested in  [this other notebook](https://www.kaggle.com/hoangnguyen719/null-imputation). Here I will perform only the optimal method.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train = reload()\n# based on this notebook: https://www.kaggle.com/hoangnguyen719/null-imputation\nclass PreProcessor(BaseEstimator, TransformerMixin):\n    def __init__(self, old_new, numeric_to_object):\n        self.numeric_to_object = numeric_to_object\n        self.old_new = old_new\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X = X.copy()\n        sold_time = X[[\"MoSold\", \"YrSold\"]].astype(str).agg(\"/\".join, axis=1)\n        sold_time = pd.to_datetime(sold_time)\n        for old in self.old_new:\n            X[self.old_new[old]] = (sold_time - pd.to_datetime(X[old], format=\"%Y\"))\\\n                / pd.Timedelta(days=365.25)\n        X[self.numeric_to_object] = X[self.numeric_to_object].astype(str)\n        X.drop(columns=list(self.old_new.keys()), inplace=True)\n        return X\n    \nclass RelationImputer(BaseEstimator, TransformerMixin):\n    def __init__(self, related, threshold=None, strategy=\"most_frequent\", fill_value=None\n                 , missing_num=np.nan, missing_obj=np.nan\n                 , final_num=0, final_obj=\"None\"):\n        self.related = related if isinstance(related[0], list) else [related]\n        self.threshold = threshold\n        if strategy == 'constant':\n            self.fill_value = fill_value\n        elif strategy not in ['mean', 'median', 'most_frequent']:\n            raise Exception(\"Wrong strategy type! --{}\".format(strategy))\n        self.strategy = strategy\n        self.missings = [missing_num, missing_obj, final_num, final_obj]\n        self.missing_num, self.missing_obj, self.final_num, self.final_obj =\\\n            self.missings            \n            \n    def fit(self, X, y=None):\n        if self.strategy == 'constant':\n            self.statistics_ = pd.Series(data=self.fill_value, index=X.columns)\n        else:\n            # object columns always have strategy=\"most_frequent\"\n            self.statistics_ = X[X.select_dtypes(object).columns].mode().T[0]\n            num_cols = X.select_dtypes(\"number\").columns\n            if self.strategy ==  \"most_frequent\":\n                self.statistics_ = self.statistics_.append(X[num_cols].mode().T[0])\n            elif self.strategy in [\"mean\", \"median\"]:\n                strat = eval(\"X[num_cols].{}()\".format(self.strategy))\n                self.statistics_ = self.statistics_.append(strat)\n        return self\n    \n    def _indexes(self, X, cols):\n        \"\"\"\n            Return indexes of the nulls that will be imputed.\n        \"\"\"\n        if not self.threshold:\n            l = len(cols)\n        elif (self.threshold > 0) & (self.threshold < 1):\n            l = round(len(cols)*self.threshold)\n        else:\n            l = self.null_threshold\n        missing = X[cols].isin(self.missings).sum(axis=1)\n        index = (missing > 0) & (missing < l)\n        return index\n        \n    def transform(self, X, y=None):\n        X = X.copy()\n        obj_cols = list(X.select_dtypes(object).columns)\n        num_cols = list(X.select_dtypes(\"number\").columns)\n        obj_len = len(obj_cols)\n        num_len = len(num_cols)\n        to_replace = pd.Series(\n            data=[self.missing_obj]*obj_len + [self.missing_num]*num_len\n            , index=obj_cols + num_cols)\n        final_value = pd.Series(\n            data=[self.final_obj]*obj_len + [self.final_num]*num_len\n            , index=obj_cols + num_cols)\n        \n        for cols in self.related:\n            index = self._indexes(X, cols)\n            X.loc[index, cols] = X.loc[index, cols].replace(\n                to_replace=to_replace, value=self.statistics_[cols])\n            X[cols] = X[cols].replace(to_replace=to_replace, value=final_value)\n        \n        flat_related = [c for sublist in self.related for c in sublist]\n        remaining = [c for c in X.columns if c not in flat_related]\n        X[remaining] = X[remaining].replace(to_replace=to_replace\n                                            , value=self.statistics_[remaining])\n        return X\n\nRELATED = [[\"Alley\"]\n           , [\"Condition1\", \"Condition2\"]\n           , [\"Exterior2nd\"]\n           , [\"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\"\n              , \"BsmtFinSF1\", \"BsmtUnfSF\", \"TotalBsmtSF\"\n              , \"BsmtFullBath\", \"BsmtHalfBath\"] # Basement\n           , [\"BsmtFinType2\", \"BsmtFinSF2\"] # Basement 2\n           , [\"Heating\", \"HeatingQC\"] # Heating\n           , [\"KitchenQual\", \"KitchenAbvGr\"] # Kitchen\n           , [\"FireplaceQu\", \"Fireplaces\"] # Fireplaces\n           , [\"GarageType\", \"YrSinceGarageBlt\", \"GarageFinish\" # Garage\n              , \"GarageCars\", \"GarageArea\", \"GarageQual\", \"GarageCond\"]\n           , [\"MasVnrType\", \"MasVnrArea\"] # Masonry Veneer\n           , [\"PoolArea\", \"PoolQC\"] # Pool\n           , [\"Fence\"] # Fence\n           , [\"MiscFeature\", \"MiscVal\"] # Other miscellaneous\n            ]\nnonrelated = [\n    \"MSSubClass\", \"MSZoning\", \"LotArea\", \"Street\", \"Alley\"\n    , \"LotShape\", \"LandContour\", \"Utilities\", \"LotConfig\", \"LandSlope\"\n    , \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\"\n    , \"OverallQual\", \"OverallCond\", \"RoofStyle\", \"RoofMatl\", \"Exterior1st\"\n    , \"Exterior2nd\", \"ExterQual\", \"ExterCond\", \"Foundation\", \"CentralAir\"\n    , \"Electrical\", \"1stFlrSF\", \"2ndFlrSF\", \"LowQualFinSF\", \"GrLivArea\"\n    , \"BedroomAbvGr\", \"TotRmsAbvGrd\", \"Functional\", \"PavedDrive\"\n    , \"WoodDeckSF\", \"Fence\", \"MoSold\", \"YrSold\", \"SaleType\", \"LotFrontage\"\n    , \"SaleCondition\", \"Age\", \"YrSinceRemod\", \"YrSinceGarageBlt\"\n]\n\nflat_related = [c for cols in RELATED for c in cols]\n\nold_new = {\"YearBuilt\":\"Age\", \"YearRemodAdd\":\"YrSinceRemod\"\n           , \"GarageYrBlt\": \"YrSinceGarageBlt\"}\nnum_to_obj = [\"MSSubClass\", \"MoSold\", \"YrSold\"]\ncategorical_columns = list(all_data.select_dtypes(object).columns) + num_to_obj\ncat_to_transform = [c for c in categorical_columns if c in flat_related]\n\nPreI = PreProcessor(old_new, num_to_obj)\n# CI = ConstantImputer(columns=cat_to_transform + [\"Fence\", \"Alley\"], fill_value=\"None\")\nRI = RelationImputer(related = RELATED, final_obj=\"None\")\ntrain = PreI.fit_transform(train)\ntrain = RI.fit_transform(train)\n\nnumerical_columns = [col for col in all_data.columns if col not in categorical_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.SalePrice.hist(bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"SalePrice_log\"] = np.log1p(train.SalePrice)\ntrain.drop(columns=\"SalePrice\", inplace=True)\ntrain.SalePrice_log.hist(bins=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# II. Exploratory Data Analysis <a id='exploratory_data_analysis'></a>\n## 1. Basement <a id='basement'></a>","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"basement = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1'\n            , 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF','BsmtFinType2'\n            , 'BsmtFinSF2', \"BsmtFullBath\", \"BsmtHalfBath\"]\nprint(*basement, sep=\", \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.1. Basement - Numerical features <a id='basement_numerical_features'></a>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def corr_heatmap(columns=None, saleprice=[\"SalePrice_log\"], df=train\n                 , figsize=(8,6), vmin=-1, vmax=1, showvalue=True):\n    columns = df.columns if columns == None else columns + saleprice\n    corr = df[columns].corr()\n    plt.figure(figsize=figsize)\n    return sns.heatmap(corr, vmin=vmin, vmax=vmax, annot=showvalue)\ncorr_heatmap(basement)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def pairplot(columns, include_sale=True, data=train, kwargs={}):\n    if include_sale & (\"SalePrice_log\" not in columns):\n        columns = columns + [\"SalePrice_log\"]\n    sns.pairplot(data=data[columns], **kwargs)\npairplot(basement, kwargs={\"markers\":\"+\", \"height\":1.25})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nThe overall-area variable `TotalBsmtSF` seems  the most linearly predictive. If people are interested in area more than other characteristics of the basement (finished, unfinished, etc.), it may be worth removing the other three area variables `BsmtFinSF1`, `BsmtFinSF2` and `BsmtUnfSF` to save running time and prevent overfitting. At the end, we will test whether dropping these variables is a good idea. In addition, as for `BsmtFinSF1` and `BsmtFinSF2`, let's see if they are more predictive when combined with `BsmtFinType`.<br>\n<br>\nIt should also be noted that the majority of `BsmtHalfBath` is 0. Let's convert it to a dummy that evaluates to 0 if there is no basement halfbath and 1 otherwise.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.BsmtHalfBath.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will test a few potential variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"BsmtFinSF1_UnfSF\"] = train.BsmtFinSF1 + train.BsmtUnfSF\ntrain[\"BsmtFinSF\"] = train.BsmtFinSF1 + train.BsmtFinSF2\ntrain[\"BsmtBathPerSF\"] = (train.BsmtFullBath + train.BsmtHalfBath/2) / train.TotalBsmtSF\ntrain[\"BsmtBathPerFinSF\"] = (train.BsmtFullBath + train.BsmtHalfBath/2)\\\n    / (train.TotalBsmtSF - train.BsmtUnfSF)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def scatterplot(x, y=\"SalePrice_log\", df=train, figsize=(8,6), kwargs={}):\n    plt.figure(figsize=figsize)\n    sns.scatterplot(x=x, y=y, data=df, **kwargs)\ncorr_heatmap([\"BsmtFinSF1_UnfSF\", \"BsmtFinSF\", \"BsmtBathPerSF\", \"BsmtBathPerFinSF\"]\n             , figsize=(5,4)\n            )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The new variables created above doesn't show any new information, so we won't recreate them at the end.<br>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1.2. Basement - Categorical Features <a id='basement_categorical_features'></a>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(*[c for c in categorical_columns if c in basement], sep=\", \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All of these variables are ordinal so we can easily convert them to numerical features later. Let's take a look at those features (and few other newly created variables).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Score_map = {\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\":2, \"None\":1, \"Po\":0}\nBsmtFin_map = {\"GLQ\":5, \"ALQ\":4, \"BLQ\":3, \"Rec\":2, \"LwQ\":1, \"Unf\":0, \"None\":0}\nBsmtEx_map = {\"Gd\":3, \"Av\":2, \"Mn\":1, \"No\":0, \"None\":0}\ntrain[\"BsmtFinType1\"] = train.BsmtFinType1.replace(BsmtFin_map)\ntrain[\"BsmtFinType2\"] = train.BsmtFinType2.replace(BsmtFin_map)\ntrain[\"BsmtFinTypeAvg\"] = (train[\"BsmtFinType1\"] + train[\"BsmtFinType2\"]) / 2\ntrain[\"BsmtFinTypeProd\"] = train[\"BsmtFinType1\"] * train[\"BsmtFinType2\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def multiple_violinplots(Xs, df=train, y=\"SalePrice_log\"\n                         , size=4, ncol=3, nrow=None, hue=None):\n    nrow = nrow or (len(Xs) // ncol + 1)\n    fig = plt.figure(figsize=(size*ncol, size*nrow))\n    axes = []\n    for i in range(len(Xs)):\n        ax = plt.subplot(nrow, ncol, i+1)\n        sns.violinplot(x=Xs[i], y=y, data=df, hue=hue, ax=ax)\n        if (i % ncol) != 0:\n            ax.set_ylabel(\"\")\n        axes.append(ax)\n    plt.suptitle(y, fontsize=16)\n    fig.tight_layout(rect=[0,0,1,1 - 0.5/size/nrow])\n    return fig, axes\n\nfor col, m in zip([\"BsmtQual\", \"BsmtCond\", \"BsmtExposure\"]\n                 , [Score_map]*2 + [BsmtEx_map]):\n    train[col] = train[col].replace(m)\n\nfig, axes = multiple_violinplots(\n    [\"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\"\n     , \"BsmtFinType2\", \"BsmtFinTypeAvg\", \"BsmtFinTypeProd\"]\n    , size=3\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`BsmtQual`, `BsmtCond` and `BsmtExposure` all seems predictive enough to be kept, but `BsmtFinType1` and `BsmtFinType2` are not very so. We will keep them for now and review their predictive capability at the end. The additional variables don't generate much insights so we will ignore them.<br>\n<br>\nNext, let's try combining `BsmtFinSF` and `BsmtFinType`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"BsmtFinType_SF1\"] = train.BsmtFinType1 * train.BsmtFinSF1\ntrain[\"BsmtFinType_SF2\"] = train.BsmtFinType2 * train.BsmtFinSF2\nbsmt_ScSf = [\"BsmtFinSF1\", \"BsmtFinType_SF1\", \"BsmtFinSF2\", \"BsmtFinType_SF2\"]\npairplot(bsmt_ScSf, kwargs={\"height\":1.3})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"corr_heatmap(bsmt_ScSf, figsize=(4,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`BsmtFinType_SF` is more powerful than its creaters in the case of type 1 but not so in type 2; this is strange as we expect the behavior would be consistent. It's possibly because there are not many samples with type 2 basement so much of what we see here of type 2 is just noise. We will keep `BsmtFinType_SF1` and `BsmtFinType_SF2` inplace of `BsmtFinSF1` and `BsmtFinSF2`, respectively.<br>\n<br>\nI've also looked at in details how `BsmtCond` and `BsmtExposure` may possible interact with other basement numerical features. Not much information surfaced except one tiny thing between `BsmtExposure` and `BsmtUnfSF`.<br>\nIt's not very clear but we can see in the graph below that Good `BsmtExposure` has different y-intercept compared to other type of basement exposure. Let's add a variable `BsmtUnfExp`=`BsmtExposure`*`BsmtUnfSF`.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# pairplot(basement, kwargs={\"hue\":\"BsmtCond\", \"diag_kind\":\"hist\", \"height\":1.5})\n# pairplot(basement, kwargs={\"hue\":\"BsmtExposure\", \"diag_kind\":\"hist\"\n#                            , \"height\":1.5})\ndef multiple_regplots(x, hue, y=\"SalePrice_log\", df=train, kwargs={}):\n    g = sns.FacetGrid(train, hue=hue, size=5)\n    g.map(sns.regplot, x, y, ci=None, robust=1, **kwargs)\n    g.add_legend()\nmultiple_regplots(\"BsmtUnfSF\", \"BsmtExposure\"\n                  , kwargs={\"scatter_kws\":{\"alpha\":0.5}})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SUMMARY**: for basement features, we will\n- Convert the following to numerical:\n    - `BsmtHalfBath` to `BsmtHfBthDummy`: 0 if `BsmtHalfBath`=0, 1 otherwise\n    - `BsmtFinType1`, `BsmtFinType2`\n        - Mapping: `Bsmt_map` = {\"GLQ\":5, \"ALQ\":4, \"BLQ\":3, \"Rec\":2, \"LwQ\":1, \"Unf\":0, \"None\":0}\n    - `BsmtExposure`\n        - Mapping: `Bsmt_ExMap` = {\"Gd\":3, \"Av\":2, \"Mn\":1, \"No\":0, \"None\":0}\n    - `BsmtQual`, `BsmtCond`\n        - Mapping: `Score_map` = {\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\":2, \"None\":1, \"Po\":0}\n- Create the following new variables:\n    - `BsmtFinType_SF1` = `BsmtFinType1` * `BsmtFinSF1`\n    - `BsmtFinType_SF2` = `BsmtFinType2` * `BsmtFinSF2`\n    - `BsmtUnfExp`=`BsmtExposure`*`BsmtUnfSF`","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will also test the predictive power of the following features during model evaluation (and drop them if appropriate).\n- `BsmtFinSF1`, `BsmtFinSF2`, `BsmtUnfSF`, `BsmtFullBath`, `BsmtFinType1`, `BsmtFinType2`, `BsmtUnfExp`","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"test_features = [[\"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\"\n                  ,\"BsmtFinType1\", \"BsmtFinType2\", \"BsmtUnfExp\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Bath <a id='bath'></a>","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"bath = [\"BsmtFullBath\", \"BsmtHalfBath\", \"FullBath\", \"HalfBath\"]\nprint(*bath, sep=\", \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will look at these features in addition to a few of their possible combinations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"div_0_factor = 0.0001 # To avoid division by 0\ntrain[\"AllBath\"] = train.BsmtFullBath + train.FullBath +\\\n    (train.BsmtHalfBath + train.HalfBath)/2 # Half bath is considered 1/2 of Fullbath\ntrain[\"AllBathPerSF\"] = train.AllBath / (train.TotalBsmtSF + train.GrLivArea + div_0_factor)\ntrain[\"BathPerSF\"] = (train.FullBath + train.HalfBath/2) / (train.GrLivArea + div_0_factor)\ntrain[\"BathPerBedrooms\"] = (train.FullBath + train.HalfBath/2)\\\n    / (train.BedroomAbvGr + div_0_factor)\n\ncorr_heatmap(bath + [\"AllBath\", \"AllBathPerSF\", \"BathPerSF\", \"BathPerBedrooms\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pairplot(bath + [\"AllBath\", \"AllBathPerSF\", \"BathPerSF\", \"BathPerBedrooms\"]\n        , kwargs={\"height\":1.24})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Few things to note from the graphs above:\n- People seem to care more about upper-ground baths than about basement baths.\n- The bath-to-room and bath-to-area ratio does not matter as much as the absolute number of baths itself.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Therefore, an overall `AllBath` seems more reasonable than 4 separate bath variables. We will create the aggregate variable and compare it with the four separate ones to confirm our assumption.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"test_features.append(\n    [\"AllBath\", \"BsmtHalfBath\", \"BsmtFullBath\", \"HalfBath\", \"FullBath\"]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Garage <a id='garage'></a>\n### 3.1. Garage - Categorical Features <a id='garage_categorical_features'></a>","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(*[\"GarageType\", \"GarageQual\", \"GarageCond\", \"GarageFinish\"], sep=\", \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will first convert `GarageQual`, `GarageCond` and `GarageFinish` to ordinal, numerical variables.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"GargFin_map = {\"Fin\": 3, \"RFn\": 2, \"Unf\": 1, \"None\": 0}\nfor c, m in zip([\"GarageQual\", \"GarageCond\", \"GarageFinish\"]\n               , [Score_map]*2 + [GargFin_map]):\n    train[c] = train[c].replace(m)\nfig, axes = multiple_violinplots(\n    [\"GarageType\", \"GarageQual\", \"GarageCond\", \"GarageFinish\"]\n    , ncol=2\n)\naxes[0].set_xticklabels(labels=axes[0].get_xticklabels(), rotation=45)\nfig.tight_layout(rect=[0,0,1,0.95])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It doesn't seem obvious but there is some similarity between different values of `GarageType`. Specifically:\n- *Attchd*, *BuiltIn* and *Basement* are all built-in garages.\n- *Detch* and *CarPort* are all separate-from-house garages.<br>\n<br>\nTherefore it seems reasonable to consolidate the information.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"GargType_map = {\"BuiltIn\": \"Attchd\"\n                , \"Basment\": \"Attchd\"\n                , \"CarPort\": \"Detchd\"}\ntrain[\"GarageTypeSum\"] = train.GarageType.replace(GargType_map)\nfig, axes = multiple_violinplots([\"GarageTypeSum\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2. Garage - Numerical Features <a id='garage_numerical_features'></a>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"garage_cat = [\"YrSinceGarageBlt\", \"GarageCars\", \"GarageArea\"]\nprint(*garage_cat, sep=\", \")\ncorr_heatmap(garage_cat + [\"GarageCond\", \"GarageQual\"], figsize=(4.5, 4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`GarageCond` and `GarageQual` are highly correlated. Intuitively, these variables (one is \"Garage condition\", other \"Garage quality\", as described in the data description) seem very similar, or at least represent features that are similar. Therefore, we will drop one (`GarageCond` in this case).","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pairplot(garage_cat, kwargs={\"height\":1.5})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both `GarageCars` and `GarageArea` are indicator of the area's size so one may be tempting to drop one. In fact it may be good to do so. However, it should be noted that these two variables do NOT measure the same thing: `GarageArea` measures the total SPACE of the garage, while `GarageCars` in some senses represent the garage's WIDTH. Therefore, we will keep both fields for now and compare them during model training before choosing which field to drop, if any.\n<br><br>\n**SUMMARY**: For garage features, we will\n- Convert the following:\n    - `GarageQual`\n        - Using Score_map\n    - `GarageFinish`\n        - GargFin_map = {\"Fin\": 3, \"RFn\": 2, \"Unf\": 1, \"None\": 0}\n    - `GarageType` to `GarageTypeSum`\n        - GargType_map = {\"BuiltIn\": \"Attchd\", \"Basment\": \"Attchd\", \"CarPort\": \"Detchd\"}\n- Drop `GarageCond`\n- Compare `GarageArea` and `GarageCars`.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"test_features.append([\"GarageCars\", \"GarageArea\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Miscellaneous features <a id='miscellaneous_features'></a>\n### 4.1. Heating <a id='heating'></a>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"heating = ['Heating', 'HeatingQC']\nfig, ax = multiple_violinplots(heating, size=5, ncol=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[This notebook](https://www.kaggle.com/humananalog/xgboost-lasso) suggests using a score systems where *Fa* and *Po* are grouped together and *Ex*, *Gd* and *TA* are grouped together. That's an option, but based on the graph above it doesn't seem optimal. If anything, we may either use the score mapping above (*Ex*=5, *Gd*=4, *TA*=3, *Fa*=2, *None*=1, *Po*=0), or group *TA* and *FA* together and leave the rest as is.<br><br>\nAs for `Heating`, based on my own research, *Grav* is a pretty old system (invented and used during 1800s and 1900s). However, other than that there is no clear information about the preference/ordinality of the different types of heating system. Therefore, we will convert those to dummy variables.<br>\n<br>\n**SUMMARY**: \n- Convert `HeatingQC` to `HeatingQCScore` using our standard`Score_map`.\n- Convert `Heating` to dummies","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4.2. Kitchen <a id='kitchen'></a>\nIt can be easily seens that `KitchenQual` can be converted to an ordinal numerical feature as we did above.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train[\"KitchenQualScore\"] = train.KitchenQual.replace(Score_map)\ntrain[\"KitchenScore\"] = train.KitchenQualScore * train.KitchenAbvGr\ncorr_heatmap([\"KitchenAbvGr\", \"KitchenQualScore\", \"KitchenScore\"], figsize=(4,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.KitchenAbvGr.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the matrix above `KitchenAbvGr` is not at all linearly predictive of sales price. The reason could be that most of the sample has only 1 kitchen and there is not much information about properties with more (or even less) than 1 kitchen. Therefore, let's drop this variable. We willt try to retain its information in the newly created `KitchenScore`.<br>\n<br>\n**SUMMARY**:\n- Convert `KitchenQual` to `KitchenQualScore` using `Score_map`\n- Create `KitchenScore` = `KitchenQualScore` * `KitchenAbvGr*\n- Test `KitchenAbvGr`","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"test_features.append([\"KitchenAbvGr\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3. Fireplace <a id='fireplace'></a>\nSimilar to `KitchenQual`, let's first convert `FireplaceQu` to numerical.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"FireplaceQu\"] = train.FireplaceQu.replace(Score_map)\ntrain[\"FireplaceScore\"] = train.FireplaceQu * train.Fireplaces\ncorr_heatmap([\"Fireplaces\", \"FireplaceQu\", \"FireplaceScore\"]\n            , figsize=(4,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pairplot([\"Fireplaces\", \"FireplaceQu\", \"FireplaceScore\"]\n        , kwargs={\"plot_kws\":{\"alpha\":0.5}, \"height\":1.75})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly that properties with more and better fireplaces imply better fire safety and come with higher prices. The additional `FireplaceScore` doesn't seem to bring much more information, however, so we won't reproduce it at the end.<br>\n<br>\n**SUMMARY**\n- Convert `FireplaceQu` to numerical using `Score_map`.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4.4. Masonry <a id='masonry'></a>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"multiple_regplots(\"MasVnrArea\", \"MasVnrType\", kwargs={\"scatter_kws\":{\"alpha\":0.5}})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Together the two fields seem to have some predictive power (specifically, generally higher `MasVnrArea` is correlated with higher `SalePrice`, but we can see that different `MasVnrType` has different y-intercept). Let's keep both.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4.5. Pool <a id='pool'></a>\nLet's convert `PoolQC` to numerical values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"PoolQC\"] = train.PoolQC.replace(Score_map)\ntrain[\"PoolArea_QC\"] = train.PoolArea * train.PoolQC\ncorr_heatmap([\"PoolArea\", \"PoolQC\", \"PoolArea_QC\"]\n            , figsize=(4,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.PoolArea.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pool is often a luxurious features that often suggest a higher housing price. However in this case, again similar to the kitchen features, there are two few samples with pools and therefore the information they carry may be misleading. We will retain `PoolArea` and `PoolQC` but will need to keep an eye on them during model training. As for the addition feature `PoolArea_QC`, we will ignore it.<br>\n<br>\n**SUMMARY**:\n- Convert `PoolQC` to numerical\n- Test: `PoolArea` and `PoolQC`.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"test_features.append([\"PoolArea\", \"PoolQC\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.6. Miscellaneous <a id='miscellaneous'></a>\nSimilar to Pool, miscellaneous features don't contain much information. However, different from Pool, we don't have any intuitive reasoning for these miscellaneous features' practical importance. It's likely we will drop them.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axes = multiple_violinplots([\"MiscFeature\"], ncol=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"corr_heatmap([\"MiscVal\"], figsize=(3.5,3))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"test_features.append([\"MiscFeature\", \"MiscVal\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.7. Porch <a id='porch'></a>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"porch = ['OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch']\nprint(\"In the train set, there are:\")\nfor p in porch:\n    print(\"\\t{}/{} samples with {}\".format(sum(train[p]>0), len(train), p))\ntrain[\"PorchSF\"] = train[porch].sum(axis=1)\nfig = corr_heatmap(porch + [\"PorchSF\"], figsize=(6,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pairplot(porch + [\"PorchSF\"], kwargs={\"height\":1.5})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The majority of samples either has only open porch or doesn't have a porch. However, different from Pool and Kichen, the number of samples is still large enough not to be ignored. Therefore we will keep all of these 4 features (and not reproduce the overall `PorchSF`)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5. Other features <a id='other_features'></a>\n(that are not related to any other features in the set)","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(*nonrelated, sep=\", \")\nobj_nrlt = list(train[nonrelated].select_dtypes(object).columns)\nnum_nonrlt = list(train[nonrelated].select_dtypes(np.number).columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### 5.1. Other - Categorical Features <a id='other_categorical_features'></a>\n Lots of things about these variables can be infered from their [data description](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data). With a quick look into it, here are few things we can notice immediately:\n- `MSSubClass` seems to be a combination of `HouseStyle`, `BldgType` and `YearBuilt`, so let's drop it to avoid noise.\n- Many of the features are ordinal (`LotShape`, `LandSlope`, `PavedDrive` etc.) that can be converted to numerical values.\n- `Condition1` and `Condition2` can be combined and converted to 4 features of approximity as follows\n    - `Str`: Feedr=1, Artery=2, otherwise=0\n    - `NSRR`: RRNn=1, RRAn=2, otherwise=0\n    - `EWRR`: RRNe=1, RRAe=2, otherwise=0\n    - `Offsite`: PosN=1, PosA=2, otherwise=0<br>\n    <br>\n    One thing to note is that `Utilities` and `Electrical` are heavily skewed in the train set. Might make sense to drop them later. We'll see how they perform during model training.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.Utilities.value_counts(), end=\"\\n\\n\")\nprint(train.Electrical.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The remaining features will be one-hot encoded. Check out the Feature Engineering part for more details.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 5.2. Numerical features <a id='other_numerical_features'></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"2floorsSF\"] = train[\"1stFlrSF\"] + train[\"2ndFlrSF\"]\ntrain[\"TotalArea\"] = train[[\"1stFlrSF\", \"2ndFlrSF\", \"TotalBsmtSF\"]].sum(axis=1)\ncorr_heatmap(nonrelated + [\"2floorsSF\", \"TotalArea\"], figsize=(12,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Things to note:\n- `1stFlrSF`+`2ndFlrSF` = `GrLivArea`. Let's drop them.\n    - Drop: `1stFlrSF`, `2ndFlrSF`\n- Surprisingly, `OverallCond` is almost non-correlated with sale prices. Based on the data description, I had expected it to be somewhat related to `OverallQual` and more-or-less predictive of `SalePrice`.\n- Test:\n    - `Utilities`, `Electrical`, `OverallCond`, `LowQualFinSF`\n    - `Age` vs. `YrSinceRemod`","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"test_features.append(\n    [\"Utilities\", \"Electrical\", \"OverallCond\", \"LowQualFinSF\"]\n)\ntest_features.append([\"Age\", \"YrSinceRemod\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# III. Feature Engineering & Testing <a id='feature_engineering_testing'></a>\nIn this part we are going to test the data transformation ideas we have proposed above using some basic regressor models. We will be using the following models to test how our feature processing and selections affect the score:\n- `GradientBoostingRegressor`\n- `ExtraTreesRegressor`\n- `RandomForestRegressor`\n- `XGBRegressor`\n- `RidgeCV`\n- `ElasticNetCV`\n- `LinearSVR`\n- `LGBMRegressor`","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*Note: in the code and previous version of this notebook, I also tried incorporating a simple feedforward neural network; however, without careful hyper-param tuning, the net often experienced gradient-exploding and was very unstable, so I've left it out. You can try adding it again, but it will need some tweaking to avoid gradient explosion and ensuring the score is not so off*","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class FeatureEngineer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    This class performs the data transformation steps\n    we've discussed above.\n    \"\"\"\n    Score_map = {\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\":2, \"None\":1, \"Po\":0}\n    BsmtFin_map = {\"GLQ\":5, \"ALQ\":4, \"BLQ\":3, \"Rec\":2, \"LwQ\":1, \"Unf\":0, \"None\":0}\n    BsmtEx_map = {\"Gd\":3, \"Av\":2, \"Mn\":1, \"No\":0, \"None\":0}\n    GargFin_map = {\"Fin\": 3, \"RFn\": 2, \"Unf\": 1, \"None\": 0}\n    GargType_map = {\"BuiltIn\": \"Attchd\", \"Basment\": \"Attchd\"\n                    , \"CarPort\": \"Detchd\",}\n    StreetAlley_map = {\"Pave\":2, \"Grvl\":1, \"None\":0}\n    CentralAir_map = {\"Y\":1, \"N\":0}\n    LotShape_map = {\"Reg\":0, \"IR1\":1, \"IR2\":2, \"IR3\":3}\n    LandSlope_map = {\"Gtl\":0, \"Mod\":1, \"Sev\":2}\n    Electrical_map = {\"SBrkr\":3, \"FuseA\":2, \"FuseF\":1, \"FuseP\":0}\n    ElectricMix_map = {\"Mix\":1}\n    Functional_map = {\"Typ\":5, \"Min1\":4, \"Min2\":4, \"Mod\":3\n                      , \"Maj1\":2, \"Maj2\":2, \"Sev\":1, \"Sal\":0}\n    PavedDrive_map = {\"Y\":2, \"P\":1, \"N\":0}\n    Fence_map = {\"GdPrv\":4, \"MnPrv\":3, \"GdWo\":2, \"MnWw\":1, \"None\":0}\n    CondStr_map = {\"Feedr\":1, \"Artery\":2}\n    CondNSRR_map = {\"RRNn\":1, \"RRAn\":2}\n    CondEWRR_map = {\"RRNe\":1, \"RRAe\":2}\n    CondOff_map = {\"PosN\":1, \"PosA\":2}\n    \n    to_drop_ = [\"MSSubClass\", \"GarageCond\", \"1stFlrSF\", \"2ndFlrSF\"]\n    test_cols_ = [[\"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\"\n                   ,\"BsmtFinType1\", \"BsmtFinType2\", \"BsmtUnfExp\"]\n                  , [\"AllBath\", \"BsmtFullBath\", \"BsmtHalfBath\"\n                     , \"FullBath\", \"HalfBath\"]\n                  , [\"GarageCars\", \"GarageArea\"]\n                  , ['KitchenAbvGr']\n                  , [\"PoolArea\", \"PoolQC\"]\n                  , [\"MiscFeature\", \"MiscVal\"]\n                  , [\"Utilities\", \"Electrical\", \"OverallCond\", \"LowQualFinSF\"]\n                  , [\"Age\", \"YrSinceRemod\"]\n                 ]\n    \n    def __init__(self, execute=True\n                 , cols_to_keep_or_drop=None\n                 , keep_or_drop=None):\n        \"\"\"\n        ``execute``: if True then ``_feature_engineer`` method,\n        otherwise pass\n        ``cols_to_keep_or_drop``: list of columns to be kept or dropped\n        ``keep_or_drop``: if \"keep\" then keeping ``cols_to_keep_or_drop``\n        columns, else if \"drop\" then dropping ``cols_to_keep_or_drop`` columns,\n        otherwise pass\n        \"\"\"\n        self.execute = execute\n        self.cols_to_keep_or_drop = cols_to_keep_or_drop\n        self.keep_or_drop=keep_or_drop\n    \n    def map_val(self, x, mp):\n        return mp[x] if x in mp else 0\n    \n    def map_s(self, s, mp):\n        return s.apply(lambda x: self.map_val(x, mp))\n    \n    def _feature_engineer(self, X):\n        X = X.copy()\n        # Score_map mapping\n        for c in [\"BsmtQual\", \"BsmtCond\", \"GarageQual\"\n                  , \"HeatingQC\", \"KitchenQual\", \"FireplaceQu\"\n                  , \"PoolQC\", \"ExterQual\", \"ExterCond\"]:\n            X[c] = self.map_s(X[c], self.Score_map)\n        \n        # Other mapping\n        for c, mpping in (\n            [\"BsmtFinType1\", self.BsmtFin_map]\n            , [\"BsmtFinType2\", self.BsmtFin_map]\n            , [\"BsmtExposure\", self.BsmtEx_map]\n            , [\"GarageFinish\", self.GargFin_map]\n            , [\"Alley\", self.StreetAlley_map]\n            , [\"Street\", self.StreetAlley_map]\n            , [\"CentralAir\", self.CentralAir_map]\n            , [\"LotShape\", self.LotShape_map]\n            , [\"LandSlope\", self.LandSlope_map]\n            , [\"Electrical\", self.Electrical_map]\n            , [\"Functional\", self.Functional_map]\n            , [\"PavedDrive\", self.PavedDrive_map]\n            , [\"Fence\", self.Fence_map]\n        ):\n            if c == \"Electrical\":\n                X[c + \"Mix\"] = self.map_s(X[c], self.ElectricMix_map)\n            X[c] = self.map_s(X[c], mpping)\n        # GarageType\n        # self.map_s works for string->int conversion only\n        # and therefore won't work for GarageType\n        X[\"GarageType\"] = X.GarageType.replace(self.GargType_map)\n        # Condition1 and Condition2\n        for c, m in [\n            [\"Str\", self.CondStr_map]\n            , [\"NSRR\", self.CondNSRR_map]\n            , [\"EWRR\", self.CondEWRR_map]\n            , [\"Offsite\", self.CondOff_map]\n        ]:\n            cond1 = self.map_s(X.Condition1, m)\n            cond2 = self.map_s(X.Condition2, m)\n            X[c] = pd.concat([cond1, cond2], axis=1).max(axis=1)\n        X.drop(columns = [\"Condition1\", \"Condition2\"], inplace=True)\n            \n        # Additional features\n        X[\"BsmtHfBthDummy\"] = np.where(X.BsmtHalfBath == 0, 0, 1)\n        X[\"BsmtFinType_SF1\"] = X.BsmtFinType1 * X.BsmtFinSF1\n        X[\"BsmtFinType_SF2\"] = X.BsmtFinType2 * X.BsmtFinSF2\n        X[\"BsmtUnfExp\"] = X.BsmtUnfSF * X.BsmtExposure\n        X[\"AllBath\"] = X.BsmtFullBath + X.FullBath +\\\n            (X.BsmtHalfBath + X.HalfBath)/2\n        X[\"KitchenScore\"] = X.KitchenQual * X.KitchenAbvGr  \n        \n        # Dropping unnecessary columns\n        X.drop(columns = self.to_drop_, inplace=True)\n        return X\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        if self.execute:\n            X = self._feature_engineer(X)\n        if self.cols_to_keep_or_drop:\n            if self.keep_or_drop == \"keep\":\n                X = X[self.cols_to_keep_or_drop]\n            elif self.keep_or_drop == \"drop\":\n                X = X.drop(columns=self.cols_to_keep_or_drop, inplace=True)\n        return X\n    \nclass CustomizedOHE(BaseEstimator, TransformerMixin):\n    \"\"\"Perform one-hot encoding. This class always returns a dense pandas\n    dataframe (sklearn's always returns np.array).\n    \n    ``columns``: \"categorical\" (transformer applied to all categorical\n    features) or list of column names to be transformed.\n    ``keep_header``: if True returns pd.DataFrame with\n    column names, otherwise return np.array\n    \"\"\"\n    def __init__(self, columns=\"categorical\"\n                 , OHE=OneHotEncoder(handle_unknown=\"ignore\")\n                 , keep_header=True):\n        self.columns = columns\n        self.OHE = OHE\n        self.keep_header = keep_header\n    \n    def fit(self, X, y=None):\n        self.header_ = list(X.columns)\n        if self.columns == \"categorical\":\n            self.transformed_cols_ = X.select_dtypes(object).columns\n        else:\n            self.transformed_cols_ = self.columns\n        self.not_transformed_cols_ = [\n            c for c in self.header_ if c not in self.transformed_cols_\n        ]\n        if self.keep_header:\n            self.CT = ColumnTransformer(\n                [(\"OHE\", self.OHE, self.transformed_cols_)]\n                , remainder=\"passthrough\"\n                , sparse_threshold=0\n            )\n            self.CT.fit(X)\n            if len(self.transformed_cols_) > 0:\n                self.transformed_cols_new_ = \\\n                    self.CT.transformers_[0][1].get_feature_names()\n            else:\n                self.transformed_cols_new_ = []\n            self.header_new_ = np.concatenate(\n                (self.transformed_cols_new_,\n                 self.not_transformed_cols_)\n            )\n        else:\n            self.CT = ColumnTransformer(\n                [(\"OHE\", self.OHE, self.transformed_cols_)]\n                , remainder=\"passthrough\"\n            )\n            self.CT.fit(X)\n        return self\n\n    def transform(self, X, y=None):\n        transformed = self.CT.transform(X)\n        self.a_ = transformed\n        if self.keep_header:\n            return pd.DataFrame(data=transformed, columns=self.header_new_)\n        else:\n            return transformed\n        \nclass CustomizedScaler(BaseEstimator, TransformerMixin):\n    \"\"\"Performing standardization using sklearn's ``StandardScaler``\n    transformer but allowing column names to be retained\n    \"\"\"\n    def __init__(self, scaler=StandardScaler(), keep_header=True):\n        self.scaler,self.keep_header = scaler, keep_header\n    \n    def fit(self, X, y=None):\n        self.header = list(X.columns)\n        self.scaler.fit(X, y)\n        return self\n    \n    def transform(self, X, y=None):\n        transformed = self.scaler.transform(X)\n        if self.keep_header:\n            return pd.DataFrame(data=transformed, columns=self.header)\n        else:\n            return transformed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Constants\nADD_NN = False # whether to use a feedforward neural network\nALPHAS = [0.1, 0.3, 1, 3, 10]\nRS = 713\nKFOLDS = 3\ntest_features = [['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF'\n                  ,'BsmtFinType1', 'BsmtFinType2', 'BsmtUnfExp']\n                 , ['AllBath', 'BsmtHalfBath', 'BsmtFullBath'\n                    , 'HalfBath', 'FullBath']\n                 , ['GarageCars', 'GarageArea']\n                 , ['KitchenAbvGr']\n                 , ['PoolArea', 'PoolQC']\n                 , ['MiscFeature', 'MiscVal']\n                 , ['Utilities', 'Electrical', 'OverallCond', 'LowQualFinSF']\n                 , ['Age', 'YrSinceRemod']]\n\n# Functions to get transformer pipeline and estimators\n# For estimators, their hyperparams are chosen arbitrarily \n# and not selected for optimization\ndef get_pipe(additional_transform=[]):\n    transform = [(\"PreI\", PreProcessor(old_new, num_to_obj))\n                 , (\"RI\", RelationImputer(related = RELATED))]\n    transform += additional_transform\n    return Pipeline(transform)\n\ndef get_GBR():\n    return GradientBoostingRegressor(n_estimators=500\n                                     , max_depth=2\n                                     , max_features=0.8\n                                     , random_state=RS)\n\ndef get_ETR():\n    return ExtraTreesRegressor(n_estimators=700\n                               , max_depth=3\n                               , max_features=0.8\n                               , random_state=RS)\n\ndef get_RFR():\n    return RandomForestRegressor(n_estimators=700\n                                 , max_depth=3\n                                 , max_features=0.8\n                                 , max_samples=0.8\n                                 , random_state=RS)\n\ndef get_XGBR():\n    return XGBRegressor(n_estimators=500\n                        , learning_rate=0.1\n                        , max_depth=3\n                        , colsample_bytree=0.8\n                        , n_jobs=-1, random_state=RS)\n\ndef get_Ridge():\n    return RidgeCV(alphas=ALPHAS, normalize=False, store_cv_values=True)\n\ndef get_Enet():\n    return ElasticNetCV(l1_ratio=0.5\n                        , alphas=ALPHAS\n                        , normalize=False\n                        , max_iter=1000\n                        , random_state=RS\n                       )\n\ndef get_NN(hidden_layers=1, compiled=True):\n    NN = Sequential()\n    regu = regularizers.l1_l2(l1=0.01, l2=0.01)\n    for _ in range(hidden_layers):\n        NN.add(layers.Dense(100\n                            , activation=\"relu\"\n                            , kernel_regularizer=regu\n                            , bias_regularizer=regu))\n    NN.add(layers.Dense(1))\n    if compiled:\n        optimizer = optimizers.SGD(0.001)\n        NN.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"MSE\"])\n    return NN\n\ndef get_SVM():\n    return LinearSVR(random_state=RS, max_iter=1000)\n\ndef get_LGBR():\n    return LGBMRegressor(max_depth=3, n_estimators=500\n                         , random_state=RS, n_jobs=-1)\n\nestimators = [get_GBR, get_ETR, get_RFR, get_XGBR, get_Ridge\n              , get_Enet, get_SVM, get_LGBR]\nif ADD_NN:\n    estimators.add(get_NN)\n\n# Neural Network's fitting hyperparameters\nearly_stop = callbacks.EarlyStopping(monitor='val_loss', patience=5)\nNN_kwargs = dict(epochs=100\n                 , batch_size=10\n                 , validation_split=0.2\n                 , verbose=0\n                 , callbacks=[early_stop])\n\n# Functions to prepare visualization output\ndef RMSE(y_true, y_pred):\n    return math.sqrt(mean_squared_error(y_true, y_pred))\n# rmse_scorer = make_scorer(RMSE)\n\ndef reload_xy(msg=False):\n    df = reload(msg=msg)\n    X = df[[c for c in df.columns if c != \"SalePrice\"]].copy()\n    y = df[\"SalePrice\"].copy()\n    return X, np.array(np.log1p(y))\nX, y_trans = reload_xy()\n\ndef new_output(bl):\n    cols = [\"Est\", bl, \"Score_mean\", \"Score_delt(%)\"\n            , \"Score_std\", \"Time_run\", \"Time_delt(%)\"]\n    return pd.DataFrame(columns=cols)\n\ndef fit_timer(get_est, X, y, params={}):\n    e = get_est()\n    t = dt.now()\n    e.fit(X, y, **params)\n    return timer(t)\n        \ndef scoring(get_est, additional_transform\n            , X=X, y=y_trans, scorer=RMSE, splitter=KFold, cv=KFOLDS):\n    scores = []\n    splitter = splitter(cv, shuffle=True, random_state=713)\n    kw = {}\n    if get_est.__name__ == 'get_NN':\n        kw = NN_kwargs\n    for tr, te in splitter.split(X):\n        X_tr, X_te = X.iloc[tr, :], X.iloc[te, :]\n        y_tr, y_te = y[tr], y[te]\n        p = get_pipe(additional_transform)\n        e = get_est()\n        X_tr = p.fit_transform(X_tr)\n        X_te = p.transform(X_te)\n        e.fit(X_tr, y_tr, **kw)\n        y_pre = e.predict(X_te)\n        scores.append(RMSE(y_te, y_pre))\n    pipe = get_pipe(additional_transform)\n    X_trans = pipe.fit_transform(X)\n    t = fit_timer(get_est, X_trans, y_trans, params=kw)\n    return scores, t\n\ndef new_row(df, tactics, i, get_est, bl, scores, time):\n    scr_m, scr_std = np.mean(scores), np.std(scores)\n    est = get_est()\n    if (i % len(tactics)) == 0:\n        scr_d, time_d = np.nan, np.nan\n    else:\n        last_scr, last_t = df.loc[i-1, [\"Score_mean\", \"Time_run\"]]\n        scr_d = (scr_m - last_scr) / (last_scr) * 100\n        time_d = (t - last_t) / (last_t) * 100\n    output.loc[i] = [\n        type(est).__name__, bl, scr_m, scr_d, scr_std, time, time_d\n    ]\n    \ndef timer(t):\n    t = dt.now() - t\n    return t.seconds + t.microseconds/1e6\n\ndef for_loop(bl, estimators=estimators):\n    l = list(product(estimators, bl))\n    return tqdm(l, position=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"As shown below, the feature engineering suggestions prove effective in improving test score (lower score is better) in all 8 models.. With few dimensions (fewer categorical variables), training time also improves significantly. ","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Comparison & Output\noutput = new_output(\"Feature_Engineer?\")\ni = 0\ntactics = [False, True]\nfor get_est, ex in for_loop(tactics):\n    add_trans = [(\"FE\", FeatureEngineer(execute=ex))\n             , (\"CusOHE\", CustomizedOHE())\n             , (\"Stdize\", CustomizedScaler(keep_header=False))]\n    scores, t = scoring(get_est, add_trans)\n    new_row(output, tactics, i, get_est, ex, scores, t)\n    i += 1\noutput.round(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One thing to note is that the tree-based models took approximately 3-6 seconds to run (using CPU only). That is quite concerning given the fact that there are less than 1500 training samples. When we perform model selection and hyperparameter optimization, the running time is going to scale remarkably. It's better if we can somehow reduce the complexity of the models and save that precious training time, and one way is to **continue cutting down on number of features.**<br>\n<br>\nAmong the 84 features resulted from our feature engineering decisions, not all of them will be useful. Let's see if there are redundant features we can throw away. We will look at feature importances in the first 4 tree-based models mentioned above. It should be noted that this is not the optimal method (sometimes even misleading), so to be careful we will average the feature's importance scores across 4 models.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Less first re-train our ensemble models\nadd_trans = [(\"FE\", FeatureEngineer())\n             , (\"CusOHE\", CustomizedOHE())\n             , (\"Stdize\", CustomizedScaler())]\npipe = get_pipe(add_trans)\nX_trans = pipe.fit_transform(X, y_trans)\n# Function to return original features' names\ndef col_name(col, ohe_cols=pipe.steps[3][1].transformed_cols_):\n    mat = re.match(r\"(^x\\d{1,2})_(.+)\", col)\n    if mat:\n        return ohe_cols[int(mat.group(1)[1:])]\n    else:\n        return col\n    \nget_ests = [get_GBR, get_ETR, get_RFR, get_XGBR]\nfeat_imp = pd.DataFrame(\n    columns = [\"OHE_feat\", \"Features\"] \n    + [type(e()).__name__ for e in get_ests] \n    + [\"Average\"]\n)\nfeat_imp.OHE_feat = list(X_trans.columns)\nfeat_imp.Features = feat_imp.OHE_feat.apply(lambda x: col_name(x))\nfor get_est in get_ests:\n    est = get_est()\n    est.fit(X_trans, y_trans)\n    feat_imp[type(est).__name__[:-9]] = est.feature_importances_\nfeat_imp = feat_imp.groupby([\"Features\"]).sum().reset_index()\nfeat_imp[\"Average\"] = feat_imp.iloc[:,[1,2,3,4]].mean(axis=1)\nfeat_imp = feat_imp.sort_values(by=\"Average\", ascending=False)\nfeat_imp = feat_imp.reset_index(drop=True)\n\n# Feature importance examination\nn = len(feat_imp)\nall_columns = list(feat_imp.Features)\nweak_cols = list(feat_imp[feat_imp.Average < (1/n)].Features)\ninsig_cols = list(feat_imp[\n    (feat_imp.iloc[:,1:5] > (1/n)).sum(axis=1) == 0\n].Features)\nzero_cols = list(feat_imp[feat_imp.Average == 0].Features)\n\nprint(f\"Features with AVERAGE importance score < (1/n_features):\\\n    {len(weak_cols)}/{n}\")\nprint(*weak_cols[:5], sep=\", \", end=\", etc.\\n\\n\")\nprint(f\"Features with ALL importance scores < (1/len(features)):\\\n    {len(insig_cols)}/{n}\")\nprint(*insig_cols[:5], sep=\", \", end=\", etc.\\n\\n\")\nprint(f\"Features with 0 importance scores: {len(zero_cols)}/{n}\")\nprint(*zero_cols, sep=\", \")\n\nfeat_imp.to_csv(\"Column_importances.csv\", index=False)\na = feat_imp.iloc[:25,:]\nax = a.plot(y=[1,2,3,4], kind=\"line\", figsize=(13,7), rot=90)\na.plot(x=\"Features\", y=\"Average\"\n       , kind=\"bar\", ax=ax, color=\"lightsteelblue\"\n       , title=\"Top 25 Most Important Features\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will look at the features that we deemed necessary to be tested earlier.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(f\"Average importance: {round(1/n,3)}\")\nfor c in test_features:\n    print(\"\\n\" + \"=\"*20)\n    print(feat_imp[feat_imp.Features.isin(c)].round(3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can draw the following conclusions from above:\n- `BsmtUnfExp` is redundant and should not be created.\n- `BsmtUnfSF`, `BsmtFinType2` and `BsmtFinSF2` should be removed.\n- Should drop `BsmtHalfBath`, `BsmtFullBath`, `HalfBath`\n- Keep both garage area features\n- `KitchenAbvGr` can be dropped\n- `PoolQC` should be dropped\n- `MiscFeature` and `MiscVal` should be dropped\n- `Electrical`, `LowQualFinSF` and `Utilities` should be dropped\n- Keep both `Age` and `YrSinceRemod`","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class ColumnDropper(BaseEstimator, TransformerMixin):\n    col_imp = pd.read_csv(\"/kaggle/working/Column_importances.csv\")\n    all_cols = list(col_imp.Features)\n    n = len(all_cols)\n    weak_cols = list(col_imp[col_imp.Average < (1/n)].Features)\n    insig_cols = list(col_imp[\n        (col_imp.iloc[:,1:5] > (1/n)).sum(axis=1) == 0\n    ].Features)\n    insig2_cols = list(col_imp[\n        (col_imp.iloc[:,1:5] > (1/n/10)).sum(axis=1) == 0\n    ].Features)\n    zero_cols = list(col_imp[col_imp.Average == 0].Features)\n    \n    def __init__(self, strategy=None, others=None):\n        \"\"\" ``strategy``: int, \"weak\", \"insignificant\" or \"zero\".\n        Default None.\n            ~ int then drop all columns ranked (starting from 0)\n                ``cols_drop`` or below\n            ~ \"weak\" then drop all weak_cols.\n            ~ \"insignificant\" then drop all insig_cols.\n            ~ \"insignificant2\"\n            ~ \"zero\" then drop all zero_cols\n        \n        ``others``: list of other columns to be dropped\n        \"\"\"\n        self.strategy = strategy\n        self.others = others\n        if isinstance(strategy, (int,np.integer)):\n            self.columns = self.all_cols[strategy:]\n        elif strategy == \"weak\":\n            self.columns = self.weak_cols\n        elif strategy == \"insignificant\":\n            self.columns = self.insig_cols\n        elif strategy == \"insignificant2\":\n            self.columns = self.insig2_cols            \n        elif strategy == \"zero\":\n            self.columns = self.zero_cols\n        elif strategy is None:\n            self.columns = []\n        else:\n            raise Exception(f\"Wrong 'strategy' input: \\\"{strategy}\\\"\")\n        if others:\n            self.columns += others\n            self.columns = list(set(self.columns))\n    \n    def fit(self, X, y=None):\n        if X.shape[1] != self.n:\n            msg = f\"Wrong number of features: expecting {self.n}, got {X.shape[1]}\"\n            raise Exception(msg)\n        missing_cols = [c for c in X.columns if c not in self.all_cols]\n        if len(missing_cols) > 0:\n            msg = \"Columns not existing: \" + \", \".join(missing_cols)\n            raise Exception(msg)\n        else:\n            return self\n    def transform(self, X, y=None):\n        if len(self.columns) > 0:\n            X = X.drop(columns=self.columns)\n        return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's test and see if our decisions will improve the scores.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cols_to_drop = [\"BsmtUnfExp\", \"BsmtUnfSF\", \"BsmtFinType2\"\n                , \"BsmtFinSF2\", \"BsmtHalfBath\", \"BsmtFullBath\"\n                , \"HalfBath\", \"KitchenAbvGr\", \"PoolQC\"\n                , \"MiscFeature\", \"MiscVal\", \"Electrical\"\n                , \"LowQualFinSF\", \"Utilities\"]\n\noutput = new_output(\"Drop?\")\ni = 0\ntactics = [None, cols_to_drop]\nfor get_est, drop in for_loop(tactics):    \n    add_trans = [(\"FE\", FeatureEngineer())\n                 , (\"Drop\", ColumnDropper(others=drop))\n                 , (\"CusOHE\", CustomizedOHE())\n                 , (\"Stdize\", CustomizedScaler(keep_header=False))]\n    scores, t = scoring(get_est, add_trans)\n    d = True if drop else False\n    new_row(output, tactics, i, get_est, d, scores, t)\n    i += 1\noutput.round(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the mean scores (lower is better) all improve slightly. More importantly, fitting time (lower is better) also shows great improvement. Therefore, we will drop the columns mentioned above.<br>\n<br>\n**Is it possible to drop more columns and shorten running time even further?**<br>\n<br>\nWe may try dropping all 0-importance columns, or we can even think about keeping only top 10, top 20 most important features, etc. There is an **abundant** number of options for us to explore (2^`n_features` to be exact), and it's impossible to test all of them.<br>\n<br>\nTo find the number of features/which specific columns to drop, I will perform a simple method: add features one by one, from the most to the least important, and observe how the `RMSE` score and training time change.<br>\n<br>\nNote that this might not be the best method to select features as any two features can interact in some way that we don't know. It, however, should serve as a good guess for us to grasp (hopefully) some of the most predictive variables from the dataset.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"scrs = {}\ntime = {}\ntactics = np.arange(84, 0, -1)\nfor get_est, tac in for_loop(tactics):\n    dropper = ColumnDropper(strategy=tac, others=cols_to_drop)\n    add_trans = [(\"FE\", FeatureEngineer())\n                 , (\"Drop\", dropper)\n                 , (\"CusOHE\", CustomizedOHE())\n                 , (\"Stdize\", CustomizedScaler(keep_header=False))]\n    scores, t = scoring(get_est, add_trans)\n    est_name = type(get_est()).__name__\n    if est_name in scrs:\n        scrs[est_name].append(np.mean(scores))\n        time[est_name].append(t)\n    else:\n        scrs[est_name] = [np.mean(scores)]\n        time[est_name] = [t]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"PLOT_CHANGE = False\ndef running_delt(l, transform=PLOT_CHANGE):\n    pre = np.nan\n    o = []\n    for nex in l:\n        o.append((nex-pre)/pre)\n        pre = nex\n    return o\nplt.style.use(\"seaborn-whitegrid\")\nfig, (ax1,ax2) = plt.subplots(2,1,figsize=(14,10))\nest_names = [type(e()).__name__ for e in estimators]\n\ncolors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"olive\", \"k\", \"darkorange\", \"saddlebrown\"]\nfor n, c in zip(est_names, colors):\n    ax1.plot(tactics, scrs[n], c, label=n)\n    ax2.plot(tactics, time[n], c=c)\nax1.set_ylabel(\"Mean score\")\nax2.set_xlabel(\"Features being kept\")\nax2.set_ylabel(\"Train time\")\nax1.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Generally, dropping variables always significantly shortens training time, but removing too much information will hurt model's performance. Based on the graphs above:\n- Most models stop improving from after 20 features are added.\n- Our A-players, including `GradientBoostingRegressor`, `XGBRegressor` and `LGBMRegressor` continue to improve as more features are added, up until around the 40-th most important feature. After that, they are mostly noises.\n- Running time increases roughly proportionally with number of features for most models, as expected.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Given these observations, we will drop the bottom half of the features (meaning that we keep only top 42/84 features).","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"DROP = 42 # Keeping only top ``DROP`` most important features\ntrain = reload(msg=False)\nX_train = train[[c for c in train.columns if c != \"SalePrice\"]].copy()\ny_train = train[\"SalePrice\"].copy()\nX_test = reload(\"test.csv\", msg=False)\nadd_trans = [(\"FE\", FeatureEngineer())\n    , (\"Drop\", ColumnDropper(strategy=DROP, others=cols_to_drop))\n    , (\"CusOHE\", CustomizedOHE())]\npipe = get_pipe(add_trans)\nX_train = pipe.fit_transform(X_train)\nX_test = pipe.transform(X_test)\n\ndropped_cols = pipe.steps[3][1].columns\nprint(f\"Columns dropped: {len(dropped_cols)}/84\")\nprint(\"Examples: \" + \", \".join(dropped_cols[:6]) + \", etc.\")\n\nsubmission = reload(\"sample_submission.csv\", dropped_columns=[], msg=False)\nest = get_XGBR()\nest.fit(X_train, np.log1p(y_train))\nsubmission[\"SalePrice\"] = np.expm1(est.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X_train[\"SalePrice\"] = train[\"SalePrice\"]\nsubmission.to_csv(\"Submission.csv\", index=False)\nX_train.to_csv(\"train_preprocessed.csv\", index=False)\nX_test.to_csv(\"test_preprocessed.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}