{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Pipeline playground for 12+ classifiers (0.11860 LB)\nI want to **provide a clean, simple and beginner friendly template** that makes use of a flexible [scikit-learn pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). I aim to **automate the datacleaning and feature engineering** as much as possible while **allowing for fast iteration**. \n\nThe **total training and processing time of the pipeline for 12 classifiers is around 20secs** (w/o scoring the models) and gets you **0.11860 on the Leaderboard** with practically no fuss."},{"metadata":{},"cell_type":"markdown","source":"## Imports and globals ü§ñ\nFirst we **import all the necessary libraries** and **set a base file path to the data sets**."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.svm import LinearSVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nBASE_PATH = \"/kaggle/input/house-prices-advanced-regression-techniques/\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Superquick intro: What is a Pipeline?\nA pipeline is a [supercool class that scikit-learn provides](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), which allows to chain so called transformers (that ‚Äì uhmmm.... ‚Äì transform your data) with a final estimator at the end. Let's look a a simple example.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(f\"{BASE_PATH}train.csv\")\nX = df.select_dtypes(\"number\").drop(\"SalePrice\", axis=1)\ny = df.SalePrice\npipe = make_pipeline(SimpleImputer(), RobustScaler(), LinearRegression())\nprint(f\"The R2 score is: {cross_val_score(pipe, X, y).mean():.4f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"darkred\">How cool is that? \n> With **only 5 lines of code we imported our training data, separated describing features from the target variable, setup a pipeline with an Imputer (that fills in missing values), a Scaler and a LinearRegression classifier. We crossvalidated and printed out the result.** \n    \nIt can't be easier than that I think...  \n\nNow let's setup a pipeline that is able to **work on our categorical data as well.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = df.drop(\"SalePrice\", axis=1).select_dtypes(\"number\").columns\ncat_cols = df.select_dtypes(\"object\").columns\n\n# we instantiate a first Pipeline, that processes our numerical values\nnumeric_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer()),\n        ('scaler', RobustScaler())])\n\n# the same we do for categorical data\ncategorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n    \n# a ColumnTransformer combines the two created pipelines\n# each tranformer gets the proper features according to ¬´num_cols¬ª and ¬´cat_cols¬ª\npreprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, num_cols),\n            ('cat', categorical_transformer, cat_cols)])\n\npipe = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', LinearRegression())])\n\nX = df.drop(\"SalePrice\", axis=1)\ny = df.SalePrice\nprint(f\"The R2 score is: {cross_val_score(pipe, X, y).mean():.4f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"darkred\">Even cooler... \n> With **only 9 lines of code** we processed all our features automagically. Our score improves accordingly.\n    \nNow we expand this to a playground for all the regression classifiers you can think of."},{"metadata":{},"cell_type":"markdown","source":"## Choosing the estimators ü§ì\nSince this is meant as a sandbox for experimentation we set a list with 10 common classifiers (and their respective names) which we will use in our pipeline. \n\nThe initial hyperparameters I have [grid-searched](https://www.kaggle.com/chmaxx/extensive-data-exploration-modelling-python)."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# comment out all classifiers that you don't want to use\n# and do so for clf_names accordingly\nclassifiers = [\n               DummyRegressor(),\n               LinearRegression(n_jobs=-1), \n               Ridge(alpha=0.003, max_iter=30), \n               Lasso(alpha=.0005), \n               ElasticNet(alpha=0.0005, l1_ratio=.9),\n               KernelRidge(alpha=0.6, kernel=\"polynomial\", degree=2, coef0=2.5),\n               SGDRegressor(),\n               SVR(kernel=\"linear\"),\n               LinearSVR(),\n               RandomForestRegressor(n_jobs=-1, n_estimators=350, \n                                     max_depth=12, random_state=1),\n               GradientBoostingRegressor(n_estimators=500, max_depth=2),\n               lgb.LGBMRegressor(n_jobs=-1, max_depth=2, n_estimators=1000, \n                                 learning_rate=0.05),\n               xgb.XGBRegressor(objective=\"reg:squarederror\", n_jobs=-1, \n                                max_depth=2, n_estimators=1500, learning_rate=0.075),\n]\n\nclf_names = [\n            \"dummy\", \n            \"linear\", \n            \"ridge\",\n            \"lasso\",\n            \"elastic\",\n            \"kernlrdg\",\n            \"sgdreg\",\n            \"svr\",\n            \"linearsvr\",\n            \"randomforest\", \n            \"gbm\", \n            \"lgbm\", \n            \"xgboost\"\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setting up the Pipeline üí°\nWe will now setup simple functions to:\n*     **clean and prepare the data**\n*     **build the Pipeline**     \n*     **score models** \n*     **train models**\n*     **predict from models**"},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:darkgreen\">üêö Encapsulate all our feature cleaning and engineering \nTo experiment, just add your code to this function.</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_data(data, is_train_data=True):\n    # add your code for data cleaning and feature engineering here\n    # e.g. create a new feature from existing ones\n    data['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\n\n    # add here the code that you only want to apply to your training data and not the test set\n    # e.g. removing outliers from the training data works... \n    # ...but you cannot remove samples from your test set.\n    if is_train_data == True:\n        data = data[data.GrLivArea < 4000]\n        \n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:darkgreen\">üë∑‚Äç‚ôÇÔ∏è Prepare our data for the pipeline "},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_data(df, is_train_data=True):\n    \n    # split data into numerical & categorical in order to process seperately in the pipeline \n    numerical   = df.select_dtypes(\"number\").copy()\n    categorical = df.select_dtypes(\"object\").copy()\n    \n    # for training data only...\n    # ...convert SalePrice to log values and drop \"Id\" and \"SalePrice\" columns\n    if is_train_data == True :\n        SalePrice = numerical.SalePrice\n        y = np.log1p(SalePrice)\n        numerical.drop([\"Id\", \"SalePrice\"], axis=1, inplace=True)\n        \n    # for the test data: just drop \"Id\" and set \"y\" to None\n    else:\n        numerical.drop([\"Id\"], axis=1, inplace=True)\n        y = None\n    \n    # concatenate numerical and categorical data to X (our final training data)\n    X = pd.concat([numerical, categorical], axis=1)\n    \n    # in addition to X and y return the separated columns to use these separetely in our pipeline\n    return X, y, numerical.columns, categorical.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:darkgreen\">üë∑‚Äç‚ôÇÔ∏è Create the pipeline "},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_pipeline(classifier, num_cols, cat_cols):\n    # the numeric transformer gets the numerical data acording to num_cols\n    # the first step is the imputer which imputes all missing values to the mean\n    # in the second step all numerical data gets scaled by the StandardScaler()\n    numeric_transformer = Pipeline(steps=[\n        ('imputer', make_pipeline(SimpleImputer(strategy='mean'))),\n        ('scaler', StandardScaler())])\n    \n    # the categorical transformer gets all categorical data according to cat_cols\n    # again: first step is imputing missing values and one hot encoding the categoricals\n    categorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n    \n    # the column transformer creates one Pipeline for categorical and numerical data each\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, num_cols),\n            ('cat', categorical_transformer, cat_cols)])\n    \n    # return the whole pipeline with the classifier provided in the function call    \n    return Pipeline(steps=[('preprocessor', preprocessor), ('classifier', classifier)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span style=\"color:darkgreen\">üå° Score the models with crossvalidation "},{"metadata":{"trusted":true},"cell_type":"code","source":"def score_models(df):\n    # retrieve X, y and the seperate columns names\n    X, y, num_cols, cat_cols = prepare_data(df)\n    \n    # since we converted SalePrice to log values, we use neg_mean_squared_error... \n    # ...rather than *neg_mean_squared_log_error* \n    scoring_metric = \"neg_mean_squared_error\"\n    scores = []\n    \n    for clf_name, classifier in zip(clf_names, classifiers):\n        # create a pipeline for each classifier\n        clf = get_pipeline(classifier, num_cols, cat_cols)\n        # set a kfold with 3 splits to get more robust scores. \n        # increase to 5 or 10 to get more precise estimations on models score\n        kfold = KFold(n_splits=3, shuffle=True, random_state=1)  \n        # crossvalidate and return the square root of the results\n        results = np.sqrt(-cross_val_score(clf, X, y, cv=kfold, scoring=scoring_metric))\n        scores.append([clf_name, results.mean()])\n\n    scores = pd.DataFrame(scores, columns=[\"classifier\", \"rmse\"]).sort_values(\"rmse\", ascending=False)\n    # just for good measure: add the mean of all scores to dataframe\n    scores.loc[len(scores) + 1, :] = [\"mean_all\", scores.rmse.mean()]\n    return scores.reset_index(drop=True)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span style=\"color:darkgreen\">  üèãÔ∏è‚Äç‚ôÇÔ∏è Finally: Train the models\nFor each classifier we create and fit a pipeline."},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_models(df): \n    X, y, num_cols, cat_cols = prepare_data(df)\n    pipelines = []\n    \n    for clf_name, classifier in zip(clf_names, classifiers):\n        clf = get_pipeline(classifier, num_cols, cat_cols)\n        clf.fit(X, y)\n        pipelines.append(clf)\n    \n    return pipelines","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span style=\"color:darkgreen\">üîÆ Make predictions with trained models  \nFor each fitted pipeline we retrieve predictions for SalePrice"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_from_models(df_test, pipelines):\n    X_test, _ , _, _ = prepare_data(df_test, is_train_data=False)\n    predictions = []\n    \n    for pipeline in pipelines:\n        preds = pipeline.predict(X_test)\n        # we return the exponent of the predictions since we have log converted y for training\n        predictions.append(np.expm1(preds))\n    \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## üöÄ And now: Let's use our pipeline... \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(f\"{BASE_PATH}train.csv\")\ndf_test = pd.read_csv(f\"{BASE_PATH}test.csv\")\n\n# We clean the data\ndf = clean_data(df)\ndf_test = clean_data(df_test, is_train_data=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We score the models on the preprocessed training data\nmy_scores = score_models(df)\ndisplay(my_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We train the models on the whole training set and predict on the test data\nmodels = train_models(df)\npredictions = predict_from_models(df_test, models)\n# We average over the results of all 12 classifiers (simple ensembling)\n# we exclude the DummyRegressor and the SGDRegressor: they perform worst...\nprediction_final = pd.DataFrame(predictions[2:]).mean().T.values\n\nsubmission = pd.DataFrame({'Id': df_test.Id.values, 'SalePrice': prediction_final})\nsubmission.to_csv(f\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Have feedback? Found errors? Please let me know in the comments. üëåüòé"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}