{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Advanced House Pricing Prediction : EDA & Modelling\n\n![](https://storage.googleapis.com/kaggle-competitions/kaggle/5407/logos/front_page.png)\n\n\nI have recently started working on AMES Housing Prices Regression Dataset. This notebook showcases some of the exploratory analysis, data visualization, data processing, missing value treatment, tree based models and model blending etc. I have applied on the datasets. Please provide your feedback in the comments.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Table of Contents - \n* [Extrapolatory Analysis & Data Transformation](#DataTransformations)\n* [Feature Engineering](#feature)\n* [Model Development ](#model)\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Extrapolatory Analysis\n\n\n## Importing the Raw Datasets<a name=\"DataTransformations\"></a>\n\nThe Competition primarily provides two datasets, training data and testing dataset. The data is split 50-50 between training and testing datasets, each dataset containing 1460 records. Each record corresponds to a single transaction for a house purchase, the dependent variable is **SalePrice**, the price at which house was sold. There are about 79 independent variables capturing different pieces of information about a house being sold - area, location, number of rooms etc. \n\nLet's start off by importing the datasets and having a quick glance at the data. \n\n","execution_count":null},{"metadata":{"_uuid":"3f4565e5-f6c8-4de0-934b-d3443b4ee434","_cell_guid":"cf967491-0899-487e-9cdf-5fc9a1cf3dfd","trusted":true,"scrolled":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#Viz libraries used in the notebook\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport matplotlib.pyplot as plt                    \nimport matplotlib\n\n\n#Importing sklearn libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor #Random forest libraries\nfrom sklearn.model_selection import cross_validate #cross validation\nfrom sklearn.impute import SimpleImputer           #Treatment of missing values\nfrom sklearn.preprocessing import OrdinalEncoder   #Ordinal Encoder package\nfrom sklearn.preprocessing import LabelEncoder     #For Label Encoding\nfrom sklearn.metrics import mean_squared_log_error #Mean Squared Log Error metric from sklearn\nimport xgboost as xgb                              #XGboost package\nfrom sklearn.model_selection import GridSearchCV   #Grid search for finding out the best package\n\n\n#AV = AutoViz_Class()\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\ntrain=pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest=pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n\ntrain","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79695a61-dc5c-4d88-9670-43cb27615476","_cell_guid":"baa779d6-a1d8-4771-88cb-a6a32b104d01","trusted":true,"scrolled":false},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some things immediately stand out from the outputs above - \n\n* The dataset is very rich in terms of the number of properties available for a house, everything from construction material, housetype, garage, basement, neighborhood properties etc. are covered in the dataset\n* The ID column is the primary key in the dataset,it is a unique differentiator for every house in the dataset. There are 1460 records in the training dataset, each corresponding to a single house\n* **LotArea** variable corresponds to the area of the property. There is a huge difference in the minimum and maximum values for the Lot Area, ranging from 1300 sq ft. to 215245 sq. ft\n* There are several Quality variables indicating the quality of overall house, Exteriors, Kitchens, Garage etc. These variables have values on a 1-10 scale\n* **YrBuilt** has values from 1872 to 2010, showing that there is a lot of variation in age of the house at the time of sale\n* **YrSold** variables refers to the year in which house was sold, it shows that sales transactions are in between **2006-2010**, so a fairly short time period. With such a short time range  we can discount the impact of inflation or any other external factors on house prices\n* There are several categorical variables as well like Neighborhood, Lot Shape, Street, Condition etc. We will analyze these variables separately\n* **SalePrice** shows the price of house sold in Dollars. The value of SalePrice ranges from 34900 to 755,000. This is the dependent variable in this compeition. We will soon be having a more detailed look at this variable. \n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Dependent Variable - SalePrice","execution_count":null},{"metadata":{"_uuid":"1055e7cd-3ab1-4b11-894e-ed5fb283cb5d","_cell_guid":"70e5e504-e903-4dbf-a55a-fa7d8e3c6a96","trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"train['logSalePrice']=np.log(train.SalePrice) \n\nfig = px.histogram(train, x=\"SalePrice\",title='Distribution of SalePrice',height=400)\nfig.show()\n\nfig1 = px.violin(train, x=\"SalePrice\",title='Violin Plot for SalePrice',height=300)\nfig1.update_traces(box_visible=True, meanline_visible=True)\nfig1.show()\n\n\nfig2 = px.violin(train, x=\"logSalePrice\",title='Violin Plot for Log(SalePrice)',height=300)\nfig2.update_traces(box_visible=True, meanline_visible=True)\nfig2.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the plots above show, histogram of SalePrice variable shows that values are not equally distributed around the mean, there is a **right skew** in the distribution. Plotting the Violin plot for SalePrice shows us that the mean is around 173k and third quartile value is at 213k, but there are quite a few values in the long tail with SalePrice values in excess of 300k. These values are also pulling the mean away from Median.\n\nWe will be training linear regression & tree based models for predicting the SalePrice values later in the notebook, therefore these outlier values can cause lot of issues in model training and generate lot of variance for model predictions. One of the common ways to deal with this issue is to log transform the SalePrice values to reduce the skew of the distribution. We did this and plotted the distribution for log(SalePrice), it seems to be centered around mean and normally distributed.\n\nIf your variable is skewed, high values will affect the variances and push your split points towards higher values - forcing your decision tree to make less balanced splits and trying to \"isolate\" the tail from the rest of the points.Link below provides more details on impact of outlier values on Tree based models - \n\nhttps://stats.stackexchange.com/questions/447863/log-transforming-target-var-for-training-a-random-forest-regressor\n ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Univariate Analysis - Relationship between Independent variables on SalePrice\n\nAfter having understood basic structure of the training dataset and the target variable SalePrice, let's now look at relationship between individual variables in the training dataset. We will break this analysis into two parts - first we will be looking at relationship between continous variables and target variable, and then we would perform the same analysis for categorical variables and target variable. First let's start off by looking at the count of missing values for all the columns in the training dataset and then we will look at distributions.\n","execution_count":null},{"metadata":{"_uuid":"96fe5bf5-c5ad-4133-9ab6-28f0c923853c","_cell_guid":"3f35492d-4f4f-46e3-be7d-b0fd71d8da1d","trusted":true,"scrolled":false},"cell_type":"code","source":"#Missing Values for Categorical Variables\nk=train.isnull().sum()\nk[k>0].sort_values(ascending=False)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e18632b-6246-4e48-9c61-5d1efbc99b40","_cell_guid":"38f3ffe3-af3a-445f-90c5-8d8f8992bade","trusted":true,"scrolled":false},"cell_type":"code","source":"numcols=train._get_numeric_data().columns\ntrain_num=train.loc[:,numcols]\ncols=train_num.columns\ntup=tuple(cols[1:])\n#train_num\ncols=train_num.columns\n\nfig = make_subplots(\n    rows=9, cols=4,shared_yaxes=True,subplot_titles=tup)\n#    subplot_titles=(\"Plot 1\", \"Plot 2\", \"Plot 3\", \"Plot 4\"))\nk=1\nj=1\n\nfor i in range(1,36):\n#    print(k,j)\n    fig.add_trace(\n    go.Scatter(y=train_num['SalePrice'], x=train_num.iloc[:,i],mode='markers',name=cols[i]),\n    row=k, col=j\n    )\n#    k=k%8\n    j=j%4\n    j=j+1\n    \n#    k=k+1\n    if(i%4==0):\n        k=k+1\n      #  j=j+1\n\n        \nfig.update_layout(height=1800, width=800,\n                  title_text=\"Dependency between SalePrice & Continous Variables\",showlegend=False)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First thoughts - that's a lot of Violin plots to look at - let's look at them in detail and summarise our findings - \n* Y Axis on each of the violin plots above is the SalePrice variable and x axis represents the variable being plotted. As is to be expected, not all variables have a correlation with SalePrice, for quite a few variables, there seems to be no dependency with SalePrice\n* **OverallQual** variable seems to be much more positively correlated with SalePrice than **OverallCond**\n* **Lot Area** does not seem to have any correlation with SalePrice, SalePrice seems to vary wildly for same values of Lot Area\n* **GrLivArea,TotalBsmtSF & 1stFlrSF** all look to have a positive linear correlation with SalePrice\n* Houses built more recently sell for higher values, which is intuitive because newly built houses would be expected to fetch higher values than older houses\n* SalePrice tends to generally increase with increase in **TotRmsAbvGrd**\n* **Avg. SalePrice** increases with increase in number of bathrooms\n* SalePrice seems to increase with increase in **GarageArea**\n* Majority of houses don't have a swimming pool\n* The **month and year** of purchase does not seem to have any impact on SalePrice\n\nFirst glance at this distribution gives out several ideas for Feature Engineering as well, wherein lot of individual variables don't have a high correlation with SalePrice, but when combined with other variables could generate meaningful features. Here are some examples - \n* Combining the bathroom count variables for different floors & basement to derive total bathrooms\n* Total area in a house combining area for different floors and basements\n* Age of a house at sales using YrSold and YrBuilt values\n* Combining area for different porch variables\n\nWe will be focusing on this soon after we've completed the extrapolatory analysis and are getting ready for model building. Let's now get back to extrapolatory analysis. ","execution_count":null},{"metadata":{"_uuid":"9961a499-1165-4c05-a056-5ba952b7b25a","_cell_guid":"9e66a1b7-d862-4800-9301-2ace2832ab00","trusted":true,"scrolled":true},"cell_type":"code","source":"#fig=go.Figure()\ntrain_str_data=train.select_dtypes(include='object')\ntrain_str_data['SalePrice']=train['SalePrice']\n\ncols=train_str_data.columns\ntup=tuple(cols[1:])\nfig = make_subplots(\n    rows=10, cols=4,shared_yaxes=True,subplot_titles=tup)\n\nrow=1\ncol=1\n\nfor i in range(1,41):\n    uniqvals=train_str_data.iloc[:,i].unique() \n    lenvals=len(uniqvals)\n    for j in range(lenvals):\n        k=train_str_data.iloc[:,i]==uniqvals[j]\n        df=train_str_data.loc[k]\n        fig.add_trace(go.Violin(x=df.iloc[:,i],\n                        y=df['SalePrice']),row=row,col=col)\n    \n    col=col%4\n    col=col+1\n    if(i%4==0):\n        row=row+1\n    #print(row,col)\nfig.update_traces(box_visible=True, meanline_visible=True)\nfig.update_layout(height=2800, width=800,\n                  title_text=\"Dependency between SalePrice & Categorical Variables\",showlegend=False)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar to Continous variables in the train dataset, we've now plotted categorical variables against SalePrice. We have plotted the distribution of SalePrice for each of the distinct values for a categorical variable and analyzing difference in mean value of SalePrice for all the different values of a categorical variable. Below are some of the key findings - \n* Similar to continous variables, there are a lot of categorical variables where there is no difference between mean SalePrice for different distinct values of a categorical variable. In fact an initial look suggests that majority of categorical variables don't have any relationship with SalePrice\n* Average value of SalePrice varies drastically with changes in **Neighborhood**. This is intuitive and expected as well, since high income neighborhoods in a city generally command a higher price\n* Variables with **Qual and Cond** suffixes seem to have variation in SalePrice for different values. The distinct values for these variables range from Excellent to Poor.These variables can also be classified as Ordinal categorical variables, since there is an intrinsic order between them\n* SalePrice seems to vary to an extent with different types of **RoofType and RoofMatl** as well, however certain values of **RoofMatl** have very small violins(small number of records)\n* **Built in Garages** command higher price than other types\n\nSimilar to the violin plots for continous variables, these violin plots also give us an indication on how to perform feature engineering for categorical variables. Here are some ideas - \n* Ordinal categorical variables for various quality measures can be converted to numeric variables\n* For variables with lot of distinct levels, we can combine them to reduce number of levels\n\nLet's now have a deeper look at distribution of prices for Neighborhood variable. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Variation of SalePrice by Neighborhood","execution_count":null},{"metadata":{"_uuid":"feb0eb1a-2931-4bd3-a87b-e1660a084bd9","_cell_guid":"75d35e72-be76-4e51-8ca2-1db12c7fb568","trusted":true,"scrolled":false},"cell_type":"code","source":"fig = px.violin(train, y=\"SalePrice\", x=\"Neighborhood\", box=True, color='Neighborhood',title='Distribution of SalePrice by Neighborhood')\nfig.update_layout(showlegend=False)\nfig.show()\n\nneighborhood_md=train.groupby('Neighborhood').agg({'SalePrice':'median'}).reset_index().sort_values(by='SalePrice',ascending=False)\nneighborhood_md\n\nfig1 = px.bar(neighborhood_md, y=\"SalePrice\", x=\"Neighborhood\",color='SalePrice',title='Median SalePrice by Neighborhood')\nfig1.update_layout(showlegend=False)\nfig1.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is a huge variance in the median SalePrice across different neighborhoods, it can vary from 300k+ for NridgHt to less than 100k for MeadowV\n* Even within a single neighborhood the SalePrice values can sometimes be spread across a big range\n* There are total of about 25 distinct values for Neighborhood variable, this can be a problem while training the model, since creating dummy variables from Neigborhood would add 25 additional variables, and since the number of records in training data are only about 1460, this would cause the Curse of Dimensionality. Therefore we can group the Neighborhood values into different groups based on the median SalePrice in the Neighborhood\n","execution_count":null},{"metadata":{"_uuid":"daed05fb-a7b3-44ef-b22e-d195a82b5101","_cell_guid":"c395a376-f384-4e97-bb03-1047320caed3","trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"\n# zoning=train.groupby('MSZoning').agg({'SalePrice':'median'}).reset_index().sort_values(by='SalePrice',ascending=False)\n# neighborhood_md\n\n# fig1 = px.bar(zoning, y=\"SalePrice\", x=\"MSZoning\",color='SalePrice',title='Median SalePrice by MSZoning')\n# fig1.update_layout(showlegend=False)\n# fig1.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"853d76a2-bc44-4d90-a63d-165dff38cd2c","_cell_guid":"de7a663c-9094-4064-9d36-6ff82bd48c46","trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"# exterior=train.groupby('Exterior1st').agg({'SalePrice':'median'}).reset_index().sort_values(by='SalePrice',ascending=False)\n\n# fig = px.bar(exterior, y=\"SalePrice\", x=\"Exterior1st\",color='SalePrice',title='Median SalePrice by Exterior1st')\n# fig.update_layout(showlegend=False)\n# fig.show()\n\n# house=train.groupby('HouseStyle').agg({'SalePrice':'median'}).reset_index().sort_values(by='SalePrice',ascending=False)\n\n# fig1 = px.bar(house, y=\"SalePrice\", x=\"HouseStyle\",color='SalePrice',title='Median SalePrice by HouseStyle')\n# fig1.update_layout(showlegend=False)\n# fig1.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Condition1 and Condition 2 : Impact of Proximity to various conditions on SalePrice","execution_count":null},{"metadata":{"_uuid":"8a5a006e-6cbb-420f-b7b4-e8263d8a1c15","_cell_guid":"234a053c-66ad-4918-827e-4a9a7c9b3586","trusted":true,"scrolled":false},"cell_type":"code","source":"condition=train.groupby(['Condition1','Condition2']).agg({'SalePrice':'median','Utilities':'count'}).reset_index().sort_values(by='SalePrice',ascending=False)\ntrans=condition.pivot(index='Condition1',columns='Condition2',values='SalePrice')\ntrans=trans.fillna(0)\ncm = sns.light_palette(\"green\", as_cmap=True)\ns = trans.style.background_gradient(cmap=cm)\ns\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"037d06f1-dd88-449b-828d-05dd498183ea","_cell_guid":"a9553e64-85e2-4fd9-872b-963d5cb05d2a","trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"# condition.loc[condition['SalePrice']<100000,'condition_flag']=0\n# condition.loc[condition['SalePrice']>=100000,'condition_flag']=1\n# condition.loc[condition['SalePrice']>=150000,'condition_flag']=2\n# condition.loc[condition['SalePrice']>=200000,'condition_flag']=3\n# del condition['SalePrice']\n#condition\n\ncondition=condition.rename(columns={\"SalePrice\": \"avg_sale_price_cond\",\"Utilities\": \"total_records\"})\ncondition.sort_values(by='total_records',ascending=False,inplace=True)\ncondition","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen above in the table, prices of a house varies a great deal based on proximity to different features-\n* Houses Adjacent to Feeder Streets have low prices\n* Houses nearby or adjacent to positive offsite features like parks etc. have a higher price\n* Houses close to East-West Railroad have low selling prices\n\nWe also looked at count of unique records across different values of Condition1 - Condition 2 to check if we can look at distinct values to extract information around pricing. As it turns out close to 90% of records in train dataset have both Condition1 and Condition 2 as Norm, which tells us that it does not make sense to create features using distinct values for Conditions","execution_count":null},{"metadata":{"_uuid":"53734733-b139-4003-adf5-35fd5d575c02","_cell_guid":"2f624b36-0d0e-4226-9de9-a92ea587cb03","trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"# exterior.loc[exterior['SalePrice']>=110000,'ExteriorFlag']=0\n# exterior.loc[exterior['SalePrice']>=125000,'ExteriorFlag']=1\n# exterior.loc[exterior['SalePrice']>=130000,'ExteriorFlag']=2\n# exterior.loc[exterior['SalePrice']>=150000,'ExteriorFlag']=3\n# exterior.loc[exterior['SalePrice']>=200000,'ExteriorFlag']=4\n\n# #exterior['ExteriorFlag']=int(exterior['ExteriorFlag'])\n# del exterior['SalePrice']\n# #exterior","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4dcd8e8-d1b5-4887-911e-fc179369336b","_cell_guid":"b676d3f5-6314-466d-a83a-926d2cf4a378","trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"# house.loc[house['SalePrice']>=110000,'StyleFlag']=1\n# house.loc[house['SalePrice']>=150000,'StyleFlag']=2\n# house.loc[house['SalePrice']>=180000,'StyleFlag']=3\n# #exterior['ExteriorFlag']=int(exterior['ExteriorFlag'])\n# del house['SalePrice']\n# #exterior","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Month & Year of Sale : Checking for Seasonality in House Sales\n\n\n","execution_count":null},{"metadata":{"_uuid":"f8dc8ea2-dce1-441f-bd18-8a7cce69c422","_cell_guid":"a3750fa9-d640-49fa-b96a-35834fb17366","trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"gp=train.groupby(['YrSold','MoSold']).agg({'SalePrice':'median','LotArea':'count'}).reset_index()\ngp['MoSold'] = gp['MoSold'].apply(str)\ngp['YrSold'] = gp['YrSold'].apply(str)\n\ngp['month_year']=gp['YrSold']+\"-\"+gp['MoSold']\ngp.columns=['YrSold','MoSold','SalePrice','SaleCount','month_year']\n\n# fig=px.line(gp,x='month_year',y='SalePrice')\n# fig.show()\n\n\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n# Add traces\nfig.add_trace(\n    go.Bar(x=gp['month_year'], y=gp['SalePrice'], name=\"SalePrice\"),\n    secondary_y=False,\n)\n\nfig.add_trace(\n    go.Scatter(x=gp['month_year'],y=gp['SaleCount'], name=\"SaleCount\"),\n    secondary_y=True,\n)\n\nfig.update_layout(title_text=\"Median SalePrice and SaleCount over Time\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We looked at median SalePrices for different months and years in the SalePrice to check if houses sell for a greater price in a specific month of the year or during a certain year. Also 2008 was the year of Global Housing Crisis, we also wanted to check if house prices collapsed during the year. \n\nEven though the count of Sales transactions does go up and down each year, there is hardly enough variation in SalePrice across different months to justify that the SalePrices go up/down at any specific time of the year. ","execution_count":null},{"metadata":{"_uuid":"cb5dd8bf-36ac-4634-9a8d-0666965210d6","_cell_guid":"153963f6-6217-403a-8a48-f2380b2c4449","trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"# gp.MoSold=pd.to_numeric(gp.MoSold, errors='coerce')\n# gp.YrSold=pd.to_numeric(gp.YrSold, errors='coerce')\n# gp=gp[['YrSold','MoSold','SaleCount']]\n\n#train=pd.merge(train,gp,left_on=['MoSold','YrSold'],right_on=['MoSold','YrSold'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering<a name=\"feature\"></a>\n\nFeature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data. The features in your data will directly influence the predictive models you use and the results you can achieve.\n\nHaving had a look at all the different variables present in the dataset, we will now start to look at creating new features out of existing features to extract more meaningful information from the variables and improve model performance. We will use two basic methods to create new features - \n1. Apply a mathematical operation to combine different continous variables\n2. Assign an intrinsic order or combine different levels for categorical variables\n","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#Feature Engineering\n#Grouping neighborhoods based on value of median SalePrice for the neighborhood\nneighborhood_md.loc[neighborhood_md['SalePrice']>=5000,'neighborhood_flag']=0\nneighborhood_md.loc[neighborhood_md['SalePrice']>=150000,'neighborhood_flag']=1\nneighborhood_md.loc[neighborhood_md['SalePrice']>=200000,'neighborhood_flag']=2\nneighborhood_md.loc[neighborhood_md['SalePrice']>=250000,'neighborhood_flag']=3\ndel neighborhood_md['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ba9cc60-f3ce-48a1-ab42-960dfcc6a507","_cell_guid":"4b266daf-cf5c-4749-a0aa-5e7599077894","trusted":true,"scrolled":false},"cell_type":"code","source":"neighborhood_md.columns=['Neighborhood','neighborhood_flag']\ntrain=pd.merge(train,neighborhood_md,left_on='Neighborhood',right_on='Neighborhood')\n\n#train=pd.merge(train,exterior,left_on='Exterior1st',right_on='Exterior1st')\n#train=pd.merge(train,house,left_on='HouseStyle',right_on='HouseStyle')\n#train=pd.merge(train,condition,left_on=['Condition1','Condition2'],right_on=['Condition1','Condition2'])\n\ntest=pd.merge(test,neighborhood_md,left_on='Neighborhood',right_on='Neighborhood')\n\n#test=pd.merge(test,exterior,left_on='Exterior1st',right_on='Exterior1st')\n#test=pd.merge(test,house,left_on='HouseStyle',right_on='HouseStyle')\n#test=pd.merge(test,condition,left_on=['Condition1','Condition2'],right_on=['Condition1','Condition2'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08b1caf2-3e20-481e-b202-35cd0e367767","_cell_guid":"607ca21d-8f79-4427-9b93-f7ca12996819","trusted":true,"scrolled":false},"cell_type":"code","source":"#Feature Engineering for Continous Variables\ntrain['tot_bath']=train['BsmtFullBath']+0.5*train['BsmtHalfBath']+train['FullBath']+0.5*train['HalfBath']\ntrain['bsmt_bath']=train['BsmtFullBath']+0.5*train['BsmtHalfBath']\n\ntrain['bed_bath_kitch']=train['tot_bath']+train['BedroomAbvGr']+train['KitchenAbvGr']\ntrain['area_floors']=train['1stFlrSF']+train['2ndFlrSF']+train['BsmtFinSF1']+train['BsmtFinSF2']\ntrain['bsmt_by_total']=(train['BsmtFinSF1']+train['BsmtFinSF2'])/(train['area_floors'])\ntrain['unf_bsmt']=(train['BsmtUnfSF']/train['TotalBsmtSF']).fillna(0)\ntrain[\"unf_bsmt\"].replace([np.inf, -np.inf], 0)\n\ntrain['porch_area_tot']=train['OpenPorchSF']+train['EnclosedPorch']+train['3SsnPorch']+train['ScreenPorch']\ntrain['wood_deck_porch']=(train['WoodDeckSF']/train['porch_area_tot']).replace(np.inf, 0)\ntrain['sale_built_yr']=train['YrSold']-train['YearBuilt']\ntrain['remod_built_yr']=train['YearRemodAdd']-train['YearBuilt']\ntrain['new_flag']=0\ntrain.loc[train['YrSold']-train['YearBuilt'],'new_flag']=1\ntrain['remod_flag']=0\ntrain.loc[train['remod_built_yr']>=2,'remod_flag']=1\ntrain['floor_by_lot']=train['area_floors']/train['LotArea']\n\ntrain['garage_area_per_car']=(train['GarageArea']/train['GarageCars']).fillna(0)\ntrain[\"garage_area_per_car\"]=train[\"garage_area_per_car\"].replace([np.inf, -np.inf], 0)\n\n\ntest['tot_bath']=test['BsmtFullBath']+0.5*test['BsmtHalfBath']+test['FullBath']+0.5*test['HalfBath']\ntest['bsmt_bath']=test['BsmtFullBath']+0.5*test['BsmtHalfBath']\ntest['bed_bath_kitch']=test['tot_bath']+test['BedroomAbvGr']+test['KitchenAbvGr']\ntest['area_floors']=test['1stFlrSF']+test['2ndFlrSF']+test['BsmtFinSF1']+test['BsmtFinSF2']\ntest['bsmt_by_total']=(test['BsmtFinSF1']+test['BsmtFinSF2'])/(test['area_floors'])\ntest['unf_bsmt']=(test['BsmtUnfSF']/test['TotalBsmtSF']).fillna(0)\ntest[\"unf_bsmt\"].replace([np.inf, -np.inf], 0)\n\ntest['porch_area_tot']=test['OpenPorchSF']+test['EnclosedPorch']+test['3SsnPorch']+test['ScreenPorch']\ntest['wood_deck_porch']=(test['WoodDeckSF']/test['porch_area_tot']).replace(np.inf, 0)\ntest['sale_built_yr']=test['YrSold']-test['YearBuilt']\ntest['remod_built_yr']=test['YearRemodAdd']-test['YearBuilt']\ntest['new_flag']=0\ntest.loc[test['YrSold']-test['YearBuilt']==0,'new_flag']=1\ntest['remod_flag']=0\ntest.loc[test['remod_built_yr']>=2,'remod_flag']=1\ntest['floor_by_lot']=test['area_floors']/test['LotArea']\n\ntest['garage_area_per_car']=(test['GarageArea']/test['GarageCars']).fillna(0)\ntest[\"garage_area_per_car\"]=test[\"garage_area_per_car\"].replace([np.inf, -np.inf], 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6927ea6-f976-4b4a-badf-57994fe905a5","_cell_guid":"a0ed1e03-b4bb-4f24-ae54-eefe0c06b851","trusted":true,"scrolled":false},"cell_type":"code","source":"mapping = { \"NA\" : 0,\"Po\" : 1,\"Fa\":2,\"TA\" : 3,\"Gd\":4,\"Ex\":5 }\nmapping_shape = { \"Reg\" : 0,\"IR1\" : 1,\"IR2\":2,\"IR3\" : 3}\nmapping_contour = { \"Bnk\" : 0,\"Lvl\" : 1,\"Low\":2,\"HLS\" : 3}\nmapping_ms_zoning={\"FV\":4,\"RL\":3,\"RH\":2,\"RM\":1,\"C (all)\":0,\"NA\":-1}\nmapping_paved_Drive={\"Y\":2,\"P\":1,\"N\":0}\nmapping_utilities={\"AllPub\":1,\"NoSeWa\":0,\"NoSeWr\":0,\"ELO\":0,\"NA\":0}\nmapping_functional={\"Typ\":4,\"Min1\":3,\"Min2\":3,\"Mod\":2,\"Maj1\":1,\"Maj2\":1,\"Sev\":0,\"Sal\":0,\"NA\":0}\n\n#mapping_cond={\"RRNn\":2,\"PosN\":1,\"PosA\":0,\"Artery\":,\"\"}\n\nl_col=['BsmtCond','BsmtQual','ExterQual','ExterCond','HeatingQC','GarageQual','GarageCond','FireplaceQu','KitchenQual']\nl_col_zon=['MSZoning']\nl_col_pav=['PavedDrive']\nl_col_shape=['LotShape']\nl_col_util=['Utilities']\nl_col_con=['LandContour']\nl_col_fun=['Functional']\n\n#l_col_cond=['Condition1']\ndef mapping_var(mapping,df,varlist):\n    \n    for i in range(len(varlist)):\n        varname=varlist[i]\n        varname_o=varname+'_o'\n        df[varname]=df[varname].fillna('NA')\n        df[varname_o] =df[varname].apply(lambda x : mapping[x])\n    return df\n\ntrain=mapping_var(mapping,train,l_col)\ntrain=mapping_var(mapping_ms_zoning,train,l_col_zon)\ntrain=mapping_var(mapping_paved_Drive,train,l_col_pav)\ntrain=mapping_var(mapping_shape,train,l_col_shape)\ntrain=mapping_var(mapping_utilities,train,l_col_util)\ntrain=mapping_var(mapping_contour,train,l_col_con)\ntrain=mapping_var(mapping_functional,train,l_col_fun)\n\n\ntest=mapping_var(mapping,test,l_col)\ntest=mapping_var(mapping_ms_zoning,test,l_col_zon)\ntest=mapping_var(mapping_paved_Drive,test,l_col_pav)\ntest=mapping_var(mapping_shape,test,l_col_shape)\ntest=mapping_var(mapping_utilities,test,l_col_util)\ntest=mapping_var(mapping_contour,test,l_col_con)\ntest=mapping_var(mapping_functional,test,l_col_fun)\n\nlabelencoder = LabelEncoder()\ntrain.loc[:, 'Street'] = labelencoder.fit_transform(train.loc[:, 'Street'])\ntrain.loc[:, 'CentralAir'] = labelencoder.fit_transform(train.loc[:, 'CentralAir'])\n\ntest.loc[:, 'Street'] = labelencoder.fit_transform(test.loc[:, 'Street'])\ntest.loc[:, 'CentralAir'] = labelencoder.fit_transform(test.loc[:, 'CentralAir'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5ea3b4d-d872-4d6b-814f-839e5ec85248","_cell_guid":"f192afeb-7680-477b-a729-02fe06208b61","trusted":true,"scrolled":false},"cell_type":"code","source":"train['tot_quality']=train['BsmtQual_o']+train['ExterQual_o']+train['KitchenQual_o']+train['GarageQual_o']+train['FireplaceQu_o']\ntest['tot_quality']=test['BsmtQual_o']+test['ExterQual_o']+test['KitchenQual_o']+test['GarageQual_o']+test['FireplaceQu_o']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is a summary of all the new variables created above - \n* Neighborhood values are grouped into different groups based on value of SalePrice\n* Total bathrooms column based on total number of full and half bathrooms in different parts of house\n* Total number of bathrooms and kitchen in the house\n* Total house area combining total area of different floors and basement\n* Portion of basement that is unfinished\n* Whether a house is newly built or old\n* Age of the house at the time of Sale\n* Converting categorical variables to numeric by assigning a numeric value to different lables and assigning an intrinsic order\n* Combining different quality variables to create a total quality variable\n\nLet's now look at Correlation matrix between all the variables and SalePrice to check if all the old and new numeric variables have a correlation with dependent variable Sale Price and also to check correlation between different independent variables.","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#Correlation matrix for Variables\nnumcols=train._get_numeric_data().columns\ntrain_num=train.loc[:,numcols]\n\ncorr = train_num.corr(method='pearson')\n# corr\nf, ax = plt.subplots(figsize=(25, 25))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.8, cbar_kws={\"shrink\": .5})\n\n#ax = sns.heatmap(corr,linewidths=0.8,cmap=cmap)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Sale Price seems to be highly correlated to neighborhood flag, tot_quality, OverallQual\n* OverallQual is mildly correlated to individual quality variables for Bsmt, Exterior, Garage etc.\n* As expected, MoSold and YrSold aren't correlatd with SalePrice\n* Lot Area is weakly correlated with SalePrice\n* Pool and Porch related variables are not correlated with SalePrice\n\n### Multicollinearity\nWe will also be checking for multicollinearity in the independent variables in the dataset. According to Wikipedia - Multicollinearity is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.\nMulticollinearity can be problematic for linear regression because it can give unreliable coefficients for correlated variables\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check for variable combinations where correlation is greater than 80%\nk=corr.unstack().sort_values().drop_duplicates()\nk[(k>0.8) | (k<-0.8)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"During feature engineering, some of the new variables created are highly correlated to the base variables. for ex. bsmt_by_total has more than 80% positive correlation with BsmtFinSF1 and GarageCars has a 88% correlation with GarageArea. During model training, we should ensure that we should only take one variable out of the correlated pair for model training, since they are both adding the same effect in model. \n\nAs a next step we will be combining the train and test datasets and identifying the input variables with a skew. We will then be taking a log transformation of variables with a skew. ","execution_count":null},{"metadata":{"_uuid":"270625a8-eaf6-40ca-b9a4-b96b967e0871","_cell_guid":"51845761-a748-4a47-b664-9352d81f387b","trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"#pd.get_dummies[train,['Neighborhood']]\n#train.columns\n#train=pd.get_dummies(data=train, columns=['Neighborhood'])\n#test=pd.get_dummies(data=test, columns=['Neighborhood'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3fed0082-6031-46b2-bf4d-9bb027ae9cc2","_cell_guid":"125fcb29-7676-4040-af44-4aca342a1f94","trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"#train=train.loc[train['SalePrice']<500000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"test.sort_values(by='Id',inplace=True)\ntest\ntrain.sort_values(by='Id',inplace=True)\n\nsalePrice=train['SalePrice']\ndel train['logSalePrice']\ndel train['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train['flag']='train'\ntest['flag']='test'\nall_data = pd.concat([train, test], ignore_index=True, sort=False)\n#all_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a262831f-227a-4bd2-93f3-6c471aef600a","_cell_guid":"6c24a521-55b6-41c4-87b2-52e30fedeb64","trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"from scipy.stats import skew\n\n#log transform skewed numeric features:\nnumeric_feats = all_data.dtypes[train.dtypes != \"object\"].index\n#numeric_feats\n\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.5]\nskewed_feats = skewed_feats.index\n#skewed_feats=skewed_feats.drop('SalePrice')\nall_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n#all_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"skewed_feats","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6641ea3-4bd7-4060-b7aa-dc9b2a512891","_cell_guid":"de444528-468d-4ad7-9e67-b60758864a88","trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"#df['feature'] = df['feature'].replace(-np.inf, np.nan)\n\n#type(skewed_feats)\n#\nnumcols=all_data._get_numeric_data().columns\nall_data_num=all_data.loc[:,numcols]\n\nk=all_data_num.columns.to_series()[np.isinf(all_data_num).any()]\n#test['remod_built_yr'].unique()\nall_data[k] = all_data[k].replace(-np.inf, np.nan)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building <a name=\"model\"></a>\nIn this section, we will look to build out different models for predicting house prices on the data. We will be training three different models, RandomForest, XGBoost and Lasso and we will be analyzing how the different models perform on the training dataset. \n\nFirst let's start with Missing Value Treatment. For sake of simplicity, we will be using the Simple Imputer in scikit-learn to subsitute missing values in features with the respective mean for the column. \n\nNext we will create a basic Random Forest model and plot feature importance graph to understand which of the variables are important in model prediction. \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(all_data.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data_mod=all_data[['LotArea','GrLivArea','TotRmsAbvGrd','OverallQual','OverallCond','YearBuilt','YearRemodAdd','TotalBsmtSF','BsmtQual_o','ExterQual_o','GarageCars','1stFlrSF','2ndFlrSF','FullBath','FireplaceQu_o','KitchenQual_o','MSZoning_o','PavedDrive_o','LotShape_o', 'Utilities_o', 'LandContour_o', 'Functional_o','tot_bath','bed_bath_kitch','area_floors','GarageArea','Fireplaces','BedroomAbvGr','tot_quality','sale_built_yr','remod_built_yr','MSSubClass','neighborhood_flag','MasVnrArea','garage_area_per_car','floor_by_lot','remod_flag','new_flag','unf_bsmt','bsmt_by_total','bsmt_bath','flag','BsmtFinSF1','BsmtFinType2','BsmtFinSF2','BsmtUnfSF']]\nall_data_mod","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78db2937-cd22-4a48-80f5-5331f4d1ea54","_cell_guid":"a661dc15-e0e7-4655-882f-9c73b2914fcb","trusted":true,"scrolled":false},"cell_type":"code","source":"# train_mod=train[['LotArea','GrLivArea','TotRmsAbvGrd','OverallQual','OverallCond','YearBuilt','YearRemodAdd','TotalBsmtSF','BsmtQual_o','ExterQual_o','GarageCars','1stFlrSF','2ndFlrSF','FullBath','FireplaceQu_o','KitchenQual_o','MSZoning_o','tot_bath','bed_bath_kitch','area_floors','GarageArea','Fireplaces','BedroomAbvGr','tot_quality','sale_built_yr','remod_built_yr','MSSubClass','neighborhood_flag','MasVnrArea','garage_area_per_car','floor_by_lot','remod_flag','new_flag','unf_bsmt','bsmt_by_total','bsmt_bath']]\n# test_mod=test[['LotArea','GrLivArea','TotRmsAbvGrd','OverallQual','OverallCond','YearBuilt','YearRemodAdd','TotalBsmtSF','BsmtQual_o','ExterQual_o','GarageCars','1stFlrSF','2ndFlrSF','FullBath','FireplaceQu_o','KitchenQual_o','MSZoning_o','tot_bath','bed_bath_kitch','area_floors','GarageArea','Fireplaces','BedroomAbvGr','tot_quality','sale_built_yr','remod_built_yr','MSSubClass','neighborhood_flag','MasVnrArea','garage_area_per_car','floor_by_lot','remod_flag','new_flag','unf_bsmt','bsmt_by_total','bsmt_bath']]\nall_data_mod=all_data[['LotArea','GrLivArea','TotRmsAbvGrd','OverallQual','OverallCond','YearBuilt','YearRemodAdd','TotalBsmtSF','BsmtQual_o','ExterQual_o','GarageCars','1stFlrSF','2ndFlrSF','FullBath','FireplaceQu_o','KitchenQual_o','MSZoning_o','PavedDrive_o','LotShape_o', 'Utilities_o', 'LandContour_o', 'Functional_o','tot_bath','bed_bath_kitch','area_floors','GarageArea','Fireplaces','BedroomAbvGr','tot_quality','sale_built_yr','remod_built_yr','MSSubClass','neighborhood_flag','MasVnrArea','floor_by_lot','remod_flag','new_flag','unf_bsmt','bsmt_by_total','bsmt_bath','flag','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF']]\n# Missing Value Treatment using Simple Imputer\ntrain_mod=all_data_mod.loc[all_data_mod['flag']=='train',:]\ntest_mod=all_data_mod.loc[all_data_mod['flag']=='test',:]\ndel train_mod['flag']\ndel test_mod['flag']\nmy_imputer = SimpleImputer(missing_values=np.nan,strategy='mean')\ntrain_imp = my_imputer.fit_transform(train_mod)\ntest_imp = my_imputer.fit_transform(test_mod)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"# train_mod=train[['LotArea','tot_bath','sale_built_yr','TotalBsmtSF','YearBuilt','1stFlrSF','GarageArea','KitchenQual_o','GarageCars','neighborhood_flag','GrLivArea','ExterQual_o','tot_quality','area_floors','OverallQual']]\n# test_mod=test[['LotArea','tot_bath','sale_built_yr','TotalBsmtSF','YearBuilt','1stFlrSF','GarageArea','KitchenQual_o','GarageCars','neighborhood_flag','GrLivArea','ExterQual_o','tot_quality','area_floors','OverallQual']]\n\n# from sklearn.impute import SimpleImputer\n# my_imputer = SimpleImputer(missing_values=np.nan,strategy='mean')\n# train_imp = my_imputer.fit_transform(train_mod)\n# test_imp = my_imputer.fit_transform(test_mod)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d56f80d-b807-4805-b55c-bc5a46ad8d63","_cell_guid":"4be4a22e-ebb4-43e1-94d0-d4cc02e49452","trusted":true,"scrolled":false},"cell_type":"code","source":"X=train_imp\ny=np.log1p(salePrice)#log of dependent variable\n#y=train['SalePrice']\n#Split train data into train and test datasets\nX_train,X_test,Y_train,Y_Test=train_test_split(X, y,test_size=0.3,random_state=1)\n\n#Defining a basic Random Forest model with 100 trees \nregressor = RandomForestRegressor(n_estimators=100, random_state=1,max_depth=10,max_features=10,min_samples_leaf=5)  \nfeature_list=train_mod.columns\n\n# # fit the regressor with X and Y data \nmodel=regressor.fit(X_train,Y_train) \n\n#Make predictions on test dataset\npred_rf=model.predict(X_test)\n#X_test['pred']=pred\n#X_test['actual']=Y_Test\n\n#Calculate Cross Validation results\ncv_results = cross_validate(regressor, X_train, Y_train, cv=5,scoring='r2')\nsorted(cv_results.keys())\n\nscoreOfModel = model.score(X_test, Y_Test)\nprint(\"RSquared value for Model\",scoreOfModel)\n\n#Calculate Feature Importances\nfeat_importances = pd.Series(model.feature_importances_, index=feature_list)\nfeat_importances=feat_importances.sort_values()\nfeat_importances.plot(kind='barh',figsize=(12,9))\n\n#x.style.background_gradient(cmap = 'Wistia')\n#Print Root Mean Squared Log Error\nprint(\"RMSLE for Model\",np.sqrt(mean_squared_log_error(np.exp(Y_Test),np.exp(pred_rf) )))\n#print(np.sqrt(mean_squared_log_error(Y_Test,pred_rf )))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d59992d5-721f-4259-8b1b-1d5c42df7814","_cell_guid":"43e40889-8509-4d85-9e52-36ed76ee544b","trusted":true,"scrolled":false},"cell_type":"code","source":"print(\"Cross Validation Results\")\nprint(cv_results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our initial model had a RMSLE of 0.159 and RSquared values of 0.86, which is not too bad. We also applied Cross validation checks to avoid Overfitting and found out that different cross validation scores are quite close to each other, indicating that the model is not overfitting and doing well on records it has not seen before. Most important part of the output above is the Feature Importance Graph - \n* **OverallQual** had the highest feature importance, followed by area_floors\n* Our variables created through Feature Engineering **neighborhood_flag** and **tot_quality** also did quite well and had the third highest feature importance\n* Some of the variables like **TotRmsAbvGrd, bsmt_bath, Fireplaces** did not have a lot of importance in the model. \n\nWith this initial understanding on the variables, let's now use Grid Search to identify the best possible set of hyperparameters which will give us the best results. ","execution_count":null},{"metadata":{"_uuid":"67821aa8-0a27-44ab-88fa-a99d1d0c6bd7","_cell_guid":"231f9c19-7076-4cd2-b4f5-fe3dbf88e837","trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"pred=model.predict(test_imp)\n#pred=best_grid_rf.predict(test_imp)\nprint(len(pred))\n#test.shape\n#test_mod.shape\nId=pd.Series(range(1461,2920))\n#pred\n#Id\nd = {'Id': Id, 'SalePrice': pred}\ndf = pd.DataFrame(data=d)\ndf['SalePrice']=np.exp(df['SalePrice'])\ndf\n\ndf.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b4117b8-115f-45b9-b775-2dd91613382a","_cell_guid":"d701261a-56e6-4f6e-bcc3-a36128f60c33","trusted":true,"scrolled":false},"cell_type":"code","source":"\n# Create the parameter grid based on the results of Grid search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [5, 3,7,10,15],\n    'max_features': [5, 12,10,8],\n  #  'max_leaf_nodes': [4, 8, 16,32],\n    'min_samples_split': [5, 10, 8,3],\n    'n_estimators': [100, 200,150,300]\n}\nrf = RandomForestRegressor()\ngrid_search_rf = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 5, n_jobs = -1, verbose = 2)\n# Fit the grid search to the data\ngrid_search_rf.fit(X_train,Y_train)\ngrid_search_rf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59153030-a546-43d4-bb25-8c4d0141a3e4","_cell_guid":"08527391-2bed-4c70-9bd6-11b0cbdcec71","trusted":true,"scrolled":false},"cell_type":"code","source":"def evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    errors = abs(predictions - test_labels)\n    mape = 100 * np.mean(errors / test_labels)\n    accuracy = 100 - mape\n    print('Model Performance')\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    \n    return accuracy","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"345291ce-6231-4e69-a6d9-f094e19e1c19","_cell_guid":"19998537-f5f6-4f60-a4b4-dfaf9ff31f08","trusted":true,"scrolled":false},"cell_type":"code","source":"best_grid_rf = grid_search_rf.best_estimator_\n# grid_accuracy = evaluate(best_grid, X_test, Y_Test)\n# grid_accuracy\n\npred_gs_rf=best_grid_rf.predict(X_test)\n\nscoreOfModel = best_grid_rf.score(X_test, Y_Test)\nprint(\"Rqusared for the Model\",scoreOfModel)\n\npred_gs_rf=best_grid_rf.predict(X_test)\n\nprint(\"RMSLE for the Model\",np.sqrt(mean_squared_log_error(np.exp(Y_Test),np.exp(pred_gs_rf) )))\n#print(np.sqrt(mean_squared_log_error(Y_Test,pred_gs_rf)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4dc30518-9b72-4758-a8ee-e191900e57c9","_cell_guid":"3ba18010-72b6-4d92-bebd-d592664da4ba","trusted":true,"scrolled":false},"cell_type":"code","source":"actuals=Y_Test\npredictions=pred_gs_rf\n\nactuals=np.exp(Y_Test)\npredictions=np.exp(pred_gs_rf)\nresiduals=predictions-actuals\n\n\nplt.scatter(predictions, residuals)\nplt.xlabel(\"Predicted SalePrice\")\nplt.ylabel(\"Residuals\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plt.scatter(actuals,predictions)\nplt.ylabel(\"Predicted SalePrice\")\nplt.xlabel(\"Actual SalePrice\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Grid Search model does only slightly better than the basic RandomForest model, which is an indication that we need to further tune the hyperparameters to improve model performance. We also plotted the Fitted vs Residual plot and Actual vs Predicted plots to measure model performance across different values of dependent variable. The Variance for Residuals is quite low for fitted values less than 250k but increases substantially for values greater than 250k, indicating that our model isn't performing very well on higher values of SalePrice ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### XGBoost ","execution_count":null},{"metadata":{"_uuid":"28f8744a-6dd1-429c-8877-1d277deb7ce8","_cell_guid":"92d8d0a4-aad6-456c-b5d6-f9e72fb31a80","trusted":true,"scrolled":false},"cell_type":"code","source":"import xgboost\nxgb = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n                           colsample_bytree=1, max_depth=7)\nxgb.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1024978f-1ab2-428d-a7bf-9aba33112798","_cell_guid":"eb151e7a-77dd-49d1-9072-84abf7cf1343","trusted":true,"scrolled":false},"cell_type":"code","source":"param_grid = {\n    'max_depth': [8,10,5],\n    'learning_rate': [0.05, 0.1, 0.3],\n    'colsample_bytree': [0.5, 0.7],\n    'lambda':[0.75,1],\n  #  'max_leaves':[4,8,16],\n    'min_child_weight':[2,4,6],\n    'subsample': [0.5, 0.7, 0.8],\n    'n_estimators': [150, 200,250,300]\n}\nxg = xgboost.XGBRegressor()\ngrid_search_xg = GridSearchCV(estimator = xg, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\ngrid_search_xg.fit(X_train,Y_train)\nbest_grid_xg=grid_search_xg.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6ec2c20-ce96-4d39-ba05-bf68492e7988","_cell_guid":"9a1b50a8-2f2d-4fb9-8d69-fd8e88d0a844","trusted":true,"scrolled":false},"cell_type":"code","source":"best_grid_xg=grid_search_xg.best_params_\n#xgboost.plot_importance(best_grid_xg,importance_type='gain')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"706c6add-6cc7-4aca-8922-e046e2f92b54","_cell_guid":"d444f37d-e4cc-47df-b402-588b04d3f2ce","trusted":true,"scrolled":false},"cell_type":"code","source":"from sklearn.metrics import explained_variance_score\n\nbest_grid_xg = grid_search_xg.best_estimator_\n\npred_gs_xg = best_grid_xg.predict(X_test)\n#print(explained_variance_score(predictions,y_test))\nprint(np.sqrt(mean_squared_log_error(np.exp(Y_Test), np.exp(pred_gs_xg) )))\nprint(explained_variance_score(pred_gs_xg,Y_Test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"875eb8fd-4dca-47f0-8371-d06c643af2ef","_cell_guid":"4cfacb24-bfab-42a5-8058-f7a94c2f1b63","trusted":true,"scrolled":false},"cell_type":"code","source":"actuals=np.exp(Y_Test)\n\nimport matplotlib.pyplot as plt\n\npredictions=np.exp(pred_gs_rf)\nresiduals=predictions-actuals\n\nplt.scatter(predictions, residuals)\nplt.xlabel(\"SalePrice\")\nplt.ylabel(\"Residuals\")\nplt.show()\n\npredictions=np.exp(pred_gs_xg)\nresiduals=predictions-actuals\n\nplt.scatter(predictions, residuals)\nplt.xlabel(\"SalePrice\")\nplt.ylabel(\"Residuals\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression\nAfter fitting XGBoost & Random Forest, both of which are tree based models, we look to fit Regularized linear models Lasso & Ridge Regression. Both Lasso & Ridge Regression require a learning parameter, alpha as an input. Similar to the tree models above, we would be identifying the best possible value of this parameter through the Grid Search. Next we would be looking at Residual plots and see if these models perform differently than tree based models. \n\n\n[Here](https://www.google.com/search?q=analytics+vidhya+lasso+regression&rlz=1C1CHBF_enIN875IN875&oq=analytics+vidhya+lasso+regression&aqs=chrome..69i57j35i39j0l3j69i60l3.6383j0j7&sourceid=chrome&ie=UTF-8) is a good guide on Lasso & Ridge Regression and how they prevent overfitting. \n\n\n### Lasso Regression\n","execution_count":null},{"metadata":{"_uuid":"ff5dcfa1-d613-43d2-89d0-33eb30e1170f","_cell_guid":"ea8ccc2d-9f2a-417e-b67b-3d78fbcb2476","trusted":true,"scrolled":false},"cell_type":"code","source":"#Lasso Regression - Select the best learning parameter with Grid Search\nfrom sklearn.linear_model import Lasso,LassoCV\nlasso=Lasso(max_iter=10000)\nparameters={'alpha': [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5]}\nlasso_regressor=GridSearchCV(lasso,parameters,scoring='r2',cv=5)\nlasso_regressor.fit(X_train,Y_train)\n\n#model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y)\nbest_grid_lasso=lasso_regressor.best_params_\n\nprint(lasso_regressor.best_params_)\nprint(\"R Squared value for the model\",lasso_regressor.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5a809ff-f3a5-4926-9503-4409ec460204","_cell_guid":"db9c0d3d-41dd-4916-9d47-ad88a23f5739","trusted":true,"scrolled":false},"cell_type":"code","source":"lasso_model=Lasso(alpha=0.0001,max_iter=10000).fit(X_train,Y_train)\npred_gs_lasso=lasso_model.predict(X_test)\nprint(\"RMSLE for the model\",np.sqrt(mean_squared_log_error(np.exp(Y_Test), np.exp(pred_gs_lasso) )))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d9fe16a-2dfc-4235-a2db-9d2497f0dd0e","_cell_guid":"62b524bc-876d-42a1-bf85-175542613ca7","trusted":true,"scrolled":false},"cell_type":"code","source":"coef = pd.Series(lasso_model.coef_, index = train_mod.columns)\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51be9ae0-7670-4126-a662-7e7a4a1b0074","_cell_guid":"1f66f2c7-e7a4-486f-9173-dcee5aa14846","trusted":true,"scrolled":false},"cell_type":"code","source":"import matplotlib\nimp_coef = pd.concat([coef.sort_values().head(10),\n                     coef.sort_values().tail(10)])\n\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71ffe15e-1a9a-4db3-a189-b11ec74b5fd4","_cell_guid":"a3e6b6d3-f522-49fe-92eb-8239633982ed","trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"actuals=np.exp(Y_Test)\n\npredictions=np.exp(pred_gs_lasso)\nresiduals=predictions-actuals\n\nplt.scatter(predictions, residuals)\nplt.xlabel(\"SalePrice\")\nplt.ylabel(\"Residuals\")\nplt.show()\n\nplt.scatter(predictions, actuals)\nplt.xlabel(\"SalePrice\")\nplt.ylabel(\"Actuals\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ridge Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"####Lasso Regression\nfrom sklearn.linear_model import Ridge,RidgeCV\nridge=Ridge(max_iter=10000)\nparameters={'alpha': [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5]}\nridge_regressor=GridSearchCV(ridge,parameters,scoring='r2',cv=5)\nridge_regressor.fit(X_train,Y_train)\n\n#model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y)\nbest_grid_ridge=ridge_regressor.best_params_\n\nprint(ridge_regressor.best_params_)\nprint(\"R Squared value for the model\",ridge_regressor.best_score_)\n\nridge_model=Ridge(alpha=0.3,max_iter=10000).fit(X_train,Y_train)\npred_gs_ridge=ridge_model.predict(X_test)\n#print(explained_variance_score(predictions,y_test))\nprint(\"RMSLE for the model\",np.sqrt(mean_squared_log_error(np.exp(Y_Test), np.exp(pred_gs_lasso) )))\n\n\ncoef = pd.Series(ridge_model.coef_, index = train_mod.columns)\nprint(coef)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"actuals=np.exp(Y_Test)\n\npredictions=np.exp(pred_gs_ridge)\nresiduals=predictions-actuals\n\nplt.scatter(predictions, residuals)\nplt.xlabel(\"SalePrice\")\nplt.ylabel(\"Residuals\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here are some of the observations on Lasso & Ridge models above - \n* Both models are quite simple compared to ensemble models like Random Forests & XG Boost where we are fitting hundreds of trees to fit the training dataset. Even though these models are simpler, the **RMSLE metric for Lasso & Ridge Regression are comparable to tree based models**\n* Lasso Regression prevents overfitting by setting coefficients of irrelevant variables to zero, in this instance **Lasso picked 40 variables and eliminated the other 3 variables**\n* Ridge Regression on the other hand does not explicitly set the coefficients of irrelevant variables to zero, but brings them quite close to zero. Based on grid search results, the resultant value of **alpha was quite high for Ridge Regression compared to Lasso Regression**, hence Ridge model was quite aggressive in preventing overfitting and set the coefficients for higher number of variables close to zero\n* Similar to tree based models, both of these models have **high residual values for higher values of SalePrice\n**\n\n\n### Model Blending\nFinally we would look to combine results from different models and check if the RMSLE value for combined results is better than the individual models. \n","execution_count":null},{"metadata":{"_uuid":"9ae05136-d8cb-4915-a8df-be88dd186624","_cell_guid":"45621bff-de53-463b-a008-d749098071c5","trusted":true,"scrolled":false},"cell_type":"code","source":"pred_rf=best_grid_rf.predict(X_test)\npred_xg=best_grid_xg.predict(X_test)\npred_ls=lasso_model.predict(X_test)\npred_rg=ridge_model.predict(X_test)\n\npred_xg_ls_rg=(pred_xg+pred_ls+pred_rg)/3\n\nprint('RMSLE for Random Forest')\nprint(np.sqrt(mean_squared_log_error(np.exp(Y_Test), np.exp(pred_rf) )))\n\n\nprint('RMSLE for XG Boost')\nprint(np.sqrt(mean_squared_log_error(np.exp(Y_Test), np.exp(pred_xg) )))\n\n\nprint('RMSLE for Lasso')\nprint(np.sqrt(mean_squared_log_error(np.exp(Y_Test), np.exp(pred_ls) )))\n\nprint('RMSLE for Xgboost & Lasso & Ridge Blended')\nprint(np.sqrt(mean_squared_log_error(np.exp(Y_Test), np.exp(pred_xg_ls_rg) )))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The output above suggests that XGBoost, Lasso & Ridge Regression perform much better than Random Forest models, and the combine results of XGBoost, Lasso & Ridge Regression performs even better than the individual models. For our final submission we would be submitting combined results from  Xgboost & Lasso & Ridge Blended model. ","execution_count":null},{"metadata":{"_uuid":"d9f45b10-4ff7-4365-ae68-3162aedf31c1","_cell_guid":"a679d961-f0e8-4d0c-9768-39e55bcedc69","trusted":true,"scrolled":false},"cell_type":"code","source":"pred_test_rf=best_grid_rf.predict(test_imp)\npred_test_xg=best_grid_xg.predict(test_imp)\npred_test_ls=lasso_model.predict(test_imp)\npred_test_rg=lasso_model.predict(test_imp)\n\npred=(pred_test_ls+pred_test_xg+pred_test_rg)/3\n#pred=pred_test_xg\nprint(len(pred))\n#test.shape\n#test_mod.shape\nId=pd.Series(range(1461,2920))\n#pred\n#Id\nd = {'Id': Id, 'SalePrice': pred}\ndf = pd.DataFrame(data=d)\ndf['SalePrice']=np.exp(df['SalePrice'])\ndf\n\ndf.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Next Steps \n\nDive deeper into some aspects of Feature Engineering and model performance and improve score on the public LB.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}