{"nbformat_minor":1,"cells":[{"execution_count":null,"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","cell_type":"code","outputs":[],"metadata":{"_uuid":"4745983da3c4d83522bfdacc0dad749d61ceed21","_kg_hide-output":true,"_cell_guid":"029efcc8-dece-4be6-9443-80506447cdf7","_kg_hide-input":true}},{"source":"# **Practice with Python, Pandas + Sklearn: Features and How-Tos**\n\n<img src='https://la-community-cdn.linuxacademy.com/img/user_23708_5849f32880efa.jpg' style='width:500px' />\n\nGreetings all! The goal of this guide is to show off the tools and options available under the pandas and sk learn library. Instead of looking at small 3x3 matrix toy examples, thought it would be good to compile all of the functions with a regular dataset. The Iowa dataset is relatable, the sale price of houses are somewhat inituitve, and all the data can fit locally on the computer.  \n\nThis will be a **growing document**, so this initial post only covers basic data ingestion and manipulation. If any of you have found some neat tricks, or good libraries for getting around data science, feel free to comment and Ill make the update! Hope those of you that are just getting started will find the walkthrough helpful!\n\n\n### **Version Notes:**\n- 3: Added detail about classification scoring, how to get predictions out of your model, and overview of loss functions\n- 2: Added additional word documentation, some pandas creation notes and formatting \n- 1: Inital Post\n\n## **Topics:**\n### **1. [Load your Libraries](#section1)**\n### **2. [Navigating with Pandas](#section2)**\n### **3. [Cleaning your data](#section3)**\n### **4. [Feature Engineering](#section4)**\n### **5. [Model Prep: train, test and split](#section5)**\n### **6. [Modeling: Linear Regression, Logistic Regression, Random Forests](#section6)**\n### **7. [Quest for the best parameters](#section7)**\n### **8. [Scores, Loss, and whats under the hood](#section8)**\n\n\n### ** Problem: How much does a house cost? What makes it that expensive?**\nhttps://www.kaggle.com/c/house-prices-advanced-regression-techniques\n\nThe dataset we will be using was compiled by Dean De Cock, and \"This paper presents a data set describing the sale of individual residential property in Ames, Iowa from 2006 to 2010. The data set contains 2930 observations and a large number of explanatory \nvariables (23 nominal, 23 ordinal, 14 discrete, and 20 continuous) involved in assessing home \nvalues\"\n\n* Check out his paper here: https://ww2.amstat.org/publications/jse/v19n3/decock.pdf \n![](http://www.hubbellhomes.com/images/community/368/tb/Pemberley-Hills-thumbNEW.jpg)","cell_type":"markdown","metadata":{"_uuid":"e8eb0dc89c214bdb6d9ea3b9337443f563202d0a","_cell_guid":"678e226e-cf4d-4827-b6c2-790273eb5f4b"}},{"source":"# **Before we get started: Beginners Tips**\n\n<img src='http://www.allwebdesigninfo.com/wp-content/uploads/2014/03/Beginner-Programming-Mistakes.jpg'  width='400px' />\n\n### **Choose good Words for your Variables. Keep track of them!** \nFor long analysis, becareful how you name your variables. With really complicated notebooks, a simple rewrite can ruin the rest of your programming setup. especially becareful of these keywords:\n\n**` X, y, n, df, data, dataframe, train, test, results, final_results, predict` **\nJust know up front that you will most likely be using these words. Becareful of using these variables elsewhere in coding loops etc.. Every one is taught using `a`, `b`, `x`, `y`,`z` in loops and lists. \n\nIn data science:\n- **`X`** usually refers to a **dataframe** of source data in which we will use to make a prediction. \n- **`y`** usually refers to the thing we are trying to predict, either money, or 1 / 0 if someone has a disease. \n- **`test`** should not be the function you are prototyping, this too represents a **dataframe** of records that are used for evaluating our work. Name that new funcion you are writing something like `foobar` or `mygreatfunc`\n\n```python\n    # currently y is a matrix\n    y,X = train_test_split(mydf)\n\n    ## now, y is a temp variable in a loop\n    for y in list_of_columns:\n        print('column named' + y)\n\n    ## what the heck is in 'y' now?\n    model.fit(X,y)\n    ```\n    \n  \n### **Becareful of Self-Assignment of Vars (a = a +1 ) ** \nAs you are going on, running through your python notebooks, you will be running chunks of code. Becareful of reusing names and looping coding blocks,  code lines are divided between different blocks. \n\nA standalone block where you do a self-assignment AKA variable name is reused, can lead to a recursive like effects. See below\n\n```python\n    # this is fine because the data is reloaded, each time this block is run\n    mydf = load_data()\n    mydf = replace_any_zeroes(df)\n```\n\n```python\n    # left join will add columns on the right side of your data frame. if you keep re-running this block, it will continously append new columns over and over with no end, your dataframe will keep growing horizontally!\n    mydf = mydf.left_join(new_dataset)\n    my_counter = my_counter + 1\n```\n\n\n ### **Pandas Warning on DataFrame copying** \n \n Pandas dataframes as they get manipulated for a majority of the time are still referencing the same memory or dataframe:\n \n```python\n\n# IS STILL POINTING TO ORIGINAL, EDITS TO NEW WILL ALSO BE APPLIED TO ORIGINAL\nnew_df = master_df \n\nnew_df[ 1,2 ] = b # will also update `master_df` as well. Careful!\n\n# makes a new copy to avoid this issue\nnew_df = df.copy()\n```\n\n","cell_type":"markdown","metadata":{"_uuid":"fd884311aba3a97d0c8f5dd2adc4c4379100dd80","_cell_guid":"b97b88b1-62a2-4e75-9655-e48bfa4015ed"}},{"source":"<a id='section1'></a>\n# **1. Load Your Data, Load your Libraries**\n\nWell, lets get started loading a bunch of libraries that we will be showing off with the data\n\n<img src='https://ak8.picdn.net/shutterstock/videos/17983978/thumb/7.jpg' style='width:300px'/>","cell_type":"markdown","metadata":{"_uuid":"1b252eb11e02bf94ac544c45603b6f98621bb5bb","_cell_guid":"35ed2854-3d06-4f8b-af58-2dd409335a4b"}},{"execution_count":null,"source":"import pandas as pd\nimport numpy as np\nimport itertools\nimport matplotlib.pyplot as plt\n\n#prep\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MaxAbsScaler, QuantileTransformer\n\n#models\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV, LinearRegression, Ridge, RidgeCV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n#validation libraries\nfrom sklearn.cross_validation import KFold, StratifiedKFold\nfrom IPython.display import display\nfrom sklearn import metrics\n\n\n%matplotlib inline","cell_type":"code","outputs":[],"metadata":{"_uuid":"861174e895dcc17cceff4d1c44658676cc1c05f7","_cell_guid":"43e42a9f-d705-420a-9b57-e6a1488d6d70"}},{"source":"<a id='section2'></a>\n# **2. Navigating your data**\nThe following section is a primer for loading data into **dataframes** which are essentially very powerful data grids that have a lot of great functionality for manipulating data. These can also be thought of as super powerful excel sheets. Below we will show some basic syntax usage of getting around your dataframe. \n\n<img src='https://www.eurisy.org/data_files/mce_images/Blog_images/Human_hand_with_a_laptop_surrounded_by_iconsellagrin.jpg' style='width:400px' />\n#### Dataset size\n\n- **Train**: 1460\n- **Test**: 1458\n\n### **reading in CSV's from a file path**","cell_type":"markdown","metadata":{"_uuid":"5b81f11621ab3bd4af4e0a0a73f28689dad5f571","_cell_guid":"4178494e-16f5-4985-8be5-2a67d8c9a4c2"}},{"execution_count":null,"source":"train_df = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')","cell_type":"code","outputs":[],"metadata":{"_uuid":"8e21cf3a1e6a4865182453f78bfa7668f6e800ab","_cell_guid":"44fe3e05-0dee-4b04-bd9e-ecc1cbcbda05","collapsed":true}},{"source":"### **Making a pandas dataframe from either a list or a dictionary**\n<img src='https://snag.gy/toRSjY.jpg' />\n<img src='http://pbpython.com/images/pandas-dataframe-shadow.png' />\n\nOften times we don't have a CSV available, but the data comes from a database or another python program. This nifty reference shows how we can make a dataframe from python's common **`list`** or **`dict`** objects","cell_type":"markdown","metadata":{"_uuid":"8e5935671ffe0027c7191dacaee44f52b0f487f3","_cell_guid":"3d952efa-875f-44dd-bac9-ec02e81eab83"}},{"execution_count":null,"source":"# here's one sample\nsample_dict = [\n    {'label': 'house i would like', 'sqft':5000},\n    {'label': 'house i would hate','sqft':500},\n    {'label': 'house i live in', 'sqft':800}\n]\npd.DataFrame(sample_dict)","cell_type":"code","outputs":[],"metadata":{"_uuid":"0a543107eb22e3754e938caf980bb22f96acec9c","scrolled":true,"_cell_guid":"86d132b4-3e2a-4034-b976-7d1d7e36f6eb"}},{"source":"### **Pandas: whats the data row count?**","cell_type":"markdown","metadata":{"_uuid":"7093a5d7dfd1b2ec86a48310e7d4d1bd74a6fbb0","_cell_guid":"c55010ee-eeb6-4a5e-87b6-9cf6561c6f30"}},{"execution_count":null,"source":"train_df.shape","cell_type":"code","outputs":[],"metadata":{"_uuid":"9c96a170cd21e1923a3732e0d0953153c2f2509e","_cell_guid":"e237bd94-0e2a-43c2-8900-1986bb924245"}},{"source":"### ** Pandas: whats the distribution of the data?**","cell_type":"markdown","metadata":{"_uuid":"406c96366d258c6810cf12409078ca83c6c2bf54","_cell_guid":"55d40f7c-1b03-447b-9f76-11bf2979dad6"}},{"execution_count":null,"source":"train_df.describe()","cell_type":"code","outputs":[],"metadata":{"_uuid":"ee8f31fc2de1835d5733fa101f2991941ea0da39","_cell_guid":"37e58ab9-f358-4df1-ad8a-95a4c522a937"}},{"source":"### **Pandas: What types of data do i have?**\n\nThe main data types for pandas dataframes are \n- **in64** - whole numbers\n- **float** - decimals \n- **object** - most general type, mixed numbers and letters\n- **category** - assigned labels such as [small, medium, large]\n- **datetime**","cell_type":"markdown","metadata":{"_uuid":"97bd84e48e33e6c7fca422790b7a118963671f00","_cell_guid":"9c3731e1-9361-48e4-9f3a-ee4188a641ab"}},{"execution_count":null,"source":"train_df.info()","cell_type":"code","outputs":[],"metadata":{"_uuid":"d9f07241b82fdd9b49a3cd69d25c9665c0943e4d","scrolled":true,"_cell_guid":"ae23632e-4abf-49b4-a952-f07255fc08b9"}},{"source":"### **Filtering and Peering inside the dataframe**\n\n<img src='http://www.blastam.com/wp-content/uploads/ecommerce-filter-feature.jpg' />","cell_type":"markdown","metadata":{"_uuid":"6fd435814600618dcdace8b86d0aeb41297e55ae","_cell_guid":"3a256eb8-16d9-4b6e-8a98-eae5b1e896b0"}},{"source":"#### ** Pandas: take a peek at the first few rows,  note what may be categorical , what may be numeric**","cell_type":"markdown","metadata":{"_uuid":"4f987eeb0fdd7b39d6994dcc1df3d770b090c7f6","_cell_guid":"db00c04d-7f06-4309-86f0-7454f02c378d"}},{"execution_count":null,"source":"train_df.head(2)","cell_type":"code","outputs":[],"metadata":{"_uuid":"cdd8b17a9df4f72b4cc86b88efdc8774bc91c71d","_cell_guid":"7de83397-221c-44e4-bc0a-60140384c4d8"}},{"source":"## **When is it an array? When is it a Dataframe?, note the difference `[ ]` vs `[[ ]]`**\n<img src='http://venus.ifca.unican.es/Rintro/_images/dataStructuresNew.png' style='width:400px'>\n- An **vector** is a 1-D collection, either a `list`, a `set`, `numpy.array`, or even a `pandas.series` \n- A **matrix** is typically a 2-D collection of rows and columns such as a nested `list`, a `numpy.array` or a `pandas.DataFrame`\n- When working with **DataFrames** and matrices, there's a lot of methods to convert one to the other:\n        - `pandas.DataFrame.asmatrix()`\n        - `pandas.DataFrame.to_dense()`\n","cell_type":"markdown","metadata":{"_uuid":"283bf94b90aec33d5642aec625e39020c3ed6f82","_cell_guid":"16df92bf-0415-4045-8a6e-8705822f1254"}},{"execution_count":null,"source":"train_df['SalePrice'].head(5)","cell_type":"code","outputs":[],"metadata":{"_uuid":"b4bb574a0666967e4bded1e2962900a71634ce90","_cell_guid":"6530f678-959f-4181-b875-fb274084d594"}},{"execution_count":null,"source":"train_df[['SalePrice']].head(5)","cell_type":"code","outputs":[],"metadata":{"_uuid":"a66a10627b06643aadacd0cea79850dda9aa77de","_cell_guid":"dfad3eaa-0eb7-4a7f-84cf-3aaefcdcfd40"}},{"execution_count":null,"source":"train_df.as_matrix()","cell_type":"code","outputs":[],"metadata":{"_uuid":"2e3848ffc7084afa5a2e1ff41b48a88a7a772bca","scrolled":true,"_cell_guid":"c1c23f93-0854-43f5-836c-3cd1e6aa9ef6"}},{"execution_count":null,"source":"pd.DataFrame(train_df.as_matrix()).head()","cell_type":"code","outputs":[],"metadata":{"_uuid":"8ad8ea9a0349e5b04bce490fc73f067dfcbe47b6","_cell_guid":"0984d429-421c-45c0-b600-2393b30d560f"}},{"source":"### **Pandas: Filtering  and slicing your dataframe**\n\n<img src='http://bookdata.readthedocs.io/en/latest/_images/base_01_pandas_5_0.png' style='width:400px' />","cell_type":"markdown","metadata":{"_uuid":"d29bb6f0593fb36d046ea016b509a2f6547f01f8","_cell_guid":"ac596b81-b488-4510-a3a7-8be65108ee89"}},{"execution_count":null,"source":"train_df[['SalePrice','LotShape']].head(4)","cell_type":"code","outputs":[],"metadata":{"_uuid":"f88803e8c7858a4c226429cae671b2393a5b7be3","_cell_guid":"229b56cd-a2d0-4876-9b51-bbabec28a2f9"}},{"execution_count":null,"source":"train_df.iloc[range(3),]","cell_type":"code","outputs":[],"metadata":{"_uuid":"a86efeaca62cb311acbf1c762f921826defb2056","_cell_guid":"ff6d8320-ba8a-46d4-ae22-419e6d0d759c"}},{"execution_count":null,"source":"train_df[train_df['SalePrice']>200000].head(3)","cell_type":"code","outputs":[],"metadata":{"_uuid":"79f444576581497ce5c6cdb8bbc4abee548d9747","_cell_guid":"468147d7-9ade-4788-996e-2c0856b9cc89"}},{"execution_count":null,"source":"train_df[train_df['LotShape'].isin(['Reg','IR1'])].head(3)","cell_type":"code","outputs":[],"metadata":{"_uuid":"6fc548d131d761f312d00c8f09555c17c2b10bfc","_cell_guid":"cf902d73-d3a0-4cbb-9a71-02b5b6ed1609"}},{"source":"#### **Pandas: Take a look at the column names of all the fields**","cell_type":"markdown","metadata":{"_uuid":"78d5a003580bea37249292d3eb507b962ddacbdd","_cell_guid":"03839c6f-b8bc-45f5-a499-e5ad4b09c605"}},{"execution_count":null,"source":"print('this many columns:%d ' % len(train_df.columns))\ntrain_df.columns","cell_type":"code","outputs":[],"metadata":{"_uuid":"f6119c8bc715b7232e6b900d8f0c2f3813b95ca3","_cell_guid":"d404bc01-c982-4357-9c63-7d97dbf5d2d6"}},{"source":"#### **Changing the Column names starting with numbers , later functions sometimes have issues**","cell_type":"markdown","metadata":{"_uuid":"dd0934fab82d823934e31b3c0af196e94058579c","_cell_guid":"d7b2ee22-fb70-4d8e-b372-5351ebc2ba88"}},{"execution_count":null,"source":"train_df.columns = ['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n       'HeatingQC', 'CentralAir', 'Electrical', 'FirsstFlrSF', 'SecondFlrSF',\n       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n       'EnclosedPorch', 'ThreeSsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n       'SaleCondition', 'SalePrice']","cell_type":"code","outputs":[],"metadata":{"_uuid":"0cfa1eacf242e7fb321a44ad30860457b50c30ba","_cell_guid":"a5fe1fca-5535-41c1-8d13-02a65791eb64","collapsed":true}},{"source":"<a id='section3'></a>\n# **3. Clean your Data**\n<img src='https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAALAAAAAJGU3YjAwY2I5LTg1ODMtNDg5Ny04MTg2LTNhMWUwYTdkZDk2Zg.png'  width='300px'/>****","cell_type":"markdown","metadata":{"_uuid":"df5ba8860ae0ce6a3d08bc6edfd89c41b58ace9f","_cell_guid":"aaac4167-8070-4cb5-b0fd-049fd85ee31b"}},{"source":"### **Check for Blanks or NA's!**\n\nNote that there is a difference between **`NaN`, `''`, `None`** so be aware when looking at your data\n- **`NaN`** = not a number, still a `float` type, so think of it of empty space that can still be passed through numerical operations\n- **`''`** = is a empty string type\n- **`None`** = is also a empty space, but in DataFrames it is considered an `object` which cannot be processed through optimized numerical operations","cell_type":"markdown","metadata":{"_uuid":"21d1c44d5bb85a153072bc79a0786605f50cb536","_cell_guid":"aac1225c-7dd7-4385-86f7-cd226d52624b"}},{"source":"#### **Pandas: Check for NA's in a column**","cell_type":"markdown","metadata":{"_uuid":"bbbd83c35151601f7082141253d1af00e164dae9","_cell_guid":"b8a66448-5199-4a1d-9421-c47b503dde23"}},{"execution_count":null,"source":"train_df[train_df['Alley'].isnull()].head(10)","cell_type":"code","outputs":[],"metadata":{"_uuid":"bc49a567ddc5d3b0e64a432f17a8113e89a4f859","_cell_guid":"ad903f67-116d-4cd6-b238-cc241d2a9479"}},{"source":"### **Pandas: Fill in NA's**\n\n<img src='http://csharpcorner.mindcrackerinc.netdna-cdn.com/UploadFile/219d4d/different-ways-to-replace-null-in-sql-server/Images/records.jpg'/>\n\nThe `fillna` function is much handier to fill in NA's that trying to the boolean. If needed you can do a comparison with `numpy.NaN` or in our case `==np.nan`, `np.isnan(some number )`","cell_type":"markdown","metadata":{"_uuid":"d98e5a5a842af5d5093252622c91c992b63842e5","_cell_guid":"c8e25530-ed7a-4769-93b5-b68936d0cf75"}},{"execution_count":null,"source":"train_df['Alley'].fillna(0, inplace=True)","cell_type":"code","outputs":[],"metadata":{"_uuid":"4d161a32b30813e74c1874a7b3f7f3ad91ca5b2f","_cell_guid":"2780bbb7-c7d0-4db0-8726-b325dc981ea9","collapsed":true}},{"execution_count":null,"source":"na_totals = train_df.isnull().sum().sort_values(ascending=False)\nna_totals[na_totals>0]","cell_type":"code","outputs":[],"metadata":{"_uuid":"351d74540ead368d528116e453d4037063bd9279","_cell_guid":"b324a6a7-03b2-4554-95cf-066196c05c5a"}},{"execution_count":null,"source":"train_df.fillna(0, inplace=True)","cell_type":"code","outputs":[],"metadata":{"_uuid":"0cb87f7ba4fffe32dfacc963323e73ec0d66df04","_cell_guid":"03bc1f54-9b5b-43aa-9a49-9f4c50adf150","collapsed":true}},{"source":"## **Categorical vs. Continuous**\n<img src='https://i.ytimg.com/vi/7bsNWq2A5gI/hqdefault.jpg' />\n\nWhen looking at your data, its important to understand the nature of the data. If you had $1000 vs. $2000 is there such thing as $1556 dollars? Yes, of course. But between a 1 and 2 story house, is there such a thing as a 2.33 story house? Good model design always starts with good data understand and decisions on how each cell should be treated.\n\n#### **Best Practice: Check and Assign numerical features**\n\n\n\nIdentified square area as numeric. Identified any field that had that aspect to them","cell_type":"markdown","metadata":{"_uuid":"f0346d9ed77025e798a38da7e7d497243420ac1e","_cell_guid":"e95c54de-927c-45d8-8407-509a0c69146c"}},{"execution_count":null,"source":"numeric_cols = [x for x in train_df.columns if ('Area' in x) | ('SF' in x)] + ['SalePrice','LotFrontage','MiscVal','EnclosedPorch','ThreeSsnPorch','ScreenPorch','OverallQual','OverallCond','YearBuilt']\n\nfor col in numeric_cols:\n    train_df[col] = train_df[col].astype(float)\nnumeric_cols","cell_type":"code","outputs":[],"metadata":{"_uuid":"4fb6e09c3625896598959e291e5e2b62923a52ed","_cell_guid":"eefb9dac-6412-43f5-b922-50c7740eef33"}},{"source":"#### **Best Practice: Check and Convert the rest into categorical**","cell_type":"markdown","metadata":{"_uuid":"70aa2adda1a074083f405e3179eadbc45700830d","_cell_guid":"8398214a-2cd2-46da-a645-3ace7b064d79"}},{"execution_count":null,"source":"categorical_cols = [x for x in train_df.columns if x not in numeric_cols]\n\nfor col in categorical_cols:\n    train_df[col] = train_df[col].astype('category')","cell_type":"code","outputs":[],"metadata":{"_uuid":"da896e2385e79b3afc1476dedb628d2658bda10a","_cell_guid":"93323fb4-21ee-4239-ae93-5d026135d16e","collapsed":true}},{"source":"<a id='section4'></a>\n# **4. Feature engineering**\n\n<img src='https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAAiuAAAAJGRkNGQzNjU5LTQzOTAtNDRlZC05NTZkLTcxZWM5YmYwZmRkNA.png' style='width:300px'>\n\n### **Pandas: How to transform a column / apply a function against it**\nIn this case we will be looking at Log sale price. Here's how to transform a column. Both ways are valid, but the 2nd method only works because `np.log` was designed for handling vectors. If you tried \n\n`df['full name'] = df['first name'] + df['last name']` it will not work with this approach because there is no + operator that concats two vectors elementwise.","cell_type":"markdown","metadata":{"_uuid":"e21c0ab8f6ec9436279e1cde2dfb03cafc8cdca6","_cell_guid":"58bd7699-bc95-4ed3-b6c6-05cdab3640d8"}},{"source":"### **Create calculated fields from other features (2 ways outlined below)**\n\n<img src='https://snag.gy/sQa5qS.jpg' />\n\n1. Use the `.map( func )` or  `.apply(func, axis )` functions to element-wise apply a function\n2. Assign the either column with another vector , or vectorized operations","cell_type":"markdown","metadata":{"_uuid":"663456d0d8d1c76f4b6eb53beba2a8b881b1beae","_cell_guid":"e76be153-4c64-4c6e-90f2-29a4b95848fe"}},{"execution_count":null,"source":"## Applying an element wise function\ntrain_df['LogSalePrice'] = train_df['SalePrice'].map(lambda x : np.log(x)) \n\n#Vectorized log function acting on a vector\n# then assigning all the values at once\ntrain_df['LogSalePrice'] = np.log(train_df['SalePrice'])","cell_type":"code","outputs":[],"metadata":{"_uuid":"b2b3133ac2149ae0d96cf62c5f23c7e8d80c0a33","_cell_guid":"591c22ae-1ace-47bc-bd65-b6f1eec22352","collapsed":true}},{"execution_count":null,"source":"train_df['SalePrice'].hist()","cell_type":"code","outputs":[],"metadata":{"_uuid":"7e6ad0b16f665b1e2593fe0249bc56f1ebf91434","_cell_guid":"cd37ff57-ff62-4039-9699-c7ec019e7b4a"}},{"execution_count":null,"source":"train_df['LogSalePrice'].hist()","cell_type":"code","outputs":[],"metadata":{"_uuid":"b9877b7b4b4230b06b7cef7a3ad5c09e1661b6a9","_cell_guid":"13d70bac-b430-4248-97b1-627576d8e86e"}},{"source":"### **Python, how to make a continuous variable categorical? (2 ways)**\n\n<img src='https://snag.gy/sIBulc.jpg' />","cell_type":"markdown","metadata":{"_uuid":"926316469f8cd95b9d10507d347875cd7535e327","_cell_guid":"954ffebd-26c3-4c6c-aa8a-8f22e450b255"}},{"execution_count":null,"source":"# element wise function to transform\ntrain_df['above_200k'] = train_df['SalePrice'].map(lambda x : 1 if x > 200000 else 0) \ntrain_df['above_200k'] = train_df['above_200k'].astype('category')","cell_type":"code","outputs":[],"metadata":{"_uuid":"2907421cee60cee8c58c2776602828db3fefc42b","_cell_guid":"f14e19c4-d9df-40cd-9702-386e5394b922","collapsed":true}},{"source":"or","cell_type":"markdown","metadata":{"_uuid":"c14782aed3b6a668d0133370376f7730b1d96b2b","_cell_guid":"9768b2e6-3321-41bc-b256-8ebb21219e58"}},{"execution_count":null,"source":"# manually assign the values to your new field, section by section\n# with row filtering\ntrain_df.loc[train_df['SalePrice']>200000,'above_200k'] = 1\ntrain_df.loc[train_df['SalePrice']<=200000,'above_200k'] = 0\ntrain_df['above_200k'] = train_df['above_200k'].astype('category')","cell_type":"code","outputs":[],"metadata":{"_uuid":"8e2d7f47e5536a414d0a5c3056a3b542d6515e8f","_cell_guid":"0f456c78-1417-43d6-b4a0-74ba02890eb5","collapsed":true}},{"source":" ### **Pandas: how to add columns together (numeric)**","cell_type":"markdown","metadata":{"_uuid":"44d4ffb9ce3cbd3c7c82861248b821b185c69503","_cell_guid":"6dedf65c-b3be-4a01-8f09-2844655eb593"}},{"execution_count":null,"source":"train_df['LivArea_Total'] = train_df['GrLivArea'] + train_df['GarageArea'] + train_df['PoolArea']\ntrain_df[['LivArea_Total','GrLivArea','GarageArea','PoolArea']].head()","cell_type":"code","outputs":[],"metadata":{"_uuid":"34fea4f0f84dec44b1178d1a925b2c3a32fa987f","_cell_guid":"0d649fa3-375f-472e-bda5-45ba5ce56e4f"}},{"source":"### **Pandas: how to apply a function row - wise (example is adding strings)**\n\n- `axis=0` - pass a **column** of data\n- `axis=1` - pass a **row** of data","cell_type":"markdown","metadata":{"_uuid":"e294672fe753a966b9e33d58f60a6302cd429da0","_cell_guid":"57336c2f-470d-42c6-abfd-9ec009777d1b"}},{"execution_count":null,"source":"## concatenating two different fields together in the same row\ntrain_df['Lot_desc'] = train_df.apply(lambda val : val['MSZoning'] + val['LotShape'], axis=1)\ntrain_df[['Lot_desc','MSZoning','LotShape']].head()","cell_type":"code","outputs":[],"metadata":{"_uuid":"682e3789c6b8e719c11d9bb84997f2ff71c8a2c1","_cell_guid":"419c2a12-4bec-498a-86d1-b70ad670694b"}},{"source":"## **Sklearn: How to scale fields**\n\nFor some of the models, comparing a field such as years old vs. square footage will have vastly different scales. Years old will vary from **0-30** and square footage will vary from **500-4000**, as a result, some models will have a difficult time or take much longer to find an optimal solution. As a result, there are a number of tools provided to assist with **normalizing** your data\n<img src='http://images.slideplayer.com/38/10783489/slides/slide_36.jpg' style='width:500px'/>\n\n- `StandardScaler` - subtract the mean and divide by std\n- `MaxAbsScaler` - transform down to [-1, 1] bounds\n- `QuantileTransformer` - transform down to [0 1] bounds\n\n## **Word to the wise: `fit` vs. `fit_transform` vs. `transform`**\n\n`fit_transform`\n**$$X_{train-norm} =\\frac{ X_{train} - \\mu_{train} }{\\sigma_{train}}$$**\n\n`transform` - note that we divide by the previously fit values\n**$$X_{test-norm} =\\frac{ X_{test} - \\mu_{train} }{\\sigma_{train}}$$**\n\n- **`fit`** - when you fit a scaler to dataset A, it calculates mean of A, and the standard deviation of A\n- **`transform`** - this will actually look at ANY dataset and subtract previously fitted (calculated) variables mean A and divide by standard deviation of A\n**`fit_transform`** does both of these things in two steps.\n\n#### **For consistency purposes, it is best to `fit_transform` on your training dataset, but only `transform` your validation set. This ensures your validation and training set has been  consistently transformed**\n\n","cell_type":"markdown","metadata":{"_uuid":"ac5661b8ae4f041e2928fb1f20c978d1b9f69fd9","_cell_guid":"9b78e6cc-1ae7-407a-8014-51d73c889c4d"}},{"execution_count":null,"source":"train_df['LotArea_norm'] = train_df['LotArea']\n\nss = StandardScaler()\nmas = MaxAbsScaler()\nqs = QuantileTransformer()\n\ntrain_df['LotArea_norm'] = ss.fit_transform(train_df[['LotArea']])\ntrain_df['LotArea_mas'] = mas.fit_transform(train_df[['LotArea']])\ntrain_df['LotArea_qs'] = qs.fit_transform(train_df[['LotArea']])\n\n\ntrain_df[['LotArea_norm','LotArea_mas','LotArea_qs', 'LotArea']].head(5)","cell_type":"code","outputs":[],"metadata":{"_uuid":"f61856304dc2f5982201c7c7ef1c8dbf2c82441e","_cell_guid":"43c2c24a-0041-4d6c-94ed-a34c2d960b58"}},{"source":"## **Words/Labels as Features**\n\n<img src='https://www.analyticsvidhya.com/wp-content/uploads/2016/01/eg2_new_4.png' />\n\nHow does a computer deal with words or labels? A computer doesn't know \"red car\" vs. \"blue car\", instead they represent say `color` with a field called `is_it_red` filled with `0`'s and `1`'s. All of these word labels are represented this way in the dataframe. But what if there's more than 2 options ? If there's multiple **labels**, we have the following options: \n\n\n### **Sklearn: 1 hot encoding  Method ((a column for every label, left table in pic)**\n<img src='https://cdn-images-1.medium.com/max/1600/1*ZsYkXEa1qrKeGiplNnFHyQ.jpeg' width='500px' />\n\nWe know that MSSubclass is a categorical, lets turn it into 1 hot encoding. Note if you pass your entire dataframe, it will convert all if it AUTOMATICALLY. \n\n#### **If you want to do this piece meal, isolate the single target categorical column, transform it and join it back together**","cell_type":"markdown","metadata":{"_uuid":"4377297430d312d17f51dfd95062b30708586cc6","_cell_guid":"17dc9440-e4eb-42ad-b29d-72517dd15b19"}},{"execution_count":null,"source":"small_df = train_df[['MSZoning','SalePrice']].copy()\nsmall_df['MSZoning'] = small_df['MSZoning'].astype('category')\nsmall_df.head()\n","cell_type":"code","outputs":[],"metadata":{"_uuid":"6db1f4962ad52fe17dfe3df0ece5e3a733bedcfb","_cell_guid":"d613ebc5-739e-4ce0-9455-22f1e5f6cc4f"}},{"execution_count":null,"source":"pd.get_dummies(small_df).head(5)","cell_type":"code","outputs":[],"metadata":{"_uuid":"a11fdd30c4d762863e872b6c5c7469c7e2af5495","_cell_guid":"28a662ee-b4ad-4049-b2f0-152083d5fcf8"}},{"source":"### **Sklearn: Label Encoding Method ( multiple int options, but 1 column, right table in pic)**\n<img src='https://cdn-images-1.medium.com/max/1600/1*ZsYkXEa1qrKeGiplNnFHyQ.jpeg' width='500px' />\n","cell_type":"markdown","metadata":{"_uuid":"a301ab63fd7acb6836acd74a472e3a6292aaac64","_cell_guid":"8d7ce98b-a5ce-45a9-807f-d33f1c5f01ae"}},{"execution_count":null,"source":"small_df = train_df[['MSSubClass','SalePrice']].copy()\nsmall_df['MSSubClass'] = small_df['MSSubClass'].astype('category')\nsmall_df.head()\n","cell_type":"code","outputs":[],"metadata":{"_uuid":"43d56c57f4586bda6f0937c8f7f67b6d940da0d0","_cell_guid":"bbab9c27-1408-4faa-b3d8-5bbd43482653"}},{"execution_count":null,"source":"le = LabelEncoder()\ntrf_MSSubClass = le.fit_transform(small_df['MSSubClass'])\ntrf_MSSubClass","cell_type":"code","outputs":[],"metadata":{"_uuid":"84a4f8c27327c991bc7453335645819f4d736005","_cell_guid":"72a59da6-4888-43da-838c-7ed03e06572f"}},{"execution_count":null,"source":"le.classes_","cell_type":"code","outputs":[],"metadata":{"_uuid":"feb3235294471ea6578d8c97710e4beaf0befe5e","_cell_guid":"08a8f323-a4bd-4d26-97cd-c6ea03632840"}},{"execution_count":null,"source":"le.inverse_transform(trf_MSSubClass)","cell_type":"code","outputs":[],"metadata":{"_uuid":"74363dce78029dc5eb90cd99edc9bcf3b4f50124","_cell_guid":"2f1c6226-e9c6-4ade-b5e1-673779649ae8"}},{"source":"<a id='section5'></a> \n## **5. Model Prep! Splitting DataFrames by Columns with 2 methods**\n\n<img src='http://okfnlabs.org/img/posts/olap-slice_and_dice-overview.png' style='width:400px' />\n\nThe general structure that any model will follow is outlined as below:\n\n>**`what im predicting(cols) = some_function ( collected_data(cols) )`**\n\nThis is often described in the following two ways:\n\n>**`response = some_model( features )`**\n\n>**`target = some_model( features )`**\n\n>**`dependent variables = some_model( independent variables )`**\n\nOr borrowing from the statistics world:\n\n>**`y = some_model(X)`**\n\nTo split out these two different components that we need, we will need to **split up our dataframe into y, X, or our (target, features)**\n\n","cell_type":"markdown","metadata":{"_uuid":"94584c669dbd3f56635a0d8d16547fdab6f5fa1e","_cell_guid":"9ba1df65-f926-4648-8e1b-f95c4f5ca75a"}},{"execution_count":null,"source":"feature_cols = [col for col in train_df.columns if 'Price' not in col]","cell_type":"code","outputs":[],"metadata":{"_uuid":"87b94a16bbbe37aac1b0a2144acb5165329273ba","_cell_guid":"47070e8c-6221-4fee-aedb-927f33b6172c","collapsed":true}},{"source":"### **Method 1 manually use dataframe methods to split X and y**","cell_type":"markdown","metadata":{"_uuid":"fb6db82bf093c66b8c09f857150612f66a06339f","_cell_guid":"14268159-c128-4b1d-b52b-3b2eeac4745c"}},{"execution_count":null,"source":"y = train_df['LogSalePrice']\nX = train_df[feature_cols]\nprint(y.head(2),'\\n\\n', X.head(2))","cell_type":"code","outputs":[],"metadata":{"_uuid":"5a4f21ffb9ae866df4a7c9c29ebabf0a1ab61aca","_cell_guid":"abc80dfb-dca6-4cd5-80ba-ff27595f0bf9"}},{"source":"### **Method 1b: Quick and Dirty way to get 100% numerical matrix (swap categories to numbers)**","cell_type":"markdown","metadata":{"_uuid":"c35242276f56cd03ffd83c0284be5a4cb680c4ad","_cell_guid":"579bd428-eceb-4747-9f92-15e1773ec997"}},{"execution_count":null,"source":"X_numerical = pd.get_dummies(X)\nX_numerical.head(5)","cell_type":"code","outputs":[],"metadata":{"_uuid":"a8b1fc00333944b2cd067ccf9c20e8fc2f1ccf23","_cell_guid":"570c5776-89e0-42e4-bfe5-d7b6e99ed830"}},{"source":"### **Method 2: With Patsy, using R-like formulas to split the dataframe**\n\nFor more information on R like formulas, see the documentation below\nhttps://patsy.readthedocs.io/en/latest/formulas.html\n\nExamples:\n- ` SalePrice ~ GrLivArea + RmsAboveGrd` \n    - `y` is the sale price\n    - `X` will have general living area and # of rooms above ground\n- ` GrLivArea ~ PorchArea + PoolArea`\n    - `y` is the general living area\n    - `X` will be the porch area and pool area","cell_type":"markdown","metadata":{"_uuid":"54b2b8827e39203f2062d14203d54c16342d5838","_cell_guid":"90746a7d-75b3-4658-b2f8-5b85acb6a937"}},{"execution_count":null,"source":"import patsy\nformula = 'LogSalePrice ~ %s' % (' + '.join(feature_cols)) \ny, X = patsy.dmatrices(formula, train_df, return_type='dataframe')\nprint(y.head(2),'\\n\\n', X.head(2))","cell_type":"code","outputs":[],"metadata":{"_uuid":"7166819d5ae904589e192ab0588a6ababf0e4145","_cell_guid":"805eda88-ba3e-4b4e-9fce-05c52d17a00a"}},{"source":"## **Train Test Split: how to grade your model**\n\n<img src='https://cdn-images-1.medium.com/max/948/1*4G__SV580CxFj78o9yUXuQ.png' width='400px'/>\n\nSome vocabulary:\n\n|Terms | Description | variables | Analogy |\n|----|-------|----|---|\n|Training Rows | This the data that the model will use to learn patterns and find an optimal solution| X_train, y_train| These are old exams a student might study repeatedly\n|Validation Rows - Dev Set | Once the model is completed, this is the data that the model hasn't seen, and will be used to score the model. This tests how \"general\" the model is, by seeing its performance with unseen data | X_val, y_val| These are old exams a student will only take once or twice as a diagnostic\n| Test set | this data is usually just the independent variables, and may or may not have the response. This is  data we actually need predictions for | X_test | This is the real exam.\n\nSo in short:\n\n- **`train`** will use this to find optimal models, can be checked for performance, but more for a gut-check and troubleshooting\n- **`valid`** will use this dataset to determine the model's performance and robustness\n\nSo to create these different row subsets, we will outline the following methods. \n\n\n### **Method 1 - split out different rowsets manually with indexing**","cell_type":"markdown","metadata":{"_uuid":"0571588eaedd30bbb071839cf89b2199ae325f36","_cell_guid":"5ae5a36b-b1d3-4c7d-a45d-c15ffb6f20be"}},{"execution_count":null,"source":"def split_vals(a,n): return a[:n], a[n:]\nn_valid = 170\nn_trn = len(y)-n_valid\nX_train, X_valid = split_vals(X, n_trn)\ny_train, y_valid = split_vals(y, n_trn)\n\nprint(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)\n","cell_type":"code","outputs":[],"metadata":{"_uuid":"848805b5af5f045bfbe8d4f33fe863eb55d7b1ee","_cell_guid":"e5b5747d-9825-450b-b9f5-5d4fb3c4f9df"}},{"source":"### **Method 2 - split out rowsets using sklearn's train_test split function**\n> ","cell_type":"markdown","metadata":{"_uuid":"9c4682e5f8bd8ddaca58213e5f98adc82e529074","_cell_guid":"ce46bd45-c1d9-42cd-8eae-d740213dd3cc"}},{"execution_count":null,"source":"X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.2)\nprint(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)\n","cell_type":"code","outputs":[],"metadata":{"_uuid":"d4151eb948d338721db8df19ae60fa7d21247733","_cell_guid":"60557fda-5da4-4834-a746-18a75ba94f15"}},{"source":"<a id='section6'></a>\n# **6. Modeling - An Overview of Linear Model, Regularized Model, and Random Forest**\n<img src='https://www.lintao-dashboards.com/wp-content/uploads/2016/02/ldm930.jpg' style='width:400px' />\n","cell_type":"markdown","metadata":{"_uuid":"a6c864c557eaf8c47b91f872c92254f662562d92","_cell_guid":"a9c13d72-05fa-477f-9a99-370251fcc1dd"}},{"source":"## ** Regression** \n<img src='https://i.stack.imgur.com/SbqXz.png' style='width:400px' />\n\n**Wikipedia:** In statistics, linear regression is a linear approach for modeling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables) denoted X. The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.\n\n**Caveat:** We are assuming that the relationship is linear. If we are modeling something like compound interest for a savings account, then fitting a linear model is not appropriate. ","cell_type":"markdown","metadata":{"_uuid":"a633af93627ab66009507fd92d7cd20034ad8120","_cell_guid":"2a0355e7-42f5-4494-af89-228d39750ebe"}},{"source":"### **Fitting a Linear Model**","cell_type":"markdown","metadata":{"_uuid":"8493331b08ab3fda01fb87058e463a19e2e83cf3","_cell_guid":"38d9239d-85b6-4620-9169-e78b70e348f9"}},{"execution_count":null,"source":"lm = LinearRegression()\nlm.fit(X_train,y_train)","cell_type":"code","outputs":[],"metadata":{"_uuid":"7465323a658bef722d904c909df374df5f15008d","_cell_guid":"e5f1e7ee-a82b-4617-a2ed-519bded97bd3"}},{"execution_count":null,"source":"lm.score(X_train,y_train)","cell_type":"code","outputs":[],"metadata":{"_uuid":"38bb4e88c86ca9372f13c5073e39bd810afa31d7","_cell_guid":"d2e3a53a-e570-4bb6-99b6-5fd9c7d8284e"}},{"execution_count":null,"source":"lm.score(X_valid,y_valid)","cell_type":"code","outputs":[],"metadata":{"_uuid":"5f9969114c685dcaf62f1870b5b8244d5032aec1","_cell_guid":"cfab3376-cfc7-491f-8d20-93d0752c1134"}},{"execution_count":null,"source":"y_pred = lm.predict(X_valid)\nrmse = np.sqrt(metrics.mean_squared_error(y_pred, y_valid))\nrmse","cell_type":"code","outputs":[],"metadata":{"_uuid":"071754d583c2a1f1789afc582da8d588b88f0b35","_cell_guid":"e2ade1af-d814-4c88-80b7-8c52f2eb19c8"}},{"source":"### **Fitting a regularized linear Model (with k Folds)**\n\n<img src='https://onlinecourses.science.psu.edu/stat857/sites/onlinecourses.science.psu.edu.stat857/files/lesson04/ridge_regression_geomteric.png' style='width:400px' />\n\n**What do you do if you have a lot of variables?** As nice as plotting is, if there's 100's of other columns to compare to, it can be very difficult. Regularization is a mathy approach deal with some of these issues, helping with collinear (related) columns and also helping with variable selection.","cell_type":"markdown","metadata":{"_uuid":"603351c45156e0f0f35740579711506b3039acf1","_cell_guid":"201d3ba2-ebe2-49e7-9a6a-00a133eb9910"}},{"execution_count":null,"source":"rdgCV = RidgeCV(alphas=[0.01,0.1,1,10,100,1000], cv=5)\nrdgCV.fit(X_train,y_train)","cell_type":"code","outputs":[],"metadata":{"_uuid":"08ba32853846fde477af16f081134c1b6f34a664","_cell_guid":"cde90b63-2833-459f-be59-9acac2e39b40"}},{"execution_count":null,"source":"print(rdgCV.alpha_)","cell_type":"code","outputs":[],"metadata":{"_uuid":"1db9458ca2e628fa1d627156ee00c23095d0c914","_cell_guid":"8e9995c9-522a-452c-9dc8-14cd489275c6"}},{"execution_count":null,"source":"rdg = Ridge(alpha=10)\nrdg.fit(X_train, y_train)\nrdg.score(X_valid, y_valid)","cell_type":"code","outputs":[],"metadata":{"_uuid":"696ec1bb61f908c07dd7508f4809309b91b025c1","_cell_guid":"f4fc3ffe-15d6-4e31-a2d7-6c53b7a938e9"}},{"execution_count":null,"source":"y_pred = rdg.predict(X_valid)\nrmse = np.sqrt(metrics.mean_squared_error(y_pred, y_valid))\nrmse","cell_type":"code","outputs":[],"metadata":{"_uuid":"d9c10b622561615596b9eb37e05b5e11055a5d52","_cell_guid":"65b7e898-59d9-409d-9479-7d93054a841f"}},{"source":"### **Fitting a RandomForest Regressor**\n\n<img src='https://d2wh20haedxe3f.cloudfront.net/sites/default/files/random_forest_diagram_complete.png' style='width:400px' />\n\n\n* #### **Parameters**:\n**`df`**: The data frame you wish to process.\n\n**`y_fld`**: The name of the response variable\n\n**`skip_flds`**: A list of fields that dropped from df.\n**`do_scale`**: Standardizes each column in df.\n\n**`na_dict`**: a dictionary of na columns to add. Na columns are also added if there\n    are any missing values.\n\n**`preproc_fn`**: A function that gets applied to df.\n**`max_n_cat`**: The maximum number of categories to break into dummy values, instead\n    of integer codes.\n\nsubset: Takes a random subset of size subset from df.\n\n#### Returns:\n**`[x, y, nas]`**:\n    **`x`**: x is the transformed version of df. x will not have the response variable\n        and is entirely numeric.\n\n    **`y`**: y is the response variable\n\n    nas: returns a dictionary of which nas it created, and the associated median.\n","cell_type":"markdown","metadata":{"_uuid":"cfc7abdba488d8c7ba78d5255a399064d634c23e","_cell_guid":"bcbeec50-aadd-411d-9821-8325b2a49107"}},{"execution_count":null,"source":"rfr = RandomForestRegressor(n_jobs=-1, n_estimators=100)\nrfr.fit(X,y)","cell_type":"code","outputs":[],"metadata":{"_uuid":"319e768779413b54d380439babd7af1ec18092e1","_cell_guid":"2bd67b78-10bf-4793-a132-b50963b6b470"}},{"execution_count":null,"source":"rfr.score(X_valid,y_valid)","cell_type":"code","outputs":[],"metadata":{"_uuid":"9632e49ae158c188b01ed8a7d1a6dea23e119f20","_cell_guid":"0b81d777-fb08-408d-8cae-c52af5a54185"}},{"execution_count":null,"source":"y_pred = rfr.predict(X_valid)\nrmse = np.sqrt(metrics.mean_squared_error(y_pred, y_valid))\nrmse","cell_type":"code","outputs":[],"metadata":{"_uuid":"3770eb3c5428d959b991caf33ad5b355b392f492","_cell_guid":"557d53cd-c341-41d3-8ab4-a2bb90a2ce5f"}},{"source":"## **Classification**","cell_type":"markdown","metadata":{"_uuid":"044953af45f2712a5094c8bd0844a4227eb3ffc0","_cell_guid":"690868d8-13a7-4e00-8bf2-35ce55737b7f"}},{"source":"### **Fitting a Logistic Regression**\n\n<img src='https://i1.wp.com/dataaspirant.com/wp-content/uploads/2016/04/logisticregression.png' style='width:600px' />\n#### **Resplitting data for categorical prediction**","cell_type":"markdown","metadata":{"_uuid":"7774f504adcc825784641778d460f99b1eeccf4e","_cell_guid":"54af2b86-6cea-4907-8c49-201fb63dcac5"}},{"execution_count":null,"source":"import patsy\n\ntrain_df['above_200k'] = train_df['above_200k'].astype(float)\nformula = 'above_200k ~ %s' % (' + '.join(feature_cols)) \ny_cls, X_cls = patsy.dmatrices(formula, train_df, return_type='dataframe')\nprint(y.head(2),'\\n\\n', X.head(2))\n\n\nX_cls_train, X_cls_valid, y_cls_train, y_cls_valid = train_test_split(X_cls,y_cls, test_size=0.2)\nprint(X_cls_train.shape, X_cls_valid.shape, y_cls_train.shape, y_valid.shape)","cell_type":"code","outputs":[],"metadata":{"_uuid":"30c74495f335fc0c6d7cd4febad4aecc62005495","_cell_guid":"9b4ec272-6829-4167-8b07-d3e4f165fae3"}},{"execution_count":null,"source":"lgm = LogisticRegression()\nlgm.fit(X_cls_train,y_cls_train)","cell_type":"code","outputs":[],"metadata":{"_uuid":"c363135c8b473f53ab8f9cab6515bb749e5237c5","_cell_guid":"22682131-b81a-461e-a08c-37eef11d2c1c"}},{"execution_count":null,"source":"lgm.score(X_cls_valid,y_cls_valid)","cell_type":"code","outputs":[],"metadata":{"_uuid":"868e27fe26be45bb3f74136c12123b79f18aab7c","_cell_guid":"f16a3128-1e06-4da4-99cd-bd7d128a4f14"}},{"source":"### **Generating a Confusion Matrix**\n\nNote, this code is taken straight from the SKLEARN website, an nice way of viewing confusion matrix. This is useful in classification problems. Consider this issue:\n- Your underlying data only has **10 / 90** virus to non-virus samples to detect\n- if you get an accuracy of 90%, what does that mean? Is that better than guessing? Nope. It's the same\n- **Practical sense** if you are a hospital, you want to be very careful with **False Positives: telling people they have a disease when they don't**. That's much worse than **False Negatives**, not detecting a disease. As a result, people will design around the priority of these two different metrics. \n\n#### A sample confusion matrix\n<img src='https://snag.gy/GiWgFn.jpg'  style='width:400px'/>","cell_type":"markdown","metadata":{"_uuid":"2ed62d9286d03f540e697fee591b833c7e34c8f9","_cell_guid":"ed2aabfd-351d-4e49-8e12-0b9636781a01"}},{"execution_count":null,"source":"y_cls_pred = lgm.predict(X_cls_valid)\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n    \ncnf_matrix = metrics.confusion_matrix(y_cls_valid, y_cls_pred)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['above_200k', 'below_200k'],\n                      title='Confusion matrix, without normalization')\n","cell_type":"code","outputs":[],"metadata":{"_uuid":"25318c1488a2303799c3e15bd157a56758f431e6","_cell_guid":"7cc84ccc-308d-4c8c-b579-451c06bbde1e"}},{"source":"<a id='section7'></a>\n## ** 7. Grid Search/Randomized Search: the quest for hyperparameters ** \n> <img src='https://blogs.sas.com/content/subconsciousmusings/files/2016/09/patricks-hyperparameter-grid-search-smaller2.png' />\n\n#### **Look at how many options are in  logistic regression:**\n>        LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n>                  intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n>                 penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n>                  verbose=0, warm_start=False)\n\n\nMany of the advanced machine learning functions have a large number of model options that can be entered. these are often called **hyper parameters**. These address questions such as: \n- \"how long should the model run\", or \n- \"how many times should my computer re-look at the data\" or \n- \"how slow should the computer work through the problem?\" \n\nTo assist answering some of these questions, `sklearn` has `GridSearch` and `RandomizedSearch` which will try various combinations with a provided model, compare scores and return the optimal model that should be tried.","cell_type":"markdown","metadata":{"_uuid":"d0115189281766f5eb2b3a5ea76bf2ed1e02442d","_cell_guid":"b113ed03-97aa-4106-9c78-3e4ad732e3a3","collapsed":true}},{"execution_count":null,"source":"rfr = RandomForestRegressor(n_jobs=-1)","cell_type":"code","outputs":[],"metadata":{"_uuid":"faf2fe1cd6f7fb685483d9efc5b9546fb06e6edc","_cell_guid":"c4f3ebce-2859-4510-9fd0-b23bd3fbd966","collapsed":true}},{"source":" #### ** Using GridSearch we can find the optimal parameters for Random forest **","cell_type":"markdown","metadata":{"_uuid":"41760b385c024b33cef1c271fc82dac59ffb9e7c","_cell_guid":"355e70f3-5f2b-453a-a54a-6d38aa8f72f5"}},{"execution_count":null,"source":"params = {\n    'max_features': [0.25, 0.5, 0.7],\n    'max_depth' : [ 2,5,10,20]\n}\ngs = GridSearchCV(cv=5, param_grid=params, estimator=rfr, verbose=0)\ngs.fit(X_train,y_train.LogSalePrice.ravel())\nprint(gs.best_params_, gs.best_score_)","cell_type":"code","outputs":[],"metadata":{"_uuid":"cf81f536a58089f4775ccdd25196de4cc5867563","_cell_guid":"048e88c3-8e56-427b-a4d8-84183956e18f"}},{"source":" #### ** Using RandomSearch we can find the optimal parameters for Random forest **","cell_type":"markdown","metadata":{"_uuid":"b5952d024b5f37cfa21374a08bb92809cd5a5631","_cell_guid":"8cfb4b63-6450-466b-92dd-789a58ec1dcf"}},{"execution_count":null,"source":"params = {\n    'max_features': [0.25, 0.5, 0.7],\n    'max_depth' : [ 2,5,10,20]\n}\nrs = RandomizedSearchCV(cv=5, param_distributions=params, estimator=rfr, verbose=0)\nrs.fit(X_train,y_train.LogSalePrice.ravel())\nprint(rs.best_params_, rs.best_score_)","cell_type":"code","outputs":[],"metadata":{"_uuid":"0fc3f25a47ff2fda296f7892bf4d3b67f31bbd6d","_cell_guid":"6249b642-1ea2-4ba0-b6f2-8d6f2dc25081"}},{"source":"## **Finding the Tree in the Forest - Exploration of Random Forest Results**\n\nThis section was moved to an entirely different post, please visit :\nhttps://www.kaggle.com/timolee/finding-trees-through-the-forest-a-rf-dissection","cell_type":"markdown","metadata":{"_uuid":"5126cf53f5bf2b5eed3bee938c4fd26d6f1f74fe","_cell_guid":"edbab52a-6fb1-4d46-a348-bcd17773e1e3","collapsed":true}},{"source":"<a id='section8'></a>\n\n# **8. Scores, Loss, and Whats Under the Hood**\n<img src='http://2.bp.blogspot.com/-46ThgvQL3lo/UBCmiVWyHtI/AAAAAAAAAAc/tz-Mz62PSjQ/s1600/Low+Score+Judges.jpg' style='width:600px' />\n\nSo great, now you have you have some models. But now what? how do you score them all? lets take a look at some of the ways we can look at the models in hindsight\n- **`.coef_`** for linear models these are the coefficients that are assigned to your different features\n- **Does coefficient magnitude indicate importance?** Hard to say\n- **Did you scale your variables so they are comparable?**  Note from the table below, that the different coefficients aren't a good indicator of importance in this sample case. Since the sample data is not on the same scale, the coefficients can't be compared!\n\n| field | coef | sample data|\n|----|-----|-----|\n|house_pct_of_land | 100 | 0.03|\n|house_price| 20 | 100,0000|\n\n- **Big impact not same as Better predictions** A larger coefficient simple means it has a strong weight in calculating predictions, but that could lead to large error!\n- We will talk about better techniques to estimate importance later","cell_type":"markdown","metadata":{"_uuid":"cd1e29795a05b7f2529785978c8c49279c79f6e4","_cell_guid":"862cecb5-a117-4baf-bda1-84ebe8f44dfc","collapsed":true}},{"execution_count":null,"source":"print(lm.coef_)\nprint(rdgCV.coef_)\nprint(lgm.coef_)","cell_type":"code","outputs":[],"metadata":{"_uuid":"b9a719a80a85de53b7fd48b5a77df34c1e16fbac","_cell_guid":"85283aa0-ab04-46cb-b368-c64283c386b3"}},{"source":"## **Predictions and scoring regressions (continuous variables)**\n\n<img src='http://www.roperld.com/science/minerals/GasolinePricePrediction.jpg' />","cell_type":"markdown","metadata":{"_uuid":"173b1798175f939499cc7cc28885b1a55ce8b76d","_cell_guid":"4ea00a0a-a4eb-43f6-a412-0e8955e8e33e"}},{"execution_count":null,"source":"rfr = RandomForestRegressor(n_jobs=-1, n_estimators=100)\nrfr.fit(X_train,y_train)\n\ny_lm_pred = lm.predict(X_train)\ny_rdgCV_pred = rdgCV.predict(X_train)\ny_rfr_pred = rfr.predict(X_train)\n\nprint('-----training score ---')\nprint(lm.score(X_train, y_train))\nprint(rdgCV.score(X_train, y_train))\nprint(rfr.score(X_train, y_train))\nprint('----Validation score ---')\nprint(lm.score(X_valid, y_valid))\nprint(rdgCV.score(X_valid, y_valid))\nprint(rfr.score(X_valid, y_valid))","cell_type":"code","outputs":[],"metadata":{"_uuid":"a0bd553bffb636a04c4bd77e378077100c7d729d","_cell_guid":"20166d61-7bf0-43c7-8752-abb164be351f"}},{"source":"## **Predictions and Scoring Classifications**\n\n<img src='http://blogs.discovermagazine.com/discoblog/files/2012/04/shutterstock_15484942-e1335545917218.jpg' />\nClassification is a bit different than regression. We are trying to predict the class of something, instead of a continuous value. Such as identifying dogs and cats.\n\nWhen using clasification, or logistic regression, we can always get the predictions  `.predict`(usually 0, 1, 2 or discrete values). But there's a second function all `.predict_logproba` and `.predict_proba` with will give a [0,1] probability for every row. This is often used to rank predictions for classification scores (listed below)\n\n**why log prob vs. prob?** Gradient methods generally work better optimizing logp(x)logp(x) than p(x)p(x) because the gradient of logp(x)logp(x) is generally more well-scaled. That is, it has a size that consistently and helpfully reflects the objective function's geometry, making it easier to select an appropriate step size and get to the optimum in fewer steps.\n\n- **`.predict`** - gives 1's and 0's\n- **`.predict_logproba`** - gives array of log probabilities, obs vs. classes\n- **`.predict_proba`** - gives array of probabilities, obs vs. classes","cell_type":"markdown","metadata":{"_uuid":"6c4af1ee6060b90c782ef1bebfd6d16000e104b0","_cell_guid":"0c15d055-7b8a-46d0-85cb-bbbf54e0459d"}},{"execution_count":null,"source":"y_cls_train['above_200k'].values","cell_type":"code","outputs":[],"metadata":{"_uuid":"9bc31ae98a72657426c15fce45118fe258c55610","_cell_guid":"c1f16c39-270c-4d17-bb9d-cb71d5db49b5"}},{"execution_count":null,"source":"y_lgm_p = lgm.predict(X_cls_train)\ny_lgm_lpr = lgm.predict_log_proba(X_cls_train)\ny_lgm_pr = lgm.predict_proba(X_cls_train)\n\ny_lgm_lpr[:,0]\ny_lgm_pr[:,0]\ny_lgm_pr[:,1]\npd.DataFrame({'true': y_cls_train['above_200k'].values,\n              'predict':y_lgm_p, \n              'log_prob_0':y_lgm_lpr[:,0],\n              'log_prob_1':y_lgm_lpr[:,1],\n              'prob_0': y_lgm_pr[:,0],\n              'prob_1': y_lgm_pr[:,1]\n             }).head(20)","cell_type":"code","outputs":[],"metadata":{"_uuid":"e434a1ce22f8cfb5ba666c01d8bf312277336559","scrolled":true,"_cell_guid":"182c1a09-fe7e-41bb-a2ad-649acb35bcf5"}},{"source":"# **A Short Explanation of Loss**\n<img src='https://iwellnesslife.com/wp-content/uploads/2016/06/omg.jpg' style='width:400px' />\n\n## ** Continuous**\n\n**1. Mean Squared Error (MSE) L2 loss: minimizes the mean ** this is the most common loss for continous variables. It calculates the delta between prediction and actual and gets the squared average.  \n\n**2. Mean Absolute Error (MAE) L1 loss: minimizes the mean ** this is the most common loss for continous variables. It calculates the delta between prediction and actual and gets the absolute average.  \n\n**3. Quantile ** this is not normally implemented in machine learning, but instead of looking at absolute error, it looks at that rank error. In a race, MSE /MAE would care that the leader is 500ft ahead, but Quantile would only care that the runner is first.\n\n## **Classification**\n\n**4.  Log Loss AKA cross entropy loss**\nCross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0\n\n<img src='http://ml-cheatsheet.readthedocs.io/en/latest/_images/cross_entropy.png' style='width:400px' />\n\n\n**5. ROC - AUC - (Receiver Operating Characteristic) AUC - area under the curve ** This curve is plotting True Positve Rate against the False Positive Rate. The idea is for the line to hug the upper left hand corner. An english way of understanding this chart is \"what % of bad predictions do I have to accept (x-axis) to get maximum coverage of the true populate (y-axis) \n<img src='http://www.chioka.in/wp-content/uploads/2014/04/sample-ROC-curve.png' />\n\n**6. PR - AUC (precision - recall curve)** The idea curve is for the line to be in the uppe right hand corner. First some definitions:\n- Precision: when you guess true, how often are you right?\n- Recall: how many of true cases did you guess correctly out of all trues.\n<img src='http://www.chioka.in/wp-content/uploads/2014/04/sample-PR-curve.png' />\n\n\n** 7. Hinge : maximum-margin classification** - this is mainly used for SVM classifiers. \n\n** 8. Gini Coefficient ** pplies to binary classification and requires a classifier that can in some way rank examples according to the likelihood of being in a positive class.\n<img src='https://staesthetic.files.wordpress.com/2014/04/gini-calculation.png' style='width:400px' />","cell_type":"markdown","metadata":{"_uuid":"884ae3337d1f1930cad44fe230ca335fdc326518","_cell_guid":"7a3fe70e-a005-4fa5-ae70-b88de558e0f1"}},{"source":"## **Sklearn metrics - good ones to know**\n\n<img src='https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAAyxAAAAJDM5NmViNWQ3LTUxOTItNDg3Ny1hMjViLWNkZWI5ZWY5MGVkMQ.jpg' style='width:400px'/>\n\n- **`metrics.accuracy_score(y_true, y_pred[, ])`**\tAccuracy classification score.\n- **`metrics.auc(x, y[, reorder])`**\tCompute Area Under the Curve (AUC) using the trapezoidal rule\n- **`metrics.classification_report(y_true, y_pred)`**\tBuild a text report showing the main classification metrics\n- **`metrics.confusion_matrix(y_true, y_pred[, ])`**\tCompute confusion matrix to evaluate the accuracy of a classification\n- **`metrics.f1_score(y_true, y_pred[, labels, ])`**\tCompute the F1 score, also known as balanced F-score or F-measure\n- **`metrics.fbeta_score(y_true, y_pred, beta[, ])`**\tCompute the F-beta score\n- **`metrics.hinge_loss(y_true, pred_decision[, ])`**\tAverage hinge loss (non-regularized)\n- **`metrics.log_loss(y_true, y_pred[, eps, ])`**\tLog loss, aka logistic loss or cross-entropy loss.\n- **`metrics.precision_recall_curve(y_true, )`**\tCompute precision-recall pairs for different probability thresholds\n- **`metrics.precision_recall_fscore_support()`**\tCompute precision, recall, F-measure and support for each class\n- **`metrics.precision_score(y_true, y_pred[, ])`**\tCompute the precision\n- **`metrics.recall_score(y_true, y_pred[, ])`**\tCompute the recall\n- **`metrics.roc_auc_score(y_true, y_score[, ])`**\tCompute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n- **`metrics.roc_curve(y_true, y_score[, ])`**\tCompute Receiver operating characteristic (ROC)\n- **`metrics.zero_one_loss(y_true, y_pred[, ])`**\tZero-one classification loss.\n\n","cell_type":"markdown","metadata":{"_uuid":"cd890543d462fefa6e7717c5c6f0b8a32233a013","_cell_guid":"4026a5c1-1aaa-4848-8826-36a4cc83e089","collapsed":true}},{"execution_count":null,"source":"from sklearn.metrics import accuracy_score, auc, classification_report, \\\nconfusion_matrix, f1_score, log_loss, precision_recall_curve, roc_auc_score, roc_curve\n\nprint('Log Loss: ', log_loss(y_lgm_p, y_cls_train))\nprint('Accuracy_score: ', accuracy_score(y_lgm_p, y_cls_train))\nprint('confusion_matrix: ', confusion_matrix(y_lgm_p, y_cls_train))\nprint('Classification_Report: ', classification_report(y_lgm_p, y_cls_train))","cell_type":"code","outputs":[],"metadata":{}},{"execution_count":null,"source":"","cell_type":"code","outputs":[],"metadata":{"collapsed":true}}],"nbformat":4,"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","mimetype":"text/x-python","version":"3.6.3","nbconvert_exporter":"python","pygments_lexer":"ipython3","name":"python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}}}