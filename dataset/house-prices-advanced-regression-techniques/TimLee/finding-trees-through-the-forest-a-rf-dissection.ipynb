{"metadata":{"language_info":{"nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","file_extension":".py","name":"python","mimetype":"text/x-python","pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"cells":[{"metadata":{"_kg_hide-output":true,"_uuid":"96f13354f14a121b8daadedc3d3f5a8c210b8681","_kg_hide-input":true,"_cell_guid":"1aab2ed5-e5ed-4e73-8dd2-5c2a21565db7"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport numpy as np\nimport IPython\nimport graphviz\nimport re\nfrom IPython.display import display\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"17f0250c95241b965299c376298da737c1226ca0","_cell_guid":"8428b647-b58c-4e26-8c69-42645ce08f8e"},"source":"# **Random Forests are Great, but how do they work?**\n\n<img src='http://www.earthtimes.org/newsimage/rising-temperatures-affect-forests-carbon-storage-role-study_265.jpg' />\n\n**Wikipedia** defines random forests as: Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. \n\n## Table of Contents:\n\n**1. [Decision Tree - Baby Steps](#section1)**  <br>\n**2. [Real Data](#section2)**<br>\n**3. [Decision Tree from scratch](#section3)**<br>\n**4. [Nested Tree - TBD](#section4)**<br>\n**5. [Ensemble of Trees- TBD](#section5)**<br>\n**6. [Tree Confidence Intervals](#section6)**<br>\n**7. [Feature Importance](#section7)**<br>\n**8. [More or Less Features?](#section8)**<br>\n**10. [Feature Contribution](#section10)**<br>\n\n**Updated 12/7/2017 - added tree variance and feature importance at the bottom **\n\n### **Summary: A Random Forest, like a real forest is made of trees** \n\n<img src='https://i1.wp.com/dataaspirant.com/wp-content/uploads/2017/04/Random-Forest-Introduction.jpg?resize=690%2C345' />\n## **What is a Decision Tree?**\nDecision tree learning uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modelling approaches used in statistics, data mining and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees.\n\n### **Best way to understand one is to build one from scratch! Let's go!**","cell_type":"markdown"},{"metadata":{"_uuid":"a25f3d1968b336e58d9e766818d04f486b7c266d","_cell_guid":"5b1d399d-68a7-430d-88f9-0e9c36115a7f"},"source":"<a id='section1'></a>\n# **1. Decision Tree by Hand**\n<img src='https://eight2late.files.wordpress.com/2016/02/7214525854_733237dd83_z1.jpg?w=700' />\n### **For our baby case lets work through a tiny dataset**\n\n## **x = [1,2,3,4,5,6]**\n## **y = [1,1,0,1,0,0]**","cell_type":"markdown"},{"metadata":{"_uuid":"3e4ec79b9bec20ab21894dfefd1115904a8bb64e","collapsed":true,"_cell_guid":"5cf461da-997e-4efc-a3f3-21cb82d99fba"},"outputs":[],"source":"x = pd.DataFrame({'feature':[1,2,3,4,5,6]})\nx_vec = np.array([1,2,3,4,5,6], dtype=int)\ny = np.array([1,1,0,1,0,0], dtype=int)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"9b7ed144adaa1b207e919ec1a3f9ce754185ec5a","_cell_guid":"28029067-5e81-4642-9ff6-1271ccfccd43"},"source":"###  **The Decision Tree Algorithm - in English**\n\n1. Target: Looking for a division of the dataset where the groups are most evenly split. in this baby example, the ideal split has mostly `1`'s on one side and mostly `0`'s on other side\n2. Will check every element and separate the set into a left handed group and a right handed group and calculate a 'purity' score\n3. will loop and search for the **lowest** score\n\n### **Let's go from scratch, step by step**\n<img src='https://godmoneyandme.files.wordpress.com/2012/01/pencil-and-paper.jpg' />\n\n\n### **i=1, Split at 1st element**\n\n### **x = [1] // [2,3,4,5,6]**\n### **y = [1] // [1,0,1,0,0]**\n","cell_type":"markdown"},{"metadata":{"_uuid":"32655e5044f3ccbf0442bdc6f2df266ea8cb224a","_cell_guid":"4f6e20fb-2a60-46ca-91b1-a8e69189c221"},"outputs":[],"source":"lhs_x = x_vec<=x_vec[0]\nrhs_x = x_vec>x_vec[0]\nlhs_y = y[lhs_x]\nrhs_y = y[rhs_x]\n\nprint(lhs_x, rhs_x,lhs_y,rhs_y)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"6ef8b9217dd8813ea3dc419bb717429add061f7a","_cell_guid":"33fefc6f-a1a0-42b8-864e-f9fb9ebf4da4"},"source":"### **i=1, Calculate the purity score**\n\n$$\\sigma_{ylhs}n_{xlhs} + \\sigma_{yrhs}n_{xrhs} $$\n\nThe score is roughly the standard deviation of the y times the number of samples. Lets consider the extremes:\n\n1. If the split is perfect, one side will have all 1's and the other will have all 0's, the deviations will equal =0 and the score will be zero\n2. If the split is terrible, we will have a huge deviation times a large number of points that will give a very large score","cell_type":"markdown"},{"metadata":{"_uuid":"86ea0abfd2bf1f7469af4b4568e9afdc7107f0e8","_cell_guid":"eacb3903-bc9a-4d23-ae0d-317b300a754b"},"outputs":[],"source":"np.std(lhs_y) * sum(lhs_x) + np.std(rhs_y) * sum(rhs_x)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"5af511e98e9a5e04b34364fd280ddcb25ca8593b","_cell_guid":"f35ed440-6ce2-4cdc-b96d-6b817608a95f"},"source":"\n### **i=2, Calculate the purity score**\n\n### **x = [1,2] // [3,4,5,6]**\n### **y = [1,1] // [0,1,0,0]**\n","cell_type":"markdown"},{"metadata":{"_uuid":"ca0938753215b507bc1f69f1134b5e5a3b3887b2","_cell_guid":"7666d2ec-749f-47f6-96fc-0a2aa34873c5"},"outputs":[],"source":"lhs_x = x_vec<=x_vec[1]\nrhs_x = x_vec>x_vec[1]\nlhs_y = y[lhs_x]\nrhs_y = y[rhs_x]\n\nprint(lhs_x, rhs_x,lhs_y,rhs_y)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"639f8a723bf8bfdec48d69cdfcdfd1c712bf1463","_cell_guid":"1224a671-4df3-486e-a358-77d74b456e14"},"outputs":[],"source":"np.std(lhs_y) * sum(lhs_x) + np.std(rhs_y) * sum(rhs_x)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"ec33f1d236b6db279259869796f2f26b19e072f0","_cell_guid":"f4923772-735d-40b4-a7a6-5192792df78c"},"source":"### **i=3, Split at 3rd element**\n\n\n### **x = [1,2,3] // [4,5,6]**\n### **y = [1,1,0] // [1,0,0]**\n","cell_type":"markdown"},{"metadata":{"_uuid":"d5a25e80ae2aab9821b69fb55604da0e8df8c436","_cell_guid":"991be7ea-6947-4b94-af1f-c1d048e1b96d"},"outputs":[],"source":"lhs_x = x_vec<=x_vec[2]\nrhs_x = x_vec>x_vec[2]\nlhs_y = y[lhs_x]\nrhs_y = y[rhs_x]\n\nprint(lhs_x, rhs_x,lhs_y,rhs_y)\nnp.std(lhs_y) * sum(lhs_x) + np.std(rhs_y) * sum(rhs_x)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"8e47513e807908d0b619fa6513f48b619a2cc185","_cell_guid":"75e9f09b-bc2c-41ac-9a62-162b887ba1a8"},"source":"### **Conclusion: it seems that element 2~3 is a good split, with 2 samples going left, and 4 going right!**\n\n### **Let's compare to sklearn! We will use 1 decision tree with depth 1**\n","cell_type":"markdown"},{"metadata":{"_uuid":"fe8ec880308f025cc06ec60d27c13d292d045066","scrolled":true,"_cell_guid":"22228cd5-d82a-40b3-ab18-be517c7b300f"},"outputs":[],"source":"dtr = DecisionTreeRegressor(max_depth=1)\ndtr.fit(x,y)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"21f9114ad0059a97c9668d8c5a0b09bdd4583a93","_cell_guid":"f398956f-e9bd-4756-a3c6-1d1b6770e590"},"source":"### **Tree Drawing function**","cell_type":"markdown"},{"metadata":{"_uuid":"e2f2eec9dbc427245bfd140a471a54babde2ff97","_cell_guid":"496e9195-74e9-47d4-92cb-f7f81e4e2ed4"},"outputs":[],"source":"def draw_tree(t, df, size=10, ratio=0.6, precision=0):\n    \"\"\" Draws a representation of a random forest in IPython.\n    Parameters:\n    -----------\n    t: The tree you wish to draw\n    df: The data used to train the tree. This is used to get the names of the features.\n    \"\"\"\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,\n                      special_characters=True, rotate=True, precision=precision)\n    IPython.display.display(graphviz.Source(re.sub('Tree {',\n       f'Tree {{ size={size}; ratio={ratio}', s)))\ndraw_tree(dtr, x, precision=3)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"f63addb03df178d220508b1cf08d7c4cacc123b0","_cell_guid":"1e1c6adc-5670-460d-80ae-39214e3fd649"},"source":"### **We see we are consistent with sklearn**","cell_type":"markdown"},{"metadata":{"_uuid":"55734be2a55fdab9a87cc323091e95d81190ad21","_cell_guid":"bc983194-6d9f-42c8-b6ca-443b5799460e"},"source":"<a id='section2'></a>\n\n# **2. On to the real Data:**\n\n### **We will use Iowa Housing Data from this kaggle competition for the rest of our exploration**\n<img src='https://www.reno.gov/Home/ShowImage?id=7739&t=635620964226970000' />","cell_type":"markdown"},{"metadata":{"_uuid":"ab0c0c9d756b58f9b174ade7f5414d39df3b26e9","collapsed":true,"_cell_guid":"35f61dfc-7e9a-42fa-963e-52b639840762"},"outputs":[],"source":"train_df = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"abd7f24ebf1dca0d3f67c33b3ebc1f6a6b4732dd","_cell_guid":"b5f3386c-d5d9-4035-88d4-30c287a95756"},"source":"### **For simplicity lets choose only a few features**","cell_type":"markdown"},{"metadata":{"_uuid":"dd5387ebd309c3e24347ff8a74db2edb93df285d","_cell_guid":"e9c928c2-104c-409b-88a5-6bed0314c55c"},"outputs":[],"source":"train_df.columns","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"4fe679a0dc36394d6886fff77b320d93e3d32541","_cell_guid":"7263cf42-a33f-47cf-8f9f-07524a4d1328"},"outputs":[],"source":"sample_df = train_df[['SalePrice','YrSold','GrLivArea','TotRmsAbvGrd']].copy()\nsample_df.head()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"4ac4c182a8ebb2a06d5ad9b26cec650206bacda9","_cell_guid":"972dce24-3745-4f16-8a3f-58bf4a226d62"},"source":"### **Split into X and y variables**\n","cell_type":"markdown"},{"metadata":{"_uuid":"ba0b0a56c6d7036d65922b578df00da08369e038","_cell_guid":"1b3859b9-f697-4911-9d61-65c69145cdf1"},"outputs":[],"source":"y_train = sample_df['SalePrice']\nX_train = sample_df[[x for x in sample_df.columns if x != 'SalePrice']]\nprint(y_train.shape, X_train.shape)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"dd589e7205c67f14a4b128dfa516cc86a1acdad1","_cell_guid":"a2a3b676-af4d-4b57-8370-90cbaee64206"},"source":"<a id='section2'></a>\n# **3. Make a 1-level Decision Tree**\n### **Sklearn: Make a Decision Tree with Depth 1**","cell_type":"markdown"},{"metadata":{"_uuid":"19e618bd3070c08bfe54ccade18c345432113982","_cell_guid":"bb233d3a-778c-4304-9dc4-1af7de6375b7"},"outputs":[],"source":"dtr = DecisionTreeRegressor(max_depth=1)\ndtr.fit(X_train,y_train)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"7cb07cf62033e4b0410f739d1fe9c9c16ae8c507","_cell_guid":"51772e3f-3ebf-4bcc-9f4c-f3e97a56d063"},"outputs":[],"source":"draw_tree(dtr, X_train, precision=2)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"2db3b9dcb1535b59bf5f7c60cd39c26d7332d1a3","_cell_guid":"61e24549-2d3c-4bd7-bd6f-89f32d8fae3f"},"source":"## **Splitting Algorithm : A simple loop**\n\n","cell_type":"markdown"},{"metadata":{"_uuid":"d13c684ead3e69b36a0b4cbf9fcdd5b66ea4777e","_cell_guid":"fb3b4ba5-4dea-4276-9f24-9448ac6b814d"},"source":"###  **English**\n1. Target: Looking for a division of the dataset where mostly 1's on one side and mostly 0's on other side\n2. Will check every element and separate the set into a left handed group and a right handed group and calculate a 'purity' score\n3. will loop and search for the lowest score\n\n### **Score Calculation**:\n\n    - left_grp_Ytrain_deviation * left_Xtrain_sum + right_grp_Ytrain_deviation * right_Xtrain_sum\n\n### **Python Explicit Code: Setup**\n- `feature_idx =1` : we can choose which feature we want to look at \n- `stored_score = float('inf')` : choose a very high starter score\n- `stored_split_feature = 0` this is redundant right now, but it would be the idea feature to split on\n- `stored_split_value = 0` - this is the actual data-frame value to split on, for us `GrLivArea` is in ft\n- `all_indexes = [x for x in X_train.index]` : makes list of all possible indexes\n- `feature_list = X_train.columns` - feature list of column names, for labeling\n\n### **Datasetup**\n\n- `x,y = X_train.iloc[all_indexes,feature_idx],y_train.values[all_indexes]` pulls out the actual values\n\n### **Split logic: for a given index**\n\n#### **X values split**\n- `lhs_x = x<=x[split_index]` : for whatever split value you are at, how many values are less\n- `rhs_x = x>x[split_index]` : how many values are above the selected value\n\n#### **y values split**\n- `lhs_y_std = y[lhs_x].std()` : the stdev of the left hand y values\n- `rhs_x_std = y[rhs_x].std()` : the stdev of the right hand y values\n\n#### **Calc the score**\n- `curr_score = lhs_y_std*lhs_x.sum() + rhs_x_std*rhs_x.sum()` \n\n#### **If score is better, store the data**\n\n```python\n if curr_score<stored_score: \n            print('split index :%d lhs ct: %d | rhs ct: %d| %f' % (split_index, \n                                                                   len(lhs_x),\n                                                                   len(rhs_x), \n                                                                   curr_score))\n            stored_split_feature = feature_idx\n            stored_score = curr_score\n            stored_split_value = x[split_index]\n```\n","cell_type":"markdown"},{"metadata":{"_uuid":"776b2860720aea183bb9de1f49588b7cecb71ab5","_cell_guid":"9b5292b7-6d43-475b-8e53-7c09620c304a"},"source":"## **3.1 Our Loop representation of a 1-level Tree**\n<img src='http://www.onlineteachinghub.com/wp-content/uploads/2015/09/prog101-2.jpg' />","cell_type":"markdown"},{"metadata":{"_uuid":"40cef8c8b191d24fdb41149a933ab1494cc5b551","_cell_guid":"ec70f468-1409-44b1-8294-85232cf74bde"},"outputs":[],"source":"feature_idx =1\nstored_score = float('inf')\nstored_split_feature = 0\nstored_split_value = 0\n\nall_indexes = [x for x in X_train.index]\n\nfeature_list = X_train.columns\nx,y = X_train.iloc[all_indexes,feature_idx],y_train.values[all_indexes]\n\nprint('start loop')\nfor split_index in range(1, x.shape[0]-1):\n    lhs_x = x<=x[split_index]\n    rhs_x = x>x[split_index]\n\n    if rhs_x.sum()==0:\n        continue\n    else:\n        lhs_y_std = y[lhs_x].std()\n        rhs_y_std = y[rhs_x].std()\n\n        curr_score = lhs_y_std*lhs_x.sum() + rhs_y_std*rhs_x.sum()\n        \n        if curr_score<stored_score: \n            print('split index :%d lhs ct: %d | rhs ct: %d| score: %f' % (split_index, sum(lhs_x),sum(rhs_x), curr_score))\n            stored_split_feature = feature_idx\n            stored_score = curr_score\n            stored_split_value = x[split_index]\n\n\nprint(stored_score, feature_list[stored_split_feature], stored_split_value)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"4fb06507746da837435e4b539220bd6073d2a94c","_cell_guid":"34ee82fa-7546-417e-87a9-98678d3879bb"},"source":"### **We see we have the same answer! GrLiv Area around 1487 sqft!**","cell_type":"markdown"},{"metadata":{"_uuid":"b777df6dd3dfc71599cace8c1043be03113326df","_cell_guid":"9bec9d85-8647-4020-980b-61d895d37fb3"},"source":"### **3.2 the Class version: Lets turn it into a class with a single function (still only doing 1 feature)**\n\n<img src='http://4.bp.blogspot.com/-ANYNUQrNZug/T4FMIgqFMlI/AAAAAAAAACo/al-4GKDFawM/s1600/oop%5B1%5D.jpg' />\n\n- **Tricky hard to change `variables` to `self.variables`:** be sure to keep track of these, its hard to find and replace all of them. I highly recommend restartting your kernel to make sure old variable values don't remain from a previous calculation and then fail to throw an error when prototyping your tree \n\n- **Dont repeat yourself** try to streamline as much as possible\n\n- **Don't send data to functions in the same class**: when acting on the data, always use the `self.` versions. The persistence between functions (all functions can access the same data) will help avoid version issues and naming problems","cell_type":"markdown"},{"metadata":{"_uuid":"6a5ac209e0b292d21f0d5091285f7f3073083f89","collapsed":true,"_cell_guid":"89b89ebc-7409-4196-a428-40c5fad308d1"},"outputs":[],"source":"\nclass myIndecisionTree():\n    def __init__(self,x,y):\n        self.indexes = x.index\n        self.x = x\n        self.y = y\n        self.feature_idx =1\n        self.stored_score = float('inf')\n        self.stored_split_feature = 0\n        self.stored_split_value = 0\n        self.feature_list = x.columns\n    \n    def find_split_in_single_feature(self, feature_idx):\n        X_train = self.x\n        y_train = self.y\n\n        x,y = X_train.iloc[self.indexes,feature_idx],y_train.values[self.indexes]\n        print('start loop')\n        for split_index in range(1, self.x.shape[0]-1):\n            lhs_x = x<=x[split_index]\n            rhs_x = x>x[split_index]\n\n            if rhs_x.sum()==0:\n                print(self.stored_score, feature_list[self.stored_split_feature], self.stored_split_value)\n            else:\n                lhs_y_std = y[lhs_x].std()\n                rhs_x_std = y[rhs_x].std()\n\n                curr_score = lhs_y_std*lhs_x.sum() + rhs_x_std*rhs_x.sum()\n                if curr_score<self.stored_score: \n                    print('split index :%d lhs ct: %d | rhs ct: %d| score: %f' % (split_index, sum(lhs_x),sum(rhs_x), curr_score))\n                    self.stored_split_feature = feature_idx\n                    self.stored_score = curr_score\n                    self.stored_split_value = x[split_index]\n\n        print(self.stored_score, self.feature_list[self.stored_split_feature], self.stored_split_value)\n","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"6a0636f6c6516703db3122cf1870081f8360e6b7","_cell_guid":"e3a5ec70-1b6f-49a1-9fd6-92d0c9565f33"},"outputs":[],"source":"myTree = myIndecisionTree(X_train,y_train)\nmyTree.find_split_in_single_feature(1)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"54d312aa3e2ffe37b4536fa51a3b71eb79f1c652","_cell_guid":"bbfc63cb-8aea-4e37-a4c0-20f78662b296"},"source":"### **3.3: Make a function for all features**\n\n<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/8/82/CPT-OOP-interfaces.svg/300px-CPT-OOP-interfaces.svg.png' />\n\nNote that when we call this function we don't have to keep passing around updated scores or split values. That is taken care of by using the `self.` notation. ","cell_type":"markdown"},{"metadata":{"_uuid":"f782f5d6262ead0fb1a1693448427e512a896fb7","collapsed":true,"_cell_guid":"d4a258be-b635-46d6-a235-b339ffdf1bb5"},"outputs":[],"source":"class myIndecisionTree():\n    def __init__(self,x,y):\n        self.indexes = x.index\n        self.x = x\n        self.y = y\n        # self.feature_idx =1\n        self.stored_score = float('inf')\n        self.stored_split_feature = 0\n        self.stored_split_value = 0\n        self.feature_list = x.columns\n\n    \n    # ======== NEW FUNCTION =========================\n    def check_all_features(self):\n        for i in range(len(feature_list)-1):\n            self.find_split_in_single_feature(i)  \n            \n        print(self.stored_score, self.feature_list[self.stored_split_feature], self.stored_split_value)\n    # ======== NEW FUNCTION =========================    \n    \n    def find_split_in_single_feature(self, feature_idx):\n        X_train = self.x\n        y_train = self.y\n\n        x,y = X_train.iloc[self.indexes,feature_idx],y_train.values[self.indexes]\n        print('start loop')\n        for split_index in range(1, self.x.shape[0]-1):\n            lhs_x = x<=x[split_index]\n            rhs_x = x>x[split_index]\n\n            if rhs_x.sum()==0:\n                continue\n            else:\n                lhs_y_std = y[lhs_x].std()\n                rhs_x_std = y[rhs_x].std()\n\n                curr_score = lhs_y_std*lhs_x.sum() + rhs_x_std*rhs_x.sum()\n                if curr_score<self.stored_score: \n                    print('split index :%d lhs ct: %d | rhs ct: %d| score: %f' % (split_index, sum(lhs_x),sum(rhs_x), curr_score))\n                    self.stored_split_feature = feature_idx\n                    self.stored_score = curr_score\n                    self.stored_split_value = x[split_index]\n\n        \n","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"f87c1c21b40552451323ea59ffa014f74c903b1a","_cell_guid":"a04e5470-9dd8-40b9-910c-0f853d2e3501"},"source":"### **Test it with a different data set - Single Decision Tree**","cell_type":"markdown"},{"metadata":{"_uuid":"156f6a808994ec6c2a549c0a5fd54aa5bd0e589f","collapsed":true,"_cell_guid":"9108a4ce-e419-4172-ab59-67cc701c62d4"},"outputs":[],"source":"y_train = sample_df['SalePrice']\n#X_train = sample_df[['GrLivArea']]\nX_train = sample_df[[x for x in sample_df.columns if x not in ['SalePrice','GrLivArea']]]","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"0884ed9b27c956f1942ce8bbb689e6f9f83acf71","_cell_guid":"46b7f1e0-e876-4686-8009-581777f9679a"},"outputs":[],"source":"dtr = DecisionTreeRegressor(max_depth=1)\ndtr.fit(X_train,y_train)\n","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"c15f5e7895edef9ff5ccc6f1aa810566a5011681","_cell_guid":"0b281fbf-3cdc-4c9d-9d67-23594cc48313"},"source":"### **Sklearn**","cell_type":"markdown"},{"metadata":{"_uuid":"f536b667ea2439ee52a5db17969eb5dcccef9963","_cell_guid":"4fb44344-73d3-436a-9911-4cc2ae66ac09"},"outputs":[],"source":"draw_tree(dtr, X_train, precision=2)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"4f6fefd4460a282fd80d4e9b3ba0791a9fb9dc08","_cell_guid":"b448f076-0e7a-4c94-83af-fb53bc8715e7"},"outputs":[],"source":"myTree = myIndecisionTree(X_train,y_train)\nmyTree.check_all_features()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"d5ad1875291058dd215a39874fa4a9f793d155b1","_cell_guid":"c40157f4-0b64-4b80-a9aa-8238d52c476f"},"source":"<a id='section4'></a>\n## **4. Making our multi-level tree!**\n-- TBD\n\n<a id='section5'></a>\n## **5. Making our ensemble tree!**\n-- TBD","cell_type":"markdown"},{"metadata":{"_uuid":"c21154b061a79cac9929a66067f499b4b6be2cb3","collapsed":true,"_cell_guid":"3dfdb4f8-65ba-426f-825f-a5a1923865f9"},"source":"<a id='section6'></a>\n## 6. **Confidence Interval for Random Forests**\n\n<img src='http://berkeleysciencereview.com/wp-content/uploads/2014/04/to_err_is_human_by_velica-d4i9wjr.jpg' />\n\nHow can we get an idea of how much the predictions will vary for a random forest? Is there a way to understand the range or prediction? Yes! Let's go through a quick walkthrough","cell_type":"markdown"},{"metadata":{"_uuid":"70dd5422a948c210750d9edc9c224f611b5d246c","_cell_guid":"8ebeb0c8-651d-4634-95d0-0e7a694fc82d"},"source":"### **Let's Setup a small subset of the data**","cell_type":"markdown"},{"metadata":{"_uuid":"0ae924331fba3836344ece8023993fc03520b5e9","_cell_guid":"af9a7aa8-8a24-44e1-ae7e-172660d63c97"},"outputs":[],"source":"n = 100\nindexes = [x for x in range(n)]\nsample_df = train_df.loc[indexes,['SalePrice','YrSold','TotRmsAbvGrd','Neighborhood']].copy()\nsample_df.head()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"abccaa7c241f92ad10632f13d34e7cf84058bd93","collapsed":true,"_cell_guid":"31032b0c-017c-44e2-b9a2-0b818283a2c7"},"outputs":[],"source":"y_train = sample_df['SalePrice']\nX_train = sample_df[[x for x in sample_df.columns if x not in ['SalePrice']]]\nX_matrix = pd.get_dummies(X_train)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"ecf533a70381e8e508644ce7bbd68aa6c4a53086","_cell_guid":"30374d15-dd27-49ba-b97d-4dae4fa283e8"},"outputs":[],"source":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor(n_estimators=10, max_depth=3)\nrfr.fit(X_matrix, y_train)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"0388f7f05fae1128ce3b4dd70b4464c872c91019","_cell_guid":"9552d3b2-9e89-4f43-816b-cfaf9c723d9b"},"source":"### ** Note ! The underlying trees are very different!******","cell_type":"markdown"},{"metadata":{"_uuid":"69a7d5165826a54c57f620aa2bbd5802fd7bee94","_cell_guid":"a842a51f-8eb3-44bc-a57e-22e3f46fed97"},"outputs":[],"source":"draw_tree(rfr.estimators_[0], X_matrix)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"9f8ac0ce9c64ba99dbf5f89e1e5e7bc3c09da93a","_cell_guid":"2d82ce71-26f1-44fd-b61d-6b873e72c74d"},"outputs":[],"source":"draw_tree(rfr.estimators_[1], X_matrix)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"29b734656f0f80b2776e0945fb91a957baf7a64e","_cell_guid":"bec73d73-bc14-4465-a7eb-7dbdc82c12c2"},"source":"## **Since  Every Tree is Different, every prediction for the same point will vary as well **\n<img src='http://monstermathclub.com/wp-content/uploads/2017/02/types-of-trees-delightful-miti-types-of-trees-mbetula-birch-mchikichi-palm-tree-mfune-beech-tree.jpg'  style='width:500px'/>\n\n### **Let's collect the individual predictions by tree **","cell_type":"markdown"},{"metadata":{"_uuid":"238f1a4aeb51afab942aae8a5bf1551bee2a1662","_cell_guid":"94bd2918-bd29-4aed-ac6b-403526dbf8bd"},"outputs":[],"source":"y_pred = rfr.predict(X_matrix)\nall_trees = rfr.estimators_\nall_predictions = [tree.predict(X_matrix) for tree in all_trees]\nfor x in all_predictions:\n    print(len(x))","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"ac12822e6b16d81bdec765930102a0116373f109","_cell_guid":"54cebb53-86af-4c3b-9656-f956005579ad"},"source":"### ** Stack them into a single array **","cell_type":"markdown"},{"metadata":{"_uuid":"cec6d9379ea66f845d30b9f56f75200d90ccdf8b","_cell_guid":"7fea4840-fce1-49be-ac11-f65db950ecb9"},"outputs":[],"source":"all_predictions_by_point = np.stack(all_predictions, axis=0)\nprint(all_predictions_by_point.shape)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"715630a7f0f7d3d36cd1e0cc2e214ec414f6ae57","_cell_guid":"009a53ec-7dfa-4b17-98b3-41fc0d389ed2"},"source":"### ** Let's work through a single point sample, calculate a 95% Confidence interval**","cell_type":"markdown"},{"metadata":{"_uuid":"8c8ca3b463d4e3c7b45e8fc9a0ede8d7682b0b53","_cell_guid":"d783a3c5-0b94-4f6b-8ffc-ca27a5c897f4"},"outputs":[],"source":"point_30 = [x[30] for x in all_predictions_by_point]\npoint_30 = sorted(point_30)\npoint_30","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"b890abaf1f41b65d8e82e79b36a76c680905cabd","_cell_guid":"cf10fb2e-517d-4981-a567-dfeb81505448"},"outputs":[],"source":"ci_percent = 95\nnp.percentile(point_30, (100-ci_percent) / 2 )","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"e4220750c7e916d0094fb7bd037530da2449baf0","_cell_guid":"3eb8f762-ac0b-4943-8bcd-9e40a342752c"},"outputs":[],"source":"np.percentile(point_30, 100-(100-ci_percent) / 2 )","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"597de12a39662ef1e76db2766d83fe476d139265","_cell_guid":"8244f596-2695-4bb8-8745-29709543d79e"},"source":"### ** Now lets calc the CI's for the rest of the samples and Collect in a table**\n\n<img src='http://1.bp.blogspot.com/-S4ypXNVNeh0/VwmnJBXJ3ZI/AAAAAAAACas/AgWJ5aLwl7QFEkOYPgzDtBV8_HOAZFHQg/s1600/Garfield-1.jpg' />","cell_type":"markdown"},{"metadata":{"_uuid":"dffa39353a97287f4c4a8356615530ab7bf9e42c","_cell_guid":"17604989-ca01-4633-81ba-c1305d8a97ec"},"outputs":[],"source":"ci_percent= 95\nci_down = np.percentile(all_predictions_by_point, (100-ci_percent) / 2 , axis=0)\nci_up = np.percentile(all_predictions_by_point, 100-(100-ci_percent) / 2 , axis=0)\n\ntree_ci = pd.DataFrame({\n    'x' : [x for x in range(n)],\n    'y_actual' : y_train,\n    'y_pred' :y_pred,\n    'y_95' : ci_up,\n    'y_05' : ci_down,\n    'y_ci_range' : ci_up-ci_down\n})\n\ntree_ci.sort_values(by='y_actual', inplace=True)\ntree_ci['x_sorted'] = [x for x in range(n)]\ntree_ci.head()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"4eea46f608e6f181ea5f0122992d72bc20079cd0","_cell_guid":"3cb7d04b-669f-487b-9c52-4a04ecf42236"},"source":"### **Now lets plot our intervals! Note that we chose a low-level forest**","cell_type":"markdown"},{"metadata":{"_uuid":"7131c248d4298351c2d53a79e56a8ee53be1669a","_cell_guid":"f34e1adb-6b4e-4e0b-9379-e9d9166c0534"},"outputs":[],"source":"import matplotlib.pyplot as plt\n%matplotlib inline\nfig = plt.figure(figsize=(10,6))\nax = fig.add_subplot(111)\nax.scatter(tree_ci['x_sorted'], tree_ci['y_pred'])\nax.plot(tree_ci['x_sorted'], tree_ci['y_actual'], c='green')\nax.scatter(tree_ci['x_sorted'], tree_ci['y_05'], c='red', alpha='0.1')\nax.scatter(tree_ci['x_sorted'], tree_ci['y_95'], c='red', alpha='0.1')\n","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"75841fd7f1f88c06cf82cf764059e86a138a2ed3","_cell_guid":"02f876df-f263-4020-9b5e-23ebf50a459f"},"source":"### **Now lets plot our intervals! Note that we chose a low-level forest**","cell_type":"markdown"},{"metadata":{"_uuid":"195ff8f602abcda657a6fc20c8f732d7f84c5ae5","collapsed":true,"_cell_guid":"8c763b3f-8e4a-4141-a217-9fcd574bd155"},"outputs":[],"source":"x_index_top20_widest = tree_ci.sort_values(by='y_ci_range', ascending=False)[:20]['x'].values","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"b5a6b3982423f5d7e980dcd053e0968a9a6c7502","_cell_guid":"339612b4-1d24-49e8-88de-f5c2030254cf"},"outputs":[],"source":"worst_datapoints_w_features = X_train.iloc[x_index_top20_widest,:]\nworst_datapoints_w_features.head(5)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"4a16fd8ba2de2a92ec8d1ab67c2503839f7c85e0","_cell_guid":"005c02b1-3d20-4fbb-a7ad-2eae45fe57be"},"source":"## ** Which specific attributes lead to more uncertainty?**\n\n<img src='https://hopewissel.files.wordpress.com/2016/04/6a00d8341ca4d953ef01b7c7c17599970b.jpg'  style='width:300px'/>","cell_type":"markdown"},{"metadata":{"_uuid":"9292ef7b5af4dc3ff0a59d8724d725149976e6f7","_cell_guid":"d9083b7c-0ecf-4487-977a-7ca58ab1d3a3"},"outputs":[],"source":"X_train.iloc[x_index_top20_widest,:]['YrSold'].value_counts().plot.barh()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"e44b89c3f739bc4beadc17f3dd07ad62803dd7a4","_cell_guid":"b76819e0-7dbf-4714-aff8-174e6ed58d4d"},"outputs":[],"source":"X_train.iloc[x_index_top20_widest,:]['TotRmsAbvGrd'].value_counts().plot.barh()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"b50dfb2b7c0e33da2f6d043e066fb58a2738ddc8","_cell_guid":"ee130d6e-67e9-403c-a0ed-ee192e4d1d80"},"outputs":[],"source":"X_train.iloc[x_index_top20_widest,:]['Neighborhood'].value_counts().plot.barh()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"00f4fe9d8810efff388bd424174bb3679ca00c5c","_cell_guid":"1d97a101-d415-48a3-98ca-9343b81c0d1d"},"source":"<a id='section7'></a>\n# **7. Features: Not all of them are important**\n\n<img src='http://www.mirchu.net/wp-content/uploads/2014/09/iPhone-6-important-features.png' />","cell_type":"markdown"},{"metadata":{"_uuid":"4d72dd1884b5623af44e94eb6c767ce5b592e4ed","_cell_guid":"d4305156-b329-485f-9bce-d4596f083f7d"},"source":"## **How do we determine what are the important features?**\n\nLet's prep and train the data as before","cell_type":"markdown"},{"metadata":{"_uuid":"2ef2cf7024737f60f254eccd6f9d75bc48799940","_cell_guid":"0bdcc94b-c976-40c6-8c3d-2bf125f14332"},"outputs":[],"source":"n = 100\nindexes = [x for x in range(n)]\nsample_df = train_df.loc[indexes,['SalePrice','YrSold','TotRmsAbvGrd','Neighborhood']].copy()\ny_train = sample_df['SalePrice']\nX_train = sample_df[[x for x in sample_df.columns if x not in ['SalePrice']]]\nX_matrix = pd.get_dummies(X_train)\nrfr = RandomForestRegressor()\nrfr.fit(X_matrix,y_train)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"c27f4a04b1691fc6009430a8eda599530c761041","_cell_guid":"e8635847-f094-4627-8150-ead2914226c4"},"source":"## **Define our Metric**","cell_type":"markdown"},{"metadata":{"_uuid":"78e68439d638230cff4c8132d0069f36b74b2760","_cell_guid":"c2f27e2d-9928-40f0-a751-87a987845ad6"},"outputs":[],"source":"from sklearn.metrics import mean_squared_error\ndef rmse(x1,x2):\n    return np.sqrt(mean_squared_error(x1,x2))\n\ny_pred = rfr.predict(X_matrix)\norig_score = rmse(y_pred,y_train)\norig_score","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"04727ec72415397f796dbd2464c0ff4e13804d26","_cell_guid":"afdb014f-c691-457a-b5e5-4d511c89092f"},"source":"## **Our Approach: Jumble one column and see how the prediction changes**\n\n<img src='https://lh3.ggpht.com/_oZ6h3pArsOdjbJZ1GCNTlpcvnsO9PU0wjpAuRuhxNBkI2CMxnMjDydm82-AiNqGEYqZ=w300' />","cell_type":"markdown"},{"metadata":{"_uuid":"89d24ba1d08b1031da4727763b668d2f3418cbf2","_cell_guid":"cd98e7c3-0a67-487c-b825-9aeb210ff28a"},"outputs":[],"source":"def jumble_column(df, column_name):\n    idx = X_matrix.index\n    scrambled_idx = np.random.permutation(idx)\n    jumbled = df.copy()\n    values = jumbled[column_name].values\n    jumbled[column_name] = values[scrambled_idx]\n    return jumbled\n\nX_jumble_year = jumble_column(X_matrix,'YrSold')\ny_pred = rfr.predict(X_jumble_year)\njumbled_year_score = rmse(y_pred,y_train)\njumbled_year_score - orig_score","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"fc3dcde7040cbf68c25573d608035df019fb369d","_cell_guid":"ab08fee3-4f49-4fc8-a12e-bd59f8bca006"},"outputs":[],"source":"X_jumble_rm = jumble_column(X_matrix,'TotRmsAbvGrd')\ny_pred = rfr.predict(X_jumble_rm)\njumbled_rm_score = rmse(y_pred,y_train)\njumbled_rm_score - orig_score","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"2c8077d2e7b689ca87c67031be46705e4b239b1b","_cell_guid":"d76b657a-2c05-404f-bf0a-fde0d23927b0"},"outputs":[],"source":"X_jumble_ngh = jumble_column(X_train,'Neighborhood')\nX_jumble_ngh = pd.get_dummies(X_jumble_ngh)\ny_pred = rfr.predict(X_jumble_ngh)\njumbled_ngh_score = rmse(y_pred,y_train)\njumbled_ngh_score - orig_score","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"8bc11090540ae63962af9bf193611bcd91015293","_cell_guid":"2c753ad7-f0a0-4f73-8031-fcc247b07c0a"},"source":"## **Finally we see that Neighborhood matters the most, then number of rooms**","cell_type":"markdown"},{"metadata":{"_uuid":"642fc8fc602ea2e79398948608d58ffe2221bc78","_cell_guid":"6bce1efc-ff63-46dc-84db-b27dd24c7b3d"},"outputs":[],"source":"to_plot= pd.DataFrame({'headings' : ['Year Sold', 'Total Rooms', 'Neighborhood'],\n              'change_in_error':[jumbled_year_score-orig_score, jumbled_rm_score-orig_score, jumbled_ngh_score-orig_score]\n             })\n\nto_plot.plot.bar('headings','change_in_error')","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"b30f4904a80f6c6a67fcbfc44ed0efae3d52aa7b","_cell_guid":"b183fe6c-d450-4d04-8b87-16f6156f5a42"},"source":" ## **But what about specific neighborhoods?**\n \n <img src='https://s3.amazonaws.com/lowres.cartoonstock.com/social-issues-skid_row-muggings-criminal-bad_neighbourhood-bad_neighborhood-ksmn3365_low.jpg' />\n \n#### **Will jumble the one-hot encoded variables instead of the full categorical column**","cell_type":"markdown"},{"metadata":{"_uuid":"d78644de829b9f8e22f8b4e682c35e40af28f5fa","_cell_guid":"f49d96c9-b78b-4c36-a414-fc7fea0b30f9"},"outputs":[],"source":"def jumble_score(df, column_name, model):\n    orig_predict = model.predict(df)\n    orig_score = rmse(orig_predict,y_train)\n    tmp_jumble = jumble_column(df,column_name)\n    tmp_predict = model.predict(tmp_jumble) \n    return rmse(tmp_predict,y_train) - orig_score\n    \nscores =[]\nfor col in X_matrix:\n    scores.append({'feature': col,\n                   'imp': jumble_score(X_matrix, col, rfr) \n                  })\npd.DataFrame(scores).sort_values(by='imp', ascending=False).plot('feature','imp', figsize=(10,4))","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"a7a6432cd472a372afcc397c1b6657226fca716a","_cell_guid":"46481a32-d73f-4b9f-866c-1eeb9acac065"},"source":"### **Let's see what SKLearn has to say**","cell_type":"markdown"},{"metadata":{"_uuid":"0b9f666dc846a0aa5c24e351ff8710a0de649576","_cell_guid":"239b485f-f43b-4ee4-9459-4b1474752075"},"outputs":[],"source":"rfr = RandomForestRegressor(n_jobs=-1)\nrfr.fit(X_matrix,y_train)\nfeat_imp = pd.DataFrame({\n    'features': X_matrix.columns,\n    'imp' :rfr.feature_importances_\n})\nfeat_imp.sort_values(by='imp', inplace=True, ascending=False)\nfeat_imp.plot('features','imp', figsize=(10,5))","execution_count":null,"cell_type":"code"},{"metadata":{},"source":"<a id='section8'></a>\n# **8. Reducing Features?**\n\n<img src='https://www.homedepot.com/hdus/en_US/DTCCOMNEW/fetch/DIY_Projects_and_Ideas/Outdoor/Guides/1440-desktop-pruning-hero.jpg' />\nSometimes there's a lot of features. And you may suspect that some of them are the same. How can we trim down the feature selection?\n","cell_type":"markdown"},{"metadata":{},"source":"#### ** Let's train another baseline RF model to get some baseline importance** ","cell_type":"markdown"},{"metadata":{},"outputs":[],"source":"y_feat = train_df['SalePrice'].values\nfeatures = [x  for x in train_df.columns.values if x not in ['SalePrice','GarageYrBlt','LotFrontage','MasVnrArea']]\nx_feat = train_df[features]\nx_feat_matrix = pd.get_dummies(x_feat)","execution_count":null,"cell_type":"code"},{"metadata":{},"outputs":[],"source":"m_rf = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm_rf.fit(x_feat_matrix,y_feat)","execution_count":null,"cell_type":"code"},{"metadata":{},"outputs":[],"source":"m_rf.score(x_feat_matrix, y_feat)","execution_count":null,"cell_type":"code"},{"metadata":{},"source":"#### **From the RF object, we can pull feature importance and plot**","cell_type":"markdown"},{"metadata":{},"outputs":[],"source":"feature_imp = m_rf.feature_importances_\nfeature_imp_df = pd.DataFrame({'cols':x_feat_matrix.columns.values, 'feat_imp':feature_imp})\nfeature_imp_df.sort_values(by='feat_imp', ascending=False, inplace=True)\nfeature_imp_df[:20].plot.barh(x='cols', y='feat_imp',figsize=(10,6))\n","execution_count":null,"cell_type":"code"},{"metadata":{},"source":"### **Keep top features, and re-run the model. Checking to see if the feature importance re-distributes at all**","cell_type":"markdown"},{"metadata":{},"outputs":[],"source":"to_keep = feature_imp_df[feature_imp_df['feat_imp'] > 0.01].cols\nX_bestfeat_matrix = x_feat_matrix[to_keep]","execution_count":null,"cell_type":"code"},{"metadata":{},"outputs":[],"source":"m_best = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm_best.fit(X_bestfeat_matrix, y_feat)","execution_count":null,"cell_type":"code"},{"metadata":{},"source":"#### **We do note that the columns have changed as well as the order. That means some of the fields have absorbed some of the smaller importances into their score**","cell_type":"markdown"},{"metadata":{},"outputs":[],"source":"feature_imp = m_best.feature_importances_\nfeature_imp_df = pd.DataFrame({'cols':X_bestfeat_matrix.columns.values, 'feat_imp':feature_imp})\nfeature_imp_df.sort_values(by='feat_imp', ascending=False, inplace=True)\nfeature_imp_df[:20].plot.barh(x='cols', y='feat_imp',figsize=(10,6))\n","execution_count":null,"cell_type":"code"},{"metadata":{},"source":"### **Another way to see the \"hierarchy\" of features : Dendrograms**\n\nHere's the process, first of all, the **spearman** coefficient is used to compare columns. Spearman's coefficient looks at **RANK** instead of actual distance. So if one dataset had an outlier 10x, it would not affect its RANK when looking at the order. So this `scipy` library will iteratively compare columns (features) to one another and compare if the ranks are very similiar. If they are, they will be grouped together on the same branch\n\n<img src = 'https://camo.githubusercontent.com/bc269d7bd72e9ba9e5af0957b2fbad1883aaf2bc/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f7468756d622f342f34652f53706561726d616e5f666967312e7376672f33303070782d53706561726d616e5f666967312e7376672e706e67'>","cell_type":"markdown"},{"metadata":{},"outputs":[],"source":"import scipy\nfrom scipy.cluster import hierarchy as hc","execution_count":null,"cell_type":"code"},{"metadata":{},"source":"#### **More rooms = more Squarefoot, More Garage Area = Garage Cars, Total Basement SQFT = 1st Flr SQFT**\n\nSo intepretting the chart below we some see of the columns are very closely related.\n- this affects feature importance, because it is split over two columns instead of 1\n- if we reduce the features and then recalculate, chances are the relative importance to \"overall qualit\" will increase\n- Is a good way to eliminate features post-modeling","cell_type":"markdown"},{"metadata":{},"outputs":[],"source":"corr = np.round(scipy.stats.spearmanr(X_bestfeat_matrix).correlation, 3)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16,10))\ndendrogram = hc.dendrogram(z, labels=X_bestfeat_matrix.columns, orientation='left', leaf_font_size=16)\nplt.show()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"ac8a1ec45b74226690c92479543c30a5bdfdc5df","_cell_guid":"17f40c46-e347-4c15-81a5-237b69cb909e"},"source":"<a id='section10'></a>\n## **10. Can we Quantify the $ of the decisions with in a Tree?**\n\n<img src='https://www.payoff.com/lift/articles/wp-content/uploads/2015/02/how-to-make-smart-money-decisions-810x355.jpg' />\n\n#### ** how much does each factor contribute to the overall score? **","cell_type":"markdown"},{"metadata":{"_uuid":"c51f3c329a5a869bdc06bd5c50bcb9ca27cb6597","_cell_guid":"0ea9f271-1622-4153-934a-3dd1f40175dc"},"outputs":[],"source":"rfr = RandomForestRegressor(n_jobs=-1, max_depth=5)\nrfr.fit(X_matrix,y_train)\nsingle_row = X_matrix.head(1)\nsingle_tree = rfr.estimators_[0]\nsingle_tree.predict(single_row)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"00d0a5c72df3cf31f5161cdb2d144a21f412fc9f","_cell_guid":"95fab28d-0b6f-4af9-814d-bc4a9ebc499d"},"outputs":[],"source":"draw_tree(single_tree, X_matrix)","execution_count":null,"cell_type":"code"},{"metadata":{"_kg_hide-output":true,"_uuid":"3b2777cc51b198b7c5ea3fb254bc28d076a6b405","_kg_hide-input":true,"collapsed":true,"_cell_guid":"c9f95980-612b-4283-901f-3b2229b1d812"},"outputs":[],"source":"import numpy as np\nimport sklearn\n\nfrom sklearn.ensemble.forest import ForestClassifier, ForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, _tree\nfrom distutils.version import LooseVersion\nif LooseVersion(sklearn.__version__) < LooseVersion(\"0.17\"):\n    raise Exception(\"treeinterpreter requires scikit-learn 0.17 or later\")\n\n\ndef _get_tree_paths(tree, node_id, depth=0):\n    \"\"\"\n    Returns all paths through the tree as list of node_ids\n    \"\"\"\n    if node_id == _tree.TREE_LEAF:\n        raise ValueError(\"Invalid node_id %s\" % _tree.TREE_LEAF)\n\n    left_child = tree.children_left[node_id]\n    right_child = tree.children_right[node_id]\n\n    if left_child != _tree.TREE_LEAF:\n        left_paths = _get_tree_paths(tree, left_child, depth=depth + 1)\n        right_paths = _get_tree_paths(tree, right_child, depth=depth + 1)\n\n        for path in left_paths:\n            path.append(node_id)\n        for path in right_paths:\n            path.append(node_id)\n        paths = left_paths + right_paths\n    else:\n        paths = [[node_id]]\n    return paths\n\n\ndef _predict_tree(model, X, joint_contribution=False):\n    \"\"\"\n    For a given DecisionTreeRegressor, DecisionTreeClassifier,\n    ExtraTreeRegressor, or ExtraTreeClassifier,\n    returns a triple of [prediction, bias and feature_contributions], such\n    that prediction ≈ bias + feature_contributions.\n    \"\"\"\n    leaves = model.apply(X)\n    paths = _get_tree_paths(model.tree_, 0)\n\n    for path in paths:\n        path.reverse()\n\n    leaf_to_path = {}\n    #map leaves to paths\n    for path in paths:\n        leaf_to_path[path[-1]] = path         \n    \n    # remove the single-dimensional inner arrays\n    values = model.tree_.value.squeeze()\n    # reshape if squeezed into a single float\n    if len(values.shape) == 0:\n        values = np.array([values])\n    if isinstance(model, DecisionTreeRegressor):\n        biases = np.full(X.shape[0], values[paths[0][0]])\n        line_shape = X.shape[1]\n    elif isinstance(model, DecisionTreeClassifier):\n        # scikit stores category counts, we turn them into probabilities\n        normalizer = values.sum(axis=1)[:, np.newaxis]\n        normalizer[normalizer == 0.0] = 1.0\n        values /= normalizer\n\n        biases = np.tile(values[paths[0][0]], (X.shape[0], 1))\n        line_shape = (X.shape[1], model.n_classes_)\n    direct_prediction = values[leaves]\n    \n    \n    #make into python list, accessing values will be faster\n    values_list = list(values)\n    feature_index = list(model.tree_.feature)\n    \n    contributions = []\n    if joint_contribution:\n        for row, leaf in enumerate(leaves):\n            path = leaf_to_path[leaf]\n            \n            \n            path_features = set()\n            contributions.append({})\n            for i in range(len(path) - 1):\n                path_features.add(feature_index[path[i]])\n                contrib = values_list[path[i+1]] - \\\n                         values_list[path[i]]\n                #path_features.sort()\n                contributions[row][tuple(sorted(path_features))] = \\\n                    contributions[row].get(tuple(sorted(path_features)), 0) + contrib\n        return direct_prediction, biases, contributions\n        \n    else:\n\n        for row, leaf in enumerate(leaves):\n            for path in paths:\n                if leaf == path[-1]:\n                    break\n            \n            contribs = np.zeros(line_shape)\n            for i in range(len(path) - 1):\n                \n                contrib = values_list[path[i+1]] - \\\n                         values_list[path[i]]\n                contribs[feature_index[path[i]]] += contrib\n            contributions.append(contribs)\n    \n        return direct_prediction, biases, np.array(contributions)\n\n\ndef _predict_forest(model, X, joint_contribution=False):\n    \"\"\"\n    For a given RandomForestRegressor, RandomForestClassifier,\n    ExtraTreesRegressor, or ExtraTreesClassifier returns a triple of\n    [prediction, bias and feature_contributions], such that prediction ≈ bias +\n    feature_contributions.\n    \"\"\"\n    biases = []\n    contributions = []\n    predictions = []\n\n    \n    if joint_contribution:\n        \n        for tree in model.estimators_:\n            pred, bias, contribution = _predict_tree(tree, X, joint_contribution=joint_contribution)\n\n            biases.append(bias)\n            contributions.append(contribution)\n            predictions.append(pred)\n        \n        \n        total_contributions = []\n        \n        for i in range(len(X)):\n            contr = {}\n            for j, dct in enumerate(contributions):\n                for k in set(dct[i]).union(set(contr.keys())):\n                    contr[k] = (contr.get(k, 0)*j + dct[i].get(k,0) ) / (j+1)\n\n            total_contributions.append(contr)    \n            \n        for i, item in enumerate(contribution):\n            total_contributions[i]\n            sm = sum([v for v in contribution[i].values()])\n                \n\n        \n        return (np.mean(predictions, axis=0), np.mean(biases, axis=0),\n            total_contributions)\n    else:\n        for tree in model.estimators_:\n            pred, bias, contribution = _predict_tree(tree, X)\n\n            biases.append(bias)\n            contributions.append(contribution)\n            predictions.append(pred)\n        \n        \n        return (np.mean(predictions, axis=0), np.mean(biases, axis=0),\n            np.mean(contributions, axis=0))\n\n\ndef predict(model, X, joint_contribution=False):\n    \"\"\" Returns a triple (prediction, bias, feature_contributions), such\n    that prediction ≈ bias + feature_contributions.\n    Parameters\n    ----------\n    model : DecisionTreeRegressor, DecisionTreeClassifier,\n        ExtraTreeRegressor, ExtraTreeClassifier,\n        RandomForestRegressor, RandomForestClassifier,\n        ExtraTreesRegressor, ExtraTreesClassifier\n    Scikit-learn model on which the prediction should be decomposed.\n    X : array-like, shape = (n_samples, n_features)\n    Test samples.\n    \n    joint_contribution : boolean\n    Specifies if contributions are given individually from each feature,\n    or jointly over them\n    Returns\n    -------\n    decomposed prediction : triple of\n    * prediction, shape = (n_samples) for regression and (n_samples, n_classes)\n        for classification\n    * bias, shape = (n_samples) for regression and (n_samples, n_classes) for\n        classification\n    * contributions, If joint_contribution is False then returns and  array of \n        shape = (n_samples, n_features) for regression or\n        shape = (n_samples, n_features, n_classes) for classification, denoting\n        contribution from each feature.\n        If joint_contribution is True, then shape is array of size n_samples,\n        where each array element is a dict from a tuple of feature indices to\n        to a value denoting the contribution from that feature tuple.\n    \"\"\"\n    # Only single out response variable supported,\n    if model.n_outputs_ > 1:\n        raise ValueError(\"Multilabel classification trees not supported\")\n\n    if (isinstance(model, DecisionTreeClassifier) or\n        isinstance(model, DecisionTreeRegressor)):\n        return _predict_tree(model, X, joint_contribution=joint_contribution)\n    elif (isinstance(model, ForestClassifier) or\n          isinstance(model, ForestRegressor)):\n        return _predict_forest(model, X, joint_contribution=joint_contribution)\n    else:\n        raise ValueError(\"Wrong model type. Base learner needs to be a \"\n                         \"DecisionTreeClassifier or DecisionTreeRegressor.\")","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"f3d0db3bf491354efe4b96586e661450b2925f87","_cell_guid":"138f1045-6e53-4c86-bd11-ae64440a2a8c"},"source":"## **Below i'm using the `predict` function from the tree interpreter package.**\nhttps://github.com/andosa/treeinterpreter\nThis great package decomposes the tree into a number of components","cell_type":"markdown"},{"metadata":{"_uuid":"859f2fc1c5524ce4b1e026cf1debf5b72631163f","collapsed":true,"_cell_guid":"cab883ad-4e23-48c8-ab9c-38a99c5e3818"},"outputs":[],"source":"prediction, bias, contributions = predict(single_tree, X_matrix)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"f856649c0d970d031df707d806f82644895e8237","_cell_guid":"fdb58424-fb8d-4702-bd69-09a687dced98"},"source":"### ** We now have a table of contributions. Each row is a sample, and every column is a field and the $ contribution to the predicted sale price**","cell_type":"markdown"},{"metadata":{"_uuid":"97e8a1e9b4cb48439a7c691441f585d574064276","_cell_guid":"32497ec6-8312-4a75-be13-39ad4992f148"},"outputs":[],"source":"contribution_matrix =  pd.DataFrame(contributions, columns=X_matrix.columns)\ncontribution_matrix['bias'] = bias\n\ntmp_sums = contribution_matrix.sum()\nnonzero_cols = tmp_sums[tmp_sums!=0].index\n\ncontribution_matrix[nonzero_cols].head(5)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"97d85a5f299903bb8545f8d58d46f04cc2e55a68","_cell_guid":"d9a51052-f96f-416c-bd48-f3f480ae73c0"},"source":"## **Let's Check to make sure the addition of the components equal the prediction below**","cell_type":"markdown"},{"metadata":{"_uuid":"1cf818b7498847d43ef5d66bb7e0a4e4de6a1edc","_cell_guid":"c494138c-bab5-43a0-b983-55ac1be1b109"},"outputs":[],"source":"contribution_matrix[nonzero_cols].head(4).sum(axis=1)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"020483cf3685e1d6a5ecc688927403efa4ae8089","_cell_guid":"8a17dcfc-5f7b-40f6-b83f-98ee4c2fd80c"},"outputs":[],"source":"prediction[[0,1,2,3,4]]","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"56b60a1d37a8ed27b68d76a9683016d6ceb85927","collapsed":true,"_cell_guid":"0f49ce57-bab9-4dbc-86e7-d3bec9a43e79"},"outputs":[],"source":"","execution_count":null,"cell_type":"code"}],"nbformat":4,"nbformat_minor":1}