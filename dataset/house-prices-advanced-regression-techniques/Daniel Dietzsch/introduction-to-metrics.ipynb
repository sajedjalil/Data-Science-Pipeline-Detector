{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"text-align:center\">Introduction to Metrics</h1>"},{"metadata":{},"cell_type":"markdown","source":"<div style=\"text-align:center;\"><img src=\"https://images.unsplash.com/photo-1501516069922-a9982bd6f3bd?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1650&q=80\" /></div>"},{"metadata":{},"cell_type":"markdown","source":"### About the Notebook:\n\n**In this notebook, I want to address different evaluation metrics.**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Data Processing\nimport numpy as np \nimport pandas as pd \n\n\n# Data Visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing the data\n\nI would like to skip right to the point where we have prepared data without desciption.\nI am using the same approach as in this [Kernal](https://www.kaggle.com/dietzschdaniel/my-simplistic-titanic-approach-0-79665) for Titanic."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Loading the data\ndf_train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\n\n# Handling NaN values\ndf_train['Age'] = df_train['Age'].fillna(df_train['Age'].mean())\ndf_test['Age'] = df_test['Age'].fillna(df_test['Age'].mean())\n\ndf_train['Cabin'] = df_train['Cabin'].fillna(\"Missing\")\ndf_test['Cabin'] = df_test['Cabin'].fillna(\"Missing\")\n\ndf_train = df_train.dropna()\n\ndf_test['Fare'] = df_test['Fare'].fillna(df_test['Fare'].mean())\n\n# Cleaning the data\ndf_train = df_train.drop(columns=['Name'], axis=1)\ndf_test = df_test.drop(columns=['Name'], axis=1)\n\nsex_mapping = {\n    'male': 0,\n    'female': 1\n}\ndf_train.loc[:, \"Sex\"] = df_train['Sex'].map(sex_mapping)\ndf_test.loc[:, \"Sex\"] = df_test['Sex'].map(sex_mapping)\n\ndf_train = df_train.drop(columns=['Ticket'], axis=1)\ndf_test = df_test.drop(columns=['Ticket'], axis=1)\n\ndf_train = df_train.drop(columns=['Cabin'], axis=1)\ndf_test = df_test.drop(columns=['Cabin'], axis=1)\n\ndf_train = pd.get_dummies(df_train, prefix_sep=\"__\",\n                              columns=['Embarked'])\ndf_test = pd.get_dummies(df_test, prefix_sep=\"__\",\n                              columns=['Embarked'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our data now looks as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Metrics"},{"metadata":{},"cell_type":"markdown","source":"### Preparing the data for Modeling (Classification)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Everything except the target variable\nX = df_train.drop(\"Survived\", axis=1)\n\n# Target variable\ny = df_train['Survived'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random seed for reproducibility\nnp.random.seed(42)\n\n# Splitting the data into train & test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For our model, I will be using `RandomForestClassifier()`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting up RandomForestClassifier()\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\n\n# Predicting values\ny_pred = rfc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification Metrics"},{"metadata":{},"cell_type":"markdown","source":"For classification problems, the most common metrics are:\n* Accuracy\n* Precision\n* Recall\n* F1 score\n* Area under ROC curve (AUC)"},{"metadata":{},"cell_type":"markdown","source":"At this point, I would like to introduce some vocabulary first.\n* True Positive (tp), is a result that indicates a given condition exists when it does.\n* False Positive (fp), is a result that indicates a given condition exists when it does not.\n* True Negative (tn), is a test result which rightfully indicates that a condition does not hold.\n* False Negative (fn), is a test result which wrongly indicates that a condition does not hold."},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix"},{"metadata":{},"cell_type":"markdown","source":"These values ar usually presented in a **confusion matrix**.\n\n<div style=\"text-align:center;\"><img src=\"https://miro.medium.com/max/356/1*Z54JgbS4DUwWSknhDCvNTQ.png\" /></div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n\n# Computing the confusion_matrix\nconfusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing plot_confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\n# Plot plot_confusion_matrix\nplot_confusion_matrix(rfc, X_test, y_test);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case we have 50 True Positives, 90 True Negatives, 19 False Positives and 19 False Negatives."},{"metadata":{},"cell_type":"markdown","source":"## Accuracy\nAccuracy is the fraction of predictions our model got right.\nIt is defined by the number of correct predictions divided by the number of total predictions.\n\n<div style=\"text-align:center;\"><img src=\"https://miro.medium.com/max/373/1*yRa2inzTnyASJOre93ep3g.gif\" /></div>\n\nAccuracy can give us a *good first impression* of our model's performance. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing accuracy_score\nfrom sklearn.metrics import accuracy_score\n\n# Computing the accuracy_score\naccuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case our model predicted about 78% of the samples correctly. However, the accuracy does not always show, how good our model is.  \n\n**For example:**  \nIf we have 100 Samples with 90 people that did survive and 10 people that did not survive and our model would always predict that a passenger survived, it would have an accuracy of 0.9. However, such a model would not provide much value. Therefore, we should also look at other metrics."},{"metadata":{},"cell_type":"markdown","source":"## Precision\nFrom [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score):\n> The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing precision_score\nfrom sklearn.metrics import precision_score\n\n# Computing the precision_score\nprecision_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recall\nFrom [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score):\n> The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing recall_score\nfrom sklearn.metrics import recall_score\n\n# Computing the recall_score\nrecall_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## F1 score\nFrom [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score):\n> The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. \n\nThe  F1 score is defined as follows:\n<div style=\"text-align:center;\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/9c94f59b68f5ae0dc92185906c7ec4214fd04e1e\" /></div>\n\nIt should mainly be used, if you seek a balance between precision and recall."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing f1_score\nfrom sklearn.metrics import f1_score\n\n# Computing the f1_score\nf1_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Area under ROC curve (AUC)\nFrom [Wikipedia](https://en.wikipedia.org/wiki/Receiver_operating_characteristic):\n> A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. \n\n> The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n\nThe True Positive Rate (TPR) is defined as: `TPR = TP / (TP + FN)`\nThe False Positive Rate (FPR) is defined as `FPR = FP / (TN + FP)`\n\nIf we calculate the area under this curve, we get Area under ROC Curve or simply ROC."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing roc_curve\nfrom sklearn.metrics import roc_curve\n\n# Plot ROC curve\nfpr, tpr, _ = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\n\n# Importing roc_auc_score\nfrom sklearn.metrics import roc_auc_score\n\n# Computing the roc_auc_score\nroc_auc_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Regression Metrics"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n\ntrain = train.fillna(train.median())\n\ntrain['Alley'] = train['Alley'].fillna('None')\ntrain = train.drop(['Utilities'], axis=1)\ntrain['MasVnrType'] = train['MasVnrType'].fillna('Missing')\ntrain['BsmtQual'] = train['BsmtQual'].fillna('None')\ntrain['BsmtCond'] = train['BsmtCond'].fillna('None')\ntrain['BsmtExposure'] = train['BsmtExposure'].fillna('None')\ntrain['BsmtFinType1'] = train['BsmtFinType1'].fillna('None')\ntrain['BsmtFinType2'] = train['BsmtFinType2'].fillna('None')\ntrain['Electrical'] = train['Electrical'].fillna('None')\ntrain['FireplaceQu'] = train['FireplaceQu'].fillna('None')\ntrain['GarageType'] = train['GarageType'].fillna('None')\ntrain['GarageFinish'] = train['GarageFinish'].fillna('None')\ntrain['GarageQual'] = train['GarageQual'].fillna('None')\ntrain['GarageCond'] = train['GarageCond'].fillna('None')\ntrain = train.drop(['PoolQC'], axis=1)\ntrain['Fence'] = train['Fence'].fillna('None')\ntrain['MiscFeature'] = train['MiscFeature'].fillna('None')\ntrain['SaleType'] = train['SaleType'].fillna('None')\n\nfrom sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(train[c].values)) \n    train[c] = lbl.transform(list(train[c].values))\n    \ntrain = pd.get_dummies(train)\n\n\n\n# Everything except the target variable\nX = train.drop(\"SalePrice\", axis=1)\n\n# Target variable\ny = train['SalePrice'].values\n\n\n# Random seed for reproducibility\nnp.random.seed(42)\n\n# Splitting the data into train & test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n\n\n# Setting up GradientBoostingRegressor()\nfrom sklearn.ensemble import GradientBoostingRegressor\nmodel = GradientBoostingRegressor()\nmodel.fit(X_train, y_train)\n\n# Predicting values\ny_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For this part we use the date from the [house price prediction competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques).\nMy Approach is explained in this [Kernal](https://www.kaggle.com/dietzschdaniel/my-house-price-prediction-approach)."},{"metadata":{},"cell_type":"markdown","source":"For regression problems, the most common metrics are:\n* Mean absolute error (MAE)\n* Mean squared error (MSE)\n* Root mean squared error (RMSE)\n* Root mean squared logarithmic error (RMSLE)\n* Mean percentage error (MPE)\n* Mean absolute percentage error (MAPE)\n* R^2"},{"metadata":{},"cell_type":"markdown","source":"## Mean absolute error (MAE)\nFrom [Wikipedia](https://en.wikipedia.org/wiki/Mean_absolute_error):\n> In statistics, mean absolute error (MAE) is a measure of errors between paired observations expressing the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement.\n\n<div style=\"text-align:center;\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/3ef87b78a9af65e308cf4aa9acf6f203efbdeded\" /></div>\n\n> yi is the prediction and xi is the true value.\n\nMAE should be used, when you do not want to penalize large errors to much. This [Article](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d) describes this in detail."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing mean_absolute_error\nfrom sklearn.metrics import mean_absolute_error\n\n# Computing the mean_absolute_error\nmean_absolute_error(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mean squared error (MSE) \nFrom [Wikipedia](https://en.wikipedia.org/wiki/Mean_squared_error):\n> In statistics, the mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errorsâ€”that is, the average squared difference between the estimated values and the actual value."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing mean_squared_error\nfrom sklearn.metrics import mean_squared_error\n\n# Computing the mean_squared_error\nmean_squared_error(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Root mean squared error (RMSE)\nFrom [Wikipedia](https://en.wikipedia.org/wiki/Root-mean-square_deviation):\n> The root-mean-square deviation (RMSD) or root-mean-square error (RMSE) is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed. The RMSD represents the square root of the second sample moment of the differences between predicted values and observed values or the quadratic mean of these differences."},{"metadata":{},"cell_type":"markdown","source":"The Root mean squared error (RMSE) is the root of the square of the sum of the predicted values (yi) minus the true values (yhead i) divided by the number of predicted values (n).\n\n<img src=\"https://miro.medium.com/max/327/1*9hQVcasuwx5ddq_s3MFCyw.gif\" />\n\nRoot Mean Squared Error should be used, when you want to penalize large errors. This [article](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d) gives a good overview."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing mean_squared_error and numpy\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\n# Computing the root of mean_squared_error\nmean_squared_error(y_test, y_pred)\nnp.sqrt(mean_squared_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Root mean squared logarithmic error (RMSLE)\nFrom [this Kaggle Discussion](https://www.kaggle.com/c/ashrae-energy-prediction/discussion/113064):\n> In case of RMSLE, you take the log of the predictions and actual values. So basically, what changes is the variance that you are measuring. I believe RMSLE is usually used when you don't want to penalize huge differences in the predicted and the actual values when both predicted and true values are huge numbers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing mean_squared_log_error and numpy\nfrom sklearn.metrics import mean_squared_log_error\nimport numpy as np\n\n# Computing the root of mean_squared_error\nnp.sqrt(mean_squared_log_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## R^2\nR^2 or R-squared is a popular metric for regression problems. It specifies how close the data is to the fitted regression line.\nFrom [Wikipedia](https://en.wikipedia.org/wiki/Coefficient_of_determination):\n> In statistics, the coefficient of determination, denoted R2 or r2 and pronounced \"R squared\", is the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n\n<div style=\"text-align:center;\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/R2values.svg/330px-R2values.svg.png\" /></div>\n\n\n\nThe definition of the scores as explained by the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html):\n> Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing r2_score\nfrom sklearn.metrics import r2_score\n\n# Computing the root of r2_score\nr2_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Work in Progress. To be continued.."},{"metadata":{},"cell_type":"markdown","source":"**If you liked this notebook or found it helpful in any way, feel free to leave an upvote - That will keep me motivated :)**\n\n**If you have any suggestions for improvement, leave a comment :)**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}