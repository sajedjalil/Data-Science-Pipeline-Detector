{"cells":[{"metadata":{},"cell_type":"markdown","source":"Getting started with Kaggle can be an intimidating prospect!\n\nSo I wrote this kernel to help you get started quickly. You can fork it, plug in a dataset you wanna youâ€™ve been itching to explore and be doing analysis in 5 mins.\n\n## If you like this kernel, please give it an upvote. Thank you! :)\n\nHappy Kagglin!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\nfigure(num=None, figsize=(20, 10), dpi=80, facecolor='w', edgecolor='k')\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom datetime import datetime\nimport json\nimport os\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import Imputer\nfrom scipy.stats import skew \nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n# models\nfrom xgboost import XGBRegressor\nimport warnings\n# Ignore useless warnings\nwarnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n\n# Avoid runtime error messages\npd.set_option('display.float_format', lambda x:'%f'%x)\n\n# make notebook's output stable across runs\nnp.random.seed(42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read CSVs\nfetch_from = '../input/train.csv'\ntrain = pd.read_csv(fetch_from)\n\nfetch_from = '../input/test.csv'\ntest = pd.read_csv(fetch_from)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Overview of training and test datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many datapoints in the training set?\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many datapoints in the test set?\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look at sample datapoints in the training set\ntrain.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look at sample datapoints in the test set\ntest.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What's missing?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many missing values does the dataset have?\ntrain.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Which columns have the most missing values?\ndef missing_data(df):\n    total = df.isnull().sum()\n    percent = (df.isnull().sum()/train.isnull().count()*100)\n    missing_values = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in df.columns:\n        dtype = str(df[col].dtype)\n        types.append(dtype)\n    missing_values['Types'] = types\n    missing_values.sort_values('Total',ascending=False,inplace=True)\n    return(np.transpose(missing_values))\nmissing_data(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's plot these missing values(%) vs column_names\nmissing_values_count = (train.isnull().sum()/train.isnull().count()*100).sort_values(ascending=False)\nplt.figure(figsize=(15,10))\nbase_color = sns.color_palette()[0]\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)\nsns.barplot(missing_values_count[:10].index.values, missing_values_count[:10], color = base_color)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Time to deep dive into the data!"},{"metadata":{},"cell_type":"markdown","source":"## Numerical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training and test sets seem to have consistently distributed features. But just to make sure this is the case, let's plot features from both the training and test sets!"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.hist(bins=50, figsize=(20,15))\nplt.tight_layout(pad=0.4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.hist(bins=50, figsize=(20,15))\nplt.tight_layout(pad=0.4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distributions of features across the training and test data do indeed seem consistent. This is excellent!"},{"metadata":{},"cell_type":"markdown","source":"## Categorical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe(include='O')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe(include='O')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we mvoe forward, let's make a copy of the training set!"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_eda = train.copy()\nlabel_col = 'SalePrice'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Closer look at a few of these variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_name = 'HouseStyle'\nfreq_table = pd.crosstab(index=train_eda[col_name],  # Make a crosstab\n                              columns=\"count\")      # Name the count column\nfreq_table_per = pd.crosstab(index=train_eda[col_name],  # Make a crosstab\n                              columns=\"percentage\", normalize=True)\nfreq_table['percentage'] = freq_table_per['percentage']\nfreq_table.sort_values(by='count', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, we need to create a function to decide how many blocks to allocate to each category\ndef percentage_blocks(df, var):\n    \"\"\"\n    Take as input a dataframe and variable, and return a Pandas series with\n    approximate percentage values for filling out a waffle plot.\n    \"\"\"\n    # compute base quotas\n    percentages = 100 * df[var].value_counts() / df.shape[0]\n    counts = np.floor(percentages).astype(int) # integer part = minimum quota\n    decimal = (percentages - counts).sort_values(ascending = False)\n    # add in additional counts to reach 100\n    rem = 100 - counts.sum()\n    for cat in decimal.index[:rem]:\n        counts[cat] += 1\n    return counts\n\n# Second, plot those counts as boxes in the waffle plot form\nwaffle_counts = percentage_blocks(train_eda, col_name)\nprev_count = 0\n# for each category,\nfor cat in range(waffle_counts.shape[0]):\n    # get the block indices\n    blocks = np.arange(prev_count, prev_count + waffle_counts[cat])\n    # and put a block at each index's location\n    x = blocks % 10 # use mod operation to get ones digit\n    y = blocks // 10 # use floor division to get tens digit\n    plt.bar(x = x, height = 0.9, width = 0.9, bottom = y)\n    prev_count += waffle_counts[cat]\n\n# Third, we need to do involve aesthetic cleaning to polish it up for interpretability. We can take away the plot border and ticks, since they're arbitrary, but we should change the limits so that the boxes are square. We should also add a legend so that the mapping from colors to category levels is clear.\n# aesthetic wrangling\nplt.legend(waffle_counts.index, bbox_to_anchor = (1, 0.5), loc = 6)\nplt.axis('off')\nplt.axis('square')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Area depicts the distribution of points. > width = > the number of points\nbase_color = sns.color_palette()[0]\nplt.figure(figsize=(20,15))\nplt.xticks(rotation=45)\nsns.boxplot(data = train_eda, x = 'HouseStyle', y = 'SalePrice', color = base_color);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_name = 'HouseStyle'\nfreq_table = pd.crosstab(index=train_eda[col_name],  # Make a crosstab\n                              columns=\"count\")      # Name the count column\nfreq_table_per = pd.crosstab(index=train_eda[col_name],  # Make a crosstab\n                              columns=\"percentage\", normalize=True)\nfreq_table['percentage'] = freq_table_per['percentage']\nfreq_table.sort_values(by='count', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Relative frequency variation - Plotting absolute counts on axis and porportions on the bars\n# Barchart sorted by frequency\nbase_color = sns.color_palette()[0]\ncat_order = train_eda[col_name].value_counts().index\nplt.figure(figsize=(15,10))\nplt.xticks(rotation = 90)\nsns.countplot(data = train_eda, x = col_name, order = cat_order, color = base_color);\n\n# add annotations\nn_points = train_eda.shape[0]\ncat_counts = train_eda[col_name].value_counts()\nlocs, labels = plt.xticks() # get the current tick locations and labels\n\n# loop through each pair of locations and labels\nfor loc, label in zip(locs, labels):\n\n    # get the text property for the label to get the correct count\n    count = cat_counts[label.get_text()]\n    pct_string = '{:0.1f}%'.format(100*count/n_points)\n\n    # print the annotation just below the top of the bar\n    plt.text(loc, count+4, pct_string, ha = 'center', color = 'black')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Area depicts the distribution of points. > width = > the number of points\nbase_color = sns.color_palette()[0]\nplt.figure(figsize=(20,15))\nplt.xticks(rotation=45)\nsns.boxplot(data = train_eda, x = 'Neighborhood', y = 'SalePrice', color = base_color);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_name = 'GrLivArea'\nhist_kws={\"alpha\": 0.3}\nplt.figure(figsize=(15,10))\n# Trim long-tail/other values\n# plt.xlim(0, 1200)\nsns.distplot(train_eda[col_name], hist_kws=hist_kws);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_name = 'OverallQual'\nhist_kws={\"alpha\": 0.3}\nplt.figure(figsize=(15,10))\n# Trim long-tail/other values\n# plt.xlim(0, 1200)\nsns.distplot(train_eda[col_name], hist_kws=hist_kws);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_name = 'OverallQual'\nbase_color = sns.color_palette()[0]\nplt.figure(figsize=(20,15))\nplt.xticks(rotation=45)\nsns.boxplot(data = train_eda, x = col_name, y = 'SalePrice', color = base_color)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alright, we have some understanding of the features in our training set! Let's see how useful they might be in predicting our SalePrice!"},{"metadata":{},"cell_type":"markdown","source":"# What're the most correlated features?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Which features are the most correlated to our target variable, SalePrice?\ncorr_matrix = train_eda.corr()\nplt.subplots(figsize=(15,10))\nsns.heatmap(corr_matrix, vmax=1.0, square=True, cmap=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the top 10 most correlated features\ncorr_matrix = train_eda.corr()\ncorr_matrix[label_col].sort_values(ascending=False)[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note: .corr() only captures linear relationships, so it's not the most reliable way to detect correlations. So let's plot a few of these features on a scatterplot matrix!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\n\nattributes = [label_col, \"OverallQual\", \"LotArea\", \"BedroomAbvGr\", \"GrLivArea\"]\nscatter_matrix(train_eda[attributes], figsize=(15, 15));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How good a predictor is GarageArea for SalePrice?"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_name = 'GarageArea'\ntrain_eda.plot(kind=\"scatter\", x=label_col, y=col_name, alpha=0.2, figsize=(15,10))\n# changing axis labels to only show part of the graph\nplt.axis([0, 400000, 0, 1200])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's add some more context to our GarageArea vs SalePrice plot! Let's color it by the year sold (YrSold) and change the size accoding to the number of bedrooms above ground (BedroomAbvGr)."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_eda.plot(kind=\"scatter\", x=label_col, y=\"GarageArea\", alpha=0.4,\n             s=train_eda[\"BedroomAbvGr\"], label=\"BedroomAbvGr\", figsize=(20,15),\n             c=\"YrSold\", cmap=plt.get_cmap(\"jet\"), colorbar=True,)\nplt.axis([0, 400000, 0, 1200])\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering and Prep"},{"metadata":{},"cell_type":"markdown","source":"Before we go forward, let's make a copy of the training set!"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_fe = train.copy()\ntest_fe = test.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_fe = train_fe[train_fe.GrLivArea < 4300]\ntrain_fe.reset_index(drop=True, inplace=True)\n\ntrain_fe[\"SalePrice\"] = np.log1p(train_fe[\"SalePrice\"])\ny = train_fe.SalePrice.reset_index(drop=True)\ntrain_features = train_fe.drop(['SalePrice'], axis=1)\ntest_features = test.copy()\n\nfeatures = pd.concat([train_features, test_features]).reset_index(drop=True)\nprint(features.shape)\n\nobjects = []\nfor i in features.columns:\n    if features[i].dtype == object:\n        objects.append(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fill missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"features['Functional'] = features['Functional'].fillna('Typ')\nfeatures['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\nfeatures['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")\nfeatures['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\nfeatures['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\nfeatures['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\nfeatures[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    features[col] = features[col].fillna(0)\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    features[col] = features[col].fillna('None')\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    features[col] = features[col].fillna('None')\nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\nfeatures['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\nfeatures.update(features[objects].fillna('None'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reconcile feature types"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert categorical variables stored as numbers to strings\nfeatures['MSSubClass'] = features['MSSubClass'].apply(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Other Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove Id\ntrain_ID = train_fe['Id']\ntest_ID = test_fe['Id']\n\ntrain_fe.drop(['Id'], axis=1, inplace=True)\ntest_fe.drop(['Id'], axis=1, inplace=True)\n\n# Create new features\nfeatures = features.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\n\nfeatures['YrBltAndRemod']=features['YearBuilt']+features['YearRemodAdd']\nfeatures['TotalSF']=features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n\nfeatures['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                                 features['1stFlrSF'] + features['2ndFlrSF'])\n\nfeatures['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n\nfeatures['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] + features['ScreenPorch'] +\n                              features['WoodDeckSF'])\n\nfeatures['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics.append(i)\nfeatures.update(features[numerics].fillna(0))\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics2 = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics2.append(i)\n\n# Fix skewed features\nskew_features = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))\nfinal_features = pd.get_dummies(features).reset_index(drop=True)\n\nX = final_features.iloc[:len(y), :]\nX_sub = final_features.iloc[len(X):, :]\n\noutliers = [30, 88, 462, 631, 1322]\nX = X.drop(X.index[outliers])\ny = y.drop(y.index[outliers])\n\noverfit = []\nfor i in X.columns:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros / len(X) * 100 > 99.94:\n        overfit.append(i)\n\noverfit = list(overfit)\noverfit.append('MSZoning_C (all)')\n\nX = X.drop(overfit, axis=1).copy()\nX_sub = X_sub.drop(overfit, axis=1).copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_train = train.copy()\nfeatures_train.dropna(axis=0, subset=['SalePrice'], inplace=True)\nlabel = features_train.SalePrice\nfeatures_train = features_train.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\ntrain_X, test_X, train_y, test_y = train_test_split(features_train.as_matrix(), label.as_matrix(), test_size=0.25)\n\nmy_imputer = Imputer()\ntrain_X = my_imputer.fit_transform(train_X)\ntest_X = my_imputer.transform(test_X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train a basic XGBoost to serve as a benchmark"},{"metadata":{},"cell_type":"markdown","source":"Defining a basic XGBoost model."},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006, random_state=42);\n\nxgb.fit(train_X, train_y)\npredictions = xgb.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining our scoring metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)\n\ndef mae(y, y_pred):\n    return mean_absolute_error(predictions, test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('MAE score on train data:')\nprint(mae(predictions, test_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Next steps:\n- Try a bunch of other models, using GridSearch and CrossValidation\n- A combo of XGRegressor, CatBoostRegressor, RandomForestRegressor, SVR and RidgeRegression worked best for me\n\n## If you like this kernel, please give it an upvote. Thank you! :)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}