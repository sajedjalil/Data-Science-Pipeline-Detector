{"cells":[{"metadata":{},"cell_type":"markdown","source":"# House Price Prediction - Advanced Regression"},{"metadata":{},"cell_type":"markdown","source":"### Import all the required libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pa\nimport seaborn as sn\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport matplotlib.patches as matp\n%matplotlib inline\nimport statsmodels.api as sm\nimport random\n\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,RandomizedSearchCV,learning_curve\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder,RobustScaler,Imputer\n\nfrom sklearn.linear_model import LinearRegression,ElasticNet,Lasso,Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor,AdaBoostRegressor,BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\n\nfrom sklearn import metrics\nfrom scipy import stats\nfrom scipy.special import boxcox1p\ncolor = sn.color_palette()\nsn.set_style('darkgrid')\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsample_submission = pa.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\ntest = pa.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\ntrain = pa.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\n\nprint(\"Shape of house data {}\".format(train.shape))\nprint(\"Length of house data {}\\n\".format(train.shape[0]))\n\ntest_id = test.Id\n\nprint(\"Shape of train data {}\".format(train.shape))\nprint(\"Length of house data {}\\n\".format(train.shape[0]))\n\nprint(\"Shape of test data {}\".format(test.shape))\nprint(\"Length of test data {}\\n\".format(test.shape[0]))\n\n# Here we can see that our train dataset contains 81 features and length is 1000 \n\n#Y_actual_test = pa.read_csv('Y_test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Visualizing features and first 5 data"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = list(train.columns)\nprint(features)\nprint(\"\\nTotal Number Of Features {}\".format(len(features)))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Oulier Analysis"},{"metadata":{},"cell_type":"markdown","source":"From this visualization we can see that on right bottom there is a oulier because the price of the house is nearly 150000 but\nGrLivArea is nearly 5000 which is impossible. So, we will drop the oulier in the next step.We did't remove other outliers for making this model a roboust model if our test data contains outliers removeing outliers from our train and then training the model will not teach the model how to handle ouliers so from this we concluded that the GrLivArea is an important feature and removing 1  outliers which is truely a outlier is completely fine."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (axis1,axis2) = plt.subplots(1,2,figsize=(16,5))\n\nsn.scatterplot(x='GrLivArea',y='SalePrice',data=train,ax=axis1).set_title(\"With Outliers \")\n\n\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<250000)].index)\nsn.scatterplot(x='GrLivArea',y='SalePrice',data=train,ax=axis2).set_title(\"Without Outliers\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"##### Visualizing OverallCond,OverQual,YearBuilt"},{"metadata":{},"cell_type":"markdown","source":"In this step we have analysed overall quality,overall condition & YearBuilt with Sale Price from here we can see that overallquality has a linear relation with saleprice because as the quality of house increses then the sale price also increses\nagain the feature overallcondition has a linear relationship with saleprice but if the overallcondition is 5 then there is too much increse in the price which is quite fishy as it is not possible in real life. After analysing YearBulit we can say that there is a positive increasing trend(Modern house cost much more than older house as simple as that) and the data is also cyclical."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (axis1,axis2,axis3,axis4) = plt.subplots(1,4,figsize=(20,4))\n\nsn.lineplot(x='OverallQual',y='SalePrice',data=train,ax=axis1)\nsn.barplot(x='OverallQual',y='SalePrice',data=train,ax=axis2)\nsn.lineplot(x='OverallCond',y='SalePrice',data=train,ax=axis3)\nsn.barplot(x='OverallCond',y='SalePrice',data=train,ax=axis4)\n\nplt.figure(figsize=(20,6))\nsn.lineplot(x='YearBuilt',y='SalePrice',data=train)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The price of house is increasing as the area of the totalbasement increases ( LinearRelationship) same in case of 1stFloorArea and 2ndFloorArea"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,6))\nsn.lineplot(x='TotalBsmtSF',y='SalePrice',data=train)\n\nplt.show()\nplt.figure(figsize=(20,6))\n\nsn.scatterplot(x='TotalBsmtSF',y='SalePrice',data=train)\nfig, (axis1,axis2) = plt.subplots(1,2,figsize=(20,8))\nsn.scatterplot(x='1stFlrSF',y='SalePrice',data=train,ax=axis1)\nsn.scatterplot(x='2ndFlrSF',y='SalePrice',data=train,ax=axis2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After visualizing TotalBsmtSF,1stFlrSF & 2ndFlrSF we can conclude that totalbsmtSF has a linear relationship and 1stFlRSF and 2ndFlrSF also has a linear relationship but 1stFLR area seems more co-related with SalePrice than 2ndFlrSF "},{"metadata":{},"cell_type":"markdown","source":"###### Visualizing FullBath,HalfBath"},{"metadata":{},"cell_type":"markdown","source":"If the house having 0 or 1 FullBath then the price is almost same but if the house having 2 or 3 FullBath then there is a significant increase in price. Again the houses which have 1 halfbath cost more than the houses with 0 or 2 halfbath."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (axis1,axis2) = plt.subplots(1,2,figsize=(16,6))\nsn.boxplot(x='FullBath',y='SalePrice',data=train,ax=axis1)\n#sn.barplot(x='HalfBath',y='SalePrice',data=train,ax=axis2)\nsn.boxplot(x='HalfBath',y='SalePrice',data=train,ax=axis2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Visualizing KitchenAbvGr, BedroomAbvGr, TotRmsAbvGrd"},{"metadata":{},"cell_type":"markdown","source":"KitchenAdvGr is quite interesting, It can be seen that if a house has more kitchen than rooms then tyhe price is decresing significantly and has a strong negative relationship if the number of kitchen increases then saleprice drops significantly .We can also see that if the totalrooms inreases then saleprice also increases which means it as linear relationship with saleprice and it is also a statistically significant feature "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(16,6))\nsn.lineplot(x='KitchenAbvGr',y='SalePrice',data=train,ax=axis1)\nsn.lineplot(x='BedroomAbvGr',y='SalePrice',data=train,ax=axis2)\nsn.lineplot(x='TotRmsAbvGrd',y='SalePrice',data=train,ax=axis3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Pair Plot on area features "},{"metadata":{},"cell_type":"markdown","source":"LotFrontage,GarageArea,OpenPorchSF are strongly co-rlated with saleprice \n(Note: The diagonal shows the distribution of the variable)"},{"metadata":{"trusted":true},"cell_type":"code","source":"area = ['LotFrontage','GarageArea','OpenPorchSF','EnclosedPorch','ScreenPorch','SalePrice']\nsn.pairplot(train[area],size=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Visualizing Neighborhood"},{"metadata":{},"cell_type":"markdown","source":"Here we noticed that price of houses with neighborhood ( NridHt,NoRidge,Veenker,StoneBr,Timber) is much more than others."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,6))\nsn.barplot(x='Neighborhood',y='SalePrice',data=train)\nplt.xticks(rotation=45)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Visualizing PoolQC,PavedRive,RoofStyle"},{"metadata":{},"cell_type":"markdown","source":"Cost of the houses with roofstyle shed and Hp cost much more than other types , PoolQuality - If the pool quality is Good and Excellent then price is also excellent it's a important feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(20,5))\n\nsn.barplot(x='PoolQC',y='SalePrice',data=train,ax=axis1)\nsn.barplot(x='PavedDrive',y='SalePrice',data=train,ax=axis2)\nsn.barplot(x='RoofStyle',y='SalePrice',data=train,ax=axis3)\n\nplt.figure(figsize=(20,6))\n## Here we will create a waffle chart for roofstyle to check the proportion of each roof style \n\nroof_dataframe = pa.DataFrame(train['RoofStyle'].value_counts())\nroof_dataframe.rename(columns={'RoofStyle':'Total'},inplace=True)\ntotal_values = sum(roof_dataframe['Total'])\n\ncategory_proportions = [(float(value) / total_values) for value in roof_dataframe['Total']]\n\nwidth = 90\nheight = 20\ntotal_number_tiles = width * height\n\n# compute the number of tiles for each catagory\ntiles_per_category = [round(proportion * total_number_tiles) for proportion in category_proportions]\n\n# initialize the waffle chart as an empty matrix\nwaffle_chart = np.zeros((height, width))\n\n# define indices to loop through waffle chart\ncategory_index = 0\ntile_index = 0\n\n# populate the waffle chart\nfor col in range(width):\n    for row in range(height):\n        tile_index += 1\n\n        # if the number of tiles populated for the current category is equal to its corresponding allocated tiles...\n        if tile_index > sum(tiles_per_category[0:category_index]):\n            \n            category_index += 1       \n            \n        # set the class value to an integer, which increases with class\n        waffle_chart[row, col] = category_index\n        \nprint ('Waffle chart populated!')\n\nplt.figure(figsize=(20,6))\n\ncolormap = plt.cm.coolwarm\nplt.matshow(waffle_chart,cmap=colormap)\nplt.colorbar()\n\nax = plt.gca()\n\nax.set_xticks(np.arange(-.5,(width),1),minor=True)\nax.set_yticks(np.arange(-.5,(height),1),minor=True)\n\nax.grid(which='minor',color='w',linestyle='-',linewidth=2)\nplt.xticks([])\n\nplt.yticks([])\n\nvalues_cumsum = np.cumsum(roof_dataframe['Total'])\ntotal_values = values_cumsum[len(values_cumsum) - 1]\n\n# create legend\nlegend_handles = []\nfor i, category in enumerate(roof_dataframe.index.values):\n    label_str = category + ' (' + str(roof_dataframe['Total'][i]) + ')'\n    color_val = colormap(float(values_cumsum[i])/total_values)\n    legend_handles.append(matp.Patch(color=color_val, label=label_str))\n\n# add legend to chart\nplt.legend(handles=legend_handles,\n           loc='lower center', \n           ncol=len(roof_dataframe.index.values),\n           bbox_to_anchor=(0., -0.2, 0.95, .1)\n          )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that roofstyle Gable is mostly present in most of the house and we can also see that the house with shed roofstyle has a very high price"},{"metadata":{},"cell_type":"markdown","source":"#### SalePrice (Dependent Variable)"},{"metadata":{},"cell_type":"markdown","source":"Here we can see that our SalePrice is not normalized and left skewed so we need to transform it."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (axis1) = plt.subplots(1,1,figsize=(16,5))\n\n\nplt.figure(figsize=(16,5))\nsn.distplot(train['SalePrice'],color='k',label='Skewness : %.2f'%train['SalePrice'].skew(),ax=axis1).set_title(\"Density Plot of SalePrice\")\nplt.ylabel(\"Frequency\")\n#plt.legend(loc='best')\n\nstats.probplot(train.SalePrice,plot=plt)\nplt.title(\"Probability plot of SalePrice\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we can see that it need to be normalize \n### 3 types of normal check whch perfomr better means which better normalize the value\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(16,5))\nsn.distplot(train['SalePrice'],fit=stats.johnsonsu,ax=axis1,label='Skewness :%.2f'%train['SalePrice'].skew()).set_title(\"JhonsonSU Transformation\")\n\nsn.distplot(train['SalePrice'],fit=stats.lognorm,ax=axis2,label='Skewness :%.2f'%train['SalePrice'].skew()).set_title(\"Log Transformation\")\n\nsn.distplot(train['SalePrice'],fit=stats.norm,ax=axis3,label='Skewness :%.2f'%train['SalePrice'].skew()).set_title(\"Normal Transformation\")\n\nprint(\"From here we can see that log transformation performs much better than others so we will use Log Transformation to normalize\\n SalePrice\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3 types of normalalization checking - \nFrom here we can see that log transformation performs much better than others so we will use Log Transformation to normalize SalePrice "},{"metadata":{},"cell_type":"markdown","source":"##### Apply Log Transformation "},{"metadata":{"trusted":true},"cell_type":"code","source":"train['SalePrice'] = np.log1p(train['SalePrice'])\nplt.figure(figsize=(16,5))\nsn.distplot(train['SalePrice'],color='k',label='Skewness : %.2f'%train['SalePrice'].skew()).set_title(\"Density Plot of Sale Price(Normalized)\")\nplt.ylabel(\"Frequency\")\nplt.legend(loc='best')\nplt.show()\nplt.figure(figsize=(16,5))\n\nstats.probplot(train.SalePrice,plot=plt)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"house_data = pa.concat([train,test])\nhouse_data.drop('SalePrice',axis=1,inplace=True)\nhouse_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handling Missing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data = (house_data.isnull().sum() / len(house_data)) * 100\nmissing_data = missing_data.drop(missing_data[missing_data == 0].index).sort_values(ascending=False)\nmissing_data_ratio = pa.DataFrame({'Missing Ratio':missing_data})\n\nplt.figure(figsize=(20,10))\ng = sn.barplot(x=missing_data_ratio.index,y='Missing Ratio',data=missing_data_ratio).set_title(\"Misssing Data Ratio\")\nplt.xticks(rotation=45)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Co-relation plot "},{"metadata":{"trusted":true},"cell_type":"code","source":"corrmat = train.corr()\nplt.figure(figsize=(20,10))\nsn.heatmap(corrmat, vmax=0.9, square=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### From the corelation plot we noticed : \n\nGaragecars and GarageArea is highly corelated                                                                              \nTotalBsmtArea and 1stFlrArea is highly corelated                                                                          \nGrLivArea amd TotalRmsAbrGr is highly corelated      \nGarageYearBuilt and YearBulit is highly corelated"},{"metadata":{},"cell_type":"markdown","source":"#### Feature Engineering "},{"metadata":{"trusted":true},"cell_type":"code","source":"house_data[\"PoolQC\"] = house_data[\"PoolQC\"].fillna(\"None\")\nhouse_data[\"MiscFeature\"] = house_data[\"MiscFeature\"].fillna(\"None\")\nhouse_data[\"Alley\"] = house_data[\"Alley\"].fillna(\"None\")\nhouse_data[\"Fence\"] = house_data[\"Fence\"].fillna(\"None\")\nhouse_data[\"FireplaceQu\"] = house_data[\"FireplaceQu\"].fillna(\"None\")\n\nhouse_data['LotFrontage'] = house_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x:x.fillna(x.median()))\n\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    house_data[col] = house_data[col].fillna('None')\n    \nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    house_data[col] = house_data[col].fillna(0)\n\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', \n            'BsmtFullBath', 'BsmtHalfBath'):\n    house_data[col] = house_data[col].fillna(0)\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n            'BsmtFinType2'):\n    house_data[col] = house_data[col].fillna('None')\n    \nhouse_data[\"MasVnrType\"] = house_data[\"MasVnrType\"].fillna(\"None\")\nhouse_data[\"MasVnrArea\"] = house_data[\"MasVnrArea\"].fillna(0)\nhouse_data['MSZoning'] = house_data['MSZoning'].fillna(house_data['MSZoning'].mode()[0])\n\nhouse_data = house_data.drop('Utilities',axis=1)\n\nhouse_data.Functional.value_counts()\nhouse_data[\"Functional\"] = house_data[\"Functional\"].fillna(\"Typ\")\n\nhouse_data.Electrical.value_counts()\nhouse_data['Electrical'] = house_data['Electrical'].fillna(house_data['Electrical'].mode()[0])\n\nhouse_data['KitchenQual'] = house_data['KitchenQual'].fillna(house_data['KitchenQual'].mode()[0])\nhouse_data['KitchenQual'] = house_data['KitchenQual'].fillna(house_data['KitchenQual'].mode()[0])\nhouse_data['Exterior1st'] = house_data['Exterior1st'].fillna(house_data['Exterior1st'].mode()[0])\nhouse_data['Exterior2nd'] = house_data['Exterior2nd'].fillna(house_data['Exterior2nd'].mode()[0])\n\nhouse_data['SaleType'] = house_data['SaleType'].fillna(house_data['SaleType'].mode()[0])\nhouse_data['MSSubClass'] = house_data['MSSubClass'].fillna(\"None\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#MSSubClass=The building class\nhouse_data['MSSubClass'] = house_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nhouse_data['OverallCond'] = house_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nhouse_data['YrSold'] = house_data['YrSold'].astype(str)\nhouse_data['MoSold'] = house_data['MoSold'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(house_data[c].values)) \n    house_data[c] = lbl.transform(list(house_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(house_data.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"house_data['TotalSF'] = house_data['TotalBsmtSF'] + house_data['1stFlrSF'] + house_data['2ndFlrSF']\n\nhouse_data['Total_Bathrooms'] = (house_data['FullBath'] + (0.5 * house_data['HalfBath']) +\n                               house_data['BsmtFullBath'] + (0.5 * house_data['BsmtHalfBath']))\n\nhouse_data['Total_porch_sf'] = (house_data['OpenPorchSF'] + house_data['3SsnPorch'] +\n                              house_data['EnclosedPorch'] + house_data['ScreenPorch'] +\n                              house_data['WoodDeckSF'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"house_data['haspool'] = house_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nhouse_data['has2ndfloor'] = house_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nhouse_data['hasgarage'] = house_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nhouse_data['hasbsmt'] = house_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nhouse_data['hasfireplace'] = house_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Transformation of Skewed Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"numercic_features = house_data.dtypes[house_data.dtypes != 'object'].index\nskewed_fetures = house_data[numercic_features].apply(lambda x:x.skew()).sort_values(ascending=False)\nskewness = pa.DataFrame({'Skew' :skewed_fetures})\nskewness.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skewness = skewness.drop(['haspool','has2ndfloor','hasgarage','hasbsmt','hasfireplace'],axis=0)\nplt.figure(figsize=(20,6))\nskewed_feaures_more = skewness[skewness['Skew'] > 0.75].index\nskewd_data = skewness.Skew[skewness['Skew'] > 0.75]\nsn.barplot(skewed_feaures_more,skewd_data)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Box-Cox Transformation on Skewed Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"skewness = skewness[abs(skewness.Skew) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    house_data[feat] = boxcox1p(house_data[feat], lam)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Encoding remaining Catagorical Features "},{"metadata":{"trusted":true},"cell_type":"code","source":"house_data = pa.get_dummies(house_data)\nprint(\"After converting the remaining catagorical features into dummy variables we get {}\".format(house_data.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling "},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train =train.SalePrice.values\ntrain = house_data[:len(train)]\ntest = house_data[len(train):]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#linear = make_pipeline(RobustScaler(),LinearRegression())\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\nsvr = make_pipeline(RobustScaler(),\n                      SVR(C= 20, epsilon= 0.008, gamma=0.0003,))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(svr)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(averaged_models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_averaged_models = StackingAveragedModels(base_models = (KRR, lasso, ENet, GBoost, model_xgb, model_lgb),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Xtreme Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### LGB"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Lasso Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso.fit(train,y_train)\nlasso_train_pred = lasso.predict(train)\nlasso_pred = np.expm1(lasso.predict(test))\nprint(rmsle(y_train,lasso_train_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVR"},{"metadata":{"trusted":true},"cell_type":"code","source":"svr.fit(train,y_train)\nsvr_train_pred = svr.predict(train)\nsvr_pred = np.expm1(svr.predict(test.values))\nprint(rmsle(y_train,svr_train_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### KRR"},{"metadata":{"trusted":true},"cell_type":"code","source":"KRR.fit(train,y_train)\nKRR_train_pred = KRR.predict(train)\nKRR_pred = np.expm1(KRR.predict(test.values))\nprint(rmsle(y_train,KRR_train_pred)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ENet"},{"metadata":{"trusted":true},"cell_type":"code","source":"ENet.fit(train,y_train)\nENet_train_pred = ENet.predict(train)\nENet_pred = np.expm1(ENet.predict(test.values))\nprint(rmsle(y_train,ENet_train_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"GBoost.fit(train,y_train)\nGBoost_train_pred = GBoost.predict(train)\nGBoost_pred = np.expm1(GBoost.predict(test.values))\nprint(rmsle(y_train,GBoost_train_pred)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Averaged Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"averaged_models.fit(train,y_train)\naveraged_models_train_pred = averaged_models.predict(train)\naveraged_models_pred =np.expm1(averaged_models.predict(test))\nprint(rmsle(y_train,averaged_models_train_pred)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Rmsle score on train Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.25 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.10+GBoost_train_pred*0.15 + svr_train_pred*0.15 +\n               lasso_train_pred*0.05+KRR_train_pred*0.15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble = stacked_pred*0.25 + xgb_pred*0.15 + lgb_pred*0.10 + GBoost_pred*0.15 + svr_pred*0.15 + lasso_pred*0.05+KRR_pred*0.15","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test = pa.DataFrame(ensemble + 0.11658)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pa.DataFrame()\nsub['Id'] = test_id\nsub['SalePrice'] = ensemble\nsub.to_csv('final_sub.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}