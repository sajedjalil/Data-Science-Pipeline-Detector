{"cells":[{"metadata":{"_uuid":"8cf32ed0a9bf164c275e9c7bd3fce38f0f59a3ff"},"cell_type":"markdown","source":"**In this kernel, I try to use Pretrain Bert Model and Feed Forword Network.\n※I am just starter for deep learning.**\n**If there are some mistakes, please comment.**\n\n1. used code from the kernel below to get word Embedding from pretrain Bert mode.\nhttps://www.kaggle.com/mateiionita/taming-the-bert-a-baseline\n\n2. Inspired by https://arxiv.org/pdf/1805.04893v1.pdf and https://cs.stanford.edu/people/kevclark/resources/clark-manning-emnlp2016-deep.pdf.\nI assume that FFNN reduce A, B, Pronoun dimensions(from Bert)  and  only  keep  information relevant to coreference decisions. \n\n"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport zipfile\nimport gc\nfrom tqdm import tqdm_notebook as tqdm\nimport re\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\nwith zipfile.ZipFile(\"uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n    zip_ref.extractall()\n!ls 'uncased_L-12_H-768_A-12'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e265e7becd34d793d13009f851a4fc7c6f7f95fa","trusted":true},"cell_type":"code","source":"!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n!wget https://raw.githubusercontent.com/google-research/bert/master/extract_features.py \n!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"440d7107d50a014003a2c53485554dc693f81982","trusted":true},"cell_type":"code","source":"import modeling\nimport extract_features\nimport tokenization\nimport tensorflow as tf\nimport spacy\nnlp = spacy.load('en_core_web_lg')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a15e5f4c32659d3408dee927035725ae0135a39","trusted":true},"cell_type":"code","source":"test_df  = pd.read_table('../input/gap-coreference/gap-development.tsv')\ntrain_df = pd.read_table('../input/gap-coreference/gap-test.tsv')\nval_df   = pd.read_table('../input/gap-coreference/gap-validation.tsv')\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3d1e4d3037745835e8f34b44a6abea86a85ba7f","trusted":true},"cell_type":"code","source":"#This code is referenced from \n#https://www.kaggle.com/keyit92/coref-by-mlp-cnn-coattention\n\ndef bs(lens, target):\n    low, high = 0, len(lens) - 1\n\n    while low < high:\n        mid = low + int((high - low) / 2)\n\n        if target > lens[mid]:\n            low = mid + 1\n        elif target < lens[mid]:\n            high = mid\n        else:\n            return mid + 1\n\n    return low\n\ndef bin_distance(dist):\n    \n    buckets = [1, 2, 3, 4, 5, 8, 16, 32, 64]  \n    low, high = 0, len(buckets)\n    while low < high:\n        mid = low + int((high-low) / 2)\n        if dist > buckets[mid]:\n            low = mid + 1\n        elif dist < buckets[mid]:\n            high = mid\n        else:\n            return mid\n\n    return low\n\ndef distance_features(P, A, B, char_offsetP, char_offsetA, char_offsetB, text, URL):\n    \n    doc = nlp(text)\n    \n    lens = [token.idx for token in doc]\n    mention_offsetP = bs(lens, char_offsetP) - 1\n    mention_offsetA = bs(lens, char_offsetA) - 1\n    mention_offsetB = bs(lens, char_offsetB) - 1\n    \n    mention_distA = mention_offsetP - mention_offsetA \n    mention_distB = mention_offsetP - mention_offsetB\n    \n    splited_A = A.split()[0].replace(\"*\", \"\")\n    splited_B = B.split()[0].replace(\"*\", \"\")\n    \n    if re.search(splited_A[0], str(URL)):\n        contains = 0\n    elif re.search(splited_B[0], str(URL)):\n        contains = 1\n    else:\n        contains = 2\n    \n    dist_binA = bin_distance(mention_distA)\n    dist_binB = bin_distance(mention_distB)\n    output =  [dist_binA, dist_binB, contains]\n    \n    return output\n\ndef extract_dist_features(df):\n    \n    index = df.index\n    columns = [\"D_PA\", \"D_PB\", \"IN_URL\"]\n    dist_df = pd.DataFrame(index = index, columns = columns)\n\n    for i in tqdm(range(len(df))):\n        \n        text = df.loc[i, 'Text']\n        P_offset = df.loc[i,'Pronoun-offset']\n        A_offset = df.loc[i, 'A-offset']\n        B_offset = df.loc[i, 'B-offset']\n        P, A, B  = df.loc[i,'Pronoun'], df.loc[i, 'A'], df.loc[i, 'B']\n        URL = df.loc[i, 'URL']\n        \n        dist_df.iloc[i] = distance_features(P, A, B, P_offset, A_offset, B_offset, text, URL)\n        \n    return dist_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"547d5cdb18889257b2f4e460cd06cfeae9bd388b","trusted":true},"cell_type":"code","source":"test_dist_df = extract_dist_features(test_df)\ntest_dist_df.to_csv('test_dist_df.csv', index=False)\nval_dist_df = extract_dist_features(val_df)\nval_dist_df.to_csv('val_dist_df.csv', index=False)\ntrain_dist_df = extract_dist_features(train_df)\ntrain_dist_df.to_csv('train_dist_df.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"128bf9ecc2123276f55c58cc7f49ed3e2c9eb30b","trusted":true},"cell_type":"code","source":"def count_char(text, offset):   \n    count = 0\n    for pos in range(offset):\n        if text[pos] != \" \": count +=1\n    return count\n\ndef candidate_length(candidate):\n    count = 0\n    for i in range(len(candidate)):\n        if candidate[i] !=  \" \": count += 1\n    return count\n\ndef count_token_length_special(token):\n    count = 0\n    special_token = [\"#\", \" \"]\n    for i in range(len(token)):\n        if token[i] not in special_token: count+=1\n    return count\n\ndef embed_by_bert(df):\n    \n    text = df['Text']\n    text.to_csv('input.txt', index=False, header=False)\n    os.system(\"python3 extract_features.py \\\n               --input_file=input.txt \\\n               --output_file=output.jsonl \\\n               --vocab_file=uncased_L-12_H-768_A-12/vocab.txt \\\n               --bert_config_file=uncased_L-12_H-768_A-12/bert_config.json \\\n               --init_checkpoint=uncased_L-12_H-768_A-12/bert_model.ckpt \\\n               --layers=-1 \\\n               --max_seq_length=256 \\\n               --batch_size=8\")\n    \n    bert_output = pd.read_json(\"output.jsonl\", lines = True)\n    bert_output.head()\n    \n    os.system(\"rm input.txt\")\n    os.system(\"rm output.jsonl\")\n    \n    index = df.index\n    columns = [\"emb_A\", \"emb_B\", \"emb_P\", \"label\"]\n    emb = pd.DataFrame(index = index, columns = columns)\n    emb.index.name = \"ID\"\n    \n    for i in tqdm(range(len(text))):\n        \n        features = bert_output.loc[i, \"features\"]\n        P_char_start = count_char(df.loc[i, 'Text'], df.loc[i, 'Pronoun-offset'])\n        A_char_start = count_char(df.loc[i, 'Text'], df.loc[i, 'A-offset'])\n        B_char_start = count_char(df.loc[i, 'Text'], df.loc[i, 'B-offset'])\n        A_length = candidate_length(df.loc[i, 'A'])\n        B_length = candidate_length(df.loc[i, 'B'])\n        \n        emb_A = np.zeros(768)\n        emb_B = np.zeros(768)\n        emb_P = np.zeros(768)\n        \n        char_count = 0\n        cnt_A, cnt_B = 0, 0\n        \n        for j in range(2, len(features)):\n            token = features[j][\"token\"]\n            token_length = count_token_length_special(token)\n            if char_count == P_char_start:\n                emb_P += np.asarray(features[j][\"layers\"][0]['values']) \n            if char_count in range(A_char_start, A_char_start+A_length):\n                emb_A += np.asarray(features[j][\"layers\"][0]['values'])\n                cnt_A += 1\n            if char_count in range(B_char_start, B_char_start+B_length):\n                emb_B += np.asarray(features[j][\"layers\"][0]['values'])\n                cnt_B += 1                \n            char_count += token_length\n        \n        emb_A /= cnt_A\n        emb_B /= cnt_B\n        \n        label = \"Neither\"\n        if (df.loc[i,\"A-coref\"] == True):\n            label = \"A\"\n        if (df.loc[i,\"B-coref\"] == True):\n            label = \"B\"\n\n        emb.iloc[i] = [emb_A, emb_B, emb_P, label]\n        \n    return emb     ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d2ca43cace4ee9eeea274df5c57ae5f5db04604","trusted":true},"cell_type":"code","source":"test_emb = embed_by_bert(test_df)\ntest_emb.to_json(\"contextual_embeddings_gap_test.json\", orient = 'columns')\nvalidation_emb = embed_by_bert(val_df)\nvalidation_emb.to_json(\"contextual_embeddings_gap_validation.json\", orient = 'columns')\ntrain_emb = embed_by_bert(train_df)\ntrain_emb.to_json(\"contextual_embeddings_gap_train.json\", orient = 'columns')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65d917c5ca0c0e308cf766f700488fcf183f748c","trusted":true},"cell_type":"code","source":"from keras.layers import *\nimport keras.backend as K\nfrom keras.models import *\nimport keras\nfrom keras import optimizers\nfrom keras import callbacks\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nclass End2End_NCR():\n    \n    def __init__(self, word_input_shape, dist_shape, embed_dim=20): \n        \n        self.word_input_shape = word_input_shape\n        self.dist_shape   = dist_shape\n        self.embed_dim    = embed_dim\n        self.buckets      = [1, 2, 3, 4, 5, 8, 16, 32, 64] \n        self.hidden_dim   = 150\n        \n    def build(self):\n        \n        A, B, P = Input((self.word_input_shape,)), Input((self.word_input_shape,)), Input((self.word_input_shape,))\n        dist1, dist2 = Input((self.dist_shape,)), Input((self.dist_shape,))\n        inputs = [A, B, P]\n        dist_inputs = [dist1, dist2]\n        \n        self.dist_embed = Embedding(len(self.buckets)+1, self.embed_dim)\n        self.ffnn       = Sequential([Dense(self.hidden_dim, use_bias=True),\n                                     Activation('relu'),\n                                     Dropout(rate=0.2, seed = 7),\n                                     Dense(1, activation='linear')])\n        \n        dist_embeds = [self.dist_embed(dist) for dist in dist_inputs]\n        dist_embeds = [Flatten()(dist_embed) for dist_embed in dist_embeds]\n        \n        #Scoring layer\n        #In https://www.aclweb.org/anthology/D17-1018, \n        #used feed forward network which measures if it is an entity mention using a score\n        #because we already know the word is mention.\n        #In here, I just focus on the pairwise score\n        PA = Multiply()([inputs[0], inputs[2]])\n        PB = Multiply()([inputs[1], inputs[2]])\n        #PairScore: sa(i,j) =wa·FFNNa([gi,gj,gi◦gj,φ(i,j)])\n        # gi is embedding of Pronoun\n        # gj is embedding of A or B\n        # gi◦gj is element-wise multiplication\n        # φ(i,j) is the distance embedding\n        PA = Concatenate(axis=-1)([P, A, PA, dist_embeds[0]])\n        PB = Concatenate(axis=-1)([P, B, PB, dist_embeds[1]])\n        PA_score = self.ffnn(PA)\n        PB_score = self.ffnn(PB)\n        # Fix the Neither to score 0.\n        score_e  = Lambda(lambda x: K.zeros_like(x))(PB_score)\n        \n        #Final Output\n        output = Concatenate(axis=-1)([PA_score, PB_score, score_e]) # [Pronoun and A score, Pronoun and B score, Neither Score]\n        output = Activation('softmax')(output)        \n        model = Model(inputs+dist_inputs, output)\n        \n        return model\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78ad73eabc65583a927c7f2ac721aa4551ae0992","trusted":true},"cell_type":"code","source":"def create_input(embed_df, dist_df):\n    \n    assert len(embed_df) == len(dist_df)\n    all_P, all_A, all_B = [] ,[] ,[]\n    all_label = []\n    all_dist_PA, all_dist_PB = [], []\n    url_A, url_B = [], []\n    \n    for i in tqdm(range(len(embed_df))):\n        \n        all_P.append(embed_df.loc[i, \"emb_P\"])\n        all_A.append(embed_df.loc[i, \"emb_A\"])\n        all_B.append(embed_df.loc[i, \"emb_B\"])\n        all_dist_PA.append(dist_df.loc[i, \"D_PA\"])\n        all_dist_PB.append(dist_df.loc[i, \"D_PB\"])\n        \n        if dist_df.loc[i, \"IN_URL\"] == 0:\n            url_A.append(1)\n            url_B.append(0)\n        elif dist_df.loc[i, \"IN_URL\"] == 1:\n            url_A.append(0)\n            url_B.append(1)\n        else:\n            url_A.append(0)\n            url_B.append(0)\n        \n        label = embed_df.loc[i, \"label\"]\n        if label == \"A\": \n            all_label.append(0)\n        elif label == \"B\": \n            all_label.append(1)\n        else: \n            all_label.append(2)\n    \n    return [np.asarray(all_A), np.asarray(all_B), np.asarray(all_P),\n            np.expand_dims(np.asarray(all_dist_PA),axis=1),\n            np.expand_dims(np.asarray(all_dist_PB),axis=1)],all_label","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78cb5a82884acc0592330f65ac2df5e40408a747","trusted":true},"cell_type":"code","source":"new_emb_df = pd.concat([train_emb, validation_emb])\nnew_emb_df = new_emb_df.reset_index(drop=True)\nnew_dist_df = pd.concat([train_dist_df, val_dist_df])\nnew_dist_df = new_dist_df.reset_index(drop=True)\n\nnew_emb_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9eecf8cbabf3ebd4e98ba5c80c00b6681a0b4943","trusted":true},"cell_type":"code","source":"X_train, y_train = create_input(new_emb_df, new_dist_df)\nX_test, y_test = create_input(test_emb, test_dist_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bb874eb34b26635fbe65bc271a0acda9f9af237"},"cell_type":"code","source":"model = End2End_NCR(word_input_shape=X_train[0].shape[1], dist_shape=X_train[3].shape[1]).build()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73812eb91330412324d1da9eb641f672e43e0a59","trusted":true},"cell_type":"code","source":"SVG(model_to_dot(model).create(prog='dot', format='svg'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"727130aaa44085dec2282398e86762c818d1ae57","trusted":true},"cell_type":"code","source":"min_loss = 1.0\nbest_model = 0\n# Use Kfold to get best model\n\nfrom sklearn.model_selection import KFold\nn_fold = 5\nkfold = KFold(n_splits=n_fold, shuffle=True, random_state=3)\nfor fold_n, (train_index, valid_index) in enumerate(kfold.split(X_train[0])):\n    \n    X_tr  = [inputs[train_index] for inputs in X_train]\n    X_val = [inputs[valid_index] for inputs in X_train]\n    y_tr  = np.asarray(y_train)[train_index]\n    y_val = np.asarray(y_train)[valid_index]\n    \n    model = End2End_NCR(word_input_shape=X_train[0].shape[1], dist_shape=X_train[3].shape[1]).build()\n    model.compile(optimizer=optimizers.Adam(lr=0.001), loss=\"sparse_categorical_crossentropy\")\n    file_path = \"best_model_{}.hdf5\".format(fold_n+1)\n    check_point = callbacks.ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 0, save_best_only = True, mode = \"min\")\n    early_stop = callbacks.EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience=100)\n    hist = model.fit(X_tr, y_tr, batch_size=128, epochs=1000, validation_data=(X_val, y_val), verbose=0,\n              shuffle=True, callbacks = [check_point, early_stop])\n    \n    if min(hist.history['val_loss']) < min_loss:\n        min_loss = min(hist.history['val_loss'])\n        best_model = fold_n + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fac0215cd009f4b1a9af60b0f795606a9924ab8"},"cell_type":"code","source":"del model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f16dbf78d029c6840415e69021a242bddfd3695","scrolled":true,"trusted":true},"cell_type":"code","source":"#Use best model to predict\nmodel = End2End_NCR(word_input_shape=X_train[0].shape[1], dist_shape=X_train[3].shape[1]).build()\nmodel.load_weights(\"./best_model_{}.hdf5\".format(best_model))\npred = model.predict(x = X_test, verbose = 0)\n\nsub_df_path = os.path.join('../input/gendered-pronoun-resolution/', 'sample_submission_stage_1.csv')\nsub_df = pd.read_csv(sub_df_path)\nsub_df.loc[:, 'A'] = pd.Series(pred[:, 0])\nsub_df.loc[:, 'B'] = pd.Series(pred[:, 1])\nsub_df.loc[:, 'NEITHER'] = pd.Series(pred[:, 2])\n\nsub_df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b95c352c3847f2adeaf8153b4eaa46fab958096","trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\ny_one_hot = np.zeros((2000, 3))\nfor i in range(len(y_test)):\n    y_one_hot[i, y_test[i]] = 1\nlog_loss(y_one_hot, pred) # Calculate the log loss ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d17c4e194ecfd465f20b465effe3b7a33038a6dc","trusted":true},"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}