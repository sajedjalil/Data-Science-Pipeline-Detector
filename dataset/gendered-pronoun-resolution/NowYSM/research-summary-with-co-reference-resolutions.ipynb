{"cells":[{"metadata":{"_uuid":"821bfad40b4ca0c423c33b8ca049f9420817ea85"},"cell_type":"markdown","source":"## Research topic Name : **Entity Coreference Resolutions - NLU Task**\n\n----\n\n![](https://camo.githubusercontent.com/94157dbf6ab835f0608aa44d8fca92b4ae74eeec/68747470733a2f2f68756767696e67666163652e636f2f636f7265662f6173736574732f7468756d626e61696c2d6c617267652e706e67)\n![](https://storage.googleapis.com/kagglesdsdata/datasets/106140/287946/sample.JPG?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1550303436&Signature=k15Uq4bg8h9Raqs3FVQArPATMazCL1KFJSsjBZkwkBVAqScdnH54cfhzIrmoZI8ZmJqX%2B17KuJzr9bsasoeP2FnUPHIM5LNLbvrJdgutkh8ax3iJU6xOP0Kw6uUXAlt1CBd02qgL0Bq10Nbn96UOLFMDqdUJt%2B7JR27JsCI8f9XOxee1UFQOv8HpR022V4b5ZYKYs9kaBK7QJyu6IvYmsJtl0py1I2uxGO8K%2F1miBhKlFtIP%2BRZEQqOxMSw5fcekp5p3Sgz47ZmsNuDDyMIddKeYaNzLXYZIvDCcNaUkcwWHT6uTqaKIXyWUEVxVUZ45bKqSAk3UxcrO0bBz15f8cw%3D%3D)\n\n**Note : Here, I have tried to giving full insight about this topic as much as possible.I have gather all information by research from the internet. so if I missed something suggest. **\n\n* For this topic I will explaied here some basic so It will make it clear more to understand this topic\n\n\n\n---\n## Index : \n[**1 .What is Coreference?**](#1-.What-is-Coreference?) | [**2. Types of Coreference**](#2.-Types-of-Coreference) | [**3.What is Coreference Resolutions?**](#3.What-is-Coreference-Resolutions?) | [**4. Research Paper Summary**](#4.-Research-Paper-Summary)\n\n---\n\n## 1 .What is Coreference?\n\n---\n\n* In Simple word, ***When two or more expression in the text refer to the same person or thing.*** they have same **Coreference**."},{"metadata":{"_uuid":"285d06f90473bf0b4941d8e2fd4306e14c5f22bc"},"cell_type":"markdown","source":"## 2. Types of Coreference\n\n---\n\nThere are many types of coreferences some are below:\n* **Anaphora**\n* **Cataphora**\n* **Split antecedents**\n* **coreferring noun phrases**"},{"metadata":{"_uuid":"45f70a4db5dfa2ee59308dd262f6f152357c7d9b"},"cell_type":"markdown","source":"### **Anaphora**\n\n* ***In rhetoric, an anaphora is a rhetorical device that consists of repeating a sequence of words at the beginnings of neighboring clauses, thereby lending them emphasis. In contrast, an epistrophe is repeating words at the clauses' ends. The combination of anaphora and epistrophe results in symploce...Wikipedia***\n\n![](https://i.pinimg.com/originals/52/77/f6/5277f6466e3406ed2995f94418ac0d77.png)\n\n#### **Example**\n* Anaphora appears **frequently in literature, politics, and music.** Below are a few **famous examples of anaphora, which offer some insight into the way it works in various contexts.**\n\n**Anaphora Example in Bible** \n> ***Look at you!*** You are beautiful, my darling.  \n***Look at you!*** You are so beautiful.   \n***Your*** eyes behind your veil are doves  \n***your*** hair is like a flock of goats  \ncoming down from Mt. Gilead.  \n***Your*** teeth are like a flock of sheep about to be sheared,   \nwho are coming up from being washed.  \n\n*you can see that highlighted word are Anaphora Resolutions*\n\n![](https://i.pinimg.com/originals/67/e3/fd/67e3fd11a96bf96301e64e7d3dd91752.jpg)\n"},{"metadata":{"_uuid":"0268b874042eff62947458437d73309da2c5f786"},"cell_type":"markdown","source":"### **Cataphora**\n\n* ***In linguistics, cataphora is the use of an expression or word that co-refers with a later, more specific, expression in the discourse. The preceding expression, whose meaning is determined or specified by the later expression, may be called a cataphor...Wikipedia***\n* In simple word, **Cataphora** is a **figure of speech or literary device in which a pronoun or pro-verb used initially in a sentence refers to an expression or subject which is used afterward. **\n\n![](https://ars.els-cdn.com/content/image/1-s2.0-S0378216615003525-gr2.jpg)\n\n#### **Example**\n\n> \"Why do we envy him, the ***bankrupt man***?\"  \n(John Updike, Hugging the Shore, 1984)\nA few weeks before he died, ***my father** gave me an old cigar box filled with faded letters.  \n\nHighlighted words are example of cataphora\n\n\n![](https://images.slideplayer.com/21/6267327/slides/slide_7.jpg)"},{"metadata":{"_uuid":"c37788185d73e87fb36730303a3de4567858a03b"},"cell_type":"markdown","source":"### **Split antecedents**\n\n* ***Split antecedent is antecedent which consists of more than one NounPhrase***.\n\n![](https://www.researchgate.net/profile/Lucie_Polakova2/publication/294860409/figure/fig1/AS:497837280591872@1495705065907/Coordinative-constructions-and-split-antecedents-Example-57_W640.jpg)\n\n#### **Example :**\n\n> ***Caroli*** told ***Bobi*** to attend the party. ***They*** arrived together.   \nThe **anaphor** they has a **split antecedent**, referring to both **Carol and Bob.** (*...Wikipedia Example*)\n"},{"metadata":{"_uuid":"161b07cccb66e55207d7b6e478308773170780f2"},"cell_type":"markdown","source":"### **Coreferring noun phrases**\n\n* ***When some noun refering for some pronouns.***\n\n#### **Example**\n\n> ***The project leader*** is refusing to help. ***The jerk*** thinks only of himself.   \n**Coreferring noun phrases,** whereby the **second noun phrase** is a **predication over the first.**  \n> ***Some of our colleagues*** are going to be supportive. ***These kinds of people*** will earn our gratitude.  \n**Coreferring noun phrases,** whereby the **second noun phrase** is a **predication over the first.**  "},{"metadata":{"_uuid":"cd762c4ba4aba8299880537ce7febf8207b66510"},"cell_type":"markdown","source":"### *After basic understanding of Coreferences we are now going to understand the topic coreferences resolutions.*\n\n\n## **3.What is Coreference Resolutions?**\n\n---\n\n\n* ***Coreference resolution is the process of determining whether two expressions in natural language refer to the same entity in the world. It is an important subtask in natural language processing systems.*** \n\nReference : [A Machine Learning Approach to Coreference Resolution of Noun Phrases](https://www.mitpressjournals.org/doi/pdfplus/10.1162/089120101753342653)\n\n![](https://i0.wp.com/ai.orbifold.net/default/wp-content/uploads/2018/06/corefexample.png?resize=1030%2C550)\n\n* In the example below you can see an example of **(positive and negative) coreferences** in the ***sentence ‘My mother’s name is Sasha, she likes dogs.‘.*** In this, ***the token ‘she’ refers to ‘Sasha’ and not ‘dogs’.*** It takes a ***lot of (annotated) data and neural training to make this happen.*** Nowadays a precision of around ***75% can be achieved in English while other languages are slowly catching up.***\n\n* There are a lot of linguistic subtleties when it comes to ***coreferences points(pleonasms, anaphora, cataphora etc.)***  but ***according to the AI of the consumer and the chatbots, the correction is something wonderful.*** As long as the given ***text is not too big, everything is fine.*** On the contrary, imagine that you gave twenty pages of ***Shakespeare and ask for \"him\", which is mentioned on the last page. The time between coreferences can not be too long. Short entries of chatbot records, emergency calls and messaging applications are ideal.***\n* Basic references can be part of a larger semantic graph (and semantic reasoner) that represents the essence of a particular context (for example, an emergency call). Essentially, any correlated (together) entry is a separate subgraph, but references create a connected graph. A little like substances that create weak bonds through cohesion.\n\n"},{"metadata":{"_uuid":"307410356d9034fdb42d6cbffbfeb8e8a5147cda"},"cell_type":"markdown","source":"## **4. Research Paper Summary**\n\n**Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns**  \n*by $Kellie Webster$ and $Marta Recasens$ and $Vera Axelrod$ and $Jason Baldridge$*"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"4461f6d7dd96f3f75ef483b056b71de259bf9461"},"cell_type":"code","source":"from IPython.display import HTML\nHTML(\"<iframe width = 100%, height = 800, src = 'https://arxiv.org/pdf/1810.05201.pdf'></iframe>\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c91a55092a6c821e3971d4184463cc991d821de2"},"cell_type":"markdown","source":"### **Abstract :**\n\n* *Coreference resolution is an important task for natural language understanding, and the resolution of ambiguous pronouns a longstanding challenge.*\n* Existing Corpus of the data don't detect the **ambigous pronouns in sufficient volume.**\n* In this research article they have find **gender bias in existing corpura and musculine entities.**\n* Represent the ***GAP Dataset which contain 8908 ambigious pronoun-name pairs sampled to provide diverse coverage of challenges posed by real-world text.*** \n* ***Google Research Team*** has been achiving F1 score till now **66.9 %**"},{"metadata":{"_uuid":"0d09643628bf0a0aa4aca4c8c967bdffb27a3926"},"cell_type":"markdown","source":"### **1.Introdution**\n\n* Coreference resolutions is same task like $CoNLL 2011/12 (Pradhan et al., 2012)$ and MUC $(Grishman and Sundheim, 1996)$\n*  high-scoring systems successfully identify coreference relationships between string-matching proper names, but fare worse on anaphoric mentions such as pronouns and common noun phrases.\n* There consideration of problem of resolving gendered ambiguous pronouns in English, such as $she$ in\n> In May, $Fujisawa$ joined $Mari Motohashi’s rink$ as the team’s skip,moving back from Karuizawato Kitami where **$she$** had spent her junior days.\n\n#### **Three Contribution by Research team**\n\n1. *They design an* ***extensible, langauge independent mechanism for extracting challanging ambigious pronouns from text.***\n2. ***Design dataset GAP, which contains human labeled corpus of 8,908 ambigious pronoun name pairs derived from wikipedia.***\n3.  *They run four state-of-the-art coreference resolvers and several competitive simple baselines on GAP to understand limitations in current modeling, including gender bias. We ﬁnd that syntactic structure and Transformer models (Vaswani et al., 2017) provide promising, complementary cues for approaching GAP.*"},{"metadata":{"_uuid":"ab64adc5444f8de25e2dfa81b020fb0a9e1c7116"},"cell_type":"markdown","source":"### **2.Background**\n\n* Existing datasets do not capture ambiguous pronouns in sufﬁcient volume or diversity to benchmark systems for practical applications.\n\n#### ***2.1.Datasets with Ambiguous Pronouns***\n\nHere they are talking about similar dataset which is contain pronouns ambigiousness\n\n1. The Deﬁnite Pronoun Resolution Dataset $(Rahman and Ng, 2012)$ comprises **943 Winograd schemas**\n2.  Rudinger et al. (2018) and Zhao et al. (2018) have created two **Winograd schema-style datasets** containing **720 and 3160 sentences**\n3.  **GAP examples are not strictly *Winograd schemas* because they have no *reference-ﬂipping word.* Nonetheless, they *contain two person named entities of the same gender and an ambiguous pronoun* that may refer to either (or neither).**\n\n#### ***2.2.Modeling Ambiguous Pronouns***\n\nState-of-the-art coreference systems struggle to resolve ambiguous pronouns that require world knowledge and commonsense reasoning $(Durrett and Klein, 2013).$\n\n![](https://storage.googleapis.com/kaggle-datasets/106140/281119/table.JPG?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1550293672&Signature=J%2Be87iuHlNusas%2F%2F%2BID2yBidzcw2m6QJQIHXD%2BaUj%2FubHwWq9AhDtV5n0MhFE7BmxSBmwjspt4nuGnq%2FMi89M3HVmFiC25CtqabKTSYZE7B8gqQrtuiLgKJcMDUPp11oZzlRUHYodSHkHw%2Ff8gtmPP4rf0g7Ca9iHher57tJkPRBSTdqkH1hPVVnqjFHPrhLyUoRGcFZBYyzGXJhSiLumyLABCrfvJdtTWMEVBFhuPuNRd3z%2FJ1DHuOMOH8gXLgzhnz1b9DCeHRxIhvkX95LVZq%2BjJCm8KVClm4Rqg%2F5C6e6jCXBQCjg%2F%2BMTuFuGUbxy8I%2FnBVQ08U9kkiNohfqWzA%3D%3D)\n\n* Past effort tried to mine semantic preferences and inferential knowledge via predicate-argument statistics mined from corpora, sematic rules, contextual compatibility features and event role sequences this will give small improvement in dataset.\n*  ***Rahman and Ng $(2012)$ scored*** $73.05%$ precision on their ***Winograd dataset** after incorporating **targeted features such as narrative chains, Webbased counts, and selectional preferences.*** \n* ***Peng et al. (2015)’s system*** improved the state of the art to $76.41%$ by ***acquiring subject, verb, objecti and subject/object, verb, verb knowledge triples.***\n\n#### ***2.3 Bias in Machine Learning***\n\n* While existing corpora have promoted research into coreference resolution, they suffer from gender bias. Speciﬁcally, of the over 2,000 gendered pronouns in the OntoNotes test corpus, less than 25% are feminine $(Zhao et al., 2018).$\n* ***The imbalance is more pronounced on the development andtrainingsets,withlessthan20%femininepronouns each.***\n* ***WinoBias datasets focus on pronominal coreference where the antecedent pronominal mention,** while **GAP focuses on relations where the antecedent** is a named entity.\n\n> <u>The salesperson</u> sold some books to the librarian because she was trying the sell them.\n\n#### **Biases Direct Impact on dataset**\nThe pervasive bias in existing datasets is concerning given: \n\n* NLP systems often reﬂect and even amplify training biase $(Bolukbasi et al., 2016; Caliskan et al., 2017; Zhao et al., 2017).$\n* A growing body of work deﬁnes notions of fairness, bias, and equality in data and machine-learned systems $ (Pedreshi et al., 2008; Hardt et al., 2016; Zafar et al., 2017; Skirpan and Gorelick, 2017),$\n* Debiasing strategies include expanding and rebalancing data $(Torralba and Efros, 2011; Ryu etal.,2017;Shankaretal.,2017;Buda,2017)$\n* Balancing performance across subgroups.$(Dwork et al., 2012).$\n* In the context of coreference resolution have showed how debiasing tecniques (e.g. swapping the gender of male pronouns and antecedents in OntoNotes,using debiased word embeddings, balancing Bergsma and Lin’s (2006)’s gender list) succeed at reducing the gender bias of multiple off-the-shelf coreference systems. \n"},{"metadata":{"_uuid":"bbb44e7ec2033099976954cca3c3f6ab640bdcb4"},"cell_type":"markdown","source":"### **3.GAP Corpus**\n\n* **GAP Corpus of 8,908-human annotated ambiguous pronoun-name examples from Wikipedia.**\n\n![](https://storage.googleapis.com/kagglesdsdata/datasets/106140/281119/table1.JPG?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1550293661&Signature=ozD7xD1MUIx4%2BOJxdc5KMJSYwxTvslVmuk2zmfbTfYl0PLRmOj0rjsWR113iUJfqEF%2FiibczCY1FjNTXk9W477v9cEMiqTpUmfuADHzzCBupihhayOdqyNJ58DkHNOyjfTMxtsAj%2BpdakdsYA8vzBR5C7sldrdSXVBAJAdU1s%2B6li6dES%2B4jwJG7Zv1q3MF8okAGCesDM6R27gzN6KBsSzmROqyL%2FT%2FNj314Z0dTK8DrSaexsFe25yvngYgidXaTPwGbgzui9GBegndXZ1oRYtGcGJ%2FTKyO7lKthrhssPzIjONL9jCdedII2%2BuSrjYTu4Ctwa1xKkdnnN63yn%2F5DyA%3D%3D)\n\n* **Examples are obtained from a large set of candidate contexts and are ﬁltered through multi-stage process designed to improve quality and diversity.**\n\n#### **3.1 Extraction and Filtering**\n\nThe extract is intended for the three samples shown in Table 1, which characterize ambiguous local pronoun contexts. We confine ourselves to singular references, non-reflective pronouns of the genus, and names whose head tokens differ from each other.\n\n* ***FINALPRO.*** Both names must be in the same sentence, and the pronoun may appear in the same or directly following sentence.\n* ***MEDIALPRO.*** The first name must be in the sentence directly preceding the pronoun and the second name, both of which are in the same sentence. To decrease the bias for the pronoun to be coreferential with the first name, the pronoun must be in an initial sub- ordinate clause or be a possessive in an initial prepositional phrase.\n* ***INITIALPRO.*** All three mentions must be in the same sentence and the pronoun must be in an initial subordinate clause or a possessive in an initial prepositional phrase.\n\n*Convert and improve this diversity in five dimension*\n\n* **Page Coverage.** We retain at most 3 exam- ples per page-gender pair to ensure a broad coverage of domains.\n* **Gender.** The raw pipeline extracts contexts with a m:f ratio of 9:1. We oversampled fem- inine pronouns to achieve a 1:1 ratio.5\n* **Extraction Pattern.** The raw pipeline output contains 7 times more FINALPRO contexts than MEDIALPRO and INITIALPRO com- bined, so we oversampled the latter two to lower the ratio to 6:1:1.\n* **Page Entity.** Pronouns in a Wikipedia page often refer to the entity the page is about. We include such examples in our dataset but bal- ance them 1:1 against examples that do not include mentions of the page entity.\n* **Coreferent Name.** To ensure mention order is not a cue for systems, our final dataset is balanced for label — i.e. whether Name A or Name B is the pronoun’s referent."},{"metadata":{"trusted":true,"_uuid":"144ed1060f357c5a3635305e0369763a7d6cfa99"},"cell_type":"markdown","source":"#### **3.2 Annotation**\nWe used a pool of in-house raters for human an- notation of our examples. Each example was pre- sented to three workers, who selected one of five labels (Table 3). Full sentences of at least 50 to- kens preceding each example were presented as context (prior context beyond a section break is not included). Rating instructions accompany the dataset release.\n\n<img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/106140/281119/table3.JPG?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1550295010&Signature=AamP7mSC0y1Jp1pw%2BRdYoBBLTj3Uj%2Fz7%2B6VZGAjQ1xbXx3EFnWgm8MzjRsIDwFqpRq37PPgcinW5wM3d43j1Yv2dILAFwFlfSD%2FutfTWOmAh9F3v5dqCH9FrzfgI7HcH1KJNIQuDbCdZRXJxLVM0gIApqtso55mXdaF5rGBfzfk9b7ih5DfxvDccSsMvs1hMOjP8zaOngbTHG2jQbaJGB0hYclLRtMv9N9eQkmQkAM5ukSu5c79095KsWqdZ3P7B0DMJvBjm%2BoXd1IHwbMpiDIvtDiy3DZHOqlRQsLzCywPSjmJrMJGuFiQd18MG1UeeOwo94nyszAJbrewvXrDcnA%3D%3D\"></img>"},{"metadata":{"_uuid":"f9d51d7c17b0cc4f9e860b458272a88ac40c7ef6"},"cell_type":"markdown","source":"### **4.Experiments**\n\nThey have conduct experiment with GAP Dataset and Challange the community for this.\n\n![](https://storage.googleapis.com/kagglesdsdata/datasets/106140/287877/f1.JPG?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1550298497&Signature=j%2BLOpSK66RVcbO%2FFWT9tFNJUEz%2B5YUGgPBYCUQy5GmJMtlGxyj89l9nPQUlmG8js4ZXVZKME8C2%2BW9YbbLX%2FlhpKGb6Pg8gGt4ep2UjCec3OUm2f1Qlks%2B4DR7m%2Fksp5pbMbwSS47Pgtuh7ao14H7BbSnJAxeyalUujGidG13MWg1N1uqoZugKGldHUlXFdoVONnTB6N0Yd4jhCtAS5lVDqvaUKVRnmDOCZd6dILnPdyyk4TsGppb%2FJWs49Cdnzglmkb6%2BTgxcTqT198uWYwhZ%2BuWfNx1%2BLzzU6QVoTUjrEDy9SAcOtlBSE7bZdQyZzenBK5egNQEzUiMRkATIyBlQ%3D%3D)\n\n* All experiments use the Google Cloud NL API7 for pre-processing, \n\n#### **4.1 GAP Challenge**\n\n* GAP is an evaluation corpus and we segment the final dataset into a development and test set of **4,000 examples each8;** we reserve the remaining **908 examples as a small validation set for param- eter tuning.** All examples are presented with the URL of the source Wikipedia page, allowing us to define two task settings: snippet-context in which the URL may not be used, and page-context in which it may. While name spans are given in the data, we urge the community not to treat this as  a gold mention or Winograd-style task. \n* That is, systems should detect mentions for inference auto- matically, and access labeled spans only to output predictions.\n* To reward unbiased modeling, they define ***two evaluation metrics: F1 score and Bias. Concretely, we calculate F1 score Overall as well as by the gender of the pronoun (Masculine and Feminine). Bias is calculated by taking the ratio of feminine to masculine F1 scores, typically less than one.***\n\n\n### **4.2 Off-the-Shelf Resolvers**\n\nSome Resolver results\n\n![](https://storage.googleapis.com/kagglesdsdata/datasets/106140/287877/table4.JPG?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1550299006&Signature=M3VLkh%2BycCvvplLqen2nJOBnw%2BWINKdL6O1%2Fv7KcDVAamXypPMPUANIZRccyf0bwssUyICjDGspf7U16HYg5Do%2FUmJK75uf79HOR2GwW62XehYEWPJknb%2BPDS9u5mt%2F4ZjeDivPorrLUKY9qze6RrLRAs%2FIMAmdgqDeNMOJOGADLZ19Y%2BpItmUXtQ8Bxi0kSh52%2BXjjzXR5fPPJAQksutZrVkNS46pgEpiQAKu2FNF1THLLD0MYuOs3NvuEXan92K9%2FweVzvDxqIb4rs7Iry62u3RfHlfWb%2FPHEteXIJVz5Zy%2Fq3QAUaL%2BlPXhEW0SN77aqQeqixgUgMdBwOqgiXFw%3D%3D)\n\n* Table 4 shows that all systems struggle on GAP. That is, despite modeling improvements in recent years, ambiguous pronoun resolution remains a challenge. We note particularly the large differ- ence in performance between genders, which tra- ditionally has not been tracked but has fairness im- plications for downstream tasks using these pub- licly available models.\n\n![](https://storage.googleapis.com/kagglesdsdata/datasets/106140/287877/table5.JPG?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1550299054&Signature=dSpbbN%2FUpOp7RLdKOiurOvtI%2B5%2F2mu14TRZ4YLh2kJtufCuP2Q%2FgBgYypefWPLnMdaZx7m31dUmTMzWCN781Ozhv4H2j9oQr4Z2FLlrZWfphpyrI2YTJIZN3QsuLE%2FZlauQOQnfx4RuT4kg4siBazN2Z36nJqvA8rD%2BzZH8ia5sQ7YuW4yGVDeK2gxS4rUPFOOMGqg6177nlysuhsyaeUzVjDgg1Brh2eo0GFaMLgi2a8w0ZGkHRkOWWcW0OeGtkAEuQTY7Q8Hl9lnDzqyXpymI6hVd7kjOfuDtGTVLv86sSbFORjiN68VNt3D5vcTvkDWLhx8X8yJb8YpODwcZiNg%3D%3D)\n\n* Table 5 provides evidence that this low performance is not solely due to domain and task differences between GAP and OntoNotes. Specif- ically, with the exception of Clark and  Man- ning (2015), the table shows that system perfor- mance on pronoun-name coreference relations in the OntoNotes test set14 is not vastly better compared to GAP. One possible reason that in-domain OntoNotes performance and out-of-domain GAP performance are not very different could be that state-of-the-art systems are highly tuned for re- solving names rather than ambiguous pronouns.\n\n### **4.3 Coreference-Cue Baselines**\n\n![](https://storage.googleapis.com/kagglesdsdata/datasets/106140/287877/table6.JPG?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1550299165&Signature=YYlLuPGVfn%2BNYkJXWYz3vThMfOqYgpCrlvCpFGkpWKwn4%2Bwt0PwiE0qSZaM4seUUzXbBjbzDA1FetBjuAWMSGracKpNIFZouOLFyVoNdtSzmrrhoOh4IaaByj9grE%2F20F5RL%2BMjUaYVpjAerFtAMY4aoSI2fUzm6vNLeFnztL0Y8ytWtfaAmP6IMrTs113wy8kSTeZIGuqHDnblC1jd9%2FeQLT3FfCi1J0exctkOXugmvTQGOhGVzg5PNG30J042%2FN1boCRO0YNDw1Cotkz7amLNbbjc5dUs6LABEH2ZlX6HIqXd8DdKvGfq4dVFuqEakI1WC3FBrNuefO6Ekx%2FsAJA%3D%3D)\n\n* To understand the shortcomings of state-of-the-art coreference systems on GAP, the upper sections of Table 6 consider several simple baselines based on traditional cues for coreference.They don't require gender match because of constrain of Google NL API. and Second, They select among the candidates using one of the heuristics described next.For scoring purposes, we do not require exact string match for mention alignment, that is, if the selected candidate is a substring of a given name (or vice versa), we infer a coreference relation between that name and the target pronoun.\n\n#### Surface Cues Baseline cues which require only access to the input text are:\n* **RANDOM.** Select a candidate uniformly at random.\n* **TOKEN DISTANCE.** Select the closest can- didate to the pronoun, with distance mea- sured as the number of tokens between spans. \n* **TOPICAL ENTITY.** Select the closest candi- date which contains the most frequent token string among extracted candidates.\n\n![](https://storage.googleapis.com/kagglesdsdata/datasets/106140/287877/table7.JPG?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1550299936&Signature=D0uBXmnzuYOTKB34sw%2FYSV09Fkpc1VsSTfYe6I9TdcgYjx628MccO%2F%2BVd0BEJl3q%2FzGYwBTEW0Zrvlq%2B9xK1H9ASplUgtdcTZaI1b%2FPeCJC2tqVVIuaoOMFwBp2dE7FAoK2VAH%2Fd03rVKZ6KtwB4oJnuzpfi4vDXn3iohfGlbJmjDwMDvvhYd%2BQ4ppq63C0Y3PYjSvER%2BME%2BnA2JvriOPvuP2L7EFxtxnr8PUFzXNo3JrnNgwAYLv%2Bv7e2UpLACtTkVcP0MN8lFvFKSVnrlo80mDV7%2FFcd4AMu%2F5ZqRfnB0TVYKIoNsfjvqtkpzDljqBe%2FtvS63bqFnzb74omiP2fw%3D%3D)\n\n* The performance of ***RANDOM (41.5 Overall) is lower than an otherwise possible guess rate of 50%. This is because the baseline considers all possible candidates, not just the two annotated names. Moreover, the difference between  masculine and feminine examples suggests that there are more distractor mentions in the context of fem- inine pronouns in GAP. To measure the impact  of pronoun context, we include performance on the artificial gold-two-mention setting where only the two name spans are candidates for inference (Table 7).*** RANDOM is indeed closer here to the expected 50% and other baselines are closer to gender-parity.\n* TOKEN DISTANCE and TOPICAL ENTITY are only weak improvements above RANDOM, vali- dating that our dataset creation methodology con- trolled for these factors.\n\n#### Structural Cues Baseline cues which may ad- ditionally access syntactic structure are:\n* **SYNTACTIC DISTANCE.** Select the syntac- tically closest candidate to the pronoun. Back off to TOKEN DISTANCE.\n* **PARALLELISM.** If the pronoun is a subject or direct object, select the closest candidate with the same grammatical argument. Back off to SYNTACTIC DISTANCE.\n\n* Both cues yield strong baselines comparable to the strongest **OntoNotes-trained systems** (cf. Table 4). In fact, $Lee et al. (2017)$ and **PARALLELISM** produce remarkably ***similar output: of the 2000 example pairs in the development set, the two have completely opposing predictions (i.e. Name A  vs.  Name B) on only 325 examples.***\n\n### ***Transfomer Model on validation dataset and results***\n\n![](https://storage.googleapis.com/kagglesdsdata/datasets/106140/287877/table8.JPG?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1550300184&Signature=cmzSdi2w0TS3eQ1k3AsdWDaQxYYYAoFbkt5TVty%2BHL7xLAL8jRaWNSqhd6AZ9YrPfS1m%2Byz2XWsIuhxjAU47G18TuDNVeU%2B9RbvQjjKgk3lpiLZsAYywEIiiutheXZaeXi8AJ%2Fkw4R1RXDG2hRGTzgaW8xzNrT4LtP0SUKkZOBkFvZmNVaYT3pjw%2FPKbUoY498hS2lS6ee5cC%2B8AWHdIgQeZ71kYA%2FWJAtHiOtkxDu3P5ZxNigxsYYwtmwvEgEZVmcNSPjOTIjJn%2F0jrwuSQcWmY215t%2BjWEawylVBdG%2Buh4h04AN3koiqq%2FUTSU6ni86Uw6tF%2BKe6Bd2fP4h%2B8lXg%3D%3D)\n\n* ***Wikipedia Cues\tTo explore the page-context** setting, they consider a Wikipedia-specific cue: URL. Select the syntactically closest candi- date which has a token overlap with the page title. Back off to PARALLELISM. The heuristic gives a performance gain of 2% overall compared to PARALLELISM. That the feature is not more helpful again validates our methodology for extracting diverse examples. We expect future work to greatly improve on this base- line by using the wealth of cues in Wikipedia arti- cles, including page text.\n\n### **4.4Transformer Models for Coreference**\n\n![](https://4.bp.blogspot.com/-ovHQGevt5Ks/WaiCfS0OPUI/AAAAAAAAB_U/nEqsh9fgecM1v98NAvGp8Zgr5BwBbOGBQCEwYBhgL/s640/image4.png)\n\n* ***The Transformer model underlying our experiments is trained for 350k steps on the 2014 English-German NMT task, using the same settings as $Vaswani et al. (2017).$*** The model pro- cesses texts as a series of subtokens (text frag- ments the size of a token or smaller) and learns three multi-head attention matrices over these, two self-attention matrices (one over the subtokens of the source sentences and one over those of the target sentences), and a cross-attention matrix be- tween the source and target. \n\n* **The recent Transformer model (Vaswani et al., 2017) demonstrated tantalizing representations for coreference: when trained for machine transla- tion, some self-attention layers appear to show stronger attention weights between coreferential elements.17** $Voita et al. (2018)$ found evidence for this claim for the English pronouns it, you, and I in a movie subtitles dataset $(Lison et al., 2018).$ GAP allows us to explore this claim on Wikipedia for ambiguous personal pronouns. To do so, we investigate the heuristic:\n\n![](https://1.bp.blogspot.com/-iPcRnix4isk/WaiC02QrGbI/AAAAAAAAB_Y/KGP356uQ2uQ6ZtFm9kz9nsZ1oIgNsImGACEwYBhgL/s640/image1.png)\n\n\n## **Transformer by GOOGLE**\n\n#### **More you can read from here : http://jalammar.github.io/illustrated-transformer/**\n\n![](https://3.bp.blogspot.com/-aZ3zvPiCoXM/WaiKQO7KRnI/AAAAAAAAB_8/7a1CYjp40nUg4lKpW7covGZJQAySxlg8QCLcBGAs/s640/transform20fps.gif)\n\n* **The TRANSFORMER-SINGLE** baseline in Table 6 is the one set by L3H7 in Table 8.  Despite not having access to syntactic structure, TRANSFORMER-SINGLE far outperforms all surface cues above. That is, we find evidence for the claim that **Transformer models implicitly learn language understanding relevant to coreference resolution.** Even more promising, we find that the instances of coreference that **TRANSFORMER- SINGLE can handle is substantially different from those of PARALLELISM,** see Table 9.\n\n![](https://storage.googleapis.com/kagglesdsdata/datasets/106140/287877/table9.JPG?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1550301118&Signature=Qkdo6eVhBHXJYyjLkzbMZKjd29mO2fCWhQUzNZ5yE3%2BhwzFAyBz5DEuinE9TeYymUB5%2B4flzzBvhp3peKzd7RmqqbvyAH7F8Om0x2Ib76z1VmKTQIGi56pVmxfKuk%2FTCEL4Nyl1bFuA6V6Jskn%2FW1ASDUtSOml99ucFNLaXQkp5toDU3CXgtixwWR7SllJY6WqdCOtw28q2GZ55Qgjj6UKNopzu%2FadPgV1BcH0qWWaJczVj6Qpb2zYw0tgRGo%2FPjWni1iKnFLaK2AL%2BFC9WCmgT1UrKB6WPAX7ubtq99Gk0hFsUKDScoaQhMv9inBp%2BreP8BHLVQNZPddKXno4rxmQ%3D%3D)\n\n* **TRANSFORMER-MULTI** We learn to compose ***the signals from different self-attention heads using extra tree classifiers $(Geurts et al., 2006).$ We choose this classifier since we have little available training data and a small feature set. Specifically, for each candidate antecedent,*** \n    * Extract one feature for each of the ***48 Transformer heads. The feature value is True if there is a substring overlap between the candidate and the prediction of TRANSFORMER SINGLE.*** \n    * Use the $χ2$ statistic to reduce dimensionality. \n    * We found $k=3$ worked well. Learn an extra trees classifier over these three features with the validation dataset.\n    \n![](https://storage.googleapis.com/kagglesdsdata/datasets/106140/287877/table10.JPG?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1550302030&Signature=au30qwa337V2roE2oEGz%2FgAORIXrP5J84qZQhRR%2F%2BOuQSg991o1WEhzEve6IJW3qEmJbgBeyhzdDgYu%2BEvoaFR6%2BTJqr6tQ3e1E5aAXNwuEy1uu4bVpN3EizCaIapf92CIPXwip3uHzQvbU1bmCYJKHqiVQWVgaP6AyAY1hMQubGBvH1QMkJMFp1q3PnnOc9SAVg6OWTO0zd3qI13ud%2BwaZTOyz0tasiqjs4ZWWrY5KXOsKqmj4DKo%2BxkyeHH2NyJCR09WY20TnYCMo55XN8OUTRLCLyWzmwXbSE1F7f2Hg9n9Dy5Pzm%2FgeRlmtjlMJYmnWRORfnTcq9OPANqzzDEQ%3D%3D)\n    \n * That **TRANSFORMER-MULTI** is stronger than **TRANSFORMER-SINGLE** in ***Table 6 suggests that different self-attention heads encode different dimensions of the coreference problem. Though the gain is modest when all mentions are under consideration, Table 7 shows a 4.2% overall improve- ment over TRANSFORMER-SINGLE for the gold- two-mention task. Future work could explore fil- tering the candidate list presented to Transformer models to reduce the impact of distractor men- tions in a pronoun’s context—for example, by gen- der in the page-context setting. It is also worth stressing that these models are trained on very lit- tle data (the GAP validation set). ***\n \n ![](https://storage.googleapis.com/kagglesdsdata/datasets/106140/287877/table11.JPG?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1550302051&Signature=OIjuNwZhseWHqRYycTtBhgf3w1V0MJKDAj%2BzTS85jmNhIrLDXcO0rfH8g6OLWWGJMbxZCefASGBxEjWYtmzXZT7YIzXpfWOL2LKV8Ygf8Xp32J09bpzLQ9GrzfGWpJgWWEnUaEpeuHS02FOlrkqugS8WkQJW3JGvZDAnLA52cfLkaYAVfolM2wXDX4CRX5o6lmv%2BTAf%2Fc99mdynQ22Lm0sZlo8vgQgheeXV%2FIVJ0BKWdJaOxvoKF0jV1OUxltGW1ceQ7oewcw7frmPFAvX8kUp%2Fg2Xb0aq666A2S%2Ft6S6Nb7R4isfiitrLlpcFlSFUeybCACs5d6%2BGZAKcFgy%2Bb38A%3D%3D)\n \n ### **4.5 GAP Benchmarks**\n \n* **Table 10 sets the baselines for the GAP challenge.** We include the ***off-the-shelf system which performed best Overall on the development set $(Lee et al., 2017),$ as well as our strongest baseline for the two task settings, PARALLELISM20 and URL.*** We note that ***strict comparisons cannot be made between our snippet-context baselines given that $Lee et al. (2017)$ has access to OntoNotes annotations that we do not, and we have access to pro- noun ambiguity annotations that Lee et al. (2017) do not.***"},{"metadata":{"_uuid":"4ae32d3148094bb591583a75a52a92a921cbfc53"},"cell_type":"markdown","source":"### **5.Error Analysis**\n\n*  That ***GAP is  challenging  for both off-the-shelf systems and our baselines. To assess the variance between these systems and gain a more qualitative understanding of what as- pects of GAP are challenging, we use the num- ber of off-the-shelf systems that agree with the rater-provided labels (Agreement with Gold) as a proxy for difficulty. Table 11 breaks down the name-pronoun examples in the development set by Agreement with Gold (the smaller the agreement the harder the example).**\n* *Agreement with **Gold is low (average 2.1) and spread. Less than 30% of the examples are successfully solved by all systems (labeled Green), and just under 15% are so challenging that none of the systems gets them right (Red). The majority are in between (Yellow). Many Green cases have syntactic cues for coreference, but we find no sys- tematic trends within Yellow.***\n\n![](https://storage.googleapis.com/kagglesdsdata/datasets/106140/287877/table12.JPG?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1550302390&Signature=FTSW5FTKAVYA%2BjPM7AFIQNJEsPQQAImXuSaKH%2B5G5Mz9ATqjagoiXevM3iVEYrnVLI63828uVXcf1RteEVifgB7XI69ZhDwBX8uxV18Q9cTP0n5RU5KgJvGNYO1vQamd5R6Hz6Pj8z9EJLj1MyZk6sDHTevtB6J%2F7x1lwML%2Ff5Rinl2gT%2FZg6%2F6stZ3hOI9JQi6MMYA%2BMaS7mlJafs%2B2boezz1O04lm4IEZmrodLAO7SEdIQvz0%2BbV7rJ7hsV1aSBAcWZX0dy%2FNO%2BTJpW82OKmnw7T3c1%2FzBjszLVb7%2B5vBDi6FEzlCfI%2F5INdNlq3vGIrJTZklK9c76%2FnsDRxhc7g%3D%3D)\n\n\n### **6.Conclusion** \n> I am not changing conclusion. because it helps to getting all insights\n\n* They have presented a **dataset and a set of strong baselines for a new coreference task, GAP.** *Designed GAP to represent the challenges posed by real-world text,* in which **ambiguous pronouns are important and difficult to resolve.** *Highlighted gaps in the existing state of the art, and proposed the **application of Transformer models to address these.** Specifically, we show how traditional linguistic features and modern sentence encoder technology are complementary.*\n* ***Work contributes to the emerging body of work on the impact of bias in machine learning. We saw systematic differences between genders in analysis; this is consistent with many studies which call out differences in how males and fe- males are discussed publicly. By rebalancing our dataset for gender, we hope to reward systems which are able to capture these complexities fairly. It has been outside the scope of this paper to explore bias in other dimensions, to analyze coreference in other languages, and to study the impact on downstream systems of improved coreference res- olution. We look forward to future work in these directions.***\n\n\n### Thanks **Google  and  the Google AI Language team** for sharing such wonderful problem and this research paper\n\n### ***Note : Correct me if i had done mistake any where.***"},{"metadata":{"_uuid":"665c25d55eb790ef3f7ee26a094e3a22d85c40a8"},"cell_type":"markdown","source":"## ***Research Paper References :***\n\n1. https://arxiv.org/pdf/1810.05201.pdf\n2. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3226856/\n3. https://en.wikipedia.org/wiki/Coreference\n4. https://www.mitpressjournals.org/doi/pdfplus/10.1162/089120101753342653"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}