{"cells":[{"metadata":{"_uuid":"d48a6972378bf6116b7a6b9883992d548128640c"},"cell_type":"markdown","source":"This Kernel implements a modified version of **a state-of-art end-to-end neural correference resolution model** published in 2017: https://www.aclweb.org/anthology/D17-1018.\nThis completition only focus on a specific case of  the generic reference resolution problem, and we only need pick out the correct mention from two candidates, which simplifies the model implementation.\n\nYou can compare the result of this model  with the result by other non-RNN based DL models implemented in another kernel: https://www.kaggle.com/keyit92/coreference-resolution-by-mlp-cnn-coattention-nn. \n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport gc\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"DATA_ROOT = '../input/'\nGAP_DATA_FOLDER = os.path.join(DATA_ROOT, 'gap-coreference')\nSUB_DATA_FOLDER = os.path.join(DATA_ROOT, 'gendered-pronoun-resolution')\nFAST_TEXT_DATA_FOLDER = os.path.join(DATA_ROOT, 'fasttext-crawl-300d-2m')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"461cb23b791e2d210e712c933e62de59d19aa20d"},"cell_type":"markdown","source":"# Import Data"},{"metadata":{"trusted":true,"_uuid":"e42e2e7f636cf0702cc472f3d855b451554927dd"},"cell_type":"code","source":"# test_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-development.tsv')\nsub_test_df_path = os.path.join(SUB_DATA_FOLDER, 'test_stage_2.tsv')\ntest_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-test.tsv')\ntrain_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-development.tsv')\ndev_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-validation.tsv')\n\ntrain_df = pd.read_csv(train_df_path, sep='\\t')\ntest_df = pd.read_csv(test_df_path, sep='\\t')\ndev_df = pd.read_csv(dev_df_path, sep='\\t')\nsub_test_df = pd.read_csv(sub_test_df_path, sep='\\t')\n\n# pd.options.display.max_colwidth = 1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2eb32fae335b77dae0a60ade52b7ea0a32d8cb3d"},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d60addb918fc181bd736f5497c3114dbf3d3bfd4"},"cell_type":"markdown","source":"# Explore Features for Building Mention-Pair Distributed Representation"},{"metadata":{"trusted":true,"_uuid":"8b82c9f910c289c5b21ec982890911bfb579c32b"},"cell_type":"code","source":"spacy_model = \"en_core_web_lg\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"177d5f8cca6584d7e542ea1ed6112e091e78d787"},"cell_type":"code","source":"from spacy.lang.en import English\nfrom spacy.pipeline import DependencyParser\nimport spacy\nfrom nltk import Tree\n\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing import text as ktext","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading FastText Crawl Embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_FILE = os.path.join(FAST_TEXT_DATA_FOLDER, 'crawl-300d-2M.vec')\ndef get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1668e73e26b428fa0739c1c38653217d14d50aaa"},"cell_type":"code","source":"nlp = spacy.load(spacy_model)\n\ndef bs(list_, target_):\n    lo, hi = 0, len(list_) -1\n    \n    while lo < hi:\n        mid = lo + int((hi - lo) / 2)\n        \n        if target_ < list_[mid]:\n            hi = mid\n        elif target_ > list_[mid]:\n            lo = mid + 1\n        else:\n            return mid + 1\n    return lo\n\ndef bs_(list_, target_):\n    lo, hi = 0, len(list_) -1\n    \n    while lo < hi:\n        mid = lo + int((hi - lo) / 2)\n        \n        if target_ < list_[mid]:\n            hi = mid\n        elif target_ > list_[mid]:\n            lo = mid + 1\n        else:\n            return mid\n    return lo\n\ndef ohe_dist(dist, buckets):\n    idx = bs_(buckets, dist)\n    oh = np.zeros(shape=(len(buckets),), dtype=np.float32)\n    oh[idx] = 1\n    \n    return oh","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2755927ba88262427347c567ae777c92510ffa68"},"cell_type":"markdown","source":" ##  Position Features"},{"metadata":{"_uuid":"15165f2b90503a90de589b37558ebcea28daa867"},"cell_type":"markdown","source":"Encode the absolute positions in the sentence and the relative position between the pronoun and the entities."},{"metadata":{"trusted":true,"_uuid":"c1c14436f57e74e80ac4425f77df98768c71f7b9"},"cell_type":"code","source":"num_pos_features = 45","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f3d21e9a626a3290c81c6bc98b817c3e47275be","trusted":true},"cell_type":"code","source":"def extrac_positional_features(text, char_offset1, char_offset2):\n    doc = nlp(text)\n    max_len = 64\n    \n    # char offset to token offset\n    lens = [token.idx for token in doc]\n    mention_offset1 = bs(lens, char_offset1) - 1\n    mention_offset2 = bs(lens, char_offset2) - 1\n    \n    # token offset to sentence offset\n    lens = [len(sent) for sent in doc.sents]\n    acc_lens = [len_ for len_ in lens]\n    pre_len = 0\n    for i in range(0, len(acc_lens)):\n        pre_len += acc_lens[i]\n        acc_lens[i] = pre_len\n    sent_index1 = bs(acc_lens, mention_offset1)\n    sent_index2 = bs(acc_lens, mention_offset2)\n    \n    sent1 = list(doc.sents)[sent_index1]\n    sent2 = list(doc.sents)[sent_index2]\n    \n    # buckets\n    bucket_dist = [1, 2, 3, 4, 5, 8, 16, 32, 64]\n    \n    # relative distance\n    dist = mention_offset2 - mention_offset1\n    dist_oh = ohe_dist(dist, bucket_dist)\n    \n    # buckets\n    bucket_pos = [0, 1, 2, 3, 4, 5, 8, 16, 32]\n    \n    # absolute position in the sentence\n    sent_pos1 = mention_offset1 + 1\n    if sent_index1 > 0:\n        sent_pos1 = mention_offset1 - acc_lens[sent_index1-1]\n    sent_pos_oh1 = ohe_dist(sent_pos1, bucket_pos)\n    sent_pos_inv1 = len(sent1) - sent_pos1\n    assert sent_pos_inv1 >= 0\n    sent_pos_inv_oh1 = ohe_dist(sent_pos_inv1, bucket_pos)\n    \n    sent_pos2 = mention_offset2 + 1\n    if sent_index2 > 0:\n        sent_pos2 = mention_offset2 - acc_lens[sent_index2-1]\n    sent_pos_oh2 = ohe_dist(sent_pos2, bucket_pos)\n    sent_pos_inv2 = len(sent2) - sent_pos2\n    if sent_pos_inv2 < 0:\n        print(sent_pos_inv2)\n        print(len(sent2))\n        print(sent_pos2)\n        raise ValueError\n    sent_pos_inv_oh2 = ohe_dist(sent_pos_inv2, bucket_pos)\n    \n    sent_pos_ratio1 = sent_pos1 / len(sent1)\n    sent_pos_ratio2 = sent_pos2 / len(sent2)\n    \n    return dist_oh, sent_pos_oh1, sent_pos_oh2, sent_pos_inv_oh1, sent_pos_inv_oh2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a9ec346bff260b62ebea6d9223c6a11e683a9ee"},"cell_type":"code","source":"def create_dist_features(df, text_column, pronoun_offset_column, name_offset_column):\n    text_offset_list = df[[text_column, pronoun_offset_column, name_offset_column]].values.tolist()\n    num_features = num_pos_features\n    \n    pos_feature_matrix = np.zeros(shape=(len(text_offset_list), num_features))\n    for text_offset_index in range(len(text_offset_list)):\n        text_offset = text_offset_list[text_offset_index]\n        dist_oh, sent_pos_oh1, sent_pos_oh2, sent_pos_inv_oh1, sent_pos_inv_oh2 = extrac_positional_features(text_offset[0], text_offset[1],text_offset[2])\n        \n        feature_index = 0\n        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(dist_oh)] = np.asarray(dist_oh)\n        feature_index += len(dist_oh)\n        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_oh1)] = np.asarray(sent_pos_oh1)\n        feature_index += len(sent_pos_oh1)\n        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_oh2)] = np.asarray(sent_pos_oh2)\n        feature_index += len(sent_pos_oh2)\n        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_inv_oh1)] = np.asarray(sent_pos_inv_oh1)\n        feature_index += len(sent_pos_inv_oh1)\n        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_inv_oh2)] = np.asarray(sent_pos_inv_oh2)\n        feature_index += len(sent_pos_inv_oh2)\n    \n    return pos_feature_matrix","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"488aaec745f7f6eea342a5746bcf7bef206c60d3"},"cell_type":"markdown","source":"## Extract Sentences"},{"metadata":{"_uuid":"51de0d6b4223279969593956248e0fb99284c5d7"},"cell_type":"markdown","source":"Select the surrounding 100 words around the mention in the sentence."},{"metadata":{"trusted":true,"_uuid":"2edcfef36582c91246e7ab772d5a23c27a272f92"},"cell_type":"code","source":"max_len = 50 # longer than 99% of the sentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11af138a12f54e9c0a8ce92a782383c9ad5a92ff"},"cell_type":"code","source":"seq_list = list()\ndef extract_sents(text, char_offset_p, char_offset_a, char_offset_b, id):\n    global max_len\n    global seq_list\n\n    seq_list.append(list())\n    \n    doc = nlp(text)\n    token_lens = [token.idx for token in doc]\n    \n    char_offsets = [char_offset_p, char_offset_a, char_offset_b]\n    sent_list = list()\n    \n    for char_offset in char_offsets:\n        # char offset to token offset\n        mention_offset = bs(token_lens, char_offset) - 1\n        # mention_word\n        mention = doc[mention_offset]\n        # token offset to sentence offset\n        lens = [len(sent) for sent in doc.sents]\n        acc_lens = [len_ for len_ in lens]\n        pre_len = 0\n        for i in range(0, len(acc_lens)):\n            pre_len += acc_lens[i]\n            acc_lens[i] = pre_len\n        sent_index = bs(acc_lens, mention_offset)\n        # mention sentence\n        sent = list(doc.sents)[sent_index]\n        \n        # absolute position in the sentence\n        sent_pos = mention_offset + 1\n        if sent_index > 0:\n            sent_pos = mention_offset - acc_lens[sent_index-1]\n        \n        # clip the sentence if it is longer than max length\n        if len(sent) > max_len:\n            # make sure the mention is in the sentence span\n            if sent_pos < max_len-1:\n                sent_list.append(sent[0:max_len].text)\n                sent_list.append(sent_pos)\n                seq_list[-1].append(sent[0:max_len])\n            else:\n                sent_list.append(sent[sent_pos-max_len+2 : min(sent_pos+2, len(sent))].text)\n                sent_list.append(max_len-2)\n                seq_list[-1].append(sent[sent_pos-max_len+2 : min(sent_pos+2, len(sent))])\n        else:\n            sent_list.append(sent.text)\n            sent_list.append(sent_pos)\n            seq_list[-1].append(sent)\n        \n    return pd.Series([id] + sent_list, index=['ID', 'Pronoun-Sent', 'Pronoun-Sent-Offset', 'A-Sent', 'A-Sent-Offset', 'B-Sent', 'B-Sent-Offset'])\n\ndef add_sent_columns(df, text_column, pronoun_offset_column, a_offset_column, b_offset_column):\n    global seq_list\n    seq_list = list()\n    sent_df = df.apply(lambda row: extract_sents(row.loc[text_column], row[pronoun_offset_column], row[a_offset_column], row[b_offset_column], row['ID']), axis=1)\n    df = df.join(sent_df.set_index('ID'), on='ID')\n    return df, seq_list","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c0630e230a389ab7ba9bc4bc8140cabf769f5ff"},"cell_type":"markdown","source":"## Create Train, Dev and Test Data"},{"metadata":{"trusted":true,"_uuid":"f455cc1f3150d1dcc42a3aef2434c8d5e703c990"},"cell_type":"code","source":"seq_list = list()\ntrain_df, train_tokenized = add_sent_columns(train_df, 'Text', 'Pronoun-offset', 'A-offset', 'B-offset')\nseq_list = list()\ntest_df, test_tokenized = add_sent_columns(test_df, 'Text', 'Pronoun-offset', 'A-offset', 'B-offset')\nseq_list = list()\ndev_df, dev_tokenized = add_sent_columns(dev_df, 'Text', 'Pronoun-offset', 'A-offset', 'B-offset')\nseq_list = list()\nsub_test_df, sub_test_tokenized = add_sent_columns(sub_test_df, 'Text', 'Pronoun-offset', 'A-offset', 'B-offset')\n\n# df apply will call the first row twice, remove the first one\ntrain_tokenized = train_tokenized[1:]\ntest_tokenized = test_tokenized[1:]\ndev_tokenized = dev_tokenized[1:]\nsub_test_tokenized = sub_test_tokenized[1:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2873a775c7be71521ef5b8922a61f11a93e62b57"},"cell_type":"markdown","source":"### Create Vocab and Embedding Matrix"},{"metadata":{"trusted":true,"_uuid":"2d34b3f5fdce49c97e9fb9d153126546465b3965"},"cell_type":"code","source":"embed_size = 300\nmax_features = 80000\n\n# generate word index\nword_index = dict()\nidx = 1\nfor text_ in train_tokenized+test_tokenized+dev_tokenized+sub_test_tokenized:\n    for sent_ in text_:\n        for word_ in sent_:\n            if word_.text not in word_index and word_.text in embeddings_index:\n                word_index[word_.text] = idx\n                idx += 1\n\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words + 1, embed_size))\n        \nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = None\n    if word in embeddings_index:\n        embedding_vector = embeddings_index[word]\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n        \nprint(embedding_matrix.shape)\n\n# generate pos tag index\npos_index = dict()\nidx = 1\nfor text_ in train_tokenized+test_tokenized+dev_tokenized+sub_test_tokenized:\n    for sent_ in text_:\n        for word_ in sent_:\n            if word_.pos not in pos_index:\n                pos_index[word_.pos] = idx\n                idx += 1\n\ndef sentences_to_sequences(tokenized_):\n    return list(map(\n        lambda sent_tokenized: list(map(\n            lambda token_: word_index[token_.text] if token_.text in word_index else 0,\n            sent_tokenized\n        )),\n        tokenized_\n    ))\n\ndef poses_to_sequences(tokenized_):\n    return list(map(\n        lambda sent_tokenized: list(map(\n            lambda token_: pos_index[token_.pos] if token_.pos in pos_index else 0,\n            sent_tokenized\n        )),\n        tokenized_\n    ))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41ca3b1434037a8f7296850ba4af9c837e247368"},"cell_type":"code","source":"import seaborn as sns\n\nsns.distplot(train_df['Pronoun-Sent'].map(lambda ele: len(ele.split(\" \"))), kde_kws={\"label\": \"P\"})\n\nsns.distplot(train_df['A-Sent'].map(lambda ele: len(ele.split(\" \"))), kde_kws={\"label\": \"A\"})\n\nsns.distplot(train_df['B-Sent'].map(lambda ele: len(ele.split(\" \"))), kde_kws={\"label\": \"B\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35dda1595e80bed9e310dd5af9b89b726a878e8b"},"cell_type":"code","source":"train_p_tokenized = sentences_to_sequences([row[0] for row in train_tokenized])\ntrain_a_tokenized = sentences_to_sequences([row[1] for row in train_tokenized])\ntrain_b_tokenized = sentences_to_sequences([row[2] for row in train_tokenized])\n\ntest_p_tokenized = sentences_to_sequences([row[0] for row in test_tokenized])\ntest_a_tokenized = sentences_to_sequences([row[1] for row in test_tokenized])\ntest_b_tokenized = sentences_to_sequences([row[2] for row in test_tokenized])\n\nsub_test_p_tokenized = sentences_to_sequences([row[0] for row in sub_test_tokenized])\nsub_test_a_tokenized = sentences_to_sequences([row[1] for row in sub_test_tokenized])\nsub_test_b_tokenized = sentences_to_sequences([row[2] for row in sub_test_tokenized])\n\ndev_p_tokenized = sentences_to_sequences([row[0] for row in dev_tokenized])\ndev_a_tokenized = sentences_to_sequences([row[1] for row in dev_tokenized])\ndev_b_tokenized = sentences_to_sequences([row[2] for row in dev_tokenized])\n\nseq_p_train = sequence.pad_sequences(train_p_tokenized, maxlen = max_len, padding='post')\nseq_a_train = sequence.pad_sequences(train_a_tokenized, maxlen = max_len, padding='post')\nseq_b_train = sequence.pad_sequences(train_b_tokenized, maxlen = max_len, padding='post')\n\nseq_p_test = sequence.pad_sequences(test_p_tokenized, maxlen = max_len, padding='post')\nseq_a_test = sequence.pad_sequences(test_a_tokenized, maxlen = max_len, padding='post')\nseq_b_test = sequence.pad_sequences(test_b_tokenized, maxlen = max_len, padding='post')\n\nseq_p_sub_test = sequence.pad_sequences(sub_test_p_tokenized, maxlen = max_len, padding='post')\nseq_a_sub_test = sequence.pad_sequences(sub_test_a_tokenized, maxlen = max_len, padding='post')\nseq_b_sub_test = sequence.pad_sequences(sub_test_b_tokenized, maxlen = max_len, padding='post')\n\nseq_p_dev = sequence.pad_sequences(dev_p_tokenized, maxlen = max_len, padding='post')\nseq_a_dev = sequence.pad_sequences(dev_a_tokenized, maxlen = max_len, padding='post')\nseq_b_dev = sequence.pad_sequences(dev_b_tokenized, maxlen = max_len, padding='post')\n\ntrain_p_pos = poses_to_sequences([row[0] for row in train_tokenized])\ntrain_a_pos = poses_to_sequences([row[1] for row in train_tokenized])\ntrain_b_pos = poses_to_sequences([row[2] for row in train_tokenized])\n\ntest_p_pos = poses_to_sequences([row[0] for row in test_tokenized])\ntest_a_pos = poses_to_sequences([row[1] for row in test_tokenized])\ntest_b_pos = poses_to_sequences([row[2] for row in test_tokenized])\n\nsub_test_p_pos = poses_to_sequences([row[0] for row in sub_test_tokenized])\nsub_test_a_pos = poses_to_sequences([row[1] for row in sub_test_tokenized])\nsub_test_b_pos = poses_to_sequences([row[2] for row in sub_test_tokenized])\n\ndev_p_pos = poses_to_sequences([row[0] for row in dev_tokenized])\ndev_a_pos = poses_to_sequences([row[1] for row in dev_tokenized])\ndev_b_pos = poses_to_sequences([row[2] for row in dev_tokenized])\n\npos_p_train = sequence.pad_sequences(train_p_pos, maxlen = max_len, padding='post')\npos_a_train = sequence.pad_sequences(train_a_pos, maxlen = max_len, padding='post')\npos_b_train = sequence.pad_sequences(train_b_pos, maxlen = max_len, padding='post')\n\npos_p_test = sequence.pad_sequences(test_p_pos, maxlen = max_len, padding='post')\npos_a_test = sequence.pad_sequences(test_a_pos, maxlen = max_len, padding='post')\npos_b_test = sequence.pad_sequences(test_b_pos, maxlen = max_len, padding='post')\n\npos_p_sub_test = sequence.pad_sequences(sub_test_p_pos, maxlen = max_len, padding='post')\npos_a_sub_test = sequence.pad_sequences(sub_test_a_pos, maxlen = max_len, padding='post')\npos_b_sub_test = sequence.pad_sequences(sub_test_b_pos, maxlen = max_len, padding='post')\n\npos_p_dev = sequence.pad_sequences(dev_p_pos, maxlen = max_len, padding='post')\npos_a_dev = sequence.pad_sequences(dev_a_pos, maxlen = max_len, padding='post')\npos_b_dev = sequence.pad_sequences(dev_b_pos, maxlen = max_len, padding='post')\n\nindex_p_train = train_df['Pronoun-Sent-Offset'].values\nindex_a_train = train_df['A-Sent-Offset'].values\nindex_b_train = train_df['B-Sent-Offset'].values\n\nindex_p_test = test_df['Pronoun-Sent-Offset'].values\nindex_a_test = test_df['A-Sent-Offset'].values\nindex_b_test = test_df['B-Sent-Offset'].values\n\nindex_p_sub_test = sub_test_df['Pronoun-Sent-Offset'].values\nindex_a_sub_test = sub_test_df['A-Sent-Offset'].values\nindex_b_sub_test = sub_test_df['B-Sent-Offset'].values\n\nindex_p_dev = dev_df['Pronoun-Sent-Offset'].values\nindex_a_dev = dev_df['A-Sent-Offset'].values\nindex_b_dev = dev_df['B-Sent-Offset'].values\n\npa_pos_tra = create_dist_features(train_df, 'Text', 'Pronoun-offset', 'A-offset')\npa_pos_dev = create_dist_features(dev_df, 'Text', 'Pronoun-offset', 'A-offset')\npa_pos_test = create_dist_features(test_df, 'Text', 'Pronoun-offset', 'A-offset')\npa_pos_sub_test = create_dist_features(sub_test_df, 'Text', 'Pronoun-offset', 'A-offset')\n\npb_pos_tra = create_dist_features(train_df, 'Text', 'Pronoun-offset', 'B-offset')\npb_pos_dev = create_dist_features(dev_df, 'Text', 'Pronoun-offset', 'B-offset')\npb_pos_test = create_dist_features(test_df, 'Text', 'Pronoun-offset', 'B-offset')\npb_pos_sub_test = create_dist_features(sub_test_df, 'Text', 'Pronoun-offset', 'B-offset')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f46a80420d56262e4fed998d16f9f9105d502c02"},"cell_type":"code","source":"X_train = [seq_p_train, seq_a_train, seq_b_train, pos_p_train, pos_a_train, pos_b_train, index_p_train, index_a_train, index_b_train, pa_pos_tra, pb_pos_tra]\nX_dev = [seq_p_dev, seq_a_dev, seq_b_dev, pos_p_dev, pos_a_dev, pos_b_dev, index_p_dev, index_a_dev, index_b_dev, pa_pos_dev, pb_pos_dev]\nX_test = [seq_p_test, seq_a_test, seq_b_test, pos_p_test, pos_a_test, pos_b_test, index_p_test, index_a_test, index_b_test, pa_pos_test, pb_pos_test]\nX_sub_test = [seq_p_sub_test, seq_a_sub_test, seq_b_sub_test, pos_p_sub_test, pos_a_sub_test, pos_b_sub_test, index_p_sub_test, index_a_sub_test, index_b_sub_test, pa_pos_sub_test, pb_pos_sub_test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2842410a113a578c1c274630b195038af0dfd75c"},"cell_type":"code","source":"def _row_to_y(row):\n    if row.loc['A-coref']:\n        return 0\n    if row.loc['B-coref']:\n        return 1\n    return 2\n\ny_tra = train_df.apply(_row_to_y, axis=1)\ny_dev = dev_df.apply(_row_to_y, axis=1)\ny_test = test_df.apply(_row_to_y, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33ee19bc3e355ea58dd27bd5c373a13682536dd4"},"cell_type":"markdown","source":"# Define Keras Layers"},{"metadata":{"trusted":true,"_uuid":"cb3cacc8e7dc332a6b5cacf3d733f314b7f03cba"},"cell_type":"code","source":"import numpy as np\nfrom keras import backend\nfrom keras import layers\nfrom keras import models\n\nfrom keras import initializers, regularizers, constraints, activations\nfrom keras.engine import Layer\nimport keras.backend as K\nfrom keras.layers import merge\n\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3830a23390eaf527992b3676e03857699969957e"},"cell_type":"code","source":"def _dot_product(x, kernel):\n    \"\"\"\n    Wrapper for dot product operation, in order to be compatible with both\n    Theano and Tensorflow\n    Args:\n        x (): input\n        kernel (): weights\n    Returns:\n    \"\"\"\n    if K.backend() == 'tensorflow':\n        # todo: check that this is correct\n        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n    else:\n        return K.dot(x, kernel)\n    \n    \nclass AttentionWeight(Layer):\n    \"\"\"\n        This code is a modified version of cbaziotis implementation:  GithubGist cbaziotis/AttentionWithContext.py\n        Attention operation, with a context/query vector, for temporal data.\n        Supports Masking.\n        Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n        \"Hierarchical Attention Networks for Document Classification\"\n        by using a context vector to assist the attention\n        # Input shape\n            3D tensor with shape: `(samples, steps, features)`.\n        # Output shape\n            2D tensor with shape: `(samples, steps)`.\n        :param kwargs:\n        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n        The dimensions are inferred based on the output shape of the RNN.\n        Example:\n            model.add(LSTM(64, return_sequences=True))\n            model.add(AttentionWeight())\n        \"\"\"\n\n    def __init__(self,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        super(AttentionWeight, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        shape1 = input_shape[0]\n        shape2 = input_shape[1]\n\n        self.W = self.add_weight((shape2[-1], shape1[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        if self.bias:\n            self.b = self.add_weight((shape2[-1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, inputs, mask=None):\n        x = inputs[0]\n        u = inputs[1]\n        \n        uit = _dot_product(x, self.W)\n\n        if self.bias:\n            uit += self.b\n\n        uit = K.tanh(uit)\n        ait = K.batch_dot(uit, u)\n\n        a = K.exp(ait)\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n\n        # in some cases especially in the early stages of training the sum may be almost zero\n        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        \n        return a\n\n    def compute_output_shape(self, input_shape):\n        if not isinstance(input_shape, list) or len(input_shape) != 2:\n            raise ValueError('A `Dot` layer should be called '\n                             'on a list of 2 inputs.')\n        shape1 = list(input_shape[0])\n        shape2 = list(input_shape[1])\n        \n        return shape1[0], shape1[1]\n\n    def get_config(self):\n        config = {\n            'W_regularizer': regularizers.serialize(self.W_regularizer),\n            'b_regularizer': regularizers.serialize(self.b_regularizer),\n            'W_constraint': constraints.serialize(self.W_constraint),\n            'b_constraint': constraints.serialize(self.b_constraint),\n            'bias': self.bias\n        }\n        base_config = super(AttentionWeight, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n    \n    \nclass FeatureSelection1D(Layer):\n    \"\"\"\n        Normalize feature along a specific axis.\n        Supports Masking.\n\n        # Input shape\n            A ND tensor with shape: `(samples, timesteps, features)\n            A 2D tensor with shape: [samples, num_selected_features]\n        # Output shape\n            ND tensor with shape: `(samples, num_selected_features, features)`.\n        :param kwargs:\n        \"\"\"\n\n    def __init__(self, num_selects, **kwargs):\n\n        self.num_selects = num_selects\n        self.supports_masking = True\n        super(FeatureSelection1D, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n\n        super(FeatureSelection1D, self).build(input_shape)\n\n    def compute_mask(self, input, input_mask=None):\n        # don't pass the mask to the next layers\n        return None\n\n    def call(self, inputs, mask=None):\n        if not isinstance(inputs, list) or len(inputs) != 2:\n            raise ValueError('FeatureSelection1D layer should be called '\n                             'on a list of 2 inputs.')\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a = K.cast(mask, K.floatx()) * inputs[0]\n        else:\n            a = inputs[0]    # this is the features of the sentence (50, 100)\n\n        b = inputs[1]        # this is the index of the mention(p,a,b)\n\n        a = tf.batch_gather(a, b)   # this is the feature of the mention(p,a,b) from the input features\n\n        return a\n\n    def compute_output_shape(self, input_shape):\n        if not isinstance(input_shape, list) or len(input_shape) != 2:\n            raise ValueError('A `FeatureSelection1D` layer should be called '\n                             'on a list of 2 inputs.')\n        shape1 = list(input_shape[0])\n        shape2 = list(input_shape[1])\n\n        if shape2[0] != shape1[0]:\n            raise ValueError(\"batch size must be same\")\n\n        if shape2[1] != self.num_selects:\n            raise ValueError(\"must conform to the num_select\")\n\n        return (shape1[0], self.num_selects, shape1[2])\n\n    def get_config(self):\n        config = {\n            'num_selects': self.num_selects\n        }\n        base_config = super(FeatureSelection1D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf87bf600dac037777be8e3dda560ec65b6640a2"},"cell_type":"markdown","source":"# Build and Train Model"},{"metadata":{"_uuid":"03e8297383ee57a4a23c707a4a6c1cca13da1868","trusted":true},"cell_type":"code","source":"from keras import callbacks as kc\nfrom keras import optimizers as ko\nfrom keras import initializers, regularizers, constraints\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import SVG\n\nhistories = list()\nfile_paths = list()\ncos = list()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5503bd66a2a7fea5bcb093c775d69baded3e759d"},"cell_type":"markdown","source":"## End-to-End RNN Attention Model"},{"metadata":{"_uuid":"5a954b42159bc4a8a17b4e15083f026f53362a43"},"cell_type":"markdown","source":"### Define Model"},{"metadata":{"trusted":true,"_uuid":"8feec4878e5b817dcbf5ed7f49d9b78cce36ebd6"},"cell_type":"code","source":"def build_e2e_birnn_attention_model(\n        voca_dim, time_steps, pos_tag_size, pos_tag_dim, extra_feature_dims, output_dim, rnn_dim, model_dim, mlp_dim,\n        item_embedding=None, rnn_depth=1, mlp_depth=1,\n        drop_out=0.5, rnn_drop_out=0., rnn_state_drop_out=0.,\n        trainable_embedding=False, gpu=False, return_customized_layers=False):\n    \"\"\"\n    Create A End-to-End Bidirectional RNN Attention Model.\n\n    :param voca_dim: vocabulary dimension size.\n    :param time_steps: the length of input\n    :param extra_feature_dims: the dimention size of the auxilary feature\n    :param output_dim: the output dimension size\n    :param model_dim: rrn dimension size\n    :param mlp_dim: the dimension size of fully connected layer\n    :param item_embedding: integer, numpy 2D array, or None (default=None)\n        If item_embedding is a integer, connect a randomly initialized embedding matrix to the input tensor.\n        If item_embedding is a matrix, this matrix will be used as the embedding matrix.\n        If item_embedding is None, then connect input tensor to RNN layer directly.\n    :param rnn_depth: rnn depth\n    :param mlp_depth: the depth of fully connected layers\n    :param drop_out: dropout rate of fully connected layers\n    :param rnn_drop_out: dropout rate of rnn layers\n    :param rnn_state_drop_out: dropout rate of rnn state tensor\n    :param trainable_embedding: boolean\n    :param gpu: boolean, default=False\n        If True, CuDNNLSTM is used instead of LSTM for RNN layer.\n    :param return_customized_layers: boolean, default=False\n        If True, return model and customized object dictionary, otherwise return model only\n    :return: keras model\n    \"\"\"\n    \n    # sequences inputs\n    if item_embedding is not None:\n        inputp = models.Input(shape=(time_steps,), dtype='int32', name='inputp') # the sentence containing p\n        inputa = models.Input(shape=(time_steps,), dtype='int32', name='inputa') # the sentence containing a\n        inputb = models.Input(shape=(time_steps,), dtype='int32', name='inputb') # the sentence containing b\n        inputs = [inputp, inputa, inputb]\n        \n        if isinstance(item_embedding, np.ndarray):\n            assert voca_dim == item_embedding.shape[0]\n            embed_dim = item_embedding.shape[1]\n            emb_layer = layers.Embedding(\n                voca_dim, item_embedding.shape[1], input_length=time_steps,\n                weights=[item_embedding, ], trainable=trainable_embedding,\n                mask_zero=False, name='embedding_layer0'\n            )\n        elif utils.is_integer(item_embedding):\n            embed_dim = item_embedding\n            emb_layer = layers.Embedding(\n                voca_dim, item_embedding, input_length=time_steps,\n                trainable=trainable_embedding,\n                mask_zero=False, name='embedding_layer0'\n            )\n        else:\n            raise ValueError(\"item_embedding must be either integer or numpy matrix\")\n\n        xs = list(map(\n            lambda input_: emb_layer(input_),\n            inputs\n        ))\n    else:\n        inputp = models.Input(shape=(time_steps, voca_dim), dtype='float32', name='inputp')\n        inputa = models.Input(shape=(time_steps, voca_dim), dtype='float32', name='inputa')\n        inputb = models.Input(shape=(time_steps, voca_dim), dtype='float32', name='inputb')\n        embed_dim = voca_dim\n        xs = [inputp, inputa, inputb]\n        \n    # pos tag # the pos of words in sentence containing p\n    inputposp = models.Input(shape=(time_steps,), dtype='int32', name='inputposp') \n    inputposa = models.Input(shape=(time_steps,), dtype='int32', name='inputposa')\n    inputposb = models.Input(shape=(time_steps,), dtype='int32', name='inputposb')\n\n    inputpos = [inputposp, inputposa, inputposb]\n    pos_emb_layer = layers.Embedding(\n        pos_tag_size, pos_tag_dim, input_length=time_steps,\n        trainable=True, mask_zero=False, name='pos_embedding_layer0'\n    )\n    xpos = list(map(\n        lambda input_: pos_emb_layer(input_),\n        inputpos\n    ))\n    \n    embed_concate_layer = layers.Concatenate(axis=2, name=\"embed_concate_layer\")  # concatenate 300D'w_em'+5D'pos_em'\n    for i in range(len(xs)):\n        xs[i] = embed_concate_layer([xs[i], xpos[i]])\n    \n    # mention position in the sentence\n    inputpi = models.Input(shape=(1,), dtype='int32', name='inputpi') # position of p in the sentence\n    inputai = models.Input(shape=(1,), dtype='int32', name='inputai')\n    inputbi = models.Input(shape=(1,), dtype='int32', name='inputbi')\n    xis = [inputpi, inputai, inputbi]\n    \n    # addtional mention-pair features\n#     inputpa = models.Input(shape=(extra_feature_dims,), dtype='float32', name='inputpa') # distance between p and a\n#     inputpb = models.Input(shape=(extra_feature_dims,), dtype='float32', name='inputpb')\n#     xextrs = [inputpa, inputpb]\n    \n    # rnn\n    birnns = list()\n    rnn_batchnorms = list()\n    rnn_dropouts = list()\n    if gpu:\n        # rnn encoding\n        for i in range(rnn_depth):\n            rnn_dropout = layers.SpatialDropout1D(rnn_drop_out)\n            birnn = layers.Bidirectional(\n                layers.CuDNNGRU(rnn_dim, return_sequences=True),\n                name='bi_lstm_layer' + str(i))\n            rnn_batchnorm = layers.BatchNormalization(name='rnn_batch_norm_layer' + str(i))\n            \n            birnns.append(birnn)\n            rnn_dropouts.append(rnn_dropout)\n            rnn_batchnorms.append(rnn_batchnorm)\n        \n        xs_ = list()\n        for x_ in xs:\n            for i in range(len(birnns)):\n                x_ = rnn_dropouts[i](x_)\n                x_ = birnns[i](x_)\n                x_ = rnn_batchnorms[i](x_)\n            xs_.append(x_)\n        xs = xs_\n    else:\n        # rnn encoding\n        for i in range(rnn_depth):\n            birnn = layers.Bidirectional(\n                layers.GRU(rnn_dim, return_sequences=True, dropout=rnn_drop_out,\n                            recurrent_dropout=rnn_state_drop_out),\n                name='bi_lstm_layer' + str(i))\n            rnn_batchnorm = layers.BatchNormalization(name='rnn_batch_norm_layer' + str(i))\n            \n            birnns.append(birnn)\n            rnn_batchnorms.append(rnn_batchnorm)\n            \n        xs_ = list()\n        for x_ in xs:\n            for i in range(len(birnns)):\n                x_ = birnns[i](x_)\n                x_ = rnn_batchnorms[i](x_)\n            xs_.append(x_)\n        xs = xs_\n    \n    # attention aggregated rnn embedding + mention rnn embedding + mention-pair features\n    select_layer = FeatureSelection1D(1, name='boundary_selection_layer')\n    flatten_layer1 = layers.Flatten('channels_first', name=\"flatten_layer1\")\n    permute_layer = layers.Permute((2, 1), name='permuted_attention_x')\n    attent_weight = AttentionWeight(name=\"attention_weight\")\n    focus_layer = layers.Dot([2, 1], name='focus' + '_layer')\n    reshape_layer = layers.Reshape((1, rnn_dim*2), name=\"reshape_layer\")\n    concate_layer = layers.Concatenate(axis=1, name=\"attention_concate_layer\")\n    atten_dropout_layer = layers.Dropout(drop_out, name='attention_dropout_layer')\n    map_layer1 = layers.Dense(model_dim, activation=\"relu\", name=\"map_layer1\")\n    #map_layer2 = layers.TimeDistributed(layers.Dense(model_dim, activation=\"relu\"), name=\"map_layer2\")\n    map_layer2 = map_layer1\n    flatten_layer = layers.Flatten('channels_first', name=\"flatten_layer\")\n    for i in range(len(xs)):\n        if i == 0:\n            map_layer = map_layer1\n        else:\n            map_layer = map_layer2\n            \n        select_ = select_layer([xs[i], xis[i]])\n        flatten_select_ = flatten_layer1(select_)\n        att = attent_weight([xs[i], flatten_select_])\n        \n        focus = focus_layer([permute_layer(xs[i]), att])\n        xs[i] = concate_layer([select_, reshape_layer(focus)])\n        xs[i] = flatten_layer(xs[i])\n        xs[i] = atten_dropout_layer(xs[i])\n        xs[i] = map_layer(xs[i])\n    \n#     feature_dropout_layer = layers.Dropout(rate=drop_out, name=\"feature_dropout_layer\")\n#     feature_map_layer = layers.Dense(model_dim, activation=\"relu\",name=\"feature_map_layer\")\n#     xextrs = [feature_map_layer(feature_dropout_layer(xextr)) for xextr in xextrs]\n    \n    x = layers.Concatenate(axis=1, name=\"concat_feature_layer\")(xs )#+ xextrs)\n    x = layers.Dropout(drop_out, name='dropout_layer')(x)\n\n    # MLP Layers\n    for i in range(mlp_depth - 1):\n        x = layers.Dense(mlp_dim, activation='selu', kernel_initializer='lecun_normal', name='selu_layer' + str(i))(x)\n        x = layers.AlphaDropout(drop_out, name='alpha_layer' + str(i))(x)\n\n    outputs = layers.Dense(output_dim, activation=\"softmax\", name=\"softmax_layer0\")(x)\n\n    model = models.Model([inputp, inputa, inputb] + inputpos + xis '''+ [inputpa, inputpb]''', outputs)\n\n    if return_customized_layers:\n        return model, {'FeatureSelection1D': FeatureSelection1D, 'AttentionWeight': AttentionWeight}\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f134f020b4c1c7f9e8ce4cfc801b015d9090e249"},"cell_type":"markdown","source":"### Build Model"},{"metadata":{"trusted":true,"_uuid":"7c0e4a38b4fa5e55811bdb4672dc5a596e70c433"},"cell_type":"code","source":"voca_dim = embedding_matrix.shape[0]\npos_tag_size = len(pos_index)\ntime_steps = max_len\n\nembed_dim = embedding_matrix.shape[1]\npos_tag_dim = 5\nextra_feature_dims = num_pos_features\noutput_dim = 3\nrnn_dim = 50\nmodel_dim = 10\nmlp_dim = 10\nrnn_depth = 1\nmlp_depth=1\ndrop_out=0.2\nrnn_drop_out=0.5\ngpu = True\nreturn_customized_layers=True\n\nmodel, co = build_e2e_birnn_attention_model(\n        voca_dim, time_steps, pos_tag_size, pos_tag_dim, extra_feature_dims, output_dim, rnn_dim, model_dim, mlp_dim,\n        item_embedding=embedding_matrix, rnn_depth=rnn_depth, mlp_depth=mlp_depth,\n        drop_out=drop_out, rnn_drop_out=rnn_drop_out, rnn_state_drop_out=rnn_drop_out,\n        trainable_embedding=False, gpu=gpu, return_customized_layers=return_customized_layers)\ncos.append(co)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f56526943859c2cd1c3738e043059a4951dd92f6"},"cell_type":"code","source":"print(model.summary())\nfrom keras.utils import plot_model\nplot_model(model, to_file='model.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nimg = Image.open('model.png')\ndisplay(img)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c926e888dce8ff928430dc76d4771cb67aea2a6"},"cell_type":"markdown","source":"### Train Model"},{"metadata":{"trusted":true,"_uuid":"02490fd68b2c304432f68d8c356b3013084d8d3d"},"cell_type":"code","source":"adam = ko.Nadam(clipnorm=1.0)\nmodel.compile(adam, loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n\nfile_path = \"best_e2e_rnn_atten_model.hdf5\"\ncheck_point = kc.ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")\n# early_stop = kc.EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience=5)\nhistory = model.fit(X_train, y_tra, batch_size=30, epochs=40, validation_data=(X_dev, y_dev), callbacks = [check_point])\n\nfile_paths.append(file_path)\nhistories.append(np.min(np.asarray(history.history['val_loss'])))\n\ndel model, history\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bb147615d4920a2f3500a0fba12d31cb0869304"},"cell_type":"markdown","source":"###  Make Prediction"},{"metadata":{"trusted":true,"_uuid":"79cb8d339da04ad2d52e3675a4295f55a6ccd148"},"cell_type":"code","source":"print(\"load best model: \" + str(file_paths[np.argmin(histories)]))\nmodel = models.load_model(\n    file_paths[np.argmin(histories)], cos[np.argmin(histories)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, score = model.evaluate(x=X_test, y=y_test)\nprint('model evalutation loss = {} and score = {}'.format(loss, score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true,"_uuid":"4c465342c3a99a4996a104fdb00ceab4e85b9649"},"cell_type":"code","source":"y_sub_preds = model.predict(X_sub_test, batch_size = 1024, verbose = 1)\n\nsub_df_path = os.path.join(SUB_DATA_FOLDER, 'sample_submission_stage_2.csv')\nsub_df = pd.read_csv(sub_df_path)\nsub_df.loc[:, 'A'] = pd.Series(y_sub_preds[:, 0])\nsub_df.loc[:, 'B'] = pd.Series(y_sub_preds[:, 1])\nsub_df.loc[:, 'NEITHER'] = pd.Series(y_sub_preds[:, 2])\n\nsub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6cfa1933801af3de8d5d6cfc1cd9c3e23481e0bd"},"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}