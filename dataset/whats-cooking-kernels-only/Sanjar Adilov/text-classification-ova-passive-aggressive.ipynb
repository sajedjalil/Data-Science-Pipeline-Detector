{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Text Classification with Passive-Aggressive Algorithms\n\n*Passive-Aggressive algorithms* (PA) [[1]](http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf)\nare a family of margin based online learning algorithms with regularization and no learning rate.\nIn this notebook, we seek to evaluate PA classifiers on\n[What's Cooking?](https://www.kaggle.com/c/whats-cooking-kernels-only/overview)\nmulti-class classification data. We estimate multiple binary PA classifiers in a one-versus-all (OvA) setting.\n\n---\n\nSuppose $X = \\mathbb{R}^n$ is a set of instances and\n$Y = \\{c_1, c_2, \\dots, c_k\\}$ is a set of labels. Our classification function is defined by\n\n$$ a(x) = sign(\\langle w, x \\rangle) \\text{, } w \\in \\mathbb{R}^n $$\n\nThe task is to learn the weight vector $w$ updating it incrementally by *hinge-loss*:\n\n$$\nl(w; (x, y)) =\n\\begin{cases}\n    0, & \\text{if } y \\langle w, x \\rangle \\geq 1; \\\\\n    1 - y \\langle w, x \\rangle, & \\text{otherwise.}\n\\end{cases}\n$$\n\nThe update rule comes in two possible forms.\n\n**PA-I**\n$$\nw_{t+1} = argmin_{w}\\frac{1}{2}\\| w - w_t \\|^2 + C\\xi \\\\ \\text{ s.t. } \\\\\nl(w; (x_t, y_t)) \\leq \\xi \\text{ and } \\xi \\geq 0.\n$$\n\n**PA-II**\n$$\nw_{t+1} = argmin_{w}\\frac{1}{2}\\| w - w_t \\|^2 + C\\xi^2 \\\\ \\text{ s.t. } \\\\\nl(w; (x_t, y_t)) \\leq \\xi.\n$$\n\nwhere $t$ is the iteration round, $C$ is the regularization term, and $\\xi$ is the slack term on the objective.\n\nThe updates share the form\n\n$$w_{t+1} = w_t + \\tau_t y_t x_t; $$\n$$\n\\tau_t = \\min \\left( C, \\frac{l_t}{\\| x_t \\|^2} \\right) \\text{ (PA-I),} \\\\\n\\tau_t = \\frac{l_t}{\\| x \\|^2 + \\frac{1}{2C}} \\text{ (PA-II).}\n$$\n\n---\n\nIn conjunction with NLP tools, we use scikit-learn's\n[`PassiveAggressiveClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PassiveAggressiveClassifier.html)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import functools\nimport json\nimport itertools\nimport re\nimport shutil\nimport tempfile\n\nimport matplotlib.pyplot as plt\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import (\n    ENGLISH_STOP_WORDS,\n    TfidfVectorizer,\n)\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.model_selection import (\n    train_test_split,\n    RandomizedSearchCV,\n    StratifiedShuffleSplit,\n)\nfrom sklearn.pipeline import (\n    make_pipeline,\n    Pipeline,\n)\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Parse training data and load it into a dataframe."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def load_dataframe(filepath, is_train=True):\n    \"\"\"Load data.\n\n    Arguments\n    ---------\n    filepath : str, file-like, or path-like\n        Path to JSON file.\n    is_train : bool\n        Load training data if True, else testing data.\n\n    Returns\n    -------\n    data_tuple : tuple\n        data : pd.DataFrame\n            Input features.\n        target : None or pd.Series\n            Targets (training data).\n    \"\"\"\n    with open(filepath) as fh:\n        deserialized_data = json.load(fh)\n\n        data_frame = pd.DataFrame(\n            data={\n                'ingredients': [\n                    ' '.join(entry['ingredients'])\n                    for entry in deserialized_data\n                ],\n            },\n            index=[entry['id'] for entry in deserialized_data],\n        )\n\n        target_series = None\n        if is_train:\n            target_series = pd.Series(\n                data=[entry['cuisine'] for entry in deserialized_data],\n                name='cuisine',\n            )\n\n        return data_frame, target_series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train, target = load_dataframe('../input/train.json')\ndata_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed5adb95aabe33b3bd007376c8aac359f85f1c35"},"cell_type":"code","source":"value_counts = target.value_counts()\n\nplt.figure(figsize=(20, 7))\n\nplt.bar(range(value_counts.shape[0]), value_counts,\n        color='SkyBlue')\n\nplt.xticks(range(value_counts.shape[0]), value_counts.index,\n           rotation=60, ha='right')\nplt.title('Label Distribution')\nplt.xlabel('Labels')\nplt.ylabel('Total')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 5))\n\ningredient_distribution = nltk.FreqDist(\n    itertools.chain(*data_train['ingredients'].apply(str.split)))\ningredient_distribution.plot(\n    50, cumulative=False, title='Most Common Word Occurences')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{"trusted":true},"cell_type":"code","source":"STOP_WORDS = frozenset(\n    nltk.corpus.stopwords.words('english')\n    + list(ENGLISH_STOP_WORDS)\n)\n\nprint(f'The number of stop words: {len(STOP_WORDS)}.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Implement a tokenizer functor to use with token vectorizers."},{"metadata":{"trusted":true},"cell_type":"code","source":"class IngredientTokenizer:\n    \"\"\"Custom tokenizer used in document transformations.\n\n    Arguments\n    ---------\n    token_pattern_re : str or regex pattern\n        Pattern indicating tokens.\n    lemmatizer : nltk tokenizer or nltk lemmatizer\n        Tokenizer or lemmatizer class transforming words into tokens.\n    \"\"\"\n\n    def __init__(self,\n                 token_pattern_re=None,\n                 lemmatizer=None):\n        self.token_pattern_re = token_pattern_re or re.compile(\n            r'(?iu)\\b\\w\\w+\\b')\n        self.lemmatizer = lemmatizer or nltk.wordnet.WordNetLemmatizer()\n\n    def __repr__(self):\n        return (\n            f'{self.__class__.__name__}(\\n\\t'\n            f'{self.token_pattern_re!r},\\n\\t{self.lemmatizer!r})'\n        )\n\n    def __call__(self, document):\n        \"\"\"Retrieve and tokenize/lemmatize valid words.\n\n        Arguments\n        ---------\n        document : array-like\n            Iterable containing texts.\n\n        Returns\n        -------\n        lemmas : list\n            Lemmatized/Tokenized words.\n        \"\"\"\n        words = self.token_pattern_re.findall(document)\n        call_fn = getattr(self.lemmatizer, 'lemmatize', 'tokenize')\n\n        lemmas = []\n        for word in words:\n            word = word.lower()\n            if word not in STOP_WORDS:\n                lemmas.append(call_fn(word))\n\n        return lemmas","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generate a TF-IDF matrix and fit a PA classifier."},{"metadata":{"trusted":true,"_uuid":"0bf478bb520d2bd7e6bbc20b888b15d8d230adea"},"cell_type":"code","source":"vectorizer = TfidfVectorizer(\n    min_df=20,\n    max_df=0.95,\n    stop_words=STOP_WORDS,\n    tokenizer=IngredientTokenizer(),\n)\nbag_of_words = vectorizer.fit_transform(data_train['ingredients'])\nbag_of_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer.get_feature_names()[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fa4001601ca7e0fe2ee0ed24d09e266c83d960a"},"cell_type":"code","source":"label_encoder = LabelEncoder()\ntarget_encoded = label_encoder.fit_transform(target)\n\nx_train, x_test, y_train, y_test = train_test_split(\n    bag_of_words, target_encoded, random_state=0)\n\nestimator = PassiveAggressiveClassifier(\n    early_stopping=True,\n    loss='hinge',\n    average=True,\n    class_weight='balanced',\n    n_jobs=-1,\n    verbose=False,\n    random_state=0,\n)\nestimator.fit(x_train, y_train)\n\nprint(f'Train accuracy: {estimator.score(x_train, y_train):.3f}')\nprint(f'Test accuracy: {estimator.score(x_test, y_test):.3f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb9bcf504a0712b1db23310979bead89ccfab652"},"cell_type":"code","source":"def visualize_coefficients(feature_names, estimator_coefficients,\n                           class_=0, n_top=20, **kwargs):\n    \"\"\"Plot coefficient magnitude for a specified label.\n\n    Parameters\n    ----------\n    feature_names : array-like\n        Vectorized features.\n    estimator_coefficients : array-like\n        Estimator weights/importances.\n    class_ : int, default 0\n        Label.\n    n_top : int, default 20\n        The number of positive/negative coefficients to plot.\n    kwargs : dict-like\n        Additional key-word arguments for plot function.\n    \"\"\"\n    coefficients = np.ravel(estimator_coefficients[class_])\n    positive_coef = np.argsort(coefficients)[-n_top:]\n    negative_coef = np.argsort(coefficients)[:n_top]\n    coef_matrix = np.hstack([negative_coef, positive_coef])\n\n    plt.figure(figsize=(22, 6))\n    plt.bar(np.arange(2 * n_top), coefficients[coef_matrix],\n            color=['b' if c < 0 else 'r' for c in coefficients[coef_matrix]])\n    plt.xticks(np.arange(2 * n_top), feature_names[coef_matrix],\n               rotation=60, ha='right')\n    plt.subplots_adjust(bottom=0.3)\n    plt.xlabel('Feature values')\n    plt.ylabel('Coefficient magnitude')\n    plt.title(kwargs.get('target_value'))\n    plt.show()\n\n\nfeature_names = np.array(vectorizer.get_feature_names())\ncoefficients = estimator.coef_\nvisualize_coefficients_default = functools.partial(\n    visualize_coefficients, feature_names, coefficients)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1605002358ee08e7b26fba9a30ef4e0a559c74c8"},"cell_type":"code","source":"label = 15\nlabel_encoded = label_encoder.inverse_transform([label])[0]\n\nvisualize_coefficients_default(\n    class_=label, n_top=15, target_value=label_encoded)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"Define an estimator pipeline comprising *latent semantic analysis*\n(TF-IDF features + Truncated SVD) and OvA PA classification.\nRun cross-validated hyperparameter optimization."},{"metadata":{"trusted":true,"_uuid":"7421771785a2c7fb6355b6b75ed51409e0902dfa"},"cell_type":"code","source":"cachedir = tempfile.mkdtemp()\n\npipe = Pipeline(\n    steps=[\n        ('tfidf', TfidfVectorizer()),\n        ('tsvd', TruncatedSVD(algorithm='arpack')),\n        ('pa', PassiveAggressiveClassifier(\n            max_iter=1000,\n            average=True,\n            early_stopping=True,\n            validation_fraction=0.1,\n            n_jobs=-1,\n            n_iter_no_change=20,\n            random_state=0,\n        ))\n    ],\n    memory=cachedir)\n\nparam_distributions = {\n    'tfidf__min_df': range(1, 51),\n    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n    'tsvd__n_components': range(100, 501),\n\n    'pa__C': stats.expon(1e-4, 0.5), # The regularization term C\n    'pa__loss': ['hinge', 'squared_hinge'], # PA-I or PA-II\n    'pa__class_weight': [None, 'balanced'],\n}\n\ncv = StratifiedShuffleSplit(\n    n_splits=3,\n    test_size=0.2,\n    random_state=0,\n)\n\ngrid = RandomizedSearchCV(\n    estimator=pipe,\n    param_distributions=param_distributions,\n    n_iter=5,\n    cv=cv,\n    scoring='accuracy',\n    n_jobs=-1,\n    iid=True,\n    refit=True,\n    error_score=np.nan,\n    verbose=True,\n)\n\n# Uncomment the line below to run hyperparameter optimization.\n# grid.fit(data_train['ingredients'], target)\n\nshutil.rmtree(cachedir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14e9528ef6c3912d64b61ceab5d1fe20906c7890"},"cell_type":"code","source":"%%time\n\nbest_pipe = make_pipeline(\n    TfidfVectorizer(min_df=20),\n    TruncatedSVD(n_components=500, algorithm='arpack'),\n    PassiveAggressiveClassifier(\n        C=0.1,\n        loss='hinge',\n        class_weight='balanced',\n        max_iter=1000,\n        early_stopping=True,\n        validation_fraction=0.2,\n        n_jobs=-1,\n        random_state=0,\n        average=True,\n    )\n)\n# best_pipe = grid.best_estimator_\nbest_pipe.fit(data_train['ingredients'], target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"Parse test data and submit predictions."},{"metadata":{"trusted":true,"_uuid":"77c1ac350ec1f1f27ebd8906a05c1b55465a99a2"},"cell_type":"code","source":"data_test, _ = load_dataframe(\"../input/test.json\", is_train=False)\n\npredictions = best_pipe.predict(data_test['ingredients'])\n\nsubmission = pd.DataFrame(data={\"id\": data_test.index, \"cuisine\": predictions})\nsubmission.to_csv(\"submission.csv\", index=None)\nsubmission.head(n=10).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Notes**\n\nThe script will not produce high scores since PA shows relatively poor performance on this data.\nThe main purpose was to introduce a new family of algorithms and apply them to the text data processed with LSA.\nFor higher scores, try more advanced techniques, or replace PA with SVMs, launch hyperparameter optimization.\n\n**References**\n\n[1] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. Online Passive-Aggressive Algorithms. In *Journal of Machine Learning Research 7 (2006) 551â€“585*."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}