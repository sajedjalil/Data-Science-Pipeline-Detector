{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nif os.path.exists(\"../input/whats-cooking-kernels-only\"):\n    challenge_path='../input/whats-cooking-kernels-only'\nelse:\n    challenge_path='../input'\nmode = 'train' #'train', 'commit', 'submission'\ntest_size=0.1\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"# Import the required libraries \n\nrandom_state = 7\nimport time\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nimport re\n\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedShuffleSplit\nfrom sklearn.utils import class_weight\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.sparse import csr_matrix\nfrom scipy.sparse import hstack, csr_matrix\n\nfrom keras import backend as K\nfrom keras.models import Sequential, load_model, Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout, BatchNormalization,UpSampling2D,Concatenate, Dense, Activation,Flatten\nfrom keras import regularizers\n\nimport numpy as np\n\nimport pandas as pd\nimport json\nimport pdb\n\ndef read_dataset(path):\n    return json.load(open(path, encoding='utf-8')) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c0ffdd3d1894a28b6f160119bb6723dff9537c2"},"cell_type":"code","source":"# Dataset Preparation\nprint (\"Read Dataset ... \")\n\ndf_train_raw=pd.read_json(os.path.join(challenge_path,'train.json')).set_index('id')\n\ndf_train_raw['invalid']=[False] * (len(df_train_raw))\nx_field='ingredients'\ny_field='cuisine'\ndf_train_raw=df_train_raw.loc[df_train_raw.cuisine !=None]\ndf_train_raw.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"44847da2cd5223683592ef3f47b51cbfea62a7f4","collapsed":true},"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\ndef preprocess(ingredients):\n    ingredients_text = ' '.join(ingredients)\n    ingredients_text = ingredients_text.lower()\n    ingredients_text = ingredients_text.replace('-', ' ')\n    words = []\n    for word in ingredients_text.split():\n        if re.findall('[0-9]', word): continue\n        if len(word) <= 2: continue\n        if 'â€™' in word: continue\n        word = lemmatizer.lemmatize(word)\n        if len(word) > 0: words.append(word)\n    return ' '.join(words)\n    \ning=df_train_raw[x_field].values.tolist()\ncou=df_train_raw[y_field].values.tolist()\n\n#tag to remove some invalid samples\nfind_duplicated=False\ndup=0\ntexts=[]\nfor i in range(len(ing)):\n    if df_train_raw['invalid'].values[i]==False:\n        if len(ing[i])<1 or pd.isna(cou[i]):\n            df_train_raw['invalid'].values[i]=True\n        elif find_duplicated:\n            if ing[i] in ing[i+1:]:\n                dup+=1\n                print(\"found dup at {}\".format(i))\n                df_train_raw['invalid'].values[i]=True\n    newtext=preprocess(df_train_raw[x_field].values[i])\n    texts.append(newtext)\n\ndf_train_raw['texts']=texts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35591a6f10897308f3684e858603648bdbb14c51"},"cell_type":"code","source":"len_orig=[len(x) for x in df_train_raw[x_field].values.tolist()]\nlen_proc=[len(x.split(' ')) for x in texts]\ndf_train_raw['len_orig']=len_orig\ndf_train_raw['len_proc']=len_proc\ndf_train_raw.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c433f26913c76ae0fc2d9135a520252c1e56245"},"cell_type":"code","source":"df_train=df_train_raw.loc[df_train_raw.invalid == False]\ndup,len(df_train),len(df_train_raw)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3cc24933649354d049b82a8f5cea03b29d8eb919"},"cell_type":"code","source":"df_submission = pd.read_json(os.path.join(challenge_path,'test.json')).set_index('id')\ndf_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c95219e43cb86a0464b21ad76abdaec3a50ae2f4","collapsed":true},"cell_type":"code","source":"#let's read plain text before encoded\ny_j=df_train[y_field].values\nx_j=df_train['texts'].values\n\nfor i in range(len(df_submission)):\n    df_submission[x_field].values[i]=preprocess(df_submission[x_field].values[i])\nx_j_sub = df_submission[x_field].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53884d9d9deaac07aaff68719834657e275bdae7","collapsed":true},"cell_type":"code","source":"y_lbr=LabelBinarizer()\ny_j_conv=y_lbr.fit_transform(y_j)\n\ny_classes=y_lbr.classes_\ny_classes_transformed=y_lbr.transform(y_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71a1bcd524e26e33c7eff40816a9752c3e3687d5"},"cell_type":"code","source":"#not used\nclass_weights = class_weight.compute_class_weight('balanced', y_classes, y_j)\nclass_weights={i:class_weights[i] for i in range(len(y_lbr.classes_))}\nclass_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"075141c8e97ad437fb9742feada404604c9ce6ef","collapsed":true},"cell_type":"code","source":"#vectorize texts\n\ndef tokenize(text):\n    #print(text)\n    tokens = nltk.word_tokenize(text)\n    return tokens\n\nx_cvr = TfidfVectorizer(analyzer='word', tokenizer=tokenize, token_pattern=None, stop_words='english') \n\nx_cvr.fit(x_j)\nx_analyze = x_cvr.build_analyzer()\n\nx_j_conv =x_cvr.transform(x_j).toarray()\nx_j_sub_conv=x_cvr.transform(x_j_sub).toarray()\n\n#x_cvr,x_j_conv.shape,x_j_sub_conv.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d4b650b72ab00c6e438a2c1560c59d40aaa605a","collapsed":true},"cell_type":"code","source":"xt,xv,yt,yv,yjt,yjv=train_test_split(x_j_conv,y_j_conv,y_j, test_size=test_size, stratify=y_j, random_state=7, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47b3c58bc0eb87067598a1575be58bf3d158191b","scrolled":true},"cell_type":"code","source":"K.clear_session()\n\nnp.random.seed(7)\n\ndo=0.50\n\ni = Input(shape=xt.shape[1:])\nx=Dropout(do-0.1)(i)\nx=Dense(640)(x) #, use_bias=False\nx=BatchNormalization()(x)\nx=Activation('relu')(x)\nx=Dropout(do)(x)\n\no=Dense(yt.shape[1],activation='linear')(x)  \no=Dense(yt.shape[1],activation='softmax')(o)\n\nmodel=Model(inputs=[i,], outputs=[o,])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72222a0ed4ccaed3c872a36c64326b506b63ff07","scrolled":true,"collapsed":true},"cell_type":"code","source":"lr=0.005\nepochs=100\nverbose=2\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=lr), metrics=[\"accuracy\"])\n\nmi=''\nif mode==\"train\":\n    early_stopping = EarlyStopping(monitor='val_loss', patience=33, verbose=1)\n    model_checkpoint2 = ModelCheckpoint(\"./best_loss{}.model\".format(mi),monitor='val_loss', \n                                       save_best_only=True, verbose=1)\n    model_checkpoint3 = ModelCheckpoint(\"./best_acc{}.model\".format(mi),monitor='val_acc', \n                                       save_best_only=True, verbose=1)\n    reduce_lr = ReduceLROnPlateau(factor=0.3, patience=7, min_lr=0.00001, verbose=verbose) \n    model.fit(x=[xt,],y=[yt]*1,validation_data=[[xv,],[yv]*1],callbacks=[early_stopping, model_checkpoint2, model_checkpoint3, reduce_lr], epochs=epochs, batch_size=256, shuffle=True, verbose=2)\nelif mode == 'commit':\n    early_stopping = EarlyStopping(monitor='loss', patience=19, verbose=1)\n    model_checkpoint2 = ModelCheckpoint(\"./best_loss{}.model\".format(mi),monitor='loss', \n                                       save_best_only=True, verbose=1)\n    model_checkpoint3 = ModelCheckpoint(\"./best_acc{}.model\".format(mi),monitor='acc', \n                                       save_best_only=True, verbose=1)\n    reduce_lr = ReduceLROnPlateau(factor=0.3, patience=5, min_lr=0.00001, verbose=verbose) \n    model.fit(x=[x_j_conv,],y=[y_j_conv]*1,callbacks=[early_stopping, model_checkpoint2, model_checkpoint3, reduce_lr], epochs=epochs, batch_size=256, shuffle=True, verbose=2)\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0287a38118e2ba27665a85fde2f68c2d4918d4c8","collapsed":true},"cell_type":"code","source":"if mode == 'train' or mode == 'commit':\n    model_loss=load_model(os.path.join('.',\"best_loss.model\"))\n    model_acc=load_model(os.path.join('.',\"best_acc.model\"))\nelif mode=='submission':\n    model_loss=load_model(os.path.join(user_path,\"best_loss.model\"))\n    model_acc=load_model(os.path.join(user_path,\"best_acc.model\"))        \nprint(model_loss.evaluate([xv,],[yv]*1))\nprint(model_acc.evaluate([xv,],[yv]*1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6838886ab3c3653a2b84911bfd29389c9dff9e30","collapsed":true},"cell_type":"code","source":"y_j_sub_conv=model_loss.predict(x_j_sub_conv,verbose=1)\ny_j_sub_conv2=model_acc.predict(x_j_sub_conv,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45d810cab8601f32a57b8391e2e777468dd19760","collapsed":true},"cell_type":"code","source":"y_j_sub=y_lbr.inverse_transform(y_j_sub_conv)\ny_j_sub2=y_lbr.inverse_transform(y_j_sub_conv2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e9757a87e277d937e658f1de14c49550a26d899","collapsed":true},"cell_type":"code","source":"df_submission[y_field]= y_j_sub\ndf_submission.head()\n\n# Submission\nprint (\"Generate Submission File ... \")\nsub=df_submission.drop(x_field,axis=1)\nsub.head(50)\nsub.to_csv('svm_output_loss.csv', index=True)\nsub.to_csv('submission.csv', index=True)\n\nwith open('submission.csv', mode='r', encoding='utf-8') as f:\n    csvtext=f.read()\nprint(csvtext)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"393322caf304a0e51239d046c03ae09324a1b664","collapsed":true},"cell_type":"code","source":"df_submission[y_field]= y_j_sub2\ndf_submission.head()\n\n# Submission\nprint (\"Generate Submission File ... \")\nsub=df_submission.drop(x_field,axis=1)\nsub.head(50)\nsub.to_csv('svm_output_acc.csv', index=True)\nsub.to_csv('submission.csv', index=True)\n\nwith open('submission.csv', mode='r', encoding='utf-8') as f:\n    csvtext=f.read()\nprint(csvtext)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}