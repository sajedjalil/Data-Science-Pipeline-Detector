{"cells":[{"metadata":{"_uuid":"3e546120590bdb2101321fb2da8bf17da1fa6b84"},"cell_type":"markdown","source":"# **This is my first kernel on kaggle! **\n\nI'm welcome to any **comments/suggestions** on the code or the format of this kernel :) I'll be looking to improve it in the future.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #plotting\nimport seaborn as sns #plotting\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a879007ae3099ff53b75068bbc6aa68a30884d2"},"cell_type":"markdown","source":"First of all, let's load in the data and have a look at the overall structure (columns, rows, and anything that looks interesting!)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Loading the data\n\ntrain_df = pd.read_json('../input/train.json')\ntest_df = pd.read_json('../input/test.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ffc9ba1702567b9ca7aa259fa5026aabced0a22","scrolled":true},"cell_type":"code","source":"# Exploration\n\nprint ('Exploring the training data')\nprint (train_df.head(5))\nprint (train_df.shape)\nprint (train_df.columns)\nprint (train_df.info())\n\nprint ('-----------------------------------------------------------------------------------------------')\nprint ('Exploring the test data')\nprint (test_df.head(5))\nprint (test_df.shape)\nprint (test_df.info())\n\nprint ('-----------------------------------------------------------------------------------------------')\nprint ('How many cuisine types are there? How common are they in the data set?')\n\nunique_cuisine_types = train_df['cuisine'].nunique()\nprint ('There are %d unique cuisine types'  %  (unique_cuisine_types))\n\nfreq_cuisines = train_df['cuisine'].value_counts()\nplt.figure(figsize=(20,6))\nsns.barplot(x= freq_cuisines.index, y= freq_cuisines.values, color = 'b')\nplt.xlabel('type of cuisine')\nplt.ylabel('# of recipes')\nplt.title('# of recipes per type of cuisine in training data')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3bdd7280e3f38d8e4560f2fdde7d75408ac2456"},"cell_type":"markdown","source":"From briefly looking at the training and test data we can see that:\n1. The 'ingredients' column has a list of ingredients delimited by ','. We will have to deal with this when we tokenize the data. \n2. We can also spot  non-important words such as 'of' (in 'cream of tartar' which could be removed as they will not likely be linked to any particular cuisine\n3. There is a trade-off in tokenizing whole phrases, or individual words. Let's try individual words first as my hypothesis is that they will be easier to match in new data sets.\n\nThere are also 20 cuisine types, with italian being the most popular, and brazilian being the least popular. \n\nNow let's look at how many ingredients we might be working with..."},{"metadata":{"trusted":true,"_uuid":"50439aa069351b4687988a2f7305267ac94f6959"},"cell_type":"code","source":"# Exploration continued...\n\n# 1. Counting the number of ingredients in each recipe list\nnumber_of_ingredients = []\nfor i in range(len(train_df['ingredients'])):\n    number_of_ingredients.append(len(train_df['ingredients'][i]))\n\ntrain_df['number_of_ingredients'] = number_of_ingredients\n\nprint ('The average number of ingredients is %d' % np.average(number_of_ingredients))\nprint ('The max number of ingredients is %d' % np.max(number_of_ingredients))\nprint ('The min number of ingredients is %d' % np.min(number_of_ingredients))\n\n# What do the ranges of # of ingredients look like for the different cuisines?\n\n# Getting the min and max values of the boxplots to order it by size of range\nlowIQ = train_df.groupby(['cuisine']).quantile(0.25)['number_of_ingredients']\nhighIQ = train_df.groupby(['cuisine']).quantile(0.75)['number_of_ingredients']\nIQR = highIQ - lowIQ\nminvalue = train_df.groupby(['cuisine']).min()['number_of_ingredients']\nmaxvalue = highIQ + (IQR * 1.5)\noverall_range = maxvalue - minvalue\nordered_cuisines = (overall_range.sort_values(ascending = False).index)\n\nplt.figure(figsize=(20,6))\nsns.boxplot(x=\"cuisine\", y=\"number_of_ingredients\",data= train_df, width = 0.7, color = 'b', order = ordered_cuisines)\nplt.xlabel('type of cuisine')\nplt.ylabel('# of ingredients')\nplt.title('spread of # of ingredients by type of cuisine, ordered by range in # of ingredients')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a35019a13fae1a03e9fa49168c681c349cbe8234"},"cell_type":"markdown","source":"The average number of ingredients is 10 but there is a huge range in all cuisine types, ranging from 65 to 1! \n\nAs you would expect... the British range excluding outliers has a maximum value which is relatively low (I can say this as I'm British and fully accept that I live off a diet of pies, peas and chips...), whilst Vietnamese and Indian cuisines appear to use many more ingredients. \n\nThere are outliers  for # of ingredients for all the cuisine types but this will not affect our predictions for cuisine type. We should still include recipes that have lots of ingredients!\n\n.... Now let's turn the data into something we can work with for predictions and create our test and training data sets"},{"metadata":{"trusted":true,"_uuid":"9fbf45a7f51b435c32f72af22f11e3411a824314"},"cell_type":"code","source":"# Turn the ingredients into a single string so we can process them as individual words, as important words may be more easily recognised as common between \n# recipes.\ntrain_df['seperated_ingredients'] = train_df['ingredients'].apply(','.join)\ntest_df['seperated_ingredients'] = test_df['ingredients'].apply(','.join)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7cbfc39896c6ba6daddea4b21dd3704321ede5e"},"cell_type":"code","source":"#Splitting the training data\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(train_df['seperated_ingredients'], train_df['cuisine'], test_size = 0.30, random_state = 102)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4186d49dbf8a05a0e64c40bf0f9d8c9cdd85bf0","trusted":true},"cell_type":"markdown","source":"To predict the cuisine types for the test data, I will use both CountVectorizer and TfidfVectorizer, and Multinomial Naive Bayes. \n\nCountVectorizer simply counts the number of text tokens and puts them in a matrix. \n\nTfidfVectorizer is slightly different and gives word frequencies that, unlike CountVectorizer, reflect the more 'interesting' words and gives less power to words that may be common across all texts. For example, salt may be used in every single cuisine and therefore is not very meaningful for us to use. The Tfidf (Term Frequency â€“ Inverse Document Frequency ) penalises 'salt' using the inverse document frequency term.\n\nThere will also be several conditions within the vectorization steps: \n1) Lemmatizing the words so that similar words can be recognised e.g. olives and olive\n2) Removing stopwords such as 'the' and 'and' which will likely not be linked to any cuisine type and will contribute to noise\n3) Tokenizing on words only with a token pattern, ignoring any numbers which again will likely just be noise. \n4) Ensuring all words are lowercase as capslock can interfere with matching. \n5) For Tfidf, we also can specify max_df which gives the threshold for words that could be left out. For example, the threshold could be 1 (word features in all docs/sentences/groups) and so any word that has 1 for a value and features in every group will not be included. I have set the threshold to 0.6 for now but we can play with this. \n6) Using bi-grams rather than uni-grams\n\n"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"aee975189c8d9a3e2e030bc961c690811fbee80c"},"cell_type":"code","source":"# Creating a pipeline\n\nfrom sklearn.pipeline import Pipeline\n\n#Vectorize imports\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom nltk import word_tokenize          \nfrom nltk.stem import WordNetLemmatizer \nclass LemmaTokenizer(object):\n    def __init__(self):\n        self.wnl = WordNetLemmatizer()\n    def __call__(self, doc):\n        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\npattern = r\"[A-Za-z]\" \nfrom sklearn.preprocessing import MaxAbsScaler\n\n# Set up a vectorizer which can be tested and words substituted into it.\n\nvec = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words = 'english', lowercase = True, token_pattern = pattern, ngram_range = (1,2))\n#vec = TfidfVectorizer(tokenizer=LemmaTokenizer(), stop_words = 'english', lowercase = True, token_pattern = pattern, max_df = 0.1, ngram_range = (1,2))\n\n#Classifier imports\n#from sklearn.naive_bayes import MultinomialNB - 0.74\n#from sklearn.neighbors import KNeighborsClassifier - 0.40\n#from sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.learning_curve import validation_curve\n\nC_param_range = [0.5,0.8,1,2,3]\naccuracy_list = []\n#Set up a classifier\nfor i in C_param_range:  \n    classifier = LogisticRegression(penalty = 'l2', C = i,random_state = 0)\n    # Create and fit the pipeline\n    pl = Pipeline([\n        ('vec',vec), \n        ('scale', MaxAbsScaler()),\n        ('classifier', classifier)])\n    pl.fit(X_train, y_train)\n    accuracy = pl.score(X_test, y_test)\n    print (accuracy)\n    accuracy_list.append(accuracy)\n\nzipped = list(zip(C_param_range,accuracy_list))\nprint (zipped)\n#print ('The score is: %.5f '% accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cf3e98bc091fc8443aae2ed2a54e688f5945e4c"},"cell_type":"code","source":"# Best score so far is: 0.78856\n# What is the maximum accuracy we have got so far and which parameter value does it correspond to?\nfrom operator import itemgetter\nfrom heapq import nlargest\nimport itertools\n\nresult = max(zipped,key=lambda x:x[1])\nprint (result[0])\n\nclassifier = LogisticRegression(penalty = 'l2', C = result[0],random_state = 0)\n\n#Create and fit the pipeline\npl = Pipeline([\n    ('vec',vec), \n    ('scale', MaxAbsScaler()),\n    ('classifier', classifier)])\npl.fit(X_train, y_train)\ny_predicted = pl.predict(X_test)\naccuracy = pl.score(X_test, y_test)\nprint ('The score is: %.5f '% accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12939200a995381d6ebd34a637d15793073e1bfb"},"cell_type":"code","source":"# Using a confusion matrix to identify areas of error - where could we focus a model fix?\nfrom sklearn.metrics import confusion_matrix\n\ncm = pd.DataFrame(confusion_matrix(y_test, y_predicted, labels=train_df['cuisine'].unique()), index=train_df['cuisine'].unique(), columns=train_df['cuisine'].unique())\n\ntotals = y_test.value_counts()\ntotals_from_training = y_train.value_counts()\njoined = pd.concat([cm, totals], axis = 'columns', sort = False)\njoined['totals_from_test'] = joined.iloc[:,-1]\njoined['totals_from_training'] = totals_from_training\njoined['%_correct'] = [round((joined.loc[i,i])/(joined.loc[i,'totals_from_test']),2) for i in joined.columns[:-3]]\njoined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eac82bc988f66a28384ac05d4188c2a1082f5234"},"cell_type":"code","source":"#Are the less common cuisines predicted correctly less of the time? \nfrom scipy.stats import pearsonr\n\ncorrelation = pearsonr(joined['totals_from_training'], joined['%_correct'])\nprint ('The correlation score is: %.2f' % correlation[0])\n\norder = (joined['totals_from_training'].sort_values(ascending = True).index)\n#print (order)\nplt.figure(figsize=(20,6))\nsns.barplot(x= joined.index, y= joined[\"%_correct\"], order = order)\nplt.xlabel('type of cuisine')\nplt.ylabel('%_correct')\nplt.title('% of predictions correct by cuisine, ordered by cuisine frequency in test-set')\nplt.show()\nsns.regplot(x= joined['totals_from_training'], y= joined[\"%_correct\"])\nplt.xlabel('Total cuisine frequency in training set')\nplt.ylabel('% correct in test set')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30162113031a11b8537a61af10edfa8d2e93e886","scrolled":true},"cell_type":"code","source":"# Writing the test submission file \nsubmission = pl.predict(test_df['seperated_ingredients'])\n\nsubmission_file = pd.DataFrame(data = submission, columns = ['cuisine'], index = test_df['id'])\n\nsubmission_file.reset_index(level=0, inplace=True)\n\nprint (submission_file.head(5))\nsubmission_file.to_csv('submission5.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}