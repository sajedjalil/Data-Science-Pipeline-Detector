{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <h1><center><b> Part-2: U.S. Patent Phrase to Phrase Matching - Transformer</b></center></h1>\n\n## <center>My learning process of transformer followed by this dicussion (below link)</center>\n\n### <center>https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/discussion/320962</center>\n\n## **Now I learn in hugging face (Text data), This notebook fully followed by NLP with transformer book and hugging face course**\n\n\n\n<img src='https://i.ytimg.com/vi/C6eehLgLADs/maxresdefault.jpg'>\n\n\n\n\n\n\n\n### **Hello Guys! This is my first competition Based on NLP --> Two members(Me and my friend Edoziem Enyinnaya)** \n\n## ***I try to apply the concept in Competition data later! Now learn Transformers***\n\n### ***I am trying in Coming weeks (below the content) of part 2 and finally apply the concepts in Competition data | If anyone interested in the process suggest your idea and topics guys***\n\n## **Part 1: https://www.kaggle.com/code/venkatkumar001/nlp-starter-almost-all-basic-concept**\n\n### **1. Data Cleaning and Preprocessing:**\n\n- Data-Punctuations,stopwords,number,caseconversion (Remove noise)\n- word normalization(stemming,tokenization,lemmatization,POS,NER)\n- word standartization(tables,regularexp)--> Clean text data\n\n### **2. Text Representation and word_Embedding:**\n\n- BOW\n- TF-IDF\n- Word-Embedding (word2vec,doc2vec,Glove)\n- TSNE visualization\n\n## **Part 2: https://www.kaggle.com/venkatkumar001/nlp-starter-transformer-pipeline**\n\n### **Transformer**\n\n- NLP with transforer book & Hugging face course\n","metadata":{}},{"cell_type":"markdown","source":"# <center>Hugging Face</center>\n\n<img src = 'https://analyticsindiamag.com/wp-content/uploads/2021/06/Hugging-Face-1024x694.jpg'>\n\n# **1. Hello transformer**\n\n<img src = 'https://github.com/nlp-with-transformers/notebooks/raw/c2db380169af910eed0ca5c61b7882f3caa18cf3//images/chapter01_timeline.png'>\n\nTo understand what is novel about transformers,we first need to explain:\n\n‚Ä¢ The encoder-decoder framework:\n\nThe job of the encoder is to encode the information from the input sequence into a numerical representation that is often called the last hidden state. This state is then passed to the decoder, which generates the output sequence.\n\n<img src = 'https://github.com/nlp-with-transformers/notebooks/raw/c2db380169af910eed0ca5c61b7882f3caa18cf3//images/chapter01_enc-dec.png'>\n\n‚Ä¢ Attention mechanisms:\n\n\nThe idea behind the attention mechanism was to permit the decoder to utilize the most relevant parts of the input sequence in a flexible manner, by a weighted combination of all of the encoded input vectors, with the most relevant vectors being attributed the highest weights\n\n<img src = 'https://github.com/nlp-with-transformers/notebooks/raw/c2db380169af910eed0ca5c61b7882f3caa18cf3//images/chapter01_enc-dec-attn.png'>\n\n‚Ä¢ Transfer learning\n\nTransfer learning is first build model large amount of data and train then the model weights used for our usecase\n\n<img src = 'https://github.com/nlp-with-transformers/notebooks/raw/c2db380169af910eed0ca5c61b7882f3caa18cf3//images/chapter01_transfer-learning.png'>\n\nThis image credit: NLP with transformer book","metadata":{}},{"cell_type":"markdown","source":"# ***Now little experiment in Hugging face using Pipeline API***\n\nHugging face: Already build the model and finetune throught the transformer. This pipeline command single line command predict the result. Let see!\n\n## ***Pipeline API: The pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of the complex code from the library*** \n\n<img src='https://github.com/nlp-with-transformers/notebooks/raw/c2db380169af910eed0ca5c61b7882f3caa18cf3//images/chapter01_hf-ecosystem.png'>\n\n1. Text classification\n2. Name Entity Recognition\n3. Question and Answering\n4. Summarization \n5. Translation\n\nI will try one application guys! Its awesome","metadata":{}},{"cell_type":"markdown","source":"# **Install the Hugging face transformer**","metadata":{}},{"cell_type":"code","source":"# install Hugging face transformer\n!pip install transformers","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-30T03:33:50.251978Z","iopub.execute_input":"2022-05-30T03:33:50.252336Z","iopub.status.idle":"2022-05-30T03:34:01.265783Z","shell.execute_reply.started":"2022-05-30T03:33:50.252252Z","shell.execute_reply":"2022-05-30T03:34:01.264958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **1.1: Pipline - Text classification**\n\nAll are know text classification! this package used to just two line to predict output","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom transformers import pipeline\n\n# Input data\n\n## This input is negative command so output is negative\n\ntext = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure \\\nfrom your online store in Germany. Unfortunately, when I opened the package, \\\nI discovered to my horror that I had been sent an action figure of Megatron \\\ninstead! As a lifelong enemy of the Decepticons, I hope you can understand my \\\ndilemma. To resolve the issue, I demand an exchange of Megatron for the \\\nOptimus Prime figure I ordered. Enclosed are copies of my records concerning \\\nthis purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\"\n\n#pipeline in text classification package interface\n\nclassifier = pipeline(\"text-classification\")\n\n# output\n\ntext_class = classifier(text)\npd.DataFrame(text_class)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:34:01.267929Z","iopub.execute_input":"2022-05-30T03:34:01.268187Z","iopub.status.idle":"2022-05-30T03:34:53.921324Z","shell.execute_reply.started":"2022-05-30T03:34:01.268153Z","shell.execute_reply":"2022-05-30T03:34:53.920591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this input positive! try it yourself\ntext1 = \"Now i learn Hugging face and develop the transformer skills in NLP\"\nout = classifier(text1)\npd.DataFrame(out)    ","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:34:53.922587Z","iopub.execute_input":"2022-05-30T03:34:53.924054Z","iopub.status.idle":"2022-05-30T03:34:53.981562Z","shell.execute_reply.started":"2022-05-30T03:34:53.924011Z","shell.execute_reply":"2022-05-30T03:34:53.980782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <center>2. Text Classification</center>\n\n### **Following steps: Step by Step process**\n\n<img src='https://github.com/nlp-with-transformers/notebooks/raw/c2db380169af910eed0ca5c61b7882f3caa18cf3//images/chapter02_hf-libraries.png'>\n\n### **Twitter emotion dataset: Data taken Hugging face Hub dataset**\n\n#### ***üì¢ All are know the process of above steps but this notebook is \"How to use Hugging face dataset, Model, Transformer ?\"***","metadata":{}},{"cell_type":"code","source":"#import datasets\nfrom datasets import list_datasets\n\nall_datasets = list_datasets()\n\nprint(f\"There are {len(all_datasets)} datasets currently available on the Hub\")\nprint('')\nprint(f\"The first 10 are: {all_datasets[:10]}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:34:53.983791Z","iopub.execute_input":"2022-05-30T03:34:53.984071Z","iopub.status.idle":"2022-05-30T03:34:56.732411Z","shell.execute_reply.started":"2022-05-30T03:34:53.984032Z","shell.execute_reply":"2022-05-30T03:34:56.731591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2.1. Emotion dataset**","metadata":{}},{"cell_type":"code","source":"# import load emotion dataset\nfrom datasets import load_dataset\n#load data\nemotions = load_dataset(\"emotion\")\nemotions","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:34:56.733641Z","iopub.execute_input":"2022-05-30T03:34:56.733883Z","iopub.status.idle":"2022-05-30T03:35:19.235563Z","shell.execute_reply.started":"2022-05-30T03:34:56.733846Z","shell.execute_reply":"2022-05-30T03:35:19.234746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#split the data\ntrain_ds = emotions[\"train\"]\ntrain_ds","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:35:19.236799Z","iopub.execute_input":"2022-05-30T03:35:19.237225Z","iopub.status.idle":"2022-05-30T03:35:19.243465Z","shell.execute_reply.started":"2022-05-30T03:35:19.237187Z","shell.execute_reply":"2022-05-30T03:35:19.242683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#length of the data\nprint('Length Of Data: ',len(train_ds))\nprint('')\n# Single example\nprint(\"Single example by its index in all taken emotion data: \", train_ds[0])\nprint('')\n\n#column names\nprint('Column_Names: ', train_ds.column_names)\nprint('')\n#features\nprint(\"Features of our data: \",train_ds.features)\nprint('')\n\n# data visualize in head (text and label)\nprint('First 5 rows of data and label: ',train_ds[:5])\nprint('')\n\n# text feature first 5\nprint('First 5 rows of data (Text):',train_ds['text'][:5])","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:35:19.244758Z","iopub.execute_input":"2022-05-30T03:35:19.245645Z","iopub.status.idle":"2022-05-30T03:35:19.282689Z","shell.execute_reply.started":"2022-05-30T03:35:19.245607Z","shell.execute_reply":"2022-05-30T03:35:19.281922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2.2. From Datasets to DataFrames(set format: Pandas)**","metadata":{}},{"cell_type":"code","source":"# setformat in pandas\nemotions.set_format(type=\"pandas\")\n\n# convert all data in the format\ndf = emotions[\"train\"][:]\n\n# Tail the process(display)\ndisplay(df.tail())\n\n# Tail the label(integer) -> label names (string) of one column (label_int2str)\ndef label_int2str(row):\n    return emotions[\"train\"].features[\"label\"].int2str(row)\n\n#create one features\ndf[\"label_name\"] = df[\"label\"].apply(label_int2str)\n#display(head)\ndisplay(df.head())","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:35:19.283929Z","iopub.execute_input":"2022-05-30T03:35:19.284224Z","iopub.status.idle":"2022-05-30T03:35:19.361259Z","shell.execute_reply.started":"2022-05-30T03:35:19.284189Z","shell.execute_reply":"2022-05-30T03:35:19.360613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Class distribution of label_name\nimport matplotlib.pyplot as plt\n\ndf[\"label_name\"].value_counts(ascending=True).plot.barh()\nplt.title(\"Frequency distribution of class\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:35:19.362274Z","iopub.execute_input":"2022-05-30T03:35:19.362585Z","iopub.status.idle":"2022-05-30T03:35:19.558394Z","shell.execute_reply.started":"2022-05-30T03:35:19.36255Z","shell.execute_reply":"2022-05-30T03:35:19.557729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#How long are our tweets\n# Identify the imbalance of data\n\ndf[\"Words Per Tweet\"] = df[\"text\"].str.split().apply(len)\ndf.boxplot(\"Words Per Tweet\", by=\"label_name\", grid=False,\nshowfliers=False, color=\"black\")\nplt.suptitle(\"\")\nplt.xlabel(\"\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:35:19.561284Z","iopub.execute_input":"2022-05-30T03:35:19.561474Z","iopub.status.idle":"2022-05-30T03:35:19.84544Z","shell.execute_reply.started":"2022-05-30T03:35:19.561451Z","shell.execute_reply":"2022-05-30T03:35:19.844732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Before distribution all classes are range is 15 counts\n#reset the format\nemotions.reset_format()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:35:19.846712Z","iopub.execute_input":"2022-05-30T03:35:19.846973Z","iopub.status.idle":"2022-05-30T03:35:19.852541Z","shell.execute_reply.started":"2022-05-30T03:35:19.846936Z","shell.execute_reply":"2022-05-30T03:35:19.851799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2.3. From Text to Tokens**\n\nTransformer models like distilbert cannot receive raw string as input instead, they assume the text has been token and encode as numerical vector","metadata":{}},{"cell_type":"markdown","source":"## ***2.3.1. Character Tokenization***","metadata":{}},{"cell_type":"code","source":"# Character Tokenization\ntext = \"Its all about hugging face! Learning NLP\"\nprint('Character Tokenized: ')\ntokenized_text = list(text)\nprint(tokenized_text)\nprint('')\n\n# character to numerical token\nprint('token to Numerical values: ')\ntoken2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}\nprint(token2idx)\nprint('')\n\n#Mapping the each character\nprint('Mapping the all character to numerical: ')\ninput_ids = [token2idx[token] for token in tokenized_text]\nprint(input_ids)\nprint('')\n\n# pandas dataframe to name and give label\nprint(\"Create dataframe of own category: \")\ncategorical_df = pd.DataFrame(\n{\"Name\": [\"VK\", \"VK_ANT\", \"Beast\"], \"Label ID\": [0,1,2]})\ndisplay(categorical_df)\nprint('')\n\n#get dummies\nprint('Get dummies-one hot method: ')\ndisplay(pd.get_dummies(categorical_df['Name']))\nprint('')\n#One hot encoding approach(Pytorch-Tensor)\nprint('OneHotEncoding - Pytorch: ')\n\nimport torch\nimport torch.nn.functional as F\n\ninput_ids = torch.tensor(input_ids)\nOHE = F.one_hot(input_ids,num_classes=len(token2idx))\nprint('Shape:',OHE.shape)\nprint('')\n\n#By examining the first vector, we can verify that a 1 appears in the location indicated by input_ids[0] :\nprint(f\"Token: {tokenized_text[0]}\")\nprint(f\"Tensor index: {input_ids[0]}\")\nprint(f\"One-hot: {OHE[0]}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:35:19.854119Z","iopub.execute_input":"2022-05-30T03:35:19.854576Z","iopub.status.idle":"2022-05-30T03:35:19.889184Z","shell.execute_reply.started":"2022-05-30T03:35:19.854527Z","shell.execute_reply":"2022-05-30T03:35:19.888531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ***2.3.2. Word tokenization***","metadata":{}},{"cell_type":"code","source":"tokenized_text = text.split()\nprint(tokenized_text)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:35:19.890343Z","iopub.execute_input":"2022-05-30T03:35:19.890851Z","iopub.status.idle":"2022-05-30T03:35:19.895711Z","shell.execute_reply.started":"2022-05-30T03:35:19.890817Z","shell.execute_reply":"2022-05-30T03:35:19.894938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <center>3. Transformer Approach - What a different traditional methods</center>\n\n## **Always Bert base model Popular one:**\n\n<img src='https://miro.medium.com/max/2000/0*EmHoJfuewF-gEsMv.png'>\n\nBERT stands for Bidirectional Encoder Representations from Transformers and is a language representation model by Google. It uses two steps, pre-training and fine-tuning, to create state-of-the-art models for a wide range of tasks.\n\n- L = Number of layers (i.e., #Transformer encoder blocks in the stack).\n- H = Hidden size (i.e. the size of q, k and v vectors).\n- A = Number of attention heads.\n\n1. BERT Base: L=12, H=768, A=12. (Total Parameters=110M!)\n2. BERT Large: L=24, H=1024, A=16 (Total Parameters=340M!)\n\n### And two main purpose:\n\n1. pretrained (Mask language Model & Next sentence Prediction)\n2. Finetune\n\n<img src='https://miro.medium.com/max/1400/1*UKcrS0VO_ZBEysVh-AvL7w.png'>\n\n\n\n## **Then this task used distilbert - sentence and word embedding:**\n\n## **Distilbert:**\n\nThe DistilBERT model was proposed in the blog post Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT, and the paper DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT‚Äôs performances as measured on the GLUE language understanding benchmark.\n\n<img src='https://www.freesion.com/images/36/08f26e2db11ec7a2bdfaaeff46878c64.png'>","metadata":{}},{"cell_type":"code","source":"#common large auto tokenizer\nfrom transformers import AutoTokenizer\n\nmodel_ckpt = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:35:19.897095Z","iopub.execute_input":"2022-05-30T03:35:19.89777Z","iopub.status.idle":"2022-05-30T03:35:33.984553Z","shell.execute_reply.started":"2022-05-30T03:35:19.897732Z","shell.execute_reply":"2022-05-30T03:35:33.983794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#specifically tokenize the data (particularly distilberttoken)\n\nfrom transformers import DistilBertTokenizer\n\ndistilbert_token = DistilBertTokenizer.from_pretrained(model_ckpt)\n\n#check tokenizer model\n#id and attention\nencoded_text = tokenizer(text)\nprint(encoded_text)\nprint('')\n\n#id to token word\ntokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\nprint(tokens)\nprint('')\nprint(tokenizer.convert_tokens_to_string(tokens))\nprint('')\n#tokenzier size\nprint('tokenizer vocab size: ',tokenizer.vocab_size)\nprint('')\n\n#Model maximum context size\nprint('Model max length: ',tokenizer.model_max_length)\nprint('')\n\n#features name\nprint('Features name: ',tokenizer.model_input_names)\nprint('')","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:35:33.985851Z","iopub.execute_input":"2022-05-30T03:35:33.986132Z","iopub.status.idle":"2022-05-30T03:35:38.687324Z","shell.execute_reply.started":"2022-05-30T03:35:33.986096Z","shell.execute_reply":"2022-05-30T03:35:38.686569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3.1. Emotion dataset - Ok lets apply the knowledge in real data**\n\nOutput - How to work attention masks\n\n<img src='https://github.com/nlp-with-transformers/notebooks/raw/c2db380169af910eed0ca5c61b7882f3caa18cf3//images/chapter02_attention-mask.png'>","metadata":{}},{"cell_type":"code","source":"# Tokenize the emotion input data\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n\n# special token id pad=0,unknown = 100, cls=101, sep=102, mask=103\n\nprint('Tokenize the emotion data: ',tokenize(emotions['train'][:2]))\nprint('')\n\n#special token ids\ntokens2ids = list(zip(tokenizer.all_special_tokens, tokenizer.all_special_ids))\ndata = sorted(tokens2ids, key=lambda x : x[-1])\ndf = pd.DataFrame(data, columns=[\"Special Token\", \"Special Token ID\"])\nprint('Special token ids:')\ndisplay(df.T)\nprint('')\n\n# Map the all input data's\n# map = all corpus of data, batch =True is means single time train, batchsize not defined\nemotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\n\nprint('Features Name of emotion data: ',emotions_encoded[\"train\"].column_names)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:35:38.688556Z","iopub.execute_input":"2022-05-30T03:35:38.688909Z","iopub.status.idle":"2022-05-30T03:35:41.064697Z","shell.execute_reply.started":"2022-05-30T03:35:38.688856Z","shell.execute_reply":"2022-05-30T03:35:41.064045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3.2. Training a Text Classifier - DistilBERT**\n\n### **Step by step process of training data:**\n\n### ***Token Encoding > Token Embedding > Encoder Stack > Hidden states > Classification head > Predictions***","metadata":{}},{"cell_type":"markdown","source":"### **Two options to train such a model on our Twitter-Emotion dataset:**\n\n**1. Feature extraction** \nWe use the hidden states as features and just train a classifier on them, without modifying the pretrained model.\n\n**2. Fine-tuning**\nWe train the whole model end-to-end, which also updates the parameters of the pretrained model.","metadata":{}},{"cell_type":"markdown","source":"## **3.2.1 Using Pretrained Model - DistilBERT**\n","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModel\n\n#Checkpoints\nmodel_ckpt = \"distilbert-base-uncased\"\n\n#Device selected\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n#Initiate the model\nmodel = AutoModel.from_pretrained(model_ckpt).to(device)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:35:41.066164Z","iopub.execute_input":"2022-05-30T03:35:41.066623Z","iopub.status.idle":"2022-05-30T03:36:14.731973Z","shell.execute_reply.started":"2022-05-30T03:35:41.066583Z","shell.execute_reply":"2022-05-30T03:36:14.731183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extracting last hidden states\n\ntext = \"I am Vk - Best Engineer\"\ninputs = tokenizer(text,return_tensors='pt')\nprint(f\"Input tensor shape: {inputs['input_ids'].size()}\")\nprint('')\n\n#Best model outputs\nprint('Best model outputs: ')\nprint('')\ninputs = {k:v.to(device) for k,v in inputs.items()}\nwith torch.no_grad():\n    outputs = model(**inputs)\nprint(outputs)\nprint('')\n#print last hidden size\nprint('Last hidden size: ')\nprint(outputs.last_hidden_state.size())","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:36:14.733381Z","iopub.execute_input":"2022-05-30T03:36:14.733633Z","iopub.status.idle":"2022-05-30T03:36:15.601013Z","shell.execute_reply.started":"2022-05-30T03:36:14.733599Z","shell.execute_reply":"2022-05-30T03:36:15.600241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Extract the hidden layers\ndef extract_hidden_states(batch):\n    # Place model inputs on the GPU\n    inputs = {k:v.to(device) for k,v in batch.items() \n              if k in tokenizer.model_input_names}\n    # Extract last hidden states\n    with torch.no_grad():\n        last_hidden_state = model(**inputs).last_hidden_state\n    # Return vector for [CLS] token\n    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}\n\n# Since our model expects tensors as inputs, the next thing to do is convert the input_ids and attention_mask columns to the \"torch\" format, as follows:\nemotions_encoded.set_format(\"torch\",\n                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\n\n# extract the hidden states across all splits in one go\nemotions_hidden = emotions_encoded.map(extract_hidden_states,batched=True)\n\nemotions_hidden['train'].column_names","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:36:15.602186Z","iopub.execute_input":"2022-05-30T03:36:15.602701Z","iopub.status.idle":"2022-05-30T03:36:41.575565Z","shell.execute_reply.started":"2022-05-30T03:36:15.602662Z","shell.execute_reply":"2022-05-30T03:36:41.574818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3.2.1.1. Creating a feature matrix**","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nX_train = np.array(emotions_hidden[\"train\"][\"hidden_state\"])\nX_valid = np.array(emotions_hidden['validation'][\"hidden_state\"])\ny_train = np.array(emotions_hidden[\"train\"][\"label\"])\ny_valid = np.array(emotions_hidden[\"validation\"][\"label\"])\nprint(f'Xtrain_shape : {X_train.shape}, X_valid_shape: {X_valid.shape}')","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:36:41.576891Z","iopub.execute_input":"2022-05-30T03:36:41.577189Z","iopub.status.idle":"2022-05-30T03:36:41.786583Z","shell.execute_reply.started":"2022-05-30T03:36:41.577154Z","shell.execute_reply":"2022-05-30T03:36:41.785814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3.2.1.2. Visualize the training set - UMAP-learn**\n\n***UMAP Visualization:***\n\nUniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualisation similarly to t-SNE, but also for general non-linear dimension reduction. The algorithm is founded on three assumptions about the data:\n\n- The data is uniformly distributed on a Riemannian manifold\n- The Riemannian metric is locally constant (or can be approximated as such);\n- The manifold is locally connected.\n\n\n* From these assumptions it is possible to model the manifold with a fuzzy topological structure. The embedding is found by searching for a low dimensional projection of the data that has the closest possible equivalent fuzzy topological structure.","metadata":{}},{"cell_type":"code","source":"!pip install umap-learn","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-30T03:36:41.788036Z","iopub.execute_input":"2022-05-30T03:36:41.788298Z","iopub.status.idle":"2022-05-30T03:36:50.605268Z","shell.execute_reply.started":"2022-05-30T03:36:41.788261Z","shell.execute_reply":"2022-05-30T03:36:50.604373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# in this data hidden state 768 dimensions and featured scale [0,1] interval like MinMaxScale\nfrom umap import UMAP\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Scale features to [0,1] range\nX_scaled = MinMaxScaler().fit_transform(X_train)\n\n# initialize and fit UMAP\nmapper = UMAP(n_components = 2, metric=\"cosine\").fit(X_scaled)\n\n# Create a Dataframe of 2D embeddings\n\ndf_emb = pd.DataFrame(mapper.embedding_,columns=[\"X\",\"Y\"])\ndf_emb[\"label\"] = y_train\ndisplay(df_emb.head())","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:36:50.608363Z","iopub.execute_input":"2022-05-30T03:36:50.608619Z","iopub.status.idle":"2022-05-30T03:37:32.450249Z","shell.execute_reply.started":"2022-05-30T03:36:50.608592Z","shell.execute_reply":"2022-05-30T03:37:32.449496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ***Ploting the data - UMAP***","metadata":{}},{"cell_type":"code","source":"fig,axes = plt.subplots(2, 3, figsize=(7,5))\n\n#1D axises\naxes = axes.flatten()\n\n#Color used\ncmaps = ['Greys', \"Blues\", \"Oranges\", \"Reds\", \"Purples\", \"Greens\"]\n\n#Labels name of emotions\nlabels = emotions[\"train\"].features[\"label\"].names\n\n#separate\nfor i,(label, cmap) in enumerate(zip(labels, cmaps)):\n    df_emb_sub = df_emb.query(f\"label == {i}\")\n    axes[i].hexbin(df_emb_sub[\"X\"], df_emb_sub[\"Y\"], cmap=cmap,\n                  gridsize= 20, linewidths=(0,))\n    axes[i].set_title(label)\n    axes[i].set_xticks([]),axes[i].set_yticks([])\n    \nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-30T03:37:32.451489Z","iopub.execute_input":"2022-05-30T03:37:32.453053Z","iopub.status.idle":"2022-05-30T03:37:32.808648Z","shell.execute_reply.started":"2022-05-30T03:37:32.453013Z","shell.execute_reply":"2022-05-30T03:37:32.80797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3.2.1.3. Training a simple Classifier**\n\n**As a UMAP plot shows one result - What its means separate equally but fear and anger both are much similar compare with others**\n\n## ***Logistic Regression***","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n# Max iteration (3000)\nlr_clf = LogisticRegression(max_iter = 3000)\nlr_clf.fit(X_train, y_train)\nprint('Logistic Regression_Score',lr_clf.score(X_valid, y_valid))","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:37:32.8096Z","iopub.execute_input":"2022-05-30T03:37:32.80983Z","iopub.status.idle":"2022-05-30T03:40:00.69157Z","shell.execute_reply.started":"2022-05-30T03:37:32.809796Z","shell.execute_reply":"2022-05-30T03:40:00.690706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Dummy Classifier**\n\nDummyClassifier makes predictions that ignore the input features, This classifier serves as a simple baseline to compare against other more complex classifiers.","metadata":{}},{"cell_type":"code","source":"#Dummy Classifier why choose means? Multiclass?\n\nfrom sklearn.dummy import DummyClassifier\n\ndummy_clf = DummyClassifier(strategy=\"most_frequent\")\ndummy_clf.fit(X_train,y_train)\nprint('Dummy_Classifier_Score:',dummy_clf.score(X_valid,y_valid))","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:40:00.696011Z","iopub.execute_input":"2022-05-30T03:40:00.698113Z","iopub.status.idle":"2022-05-30T03:40:00.711992Z","shell.execute_reply.started":"2022-05-30T03:40:00.698059Z","shell.execute_reply":"2022-05-30T03:40:00.711028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3.2.1.4. Performance Analysis in Simple Model || Confusion Matrix**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n\ndef plot_confusion_matrix(y_preds, y_true, labels):\n    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n    fig, ax = plt.subplots(figsize=(6, 6))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n    plt.title(\"Normalized confusion matrix\")\n    plt.show()\n\ny_preds = lr_clf.predict(X_valid)\nplot_confusion_matrix(y_preds, y_valid, labels)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-30T03:40:00.713461Z","iopub.execute_input":"2022-05-30T03:40:00.713712Z","iopub.status.idle":"2022-05-30T03:40:01.025949Z","shell.execute_reply.started":"2022-05-30T03:40:00.713679Z","shell.execute_reply":"2022-05-30T03:40:01.025166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Just try simple model and you see the confusion matrix and after i will try fine tuning transformer --> what's different (Its great experience) of see the result.**","metadata":{}},{"cell_type":"markdown","source":"# **3.2.2. Fine-Tuning Transformers**\n\n<img src='https://d3njjcbhbojbot.cloudfront.net/api/utilities/v1/imageproxy/https://d15cw65ipctsrr.cloudfront.net/67/f199f04ecf4fe59ee6f88c8c8e2621/BERT_diagrams.png?auto=format%2Ccompress&dpr=1&w=552&h=414&fit=crop'>","metadata":{}},{"cell_type":"code","source":"#Loading Pretrained Model\n\nfrom transformers import AutoModelForSequenceClassification\n\nnum_labels = 6\n\nmodel = (AutoModelForSequenceClassification.from_pretrained(model_ckpt,num_labels = num_labels).to(device))\n\n\n# Defining the performance Metrics \n\nfrom sklearn.metrics import accuracy_score, f1_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    f1 = f1_score(labels, preds, average=\"weighted\")\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc, \"f1\": f1}","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:40:01.02728Z","iopub.execute_input":"2022-05-30T03:40:01.027704Z","iopub.status.idle":"2022-05-30T03:40:03.784955Z","shell.execute_reply.started":"2022-05-30T03:40:01.027664Z","shell.execute_reply":"2022-05-30T03:40:03.784168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3.2.2.1. Model_Hub**","metadata":{}},{"cell_type":"markdown","source":"## **Configuration**","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\nbatch_size = 12\nlogging_steps = len(emotions_encoded[\"train\"]) // batch_size\nmodel_name = f\"{model_ckpt}-finetuned-emotion\"\ntraining_args = TrainingArguments(output_dir=model_name,\n                                  num_train_epochs=2,\n                                  learning_rate=2e-5,\n                                  per_device_train_batch_size=batch_size,\n                                  per_device_eval_batch_size=batch_size,\n                                  weight_decay=0.01,\n                                  evaluation_strategy=\"epoch\",\n                                  disable_tqdm=False,\n                                  logging_steps=logging_steps,\n                                  push_to_hub=False, \n                                  log_level=\"error\")\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:40:03.789724Z","iopub.execute_input":"2022-05-30T03:40:03.789962Z","iopub.status.idle":"2022-05-30T03:40:03.930628Z","shell.execute_reply.started":"2022-05-30T03:40:03.789933Z","shell.execute_reply":"2022-05-30T03:40:03.929945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Train the Model**","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(model=model, args=training_args,\n                    compute_metrics=compute_metrics,\n                    train_dataset=emotions_encoded[\"train\"],\n                    eval_dataset=emotions_encoded[\"validation\"],\n                    tokenizer=tokenizer)\ntrainer.train();","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:40:03.932Z","iopub.execute_input":"2022-05-30T03:40:03.932292Z","iopub.status.idle":"2022-05-30T03:58:00.735959Z","shell.execute_reply.started":"2022-05-30T03:40:03.932254Z","shell.execute_reply":"2022-05-30T03:58:00.735162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Output predicition**","metadata":{}},{"cell_type":"code","source":"preds_output = trainer.predict(emotions_encoded[\"validation\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:58:00.737868Z","iopub.execute_input":"2022-05-30T03:58:00.738359Z","iopub.status.idle":"2022-05-30T03:58:04.040628Z","shell.execute_reply.started":"2022-05-30T03:58:00.738316Z","shell.execute_reply":"2022-05-30T03:58:04.039811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_output.metrics","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:58:04.042107Z","iopub.execute_input":"2022-05-30T03:58:04.042386Z","iopub.status.idle":"2022-05-30T03:58:04.051037Z","shell.execute_reply.started":"2022-05-30T03:58:04.042352Z","shell.execute_reply":"2022-05-30T03:58:04.050199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds = np.argmax(preds_output.predictions, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:58:04.052249Z","iopub.execute_input":"2022-05-30T03:58:04.052697Z","iopub.status.idle":"2022-05-30T03:58:04.063584Z","shell.execute_reply.started":"2022-05-30T03:58:04.05266Z","shell.execute_reply":"2022-05-30T03:58:04.062604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Confusion Matrix - Compare normal model vs finetune model**","metadata":{}},{"cell_type":"code","source":"plot_confusion_matrix(y_preds, y_valid, labels)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:58:04.064727Z","iopub.execute_input":"2022-05-30T03:58:04.066589Z","iopub.status.idle":"2022-05-30T03:58:04.356013Z","shell.execute_reply.started":"2022-05-30T03:58:04.066533Z","shell.execute_reply":"2022-05-30T03:58:04.355256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3.2.2.2. Error Analysis**","metadata":{}},{"cell_type":"code","source":"from torch.nn.functional import cross_entropy\n\ndef forward_pass_with_label(batch):\n    # Place all input tensors on the same device as the model\n    inputs = {k:v.to(device) for k,v in batch.items() \n              if k in tokenizer.model_input_names}\n\n    with torch.no_grad():\n        output = model(**inputs)\n        pred_label = torch.argmax(output.logits, axis=-1)\n        loss = cross_entropy(output.logits, batch[\"label\"].to(device), \n                             reduction=\"none\")\n\n    # Place outputs on CPU for compatibility with other dataset columns   \n    return {\"loss\": loss.cpu().numpy(), \n            \"predicted_label\": pred_label.cpu().numpy()}","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:58:04.357473Z","iopub.execute_input":"2022-05-30T03:58:04.357948Z","iopub.status.idle":"2022-05-30T03:58:04.367227Z","shell.execute_reply.started":"2022-05-30T03:58:04.357909Z","shell.execute_reply":"2022-05-30T03:58:04.366302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_output\n# Convert our dataset back to PyTorch tensors\nemotions_encoded.set_format(\"torch\", \n                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\n# Compute loss values\nemotions_encoded[\"validation\"] = emotions_encoded[\"validation\"].map(\n    forward_pass_with_label, batched=True, batch_size=16)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:58:04.36834Z","iopub.execute_input":"2022-05-30T03:58:04.368724Z","iopub.status.idle":"2022-05-30T03:58:07.689147Z","shell.execute_reply.started":"2022-05-30T03:58:04.368688Z","shell.execute_reply":"2022-05-30T03:58:07.688455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3.2.2.3. Generate output in Dataframe format**  ","metadata":{}},{"cell_type":"code","source":"emotions_encoded.set_format(\"pandas\")\ncols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\ndf_test = emotions_encoded[\"validation\"][:][cols]\ndf_test[\"label\"] = df_test[\"label\"].apply(label_int2str)\ndf_test[\"predicted_label\"] = (df_test[\"predicted_label\"]\n                              .apply(label_int2str))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:58:07.692731Z","iopub.execute_input":"2022-05-30T03:58:07.693394Z","iopub.status.idle":"2022-05-30T03:58:07.741154Z","shell.execute_reply.started":"2022-05-30T03:58:07.693354Z","shell.execute_reply":"2022-05-30T03:58:07.740338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_output\ndf_test.sort_values(\"loss\", ascending=False).head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:58:07.752522Z","iopub.execute_input":"2022-05-30T03:58:07.756158Z","iopub.status.idle":"2022-05-30T03:58:07.780304Z","shell.execute_reply.started":"2022-05-30T03:58:07.756064Z","shell.execute_reply":"2022-05-30T03:58:07.779536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_output\ndf_test.sort_values(\"loss\", ascending=True).head(10)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T03:58:07.786706Z","iopub.execute_input":"2022-05-30T03:58:07.78716Z","iopub.status.idle":"2022-05-30T03:58:07.820536Z","shell.execute_reply.started":"2022-05-30T03:58:07.787121Z","shell.execute_reply":"2022-05-30T03:58:07.819845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **‚≠êÔ∏èThankyou for visiting Guys!‚≠êÔ∏è**\n\nReference: Full credit of this notebook\n\n1. NLP with transformer book\n2. https://huggingface.co/course/\n\n## **\"if you see any errors and your opinion! feel free to share with me\"**\n## **‚≠êÔ∏è...NextProcess_ComingSoon_NextProcess...‚≠êÔ∏è**\n\n","metadata":{}}]}