{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport datasets, transformers\n\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nimport pandas as pd\nimport numpy as np\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nclass CFG:\n    input_path = '../input/us-patent-phrase-to-phrase-matching/'\n    model_path = ['../input/deberta-v3-5folds/',\n                  '../input/bert-for-patent-5fold/', \n                  '../input/deberta-large-v1/',\n                 ]\n    model_num = 3\n    num_fold = 5\n\ntitles = pd.read_csv('../input/cpc-codes/titles.csv')\n\ntest = pd.read_csv(f\"{CFG.input_path}test.csv\")\ntest = test.merge(titles, left_on='context', right_on='code')\ntest['input'] = test['title']+'[SEP]'+test['anchor']\ntest = test.drop(columns=[\"context\", \"code\", \"class\", \"subclass\", \"group\", \"main_group\", \"anchor\", \"title\", \"section\"])\n\npredictions = []\nweights = [0.5, 0.3, 0.2]\n\nfor i in range (CFG.model_num):   \n    tokenizer = AutoTokenizer.from_pretrained(f'{CFG.model_path[i]}fold0')\n\n    def process_test(unit):\n            return {\n            **tokenizer( unit['input'], unit['target'])\n        }\n    \n    def process_valid(unit):\n        return {\n        **tokenizer( unit['input'], unit['target']),\n        'label': unit['score']\n    }\n    \n    test_ds = datasets.Dataset.from_pandas(test)\n    test_ds = test_ds.map(process_test, remove_columns=['id', 'target', 'input', '__index_level_0__'])\n\n    for fold in range(CFG.num_fold):        \n        model = AutoModelForSequenceClassification.from_pretrained(f'{CFG.model_path[i]}fold{fold}', \n                                                                   num_labels=1)\n        trainer = Trainer(\n                model,\n                tokenizer=tokenizer,\n            )\n        \n        outputs = trainer.predict(test_ds)\n        prediction = outputs.predictions.reshape(-1) * (weights[i] / 5)\n        predictions.append(prediction)\n        \npredictions = np.sum(predictions, axis=0)\n\nsubmission_1 = datasets.Dataset.from_dict({\n    'id': test['id'],\n    'score': predictions,\n})","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:43:02.540921Z","iopub.execute_input":"2022-06-19T09:43:02.541629Z","iopub.status.idle":"2022-06-19T09:47:46.089173Z","shell.execute_reply.started":"2022-06-19T09:43:02.541532Z","shell.execute_reply":"2022-06-19T09:47:46.088297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_1 = submission_1.to_pandas()\nfrom sklearn.preprocessing import MinMaxScaler\nmm = MinMaxScaler()\nsubmission_1['score'] = mm.fit_transform(np.array(submission_1['score']).reshape(-1, 1))","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:47:46.091492Z","iopub.execute_input":"2022-06-19T09:47:46.091896Z","iopub.status.idle":"2022-06-19T09:47:46.125945Z","shell.execute_reply.started":"2022-06-19T09:47:46.091849Z","shell.execute_reply":"2022-06-19T09:47:46.125059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_1","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:47:46.127738Z","iopub.execute_input":"2022-06-19T09:47:46.128081Z","iopub.status.idle":"2022-06-19T09:47:46.150886Z","shell.execute_reply.started":"2022-06-19T09:47:46.128037Z","shell.execute_reply":"2022-06-19T09:47:46.149811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport random\n\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom dataclasses import dataclass\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AutoModel\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True    \n    torch.backends.cudnn.benchmark = False\n\n    \ndef inference_fn(test_loader, model, device, is_sigmoid=True):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    \n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n            \n        with torch.no_grad():\n            output = model(inputs)\n        \n        if is_sigmoid == True:\n            preds.append(output.sigmoid().to('cpu').numpy())\n        else:\n            preds.append(output.to('cpu').numpy())\n\n    return np.concatenate(preds)    \n    \n\ndef upd_outputs(data, is_trim=False, is_minmax=True, is_reshape=False):\n    min_max_scaler = MinMaxScaler()\n    \n    if is_trim == True:\n        data = np.where(data <=0.01, 0, data)\n        data = np.where(data >=0.99, 1, data)\n\n    if is_minmax ==True:\n        data = min_max_scaler.fit_transform(data)\n    \n    if is_reshape == True:\n        data = data.reshape(-1)\n        \n    return data\n\npd.set_option('display.precision', 4)\ncm = sns.light_palette('green', as_cmap=True)\nprops_param = \"color:white; font-weight:bold; background-color:green;\"\n\nCUSTOM_SEED = 42\nCUSTOM_BATCH = 24\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ncompetition_dir = \"../input/us-patent-phrase-to-phrase-matching/\"\n\nsubmission = pd.read_csv(competition_dir+'sample_submission.csv')\ntest_origin = pd.read_csv(competition_dir+'test.csv')\ndef prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           truncation=True)\n    \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg        \n        self.text = df['text'].values\n        \n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.text[item])\n        \n        return inputs\n   \n    \nclass CustomModel(nn.Module):\n    def __init__(self, model_path):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(model_path)\n        config.num_labels = 1\n        self.base = AutoModelForSequenceClassification.from_config(config=config)\n        dim = config.hidden_size\n        self.dropout = nn.Dropout(p=0)\n        self.cls = nn.Linear(dim,1)\n        \n    def forward(self, inputs):\n        output = self.base(**inputs)\n\n        return output[0]\n    \nseed_everything(CUSTOM_SEED)\n\nclass CFG:\n    model_path='../input/deberta-v3-large/deberta-v3-large'\n    batch_size=CUSTOM_BATCH\n    num_workers=2\n    max_len=130\n    trn_fold=[0, 1, 2, 3]\n\nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)\n\ncontext_mapping = torch.load(\"../input/folds-dump-the-two-paths-fix/cpc_texts.pth\")\n\ntest = test_origin.copy()\ntitles = pd.read_csv('../input/cpc-codes/titles.csv')\n\ntest.reset_index(inplace=True)\ntest = test.merge(titles, left_on='context', right_on='code')\ntest.sort_values(by='index', inplace=True)\ntest.drop(columns='index', inplace=True)\n\ntest['context_text'] = test['context'].map(context_mapping)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\ntest['text'] = test['text'].apply(str.lower)\n\ntest.head()\n\ndeberta_predicts_1 = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_dataloader = DataLoader(test_dataset,\n                             batch_size=CFG.batch_size, shuffle=False,\n                             num_workers=CFG.num_workers,\n                             pin_memory=True, drop_last=False)\n\ndeberta_simple_path = \"../input/us-patent-deberta-simple/microsoft_deberta-v3-large\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f\"{deberta_simple_path}_best{fold}.pth\"\n    \n    model = CustomModel(CFG.model_path)    \n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state['model'])\n    \n    prediction = inference_fn(test_dataloader, model, DEVICE, is_sigmoid=False)\n    \n    deberta_predicts_1.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    # -------------- inference_fn([...], is_sigmoid=False)\ndeberta_predicts_1 = [upd_outputs(x, is_minmax=True, is_reshape=True) for x in deberta_predicts_1]\ndeberta_predicts_1 = pd.DataFrame(deberta_predicts_1).T\ndeberta_predicts_1['med'] = deberta_predicts_1.median(axis=1)\ndeberta_predicts_1['minmax']= (deberta_predicts_1.max(axis=1) + deberta_predicts_1.min(axis=1)) /2\ndeberta_predicts_1['quantile'] = (deberta_predicts_1.quantile(0.25, axis=1) + deberta_predicts_1.quantile(0.75,axis=1)) /2\n\ndel test, test_dataset\ngc.collect()\n\ndef prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    \n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n\n    \nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n            \n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self._init_weights(self.attention)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        \n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        \n        return output\n    \nseed_everything(CUSTOM_SEED)\n\nclass CFG:\n    num_workers=2\n    path=\"../input/pppm-deberta-v3-large-baseline-w-w-b-train/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v3-large\"\n    batch_size=CUSTOM_BATCH\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    trn_fold=[0, 1, 2, 3]\n    \nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n\ncontext_mapping = torch.load(CFG.path+\"cpc_texts.pth\")\n\ntest = test_origin.copy()\n\ntest['context_text'] = test['context'].map(context_mapping)\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n\ntest.head()\n\ndeberta_predicts_2 = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers,\n                         pin_memory=True, drop_last=False)\n\nfolds_path = CFG.path + f\"{CFG.model.replace('/', '-')}\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f\"{folds_path}_fold{fold}_best.pth\"\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state['model'])\n    \n    prediction = inference_fn(test_loader, model, DEVICE)\n    deberta_predicts_2.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()\n    gc.collect()\n    \ndeberta_predicts_2 = [upd_outputs(x, is_reshape=True) for x in deberta_predicts_2]\ndeberta_predicts_2 = pd.DataFrame(deberta_predicts_2).T\n\ndeberta_predicts_2['med'] = deberta_predicts_2.median(axis=1)\ndeberta_predicts_2['minmax']= (deberta_predicts_2.max(axis=1) + deberta_predicts_1.min(axis=1)) /2\ndeberta_predicts_2['quantile'] = (deberta_predicts_2.quantile(0.25, axis=1) + deberta_predicts_2.quantile(0.75,axis=1)) /2\n\ndeberta_predicts_2.head(10).style.background_gradient(cmap=cm, axis=1)\ndel test, test_dataset\ngc.collect()\n\ndef prepare_input(cfg, text, target):\n    inputs = cfg.tokenizer(text, target,\n                           padding=\"max_length\",\n                           max_length=cfg.max_len,\n                           truncation=True)\n\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n        self.target = df['target'].values\n        \n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = self.texts[item]\n        target = self.target[item]\n        \n        inputs = prepare_input(self.cfg, text, target)\n        \n        return inputs\n\n    \nclass CustomModel(nn.Module):\n    def __init__(self):\n        super(CustomModel, self).__init__()\n        hidden_dropout_prob: float = 0.1\n        layer_norm_eps: float = 1e-7\n\n        config = AutoConfig.from_pretrained(CFG.config_path)\n\n        config.update({\"output_hidden_states\": True,\n                       \"hidden_dropout_prob\": hidden_dropout_prob,\n                       \"layer_norm_eps\": layer_norm_eps,\n                       \"add_pooling_layer\": False})\n        \n        self.transformer = AutoModel.from_pretrained(CFG.config_path, config=config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.dropout1 = nn.Dropout(0.1)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dropout3 = nn.Dropout(0.3)\n        self.dropout4 = nn.Dropout(0.4)\n        self.dropout5 = nn.Dropout(0.5)\n        self.output = nn.Linear(config.hidden_size, CFG.num_targets)\n        \n    def forward(self, inputs):\n        transformer_out = self.transformer(**inputs)\n        last_hidden_states = transformer_out[0]\n        last_hidden_states = self.dropout(torch.mean(last_hidden_states, 1))\n        logits1 = self.output(self.dropout1(last_hidden_states))\n        logits2 = self.output(self.dropout2(last_hidden_states))\n        logits3 = self.output(self.dropout3(last_hidden_states))\n        logits4 = self.output(self.dropout4(last_hidden_states))\n        logits5 = self.output(self.dropout5(last_hidden_states))\n        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n        \n        return logits\n    \nseed_everything(CUSTOM_SEED)\n\n@dataclass(frozen=True)\nclass CFG:\n    num_workers=2\n    config_path='../input/robertalarge'\n    model_path='../input/phrase-matching-roberta-training-pytorch-wandb'\n    model_name='roberta-large'\n    batch_size=CUSTOM_BATCH\n    max_len=128\n    num_targets=1\n    trn_fold=[0, 1, 2, 3, 4]\n    tokenizer=AutoTokenizer.from_pretrained('../input/robertalarge')\n\ncontext_mapping = {\n        \"A\": \"Human Necessities\",\n        \"B\": \"Operations and Transport\",\n        \"C\": \"Chemistry and Metallurgy\",\n        \"D\": \"Textiles\",\n        \"E\": \"Fixed Constructions\",\n        \"F\": \"Mechanical Engineering\",\n        \"G\": \"Physics\",\n        \"H\": \"Electricity\",\n        \"Y\": \"Emerging Cross-Sectional Technologies\",\n}\n\ntest = test_origin.copy()\n\ntest['context_text'] = test['context'].str.slice(stop=1).map(context_mapping)\ntest['text'] = test['context_text'] + ' ' + test['anchor']\n\nroberta_predicts = []\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers,\n                         pin_memory=True, drop_last=False)\n\nfolds_path = CFG.model_path + f\"/{CFG.model_name.replace('-','_')}\"\n\nfor fold in CFG.trn_fold:\n    fold_path = f\"{folds_path}_patent_model_{fold}.pth\"\n    \n    model = CustomModel()\n    state = torch.load(fold_path, map_location=torch.device('cpu'))  # DEVICE\n    model.load_state_dict(state)\n\n    prediction = inference_fn(test_loader, model, DEVICE)\n    roberta_predicts.append(prediction)\n    \n    del model, state, prediction\n    torch.cuda.empty_cache()    \n    gc.collect()\n    \nroberta_predicts = [upd_outputs(x, is_reshape=True) for x in roberta_predicts]\nroberta_predicts = pd.DataFrame(roberta_predicts).T\nroberta_predicts['med'] = roberta_predicts.median(axis=1)\nroberta_predicts['minmax']= (roberta_predicts.max(axis=1) + roberta_predicts.min(axis=1)) /2\nroberta_predicts['quantile'] = (roberta_predicts.quantile(0.25, axis=1) + roberta_predicts.quantile(0.75,axis=1)) /2\n\ndel test, test_dataset\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:47:46.154038Z","iopub.execute_input":"2022-06-19T09:47:46.154424Z","iopub.status.idle":"2022-06-19T09:53:05.677223Z","shell.execute_reply.started":"2022-06-19T09:47:46.154383Z","shell.execute_reply":"2022-06-19T09:53:05.676171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_predictions = pd.concat(\n    [deberta_predicts_1, deberta_predicts_2, roberta_predicts],\n    keys=['deberta 1', 'deberta 2', 'roberta'],\n    axis=1\n)\n\nall_predictions.head(10) \\\n    .assign(mean=lambda x: x.mean(axis=1)) \\\n        .style.background_gradient(cmap=cm, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:53:05.678897Z","iopub.execute_input":"2022-06-19T09:53:05.68032Z","iopub.status.idle":"2022-06-19T09:53:05.865979Z","shell.execute_reply.started":"2022-06-19T09:53:05.680261Z","shell.execute_reply":"2022-06-19T09:53:05.865027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_mean = pd.DataFrame({\n    'deberta 1': deberta_predicts_1.mean(axis=1),\n    'deberta 2': deberta_predicts_2.mean(axis=1),\n    'roberta': roberta_predicts.mean(axis=1),\n})\n\n\nfinal_predictions = pd.DataFrame()\nweights_ = [0.33, 0.33, 0.33]\nfinal_predictions['N11'] = all_mean.mul(weights_).sum(axis=1)\n\n# === N2 ===\nfinal_predictions['N22'] = all_mean.median(axis=1)\nfinal_predictions['N33'] = all_mean.mean(axis=1)\n\nfp = final_predictions.mean(axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:53:05.867532Z","iopub.execute_input":"2022-06-19T09:53:05.868453Z","iopub.status.idle":"2022-06-19T09:53:05.885398Z","shell.execute_reply.started":"2022-06-19T09:53:05.868405Z","shell.execute_reply":"2022-06-19T09:53:05.88414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_2 = pd.DataFrame({\n    'id': test_origin['id'],\n    'score': fp,\n})","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:53:05.887573Z","iopub.execute_input":"2022-06-19T09:53:05.887917Z","iopub.status.idle":"2022-06-19T09:53:05.895404Z","shell.execute_reply.started":"2022-06-19T09:53:05.88787Z","shell.execute_reply":"2022-06-19T09:53:05.894253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mix_scores = submission_1.merge(submission_2, on='id')","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:53:05.897016Z","iopub.execute_input":"2022-06-19T09:53:05.898031Z","iopub.status.idle":"2022-06-19T09:53:05.91383Z","shell.execute_reply.started":"2022-06-19T09:53:05.897983Z","shell.execute_reply":"2022-06-19T09:53:05.912586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final = 0.6 * mix_scores ['score_x'] + 0.4 * mix_scores['score_y']\nsubmission_final_1 = pd.DataFrame({\n    'id': mix_scores ['id'],\n    'score': final,\n})","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:53:05.915553Z","iopub.execute_input":"2022-06-19T09:53:05.915985Z","iopub.status.idle":"2022-06-19T09:53:05.927666Z","shell.execute_reply.started":"2022-06-19T09:53:05.915936Z","shell.execute_reply":"2022-06-19T09:53:05.926549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport torch\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:53:05.933402Z","iopub.execute_input":"2022-06-19T09:53:05.933654Z","iopub.status.idle":"2022-06-19T09:53:06.145785Z","shell.execute_reply.started":"2022-06-19T09:53:05.933622Z","shell.execute_reply":"2022-06-19T09:53:06.144583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport datasets, transformers\n\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport pandas as pd\nimport numpy as np\n\nimport gc\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nclass CFG:\n    input_path = '../input/us-patent-phrase-to-phrase-matching/'\n    model_path = '../input/debertav3uspppm0/deberta-v3-large-1/checkpoint-1020/',\n    \n    \ntitles = pd.read_csv('../input/cpc-codes/titles.csv')\n\ntest = pd.read_csv(f\"{CFG.input_path}test.csv\")\ntest = test.merge(titles, left_on='context', right_on='code')\ntest['input'] = test['title']+'[SEP]'+test['anchor']\ntest = test.drop(columns=[\"context\", \"code\", \"class\", \"subclass\", \"group\", \"main_group\", \"anchor\", \"title\", \"section\"])\n\n\npredictions = []\n\n\ntokenizer = AutoTokenizer.from_pretrained('../input/debertav3uspppm0/uspppm_4')\n\ndef process_test(unit):\n        return {\n        **tokenizer( unit['input'], unit['target'])\n    }\n\ndef process_valid(unit):\n    return {\n    **tokenizer( unit['input'], unit['target']),\n    'label': unit['score']\n}\n\ntest_ds = datasets.Dataset.from_pandas(test)\ntest_ds = test_ds.map(process_test, remove_columns=['id', 'target', 'input', '__index_level_0__'])\n\n     \nmodel = AutoModelForSequenceClassification.from_pretrained('../input/debertav3uspppm0/deberta-v3-large-1/checkpoint-1020/', \n                                                               num_labels=1)\ntrainer = Trainer(\n        model,\n        tokenizer=tokenizer,\n    )\n\noutputs = trainer.predict(test_ds)\n\npredictions = outputs.predictions.reshape(-1)\n\nsubmission_4 = datasets.Dataset.from_dict({\n    'id': test['id'],\n    'score': predictions,\n})","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:53:06.148238Z","iopub.execute_input":"2022-06-19T09:53:06.148937Z","iopub.status.idle":"2022-06-19T09:53:33.063872Z","shell.execute_reply.started":"2022-06-19T09:53:06.148844Z","shell.execute_reply":"2022-06-19T09:53:33.062892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_4 = submission_4.to_pandas()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:53:33.065372Z","iopub.execute_input":"2022-06-19T09:53:33.06593Z","iopub.status.idle":"2022-06-19T09:53:33.074856Z","shell.execute_reply.started":"2022-06-19T09:53:33.065894Z","shell.execute_reply":"2022-06-19T09:53:33.073768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:53:33.076621Z","iopub.execute_input":"2022-06-19T09:53:33.077096Z","iopub.status.idle":"2022-06-19T09:53:33.368373Z","shell.execute_reply.started":"2022-06-19T09:53:33.077047Z","shell.execute_reply":"2022-06-19T09:53:33.367022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/tez-lib/\")\n\nimport torch\n\nimport pandas as pd\nimport torch.nn as nn\n\nfrom scipy import stats\nfrom tez import Tez, TezConfig\nfrom tez.callbacks import EarlyStopping\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer, get_linear_schedule_with_warmup\n\nclass args:\n    model = \"../input/anferico-bert-for-patents/\"\n    max_len = 32\n    accumulation_steps = 1\n    batch_size = 64\n    epochs = 5\n    learning_rate = 2e-5\n\nclass PhraseDataset:\n    def __init__(self, anchor, target, context, tokenizer, max_len):\n        self.anchor = anchor\n        self.target = target\n        self.context = context\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.anchor)\n\n    def __getitem__(self, item):\n        anchor = self.anchor[item]\n        context = self.context[item]\n        target = self.target[item]\n\n        encoded_text = self.tokenizer.encode_plus(\n            context + \" \" + anchor,\n            target,\n            padding=\"max_length\",\n            max_length=self.max_len,\n            truncation=True,\n        )\n        input_ids = encoded_text[\"input_ids\"]\n        attention_mask = encoded_text[\"attention_mask\"]\n        token_type_ids = encoded_text[\"token_type_ids\"]\n\n        return {\n            \"ids\": torch.tensor(input_ids, dtype=torch.long),\n            \"mask\": torch.tensor(attention_mask, dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n        }\n    \nclass PhraseModel(nn.Module):\n    def __init__(self, model_name):\n        super().__init__()\n        self.model_name = model_name\n\n        config = AutoConfig.from_pretrained(model_name)\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"add_pooling_layer\": True,\n                \"num_labels\": 1,\n            }\n        )\n        self.transformer = AutoModel.from_pretrained(model_name, config=config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.output = nn.Linear(config.hidden_size, 1)\n\n    def forward(self, ids, mask, token_type_ids):\n        transformer_out = self.transformer(ids, mask, token_type_ids)\n        output = transformer_out.pooler_output\n        output = self.dropout(output)\n        output = self.output(output)\n        return output, 0, {}\n    \ndf = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/test.csv\")\n\ncontext_mapping = {\n    \"A\": \"Human Necessities\",\n    \"B\": \"Operations and Transport\",\n    \"C\": \"Chemistry and Metallurgy\",\n    \"D\": \"Textiles\",\n    \"E\": \"Fixed Constructions\",\n    \"F\": \"Mechanical Engineering\",\n    \"G\": \"Physics\",\n    \"H\": \"Electricity\",\n    \"Y\": \"Emerging Cross-Sectional Technologies\",\n}\n\ndf.context = df.context.apply(lambda x: context_mapping[x[0]])\n\ntokenizer = AutoTokenizer.from_pretrained(args.model)\ntest_dataset = PhraseDataset(\n    anchor=df.anchor.values,\n    target=df.target.values,\n    context=df.context.values,\n    tokenizer=tokenizer,\n    max_len=args.max_len,\n)\n\nmodel = PhraseModel(model_name=args.model)\nmodel = Tez(model)\nmodel_path = \"../input/uspppm-tez-models/model_f0.bin\"\nconfig = TezConfig(\n    test_batch_size=64,\n    device=\"cuda\",\n)\nmodel.load(model_path, weights_only=True, config=config)\n\npreds_iter = model.predict(test_dataset)\nfinal_preds = []\nfor preds in preds_iter:\n    preds[preds < 0] = 0\n    preds[preds > 1] = 1\n    final_preds.extend(preds.ravel().tolist())\n    \nsubmission_5 = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/sample_submission.csv\")\nsubmission_5.score = final_preds","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:53:33.370833Z","iopub.execute_input":"2022-06-19T09:53:33.371423Z","iopub.status.idle":"2022-06-19T09:54:04.27815Z","shell.execute_reply.started":"2022-06-19T09:53:33.371372Z","shell.execute_reply":"2022-06-19T09:54:04.276939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:54:04.280398Z","iopub.execute_input":"2022-06-19T09:54:04.28075Z","iopub.status.idle":"2022-06-19T09:54:04.373346Z","shell.execute_reply.started":"2022-06-19T09:54:04.280694Z","shell.execute_reply":"2022-06-19T09:54:04.372179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:54:04.376086Z","iopub.execute_input":"2022-06-19T09:54:04.376402Z","iopub.status.idle":"2022-06-19T09:54:04.636579Z","shell.execute_reply.started":"2022-06-19T09:54:04.376368Z","shell.execute_reply":"2022-06-19T09:54:04.635489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import Trainer\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nmodel = AutoModelForSequenceClassification.from_pretrained('../input/patent-phrase-matching/patent_phrase/checkpoint-1026', num_labels=5, local_files_only=False)\n\ntokenizer = AutoTokenizer.from_pretrained('../input/patent-phrase-matching/patent_phrase/checkpoint-1026')\n\nclass MyDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n    \ndef encode_row(row, test=False):\n    ret = tokenizer(row['context'][0] + ' ' + row['anchor'], row['target'])\n    if not test:\n        ret['label'] = np.digitize(row['score'], bins=np.linspace(0, 1, 5)) - 1\n    \n    return ret\n\ntest_df = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/test.csv\")\ntest_data = [encode_row(row, test=True) for _, row in test_df.iterrows()]\ntestset = MyDataset(test_data)\n\ntrainer = Trainer(model,tokenizer=tokenizer)\n\noutputs = trainer.predict(testset)\n\ndel model\ntorch.cuda.empty_cache()\n\nprob = np.exp(outputs.predictions)\nprob = prob / np.sum(prob, axis=1, keepdims=True)\npred = prob * np.linspace(0, 1, 5)\npred = np.sum(pred, axis=1)\n\nsubmission_6 = pd.DataFrame({'id': test_df['id'], 'score': pred})","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:54:04.638256Z","iopub.execute_input":"2022-06-19T09:54:04.639375Z","iopub.status.idle":"2022-06-19T09:54:25.206486Z","shell.execute_reply.started":"2022-06-19T09:54:04.639288Z","shell.execute_reply":"2022-06-19T09:54:25.205473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:54:25.208455Z","iopub.execute_input":"2022-06-19T09:54:25.208786Z","iopub.status.idle":"2022-06-19T09:54:25.434526Z","shell.execute_reply.started":"2022-06-19T09:54:25.208741Z","shell.execute_reply":"2022-06-19T09:54:25.433494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport datasets, transformers\n\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport pandas as pd\nimport numpy as np\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nclass CFG:\n    input_path = '../input/us-patent-phrase-to-phrase-matching/'\n    model_path = ['../input/xlm-roberta-large-5folds/',\n                 ]\n    model_num = 1\n    num_fold = 5\n    \ntitles = pd.read_csv('../input/cpc-codes/titles.csv')\n\ntest = pd.read_csv(f\"{CFG.input_path}test.csv\")\ntest = test.merge(titles, left_on='context', right_on='code')\ntest['input'] = test['title']+'[SEP]'+test['anchor']\ntest = test.drop(columns=[\"context\", \"code\", \"class\", \"subclass\", \"group\", \"main_group\", \"anchor\", \"title\", \"section\"])\n\npredictions = []\nweights = [1]\n\nfor i in range (CFG.model_num):   \n    tokenizer = AutoTokenizer.from_pretrained(f'{CFG.model_path[i]}fold0')\n\n    def process_test(unit):\n            return {\n            **tokenizer( unit['input'], unit['target'])\n        }\n    \n    def process_valid(unit):\n        return {\n        **tokenizer( unit['input'], unit['target']),\n        'label': unit['score']\n    }\n    \n    test_ds = datasets.Dataset.from_pandas(test)\n    test_ds = test_ds.map(process_test, remove_columns=['id', 'target', 'input', '__index_level_0__'])\n\n    for fold in range(CFG.num_fold):        \n        model = AutoModelForSequenceClassification.from_pretrained(f'{CFG.model_path[i]}fold{fold}', \n                                                                   num_labels=1)\n        trainer = Trainer(\n                model,\n                tokenizer=tokenizer,\n            )\n        \n        outputs = trainer.predict(test_ds)\n        prediction = outputs.predictions.reshape(-1) * (weights[i] / 8)\n        predictions.append(prediction)\n        del model\n        torch.cuda.empty_cache()\n        \n    current_model_predictions = np.array(predictions[-5::])\n    predictions.append(np.median(current_model_predictions, axis=0))\n    predictions.append((np.max(current_model_predictions, axis=0) + np.min(current_model_predictions, axis=0)) / 2)\n    predictions.append((np.quantile(current_model_predictions,0.75, axis=0) + np.quantile(current_model_predictions,0.25, axis=0)) / 2)\n                    \npredictions = np.sum(predictions, axis=0)\n\nsubmission_xlm = datasets.Dataset.from_dict({\n    'id': test['id'],\n    'score': predictions,\n})\nsubmission_xlm = submission_xlm.to_pandas()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:54:25.436395Z","iopub.execute_input":"2022-06-19T09:54:25.436934Z","iopub.status.idle":"2022-06-19T09:56:35.197344Z","shell.execute_reply.started":"2022-06-19T09:54:25.436887Z","shell.execute_reply":"2022-06-19T09:56:35.196354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:56:35.199844Z","iopub.execute_input":"2022-06-19T09:56:35.200191Z","iopub.status.idle":"2022-06-19T09:56:35.546754Z","shell.execute_reply.started":"2022-06-19T09:56:35.200146Z","shell.execute_reply":"2022-06-19T09:56:35.545456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_DIR = '../input/us-patent-phrase-to-phrase-matching/'\nOUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n \n    import os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport shutil\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nprint(f\"torch.__version__: {torch.__version__}\")\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nos.system('pip uninstall -y transformers')\nos.system('pip uninstall -y tokenizers')\nos.system('python -m pip install --no-index --find-links=../input/pppm-pip-wheels-dataset transformers')\nos.system('python -m pip install --no-index --find-links=../input/pppm-pip-wheels-dataset tokenizers')\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n%env TOKENIZERS_PARALLELISM=true\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass CFG:\n    num_workers=4\n    path=\"../input/electrav1/\"\n    config_path=path+'config.pth'\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]\n    tokenizer = AutoTokenizer.from_pretrained('../input/electrav1/tokenizer/tokenizer/')   \n    \ndef get_score(y_true, y_pred):\n    score = sp.stats.pearsonr(y_true, y_pred)[0]\n    return score\n\n\ndef get_logger(filename=OUTPUT_DIR+'train'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)\n\ntest = pd.read_csv(INPUT_DIR+'test.csv')\nsubmission_electra = pd.read_csv(INPUT_DIR+'sample_submission.csv')\n\ncpc_texts = torch.load(\"../input/pppm-deberta-v3-large-baseline-w-w-b-train/\"+\"cpc_texts.pth\")\ntest['context_text'] = test['context'].map(cpc_texts)\n\ntest['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n\ndef prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n    \n    \nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self.layer_norm1 = nn.LayerNorm(self.config.hidden_size)\n        self._init_weights(self.attention)\n        self.linear = nn.Linear(self.config.hidden_size, 1)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        out = sum_embeddings / sum_mask\n        \n        out = self.layer_norm1(out)\n        output = self.fc(out)\n        \n        \n        return output\n    \ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:56:35.54952Z","iopub.execute_input":"2022-06-19T09:56:35.549976Z","iopub.status.idle":"2022-06-19T09:57:05.107162Z","shell.execute_reply.started":"2022-06-19T09:56:35.549931Z","shell.execute_reply":"2022-06-19T09:57:05.105911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    num_workers=4\n    path=\"../input/electrav1/\"\n    config_path=path+'config.pth'\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]\n    tokenizer = AutoTokenizer.from_pretrained('../input/electrav1/tokenizer/tokenizer/')\n    \n    \ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in range(4):\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(f'../input/electrav1/google-electra-large-discriminator_fold{fold}_best/google-electra-large-discriminator_fold{fold}_best.pth',\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\n    \npredictions.append(np.median(predictions, axis=0))\npredictions.append((np.max(predictions, axis=0) + np.min(predictions, axis=0)) / 2)\npredictions.append((np.quantile(predictions, 0.75, axis=0) + np.quantile(predictions,0.25, axis=0)) / 2)\np2 = np.mean(predictions, axis=0)\nsubmission_electra['score'] = p2","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:57:05.109088Z","iopub.execute_input":"2022-06-19T09:57:05.109713Z","iopub.status.idle":"2022-06-19T09:59:06.153429Z","shell.execute_reply.started":"2022-06-19T09:57:05.109665Z","shell.execute_reply":"2022-06-19T09:59:06.152093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_electra","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:59:06.155875Z","iopub.execute_input":"2022-06-19T09:59:06.156247Z","iopub.status.idle":"2022-06-19T09:59:06.192183Z","shell.execute_reply.started":"2022-06-19T09:59:06.156194Z","shell.execute_reply":"2022-06-19T09:59:06.190252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:59:06.193653Z","iopub.execute_input":"2022-06-19T09:59:06.194325Z","iopub.status.idle":"2022-06-19T09:59:06.465988Z","shell.execute_reply.started":"2022-06-19T09:59:06.194216Z","shell.execute_reply":"2022-06-19T09:59:06.464579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AutoModel\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nclass CFG_DEB_SIMPLE:\n    input_path = '../input/us-patent-phrase-to-phrase-matching/'\n    model_path = '../input/deberta-v3-large/deberta-v3-large'\n    batch_size = 24\n    num_workers = 2\n    num_fold = 4\n    max_input_length = 130\n    \n    \n\nclass TestDataset(Dataset):\n    def __init__(self, df, tokenizer, max_input_length):\n        self.text = df['text'].values.astype(str)\n        self.tokenizer = tokenizer\n        self.max_input_length = max_input_length\n        \n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        inputs = self.text[item]\n        \n        inputs = self.tokenizer(inputs,\n                    max_length=self.max_input_length,\n                    padding='max_length',\n                    truncation=True)\n        \n        return torch.as_tensor(inputs['input_ids'], dtype=torch.long), \\\n               torch.as_tensor(inputs['token_type_ids'], dtype=torch.long), \\\n               torch.as_tensor(inputs['attention_mask'], dtype=torch.long)\n    \n    \nclass Custom_Bert_Simple(nn.Module):\n    def __init__(self, model_path):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(model_path)\n        config.num_labels = 1\n        self.base = AutoModelForSequenceClassification.from_config(config=config)\n        dim = config.hidden_size\n        self.dropout = nn.Dropout(p=0)\n        self.cls = nn.Linear(dim,1)\n        \n    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n        base_output = self.base(input_ids=input_ids,\n                                attention_mask=attention_mask,\n                                token_type_ids=token_type_ids\n        )\n\n        return base_output[0]\n    \n\ndef valid_fn(valid_loader, model, device):\n    model.eval()\n    preds = []\n    labels = []\n    \n    for step, batch in enumerate(valid_loader):\n        input_ids, token_type_ids, attention_mask = [i.to(device) for i in batch]\n    \n        with torch.no_grad():\n            y_preds = model(input_ids, attention_mask, token_type_ids)\n        \n        preds.append(y_preds.to('cpu').numpy())\n    \n    predictions = np.concatenate(preds)\n    \n    return predictions\n\n\nmin_max_scaler = MinMaxScaler()\n\ndef upd_outputs(data, is_trim=True, is_minmax=True, is_reshape=True):\n    \"\"\"\\o/\"\"\"\n    if is_trim == True:\n        data = np.where(data <=0, 0, data)\n        data = np.where(data >=1, 1, data)\n\n    if is_minmax ==True:\n        data = min_max_scaler.fit_transform(data)\n    \n    if is_reshape == True:\n        data = data.reshape(-1)\n        \n    return data\n\ntest_df = pd.read_csv(f\"{CFG_DEB_SIMPLE.input_path}test.csv\")\ntitles = pd.read_csv('../input/cpc-codes/titles.csv')\ntest_df = test_df.merge(titles, left_on='context', right_on='code')\n\ncpc_texts = torch.load(\"../input/folds-dump-the-two-paths-fix/cpc_texts.pth\")\n\ntest_df['context_text'] = test_df['context'].map(cpc_texts)\ntest_df['text'] = test_df['anchor'] + '[SEP]' + test_df['target'] + '[SEP]'  + test_df['context_text']\ntest_df['text'] = test_df['text'].apply(str.lower)\n\ntest_df.head()\n\ntokenizer_deberta_v3 = AutoTokenizer.from_pretrained(CFG_DEB_SIMPLE.model_path)\npredictions = []\n\nte_dataset = TestDataset(test_df, tokenizer_deberta_v3, CFG_DEB_SIMPLE.max_input_length)\nte_dataloader = DataLoader(te_dataset,\n                              batch_size=CFG_DEB_SIMPLE.batch_size, shuffle=False,\n                              num_workers=CFG_DEB_SIMPLE.num_workers,\n                              pin_memory=True, drop_last=False)\n\ndeberta_simple_path = \"../input/us-patent-deberta-simple/microsoft_deberta-v3-large\"\n\nfor fold in tqdm(range(CFG_DEB_SIMPLE.num_fold)):\n    fold_path = f\"{deberta_simple_path}_best{fold}.pth\"\n    \n    model = Custom_Bert_Simple(CFG_DEB_SIMPLE.model_path)\n    model.load_state_dict(torch.load(fold_path)['model'])\n    model.to('cuda')\n    \n    prediction = valid_fn(te_dataloader, model, 'cuda')\n    \n    predictions.append(prediction)\n    del model\n    torch.cuda.empty_cache()\n    gc.collect()\n\nupd_predictions = [upd_outputs(x, is_trim=False) for x in predictions]\nupd_predictions = pd.DataFrame(upd_predictions).T\nupd_predictions['med'] = upd_predictions.median(axis=1)\nupd_predictions['minmax']= (upd_predictions.max(axis=1) + upd_predictions.min(axis=1)) /2\nupd_predictions['quantile'] = (upd_predictions.quantile(0.25, axis=1) + upd_predictions.quantile(0.75,axis=1)) /2\n\nsubmission_upd = pd.DataFrame({\n    'id': test_df['id'],\n    'score': upd_predictions.mean(axis=1),\n})","metadata":{"execution":{"iopub.status.busy":"2022-06-19T09:59:06.468171Z","iopub.execute_input":"2022-06-19T09:59:06.468671Z","iopub.status.idle":"2022-06-19T10:01:01.834554Z","shell.execute_reply.started":"2022-06-19T09:59:06.468618Z","shell.execute_reply":"2022-06-19T10:01:01.833444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport tensorflow as tf\nfrom tensorflow import keras\n# import bert\nimport math\nimport os\n\nfrom tensorflow.keras.layers import Dense, GRU, LSTM, Bidirectional\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import callbacks\nimport tensorflow.keras.backend as K\n\nimport codecs\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport pickle\n\nfrom transformers import BertTokenizer, TFBertModel, AutoTokenizer\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\ndataset_path = \"/kaggle/input/us-patent-phrase-to-phrase-matching/\"\npt_model_dir = \"/kaggle/input/bert-for-patents/bert-for-patents/\"\nft_model_dir = \"/kaggle/input/uspppm-bertforpatent-keras-train/usppm_bfp_v5_lstm.h5\"\nmax_seq_len = 80\nbatch_size = 32\nlearning_rate = 2e-5\n\ntokenizer = BertTokenizer.from_pretrained(pt_model_dir)\n# tokenizer = AutoTokenizer.from_pretrained(pt_model_dir)\npad_idx = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\nprint(tokenizer)\nprint(\"Padding token index : \", pad_idx)\n\ndef dataset_split(dataset, split_val):\n    lengths = int(len(dataset) * split_val)\n    train_data = dataset[:lengths]\n    valid_data = dataset[lengths:]\n    return train_data, valid_data\n\ndef dataset_load(train_url, test_url):\n    train_data = pd.read_csv(train_url, sep=',')\n    train_data['sep_token'] = '[SEP]'\n    train_data['cls_token'] = '[CLS]'\n    train_data['context_token'] = '[' + train_data.context + ']'\n    context_tokens = list(train_data.context_token.unique())\n    train_data = train_data.sample(frac=1).reset_index(drop=True)\n    train_data, valid_data = dataset_split(dataset=train_data, split_val=0.9)\n    test_data = pd.read_csv(test_url, sep=',')\n    test_data['sep_token'] = '[SEP]'\n    test_data['cls_token'] = '[CLS]'\n    test_data['context_token'] = '[' + test_data.context + ']'\n    \n    return train_data, valid_data, test_data, context_tokens\n\ndef create_learning_rate_scheduler(max_learn_rate=5e-5,\n                                   end_learn_rate=1e-7,\n                                   warmup_epoch_count=10,\n                                   total_epoch_count=90):\n\n    def lr_scheduler(epoch):\n        \n        if epoch < warmup_epoch_count:\n            res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)\n        else:\n            res = max_learn_rate*math.exp(math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))\n        return float(res)\n    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n\n    return learning_rate_scheduler\n\ndef encode_text(text, \n                tokenizer,\n                max_length):\n    \n    # With tokenizer's batch_encode_plus batch of both the sentences are\n    # encoded together and separated by [SEP] token.\n    encoded = tokenizer.batch_encode_plus(\n        text,\n        add_special_tokens=True,\n        max_length=max_length,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_token_type_ids=True,\n        return_tensors=\"tf\",\n    )\n\n    # Convert batch of encoded features to numpy array.\n    input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n    attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n    token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_masks\": attention_masks,\n        \"token_type_ids\": token_type_ids\n    }\n\ntrain_data, valid_data, test_data, context_tokens = dataset_load(dataset_path + \"train.csv\", dataset_path + \"test.csv\")\nlabels = list(set(train_data[\"score\"].values))\nlabels.sort()\n\nprint(len(train_data), len(valid_data), len(test_data))\nprint(labels)\nprint(context_tokens)\n\ncpc_codes = pd.read_csv(\"/kaggle/input/cpc-codes/titles.csv\")\ncpc_codes = cpc_codes[[\"code\", \"title\"]]\n\ncondition = cpc_codes['code'].map(len) == 3\ncpc_codes = cpc_codes[condition].reset_index(drop=True)\n\ntest_data = test_data.merge(cpc_codes, left_on='context', right_on='code', how='left')\n\ntest_data['title'] = test_data['title'].str.lower().str.replace(\";\",\"\")\ntest_data['anchor'] = test_data['anchor'].str.lower()\ntest_data['target'] = test_data['target'].str.lower()\n\ntest_data['text'] = test_data['title'] + \" \" + test_data['anchor']\n\nencoded_test_data = encode_text(test_data[[\"text\", \"target\"]].values.tolist(), tokenizer, max_seq_len)\n\ntest_x = [encoded_test_data[\"input_ids\"], encoded_test_data[\"attention_masks\"], encoded_test_data[\"token_type_ids\"]]\n\nmirrored_strategy = tf.distribute.MirroredStrategy()\nwith mirrored_strategy.scope():\n    # Encoded token ids from BERT tokenizer.\n    input_ids = tf.keras.layers.Input(\n        shape=(max_seq_len,), dtype=tf.int32, name=\"input_ids\"\n    )\n    # Attention masks indicates to the model which tokens should be attended to.\n    attention_masks = tf.keras.layers.Input(\n        shape=(max_seq_len,), dtype=tf.int32, name=\"attention_masks\"\n    )\n    # Token type ids are binary masks identifying different sequences in the model.\n    token_type_ids = tf.keras.layers.Input(\n        shape=(max_seq_len,), dtype=tf.int32, name=\"token_type_ids\"\n    )\n    # Loading pretrained BERT model.\n    base_model = TFBertModel.from_pretrained(pt_model_dir, from_pt=True)\n\n    base_model_output = base_model(\n        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n    )\n\n    last_hidden_state = base_model_output.last_hidden_state\n    print(last_hidden_state.shape)\n    \n#     cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(last_hidden_state)\n#     output = tf.keras.layers.Dense(1, activation=\"linear\")(cls_out)\n\n#     gru = GRU(units=max_seq_len, return_sequences=False)(last_hidden_state)\n    lstm = Bidirectional(LSTM(units=max_seq_len, return_sequences=False))(last_hidden_state)\n    output = tf.keras.layers.Dense(1, activation=\"linear\", name=\"uspppm_output\")(lstm)\n    \n#     avg_pool = tf.keras.layers.GlobalAveragePooling1D()(last_hidden_state)\n#     dropout = tf.keras.layers.Dropout(0.1, name=\"uspppm_dropout\")(avg_pool)\n#     output = tf.keras.layers.Dense(1, activation=\"linear\", name=\"uspppm_output\")(dropout)\n\n    model = tf.keras.models.Model(\n        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n    )\n\n    model.compile(\n        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n#         optimizer = tf.keras.optimizers.Adam(),\n        loss='mse'\n#         loss=tf.keras.losses.BinaryCrossentropy()\n    )\n\n#     \nmodel.summary()\n\nmodel.load_weights(ft_model_dir)\n\npred = model.predict(test_x)\n\nsubmission_3 = pd.read_csv(\"/kaggle/input/us-patent-phrase-to-phrase-matching/sample_submission.csv\")\nsubmission_3['score'] = pred\nsubmission_3['score'] = submission_3.score.apply(lambda x: 0 if x < 0 else x)\nsubmission_3['score'] = submission_3.score.apply(lambda x: 1 if x > 1 else x)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T10:01:01.837005Z","iopub.execute_input":"2022-06-19T10:01:01.837691Z","iopub.status.idle":"2022-06-19T10:01:59.773519Z","shell.execute_reply.started":"2022-06-19T10:01:01.837639Z","shell.execute_reply":"2022-06-19T10:01:59.771303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from functools import reduce\nsb = [submission_3, submission_4, submission_5, submission_6, submission_xlm]\ndf_merged = reduce(lambda  left,right: pd.merge(left,right,on=['id'],\n                                            how='outer'), sb)\nmm = MinMaxScaler()\ndf_scaled = pd.DataFrame(mm.fit_transform(df_merged[df_merged.columns[1::]]))\ndf_scaled['id'] = df_merged[df_merged.columns[0]]\n\noriginal_final = df_scaled[df_scaled.columns[:-1:]]\nfinal = df_scaled[df_scaled.columns[:-1:]].copy()\n\nfinal['med'] = original_final.median(axis=1)\nfinal['minmax']= (original_final.max(axis=1) + original_final.min(axis=1)) / 2\nfinal['quantile'] = (original_final.quantile(0.25, axis=1) + original_final.quantile(0.75,axis=1)) /2","metadata":{"execution":{"iopub.status.busy":"2022-06-19T10:01:59.779703Z","iopub.execute_input":"2022-06-19T10:01:59.782861Z","iopub.status.idle":"2022-06-19T10:01:59.853796Z","shell.execute_reply.started":"2022-06-19T10:01:59.782811Z","shell.execute_reply":"2022-06-19T10:01:59.852683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final","metadata":{"execution":{"iopub.status.busy":"2022-06-19T10:01:59.864823Z","iopub.execute_input":"2022-06-19T10:01:59.86744Z","iopub.status.idle":"2022-06-19T10:01:59.928211Z","shell.execute_reply.started":"2022-06-19T10:01:59.867391Z","shell.execute_reply":"2022-06-19T10:01:59.924941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final = final.mean(axis=1)\nsubmission_final_2 = pd.DataFrame({\n    'id': df_merged['id'],\n    'score': final,\n})","metadata":{"execution":{"iopub.status.busy":"2022-06-19T10:01:59.930335Z","iopub.execute_input":"2022-06-19T10:01:59.932299Z","iopub.status.idle":"2022-06-19T10:01:59.9429Z","shell.execute_reply.started":"2022-06-19T10:01:59.932248Z","shell.execute_reply":"2022-06-19T10:01:59.941597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mix_scores = submission_final_1.merge(submission_final_2, on='id')","metadata":{"execution":{"iopub.status.busy":"2022-06-19T10:01:59.944846Z","iopub.execute_input":"2022-06-19T10:01:59.946392Z","iopub.status.idle":"2022-06-19T10:01:59.958633Z","shell.execute_reply.started":"2022-06-19T10:01:59.946314Z","shell.execute_reply":"2022-06-19T10:01:59.95719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mix_scores ","metadata":{"execution":{"iopub.status.busy":"2022-06-19T10:01:59.960059Z","iopub.execute_input":"2022-06-19T10:01:59.960914Z","iopub.status.idle":"2022-06-19T10:01:59.987815Z","shell.execute_reply.started":"2022-06-19T10:01:59.960864Z","shell.execute_reply":"2022-06-19T10:01:59.986261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final = 0.75 * mix_scores ['score_x'] + 0.25 * mix_scores['score_y']","metadata":{"execution":{"iopub.status.busy":"2022-06-19T10:01:59.99328Z","iopub.execute_input":"2022-06-19T10:01:59.993731Z","iopub.status.idle":"2022-06-19T10:02:00.005203Z","shell.execute_reply.started":"2022-06-19T10:01:59.993615Z","shell.execute_reply":"2022-06-19T10:02:00.004157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'id': mix_scores ['id'],\n    'score': final,\n})","metadata":{"execution":{"iopub.status.busy":"2022-06-19T10:02:00.011732Z","iopub.execute_input":"2022-06-19T10:02:00.013628Z","iopub.status.idle":"2022-06-19T10:02:00.023851Z","shell.execute_reply.started":"2022-06-19T10:02:00.013532Z","shell.execute_reply":"2022-06-19T10:02:00.022479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mix_scores = submission_upd.merge(submission_electra, on='id')\nfinal = 0.3 * mix_scores ['score_x'] + 0.7 * mix_scores['score_y']\nsubmission_electra = pd.DataFrame({\n    'id': mix_scores ['id'],\n    'score': final,\n})","metadata":{"execution":{"iopub.status.busy":"2022-06-19T10:02:00.027862Z","iopub.execute_input":"2022-06-19T10:02:00.02994Z","iopub.status.idle":"2022-06-19T10:02:00.051417Z","shell.execute_reply.started":"2022-06-19T10:02:00.029894Z","shell.execute_reply":"2022-06-19T10:02:00.049976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mix_scores = submission.merge(submission_electra, on='id')","metadata":{"execution":{"iopub.status.busy":"2022-06-19T10:02:00.056627Z","iopub.execute_input":"2022-06-19T10:02:00.059562Z","iopub.status.idle":"2022-06-19T10:02:00.072547Z","shell.execute_reply.started":"2022-06-19T10:02:00.059515Z","shell.execute_reply":"2022-06-19T10:02:00.071263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final = 0.7 * mix_scores ['score_x'] + 0.3 * mix_scores['score_y']","metadata":{"execution":{"iopub.status.busy":"2022-06-19T10:02:00.078503Z","iopub.execute_input":"2022-06-19T10:02:00.081128Z","iopub.status.idle":"2022-06-19T10:02:00.088957Z","shell.execute_reply.started":"2022-06-19T10:02:00.081071Z","shell.execute_reply":"2022-06-19T10:02:00.08791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mix_scores","metadata":{"execution":{"iopub.status.busy":"2022-06-19T10:02:00.095582Z","iopub.execute_input":"2022-06-19T10:02:00.100254Z","iopub.status.idle":"2022-06-19T10:02:00.132404Z","shell.execute_reply.started":"2022-06-19T10:02:00.100193Z","shell.execute_reply":"2022-06-19T10:02:00.131055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'id': mix_scores ['id'],\n    'score': final,\n})","metadata":{"execution":{"iopub.status.busy":"2022-06-19T10:02:00.134354Z","iopub.execute_input":"2022-06-19T10:02:00.135975Z","iopub.status.idle":"2022-06-19T10:02:00.143583Z","shell.execute_reply.started":"2022-06-19T10:02:00.135916Z","shell.execute_reply":"2022-06-19T10:02:00.141971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['score'] = submission['score'].apply(lambda x: 0 if x < 0.02 else x)\n\n#submission['score'] = submission['score'].apply(lambda x: 0.25 if x < 0.26 and x > 0.24 else x)\n#submission['score'] = submission['score'].apply(lambda x: 0.5 if x < 0.51 and x > 0.49 else x)\n#submission['score'] = submission['score'].apply(lambda x: 0.75 if x < 0.76 and x > 0.74 else x)\n\nsubmission['score'] = submission['score'].apply(lambda x: 1 if x > 0.98 else x)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T10:02:00.146107Z","iopub.execute_input":"2022-06-19T10:02:00.146972Z","iopub.status.idle":"2022-06-19T10:02:00.160756Z","shell.execute_reply.started":"2022-06-19T10:02:00.146913Z","shell.execute_reply":"2022-06-19T10:02:00.159395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2022-06-19T10:02:00.163177Z","iopub.execute_input":"2022-06-19T10:02:00.163758Z","iopub.status.idle":"2022-06-19T10:02:00.184508Z","shell.execute_reply.started":"2022-06-19T10:02:00.16371Z","shell.execute_reply":"2022-06-19T10:02:00.183197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T10:02:00.188511Z","iopub.execute_input":"2022-06-19T10:02:00.1888Z","iopub.status.idle":"2022-06-19T10:02:00.201444Z","shell.execute_reply.started":"2022-06-19T10:02:00.18876Z","shell.execute_reply":"2022-06-19T10:02:00.200394Z"},"trusted":true},"execution_count":null,"outputs":[]}]}