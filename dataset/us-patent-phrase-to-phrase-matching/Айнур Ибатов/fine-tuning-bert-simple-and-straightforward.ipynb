{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-10T16:25:00.327656Z","iopub.execute_input":"2022-05-10T16:25:00.328288Z","iopub.status.idle":"2022-05-10T16:25:00.37082Z","shell.execute_reply.started":"2022-05-10T16:25:00.328171Z","shell.execute_reply":"2022-05-10T16:25:00.370176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/us-patent-phrase-to-phrase-matching/train.csv')\nprint(train_df.info())\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T16:25:00.372261Z","iopub.execute_input":"2022-05-10T16:25:00.37328Z","iopub.status.idle":"2022-05-10T16:25:00.497933Z","shell.execute_reply.started":"2022-05-10T16:25:00.373244Z","shell.execute_reply":"2022-05-10T16:25:00.49725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in ['anchor','target','context','score']:\n    print('\\n====',col,'=====')\n    print(train_df[col].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-05-10T16:25:00.499188Z","iopub.execute_input":"2022-05-10T16:25:00.500185Z","iopub.status.idle":"2022-05-10T16:25:00.538694Z","shell.execute_reply.started":"2022-05-10T16:25:00.500145Z","shell.execute_reply":"2022-05-10T16:25:00.537982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cpc = pd.read_csv('../input/cpc-codes/titles.csv')\ncpc.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T16:25:00.540779Z","iopub.execute_input":"2022-05-10T16:25:00.541267Z","iopub.status.idle":"2022-05-10T16:25:01.217433Z","shell.execute_reply.started":"2022-05-10T16:25:00.541229Z","shell.execute_reply":"2022-05-10T16:25:01.216758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- adding cpc title to add more context","metadata":{}},{"cell_type":"code","source":"cpc = cpc.rename(columns = {\"code\" : \"context\"})\ntrain_df = pd.merge(train_df, cpc[[\"context\",\"title\"]], on =\"context\", how = \"left\")\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T16:25:01.218662Z","iopub.execute_input":"2022-05-10T16:25:01.219265Z","iopub.status.idle":"2022-05-10T16:25:01.366398Z","shell.execute_reply.started":"2022-05-10T16:25:01.219224Z","shell.execute_reply":"2022-05-10T16:25:01.365529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean(x):\n    t = x.lower()\n    t = t.replace(\"[\",'')\n    t = t.replace(\";\",'')\n    t = t.replace(\",\",'')\n    t = t.replace(\"]\",'')\n    t = t.replace(\":\",'')\n    return t\n\ntrain_df['title'] = train_df['title'].apply(lambda x: clean(x))\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T16:25:01.367761Z","iopub.execute_input":"2022-05-10T16:25:01.368083Z","iopub.status.idle":"2022-05-10T16:25:01.43224Z","shell.execute_reply.started":"2022-05-10T16:25:01.368047Z","shell.execute_reply":"2022-05-10T16:25:01.431351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['sen1'] = train_df['anchor'].astype('str')+' '+train_df['title'].astype('str')\ntrain_df = train_df.drop(['anchor','context','title'],axis=1)\n# train_df['all_sen'] = train_df['sen1']+' [SEP '+train_df['target']\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T16:25:01.433758Z","iopub.execute_input":"2022-05-10T16:25:01.434016Z","iopub.status.idle":"2022-05-10T16:25:01.46662Z","shell.execute_reply.started":"2022-05-10T16:25:01.433982Z","shell.execute_reply":"2022-05-10T16:25:01.465961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq1_len = [len(i.split()) for i in train_df['sen1'].values]\npd.Series(seq1_len).hist(bins = 30)\n\ntar_len = [len(i.split()) for i in train_df['target'].values]\npd.Series(tar_len).hist(bins = 30)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T16:25:01.467895Z","iopub.execute_input":"2022-05-10T16:25:01.468173Z","iopub.status.idle":"2022-05-10T16:25:01.937737Z","shell.execute_reply.started":"2022-05-10T16:25:01.468137Z","shell.execute_reply":"2022-05-10T16:25:01.937071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(train_df[['target','sen1']],train_df['score'],random_state=1234,test_size=0.3)\nprint(x_train.shape,x_test.shape)\nprint(y_train.shape,y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T16:25:01.939081Z","iopub.execute_input":"2022-05-10T16:25:01.939323Z","iopub.status.idle":"2022-05-10T16:25:02.77051Z","shell.execute_reply.started":"2022-05-10T16:25:01.939288Z","shell.execute_reply":"2022-05-10T16:25:02.769714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T16:25:02.773021Z","iopub.execute_input":"2022-05-10T16:25:02.77334Z","iopub.status.idle":"2022-05-10T16:25:02.784686Z","shell.execute_reply.started":"2022-05-10T16:25:02.773302Z","shell.execute_reply":"2022-05-10T16:25:02.783886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T16:25:02.785976Z","iopub.execute_input":"2022-05-10T16:25:02.786365Z","iopub.status.idle":"2022-05-10T16:25:02.795458Z","shell.execute_reply.started":"2022-05-10T16:25:02.786328Z","shell.execute_reply":"2022-05-10T16:25:02.794523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport transformers\nfrom torch.nn.utils.clip_grad import clip_grad_norm\n\n\ntorch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T16:25:22.941296Z","iopub.execute_input":"2022-05-10T16:25:22.941847Z","iopub.status.idle":"2022-05-10T16:25:29.11848Z","shell.execute_reply.started":"2022-05-10T16:25:22.941806Z","shell.execute_reply":"2022-05-10T16:25:29.117727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T16:25:29.119994Z","iopub.execute_input":"2022-05-10T16:25:29.120606Z","iopub.status.idle":"2022-05-10T16:25:29.126132Z","shell.execute_reply.started":"2022-05-10T16:25:29.120568Z","shell.execute_reply":"2022-05-10T16:25:29.125486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### class defination for model , dataset and training functions","metadata":{}},{"cell_type":"code","source":"class my_model(nn.Module):\n    def __init__(self,bert_path):\n        super(my_model,self).__init__()\n        self.bert_path = bert_path\n        self.bert = transformers.BertModel.from_pretrained(self.bert_path)\n        self.fc_layer = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(1024,256),\n            nn.ReLU(inplace=True),\n            nn.Linear(256,1),\n            nn.Sigmoid()\n        )\n    def forward(self,ids,mask,token_type_ids):\n        out = self.bert(input_ids=ids,attention_mask=mask,token_type_ids=token_type_ids)\n        pooler_output = out.get('pooler_output')\n        bo = self.fc_layer(pooler_output)\n        return bo\n     ","metadata":{"execution":{"iopub.status.busy":"2022-05-07T12:23:43.022399Z","iopub.execute_input":"2022-05-07T12:23:43.022893Z","iopub.status.idle":"2022-05-07T12:23:43.034939Z","shell.execute_reply.started":"2022-05-07T12:23:43.022832Z","shell.execute_reply":"2022-05-07T12:23:43.034197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class my_dataset_train:\n    def __init__(self,text1,text2,label,tokenizer,max_len):\n        self.text1=text1\n        self.text2=text2\n        self.label=label\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.text1)\n    \n    def __getitem__(self,idx):\n        text_1 = str(self.text1[idx])\n        text_2 = str(self.text2[idx])\n        label = self.label[idx]\n        \n        inputs = self.tokenizer(\n            text_1,\n            text_2,\n            add_special_tokens=True,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_len,\n            return_attention_mask=True\n        )\n        \n        ids = inputs['input_ids']\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs['attention_mask']\n        \n        padding_len = self.max_len - len(ids)\n        ids = ids + ([0]*padding_len)\n        token_type_ids = token_type_ids + ([0]*padding_len)\n        mask = mask + ([0]*padding_len)\n        \n        return {\n            \"ids\": torch.tensor(ids,dtype=torch.long),\n            \"mask\": torch.tensor(mask,dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids,dtype=torch.long),\n            \"targets\": torch.tensor(label,dtype=torch.float),\n        }\n        ","metadata":{"execution":{"iopub.status.busy":"2022-05-07T12:23:43.036346Z","iopub.execute_input":"2022-05-07T12:23:43.036787Z","iopub.status.idle":"2022-05-07T12:23:43.050542Z","shell.execute_reply.started":"2022-05-07T12:23:43.036678Z","shell.execute_reply":"2022-05-07T12:23:43.049705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### dataset for train and valid","metadata":{}},{"cell_type":"code","source":"max_len=64\ntrain_batch_size = 20\nepochs=6\nbert_path = '../input/bert-for-patents/bert-for-patents'#'../input/bert-base-uncased'\n\ntokenizer = transformers.BertTokenizer.from_pretrained(bert_path)\n\n# Training dataset prep\n\ntrain_text1 = list(x_train['target'].values)\ntrain_text2 = list(x_train['sen1'].values)\ntrain_label = list(y_train.values)\n\ntrain_dataset = my_dataset_train(\n    text1 = train_text1,\n    text2 = train_text2,\n    label = train_label,\n    tokenizer=tokenizer ,\n    max_len=max_len\n)\n\ntrain_data_loader = torch.utils.data.DataLoader(train_dataset,batch_size=train_batch_size,shuffle=True)\n\n# validation dataset prep\nval_text1 = list(x_test['target'].values)\nval_text2 = list(x_test['sen1'].values)\nval_label = list(y_test.values)\n\nvalid_dataset = my_dataset_train(\n    text1 = val_text1,\n    text2 = val_text2,\n    label = val_label,\n    tokenizer=tokenizer,\n    max_len=max_len\n)\n\nvalid_data_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=train_batch_size,shuffle=True)\n\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-07T12:23:43.052732Z","iopub.execute_input":"2022-05-07T12:23:43.053181Z","iopub.status.idle":"2022-05-07T12:23:43.495068Z","shell.execute_reply.started":"2022-05-07T12:23:43.053142Z","shell.execute_reply":"2022-05-07T12:23:43.494326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train","metadata":{}},{"cell_type":"code","source":"def train(model, optimizer, scheduler, loss_function, epochs,train_dataloader, device, clip_value=2):\n    model.train()\n    for epoch in range(epochs):\n#         print(epoch)\n#         print(\"-----\")\n        best_loss = []\n        for step, batch in enumerate(train_dataloader): \n            batch_inputs, batch_masks, batch_labels = batch['ids'].to(device), batch['mask'].to(device), batch['targets'].to(device)\n            batch_token_type_ids = batch['token_type_ids'].to(device)\n            model.zero_grad()\n            outputs = model(batch_inputs, batch_masks, batch_token_type_ids)\n            loss = loss_function(outputs.squeeze(),batch_labels.squeeze())\n            best_loss.append(loss)\n            loss.backward()\n            clip_grad_norm(model.parameters(), clip_value)\n            optimizer.step()\n            scheduler.step()\n#             print(f\"step > {step},loss > {loss}\")\n        loss2 = sum(best_loss)/len(best_loss)\n        print(f'Epoch : {epoch} ,Train loss : {loss2}')\n                \n    return model\n\ndef r2_score(outputs, labels):\n    labels_mean = torch.mean(labels)\n    ss_tot = torch.sum((labels - labels_mean) ** 2)\n    ss_res = torch.sum((labels - outputs) ** 2)\n    r2 = 1 - ss_res / ss_tot\n    return r2\n\ndef evaluate(model,loss_function,test_dataloader,device):\n    model.eval()\n    test_loss, test_r2 = [], []\n    for step,batch in enumerate(test_dataloader):\n        batch_inputs, batch_masks, batch_labels = batch['ids'].to(device), batch['mask'].to(device), batch['targets'].to(device)\n        batch_token_type_ids = batch['token_type_ids'].to(device)\n        with torch.no_grad():\n            outputs = model(batch_inputs, batch_masks, batch_token_type_ids)\n        loss = loss_function(outputs, batch_labels)\n        test_loss.append(loss.item())\n        r2 = r2_score(outputs, batch_labels)\n        test_r2.append(r2.item())\n    return test_loss, test_r2","metadata":{"execution":{"iopub.status.busy":"2022-05-07T12:23:43.496762Z","iopub.execute_input":"2022-05-07T12:23:43.49718Z","iopub.status.idle":"2022-05-07T12:23:43.509952Z","shell.execute_reply.started":"2022-05-07T12:23:43.497142Z","shell.execute_reply":"2022-05-07T12:23:43.509246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_train_steps = len(train_data_loader) * epochs\n\nmodel = my_model(bert_path).to(device)\n\noptimizer = transformers.AdamW(model.parameters(),lr=3e-5,eps=1e-8)\n\nscheduler = transformers.get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_train_steps\n)\n\nloss_function = nn.MSELoss()\n\n\nmodel = train(model, optimizer, scheduler, loss_function, epochs,train_data_loader, device)\n\n\nloss1,r2_ = evaluate(model,loss_function,valid_data_loader,device)\n\nloss = sum(loss1)/len(loss1)\nr2 = sum(r2_)/len(r2_)\nprint(f\"eval mean result : loss {loss}, r2 {r2}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T12:23:43.511545Z","iopub.execute_input":"2022-05-07T12:23:43.511842Z","iopub.status.idle":"2022-05-07T13:54:02.337525Z","shell.execute_reply.started":"2022-05-07T12:23:43.511809Z","shell.execute_reply":"2022-05-07T13:54:02.335852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(),f'./my_bert')","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:54:02.338625Z","iopub.status.idle":"2022-05-07T13:54:02.339132Z","shell.execute_reply.started":"2022-05-07T13:54:02.338878Z","shell.execute_reply":"2022-05-07T13:54:02.338904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference\n\n- follow evaluate function\n- create dataset class","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:54:02.340591Z","iopub.status.idle":"2022-05-07T13:54:02.341405Z","shell.execute_reply.started":"2022-05-07T13:54:02.341165Z","shell.execute_reply":"2022-05-07T13:54:02.341191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:54:02.34257Z","iopub.status.idle":"2022-05-07T13:54:02.343328Z","shell.execute_reply.started":"2022-05-07T13:54:02.343088Z","shell.execute_reply":"2022-05-07T13:54:02.343117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/test.csv')\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:54:02.34448Z","iopub.status.idle":"2022-05-07T13:54:02.345257Z","shell.execute_reply.started":"2022-05-07T13:54:02.345011Z","shell.execute_reply":"2022-05-07T13:54:02.345036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cpc = pd.read_csv('../input/cpc-codes/titles.csv')\ncpc = cpc.rename(columns = {\"code\" : \"context\"})\ntest_df = pd.merge(test_df, cpc[[\"context\",\"title\"]], on =\"context\", how = \"left\")\n\ndef clean(x):\n    t = x.lower()\n    t = t.replace(\"[\",'')\n    t = t.replace(\";\",'')\n    t = t.replace(\",\",'')\n    t = t.replace(\"]\",'')\n    t = t.replace(\":\",'')\n    return t\n\ntest_df['title'] = test_df['title'].apply(lambda x: clean(x))\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:54:02.346302Z","iopub.status.idle":"2022-05-07T13:54:02.347058Z","shell.execute_reply.started":"2022-05-07T13:54:02.346799Z","shell.execute_reply":"2022-05-07T13:54:02.346826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:54:02.348229Z","iopub.status.idle":"2022-05-07T13:54:02.349004Z","shell.execute_reply.started":"2022-05-07T13:54:02.348746Z","shell.execute_reply":"2022-05-07T13:54:02.348774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['sen1'] = test_df['anchor'].astype('str')+' '+test_df['title'].astype('str')\ntest_df = test_df.drop(['anchor','context','title'],axis=1)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:54:02.35016Z","iopub.status.idle":"2022-05-07T13:54:02.350887Z","shell.execute_reply.started":"2022-05-07T13:54:02.350645Z","shell.execute_reply":"2022-05-07T13:54:02.350672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class my_dataset_test:\n    def __init__(self,text1,text2,idf,tokenizer,max_len):\n        self.text1=text1\n        self.text2=text2\n        self.idf = idf\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.text1)\n    \n    def __getitem__(self,idx):\n        text_1 = str(self.text1[idx])\n        text_2 = str(self.text2[idx])\n        idf = self.idf[idx]\n        \n        inputs = self.tokenizer(\n            text_1,\n            text_2,\n            add_special_tokens=True,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_len,\n            return_attention_mask=True\n        )\n        \n        ids = inputs['input_ids']\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs['attention_mask']\n        \n        padding_len = self.max_len - len(ids)\n        ids = ids + ([0]*padding_len)\n        token_type_ids = token_type_ids + ([0]*padding_len)\n        mask = mask + ([0]*padding_len)\n        \n        return {\n            \"ids\": torch.tensor(ids,dtype=torch.long),\n            \"mask\": torch.tensor(mask,dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids,dtype=torch.long),\n            \"idf\": idf\n        }\n        ","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:54:02.352107Z","iopub.status.idle":"2022-05-07T13:54:02.352849Z","shell.execute_reply.started":"2022-05-07T13:54:02.352606Z","shell.execute_reply":"2022-05-07T13:54:02.352633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_path = '../input/bert-for-patents/bert-for-patents'#'../input/bert-base-uncased'\nmax_len=64\n# tokenizer = transformers.BertTokenizer.from_pretrained(bert_path)\n\ntest_text1 = list(test_df['target'].values)\ntest_text2 = list(test_df['sen1'].values)\n\ntest_dataset = my_dataset_test(\n    text1 = test_text1,\n    text2 = test_text2,\n    idf=list(test_df['id'].values),\n    tokenizer=tokenizer,\n    max_len=max_len\n)\n\ntest_data_loader = torch.utils.data.DataLoader(test_dataset,batch_size=64,shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:54:02.354014Z","iopub.status.idle":"2022-05-07T13:54:02.354764Z","shell.execute_reply.started":"2022-05-07T13:54:02.354523Z","shell.execute_reply":"2022-05-07T13:54:02.35455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model,test_dataloader,device):\n    model.eval()\n    result = []\n    for step,batch in enumerate(test_dataloader):\n        batch_inputs, batch_masks = batch['ids'].to(device), batch['mask'].to(device)\n        batch_token_type_ids = batch['token_type_ids'].to(device)\n        with torch.no_grad():\n            outputs = model(batch_inputs, batch_masks, batch_token_type_ids)\n        out = [i[0] for i in outputs.cpu().detach().numpy()]\n        batch_idf = batch['idf']\n        temp = [[i,j] for i,j in zip(batch_idf,out)]\n        result.extend(temp)\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:54:02.355957Z","iopub.status.idle":"2022-05-07T13:54:02.356794Z","shell.execute_reply.started":"2022-05-07T13:54:02.356537Z","shell.execute_reply":"2022-05-07T13:54:02.356567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = my_model(bert_path).to(device)\nmodel.load_state_dict(torch.load('my_bert'))\n\nfinal_res = predict(model,test_data_loader,device)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:54:02.358126Z","iopub.status.idle":"2022-05-07T13:54:02.358531Z","shell.execute_reply.started":"2022-05-07T13:54:02.358312Z","shell.execute_reply":"2022-05-07T13:54:02.358334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_res","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:54:02.359967Z","iopub.status.idle":"2022-05-07T13:54:02.360565Z","shell.execute_reply.started":"2022-05-07T13:54:02.360334Z","shell.execute_reply":"2022-05-07T13:54:02.36036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/sample_submission.csv')\nsample.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:54:02.361502Z","iopub.status.idle":"2022-05-07T13:54:02.36246Z","shell.execute_reply.started":"2022-05-07T13:54:02.362201Z","shell.execute_reply":"2022-05-07T13:54:02.362225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_csv = pd.DataFrame(final_res,columns=['id','score'])\nsubmit_csv.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:54:02.363653Z","iopub.status.idle":"2022-05-07T13:54:02.364307Z","shell.execute_reply.started":"2022-05-07T13:54:02.364071Z","shell.execute_reply":"2022-05-07T13:54:02.364096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_csv.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:54:02.365521Z","iopub.status.idle":"2022-05-07T13:54:02.366203Z","shell.execute_reply.started":"2022-05-07T13:54:02.365938Z","shell.execute_reply":"2022-05-07T13:54:02.365963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}