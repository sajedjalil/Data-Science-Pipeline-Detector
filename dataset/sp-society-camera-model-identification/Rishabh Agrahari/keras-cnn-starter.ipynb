{"nbformat_minor":1,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","pygments_lexer":"ipython3","name":"python","version":"3.6.3","file_extension":".py","nbconvert_exporter":"python"}},"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom glob import glob\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n%matplotlib inline","execution_count":null,"metadata":{"_cell_guid":"6764c4d6-21f8-46aa-911e-09034f29acc0","_uuid":"83eb2162c9c0bd3dbbb5333182d9e61acb5a5bdf"},"outputs":[]},{"cell_type":"code","source":"!ls ../input/train/","execution_count":null,"metadata":{"_cell_guid":"28000d9f-427e-4f08-baf1-d547a37cee2c","_uuid":"a01d5d5746a38cc75a0ad28a6c5fa49dfbfd10d4"},"outputs":[]},{"cell_type":"code","source":"datagen = ImageDataGenerator(rescale=1./255)\ntrain_generator = datagen.flow_from_directory(\n        '../input/train/',  \n        batch_size=1,\n        class_mode='categorical')","execution_count":null,"metadata":{"_cell_guid":"21fbf77d-ecc1-43a6-b28a-e681cfa4c092","_uuid":"e0011027cda5aaeec859941f70d184248dad2b9d"},"outputs":[]},{"cell_type":"code","source":"# let's have a look at the images\nx, y = train_generator.next()\nplt.imshow((x[0]*255).astype('uint8'));\nprint(list(train_generator.class_indices.keys())[np.argmax(y)])","execution_count":null,"metadata":{"_cell_guid":"435df950-3b7a-4b90-806f-e710d3530069","_uuid":"5a549ba3eecd321b97bfc4bcc6b7ff29efeb2c4d"},"outputs":[]},{"cell_type":"code","source":"X_data, Y_data = [], []\nfor _ in tqdm(range(2750)):\n    x, y = train_generator.next()\n    X_data.append(x[0])\n    Y_data.append(y[0])\nX_data = np.asarray(X_data)\nY_data = np.asarray(Y_data)","execution_count":null,"metadata":{"_cell_guid":"1e68570c-57ea-4401-b901-af72602c2605","_uuid":"fef53824681f47e26100d365b86b8c9fb678a39f"},"outputs":[]},{"cell_type":"code","source":"def get_model():\n    input_img = Input((256, 256, 3))\n    X = BatchNormalization()(input_img)\n    X = Convolution2D(16, (3, 3), activation='relu')(X)\n    X = BatchNormalization()(X)\n    X = Convolution2D(16, (3, 3), activation='relu')(X)\n    X = MaxPooling2D()(X)\n    X = Convolution2D(32, (3, 3), activation='relu')(X)\n    X = BatchNormalization()(X)\n    X = Convolution2D(32, (3, 3), activation='relu')(X)\n    X = GlobalMaxPooling2D()(X)\n#     X = Flatten()(X)\n    X = BatchNormalization()(X)\n    X = Dense(512, activation='relu')(X)\n    X = Dropout(0.2)(X)\n    X = Dense(10, activation='softmax')(X)\n    model = Model(inputs=input_img, outputs=X)\n\n    model.compile(optimizer='adam', loss='categorical_crossentropy', \n                  metrics=['acc'])\n    model.summary()\n    return model","execution_count":null,"metadata":{"_cell_guid":"319c1dc7-1dc4-44d2-9308-145dc0ae1cfc","_uuid":"cf205c9ae3b189e2dda527594ce2ab8e95e64bbe","collapsed":true},"outputs":[]},{"cell_type":"code","source":"model = get_model()","execution_count":null,"metadata":{"_cell_guid":"994b49b5-45b2-41e9-9ebe-47a25d0a0345","_uuid":"fd044b84539055e7b13ca7afaba397014014a0f6"},"outputs":[]},{"cell_type":"code","source":"model_history = model.fit(X_data, Y_data, batch_size=10, epochs=3, validation_split=0.2,\n                          callbacks=[EarlyStopping(monitor='val_acc', patience=3, verbose=1)])","execution_count":null,"metadata":{"_cell_guid":"0684b2ee-7d39-47b0-969b-2c0ce58bee65","_uuid":"6269c944ea56e36aa318e407a05812b905e7fb95"},"outputs":[]},{"cell_type":"code","source":"# load test images\nX_test = []\nsub = pd.read_csv('../input/sample_submission.csv')\n\nfor fname in tqdm(sub['fname']):\n    filepath = '../input/test/' + fname\n    X_test.append(img_to_array(load_img(filepath, target_size=(256, 256))))\nX_test = np.asarray(X_test)","execution_count":null,"metadata":{"_cell_guid":"96a5d10b-96ab-4430-891c-b6a87c32d9d0","_uuid":"e4296cb7b854d07c1e95cdf4679f8c7b15365517"},"outputs":[]},{"cell_type":"code","source":"preds = model.predict(X_test, verbose=1)\npreds = np.argmax(preds, axis=1)\npreds = [list(train_generator.class_indices.keys())[p] for p in tqdm(preds)]","execution_count":null,"metadata":{"_cell_guid":"8d920bf5-35c7-452b-82da-bb124d34d568","_uuid":"c9e760d383a6b978db334d25d4bd2144927d7dec"},"outputs":[]},{"cell_type":"code","source":"sub['camera'] = preds\nsub.to_csv('sub.csv', index=False)","execution_count":null,"metadata":{"_cell_guid":"18eeacdd-9b05-426e-b6f5-e9da4e0a70ed","_uuid":"42bd027dad7b2e8a3d2757f7e68159d8bed1cb8c","collapsed":true},"outputs":[]},{"cell_type":"code","source":"sub.head()","execution_count":null,"metadata":{"_cell_guid":"d3309a22-0ebf-4896-94f3-89909b60d53c","_uuid":"2f247281e722786f627ed4878b1b8be1d3a1155e"},"outputs":[]},{"cell_type":"code","source":"","execution_count":null,"metadata":{"collapsed":true},"outputs":[]}],"nbformat":4}