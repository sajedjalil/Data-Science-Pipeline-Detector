{"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat_minor":1,"cells":[{"metadata":{"_cell_guid":"033e27a4-d600-4bf1-adf9-d9a0ba316f3e","_uuid":"1a48ea0f604b56754cd25a290a0f37a8dfd5623c"},"cell_type":"markdown","source":"# Overview\nA notebook which uses a few simple convolutions in an AlexNet like structure to identify patterns common to specific camera types. The model is currently quite simple but can easily be improved and made to match some of the more state of the art designs like ResNet, DenseNet or Inception"},{"metadata":{"_cell_guid":"8e119507-463b-4349-bf53-a0d70f59d0e1","collapsed":true,"_uuid":"c17b461b803801610e2eabc2e6f492301f9084c6"},"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom skimage.io import imread # read image\nfrom PIL import Image \n# imread fails on some of the tiffs so we use PIL\npil_imread = lambda c_file: np.array(Image.open(c_file)) \nfrom skimage.exposure import equalize_adapthist\nfrom glob import glob\n\n%matplotlib inline\nimport matplotlib.pyplot as plt","cell_type":"code","outputs":[],"execution_count":33},{"metadata":{"_cell_guid":"941710b6-33b6-4c40-89c5-c30452eeba7e","_uuid":"a7e4afe4d4802460f236159cce4b73b031932c77"},"source":"list_train = glob(os.path.join('..', 'input', 'train', '*', '*.jpg'))\nprint('Train Files found', len(list_train), 'first file:', list_train[0])\nlist_test = glob(os.path.join('..', 'input', '*', '*.tif'))\nprint('Test Files found', len(list_test), 'first file:', list_test[0])","cell_type":"code","outputs":[],"execution_count":34},{"metadata":{"_cell_guid":"7c57e7f2-fab6-4a3a-90f7-2ce782a2d71d","_uuid":"a9c632622efc366a3d2b44b3b5b114f879072a18"},"source":"from sklearn.preprocessing import LabelEncoder\ndef get_class_from_path(filepath):\n    return os.path.dirname(filepath).split(os.sep)[-1]\nfull_train_df = pd.DataFrame([{'path': x, 'category': get_class_from_path(x)} for x in list_train])\ncat_encoder = LabelEncoder()\ncat_encoder.fit(full_train_df['category'])\nnclass = cat_encoder.classes_.shape[0]\nfull_train_df.sample(3)","cell_type":"code","outputs":[],"execution_count":35},{"metadata":{"_cell_guid":"a488fbfe-53f2-4091-847b-de9b91ba412a","_uuid":"b563f2c97d0d3a106c338dd5f63669cc0b881ed8"},"cell_type":"markdown","source":"# Camera Distribution\nA quick look at how the training data are distributed to get a feeling for how common each camera type is. To make sure the training data isn't all too skewed"},{"metadata":{"_cell_guid":"775aaa10-4bcc-45b3-a13d-79e6de2f2e4e","_uuid":"5166e06773a0cce7614496c51d9cf48d9e4c2d24"},"source":"fig, ax1 = plt.subplots(1,1,figsize = (8, 6))\nax1.hist(cat_encoder.transform(full_train_df['category']), np.arange(nclass+1))\nax1.set_xticks(np.arange(nclass))\n_ = ax1.set_xticklabels(cat_encoder.classes_, rotation = 45)","cell_type":"code","outputs":[],"execution_count":36},{"metadata":{"_cell_guid":"4ea59577-eb9c-4cb2-9451-bfd4dd7d373c","_uuid":"250ea42bf046505d50597fb22f48fd11c50219a7"},"cell_type":"markdown","source":"## Preprocessing\nHere is some basic preprocessing code to try and correct for things we are not interested in light illumination, and low frequency scene information"},{"metadata":{"_cell_guid":"e533b8e1-5c20-4bbf-98ac-f973f8e021a0","collapsed":true,"_uuid":"039600ab3e17e88347f65d5c455ac30cfa99e7b0"},"source":"import cv2\nclahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(24, 24))\ndef imread_and_normalize(im_path):\n    img_data = pil_imread(im_path)\n    img_data = cv2.cvtColor(img_data[:,:,[2,1,0]], cv2.COLOR_BGR2LAB)\n    img_data[:,:,0] = clahe.apply(img_data[:,:,0])\n    img_data = cv2.cvtColor(img_data, cv2.COLOR_LAB2BGR)\n    # don't run channel by channel\n    #for i in range(3):\n    #    img_data[:,:,i] = clahe.apply(img_data[:,:,i])\n    return (img_data.astype(np.float32))/255.0","cell_type":"code","outputs":[],"execution_count":37},{"metadata":{"_cell_guid":"54120bd6-085a-48d6-a13a-d0b7b6872e04","_uuid":"204d7d6ddaacd2240696a4ae33fc9ac272b6a82b"},"source":"%%time\n# code for reading in a random chunk of the image\ndef read_chunk(im_path, n_chunk = 5, chunk_x = 96, chunk_y = 96):\n    img_data = imread_and_normalize(im_path)\n    img_x, img_y, _ = img_data.shape\n    out_chunk = []\n    for _ in range(n_chunk):\n        x_pos = np.random.choice(range(img_x-chunk_x))\n        y_pos = np.random.choice(range(img_y-chunk_y))\n        out_chunk += [img_data[x_pos:(x_pos+chunk_x), y_pos:(y_pos+chunk_y),:3]]\n    return np.stack(out_chunk, 0)\n\nt_img = read_chunk(full_train_df['path'].values[0])\nfig, c_axs = plt.subplots(2, 3, figsize = (12, 4))\nfor i, (c_ax, m_ax) in enumerate(c_axs.T):\n    c_ax.imshow(t_img[0,:,:,i], interpolation='none')\n    c_ax.axis('off')\n    c_ax.set_title(['Red', 'Green', 'Blue'][i])\n    m_ax.hist(t_img[0,:,:,i].ravel())","cell_type":"code","outputs":[],"execution_count":38},{"metadata":{"_cell_guid":"7c0bfe63-629d-4169-8635-a5c79481da32","_uuid":"33d90eca35fdfb61df9214986ecd25f0597a34de"},"source":"from keras.utils.np_utils import to_categorical\ndef generate_even_batch(base_df, sample_count = 1, chunk_count = 5):\n    while True:\n        cur_df = base_df.groupby('category').apply(lambda x: x[['path']].sample(sample_count)).reset_index()\n        x_out = np.concatenate(cur_df['path'].map(lambda x: read_chunk(x, n_chunk=chunk_count)),\n                             0)\n        y_raw = [x for x in cur_df['category'].values for _ in range(chunk_count)]\n        y_out = to_categorical(cat_encoder.transform(y_raw))\n        yield x_out, y_out","cell_type":"code","outputs":[],"execution_count":39},{"metadata":{"_cell_guid":"eadbb3e3-8796-4788-bbdb-af33abc6624f","_uuid":"8d914bbd80be00cbb14e0d9bc9a9cdc3929e7665"},"source":"d_gen = generate_even_batch(full_train_df)\nfor _, (x, y) in zip(range(1), d_gen):\n    print(x.shape, y.shape)","cell_type":"code","outputs":[],"execution_count":40},{"metadata":{"_cell_guid":"d9d73db0-fc4c-449b-9ec6-865688540797","_uuid":"68335a88200a19c6754224c074a5fd56803af84d"},"cell_type":"markdown","source":"# Build Model\nHere we make a model for processing the snippets"},{"metadata":{"_cell_guid":"23c1d5e9-0202-4905-b790-aba502e34ccc","collapsed":true,"_uuid":"cdcc74a8f7f8eb58e9577353a250edb9b1151b68"},"source":"from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom keras import optimizers, losses, activations, models\nfrom keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D, GlobalMaxPool2D, concatenate\ndef gap_drop(in_layer): \n    gap_layer = GlobalAveragePooling2D()(Convolution2D(16, kernel_size = 1)(in_layer))\n    gmp_layer = GlobalMaxPool2D()(Convolution2D(16, kernel_size = 1)(in_layer))\n    return Dropout(rate = 0.5)(concatenate([gap_layer, gmp_layer]))\n\ndef create_model():\n    inp = Input(shape=(None, None, 3))\n    norm_inp = BatchNormalization()(inp)\n    gap_layers = []\n    img_1 = Convolution2D(16, kernel_size=3, activation=activations.relu, padding=\"same\")(norm_inp)\n    img_1 = Convolution2D(16, kernel_size=3, activation=activations.relu, padding=\"same\")(img_1)\n    #gap_layers += [gap_drop(img_1)]\n    img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n    img_1 = Dropout(rate=0.2)(img_1)\n    img_1 = Convolution2D(32, kernel_size=3, activation=activations.relu, padding=\"same\")(img_1)\n    img_1 = Convolution2D(32, kernel_size=3, activation=activations.relu, padding=\"same\")(img_1)\n    gap_layers += [gap_drop(img_1)]\n    img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n    img_1 = Dropout(rate=0.2)(img_1)\n    img_1 = Convolution2D(64, kernel_size=2, activation=activations.relu, padding=\"same\")(img_1)\n    img_1 = Convolution2D(64, kernel_size=2, activation=activations.relu, padding=\"same\")(img_1)\n    gap_layers += [gap_drop(img_1)]\n    \n    gap_cat = concatenate(gap_layers)\n    \n    dense_1 = Dense(32, activation=activations.relu)(gap_cat)\n    dense_1 = Dense(nclass, activation='softmax')(dense_1)\n\n    model = models.Model(inputs=inp, outputs=dense_1)\n    opt = optimizers.Adam(lr=1e-3) # karpathy's magic learning rate\n    model.compile(optimizer=opt, \n                  loss='categorical_crossentropy', \n                  metrics=['acc'])\n    model.summary()\n    return model","cell_type":"code","outputs":[],"execution_count":41},{"metadata":{"_cell_guid":"26bcaba6-cecf-4913-908a-ff9d2ef468a7","_uuid":"d47f14aa01ef7656e9c2a6f276f30f3300e21f3a"},"cell_type":"markdown","source":"# Training Testing Split\nSplit the groups apart to have an untainted metric of the success\n"},{"metadata":{"_cell_guid":"824d617b-ed40-46e5-8658-b4ca3d390928","_uuid":"f012e4bfb32530ea42bd6afc0e34534be5cdc275"},"source":"%%time\nfrom sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(full_train_df, \n                                     test_size = 0.15,\n                                    random_state = 2018,\n                                    stratify = full_train_df['category'])\nprint('Train', train_df.shape[0], 'Test', test_df.shape[0])\ntrain_gen = generate_even_batch(train_df, 3, chunk_count = 3)\ntest_gen = generate_even_batch(test_df, 10)\n# cache the test_gen_data\n(test_x, test_y) = next(test_gen)\nprint('Test Data', test_x.shape)","cell_type":"code","outputs":[],"execution_count":42},{"metadata":{"_cell_guid":"a93fd2f8-6a74-45f9-8cc9-f4e9e35fb292","_uuid":"63c6c88123cc34dc144b7ac34403941cb5d03f3b"},"source":"model = create_model()\nfile_path=\"weights.best.hdf5\"\n\ncheckpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n\nearly = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=3)\ncallbacks_list = [checkpoint, early] #early","cell_type":"code","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"6ce307c4-ba6a-4e6e-b315-f88914402645","_uuid":"5c6c99418b781aa887e193475b6acf488762bbca"},"source":"history = model.fit_generator(train_gen, \n                              steps_per_epoch = 10,\n                              validation_data = (test_x, test_y), \n                              epochs = 10, \n                              verbose = True,\n                              workers = 2,\n                              use_multiprocessing = False,\n                              callbacks = callbacks_list)\n\n#print(history)\n\nmodel.load_weights(file_path)","cell_type":"code","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"05b97451-491b-456c-a1a9-55b404274d9b","_uuid":"ee2d9cd9418aba722fbfd9cc019f0c3cc0d8f896"},"cell_type":"markdown","source":"# Predict on output\nWe run the model on the full test image, one at a time, and save the category"},{"metadata":{"_cell_guid":"1d0318af-3195-400a-9e38-d8d055671b59","_uuid":"e1c8db8e1c6f09b00319154e489f27de265e3f74"},"source":"# show the processed image\nt_img = imread_and_normalize(np.random.choice(list_test))\nfig, c_axs = plt.subplots(2, 3, figsize = (12, 4))\nfor i, (c_ax, m_ax) in enumerate(c_axs.T):\n    c_ax.imshow(t_img[:,:,i], interpolation='none')\n    c_ax.axis('off')\n    m_ax.hist(t_img[:,:,i].ravel())","cell_type":"code","outputs":[],"execution_count":null},{"metadata":{},"source":"from tqdm import tqdm\nout_dict_list = []\nfor c_file in tqdm(list_test):\n    img_data = imread_and_normalize(c_file)\n    n_image = np.expand_dims(img_data,0)\n    out_dict_list += [{\n        'fname': os.path.basename(c_file),\n        'camera': np.argmax(model.predict(n_image)[0])\n    }]  ","cell_type":"code","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"3078ba99-1e6e-4f72-8f95-cd579236a930","collapsed":true,"_uuid":"5d11a7d9c02cf6508332ec5d19db2f5848c46f79"},"source":"df = pd.DataFrame(out_dict_list)\ndf['camera'] = df['camera'].map(cat_encoder.inverse_transform)\ndf[['fname', 'camera']].to_csv(\"submission.csv\", index=False)\ndf.sample(3)","cell_type":"code","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"26cb2059-1f98-4182-8076-38bc97a55195","collapsed":true,"_uuid":"31775597358630adb28c5874f260d672cb449747"},"source":"fig, ax1 = plt.subplots(1,1,figsize = (8, 6))\nax1.hist(cat_encoder.transform(df['camera']), np.arange(nclass+1))\nax1.set_xticks(np.arange(nclass)+0.5)\n_ = ax1.set_xticklabels(cat_encoder.classes_, rotation = 90)","cell_type":"code","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"75d80c11-6e0f-4fb4-bc6a-1bce0e302fd3","collapsed":true,"_uuid":"91727a7713a92be6c3c0f312d848216cce202168"},"source":"","cell_type":"code","outputs":[],"execution_count":null}],"nbformat":4}