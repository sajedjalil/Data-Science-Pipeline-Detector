{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"data set class defined","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\nfrom skimage import io\n\n\nclass CatsAndDogsDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None\n                 ):\n        self.annotations = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.root_dir, self.annotations.iloc[idx, 0])\n        image = io.imread(img_path)\n        y_label = torch.tensor(int(self.annotations.iloc[idx, 1]))\n        if self.transform:\n            image = self.transform(image)\n\n        return image, y_label\n\n    \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-01T06:34:59.462935Z","iopub.execute_input":"2022-02-01T06:34:59.46364Z","iopub.status.idle":"2022-02-01T06:35:01.285667Z","shell.execute_reply.started":"2022-02-01T06:34:59.463544Z","shell.execute_reply":"2022-02-01T06:35:01.284959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"zip file extraction of dog vs cat data set","metadata":{}},{"cell_type":"code","source":"import zipfile\nwith zipfile.ZipFile(\"../input/dogs-vs-cats/train.zip\",\"r\") as z:\n    z.extractall(\".\")","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:35:05.048055Z","iopub.execute_input":"2022-02-01T06:35:05.048759Z","iopub.status.idle":"2022-02-01T06:35:18.171449Z","shell.execute_reply.started":"2022-02-01T06:35:05.048721Z","shell.execute_reply":"2022-02-01T06:35:18.170453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef loss_plot(epochs, loss):\n    plt.plot(epochs, loss)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T21:09:01.080625Z","iopub.status.idle":"2022-01-30T21:09:01.081327Z","shell.execute_reply.started":"2022-01-30T21:09:01.081108Z","shell.execute_reply":"2022-01-30T21:09:01.081137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"googlenet model usage ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torch.utils.data import DataLoader\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# hyperparameter for rnn\n# inputsize means depth or features\nlearning_rate = 0.001\nbatch_size = 16\nnum_epochs = 1\n\n\nclass Identity(nn.Module):\n    def __init__(self):\n        super(Identity, self).__init__()\n\n\ndataset = CatsAndDogsDataset(csv_file=\"../input/catvsdog/catvsdog.csv\", root_dir='./train',\n                             transform=transforms.Compose([transforms.ToTensor(),transforms.Resize([100, 100]) ]))\ntrain_set, test_set = torch.utils.data.random_split(dataset, [20000, 5000])\ntrain_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n\n# initialize network\nmodel = torchvision.models.googlenet(pretrained=True)\nmodel.to(device)\n\n# loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nfor epoch in range(num_epochs):\n    print(\"epoch\",epoch)\n    for batch_index, (data, targets) in enumerate(train_loader):\n        data = data.to(device=device)\n        targets = targets.to(device=device)\n\n        # forward\n        # print(data.shape)\n        scores = model(data)\n        loss = criterion(scores, targets)\n        \n        if batch_index % 10 == 0:\n            print(scores.shape)\n        \n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n\n        optimizer.step()\n    print(\"loss = \", loss)\n\n\ndef check_accuracy(loader, model,train=1):\n    if train:\n        print(\"training model\")\n    else:\n        print(\"testing model\")\n\n    num_correct = 0\n    num_samples = 0\n    model.eval()\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device)\n            y = y.to(device=device)\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n        print(\"accuracy = \", (float(num_correct) / float(num_samples)) * 100)\n    model.train()\n\n\ncheck_accuracy(train_loader, model)\ncheck_accuracy(test_loader, model,0)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-30T21:09:01.082868Z","iopub.status.idle":"2022-01-30T21:09:01.083262Z","shell.execute_reply.started":"2022-01-30T21:09:01.083048Z","shell.execute_reply":"2022-01-30T21:09:01.08307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"basic NN model implementation","metadata":{}},{"cell_type":"code","source":"import sys\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ninput_size = 100\nnum_classes = 2\nlearning_rate = 0.001\nbatch_size = 128\nnum_epoch = 2\n\ndataset = CatsAndDogsDataset(csv_file=\"../input/catvsdog/catvsdog.csv\", root_dir='./train',\n                             transform=transforms.Compose([transforms.ToTensor(), transforms.Resize([100, 100])]))\ntrain_set, test_set = torch.utils.data.random_split(dataset, [20000, 5000])\ntrain_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n\n\nclass NN(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(NN, self).__init__()\n        self.fc1 = nn.Linear(input_size*300, 500)\n        self.fc2 = nn.Linear(500, 50)\n        self.fc3 = nn.Linear(50, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nmodel = NN(input_size, num_classes)\nmodel.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nfor epoch in range(num_epoch):\n    losses=[]\n    batch_idxs=[]\n    for batch_idx, (data, targets) in enumerate(train_loader):\n#         print(batch_idx)\n        data = data.to(device=device)\n        targets = targets.to(device=device)\n\n        data = data.reshape(data.shape[0], -1)\n\n        # forward\n        scores = model(data)\n        loss = criterion(scores, targets)\n        \n        if batch_idx % 10 ==0:\n            print(loss,score)\n\n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n\n        optimizer.step()\n        \n#     loss_plot(batch_idxs,losses)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-30T21:09:01.084615Z","iopub.status.idle":"2022-01-30T21:09:01.085005Z","shell.execute_reply.started":"2022-01-30T21:09:01.084794Z","shell.execute_reply":"2022-01-30T21:09:01.084816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"accuracy check for nn model","metadata":{}},{"cell_type":"code","source":"\ndef check_accuracy(loader, model):\n    num_correct = 0\n    total_sample = 0\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            y = y.to(device)\n            x = x.reshape(x.shape[0], -1)\n\n            scores = model(x)\n            print(scores)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            total_sample += predictions.size(0)\n\n        print(\"accuracy = \", (float(num_correct) / float(total_sample)) * 100)\n    model.train()\n\n\ncheck_accuracy(train_loader, model)\ncheck_accuracy(test_loader, model)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T21:09:01.086292Z","iopub.status.idle":"2022-01-30T21:09:01.086719Z","shell.execute_reply.started":"2022-01-30T21:09:01.086503Z","shell.execute_reply":"2022-01-30T21:09:01.086526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CNN model implementation","metadata":{}},{"cell_type":"code","source":"import sys\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nin_channels = 3\nnum_classes = 2\nlearning_rate = 0.0001\nbatch_size = 64\nnum_epoch = 5\n\nclass CNN(nn.Module):\n    def __init__(self, in_channles, num_classes):\n        super(CNN, self).__init__()\n        self.conv6 = nn.Conv2d(in_channels=in_channles, out_channels=6, kernel_size=(3, 3), stride=(1, 1),\n                               padding=(1, 1))\n        self.pool1 = nn.MaxPool2d(2, stride=2)\n        self.conv1 = nn.Conv2d(in_channels=6, out_channels=8, kernel_size=(3, 3), stride=(2, 2),\n                               padding=(0, 0))\n        self.pool2 = nn.MaxPool2d(2, stride=2)\n        self.conv2 = nn.Conv2d(in_channels=8, out_channels=17, kernel_size=(3, 3), stride=(1, 1),\n                               padding=(0, 0))\n        self.pool3 = nn.MaxPool2d(2, stride=2)\n        self.fc1 = nn.Linear(5 * 5 * 17, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.conv6(x))\n#         print(x.shape)\n        x = self.pool1(x)\n#         print(x.shape)\n        x = F.relu(self.conv1(x))\n#         print(x.shape)\n        x = self.pool2(x)\n#         print(x.shape)\n        x = F.relu(self.conv2(x))\n#         print(x.shape)\n        x = self.pool3(x)\n#         print(x.shape)\n#         x = F.relu(self.conv3(x))\n# #         print(x.shape)\n#         x = F.relu(self.conv4(x))\n# #         print(x.shape)\n#         x = F.relu(self.conv5(x))\n#         print(x.shape)\n        x = x.reshape(x.shape[0], -1)\n        x = F.softmax(self.fc1(x),dim=1)\n        return x\n    \n\ndataset = CatsAndDogsDataset(csv_file=\"../input/catvsdog/catvsdog.csv\", root_dir='./train',\n                             transform=transforms.Compose([transforms.ToTensor(), transforms.Resize([101, 101])]))\ntrain_set, test_set = torch.utils.data.random_split(dataset, [20000, 5000])\ntrain_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n\nmodel = CNN(in_channels, num_classes).to(device)\n# model = torchvision.models.vgg16(pretrained=True)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:42:51.718699Z","iopub.execute_input":"2022-02-01T06:42:51.718961Z","iopub.status.idle":"2022-02-01T06:42:51.754909Z","shell.execute_reply.started":"2022-02-01T06:42:51.718925Z","shell.execute_reply":"2022-02-01T06:42:51.754238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CNN model training","metadata":{}},{"cell_type":"code","source":"for epoch in range(num_epoch):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        data = data.to(device=device)\n        targets = targets.to(device=device)\n\n        # forward\n        scores = model(data)\n        loss = criterion(scores, targets)\n        \n        if batch_idx%100==0:\n            print(loss)\n\n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n\n        optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:42:55.477478Z","iopub.execute_input":"2022-02-01T06:42:55.477927Z","iopub.status.idle":"2022-02-01T06:48:52.756768Z","shell.execute_reply.started":"2022-02-01T06:42:55.477889Z","shell.execute_reply":"2022-02-01T06:48:52.756077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CNN model accuracy check","metadata":{}},{"cell_type":"code","source":"\ndef check_accuracy(loader, model):\n    num_correct = 0\n    total_sample = 0\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            y = y.to(device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            total_sample += predictions.size(0)\n\n        print(\"accuracy = \", (float(num_correct) / float(total_sample)) * 100)\n    model.train()\n\n\ncheck_accuracy(train_loader, model)\ncheck_accuracy(test_loader, model)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:49:47.554738Z","iopub.execute_input":"2022-02-01T06:49:47.555006Z","iopub.status.idle":"2022-02-01T06:51:16.226674Z","shell.execute_reply.started":"2022-02-01T06:49:47.554974Z","shell.execute_reply":"2022-02-01T06:51:16.225115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ninput_size = 100\nnum_layers=2\nsequence_length=100\nhidden_nodes = 100\nnum_classes = 2\nlearning_rate = 0.001\nbatch_size = 128\nnum_epoch = 5\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_nodes,num_layers,num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size=hidden_nodes\n        self.num_layers=num_layers\n        self.rnn=nn.RNN(input_size,hidden_nodes,num_layers,batch_first=True)\n        self.fc = nn.Linear(hidden_nodes * sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers,x.size(0),self.hidden_size).to(device)\n        x,_=self.rnn(x,h0)\n        x=x.reshape(x.shape[0],-1)\n        x = self.fc(x)\n        \n        return x\n    \n\ndataset = CatsAndDogsDataset(csv_file=\"../input/catvsdog/catvsdog.csv\", root_dir='./train',\n                             transform=transforms.Compose([transforms.ToTensor(), transforms.Resize([100, 100])]))\ntrain_set, test_set = torch.utils.data.random_split(dataset, [20000, 5000])\ntrain_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n\n\nmodel = RNN(input_size, hidden_nodes,num_layers,num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T21:09:01.09429Z","iopub.status.idle":"2022-01-30T21:09:01.094707Z","shell.execute_reply.started":"2022-01-30T21:09:01.094492Z","shell.execute_reply":"2022-01-30T21:09:01.094514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(num_epoch):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        data = data[:,-1,:,:]\n        data = data.to(device=device)\n        targets = targets.to(device=device)\n\n        # forward\n        scores = model(data)\n        loss = criterion(scores, targets)\n        \n        if batch_idx%100==0:\n            print(loss)\n\n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n        \n        optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T21:09:01.096313Z","iopub.status.idle":"2022-01-30T21:09:01.096739Z","shell.execute_reply.started":"2022-01-30T21:09:01.096526Z","shell.execute_reply":"2022-01-30T21:09:01.096548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef check_accuracy(loader, model):\n    num_correct = 0\n    total_sample = 0\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x=x[:,-1,:,:]\n            x = x.to(device)\n            y = y.to(device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            total_sample += predictions.size(0)\n\n        print(\"accuracy = \", (float(num_correct) / float(total_sample)) * 100)\n    model.train()\n\n\ncheck_accuracy(train_loader, model)\ncheck_accuracy(test_loader, model)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T21:09:01.098141Z","iopub.status.idle":"2022-01-30T21:09:01.098555Z","shell.execute_reply.started":"2022-01-30T21:09:01.09832Z","shell.execute_reply":"2022-01-30T21:09:01.098354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}