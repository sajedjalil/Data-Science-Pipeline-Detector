{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:49:55.10671Z","iopub.execute_input":"2022-07-04T04:49:55.107056Z","iopub.status.idle":"2022-07-04T04:49:55.145609Z","shell.execute_reply.started":"2022-07-04T04:49:55.106971Z","shell.execute_reply":"2022-07-04T04:49:55.144746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this competition, you'll write an algorithm to classify whether images contain either a dog or a cat.\n\nThis Competion contains 25000 Training Images of either Dog/Cat. Each image contains 224-Pixels\n\n**Each image is having 3 Channels / Layers i.e Red-channel, Green-channel and Blue-channel. Each Pixel in each Channel each has a Pixel-intensity value between 0 & 255. These 224-Pixels are our Features and their values are Feature Values to train our Model**\n\nThis is how 'computer-sees', also called ***Computer Vision***. I also will not do much Feature Engineering as it may lead to TAMPERED Data","metadata":{}},{"cell_type":"markdown","source":"A colored image is typically composed of multiple colors and almost all colors can be generated from three primary colors – red, green and blue.\n\nHence, in the case of a colored image, there are three Matrices (or channels) – Red, Green, and Blue. Each matrix has values between 0-255 representing the intensity of the color for that pixel. Consider the below image to understand this concept:\n\n![RGB-Image](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/article-image-41.png)","metadata":{}},{"cell_type":"markdown","source":"## **Get Image Pixels**\n\nColor images are represented as three-dimensional Numpy arrays - a collection of three two-dimensional arrays, one each for red, green, and blue channels. Each one, like grayscale arrays, has one value per pixel and their ranges are identical.\n\n![rgb](https://e2eml.school/images/image_processing/three_d_array.png)","metadata":{}},{"cell_type":"markdown","source":"## **Convolution Operation**\n\nThe fundamental difference between a densely connected layer and a convolution layer is this: Dense layers learn global patterns in their input feature space (for example, for a MNIST digit, patterns involving all pixels), whereas convolution layers learn local patterns in the case of images, patterns found in small 2D windows of the inputs.\n\n**This key characteristic gives convnets two interesting properties:**\n1. The patterns they learn are translation invariant. After learning a certain pattern in the lower-right corner of a picture, a convnet can recognize it anywhere: forexample, in the upper-left corner. A densely connected network would have to learn the pattern anew if it appeared at a new location.\n1. They can learn spatial hierarchies of patterns (see figure 5.2). A first convolution layer will learn small local patterns such as edges, a second convolution layer will learn larger patterns made of the features of the first layers, and so on. This allows convnets to efficiently learn increasingly complex and abstract visual concepts (because the visual world is fundamentally spatially hierarchical).","metadata":{}},{"cell_type":"markdown","source":"# **Import Basic Libraries**\n\n**more to be imported as needed**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport PIL\nimport PIL.Image\n\nimport tensorflow as tf","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:49:55.147616Z","iopub.execute_input":"2022-07-04T04:49:55.147919Z","iopub.status.idle":"2022-07-04T04:50:00.499457Z","shell.execute_reply.started":"2022-07-04T04:49:55.147879Z","shell.execute_reply":"2022-07-04T04:50:00.498735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Load Data**","metadata":{}},{"cell_type":"code","source":"train_dir = \"../input/dogs-vs-cats/train.zip\"\n\nimport zipfile\nwith zipfile.ZipFile(train_dir, \"r\") as f:\n    f.extractall('.')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:50:00.500673Z","iopub.execute_input":"2022-07-04T04:50:00.50093Z","iopub.status.idle":"2022-07-04T04:50:11.786512Z","shell.execute_reply.started":"2022-07-04T04:50:00.500898Z","shell.execute_reply":"2022-07-04T04:50:11.785739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# File Path\nIMAGE_FOLDER_PATH = \"../working/train\"\nFILE_PATH = [os.path.join(IMAGE_FOLDER_PATH,i) for i in os.listdir(IMAGE_FOLDER_PATH)]\n\n# Labels\nlabels=[i for i in os.listdir(IMAGE_FOLDER_PATH)]\n\n# Targets\ntargets=[i.split('.')[0] for i in labels]\ny=targets\n\n# Dataframe\ndata = pd.DataFrame({'PATH':FILE_PATH,'targets':targets}) \ndata","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:50:11.788503Z","iopub.execute_input":"2022-07-04T04:50:11.788766Z","iopub.status.idle":"2022-07-04T04:50:11.888267Z","shell.execute_reply.started":"2022-07-04T04:50:11.78873Z","shell.execute_reply":"2022-07-04T04:50:11.887407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Train-Val Split**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_df, val_df = train_test_split(data, test_size=0.20, random_state=42)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:50:11.889789Z","iopub.execute_input":"2022-07-04T04:50:11.890053Z","iopub.status.idle":"2022-07-04T04:50:12.65012Z","shell.execute_reply.started":"2022-07-04T04:50:11.890017Z","shell.execute_reply":"2022-07-04T04:50:12.649365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Train Data**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndatagen=ImageDataGenerator()\n\ntrain=datagen.flow_from_dataframe(dataframe=train_df,\n                                     x_col=\"PATH\",\n                                     y_col=\"targets\",\n                                     target_size=(224, 224),\n                                     class_mode=\"binary\")\n\ntrain","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:50:12.651309Z","iopub.execute_input":"2022-07-04T04:50:12.653174Z","iopub.status.idle":"2022-07-04T04:50:13.829389Z","shell.execute_reply.started":"2022-07-04T04:50:12.653131Z","shell.execute_reply":"2022-07-04T04:50:13.828027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.class_indices","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:50:13.832271Z","iopub.execute_input":"2022-07-04T04:50:13.833092Z","iopub.status.idle":"2022-07-04T04:50:13.842343Z","shell.execute_reply.started":"2022-07-04T04:50:13.833053Z","shell.execute_reply":"2022-07-04T04:50:13.840789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Validation Data**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndatagen=ImageDataGenerator()\n\nval=datagen.flow_from_dataframe(dataframe=val_df,\n                                     x_col=\"PATH\",\n                                     y_col=\"targets\",\n                                     target_size=(224, 224),\n                                     class_mode=\"binary\")\n\nval","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:50:13.844003Z","iopub.execute_input":"2022-07-04T04:50:13.844359Z","iopub.status.idle":"2022-07-04T04:50:13.909721Z","shell.execute_reply.started":"2022-07-04T04:50:13.844322Z","shell.execute_reply":"2022-07-04T04:50:13.909038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val.class_indices","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:50:13.912266Z","iopub.execute_input":"2022-07-04T04:50:13.912495Z","iopub.status.idle":"2022-07-04T04:50:13.917845Z","shell.execute_reply.started":"2022-07-04T04:50:13.912464Z","shell.execute_reply":"2022-07-04T04:50:13.917034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **EDA - Further Analysis of Data**","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:50:13.921909Z","iopub.execute_input":"2022-07-04T04:50:13.922567Z","iopub.status.idle":"2022-07-04T04:50:13.943858Z","shell.execute_reply.started":"2022-07-04T04:50:13.92253Z","shell.execute_reply":"2022-07-04T04:50:13.943095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['targets'].value_counts()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:50:13.945248Z","iopub.execute_input":"2022-07-04T04:50:13.945511Z","iopub.status.idle":"2022-07-04T04:50:13.956174Z","shell.execute_reply.started":"2022-07-04T04:50:13.945476Z","shell.execute_reply":"2022-07-04T04:50:13.955448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualize the Data**","metadata":{}},{"cell_type":"markdown","source":"## Image-Gallery Function","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.image import load_img\n\ndef gallery(nrow,ncol,name):    # This function creates a grid of images from  dataset.\n    \n    img_df = train_df[train_df['targets'] == name]   # little data sorting\n    path_df = img_df['PATH']\n    \n    # Gallery using Matplotlib \n    fig, ax = plt.subplots(nrow,ncol,figsize = (12,12), dpi = 100)\n    axes = ax.ravel()\n    \n    for idx,ax  in enumerate(axes):\n        ax.imshow(load_img(path_df.iloc[idx]))\n    return fig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:50:13.957552Z","iopub.execute_input":"2022-07-04T04:50:13.958017Z","iopub.status.idle":"2022-07-04T04:50:13.965854Z","shell.execute_reply.started":"2022-07-04T04:50:13.957982Z","shell.execute_reply":"2022-07-04T04:50:13.965098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gallery(5,5,name='cat')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:50:13.967256Z","iopub.execute_input":"2022-07-04T04:50:13.96759Z","iopub.status.idle":"2022-07-04T04:50:17.273933Z","shell.execute_reply.started":"2022-07-04T04:50:13.967555Z","shell.execute_reply":"2022-07-04T04:50:17.273066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gallery(5,5,name='dog')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:50:17.276021Z","iopub.execute_input":"2022-07-04T04:50:17.276469Z","iopub.status.idle":"2022-07-04T04:50:19.837025Z","shell.execute_reply.started":"2022-07-04T04:50:17.276431Z","shell.execute_reply":"2022-07-04T04:50:19.836218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Simple CNN + MaxPooling**\n\nImportantly, a CNN takes as input tensors of shape (image_height, image_width,image_channels)\n\n* **Conv2D-**This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers or None, does not include the sample axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\". You can use None when a dimension has variable size.\n\nConvolutions are defined by two key parameters:\n1. Size of the patches extracted from the inputs—These are typically 3 × 3 or 5 × 5. Here they were 3 × 3, which is a common choice.\n1. Depth of the output feature map—The number of filters computed by the convolution.\n\nA convolution works by sliding these windows of size 3 × 3 or 5 × 5 over the 3D input feature map, stopping at every possible location, and extracting the 3D patch of surrounding features (shape (window_height, window_width, input_depth)). Each such 3D patch is then transformed (via a tensor product with the same learned weight matrix, called the convolution kernel) into a 1D vector of shape (output_depth,). All of\nthese vectors are then spatially reassembled into a 3D output map of shape (height,width, output_depth). Every spatial location in the output feature map corresponds to the same location in the input feature map (for example, the lower-right corner of the output contains information about the lower-right corner of the input). For instance, with 3 × 3 windows, the vector output[i, j, :] comes from the 3D patch input[i-1:i+1, j-1:j+1, :]. \n\n![cnn](https://miro.medium.com/max/1400/1*vkQ0hXDaQv57sALXAJquxA.jpeg)\n\n* **MaxPool2D-**Downsamples the input along its spatial dimensions (height and width) by taking the maximum value over an input window (of size defined by pool_size) for each channel of the input. The window is shifted by strides along each dimension.\n\nMax pooling consists of extracting windows from the input feature maps and outputting the max value of each channel. It’s conceptually similar to convolution, except that instead of transforming local patches via a learned linear transformation (the convolution kernel), they’re transformed via a hardcoded max tensor operation. A big difference from convolution is that max pooling is usually done with 2 × 2 windows and","metadata":{}},{"cell_type":"markdown","source":"### **I am deliberately keeping training_epochs=5 to fasten the training time, the purpose of this notebook is to illustrate(not just compete)**","metadata":{}},{"cell_type":"markdown","source":"## **Build the Model**","metadata":{}},{"cell_type":"code","source":"from keras.layers import Conv2D,MaxPool2D\nfrom keras.models import Sequential\n\nmodel1 = Sequential()\n\nmodel1.add(Conv2D(32,(3,3),activation='relu',input_shape=(224,224,3))),\nmodel1.add(MaxPool2D((2,2))),\n\nmodel1.add(Conv2D(64,(3,3),activation='relu')),\nmodel1.add(MaxPool2D((2,2))),\n\nmodel1.add(Conv2D(128,(3,3),activation='relu')),\nmodel1.add(MaxPool2D((2,2))),\n\nmodel1.add(Conv2D(256,(3,3),activation='relu')),\nmodel1.add(MaxPool2D((2,2))),\n\nmodel1.add(Conv2D(256,(3,3),activation='relu')),\nmodel1.add(MaxPool2D((2,2))),\n\nmodel1.summary()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:50:19.838089Z","iopub.execute_input":"2022-07-04T04:50:19.838335Z","iopub.status.idle":"2022-07-04T04:50:23.569344Z","shell.execute_reply.started":"2022-07-04T04:50:19.838302Z","shell.execute_reply":"2022-07-04T04:50:23.567521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **+ Dense + Flatten**","metadata":{}},{"cell_type":"markdown","source":"**You can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink. As you go deeper in the network. The number of channels is controlled by the first argument passed to the Conv2D layers (32 or 64).**\n\n**The next step is to feed the last output tensor into a densely connected classifier network : a stack of Dense layers. These classifiers process vectors, which are 1D, whereas the current output is a\n3D tensor. First we have to flatten the 3D outputs to 1D, and then add a few Dense layers on top**\n\n* **Flatten-**Flattens the input. Does not affect the batch size.\n\n* **Dense-**Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True). These are all attributes of Dense.","metadata":{}},{"cell_type":"code","source":"from keras.layers import Conv2D,MaxPool2D,Flatten,Dense\n   \nmodel1.add(Flatten()),\nmodel1.add(Dense(1024,activation='relu')),\nmodel1.add(Dense(256,activation='relu')),\nmodel1.add(Dense(32,activation='relu')),\nmodel1.add(Dense(1,activation='sigmoid')),\n\nmodel1.summary()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:50:23.570576Z","iopub.execute_input":"2022-07-04T04:50:23.57206Z","iopub.status.idle":"2022-07-04T04:50:23.615691Z","shell.execute_reply.started":"2022-07-04T04:50:23.572028Z","shell.execute_reply":"2022-07-04T04:50:23.613799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Plotting**","metadata":{}},{"cell_type":"code","source":"!pip install visualkeras\nimport visualkeras\nvisualkeras.layered_view(model1, legend=True) ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:50:23.616877Z","iopub.execute_input":"2022-07-04T04:50:23.61711Z","iopub.status.idle":"2022-07-04T04:50:35.476717Z","shell.execute_reply.started":"2022-07-04T04:50:23.617081Z","shell.execute_reply":"2022-07-04T04:50:35.475986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(model1, 'model.png',show_shapes=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:50:35.478204Z","iopub.execute_input":"2022-07-04T04:50:35.478991Z","iopub.status.idle":"2022-07-04T04:50:36.632908Z","shell.execute_reply.started":"2022-07-04T04:50:35.478953Z","shell.execute_reply":"2022-07-04T04:50:36.632026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Compile**","metadata":{}},{"cell_type":"code","source":"model1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n                loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n                metrics=tf.keras.metrics.BinaryAccuracy())","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:50:36.634722Z","iopub.execute_input":"2022-07-04T04:50:36.636351Z","iopub.status.idle":"2022-07-04T04:50:36.657777Z","shell.execute_reply.started":"2022-07-04T04:50:36.636308Z","shell.execute_reply":"2022-07-04T04:50:36.656958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Train**","metadata":{}},{"cell_type":"code","source":"history1=model1.fit(train,  \n                   epochs=5,\n                    #     verbose=\"auto\",\n                    #     callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3) # This callback will stop the training when there is no improvement in the loss for three consecutive epochs.,\n                    #     validation_split=0.0,\n                            validation_data=val,\n                            shuffle=True,\n                    #     class_weight=None,\n                    #     sample_weight=None,\n                    #     initial_epoch=0,\n                    #     steps_per_epoch=None,\n                    #     validation_steps=None,\n                    #     validation_batch_size=None,\n                    #     validation_freq=1,\n                    #     max_queue_size=10,\n                    #     workers=1,\n                    #     use_multiprocessing=False,\n                    )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:50:36.658961Z","iopub.execute_input":"2022-07-04T04:50:36.659604Z","iopub.status.idle":"2022-07-04T04:57:28.210028Z","shell.execute_reply.started":"2022-07-04T04:50:36.659574Z","shell.execute_reply":"2022-07-04T04:57:28.209328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Plotting**","metadata":{}},{"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history1.history['binary_accuracy'])\nplt.plot(history1.history['val_binary_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:57:28.211339Z","iopub.execute_input":"2022-07-04T04:57:28.211663Z","iopub.status.idle":"2022-07-04T04:57:28.660812Z","shell.execute_reply.started":"2022-07-04T04:57:28.211624Z","shell.execute_reply":"2022-07-04T04:57:28.660131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **+ Padding + Strides**\n\n**Padding-**If you want to get an output feature map with the same spatial dimensions as the input, you can use padding. Padding consists of adding an appropriate number of rows and columns on each side of the input feature map so as to make it possible to fit center convolution windows around every input tile. For a 3 × 3 window, you add one column on the right, one column on the left, one row at the top, and one row at the bottom. For a 5 × 5 window, you add two rows \n\n\n**Strides-**The description of convolution so far has assumed that the center tiles of the convolution windows are all contiguous. But the distance between two successive windows is a parameter of the convolution, called its stride, which defaults to 1. It’s possible to have strided convolutions: convolutions with a stride higher than 1\n\n![cnn-padding-stride](https://jehyunlee.github.io/2020/11/29/Python-DL-1-conv2d/keras_conv2d_padding.gif)","metadata":{}},{"cell_type":"markdown","source":"## **Build Model**","metadata":{}},{"cell_type":"code","source":"from keras.layers import Conv2D,MaxPool2D,Flatten,Dense\nfrom keras.models import Sequential\n\nmodel2 = Sequential()\n\nmodel2.add(Conv2D(filters=32,kernel_size=3,strides=(3,3),padding='same',activation='relu',input_shape=(224,224,3))),\nmodel2.add(MaxPool2D(pool_size=(2, 2), strides=None, padding=\"same\")),\n\nmodel2.add(Conv2D(filters=64,kernel_size=3,strides=(3,3),padding='same',activation='relu')),\nmodel2.add(MaxPool2D(pool_size=(2, 2), strides=None, padding=\"same\")),\n\nmodel2.add(Conv2D(filters=128,kernel_size=3,strides=(3,3),padding='same',activation='relu')),\nmodel2.add(MaxPool2D(pool_size=(2, 2), strides=None, padding=\"same\")),\n\nmodel2.add(Conv2D(filters=256,kernel_size=3,strides=(3,3),padding='same',activation='relu')),\nmodel2.add(MaxPool2D(pool_size=(2, 2), strides=None, padding=\"same\")),\n\nmodel2.add(Conv2D(filters=256,kernel_size=3,strides=(3,3),padding='same',activation='relu')),\nmodel2.add(MaxPool2D(pool_size=(2, 2), strides=None, padding=\"same\")),\n\nmodel2.add(Flatten()),\nmodel2.add(Dense(units=1024,activation='relu')),\nmodel2.add(Dense(units=256,activation='relu')),\nmodel2.add(Dense(units=32,activation='relu')),\nmodel2.add(Dense(units=1,activation='sigmoid')),\n\nmodel2.summary()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:57:28.662072Z","iopub.execute_input":"2022-07-04T04:57:28.662492Z","iopub.status.idle":"2022-07-04T04:57:28.761017Z","shell.execute_reply.started":"2022-07-04T04:57:28.662456Z","shell.execute_reply":"2022-07-04T04:57:28.759798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Visualize**","metadata":{}},{"cell_type":"code","source":"!pip install visualkeras\nimport visualkeras\nvisualkeras.layered_view(model2, legend=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:57:28.762355Z","iopub.execute_input":"2022-07-04T04:57:28.762607Z","iopub.status.idle":"2022-07-04T04:57:37.720063Z","shell.execute_reply.started":"2022-07-04T04:57:28.762573Z","shell.execute_reply":"2022-07-04T04:57:37.719199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(model1, 'model.png',show_shapes=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:57:37.721874Z","iopub.execute_input":"2022-07-04T04:57:37.722135Z","iopub.status.idle":"2022-07-04T04:57:37.937603Z","shell.execute_reply.started":"2022-07-04T04:57:37.722101Z","shell.execute_reply":"2022-07-04T04:57:37.93684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Compile & Train**","metadata":{}},{"cell_type":"code","source":"# Model Compile\nmodel2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n                loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n                metrics=tf.keras.metrics.BinaryAccuracy())\n\n\n# Train\nhistory2=model2.fit(train,  \n                   epochs=5,\n                    #     verbose=\"auto\",\n                    #     callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3) # This callback will stop the training when there is no improvement in the loss for three consecutive epochs.,\n                    #     validation_split=0.0,\n                    validation_data=val,\n                    shuffle=True,\n                    #     class_weight=None,\n                    #     sample_weight=None,\n                    #     initial_epoch=0,\n                    #     steps_per_epoch=None,\n                    #     validation_steps=None,\n                    #     validation_batch_size=None,\n                    #     validation_freq=1,\n                    #     max_queue_size=10,\n                    #     workers=1,\n                    #     use_multiprocessing=False,\n                    )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T04:57:37.939146Z","iopub.execute_input":"2022-07-04T04:57:37.939602Z","iopub.status.idle":"2022-07-04T05:04:02.673516Z","shell.execute_reply.started":"2022-07-04T04:57:37.939563Z","shell.execute_reply":"2022-07-04T05:04:02.672597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Plotting**","metadata":{}},{"cell_type":"code","source":"# Comparing Accuracy\nplt.plot(history2.history['val_binary_accuracy'],'b')\nplt.plot(history1.history['val_binary_accuracy'],'r')\nplt.title('binary_ accuracy')\nplt.ylabel('val_binary_accuracy')\nplt.xlabel('epoch')\nplt.legend(['with Padding & Stides','SimpleCNN+MaxPooling'], loc='upper left')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T05:04:02.674952Z","iopub.execute_input":"2022-07-04T05:04:02.675508Z","iopub.status.idle":"2022-07-04T05:04:02.870175Z","shell.execute_reply.started":"2022-07-04T05:04:02.675476Z","shell.execute_reply":"2022-07-04T05:04:02.869509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Bit Better**","metadata":{}},{"cell_type":"markdown","source":"# **+ Data Augmentation**\n\nData augmentation takes the approach of generating more training data from existing training samples, by augmenting the samples via a number of random transformations that yield believable-looking images. The goal is that at training time, your model will never see the exact same picture twice. This helps expose the model to more aspects of the data and generalize better.\n\nImage augmentation is a technique of applying different transformations to original images which results in multiple transformed copies of the same image. Each copy, however, is different from the other in certain aspects depending on the augmentation techniques you apply like shifting, rotating, flipping, etc.\n\nApplying these small amounts of variations on the original image does not change its target class but only provides a new perspective of capturing the object in real life. And so, we use it is quite often for building deep learning models.\n\nThese image augmentation techniques not only expand the size of your dataset but also incorporate a level of variation in the dataset which allows your model to generalize better on unseen data. Also, the model becomes more robust when it is trained on new, slightly altered images.","metadata":{}},{"cell_type":"markdown","source":"## **Train Data**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndatagen=ImageDataGenerator(\n#     featurewise_center=True,\n#     samplewise_center=T,\n#     featurewise_std_normalization=False,\n#     samplewise_std_normalization=False,\n#     zca_whitening=True,\n#     zca_epsilon=1e-06,\n#     rotation_range=90,\n#     width_shift_range=0.5,\n#     height_shift_range=0.5,\n#     brightness_range=None,\n#     shear_range=0.0,\n#     zoom_range=0.0,\n#     channel_shift_range=0.0,\n#     fill_mode='nearest',\n#     cval=0.0,\n    horizontal_flip=True,\n    vertical_flip=True,\n    rescale=1./255,\n#     preprocessing_function=None,\n#     data_format=None,\n#     validation_split=0.0,\n#     interpolation_order=1,\n#     dtype=None\n)\n\ntrain=datagen.flow_from_dataframe(dataframe=train_df,\n                                     x_col=\"PATH\",\n                                     y_col=\"targets\",\n                                     target_size=(224, 224),\n                                     class_mode=\"binary\")\n\ntrain","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T05:04:02.871501Z","iopub.execute_input":"2022-07-04T05:04:02.871883Z","iopub.status.idle":"2022-07-04T05:04:03.062902Z","shell.execute_reply.started":"2022-07-04T05:04:02.871846Z","shell.execute_reply":"2022-07-04T05:04:03.062214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The Keras ImageDataGenerator class is not an “additive” operation. It’s not taking the original data, randomly transforming it, and then returning both the original data and transformed data. Instead, the ImageDataGenerator accepts the original data, randomly transforms it, and returns only the new, transformed data.**","metadata":{}},{"cell_type":"markdown","source":"## **Validation Data**","metadata":{}},{"cell_type":"code","source":"val=datagen.flow_from_dataframe(dataframe=val_df,\n                                     x_col=\"PATH\",\n                                     y_col=\"targets\",\n                                     target_size=(224, 224),\n                                     class_mode=\"binary\")\n\nval","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T05:04:03.068294Z","iopub.execute_input":"2022-07-04T05:04:03.0689Z","iopub.status.idle":"2022-07-04T05:04:03.124386Z","shell.execute_reply.started":"2022-07-04T05:04:03.068862Z","shell.execute_reply":"2022-07-04T05:04:03.12359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Build Model**","metadata":{}},{"cell_type":"code","source":"# Build Model\n\nmodel3 = Sequential()\n\nmodel3.add(Conv2D(filters=32,kernel_size=3,strides=(3,3),padding='same',activation='relu',input_shape=(224,224,3))),\nmodel3.add(MaxPool2D(pool_size=(2, 2), strides=None, padding=\"same\")),\n\nmodel3.add(Conv2D(filters=64,kernel_size=3,strides=(3,3),padding='same',activation='relu')),\nmodel3.add(MaxPool2D(pool_size=(2, 2), strides=None, padding=\"same\")),\n\nmodel3.add(Conv2D(filters=128,kernel_size=3,strides=(3,3),padding='same',activation='relu')),\nmodel3.add(MaxPool2D(pool_size=(2, 2), strides=None, padding=\"same\")),\n\nmodel3.add(Flatten()),\n\nmodel3.add(Dense(units=1,activation='sigmoid')),\n\nmodel3.summary()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T05:04:03.137987Z","iopub.execute_input":"2022-07-04T05:04:03.138699Z","iopub.status.idle":"2022-07-04T05:04:03.199143Z","shell.execute_reply.started":"2022-07-04T05:04:03.138661Z","shell.execute_reply":"2022-07-04T05:04:03.1985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Visualize**","metadata":{}},{"cell_type":"code","source":"!pip install visualkeras\nimport visualkeras\nvisualkeras.layered_view(model3, legend=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T05:04:03.203259Z","iopub.execute_input":"2022-07-04T05:04:03.203437Z","iopub.status.idle":"2022-07-04T05:04:12.26878Z","shell.execute_reply.started":"2022-07-04T05:04:03.203412Z","shell.execute_reply":"2022-07-04T05:04:12.267837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(model3, 'model.png',show_shapes=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T05:04:12.270737Z","iopub.execute_input":"2022-07-04T05:04:12.271019Z","iopub.status.idle":"2022-07-04T05:04:12.453651Z","shell.execute_reply.started":"2022-07-04T05:04:12.270967Z","shell.execute_reply":"2022-07-04T05:04:12.452887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Compile & Train**","metadata":{}},{"cell_type":"code","source":"# Model Compile\nmodel3.compile(optimizer='adam',\n                loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n                metrics=tf.keras.metrics.BinaryAccuracy())\n\n# Train\nhistory3=model3.fit(train,  \n                   epochs=5,\n                    #     verbose=\"auto\",\n                    #     callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=3) # This callback will stop the training when there is no improvement in the loss for three consecutive epochs.,\n#                         validation_split=0.2,\n                    validation_data=val,\n                    shuffle=True,\n                    #     class_weight=None,\n                    #     sample_weight=None,\n                    #     initial_epoch=0,\n                    #     steps_per_epoch=None,\n                    #     validation_steps=None,\n                    #     validation_batch_size=None,\n                    #     validation_freq=1,\n                    #     max_queue_size=10,\n                    #     workers=1,\n                    #     use_multiprocessing=False,\n                    )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T05:04:12.455289Z","iopub.execute_input":"2022-07-04T05:04:12.456338Z","iopub.status.idle":"2022-07-04T05:11:58.372957Z","shell.execute_reply.started":"2022-07-04T05:04:12.456299Z","shell.execute_reply":"2022-07-04T05:11:58.37219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Plotting**","metadata":{}},{"cell_type":"code","source":"# Comparing Accuracy\nplt.plot(history3.history['val_binary_accuracy'],'b')\nplt.plot(history2.history['val_binary_accuracy'],'r')\nplt.title('Model Accuracy Comparison')\n# plt.xlabel('epoch')\nplt.legend(['with Data Augmentation', 'with Padding & Stides'], loc='upper left')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T05:11:58.374566Z","iopub.execute_input":"2022-07-04T05:11:58.375061Z","iopub.status.idle":"2022-07-04T05:11:58.566732Z","shell.execute_reply.started":"2022-07-04T05:11:58.375017Z","shell.execute_reply":"2022-07-04T05:11:58.565983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lot better**","metadata":{}},{"cell_type":"markdown","source":"# **+Regularization**\nRegularizers allow you to apply penalties on layer parameters or layer activity during optimization. These penalties are summed into the loss function that the network optimizes.\n\nRegularization penalties are applied on a per-layer basis. The exact API will depend on the layer, but many layers (e.g. Dense, Conv1D, Conv2D and Conv3D) have a unified API.\n\nThese layers expose 3 keyword arguments:\n\n* kernel_regularizer: Regularizer to apply a penalty on the layer's kernel\n* bias_regularizer: Regularizer to apply a penalty on the layer's bias\n* activity_regularizer: Regularizer to apply a penalty on the layer's output\n\n1. **Data Augmentation-**The simplest way to reduce overfitting is to increase the size of the training data. In machine learning, we were not able to increase the size of training data as the labeled data was too costly.\n\nBut, now let’s consider we are dealing with images. In this case, there are a few ways of increasing the size of the training data – rotating the image, flipping, scaling, shifting, etc. In the below image, some transformation has been done on the handwritten digits dataset.\n![data-aug](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/04/Screen-Shot-2018-04-04-at-12.14.45-AM.png)\nThis technique is known as data augmentation. This usually provides a big leap in improving the accuracy of the model. It can be considered as a mandatory trick in order to improve our predictions.\n\n2. **Dropout-**Dropout means that during training with some probability P a neuron of the neural network gets turned off during training. Let’s look at a visual example. A simpler version of the neural network results in less complexity that can reduce overfitting. The deactivation of neurons with a certain probability P is applied at each forward propagation and weight update step.\n\n![dropout](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/04/1IrdJ5PghD9YoOyVAQ73MJw.gif)\n\n3. **Early stopping-**Early stopping is one of the most commonly used strategies because it is very simple and quite effective. It refers to the process of stopping the training when the training error is no longer decreasing but the validation error is starting to rise.\n\n![early-stopping](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/04/Screen-Shot-2018-04-04-at-12.31.56-AM.png)\n\n4. **Batch Normalization-**Batch normalization (BN) can also be used as a form of regularization. Batch normalization fixes the means and variances of the input by bringing the feature in the same range.\n\n![bn](https://theaisummer.com/static/d42512016d9b99eabb69a61bb295cd50/2e9f9/normalization.png)","metadata":{}},{"cell_type":"markdown","source":"## **Train Data**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndatagen=ImageDataGenerator(\n#     featurewise_center=True,\n#     samplewise_center=True,\n#     featurewise_std_normalization=False,\n#     samplewise_std_normalization=False,\n#     zca_whitening=True,\n#     zca_epsilon=1e-06,\n#     rotation_range=90,\n#     width_shift_range=0.5,\n#     height_shift_range=0.5,\n#     brightness_range=None,\n#     shear_range=0.0,\n    zoom_range=0.5,\n#     channel_shift_range=0.0,\n#     fill_mode='nearest',\n#     cval=0.0,\n    horizontal_flip=True,\n    vertical_flip=True,\n    rescale=1./255,\n#     preprocessing_function=None,\n#     data_format=None,\n#     validation_split=0.0,\n#     interpolation_order=1,\n#     dtype=None\n)\n\ntrain=datagen.flow_from_dataframe(dataframe=train_df,\n                                     x_col=\"PATH\",\n                                     y_col=\"targets\",\n                                     target_size=(224, 224),\n                                     class_mode=\"binary\")\n\ntrain","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T05:11:58.56798Z","iopub.execute_input":"2022-07-04T05:11:58.568232Z","iopub.status.idle":"2022-07-04T05:11:58.756488Z","shell.execute_reply.started":"2022-07-04T05:11:58.568198Z","shell.execute_reply":"2022-07-04T05:11:58.755695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Validation Data**","metadata":{}},{"cell_type":"code","source":"val=datagen.flow_from_dataframe(dataframe=val_df,\n                                     x_col=\"PATH\",\n                                     y_col=\"targets\",\n                                     target_size=(224, 224),\n                                     class_mode=\"binary\")\n\nval","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T05:11:58.75775Z","iopub.execute_input":"2022-07-04T05:11:58.758073Z","iopub.status.idle":"2022-07-04T05:11:58.815444Z","shell.execute_reply.started":"2022-07-04T05:11:58.758036Z","shell.execute_reply":"2022-07-04T05:11:58.814735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Build Model**","metadata":{}},{"cell_type":"code","source":"from keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense, Activation, BatchNormalization\nfrom keras.models import Sequential\nfrom tensorflow.keras import regularizers\n\nmodel4 = Sequential()\n\nmodel4.add(Conv2D(filters=32,kernel_size=3,strides=(3,3),padding='same',activation='relu',input_shape=(224,224,3))),\nmodel4.add(BatchNormalization())\nmodel4.add(MaxPool2D(pool_size=(2, 2), strides=None, padding=\"same\")),\nmodel4.add(Dropout(0.25))\n\nmodel4.add(Conv2D(filters=64,kernel_size=3,strides=(3,3),padding='same',activation='relu')),\nmodel4.add(MaxPool2D(pool_size=(2, 2), strides=None, padding=\"same\")),\n\nmodel4.add(Conv2D(filters=128,kernel_size=3,strides=(3,3),padding='same',activation='relu')),\nmodel4.add(MaxPool2D(pool_size=(2, 2), strides=None, padding=\"same\")),\n\n# flatten output of conv\nmodel4.add(Flatten())\n\n# hidden layer\nmodel4.add(Dense(512, activation='relu', kernel_regularizer=regularizers.L1L2(l1=0.001,l2=0.001))) # Added Regularization\nmodel4.add(Dropout(0.25))\n\n# output layer\nmodel4.add(Dense(1, activation='sigmoid'))\n\nmodel4.summary()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T05:11:58.816605Z","iopub.execute_input":"2022-07-04T05:11:58.817281Z","iopub.status.idle":"2022-07-04T05:11:58.903947Z","shell.execute_reply.started":"2022-07-04T05:11:58.817246Z","shell.execute_reply":"2022-07-04T05:11:58.903296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Visualize**","metadata":{}},{"cell_type":"code","source":"!pip install visualkeras\nimport visualkeras\nvisualkeras.layered_view(model4, legend=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T05:11:58.90523Z","iopub.execute_input":"2022-07-04T05:11:58.90549Z","iopub.status.idle":"2022-07-04T05:12:08.420323Z","shell.execute_reply.started":"2022-07-04T05:11:58.905457Z","shell.execute_reply":"2022-07-04T05:12:08.419487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(model4, 'model.png',show_shapes=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T05:12:08.422022Z","iopub.execute_input":"2022-07-04T05:12:08.422476Z","iopub.status.idle":"2022-07-04T05:12:08.612741Z","shell.execute_reply.started":"2022-07-04T05:12:08.422438Z","shell.execute_reply":"2022-07-04T05:12:08.61199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Compile & Callback & Train**","metadata":{}},{"cell_type":"code","source":"# Model Compile\nmodel4.compile(optimizer='adam',\n                loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n                metrics=tf.keras.metrics.BinaryAccuracy())\n\n# Callbacks\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nes=tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=3) # This callback will stop the training when there is no improvement in the loss for three consecutive epochs.,\nlrr = ReduceLROnPlateau(monitor='val_binary_accuracy',patience=2) \ncb = [es, lrr]\n\n# Train\nhistory4=model4.fit(train,  \n                   epochs=5,\n                    #     verbose=\"auto\",\n                    callbacks=cb, \n                    #     validation_split=0.2,\n                    validation_data=val,\n                    shuffle=True,\n                    #     class_weight=None,\n                    #     sample_weight=None,\n                    #     initial_epoch=0,\n                    #     steps_per_epoch=None,\n                    #     validation_steps=None,\n                    #     validation_batch_size=None,\n                    #     validation_freq=1,\n                    #     max_queue_size=10,\n                    #     workers=1,\n                    #     use_multiprocessing=False,\n                    )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T05:12:08.614396Z","iopub.execute_input":"2022-07-04T05:12:08.615202Z","iopub.status.idle":"2022-07-04T05:37:01.636949Z","shell.execute_reply.started":"2022-07-04T05:12:08.615167Z","shell.execute_reply":"2022-07-04T05:37:01.636199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Save Model**","metadata":{}},{"cell_type":"code","source":"# Saving Model\nmodel4.save('with-Regularization.h5')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T05:37:01.638459Z","iopub.execute_input":"2022-07-04T05:37:01.638779Z","iopub.status.idle":"2022-07-04T05:37:01.689706Z","shell.execute_reply.started":"2022-07-04T05:37:01.638743Z","shell.execute_reply":"2022-07-04T05:37:01.689059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Plotting**","metadata":{}},{"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history4.history['binary_accuracy'],\"b\"),\nplt.plot(history3.history['val_binary_accuracy'],'r')\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['with Regularization', 'with Data Augmentation'], loc='upper left')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T05:37:01.690806Z","iopub.execute_input":"2022-07-04T05:37:01.691207Z","iopub.status.idle":"2022-07-04T05:37:01.93023Z","shell.execute_reply.started":"2022-07-04T05:37:01.691171Z","shell.execute_reply":"2022-07-04T05:37:01.92956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**I didnt expect improvement, Regularization is done to STOP THE MODEL FROM GETTING WORSE**","metadata":{}},{"cell_type":"markdown","source":"# **Transfer Learning**\n\nTransfer learning is a machine learning method where we reuse a pre-trained model as the starting point for a model on a new task.\n\n**To put it simply—a model trained on one task is repurposed on a second, related task as an optimization that allows rapid progress when modeling the second task. By applying transfer learning to a new task, one can achieve significantly higher performance than training with only a small amount of data.**\n\nTransfer learning is so common that it is rare to train a model for an image or natural language processing-related tasks from scratch.\n\n![tl](https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/616b35e31e352613b291f4c7_P46neY1rhMd5NbTxhCbaWWAnwJwhgCZjSLp2C2-6Pzf8sBEfAxtlhnAOV_Jq_gX-zOaztDWLrtFal42V-EDr86Gcd8QYrWh4uMxZ-_-X_Pd5tOge9EkBmFr7UxrEWLMwCNZi14WK%3Ds0.jpeg)\n\nTensorFlow Hub is an online repository of already trained TensorFlow models that you can use. These models can either be used as is, or they can be used for Transfer Learning. Transfer learning is a process where you take an existing trained model, and extend it to do additional work. This involves leaving the bulk of the model unchanged, while adding and retraining the final layers, in order to get a different set of possible outputs.\n\n![tl2](https://miro.medium.com/max/1000/0*xNjEPIZmPvKeqss6)","metadata":{}},{"cell_type":"markdown","source":"**The model that we'll use is MobileNet v2 (but any model from tf2 compatible image classifier URL from tfhub.dev would work).**","metadata":{}},{"cell_type":"markdown","source":"## **Train Data**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndatagen=ImageDataGenerator(\n#     featurewise_center=True,\n#     samplewise_center=True,\n#     featurewise_std_normalization=False,\n#     samplewise_std_normalization=False,\n#     zca_whitening=True,\n#     zca_epsilon=1e-06,\n#     rotation_range=90,\n#     width_shift_range=0.5,\n#     height_shift_range=0.5,\n#     brightness_range=None,\n#     shear_range=0.0,\n    zoom_range=0.5,\n#     channel_shift_range=0.0,\n#     fill_mode='nearest',\n#     cval=0.0,\n    horizontal_flip=True,\n    vertical_flip=True,\n    rescale=1./255,\n#     preprocessing_function=None,\n#     data_format=None,\n#     validation_split=0.0,\n#     interpolation_order=1,\n#     dtype=None\n)\n\ntrain=datagen.flow_from_dataframe(dataframe=train_df,\n                                     x_col=\"PATH\",\n                                     y_col=\"targets\",\n                                     target_size=(224, 224),\n                                     class_mode=\"binary\")\n\ntrain","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T05:37:01.931423Z","iopub.execute_input":"2022-07-04T05:37:01.931683Z","iopub.status.idle":"2022-07-04T05:37:02.124535Z","shell.execute_reply.started":"2022-07-04T05:37:01.931649Z","shell.execute_reply":"2022-07-04T05:37:02.123218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Validation Data**","metadata":{}},{"cell_type":"code","source":"val=datagen.flow_from_dataframe(dataframe=val_df,\n                                     x_col=\"PATH\",\n                                     y_col=\"targets\",\n                                     target_size=(224, 224),\n                                     class_mode=\"binary\")\n\nval","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T05:37:02.126223Z","iopub.execute_input":"2022-07-04T05:37:02.126523Z","iopub.status.idle":"2022-07-04T05:37:02.186019Z","shell.execute_reply.started":"2022-07-04T05:37:02.126484Z","shell.execute_reply":"2022-07-04T05:37:02.185349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Download MobileNet**\n\nDownload the MobileNet model and create a Keras model from it. **MobileNet** is expecting images of 224 $\\times$ 224 pixels, in 3 color channels (RGB).\n\nWith transfer learning we reuse parts of an already trained model and change the final layer, or several layers, of the model, and then retrain those layers on our own dataset.\n\nIn addition to complete models, TensorFlow Hub also distributes models without the last classification layer. These can be used to easily do transfer learning.\n\n*Note that we're calling the partial model from TensorFlow Hub (without the final classification layer) a `feature_extractor`. The reasoning for this term is that it will take the input all the way to a layer containing a number of features. So it has done the bulk of the work in identifying the content of an image, except for creating the final probability distribution. That is, it has extracted the features of the image.*\n\n![mobilenetv2](https://ars.els-cdn.com/content/image/1-s2.0-S2210670720309070-gr4.jpg)","metadata":{}},{"cell_type":"code","source":"import tensorflow_hub as hub\nmobilenet =\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/2\"\nfeature_extractor = hub.KerasLayer(mobilenet,input_shape=(224,224,3))\n\n# Freeze the variables in the feature extractor layer, so that the training only modifies the final classifier layer.\nfeature_extractor.trainable = False","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T05:37:02.187415Z","iopub.execute_input":"2022-07-04T05:37:02.187865Z","iopub.status.idle":"2022-07-04T05:37:04.013248Z","shell.execute_reply.started":"2022-07-04T05:37:02.187829Z","shell.execute_reply":"2022-07-04T05:37:04.012529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Build Model**","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nmodel5 = tf.keras.Sequential()\n\nmodel5.add(feature_extractor),\nmodel5.add(Dense(1, activation='sigmoid'))\n\nmodel5.summary()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T05:37:04.014677Z","iopub.execute_input":"2022-07-04T05:37:04.014927Z","iopub.status.idle":"2022-07-04T05:37:04.875899Z","shell.execute_reply.started":"2022-07-04T05:37:04.014894Z","shell.execute_reply":"2022-07-04T05:37:04.874437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Compile & Train**","metadata":{}},{"cell_type":"code","source":"# Model Compile\nmodel5.compile(optimizer='adam',\n                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                metrics=tf.keras.metrics.BinaryAccuracy())\n\n# Train\nhistory5=model5.fit(train,  \n                   epochs=10,\n                    #     verbose=\"auto\",\n                    #     callbacks=cb, \n                    #     validation_split=0.2,\n                    validation_data=val,\n                    shuffle=True,\n                    #     class_weight=None,\n                    #     sample_weight=None,\n                    #     initial_epoch=0,\n                    #     steps_per_epoch=None,\n                    #     validation_steps=None,\n                    #     validation_batch_size=None,\n                    #     validation_freq=1,\n                    #     max_queue_size=10,\n                    #     workers=1,\n                    #     use_multiprocessing=False,\n                    )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-04T05:37:04.877037Z","iopub.execute_input":"2022-07-04T05:37:04.877293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Save Model**","metadata":{}},{"cell_type":"code","source":"# Saving Model\nmodel5.save('transfer-learning.h5')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Plotting**","metadata":{}},{"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history5.history['binary_accuracy'],'b')\nplt.plot(history4.history['val_binary_accuracy'],'r')\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend('Transfer Learning','with Regularization' loc='upper left')\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Predict**","metadata":{}},{"cell_type":"code","source":"pred = model5.predict(val)\npred","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Converting Probabilities of Prediction**","metadata":{}},{"cell_type":"code","source":"# class map => { logit : class name }\nclass_map = { v: k for k, v in val.class_indices.items() }\n\n# apply a sigmoid because our model returns logits\npredict = tf.nn.sigmoid(pred)\npredict = tf.where(pred < 0.5, 0, 1).numpy()\n\npredict","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df['targets']=[0 if x=='cat' else 1 for x in val_df['targets']]\nval_df['predict']=predict\nval_df","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Confusion Matrix**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report \nfig, ax = plt.subplots(figsize = (9, 6))\n\ncm = confusion_matrix(val_df[\"targets\"],val_df[\"predict\"])\n\ndisp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [\"cat\", \"dog\"])\ndisp.plot(cmap = plt.cm.Blues, ax = ax)\nax.set_title(\"Validation Set\")\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **MisClassified**","metadata":{}},{"cell_type":"code","source":"error_df=[row['PATH'] for i,row in val_df.iterrows() if row[\"targets\"] != row['predict']]\nerror_df[:10]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.image import load_img\n\ndef show_error(nrow,ncol):    # This function creates a grid of images from  dataset.\n    \n    # Gallery using Matplotlib \n    fig, ax = plt.subplots(nrow,ncol,figsize = (12,12), dpi = 100)\n    axes = ax.ravel()\n    \n    for idx,ax  in enumerate(axes):\n        ax.imshow(load_img(error_df[idx]))\n    return fig.show()\n\nshow_error(5,5)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Test Data-For Submission**","metadata":{}},{"cell_type":"code","source":"test_dir = \"../input/dogs-vs-cats/test1.zip\"\n\nimport zipfile\nwith zipfile.ZipFile(test_dir, \"r\") as tst:\n    tst.extractall('.')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# File Path\nIMAGE_FOLDER_PATH = \"../working/test1\"\nFILE_PATH = [os.path.join(IMAGE_FOLDER_PATH,i) for i in os.listdir(IMAGE_FOLDER_PATH)]\n\n# Labels\nlabels=[i for i in os.listdir(IMAGE_FOLDER_PATH)]\n\n# Dataframe\ntest_df = pd.DataFrame({'PATH':FILE_PATH}) \ntest_df","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Processing Test Data**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndatagen=ImageDataGenerator(\n#     featurewise_center=True,\n#     samplewise_center=True,\n#     featurewise_std_normalization=False,\n#     samplewise_std_normalization=False,\n#     zca_whitening=True,\n#     zca_epsilon=1e-06,\n#     rotation_range=90,\n#     width_shift_range=0.5,\n#     height_shift_range=0.5,\n#     brightness_range=None,\n#     shear_range=0.0,\n    zoom_range=0.5,\n#     channel_shift_range=0.0,\n#     fill_mode='nearest',\n#     cval=0.0,\n    horizontal_flip=True,\n    vertical_flip=True,\n    rescale=1./255,\n#     preprocessing_function=None,\n#     data_format=None,\n#     validation_split=0.0,\n#     interpolation_order=1,\n#     dtype=None\n)\n\ntest=datagen.flow_from_dataframe(dataframe=test_df,\n                                     x_col=\"PATH\",\n                                     y_col=None,\n                                      class_mode=None,\n                                     shuffle=False,\n                                     target_size=(224, 224),\n                                     )\n\ntest","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx=[int(str(x).split('.')[0]) for x in labels]\nidx[:5]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\npred=model5.predict(test) \nclasses=np.argmax(pred,axis=1)\nlen(classes)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Final Submission**","metadata":{}},{"cell_type":"code","source":"submission=pd.DataFrame({'id': idx,\n                         'label': classes,\n                        })\n# submission\nsubmission.to_csv('submission.csv', index=False)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Suggestions:-\n* Kaggle - https://www.kaggle.com/pythonkumar\n* GitHub - https://github.com/KumarPython​\n* Twitter - https://twitter.com/KumarPython\n* LinkedIn - https://www.linkedin.com/in/kumarpython/","metadata":{}}]}