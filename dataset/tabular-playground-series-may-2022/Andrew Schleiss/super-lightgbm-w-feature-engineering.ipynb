{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# To Do \n\n#### Feature engineering: \n* Relative features (float cols) ----Completed outside kaggle [Notebook showing process](https://www.kaggle.com/code/slythe/feature-creation-selection-feature-engine/edit)\n* Grouped Mathematical features (float cols)\n\n* Categorical columns (Int columns)\n\n* Text Features - Done \n\n* Hyper parameter tuning","metadata":{}},{"cell_type":"markdown","source":"# ðŸ“© Import Libraries ðŸ“© ","metadata":{}},{"cell_type":"code","source":"# Data and visualization\nimport pandas as pd \nimport numpy as np \nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\nfrom collections import Counter\n\n# hyperparameter tuning \nimport optuna \n\n#modelling\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.calibration import calibration_curve, CalibratedClassifierCV","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-07T10:06:36.003881Z","iopub.execute_input":"2022-05-07T10:06:36.004712Z","iopub.status.idle":"2022-05-07T10:06:39.115361Z","shell.execute_reply.started":"2022-05-07T10:06:36.004639Z","shell.execute_reply":"2022-05-07T10:06:39.11452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# parameters \nsns.set_theme()\n\nCALIBRATION = False\nEPOCHS = 5000\n\nOPTUNA = False","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:06:39.117825Z","iopub.execute_input":"2022-05-07T10:06:39.118898Z","iopub.status.idle":"2022-05-07T10:06:39.124193Z","shell.execute_reply.started":"2022-05-07T10:06:39.118858Z","shell.execute_reply":"2022-05-07T10:06:39.123651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ’¾ Load Data ðŸ’¾","metadata":{}},{"cell_type":"code","source":"train_original = pd.read_csv(\"../input/tabular-playground-series-may-2022/train.csv\",index_col = 0)\ntest_original = pd.read_csv(\"../input/tabular-playground-series-may-2022/test.csv\",index_col = 0)\n\n# train = pd.read_csv(\"../input/tabular-playground-series-may-2022/train.csv\",index_col = 0)\n# test = pd.read_csv(\"../input/tabular-playground-series-may-2022/test.csv\",index_col = 0)\nsub = pd.read_csv(\"../input/tabular-playground-series-may-2022/sample_submission.csv\",index_col = 0)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:06:39.125304Z","iopub.execute_input":"2022-05-07T10:06:39.125515Z","iopub.status.idle":"2022-05-07T10:06:53.763348Z","shell.execute_reply.started":"2022-05-07T10:06:39.125487Z","shell.execute_reply":"2022-05-07T10:06:53.762023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸŒŸ Feature Engineering ðŸŒŸ","metadata":{}},{"cell_type":"markdown","source":"* Unicode (ord) code adapted from [cabaxiom](https://www.kaggle.com/code/cabaxiom/tps-may-22-eda-lgbm-model#Feature-Engineering)\n\ncabaxiom already identified f_29 and f_30 as potential categorical columns. \nLets try improve on this","metadata":{}},{"cell_type":"code","source":"int_cols = train_original.dtypes[(train_original.dtypes ==\"int64\") & (train_original.dtypes.index != \"target\") ].index\nfloat_cols = train_original.dtypes[train_original.dtypes ==\"float64\" ].index","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:06:53.766947Z","iopub.execute_input":"2022-05-07T10:06:53.767335Z","iopub.status.idle":"2022-05-07T10:06:53.776897Z","shell.execute_reply.started":"2022-05-07T10:06:53.767304Z","shell.execute_reply":"2022-05-07T10:06:53.776013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_letters = ['A', 'B', 'D', 'E', 'P', 'C', 'S', 'G', 'F', 'Q', 'H', 'N', 'K', 'R', 'M', 'T', 'O', 'J', 'I', 'L']\n\ndef feature_engineering(df):\n    #letter count \n#     for letter in all_letters: \n#         #Count letter\n#         df[letter] = df[\"f_27\"].str.count(letter)\n#         #Contains letter\n#         df[f\"contains_{letter}\"]  = df[\"f_27\"].str.contains(letter).astype(\"category\")\n    \n    #Unicoding\n    for i in range(10):\n        df[\"f_27_\"+str(i)] = df[\"f_27\"].str[i].apply(lambda x: ord(x) - ord(\"A\"))\n    \n    # Get Unique letters\n    df[\"unique_text_str\"] = df[\"f_27\"].apply(lambda x :  ''.join([str(n) for n in list(set(x))]) )\n    df[\"unique_text_str\"] = df[\"unique_text_str\"].astype(\"category\")\n\n    df[\"unique_text_len\"] = df.f_27.apply(lambda s: len(set(s)))\n    \n#     #Merge categorical columns \n#     df[\"f29_f30\"] = df[[\"f_29\",\"f_30\"]].apply(lambda x: str( x[\"f_29\"] ) + str(x[\"f_30\"]), axis =1) \n#     df[\"f29_f30\"] = df[\"f29_f30\"].astype(\"category\")\n     \n#     # get max and min letter (use 'Counter' to get count of letters and then get max/min from this dictionary )\n#     df[\"max_letter\"] = df[\"f_27\"].apply(lambda x : Counter(x)).apply(lambda x : max(x, key=x.get))\n#     df[\"max_letter\"] = df[\"max_letter\"].astype(\"category\")\n#     df[\"min_letter\"] = df[\"f_27\"].apply(lambda x : Counter(x)).apply(lambda x : min(x, key=x.get))\n#     df[\"min_letter\"] = df[\"min_letter\"].astype(\"category\")\n    \n    return df\n\ntrain = feature_engineering(train_original)\ntest = feature_engineering(test_original)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:06:53.778123Z","iopub.execute_input":"2022-05-07T10:06:53.778392Z","iopub.status.idle":"2022-05-07T10:07:24.349463Z","shell.execute_reply.started":"2022-05-07T10:06:53.778352Z","shell.execute_reply":"2022-05-07T10:07:24.348193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mathematical Features \n* We will do this with certain columns i.e. the float columns (but certain groupings)\n* We need a list of functions and/or function names, e.g. [np.sum, â€˜meanâ€™] ","metadata":{}},{"cell_type":"code","source":"train_original[float_cols].describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:07:24.351091Z","iopub.execute_input":"2022-05-07T10:07:24.351339Z","iopub.status.idle":"2022-05-07T10:07:25.037899Z","shell.execute_reply.started":"2022-05-07T10:07:24.35131Z","shell.execute_reply":"2022-05-07T10:07:25.037092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Group Float columns \n* We can see from the above that certain columns have similar std/ min/ max, we will group them\n* f_00 to f_06 => Group1\n* f_19 to f_26 => Group2\n* f28 looks to be seperate from both groups","metadata":{}},{"cell_type":"code","source":"group1_float =['f_00','f_01','f_02','f_03','f_04','f_05','f_06']\ngroup2_float = ['f_19','f_20','f_21','f_22','f_23','f_24','f_25','f_26']\n\ndef mathematical_feats(df,cols, suffix):\n    df[f\"sum_{suffix}\"] = df[cols].sum(axis = 1)\n    df[f\"mean_{suffix}\"] = df[cols].mean(axis = 1)\n    df[f\"std_{suffix}\"] = df[cols].std(axis = 1)\n    df[f\"min_{suffix}\"] = df[cols].min(axis = 1)\n    df[f\"max_{suffix}\"] = df[cols].max(axis = 1)\n    df[f\"median_{suffix}\"] = df[cols].median(axis = 1)\n    df[f\"mad_{suffix}\"] = df[cols].mad(axis = 1)\n\n    #potentially change periods OR changes axis OR fillna with actuals\n    #df[f\"diff_{suffix}\"] = df[cols].diff(periods=1, axis = 1)\n    \n    df[f\"max-min_{suffix}\"] = df[cols].max(axis = 1) - df[cols].min(axis = 1)\n    df[f\"q01_{suffix}\"] = df[cols].quantile(q= 0.1, axis =1)\n    df[f\"q25_{suffix}\"] = df[cols].quantile(q= 0.25, axis =1) \n    #df[f\"q50_{suffix}\"] = df[cols].quantile(q= 0.5, axis =1) \n    df[f\"q75_{suffix}\"] = df[cols].quantile(q= 0.75, axis =1) \n    df[f\"q95_{suffix}\"] = df[cols].quantile(q= 0.95, axis =1) \n    df[f\"q99_{suffix}\"] = df[cols].quantile(q= 0.99, axis =1)\n    df[f\"kurt_{suffix}\"] = df[cols].kurt(axis =1) \n    df[f\"skew_{suffix}\"] = df[cols].skew( axis =1)\n    \n    return df\n\nmathematical_feats(train, float_cols, \"group2_float\")\nmathematical_feats(test, float_cols, \"group2_float\")\n# mathematical_feats(train, float_cols, \"group1_float\")\n# mathematical_feats(test, float_cols, \"group1_float\")","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:07:25.039509Z","iopub.execute_input":"2022-05-07T10:07:25.040466Z","iopub.status.idle":"2022-05-07T10:07:35.22039Z","shell.execute_reply.started":"2022-05-07T10:07:25.040426Z","shell.execute_reply":"2022-05-07T10:07:35.219013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Drop unimportant features \nFrom previous runs ","metadata":{}},{"cell_type":"code","source":"feats = ['q50_group1_float', 'mean_group1_float']\n\ndef drop_feats(df, feats):\n    df.drop(feats ,axis = 1 ,inplace = True )\n    return df \n\n# drop_feats(train, feats)\n# drop_feats(test, feats)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:07:35.221919Z","iopub.execute_input":"2022-05-07T10:07:35.222119Z","iopub.status.idle":"2022-05-07T10:07:35.227451Z","shell.execute_reply.started":"2022-05-07T10:07:35.222094Z","shell.execute_reply":"2022-05-07T10:07:35.226583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸš€ Base Model ðŸš€","metadata":{}},{"cell_type":"code","source":"categorical_features = [\"unique_text_str\"\n                        #, \"f29_f30\"\n                        #,\"min_letter\"\n                        #,\"max_letter\"\n                        #,\"f_29\",\"f_30\"\n                       ]\n\ncategorical_features.extend( [col for col in train.columns if \"contains\" in col] ) ","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:07:35.228443Z","iopub.execute_input":"2022-05-07T10:07:35.228707Z","iopub.status.idle":"2022-05-07T10:07:35.241154Z","shell.execute_reply.started":"2022-05-07T10:07:35.228676Z","shell.execute_reply":"2022-05-07T10:07:35.240211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop the text column as we already have features created earlier\nX = train.drop([\"target\",\"f_27\"],axis =1)\ny= train[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:07:35.244001Z","iopub.execute_input":"2022-05-07T10:07:35.244198Z","iopub.status.idle":"2022-05-07T10:07:36.077482Z","shell.execute_reply.started":"2022-05-07T10:07:35.244173Z","shell.execute_reply":"2022-05-07T10:07:36.076616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optuna - Hyperparameter tuning ","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    # 2. Suggest values of the hyperparameters using a trial object.\n    lgb_params = {\n        'objective': 'binary',\n        'metric': \"auc\",\n        'verbosity': -1,\n        'num_iterations': EPOCHS,\n        \"num_threads\": -1,\n        #\"force_col_wise\": True,\n        \"learning_rate\": trial.suggest_float('learning_rate',0.01,0.2),\n        'boosting_type': trial.suggest_categorical('boosting',[\"gbdt\"]),\n        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 10, 256),\n        #'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1000, 10000),\n        'max_depth': trial.suggest_int('max_depth', -1,15)\n    }\n        \n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial,metric = \"auc\")  \n    \n    train_set = lgb.Dataset(X_train, y_train)\n    valid_set = lgb.Dataset(X_test, y_test)\n\n    model = lgb.train(params=lgb_params,\n                          train_set= train_set, \n                          valid_sets= [valid_set], \n                          num_boost_round= EPOCHS,\n                          callbacks=[lgb.early_stopping(30),pruning_callback] ,categorical_feature = categorical_features ) \n\n    val_preds = model.predict(X_test)\n    auc = roc_auc_score(y_test, val_preds)\n    print(\"Val AUC:\", auc)\n    \n    return auc","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:07:36.078681Z","iopub.execute_input":"2022-05-07T10:07:36.078888Z","iopub.status.idle":"2022-05-07T10:07:36.09194Z","shell.execute_reply.started":"2022-05-07T10:07:36.07886Z","shell.execute_reply":"2022-05-07T10:07:36.090937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if OPTUNA:\n    \n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=200)\n\n    trial = study.best_trial\n    best_params = study.best_params\n\n    #Print our results\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n    print(\"Best trial:\")\n    print(\" AUC Value: {}\".format(trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:07:36.092767Z","iopub.execute_input":"2022-05-07T10:07:36.093444Z","iopub.status.idle":"2022-05-07T10:07:36.105217Z","shell.execute_reply.started":"2022-05-07T10:07:36.093415Z","shell.execute_reply":"2022-05-07T10:07:36.104568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Base model","metadata":{}},{"cell_type":"code","source":"model = lgb.LGBMClassifier(\n    objective= 'binary',\n    metric= \"auc\",\n    num_iterations = EPOCHS,\n    num_threads= -1,\n    learning_rate= 0.18319492258552644,\n    boosting= 'gbdt',\n    lambda_l1= 0.00028648667113792726,\n    lambda_l2= 0.00026863027834978876,\n    num_leaves= 229,\n    max_depth= 0,\n    min_child_samples=80,\n    max_bins=511, \n    random_state=42 \n)\n\nmodel.fit(X_train,y_train, eval_set=[(X_test,y_test)], callbacks = [lgb.early_stopping(30)],eval_metric=\"auc\" , \n          categorical_feature = categorical_features\n         )\n\nval_preds = model.predict_proba(X_test)\ny_preds = model.predict_proba(X_train)\n\nprint(\"Intrinsic AUC:\", roc_auc_score(y_train, y_preds[:,1]))\nprint(\"Validation AUC:\", roc_auc_score(y_test, val_preds[:, 1] ))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:07:36.106212Z","iopub.execute_input":"2022-05-07T10:07:36.106405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_importance = pd.DataFrame(data = model.feature_importances_, index= train.drop([\"target\",\"f_27\"],axis =1).columns).sort_values(ascending = False, by= [0] )\n\nplt.figure(figsize= (25,10))\nsns.barplot(y= feat_importance[0], x= feat_importance.index)\nplt.xticks(rotation = 90) \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_importance[feat_importance[0] <50].index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calibration \nTaken from last months kernel [TPS April ](https://www.kaggle.com/code/slythe/calibrated-xgboost-human-activity-recognition)","metadata":{}},{"cell_type":"code","source":"prob_true, prob_pred = calibration_curve(y_test, val_preds[:,1], n_bins=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calibrator = CalibratedClassifierCV(model, method = \"isotonic\", cv='prefit')\ncalibrator.fit(X_test, y_test)\ncal_preds = calibrator.predict_proba(X_test)\n\nprint(\"Validation AUC:\" , roc_auc_score(y_test, val_preds[:, 1] ))\nprint(\"Calibrated AUC:\" , roc_auc_score(y_test, cal_preds[:, 1] ))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,10))\nplt.plot(prob_pred,prob_true, marker='o', linewidth=1, label='xgb model probabilities')\n\n# reference line\nline = mlines.Line2D([0, 1], [0, 1], color='black')\ntransform = ax.transAxes\nline.set_transform(transform)\nax.add_line(line)\n#plt.axvline(x=0.2, color = \"r\")\nfig.suptitle('Calibration plot')\nax.set_xlabel('Predicted probability (mean)')\nax.set_ylabel('Fraction of positives (%True  in each bin)')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# âŽ Cross validation âŽ","metadata":{"execution":{"iopub.status.busy":"2022-05-04T13:15:44.716577Z","iopub.execute_input":"2022-05-04T13:15:44.717395Z","iopub.status.idle":"2022-05-04T13:15:44.721768Z","shell.execute_reply.started":"2022-05-04T13:15:44.717335Z","shell.execute_reply":"2022-05-04T13:15:44.720717Z"}}},{"cell_type":"code","source":"cv = KFold(n_splits = 5, shuffle = True,random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nauc_cv = []\nfor fold, (idx_train, idx_val) in enumerate(cv.split(X,y)):\n    print(\"\\n\")\n    print(\"#\"*10, f\"Fold: {fold}\",\"#\"*10)\n    X_train , X_test = X.iloc[idx_train] , X.iloc[idx_val]\n    y_train , y_test = y[idx_train] , y[idx_val]\n    \n    model = lgb.LGBMClassifier(\n    objective= 'binary',\n    metric= \"auc\",\n    num_iterations = EPOCHS,\n    num_threads= -1,\n    learning_rate= 0.18319492258552644,\n    boosting= 'gbdt',\n    lambda_l1= 0.00028648667113792726,\n    lambda_l2= 0.00026863027834978876,\n    num_leaves= 229,\n    max_depth= 0,\n    min_child_samples=80,\n    max_bins=511, \n    random_state=42 )\n    \n    model.fit(X_train,y_train, eval_set=[(X_test,y_test)], callbacks = [lgb.early_stopping(30)],eval_metric=\"auc\")\n    \n    if CALIBRATION:\n        calibrator = CalibratedClassifierCV(model, method = \"isotonic\", cv='prefit')\n        calibrator.fit(X_test, y_test)\n        auc = roc_auc_score(y_test, calibrator.predict_proba(X_test)[:, 1])\n        print(\"\\n Calibration AUC:\" , auc)\n        preds.append(calibrator.predict_proba(test.drop(\"f_27\",axis =1))[:, 1])\n    else:\n        auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n        print(\"\\n Validation AUC:\" , auc)\n        preds.append(model.predict_proba(test.drop(\"f_27\",axis =1))[:, 1])\n        \n    auc_cv.append(auc)\n    \nprint(\"FINAL AUC: \", np.mean(auc_cv))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ“¡ Submission ðŸ“¡","metadata":{}},{"cell_type":"code","source":"sub[\"target\"] = np.array(preds).mean(axis =0)\nsub.to_csv(\"submission.csv\")\nsub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.plot(kind= \"hist\",figsize= (25,8))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}