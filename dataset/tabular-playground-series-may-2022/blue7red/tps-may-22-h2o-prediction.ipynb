{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \n\nimport h2o\nfrom h2o.automl import H2OAutoML\n\nTRAIN_PATH = \"../input/tabular-playground-series-may-2022/train.csv\"\nTEST_PATH = \"../input/tabular-playground-series-may-2022/test.csv\"\nSAMPLE_SUBMISSION_PATH = \"../input/tabular-playground-series-may-2022/sample_submission.csv\"\nSUBMISSION_PATH = \"submission.csv \"\n\nID = \"id\"\nTARGET = \"target\"\n\nNEW_TRAIN_PATH = \"train.csv\"\nNEW_TEST_PATH = \"test.csv\"\n\nSEED_LIST = [7,77,777]\nMAX_RUNTIME_SECS = 60 * 3","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-07T08:52:36.889633Z","iopub.execute_input":"2022-05-07T08:52:36.889978Z","iopub.status.idle":"2022-05-07T08:52:37.160369Z","shell.execute_reply.started":"2022-05-07T08:52:36.889947Z","shell.execute_reply":"2022-05-07T08:52:37.159285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ndef create_tf_idf_feats(corpus, ngram_range = (1, 1), max_features = None):\n    vectorizer = TfidfVectorizer(analyzer = 'char', lowercase = False, \n                                 ngram_range = ngram_range, max_features = max_features)\n    X = vectorizer.fit_transform(corpus).todense()\n    char_mapper = {y:x for x, y in vectorizer.vocabulary_.items()}\n    column_names = ['tfidf_{}'.format(char_mapper[i]) for i in range(len(char_mapper))]\n    return pd.DataFrame(X, columns = column_names)\n\ntrain = pd.read_csv(TRAIN_PATH)\ntest = pd.read_csv(TEST_PATH)\n\n# Create all texts array\nall_texts = pd.concat([train[['f_27']], test[['f_27']]]).reset_index(drop = True)\ncorpus = all_texts['f_27'].values\n\n# https://www.kaggle.com/code/alexryzhkov/tps-may-22-lightautoml-here-again/notebook\n# Calculate TF-IDF features for unigrams and top-20 bigrams\nall_texts = pd.concat([all_texts, \n                      create_tf_idf_feats(corpus),\n                      create_tf_idf_feats(corpus, (2,2), 20)], axis = 1)\n\nfor i in range(10):\n    all_texts[f'ch{i}'] = all_texts.f_27.str.get(i).apply(ord) - ord('A')\n\nall_texts[\"unique_characters\"] = all_texts.f_27.apply(lambda s: len(set(s)))\n\n# How often the text occurs in the whole dataset\nall_texts['value_frequency'] = all_texts['f_27'].map(all_texts['f_27'].value_counts() / len(all_texts))\n\nall_texts.drop(columns = ['f_27'], inplace = True)\n\ntrain = pd.concat([train,\n                       all_texts.iloc[:len(train), :]], axis = 1)\ntest = pd.concat([test,\n                       all_texts.iloc[len(train):, :].reset_index(drop = True)], axis = 1)\n\nID = \"id\"\nTARGET = \"target\"\nfor col in train.columns:\n    if col == ID or col == TARGET:\n        continue\n        \n    if type(col) != str:\n        mean_minus_median = train[col].mean() - train[col].median()\n        mean_of_mean_median_sum =  (train[col].mean() + train[col].median())/2\n        train[\"mean_minus_median\"] = train[col] - mean_minus_median\n        train[\"mean_of_mean_median_sum\"] = train[col] - mean_of_mean_median_sum\n\n        mean_minus_median = test[col].mean() - test[col].median()\n        mean_of_mean_median_sum =  (test[col].mean() + test[col].median())/2\n        test[\"mean_minus_median\"] = test[col] - mean_minus_median\n        test[\"mean_of_mean_median_sum\"] = test[col] - mean_of_mean_median_sum\n    \ntrain.to_csv(NEW_TRAIN_PATH,index=False)\ntest.to_csv(NEW_TEST_PATH,index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T08:52:37.162181Z","iopub.execute_input":"2022-05-07T08:52:37.162585Z","iopub.status.idle":"2022-05-07T08:53:45.833798Z","shell.execute_reply.started":"2022-05-07T08:52:37.162546Z","shell.execute_reply":"2022-05-07T08:53:45.83276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h2o.init()\n\ntrain = h2o.import_file(NEW_TRAIN_PATH)\ntest = h2o.import_file(NEW_TEST_PATH)\n\nx = train.columns\ny = TARGET\n\nx.remove(y)\nx.remove(ID) #remove id  \n\npred_test = []\nfor selSeed in SEED_LIST:\n    aml_y = H2OAutoML(max_runtime_secs=MAX_RUNTIME_SECS, seed=selSeed)\n    aml_y.train(x=x, y=y, training_frame=train)\n\n    preds_y = aml_y.predict(test)\n    pred_test.append(preds_y.as_data_frame().predict) ","metadata":{"execution":{"iopub.status.busy":"2022-05-07T08:53:45.83473Z","iopub.status.idle":"2022-05-07T08:53:45.835151Z","shell.execute_reply.started":"2022-05-07T08:53:45.834966Z","shell.execute_reply":"2022-05-07T08:53:45.834985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import stats\nmodeResult = stats.mode(pred_test, axis=0)\nfinal_test_pred = modeResult.mode\n\nsubmission = pd.read_csv(SAMPLE_SUBMISSION_PATH)\nsubmission[TARGET] = final_test_pred[0]\nsubmission.to_csv(SUBMISSION_PATH, index=False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T08:53:45.836833Z","iopub.status.idle":"2022-05-07T08:53:45.837358Z","shell.execute_reply.started":"2022-05-07T08:53:45.837156Z","shell.execute_reply":"2022-05-07T08:53:45.837176Z"},"trusted":true},"execution_count":null,"outputs":[]}]}