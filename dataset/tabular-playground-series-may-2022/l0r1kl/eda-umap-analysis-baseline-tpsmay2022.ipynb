{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EDA for Tabular Playground Series May 2022.\n\n## What to do in this notebook.\n- Show basic infomations.\n- Plot distribution of features.\n- Decompose Train datas.\n- Classification by LightGBM and Catboost.\n- Show Feature Importances.\n- Predict Test Data.","metadata":{"id":"Z_qlEzrNNoHY"}},{"cell_type":"markdown","source":"## Competition's OverView","metadata":{"id":"7qvJYAYkXtkQ"}},{"cell_type":"markdown","source":"### English\n---\nThe May edition of the 2022 Tabular Playground series binary classification problem that includes a number of different feature interactions.  \nThis competition is an opportunity to explore various methods for identifying and exploiting these feature interactions.\n\n### Japanese\n---\n2022å¹´5æœˆå·ã®Tabular Playgroundã‚·ãƒªãƒ¼ã‚ºã®2å€¤åˆ†é¡žå•é¡Œã¯ã€å¤šãã®ç•°ãªã‚‹ç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã‚’å«ã‚“ã§ã„ã¾ã™ã€‚  \nã“ã®ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã¯ã€ã“ã‚Œã‚‰ã®ç‰¹å¾´çš„ãªç›¸äº’ä½œç”¨ã‚’è­˜åˆ¥ã—ã€åˆ©ç”¨ã™ã‚‹ãŸã‚ã®æ§˜ã€…ãªæ–¹æ³•ã‚’æŽ¢ã‚‹æ©Ÿä¼šã§ã™ã€‚\n\n## Data Description\n\nFor this challenge, you are given (simulated) manufacturing control data and are tasked to predict whether the machine is in state 0 or state 1.  \nThe data has various feature interactions that may be important in determining the machine state.\n\nGood luck!\n\n### Files\n- train.csv - the training data, which includes normalized continuous data and categorical data\n- test.csv - the test set; your task is to predict binary target variable which represents the state of a manufacturing process\n- sample_submission.csv - a sample submission file in the correct format\n","metadata":{"id":"r0A_V21mXonj","_kg_hide-input":true}},{"cell_type":"markdown","source":"# SETUP","metadata":{"id":"3QWn7ObdWWTi"}},{"cell_type":"markdown","source":"## import modules.","metadata":{"id":"wKipUeM_AeQb"}},{"cell_type":"code","source":"# !sh /content/drive/MyDrive/Colab\\ Notebooks/Competitions/kaggle/TPS_MAY_2022/install_modules.sh","metadata":{"id":"-USjw5cfXwXO","_kg_hide-input":true,"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# basic module.\nimport string\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# to decompose.\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom umap import UMAP\nfrom umap.parametric_umap import ParametricUMAP\n\n# for classification and visualizing feature importances.\nimport lightgbm as lgb\nimport catboost as cbt\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import (classification_report, roc_auc_score)\n","metadata":{"id":"Hw29tJoRWwmr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Util Function","metadata":{"id":"u0f50hHnNEmj"}},{"cell_type":"code","source":"import logging\nimport time\nfrom contextlib import contextmanager\n\nimport warnings\nwarnings.simplefilter('ignore')\n\n@contextmanager\ndef timer(name, logger=None, level=logging.DEBUG):\n    print_ = print if logger is None else lambda msg: logger.log(level, msg)\n    t0 = time.time()\n    print_(f'[{name}] start')\n    yield\n    print_(f'[{name}] done in {time.time() - t0:.0f} s')\n    ","metadata":{"id":"lgiGBS_zLozi","_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config(object):\n    def __init__(self):\n        # data paths\n        self.train_data = '../input/tabular-playground-series-may-2022/train.csv'\n        self.test_data = '../input/tabular-playground-series-may-2022/test.csv'\n        self.count_each_alphabet = '../input/tps-may-2022-eda-output/count_each_alphabet.csv'\n        self.train_decompose_X = '../input/tps-may-2022-eda-output/train_decompose_X.csv'\n        self.train_decompose_y = '../input/tps-may-2022-eda-output/train_decompose_y.csv'\n        self.submission_file = '../input/tabular-playground-series-may-2022/sample_submission.csv'\n\n        # general\n        self.random_state = 0\n\n        # training parameters\n        self.fold = 10\n        self.lightgbm = dict(\n            random_state=self.random_state,\n            # device='gpu',\n        )\n\n        self.catboost = dict(\n            random_state=self.random_state,\n            task_type='GPU',\n        )\n    def __set_seeds(self):\n        return None\n\nconfig = Config()\n","metadata":{"id":"12F1y9J_w9o1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## load dataset.","metadata":{"id":"Mtuoi0zMIC2U"}},{"cell_type":"code","source":"train_data = pd.read_csv(config.train_data, index_col='id')\ntest_data = pd.read_csv(config.test_data, index_col='id')\n\nX_cols = [f'f_{i:02}' for i in range(31)]\ny_cols = ['target']\n\ndata = pd.concat([train_data.loc[:, X_cols], test_data.loc[:, X_cols]], axis=0)\n","metadata":{"id":"sDjsdGH4w7Y5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Show Basic Infomation.","metadata":{"id":"jcB_xo2PWbPt"}},{"cell_type":"markdown","source":"## code","metadata":{}},{"cell_type":"code","source":"def get_basic_infomation(dataframe:pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    \n    \"\"\"\n    dtypes = dataframe.dtypes\n    dtypes.name = 'dtype'\n    isna = dataframe.isna().sum()\n    isna.name = 'null-count'\n    nunique = dataframe.nunique()\n    nunique.name = 'nunique'\n    infomations = pd.concat([dtypes, nunique, isna, dataframe.describe().T], axis=1)\n\n    object_columns = dtypes[dtypes == 'object'].index\n    \n    if object_columns.shape[0] > 0:\n        for idx in object_columns:\n            infomations.loc[idx, 'count'] = dataframe.loc[:, idx].notna().sum()\n    \n    return infomations\n","metadata":{"id":"lh7n3KbQGPOb","_kg_hide-input":true,"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Data","metadata":{"id":"QzVBIfjwjR03"}},{"cell_type":"code","source":"get_basic_infomation(train_data)\n","metadata":{"id":"KXHJRfWk-KRA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test Data","metadata":{"id":"iHG4jwY6jVxC"}},{"cell_type":"code","source":"get_basic_infomation(test_data)\n","metadata":{"id":"FH04gocn_9NC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## All Data(concat Train Data and Test Data)","metadata":{"id":"-2Cv6tGIjXyd"}},{"cell_type":"code","source":"get_basic_infomation(data)\n","metadata":{"id":"eoVNDMLnElWf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert data type","metadata":{}},{"cell_type":"markdown","source":"- Float Features.  \nNormal features?\n- Int Features.  \nCategorical ID?\n- Object Features.  \nToo many to handle as categories.  \nI try count the alphabet. ","metadata":{"id":"LDO8bnbjA_PJ"}},{"cell_type":"code","source":"# get columns by data type.\nflt_cols = data.dtypes[data.dtypes == 'float64'].index.to_list()\nint_cols = data.dtypes[data.dtypes == 'int64'].index.to_list()\nobj_cols = data.dtypes[data.dtypes == 'object'].index.to_list()\n","metadata":{"id":"bGaxkEneCjzl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # convert dtype\n# train_data.loc[:, flt_cols] = train_data.loc[:, flt_cols].astype(np.float32)\n# train_data.loc[:, int_cols] = train_data.loc[:, int_cols].astype(np.int8)\n\n# test_data.loc[:, flt_cols] = test_data.loc[:, flt_cols].astype(np.float32)\n# test_data.loc[:, int_cols] = test_data.loc[:, int_cols].astype(np.int8)\n\n# data.loc[:, flt_cols] = data.loc[:, flt_cols].astype(np.float32)\n# data.loc[:, int_cols] = data.loc[:, int_cols].astype(np.int8)\n","metadata":{"id":"4pGCjll8c9xk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot Distribution of Features","metadata":{"id":"Svpg8HNpZJ5M"}},{"cell_type":"markdown","source":"## Distribution of Float Features","metadata":{"id":"yz9dWlsVRiM_"}},{"cell_type":"markdown","source":"### code","metadata":{}},{"cell_type":"code","source":"def plot_violin_multicolumns(\n    dataframe:pd.DataFrame,\n    columns:list,\n    hue: str = None\n):\n    \"\"\"\n    \"\"\"\n    if not(hue is None):\n        hue_col_idx = np.argwhere(np.array(columns) == hue).flatten()[0]\n        cols = np.delete(np.array(columns), hue_col_idx)\n\n        violin_data = dataframe.loc[:, cols].melt(var_name='columns', value_name='value')\n        violin_hue = pd.concat([dataframe[hue] for _ in range(len(cols))], axis=0)\n        violin_hue.name = hue\n        violin_hue.index = np.arange(violin_hue.shape[0])\n        violin_data = pd.concat([violin_data, violin_hue], axis=1)\n    \n    else:\n        violin_data = dataframe.loc[:, columns].melt(var_name='columns', value_name='value')\n    \n    fig, axes = plt.subplots(1, 1, figsize=(20, 8))\n    sns.violinplot(\n        x='columns', \n        y='value',\n        hue=hue if not(hue is None) else None, \n        data=violin_data,\n        split=True if not(hue is None) else None,\n        ax=axes\n    )\n    \n    plt.close()\n    \n    return fig\n","metadata":{"id":"hI6SBJKQGXlV","_kg_hide-input":true,"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### edit data","metadata":{}},{"cell_type":"code","source":"# labeling train or test.\ndata['type'] = None\ndata.loc[train_data.index, 'type'] = 'train'\ndata.loc[test_data.index, 'type'] = 'test'\n\n# temporary scaling... \ntrain_data.loc[:, 'f_28'] = train_data.loc[:, 'f_28'] / 100.0\ndata.loc[:, 'f_28'] = data.loc[:, 'f_28'] / 100.0\n\n# save memory...\ndel test_data\n","metadata":{"id":"IrKyLxsEIOqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target 0 vs 1","metadata":{"id":"thevx8ogaYMY"}},{"cell_type":"code","source":"plot_violin_multicolumns(train_data, flt_cols + ['target'], 'target')\n","metadata":{"id":"zTE1IBA6OqCd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train vs Test","metadata":{"id":"tWNmBkbnaT4_"}},{"cell_type":"code","source":"plot_violin_multicolumns(data, flt_cols + ['type'], 'type')\n","metadata":{"id":"FTElm69XJMQZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inverse scaling...\ntrain_data.loc[:, 'f_28'] = train_data.loc[:, 'f_28'] * 100.0\ndata.loc[:, 'f_28'] = data.loc[:, 'f_28'] * 100.0\n","metadata":{"id":"P7lr_frvmHW8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution of Integer Colunms (using value counts)\n","metadata":{"id":"OqMx8clalcae"}},{"cell_type":"markdown","source":"### code","metadata":{}},{"cell_type":"code","source":"def plot_value_counts(\n    dataframe:pd.DataFrame,\n    column:str,\n    hue: str = None,\n    ax=None\n):\n    \"\"\"\n    \"\"\"\n    if hue is None:\n        counts = pd.DataFrame(\n            dataframe.loc[:, column].value_counts()\n        ).reset_index()\n        counts.columns = ['value', 'counts']\n\n    else:\n        l_counts = []\n        for hue_col in dataframe[hue].unique():\n            idx = dataframe[dataframe[hue]==hue_col].index\n            hue_counts = pd.DataFrame(\n                dataframe.loc[idx, column].value_counts()\n            ).reset_index()\n            hue_counts.columns = ['value', 'counts']\n            hue_counts[hue] = hue_col\n            l_counts.append(hue_counts)\n        counts = pd.concat(l_counts)\n\n    sns.barplot(\n        x='value', \n        y='counts',\n        data=counts, \n        hue=hue if not(hue is None) else None,\n        ax=ax\n    )\n    ax.set_title(column)\n    ax.legend(loc='upper right')\n    ax.grid()\n\n    return ax\n\n\ndef plot_value_counts_multicolumns(\n    dataframe:pd.DataFrame,\n    columns:list,\n    hue: str = None,\n    subplots_kws: dict = dict()\n):\n    \"\"\"\n    \"\"\"\n    if not(type(subplots_kws) is dict):\n        subplots_kws = dict()\n    \n    nrow = subplots_kws.get('nrow', 3)\n    ncol = subplots_kws.get('ncol', 5)\n    figsize = subplots_kws.get('figsize', (18, 10))\n\n    fig, axes = plt.subplots(nrow, ncol, figsize=figsize)\n    axes = axes.ravel()\n    \n    for i, col in enumerate(columns):\n        plot_value_counts(dataframe, col, hue, axes[i])\n\n    fig.tight_layout()\n    plt.close()\n    \n    return fig\n","metadata":{"id":"PPVRxKsrGfKD","_kg_hide-input":true,"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target 0 vs 1","metadata":{"id":"bVs276C9Zis0"}},{"cell_type":"code","source":"plot_value_counts_multicolumns(train_data, int_cols, 'target')\n","metadata":{"id":"jEQIpP8UmYKQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train vs Test","metadata":{"id":"d29HLBRnZnSv"}},{"cell_type":"code","source":"plot_value_counts_multicolumns(data, int_cols, 'type')\n","metadata":{"id":"lXB6tRNb-L3d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution of Object Column (using count each alphabet)","metadata":{"id":"glpc9rRsKd-L"}},{"cell_type":"code","source":"# f_27 nunique is too big... cant plot value_counts.\ndata.loc[:, 'f_27'].value_counts()\n","metadata":{"id":"ihS7ykVULf-q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Count Each alphabet in f_27.\nCount the number of each alphabet in the sequence of f_27.","metadata":{"id":"xri2Kiq07rR3"}},{"cell_type":"code","source":"def count_each_alphabet(x):\n    \"\"\"\n    \"\"\"\n    count_alphabets = pd.Series(\n        np.zeros(len(string.ascii_uppercase)).astype(np.int8),\n        index=list(string.ascii_uppercase)\n    )\n    \n    for e in x:\n        count_alphabets[e] += 1\n\n    return count_alphabets\n","metadata":{"id":"nTLHLbcjGnO7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nIf you need this data, uncomment and execute the following code.\nIt takes about 10 minutes.\n'''\n# f_27_each_counts = data.loc[:, 'f_27'].apply(count_each_alphabet)\n# f_27_each_counts = pd.concat([f_27_each_counts.loc[:, list(string.ascii_uppercase)], data.loc[:, 'type']], axis=1)\n# f_27_each_counts.to_csv(config.count_each_alphabet)\n\nf_27_each_counts = pd.read_csv(config.count_each_alphabet)\nf_27_each_counts.loc[:, list(string.ascii_uppercase)] = f_27_each_counts.loc[:, list(string.ascii_uppercase)].astype(np.int8) \nf_27_each_counts_train = pd.concat(\n    [\n        f_27_each_counts.loc[train_data.index, list(string.ascii_uppercase)],\n        train_data.loc[:, 'target']\n    ],\n    axis=1\n)\n","metadata":{"id":"0AMsHGoX4AjA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_value_counts_multicolumns(\n    f_27_each_counts_train,\n    list(string.ascii_uppercase),\n    'target',\n    dict(nrow=4, ncol=7, figsize=(24, 14))\n)\n","metadata":{"id":"dqpIdhIim609"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_value_counts_multicolumns(\n    f_27_each_counts,\n    list(string.ascii_uppercase),\n    'type',\n    dict(nrow=4, ncol=7, figsize=(24, 14))\n)\n","metadata":{"id":"BST_F64CioVm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decompose.","metadata":{"id":"-0Rit3V0RDHT"}},{"cell_type":"markdown","source":"## code","metadata":{}},{"cell_type":"code","source":"def decompose(\n    X:pd.DataFrame,\n    n_components: int = 2,\n    algorithms: list = None,\n    cluster_parameters: dict = dict(method='k-means', target='raw', k=3),\n    random_state=0\n) -> tuple:\n    \"\"\"\n    \"\"\"\n    if algorithms is None:\n        algorithms = PCA(svd_solver='full')\n\n    if not(type(algorithms) is list):\n        algorithms = [algorithms]\n    \n    decomposed_y = pd.DataFrame(None, index=X.index)\n    if cluster_parameters.get('target') in ['both', 'raw']:\n        cls_alg = KMeans(\n            n_clusters=cluster_parameters['k'],\n            random_state=random_state\n        )\n        decomposed_y[f\"{cluster_parameters['method']}\"] = cls_alg.fit_predict(X)\n\n    decomposed_X, decomposed_explained = [], []\n    for alg in algorithms:\n        name = alg.__class__.__name__\n\n        decomposed_X.append(\n            pd.DataFrame(\n                alg.fit_transform(X)[:, :n_components],\n                index=X.index,\n                columns=[f\"{name}_components_{i+1}\" for i in range(n_components)]\n            )\n        )\n\n        if cluster_parameters.get('target') in ['both', 'decomposed_X']:\n            cls_alg = KMeans(\n                n_clusters=cluster_parameters['k'],\n                random_state=random_state\n            )\n            decomposed_y[f\"{cluster_parameters['method']}\"] = cls_alg.fit_predict(decomposed_X[-1])\n\n        if name == 'PCA':\n            decomposed_explained.append(explain_pca(alg, X.columns))\n        \n    decomposed_X = pd.concat(decomposed_X, axis=1)\n\n    return decomposed_X, decomposed_y, decomposed_explained\n\n\ndef explain_pca(pca, columns):\n    return None\n\n\ndef plot_decompose_scatter(\n    decompose_result:pd.DataFrame, \n    x: int = 0, \n    y: int = 1, \n    hue: str = None, \n    methods: list = ['PCA', 'UMAP']\n):\n    \"\"\"\n    \"\"\"\n    cmap, palette = None, None\n\n    if hue:\n        hue_value = decompose_result.loc[:, hue]\n        hue_n = hue_value.unique().shape[0]\n\n        if 20 > hue_n > 8:\n            palette = sns.diverging_palette(600, 0, n=hue_n)\n        elif hue_n > 20:\n            cmap = sns.color_palette(\"viridis\", as_cmap=True)\n        else:\n            palette = sns.color_palette(n_colors=hue_n)\n    else:\n        palette = sns.diverging_palette(600, 0, n=1)\n\n    fig, axes = plt.subplots(\n        1,\n        len(methods),\n        figsize=(9*len(methods), 8), facecolor='white'\n    )\n\n    if len(methods) == 1:\n        axes = (axes, )\n    \n    for i, method in enumerate(methods):\n        axes[i].grid()\n        components = decompose_result.filter(regex=f'^({method})', axis=1)\n        sns.scatterplot(\n            x=components.iloc[:, x],\n            y=components.iloc[:, y],\n            ax=axes[i],\n            hue=decompose_result.loc[:, hue] if type(hue) is str else None,\n            palette=palette,\n            cmap=cmap\n        )\n\n    plt.close()\n\n    return fig\n","metadata":{"id":"zEsY9hUzG1G0","_kg_hide-input":true,"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_cols = flt_cols + int_cols + list(string.ascii_uppercase)\ndataset = pd.concat(\n    [train_data, f_27_each_counts_train.loc[:, list(string.ascii_uppercase)]],\n    axis=1\n).loc[:, use_cols]\n\nscaler = StandardScaler()\ndataset.loc[:, :] = scaler.fit_transform(dataset)\ndataset = dataset.astype(np.float16)\n","metadata":{"id":"WzJCkiOVRin5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nIt takes about 50 minutes.\n'''\n# decomposed_X, decomposed_y, decomposed_explained = decompose(\n#     X=dataset, \n#     n_components=20, \n#     algorithms=[\n#         PCA(svd_solver='full'), \n#         UMAP(\n#             n_components=20,\n#             n_neighbors=10,\n#             min_dist=0.01,\n#             random_state=config.random_state\n#         )\n#     ],\n# )\n# decomposed_X.to_csv(config.train_decompose_X)\n# decomposed_y.to_csv(config.train_decompose_y)\n","metadata":{"id":"NDn6jQZr2ZJp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decomposed_X = pd.read_csv(config.train_decompose_X, index_col='id')\ndecomposed_y = pd.read_csv(config.train_decompose_y, index_col='id')\n\ndecomposed_result = pd.concat(\n    [decomposed_X, decomposed_y, train_data['target']],\n    axis=1\n)\n","metadata":{"id":"rYnPVkf4tNV3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_decompose_scatter(decomposed_result, hue='target', methods=['PCA', 'UMAP'])\n","metadata":{"id":"7jSFDrmp5RWD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_decompose_scatter(decomposed_result, hue='k-means', methods=['PCA', 'UMAP'])\n","metadata":{"id":"kXbkmnZxJCQ-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification by LightGBM and Catboost","metadata":{"id":"vfX_n3huazOl"}},{"cell_type":"markdown","source":"## TrainingðŸ“š","metadata":{"id":"js_8ukBCG9V0"}},{"cell_type":"code","source":"dataset = pd.concat(\n    [train_data, f_27_each_counts_train.loc[:, list(string.ascii_uppercase)]],\n    axis=1\n).loc[:, use_cols]\n\n# scaling float features only.\nscaling_cols = flt_cols + list(string.ascii_uppercase)\nscaler = StandardScaler()\ndataset.loc[:, scaling_cols] = scaler.fit_transform(dataset.loc[:, scaling_cols])\n\nint_cols_idx = [np.argwhere(dataset.columns == col).flatten()[0] for col in int_cols]\n","metadata":{"id":"j8Gne8fJ-zLY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nTODO\n- Implement Train Classs\n\"\"\"\nvalidator = StratifiedKFold(\n    config.fold,\n    shuffle=True,\n    random_state=config.random_state\n)\n\n# for save variables.\nmodels_cbt, models_lgb = [], []\nresults_cbt, results_lgb = [], []\nroc_auc_scores_cbt, roc_auc_scores_lgb = [], []\npredict_cbt = pd.DataFrame(\n    data=-1,\n    index=dataset.index, \n    columns=['catboost_predict', 'catboost_proba', 'fold']\n)\npredict_lgb = pd.DataFrame(\n    data=-1,\n    index=dataset.index,\n    columns=['lightgbm_predict', 'lightgbm_proba','fold']\n)\n\n# cross validation loop.\nfor i, (train_idx, valid_idx) in enumerate(validator.split(dataset.index, train_data.loc[:, 'target'])):\n    \n    # Split Train and Valid.\n    train_X = dataset.loc[train_idx, :]\n    train_y = train_data.loc[train_idx, 'target']\n    valid_X = dataset.loc[valid_idx, :] \n    valid_y = train_data.loc[valid_idx, 'target']\n\n    with timer(f'Catboost Train #{(i+1):02}'):\n        # Train Catboost\n        model_cbt = cbt.CatBoostClassifier(\n            **config.catboost\n        )\n        model_cbt.fit(\n            train_X,\n            train_y,\n            eval_set=(valid_X, valid_y),\n            verbose=False,\n            cat_features=int_cols_idx,\n        )\n\n        # Predict Valid Data.\n        predict_cbt.loc[valid_idx, 'catboost_predict'] = model_cbt.predict(valid_X)\n        predict_cbt.loc[valid_idx, 'catboost_proba'] = model_cbt.predict_proba(valid_X)[:, 1]\n        predict_cbt.loc[valid_idx, 'fold'] = (i + 1)\n        models_cbt.append(model_cbt)\n        \n        # Validation.\n        roc_auc_scores_cbt.append(roc_auc_score(valid_y, predict_cbt.loc[valid_idx, 'catboost_proba']))\n        results_cbt.append(\n            pd.DataFrame(\n                classification_report(\n                    valid_y,\n                    predict_cbt.loc[valid_idx, 'catboost_predict'],\n                    output_dict=True,\n                )\n            ).transpose()\n        )\n\n    with timer(f'LightGBM Train #{(i+1):02}'):\n        # Train LightGBM\n        model_lgb = lgb.LGBMClassifier(\n            **config.lightgbm\n        )\n        model_lgb.fit(\n            train_X,\n            train_y,\n            eval_set=(valid_X, valid_y),\n            verbose=False,\n        )\n\n        # Predict Valid Data.\n        predict_lgb.loc[valid_idx, 'lightgbm_predict'] = model_lgb.predict(valid_X)\n        predict_lgb.loc[valid_idx, 'lightgbm_proba'] = model_lgb.predict_proba(valid_X)[:, 1]\n        predict_lgb.loc[valid_idx, 'fold'] = (i + 1)\n        models_lgb.append(model_lgb)\n        \n        # Validation.\n        roc_auc_scores_lgb.append(roc_auc_score(valid_y, predict_lgb.loc[valid_idx, 'lightgbm_proba']))\n        results_lgb.append(\n            pd.DataFrame(\n                classification_report(\n                    valid_y,\n                    predict_lgb.loc[valid_idx, 'lightgbm_predict'],\n                    output_dict=True,\n                )\n            ).transpose()\n        )\n\n    del train_X, train_y, valid_X, valid_y\n","metadata":{"id":"evq0We40Pf8V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot Validation Result ","metadata":{"id":"QV3l8-n_txy6"}},{"cell_type":"markdown","source":"### code","metadata":{}},{"cell_type":"code","source":"def edit_classification_report(cr:pd.DataFrame):\n    supports = ('\\n(' + cr.iloc[:-3, -1].astype(int).astype(str) + ')')\n    cr.index = (cr.index[:-3] + supports).to_list() + cr.index[-3:].to_list()\n    cr = cr.iloc[:, :-1]\n    cr.iloc[-3:, :-1] = np.nan\n    return cr\n\n\ndef plot_classification_report(cr:pd.DataFrame, ax=None):\n\n    sns.heatmap(\n        cr.iloc[:-2, :],\n        vmin=0.0,\n        vmax=1.0,\n        cmap=sns.color_palette(\"Blues\", 24),\n        fmt='0.4g',\n        linewidth=2.0,\n        annot=True,\n        annot_kws=dict(size=14),\n        ax=ax,\n    )\n    \n    return ax\n\n\ndef plot_cv_classification_report(\n    crs:list,\n    title:str=None,\n    subplots_kws: dict = dict()\n):\n\n    if not(type(subplots_kws) is dict):\n        subplots_kws = dict()\n    \n    nrow = subplots_kws.get('nrow', 2)\n    ncol = subplots_kws.get('ncol', 5)\n    figsize = subplots_kws.get('figsize', (30, 12))\n    \n    fig, axes = plt.subplots(nrow, ncol, figsize=figsize)\n    axes = axes.ravel()\n    for i, cr in enumerate(crs):\n        axes[i] = plot_classification_report(cr, axes[i])\n        axes[i].set_title(f\"Fold#{(i+1):02}\", fontsize=14)\n        axes[i].tick_params(axis='x', labelsize=14)\n        axes[i].tick_params(axis='y', labelsize=14)\n\n    rect = None\n    if type(title) is str:\n        fig.suptitle(title, fontsize=16)\n        rect = [0, 0, 1, 0.96]\n    \n    fig.tight_layout(rect=rect)\n    plt.close()\n\n    return fig\n","metadata":{"id":"mjy_xqvf-o0l","_kg_hide-input":true,"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_cbt = [edit_classification_report(cr) for cr in results_cbt]\nresults_lgb = [edit_classification_report(cr) for cr in results_lgb]\n","metadata":{"id":"TuHqQv7ODY7n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Catboost","metadata":{}},{"cell_type":"code","source":"plot_cv_classification_report(\n    results_cbt,\n    'CatBoost CrossValidation',\n    subplots_kws=dict(nrow=2, ncol=5, figsize=(30, 12))\n)\n","metadata":{"id":"MWXZLf-93fNv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print ROC AUC Scores.\ndisplay(pd.DataFrame(roc_auc_scores_cbt).T)\n\nroc_auc_cv_score_cbt = np.mean(roc_auc_scores_cbt)\nprint(f'CatBoost ROC AUC CV Score: {roc_auc_cv_score_cbt:.5f}')\n","metadata":{"id":"cq0VaNblMrh9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LightGBM","metadata":{}},{"cell_type":"code","source":"plot_cv_classification_report(\n    results_lgb,\n    'LightGBM CrossValidation',\n    subplots_kws=dict(nrow=2, ncol=5, figsize=(30, 12))\n)\n","metadata":{"id":"KqyVzvZ0BI03"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print ROC AUC Scores.\ndisplay(pd.DataFrame(roc_auc_scores_lgb).T)\nroc_auc_cv_score_lgb = np.mean(roc_auc_scores_lgb)\nprint(f'LightGBM ROC AUC CV Score: {roc_auc_cv_score_lgb:.5f}')\n","metadata":{"id":"0WGK3yYJMrHm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot Feature Importances","metadata":{"id":"m5MqZ5Pat1xo"}},{"cell_type":"markdown","source":"### code","metadata":{}},{"cell_type":"code","source":"def plot_feature_importances(\n    models,\n    columns,\n    title: str = None,\n    top: int = 50\n):\n    \n    feature_importances = pd.DataFrame(\n        index=columns,\n        columns=['feature_importances']\n    )\n    feature_importances['feature_importances'] = 0.0\n    \n    if not(type(models) is list):\n        models = [models]\n    \n    for i, model in enumerate(models):\n        feature_importances['feature_importances'] += model.feature_importances_\n    feature_importances['feature_importances'] /= len(models)\n\n    feature_importances = feature_importances.sort_values(\n        'feature_importances',\n        ascending=False\n    ).iloc[:top]\n    feature_importances.index.name = 'column'\n    feature_importances.reset_index(inplace=True)\n    \n    fig, axes = plt.subplots(figsize=(max(6, feature_importances.shape[0] * 0.4), 7))\n    sns.barplot(x='column', y='feature_importances', data=feature_importances, ax=axes)\n    axes.grid()\n\n    rect = None\n    if type(title) is str:\n        fig.suptitle(title, fontsize=16)\n        rect = [0, 0, 1, 0.96]\n    \n    fig.tight_layout(rect=rect)\n    plt.close()\n\n    return fig\n","metadata":{"id":"6NpjEauiSxDn","_kg_hide-input":true,"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Catboost","metadata":{}},{"cell_type":"code","source":"plot_feature_importances(\n    models_cbt, \n    dataset.columns, \n    'CatBoost Feature Importances'\n)\n","metadata":{"id":"aR22wrGRSxA6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LightGBM","metadata":{}},{"cell_type":"code","source":"plot_feature_importances(\n    models_lgb, \n    dataset.columns, \n    'LightGBM Feature Importances'\n)\n","metadata":{"id":"Kt-d8K8BYcLY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Precit Test Data","metadata":{"id":"BDaUbj6OZJfS"}},{"cell_type":"markdown","source":"## code","metadata":{}},{"cell_type":"code","source":"def predict(models, testdata):\n    if not(type(models) is list):\n        models = [models]\n    \n    predict = pd.DataFrame(\n        np.zeros(testdata.index.shape[0]),\n        index=testdata.index,\n        columns=['predict']\n    )\n\n    for model in models:\n        predict['predict'] += model.predict_proba(testdata)[:, 1]\n\n    predict['predict'] /= len(models)\n    \n    return predict\n    ","metadata":{"id":"NsurRfnmZJHt","_kg_hide-input":true,"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## make test dataset","metadata":{}},{"cell_type":"code","source":"test_idx = data[data.type=='test'].index\ntest_data = pd.concat(\n    [data.loc[test_idx, :], f_27_each_counts.loc[test_idx, list(string.ascii_uppercase)]],\n    axis=1,\n).loc[:, use_cols]\ntest_data = test_data.reset_index(drop=True)\n\ntest_data.loc[:, scaling_cols] = scaler.transform(test_data.loc[:, scaling_cols])\n\nsubmission_df = pd.read_csv(config.submission_file)\n","metadata":{"id":"RLqw74kmbUsl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## predict","metadata":{}},{"cell_type":"code","source":"submission_df['target'] = predict(models_cbt+models_lgb, test_data).values\n","metadata":{"id":"UtNab4IUasCQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)\n","metadata":{"id":"mpErVFCwMeqm"},"execution_count":null,"outputs":[]}]}