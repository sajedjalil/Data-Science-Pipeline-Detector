{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%HTML\n\n<style type=\"text/css\">\n\ndiv.h2 {\n    background-color: #665191; \n    color: white; \n    padding: 5px; \n    padding-right: 300px; \n    font-size: 25px;  \n    margin-top: 2px;\n    margin-bottom: 10px;\n}\n\ndiv.h3 {\n    background-color: white; \n    color: #fe0000; \n    padding: 5px; \n    padding-right: 300px; \n    font-size: 20px; \n    margin-top: 2px;\n    margin-bottom: 10px;\n}\n</style>","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-10T18:22:33.462065Z","iopub.execute_input":"2022-05-10T18:22:33.46241Z","iopub.status.idle":"2022-05-10T18:22:33.469666Z","shell.execute_reply.started":"2022-05-10T18:22:33.462369Z","shell.execute_reply":"2022-05-10T18:22:33.469057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### <div class=\"h2\">Introduction</div>\n\nThis is an Exploratory Data Analysis for the `Tabular Playground Series - May 2022` challenge. I will be using SHAP, Matplotlib and Seaborn to examine the data.\n\nThe data features are simulated manufacturing data that is to be used to predict the binary state (target) of the machine which is `0/1`. The motivation is not onlt to predict the state of the machine given the simulated features but to also discover the 'hidden' feature interaction that may help to improve the prediction accuracy.\n\nOther than that, the data description page states that the data includes normalized continuous data and categorical data.\n\n   <a id=\"toc\"></a>\n   \n1. [Data Overview](#1)\n2. [Analysing Interactions with SHAP](#2)\n3. [References](#3)\n\nI will keep adding content; its work in progress.","metadata":{}},{"cell_type":"code","source":"!pip uninstall matplotlib -y\n!pip install matplotlib==3.1.3\n\nimport matplotlib\nprint(matplotlib.__version__)\n\n!pip install mplcyberpunk\nimport mplcyberpunk\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport matplotlib \nfrom matplotlib import gridspec\nimport seaborn as sns\n\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nimport shap\nimport xgboost as xgb\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', None)\nplt.style.use(\"cyberpunk\")\n\n# runtime configuration of matplotlib\nplt.rc(\"figure\", \n    autolayout=False, \n    figsize=(20, 10),\n)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=20,\n    titlepad=10,\n)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n<a id=\"1\"></a>\n### <div class=\"h2\">1. Data Overview</div>\n\nSince there are a few extensive and insightful EDA for this competition I will keep it to an overview here, see below. There are 31 features with no missing values. I will perform feature engineering so the total number of featutres going into the final model will be different.","metadata":{}},{"cell_type":"code","source":"train_raw = pd.read_csv('../input/tabular-playground-series-may-2022/train.csv',index_col='id')\n#test_raw = pd.read_csv('../input/tabular-playground-series-may-2022/test.csv',index_col='id')\n\n#create table\nfig = plt.figure(figsize=(5, 2))\nax = fig.add_subplot(111)\n\ntable_vals = [[900000, 700000], [32, 31], [0, 0], [15, 15], [16, 16], [1, 0]]\n\n# Draw table\nthe_table = plt.table(cellText=table_vals,\n                      colWidths=[0.08]*2,\n                      rowLabels=['#rows', '#columns', '%missing', \"int64\", \"float64\", \"object\"],\n                      colLabels=['Train', 'Test'],\n                      loc='center',\n                      cellLoc='center',\n                      rowColours =[\"purple\"] * 6,  \n                      colColours =[\"purple\"] * 6,\n                      cellColours=[['r', 'r'], ['r', 'r'], ['r', 'r'], ['r', 'r'], ['r', 'r'], ['r', 'r']]\n                      )\nthe_table.auto_set_font_size(False)\nthe_table.set_fontsize(16)\nthe_table.scale(4, 4)\nax.grid(False)\nax.axis('tight')\nax.axis('off')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-10T17:58:36.256258Z","iopub.execute_input":"2022-05-10T17:58:36.256576Z","iopub.status.idle":"2022-05-10T17:58:44.891047Z","shell.execute_reply.started":"2022-05-10T17:58:36.256541Z","shell.execute_reply":"2022-05-10T17:58:44.889897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n### <div class=\"h2\">2. Analysing Interactions with SHAP</div>\n\n**2.1 Modelling**\n\nTo use the SHAP package we first need to train a model since SHAP is a model agnostic approach designed to explain any given black-box model. If you are applying SHAP to a real-world problem you should follow best practices. Specifically, you should ensure your model performs well on both a training and validation set. The better your model the more reliable your results will be. As a quick check on this model, I have calculated the `AUC` of the validation set which `99%`. The model should be fine to demonstrate the SHAP package.\n\n\n","metadata":{}},{"cell_type":"code","source":"# From https://www.kaggle.com/ambrosm/tpsmay22-eda-which-makes-sense\nfor i in range(10):\n    train_raw[f'ch{i}'] = train_raw.f_27.str.get(i).apply(ord) - ord('A')\n    train_raw[\"unique_characters\"] = train_raw.f_27.apply(lambda s: len(set(s)))\n\nfeatures = [col for col in train_raw.columns if col != \"target\" and col !=\"f_27\"]\nX=train_raw[features]\ny=train_raw[\"target\"]\nX_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.4, random_state = 42)\n\n# Train model\nlgbm_model =LGBMClassifier(n_estimators=5000, min_child_samples=80, random_state=1307)\nlgbm_model.fit(X_train.values, y_train)\ny_val_pred = lgbm_model.predict_proba(X_val.values)[:,1]\nscore = roc_auc_score(y_val, y_val_pred)\nprint(f\"Validation AUC:{(score):.3f}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-05-10T17:59:06.658318Z","iopub.execute_input":"2022-05-10T17:59:06.658602Z","iopub.status.idle":"2022-05-10T18:03:07.667608Z","shell.execute_reply.started":"2022-05-10T17:59:06.658572Z","shell.execute_reply":"2022-05-10T18:03:07.666769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2.2 SHAP**\n\n`SHAP values` are used to explain individual predictions made by a model. It does this by giving the contributions of each factor to the final prediction. `SHAP interaction` values extend on this by breaking down the contributions into their main and interaction effects. We can use these to highlight and visualise interactions in data. It can also be a useful tool to understand how your model makes predictions.\n\nThe focus will be on applying the SHAP package and interpreting the results. Specifically, we start by explaining what SHAP interaction values are and how they can be used to explain individual predictions. We then dive into different aggregations of these values which help us explain how the model makes predictions in general. This includes taking the absolute mean across all interaction values and using the SHAP summary and dependence plot.","metadata":{}},{"cell_type":"code","source":"# Using a random sample of the dataframe for better time computation\nX_sampled = X_val.sample(20000, random_state=1307)\n\n# explain the model's predictions using SHAP values\nexplainer = shap.TreeExplainer(lgbm_model)\nshap_values = explainer.shap_values(X_sampled)\n\n#Get SHAP interaction values. Beware it is time consuming to calculate the interaction values.\n#shap_interaction = explainer.shap_interaction_values(X_sampled)\n#print(np.shape(shap_interaction))\n\nloaded_arr = np.loadtxt('../input/shap-interaction/shap_interaction_20k.txt')\nload_original_arr = loaded_arr.reshape(\n    #loaded_arr.shape[0], loaded_arr.shape[1] // shap_interaction.shape[2], shap_interaction.shape[2])\n    loaded_arr.shape[0], loaded_arr.shape[1] // 41, 41)\n\nshap_interaction = load_original_arr","metadata":{"execution":{"iopub.status.busy":"2022-05-10T18:09:24.998421Z","iopub.execute_input":"2022-05-10T18:09:24.999054Z","iopub.status.idle":"2022-05-10T18:18:01.814474Z","shell.execute_reply.started":"2022-05-10T18:09:24.999011Z","shell.execute_reply":"2022-05-10T18:18:01.813443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Step1: Absolute mean plot**\n\nIndividual contribution matrices allows us to explain individual model predictions at `local` level. But what if we want to explain how the model makes predictions at a `global` level? To do this we can aggregate the values in the contribution matrices by taking the absolute mean. Presenting the results in a heatmap can be effective to highlight important main effects and interaction effects. ","metadata":{}},{"cell_type":"code","source":"# Get absolute mean of matrices\nmean_shap = np.abs(shap_interaction).mean(0)\ndf = pd.DataFrame(mean_shap, index=X.columns, columns=X.columns)\n\n# times off diagonal by 2\ndf.where(df.values == np.diagonal(df),df.values*2, inplace=True)\n\n# display \nfig = plt.figure(figsize=(35, 20), facecolor='#002637', edgecolor='r')\nax = fig.add_subplot()\nsns.heatmap(df.round(decimals=3), cmap='coolwarm', annot=True, fmt='.6g', cbar=False, ax=ax, )\nax.tick_params(axis='x', colors='w', labelsize=15, rotation=90)\nax.tick_params(axis='y', colors='w', labelsize=15)\n\nplt.suptitle(\"SHAP interaction values\", color=\"white\", fontsize=60, y=0.97)\nplt.yticks(rotation=0) \nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-10T18:33:24.96305Z","iopub.execute_input":"2022-05-10T18:33:24.96364Z","iopub.status.idle":"2022-05-10T18:33:32.19688Z","shell.execute_reply.started":"2022-05-10T18:33:24.963596Z","shell.execute_reply":"2022-05-10T18:33:32.195963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ’¡ **INSIGHTS**\n- For instance we can see that the main effect is large for features `f_21` (`0.539`) , `f_26` (`0.612`) and `unique_characters`(`0.712`). This tells us that these features tend to have large positive or negative main effects. In other words, these features tend to have a significant impact on the modelâ€™s predictions. \n- Similarly, we can see that interaction effects for (`f_00`, `f_26`) --> (`0.513`) and (`f_02`, `f_21`) --> (`0.482`) are significant. These are just some examples.\n","metadata":{}},{"cell_type":"markdown","source":"\n**Step2: Feature interaction analysis**\n\nSo, now that we have calcuated the average (main/interaction)effect of features at global level it is interseting to deep dive into features that show large interaction effects at global level. We can use utilize the `dependence` plot to better understand the nature of the interactions. For the sake of demonstration I will be focusing on `f_02` & `f_21` and `f_24` & `f_30` interaction effects at the local level.","metadata":{}},{"cell_type":"code","source":"#plot feature interaction\ndef plot_feature_interaction(f1, f2):\n    # dependence plot\n    fig = plt.figure(tight_layout=True, figsize=(20,10))\n    spec = gridspec.GridSpec(ncols=3, nrows=2, figure=fig)\n\n\n    ax0 = fig.add_subplot(spec[0, 0])\n    minv, maxv = np.percentile(X_sampled, [1, 99])\n    shap.dependence_plot(f1, shap_values[1], X_sampled, display_features=X_sampled, interaction_index=f2, ax=ax0, show=False)\n    ax0.yaxis.label.set_color('white')          #setting up Y-axis label color to blue\n    ax0.xaxis.label.set_color('white')          #setting up Y-axis label color to blue\n    ax0.tick_params(axis='x', colors='white')    #setting up X-axis tick color to red\n    ax0.tick_params(axis='y', colors='white')    #setting up X-axis tick color to red\n    ax0.set_title(f'SHAP main effect', fontsize=10)\n\n    ax1 = fig.add_subplot(spec[0, 1])\n    shap.dependence_plot((f1, f2), shap_interaction, X_sampled, display_features=X_sampled, ax=ax1, axis_color='w', show=False)\n    ax1.yaxis.label.set_color('white')          #setting up Y-axis label color to blue\n    ax1.xaxis.label.set_color('white')          #setting up Y-axis label color to blue\n    ax1.tick_params(axis='x', colors='white')    #setting up X-axis tick color to red\n    ax1.tick_params(axis='y', colors='white')    #setting up X-axis tick color to red\n    ax1.set_title(f'SHAP interaction effect', fontsize=10)\n\n\n    temp = pd.DataFrame({f1: train_raw[f1].values,\n                    'target': train_raw.target.values})\n    temp = temp.sort_values(f1)\n    temp.reset_index(inplace=True)\n    \n    ax3 = fig.add_subplot(spec[1, 0])\n    sns.scatterplot(x=temp[f1], y=temp.target.rolling(15000, center=True).mean(), data=temp, ax=ax3, s=2)\n    ax3.set_title('How the target probability depends on f_02', fontsize=10)\n\n    temp = pd.DataFrame({f1: train_raw.loc[train_raw[\"f_30\"]==2,f1].values,\n                    'target': train_raw.loc[train_raw[\"f_30\"]==2,'target'].values})\n    temp = temp.sort_values(f1)\n    temp.reset_index(inplace=True)\n    \n    ax4 = fig.add_subplot(spec[1, 1])\n    sns.scatterplot(x=temp[f1], y=temp.target.rolling(15000, center=True).mean(), data=temp, ax=ax4, s=2)\n    ax4.set_title('How the target probability depends on f_24 & f_30==2', fontsize=10)\n\n\n    temp = pd.DataFrame({f1: train_raw.loc[train_raw[\"f_30\"]!=2,f1].values,\n                    'target': train_raw.loc[train_raw[\"f_30\"]!=2,'target'].values})\n    temp = temp.sort_values(f1)\n    temp.reset_index(inplace=True)\n    \n    ax5 = fig.add_subplot(spec[1, 2])\n    sns.scatterplot(x=temp[f1], y=temp.target.rolling(15000, center=True).mean(), data=temp, ax=ax5, s=2)\n    ax5.set_title('How the target probability depends on f_24 & f_30!=2', fontsize=10)\n    \n    plt.suptitle(\"Feature Interaction Analysis\\n f_24 and f_30\", fontsize=30, y=1.15)\n    fig.tight_layout()\n\n    plt.show()\n\nplt.style.use(\"cyberpunk\")\n\nf1='f_24'\nf2='f_30'\nplot_feature_interaction(f1, f2)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-10T18:20:26.047159Z","iopub.execute_input":"2022-05-10T18:20:26.047514Z","iopub.status.idle":"2022-05-10T18:20:39.250224Z","shell.execute_reply.started":"2022-05-10T18:20:26.04748Z","shell.execute_reply":"2022-05-10T18:20:39.249232Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ’¡ **INSIGHTS**\n- From the first plot one can conclude that for `f_30 == 2` main SHAP effect of `f_24` increases for the predictions as `f_24` gets larger.\n- From the second plot one can conlcude that for `f_30 == 2` the interaction effect between `f_30` & `f_24` decreases as `f_24` get larger. The opposite is true for `f_30 != 2`.\n- The third plot indicates that as `f_24` gets larger the probability for `state==1` increases(see [AmbrosM](https://www.kaggle.com/code/ambrosm/tpsmay22-eda-which-makes-sense)). The fourth and fifth plot are the same as third plot but they are conditioned on `f_30==2` and `f_30!=2`. The results are clearly different.","metadata":{}},{"cell_type":"markdown","source":"Lets take one more example. Based on SHAP feature interactions heatmap `f_02` & `f_21` interactions effect which is `0.482` is larger than the main effect of `f_02` which is `0.27`. Lets deep dive into the local level effects.","metadata":{}},{"cell_type":"code","source":"#plot function\nplt.style.use(\"cyberpunk\")\n\ndef plot_feature_interaction(f1, f2):\n    # dependence plot\n    fig = plt.figure(tight_layout=True, figsize=(20,10))\n    spec = gridspec.GridSpec(ncols=2, nrows=2, figure=fig)\n\n\n    ax0 = fig.add_subplot(spec[0, 0])\n    shap.dependence_plot(f1, shap_values[1], X_sampled, display_features=X_sampled, interaction_index=None, ax=ax0, show=False)\n    ax0.yaxis.label.set_color('white')          #setting up Y-axis label color to blue\n    ax0.xaxis.label.set_color('white')          #setting up Y-axis label color to blue\n    ax0.tick_params(axis='x', colors='white')    #setting up X-axis tick color to red\n    ax0.tick_params(axis='y', colors='white')    #setting up X-axis tick color to red\n    ax0.set_title(f'SHAP main effect', fontsize=10)\n\n    ax1 = fig.add_subplot(spec[0, 1])\n    shap.dependence_plot((f1, f2), shap_interaction, X_sampled, display_features=X_sampled, ax=ax1, axis_color='w', show=False)\n    ax1.yaxis.label.set_color('white')          #setting up Y-axis label color to blue\n    ax1.xaxis.label.set_color('white')          #setting up Y-axis label color to blue\n    ax1.tick_params(axis='x', colors='white')    #setting up X-axis tick color to red\n    ax1.tick_params(axis='y', colors='white')    #setting up X-axis tick color to red\n    ax1.set_title(f'SHAP interaction effect', fontsize=10)\n\n    ax2 = fig.add_subplot(spec[1, 0])\n    sns.scatterplot(x=f1, y=f2, data=train_raw, hue=\"target\", ax=ax2, s=2)\n    ax2.text(-1.5, -5, \"1\", fontsize=18, verticalalignment='top', rotation=\"horizontal\", color=\"k\", fontproperties=\"smallcaps\")\n    ax2.text(0, 1, \"2\", fontsize=18, verticalalignment='top', rotation=\"horizontal\", color=\"k\", fontproperties=\"smallcaps\")\n    ax2.text(1, 7, \"3\", fontsize=18, verticalalignment='top', rotation=\"horizontal\", color=\"k\", fontproperties=\"smallcaps\")\n\n    ax2.set_title(f'scatter plot', fontsize=10)\n\n    temp = pd.DataFrame({f1: train_raw[f1].values,'target': train_raw.target.values})\n    temp = temp.sort_values(f1)\n    temp.reset_index(inplace=True)\n    \n    ax3 = fig.add_subplot(spec[1, 1])\n    sns.scatterplot(x=temp[f1], y=temp.target.rolling(15000, center=True).mean(), data=temp, ax=ax3, s=2)\n    ax3.set_title('How the target probability depends on f_02', fontsize=10)\n    \n    plt.suptitle(\"Feature Interaction Analysis\\n f_02 and f_21\", fontsize=30, y=1.15)\n    plt.show()\n\nf1='f_02'\nf2='f_21'\nplot_feature_interaction(f1, f2)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-10T18:20:39.252133Z","iopub.execute_input":"2022-05-10T18:20:39.25238Z","iopub.status.idle":"2022-05-10T18:21:19.786839Z","shell.execute_reply.started":"2022-05-10T18:20:39.252348Z","shell.execute_reply":"2022-05-10T18:21:19.786184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ’¡ **INSIGHTS**\n- From the first plot one can conclude that as `f_02` increases SHAP effect gets larger on the predictions.\n- The third plot is very interesting. We can clearly see that there is no correlation between `f_02` and `f_21` but there is an interaction with the target. Based on the `target` we can divide the plot in to 3 different regions.","metadata":{}},{"cell_type":"markdown","source":"**Step3: Feature engineering**\n\n\nTo be continued....","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<div class=\"h2\">References</div>\n\n### Domain Knowledge References\n\n    \n  1.https://github.com/slundberg/shap/blob/master/notebooks/tabular_examples/tree_based_models/Census%20income%20classification%20with%20LightGBM.ipynb\n\n### Kaggle Kernels for Inspiration\n1.https://www.kaggle.com/code/ambrosm/tpsmay22-eda-which-makes-sense\n","metadata":{}}]}