{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predicting States of Manufacturing Control Data ðŸ­","metadata":{}},{"cell_type":"markdown","source":"**Objective:** Build a powerfull GBDT Model that can provide a good estimation.\n\n**Strategy:** I think I will follow this strategy:\n\n**Level 1 Getting Started**\n\n* Quick EDA to identify potential opportunities.\n* Simple pre-processing step to encode categorical features.\n* A basic CV strategy using 90% for TRaining and 10% for Testing.\n* Looking at the feature importances.\n* Creating a submission file.\n* Submit the file to Kaggle.\n\n**Level 2 Feature Engineering**\n* Feature engineering using text information. (Massive boost in the score)\n* Cross validation loop (**Work in Progress...**)\n\n---","metadata":{}},{"cell_type":"markdown","source":"**Data Description**\n\nFor this challenge, you are given (simulated) manufacturing control data and are tasked to predict whether the machine is in state 0 or state 1. \nThe data has various feature interactions that may be important in determining the machine state.\n\nGood luck!\n\n**Files**\n* train.csv - the training data, which includes normalized continuous data and categorical data\n* test.csv - the test set; your task is to predict binary target variable which represents the state of a manufacturing process\n* sample_submission.csv - a sample submission file in the correct format","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"**Notebooks Ideas and Credits**\n\nI took ideas or inspiration from the following notebooks, if you enjoy my work, please take a look to the notebooks that inspire my work.\n\n**TPSMAY22 Gradient-Boosting Quickstart:** https://www.kaggle.com/code/ambrosm/tpsmay22-gradient-boosting-quickstart/notebook\n\n\n","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 1. Loading the Requiered Libraries","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-22T03:42:29.797204Z","iopub.execute_input":"2022-05-22T03:42:29.797763Z","iopub.status.idle":"2022-05-22T03:42:29.808052Z","shell.execute_reply.started":"2022-05-22T03:42:29.797705Z","shell.execute_reply":"2022-05-22T03:42:29.806983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n","metadata":{}},{"cell_type":"markdown","source":"# 2. Setting the Notebook","metadata":{}},{"cell_type":"code","source":"%%time\n# I like to disable my Notebook Warnings.\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:42:29.810609Z","iopub.execute_input":"2022-05-22T03:42:29.81118Z","iopub.status.idle":"2022-05-22T03:42:29.823843Z","shell.execute_reply.started":"2022-05-22T03:42:29.81113Z","shell.execute_reply":"2022-05-22T03:42:29.82252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Notebook Configuration...\n\n# Amount of data we want to load into the Model...\nDATA_ROWS = None\n# Dataframe, the amount of rows and cols to visualize...\nNROWS = 50\nNCOLS = 15\n# Main data location path...\nBASE_PATH = '...'","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:42:29.82632Z","iopub.execute_input":"2022-05-22T03:42:29.826945Z","iopub.status.idle":"2022-05-22T03:42:29.836347Z","shell.execute_reply.started":"2022-05-22T03:42:29.826895Z","shell.execute_reply":"2022-05-22T03:42:29.835164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Configure notebook display settings to only use 2 decimal places, tables look nicer.\npd.options.display.float_format = '{:,.2f}'.format\npd.set_option('display.max_columns', NCOLS) \npd.set_option('display.max_rows', NROWS)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:42:29.838457Z","iopub.execute_input":"2022-05-22T03:42:29.839222Z","iopub.status.idle":"2022-05-22T03:42:29.847931Z","shell.execute_reply.started":"2022-05-22T03:42:29.839152Z","shell.execute_reply":"2022-05-22T03:42:29.846725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 3. Loading the Information (CSV) Into A Dataframe","metadata":{}},{"cell_type":"code","source":"%%time\n# Load the CSV information into a Pandas DataFrame...\ntrn_data = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/train.csv')\ntst_data = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/test.csv')\n\nsub = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:42:29.850864Z","iopub.execute_input":"2022-05-22T03:42:29.85164Z","iopub.status.idle":"2022-05-22T03:42:37.811214Z","shell.execute_reply.started":"2022-05-22T03:42:29.851592Z","shell.execute_reply":"2022-05-22T03:42:37.810246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 4. Exploring the Information Available","metadata":{}},{"cell_type":"markdown","source":"## 4.1. Analysing the Trian Dataset","metadata":{}},{"cell_type":"code","source":"%%time\n# Explore the shape of the DataFrame...\ntrn_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:42:37.812822Z","iopub.execute_input":"2022-05-22T03:42:37.81332Z","iopub.status.idle":"2022-05-22T03:42:37.823787Z","shell.execute_reply.started":"2022-05-22T03:42:37.813275Z","shell.execute_reply":"2022-05-22T03:42:37.82253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Display simple information of the variables in the dataset...\ntrn_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:42:37.826365Z","iopub.execute_input":"2022-05-22T03:42:37.82714Z","iopub.status.idle":"2022-05-22T03:42:37.987356Z","shell.execute_reply.started":"2022-05-22T03:42:37.827093Z","shell.execute_reply":"2022-05-22T03:42:37.986351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Display the first few rows of the DataFrame...\ntrn_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:42:37.988764Z","iopub.execute_input":"2022-05-22T03:42:37.989588Z","iopub.status.idle":"2022-05-22T03:42:38.01458Z","shell.execute_reply.started":"2022-05-22T03:42:37.989539Z","shell.execute_reply":"2022-05-22T03:42:38.013543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Generate a simple statistical summary of the DataFrame, Only Numerical...\ntrn_data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:42:38.01634Z","iopub.execute_input":"2022-05-22T03:42:38.017339Z","iopub.status.idle":"2022-05-22T03:42:39.271544Z","shell.execute_reply.started":"2022-05-22T03:42:38.017292Z","shell.execute_reply":"2022-05-22T03:42:39.27054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Calculates the total number of missing values...\ntrn_data.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:42:39.273258Z","iopub.execute_input":"2022-05-22T03:42:39.27424Z","iopub.status.idle":"2022-05-22T03:42:39.425874Z","shell.execute_reply.started":"2022-05-22T03:42:39.274192Z","shell.execute_reply":"2022-05-22T03:42:39.424684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Display the number of missing values by variable...\ntrn_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:42:39.427804Z","iopub.execute_input":"2022-05-22T03:42:39.428133Z","iopub.status.idle":"2022-05-22T03:42:39.57439Z","shell.execute_reply.started":"2022-05-22T03:42:39.428086Z","shell.execute_reply":"2022-05-22T03:42:39.57339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Display the number of unique values for each variable...\ntrn_data.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:42:39.578486Z","iopub.execute_input":"2022-05-22T03:42:39.579822Z","iopub.status.idle":"2022-05-22T03:42:41.214504Z","shell.execute_reply.started":"2022-05-22T03:42:39.579774Z","shell.execute_reply":"2022-05-22T03:42:41.213373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the number of unique values for each variable, sorted by quantity...\ntrn_data.nunique().sort_values(ascending = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:42:41.216298Z","iopub.execute_input":"2022-05-22T03:42:41.216654Z","iopub.status.idle":"2022-05-22T03:42:42.844234Z","shell.execute_reply.started":"2022-05-22T03:42:41.216595Z","shell.execute_reply":"2022-05-22T03:42:42.843245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Check some of the categorical variables\ncateg_cols = ['f_29','f_30','f_13', 'f_18','f_17','f_14','f_11','f_10','f_09','f_15','f_07','f_12','f_16','f_08','f_27']\ntrn_data[categ_cols].sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:42:42.84597Z","iopub.execute_input":"2022-05-22T03:42:42.846482Z","iopub.status.idle":"2022-05-22T03:42:42.951307Z","shell.execute_reply.started":"2022-05-22T03:42:42.846435Z","shell.execute_reply":"2022-05-22T03:42:42.950249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Generate a quick correlation matrix to understand the dataset better\ncorrelation = trn_data.corr()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:42:42.952878Z","iopub.execute_input":"2022-05-22T03:42:42.953488Z","iopub.status.idle":"2022-05-22T03:42:45.319292Z","shell.execute_reply.started":"2022-05-22T03:42:42.95344Z","shell.execute_reply":"2022-05-22T03:42:45.318268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Diplay the correlation matrix\ncorrelation","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:42:45.320946Z","iopub.execute_input":"2022-05-22T03:42:45.321458Z","iopub.status.idle":"2022-05-22T03:42:45.354779Z","shell.execute_reply.started":"2022-05-22T03:42:45.321412Z","shell.execute_reply":"2022-05-22T03:42:45.353315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Check the most correlated variables to the target\ncorrelation['target'].sort_values(ascending = False)[:5]","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:42:45.356678Z","iopub.execute_input":"2022-05-22T03:42:45.357179Z","iopub.status.idle":"2022-05-22T03:42:45.370328Z","shell.execute_reply.started":"2022-05-22T03:42:45.357133Z","shell.execute_reply":"2022-05-22T03:42:45.369261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Check the least correlated variables to the target\ncorrelation['target'].sort_values(ascending = True)[:5]","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:42:45.372247Z","iopub.execute_input":"2022-05-22T03:42:45.373506Z","iopub.status.idle":"2022-05-22T03:42:45.389882Z","shell.execute_reply.started":"2022-05-22T03:42:45.373442Z","shell.execute_reply":"2022-05-22T03:42:45.388879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## 4.2. Analysing the Trian Labels Dataset","metadata":{}},{"cell_type":"code","source":"%%time\n# Check how well balanced is the dataset\ntrn_data['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:42:45.391735Z","iopub.execute_input":"2022-05-22T03:42:45.393244Z","iopub.status.idle":"2022-05-22T03:42:45.412161Z","shell.execute_reply.started":"2022-05-22T03:42:45.393198Z","shell.execute_reply":"2022-05-22T03:42:45.411004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Check some statistics on the target variable\ntrn_data['target'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:42:45.41438Z","iopub.execute_input":"2022-05-22T03:42:45.414762Z","iopub.status.idle":"2022-05-22T03:42:45.448833Z","shell.execute_reply.started":"2022-05-22T03:42:45.414714Z","shell.execute_reply":"2022-05-22T03:42:45.447816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 5. Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## 5.1 Text Base Features","metadata":{}},{"cell_type":"code","source":"%%time\n# The idea is to create a simple funtion to count the amount of letters on feature 27.\n# feature 27 seems quite important \n\ndef count_sequence(df, field):\n    '''\n    For each letter of the provided suquence it return new feature with the number of occurences.\n    '''\n    alphabet = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']    \n    \n    for letter in alphabet:\n        df[letter + '_count'] = df[field].str.count(letter)\n    \n    df[\"unique_characters\"] = df['f_27'].apply(lambda s: len(set(s)))\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:42:45.450485Z","iopub.execute_input":"2022-05-22T03:42:45.451513Z","iopub.status.idle":"2022-05-22T03:42:45.462653Z","shell.execute_reply.started":"2022-05-22T03:42:45.451464Z","shell.execute_reply":"2022-05-22T03:42:45.461344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Utilizes the new created funtions to generate more features.\n# trn_data = count_sequence(trn_data, 'f_27')\n# tst_data = count_sequence(tst_data, 'f_27')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:42:45.4651Z","iopub.execute_input":"2022-05-22T03:42:45.466238Z","iopub.status.idle":"2022-05-22T03:42:45.476607Z","shell.execute_reply.started":"2022-05-22T03:42:45.466192Z","shell.execute_reply":"2022-05-22T03:42:45.475485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndef count_chars(df, field):\n    '''\n    Describes something...\n    '''\n    \n    for i in range(10):\n        df[f'ch_{i}'] = df[field].str.get(i).apply(ord) - ord('A')\n        \n    df[\"unique_characters\"] = df[field].apply(lambda s: len(set(s)))\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:42:45.478451Z","iopub.execute_input":"2022-05-22T03:42:45.478798Z","iopub.status.idle":"2022-05-22T03:42:45.489717Z","shell.execute_reply.started":"2022-05-22T03:42:45.478752Z","shell.execute_reply":"2022-05-22T03:42:45.488726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Utilizes the new created funtions to generate more features.\ntrn_data = count_chars(trn_data, 'f_27')\ntst_data = count_chars(tst_data, 'f_27')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:42:45.491556Z","iopub.execute_input":"2022-05-22T03:42:45.492161Z","iopub.status.idle":"2022-05-22T03:43:06.996188Z","shell.execute_reply.started":"2022-05-22T03:42:45.492101Z","shell.execute_reply":"2022-05-22T03:43:06.99519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndef calculate_feat_int(df):\n    df['i_02_21'] = (df.f_21 + df.f_02 > 5.2).astype(int) - (df.f_21 + df.f_02 < -5.3).astype(int)\n    df['i_05_22'] = (df.f_22 + df.f_05 > 5.1).astype(int) - (df.f_22 + df.f_05 < -5.4).astype(int)\n    i_00_01_26 = df.f_00 + df.f_01 + df.f_26\n    df['i_00_01_26'] = (i_00_01_26 > 5.0).astype(int) - (i_00_01_26 < -5.0).astype(int)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:43:06.997983Z","iopub.execute_input":"2022-05-22T03:43:06.998297Z","iopub.status.idle":"2022-05-22T03:43:07.006723Z","shell.execute_reply.started":"2022-05-22T03:43:06.998247Z","shell.execute_reply":"2022-05-22T03:43:07.005654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Utilizes the new created funtions to generate more features.\ntrn_data = calculate_feat_int(trn_data)\ntst_data = calculate_feat_int(tst_data)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:43:07.008295Z","iopub.execute_input":"2022-05-22T03:43:07.008868Z","iopub.status.idle":"2022-05-22T03:43:07.087103Z","shell.execute_reply.started":"2022-05-22T03:43:07.008822Z","shell.execute_reply":"2022-05-22T03:43:07.085076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 7. Pre-Processing Labels","metadata":{}},{"cell_type":"code","source":"%%time\n# Define a label encoding function\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ndef encode_features(df, cols = ['f_27']):\n    for col in cols:\n        df[col + '_enc'] = encoder.fit_transform(df[col])\n    return df\n\ntrn_data = encode_features(trn_data)\ntst_data = encode_features(tst_data)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:43:07.093279Z","iopub.execute_input":"2022-05-22T03:43:07.093516Z","iopub.status.idle":"2022-05-22T03:43:13.59903Z","shell.execute_reply.started":"2022-05-22T03:43:07.093484Z","shell.execute_reply":"2022-05-22T03:43:13.597978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the results of the transformation\ntrn_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:43:13.600883Z","iopub.execute_input":"2022-05-22T03:43:13.601187Z","iopub.status.idle":"2022-05-22T03:43:13.625452Z","shell.execute_reply.started":"2022-05-22T03:43:13.601142Z","shell.execute_reply":"2022-05-22T03:43:13.624328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 8. Feature Selection for Baseline Model","metadata":{}},{"cell_type":"code","source":"%%time\n# Define what will be used in the training stage\nignore = ['id', 'target', 'f_27',  'f_27_enc'] # f_27 has been label encoded...\n\nfeatures = [feat for feat in trn_data.columns if feat not in ignore]\ntarget_feature = 'target'","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:43:13.627038Z","iopub.execute_input":"2022-05-22T03:43:13.628992Z","iopub.status.idle":"2022-05-22T03:43:13.638119Z","shell.execute_reply.started":"2022-05-22T03:43:13.628943Z","shell.execute_reply":"2022-05-22T03:43:13.636836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 9. Creating a Simple Train / Test Split Strategy","metadata":{}},{"cell_type":"code","source":"%%time\n# Creates a simple train split breakdown for baseline model\nfrom sklearn.model_selection import train_test_split\ntest_size_pct = 0.20\nX_train, X_valid, y_train, y_valid = train_test_split(trn_data[features], trn_data[target_feature], test_size = test_size_pct, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:43:13.639977Z","iopub.execute_input":"2022-05-22T03:43:13.640834Z","iopub.status.idle":"2022-05-22T03:43:14.37132Z","shell.execute_reply.started":"2022-05-22T03:43:13.640771Z","shell.execute_reply":"2022-05-22T03:43:14.370113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 10. Building a Baseline GBT Model, Simple Split","metadata":{}},{"cell_type":"markdown","source":"## 10.1 XGBoost Model","metadata":{}},{"cell_type":"code","source":"%%time\n%%script false --no-raise-error\n# Import the model libraries\nfrom xgboost  import XGBClassifier","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:43:14.372959Z","iopub.execute_input":"2022-05-22T03:43:14.373949Z","iopub.status.idle":"2022-05-22T03:43:14.430354Z","shell.execute_reply.started":"2022-05-22T03:43:14.373897Z","shell.execute_reply":"2022-05-22T03:43:14.429191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n%%script false --no-raise-error\n# Define the model parameters to get started we use default values to a certain degree\nxgb_params = {'n_estimators'     : 8192,\n              'min_child_weight' : 96,\n              #'max_depth'        : 6,\n              #'learning_rate'    : 0.15,\n              #'subsample'        : 0.95,\n              #'colsample_bytree' : 0.95,\n              #'reg_lambda'       : 1.50,\n              #'reg_alpha'        : 1.50,\n              #'gamma'            : 1.50,\n              'max_bin'          : 512,\n              'random_state'     : 46,\n              'objective'        : 'binary:logistic',\n              'tree_method'      : 'gpu_hist',\n             }","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:43:14.433238Z","iopub.execute_input":"2022-05-22T03:43:14.433986Z","iopub.status.idle":"2022-05-22T03:43:14.488987Z","shell.execute_reply.started":"2022-05-22T03:43:14.433922Z","shell.execute_reply":"2022-05-22T03:43:14.487867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n%%script false --no-raise-error\n# Instanciate the XGBoost model using the previous parameters\nxgb = XGBClassifier(**xgb_params)\nxgb.fit(X_train, y_train, eval_set = [(X_valid, y_valid)], eval_metric = ['auc'], early_stopping_rounds = 256, verbose = 250)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:43:14.490889Z","iopub.execute_input":"2022-05-22T03:43:14.491561Z","iopub.status.idle":"2022-05-22T03:43:14.547641Z","shell.execute_reply.started":"2022-05-22T03:43:14.491508Z","shell.execute_reply":"2022-05-22T03:43:14.545233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n%%script false --no-raise-error\n# Check the model performance in the validation dataset\nfrom sklearn.metrics import roc_auc_score\nval_preds = xgb.predict_proba(X_valid[features])[:, 1]\nroc_auc_score(y_valid, val_preds)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:43:14.549716Z","iopub.execute_input":"2022-05-22T03:43:14.550503Z","iopub.status.idle":"2022-05-22T03:43:14.609344Z","shell.execute_reply.started":"2022-05-22T03:43:14.550448Z","shell.execute_reply":"2022-05-22T03:43:14.607978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Record some of the model results for future improvement\n# Local Score = 0.9454953628406088 First Model Run >>> LB Score = 0.93147\n# Local Score = 0.9448767329168479 First Model Run >>> LB Score = 0.93205\n\n# 0.9816418086418166","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:43:14.611584Z","iopub.execute_input":"2022-05-22T03:43:14.613289Z","iopub.status.idle":"2022-05-22T03:43:14.618978Z","shell.execute_reply.started":"2022-05-22T03:43:14.613236Z","shell.execute_reply":"2022-05-22T03:43:14.617635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## 10.2 LGMB Model","metadata":{}},{"cell_type":"code","source":"%%time\n%%script false --no-raise-error\n# Import the model libraries\nfrom lightgbm import LGBMClassifier","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:43:14.621318Z","iopub.execute_input":"2022-05-22T03:43:14.621762Z","iopub.status.idle":"2022-05-22T03:43:14.678704Z","shell.execute_reply.started":"2022-05-22T03:43:14.621718Z","shell.execute_reply":"2022-05-22T03:43:14.677397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n%%script false --no-raise-error\n# Define the model parameters to get started we use default values to a certain degree\nlgb_params = {'n_estimators'      : 8192,\n              'min_child_samples' : 96,\n              'max_bins'          : 512,\n              'random_state'      : 46,\n             }","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:43:14.6809Z","iopub.execute_input":"2022-05-22T03:43:14.681806Z","iopub.status.idle":"2022-05-22T03:43:14.735963Z","shell.execute_reply.started":"2022-05-22T03:43:14.681755Z","shell.execute_reply":"2022-05-22T03:43:14.734761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n%%script false --no-raise-error\n# Instanciate the XGBoost model using the previous parameters\nlgb = LGBMClassifier(**lgb_params)\nlgb.fit(X_train, y_train, eval_set = [(X_valid, y_valid)], eval_metric = ['auc'], early_stopping_rounds = 256, verbose = 250)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:43:14.737908Z","iopub.execute_input":"2022-05-22T03:43:14.738379Z","iopub.status.idle":"2022-05-22T03:43:14.796499Z","shell.execute_reply.started":"2022-05-22T03:43:14.738317Z","shell.execute_reply":"2022-05-22T03:43:14.795251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n%%script false --no-raise-error\n# Check the model performance in the validation dataset\nfrom sklearn.metrics import roc_auc_score\nval_preds = lgb.predict_proba(X_valid[features])[:, 1]\nroc_auc_score(y_valid, val_preds)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:43:14.799058Z","iopub.execute_input":"2022-05-22T03:43:14.799436Z","iopub.status.idle":"2022-05-22T03:43:14.855607Z","shell.execute_reply.started":"2022-05-22T03:43:14.799385Z","shell.execute_reply":"2022-05-22T03:43:14.854309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 11. Building a Baseline GBT Model, Kfold Loop","metadata":{}},{"cell_type":"code","source":"%%time\nfrom lightgbm import LGBMClassifier\nfrom xgboost  import XGBClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport math","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:43:14.858052Z","iopub.execute_input":"2022-05-22T03:43:14.859175Z","iopub.status.idle":"2022-05-22T03:43:14.867453Z","shell.execute_reply.started":"2022-05-22T03:43:14.859114Z","shell.execute_reply":"2022-05-22T03:43:14.866362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Define the model parameters to get started we use default values to a certain degree\nlgb_params = {'n_estimators'      : 8192, # Was 8192...\n              'min_child_samples' : 96,\n              'max_bins'          : 512,\n              'random_state'      : 46,\n             }\n\nxgb_params = {'n_estimators'     : 8192,\n              'min_child_weight' : 96,\n              'max_depth'        : 6,\n              'learning_rate'    : 0.15,\n              'subsample'        : 0.95,\n              'colsample_bytree' : 0.95,\n              'reg_lambda'       : 1.50,\n              'reg_alpha'        : 1.50,\n              'gamma'            : 1.50,\n              'max_bin'          : 512,\n              'random_state'     : 46,\n              'objective'        : 'binary:logistic',\n              'tree_method'      : 'gpu_hist',\n             }\n\n\nxgb_opt_params = {'n_estimators': 798,\n                  'max_depth': 8,\n                  'learning_rate': 0.1228167536332915,\n                  'subsample': 0.5460612275431587,\n                  'colsample_bytree': 0.6250334254566884,\n                  'reg_lambda': 3.9594130578291624,\n                  'reg_alpha': 5.534583993468559,\n                  'gamma': 2.9138421416832916,\n                  'min_child_weight': 0,\n                  'max_bin': 235,\n                  'random_state': 46,\n                  'objective': 'binary:logistic',\n                  'tree_method': 'gpu_hist',}","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:43:14.869382Z","iopub.execute_input":"2022-05-22T03:43:14.870356Z","iopub.status.idle":"2022-05-22T03:43:14.888088Z","shell.execute_reply.started":"2022-05-22T03:43:14.870309Z","shell.execute_reply":"2022-05-22T03:43:14.886731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Create empty lists to store NN information...\n\nscore_list   = []\npredictions  = [] \n# Define kfolds for training purposes...\nkf = KFold(n_splits = 5)\n\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(trn_data)):\n    print(f'Training Fold {fold} ...')\n    X_train, X_valid = trn_data.iloc[trn_idx][features], trn_data.iloc[val_idx][features]\n    y_train, y_valid = trn_data.iloc[trn_idx][target_feature], trn_data.iloc[val_idx][target_feature]\n    \n    # LGBM (Uncomment to use, and Comment the XGBoost Part... LGBM Takes forever)\n    # model = LGBMClassifier(**lgb_params)\n    # model.fit(X_train, y_train, eval_set = [(X_valid, y_valid)], eval_metric = ['auc'], early_stopping_rounds = 256, verbose = 0)\n    \n    # XGBoost\n    model = XGBClassifier(**xgb_opt_params)\n    model.fit(X_train, y_train, eval_set = [(X_valid, y_valid)], eval_metric = ['auc'], early_stopping_rounds = 256, verbose = 0)\n    \n    y_valid_pred = model.predict_proba(X_valid.values)[:,1]\n    score = roc_auc_score(y_valid, y_valid_pred)\n\n    score_list.append(score)\n    print(f\"Fold {fold}, AUC = {score:.4f}\")\n    print((''))\n    \n    tst_pred = model.predict_proba(tst_data[features].values)[:,1]\n    predictions.append(tst_pred)\n\nprint(f'OOF AUC: {np.mean(score_list):.4f}')\nprint('.........')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:43:14.889907Z","iopub.execute_input":"2022-05-22T03:43:14.891096Z","iopub.status.idle":"2022-05-22T03:45:30.617324Z","shell.execute_reply.started":"2022-05-22T03:43:14.891049Z","shell.execute_reply":"2022-05-22T03:45:30.615958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 11. Undertanding Model Behavior, Feature Importance","metadata":{}},{"cell_type":"code","source":"%%time\n# Define a funtion to plot the feature importance properly\ndef plot_feature_importance(importance, names, model_type, max_features = 10):\n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n\n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n    fi_df = fi_df.head(max_features)\n\n    #Define size of bar plot\n    plt.figure(figsize=(8,6))\n    \n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + 'FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:45:30.619172Z","iopub.execute_input":"2022-05-22T03:45:30.619537Z","iopub.status.idle":"2022-05-22T03:45:30.628903Z","shell.execute_reply.started":"2022-05-22T03:45:30.619479Z","shell.execute_reply":"2022-05-22T03:45:30.627813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Utilize the feature importance function to visualize the most valueable features\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplot_feature_importance(model.feature_importances_,X_train.columns,'LGBM ', max_features = 25)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:45:30.630735Z","iopub.execute_input":"2022-05-22T03:45:30.631407Z","iopub.status.idle":"2022-05-22T03:45:31.277559Z","shell.execute_reply.started":"2022-05-22T03:45:30.631361Z","shell.execute_reply":"2022-05-22T03:45:31.276572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 12. Baseline Model Submission File Generation","metadata":{}},{"cell_type":"code","source":"%%time\n# Review the format of the submission file\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:45:31.283027Z","iopub.execute_input":"2022-05-22T03:45:31.285829Z","iopub.status.idle":"2022-05-22T03:45:31.309425Z","shell.execute_reply.started":"2022-05-22T03:45:31.285781Z","shell.execute_reply":"2022-05-22T03:45:31.308451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Populated the prediction on the submission dataset and creates an output file\nsub['target'] = np.array(predictions).mean(axis=0)\nsub.to_csv('my_submission_043022.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:45:31.314384Z","iopub.execute_input":"2022-05-22T03:45:31.316982Z","iopub.status.idle":"2022-05-22T03:45:33.664196Z","shell.execute_reply.started":"2022-05-22T03:45:31.316935Z","shell.execute_reply":"2022-05-22T03:45:33.663046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Review the submission file as a final step to upload to Kaggle.\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:45:33.665873Z","iopub.execute_input":"2022-05-22T03:45:33.666829Z","iopub.status.idle":"2022-05-22T03:45:33.682573Z","shell.execute_reply.started":"2022-05-22T03:45:33.66678Z","shell.execute_reply":"2022-05-22T03:45:33.681394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}