{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TPS-MAY22, Fast AI Model ðŸš€\n\nHello The purpose of this Notebook is to provide a detailed guide to develop a model using **fastai**\nI just started the Course a few days ago...\n\n**Objective:** Build a powerfull **fastai** model that can provide a competitive level.\n\n**Strategy:** I think I will follow this strategy:\n\n**Level 1+2 Fast AI Model**\n\n**Loading the Data + Creating some Features.**\n* Quick overview of the data to identify potential opportunities.\n* Feature engineering using text information. (Massive boost in the score)\n\n**FastAI Model Development**\n* Define all the data processing steps requiered.\n* Define the split technique that will be utilized.\n* Define some of the parameters for the TabularDataLoader.\n* Create a TabularDataLoader.\n* Show one batch of data, to identify issues.\n* Define some of the parameters of the tabular_learner.\n* Define the tabular_learner\n* Visualize the model architecture.\n* Train the model for 1 epoch, this is kind of an initialization.\n* Optimize the model learning rate.\n* Train the model for multiple epochs.\n\n\n\n\n**Level 3 Model Optimization**\n* Work in Progress, Probably Hyperparam Optimization\n* Improve the FastAI model becasue is kind of week at this stage\n\n---\n**Other Similar Implementations**\nI been working on other architechtures at the same time, to see what works better\n\nXGBoost and LGBM Models\n\nhttps://www.kaggle.com/code/cv13j0/tps-may22-eda-gbdt\n\nNN Model\n\nhttps://www.kaggle.com/code/cv13j0/tps-may22-eda-neuronal-nets/edit/run/95032660\n\n---\n\n**Data Description**\n\nFor this challenge, you are given (simulated) manufacturing control data and are tasked to predict whether the machine is in state 0 or state 1. \nThe data has various feature interactions that may be important in determining the machine state.\n\nGood luck!\n\n**Files**\n* train.csv - the training data, which includes normalized continuous data and categorical data\n* test.csv - the test set; your task is to predict binary target variable which represents the state of a manufacturing process\n* sample_submission.csv - a sample submission file in the correct format\n\n---\n**Notebooks Ideas and Credits**\n\nI took ideas or inspiration from the following notebooks, if you enjoy my work, please take a look to the notebooks that inspire my work.\n\nTPSMAY22 Gradient-Boosting Quickstart: https://www.kaggle.com/code/ambrosm/tpsmay22-gradient-boosting-quickstart/notebook\n\nFast.AI: https://www.kaggle.com/code/venkatkumar001/fast-ai-1/notebook?scriptVersionId=94623621\n\nkaggle TPS2022 May FastAI Baseline: https://www.kaggle.com/code/casati8/kaggle-tps2022-may-fastai-baseline/notebook\n\n","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 1.0 - Installing or Upgrading Libraries...\nIn this section I will install or upgrade any library that I'm not 100% sure Kaggle Kernels are keeping up to date, I can also use this section to install anything extra that i need","metadata":{}},{"cell_type":"code","source":"%%capture\n# Upgrade the fastai libraries, to use the latest updates.\n\n!pip3 install --upgrade fastai","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:36:59.326009Z","iopub.execute_input":"2022-05-17T04:36:59.326289Z","iopub.status.idle":"2022-05-17T04:37:08.995719Z","shell.execute_reply.started":"2022-05-17T04:36:59.326258Z","shell.execute_reply":"2022-05-17T04:37:08.99466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 2.0 - Importing Libraries...\n...","metadata":{}},{"cell_type":"code","source":"%%time\n# Load some of the key Python libraries for the model development.\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-17T04:37:08.999786Z","iopub.execute_input":"2022-05-17T04:37:09.000518Z","iopub.status.idle":"2022-05-17T04:37:09.012691Z","shell.execute_reply.started":"2022-05-17T04:37:09.000481Z","shell.execute_reply":"2022-05-17T04:37:09.011912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Load the fastai tabular modules.\n\nimport fastai; fastai.__version__\nfrom fastai.tabular.all import *","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:37:09.01586Z","iopub.execute_input":"2022-05-17T04:37:09.016605Z","iopub.status.idle":"2022-05-17T04:37:09.023175Z","shell.execute_reply.started":"2022-05-17T04:37:09.016565Z","shell.execute_reply":"2022-05-17T04:37:09.022407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 3.0 - Notebook Configurations\n...","metadata":{}},{"cell_type":"code","source":"%%time\n# I like to disable my Notebook Warnings.\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:37:09.025625Z","iopub.execute_input":"2022-05-17T04:37:09.026261Z","iopub.status.idle":"2022-05-17T04:37:09.032075Z","shell.execute_reply.started":"2022-05-17T04:37:09.026209Z","shell.execute_reply":"2022-05-17T04:37:09.031142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Notebook Configuration.\n\n# Amount of data we want to load into the Model.\nDATA_ROWS = None\n# Dataframe, the amount of rows and cols to visualize.\nNROWS = 25\nNCOLS = 15\n# Main data location path...\nBASE_PATH = '...'","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:37:09.033509Z","iopub.execute_input":"2022-05-17T04:37:09.034284Z","iopub.status.idle":"2022-05-17T04:37:09.040736Z","shell.execute_reply.started":"2022-05-17T04:37:09.034227Z","shell.execute_reply":"2022-05-17T04:37:09.03996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Configure notebook display settings to only use 2 decimal places, tables look nicer.\n\npd.options.display.float_format = '{:,.5f}'.format\npd.set_option('display.max_columns', NCOLS) \npd.set_option('display.max_rows', NROWS)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:37:09.042184Z","iopub.execute_input":"2022-05-17T04:37:09.042675Z","iopub.status.idle":"2022-05-17T04:37:09.048739Z","shell.execute_reply.started":"2022-05-17T04:37:09.042641Z","shell.execute_reply":"2022-05-17T04:37:09.048024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----","metadata":{}},{"cell_type":"markdown","source":"# 4.0 - Reading the Datasets\n...","metadata":{}},{"cell_type":"code","source":"%%time\n# Load the train and test CSVs information into a Pandas DataFrame.\n\ntrn_data = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/train.csv')\ntst_data = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/test.csv')\n\n# Load the submission CSV information into a Pandas DataFrame.\n\nsub = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:37:09.050166Z","iopub.execute_input":"2022-05-17T04:37:09.050623Z","iopub.status.idle":"2022-05-17T04:37:16.972567Z","shell.execute_reply.started":"2022-05-17T04:37:09.050587Z","shell.execute_reply":"2022-05-17T04:37:16.97172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 5.0 - Memory Optimization\nIn this section I will use the popular reduce memory function to optimize the memory consumption of the datatset, this is quite critical to not waste resources in the training stages of the model.","metadata":{"execution":{"iopub.status.busy":"2022-05-08T18:19:40.953547Z","iopub.execute_input":"2022-05-08T18:19:40.954241Z","iopub.status.idle":"2022-05-08T18:19:40.958116Z","shell.execute_reply.started":"2022-05-08T18:19:40.954207Z","shell.execute_reply":"2022-05-08T18:19:40.957404Z"}}},{"cell_type":"code","source":"%%time\n# Creates a funtion for reduce the memory usage, this is a quite popular \n# function that has been around Kaggle for a while.\n\ndef reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:37:16.973942Z","iopub.execute_input":"2022-05-17T04:37:16.974693Z","iopub.status.idle":"2022-05-17T04:37:16.987959Z","shell.execute_reply.started":"2022-05-17T04:37:16.974649Z","shell.execute_reply":"2022-05-17T04:37:16.987288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Here we invoque the memory reducion function.\n# Apply it to the train dataset.\n\n# trn_data = reduce_memory_usage(trn_data)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:37:16.989307Z","iopub.execute_input":"2022-05-17T04:37:16.989596Z","iopub.status.idle":"2022-05-17T04:37:17.00122Z","shell.execute_reply.started":"2022-05-17T04:37:16.989558Z","shell.execute_reply":"2022-05-17T04:37:17.000406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Here we invoque the memory reducion function.\n# Apply it to the test dataset.\n\n# tst_data = reduce_memory_usage(tst_data)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:37:17.005305Z","iopub.execute_input":"2022-05-17T04:37:17.005646Z","iopub.status.idle":"2022-05-17T04:37:17.012206Z","shell.execute_reply.started":"2022-05-17T04:37:17.005614Z","shell.execute_reply":"2022-05-17T04:37:17.011302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 6.0 - Understanding the Information Loaded\n...","metadata":{}},{"cell_type":"code","source":"%%time\n# Display the number of unique values for each variable, sorted by quantity.\n\ntrn_data.nunique().sort_values(ascending = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:37:17.013449Z","iopub.execute_input":"2022-05-17T04:37:17.013849Z","iopub.status.idle":"2022-05-17T04:37:17.880511Z","shell.execute_reply.started":"2022-05-17T04:37:17.013812Z","shell.execute_reply":"2022-05-17T04:37:17.879781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Display information about the variables.\n\ntrn_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:37:17.881975Z","iopub.execute_input":"2022-05-17T04:37:17.882478Z","iopub.status.idle":"2022-05-17T04:37:18.021553Z","shell.execute_reply.started":"2022-05-17T04:37:17.882438Z","shell.execute_reply":"2022-05-17T04:37:18.020418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 7.0 - Feature Engineering\n...","metadata":{}},{"cell_type":"markdown","source":"## 7.1 Create Features from a String of Characters","metadata":{}},{"cell_type":"code","source":"%%time\n# Define a function to exploit the information enbedded in a string of chars.\n\ndef count_chars(df, field):\n    '''\n    Describe something...\n    '''\n    \n    for i in range(10):\n        df[f'ch_{i}'] = df[field].str.get(i).apply(ord) - ord('A')\n        \n    df[\"unique_characters\"] = df[field].apply(lambda s: len(set(s)))\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:37:18.022818Z","iopub.execute_input":"2022-05-17T04:37:18.023058Z","iopub.status.idle":"2022-05-17T04:37:18.029676Z","shell.execute_reply.started":"2022-05-17T04:37:18.023025Z","shell.execute_reply":"2022-05-17T04:37:18.028282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Utilizes the new created funtion to generate more features.\n\ntrn_data = count_chars(trn_data, 'f_27')\ntst_data = count_chars(tst_data, 'f_27')","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:37:18.030781Z","iopub.execute_input":"2022-05-17T04:37:18.031148Z","iopub.status.idle":"2022-05-17T04:37:35.616027Z","shell.execute_reply.started":"2022-05-17T04:37:18.031104Z","shell.execute_reply":"2022-05-17T04:37:35.615284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## 7.2 Create Features Across Columns","metadata":{}},{"cell_type":"code","source":"%%time\n# Create new features across the dataset columns \n\ncontinuous_feat = ['f_00', 'f_01', 'f_02', 'f_03', 'f_04', 'f_05', 'f_06', 'f_19', 'f_20', 'f_21', 'f_22', 'f_23', 'f_24', 'f_25', 'f_26', 'f_28']\n\ndef stat_features(df, cols = continuous_feat):\n    '''\n    Calculate aggregated features across the selected continuous columns\n    \n    '''\n    # Base statistical features.\n    df['f_sum']  = df[continuous_feat].sum(axis=1)\n    df['f_min']  = df[continuous_feat].min(axis=1)\n    df['f_max']  = df[continuous_feat].max(axis=1)\n    df['f_std']  = df[continuous_feat].std(axis=1)    \n    df['f_mad']  = df[continuous_feat].mad(axis=1)\n    df['f_mean'] = df[continuous_feat].mean(axis=1)\n    df['f_kurt'] = df[continuous_feat].kurt(axis=1)\n\n    # Extra statistical features more complex interactions.\n    df['f_prod'] = df[continuous_feat].prod(axis=1)\n    df['f_range'] = df[continuous_feat].max(axis=1) - df[continuous_feat].min(axis=1)\n    df['f_count_pos']  = df[df[continuous_feat].gt(0)].count(axis=1)\n    df['f_count_neg']  = df[df[continuous_feat].lt(0)].count(axis=1)\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:37:35.61746Z","iopub.execute_input":"2022-05-17T04:37:35.617871Z","iopub.status.idle":"2022-05-17T04:37:35.627301Z","shell.execute_reply.started":"2022-05-17T04:37:35.617833Z","shell.execute_reply":"2022-05-17T04:37:35.626497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Utilizes the new created funtion to generate more features.\n\ntrn_data = stat_features(trn_data, continuous_feat)\ntst_data = stat_features(tst_data, continuous_feat)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:37:35.628661Z","iopub.execute_input":"2022-05-17T04:37:35.62909Z","iopub.status.idle":"2022-05-17T04:38:06.390288Z","shell.execute_reply.started":"2022-05-17T04:37:35.629052Z","shell.execute_reply":"2022-05-17T04:38:06.38955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# ...\ntrn_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:38:06.391918Z","iopub.execute_input":"2022-05-17T04:38:06.392444Z","iopub.status.idle":"2022-05-17T04:38:06.413684Z","shell.execute_reply.started":"2022-05-17T04:38:06.392401Z","shell.execute_reply":"2022-05-17T04:38:06.412915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# ...\ntrn_data.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:38:06.415102Z","iopub.execute_input":"2022-05-17T04:38:06.41542Z","iopub.status.idle":"2022-05-17T04:38:06.579698Z","shell.execute_reply.started":"2022-05-17T04:38:06.415381Z","shell.execute_reply":"2022-05-17T04:38:06.5789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndef calculate_feat_int(df):\n    df['i_02_21'] = (df.f_21 + df.f_02 > 5.2).astype(int) - (df.f_21 + df.f_02 < -5.3).astype(int)\n    df['i_05_22'] = (df.f_22 + df.f_05 > 5.1).astype(int) - (df.f_22 + df.f_05 < -5.4).astype(int)\n    i_00_01_26 = df.f_00 + df.f_01 + df.f_26\n    df['i_00_01_26'] = (i_00_01_26 > 5.0).astype(int) - (i_00_01_26 < -5.0).astype(int)\n    return df\n\ntrn_data = calculate_feat_int(trn_data)\ntst_data = calculate_feat_int(tst_data)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:38:06.581135Z","iopub.execute_input":"2022-05-17T04:38:06.581405Z","iopub.status.idle":"2022-05-17T04:38:06.640746Z","shell.execute_reply.started":"2022-05-17T04:38:06.58137Z","shell.execute_reply":"2022-05-17T04:38:06.640023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 8.0 Data Processing, for Model Development\n...","metadata":{}},{"cell_type":"code","source":"%%time\n# Create a list of Continuous features...\n\nMAX_UNIQUE = 800_000\ncontinuous_feat = [feat for feat in trn_data.columns if trn_data[feat].nunique() > MAX_UNIQUE]\ncontinuous_feat.remove('id')\nprint(continuous_feat)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:38:06.641931Z","iopub.execute_input":"2022-05-17T04:38:06.642362Z","iopub.status.idle":"2022-05-17T04:38:08.027715Z","shell.execute_reply.started":"2022-05-17T04:38:06.642322Z","shell.execute_reply":"2022-05-17T04:38:08.026931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Create a list of Continuous features based on the Feature Eng. results, to avoid confusion...\n\nadd_continuous_feat = ['ch_0', 'ch_1', 'ch_2', 'ch_3', 'ch_4', 'ch_5', 'ch_6', 'ch_7', 'ch_8', 'ch_9', 'unique_characters', \n                       'f_sum', 'f_min', 'f_max', 'f_std', 'f_mad', 'f_mean', 'f_kurt', 'f_prod', 'f_range', 'f_count_pos', 'f_count_neg']\n\nlist_1 = set(continuous_feat)\nlist_2 = set(add_continuous_feat)\ncontinuous_feat = list(list_1.union(list_2))","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:38:08.028866Z","iopub.execute_input":"2022-05-17T04:38:08.029096Z","iopub.status.idle":"2022-05-17T04:38:08.037123Z","shell.execute_reply.started":"2022-05-17T04:38:08.029062Z","shell.execute_reply":"2022-05-17T04:38:08.036361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# ...\nprint(continuous_feat)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:38:08.038827Z","iopub.execute_input":"2022-05-17T04:38:08.039384Z","iopub.status.idle":"2022-05-17T04:38:08.045722Z","shell.execute_reply.started":"2022-05-17T04:38:08.039343Z","shell.execute_reply":"2022-05-17T04:38:08.044894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Create a list of Categorical features...\n\ncategorical_feat = [feat for feat in trn_data.columns if feat not in continuous_feat]\ncategorical_feat.remove('target')\ncategorical_feat.remove('id')\ncategorical_feat.remove('f_27')\nprint(categorical_feat)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:38:08.047581Z","iopub.execute_input":"2022-05-17T04:38:08.047997Z","iopub.status.idle":"2022-05-17T04:38:08.055088Z","shell.execute_reply.started":"2022-05-17T04:38:08.047957Z","shell.execute_reply":"2022-05-17T04:38:08.05433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Display the Categorical features and the number of unique values...\n\ntrn_data[categorical_feat].nunique()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:38:08.056596Z","iopub.execute_input":"2022-05-17T04:38:08.057151Z","iopub.status.idle":"2022-05-17T04:38:08.279084Z","shell.execute_reply.started":"2022-05-17T04:38:08.057113Z","shell.execute_reply":"2022-05-17T04:38:08.278287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Overwrite the previous selection based on number of unique values..\n# I'm trying to avoid the usage of embedding layers by the fastai learner..\n# basically blanking the categorical_feat fields..\n\ncontinuous_feat = ['unique_characters', 'f_06', 'ch_7', 'ch_0', 'ch_8', 'f_std', 'f_range', 'f_24', 'f_min', 'f_21', 'ch_2', 'f_03',\n                   'f_sum', 'f_05', 'f_count_neg', 'f_22', 'f_02', 'ch_3', 'f_26', 'f_00', 'ch_6', 'f_23', 'f_mean',\n                   'f_count_pos', 'ch_9', 'f_prod', 'f_kurt', 'ch_4', 'f_mad', 'f_max',\n                   'f_25', 'f_04', 'f_20', 'f_19', 'f_01', 'f_28', 'ch_1', 'ch_5', 'f_07', 'f_08', 'f_09', 'f_10', 'f_11', 'f_12', 'f_13', 'f_14', 'f_15', 'f_16', \n                   'f_17', 'f_18', 'f_29', 'f_30']\n\ncategorical_feat = [] # Avoid the usage of embedding layers...","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:38:08.280739Z","iopub.execute_input":"2022-05-17T04:38:08.281279Z","iopub.status.idle":"2022-05-17T04:38:08.289772Z","shell.execute_reply.started":"2022-05-17T04:38:08.281221Z","shell.execute_reply":"2022-05-17T04:38:08.288826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 9.0 - Creating the fast.ai Model\nIn this step we will create the fast ai model and explain all the required steps...\nSteps...\n* Define all the data processing steps requiered.\n* Define the split technique that will be utilized.\n* Define some of the parameters for the TabularDataLoader.\n* Create a TabularDataLoader.\n* Show one batch of data, to identify issues.\n* Define some of the parameters of the tabular_learner.\n* Define the tabular_learner\n* Visualize the model architecture.\n* Train the model for 1 epoch, this is kind of an initialization.\n* Optimize the model learning rate.\n* Train the model for multiple epochs.\n* Done...","metadata":{}},{"cell_type":"code","source":"%%time\n# Creating a list of the data preprocessing steps we want to apply to the model...\n\ndata_processing = [FillMissing, # Fill the missing values in continuous columns.\n                   Categorify,  # Transform the categorical variables to something similar to pd.Categorical.\n                   Normalize,   # Normalize the datset before trainig the model.\n                  ]","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:38:08.291526Z","iopub.execute_input":"2022-05-17T04:38:08.291844Z","iopub.status.idle":"2022-05-17T04:38:08.301639Z","shell.execute_reply.started":"2022-05-17T04:38:08.291805Z","shell.execute_reply":"2022-05-17T04:38:08.300738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Create a simple split methodology for the dataset to be used during training...\n# Not used in this example...\n\nsplit_sample = np.random.choice(trn_data.shape[0], 200) # ...","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:38:08.303161Z","iopub.execute_input":"2022-05-17T04:38:08.304101Z","iopub.status.idle":"2022-05-17T04:38:08.311798Z","shell.execute_reply.started":"2022-05-17T04:38:08.303927Z","shell.execute_reply":"2022-05-17T04:38:08.310545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Create a dataloader to be used by the learner\n# Difining some data loader params\n\nbatch_size = 1024\nvalid_pct  = 0.15\n\n# Creting the tabular data loader\ndata = TabularDataLoaders.from_df(df = trn_data,                     # Here we pass tge train dataset.\n                                  path = '.',                        # Here we pass the location path of the data, in this case null = '.'\n                                  procs = data_processing,           # ...\n                                  cat_names = categorical_feat,      # ...\n                                  cont_names = continuous_feat,      # ...\n                                  valid_pct = valid_pct,             # ...   \n                                  bs = batch_size,                   # ... \n                                  y_block = CategoryBlock,           # ...\n                                  y_names = 'target',                # ...\n                                 )","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:38:08.317921Z","iopub.execute_input":"2022-05-17T04:38:08.319398Z","iopub.status.idle":"2022-05-17T04:38:13.298256Z","shell.execute_reply.started":"2022-05-17T04:38:08.319354Z","shell.execute_reply":"2022-05-17T04:38:13.297492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Display a batch of the data loaded to the data loader that will be used in the training stage\n\ndata.show_batch()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:38:13.29937Z","iopub.execute_input":"2022-05-17T04:38:13.300159Z","iopub.status.idle":"2022-05-17T04:38:14.164949Z","shell.execute_reply.started":"2022-05-17T04:38:13.30012Z","shell.execute_reply":"2022-05-17T04:38:14.164141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Define the tabular learner to be used in the training stage\n# Learner parameters\n\nlayers_definition = [1024, 256, 128, 128, 32] #...\nemb_size = None #...\nmy_config = tabular_config(y_range = (0,1)) #...\n\n\n# Learner definition\nlearn = tabular_learner(dls     = data,                #....\n                        layers  = layers_definition,   #....\n                        emb_szs = emb_size,            #....\n                        metrics = [accuracy],          #....\n                        config  = my_config,           #....\n                       ).to_fp16()                     #....","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:38:14.166329Z","iopub.execute_input":"2022-05-17T04:38:14.167071Z","iopub.status.idle":"2022-05-17T04:38:14.182165Z","shell.execute_reply.started":"2022-05-17T04:38:14.167031Z","shell.execute_reply":"2022-05-17T04:38:14.181485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Prints out the model architecture.\n# As a summary for easy visualization.\nlearn.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:38:14.183723Z","iopub.execute_input":"2022-05-17T04:38:14.184003Z","iopub.status.idle":"2022-05-17T04:38:15.085693Z","shell.execute_reply.started":"2022-05-17T04:38:14.183965Z","shell.execute_reply":"2022-05-17T04:38:15.084734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 10.0 - Training the fast.ai Model\n...","metadata":{}},{"cell_type":"code","source":"%%time\n# Fit one fastai cycle on the training dataset.\n# It's like a warm start to identify the optimal LR for the model in the subsequent steps.\n\nlearn.fit_one_cycle(1)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:38:15.08703Z","iopub.execute_input":"2022-05-17T04:38:15.087369Z","iopub.status.idle":"2022-05-17T04:38:27.541357Z","shell.execute_reply.started":"2022-05-17T04:38:15.087331Z","shell.execute_reply":"2022-05-17T04:38:27.540645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Find the optimal Learning Rate for the model based on previous information...\n\nlearn.lr_find()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:38:27.544113Z","iopub.execute_input":"2022-05-17T04:38:27.54435Z","iopub.status.idle":"2022-05-17T04:38:30.738037Z","shell.execute_reply.started":"2022-05-17T04:38:27.544324Z","shell.execute_reply":"2022-05-17T04:38:30.737135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Fit 3 epochs on the training dataset.\n# Using the LR identified in the previous step.\n\nlr = 0.0005754399462603033\nlearn.fit_one_cycle(6, lr_max = lr)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:38:30.739564Z","iopub.execute_input":"2022-05-17T04:38:30.739846Z","iopub.status.idle":"2022-05-17T04:39:43.869646Z","shell.execute_reply.started":"2022-05-17T04:38:30.739809Z","shell.execute_reply":"2022-05-17T04:39:43.868933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#...\nlearn.fine_tune(30, base_lr = lr, freeze_epochs = 3)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:39:43.870847Z","iopub.execute_input":"2022-05-17T04:39:43.871321Z","iopub.status.idle":"2022-05-17T04:46:18.283867Z","shell.execute_reply.started":"2022-05-17T04:39:43.871279Z","shell.execute_reply":"2022-05-17T04:46:18.283055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 11.0 - Creating Model Predictions\n...","metadata":{}},{"cell_type":"code","source":"%%time\n#...\ndl = learn.dls.test_dl(tst_data)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:46:18.285198Z","iopub.execute_input":"2022-05-17T04:46:18.286041Z","iopub.status.idle":"2022-05-17T04:46:19.59377Z","shell.execute_reply.started":"2022-05-17T04:46:18.286003Z","shell.execute_reply":"2022-05-17T04:46:19.593032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#...\ndl.show_batch()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:46:19.595268Z","iopub.execute_input":"2022-05-17T04:46:19.595752Z","iopub.status.idle":"2022-05-17T04:46:19.847167Z","shell.execute_reply.started":"2022-05-17T04:46:19.595712Z","shell.execute_reply":"2022-05-17T04:46:19.846387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#...\nnn_predictions, _, preds = learn.get_preds(dl = dl, with_decoded = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:46:19.848484Z","iopub.execute_input":"2022-05-17T04:46:19.849005Z","iopub.status.idle":"2022-05-17T04:46:24.239164Z","shell.execute_reply.started":"2022-05-17T04:46:19.848963Z","shell.execute_reply":"2022-05-17T04:46:24.238166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#...\nnn_predictions, preds","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:46:24.240651Z","iopub.execute_input":"2022-05-17T04:46:24.241378Z","iopub.status.idle":"2022-05-17T04:46:24.252605Z","shell.execute_reply.started":"2022-05-17T04:46:24.241337Z","shell.execute_reply":"2022-05-17T04:46:24.25153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#...\nsub.target = np.argmax(nn_predictions, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:46:24.254035Z","iopub.execute_input":"2022-05-17T04:46:24.255357Z","iopub.status.idle":"2022-05-17T04:46:24.268138Z","shell.execute_reply.started":"2022-05-17T04:46:24.255318Z","shell.execute_reply":"2022-05-17T04:46:24.267298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Submit predicitons from the CV trained Model...\nsub.to_csv('submission_fastai.csv', index = None)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:46:24.269403Z","iopub.execute_input":"2022-05-17T04:46:24.26987Z","iopub.status.idle":"2022-05-17T04:46:25.291272Z","shell.execute_reply.started":"2022-05-17T04:46:24.269834Z","shell.execute_reply":"2022-05-17T04:46:25.290382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Submit predicitons from the CV trained Model...\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T04:46:25.292802Z","iopub.execute_input":"2022-05-17T04:46:25.293796Z","iopub.status.idle":"2022-05-17T04:46:25.315708Z","shell.execute_reply.started":"2022-05-17T04:46:25.293756Z","shell.execute_reply":"2022-05-17T04:46:25.314916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}