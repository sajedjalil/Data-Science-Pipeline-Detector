{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook I've tried a new strategy, I specified for each row the time, starting from the initial time_to_eruption time associated with each dataset in order to perform a regression model on each row. I'm open to suggetion about this new strategy :)"},{"metadata":{},"cell_type":"markdown","source":"I used just 500 out of the 4000 available dataset cause otherwise it crashs. "},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ProgressBar","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom progressbar import ProgressBar\nimport glob\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frag = glob.glob(\"../input/predict-volcanic-eruptions-ingv-oe/train/*\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing_train(name):\n    train_df = pd.read_csv('../input/predict-volcanic-eruptions-ingv-oe/train.csv')\n    dataframe = pd.DataFrame()\n    pbar = ProgressBar()\n    index=[]\n    for i in range(0,len(frag[0:5])):\n        index = np.append(index,os.path.splitext(frag[i].split('{}/'.format(name))[1])[0])\n    index = index.astype('int')\n\n    for i in pbar(index):\n        df = pd.read_csv('../input/predict-volcanic-eruptions-ingv-oe/{}/{}.csv'.format(name,i))\n        value = (train_df['time_to_eruption'][train_df['segment_id']==i])\n        value = np.array(value)\n        valuelist = np.arange(value,value+60001,1)\n        df['time_to_eruption']=valuelist\n        dataframe = pd.concat([dataframe,df],axis=0)\n        del df\n    return(dataframe)\n\ndef preprocessing_test(name):\n    index = []\n    frag = glob.glob(\"../input/predict-volcanic-eruptions-ingv-oe/{}/*\".format(name))\n    df=pd.DataFrame()\n\n    pbar = ProgressBar()\n    for i in pbar(frag):\n        df = np.append(df,pd.read_csv(i).mean())\n    \n    df = pd.DataFrame(df.reshape(len(frag),10))  \n\n    for i in range(0,len(frag)):\n        index = np.append(index,os.path.splitext(frag[i].split('{}/'.format(name))[1])[0])\n        \n    df['segment_id']=index\n    df['segment_id']=df['segment_id'].astype(int)\n    df.columns= ['sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_10','segment_id']\n    return(df)\n\ndef fill_na(data):\n    for i in data.columns:\n        data[i] = data[i].fillna(np.mean(data[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndata_x = preprocessing_train('train')\ndata_y= preprocessing_test('test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,60001):\n    print(i,data_x.iloc[i,-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fill_na(data_x)\nfill_na(data_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data_x['time_to_eruption']\nx= data_x.iloc[:,0:-1]\n\ndel data_x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n\nfrom  sklearn.model_selection import train_test_split\nimport lightgbm as lgb\n\nXt, Xv, Yt, Yv = train_test_split(x, y, test_size =0.2, shuffle=False)\n\nparams = {\n        'objective': 'regression', #specify how is the dependet variable, binary can be used for logistic regression or log loss classification\n        'max_bin': 600, #max number of bins that features values will be bucketed in. Small number may reduce training accuracy but may increase general power\n        'learning_rate': 0.02, #learning_rate refers to the step size at each interation while moving toward an optimal point\n        'num_leaves': 80, # maximum number of leaves in a tree, where a leave is a final termination of a tree\n        'metric' : 'mae'\n}\n\n\nlgb_train = lgb.Dataset(Xt, Yt)\nlgb_eval = lgb.Dataset(Xv, Yv, reference=lgb_train)\n        #lightgbm need to take as argument lightgbm dataset, it is required to make this trasformation\n\nmodel = lgb.train(\n            params, lgb_train, #it is required to insert the parameters, then the train set\n            valid_sets=[lgb_train, lgb_eval],\n            verbose_eval=100,\n            num_boost_round=1500, # number of boosting iterations \n            early_stopping_rounds=15 # will stop training if one metric of one validation data doesnâ€™t improve in last early_stopping_round rounds, so if \n            #  for ten 'epochs' the model will stop, in this way the num_boost_round is a maximum value.  \n)  \n\ny_pred = model.predict(Xv)\ny_true = np.array(Yv)\nprint('mean absolute error:',mae(y_true, y_pred))\n\nprediction = model.predict(data_y.iloc[:,0:-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.DataFrame(data_y['segment_id'])\nsub_df = pd.concat([sub_df,pd.Series(prediction)],axis=1)\nsample_submission=pd.read_csv('../input/predict-volcanic-eruptions-ingv-oe/sample_submission.csv')\n        \nsample_submission = pd.merge(sample_submission,sub_df, on =['segment_id'])\nsample_submission = sample_submission.drop(columns=['time_to_eruption'])\nsample_submission.columns = ['segment_id', 'time_to_eruption']\nsample_submission.to_csv('sample_submission.csv', header=True, index=False)\nprint('saved :)')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}