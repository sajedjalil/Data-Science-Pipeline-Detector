{"cells":[{"metadata":{},"cell_type":"markdown","source":"Whenever I submit this kernel I keep getting a notebook timeout error. I do not know why this is happening because when I run it here it runs perfectly fine. Any help would be appreciated thank you. "},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom skimage import io, transform\nimport os\n#os.listdir('/kaggle/input/deepfake-detection-challenge/test_videos')\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Install facenet-pytorch\n!pip install /kaggle/input/facenet-pytorch-vggface2/facenet_pytorch-2.2.7-py3-none-any.whl\n\n\n# Copy model checkpoints to torch cache so they are loaded automatically by the package\n!mkdir -p $torch_home/checkpoints/\n!cp /kaggle/input/facenet-pytorch-vggface2/20180402-114759-vggface2-logits.pth $torch_home/checkpoints/vggface2_DG3kwML46X.pt\n!cp /kaggle/input/facenet-pytorch-vggface2/20180402-114759-vggface2-features.pth $torch_home/checkpoints/vggface2_G5aNV2VSMn.pt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport glob\nimport cv2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = models.mobilenet_v2(pretrained=False)\nmodel.classifier = nn.Sequential(\n   nn.Linear(1280,1024,bias=False), nn.ReLU(inplace=True),nn.BatchNorm1d(1024),nn.Dropout(0.4),\n   nn.Linear(1024,512,bias=False), nn.ReLU(inplace=True),nn.BatchNorm1d(512),nn.Dropout(0.4),\n   nn.Linear(512,256,bias=False), nn.ReLU(inplace=True),nn.BatchNorm1d(256),nn.Dropout(0.4),\n   nn.Linear(256,1,bias=False))\nmodel.cuda()\nmodel.load_state_dict(torch.load('/kaggle/input/deepfakemodel/mobilenet-detection.pt'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nPyTorch Video Dataset Class for loading videos using PyTorch\nDataloader. This Dataset assumes that video files are Preprocessed\n by being trimmed over time and resizing the frames.\nMohsen Fayyaz __ Sensifai Vision Group\nhttp://www.Sensifai.com\nIf you find this code useful, please star the repository.\n\"\"\"\n\nfrom __future__ import print_function, division\nfrom skimage import io, transform\nfrom tqdm import tqdm_notebook as tqdm\nimport cv2\nimport os\nimport torch\nimport numpy as np\nimport pickle\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport json\nimport torchvision\nimport torch.nn as nn\nimport time\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch.nn.functional as F\nfrom facenet_pytorch import MTCNN, InceptionResnetV1\nfrom PIL import Image\nmtcnn = MTCNN(image_size=224, select_largest=False, post_process=False, margin=100,device='cuda:0')\n\n\nimport matplotlib.pyplot as plt\n\nclass RandomCrop(object):\n    \"\"\"Crop randomly the frames in a clip.\n\tArgs:\n\t\toutput_size (tuple or int): Desired output size. If int, square crop\n\t\t\tis made.\n\t\"\"\"\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        if isinstance(output_size, int):\n            self.output_size = (output_size, output_size)\n        else:\n            assert len(output_size) == 2\n            self.output_size = output_size\n\n    def __call__(self, clip,top,bottom,left,right):\n\n       \n\n        clip = clip[top:bottom, left:right]\n        return clip\n    \nclass Rescale(object):\n    \"\"\"Rescale the image in a sample to a given size.\n\n    Args:\n        output_size (tuple or int): Desired output size. If tuple, output is\n            matched to output_size. If int, smaller of image edges is matched\n            to output_size keeping aspect ratio the same.\n    \"\"\"\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        image = sample\n\n        h, w = image.shape[:2]\n        if isinstance(self.output_size, int):\n            if h > w:\n                new_h, new_w = self.output_size * h / w, self.output_size\n            else:\n                new_h, new_w = self.output_size, self.output_size * w / h\n        else:\n            new_h, new_w = self.output_size\n\n        new_h, new_w = int(new_h), int(new_w)\n       \n        img = transform.resize(image, (new_h, new_w))\n        \n        return img\n\n\nclass videoDataset(Dataset):\n    \"\"\"Dataset Class for Loading Video\"\"\"\n\n    def __init__(self, clipsListFile, rootDir, channels, timeDepth, split, ySize, mean, transform=None):\n        \"\"\"\n\t\tArgs:\n\t\t\tclipsList (string): Path to the clipsList file with labels.\n\t\t\trootDir (string): Directory with all the videoes.\n\t\t\ttransform (callable, optional): Optional transform to be applied\n\t\t\t\ton a sample.\n\t\t\tchannels: Number of channels of frames\n\t\t\ttimeDepth: Number of frames to be loaded in a sample\n\t\t\txSize, ySize: Dimensions of the frames\n\t\t\tmean: Mean valuse of the training set videos over each channel\n\t\t\"\"\"\n        self.counter = 0 \n        self.fcounter = 0\n        clipsList = clipsListFile\n        \n        print(len(clipsList))\n        self.clipsList = clipsList\n        self.rootDir = rootDir\n        self.channels = channels\n        self.timeDepth = timeDepth\n        self.ySize = ySize\n        self.mean = mean\n        self.transform = transform\n        self.split = split\n\n    def __len__(self):\n        #print(len(self.clipsList))\n        return len(self.clipsList)\n\n    def readVideo(self, videoFile):\n        # Open the video file\n        cap = cv2.VideoCapture(videoFile)\n        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        d = 0\n        h = 0\n        new = []\n        failedclip = False\n        while d != self.timeDepth:\n            ret, frame = cap.read()\n            if ret:\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                #try:\n                frame = Image.fromarray(frame)\n                face = mtcnn(frame)\n                try:\n                    d += 1\n\n                    in_transform = transforms.Compose([\n                        transforms.Resize((224,224)),\n                        \n                        transforms.ToTensor(),\n                        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])])\n                    face = face.permute(1, 2, 0).numpy()\n                    face = Image.fromarray(np.uint8(face))\n                    frame = in_transform(face)\n                    #frame= torch.from_numpy(frame)\n                \n                    #frame = frame.permute(2,0,1)\n                    #frame = transform.resize(frame, (3,224,224))\n                    #frame = torch.from_numpy(frame)\n                    frame = frame.unsqueeze(1)\n                    new.append(frame)\n                except:\n                    h += 1\n                    d-=1\n                    if h > total:\n                        failed_clip = True\n                        break\n                 \n\n            else:\n                failed_clip = True\n                break\n\n\n\n                \n       \n        \n              \n                    \n\n            #else:\n             #   print(\"Skipped!\")\n              #  failedClip = True\n               # break\n        try:\n            frames = torch.cat((new[0:self.timeDepth]),dim=1)\n            frames = frames.squeeze(1)\n        except:\n            frames = []\n            print(total)\n                #except: \n                 #   h += 1\n                  #  if h > 150:\n                   #     break\n                    #pass\n        return frames, failedclip\n        #frames = frames.squeeze(1)\n      \n    def __getitem__(self, idx):\n\n        videoFile = os.path.join(self.rootDir, self.clipsList[idx])\n\n        clip,failedclip = self.readVideo(videoFile)\n        return clip,failedclip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfilenames = glob.glob('/kaggle/input/deepfake-detection-challenge/test_videos/*.mp4')\ndataset = videoDataset(filenames,'/kaggle/input/deepfake-detection-challenge/test_videos/',3,1,244,244,0.5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loader = DataLoader(dataset,batch_size=5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def imshow(img):\n    npimg = img.numpy() \n    #npimg = npimg[:,1,:,:]\n    \n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n   \n\"\"\"\nDON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n\"\"\"\n# obtain one batch of training images\ndataiter = iter(test_loader)\nimages,_ = dataiter.next() # _ for no labels\n\n# plot the images in the batch, along with the corresponding labels\nfig = plt.figure(figsize=(20, 8))\nplot_size=4\nfor idx in np.arange(plot_size):\n    ax = fig.add_subplot(2, plot_size/2, idx+1, xticks=[], yticks=[])\n    imshow(images[idx])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images,_ = next(iter(test_loader))\n\nprint(images.shape)\n\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm_notebook as tqdm\nmodel.eval()\npredictions = []\n\nz = 0\nfor i in tqdm(range(400)):\n    images, failedclip = dataset.__getitem__(i)\n    if failedclip:\n        predictions.append([os.path.basename(filenames[i]),0.5])\n    else:\n        images = images.cuda()\n        output = model(images.unsqueeze(0))\n    \n   \n        pred = torch.sigmoid(output).cpu()\n        \n        #if pred1 == 1:\n           # prediction = output[i][pred1].data.cpu().numpy()\n        #else:\n         #   prediction = 1-output[i][pred1].data.cpu().numpy()\n        predictions.append([os.path.basename(filenames[i]), pred.data.cpu().numpy()[0][0]])\n        \n        \n\n\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nsubmission = pd.DataFrame(predictions, columns=['filename', 'label'])\nsubmission.sort_values('filename').to_csv('submission.csv', index=False)\n\nplt.hist(submission.label, 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('submission.csv')\nsub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}