{"cells":[{"metadata":{},"cell_type":"markdown","source":"Credits: <br>\nmobileface:  https://www.kaggle.com/unkownhihi/mobilenet-face-extractor-helper-code/comments <br>\nyolo: https://www.kaggle.com/unkownhihi/mobilenet-face-extractor-comparison#Initialize-Yolo <br>\ndlib: https://www.kaggle.com/carlossouza/face-detection-in-a-couple-of-lines-with-dlib <br>\nblaze face: https://www.kaggle.com/humananalog/inference-demo <br>\nblaze face 2: https://www.kaggle.com/unkownhihi/mobilenet-face-extractor-comparison#Initialize-Yolo <br>\nfacenet_pytorch:  <br>\nMTCNN:  <br>\nRetinaFace: <br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport torch\nimport time\nimport math\nfrom tqdm.notebook import tqdm\nimport glob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_SIZE = 224\nMARGIN = 10\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1) Preparations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# MTCNN\n!pip install ../input/mtcnn-package/mtcnn-0.1.0-py3-none-any.whl -q\nfrom mtcnn import MTCNN as mMTCNN\nmtcnn_detector = mMTCNN()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# facenet_pytorch\nfrom facenet_pytorch import MTCNN as fMTCNN\nfacenet_detector = fMTCNN(image_size=150, margin=0, keep_all=True, factor=0.5, post_process=False, device=device).eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dlib\n!pip install '/kaggle/input/dlibpkg/dlib-19.19.0'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import dlib\ndlib_hog_detector = dlib.get_frontal_face_detector()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# insightface\n!pip install insightface -q\nimport insightface\ninsight_detector = insightface.app.FaceAnalysis()\ninsight_detector.prepare(ctx_id = -1, nms=0.4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mobilenet\nimport tensorflow as tf\ndetection_graph = tf.Graph()\nwith detection_graph.as_default():\n    od_graph_def = tf.compat.v1.GraphDef()\n    with tf.io.gfile.GFile('../input/mobilenet-face/frozen_inference_graph_face.pb', 'rb') as fid:\n        serialized_graph = fid.read()\n        od_graph_def.ParseFromString(serialized_graph)\n        tf.import_graph_def(od_graph_def, name='')\n        config = tf.compat.v1.ConfigProto()\n    config.gpu_options.allow_growth = True\n    sess=tf.compat.v1.Session(graph=detection_graph, config=config)\n    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n    boxes_tensor = detection_graph.get_tensor_by_name('detection_boxes:0')    \n    scores_tensor = detection_graph.get_tensor_by_name('detection_scores:0')\n    num_detections = detection_graph.get_tensor_by_name('num_detections:0')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mobile_detector(image):\n    global boxes,scores,num_detections\n    (h, w)=image.shape[:-1]\n    imgs=np.array([image])\n    (boxes, scores) = sess.run(\n        [boxes_tensor, scores_tensor],\n        feed_dict={image_tensor: imgs})\n    max_=np.where(scores==scores.max())[0][0]\n    box=boxes[0][max_]\n    ymin, xmin, ymax, xmax = box\n    (left, right, top, bottom) = (xmin * w, xmax * w, \n                                  ymin * h, ymax * h)\n    left, right, top, bottom = int(left), int(right), int(top), int(bottom)\n    return (left, right, top, bottom)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# blazeface\ndef prepare_blaze():\n    blazeface = BlazeFace()\n    blazeface.load_weights(\"/kaggle/input/blazeface-pytorch/blazeface.pth\")\n    blazeface.load_anchors(\"/kaggle/input/blazeface-pytorch/anchors.npy\")\n\n    # Optionally change the thresholds:\n    blazeface.min_score_thresh = 0.95\n    blazeface.min_suppression_threshold = 0.3\n    return blazeface\nblazeface = prepare_blaze()\n\ndef get_blaze_boxes(detections):\n    result = []\n    if isinstance(detections, torch.Tensor):\n        detections = detections.cpu().numpy()\n\n    if detections.ndim == 1:\n        detections = np.expand_dims(detections, axis=0)\n\n    # no detect\n    img_shape = (128, 128)\n    if len(detections) != 0:\n        for i in range(detections.shape[0]):\n            ymin = detections[i, 0] * img_shape[0]\n            xmin = detections[i, 1] * img_shape[1]\n            ymax = detections[i, 2] * img_shape[0]\n            xmax = detections[i, 3] * img_shape[1]\n            result.append((xmin, ymin, xmax, ymax))\n    return result\n\n\ndef scale_boxes(boxes, scale_w, scale_h):\n    sb = []\n    for b in boxes:\n        sb.append((b[0] * scale_w, b[1] * scale_h, b[2] * scale_w, b[3] * scale_h))\n    return sb\n\n\ndef blaze_detector(img):\n    img = cv2.resize(img, (128, 128))\n    output = blazeface.predict_on_image(img)\n    bbox = scale_boxes(get_blaze_boxes(output), 380/128, 380/128)\n    return bbox\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/deepfake-detection-challenge/sample_submission.csv')\nvideo_names = df['filename']\nvideo_paths = ['../input/deepfake-detection-challenge/test_videos/' + n for n in video_names]\nvideo_paths[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_video = video_paths[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2) Video Read Speed","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Read all frames","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. Opencv Read Video","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nbefore = time.time()\n\ndef read_video1(video_path):\n    v_cap = cv2.VideoCapture(sample_video)\n    v_int = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    all_frames = []\n    for i in range(v_int):\n        ret, frame = v_cap.read()\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        all_frames.append(frame)\n    return np.array(all_frames)\n    \nresult = read_video1(sample_video)\n\nafter = time.time()\nprint('result {} with time {} seconds'.format(result.shape, round(after-before, 1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"v_cap.grab() and v_cap.retrieve()","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nbefore = time.time()\n\ndef read_video2(video_path):\n    v_cap = cv2.VideoCapture(sample_video)\n    v_int = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    all_frames = []\n    for i in range(v_int):\n        v_cap.grab()\n        ret, frame = v_cap.retrieve()\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        all_frames.append(frame)\n    return np.array(all_frames)\n    \nresult = read_video2(sample_video)\n\nafter = time.time()\nprint('result {} with time {} seconds'.format(result.shape, round(after-before, 1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Image_ffmpeg","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/imageio-ffmpeg/imageio_ffmpeg-0.3.0-py3-none-manylinux2010_x86_64.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import imageio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nbefore = time.time()\n\ndef read_video3(video_path):\n    reader = imageio.get_reader(video_path, 'ffmpeg')\n    v_int = reader.count_frames()\n    meta = reader.get_meta_data()\n    all_frames = []\n    for i in range(v_int):\n        img = reader.get_data(i)\n        all_frames.append(img)\n    return np.array(all_frames)\n    \nresult = read_video3(sample_video)\n\nafter = time.time()\nprint('result {} with time {} seconds'.format(result.shape, round(after-before, 1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## read specific frames","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"frames_per_video = 72","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. v_cap.grab()","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_frames1(frames_per_video=60):\n    v_cap = cv2.VideoCapture(sample_video)\n    v_int = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    all_frames = []\n    sample_idx = np.linspace(0, v_int-1, frames_per_video, endpoint=True, dtype=np.int)\n    for i in range(v_int):\n        # speed reading without decode unwanted frame\n        ret = v_cap.grab()\n        \n        if ret is None: \n            print('The {} cannot be read'.format(i))\n            continue\n            \n#         # the frame we want\n        if i in sample_idx:\n            ret, frame = v_cap.retrieve()\n            if ret is None or frame is None:\n                continue\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            all_frames.append(frame)\n            \n    return all_frames","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nbefore = time.time()\n\nresult = read_frames1(frames_per_video)\n\nafter = time.time()\nprint('result {} with time {} seconds'.format(np.array(result).shape, round(after-before, 1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. v_cap.read()","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_frames2(frames_per_video=60):\n    v_cap = cv2.VideoCapture(sample_video)\n    v_int = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    all_frames = []\n    sample_idx = np.linspace(0, v_int-1, frames_per_video, endpoint=True, dtype=np.int)\n    for i in range(v_int):\n        ret, frame = v_cap.read()\n        if ret is None or frame is None:\n            continue\n        \n        if i in sample_idx:\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            all_frames.append(frame)\n        \n    return all_frames","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nbefore = time.time()\n\nresult = read_frames2(frames_per_video)\n\nafter = time.time()\nprint('result {} with time {} seconds'.format(np.array(result).shape, round(after-before, 1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. imageio_ffmpeg","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_frames3(frames_per_video):\n    reader = imageio.get_reader(sample_video, 'ffmpeg')\n    v_int = reader.count_frames()\n    all_frames = []\n    sample_idx = np.linspace(0, v_int-1, frames_per_video, endpoint=True, dtype=np.int)\n    for i in sample_idx:\n        img = reader.get_data(i)\n        all_frames.append(img)\n    return all_frames","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nbefore = time.time()\n\nresult = read_frames3(frames_per_video)\n\nafter = time.time()\nprint('result {} with time {} seconds'.format(np.array(result).shape, round(after-before, 1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. v_cap.cat","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_frames4(frames_per_video=72):\n    v_cap = cv2.VideoCapture(sample_video)\n    v_int = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    all_frames = []\n    sample_idx = np.linspace(0, v_int-1, frames_per_video, endpoint=True, dtype=np.int)\n    for i in sample_idx:\n        v_cap.set(cv2.CAP_PROP_POS_FRAMES, i) \n        ret, frame = v_cap.read()\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        all_frames.append(frame)\n    return all_frames","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nbefore = time.time()\n\nresult = read_frames4(frames_per_video)\n\nafter = time.time()\nprint('result {} with time {} seconds'.format(np.array(result).shape, round(after-before, 1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Face Detectoe Evaluate","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# unify show faces\nshow_faces = 9\nframes = read_video2(sample_video)\ncell = round(math.sqrt(9))\nrandom_idx = np.random.choice(len(frames), show_faces)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MTCNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from mtcnn import MTCNN\ndetector = MTCNN()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect_face(img):\n    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    final = []\n    detected_faces_raw = detector.detect_faces(img)\n    if detected_faces_raw==[]:\n        #print('no faces found')\n        return []\n    confidences=[]\n    for n in detected_faces_raw:\n        x,y,w,h=n['box']\n        final.append([x,y,w,h])\n        confidences.append(n['confidence'])\n    if max(confidences)<0.9:\n        return []\n    max_conf_coord=final[confidences.index(max(confidences))]\n    #return final\n    return max_conf_coord\n\ndef crop(img,x,y,w,h, margin, img_shape):\n    x-=margin\n    y-=margin\n    w+=margin*2\n    h+=margin*2\n    if x<0:\n        x=0\n    if y<=0:\n        y=0\n    return cv2.cvtColor(cv2.resize(img[y:y+h,x:x+w],img_shape),cv2.COLOR_BGR2RGB)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect_faces_mtcnn(video_path):\n    total_faces = []\n    \n    # full frames\n    frames = read_video2(video_path)\n    \n    for frame in tqdm(frames):\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n        bounding_box = detect_face(frame)\n        if bounding_box == []:\n            continue\n        else:\n            x,y,w,h = bounding_box\n            face = crop(frame, x, y, w, h, MARGIN, (IMG_SIZE, IMG_SIZE))\n            total_faces.append(face)\n            \n    return np.array(total_faces)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"speed and detect number","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nbefore = time.time()\n\nfaces = detect_faces_mtcnn(sample_video)\n\nafter = time.time()\nprint('result {} with time {} seconds'.format(faces.shape, round(after-before, 1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"random effects","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(cell, cell, figsize=(8, 8))\nfor i in range(cell):\n    for j in range(cell):\n        ax[i][j].imshow(faces[random_idx[i*cell+j]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Facenet_pytorch","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install ../input/facenet-pytorch-vggface2/facenet_pytorch-2.0.1-py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from facenet_pytorch import MTCNN\nmtcnn = MTCNN(image_size=IMG_SIZE, margin=MARGIN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect_faces_facenet(video_path):\n    total_faces = []\n    \n    frames = read_video2(video_path)\n    \n    for frame in tqdm(frames):\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        faces = net_facenet(Image.fromarray(frame))\n        if faces is None:\n            continue\n        else:\n            total_faces.append(faces.numpy()[0])   \n        \n    return np.array(total_faces)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nbefore = time.time()\n\nfaces = detect_faces_facenet(sample_video)\n\nafter = time.time()\nprint('result {} with time {} seconds'.format(faces.shape, round(after-before, 1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(cell, cell, figsize=(8, 8))\nfor i in range(cell):\n    for j in range(cell):\n        ax[i][j].imshow(faces[random_idx[i*cell+j]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BlazeFace","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.insert(0, \"/kaggle/input/blazeface-pytorch\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from blazeface import BlazeFace\n\nnet = BlazeFace()\nnet.load_weights(\"/kaggle/input/blazeface-pytorch/blazeface.pth\")\nnet.load_anchors(\"/kaggle/input/blazeface-pytorch/anchors.npy\")\n\n# Optionally change the thresholds:\nnet.min_score_thresh = 0.75\nnet.min_suppression_threshold = 0.3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"BlazeFace github repo offical version","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_detections(img, detections, with_keypoints=True):\n    fig, ax = plt.subplots(1, figsize=(10, 10))\n    ax.grid(False)\n    ax.imshow(img)\n    \n    if isinstance(detections, torch.Tensor):\n        detections = detections.cpu().numpy()\n\n    if detections.ndim == 1:\n        detections = np.expand_dims(detections, axis=0)\n\n    print(\"Found %d faces\" % detections.shape[0])\n        \n    for i in range(detections.shape[0]):\n        ymin = detections[i, 0] * img.shape[0]\n        xmin = detections[i, 1] * img.shape[1]\n        ymax = detections[i, 2] * img.shape[0]\n        xmax = detections[i, 3] * img.shape[1]\n\n        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n                                 linewidth=1, edgecolor=\"r\", facecolor=\"none\", \n                                 alpha=detections[i, 16])\n        ax.add_patch(rect)\n\n        if with_keypoints:\n            for k in range(6):\n                kp_x = detections[i, 4 + k*2    ] * img.shape[1]\n                kp_y = detections[i, 4 + k*2 + 1] * img.shape[0]\n                circle = patches.Circle((kp_x, kp_y), radius=0.5, linewidth=1, \n                                        edgecolor=\"lightskyblue\", facecolor=\"none\", \n                                        alpha=detections[i, 16])\n                ax.add_patch(circle)\n        \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tile_frames(frames, target_size):\n    num_frames, H, W, _ = frames.shape\n\n    split_size = min(H, W)\n    x_step = (W - split_size) // 2\n    y_step = (H - split_size) // 2\n    num_v = 1\n    num_h = 3 if W > H else 1\n\n    splits = np.zeros((num_frames * num_v * num_h, target_size[1], target_size[0], 3), dtype=np.uint8)\n\n    i = 0\n    for f in range(num_frames):\n        y = 0\n        for v in range(num_v):\n            x = 0\n            for h in range(num_h):\n                crop = frames[f, y:y+split_size, x:x+split_size, :]\n                splits[i] = cv2.resize(crop, target_size, interpolation=cv2.INTER_AREA)\n                x += x_step\n                i += 1\n            y += y_step\n\n    resize_info = [split_size / target_size[0], split_size / target_size[1]]\n    return splits, resize_info","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1-1 without ***tile image***, some faces cannot be detectd <br>\nsee how many faces are detected","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect_faces_full_frame(video_path):\n    total_detections = []\n    frames = read_video2(video_path)\n    for frame in tqdm(frames):\n        frame = cv2.resize(frame, net.input_size)\n        detections = net.predict_on_image(frame)\n        \n        if len(detections) != 0:\n            total_detections.append(detections)\n    \n    return total_detections","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nbefore = time.time()\n\ndetections = detect_faces_full_frame(sample_video)\n\nafter = time.time()\nprint('result {} with time {} seconds'.format(len(detections), round(after-before, 1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1-2 show detected result without tile image","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for frame in frames:\n    frame = cv2.resize(frame, net.input_size)\n\n    detections = net.predict_on_image(frame)\n    \n    # cannot detect\n    if len(detections) == 0:\n        continue\n    else:\n        print('Frame idx {}'.format(i))\n        plot_detections(frame, detections)\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show the tile effect\nfirst_frames, first_resize_info = tile_frames(frames[:1], (128, 128))\n\nfig = plt.figure(figsize=(14, 14))\ncolumns = 3\nrows = 1\nfor i in range(1, columns*rows +1):\n    img = first_frames[i-1]\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def resize_detections(detections, target_size, resize_info):\n    '''get detections from [0, 1] to original frame size (1080, 1080)'''\n    \n    projected = []\n    target_w, target_h = target_size  # (128, 128)\n    scale_w, scale_h = resize_info #  (1080/128, 1080/128)\n\n    # each frame\n    for i in range(len(detections)):\n        detection = detections[i].clone()\n\n        # ymin, xmin, ymax, xmax\n        for k in range(2): #0, 1\n            detection[:, k*2    ] = (detection[:, k*2    ] * target_h) * scale_h\n            detection[:, k*2 + 1] = (detection[:, k*2 + 1] * target_w) * scale_w\n         \n        # keypoints are x,y\n        for k in range(2, 8):\n            detection[:, k*2    ] = (detection[:, k*2    ] * target_w) * scale_w\n            detection[:, k*2 + 1] = (detection[:, k*2 + 1] * target_h) * scale_h\n\n        projected.append(detection)\n\n    return projected ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show resize_detection effects\n\n# update\ntarget_size = (128, 128)\nresize_info = (1080/128, 1080/128)\ntarget_w, target_h = target_size\nscale_w, scale_h = resize_info \n\nfirst_originals, _ = tile_frames(frames[:1], (1080, 1080))\n\nfirst_detections = net.predict_on_batch(first_frames, apply_nms=False)\nfirst_detections = resize_detections(first_detections, target_size, resize_info)\n    \n    \nfig = plt.figure(figsize=(14, 14))\nfor i, detection in enumerate(first_detections):\n    \n    # show the img\n    ax = fig.add_subplot(1, 3, i+1, aspect='equal')\n    ax.imshow(first_originals[i])\n    \n\n    # each face\n    for detect in detection:\n#         ymin = (detect[0] * target_h) * scale_h\n#         xmin = (detect[1] * target_w) * scale_w\n#         ymax = (detect[2] * target_h) * scale_h\n#         xmax = (detect[3] * target_w) * scale_w\n        \n        ymin = detect[0]\n        xmin = detect[1]\n        ymax = detect[2] \n        xmax = detect[3] \n\n        ax.add_patch( patches.Rectangle( (xmin, ymin), xmax-xmin, ymax-ymin, fill=False, edgecolor=\"r\", alpha=detect[16])) \n        \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del first_originals\ndel fig","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def untile_detections(num_frames, frame_size, detections):\n    \"\"\"With N tiles per frame, there also are N times as many detections.\n    This function groups together the detections for a given frame; it is\n    the complement to tile_frames().\n    detections: (192, ?, 17)\n    \"\"\"\n    combined_detections = []\n\n    W, H = frame_size\n    split_size = min(H, W)\n    x_step = (W - split_size) // 2\n    y_step = (H - split_size) // 2\n    num_v = 1\n    num_h = 3 if W > H else 1\n\n    i = 0\n    for f in range(num_frames):\n        detections_for_frame = []\n        y = 0\n        for v in range(num_v):\n            x = 0\n            for h in range(num_h):\n                # Adjust the coordinates based on the split positions.\n                detection = detections[i].clone()\n                if detection.shape[0] > 0:\n                    for k in range(2):\n                        detection[:, k*2    ] += y\n                        detection[:, k*2 + 1] += x\n                    for k in range(2, 8):\n                        detection[:, k*2    ] += x\n                        detection[:, k*2 + 1] += y\n\n                detections_for_frame.append(detection)\n                x += x_step\n                i += 1\n            y += y_step\n\n        combined_detections.append(torch.cat(detections_for_frame))\n\n    return combined_detections","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show untile detections effect\nframe_size = (frames.shape[2], frames.shape[1])\ndetections_one = untile_detections(1, frame_size, first_detections)\ndetections_one = net.nms(detections_one)[0] # only one frame\n\nfig = plt.figure(figsize=(14, 14))\nax = fig.add_subplot(1, 1, 1, aspect='equal')\nax.imshow(frames[0])\n\n# how many faces\nfor i in range(len(detections_one)):\n    ymin = detections_one[i][0]\n    xmin = detections_one[i][1]\n    ymax = detections_one[i][2] \n    xmax = detections_one[i][3] \n\n    ax.add_patch( patches.Rectangle( (xmin, ymin), xmax-xmin, ymax-ymin, fill=False, edgecolor=\"r\", alpha=detect[16])) \n        \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_margin_to_detections(detections, frame_size, margin=0.2):\n    \"\"\"Expands the face bounding box.\n\n    NOTE: The face detections often do not include the forehead, which\n    is why we use twice the margin for ymin.\n\n    Arguments:\n        detections: a PyTorch tensor of shape (num_detections, 17)\n        frame_size: maximum (width, height)\n        margin: a percentage of the bounding box's height\n\n    Returns a PyTorch tensor of shape (num_detections, 17).\n    \"\"\"\n    offset = torch.round(margin * (detections[:, 2] - detections[:, 0])) # (ymax - ymin)*margin\n    detections = detections.clone()\n    detections[:, 0] = torch.clamp(detections[:, 0] - offset*2, min=0)            # ymin\n    detections[:, 1] = torch.clamp(detections[:, 1] - offset, min=0)              # xmin\n    detections[:, 2] = torch.clamp(detections[:, 2] + offset, max=frame_size[1])  # ymax\n    detections[:, 3] = torch.clamp(detections[:, 3] + offset, max=frame_size[0])  # xmax\n    return detections","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show add margin effect\n\ndetections_margin = add_margin_to_detections(detections_one, frame_size=frame_size)\n\n\nfig = plt.figure(figsize=(14, 14))\nax = fig.add_subplot(1, 1, 1, aspect='equal')\nax.imshow(frames[0])\n\n# how many faces\nfor i in range(len(detections_margin)):\n    ymin = detections_margin[i][0]\n    xmin = detections_margin[i][1]\n    ymax = detections_margin[i][2] \n    xmax = detections_margin[i][3] \n\n    ax.add_patch( patches.Rectangle( (xmin, ymin), xmax-xmin, ymax-ymin, fill=False, edgecolor=\"r\", alpha=detect[16])) \n        \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crop_faces(frame, detections):\n    \"\"\"Copies the face region(s) from the given frame into a set\n    of new NumPy arrays.\n\n    Arguments:\n        frame: a NumPy array of shape (H, W, 3)\n        detections: a PyTorch tensor of shape (num_detections, 17)\n\n    Returns a list of NumPy arrays, one for each face crop. If there\n    are no faces detected for this frame, returns an empty list.\n    \"\"\"\n    faces = []\n    for i in range(len(detections)):\n        ymin, xmin, ymax, xmax = detections[i, :4].cpu().numpy().astype(np.int)\n        face = frame[ymin:ymax, xmin:xmax, :]\n        faces.append(face)\n    return faces","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"faces_blaze = crop_faces(frames[0], detections_margin)\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(1, 1, 1, aspect='equal')\nax.imshow(faces_blaze[0])\nprint(faces_blaze[0].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def isotropically_resize_image(img, size, resample=cv2.INTER_AREA):\n    h, w = img.shape[:2]\n    if w > h:\n        h = h * size // w\n        w = size\n    else:\n        w = w * size // h\n        h = size\n\n    resized = cv2.resize(img, (w, h), interpolation=resample)\n    return resized\n\n\ndef make_square_image(img):\n    h, w = img.shape[:2]\n    size = max(h, w)\n    t = 0\n    b = size - h\n    l = 0\n    r = size - w\n    return cv2.copyMakeBorder(img, t, b, l, r, cv2.BORDER_CONSTANT, value=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"face_resized = isotropically_resize_image(faces_blaze[0], 224)\nface_square = make_square_image(face_resized)\nplt.imshow(face_square)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BlazeFaceExtractor:\n    ''' Convenient class'''\n    \n    \n    \n    def __init__(self, weights_path=None, anchors_path=None):\n        ''' load face extractor ''' \n        \n        from blazeface import BlazeFace\n\n        self.net = BlazeFace()\n        \n        \n        if weights_path is None:\n            self.net.load_weights(\"/kaggle/input/blazeface-pytorch/blazeface.pth\")\n        else:\n            self.net.load_weights(weights_path)\n            \n        if anchors_path is None:\n            self.net.load_anchors(\"/kaggle/input/blazeface-pytorch/anchors.npy\")\n        else:\n            self.net.load_anchors(anchors_path)\n            \n\n        # Optionally change the thresholds:\n        self.net.min_score_thresh = 0.75\n        self.net.min_suppression_threshold = 0.3\n\n        \n        \n        \n    def extract_faces(self, video_path, video_read_fn, output_size=224):\n        '''\n        Arguments\n            video_path: full path for one video\n        '''\n        self.video_path = video_path\n        self.video_read_fn = video_read_fn\n        self.output_size = 224\n        \n        # all or specific number of full frames for one video\n        frames = self.video_read_fn(self.video_path)\n        \n        # 1 - tile\n        tiles_frames, tiles_resize_info = tile_frames(frames, self.net.input_size)\n        \n        # 2 - predict\n        detections = net.predict_on_batch(tiles_frames, apply_nms=False)\n        \n        # 3 - resize detection\n        detections_resized = self.resize_detections(detections, self.net.input_size, tiles_resize_info)\n        \n        # 4 - untile\n        frame_size = (frames.shape[2], frames.shape[1])\n        detections_one = self.untile_detections(len(frames), frame_size, detections_resized)\n        detections_one = self.net.nms(detections_one) # \n        \n        # for each frame\n        total_faces = []\n        for i, detection_single in enumerate(detections_one):\n            # no face\n            if len(detection_single) == 0:\n                continue\n                \n                \n            # 5 - add margin\n            detections_margin = self.add_margin_to_detections(detection_single, frame_size=frame_size)\n\n            # 6 - crop\n            faces_crop = self.crop_faces(frames[i], detections_margin)\n            \n            faces_final = []\n            for face in faces_crop:\n                # 7 - resize\n                face_resized = self.isotropically_resize_image(face, self.output_size)\n            \n                # 8 - fill blank\n                face_final = self.make_square_image(face_resized)\n                faces_final.append(face_final)\n            total_faces.append(faces_final)\n        return total_faces\n                \n    \n    \n    def tile_frames(self, frames, target_size):\n        num_frames, H, W, _ = frames.shape\n\n        split_size = min(H, W)\n        x_step = (W - split_size) // 2\n        y_step = (H - split_size) // 2\n        num_v = 1\n        num_h = 3 if W > H else 1\n\n        splits = np.zeros((num_frames * num_v * num_h, target_size[1], target_size[0], 3), dtype=np.uint8)\n\n        i = 0\n        for f in range(num_frames):\n            y = 0\n            for v in range(num_v):\n                x = 0\n                for h in range(num_h):\n                    crop = frames[f, y:y+split_size, x:x+split_size, :]\n                    splits[i] = cv2.resize(crop, target_size, interpolation=cv2.INTER_AREA)\n                    x += x_step\n                    i += 1\n                y += y_step\n\n        resize_info = [split_size / target_size[0], split_size / target_size[1]]\n        return splits, resize_info\n    \n    \n    \n    def resize_detections(self, detections, target_size, resize_info):\n        '''get detections from [0, 1] to original frame size (1080, 1080)'''\n\n        projected = []\n        target_w, target_h = target_size  # (128, 128)\n        scale_w, scale_h = resize_info #  (1080/128, 1080/128)\n\n        # each frame\n        for i in range(len(detections)):\n            detection = detections[i].clone()\n\n            # ymin, xmin, ymax, xmax\n            for k in range(2): #0, 1\n                detection[:, k*2    ] = (detection[:, k*2    ] * target_h) * scale_h\n                detection[:, k*2 + 1] = (detection[:, k*2 + 1] * target_w) * scale_w\n\n            # keypoints are x,y\n            for k in range(2, 8):\n                detection[:, k*2    ] = (detection[:, k*2    ] * target_w) * scale_w\n                detection[:, k*2 + 1] = (detection[:, k*2 + 1] * target_h) * scale_h\n\n            projected.append(detection)\n\n        return projected \n    \n    \n    \n    def untile_detections(self, num_frames, frame_size, detections):\n        \"\"\"With N tiles per frame, there also are N times as many detections.\n        This function groups together the detections for a given frame; it is\n        the complement to tile_frames().\n        detections: (192, ?, 17)\n        \"\"\"\n        combined_detections = []\n\n        W, H = frame_size\n        split_size = min(H, W)\n        x_step = (W - split_size) // 2\n        y_step = (H - split_size) // 2\n        num_v = 1\n        num_h = 3 if W > H else 1\n\n        i = 0\n        for f in range(num_frames):\n            detections_for_frame = []\n            y = 0\n            for v in range(num_v):\n                x = 0\n                for h in range(num_h):\n                    # Adjust the coordinates based on the split positions.\n                    detection = detections[i].clone()\n                    if detection.shape[0] > 0:\n                        for k in range(2):\n                            detection[:, k*2    ] += y\n                            detection[:, k*2 + 1] += x\n                        for k in range(2, 8):\n                            detection[:, k*2    ] += x\n                            detection[:, k*2 + 1] += y\n\n                    detections_for_frame.append(detection)\n                    x += x_step\n                    i += 1\n                y += y_step\n\n            combined_detections.append(torch.cat(detections_for_frame))\n\n        return combined_detections\n        \n        \n        \n    def add_margin_to_detections(self, detections, frame_size, margin=0.2):\n        \"\"\"Expands the face bounding box.\n\n        NOTE: The face detections often do not include the forehead, which\n        is why we use twice the margin for ymin.\n\n        Arguments:\n            detections: a PyTorch tensor of shape (num_detections, 17)\n            frame_size: maximum (width, height)\n            margin: a percentage of the bounding box's height\n\n        Returns a PyTorch tensor of shape (num_detections, 17).\n        \"\"\"\n        offset = torch.round(margin * (detections[:, 2] - detections[:, 0])) # (ymax - ymin)*margin\n        detections = detections.clone()\n        detections[:, 0] = torch.clamp(detections[:, 0] - offset*2, min=0)            # ymin\n        detections[:, 1] = torch.clamp(detections[:, 1] - offset, min=0)              # xmin\n        detections[:, 2] = torch.clamp(detections[:, 2] + offset, max=frame_size[1])  # ymax\n        detections[:, 3] = torch.clamp(detections[:, 3] + offset, max=frame_size[0])  # xmax\n        return detections\n\n\n      \n    def crop_faces(self, frame, detections):\n        \"\"\"Copies the face region(s) from the given frame into a set\n        of new NumPy arrays.\n\n        Arguments:\n            frame: a NumPy array of shape (H, W, 3)\n            detections: a PyTorch tensor of shape (num_detections, 17)\n\n        Returns a list of NumPy arrays, one for each face crop. If there\n        are no faces detected for this frame, returns an empty list.\n        \"\"\"\n        faces = []\n        for i in range(len(detections)):\n            ymin, xmin, ymax, xmax = detections[i, :4].cpu().numpy().astype(np.int)\n            face = frame[ymin:ymax, xmin:xmax, :]\n            faces.append(face)\n        return faces\n        \n        \n        \n    def isotropically_resize_image(self, img, size, resample=cv2.INTER_AREA):\n        h, w = img.shape[:2]\n        if w > h:\n            h = h * size // w\n            w = size\n        else:\n            w = w * size // h\n            h = size\n\n        resized = cv2.resize(img, (w, h), interpolation=resample)\n        return resized\n        \n        \n        \n    def make_square_image(self, img):\n        h, w = img.shape[:2]\n        size = max(h, w)\n        t = 0\n        b = size - h\n        l = 0\n        r = size - w\n        return cv2.copyMakeBorder(img, t, b, l, r, cv2.BORDER_CONSTANT, value=0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2-1 see how many faces are detected with ***tile image***","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"detect number of blaze face","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nbefore = time.time()\n\nblaze = BlazeFaceExtractor()\ntotal_faces = blaze.extract_faces(sample_video, read_video2, IMG_SIZE)\n\nafter = time.time()\nprint('result {} with time {} seconds'.format(len(total_faces), round(after-before, 1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"effect of blaze face, no deformed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(cell, cell, figsize=(8, 8))\nfor i in range(cell):\n    for j in range(cell):\n        ax[i][j].imshow(total_faces[random_idx[i*cell+j]][0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Yolo v2\nsource : https://www.kaggle.com/drjerk/detect-faces-using-yolo","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_mobilenetv2_224_075_detector(path):\n    input_tensor = Input(shape=(224, 224, 3))\n    output_tensor = MobileNetV2(weights=None, include_top=False, input_tensor=input_tensor, alpha=0.75).output\n    output_tensor = ZeroPadding2D()(output_tensor)\n    output_tensor = Conv2D(kernel_size=(3, 3), filters=5)(output_tensor)\n\n    model = Model(inputs=input_tensor, outputs=output_tensor)\n    model.load_weights(path)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mobilenetv2 = load_mobilenetv2_224_075_detector(\"../input/facedetection-mobilenetv2/facedetection-mobilenetv2-size224-alpha0.75.h5\")\n# mobilenetv2.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# dlib","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3) Face Detection - Imgs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"imgs_path = glob.glob('../input/dfdcfaceevaluate/*.jpg')\nimgs = np.array([cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB) for img_path in imgs_path])\nprint(imgs.shape)\n\nfig, ax = plt.subplots(4, 5, figsize=(10, 10))\nfor i in range(4):\n    for j in range(5):\n        ax[i][j].imshow(cv2.cvtColor(cv2.imread(imgs_path[i*4+j]), cv2.COLOR_BGR2RGB))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3-1) MTCNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(4, 5, figsize=(10, 10))\nfor i in range(4):\n    for j in range(5):\n        # show origin face\n        ax[i][j].imshow(imgs[i*4+j])\n        # show detected\n        f = mtcnn_detector.detect_faces(imgs[i*4+j])\n        if len(f) <= 0:\n            continue\n        for n in f:\n            x,y,w,h = n['box']\n            rect = patches.Rectangle((x,y),w,h, linewidth=1,edgecolor='r',facecolor='none')\n            ax[i][j].add_patch(rect)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3-2) facenet_pytorch","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(4, 5, figsize=(10, 10))\nfor i in range(4):\n    for j in range(5):        \n        # show detected\n        f = net_facenet(Image.fromarray(imgs[i*4+j]))\n        if f is None:\n            continue\n        else:\n            ax[i][j].imshow(imgs[i*4+j])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3-3) dlib","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(4, 5, figsize=(10, 10))\nfor i in range(4):\n    for j in range(5):     \n        # show origin face\n        ax[i][j].imshow(imgs[i*4+j])\n        \n        # show detected\n        gray = cv2.cvtColor(imgs[i*4+j], cv2.COLOR_BGR2GRAY)\n        f = dlib_hog_detector(gray, 1)\n        if len(f) > 0:\n            f = f[0]\n            x = f.left()\n            y = f.top()\n            w = f.right() - f.left()\n            h = f.bottom() - f.top()\n           \n            rect = patches.Rectangle((x,y),w,h, linewidth=1,edgecolor='r',facecolor='none')\n            ax[i][j].add_patch(rect)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3-4) Insightface","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(4, 5, figsize=(10, 10))\nfor i in range(4):\n    for j in range(5):     \n        # show origin face\n        ax[i][j].imshow(imgs[i*4+j])\n        \n        # show detected\n        f = insight_detector.get(imgs[i*4+j])\n        if len(f) > 0:\n            x = f[0].bbox[0]\n            y = f[0].bbox[1]\n            w = f[0].bbox[2] - f[0].bbox[0]\n            h = f[0].bbox[3] - f[0].bbox[1]\n            \n            rect = patches.Rectangle((x,y),w,h, linewidth=1,edgecolor='r',facecolor='none')\n            ax[i][j].add_patch(rect)\n        \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3-5) Mobilenet","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(4, 5, figsize=(10, 10))\nfor i in range(4):\n    for j in range(5):     \n        # show origin face\n        ax[i][j].imshow(imgs[i*4+j])\n        \n        # show detected\n        f = mobile_detector(imgs[4*i+j])\n        if len(f) > 0:\n            x = f[0] # left\n            y = f[2] # top\n            w = f[1] - f[0] # right - left\n            h = f[3] - f[2] # bottom - top\n            \n            rect = patches.Rectangle((x,y),w,h, linewidth=1,edgecolor='r',facecolor='none')\n            ax[i][j].add_patch(rect)\n        \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3-6) Blazeface","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(4, 5, figsize=(10, 10))\nfor i in range(4):\n    for j in range(5):     \n        # show origin face\n        ax[i][j].imshow(imgs[i*4+j])\n        \n        # show detected\n        f = blaze_detector(imgs[4*i+j])\n        if len(f) > 0:\n            x = f[0][0] # xmin\n            y = f[0][1] # ymin\n            w = f[0][2] - f[0][0] # right - left\n            h = f[0][3] - f[0][1] # bottom - top\n            \n            rect = patches.Rectangle((x,y),w,h, linewidth=1,edgecolor='r',facecolor='none')\n            ax[i][j].add_patch(rect)\n        \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Retina","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nimport torch\nsys.path.insert(0,\"/kaggle/input/retina-face/Pytorch_Retinaface/\")\nfrom data import cfg_mnet, cfg_re50\nfrom models.retinaface import RetinaFace","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg_re50['image_size'], cfg_mnet['image_size']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model(modelname=\"mobilenet\"):\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    \n    torch.set_grad_enabled(False)\n    cfg = None\n    cfg_mnet['pretrain'] = False\n    cfg_re50['pretrain'] = False\n    \n    if modelname == \"mobilenet\":\n        cfg = cfg_mnet\n        pretrained_path = \"/kaggle/input/retina-face/Pytorch_Retinaface/weights/mobilenet0.25_Final.pth\"\n    else:\n        cfg = cfg_re50\n        pretrained_path = \"/kaggle/input/retina-face/Pytorch_Retinaface/weights/Resnet50_Final.pth\"\n    \n    # net and model\n    net = RetinaFace(cfg=cfg, phase='test')\n    net.load_state_dict(torch.load(pretrained_path, map_location=torch.device('cpu')))\n    net.eval().to(device)\n    return net\n\nretina_detector = get_model()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}