{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport itertools\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold\nfrom sklearn.svm import SVC\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom catboost import CatBoostClassifier\n\nfrom mlxtend.classifier import StackingCVClassifier\nimport shap","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Changelog:\n- v1 - base submission\n- v2 - l1 models - cb, lgbm, rf, extree\n- v3/4 - model understanding (Shap), l1 models hyperparameters tuning\n- v8 (2021-04-30) - many improvements - best l1 model combination, meta-model hyperparameter search","metadata":{}},{"cell_type":"markdown","source":"## If you like it (use it - code or submission file) - **please vote**. This is my first notebook published on Kaggle. I really appreciate any feedback and next improvements, suggestions.\n\nMany thanks to Kaggle Masters:\n- [BIZEN (hiro5299834](https://www.kaggle.com/hiro5299834) - [TPS Apr 2021 pseudo labeling/voting ensemble](https://www.kaggle.com/hiro5299834/tps-apr-2021-pseudo-labeling-voting-ensemble?scriptVersionId=60616606)  \n- [Alexander Ryzhkov](https://www.kaggle.com/alexryzhkov) - [LightAutoML interpretable model - AutoWoE](https://www.kaggle.com/alexryzhkov/lightautoml-interpretable-model-autowoe?scriptVersionId=60651790)\n- [Jeong-Yoon Lee](https://www.kaggle.com/hiro5299834) - [AutoEncoder + Pseudo Label + AutoLGB](https://www.kaggle.com/jeongyoonlee/autoencoder-pseudo-label-autolgb)","metadata":{}},{"cell_type":"markdown","source":"# 1. Description","metadata":{}},{"cell_type":"markdown","source":"This is a very simple example of how to quickly prepare a preliminary analysis of classification models using the ensemble-learning meta-classifier for stacking using cross-validation. \n\nAs described originally in [MLExtend](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/)\n\n> Stacking is an ensemble learning technique to combine multiple classification models via a meta-classifier. The StackingCVClassifier extends the standard stacking algorithm (implemented as StackingClassifier) using cross-validation to prepare the input data for the level-2 classifier.\n\n> In the standard stacking procedure, the first-level classifiers are fit to the same training set that is used prepare the inputs for the second-level classifier, which may lead to overfitting. The StackingCVClassifier, however, uses the concept of cross-validation: the dataset is split into k folds, and in k successive rounds, k-1 folds are used to fit the first level classifier; in each round, the first-level classifiers are then applied to the remaining 1 subset that was not used for model fitting in each iteration. The resulting predictions are then stacked and provided -- as input data -- to the second-level classifier. After the training of the StackingCVClassifier, the first-level classifiers are fit to the entire dataset as illustrated in the figure below.\n\n![](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier_files/stacking_cv_classification_overview.png)","metadata":{}},{"cell_type":"markdown","source":"# 2. Set up script parameters","metadata":{}},{"cell_type":"code","source":"RANDOM_SEED = 2021\nPROBAS = True\nFOLDS = 5\nN_ESTIMATORS = 1000\n\nTARGET = 'Survived'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Load TPS-04 competition data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-apr-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\nsubmission = pd.read_csv('../input/tabular-playground-series-apr-2021/sample_submission.csv')\n\n# Pseudo labels taken from great BIZEN notebook: https://www.kaggle.com/hiro5299834/tps-apr-2021-pseudo-labeling-voting-ensemble\npseudo_labels = pd.read_csv(\"../input/sub-top/sub_top.csv\")\ntest[TARGET] = pseudo_labels[TARGET]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Preprocess data\nI use the way of processing data from [BIZEN](https://www.kaggle.com/hiro5299834/tps-apr-2021-pseudo-labeling-voting-ensemble) notebook (as a benchmark) to compare results.\n\n## 4A. Filling missing data","metadata":{}},{"cell_type":"code","source":"train['FirstName'] = train['Name'].apply(lambda x:x.split(', ')[0])\ntrain['n'] = 1\ngb = train.groupby('FirstName')\ndf_names = gb['n'].sum()\ntrain['SameFirstName'] = train['FirstName'].apply(lambda x:df_names[x])\n\ntest['FirstName'] = test['Name'].apply(lambda x:x.split(', ')[0])\ntest['n'] = 1\ngb = test.groupby('FirstName')\ndf_names = gb['n'].sum()\ntest['SameFirstName'] = test['FirstName'].apply(lambda x:df_names[x])\n\n\nall_df = pd.concat([train, test], axis=0)\n\nall_df['AnyMissing'] = np.where(all_df.isnull().any(axis=1) == True, 1, 0)\n\n\nall_df['FamilySize'] = all_df['SibSp'] + all_df['Parch'] + 1\nall_df['IsAlone'] = np.where(all_df['FamilySize'] <= 1, 1, 0)\n\n\nall_df['Has_Cabin'] = all_df[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\nall_df['Cabin'] = all_df['Cabin'].fillna('X').map(lambda x: x[0].strip())\ncabin_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5,\n             'F': 6, 'G': 7, 'T': 1, 'X': 8}\nall_df['Cabin'] = all_df['Cabin'].str[0].fillna('X').replace(cabin_map)\n\nall_df['Embarked'] = all_df['Embarked'].fillna(\"No\")\nconditions = [\n    (all_df['Embarked']==\"S\"),\n    (all_df['Embarked']==\"Q\"),\n    (all_df['Embarked']==\"C\"),\n    (all_df['Embarked']==\"No\")\n]\nchoices = [0, 1, 2, -1]\nall_df[\"Embarked\"] = np.select(conditions, choices)\nall_df['Embarked'] = all_df['Embarked'].astype(int)\n\n\nall_df['SecondName'] = all_df.Name.str.split(', ', 1, expand=True)[1] # to try\nall_df['IsFirstNameDublicated'] = np.where(all_df.FirstName.duplicated(), 1, 0)\n\n\nall_df['Fare'] = all_df['Fare'].fillna(train['Fare'].median())\n\nconditions = [\n    (all_df['Fare'] <= 7.91),\n    ((all_df['Fare'] > 7.91) & (all_df['Fare'] <= 14.454)),\n    ((all_df['Fare'] > 14.454) & (all_df['Fare'] <= 31)),\n    (all_df['Fare'] > 31)\n]\n\nchoices = [0, 1, 2, 3]\nall_df[\"Fare\"] = np.select(conditions, choices)\nall_df['Fare'] = all_df['Fare'].astype(int)\n\n\nall_df['Ticket'] = all_df.Ticket.str.replace('\\.','', regex=True).\\\n                    str.replace('(\\d+)', '', regex=True).\\\n                    str.replace(' ', '', regex=True).\\\n                    replace(r'^\\s*$', 'X', regex=True).\\\n                    fillna('X')\n\n \nconditions = [\n    ((all_df.Sex==\"female\")&(all_df.Pclass==1)&(all_df.Age.isnull())),\n    ((all_df.Sex==\"male\")&(all_df.Pclass==1)&(all_df.Age.isnull())),\n    ((all_df.Sex==\"female\")&(all_df.Pclass==2)&(all_df.Age.isnull())),\n    ((all_df.Sex==\"male\")&(all_df.Pclass==2)&(all_df.Age.isnull())),\n    ((all_df.Sex==\"female\")&(all_df.Pclass==3)&(all_df.Age.isnull())),\n    ((all_df.Sex==\"male\")&(all_df.Pclass==3)&(all_df.Age.isnull()))]\n\n\nchoices = all_df[['Age', 'Pclass', 'Sex']].\\\n            dropna().\\\n            groupby(['Pclass', 'Sex']).\\\n            mean()['Age']\n\nall_df[\"Age\"] = np.select(conditions, choices)\n\nconditions = [\n    (all_df['Age'].le(16)),\n    (all_df['Age'].gt(16) & all_df['Age'].le(32)),\n    (all_df['Age'].gt(32) & all_df['Age'].le(48)),\n    (all_df['Age'].gt(48) & all_df['Age'].le(64)),\n    (all_df['Age'].gt(64))\n]\nchoices = [0, 1, 2, 3, 4]\n\nall_df[\"Age\"] = np.select(conditions, choices)\nall_df['Sex'] = np.where(all_df['Sex']=='male', 1, 0)\nall_df = all_df.drop(['Name', 'n'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4B. Encoding data","metadata":{}},{"cell_type":"code","source":"label_cols = ['Ticket']\n\ndef label_encoder(c):\n    le = LabelEncoder()\n    return le.fit_transform(c)\n\nscaler = StandardScaler()\nall_df.Ticket = all_df[['Ticket']].apply(label_encoder)\n\nfeatures_selected = ['Pclass', 'Sex', 'Age','Embarked','Parch','SibSp','Fare','Cabin','Ticket','SameFirstName', 'Survived']\nall_df = all_df[features_selected]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First train data rows ... to look into ...\n\nall_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Last test data rows \nall_df.tail(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4C. Create train and test datasets","metadata":{}},{"cell_type":"code","source":"X = all_df.drop([TARGET], axis = 1)\ny = all_df[TARGET]\n\nprint (f'X:{X.shape} y: {y.shape} \\n')\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = RANDOM_SEED)\nprint (f'X_train:{X_train.shape} y_train: {y_train.shape}')\nprint (f'X_test:{X_test.shape} y_test: {y_test.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = all_df[len(train):].drop([TARGET], axis = 1)\nprint (f'test:{test.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Declare list of classifiers (level 1) for testing\nI tested only few - feel free to make experiments. Experiments are everything ...","metadata":{}},{"cell_type":"markdown","source":"## 5A. Hyperparameters","metadata":{}},{"cell_type":"code","source":"lgb_params = {\n    'metric': 'binary_logloss',\n    'n_estimators': 10000,\n    'objective': 'binary',\n    'learning_rate': 0.02,\n    'min_child_samples': 150,\n    'reg_alpha': 3e-5,\n    'reg_lambda': 9e-2,\n    'num_leaves': 20,\n    'max_depth': 16,\n    'colsample_bytree': 0.8,\n    'subsample': 0.8,\n    'subsample_freq': 2,\n    'max_bin': 240,\n    'device': 'gpu'\n}\n\ncb_params = {\n    'max_depth':6,\n    'max_ctr_complexity': 5,\n    'num_trees': 50000,\n    'od_wait': 500,\n    'od_type':'Iter', \n    'learning_rate': 0.04,\n    'min_data_in_leaf': 3,\n    'task_type': 'GPU'\n}\n\n\nrf_params = {\n    'max_depth': 15,\n    'min_samples_leaf': 8,\n    'random_state': RANDOM_SEED\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5B. Classifiers","metadata":{}},{"cell_type":"code","source":"# Here you can declare list of classifiers for prototyping purposes\n\n# I do not make any hyperparameter optimization - just taken as they are - here is room for improvement\n# You can use hyperparameters definition from previous section eg. cl6 = LGBMClassifier(**lgb_params)\n\ncl1 = KNeighborsClassifier(n_neighbors = 1)\ncl2 = RandomForestClassifier(**rf_params)\ncl3 = GaussianNB()\ncl4 = DecisionTreeClassifier(max_depth = 5)\ncl5 = CatBoostClassifier(task_type = 'GPU', verbose = None, logging_level = 'Silent')\ncl6 = LGBMClassifier(device = 'gpu')\n\n\n# I used some hyperparameter search (ExtraTrees - Genetic search)\ncl7 = ExtraTreesClassifier(bootstrap=False, criterion='entropy', max_features=0.55, min_samples_leaf=8, min_samples_split=4, n_estimators=100) # Optimized using TPOT\ncl8 = MLPClassifier(activation = \"relu\", alpha = 0.1, hidden_layer_sizes = (10,10,10),\n                            learning_rate = \"constant\", max_iter = 2000, random_state = RANDOM_SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Declare classifiers (leval 1 and level 2)","metadata":{}},{"cell_type":"markdown","source":"## 6A. level 1 classifiers","metadata":{}},{"cell_type":"code","source":"# Use classifiers from the list and build stacking cross validated classifier with meta-classifier on top (Logistic Regression, SVC ...)\n\n# Classifiers for experiment\n#classifiers = {\n#    \"KNN\": cl1,\n#    \"RandomForest\": cl2,\n#    \"GaussianNB\": cl3,\n#    \"DecisionTree\": cl4,\n#    \"CatBoost\": cl5,\n#    \"LGBM\": cl6,\n#    \"ExtraTrees\": cl7,\n#    \"MLP\": cl8\n#}\n\n# Read next sections - I took only 5 most promising classifier to speed up learning process \nclassifiers = {\n    \"RandomForest\": cl2,\n    \"DecisionTree\": cl4,\n    \"CatBoost\": cl5,\n    \"LGBM\": cl6,\n    \"ExtraTrees\": cl7,\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6B. level 2 classifier","metadata":{}},{"cell_type":"code","source":"# For this test I use Logistic Regression as a meta-classifier but you can ... take end experiment something else ...\nmlr = LogisticRegression()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Train classifiers","metadata":{}},{"cell_type":"markdown","source":"## 7A. level-1 classifiers","metadata":{}},{"cell_type":"code","source":"models_scores_results, models_names = list(), list() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This step could take some time .... it depends on classifiers you use .... So make a coffe or meditate ... \n\nprint(\">>>> Training started <<<<\")\nfor key in classifiers:\n    classifier = classifiers[key]\n    scores = model_selection.cross_val_score(classifier, X_train, y_train, cv = FOLDS, scoring='accuracy')\n    models_scores_results.append(scores)\n    models_names.append(key)\n    print(\"[%s] - accuracy: %0.5f \" % (key, scores.mean()))\n    classifier.fit(X_train, y_train)\n    \n    # Save classifier for prediction \n    classifiers[key] = classifier\n    \n#Tested during experiments --- (all classifiers)\n#[KNN] - accuracy: 0.70558 \n#[RandomForest] - accuracy: 0.88768 \n#[GaussianNB] - accuracy: 0.79445 \n#[DecisionTree] - accuracy: 0.88113 \n#[CatBoost] - accuracy: 0.88763 \n#[LGBM] - accuracy: 0.88724 \n#[ExtraTrees] - accuracy: 0.88718 \n#[MLP] - accuracy: 0.84027","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7B. l2 - meta-classifier ","metadata":{}},{"cell_type":"markdown","source":"### A. Show model scores","metadata":{}},{"cell_type":"code","source":"plt.boxplot(models_scores_results, labels=models_names, showmeans=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### B. Searching for the best l1 model combination","metadata":{}},{"cell_type":"code","source":"# I take only TOP5 classifiers (most promising) to check build and check meta (names of classifiers taken from classifiers dictionary)\ntaken_classifiers = [\"RandomForest\", \"DecisionTree\", \"CatBoost\", \"LGBM\", \"ExtraTrees\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# This function searches best stacking configuration\ndef best_stacking_search():\n    cls_list = []\n    best_auc = -1\n    i=0\n\n    best_cls_experiment = list()\n\n    print(\">>>> Training started <<<<\")\n\n    for cls_comb in range(2, len(taken_classifiers)+1):\n        for subset in itertools.combinations(taken_classifiers, cls_comb):\n            cls_list.append(subset)\n\n    print(f\"Total number of model combination: {len(cls_list)}\")\n\n\n    for cls_exp in cls_list:\n        cls_labels = list(cls_exp)\n\n        classifier_exp = []\n        for ii in range(len(cls_labels)):\n            label = taken_classifiers[ii]\n            classifier = classifiers[label]\n            classifier_exp.append(classifier)\n\n\n        sclf = StackingCVClassifier(classifiers = classifier_exp,\n                                    shuffle = False,\n                                    use_probas = True,\n                                    cv = FOLDS,\n                                    meta_classifier = mlr,\n                                    n_jobs = -1)\n\n        scores = model_selection.cross_val_score(sclf, X_train, y_train, cv = FOLDS, scoring='accuracy')\n\n        if scores.mean() > best_auc:\n            best_cls_experiment = list(cls_exp)\n        i += 1\n        print(f\"  {i} - Stacked combination - Acc {cls_exp}: {scores.mean():.5f}\")\n        \n    return best_cls_experiment","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# This step is time consuming - if you want experiment ... try this but ... you wil have to wait\n#>>>> Training started <<<<\n#Total number of model combination: 26\n#  1 - Stacked combination - Acc ('RandomForest', 'DecisionTree'): 0.88758\n#  2 - Stacked combination - Acc ('RandomForest', 'CatBoost'): 0.88768\n#  3 - Stacked combination - Acc ('RandomForest', 'ExtraTrees'): 0.88759\n#  4 - Stacked combination - Acc ('RandomForest', 'LGBM'): 0.88767\n#  5 - Stacked combination - Acc ('DecisionTree', 'CatBoost'): 0.88752\n#  6 - Stacked combination - Acc ('DecisionTree', 'ExtraTrees'): 0.88763\n#  7 - Stacked combination - Acc ('DecisionTree', 'LGBM'): 0.88760\n#  8 - Stacked combination - Acc ('CatBoost', 'ExtraTrees'): 0.88758\n#  9 - Stacked combination - Acc ('CatBoost', 'LGBM'): 0.88753\n#  10 - Stacked combination - Acc ('ExtraTrees', 'LGBM'): 0.88759\n#  11 - Stacked combination - Acc ('RandomForest', 'DecisionTree', 'CatBoost'): 0.88767\n#  12 - Stacked combination - Acc ('RandomForest', 'DecisionTree', 'ExtraTrees'): 0.88766\n#  13 - Stacked combination - Acc ('RandomForest', 'DecisionTree', 'LGBM'): 0.88760\n#  14 - Stacked combination - Acc ('RandomForest', 'CatBoost', 'ExtraTrees'): 0.88763\n#  15 - Stacked combination - Acc ('RandomForest', 'CatBoost', 'LGBM'): 0.88763\n#  16 - Stacked combination - Acc ('RandomForest', 'ExtraTrees', 'LGBM'): 0.88749\n#  17 - Stacked combination - Acc ('DecisionTree', 'CatBoost', 'ExtraTrees'): 0.88757\n#  18 - Stacked combination - Acc ('DecisionTree', 'CatBoost', 'LGBM'): 0.88766\n#  19 - Stacked combination - Acc ('DecisionTree', 'ExtraTrees', 'LGBM'): 0.88753\n#  20 - Stacked combination - Acc ('CatBoost', 'ExtraTrees', 'LGBM'): 0.88759\n#  21 - Stacked combination - Acc ('RandomForest', 'DecisionTree', 'CatBoost', 'ExtraTrees'): 0.88767\n#  22 - Stacked combination - Acc ('RandomForest', 'DecisionTree', 'CatBoost', 'LGBM'): 0.88759\n#  23 - Stacked combination - Acc ('RandomForest', 'DecisionTree', 'ExtraTrees', 'LGBM'): 0.88754\n#  24 - Stacked combination - Acc ('RandomForest', 'CatBoost', 'ExtraTrees', 'LGBM'): 0.88766\n#  25 - Stacked combination - Acc ('DecisionTree', 'CatBoost', 'ExtraTrees', 'LGBM'): 0.88756\n#  26 - Stacked combination - Acc ('RandomForest', 'DecisionTree', 'CatBoost', 'ExtraTrees', 'LGBM'): 0.88754\n\n# ------------- CODE ---------------\n# SCENARIO 1. Use this line if you want to search for best combination\n#best_cls_experiment = best_stacking_search()\n#SCENARIO 1\n\n# SCENARIO 2. else use the best found during my experimentation ...\nbest_cls_experiment = ['RandomForest', 'DecisionTree', 'ExtraTrees', 'LGBM'] \n# SCENARIO 2.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### C. Train the BASE meta-model","metadata":{}},{"cell_type":"code","source":"print(f'The best models configuration: {best_cls_experiment}')\n\nclassifier_exp = []\nfor label in best_cls_experiment:\n        classifier = classifiers[label]\n        classifier_exp.append(classifier)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check (for sure ...) if there is good classifier configuration (I made mistake previously so ... got poor results ...)\nclassifier_exp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# \"Ensemble learning works best when the base models are not correlated. \n# For instance, you can train different models such as linear models, decision trees, and neural nets on different datasets or features. \n# The less correlated the base models, the better.\" (https://neptune.ai/blog/ensemble-learning-guide)\n\n# ------------- CODE ---------------\n# # SCENARIO 1 This is for manual experimentation\n#scl = StackingCVClassifier(classifiers= [cl2, cl5, cl6, cl7], #[cl1, cl2, cl3, cl4, cl5, cl6, cl7, cl8, cl9]\n#                            meta_classifier = mlr, # use meta-classifier\n#                            use_probas = PROBAS,   # use_probas = True/False\n#                            random_state = RANDOM_SEED)\n\n\n# # SCENARIO 2. This line takes best experiment from stacking combination\n# I know this is redundand (I could use trained models from provious step) but I make this step for clarity\n\nscl = StackingCVClassifier(classifiers= classifier_exp,\n                            meta_classifier = mlr, # use meta-classifier\n                            use_probas = PROBAS,   # use_probas = True/False\n                            random_state = RANDOM_SEED)\n\nscores = model_selection.cross_val_score(scl, X_train, y_train, cv = FOLDS, scoring='accuracy')\nmodels_scores_results.append(scores)\nmodels_names.append('scl')\nprint(\"Meta model (slc) - accuracy: %0.5f \" % (scores.mean()))\nscl.fit(X_train, y_train)\n\ntop_meta_model = scl\nbase_acc = scores.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### D. Find hyperparameters for meta-model","metadata":{}},{"cell_type":"markdown","source":"#### D1. Find keys for tuning","metadata":{}},{"cell_type":"code","source":"# meta prefix for l2 classifier but feel free to optimize l1 classifier as well\nscl.get_params().keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### D2. Find the best parameter (SearchGridCV)\nI just wanted to show how to optimize so taken only one C - Inverse of regularization strength.","metadata":{}},{"cell_type":"code","source":"def meta_best_params_search():\n\n    scl_params = {'meta_classifier__C': [0.001, 0.01, 0.1, 1, 10]}\n\n    print(\">>>> Searching for best parameters started <<<<\")\n\n    grid = GridSearchCV(estimator=scl, \n                        param_grid= scl_params, \n                        cv=5,\n                        refit=True)\n    grid.fit(X_train, y_train)\n\n    cv_keys = ('mean_test_score', 'std_test_score', 'params')\n\n    for r, _ in enumerate(grid.cv_results_['mean_test_score']):\n        print(\"%0.3f +/- %0.2f %r\" % (grid.cv_results_[cv_keys[0]][r], grid.cv_results_[cv_keys[1]][r] / 2.0, grid.cv_results_[cv_keys[2]][r]))\n\n    print('Best parameters: %s' % grid.best_params_)\n    print('Accuracy: %.5f' % grid.best_score_)\n    return grid, grid.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run this if you want to search for optimum hyperparameters - in this example I am searching only for meta-model but you can tune other hyperparameters\n# but for ['CatBoost', 'ExtraTrees', 'LGBM'] I found C =  was the best\n#>>>> Searching for best parameters started <<<<\n#0.886 +/- 0.00 {'meta_classifier__C': 0.001}\n#0.886 +/- 0.00 {'meta_classifier__C': 0.01}\n#0.886 +/- 0.00 {'meta_classifier__C': 0.1}\n#0.886 +/- 0.00 {'meta_classifier__C': 1}\n#0.886 +/- 0.00 {'meta_classifier__C': 10}\n#Best parameters: {'meta_classifier__C': 0.1}\n\n# ------------- CODE ---------------\n#hyper_meta_model, hyper_acc = meta_best_params_search()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### E. Take the BEST (optimized) meta-classifier","metadata":{}},{"cell_type":"code","source":"#. SCENARIO 1. Use this if you were looking for best params (D2 section) - previous section\n#if hyper_acc > base_acc:\n#    top_meta_model = hyper_meta_model\n#. SCENARIO 1.\n    \n    \n# SCENARIO 2. or this if you want to avoid searching for hyperparameters and use the best one found during my research\n# for \n# a. ['CatBoost', 'ExtraTrees', 'LGBM'] C = 1\n# b. but sometimes .... default parameters are better :) You have to conduct experiments ... :)\nscl = StackingCVClassifier(classifiers= classifier_exp,\n                            meta_classifier = LogisticRegression(C = 0.1), # use meta-classifier\n                            use_probas = PROBAS,   # use_probas = True/False\n                            random_state = RANDOM_SEED)\n\nscores = model_selection.cross_val_score(scl, X_train, y_train, cv = FOLDS, scoring='accuracy')\nprint(\"Meta model (slc) - accuracy: %0.5f \" % (scores.mean()))\nscl.fit(X_train, y_train)\ntop_meta_model = scl\n# SCENARIO 2.\n\nclassifiers[\"scl\"] = top_meta_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Evaluate models (AUC)","metadata":{}},{"cell_type":"code","source":"# Let's see how the models work ... We will operate on probas ...\n\npreds = pd.DataFrame()\n\nfor key in classifiers:\n    y_pred = classifiers[key].predict_proba(X_test)[:,1]\n    preds[f\"{key}\"] = y_pred\n    auc = metrics.roc_auc_score(y_test, y_pred)\n    print(f\"{key} -> AUC: {auc:.5f}\")\n\npreds[TARGET] = pd.DataFrame(y_test).reset_index(drop=True)\n\n# Tested during experiments --- (all classifiers)\n#KNN -> AUC: 0.68637\n#RandomForest -> AUC: 0.93863\n#GaussianNB -> AUC: 0.87196\n#DecisionTree -> AUC: 0.92070\n#CatBoost -> AUC: 0.93794\n#LGBM -> AUC: 0.93917\n#ExtraTrees -> AUC: 0.93890\n#MLP -> AUC: 0.91257\n#scl -> AUC: 0.93899","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pandas ... show me preds and model probas please ...\n\npreds.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. Understand models","metadata":{}},{"cell_type":"markdown","source":"## 9A. Plot prediction distribution","metadata":{}},{"cell_type":"code","source":"# Seaborn will now show us how the models predict survival ...\n\ndef plot_model_AUC(cls_models):\n    NUM_CLASS = len(cls_models)\n    sns.set(font_scale = 1)\n    sns.set_style({\"axes.facecolor\": \"1.0\", \"axes.edgecolor\": \"0.85\", \"grid.color\": \"0.85\",\n                   \"grid.linestyle\": \"-\", 'axes.labelcolor': '0.4', \"xtick.color\": \"0.4\",\n                   'ytick.color': '0.4'})\n\n    f, ax = plt.subplots(figsize=(20, 5), nrows=1, ncols = NUM_CLASS)\n\n    for key, counter in zip(cls_models, range(NUM_CLASS)):\n\n        y_pred = preds[key]\n\n        auc = metrics.roc_auc_score(y_test, y_pred)\n        textstr = f\"AUC: {auc:.3f}\"\n\n\n        false_pred = preds[preds[TARGET] == 0]\n        sns.distplot(false_pred[key], hist=True, kde=True, \n                     bins=int(50), color = 'red', \n                     hist_kws={'edgecolor':'black'}, ax = ax[counter])\n\n\n        true_pred = preds[preds[TARGET] == 1]\n        sns.distplot(true_pred[key], hist=True, kde=True, \n                     bins=int(50), color = 'green', \n                     hist_kws={'edgecolor':'black'}, ax = ax[counter])\n\n\n        props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n\n        ax[counter].text(0.05, 0.95, textstr, transform=ax[counter].transAxes, fontsize=14,\n                        verticalalignment = \"top\", bbox=props)\n\n        ax[counter].set_title(f\"{key}\")\n        ax[counter].set_xlim(0,1)\n        ax[counter].set_xlabel(\"Probability\")\n\n    plt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model_AUC(classifiers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9B. Plot model explanatory (using Shap)","metadata":{}},{"cell_type":"code","source":"shap.initjs()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets explain how the models see the competition world ... What features are important .... What values drive the model for survival ...\n# I made it only for one fast model (LGBM). Feel free to understand other models ...\n\n\nexplainer = shap.TreeExplainer(classifiers[\"LGBM\"])\nshap_values = explainer.shap_values(X_train)\nshap.summary_plot(shap_values[1], X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. Final prediction (ensemble-learning with meta-classifier)","metadata":{}},{"cell_type":"code","source":"# And we approaching to the final stage ... prediction ... \n\ntest_preds = classifiers['scl'].predict_proba(test)[:,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Grandmaster tip -> Alexander Ryzhkov\n# They way of finding \"the best\" from \"the best\" :) that is, secret codes for the game ...\n\nthreshold = pd.Series(test_preds).sort_values(ascending = False).head(34911).values[-1]\nprint(f\"Current threshold is: {threshold}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# threshold from previous section was too hard for me so I decided to check only and use more reasonable value\n#threshold = 0.40\nsubmission['submit_1'] = (test_preds > threshold).astype(int)\nsubmission['submit_1'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 11. Ensemble prediction\nThis is only for competition purpose (to boost score). ","metadata":{}},{"cell_type":"code","source":"# Next Grandmaster tip -> BIZEN\n# Hacking the system :) How about mixing it with another submissions\n\nsubmission['submit_2'] = pd.read_csv(\"../input/tps04preds/dae.csv\")[TARGET]\nsubmission['submit_3'] = pseudo_labels[TARGET]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[[col for col in submission.columns if col.startswith('submit_')]].sum(axis = 1).value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[TARGET] = (submission[[col for col in submission.columns if col.startswith('submit_')]].sum(axis=1) >= 2).astype(int)\nsubmission[TARGET].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 12. Submit prediction","metadata":{}},{"cell_type":"code","source":"# and now .... hold your breath and upload the results on the server and wait for the results ... TOP? How much?\n\nsubmission[['PassengerId', TARGET]].to_csv(\"REMEK-TPS04-FINAL005.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 13. What's next? Todo's\n- [x] Stacking different combinations of classifiers (GridSearchCV)\n- [x] Hyperparameters tuning for L1 models (Optuna / GridSearchCV)\n- [x] Models understanding (Shap)\n- [x] Hyperparameters for meta-model ()\n- [x] Hacking the system :)","metadata":{}},{"cell_type":"markdown","source":"## If you like it - **please vote**. This is my first notebook published on Kaggle. I really appreciate any feedback and proposal for improvements, suggestions.","metadata":{}}]}