{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **UPDATE on 6/12/2021**\n\nToday's `Kaggler` v.9.13 release includes transfer learning between `DAE`/`SDAE`. Now you can initialize `DAE`/`SDAE` with a pretrained model as follows:\n\n```python\n# train supervised DAE only with trianing data\nsdae = SDAE(cat_cols=cat_cols, num_cols=num_cols, encoding_dim=encoding_dim, random_state=RANDOM_SEED)\n_ = sdae.fit_transform(trn[feature_cols], trn[TARGET_COL])\n\n# initialize unsupervied DAE and train it with both training and test data\ndae = DAE(cat_cols=cat_cols, num_cols=num_cols, encoding_dim=encoding_dim, random_state=RANDOM_SEED,\n          pretrained_model=sdae, freeze_embedding=True)\n_ = dae.fit_transform(df[feature_cols])\n\n# initialize another supervised DAE and train it with training data\nsdae2 = SDAE(cat_cols=cat_cols, num_cols=num_cols, encoding_dim=encoding_dim, random_state=RANDOM_SEED,\n             pretrained_model=dae, freeze_embedding=False)\n_ = sdae2.fit_transform(trn[feature_cols], trn[TARGET_COL])\n```\n\nYou can check the example of using transfer learning between DAE and SDAE in the notebook below:\n* [TPS 6 Supervised DAE + Keras (GPU)](https://www.kaggle.com/jeongyoonlee/tps-6-supervised-dae-keras-gpu)\n\nEnjoy~!\n\n## **UPDATE on 6/8/2021**\n\nToday's `Kaggler` v.9.10 release includes further improvements in `DAE`/`SDAE` as follows:\n\n* add an option, `n_encoder` to add more than 1 encoder in `DAELayer`\n* add an option, `validation_data` to add validation data in `DAE`/`SDAE`\n* make label-encoding optional in `DAE`/`SDAE`\n\nEspecially, the first one is useful. previous winning solutions using DAE usually combines multiple (3 ~ 5) encodings together to improve feature representation.\n\nThis is different from `n_layer` in `DAE`/`SDAE`, which determines the number of the encoder/decoder pairs to be stacked. e.g. `DAE` with `n_layer=3` and `n_encoder=2` will have three encoder/decoder pairs, and in each pair, there will be two encoders (dense layers).\n\nHope you find it useful.\n\n\n## **UPDATE on 6/3/2021**\n\nI added the supervised version of DAE, `SDAE` to `Kaggler` in today's v0.9.8 release. At Kaggle, DAE is mostly used as a unsupervised feature extraction method. However, it's possible to train DAE in a supervised manner with a target variable.\n\nTo transform features with `SDAE`, you can do as follows:\n\n```python\nsdae = SDAE(cat_cols=feature_cols, encoding_dim=encoding_dim, n_layer=1, noise_std=.001, random_state=seed)\nsdae.fit(trn[feature_cols], y)\nX = sdae.transform(df[feature_cols])\n```\n\nYou can find an example from this notebook, [TPS 6 Supervised DAE + Keras (GPU)](https://www.kaggle.com/jeongyoonlee/tps-6-supervised-dae-keras-gpu).\n\nHope it helps.\n\n## **UPDATE on 5/1/2021**\n\nToday, [`Kaggler`](https://github.com/jeongyoonlee/Kaggler) v0.9.4 is released with additional features for DAE as follows:\n* In addition to the swap noise (`swap_prob`), the Gaussian noise (`noise_std`) and zero masking (`mask_prob`) have been added to DAE to overcome overfitting.\n* Stacked DAE is available through the `n_layer` input argument (see Figure 3. in [Vincent et al. (2010), \"Stacked Denoising Autoencoders\"](https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf) for reference).\n\nFor example, to build a stacked DAE with 3 pairs of encoder/decoder and all three types of noises, you can do:\n```python\nfrom kaggler.preprocessing import DAE\n\ndae = DAE(cat_cols=cat_cols, num_cols=num_cols, n_layer=3, noise_std=.05, swap_prob=.2, masking_prob=.1)\nX = dae.fit_transform(pd.concat([trn, tst], axis=0))\n```\n\nIf you're using previous versions, please upgrade `Kaggler` using `pip install -U kaggler`.\n\n---\n\nToday I released a new version (v0.9.0) of the `Kaggler` package with Denoising AutoEncoder (DAE) with the swap noise. \n\nNow you can train a DAE with only 2 lines of code as follows:\n\n```python\ndae = DAE(cat_cols=cat_cols, num_cols=num_cols, encoding_dim=encoding_dim)\nX = dae.fit_transform(df[feature_cols])\n```\n\nIn addition to the new DAE feature encoder, `Kaggler` supports many of feature transformations used in Kaggle including:\n* `TargetEncoder`: with smoothing and cross-validation to avoid overfitting\n* `FrequencyEncoder`\n* `LabelEncoder`: that imputes missing values and groups rare categories\n* `OneHotEncoder`: that imputes missing values and groups rare categories\n* `EmbeddingEncoder`: that transforms categorical features into embeddings\n* `QuantileEncoder`: that transforms numerical features into quantiles\n\nIn the notebook below, I will show how to use `Kaggler`'s `LabelEncoder`, `TargetEncoder`, and `DAE` for feature engineering, then use `Kaggler`'s `AutoLGB` to do feature selection and hyperparameter optimization.","metadata":{}},{"cell_type":"markdown","source":"# Part 1: Data Loading & Feature Engineering","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score, confusion_matrix\nimport warnings","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install kaggler","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import kaggler\nfrom kaggler.model import AutoLGB\nfrom kaggler.preprocessing import DAE, TargetEncoder, LabelEncoder\n\nprint(f'Kaggler: {kaggler.__version__}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.simplefilter('ignore')\npd.set_option('max_columns', 100)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_name = 'dae_te'\nalgo_name = 'lgb'\nmodel_name = f'{algo_name}_{feature_name}'\n\ndata_dir = Path('/kaggle/input/tabular-playground-series-apr-2021/')\ntrn_file = data_dir / 'train.csv'\ntst_file = data_dir / 'test.csv'\nsample_file = data_dir / 'sample_submission.csv'\npseudo_label_file = '../input/tps-apr-2021-pseudo-label-dae/REMEK-TPS04-FINAL005.csv'\n\nfeature_file = f'{feature_name}.csv'\npredict_val_file = f'{model_name}.val.txt'\npredict_tst_file = f'{model_name}.tst.txt'\nsubmission_file = f'{model_name}.sub.csv'\n\ntarget_col = 'Survived'\nid_col = 'PassengerId'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_fold = 5\nseed = 42\nencoding_dim = 64","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn = pd.read_csv(trn_file, index_col=id_col)\ntst = pd.read_csv(tst_file, index_col=id_col)\nsub = pd.read_csv(sample_file, index_col=id_col)\npseudo_label = pd.read_csv(pseudo_label_file, index_col=id_col)\nprint(trn.shape, tst.shape, sub.shape, pseudo_label.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tst[target_col] = pseudo_label[target_col]\nn_trn = trn.shape[0]\ndf = pd.concat([trn, tst], axis=0)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature engineering code from https://www.kaggle.com/udbhavpangotra/tps-apr21-eda-model\n\ndf['Embarked'] = df['Embarked'].fillna('No')\ndf['Cabin'] = df['Cabin'].fillna('_')\ndf['CabinType'] = df['Cabin'].apply(lambda x:x[0])\ndf.Ticket = df.Ticket.map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'X')\n\ndf['Age'].fillna(round(df['Age'].median()), inplace=True,)\ndf['Age'] = df['Age'].apply(round).astype(int)\n\n# Fare, fillna with mean value\nfare_map = df[['Fare', 'Pclass']].dropna().groupby('Pclass').median().to_dict()\ndf['Fare'] = df['Fare'].fillna(df['Pclass'].map(fare_map['Fare']))\n\ndf['FirstName'] = df['Name'].str.split(', ').str[0]\ndf['SecondName'] = df['Name'].str.split(', ').str[1]\n\ndf['n'] = 1\n\ngb = df.groupby('FirstName')\ndf_names = gb['n'].sum()\ndf['SameFirstName'] = df['FirstName'].apply(lambda x:df_names[x]).fillna(1)\n\ngb = df.groupby('SecondName')\ndf_names = gb['n'].sum()\ndf['SameSecondName'] = df['SecondName'].apply(lambda x:df_names[x]).fillna(1)\n\ndf['Sex'] = (df['Sex'] == 'male').astype(int)\n\ndf['FamilySize'] = df.SibSp + df.Parch + 1\n\nfeature_cols = ['Pclass', 'Age','Embarked','Parch','SibSp','Fare','CabinType','Ticket','SameFirstName', 'SameSecondName', 'Sex',\n                'FamilySize', 'FirstName', 'SecondName']\ncat_cols = ['Pclass','Embarked','CabinType','Ticket', 'FirstName', 'SecondName']\nnum_cols = [x for x in feature_cols if x not in cat_cols]\nprint(len(feature_cols), len(cat_cols), len(num_cols))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in ['SameFirstName', 'SameSecondName', 'Fare', 'FamilySize', 'Parch', 'SibSp']:\n    df[col] = np.log2(1 + df[col])\n    \nscaler = StandardScaler()\ndf[num_cols] = scaler.fit_transform(df[num_cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Label encoding with rare category grouping and missing value imputation","metadata":{}},{"cell_type":"code","source":"lbe = LabelEncoder(min_obs=50)\ndf[cat_cols] = lbe.fit_transform(df[cat_cols]).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target encoding with smoothing and 5-fold cross-validation","metadata":{}},{"cell_type":"code","source":"cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\nte = TargetEncoder(cv=cv)\ndf_te = te.fit_transform(df[cat_cols], df[target_col])\ndf_te.columns = [f'te_{col}' for col in cat_cols]\ndf_te.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DAE","metadata":{}},{"cell_type":"code","source":"dae = DAE(cat_cols=cat_cols, num_cols=num_cols, encoding_dim=encoding_dim)\nX = dae.fit_transform(df[feature_cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_dae = pd.DataFrame(X, columns=[f'dae_{i}' for i in range(encoding_dim)])\nprint(df_dae.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 2: Model Training","metadata":{}},{"cell_type":"markdown","source":"### AutoLGB for Feature Selection and Hyperparameter Optimization","metadata":{}},{"cell_type":"code","source":"X = pd.concat([df[feature_cols], df_te, df_dae], axis=1)\ny = df[target_col]\nX_tst = X.iloc[n_trn:]\n\np = np.zeros_like(y, dtype=float)\np_tst = np.zeros((tst.shape[0],))\nprint(f'Training a stacking ensemble LightGBM model:')\nfor i, (i_trn, i_val) in enumerate(cv.split(X, y)):\n    if i == 0:\n        clf = AutoLGB(objective='binary', metric='auc', sample_size=len(i_trn), random_state=seed)\n        clf.tune(X.iloc[i_trn], y[i_trn])\n        features = clf.features\n        params = clf.params\n        n_best = clf.n_best\n        print(f'{n_best}')\n        print(f'{params}')\n        print(f'{features}')\n    \n    trn_data = lgb.Dataset(X.iloc[i_trn], y[i_trn])\n    val_data = lgb.Dataset(X.iloc[i_val], y[i_val])\n    clf = lgb.train(params, trn_data, n_best, val_data, verbose_eval=100)\n    p[i_val] = clf.predict(X.iloc[i_val])\n    p_tst += clf.predict(X_tst) / n_fold\n    print(f'CV #{i + 1} AUC: {roc_auc_score(y[i_val], p[i_val]):.6f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.savetxt(predict_val_file, p, fmt='%.6f')\nnp.savetxt(predict_tst_file, p_tst, fmt='%.6f')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'  CV AUC: {roc_auc_score(y, p):.6f}')\nprint(f'Test AUC: {roc_auc_score(pseudo_label[target_col], p_tst)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submission","metadata":{}},{"cell_type":"code","source":"n_pos = int(0.34911 * tst.shape[0])\nth = sorted(p_tst, reverse=True)[n_pos]\nprint(th)\nconfusion_matrix(pseudo_label[target_col], (p_tst > th).astype(int))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub[target_col] = (p_tst > th).astype(int)\nsub.to_csv(submission_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you find it useful, please upvote the notebook and leave your feedback. It will be greatly appreciated!\n\nAlso please check my previous notebooks as well:\n* [AutoEncoder + Pseudo Label + AutoLGB](https://www.kaggle.com/jeongyoonlee/autoencoder-pseudo-label-autolgb): shows how to build a basic AutoEncoder using Keras, and perform automated feature selection and hyperparameter optimization using Kaggler's AutoLGB.\n* [Supervised Emphasized Denoising AutoEncoder](https://www.kaggle.com/jeongyoonlee/supervised-emphasized-denoising-autoencoder): shows how to build a more sophiscated version of AutoEncoder, called supervised emphasized Denoising AutoEncoder (DAE), which trains DAE and a classifier simultaneously.\n* [Stacking Ensemble](https://www.kaggle.com/jeongyoonlee/stacking-ensemble): shows how to perform stacking ensemble.","metadata":{}}]}