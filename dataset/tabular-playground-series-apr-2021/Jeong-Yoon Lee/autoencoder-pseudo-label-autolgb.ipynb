{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook, I will show how to use autoencoder, feature selection, hyperparameter optimization, and pseudo labeling using the `Keras` and `Kaggler` Python packages.\n\nThe contents of the notebook are as follows:\n1. **Package installation**: Installing latest version of `Kaggler` using `Pip`\n2. **Regular feature engineering**: [code](https://www.kaggle.com/udbhavpangotra/tps-apr21-eda-model) by @udbhavpangotra\n3. **Feature transformation**: Using `kaggler.preprocessing.LabelEncoder` to impute missing values and group rare categories automatically.\n4. **Stacked AutoEncoder**: Notebooks for DAE will be shared later.\n5. **Model training**: with 5-fold CV and pseudo label from @hiro5299834's [data](https://www.kaggle.com/hiro5299834/tps-apr-2021-voting-pseudo-labeling).\n6. **Feature selection and hyperparameter optimization**: Using `kaggler.model.AutoLGB`\n7. **Saving a submission file**","metadata":{}},{"cell_type":"markdown","source":"## Load libraries and install `Kaggler`","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\nimport lightgbm as lgb\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport tensorflow as tf\nfrom tensorflow import keras\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score, confusion_matrix\nimport warnings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install kaggler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import kaggler\nfrom kaggler.model import AutoLGB\nfrom kaggler.preprocessing import LabelEncoder\n\nprint(f'Kaggler: {kaggler.__version__}')\nprint(f'TensorFlow: {tf.__version__}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.simplefilter('ignore')\nplt.style.use('fivethirtyeight')\npd.set_option('max_columns', 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering (ref: [code](https://www.kaggle.com/udbhavpangotra/tps-apr21-eda-model) by @udbhavpangotra)","metadata":{}},{"cell_type":"code","source":"feature_name = 'ae'\nalgo_name = 'lgb'\nmodel_name = f'{algo_name}_{feature_name}'\n\ndata_dir = Path('/kaggle/input/tabular-playground-series-apr-2021/')\ntrn_file = data_dir / 'train.csv'\ntst_file = data_dir / 'test.csv'\nsample_file = data_dir / 'sample_submission.csv'\npseudo_label_file = '/kaggle/input/tps-apr-2021-label/voting_submission_from_5_best.csv'\n\nfeature_file = f'{feature_name}.csv'\npredict_val_file = f'{model_name}.val.txt'\npredict_tst_file = f'{model_name}.tst.txt'\nsubmission_file = f'{model_name}.sub.csv'\n\ntarget_col = 'Survived'\nid_col = 'PassengerId'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn = pd.read_csv(trn_file, index_col=id_col)\ntst = pd.read_csv(tst_file, index_col=id_col)\nsub = pd.read_csv(sample_file, index_col=id_col)\npseudo_label = pd.read_csv(pseudo_label_file, index_col=id_col)\nprint(trn.shape, tst.shape, sub.shape, pseudo_label.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tst[target_col] = pseudo_label[target_col]\nn_trn = trn.shape[0]\ndf = pd.concat([trn, tst], axis=0)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature engineering code from https://www.kaggle.com/udbhavpangotra/tps-apr21-eda-model\n\ndf['Embarked'] = df['Embarked'].fillna('No')\ndf['Cabin'] = df['Cabin'].fillna('_')\ndf['CabinType'] = df['Cabin'].apply(lambda x:x[0])\ndf.Ticket = df.Ticket.map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'X')\n\ndf['Age'].fillna(round(df['Age'].median()), inplace=True,)\ndf['Age'] = df['Age'].apply(round).astype(int)\n\ndf['Fare'].fillna(round(df['Fare'].median()), inplace=True,)\n\ndf['FirstName'] = df['Name'].str.split(', ').str[0]\ndf['SecondName'] = df['Name'].str.split(', ').str[1]\n\ndf['n'] = 1\n\ngb = df.groupby('FirstName')\ndf_names = gb['n'].sum()\ndf['SameFirstName'] = df['FirstName'].apply(lambda x:df_names[x])\n\ngb = df.groupby('SecondName')\ndf_names = gb['n'].sum()\ndf['SameSecondName'] = df['SecondName'].apply(lambda x:df_names[x])\n\ndf['Sex'] = (df['Sex'] == 'male').astype(int)\n\ndf['FamilySize'] = df.SibSp + df.Parch + 1\n\nfeature_cols = ['Pclass', 'Age','Embarked','Parch','SibSp','Fare','CabinType','Ticket','SameFirstName', 'SameSecondName', 'Sex',\n                'FamilySize', 'FirstName', 'SecondName']\ncat_cols = ['Pclass','Embarked','CabinType','Ticket', 'FirstName', 'SecondName']\nnum_cols = [x for x in feature_cols if x not in cat_cols]\nprint(len(feature_cols), len(cat_cols), len(num_cols))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[num_cols].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 16))\nfor i, col in enumerate(num_cols):\n    ax = plt.subplot(4, 2, i + 1)\n    ax.set_title(col)\n    df[col].hist(bins=50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Transformation","metadata":{}},{"cell_type":"markdown","source":"Apply `log2(1 + x)` transformation followed by standardization for count variables to make them close to the normal distribution. `log2(1 + x)` has better resolution than `log1p` and it preserves the values of 0 and 1.","metadata":{}},{"cell_type":"code","source":"for col in ['SameFirstName', 'SameSecondName', 'Fare', 'FamilySize', 'Parch', 'SibSp']:\n    df[col] = np.log2(1 + df[col])\n    \ndf.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\ndf[num_cols] = scaler.fit_transform(df[num_cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Label-encode categorical variables using `kaggler.preprocessing.LabelEncoder`, which creates new categories for `NaN`s as well as rare categories (using the threshold of `min_obs`).","metadata":{}},{"cell_type":"code","source":"lbe = LabelEncoder(min_obs=50)\ndf[cat_cols] = lbe.fit_transform(df[cat_cols]).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## AutoEncoder using `Keras`","metadata":{}},{"cell_type":"markdown","source":"Basic stacked autoencoder. I will add the versions with DAE and emphasized DAE later.","metadata":{}},{"cell_type":"code","source":"encoding_dim = 64\n\ndef get_model(encoding_dim, dropout=.2):\n    num_dim = len(num_cols)\n    num_input = keras.layers.Input((num_dim,), name='num_input')\n    cat_inputs = []\n    cat_embs = []\n    emb_dims = 0\n    for col in cat_cols:\n        cat_input = keras.layers.Input((1,), name=f'{col}_input')\n        emb_dim = max(8, int(np.log2(1 + df[col].nunique()) * 4))\n        cat_emb = keras.layers.Embedding(input_dim=df[col].max() + 1, output_dim=emb_dim)(cat_input)\n        cat_emb = keras.layers.Dropout(dropout)(cat_emb)\n        cat_emb = keras.layers.Reshape((emb_dim,))(cat_emb)\n\n        cat_inputs.append(cat_input)\n        cat_embs.append(cat_emb)\n        emb_dims += emb_dim\n\n    merged_inputs = keras.layers.Concatenate()([num_input] + cat_embs)\n\n    encoded = keras.layers.Dense(encoding_dim * 3, activation='relu')(merged_inputs)\n    encoded = keras.layers.Dropout(dropout)(encoded)\n    encoded = keras.layers.Dense(encoding_dim * 2, activation='relu')(encoded)\n    encoded = keras.layers.Dropout(dropout)(encoded)    \n    encoded = keras.layers.Dense(encoding_dim, activation='relu')(encoded)\n    \n    decoded = keras.layers.Dense(encoding_dim * 2, activation='relu')(encoded)\n    decoded = keras.layers.Dropout(dropout)(decoded)\n    decoded = keras.layers.Dense(encoding_dim * 3, activation='relu')(decoded)\n    decoded = keras.layers.Dropout(dropout)(decoded)    \n    decoded = keras.layers.Dense(num_dim + emb_dims, activation='linear')(decoded)\n\n    encoder = keras.Model([num_input] + cat_inputs, encoded)\n    ae = keras.Model([num_input] + cat_inputs, decoded)\n    ae.add_loss(keras.losses.mean_squared_error(merged_inputs, decoded))\n    ae.compile(optimizer='adam')\n    return ae, encoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ae, encoder = get_model(encoding_dim)\nae.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = [df[num_cols].values] + [df[x].values for x in cat_cols]\nae.fit(inputs, inputs,\n      epochs=100,\n      batch_size=16384,\n      shuffle=True,\n      validation_split=.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoding = encoder.predict(inputs)\nprint(encoding.shape)\nnp.savetxt(feature_file, encoding, fmt='%.6f', delimiter=',')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training + Feature Selection + Hyperparameter Optimization","metadata":{}},{"cell_type":"markdown","source":"Train the `LightGBM` model with pseudo label and 5-fold CV. In the first fold, perform feature selection and hyperparameter optimization using `kaggler.model.AutoLGB`.","metadata":{}},{"cell_type":"code","source":"seed = 42\nn_fold = 5\nX = pd.concat((df[feature_cols], \n               pd.DataFrame(encoding, columns=[f'enc_{x}' for x in range(encoding_dim)])), axis=1)\ny = df[target_col]\nX_tst = X.iloc[n_trn:]\n\ncv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\np = np.zeros_like(y, dtype=float)\np_tst = np.zeros((tst.shape[0],))\nfor i, (i_trn, i_val) in enumerate(cv.split(X, y)):\n    if i == 0:\n        clf = AutoLGB(objective='binary', metric='auc', random_state=seed)\n        clf.tune(X.iloc[i_trn], y[i_trn])\n        features = clf.features\n        params = clf.params\n        n_best = clf.n_best\n        print(f'{n_best}')\n        print(f'{params}')\n        print(f'{features}')\n    \n    trn_data = lgb.Dataset(X.iloc[i_trn], y[i_trn])\n    val_data = lgb.Dataset(X.iloc[i_val], y[i_val])\n    clf = lgb.train(params, trn_data, n_best, val_data, verbose_eval=100)\n    p[i_val] = clf.predict(X.iloc[i_val])\n    p_tst += clf.predict(X_tst) / n_fold\n    print(f'CV #{i + 1} AUC: {roc_auc_score(y[i_val], p[i_val]):.6f}')\n\nnp.savetxt(predict_val_file, p, fmt='%.6f')\nnp.savetxt(predict_tst_file, p_tst, fmt='%.6f')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'  CV AUC: {roc_auc_score(y, p):.6f}')\nprint(f'Test AUC: {roc_auc_score(pseudo_label[target_col], p_tst)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission File","metadata":{}},{"cell_type":"code","source":"n_pos = int(0.34911 * tst.shape[0])\nth = sorted(p_tst, reverse=True)[n_pos]\nprint(th)\nconfusion_matrix(pseudo_label[target_col], (p_tst > th).astype(int))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub[target_col] = (p_tst > th).astype(int)\nsub.to_csv(submission_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you find it helpful, please upvote the notebook and give a star to [Kaggler](https://github.com/jeongyoonlee/Kaggler). If you have questions and/or feature requests for Kaggler, please post them as `Issue` in the `Kaggler` GitHub repository.\n\nHappy Kaggling!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}