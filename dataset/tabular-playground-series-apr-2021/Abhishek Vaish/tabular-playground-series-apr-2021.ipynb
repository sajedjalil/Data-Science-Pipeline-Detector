{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular Playground Series - Apr 2021\nIn this notebook, we perform and analyse the `Titanic Dataset` generated using the CTGAN. We need to create the machine learning model that predict the `Survived` field using the 11 different variables. Evaluation is depend upon the `accuracy` of the model. ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true}},{"cell_type":"markdown","source":"# Data Dictionary\n| Variable | Definition | Key |\n| -------- | ---------- | --- |\n| survival | Survival  |0 = No, 1 = Yes |\n| pclass | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd |\n| sex | Sex | |\n| Age | Age in years | |\n| sibsp | # of siblings / spouses aboard the Titanic | |\n| parch | # of parents / children aboard the Titanic | |\n| ticket | Ticket number | |\n| fare | Passenger fare | |\n| cabin | Cabin number | |\n| embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton |","metadata":{}},{"cell_type":"markdown","source":"# Variable Notes\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fiancÃ©s were ignored)\n\nparch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.","metadata":{}},{"cell_type":"markdown","source":"# Load the Dataset \nIn this section, we import all the useful libraries and load the dataset into the notebook.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import boxcox\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport missingno\n\nplt.style.use('dark_background')\n\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier, VotingClassifier, RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/tabular-playground-series-apr-2021/train.csv')\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\ntest_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Perform statistics opertaion\nIn this section, we perform the basic statistics operation like mean, standardization, min, max, etc.","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One of the weird observation in the dataset is in the Age column, as it had a minimum age of 0.080 which is really not possible. We need to handle this errorness in the dataset and replace it with something else.","metadata":{}},{"cell_type":"code","source":"test_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missingno.bar(train_df, color='orangered');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since, we have lots of missing value in `Cabin` column so filling out with some random value doesnot make a good call. So we going to drop out the column from the dataset and fill the rest of the missing column with the help of the EDA.","metadata":{}},{"cell_type":"markdown","source":"# Exploratory Data Analysis\nIn this section, we perform the Exploratory Data Analysis or EDA to understand the dataset and find the useful patterns within the dataset between the different variables.","metadata":{}},{"cell_type":"markdown","source":"## Univariate","metadata":{}},{"cell_type":"code","source":"plt.pie(train_df.Sex.value_counts(), labels=['Male', 'Female'], colors=['orangered', 'lightsalmon'], autopct=\"%1.2f%%\")\nplt.title('Sex Distribution Graph', fontweight='bold', fontsize=18);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def univariate_graph(title, xlabel, x, y, ylabel='Frequency'):\n    plt.bar(x, y, color='orangered')\n    plt.title(title, fontweight='bold', fontsize=14)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"univariate_graph(x=['Survived', 'Not Survived'],\n                y=train_df.Survived.value_counts(),\n                title='Survived Distribution',\n                xlabel='Survived')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"univariate_graph(x=['Lower', 'Upper', 'Middle'],\n                y=train_df.Pclass.value_counts(),\n                title='Pclass Distribution',\n                xlabel='Pclass')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(train_df.Age, bins=10, color='orangered')\nplt.title('Age Distribution', fontweight='bold', fontsize=14)\nplt.xlabel('Age')\nplt.ylabel('Frequency');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So most of the passengers are from the age 20 to 40 years. But again, passenger with the age below the 0 or 5 is not possible that they are travelling on the ship. We need to handle such case before fitting the model.","metadata":{}},{"cell_type":"code","source":"univariate_graph(x=train_df.SibSp.value_counts().index,\n                y=train_df.SibSp.value_counts(),\n                title='Sibling/Spouse Distribution',\n                xlabel='SibSp')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, most of the passengers on the Titanic are came alone. Somwe of them are come in couple or sibling while some of them come with their family.","metadata":{}},{"cell_type":"code","source":"univariate_graph(x=train_df.Parch.value_counts().index,\n                y=train_df.Parch.value_counts(),\n                title='Parch Distribution',\n                xlabel='Parch')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"univariate_graph(x=['Southampton', 'Cherbourg', 'Queenstown'],\n                y=train_df.Embarked.value_counts(),\n                title='Embarked Distribution',\n                xlabel='Embarked')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So most of the passenger are going to the `Southampton`.","metadata":{}},{"cell_type":"code","source":"plt.hist(train_df.Fare, bins=5, color='orangered')\nplt.title('Fare Distribution', fontweight='bold', fontsize=14)\nplt.xlabel('Fare')\nplt.ylabel('Frequency');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bivariate","metadata":{"trusted":true}},{"cell_type":"code","source":"sample_col = [col for col in train_df.columns if pd.api.types.is_numeric_dtype(train_df[col])]\nplt.style.use('dark_background')\ndata = train_df.dropna()\nplt.boxplot(data[sample_col[1:]], patch_artist=True, labels=sample_col[1:])\nplt.title('Outlier Chart', fontsize=24, fontweight='bold');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, we have outlier value in Fare. We have to see more deeply in SibSp and Parch column but seeing the dataset only we can say that it don't have any outlier.","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\nax1.boxplot(train_df['SibSp'], patch_artist=True, labels=['SibSp'])\nax1.set_title('SibSp Outlier Chart', fontsize=18, fontweight='bold')\nax2.boxplot(train_df['Parch'], patch_artist=True, labels=['Parch'])\nax2.set_title('Parch Outlier Chart', fontsize=18, fontweight='bold')\ndata = train_df.dropna()\nax3.boxplot(data['Age'], patch_artist=True, labels=['Age'])\nax3.set_title('Age Outlier Chart', fontsize=18, fontweight='bold');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Yeah!! We found the outlier in the `SibSp`, `Parch` and `Age` when we check these column more closely.","metadata":{}},{"cell_type":"code","source":"sns.heatmap(train_df.corr(), annot=True, cmap=\"YlOrBr\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TODO:\n   * Handle the outliers in columns: (Fare, Age, SibSp, Parch)","metadata":{}},{"cell_type":"code","source":"plt.bar(['female', 'male'], train_df['Sex'][train_df['Survived'] == 1].value_counts(), width=0.3, color='orangered')\nplt.bar(['female', 'male'], train_df['Sex'][train_df['Survived'] == 0].value_counts().sort_values(), bottom=train_df['Sex'][train_df['Survived'] == 1].value_counts(), width=0.3, color='lightsalmon')\nplt.legend(['Survived', 'NotSurvived'])\nplt.title('Sex Survived Relationship', fontsize=18, fontweight='bold')\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.bar(['Upper', 'Middle', 'Lower'], train_df['Pclass'][train_df['Survived'] == 1].value_counts().sort_values(), color='orangered')\nplt.bar(['Upper', 'Middle', 'Lower'], train_df['Pclass'][train_df['Survived'] == 0].value_counts(), color='lightsalmon', bottom=train_df['Pclass'][train_df['Survived'] == 1].value_counts().sort_values())\nplt.title('Pclass Survived Relationship', fontsize=18, fontweight='bold')\nplt.legend(['Survived', 'NotSurvived'])\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.bar([0, 1, 2, 3, 4, 8, 5], train_df['SibSp'][train_df['Survived'] == 1].value_counts(), color='orangered')\nplt.bar([0, 1, 2, 3, 4, 8, 5], train_df['SibSp'][train_df['Survived'] == 0].value_counts(), color='lightsalmon', bottom=train_df['SibSp'][train_df['Survived'] == 1].value_counts())\nplt.title('SibSp Survived Relationship', fontsize=18, fontweight='bold')\nplt.legend(['Survived', 'NotSurvived'])\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Handle Missing Value\nIn this section, we handle the missing value present in the dataset. In some case we drop the column from the dataset or in some column we handle using the median and the mode.","metadata":{}},{"cell_type":"code","source":"train_df['train_test'] = 1\ntest_df['train_test'] = 0\ntrain_copy = train_df.drop('Survived', axis=1)\ncombine_df = pd.concat([train_copy, test_df])\ncombine_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in combine_df.columns:\n    if(combine_df.isna().sum()/len(combine_df) > 0.0).sum() != 0:\n        if (combine_df[col].isna().sum()/len(combine_df) > 0.0):\n            print(f\"{col}: {combine_df[col].isna().sum()/len(combine_df)}\")\n    else:\n        print('No missing value found!!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, as we stated above that we are going to drop the `cabin` column from the dataset.","metadata":{}},{"cell_type":"code","source":"combine_df.Cabin.fillna('X', inplace=True)\ncombine_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = [f[0] for f in combine_df.Cabin]\ncombine_df['Update_Cabin'] = data\ncombine_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combine_df.drop('Cabin', axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combine_df['Age'] = combine_df['Age'].fillna(combine_df['Age'].median())\ncombine_df['Embarked'] = combine_df['Embarked'].fillna(combine_df['Embarked'].mode()[0])\ncombine_df['Fare'] = combine_df['Fare'].fillna(combine_df['Fare'].median())\ncombine_df['Ticket'] = combine_df['Ticket'].fillna(combine_df['Ticket'].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in combine_df.columns:\n    if(combine_df.isna().sum()/len(combine_df) > 0.0).sum() != 0:\n        if (combine_df[col].isna().sum()/len(combine_df) > 0.0):\n            print(f\"{col}: {combine_df[col].isna().sum()/len(combine_df)}\")\n    else:\n        print('No missing value found!!')\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We had deal with all the missing value present in our dataset. Now, its time to perform more EDA to find the normalization and the linear relationship in our dataset. This will help to choose the estimators for training the ml model.","metadata":{}},{"cell_type":"markdown","source":"# Handle Qunatile in Dataset\nIn this section, we handle the quantile value present in the dataset for making the dataset more consistent.","metadata":{}},{"cell_type":"code","source":"combine_df['Update_Age'] = boxcox(combine_df.Age)[0]\ncombine_df['Update_Fare'] = boxcox(combine_df.Fare)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combine_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quantile_col = ['Parch', 'SibSp', 'Update_Fare']\nfor i in range(len(quantile_col)):\n    q1 = combine_df[quantile_col[i]].quantile(0.25)\n    q3 = combine_df[quantile_col[i]].quantile(0.75)\n    IQR = q3 - q1\n    combine_df[quantile_col[i]] = np.where(combine_df[quantile_col[i]] < q1, q1 - (1.5 * IQR), combine_df[quantile_col[i]])\n    combine_df[quantile_col[i]] = np.where(combine_df[quantile_col[i]] > q3, q3 + (1.5 * IQR), combine_df[quantile_col[i]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_combine_df = combine_df.drop(['Age', 'Fare'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_col = [col for col in new_combine_df.columns if pd.api.types.is_numeric_dtype(new_combine_df[col])]\nplt.style.use('dark_background')\nplt.boxplot(new_combine_df[sample_col[1:]], patch_artist=True, labels=sample_col[1:])\nplt.title('Outlier Chart', fontsize=24, fontweight='bold');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20, 5))\nax1.boxplot(new_combine_df['SibSp'], patch_artist=True, labels=['SibSp'])\nax1.set_title('SibSp Outlier Chart', fontsize=18, fontweight='bold')\nax2.boxplot(new_combine_df['Parch'], patch_artist=True, labels=['Parch'])\nax2.set_title('Parch Outlier Chart', fontsize=18, fontweight='bold')\nax3.boxplot(new_combine_df['Update_Age'], patch_artist=True, labels=['Age'])\nax3.set_title('Age Outlier Chart', fontsize=18, fontweight='bold');\nax4.boxplot(new_combine_df['Update_Fare'], patch_artist=True, labels=['Fare'])\nax4.set_title('Fare Outlier Chart', fontsize=18, fontweight='bold');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# More EDA\nIn this section, we perform more EDA to find out the normalization graph in the univariate columns and the linear relationship between the different features in the dataset.","metadata":{}},{"cell_type":"markdown","source":"## TODO:\n* Handle the Age column and perform normalization.\n* Handle the Fare column and perform normalization.","metadata":{}},{"cell_type":"code","source":"new_combine_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"normal_col = ['Update_Age', 'Update_Fare', 'SibSp', 'Parch', 'Pclass']\nfor col in normal_col:\n    sns.distplot(new_combine_df[col], color='orangered')\n    plt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_combine_df['Family'] = new_combine_df['SibSp'] + new_combine_df['Parch']\nnew_combine_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(new_combine_df.corr(), annot=True, cmap='YlOrBr');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\nIn this section, we create some new columns from the existing one.","metadata":{}},{"cell_type":"code","source":"new_combine_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"value = new_combine_df.groupby('Pclass')['Pclass'].value_counts().to_dict()\nnew_combine_df['Pclass_Count'] = new_combine_df.Pclass.apply(lambda x: value.get((x,x), 0))\n\nvalue = new_combine_df.groupby('Embarked')['Embarked'].value_counts().to_dict()\nnew_combine_df['Embarked_Count'] = new_combine_df.Embarked.apply(lambda x: value.get((x, x), 0))\n\nvalue = new_combine_df.groupby('Sex')['Sex'].value_counts().to_dict()\nnew_combine_df['Sex_Count'] = new_combine_df.Sex.apply(lambda x: value.get((x, x), 0))\n\nvalue = new_combine_df.groupby('Update_Cabin')['Update_Cabin'].value_counts().to_dict()\nnew_combine_df['Cabin_Count'] = new_combine_df.Update_Cabin.apply(lambda x: value.get((x, x), 0))\n\nvalue_list = []\nfor value in new_combine_df.Ticket:\n    if (len(value.split(' ')) > 1):\n        value_list.append(value.split(' ')[0])\n    else:\n        value_list.append('X')\nnew_combine_df['Ticket_Category'] = value_list\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_combine_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Handling Categorical Datatyes\nIn this section, we transform the categorical data into numerical dataset.","metadata":{}},{"cell_type":"code","source":"sample_ds = []\nfor value in new_combine_df.Ticket:\n    if(len(value.split(' ')) > 1):\n        if(value.split(' ')[1] == ''):\n            sample_ds.append(np.nan)\n        else:\n            sample_ds.append(value.split(' ')[1])\n    else:\n        sample_ds.append(value)\nnew_combine_df['Update_Ticket'] = sample_ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_combine_df.fillna(new_combine_df.Update_Ticket.mode()[0], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_combine_df.Update_Ticket = new_combine_df.Update_Ticket.astype('int32')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_value = new_combine_df.Update_Ticket.min()\nmax_value = new_combine_df.Update_Ticket.max()\nvalue_ds = []\nfor value in new_combine_df.Update_Ticket:\n    value_ds.append((value - min_value)/(max_value - min_value))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_combine_df['Update_Ticket'] = value_ds\nnew_combine_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quantile_col = ['Update_Ticket']\nfor i in range(len(quantile_col)):\n    q1 = new_combine_df[quantile_col[i]].quantile(0.25)\n    q3 = new_combine_df[quantile_col[i]].quantile(0.75)\n    IQR = q3 - q1\n    new_combine_df[quantile_col[i]] = np.where(new_combine_df[quantile_col[i]] < q1, q1 - (1.5 * IQR), new_combine_df[quantile_col[i]])\n    new_combine_df[quantile_col[i]] = np.where(new_combine_df[quantile_col[i]] > q3, q3 + (1.5 * IQR), new_combine_df[quantile_col[i]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = LabelEncoder()\nvalue = encoder.fit_transform(new_combine_df.Sex)\nnew_combine_df.Sex = value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"value = encoder.fit_transform(new_combine_df.Embarked)\nnew_combine_df.Embarked = value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"value = encoder.fit_transform(new_combine_df.Update_Cabin)\nnew_combine_df.Update_Cabin = value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"value = encoder.fit_transform(new_combine_df.Ticket_Category)\nnew_combine_df.Ticket_Category = value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_combine_df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_combine_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_combine_df['PSib'] = new_combine_df['Pclass']\nnew_combine_df['PSib'] = new_combine_df['PSib'].map(new_combine_df.groupby('Pclass')['SibSp'].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_combine_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_combine_df['ASex'] = new_combine_df['Ticket_Category']\nnew_combine_df['ASex'] = new_combine_df['ASex'].map(new_combine_df.groupby('Ticket_Category')['Update_Cabin'].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_combine_df['PCabin'] = new_combine_df['Pclass']\nnew_combine_df['PCabin'] = new_combine_df['PCabin'].map(new_combine_df.groupby('Pclass')['Update_Cabin'].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Training and Testing Data\nIn this section, we seperate the dataset into training and testing which we had combine earlier for preprocessing and transformation purpose.","metadata":{}},{"cell_type":"code","source":"df_train = new_combine_df[new_combine_df['train_test'] == 1]\ndf_test = new_combine_df[new_combine_df['train_test'] == 0]\ndf_train.drop(['train_test'], axis=1, inplace=True)\ndf_test.drop(['train_test'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nsns.heatmap(df_train.corr(), annot=True);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del_col = ['Pclass', 'SibSp', 'Embarked', 'Sex', 'Cabin_Count', 'Parch']\ndf_train.drop(del_col, axis=1, inplace=True)\ndf_test.drop(del_col, axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training\nIn this section, we train the classification model.","metadata":{"trusted":true}},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(42)\nX_train, X_test, y_train, y_test = train_test_split(df_train, train_df.Survived, test_size=0.2)\nlen(X_train), len(y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndecision_model = DecisionTreeClassifier()\ndecision_model.fit(X_train, y_train)\nplt.barh(X_train.columns, decision_model.feature_importances_)\nplt.title('Feature Importance');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col = ['Update_Ticket', 'Sex_Count', 'Update_Fare', 'Update_Age']\ntrain = df_train[col]\nX0, X1, y0, y1 = train_test_split(train, train_df.Survived, test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'ccp_alpha': 0.0,\n 'criterion': 'friedman_mse',\n 'init': None,\n 'learning_rate': 0.1058986682719916,\n 'loss': 'exponential',\n 'max_depth': 5,\n 'max_features': None,\n 'max_leaf_nodes': None,\n 'min_impurity_decrease': 0.0,\n 'min_impurity_split': None,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'n_estimators': 100,\n 'n_iter_no_change': None,\n 'random_state': None,\n 'subsample': 1.0,\n 'tol': 0.0001,\n 'validation_fraction': 0.1,\n 'verbose': 0,\n 'warm_start': False}\n\ngradient_model = GradientBoostingClassifier(**params)\ngradient_model.fit(X_train, y_train)\ngradient_model.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gradient_model = GradientBoostingClassifier(learning_rate=0.1)\ngradient_model.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gradient_model.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_model = LGBMClassifier(n_estimators=40)\nlgbm_model.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_model.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBRFClassifier\n\nxg_model = XGBRFClassifier()\nxg_model.fit(X_train, y_train)\nxg_model.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"voting_model = VotingClassifier(estimators=[('gm', gradient_model), \n                                            ('xg', xg_model),\n                                            ('lgbm', lgbm_model)\n                                           ], voting='hard', verbose=True)\nvoting_model.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"voting_model.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submision","metadata":{}},{"cell_type":"code","source":"def submission_file(model, filename='submission.csv'):\n    y_preds = model.predict(df_test)\n    submission = pd.DataFrame(y_preds, columns=['Survived'])\n    submission.index = test_df.PassengerId\n    submission.to_csv(filename)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = submission_file(voting_model, filename='submission3.csv')\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}