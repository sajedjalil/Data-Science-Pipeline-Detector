{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing necessary requirements","metadata":{}},{"cell_type":"code","source":"!pip install pytorch-tabnet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TabNet Architecture","metadata":{}},{"cell_type":"markdown","source":"<img src= \"https://miro.medium.com/max/788/1*twB1nZHPN5Cuxu2h_jpEPg.png\" alt =\"TabNet\" style='width: 1000px;'>","metadata":{}},{"cell_type":"markdown","source":"# Loading packages and modules","metadata":{}},{"cell_type":"code","source":"# to read and manipulate data\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\npd.options.display.max_columns = None\n\n# to preprocess the data\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\n# to model the data\nfrom sklearn.metrics import log_loss, accuracy_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nimport torch\nfrom torch import nn\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\n\n# to set the environment\nimport os\nimport sys\nimport random\nfrom tqdm.notebook import tqdm\n\n# for hyperparameter optimization\nimport optuna\nfrom optuna.visualization import plot_optimization_history\nfrom optuna.visualization import plot_parallel_coordinate\nfrom optuna.visualization import plot_param_importances","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utility functions","metadata":{}},{"cell_type":"code","source":"# function to unpack train hyperparameters\ndef unpack_train_hyperpars(hyperparameters):\n            # unpacking of shared hyperparameters for the pretrainer and classifier\n            shared_hyperparameters = dict(n_d = hyperparameters['hyp_shared']['n_d'],\n                                          n_a = hyperparameters['hyp_shared']['n_a'],\n                                          n_steps = hyperparameters['hyp_shared']['n_steps'],\n                                          n_independent = hyperparameters['hyp_shared']['n_independent'],\n                                          n_shared = hyperparameters['hyp_shared']['n_shared'],\n                                          mask_type = hyperparameters['hyp_shared']['mask_type'],\n                                          lambda_sparse = hyperparameters['hyp_shared']['lambda_sparse'],\n                                          cat_idxs = cat_indexes, \n                                          cat_dims = cat_dimensions, \n                                          cat_emb_dim = [dim * 2 for dim in cat_embedding_dimensions] if hyperparameters['hyp_shared']['cat_dim_big'] else cat_embedding_dimensions,\n                                          optimizer_fn = torch.optim.Adam if hyperparameters['hyp_shared']['use_adam'] else torch.optim.RMSprop,\n                                          scheduler_fn = torch.optim.lr_scheduler.ReduceLROnPlateau,\n                                          verbose = 1)\n            \n            # unpacking the hyperparameters for the classifier\n            train_hyperparameters = dict(gamma = hyperparameters['hyp_supervised']['gamma'],\n                                         momentum = hyperparameters['hyp_supervised']['momentum'],\n                                         optimizer_params = dict(lr = hyperparameters['hyp_supervised']['lr'],\n                                                                 weight_decay = hyperparameters['hyp_supervised']['weight_decay']),\n                                         scheduler_params = dict(mode = 'min', factor = 0.5, patience = 5, min_lr = 1e-8))\n            \n            # function to return the set of hyperparameters for training\n            return shared_hyperparameters, train_hyperparameters\n        \n                                             \n# function to unpack the hyperparameters for the pretrainer       \ndef unpack_unsup_hyperpars(hyperparameters, shared_hyperparameters):\n                # unpacking the pretrainer-specific hyperparameters\n                pretrain_hyperparameters = dict(optimizer_params = dict(lr = hyperparameters['hyp_pretrainer']['lr_pretrainer'],\n                                                                        weight_decay = hyperparameters['hyp_pretrainer']['weight_decay_pretrainer']),\n                                                scheduler_params = dict(mode = \"min\", factor = 0.5, patience = 5, min_lr = 1e-8))\n                \n                # updating the unpacked hyperparameters with the shared ones\n                pretrain_hyperparameters.update(shared_hyperparameters)\n                \n                # returning the hyperparameters for the pretrainer\n                return pretrain_hyperparameters\n            \n\n# function to instantiate the pretrainer with a set of hyperparameters\ndef instantiate_pretrainer(pretrainer_hyperparameters):\n    # instantiating the pretrainer with the defined hyperparameters\n    pretrainer = TabNetPretrainer(**pretrainer_hyperparameters)\n    # returning the pretrainer instance\n    return pretrainer\n\n\n# function to train the pretrainer\ndef fit_pretrainer(model, X, val_set, pretrain_ratio, bs, vbs):\n    # training the unsupervised tabnet model instance\n    model.fit(X_train = X,\n              eval_set = [val_set],\n              pretraining_ratio = pretrain_ratio,\n              max_epochs = 100,\n              patience = 5,\n              batch_size = bs,\n              virtual_batch_size = np.int(bs / vbs),\n              drop_last = True)\n    # returning the trained pretrainer\n    return model\n\n# function to instantiate the pretrainer with a set of hyperparameters\ndef instantiate_supervised(supervised_hyperparameters):\n    # instantiating the pretrainer with the defined hyperparameters\n    supervised_model = TabNetClassifier(**supervised_hyperparameters)\n    # returning the pretrainer instance\n    return supervised_model\n\n\ndef fit_supervised(model, X, y, val_set, val_name, bs, vbs, use_pretrain):\n    # training the unsupervised tabnet model instance\n    model.fit(X_train = X, \n              y_train = y, \n              eval_set = val_set, \n              eval_name = val_name, \n              eval_metric = ['logloss'], \n              max_epochs = 100, \n              patience = 5,\n              batch_size = bs, \n              virtual_batch_size = np.int(bs / vbs), \n              from_unsupervised = use_pretrain \n             )\n    # returning the trained pretrainer\n    return model\n\n\n# function to extract the best probability threshold according to the roc curve\ndef extract_roc_threshold(y_obs, probs):\n    # extracting fpr, tpr and thresholds from the roc curve\n    fpr, tpr, thresholds = roc_curve(y_true = y_obs, y_score = probs)\n    # calculating the gmeans for each of the fpr and tpr from the roc curve\n    gmeans = np.sqrt(tpr * (1 - fpr))\n    # identifying the element with the highest gmeans - best trade-off between fpr and tpr\n    best_threshold = np.argmax(gmeans)\n    # returning the probability threshold that maximizes the gmeans\n    return thresholds[best_threshold]\n\n\n# function to encode categories for a selected probability threshold\ndef move_threshold(probabilities, threshold):\n    return (probabilities >= threshold).astype('int')\n\n\n# function to evaluate a trained model\ndef score_model(model, X_eval, y_eval, scoring_method):\n    \n    # fitting the model trained model to the data to obtain predicted probabilities\n    predicted_probabilities = model.predict_proba(X_eval)[:, 1]\n    \n    # use the traditional approach\n    if scoring_method == 'traditional':\n        # predicting the class label with the traditional approach\n        selected_threshold = 0.5\n        predicted_classes = model.predict(X_eval)\n    # use the ROC AUC curve to define the threshold and set the class\n    else:\n        selected_threshold = extract_roc_threshold(y_obs = y_eval, probs = predicted_probabilities)\n        predicted_classes = move_threshold(probabilities = predicted_probabilities, threshold = selected_threshold)\n    \n    # calculating the accuracy score of the model\n    acc = accuracy_score(y_true = y_eval, y_pred = predicted_classes)\n    \n    # calculating the log loss of the model\n    #ll = log_loss(y_true = y_eval, y_pred = predicted_probabilities)\n    \n    # returning the trained model\n    #return acc, ll, selected_threshold\n    return acc, selected_threshold\n\n\n# function to run the stratified kfold with the selected hyperparameters\ndef evaluate_stratifield_kfold(X, y, kfolds, hyperparameters, should_pretrain):\n    print(f'\\n------------- Initializing the Stratified {kfolds}-fold evaluation -------------')\n    # creating empty lists to store the accuracy and log loss\n    fold_scores = []\n    \n    # printing the hyperparameters under evaluation\n    print('Evaluating the following hyperparameters:')\n    print(hyperparameters)\n    \n    # deciding whether past submissions should be used and which ones\n    if hyperparameters['which_submission'] == 'lr': # logistic regression submission labels\n        # sampling the indices of the test dataframe that will be used\n        test_idx_leak = np.random.choice(a = range(X_test_df.shape[0]), size = np.int(X_test_df.shape[0] * 0.4), replace = False)\n        # getting the indexes of the test dataframe\n        X_leak, y_leak = X_test_df.iloc[test_idx_leak], y_lr_submission[test_idx_leak]\n    if hyperparameters['which_submission'] == 'tabnet':\n        # sampling the indices of the test dataframe that will be used\n        test_idx_leak = np.random.choice(a = range(X_test_df.shape[0]), size = np.int(X_test_df.shape[0] * 0.4), replace = False)\n        # getting the indexes of the test dataframe\n        X_leak, y_leak = X_test_df.iloc[test_idx_leak], y_tabnet_submission[test_idx_leak]\n            \n    # instanting the stratified k-fold\n    skf = StratifiedKFold(n_splits = kfolds, random_state = 42, shuffle = True)\n    \n    # unpacking the statified k-fold\n    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n        \n        print(f'\\n------------------ Starting fold {fold + 1} ------------------')\n        \n        # separating the train from the test folds\n        X_train_fold, X_val_fold, y_train_fold, y_val_fold = X.iloc[train_idx], X.iloc[test_idx], y[train_idx], y[test_idx]\n        \n        # mergining the test dataset leak to the train set if this approach should be used\n        if hyperparameters['which_submission'] is not None:\n            # merging and replacing the fold objects\n            X_train_fold, y_train_fold = pd.concat([X_train_fold, X_leak]), np.hstack((y_train_fold, y_leak))\n            # shuffling the sequence of observations\n            shuffled_indices = np.random.choice(a = range(X_train_fold.shape[0]), size = X_train_fold.shape[0], replace = False)\n            # rearranging x and y\n            X_train_fold, y_train_fold = X_train_fold.iloc[shuffled_indices], y_train_fold[shuffled_indices]\n            \n        # instantiating a pipeline to transform the data\n        pipeline = Pipeline(steps = [('age_imputation', ColumnTransformer([('impute_age', SimpleImputer(strategy = 'mean'), [0])], remainder = 'passthrough')), \n                                     ('fare_imputation', ColumnTransformer([('impute_fare', SimpleImputer(strategy = 'median'), [1])], remainder = 'passthrough')),\n                                     ('scaler', ColumnTransformer([('mmscaler', MinMaxScaler(), [0, 1, 2, 3, 4])], remainder = 'passthrough'))\n                                    ])\n        \n        # training the pipeline\n        pipeline.fit(X_train_fold)\n        \n        # fitting the pipeline to the data\n        X_train_fold = pipeline.transform(X_train_fold)\n        X_val_fold = pipeline.transform(X_val_fold)\n             \n        # unpacking the hyperparameters used for training \n        shared_hyper, train_hyper = unpack_train_hyperpars(hyperparameters)\n        \n        # setting the hyperparameters for training\n        train_hyper.update(shared_hyper)\n        \n        if should_pretrain:\n            # unpacking the hyperparameters used for the pretrainer\n            unsup_hyperparameters = unpack_unsup_hyperpars(hyperparameters, shared_hyper)\n            \n            # instantiating the hyperparameters\n            pretrainer = instantiate_pretrainer(pretrainer_hyperparameters = unsup_hyperparameters)\n            \n            # combining the data\n            X_unsup_test = np.vstack((X_train_fold, X_val_fold))\n            \n            # training the unsupervised model\n            pretrainer = fit_pretrainer(model = pretrainer, \n                                        X = X_unsup_test if hyperparameters['hyp_pretrainer']['pretrain_on_test'] else X_train_fold, \n                                        val_set = X_unsup_test if hyperparameters['hyp_pretrainer']['pretrain_on_test'] else X_train_fold, \n                                        pretrain_ratio = hyperparameters['hyp_pretrainer']['pretraining_ratio'], \n                                        bs = np.int(hyperparameters['hyp_shared']['batch_size']), \n                                        vbs = np.int(hyperparameters['hyp_shared']['virtual_bs_ratio']))\n\n        \n        # instantiate the supervised classifier\n        clf = instantiate_supervised(supervised_hyperparameters = train_hyper)\n        \n        # fit the supervised classifier\n        clf = fit_supervised(model = clf, \n                             X = X_train_fold, y = y_train_fold,\n                             val_set = [(X_val_fold, y_val_fold)], \n                             val_name = ['validation'],\n                             bs = np.int(hyperparameters['hyp_shared']['batch_size']), \n                             vbs = np.int(hyperparameters['hyp_shared']['virtual_bs_ratio']),\n                             use_pretrain = pretrainer if should_pretrain else None)\n        \n        # extracting the train score\n        train_acc, _ = score_model(model = clf, X_eval = X_train_fold, y_eval = y_train_fold, scoring_method = hyperparameters['scoring'])\n        \n        # extracting the validation score\n        val_acc, _ = score_model(model = clf, X_eval = X_val_fold, y_eval = y_val_fold, scoring_method = hyperparameters['scoring'])\n        \n        # appending accuracy and log loss to the list\n        fold_scores.append((train_acc, val_acc))\n        \n        # printing the results of this fold\n        print(f\"> Fold {fold + 1}: Training accuracy: {train_acc} | Validation accuracy: {val_acc}\")\n     \n    # structuring the training and validation scores in a dataframe\n    scores = pd.DataFrame(fold_scores, columns=['train_accuracy', 'val_accuracy'])\n     \n    # printing average scores for this run\n    print(f\"End of Run!\\nTraining loss: Training accuracy: {scores['train_accuracy'].mean()} | Validation accuracy: {scores['val_accuracy'].mean()}\")\n     \n    # returning the scores\n    return scores\n\n\n \n# objective function for optuna to evaluate\ndef objective(trial):\n    \n    # search space\n    search_space = {'hyp_supervised': {'lr': trial.suggest_float('lr', 0.001, 2e-2, log = True),\n                                       'weight_decay': trial.suggest_float('weight_decay', 0.0, 1e-3),\n                                       'gamma': trial.suggest_float('gamma', 1.3, 1.6),\n                                       'momentum': trial.suggest_float('momentum', 0.02, 0.1)},\n                    'hyp_shared': {'n_d': trial.suggest_int('n_d', 3, 14),\n                                   'n_a': trial.suggest_int('n_a', 3, 14),\n                                   'n_steps': trial.suggest_int('n_steps', 2, 6),\n                                   'n_independent': trial.suggest_int('n_independent', 1, 4),\n                                   'n_shared': trial.suggest_int('n_shared', 1, 4),\n                                   'mask_type': trial.suggest_categorical('mask_type', ['entmax', 'sparsemax']),\n                                   'lambda_sparse': trial.suggest_float('lambda_sparse', 0.0001, 0.01, log = True),\n                                   'batch_size': trial.suggest_categorical('batch_size', [1024, 2048, 4096]),\n                                   'virtual_bs_ratio': trial.suggest_categorical('virtual_bs_ratio', [2, 4, 8]),\n                                   'cat_dim_big': trial.suggest_categorical('cat_dim_big', [True, False]),\n                                   'use_adam': trial.suggest_categorical('use_adam', [True, False])},\n                    'scoring': trial.suggest_categorical('scoring', ['traditional', 'auc']),\n                    'which_submission': trial.suggest_categorical('which_submission', [None, 'lr', 'tabnet'])\n                   }\n    \n    # defining whether TabNet should be pretrained\n    pretrain_tabnet = trial.suggest_categorical('pretrain_tabnet', [True, False])\n    \n    # defining tabnet pretrainer hyperparameters if this was chosen and add it to the search space\n    if pretrain_tabnet:\n        # defining the hyperparameter dictionary\n        search_pretrain = {'hyp_pretrainer': {'pretraining_ratio': trial.suggest_float('pretraining_ratio', 0.3, 0.7),\n                                              'lr_pretrainer': trial.suggest_float('lr_pretrainer', 0.001, 2e-2, log = True),\n                                              'weight_decay_pretrainer': trial.suggest_float('weight_decay_pretrainer', 0.0, 1e-3),\n                                              'pretrain_on_test': trial.suggest_categorical('pretrain_on_test', [True, False])}}\n        \n        # updating the search space dictionary\n        search_space.update(search_pretrain)\n    \n    # fit SKF\n    skf_evaluated = evaluate_stratifield_kfold(X, y, kfolds = kfolds, \n                                               hyperparameters = search_space, should_pretrain = pretrain_tabnet)\n    \n    # unpack accuracy and loss\n    acc = skf_evaluated['val_accuracy'].mean()\n    \n    # return scores\n    return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to unpack the final hyperparameters for training\ndef unpack_final_hyperpars(hyperparameters):\n            # unpacking of shared hyperparameters for the pretrainer and classifier\n            shared_hyperparameters = dict(n_d = hyperparameters['n_d'],\n                                          n_a = hyperparameters['n_a'],\n                                          n_steps = hyperparameters['n_steps'],\n                                          n_independent = hyperparameters['n_independent'],\n                                          n_shared = hyperparameters['n_shared'],\n                                          mask_type = hyperparameters['mask_type'],\n                                          lambda_sparse = hyperparameters['lambda_sparse'],\n                                          cat_idxs = cat_indexes, \n                                          cat_dims = cat_dimensions, \n                                          cat_emb_dim = [dim * 2 for dim in cat_embedding_dimensions] if hyperparameters['cat_dim_big'] else cat_embedding_dimensions, \n                                          optimizer_fn = torch.optim.Adam if hyperparameters['use_adam'] else torch.optim.RMSprop,\n                                          scheduler_fn = torch.optim.lr_scheduler.ReduceLROnPlateau,\n                                          verbose = 1)\n            \n            # unpacking the hyperparameters for the classifier\n            train_hyperparameters = dict(gamma = hyperparameters['gamma'],\n                                         momentum = hyperparameters['momentum'],\n                                         optimizer_params = dict(lr = hyperparameters['lr'],\n                                                                 weight_decay = hyperparameters['weight_decay']),\n                                         scheduler_params = dict(mode = 'min', factor = 0.5, patience = 5, min_lr = 1e-8))\n            \n            # function to return the set of hyperparameters for training\n            return shared_hyperparameters, train_hyperparameters\n\n\n        # function to unpack the hyperparameters for the pretrainer       \ndef unpack_final_unsup(hyperparameters, shared_hyperparameters):\n                # unpacking the pretrainer-specific hyperparameters\n                pretrain_hyperparameters = dict(optimizer_params = dict(lr = hyperparameters['lr_pretrainer'],\n                                                                        weight_decay = hyperparameters['weight_decay_pretrainer']),\n                                                scheduler_params = dict(mode = \"min\", factor = 0.5, patience = 5, min_lr = 1e-8))\n                \n                # updating the unpacked hyperparameters with the shared ones\n                pretrain_hyperparameters.update(shared_hyperparameters)\n                \n                # returning the hyperparameters for the pretrainer\n                return pretrain_hyperparameters\n            \n# function to run the model with the best hyperparameters and submit it\ndef submit_stratifield_kfold(X, y, kfolds, hyperparameters, should_pretrain):\n    print(f'\\n------------- Initializing the Stratified {kfolds}-fold evaluation -------------')\n    # creating empty lists to store the accuracy and log loss\n    fold_scores = []\n    \n    # creating empty lists to store the thresholds\n    threshold_list = []\n    \n    # creating a numpy array to store probabilities\n    probas = np.zeros(len(X_test_array))\n    \n    # printing the hyperparameters under evaluation\n    print('Evaluating the following hyperparameters:')\n    print(hyperparameters)\n    \n    # instanting the stratified k-fold\n    skf = StratifiedKFold(n_splits = kfolds, random_state = 42, shuffle = True)\n\n    # deciding whether past submissions should be used and which ones\n    if hyperparameters['which_submission'] == 'lr': # logistic regression submission labels\n        # sampling the indices of the test dataframe that will be used\n        test_idx_leak = np.random.choice(a = range(X_test_df.shape[0]), size = np.int(X_test_df.shape[0] * 0.4), replace = False)\n        # getting the indexes of the test dataframe\n        X_leak, y_leak = X_test_df.iloc[test_idx_leak], y_lr_submission[test_idx_leak]\n    if hyperparameters['which_submission'] == 'tabnet':\n        # sampling the indices of the test dataframe that will be used\n        test_idx_leak = np.random.choice(a = range(X_test_df.shape[0]), size = np.int(X_test_df.shape[0] * 0.4), replace = False)\n        # getting the indexes of the test dataframe\n        X_leak, y_leak = X_test_df.iloc[test_idx_leak], y_tabnet_submission[test_idx_leak]\n        \n    # unpacking the statified k-fold\n    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n        \n        print(f'\\n------------------ Starting fold {fold + 1} ------------------')\n        \n        # separating the train from the test folds\n        X_train_fold, X_val_fold, y_train_fold, y_val_fold = X.iloc[train_idx], X.iloc[test_idx], y[train_idx], y[test_idx]\n        \n        # mergining the test dataset leak to the train set if this approach should be used\n        if hyperparameters['which_submission'] is not None:\n            # merging and replacing the fold objects\n            X_train_fold, y_train_fold = pd.concat([X_train_fold, X_leak]), np.hstack((y_train_fold, y_leak))\n            # shuffling the sequence of observations\n            shuffled_indices = np.random.choice(a = range(X_train_fold.shape[0]), size = X_train_fold.shape[0], replace = False)\n            # rearranging x and y\n            X_train_fold, y_train_fold = X_train_fold.iloc[shuffled_indices], y_train_fold[shuffled_indices]\n        \n        # instantiating a pipeline to transform the data\n        pipeline = Pipeline(steps = [('age_imputation', ColumnTransformer([('impute_age', SimpleImputer(strategy = 'mean'), [0])], remainder = 'passthrough')), \n                                     ('fare_imputation', ColumnTransformer([('impute_fare', SimpleImputer(strategy = 'median'), [1])], remainder = 'passthrough')),\n                                     ('scaler', ColumnTransformer([('mmscaler', MinMaxScaler(), [0, 1, 2, 3, 4])], remainder = 'passthrough'))\n                                    ])\n        \n        # training the pipeline\n        pipeline.fit(X_train_fold)\n        \n        # fitting the pipeline to the data\n        X_train_fold = pipeline.transform(X_train_fold)\n        X_val_fold = pipeline.transform(X_val_fold)\n        X_test_fold = pipeline.transform(X_test_array)\n        \n        # unpacking the hyperparameters used for training \n        shared_hyper, train_hyper = unpack_final_hyperpars(hyperparameters)\n        \n        # setting the hyperparameters for training\n        train_hyper.update(shared_hyper)\n        \n        if should_pretrain:\n            # unpacking the hyperparameters used for the pretrainer\n            unsup_hyperparameters = unpack_final_unsup(hyperparameters, shared_hyper)\n            \n            # instantiating the hyperparameters\n            pretrainer = instantiate_pretrainer(pretrainer_hyperparameters = unsup_hyperparameters)\n            \n            # combining the data\n            X_unsup_test = np.vstack((X_train_fold, X_val_fold))\n            \n            # training the unsupervised model\n            pretrainer = fit_pretrainer(model = pretrainer, \n                                        X = X_unsup_test if hyperparameters['pretrain_on_test'] else X_train_fold, \n                                        val_set = X_unsup_test if hyperparameters['pretrain_on_test'] else X_train_fold, \n                                        pretrain_ratio = hyperparameters['pretraining_ratio'], \n                                        bs = np.int(hyperparameters['batch_size']), \n                                        vbs = np.int(hyperparameters['virtual_bs_ratio']))\n            \n        # instantiate the supervised classifier\n        clf = instantiate_supervised(supervised_hyperparameters = train_hyper)\n        \n        # fitting the model to the data\n        clf = fit_supervised(model = clf, \n                             X = X_train_fold, y = y_train_fold,\n                             val_set = [(X_val_fold, y_val_fold)], \n                             val_name = ['validation'],\n                             bs = np.int(hyperparameters['batch_size']), \n                             vbs = np.int(hyperparameters['virtual_bs_ratio']),\n                             use_pretrain = pretrainer if should_pretrain else None)\n        \n        # extracting the train score\n        train_acc, _ = score_model(model = clf, X_eval = X_train_fold, y_eval = y_train_fold, scoring_method = hyperparameters['scoring'])\n        \n        # extracting the validation score\n        val_acc, val_threshold = score_model(model = clf, X_eval = X_val_fold, y_eval = y_val_fold, scoring_method = hyperparameters['scoring'])\n        \n        # appending accuracy and log loss to the list\n        fold_scores.append((train_acc, val_acc))\n        \n        # appending the threshold to the list\n        threshold_list.append(val_threshold)\n        \n        # printing the results of this fold\n        print(f\"> Fold {fold + 1}: Training accuracy: {train_acc} | Validation accuracy: {val_acc}\")\n        \n        # generating predictions on the test set with the model\n        probas += clf.predict_proba(X_test_fold)[:, 1]\n        \n    # structuring the training and validation scores in a dataframe\n    scores = pd.DataFrame(fold_scores, columns=['train_accuracy', 'val_accuracy'])\n    \n    # printing average scores for this run\n    print(f\"End of Run!\\nTraining loss: Training accuracy: {scores['train_accuracy'].mean()} | Validation accuracy: {scores['val_accuracy'].mean()}\")\n    \n    # returning the scores\n    return scores, probas, threshold_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the data","metadata":{}},{"cell_type":"code","source":"# train data\ntrain = pd.read_csv(filepath_or_buffer = '../input/tabular-playground-series-apr-2021/train.csv')\ntrain.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test data\ntest = pd.read_csv(filepath_or_buffer = '../input/tabular-playground-series-apr-2021/test.csv')\ntest.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission file\nsubmission = pd.read_csv(filepath_or_buffer = '../input/tabular-playground-series-apr-2021/sample_submission.csv')\nsubmission.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submitted predictions from my optimized logistic regression\nlr_submission = pd.read_csv(filepath_or_buffer = '../input/lr-submission-tps202104/lr_submission.csv')\nlr_submission.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submitted predictions from my optimized logistic regression\ntabnet_submission = pd.read_csv(filepath_or_buffer = '../input/tabnetsubmissiontps202104/tabnet_submission-tps202104.csv')\ntabnet_submission.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Merging the datasets","metadata":{}},{"cell_type":"code","source":"# putting the train on top of the test set in order to create some general features\ndf = pd.concat([train, test])\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"General features.","metadata":{}},{"cell_type":"code","source":"## adding number of missing values in the rows\ndf['nMissing'] = df.isnull().sum(axis = 1)\n\n# encoding the deck to which de cabin belongs to\ndf['Deck'] = df.Cabin.str.extract(pat = r'(^[A-Z])')\n# filling missing value for deck\ndf['Deck'] = df.Deck.fillna('U')\n\n# encoding whether the passenger travels alone\ndf['TravelsAlone'] = ((df.SibSp == 0) & (df.Parch == 0)).astype('int').astype('object')\n\n# encoding whether the passenger travels accompanied by somebody\ndf['TravelsTwo'] = ((df.SibSp == 1) & (df.Parch == 0) | (df.SibSp == 0) & (df.Parch == 1)).astype('int').astype('object')\n\n# calculating the size of the family\ndf['FamilySize'] = df.SibSp + df.Parch + 1\n\n# adding the family name\ndf['FamilyName'] = df.Name.str.extract(pat=r'(\\w+)(?=,)')\n\n# creating flags for the other columns that will be imputed\ndf['AgeImputed'] = (df.Age.isnull()).astype('int').astype('object')\ndf['FareImputed'] = (df.Fare.isnull()).astype('int').astype('object')\ndf['EmbarkedImputed'] = (df.Embarked.isnull()).astype('int').astype('object')\n\n# parsing Pclass to category\ndf['Pclass'] = df.Pclass.astype('object')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ticket letters representation.","metadata":{}},{"cell_type":"code","source":"# fixing possible typo on the ticket column \ndf['Ticket'] = df.Ticket.str.replace(pat=r'\\bSTON\\b', repl='SOTON', regex=True)\n\n# extracting the letters from the ticket column\ndf['TicketLetters'] = df.Ticket.str.findall(pat = r'([A-Za-z]+)')\n\n# filling NaNs with an empty list\ndf['TicketLetters'] = df.TicketLetters.fillna('')\n\n# parsing the list column to a string column\ndf['TicketLetters'] = df.TicketLetters.apply(lambda x: ' '.join([letras for letras in x]))\n\n# filling missing ticket letters if the string is empty\ndf['TicketLetters'] = df.TicketLetters.apply(lambda x: 'N' if x == '' else x)\n\n# putting all letters to upper\ndf['TicketLetters'] = df.TicketLetters.str.upper()\n\n# fixing some tickets that seems to be the same\ndf['TicketLetters'] = df.TicketLetters.replace(to_replace = {'C A': 'CA', 'W E P': 'WE P', 'SOTON O Q': 'SOTON OQ', 'S W PP': 'SW PP', \n                                                             'S O C': 'SO C', 'C A SOTON': 'SOTON CA', 'S C PARIS': 'SC PARIS', 'P PP': 'PP',\n                                                             'S C A': 'SC A', 'S O P P': 'S O P'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ticket numbers representation.","metadata":{}},{"cell_type":"code","source":"# extracting the ticket number\ndf['TicketNumber'] = df.Ticket.str.extract(pat=r'(?<=\\s)?([0-9]+)$').astype('float')\n\n# filling missing values in the ticket number\ndf['TicketNumber'] = df.TicketNumber.fillna(0)\n\n# number of digits\ndf['TicketDigits'] = df.TicketNumber.astype('int').astype('str').str.count(pat='[0-9]')\n\n# rounding the ticket number to an integer\ndf['TicketNumber'] = (df.TicketNumber / (df.TicketDigits * 1000)).astype('int')\n\n# scaling the ticket back to its original scale\ndf['TicketNumber'] = df.TicketNumber * (df.TicketDigits * 1000)\n\n# calculating the frequency of ticket numbers\nfrequencies_tnb = df.TicketNumber.value_counts(normalize=False)\n\n# mapping the frequencies to ticket numbers\nmask_TcktNmbFreq = df.TicketNumber.map(frequencies_tnb)\n\n# replacing ticket numbers that are very rare by a random number\ndf['TicketNumber'] = df.TicketNumber.mask(mask_TcktNmbFreq < 10, 999999)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cabin number representation.","metadata":{}},{"cell_type":"code","source":"# extracting the numeric part of the cabin\ndf['CabinNb'] = df.Cabin.fillna('0').str.extract(pat=r'([0-9]+)').astype('int')\n\n# extracting the count of digits in the cabin number\ndf['CabinDigits'] = df.CabinNb.astype('str').str.count(pat='[0-9]').astype('int')\n\n# rounding the ticket number to an integer\ndf['CabinNb'] = (df.CabinNb / (df.CabinDigits * 10)).astype('int')\n\n# scaling the ticket back to its original scale\ndf['CabinNb'] = df.CabinNb * (df.CabinDigits * 10)\n\n# calculating the frequency of cabin numbers\nfrequencies_cbn = df.CabinNb.value_counts(normalize=False)\n\n# mapping the frequencies to cabin numbers\nmask_CabinNbFreq = df.CabinNb.map(frequencies_cbn)\n\n# replacing cabin numbers that are very rare by a random number\ndf['CabinNb'] = df.CabinNb.mask(mask_CabinNbFreq < 10, 99999)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding categorical columns","metadata":{}},{"cell_type":"code","source":"## arranging columns of the dataframe\nnum_columns = ['Age', 'Fare', 'SibSp', 'Parch', 'FamilySize', 'nMissing']\ncat_columns = ['Pclass', 'Sex', 'Embarked', 'Deck', 'TravelsAlone', 'TravelsTwo', 'TicketLetters', 'TicketNumber', 'CabinNb']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a defaultdict to help on label encoding\ndict_label_encoding = defaultdict(LabelEncoder)\n\n# applying the label encoder to each of the categorical columns\ndf[cat_columns] = df[cat_columns].apply(lambda x: dict_label_encoding[x.name].fit_transform(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting the data","metadata":{}},{"cell_type":"code","source":"# arranging the columns\ndf = df[num_columns + cat_columns + ['Survived']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# index of the categorical features\ncat_indexes = [column_index for column_index, column_name in enumerate(df.columns) if column_name in cat_columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dimensions of each categorical feature\ncat_dimensions = df[cat_columns].nunique().tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# defining the embedding dimensions\ncat_embedding_dimensions = np.ceil(np.log(cat_dimensions)).astype(np.int)\n\n# ensuring the embedding dimensions are greater than 1 and smaller than 50 \ncat_embedding_dimensions = np.clip(cat_embedding_dimensions, a_min = 1, a_max = 50).tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# separating the train and test data\ntrain_df, test_df = df[:train.shape[0]], df[train.shape[0]:]\n\n# separating inputs and outputs for the training data\nX_train_df, y_train = train_df.drop(columns='Survived'), train_df.Survived\n\n# parsing y_train to a numpy array\ny_train = LabelEncoder().fit_transform(y_train)\n\n# separating inputs and outputs for the training data\nX_test_df = test_df.drop(columns='Survived')\n\n# parsing the test dataframe to a numpy array\nX_test_array = X_test_df.to_numpy()\n\n# separating the targets from the lr and tabnet submission\ny_lr_submission = LabelEncoder().fit_transform(lr_submission.Survived)\ny_tabnet_submission = LabelEncoder().fit_transform(tabnet_submission.Survived)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Optimization","metadata":{}},{"cell_type":"code","source":"# setting up the data to use\nX = X_train_df\ny = y_train\nkfolds = 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a study\nstudy = optuna.create_study(directions = ['maximize'], pruner = optuna.pruners.MedianPruner())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# running the study\nstudy.optimize(func = objective, n_trials = 100, timeout = 60 * 60 * 7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploring hyperparameter combinations","metadata":{}},{"cell_type":"code","source":"plot_optimization_history(study)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_param_importances(study)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_parallel_coordinate(study)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# study results\nstudy.trials_dataframe().sort_values('value', ascending = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving study results","metadata":{}},{"cell_type":"code","source":"study.trials_dataframe().to_csv('study_trials.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fitting the best model","metadata":{}},{"cell_type":"code","source":"# extracting the best hyperparameters\nbest_hyperparameters = study.best_trials[0].params\nprint(f'Best hyperparameter combination evaluated: {best_hyperparameters}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fitting the model on the best combination of hyperparameters\nlast_scores, probas, thrs = submit_stratifield_kfold(X, y, kfolds = kfolds, hyperparameters = best_hyperparameters, should_pretrain = best_hyperparameters['pretrain_tabnet'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adding predictions to the submission file","metadata":{}},{"cell_type":"code","source":"# adding predictions to the submission dataframe\nsubmission['Survived'] = ((probas / kfolds) >= np.mean(thrs)).astype('int')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving predictions","metadata":{}},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}