{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport optuna\n\nimport random\n\nimport warnings\n\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:300%\">Level1 Classification</h2>","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">XGBOOST</h2>","metadata":{}},{"cell_type":"code","source":"train1=pd.read_csv('../input/training-apr/train.csv')\ntest1=pd.read_csv('../input/training-apr/test.csv')\n\ntrain2=train1.copy()\ntest2=test1.copy()\n\ntrain1.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_cat'], inplace=True)\ntest1.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_cat'], inplace=True)\n\ntrain2.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_log'], inplace=True)\ntest2.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_log'], inplace=True)\n\nprint(\"set 1:\", train1.columns, test1.columns)\nprint(\"set 2:\", train2.columns, test2.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"is finite set1 Train:{} Test:{}\".format(np.isfinite(train1[['Age','Fare_log','related_log']]).any()[0],np.isfinite(test1[['Age','Fare_log','related_log']]).any()[0]))\nprint(\"is finite set2 Train:{} Test:{}\".format(np.isfinite(train2[['Age','Fare_log']]).any()[0],np.isfinite(test2[['Age','Fare_log']]).any()[0]))\n\nprint(\"is nan set1 Train:{} Test:{}\".format(sum(train1.isnull().sum()),sum(test1.isnull().sum())))\nprint(\"is nan set2 Train:{} Test:{}\".format(sum(train2.isnull().sum()),sum(train2.isnull().sum())))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_train=pd.DataFrame(columns=['xg','rf','lg','cb']) # train data\nscore_test=pd.DataFrame(columns=['xg','rf','lg','cb']) # leadeboard data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">XGBOOST set1</h2>","metadata":{}},{"cell_type":"code","source":"ohe=OneHotEncoder()\ncol=['Sex','Embarked']\nohe.fit(train1[col])\nprint(ohe.get_feature_names(col))\ndf1=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(train1[col]).toarray())\ndf2=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(test1[col]).toarray())\n\ntrain1=train1.join(df1)\ntest1=test1.join(df2)\n\ntrain1.drop(columns=['Sex','Embarked'], inplace=True)\ntest1.drop(columns=['Sex','Embarked'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds=5\nSEED=random.randint(937,8641)\n\nkf=StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\n\nfeatures=train1.columns[1:]\nX = train1[features]\ny = train1['Survived']\n\n\nimbalanced_ratio=(train1[train1['Survived']==0]['Survived'].count()/train1[train1['Survived']==1]['Survived'].count()).round(2)\nprint(\"Imbalnce ratio: {:}\".format(imbalanced_ratio))\n# x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=SEED)\n\n# print(\"Distribution of train and test:\", len(x_train), len(y_train), len(x_test), len(x_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Trial=0\ndef objective(trial, x=x_train, y=y_train):\n    global Trial\n    \n    para={\n        'verbosity': 1,\n        'objective': 'binary:logistic',\n        'random_state': SEED,\n        'seed': SEED,\n        'tree_method':'hist',\n        'scale_pos_weight': imbalanced_ratio,\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1),\n        'subsample': trial.suggest_float('subsample', 0.1, 1),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1),\n        'n_estimators': trial.suggest_int('n_estimators',500, 20000),\n        'max_depth': trial.suggest_int('max_depth', 1, 31),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 1000)\n    }\n    \n    print(\"--------------------> Trial {} <--------------------\".format(Trial))\n    Trial=Trial + 1\n    xgboost_train_preds = np.zeros(len(y),)\n    for fold, (train_ind, val_ind) in enumerate(kf.split(x, y)):\n        print(\"--> Fold {}\".format(fold + 1))\n        xtrain, xval = x.iloc[train_ind], x.iloc[val_ind]\n        ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n        xgboost = XGBClassifier(**para)\n        \n        model =  xgboost.fit(xtrain, ytrain, eval_set=[(xtrain,ytrain), (xval,yval)], \n                             eval_metric=[\"error\", \"logloss\"],verbose=0, early_stopping_rounds=50)\n        pred_train = model.predict(xtrain)\n        pred_val = model.predict(xval)\n        xgboost_train_preds[val_ind]=pred_val\n        score1 = accuracy_score(ytrain, pred_train)\n        score2 = accuracy_score(yval, pred_val)\n        print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n        \n        results = model.evals_result()\n        df=pd.DataFrame({\n                        \"validation_train_ll\":results[\"validation_0\"][\"logloss\"],\n                        \"validation_test_ll\":results[\"validation_1\"][\"logloss\"],\n                        \"validation_train_acc\":results[\"validation_0\"][\"error\"],\n                        \"validation_test_acc\":results[\"validation_1\"][\"error\"],\n                        \n        })\n        df['validation_train_acc']=(1-df['validation_train_acc'])*100.0\n        df['validation_test_acc']=(1-df['validation_test_acc'])*100.0\n#         print(df.head())\n        \n        fig, ax = plt.subplots(ncols=2, figsize=(12,6))\n        sns.lineplot(data=df, x=df.index, y=\"validation_train_ll\", ax=ax[0], label=\"Train loss\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_test_ll\", ax=ax[0], label=\"Test loss\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_train_acc\", ax=ax[1], label=\"Train acc.\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_test_acc\", ax=ax[1], label=\"Test acc.\")\n        ax[0].set_title(\"Loss curve\")\n        ax[1].set_title(\"Accuracy curve\")\n        ax[0].set_ylabel(\"Loss\")\n        ax[0].set_xlabel(\"Itertation\")\n        ax[1].set_ylabel(\"Accuracy\")\n        ax[1].set_xlabel(\"Itertation\")\n        fig.suptitle(\"XGBoost Loss/Accuracy.\")\n        plt.show()\n    acc=accuracy_score(y, xgboost_train_preds)\n    print('OOF ACCURACY: {}'.format(acc))\n    return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study=optuna.create_study(study_name=\"XGBoost set 1 Optimization\", direction='maximize')\nstudy.optimize(objective, n_trials=25)\n\ntrial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))\n\nprint(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tuning keeping the N-estimator same as the above best parameter\nTrial=0\ndef objective(trial, x=x_train, y=y_train):\n    global Trial\n    \n    para={\n        'verbosity': 1,\n        'objective': 'binary:logistic',\n        'random_state': SEED,\n        'seed': SEED,\n        'tree_method':'hist',\n        'scale_pos_weight': imbalanced_ratio,\n        'n_estimators': 7657,\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1),\n        'subsample': trial.suggest_float('subsample', 0.1, 1),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1),\n        'max_depth': trial.suggest_int('max_depth', 1, 31),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 1000)\n    }\n    \n    print(\"--------------------> Trial {} <--------------------\".format(Trial))\n    Trial=Trial + 1\n    xgboost_train_preds = np.zeros(len(y),)\n    for fold, (train_ind, val_ind) in enumerate(kf.split(x, y)):\n        print(\"--> Fold {}\".format(fold + 1))\n        xtrain, xval = x.iloc[train_ind], x.iloc[val_ind]\n        ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n        xgboost = XGBClassifier(**para)\n        \n        model =  xgboost.fit(xtrain, ytrain, eval_set=[(xtrain,ytrain), (xval,yval)], \n                             eval_metric=[\"error\", \"logloss\"],verbose=0, early_stopping_rounds=50)\n        pred_train = model.predict(xtrain)\n        pred_val = model.predict(xval)\n        xgboost_train_preds[val_ind]=pred_val\n        score1 = accuracy_score(ytrain, pred_train)\n        score2 = accuracy_score(yval, pred_val)\n        print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n        \n        results = model.evals_result()\n        df=pd.DataFrame({\n                        \"validation_train_ll\":results[\"validation_0\"][\"logloss\"],\n                        \"validation_test_ll\":results[\"validation_1\"][\"logloss\"],\n                        \"validation_train_acc\":results[\"validation_0\"][\"error\"],\n                        \"validation_test_acc\":results[\"validation_1\"][\"error\"],\n                        \n        })\n        df['validation_train_acc']=(1-df['validation_train_acc'])*100.0\n        df['validation_test_acc']=(1-df['validation_test_acc'])*100.0\n#         print(df.head())\n        \n        fig, ax = plt.subplots(ncols=2, figsize=(12,6))\n        sns.lineplot(data=df, x=df.index, y=\"validation_train_ll\", ax=ax[0], label=\"Train loss\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_test_ll\", ax=ax[0], label=\"Test loss\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_train_acc\", ax=ax[1], label=\"Train acc.\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_test_acc\", ax=ax[1], label=\"Test acc.\")\n        ax[0].set_title(\"Loss curve\")\n        ax[1].set_title(\"Accuracy curve\")\n        ax[0].set_ylabel(\"Loss\")\n        ax[0].set_xlabel(\"Itertation\")\n        ax[1].set_ylabel(\"Accuracy\")\n        ax[1].set_xlabel(\"Itertation\")\n        fig.suptitle(\"XGBoost Loss/Accuracy.\")\n        plt.show()\n    acc=accuracy_score(y, xgboost_train_preds)\n    print('OOF ACCURACY: {}'.format(acc))\n    return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study=optuna.create_study(study_name=\"XGBoost Optimization\", direction='maximize')\nstudy.optimize(objective, n_trials=15)\n\ntrial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))\n\nprint(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">XGBOOST tuned model set1</h2>","metadata":{}},{"cell_type":"code","source":"para = {\n        'verbosity': 1,\n        'objective': 'binary:logistic',\n        'random_state': SEED,\n        'seed': SEED,\n        'tree_method':'hist',\n        'scale_pos_weight': imbalanced_ratio,\n        'lambda': 0.040336438299178316, \n        'alpha': 1.6115451006296893, \n        'colsample_bytree': 0.6421153349186888, \n        'subsample': 0.9948174596370332, \n        'learning_rate': 0.08617507766633564, \n        'n_estimators': 7657, \n        'max_depth': 18, \n        'min_child_weight': 114\n      }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xg_train_preds = np.zeros(len(y),)\nxg_test = np.zeros(len(test1),)\nfor fold, (train_ind, val_ind) in enumerate(kf.split(X, y)):\n    print(\"--> Fold {}\".format(fold + 1))\n    \n    xtrain, xval = X.iloc[train_ind], X.iloc[val_ind]\n    ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n    \n    xgboost = XGBClassifier(**para)\n\n    model = xgboost.fit(xtrain, ytrain, eval_set=[(xtrain,ytrain), (xval,yval)], \n                         eval_metric=[\"error\", \"logloss\"], verbose=0, early_stopping_rounds=50)\n    pred_train = model.predict_proba(xtrain)[:,1]\n    pred_val = model.predict_proba(xval)[:,1]\n    xg_train_preds[val_ind] = pred_val\n#     xg_test_preds += (model.predict_proba(x_test)[:,1])/folds\n    xg_test += (model.predict_proba(test1)[:,1])/folds\n    score1 = accuracy_score(ytrain, np.where(pred_train<=0.5, 0, 1))\n    score2 = accuracy_score(yval, np.where(pred_val<=0.5, 0, 1))\n    print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n\n    results = model.evals_result()\n    df=pd.DataFrame({\n                    \"validation_train_ll\":results[\"validation_0\"][\"logloss\"],\n                    \"validation_test_ll\":results[\"validation_1\"][\"logloss\"],\n                    \"validation_train_acc\":results[\"validation_0\"][\"error\"],\n                    \"validation_test_acc\":results[\"validation_1\"][\"error\"],\n\n    })\n    df['validation_train_acc']=(1-df['validation_train_acc'])*100.0\n    df['validation_test_acc']=(1-df['validation_test_acc'])*100.0\n    #         print(df.head())\n\n    fig, ax = plt.subplots(ncols=2, figsize=(12,6))\n    sns.lineplot(data=df, x=df.index, y=\"validation_train_ll\", ax=ax[0], label=\"Train loss\")\n    sns.lineplot(data=df, x=df.index, y=\"validation_test_ll\", ax=ax[0], label=\"Test loss\")\n    sns.lineplot(data=df, x=df.index, y=\"validation_train_acc\", ax=ax[1], label=\"Train acc.\")\n    sns.lineplot(data=df, x=df.index, y=\"validation_test_acc\", ax=ax[1], label=\"Test acc.\")\n    ax[0].set_title(\"Loss curve\")\n    ax[1].set_title(\"Accuracy curve\")\n    ax[0].set_ylabel(\"Loss\")\n    ax[0].set_xlabel(\"Itertation\")\n    ax[1].set_ylabel(\"Accuracy\")\n    ax[1].set_xlabel(\"Itertation\")\n    fig.suptitle(\"XGBoost Loss/Accuracy.\")\n    plt.show()\n    \nacc1 = accuracy_score(y, np.where(xg_train_preds<=0.5, 0, 1))\n# acc2 = accuracy_score(y_test, np.where(xg_test_preds<=0.5, 0, 1))\nprint('OOF ACCURACY Train: {}'.format(acc1))\n\nscore_train['xg'] = xg_train_preds\nscore_test['xg'] = xg_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_train.to_csv('./score_train.csv',index=False)\nscore_test.to_csv('./score_test.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_=pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\ndf=pd.DataFrame()\ndf['PassengerId']=test_['PassengerId'].values\ndf['Survived']=score_test['xg']\ndf['Survived']=df['Survived'].apply(lambda x:0 if x<=0.5 else 1)\ndf.to_csv('./xg_tuned.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">XGBOOST set2</h2>","metadata":{}},{"cell_type":"code","source":"ohe=OneHotEncoder()\ncol=['Sex','Embarked']\nohe.fit(train2[col])\nprint(ohe.get_feature_names(col))\ndf1=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(train2[col]).toarray())\ndf2=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(test2[col]).toarray())\n\ntrain2=train2.join(df1)\ntest2=test2.join(df2)\n\ntrain2['related_cat'] = train2['related_cat'].apply(lambda x:1 if x in ['low'] else 2)\ntest2['related_cat'] = test2['related_cat'].apply(lambda x:1 if x in ['low'] else 2)\n\ntrain2.drop(columns=['Sex','Embarked'], inplace=True)\ntest2.drop(columns=['Sex','Embarked'], inplace=True)","metadata":{"_kg_hide-output":false,"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds=5\nSEED=random.randint(937,8641)\n\nkf=StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\n\nfeatures=train2.columns[1:]\nX = train2[features]\ny = train2['Survived']\n\n\nimbalanced_ratio=(train2[train2['Survived']==0]['Survived'].count()/train2[train2['Survived']==1]['Survived'].count()).round(2)\nprint(\"Imbalnce ratio: {:}\".format(imbalanced_ratio))\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=SEED)\nprint(\"Distribution of train and test:\", len(x_train), len(y_train), len(x_test), len(x_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Trial=0\ndef objective(trial, x=x_train, y=y_train):\n    global Trial\n    \n    para={\n        'verbosity': 1,\n        'objective': 'binary:logistic',\n        'random_state': SEED,\n        'seed': SEED,\n        'tree_method':'hist',\n        'scale_pos_weight': imbalanced_ratio,\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1),\n        'subsample': trial.suggest_float('subsample', 0.1, 1),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1),\n        'n_estimators': trial.suggest_int('n_estimators',500, 20000),\n        'max_depth': trial.suggest_int('max_depth', 1, 31),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 1000)\n    }\n    \n    print(\"--------------------> Trial {} <--------------------\".format(Trial))\n    Trial=Trial + 1\n    xgboost_train_preds = np.zeros(len(y),)\n    for fold, (train_ind, val_ind) in enumerate(kf.split(x, y)):\n        callbacks=[early_stopping_round]\n        print(\"--> Fold {}\".format(fold + 1))\n        xtrain, xval = x.iloc[train_ind], x.iloc[val_ind]\n        ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n        xgboost = XGBClassifier(**para)\n        \n        model =  xgboost.fit(xtrain, ytrain, eval_set=[(xtrain,ytrain), (xval,yval)], \n                             eval_metric=[\"error\", \"logloss\"],verbose=0,callbacks=[early_stopping_round])\n        pred_train = model.predict(xtrain)\n        pred_val = model.predict(xval)\n        xgboost_train_preds[val_ind]=pred_val\n        score1 = accuracy_score(ytrain, pred_train)\n        score2 = accuracy_score(yval, pred_val)\n        print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n        \n        results = model.evals_result()\n        df=pd.DataFrame({\n                        \"validation_train_ll\":results[\"validation_0\"][\"logloss\"],\n                        \"validation_test_ll\":results[\"validation_1\"][\"logloss\"],\n                        \"validation_train_acc\":results[\"validation_0\"][\"error\"],\n                        \"validation_test_acc\":results[\"validation_1\"][\"error\"],\n                        \n        })\n        df['validation_train_acc']=(1-df['validation_train_acc'])*100.0\n        df['validation_test_acc']=(1-df['validation_test_acc'])*100.0\n#         print(df.head())\n        \n        fig, ax = plt.subplots(ncols=2, figsize=(12,6))\n        sns.lineplot(data=df, x=df.index, y=\"validation_train_ll\", ax=ax[0], label=\"Train loss\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_test_ll\", ax=ax[0], label=\"Test loss\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_train_acc\", ax=ax[1], label=\"Train acc.\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_test_acc\", ax=ax[1], label=\"Test acc.\")\n        ax[0].set_title(\"Loss curve\")\n        ax[1].set_title(\"Accuracy curve\")\n        ax[0].set_ylabel(\"Loss\")\n        ax[0].set_xlabel(\"Itertation\")\n        ax[1].set_ylabel(\"Accuracy\")\n        ax[1].set_xlabel(\"Itertation\")\n        fig.suptitle(\"XGBoost Loss/Accuracy.\")\n        plt.show()\n    acc=accuracy_score(y, xgboost_train_preds)\n    print('OOF ACCURACY: {}'.format(acc))\n    return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study=optuna.create_study(study_name=\"XGBoost set 2 Optimization\", direction='maximize')\nstudy.optimize(objective, n_trials=50)\n\ntrial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))\n\nprint(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">XGBOOST set2 tuned model</h2>","metadata":{}},{"cell_type":"code","source":"para = {\n        'verbosity': 1,\n        'objective': 'binary:logistic',\n        'random_state': SEED,\n        'seed': SEED,\n        'tree_method':'hist',\n        'scale_pos_weight': imbalanced_ratio,\n        'lambda': 0.23870865587316725, \n        'alpha': 0.05851635206035666, \n        'colsample_bytree': 0.1050482977664344, \n        'subsample': 0.9719852687976757, \n        'learning_rate': 0.06744568400126143, \n        'n_estimators': 19537, 'max_depth': 3, \n        'min_child_weight': 498\n       }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xg_train_preds = np.zeros(len(y_train),)\nxg_test_preds = np.zeros(len(y_test),)\nxg_test = np.zeros(len(test2),)\nfor fold, (train_ind, val_ind) in enumerate(kf.split(x_train, y_train)):\n    print(\"--> Fold {}\".format(fold + 1))\n    \n    xtrain, xval = x_train.iloc[train_ind], x_train.iloc[val_ind]\n    ytrain, yval = y_train.iloc[train_ind], y_train.iloc[val_ind]\n    \n    xgboost = XGBClassifier(**para)\n\n    model = xgboost.fit(xtrain, ytrain, eval_set=[(xtrain,ytrain), (xval,yval)], \n                         eval_metric=[\"error\", \"logloss\"], verbose=0, early_stopping_rounds=50)\n    pred_train = model.predict_proba(xtrain)[:,1]\n    pred_val = model.predict_proba(xval)[:,1]\n    xg_train_preds[val_ind] = pred_val\n    xg_test_preds += (model.predict_proba(x_test)[:,1])/folds\n    xg_test += (model.predict_proba(test2)[:,1])/folds\n    score1 = accuracy_score(ytrain, np.where(pred_train<=0.5, 0, 1))\n    score2 = accuracy_score(yval, np.where(pred_val<=0.5, 0, 1))\n    print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n\n    results = model.evals_result()\n    df=pd.DataFrame({\n                    \"validation_train_ll\":results[\"validation_0\"][\"logloss\"],\n                    \"validation_test_ll\":results[\"validation_1\"][\"logloss\"],\n                    \"validation_train_acc\":results[\"validation_0\"][\"error\"],\n                    \"validation_test_acc\":results[\"validation_1\"][\"error\"],\n\n    })\n    df['validation_train_acc']=(1-df['validation_train_acc'])*100.0\n    df['validation_test_acc']=(1-df['validation_test_acc'])*100.0\n    #         print(df.head())\n\n    fig, ax = plt.subplots(ncols=2, figsize=(12,6))\n    sns.lineplot(data=df, x=df.index, y=\"validation_train_ll\", ax=ax[0], label=\"Train loss\")\n    sns.lineplot(data=df, x=df.index, y=\"validation_test_ll\", ax=ax[0], label=\"Test loss\")\n    sns.lineplot(data=df, x=df.index, y=\"validation_train_acc\", ax=ax[1], label=\"Train acc.\")\n    sns.lineplot(data=df, x=df.index, y=\"validation_test_acc\", ax=ax[1], label=\"Test acc.\")\n    ax[0].set_title(\"Loss curve\")\n    ax[1].set_title(\"Accuracy curve\")\n    ax[0].set_ylabel(\"Loss\")\n    ax[0].set_xlabel(\"Itertation\")\n    ax[1].set_ylabel(\"Accuracy\")\n    ax[1].set_xlabel(\"Itertation\")\n    fig.suptitle(\"XGBoost Loss/Accuracy.\")\n    plt.show()\n    \nacc1 = accuracy_score(y_train, np.where(xg_train_preds<=0.5, 0, 1))\nacc2 = accuracy_score(y_test, np.where(xg_test_preds<=0.5, 0, 1))\nprint('OOF ACCURACY Train: {} Test: {}'.format(acc1, acc2))\n\n# score_train['xg']=np.concatenate([xg_train_preds,xg_test_preds])\n# score_test['xg']=xg_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_=pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\ndf=pd.DataFrame()\ndf['PassengerId']=test_['PassengerId'].values\ndf['Survived']=xg_test\ndf['Survived']=df['Survived'].apply(lambda x:0 if x<=0.5 else 1)\ndf.to_csv('./xg_tuned_set2.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"background-color:orange; font-size:150%\">**Observation**</span>\n* For XGBOOST train set 1 gave better results in public leaderboard compared to train set 2.  ","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">LightGBM</h2>","metadata":{}},{"cell_type":"code","source":"train1=pd.read_csv('../input/training-apr/train.csv')\ntest1=pd.read_csv('../input/training-apr/test.csv')\n\ntrain2=train1.copy()\ntest2=test1.copy()\n\ntrain1.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_cat'], inplace=True)\ntest1.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_cat'], inplace=True)\n\ntrain2.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_log'], inplace=True)\ntest2.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_log'], inplace=True)\n\nprint(\"set 1:\", train1.columns, test1.columns)\nprint(\"set 2:\", train2.columns, test2.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"background-color:orange; font-size:150%\">**Important Point**</span>\n* The cat features of lightgbm should only be used when you have high cardinality in categorical features. \n* It is common to represent categorical features with one-hot encoding, but this approach is suboptimal for tree learners. Particularly for high-cardinality categorical features, a tree built on one-hot features tends to be unbalanced and needs to grow very deep to achieve good accuracy.Instead of one-hot encoding, the optimal solution is to split on a categorical feature by partitioning its categories into 2 subsets. If the feature has k categories, there are 2^(k-1) - 1 possible partitions. But there is an efficient solution for regression trees. It needs about O(k * log(k)) to find the optimal partition.","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">LightGBM set1</h2>","metadata":{}},{"cell_type":"code","source":"ohe=OneHotEncoder()\ncol=['Sex','Embarked']\nohe.fit(train1[col])\nprint(ohe.get_feature_names(col))\ndf1=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(train1[col]).toarray())\ndf2=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(test1[col]).toarray())\n\ntrain1=train1.join(df1)\ntest1=test1.join(df2)\n\ntrain1.drop(columns=['Sex','Embarked'], inplace=True)\ntest1.drop(columns=['Sex','Embarked'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds=5\nSEED=random.randint(937,8641)\n\nkf=StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\n\nfeatures=train1.columns[1:]\nX = train1[features]\ny = train1['Survived']\n\n\nimbalanced_ratio=(train1[train1['Survived']==0]['Survived'].count()/train1[train1['Survived']==1]['Survived'].count()).round(2)\nprint(\"Imbalnce ratio: {:}\".format(imbalanced_ratio))\n# x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=SEED)\n\n# print(\"Distribution of train and test:\", len(x_train), len(y_train), len(x_test), len(x_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Trial=0\n\ndef objective(trial, x=x_train, y=y_train):\n    global Trial\n    \n    para = {\n              'verbosity': 1,\n              'random_state': SEED,\n              'n_jobs': -1,\n              'is_unbalance': True,\n              'bagging_seed': SEED,\n              'feature_fraction_seed': SEED,\n              'objective': 'binary', \n              'boosting': trial.suggest_categorical('boosting', ['gbdt','rf']),\n              'n_estimators': trial.suggest_int('n_estimators',500, 20000),\n              'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1),\n              'max_depth': trial.suggest_int('max_depth', 6, 127),\n              'num_leaves': trial.suggest_int('num_leaves', 31, 128),\n              'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0),\n              'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0),\n              'feature_fraction': trial.suggest_float('feature_fraction', 0.2, 0.9),\n              'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n              'bagging_freq': trial.suggest_int('bagging_freq', 50, 15000),\n              'bagging_fraction': trial.suggest_float('bagging_fraction', 0, 0.9),\n              'max_bin': trial.suggest_int('max_bin', 128, 1024)\n            }\n    \n    print(\"--------------------> Trial {} <--------------------\".format(Trial))\n    Trial=Trial + 1\n    lgbm_train_preds = np.zeros(len(y),)\n    \n    for fold, (train_ind, val_ind) in enumerate(kf.split(x, y)):\n        print(\"--> Fold {}\".format(fold + 1))\n        xtrain, xval = x.iloc[train_ind], x.iloc[val_ind]\n        ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n        \n        lgbm = LGBMClassifier(**para)\n        \n        early_stopping = lightgbm.early_stopping(25, first_metric_only=True, verbose=True)\n        \n        model =  lgbm.fit(xtrain, ytrain, eval_set=[(xtrain,ytrain), (xval,yval)], categorical_feature=None,\n                          eval_metric = ['binary_logloss', 'binary_error'], verbose=100, callbacks=[early_stopping])\n        \n        pred_train = model.predict(xtrain, num_iteration=model.best_iteration_)\n        pred_val = model.predict(xval, num_iteration=model.best_iteration_)\n        lgbm_train_preds[val_ind]=pred_val\n        score1 = accuracy_score(ytrain, pred_train)\n        score2 = accuracy_score(yval, pred_val)\n        print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n        \n        results = model.evals_result_\n        df=pd.DataFrame({\n                        \"train_ll\":results[\"training\"][\"binary_logloss\"],\n                        \"validation_ll\":results[\"valid_1\"][\"binary_logloss\"],\n                        \"train_acc\":results[\"training\"][\"binary_error\"],\n                        \"test_acc\":results[\"valid_1\"][\"binary_error\"],\n        })\n        df['train_acc']=(1-df['train_acc'])*100.0\n        df['test_acc']=(1-df['test_acc'])*100.0\n#         print(df.head())\n        \n        fig, ax = plt.subplots(ncols=2, figsize=(12,6))\n        sns.lineplot(data=df, x=df.index, y=\"train_ll\", ax=ax[0], label=\"Train loss\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_ll\", ax=ax[0], label=\"Validation loss\")\n        sns.lineplot(data=df, x=df.index, y=\"train_acc\", ax=ax[1], label=\"Train acc.\")\n        sns.lineplot(data=df, x=df.index, y=\"test_acc\", ax=ax[1], label=\"Validation acc.\")\n        ax[0].set_title(\"Loss curve\")\n        ax[1].set_title(\"Accuracy curve\")\n        ax[0].set_ylabel(\"Loss\")\n        ax[0].set_xlabel(\"Itertation\")\n        ax[1].set_ylabel(\"Accuracy\")\n        ax[1].set_xlabel(\"Itertation\")\n        fig.suptitle(\"LightGBM Loss/Accuracy.\")\n        plt.show()\n    acc=accuracy_score(y, lgbm_train_preds)\n    print('OOF ACCURACY: {}'.format(acc))\n    return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study=optuna.create_study(study_name=\"LightGBM set 1 Optimization\", direction='maximize')\nstudy.optimize(objective, n_trials=25)\n\ntrial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))\n\nprint(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">LightGBM set1 tuned model</h2>","metadata":{}},{"cell_type":"code","source":"para = {\n        'verbosity': 1,\n        'random_state': SEED,\n        'n_jobs': -1,\n        'is_unbalance': True,\n        'bagging_seed': SEED,\n        'feature_fraction_seed': SEED,\n        'objective': 'binary',\n        'boosting': 'gbdt', \n        'n_estimators': 12792, \n        'learning_rate': 0.0998109583959103, \n        'max_depth': 67, \n        'num_leaves': 81, \n        'reg_alpha': 8.95628715211493, \n        'reg_lambda': 2.5201711907717668, \n        'feature_fraction': 0.591553893731912, \n        'min_child_samples': 206, \n        'bagging_freq': 2628, \n        'bagging_fraction': 0.661876388933661, \n        'max_bin': 230\n       }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lg_train_preds = np.zeros(len(y),)\nlg_test = np.zeros(len(test1),)\nfor fold, (train_ind, val_ind) in enumerate(kf.split(X, y)):\n    print(\"--> Fold {}\".format(fold + 1))\n    \n    xtrain, xval = X.iloc[train_ind], X.iloc[val_ind]\n    ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n    \n    early_stopping_round = lightgbm.early_stopping(25, first_metric_only=True, verbose=True)\n    \n    lgbm = LGBMClassifier(**para)\n        \n    model =  lgbm.fit(xtrain, ytrain, eval_set=[(xtrain,ytrain), (xval,yval)], categorical_feature=None,\n                      eval_metric = ['binary_logloss', 'binary_error'], verbose=100, callbacks=[early_stopping_round])\n    pred_train = model.predict_proba(xtrain, num_iteration=model.best_iteration_)[:,1]\n    pred_val = model.predict_proba(xval, num_iteration=model.best_iteration_)[:,1]\n    lg_train_preds[val_ind] = pred_val\n    lg_test += (model.predict_proba(test1, num_iteration=model.best_iteration_)[:,1])/folds\n    score1 = accuracy_score(ytrain, np.where(pred_train<=0.5, 0, 1))\n    score2 = accuracy_score(yval, np.where(pred_val<=0.5, 0, 1))\n    print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n\n    results = model.evals_result_\n    df=pd.DataFrame({\n                    \"train_ll\":results[\"training\"][\"binary_logloss\"],\n                    \"validation_ll\":results[\"valid_1\"][\"binary_logloss\"],\n                    \"train_acc\":results[\"training\"][\"binary_error\"],\n                    \"test_acc\":results[\"valid_1\"][\"binary_error\"],\n\n    })\n    df['train_acc']=(1-df['train_acc'])*100.0\n    df['test_acc']=(1-df['test_acc'])*100.0\n#         print(df.head())\n\n    fig, ax = plt.subplots(ncols=2, figsize=(12,6))\n    sns.lineplot(data=df, x=df.index, y=\"train_ll\", ax=ax[0], label=\"Train loss\")\n    sns.lineplot(data=df, x=df.index, y=\"validation_ll\", ax=ax[0], label=\"Validation loss\")\n    sns.lineplot(data=df, x=df.index, y=\"train_acc\", ax=ax[1], label=\"Train acc.\")\n    sns.lineplot(data=df, x=df.index, y=\"test_acc\", ax=ax[1], label=\"Validation acc.\")\n    ax[0].set_title(\"Loss curve\")\n    ax[1].set_title(\"Accuracy curve\")\n    ax[0].set_ylabel(\"Loss\")\n    ax[0].set_xlabel(\"Itertation\")\n    ax[1].set_ylabel(\"Accuracy\")\n    ax[1].set_xlabel(\"Itertation\")\n    fig.suptitle(\"LightGBM Loss/Accuracy.\")\n    plt.show()\n    \nacc1 = accuracy_score(y, np.where(lg_train_preds<=0.5, 0, 1))\n# acc2 = accuracy_score(y_test, np.where(lg_test_preds<=0.5, 0, 1))\n\nprint('OOF ACCURACY Train: {}'.format(acc1))\n\n# score_train=pd.read_csv('../input/score-tab-apr21/score_train.csv')\n# score_test=pd.read_csv('../input/score-tab-apr21/score_test.csv')\n\nscore_train['lg']=lg_train_preds\nscore_test['lg']=lg_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_train.to_csv('./score_train.csv',index=False)\nscore_test.to_csv('./score_test.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_=pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\ndf=pd.DataFrame()\ndf['PassengerId']=test_['PassengerId'].values\ndf['Survived']=lg_test\ndf['Survived']=df['Survived'].apply(lambda x:0 if x<=0.5 else 1)\ndf.to_csv('./lg_tuned.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">LightGBM set2</h2>","metadata":{}},{"cell_type":"code","source":"ohe=OneHotEncoder()\ncol=['Sex','Embarked']\nohe.fit(train2[col])\nprint(ohe.get_feature_names(col))\ndf1=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(train2[col]).toarray())\ndf2=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(test2[col]).toarray())\n\ntrain2=train2.join(df1)\ntest2=test2.join(df2)\n\ntrain2['related_cat'] = train2['related_cat'].apply(lambda x:1 if x in ['low'] else 2)\ntest2['related_cat'] = test2['related_cat'].apply(lambda x:1 if x in ['low'] else 2)\n\ntrain2.drop(columns=['Sex','Embarked'], inplace=True)\ntest2.drop(columns=['Sex','Embarked'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds=5\nSEED=random.randint(937,8641)\n\nkf=StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\n\nfeatures=train2.columns[1:]\nX = train2[features]\ny = train2['Survived']\n\n\nimbalanced_ratio=(train2[train2['Survived']==0]['Survived'].count()/train2[train2['Survived']==1]['Survived'].count()).round(2)\nprint(\"Imbalnce ratio: {:}\".format(imbalanced_ratio))\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=SEED)\nprint(\"Distribution of train and test:\", len(x_train), len(y_train), len(x_test), len(x_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Trial=0\n\n\ndef objective(trial, x=x_train, y=y_train):\n    global Trial\n    \n    para = {\n              'verbosity': 1,\n              'random_state': SEED,\n              'n_jobs': -1,\n              'is_unbalance': True,\n              'bagging_seed': SEED,\n              'feature_fraction_seed': SEED,\n              'objective': 'binary',\n              'boosting': trial.suggest_categorical('boosting', ['gbdt','rf']),\n              'n_estimators': trial.suggest_int('n_estimators',500, 20000),\n              'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1),\n              'max_depth': trial.suggest_int('max_depth', 6, 127),\n              'num_leaves': trial.suggest_int('num_leaves', 31, 128),\n              'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0),\n              'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0),\n              'feature_fraction': trial.suggest_float('feature_fraction', 0.2, 0.9),\n              'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n              'bagging_freq': trial.suggest_int('bagging_freq', 50, 15000),\n              'bagging_fraction': trial.suggest_float('bagging_fraction', 0, 0.9),\n              'max_bin': trial.suggest_int('max_bin', 128, 1024)\n            }\n    \n    print(\"--------------------> Trial {} <--------------------\".format(Trial))\n    Trial=Trial + 1\n    lgbm_train_preds = np.zeros(len(y),)\n    for fold, (train_ind, val_ind) in enumerate(kf.split(x, y)):\n        print(\"--> Fold {}\".format(fold + 1))\n        xtrain, xval = x.iloc[train_ind], x.iloc[val_ind]\n        ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n        lgbm = LGBMClassifier(**para)\n        \n        early_stopping_round = lightgbm.early_stopping(25, first_metric_only=True, verbose=True)\n        model =  lgbm.fit(xtrain, ytrain, eval_set=[(xtrain,ytrain), (xval,yval)], categorical_feature=None,\n                             eval_metric = ['binary_logloss', 'binary_error'], verbose=100, callbacks=[early_stopping_round])\n        \n        pred_train = model.predict(xtrain, num_iteration=model.best_iteration_)\n        pred_val = model.predict(xval, num_iteration=model.best_iteration_)\n        lgbm_train_preds[val_ind]=pred_val\n        score1 = accuracy_score(ytrain, pred_train)\n        score2 = accuracy_score(yval, pred_val)\n        print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n        \n        results = model.evals_result_\n        df=pd.DataFrame({\n                        \"train_ll\":results[\"training\"][\"binary_logloss\"],\n                        \"validation_ll\":results[\"valid_1\"][\"binary_logloss\"],\n                        \"train_acc\":results[\"training\"][\"binary_error\"],\n                        \"test_acc\":results[\"valid_1\"][\"binary_error\"],\n                        \n        })\n        df['train_acc']=(1-df['train_acc'])*100.0\n        df['test_acc']=(1-df['test_acc'])*100.0\n#         print(df.head())\n        \n        fig, ax = plt.subplots(ncols=2, figsize=(12,6))\n        sns.lineplot(data=df, x=df.index, y=\"train_ll\", ax=ax[0], label=\"Train loss\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_ll\", ax=ax[0], label=\"Validation loss\")\n        sns.lineplot(data=df, x=df.index, y=\"train_acc\", ax=ax[1], label=\"Train acc.\")\n        sns.lineplot(data=df, x=df.index, y=\"test_acc\", ax=ax[1], label=\"Validation acc.\")\n        ax[0].set_title(\"Loss curve\")\n        ax[1].set_title(\"Accuracy curve\")\n        ax[0].set_ylabel(\"Loss\")\n        ax[0].set_xlabel(\"Itertation\")\n        ax[1].set_ylabel(\"Accuracy\")\n        ax[1].set_xlabel(\"Itertation\")\n        fig.suptitle(\"LightGBM Loss/Accuracy.\")\n        plt.show()\n    acc=accuracy_score(y, lgbm_train_preds)\n    print('OOF ACCURACY: {}'.format(acc))\n    return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study=optuna.create_study(study_name=\"LightGBM set 2 Optimization\", direction='maximize')\nstudy.optimize(objective, n_trials=25)\n\ntrial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))\n\nprint(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">LightGBM set2 tuned model</h2>","metadata":{}},{"cell_type":"code","source":"para = {\n        'verbosity': 1,\n        'random_state': SEED,\n        'n_jobs': -1,\n        'is_unbalance': True,\n        'bagging_seed': SEED,\n        'feature_fraction_seed': SEED,\n        'objective': 'binary',\n        'boosting': 'gbdt', \n        'n_estimators': 10701, \n        'learning_rate': 0.06955447670117614, \n        'max_depth': 65, 'num_leaves': 110, \n        'reg_alpha': 8.557750007560998, \n        'reg_lambda': 0.016304042294640997, \n        'feature_fraction': 0.2779304475599476, \n        'min_child_samples': 233, \n        'bagging_freq': 14930, \n        'bagging_fraction': 0.8838910149186466, \n        'max_bin': 370\n       }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lg_train_preds = np.zeros(len(y_train),)\nlg_test_preds = np.zeros(len(y_test),)\nlg_test = np.zeros(len(test2),)\nfor fold, (train_ind, val_ind) in enumerate(kf.split(x_train, y_train)):\n    print(\"--> Fold {}\".format(fold + 1))\n    \n    xtrain, xval = x_train.iloc[train_ind], x_train.iloc[val_ind]\n    ytrain, yval = y_train.iloc[train_ind], y_train.iloc[val_ind]\n    \n    early_stopping_round = lightgbm.early_stopping(25, first_metric_only=True, verbose=True)\n    \n    lgbm = LGBMClassifier(**para)\n        \n    model =  lgbm.fit(xtrain, ytrain, eval_set=[(xtrain,ytrain), (xval,yval)], categorical_feature=None,\n                      eval_metric = ['binary_logloss', 'binary_error'], verbose=100, callbacks=[early_stopping_round])\n    pred_train = model.predict_proba(xtrain, num_iteration=model.best_iteration_)[:,1]\n    pred_val = model.predict_proba(xval, num_iteration=model.best_iteration_)[:,1]\n    lg_train_preds[val_ind] = pred_val\n    lg_test_preds += (model.predict_proba(x_test, num_iteration=model.best_iteration_)[:,1])/folds\n    lg_test += (model.predict_proba(test2, num_iteration=model.best_iteration_)[:,1])/folds\n    score1 = accuracy_score(ytrain, np.where(pred_train<=0.5, 0, 1))\n    score2 = accuracy_score(yval, np.where(pred_val<=0.5, 0, 1))\n    print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n\n    results = model.evals_result_\n    df=pd.DataFrame({\n                    \"train_ll\":results[\"training\"][\"binary_logloss\"],\n                    \"validation_ll\":results[\"valid_1\"][\"binary_logloss\"],\n                    \"train_acc\":results[\"training\"][\"binary_error\"],\n                    \"test_acc\":results[\"valid_1\"][\"binary_error\"],\n\n    })\n    df['train_acc']=(1-df['train_acc'])*100.0\n    df['test_acc']=(1-df['test_acc'])*100.0\n#         print(df.head())\n\n    fig, ax = plt.subplots(ncols=2, figsize=(12,6))\n    sns.lineplot(data=df, x=df.index, y=\"train_ll\", ax=ax[0], label=\"Train loss\")\n    sns.lineplot(data=df, x=df.index, y=\"validation_ll\", ax=ax[0], label=\"Validation loss\")\n    sns.lineplot(data=df, x=df.index, y=\"train_acc\", ax=ax[1], label=\"Train acc.\")\n    sns.lineplot(data=df, x=df.index, y=\"test_acc\", ax=ax[1], label=\"Validation acc.\")\n    ax[0].set_title(\"Loss curve\")\n    ax[1].set_title(\"Accuracy curve\")\n    ax[0].set_ylabel(\"Loss\")\n    ax[0].set_xlabel(\"Itertation\")\n    ax[1].set_ylabel(\"Accuracy\")\n    ax[1].set_xlabel(\"Itertation\")\n    fig.suptitle(\"LightGBM Loss/Accuracy.\")\n    plt.show()\n    \nacc1 = accuracy_score(y_train, np.where(lg_train_preds<=0.5, 0, 1))\nacc2 = accuracy_score(y_test, np.where(lg_test_preds<=0.5, 0, 1))\n\nprint('OOF ACCURACY Train: {} Test: {}'.format(acc1, acc2))\n\n# score_train=pd.read_csv('../input/score-apr21/score_train.csv')\n# score_test=pd.read_csv('../input/score-apr21/score_test.csv')\n\n# score_train['lg']=np.concatenate([lg_train_preds,lg_test_preds])\n# score_test['lg']=lg_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_=pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\ndf=pd.DataFrame()\ndf['PassengerId']=test_['PassengerId'].values\ndf['Survived']=lg_test\ndf['Survived']=df['Survived'].apply(lambda x:0 if x<=0.5 else 1)\ndf.to_csv('./lg_tuned_set2.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"background-color:orange; font-size:150%\">**Observation**</span>\n* For LightGBM train set 1 gave better results in public leaderboard compared to train set 2.","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">Random Forest</h2>","metadata":{}},{"cell_type":"code","source":"train1=pd.read_csv('../input/training-apr/train.csv')\ntest1=pd.read_csv('../input/training-apr/test.csv')\n\ntrain2=train1.copy()\ntest2=test1.copy()\n\ntrain1.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_cat'], inplace=True)\ntest1.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_cat'], inplace=True)\n\ntrain2.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_log'], inplace=True)\ntest2.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_log'], inplace=True)\n\nprint(\"set 1:\", train1.columns, test1.columns)\nprint(\"set 2:\", train2.columns, test2.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">Random Forest set1</h2>","metadata":{}},{"cell_type":"code","source":"ohe=OneHotEncoder()\ncol=['Sex','Embarked']\nohe.fit(train1[col])\nprint(ohe.get_feature_names(col))\ndf1=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(train1[col]).toarray())\ndf2=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(test1[col]).toarray())\n\ntrain1=train1.join(df1)\ntest1=test1.join(df2)\n\ntrain1.drop(columns=['Sex','Embarked'], inplace=True)\ntest1.drop(columns=['Sex','Embarked'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds=5\nSEED=random.randint(937,8641)\n\nkf=StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\n\nfeatures=train1.columns[1:]\nX = train1[features]\ny = train1['Survived']\n\n\nimbalanced_ratio=(train1[train1['Survived']==0]['Survived'].count()/train1[train1['Survived']==1]['Survived'].count()).round(2)\nprint(\"Imbalnce ratio: {:}\".format(imbalanced_ratio))\n# x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=SEED)\n\n# print(\"Distribution of train and test:\", len(x_train), len(y_train), len(x_test), len(x_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Trial=0\ndef objective(trial, x=x_train, y=y_train):\n    global Trial\n    \n    para={\n        'bootstrap': True,\n        'n_jobs': -1,\n        'verbose': 0,\n        'random_state': SEED,\n        'criterion': 'entropy',\n        'n_estimators': trial.suggest_int('n_estimators',10, 1000),\n        'max_depth': trial.suggest_int('max_depth', 3, 2000),\n        'min_samples_split': trial.suggest_float('min_samples_split', 1e-4, 1e-1),\n        'min_samples_leaf': trial.suggest_float('min_samples_leaf', 1e-4, 1e-1),\n        'max_features': trial.suggest_categorical(\"max_features\", ['sqrt', 'log2']),\n        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 500, 100000),\n        'warm_start': trial.suggest_categorical('warm_start', [True, False]),\n        'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.2),\n        'class_weight':trial.suggest_categorical(\"class_weight\", ['balanced', 'balanced_subsample']),\n        'max_samples': trial.suggest_float('max_samples', 0.5, 0.8)\n    }\n    \n    print(\"--------------------> Trial {} <--------------------\".format(Trial))\n    Trial=Trial + 1\n    rf_train_preds = np.zeros(len(y),)\n    for fold, (train_ind, val_ind) in enumerate(kf.split(x, y)):\n        print(\"--> Fold {}\".format(fold + 1))\n        xtrain, xval = x.iloc[train_ind], x.iloc[val_ind]\n        ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n        \n        rf = RandomForestClassifier(**para)\n\n        model =  rf.fit(xtrain, ytrain)\n        \n        pred_train = model.predict(xtrain)\n        pred_val = model.predict(xval)\n        rf_train_preds[val_ind]=pred_val\n        score1 = accuracy_score(ytrain, pred_train)\n        score2 = accuracy_score(yval, pred_val)\n        print('Fold {} AUC Train: {} Validation: {}'.format(fold+1, score1, score2))\n    \n    acc=accuracy_score(y, rf_train_preds)\n    print('OOF ACCURACY: {}'.format(acc))\n    return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study=optuna.create_study(study_name=\"Random Forest set 1 Optimization\", direction='maximize')\nstudy.optimize(objective, n_trials=25)\n\ntrial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))\n\nprint(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Trial=0\ndef objective(trial, x=x_train, y=y_train):\n    global Trial\n    \n    para={\n        'bootstrap': True,\n        'n_jobs': -1,\n        'verbose': 0,\n        'random_state': SEED,\n        'criterion': 'entropy',\n        'n_estimators': 330, \n        'max_depth': 1417, \n        'min_samples_split': 0.010377003772355852, \n        'min_samples_leaf': 0.01865186498148891, \n        'max_features': 'log2', \n        'max_leaf_nodes': 87127, \n        'warm_start': False, \n        'min_impurity_decrease': 0.01122129357981094, \n        'class_weight': 'balanced_subsample', \n        'max_samples': 0.5985586520207642,\n        'ccp_alpha': trial.suggest_float('ccp_alpha', 0.0, 2e-1)\n    }\n    \n    print(\"--------------------> Trial {} <--------------------\".format(Trial))\n    Trial=Trial + 1\n    rf_train_preds = np.zeros(len(y),)\n    for fold, (train_ind, val_ind) in enumerate(kf.split(x, y)):\n        print(\"--> Fold {}\".format(fold + 1))\n        xtrain, xval = x.iloc[train_ind], x.iloc[val_ind]\n        ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n        \n        rf = RandomForestClassifier(**para)\n\n        model =  rf.fit(xtrain, ytrain)\n        \n        pred_train = model.predict(xtrain)\n        pred_val = model.predict(xval)\n        rf_train_preds[val_ind]=pred_val\n        score1 = accuracy_score(ytrain, pred_train)\n        score2 = accuracy_score(yval, pred_val)\n        print('Fold {} AUC Train: {} Validation: {}'.format(fold+1, score1, score2))\n    \n    acc=accuracy_score(y, rf_train_preds)\n    print('OOF ACCURACY: {}'.format(acc))\n    return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study=optuna.create_study(study_name=\"Random Forest set 1 Pruning\", direction='maximize')\nstudy.optimize(objective, n_trials=25)\n\ntrial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))\n\nprint(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">Random Forest set1 tuned model</h2>","metadata":{}},{"cell_type":"code","source":"para={\n      'n_estimators': 898,\n      'criterion': 'entropy',\n      'max_depth': 8, \n      'max_leaf_nodes': 9058,\n      'min_samples_split': 0.0671810090247945, \n      'min_samples_leaf': 0.04742472303688006, \n      'max_features': 'sqrt', \n      'min_impurity_decrease': 0.00010583321874846287,\n      'bootstrap': True,\n      'n_jobs': -1,\n      'verbose': 0, \n      'class_weight': 'balanced_subsample', \n      'max_samples': 0.8634669615516827,\n      'random_state': SEED\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_train_preds = np.zeros(len(y),)\nrf_test = np.zeros(len(test1),)\nfor fold, (train_ind, val_ind) in enumerate(kf.split(X, y)):\n    print(\"--> Fold {}\".format(fold + 1))\n    \n    xtrain, xval = X.iloc[train_ind], X.iloc[val_ind]\n    ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n    \n    rf = RandomForestClassifier(**para)\n\n    model =  rf.fit(xtrain, ytrain)\n    \n    pred_train = model.predict_proba(xtrain)[:,1]\n    pred_val = model.predict_proba(xval)[:,1]\n    rf_train_preds[val_ind] = pred_val\n    rf_test += (model.predict_proba(test1)[:,1])/folds\n    score1 = accuracy_score(ytrain, np.where(pred_train<=0.5, 0, 1))\n    score2 = accuracy_score(yval, np.where(pred_val<=0.5, 0, 1))\n    print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n    \nacc1 = accuracy_score(y, np.where(rf_train_preds<=0.5, 0, 1))\n# acc2 = accuracy_score(y_test, np.where(rf_test_preds<=0.5, 0, 1))\n\nprint('OOF ACCURACY Train: {}'.format(acc1))\n\n# score_train=pd.read_csv('../input/score-tab-apr21/score_train.csv')\n# score_test=pd.read_csv('../input/score-tab-apr21/score_test.csv')\n\nscore_train['rf']=rf_train_preds\nscore_test['rf']=rf_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_train.to_csv('./score_train.csv',index=False)\nscore_test.to_csv('./score_test.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_=pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\ndf=pd.DataFrame()\ndf['PassengerId']=test_['PassengerId'].values\ndf['Survived']=rf_test\ndf['Survived']=df['Survived'].apply(lambda x:0 if x<=0.5 else 1)\ndf.to_csv('./rf_tuned.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">Random Forest set2</h2>","metadata":{}},{"cell_type":"code","source":"ohe=OneHotEncoder()\ncol=['Sex','Embarked']\nohe.fit(train2[col])\nprint(ohe.get_feature_names(col))\ndf1=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(train2[col]).toarray())\ndf2=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(test2[col]).toarray())\n\ntrain2=train2.join(df1)\ntest2=test2.join(df2)\n\ntrain2['related_cat'] = train2['related_cat'].apply(lambda x:1 if x in ['low'] else 2)\ntest2['related_cat'] = test2['related_cat'].apply(lambda x:1 if x in ['low'] else 2)\n\ntrain2.drop(columns=['Sex','Embarked'], inplace=True)\ntest2.drop(columns=['Sex','Embarked'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds=5\nSEED=random.randint(937,8641)\n\nkf=StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\n\nfeatures=train2.columns[1:]\nX = train2[features]\ny = train2['Survived']\n\n\nimbalanced_ratio=(train2[train2['Survived']==0]['Survived'].count()/train2[train2['Survived']==1]['Survived'].count()).round(2)\nprint(\"Imbalnce ratio: {:}\".format(imbalanced_ratio))\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=SEED)\nprint(\"Distribution of train and test:\", len(x_train), len(y_train), len(x_test), len(x_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Trial=0\ndef objective(trial, x=x_train, y=y_train):\n    global Trial\n    \n    para={\n        'bootstrap': True,\n        'n_jobs': -1,\n        'verbose': 0,\n        'random_state': SEED,\n        'criterion': 'entropy',\n        'n_estimators': trial.suggest_int('n_estimators',10, 1000),\n        'max_depth': trial.suggest_int('max_depth', 3, 2000),\n        'min_samples_split': trial.suggest_float('min_samples_split', 1e-4, 1e-1),\n        'min_samples_leaf': trial.suggest_float('min_samples_leaf', 1e-4, 1e-1),\n        'max_features': trial.suggest_categorical(\"max_features\", ['sqrt', 'log2']),\n        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 500, 100000),\n        'warm_start': trial.suggest_categorical('warm_start', [True, False]),\n        'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.2),\n        'class_weight':trial.suggest_categorical(\"class_weight\", ['balanced', 'balanced_subsample']),\n        'max_samples': trial.suggest_float('max_samples', 0.5, 0.8)\n    }\n    \n    print(\"--------------------> Trial {} <--------------------\".format(Trial))\n    Trial=Trial + 1\n    rf_train_preds = np.zeros(len(y),)\n    for fold, (train_ind, val_ind) in enumerate(kf.split(x, y)):\n        print(\"--> Fold {}\".format(fold + 1))\n        xtrain, xval = x.iloc[train_ind], x.iloc[val_ind]\n        ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n        \n        rf = RandomForestClassifier(**para)\n\n        model =  rf.fit(xtrain, ytrain)\n        \n        pred_train = model.predict(xtrain)\n        pred_val = model.predict(xval)\n        rf_train_preds[val_ind]=pred_val\n        score1 = accuracy_score(ytrain, pred_train)\n        score2 = accuracy_score(yval, pred_val)\n        print('Fold {} AUC Train: {} Validation: {}'.format(fold+1, score1, score2))\n    \n    acc=accuracy_score(y, rf_train_preds)\n    print('OOF ACCURACY: {}'.format(acc))\n    return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study=optuna.create_study(study_name=\"Random Forest set 2 Optimization\", direction='maximize')\nstudy.optimize(objective, n_trials=25)\n\ntrial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))\n\nprint(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">Random Forest set2 tuned model</h2>","metadata":{}},{"cell_type":"code","source":"para = {\n        'bootstrap': True,\n        'n_jobs': -1,\n        'verbose': 0,\n        'random_state': SEED,\n        'criterion': 'entropy',\n        'n_estimators': 137, \n        'max_depth': 1852, \n        'min_samples_split': 0.0852885149102395, \n        'min_samples_leaf': 0.04145631437008027, \n        'max_features': 'sqrt', \n        'max_leaf_nodes': 85221, \n        'warm_start': False, \n        'min_impurity_decrease': 0.004519625185607378, \n        'class_weight': 'balanced', \n        'max_samples': 0.63249673484119\n       }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_train_preds = np.zeros(len(y_train),)\nrf_test_preds = np.zeros(len(y_test),)\nrf_test = np.zeros(len(test2),)\nfor fold, (train_ind, val_ind) in enumerate(kf.split(x_train, y_train)):\n    print(\"--> Fold {}\".format(fold + 1))\n    \n    xtrain, xval = x_train.iloc[train_ind], x_train.iloc[val_ind]\n    ytrain, yval = y_train.iloc[train_ind], y_train.iloc[val_ind]\n    \n    rf = RandomForestClassifier(**para)\n\n    model =  rf.fit(xtrain, ytrain)\n    \n    pred_train = model.predict_proba(xtrain)[:,1]\n    pred_val = model.predict_proba(xval)[:,1]\n    rf_train_preds[val_ind] = pred_val\n    rf_test_preds += (model.predict_proba(x_test)[:,1])/folds\n    rf_test += (model.predict_proba(test2)[:,1])/folds\n    score1 = accuracy_score(ytrain, np.where(pred_train<=0.5, 0, 1))\n    score2 = accuracy_score(yval, np.where(pred_val<=0.5, 0, 1))\n    print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n    \nacc1 = accuracy_score(y_train, np.where(rf_train_preds<=0.5, 0, 1))\nacc2 = accuracy_score(y_test, np.where(rf_test_preds<=0.5, 0, 1))\n\nprint('OOF ACCURACY Train: {} Test: {}'.format(acc1, acc2))\n\n# score_train=pd.read_csv('../input/score-tab-apr21/score_train.csv')\n# score_test=pd.read_csv('../input/score-tab-apr21/score_test.csv')\n\n# score_train['rf']=np.concatenate([rf_train_preds,rf_test_preds])\n# score_test['rf']=rf_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_=pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\ndf=pd.DataFrame()\ndf['PassengerId']=test_['PassengerId'].values\ndf['Survived']=rf_test\ndf['Survived']=df['Survived'].apply(lambda x:0 if x<=0.5 else 1)\ndf.to_csv('./rf_tuned_set2.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">CatBoost</h2>","metadata":{}},{"cell_type":"code","source":"train1=pd.read_csv('../input/training-apr/train.csv')\ntest1=pd.read_csv('../input/training-apr/test.csv')\n\ntrain2=train1.copy()\ntest2=test1.copy()\n\ntrain1.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_cat'], inplace=True)\ntest1.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_cat'], inplace=True)\n\ntrain2.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_log'], inplace=True)\ntest2.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_log'], inplace=True)\n\nprint(\"set 1:\", train1.columns, test1.columns)\nprint(\"set 2:\", train2.columns, test2.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">CatBoost set1</h2>","metadata":{}},{"cell_type":"code","source":"ohe=OneHotEncoder()\ncol=['Sex','Embarked']\nohe.fit(train1[col])\nprint(ohe.get_feature_names(col))\ndf1=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(train1[col]).toarray())\ndf2=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(test1[col]).toarray())\n\ntrain1=train1.join(df1)\ntest1=test1.join(df2)\n\ntrain1.drop(columns=['Sex','Embarked'], inplace=True)\ntest1.drop(columns=['Sex','Embarked'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds=5\nSEED=random.randint(937,8641)\n\nkf=StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\n\nfeatures=train1.columns[1:]\nX = train1[features]\ny = train1['Survived']\n\n\nimbalanced_ratio=(train1[train1['Survived']==0]['Survived'].count()/train1[train1['Survived']==1]['Survived'].count()).round(2)\nprint(\"Imbalnce ratio: {:}\".format(imbalanced_ratio))\n# x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=SEED)\n\n# print(\"Distribution of train and test:\", len(x_train), len(y_train), len(x_test), len(x_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Trial=0\n\ndef objective(trial, x=x_train, y=y_train):\n    global Trial\n    \n    para = {\n            'random_seed': SEED,\n            'loss_function': 'Logloss',\n            'class_weights':[1,1.34],\n            'task_type': 'GPU', \n            'custom_metric': ['Logloss','Accuracy'],\n            'use_best_model': True,\n            'od_pval': 1e-4,\n            'verbose': True,\n            'max_bin': 128,\n            'bootstrap_type': trial.suggest_categorical('bootstrap_type',['Bernoulli','Poisson']),\n            'max_depth': trial.suggest_int('max_depth', 1, 16),\n            'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1),\n            'n_estimators': trial.suggest_int('n_estimators',500, 20000),\n            'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree','Depthwise','Lossguide']),\n            'random_strength': trial.suggest_int('random_strength', 1, 10),\n            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 10000),\n            'score_function': trial.suggest_categorical('score_function', ['Cosine','L2','NewtonCosine','NewtonL2']),\n            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0),\n            'subsample': trial.suggest_float('subsample', 0.5, 0.85)\n            }\n    \n    print(\"--------------------> Trial {} <--------------------\".format(Trial))\n    Trial=Trial + 1\n    cb_train_preds = np.zeros(len(y),)\n    \n    for fold, (train_ind, val_ind) in enumerate(kf.split(x, y)):\n        print(\"--> Fold {}\".format(fold + 1))\n        xtrain, xval = x.iloc[train_ind], x.iloc[val_ind]\n        ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n        \n        cb = CatBoostClassifier(**para)\n        \n        model =  cb.fit(xtrain, ytrain, eval_set=[(xval,yval)], cat_features=None, verbose=100, early_stopping_rounds=50)\n        \n        pred_train = model.predict(xtrain)\n        pred_val = model.predict(xval)\n        cb_train_preds[val_ind]=pred_val\n        score1 = accuracy_score(ytrain, pred_train)\n        score2 = accuracy_score(yval, pred_val)\n        print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n        \n        results = model.evals_result_\n#         print(results)\n        df=pd.DataFrame({\n                        \"train_ll\":results['learn'][\"Logloss:use_weights=true\"],\n                        \"validation_ll\":results[\"validation\"][\"Logloss:use_weights=true\"],\n                        \"train_acc\":results['learn']['Accuracy:use_weights=true'],\n                        \"test_acc\":results[\"validation\"]['Accuracy:use_weights=true'],\n        })\n#         print(df.head())\n        \n        fig, ax = plt.subplots(ncols=2, figsize=(12,6))\n        sns.lineplot(data=df, x=df.index, y=\"train_ll\", ax=ax[0], label=\"Train loss\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_ll\", ax=ax[0], label=\"Validation loss\")\n        sns.lineplot(data=df, x=df.index, y=\"train_acc\", ax=ax[1], label=\"Train acc.\")\n        sns.lineplot(data=df, x=df.index, y=\"test_acc\", ax=ax[1], label=\"Validation acc.\")\n        ax[0].set_title(\"Loss curve\")\n        ax[1].set_title(\"Accuracy curve\")\n        ax[0].set_ylabel(\"Loss\")\n        ax[0].set_xlabel(\"Itertation\")\n        ax[1].set_ylabel(\"Accuracy\")\n        ax[1].set_xlabel(\"Itertation\")\n        fig.suptitle(\"CatBoost Loss/Accuracy.\")\n        plt.show()\n    acc=accuracy_score(y, cb_train_preds)\n    print('OOF ACCURACY: {}'.format(acc))\n    return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study=optuna.create_study(study_name=\"CatBoost set 1 Optimization\", direction='maximize')\nstudy.optimize(objective, n_trials=30)\n\ntrial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))\n\nprint(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">CatBoost tuned set1 model</h2>","metadata":{}},{"cell_type":"code","source":"para = {  \n        'random_seed': SEED,\n        'loss_function': 'Logloss',\n        'class_weights':[1,1.34],\n        'task_type': 'GPU', \n        'custom_metric': ['Logloss','Accuracy'],\n        'use_best_model': True,\n        'od_type': \"Iter\",\n        'od_wait': 20,\n        'verbose': True,\n        'max_bin': 128,  \n        'bootstrap_type': 'Poisson', \n        'max_depth': 1, \n        'learning_rate': 0.0921929936216536, \n        'n_estimators': 879, \n        'grow_policy': 'Depthwise', \n        'mvs_reg': 45.53846095760124, \n        'random_strength': 10, \n        'min_data_in_leaf': 6644, \n        'score_function': 'NewtonL2', \n        'l2_leaf_reg': 9.586306645407689, \n        'subsample': 0.5043977454997433\n      }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cb_train_preds = np.zeros(len(y),)\ncb_test = np.zeros(len(test1),)\nfor fold, (train_ind, val_ind) in enumerate(kf.split(X, y)):\n    print(\"--> Fold {}\".format(fold + 1))\n    \n    xtrain, xval = X.iloc[train_ind], X.iloc[val_ind]\n    ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n    \n    cb = CatBoostClassifier(**para)\n        \n    model =  cb.fit(xtrain, ytrain, eval_set=[(xval,yval)], cat_features=None, verbose=100, early_stopping_rounds=50)\n    \n    pred_train = model.predict_proba(xtrain)[:,1]\n    pred_val = model.predict_proba(xval)[:,1]\n    cb_train_preds[val_ind] = pred_val\n    cb_test += (model.predict_proba(test1)[:,1])/folds\n    score1 = accuracy_score(ytrain, np.where(pred_train<=0.5, 0, 1))\n    score2 = accuracy_score(yval, np.where(pred_val<=0.5, 0, 1))\n    print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n\n    results = model.evals_result_\n#     print(results)\n    df=pd.DataFrame({\n                    \"train_ll\":results['learn'][\"Logloss:use_weights=true\"],\n                    \"validation_ll\":results[\"validation\"][\"Logloss:use_weights=true\"],\n                    \"train_acc\":results['learn']['Accuracy:use_weights=true'],\n                    \"test_acc\":results[\"validation\"]['Accuracy:use_weights=true'],\n    })\n#         print(df.head())\n\n    fig, ax = plt.subplots(ncols=2, figsize=(12,6))\n    sns.lineplot(data=df, x=df.index, y=\"train_ll\", ax=ax[0], label=\"Train loss\")\n    sns.lineplot(data=df, x=df.index, y=\"validation_ll\", ax=ax[0], label=\"Validation loss\")\n    sns.lineplot(data=df, x=df.index, y=\"train_acc\", ax=ax[1], label=\"Train acc.\")\n    sns.lineplot(data=df, x=df.index, y=\"test_acc\", ax=ax[1], label=\"Validation acc.\")\n    ax[0].set_title(\"Loss curve\")\n    ax[1].set_title(\"Accuracy curve\")\n    ax[0].set_ylabel(\"Loss\")\n    ax[0].set_xlabel(\"Itertation\")\n    ax[1].set_ylabel(\"Accuracy\")\n    ax[1].set_xlabel(\"Itertation\")\n    fig.suptitle(\"CatBoost Loss/Accuracy.\")\n    plt.show()\n    \nacc1 = accuracy_score(y, np.where(cb_train_preds<=0.5, 0, 1))\n# acc2 = accuracy_score(y_test, np.where(cb_test_preds<=0.5, 0, 1))\n\nprint('OOF ACCURACY Train: {} '.format(acc1))\n\nscore_train=pd.read_csv('../input/score-tab-apr21/score_train.csv')\nscore_test=pd.read_csv('../input/score-tab-apr21/score_test.csv')\n\nscore_train['cb'] = cb_train_preds\nscore_test['cb'] = cb_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_train.to_csv('./score_train.csv',index=False)\nscore_test.to_csv('./score_test.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_=pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\ndf=pd.DataFrame()\ndf['PassengerId']=test_['PassengerId'].values\ndf['Survived']=cb_test\ndf['Survived']=df['Survived'].apply(lambda x:0 if x<=0.5 else 1)\ndf.to_csv('./cb_tuned.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">CatBoost set2</h2>","metadata":{}},{"cell_type":"code","source":"ohe=OneHotEncoder()\ncol=['Sex','Embarked']\nohe.fit(train2[col])\nprint(ohe.get_feature_names(col))\ndf1=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(train2[col]).toarray())\ndf2=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(test2[col]).toarray())\n\ntrain2=train2.join(df1)\ntest2=test2.join(df2)\n\ntrain2['related_cat'] = train2['related_cat'].apply(lambda x:1 if x in ['low'] else 2)\ntest2['related_cat'] = test2['related_cat'].apply(lambda x:1 if x in ['low'] else 2)\n\ntrain2.drop(columns=['Sex','Embarked'], inplace=True)\ntest2.drop(columns=['Sex','Embarked'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds=5\nSEED=random.randint(937,8641)\n\nkf=StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\n\nfeatures=train2.columns[1:]\nX = train2[features]\ny = train2['Survived']\n\n\nimbalanced_ratio=(train2[train2['Survived']==0]['Survived'].count()/train2[train2['Survived']==1]['Survived'].count()).round(2)\nprint(\"Imbalnce ratio: {:}\".format(imbalanced_ratio))\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=SEED)\nprint(\"Distribution of train and test:\", len(x_train), len(y_train), len(x_test), len(x_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Trial=0\n\ndef objective(trial, x=x_train, y=y_train):\n    global Trial\n    \n    para = {\n            'random_seed': SEED,\n            'loss_function': 'Logloss',\n            'class_weights':[1,1.34],\n            'task_type': 'GPU', \n            'custom_metric': ['Logloss','Accuracy'],\n            'use_best_model': True,\n            'verbose': True,\n            'max_bin': 128,\n            'bootstrap_type': trial.suggest_categorical('bootstrap_type',['Bernoulli','Poisson']),\n            'max_depth': trial.suggest_int('max_depth', 1, 16),\n            'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1),\n            'n_estimators': trial.suggest_int('n_estimators',500, 20000),\n            'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree','Depthwise','Lossguide']),\n            'random_strength': trial.suggest_int('random_strength', 1, 10),\n            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 10000),\n            'score_function': trial.suggest_categorical('score_function', ['Cosine','L2','NewtonCosine','NewtonL2']),\n            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0),\n            'subsample': trial.suggest_float('subsample', 0.5, 0.85)\n            }\n    \n    print(\"--------------------> Trial {} <--------------------\".format(Trial))\n    Trial=Trial + 1\n    cb_train_preds = np.zeros(len(y),)\n    \n    for fold, (train_ind, val_ind) in enumerate(kf.split(x, y)):\n        print(\"--> Fold {}\".format(fold + 1))\n        xtrain, xval = x.iloc[train_ind], x.iloc[val_ind]\n        ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n        \n        cb = CatBoostClassifier(**para)\n        \n        model =  cb.fit(xtrain, ytrain, eval_set=[(xval,yval)], cat_features=None, verbose=100, early_stopping_rounds=50)\n        \n        pred_train = model.predict(xtrain)\n        pred_val = model.predict(xval)\n        cb_train_preds[val_ind]=pred_val\n        score1 = accuracy_score(ytrain, pred_train)\n        score2 = accuracy_score(yval, pred_val)\n        print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n        \n        results = model.evals_result_\n#         print(results)\n        df=pd.DataFrame({\n                        \"train_ll\":results['learn'][\"Logloss:use_weights=true\"],\n                        \"validation_ll\":results[\"validation\"][\"Logloss:use_weights=true\"],\n                        \"train_acc\":results['learn']['Accuracy:use_weights=true'],\n                        \"test_acc\":results[\"validation\"]['Accuracy:use_weights=true'],\n        })\n#         print(df.head())\n        \n        fig, ax = plt.subplots(ncols=2, figsize=(12,6))\n        sns.lineplot(data=df, x=df.index, y=\"train_ll\", ax=ax[0], label=\"Train loss\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_ll\", ax=ax[0], label=\"Validation loss\")\n        sns.lineplot(data=df, x=df.index, y=\"train_acc\", ax=ax[1], label=\"Train acc.\")\n        sns.lineplot(data=df, x=df.index, y=\"test_acc\", ax=ax[1], label=\"Validation acc.\")\n        ax[0].set_title(\"Loss curve\")\n        ax[1].set_title(\"Accuracy curve\")\n        ax[0].set_ylabel(\"Loss\")\n        ax[0].set_xlabel(\"Itertation\")\n        ax[1].set_ylabel(\"Accuracy\")\n        ax[1].set_xlabel(\"Itertation\")\n        fig.suptitle(\"CatBoost Loss/Accuracy.\")\n        plt.show()\n    acc=accuracy_score(y, cb_train_preds)\n    print('OOF ACCURACY: {}'.format(acc))\n    return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study=optuna.create_study(study_name=\"CatBoost set 2 Optimization\", direction='maximize')\nstudy.optimize(objective, n_trials=25)\n\ntrial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))\n\nprint(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">Ensemble model</h2>","metadata":{}},{"cell_type":"code","source":"score_train=pd.read_csv('../input/score-tab-apr21/score_train.csv')\nscore_test=pd.read_csv('../input/score-tab-apr21/score_test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ = pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\ndf = pd.DataFrame()\ndf['PassengerId'] = test_['PassengerId'].values\ndf['Survived'] = 0.2*score_test['xg']+0.2*score_test['rf']+0.4*score_test['lg']+0.2*score_test['cb']\ndf['Survived'] = df['Survived'].apply(lambda x:0 if x<=0.5 else 1)\ndf.to_csv('./ensembled_model.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:300%\">Level2 Classification</h2>","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv('../input/score-tab-apr21/score_train.csv')\ntest=pd.read_csv('../input/score-tab-apr21/score_test.csv')\ntrain1=pd.read_csv('../input/training-apr/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds=5\nSEED=random.randint(937,8641)\n\nkf=StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\n\nfeatures=train.columns\nX = train[features]\ny = train1['Survived']\n\n# x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=SEED)\n\n# print(\"Distribution of train and test:\", len(x_train), len(y_train), len(x_test), len(x_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lvl2_train_preds = np.zeros(len(y),)\nlvl2_test = np.zeros(len(test),)\nfor fold, (train_ind, val_ind) in enumerate(kf.split(X, y)):\n    print(\"--> Fold {}\".format(fold + 1))\n    \n    xtrain, xval = X.iloc[train_ind], X.iloc[val_ind]\n    ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n    \n    rc = CalibratedClassifierCV(\n                                RidgeClassifier(random_state=SEED, \n                                                class_weight=\"balanced\"),\n                                cv=3\n                                )\n        \n    model =  rc.fit(xtrain, ytrain)\n    \n    pred_train = model.predict_proba(xtrain)[:,1]\n    pred_val = model.predict_proba(xval)[:,1]\n    lvl2_train_preds[val_ind] = pred_val\n    lvl2_test += (model.predict_proba(test)[:,1])/folds\n    score1 = accuracy_score(ytrain, np.where(pred_train<=0.5, 0, 1))\n    score2 = accuracy_score(yval, np.where(pred_val<=0.5, 0, 1))\n    print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n\nacc1 = accuracy_score(y, np.where(lvl2_train_preds<=0.5, 0, 1))\n# acc2 = accuracy_score(y_test, np.where(lvl2_test_preds<=0.5, 0, 1))\n\nprint('OOF ACCURACY Train: {}'.format(acc1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ = pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\ndf = pd.DataFrame()\ndf['PassengerId'] = test_['PassengerId'].values\ndf['Survived'] = lvl2_test\ndf['Survived'] = df['Survived'].apply(lambda x:0 if x<=0.5 else 1)\ndf.to_csv('./level2_model.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}