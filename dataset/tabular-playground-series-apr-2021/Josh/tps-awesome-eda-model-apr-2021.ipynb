{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing libs\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.dates as mdates\nimport matplotlib.pyplot as plt\nfrom textwrap import wrap\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport plotly as py\nimport plotly.graph_objs as go\nimport os\npy.offline.init_notebook_mode(connected = True)\n#print(os.listdir(\"../input\"))\nimport datetime as dt\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport matplotlib.gridspec as grid_spec\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE\nimport scikitplot as skplt\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder\nfrom sklearn.model_selection import train_test_split,cross_val_score\n\n\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import accuracy_score, recall_score, roc_auc_score, precision_score, f1_score\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import StandardScaler\n# data splitting\nfrom sklearn.model_selection import train_test_split\n# data modeling\nfrom sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n#ensembling\nfrom mlxtend.classifier import StackingCVClassifier","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tabular Playgroung Series [Apr 2021]\n\nWelcome...\n\nThis dataset is based on the classic Titanic dataset, but is synthetic and generated using a CTGAN.\n\n**Can we predict who survives, and who perishes?**\n\n# Exploratory Data Analysis\n\nFirstly, I will explore the data. I am a Kaggler who loves data visualisation so this will be a main focus of this project.\n\n# Models\n\nNext, I will use several models to see which performs the best. I will also then stack these models together.\n\n\n\n# Work in progress\n\nI will continue to add visualisations to this project over the coming days & weeks...","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2021/train.csv')\ntest_df = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2021/test.csv')\n\nprint(train_df.shape)\nprint(test_df.shape)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Custom parameters for my plots\n\nfrom matplotlib import rc\nfont = {'size'   : 10}\nmatplotlib.rc('font', **font)\n# change font\nmatplotlib.rcParams['font.sans-serif'] = \"Times New Roman\"\nmatplotlib.rcParams['font.family'] = \"serif\"\n\n\n# colors\n\nbackground_color = '#fff1e5'\nface_color = '#fffcfa'\n\ncol = '#33302e'\nsub_col = '#a49a93'\n\n# Custom colour map \ncmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", ['#1e558b','#FF8AAF','#E6668E'])\n\nsurv_c = '#1e558b'\nno_surv_c = '#E6668E'\n\n\ntitle = 15\nsub = 10\nfont = 'serif'\nb = 'bold'\n\ncol = '#33302e'\nsub_col = '#a49a93'\n\n# Custom colour map \ncmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", ['#1e558b','#FF8AAF','#E6668E'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's get a feel for the data","metadata":{}},{"cell_type":"code","source":"missing = pd.DataFrame(columns=['% Missing values'],data=train_df.isnull().sum()/len(train_df))\nmissing.drop(['Survived'],inplace=True)\n\nmissing_tst = pd.DataFrame(columns=['% Missing values'],data=test_df.isnull().sum()/len(test_df))\n\n\nfig = plt.figure(figsize=(5, 5), dpi=150,facecolor=background_color)\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace=0.02, hspace=0)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\n\nax0.set_facecolor(face_color)\nax1.set_facecolor(face_color)\n\n\nfor s in [\"right\", \"top\",\"bottom\",\"left\"]:\n    ax0.spines[s].set_visible(False)\n    ax1.spines[s].set_visible(False)\n    \nsns.heatmap(missing,cbar=False,cmap=cmap,annot=True,fmt=\".1%\", linewidths=2.5,vmax=0.1, ax=ax0)\n\nsns.heatmap(missing_tst,cbar=False,cmap=cmap,annot=True,fmt=\".1%\", linewidths=2,vmax=0.1, ax=ax1)\n\nax0.tick_params(axis=u'both', which=u'both',length=0)\nax1.tick_params(axis=u'both', which=u'both',length=0)\n\n# remove y labels for second plot\nax1.set_yticklabels([])\nax0.set_xticklabels([])\nax1.set_xticklabels([])\n\n\n# Ax lens\n\nXstart, Xend = ax0.get_xlim()\nYstart, Yend = ax0.get_ylim()\n\nXstart1, Xend1 = ax1.get_xlim()\nYstart1, Yend1 = ax1.get_ylim()\n\n# title\n\nax0.text(Xstart,Yend-1.7,'How much data is missing?',color=col,fontsize=title,fontweight=b)\nax0.text(Xstart,Yend-0.6,'There are many missing values,\\nparticularly in the Cabin field.',color=sub_col,fontsize=sub,fontweight=b)\n\nax0.text(Xstart,Ystart+0.5,'Train',color=sub_col,fontsize=sub,fontweight=b)\nax1.text(Xend1-0.185,Ystart1+0.5,'Test',color=sub_col,fontsize=sub,fontweight=b)\n\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# We'll deal with missing data shortly\n\nBefore I do, I want to start undertsanding what it is I'd be imputing. Is it necessary? Or can I drop the data?","metadata":{}},{"cell_type":"code","source":"feature_cols = train_df.drop(['Survived','PassengerId'], axis=1).columns\ntarget_column = 'Survived'\nnumerical_columns = train_df[feature_cols].select_dtypes(include=['int64','float64']).columns\ncategorical_columns = train_df[feature_cols].select_dtypes(exclude=['int64','float64']).columns\n\n\nfig = plt.figure(figsize=(8, 5), dpi=150,facecolor=background_color)\ngs = fig.add_gridspec(1, 1)\nax0 = fig.add_subplot(gs[0, 0])\n\nax0.set_facecolor(face_color)\nax0.text(-1.1, 0.26, 'Correlation of Continuous Features with Target', fontsize=title, fontweight=b, color=col)\nax0.text(-1.1, 0.24, 'It is clear here that class has a big imapct on survival and so does fare paid - though these are likely related.' ,fontsize=sub, color=sub_col)\n\nchart_df = pd.DataFrame(train_df[numerical_columns].corrwith(train_df['Survived']))\nchart_df.columns = ['corr']\nchart_df['positive'] = chart_df['corr'] > 0\n\nsns.barplot(x=chart_df.index, y=chart_df['corr'], ax=ax0, palette=chart_df.positive.map({True: surv_c, False: no_surv_c}), zorder=3,dodge=False)\nax0.grid(which='major', axis='y', zorder=0, color='gray', linestyle=':', dashes=(1,5))\nax0.set_ylabel('')\n\n\nfor s in [\"top\",\"right\", 'left']:\n    ax0.spines[s].set_visible(False)\n    \nax0.annotate('-0.29',xy=(0,-0.29-(-0.289723*0.1)),ha='center',color='white')\nax0.annotate('0.19',xy=(4,0.19-(0.187376*0.12)),ha='center',color='white')\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How related are these features to each other?","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(12, 8), facecolor=background_color)\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace=0, hspace=0.27)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\ncolors = [\"#fbfbfb\", \"lightgray\",\"#0e4f66\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nax0.set_facecolor(background_color)\nax0.text(0, 0, 'Train', fontsize=20, fontweight='bold', fontfamily='serif',color='lightgray')\n\nax1.set_facecolor(background_color)\nax1.text(4.5, 5.5, 'Test', fontsize=20, fontweight='bold', fontfamily='serif',color='lightgray')\n\n\nfig.text(0.5,0.5,'Correlation of Features\\nFor Train & Test\\nDatasets', fontsize=title+2, fontweight='bold', color=col,va='center',ha='center')\n\ncorr = train_df[numerical_columns].corr().abs()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nsns.heatmap(corr, ax=ax0, vmin=0, vmax=1, annot=True, square=True, mask=mask,\n            cbar_kws={\"orientation\": \"horizontal\"}, cbar=False, cmap=cmap, fmt='.1g',linewidth=3,linecolor=background_color)\n\n\ncorr = test_df[numerical_columns].corr().abs()\nmask = np.tril(corr)\nsns.heatmap(corr, ax=ax1, vmin=0, vmax=1, annot=True, square=True, mask=mask,\n            cbar_kws={\"orientation\": \"horizontal\"}, cbar=False, cmap=cmap, fmt='.1g',linewidth=3,linecolor=background_color)\nax1.xaxis.tick_top()\nax1.yaxis.tick_right()\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we suspected, Fare and Class are highly related to eachother","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(8, 3), dpi=150,facecolor=background_color)\ngs = fig.add_gridspec(2, 1)\ngs.update(wspace=0.02, hspace=0.5)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[1, 0])\nax0.set_facecolor(face_color)\n\nfor s in [\"right\", \"top\",\"bottom\",\"left\"]:\n    ax0.spines[s].set_visible(False)\n    ax1.spines[s].set_visible(False)\n\n\nsurv = train_df[train_df['Survived'] == 1]\nno_surv = train_df[train_df['Survived'] == 0]\n\nsns.kdeplot(surv['Fare'],color=surv_c,shade=True,ax=ax0)\nsns.kdeplot(no_surv['Fare'],color=no_surv_c,shade=True,ax=ax0)\n\nsns.kdeplot(surv['Age'],color=surv_c,shade=True,ax=ax1)\nsns.kdeplot(no_surv['Age'],color=no_surv_c,shade=True,ax=ax1)\n\nax0.tick_params(axis=u'both', which=u'both',length=0)\nax1.tick_params(axis=u'both', which=u'both',length=0)\n\n\n #remove y labels for second plot\nax0.set_yticklabels([])\nax1.set_yticklabels([])\n\nax0.tick_params(axis='x', which='major', labelsize=8)\nax0.tick_params(axis='x', colors=sub_col)\n\nax1.tick_params(axis='x', which='major', labelsize=8)\nax1.tick_params(axis='x', colors=sub_col)\n                          \n# Ax lens\n\nXstart, Xend = ax0.get_xlim()\nYstart, Yend = ax0.get_ylim()\n\nax0.set_xlabel(\"Fare\",loc='left',fontsize=sub,color=sub_col)\nax0.set_ylabel(\" \",loc='top',fontsize=sub,color=sub_col)\n\n\nax1.set_xlabel(\"Age\",loc='left',fontsize=sub,color=sub_col)\nax1.set_ylabel(\" \",loc='top',fontsize=sub,color=sub_col)\n\nax0.text(Xstart,Yend+(Yend*0.6),'Are there differences in survival rates by Age & Fare?',fontsize=title,fontweight=b,color=col)\nax0.text(Xstart,Yend+(Yend*0.2),\n       \n'''\nWe see a noticeable differences between those who survived and those who did not.\n''',color=sub_col,fontsize=sub)\n\nax0.text(Xstart,Yend+0.005,'Survivors',fontsize=sub-2,fontweight=b,color=surv_c)\nax0.text(Xstart+85,Yend+0.005,'|',fontsize=sub-2,fontweight=b,color=col)\nax0.text(Xstart+90,Yend+0.005,'Non-Survivors',fontsize=sub-2,fontweight=b,color=no_surv_c)\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dealing with missing values\n\nI have now seen that there is seemingly relationships between Age, Fare paid, and survivorship rates.\n\nThe plots above also reveal that the Fare in particular is very positively skewed. As a result, I will impute the median values as opposed to the mean.\n\n","metadata":{}},{"cell_type":"code","source":"train_df['Age'].fillna(train_df['Age'].median(), inplace=True)\ntest_df['Age'].fillna(test_df['Age'].median(), inplace=True)\ntrain_df['Fare'].fillna(train_df['Fare'].median(), inplace=True)\ntest_df['Fare'].fillna(test_df['Fare'].median(), inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# We aren't done with missing values yet, but first I want to examine more what factors seem to be related to our primary variable: Survived, or did not survive","metadata":{}},{"cell_type":"markdown","source":"Is our dataset balanced?\n\nIf not, we will utulise methods such as SMOTE","metadata":{}},{"cell_type":"code","source":"x=train_df.groupby(['Survived'])['Survived'].count()\ny=len(train_df)\nr=((x/y)).round(2)\n\nratio = pd.DataFrame(r).T\n\nfig = plt.figure(figsize=(8, 10), dpi=150,facecolor=background_color)\ngs = fig.add_gridspec(5, 1)\ngs.update(wspace=0.02, hspace=0.8)\nax = fig.add_subplot(gs[0, 0])\n\n\n\nax.set_facecolor(background_color)\n\nax.barh(ratio.index, ratio[1.0], color=surv_c, alpha=0.9, label='Survived')\nax.barh(ratio.index, ratio[0.0], left=ratio[1.0], color=no_surv_c, alpha=0.9, label='Perished')\n\nax.set_xlim(0, 1)\nax.set_xticks([])\nax.set_yticks([])\n#ax.set_yticklabels(mf_ratio.index, fontfamily='serif', fontsize=11)\n\n\n# movie percentage\nfor i in ratio.index:\n    ax.annotate(f\"{int(ratio[1.0][i]*100)}%\", \n                   xy=(ratio[1.0][i]/2, i),\n                   va = 'center', ha='center',fontsize=32, fontweight='light', fontfamily='serif',\n                   color='white')\n\n    ax.annotate(\"Survived\", \n                   xy=(ratio[1.0][i]/2, -0.25),\n                   va = 'center', ha='center',fontsize=12, fontweight='light', fontfamily='serif',\n                   color='white')\n    \n    \nfor i in ratio.index:\n    ax.annotate(f\"{int(ratio[0.0][i]*100)}%\", \n                   xy=(ratio[1.0][i]+ratio[0.0][i]/2, i),\n                   va = 'center', ha='center',fontsize=32, fontweight='light', fontfamily='serif',\n                   color='white')\n    ax.annotate(\"Perished\", \n                   xy=(ratio[1.0][i]+ratio[0.0][i]/2, -0.25),\n                   va = 'center', ha='center',fontsize=12, fontweight='light', fontfamily='serif',\n                   color='white')\n\n\n\n# Title & Subtitle\nax.text(0,0.75,'How many survived & did not survive in our dataset?',fontsize=title, fontweight=b,color=col)\nax.text(0,0.6,'We see a fairly balanced dataset, with slightly less survivors in total.',fontsize=sub,color=sub_col)  \n\nfor s in ['top', 'left', 'right', 'bottom']:\n    ax.spines[s].set_visible(False)\n    \n#ax.legend(loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.06))\n\n# Removing legend due to labelled plot\nax.legend().set_visible(False)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our dataset is balanced, meaning we have roughly equal number of each target variable.","metadata":{}},{"cell_type":"markdown","source":"# Overview of categorical features","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(4, 10), dpi=150,facecolor=background_color)\ngs = fig.add_gridspec(5, 1)\ngs.update(wspace=0.02, hspace=0.8)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[1, 0])\nax2 = fig.add_subplot(gs[2, 0])\n\n\nax0.set_facecolor(face_color)\nax1.set_facecolor(face_color)\nax2.set_facecolor(face_color)\n\nfor s in [\"right\", \"top\",\"bottom\",\"left\"]:\n    ax0.spines[s].set_visible(False)\n    ax1.spines[s].set_visible(False)\n    ax2.spines[s].set_visible(False)\n\n#plots\nsns.countplot(x=train_df['Sex'],hue=train_df['Survived'],edgecolor=face_color,palette=[no_surv_c,surv_c],ax=ax0)\nax0.set_xlabel(\"Sex\",loc='left',fontsize=sub,color=sub_col)\nax0.set_ylabel(\" \",loc='top',fontsize=sub,color=sub_col)\nax0.get_legend().remove()\n\nsns.countplot(x=train_df['Pclass'],hue=train_df['Survived'],edgecolor=face_color,palette=[no_surv_c,surv_c],ax=ax1)\nax1.set_xlabel(\"Class\",loc='left',fontsize=sub,color=sub_col)\nax1.set_ylabel(\" \",loc='top',fontsize=sub,color=sub_col)\nax1.get_legend().remove()\n\nsns.countplot(x=train_df['Embarked'],hue=train_df['Survived'],edgecolor=face_color,palette=[no_surv_c,surv_c],ax=ax2)\nax2.set_xlabel(\"Embarkation Port\",loc='left',fontsize=sub,color=sub_col)\nax2.set_ylabel(\" \",loc='top',fontsize=sub,color=sub_col)\nax2.get_legend().remove()\n\nXstart, Xend = ax0.get_xlim()\nYstart, Yend = ax0.get_ylim()\nax0.set_yticks(np.arange(0, Yend, 10000))\n\nax0.text(Xstart,100000,'Where do the total deaths come from?',fontsize=title,fontweight=b,color=col)\nax0.text(Xstart,81500,'We observe areas that were incredibly unlikley to survive\\nfor example, males were far more likely to perish',fontsize=sub,color=sub_col)\n\nax0.text(Xend+0.5,15000,\n'''\nMales were far more likely to \nperish than females.\nThis will no doubt be important\nfor our model.\n''',fontsize=sub-2,color=sub_col)\n\n\nax0.set_yticklabels([])\nax0.tick_params(axis='x', which='major', labelsize=8)\nax0.tick_params(axis='both', colors=sub_col)\nax0.tick_params(axis=u'both', which=u'both',length=0)\n\nXstart, Xend = ax1.get_xlim()\nYstart, Yend = ax1.get_ylim()\n\n\nax1.text(Xend+0.75,10000,\n'''\nUnsuprisingly those in the\nlower classes persihed at\nmuch higher rates than those\nin higher classes.\n''',fontsize=sub-2,color=sub_col)\n\n\nax1.set_yticks(np.arange(0, Yend, 10000))\n\nax1.set_yticklabels([])\nax1.tick_params(axis='x', which='major', labelsize=8)\nax1.tick_params(axis='both', colors=sub_col)\nax1.tick_params(axis=u'both', which=u'both',length=0)\n\nXstart, Xend = ax2.get_xlim()\nYstart, Yend = ax2.get_ylim()\nax2.set_yticks(np.arange(0, Yend, 10000))\n\nax2.text(Xend+0.75,15000,\n'''\nMany of those who perished\nboarded in Southampton, but this\nwas the main embarkation port\nso this is not surprising.\n''',fontsize=sub-2,color=sub_col)\n\nax2.set_yticklabels([])\nax2.tick_params(axis='x', which='major', labelsize=8)\nax2.tick_params(axis='both', colors=sub_col)\nax2.tick_params(axis=u'both', which=u'both',length=0)\n\n\nax0.text(Xstart,60000,'Survivors',fontsize=sub-2,fontweight=b,color=surv_c)\nax0.text(Xstart+0.38,60000,'|',fontsize=sub-2,fontweight=b,color=col)\nax0.text(Xstart+0.405,60000,'Non-Survivors',fontsize=sub-2,fontweight=b,color=no_surv_c)\n\n\nimport matplotlib.lines as lines\nl1 = lines.Line2D([1, 1], [0.4, 0.88], transform=fig.transFigure, figure=fig,color='black',lw=0.2)\nfig.lines.extend([l1])\n\nax0.text(2,60000,'Insights',fontsize=sub-2,fontweight=b,color=col)\n\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function from Subin An \n# https://www.kaggle.com/subinium/tps-apr-highlighting-the-data\n\ndef age_band(num):\n    for i in range(1, 100):\n        if num < 10*i :  return f'{(i-1) * 10} ~ {i*10}'\n\ntrain_df['Age band'] = train_df['Age'].apply(age_band)\ntitanic_age = train_df[['Age band', 'Survived','Sex']].groupby('Age band')['Survived'].value_counts().sort_index().unstack().fillna(0)\ntitanic_age['Survival rate'] = titanic_age[1] / (titanic_age[0] + titanic_age[1]) * 100\nage_band = train_df['Age band'].value_counts().sort_index()\n\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We know that sex is a big factor in determining whether or not someone lived or dies, but does this hold true across all age ranges?","metadata":{}},{"cell_type":"code","source":"age_sex_surv = train_df.groupby(['Sex','Age band'])['Survived'].mean().unstack().T\nfem_mean = age_sex_surv['female'].mean()\nmale_mean = age_sex_surv['male'].mean()\n\nfig = plt.figure(figsize=(10, 4), dpi=150,facecolor=background_color)\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0.2, hspace=0.8)\nax0 = fig.add_subplot(gs[0, 0])\n\nax0.set_facecolor(face_color)\n\nfor s in [\"right\", \"top\",\"bottom\",\"left\"]:\n    ax0.spines[s].set_visible(False)\n\nmy_range=range(1,len(age_sex_surv.index)+1)\n \nax0.hlines(y=my_range, xmin=age_sex_surv['male'], xmax=age_sex_surv['female'], color='gray', alpha=0.4)\nsns.scatterplot(age_sex_surv['male'], my_range, color=surv_c, alpha=1,s=100, label='male',ax=ax0)\n\nsns.scatterplot(age_sex_surv['female'], my_range, color=no_surv_c, alpha=1,s=100, label='female',ax=ax0)\n\nax0.get_legend().remove()\n\nXstart, Xend = ax0.get_xlim()\nYstart, Yend = ax0.get_ylim()\nax0.set_xticks(np.arange(0, 1, 0.1))\nax0.set_yticklabels([' ','0 ~ 10', '10 ~ 20', '20 ~ 30', '30 ~ 40', '40 ~ 50', '50 ~ 60','60 ~ 70', '70 ~ 80', '80 ~ 90'])\n\n\nax0.tick_params(axis='x', which='major', labelsize=8)\nax0.tick_params(axis='both', colors=sub_col)\nax0.tick_params(axis=u'both', which=u'both',length=0)\n\nax0.set_xlabel(\"Survival Rate\",loc='left',fontsize=sub,color=sub_col)\n\n\nax0.text(0,11.2,'Survival rates by age & sex',fontsize=title,fontweight=b,color=col)\nax0.text(0,10.6,'Female out survive males across all age cateogories, and the mean survial rate is much higher',fontsize=sub,color=sub_col)\n\nax0.text(0,10,'Male',fontsize=sub-2,fontweight=b,color=surv_c)\nax0.text(0+0.037,10,'|',fontsize=sub-2,fontweight=b,color=col)\nax0.text(0+0.0436,10,'Female',fontsize=sub-2,fontweight=b,color=no_surv_c)\n\n#ax0.vlines(x=fem_mean)\nax0.axvline(male_mean ,color=surv_c, linewidth=0.4, linestyle='dashdot')\nax0.axvline(fem_mean ,color=no_surv_c, linewidth=0.4, linestyle='dashdot')\n\n\n# Show the graph\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Yes, it hold true in every age range, and is very significant.\n\nHowever, we also see that the elderly males stood the best chance of surviving, and interestingly, that young females struggled comparatively.\n\nwhat about other features?","metadata":{}},{"cell_type":"code","source":"# idea from Subin An - though it has been coded differently\n# https://www.kaggle.com/subinium/tps-apr-highlighting-the-data\n\ncmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", ['#E6668E','#FF8AAF','#1e558b'])\n\n\nfig = plt.figure(figsize=(6, 12), dpi=150,facecolor=background_color)\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace=0.2, hspace=0.8)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\n\n\nax0.set_facecolor(background_color)\nax1.set_facecolor(background_color)\n\n\nfor s in [\"right\", \"top\",\"bottom\",\"left\"]:\n    ax0.spines[s].set_visible(False)\n    ax1.spines[s].set_visible(False)\n\nsns.heatmap(train_df.groupby(['Sex', 'Embarked'])['Survived'].aggregate('mean').unstack()*100, ax=ax0,\n                square=True, annot=True, fmt='.2f', linewidth=2,\n                cbar=False, cmap=cmap)\n\nsns.heatmap(train_df.groupby(['Sex', 'Pclass'])['Survived'].aggregate('mean').unstack()*100, ax=ax1,\n                square=True, annot=True, fmt='.2f', linewidth=2,\n                cbar=False, cmap=cmap)\n\n#C = Cherbourg, Q = Queenstown, S = Southampton\n#ax0.set_xticklabels(['Cherbourg','Queenstown','Southampton'])\nax1.set_yticklabels([])\n\nax0.set_yticklabels(['Female','Male'], rotation=0)\nax0.tick_params(axis='x', which='major', labelsize=8)\nax0.tick_params(axis='both', colors=sub_col)\nax0.tick_params(axis=u'both', which=u'both',length=0)\nax0.set_ylabel('') \nax1.set_ylabel('') \n\nax1.tick_params(axis='x', which='major', labelsize=8)\nax1.tick_params(axis='both', colors=sub_col)\nax1.tick_params(axis=u'both', which=u'both',length=0)\n\n\nax0.set_xlabel(\"Embarkation Port\",loc='left',fontsize=sub,color=sub_col)\nax1.set_xlabel(\"Class\",loc='left',fontsize=sub,color=sub_col)\n\nax0.text(0,-1.1,'Survival rates',fontweight=b,fontsize=title,color=col)\nax0.text(0,-0.3,\n'''\nHere we clearly see the trends indicated above.\nFemales, in all categories, have high survial rates;\nespecially those who were in first and second class''',fontsize=sub,color=sub_col)\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new = age_sex_surv.mean()\n\nnew = pd.DataFrame(new)\n\nnew['no_surv'] = 1-new[0]\n\n#new\n\nfrom statsmodels.graphics.mosaicplot import mosaic\n\n# in progress\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models\n\nHere I will try a variety of models to see which performs best on our data","metadata":{}},{"cell_type":"code","source":"# model prep\n\ntrain_df = pd.get_dummies(train_df, columns=[\"Sex\", \"Embarked\"])\ntest_df = pd.get_dummies(test_df, columns=[\"Sex\", \"Embarked\"])\n\ntrain_df['Fare'] = train_df['Fare'].map(lambda i: np.log(i) if i > 0 else 0)\ntest_df['Fare'] = test_df['Fare'].map(lambda i: np.log(i) if i > 0 else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# spltting the data\n\nX = train_df[[\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Sex_male\", \"Sex_female\", \"Embarked_C\", \"Embarked_Q\", \"Embarked_S\"]]\ny = train_df['Survived']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression()\nmodel = lr.fit(X_train, y_train)\nlr_predict = lr.predict(X_test)\nlr_conf_matrix = confusion_matrix(y_test, lr_predict)\nlr_acc_score = accuracy_score(y_test, lr_predict)\nprint(\"confussion matrix\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Logistic Regression:\",lr_acc_score*100,'\\n')\nprint(classification_report(y_test,lr_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression(C=0.01, penalty='l2')\nmodel = lr.fit(X_train, y_train)\nlr_predict = lr.predict(X_test)\nlr_conf_matrix = confusion_matrix(y_test, lr_predict)\nlr_acc_score = accuracy_score(y_test, lr_predict)\nprint(\"confusion matrix\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Logistic Regression:\",lr_acc_score*100,'\\n')\nprint(classification_report(y_test,lr_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we manipulate the threshold our logisitc model uses to determine 0 or 1, can we improve the score?","metadata":{}},{"cell_type":"code","source":"#source code: https://www.kaggle.com/prashant111/extensive-analysis-eda-fe-modelling\n# modified\n\nfrom sklearn.preprocessing import binarize\n\nfor i in range(1,8):\n    \n    cm1=0\n    y_pred1 = lr.predict_proba(X_test)[:,1]\n    y_pred1 = y_pred1.reshape(-1,1)\n    y_pred2 = binarize(y_pred1, i/10)\n    y_pred2 = np.where(y_pred2 == 1, 1, 0)\n    cm1 = confusion_matrix(y_test, y_pred2)\n        \n    print ('With',i/10,'threshold the Confusion Matrix is ','\\n\\n',cm1,'\\n\\n',\n            'with',cm1[0,0]+cm1[1,1],'correct predictions, ', '\\n\\n', \n           \n            cm1[0,1],'Type I errors( False Positives), ','\\n\\n',\n           \n            cm1[1,0],'Type II errors( False Negatives), ','\\n\\n',\n           \n           'Accuracy score: ', (accuracy_score(y_test, y_pred2)), '\\n\\n',\n          \n            '====================================================', '\\n\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On this case, a threshold of 0.5 still gives the best results","metadata":{}},{"cell_type":"code","source":"nb = GaussianNB()\nnb.fit(X_train,y_train)\nnbpred = nb.predict(X_test)\nnb_conf_matrix = confusion_matrix(y_test, nbpred)\nnb_acc_score = accuracy_score(y_test, nbpred)\nprint(\"confusion matrix\")\nprint(nb_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Naive Bayes model:\",nb_acc_score*100,'\\n')\nprint(classification_report(y_test,nbpred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=20, random_state=12,max_depth=7)\nrf.fit(X_train,y_train)\nrf_predicted = rf.predict(X_test)\nrf_conf_matrix = confusion_matrix(y_test, rf_predicted)\nrf_acc_score = accuracy_score(y_test, rf_predicted)\nprint(\"confusion matrix\")\nprint(rf_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Random Forest:\",rf_acc_score*100,'\\n')\nprint(classification_report(y_test,rf_predicted))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb = XGBClassifier(learning_rate=0.1, n_estimators=64, max_depth=15,gamma=0.6, subsample=0.52,colsample_bytree=0.6,seed=27, \n                    reg_lambda=2, booster='dart', colsample_bylevel=0.6, colsample_bynode=0.5)\nxgb.fit(X_train, y_train)\nxgb_predicted = xgb.predict(X_test)\nxgb_conf_matrix = confusion_matrix(y_test, xgb_predicted)\nxgb_acc_score = accuracy_score(y_test, xgb_predicted)\nprint(\"confusion matrix\")\nprint(xgb_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Extreme Gradient Boost:\",xgb_acc_score*100,'\\n')\nprint(classification_report(y_test,xgb_predicted))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X_train, y_train)\nknn_predicted = knn.predict(X_test)\nknn_conf_matrix = confusion_matrix(y_test, knn_predicted)\nknn_acc_score = accuracy_score(y_test, knn_predicted)\nprint(\"confusion matrix\")\nprint(knn_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of K-NeighborsClassifier:\",knn_acc_score*100,'\\n')\nprint(classification_report(y_test,knn_predicted))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That is a lot of information to digest. Let's pull it all together and view it simultaneously","metadata":{}},{"cell_type":"code","source":"model_eval = pd.DataFrame({'Model': ['Logistic Regression','Naive Bayes','Random Forest','XGBoost',\n                    'K-Nearest Neighbour'], 'Accuracy': [lr_acc_score,\n                    nb_acc_score,rf_acc_score,xgb_acc_score,knn_acc_score]})\n\nmodel_eval = model_eval.set_index('Model').sort_values(by='Accuracy',ascending=False)\n\n\ncmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", ['#E6668E','#FF8AAF','#1e558b'])\n\n\nfig = plt.figure(figsize=(12, 6), dpi=150,facecolor=background_color)\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace=0.2, hspace=0.8)\nax0 = fig.add_subplot(gs[0, 0])\n\n\nsns.heatmap(model_eval, cmap=cmap,annot=True,fmt=\".2%\", linewidths=5,cbar=False,ax=ax0,annot_kws={\"fontsize\":12})\nax0.tick_params(axis=u'both', which=u'both',length=0)\n\nax0.tick_params(axis='both', colors=sub_col)\nax0.tick_params(axis=u'both', which=u'both',length=0)\nax0.set_ylabel('') \n\nax0.text(0,-0.75,'Model performance overview',fontsize=title,fontweight=b,color=col)\nax0.text(0,-0.2,'Both tree-based methods, Random Forest and\\nXGBoost, performed the best. Logistic Regression\\nalso performed well.',fontsize=sub,color=sub_col)\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random Forest is the best performing model so far.\n\nLet's try stacking our three best performing models","metadata":{}},{"cell_type":"code","source":"# Model stacking\n\nscv=StackingCVClassifier(classifiers=[xgb,rf,lr],meta_classifier= rf,random_state=42)\nscv.fit(X_train,y_train)\nscv_predicted = scv.predict(X_test)\nscv_conf_matrix = confusion_matrix(y_test, scv_predicted)\nscv_acc_score = accuracy_score(y_test, scv_predicted)\nprint(\"confussion matrix\")\nprint(scv_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of StackingCVClassifier:\",scv_acc_score*100,'\\n')\nprint(classification_report(y_test,scv_predicted))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ... work in progress","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}