{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nHyperparameter tuning has always been an iterative process that would take hours and might even take weeks for the beginner. Like for example sklearn's XGB Classifier, it has more than 13 parameters to tweak. Without enough experience, things could get dreary pretty quick.\n\nIn this notebook, we will be implementing genetic evolutionary algorithms (or in short evolutionary algorithm) to tune the hyperparameters of a GradientBoostingClassifier model to make model training less of a iterative process but an automatic one.\n\nThere are 4 types of search methods, namely;\n1. Manual search\n2. Grid search\n3. Random search\n4. Genetic search\n\nBenefits of genetic search over the rest of the search methods are;\n1. Faster and completely automatic convergence to a global minima\n2. Better scores due to iterating over denser search space\n3. Lesser parameters to handle, reducing search space\n\nDuring tuning, normally you will only input hyperparameters to the nearest 1 or 2 decimal point like learning_rate=0.01, but with genetic algorithms you will be able to train with more concise values, like 0.0123, that might result in better model performance.\n\nDrawbacks of genetic search;\n1. Possibility of premature convergence to a local minima\n2. Large population might take as long as grid search\n3. Takes more code to implement","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport re, os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm.notebook import tqdm\n\nPATH = '../input/tabular-playground-series-apr-2021'\ntrain = pd.read_csv(os.path.join(PATH, 'train.csv'))\ntest = pd.read_csv(os.path.join(PATH, 'test.csv'))\nsubmission = pd.read_csv(os.path.join(PATH, 'sample_submission.csv'))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"def ageImputer(Id):\n    if data['Age'][Id] == 0:\n        if (data['Pclass'][Id]==1) & (data['Sex'][Id]==0):\n            return train[(train['Pclass']==1) & (train['Sex']==0)]['Age'].median()\n        elif (data['Pclass'][Id]==1) & (data['Sex'][Id]==1):\n            return train[(train['Pclass']==1) & (train['Sex']==1)]['Age'].median()\n        elif (data['Pclass'][Id]==2) & (data['Sex'][Id]==0):\n            return train[(train['Pclass']==2) & (train['Sex']==0)]['Age'].median()\n        elif (data['Pclass'][Id]==2) & (data['Sex'][Id]==1):\n            return train[(train['Pclass']==2) & (train['Sex']==1)]['Age'].median()\n        elif (data['Pclass'][Id]==3) & (data['Sex'][Id]==0):\n            return train[(train['Pclass']==1) & (train['Sex']==0)]['Age'].median()\n        elif (data['Pclass'][Id]==3) & (data['Sex'][Id]==1):\n            return train[(train['Pclass']==1) & (train['Sex']==1)]['Age'].median()\n    else:\n        return data['Age'][Id]\n\ndef fareImputer(Id):\n    if data['Fare'][Id] == 0:\n        if data['Pclass'][Id]==1:\n            return train[train['Pclass']==1]['Fare'].mean()\n        elif data['Pclass'][Id]==2:\n            return train[train['Pclass']==2]['Fare'].mean()\n        elif data['Pclass'][Id]==3:\n            return train[train['Pclass']==3]['Fare'].mean()\n    else:\n        return data['Fare'][Id]\n\ndef cabin_map(x):\n    if x in [\"T\", \"G\", \"F\"]:\n        return \"E\"\n    else:\n        return x\n\nfor i, data in zip(['train', 'test'], [train, test]):\n    # Sex\n    data['Sex'] = pd.Categorical(data['Sex']).codes\n    # Pclass\n    data['Pclass'] = data['Pclass'].astype(np.uint8)\n    # Age\n    data['Age'] = data['Age'].fillna(0)\n    tqdm.pandas(desc=f\"{i} Age Imputation\")\n    data['Age'] = data.reset_index()['index'].progress_apply(ageImputer)\n    # Parch/SibSp\n    data['familySize'] = data['Parch'] + data['SibSp'] + 1\n    data['isAlone'] = pd.Categorical(data['familySize']==0).codes\n    # Fare\n    data['Fare'] = data['Fare'].fillna(0)\n    tqdm.pandas(desc=f\"{i} Fare Imputation\")\n    data['Fare'] = data.reset_index()['index'].progress_apply(fareImputer)\n    data['Fare'] = np.log1p(data['Fare'])\n    # Cabin\n    data['Cabin'] = data['Cabin'].fillna(\"M\").str[0]\n    data['Cabin'] = data['Cabin'].apply(cabin_map)\n    # Embarked\n    data['Embarked'] = data['Embarked'].fillna(train['Embarked'].mode()[0])\n    # Name\n    data['Name'] = data['Name'].str.split(', ', expand=True)[1]\n    # Ticket\n    data['Ticket'] = data['Ticket'].fillna('M').astype(str)\n    for i in tqdm(data.reset_index()['index'], desc=f\"{i} Ticket Discretization\"):\n        text = data['Ticket'][i]\n        if re.findall(r\"^(PC)\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"PC\")\n        elif re.findall(r\"(A.|A/5.|A/4|A/4.|A4)\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"A\")\n        elif re.findall(r\"(STON|SOTON)\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"SOTON\")\n        elif re.findall(r\"(W./C.|W/C)\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"WC\")   \n        elif re.findall(r\"(F.C.)\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"FCC\") \n        elif re.findall(r\"(SC|C|SO/C|S.O.C.|SCO)\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"SOC\")\n        elif re.findall(r\"(S.W.|SW)\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"SW\")\n        elif re.findall(r\"(S.O./P|S.P|S.O.P.|PP)\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"SOP\")\n        elif re.findall(r\"(WE|W.E.P.)\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"WEP\")\n        elif re.findall(r\"(Fa|FA)\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"FA\")\n        elif re.findall(r\"(LP|Lp)\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"LP\")\n        elif re.findall(r\"^[1][\\d]{4}$\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"1xxxx\")\n        elif re.findall(r\"^[1][\\d]{5}$\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"1xxxxx\")\n        elif re.findall(r\"^[2][\\d]{4}$\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"2xxxx\")\n        elif re.findall(r\"^[2][\\d]{5}$\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"2xxxxx\")  \n        elif re.findall(r\"^[3][\\d]{3}$\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"3xxx\")\n        elif re.findall(r\"^[3][\\d]{4}$\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"3xxxx\")\n        elif re.findall(r\"^[3][\\d]{5}$\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"3xxxxx\") \n        elif re.findall(r\"^[3][\\d]{6}$\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"3xxxxxx\")\n        elif re.findall(r\"^[4][\\d]{3}$\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"4xxx\")\n        elif re.findall(r\"^[4][\\d]{4}$\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"4xxxx\")\n        elif re.findall(r\"^[4][\\d]{5}$\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"4xxxxxx\")\n        elif re.findall(r\"^[4][\\d]{6}$\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"4xxxxxx\")\n        elif re.findall(r\"^[5][\\d]{3}$\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"5xxx\")\n        elif re.findall(r\"^[5][\\d]{4}$\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"5xxxx\")\n        elif re.findall(r\"^[5][\\d]{5}$\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"5xxxxx\")\n        elif re.findall(r\"^[6][\\d]{3}$\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"6xxx\")\n        elif re.findall(r\"^[6][\\d]{4}$\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"6xxxx\")\n        elif re.findall(r\"^[7][\\d]{3}$\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"7xxx\")\n        elif re.findall(r\"^[7][\\d]{4}$\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"7xxxx\")\n        elif re.findall(r\"^[8][\\d]{3}$\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"8xxx\")\n        elif re.findall(r\"^[9][\\d]{3}$\", text) != []:\n            data['Ticket'] = data['Ticket'].replace(text, \"9xxx\")\n    ticket_bins = [0, 2500, 5000, 7500, 10000, 12500, 15000, 17500]\n    data['TicketFreq'] = data['Ticket'].map(data['Ticket'].value_counts().to_dict())\n    data['TicketFreq'] = pd.cut(data['TicketFreq'], ticket_bins, labels=range(len(ticket_bins)-1))\n\n# Outliers\ntrain = train[train['Fare']>=1]\nfor data in [train, test]:\n    age_bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]\n    data['Age'] = pd.cut(data['Age'], age_bins, labels=range(len(age_bins)-1))\n\ndata = pd.concat([train, test], axis=0).reset_index(drop=True)\ndata.drop(['PassengerId'], axis=1, inplace=True)\n\n# Label encode\nfor feature in [\"Cabin\", \"Embarked\", \"Name\"]:\n    data[feature] = pd.Categorical(data[feature]).codes\n\n# One-hot encode\ndata = pd.get_dummies(data, columns=[\"Sex\", \"Cabin\", 'Ticket', \"Embarked\"])\n\ntrain = data.iloc[:train.shape[0], :]\ntest = data.iloc[train.shape[0]:, :].reset_index(drop=True)\ntest.drop([\"Survived\"], axis=1, inplace=True)\n\nprint(\"train shape:\", train.shape)\nprint(\"test shape:\", test.shape)\n\ntrain.to_csv(\"train_cleaned.csv\", index=False)\ntest.to_csv(\"test_cleaned.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nfor i, feature in enumerate(['Age', 'Fare', 'familySize', 'Pclass', 'isAlone', 'TicketFreq']):\n    plt.subplot(3, 2, i+1)\n    sns.histplot(x=train[feature], hue=train['Survived'], discrete=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning\n\n#### eXtreme Gradient Boosting Classifier (XGB) (scikit-learn)\n\n|Features|Options|Default|Bounds|\n|:-|:-:|:-:|:-:|\n|loss|{'deviance', 'exponential'}|'deviance'|0.0 to 1.0|\n|learning_rate|float|0.1|0.1 to 0.5|\n|n_estimators|Int|100|1 to 500|\n|subsample|float|1.0|0.1 to 1.0|\n|criterion|{\"friedman_mse\", \"mse\"}|\"friedman_mse\"|0.0 to 1.0|\n|min_samples_split|Int or float|2|2 to 10|\n|min_samples_leaf|Int or float|1|1 to 10|\n|min_weight_fraction_leaf|float|0.0|0.0 to 0.5|\n|max_depth|Int|None|2 to 100|\n|min_impurity_decrease|Float|0.0|0.0 to 1.0|\n|max_features|{\"auto\", \"sqrt\", \"log2\"}|\"auto\"|0.0 to 2.0|\n|max_leaf_nodes|Int|None|2 to 100|\n|ccp_alpha|Float|0.0|0.0 to 1.0|","metadata":{}},{"cell_type":"markdown","source":"## Tuning functions\n\nFor this module, we will be designing our individuals that will make up our simulated population.\n\nOur individuals will have chromosomes that are made up of an array of random float numbers. These float numbers are the settings of the hyperparameters that we will be testing with.\n\nWe have also incorporated a 5-fold stratified cross validation splits and we will be using accuracy score as our fitness function.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\n\nclass XGBGeneticTuning:\n    NUM_FOLDS = 5\n\n    def __init__(self, randomSeed):\n        self.randomSeed = randomSeed\n        self.init_data()\n        self.skf = StratifiedKFold(\n            n_splits=self.NUM_FOLDS,\n            random_state=self.randomSeed,\n            shuffle=True\n        )\n\n    def init_data(self):\n        self.data = pd.read_csv(\"./train_cleaned.csv\")\n        self.X = self.data.iloc[:, 1:]\n        self.y = self.data.iloc[:, 0]\n\n    def convertParams(self, params):\n        loss = [\"deviance\", \"exponential\"][round(params[0])]\n        learning_rate = params[1]\n        n_estimators = round(params[2])\n        subsample = params[3]\n        criterion = [\"friedman_mse\", \"mse\", \"mae\"][round(params[4])]\n        min_samples_split = round(params[5])\n        min_samples_leaf = round(params[6])\n        min_weight_fraction_leaf = params[7]\n        max_depth = round(params[8])\n        min_impurity_decrease = params[9]        \n        max_features = [\"auto\", \"sqrt\", \"log2\", None][round(params[10])]\n        max_leaf_nodes = round(params[11])\n        ccp_alpha = params[12]        \n        return loss, learning_rate, n_estimators, subsample, criterion, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_depth, min_impurity_decrease, max_features, max_leaf_nodes, ccp_alpha\n        \n    def getAccuracy(self, params):              \n        (loss, learning_rate, n_estimators, subsample, criterion,\n         min_samples_split, min_samples_leaf, min_weight_fraction_leaf,\n         max_depth, min_impurity_decrease, max_features, max_leaf_nodes,\n         ccp_alpha) = self.convertParams(params)\n            \n        self.classifier = GradientBoostingClassifier(\n            random_state=self.randomSeed,\n            loss=loss,\n            learning_rate=learning_rate,\n            n_estimators=n_estimators,\n            subsample=subsample,\n            criterion=criterion,\n            min_samples_split=min_samples_split,\n            min_samples_leaf=min_samples_leaf,\n            min_weight_fraction_leaf=min_weight_fraction_leaf,            \n            max_depth=max_depth,\n            min_impurity_decrease=min_impurity_decrease,\n            max_features=max_features,\n            max_leaf_nodes=max_leaf_nodes,\n            ccp_alpha=ccp_alpha,\n        )\n        scores = []\n        for train_idx, val_idx in self.skf.split(self.X, self.y):\n            X_train = self.X.iloc[train_idx, :]\n            X_val = self.X.iloc[val_idx, :]\n            y_train = self.y.iloc[train_idx]\n            y_val = self.y.iloc[val_idx]\n\n            self.classifier.fit(X_train, y_train)            \n            preds = self.classifier.predict(X_val)\n\n            score = accuracy_score(y_val, preds)\n            scores.append(score)\n\n        return np.mean(scores)\n\n    def formatParams(self, params):\n        return \"'loss'=%s, 'learning_rate'=%1.3f, 'n_estimators'=%3d, 'subsample'=%1.3f, 'criterion'=%s, 'min_samples_split'=%3d, 'min_samples_leaf'=%3d, 'min_weight_fraction_leaf'=%1.3f, 'max_depth'=%3d, 'min_impurity_decrease'=%1.3f, 'max_features'=%s, 'max_leaf_nodes'=%3d, 'ccp_alpha'=%1.3f\" % (self.convertParams(params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Genetic Operators\n\nIn this section, we will be designing the genetic flow using DEAP framework. It mainly consists of 5 stages; \n\n1. Population Generator\n2. Selection\n3. Crossover/Mating\n4. Mutation\n5. Evaluation\n\nStage 1 is where we initialize the population with random individuals. They will then be evaluated for their fitness functions followed by a selection phase to obtain the best candidates for crossover/mating (parents) in stage 2. Afterwards, in stage 3, the offspring from that crossover phase will be further mutated, combining the traits of its parents. Lastly, in stage 5, the newly generated population will be evaluated for their fitness functions.\n\nThis sums up the genetic flow for a single generation and will be repeated for a number of generations.\n\nWe will be using a tournament mechanism where n-individuals will be put into a survival-of-the-fittest setting where the best individuals will survive till the next generation. The individuals will be evaluated using their computed fitness functions while weak individuals will be dropped off at the end of each bout.","metadata":{}},{"cell_type":"code","source":"from deap import base, creator, tools, algorithms\n\nimport random\n\n# Initialize Toolbox\ntoolbox = base.Toolbox()\n\n# Initialize constants\nPOPULATION_SIZE = 30\nP_CROSSOVER = 0.9\nP_MUTATION = 0.5\nMAX_GENERATIONS = 30\nHALL_OF_FAME_SIZE = 5\nCROWDING_FACTOR = 20.0\n\n# Initialize state space\nBOUNDS_LOW =  [0.0, 0.1, 100, 0.1, 0.0,  2,  1, 0.0,   2, 0.0, 0.0,   2, 0.0]\nBOUNDS_HIGH = [1.0, 0.5, 250, 1.0, 1.0, 10, 10, 0.5, 100, 1.0, 3.0, 100, 1.0]\nNUM_OF_PARAMS = len(BOUNDS_HIGH)\n\n# Set random seed\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\ntuning = XGBGeneticTuning(RANDOM_SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitness strategy\ncreator.create(\n    name=\"FitnessMax\",\n    base=base.Fitness,\n    weights=(1.0,)\n)\n\n# Individuals list container\ncreator.create(\n    \"Individual\",\n    list,\n    fitness=creator.FitnessMax\n)\n\n# Register a random float operator for each hyperparameter\nfor i in range(NUM_OF_PARAMS):\n    toolbox.register(\n        alias=\"hyperparameter_\"+str(i),\n        function=random.uniform,\n        a=BOUNDS_LOW[i],\n        b=BOUNDS_HIGH[i]\n    )\n    \nhyperparameters = ()\nfor i in range(NUM_OF_PARAMS):\n    hyperparameters += (toolbox.__getattribute__(\"hyperparameter_\" + str(i)),)\n\n# Population Generator\ntoolbox.register(\n    \"individualCreator\",\n    tools.initCycle,\n    creator.Individual,\n    hyperparameters,\n    n=1\n)\n\ntoolbox.register(\n    alias=\"populationCreator\",\n    function=tools.initRepeat,\n    container=list,\n    func=toolbox.individualCreator\n)\n\n# Evaluation operator\ndef classificationAccuracy(individual):\n    return tuning.getAccuracy(individual),\n\ntoolbox.register(\n    alias=\"evaluate\",\n    function=classificationAccuracy\n)\n\n# Selection operator\ntoolbox.register(\n    alias=\"select\",\n    function=tools.selTournament,\n    tournsize=3\n)\n\n# Crossover operator\ntoolbox.register(\n    alias=\"mate\",\n    function=tools.cxSimulatedBinaryBounded,\n    low=BOUNDS_LOW,\n    up=BOUNDS_HIGH,\n    eta=CROWDING_FACTOR\n)\n\n# Mutation operator\ntoolbox.register(\n    alias=\"mutate\",\n    function=tools.mutPolynomialBounded,\n    low=BOUNDS_LOW,\n    up=BOUNDS_HIGH,\n    eta=CROWDING_FACTOR,\n    indpb=1.0/NUM_OF_PARAMS\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Genetic Search\n\n### Simple Evolutionary Algorithm","metadata":{}},{"cell_type":"code","source":"def GAsimple():\n    population = toolbox.populationCreator(n=POPULATION_SIZE)\n\n    stats = tools.Statistics(lambda ind: ind.fitness.values)\n    stats.register(\"max\", np.max)\n    stats.register(\"avg\", np.mean)\n\n    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n\n    print(\"Training...\")\n    population, logbook = algorithms.eaSimple(\n        population=population,\n        toolbox=toolbox,\n        cxpb=P_CROSSOVER,\n        mutpb=P_MUTATION,\n        ngen=MAX_GENERATIONS,\n        stats=stats,\n        halloffame=hof,\n        verbose=True\n    )\n\n    print(\"- Best solution:\", tuning.formatParams(hof.items[0]))\n    print(\"- Best accuracy:\", hof.items[0].fitness.values[0])\n\n    maxFitnessValues, meanFitnessValues = logbook.select(\"max\", \"avg\")\n\n    sns.set_style(\"whitegrid\")\n    plt.plot(maxFitnessValues, color='red', label='max')\n    plt.plot(meanFitnessValues, color='green', label='avg')\n    plt.xlabel(\"Generation\")\n    plt.ylabel(\"Fitness\")\n    plt.title(\"Max/Avg Fitness over Generations\")\n    plt.legend()\n    plt.show()\n\n    return tuning.convertParams(hof.items[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params = GAsimple()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission\n\nWe will refit the model with all the data with the best found parameters.","metadata":{}},{"cell_type":"code","source":"X = train.iloc[:, 1:]\ny = train.iloc[:, 0]\n\nxgb = GradientBoostingClassifier(\n    random_state=RANDOM_SEED,\n    loss=best_params[0],\n    learning_rate=best_params[1],\n    n_estimators=best_params[2],\n    subsample=best_params[3],\n    criterion=best_params[4],\n    min_samples_split=best_params[5],\n    min_samples_leaf=best_params[6],\n    min_weight_fraction_leaf=best_params[7],            \n    max_depth=best_params[8],\n    min_impurity_decrease=best_params[9],\n    max_features=best_params[10],\n    max_leaf_nodes=best_params[11],\n    ccp_alpha=best_params[12],\n).fit(X, y)\n\nsubmission['Survived'] = xgb.predict(test).astype(int)\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}