{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorch-tabnet","metadata":{"_uuid":"37898279-6116-4834-8a52-62552a8269d6","_cell_guid":"257680f3-d006-4d36-9878-bb55b3a479d6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport time\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.options.display.max_rows = None\npd.options.display.max_columns = None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def splitData(df: \"pd.DataFrame\", FEATS: \"List\"):\n    \"\"\"Split the dataframe into train and test\n    \n    Args:\n        df: preprocessed dataframe\n        FEATS: feature list\n        \n    Returns:\n        X_train, y_train, X_test, y_test\n    \"\"\"\n    \n    train, test = train_test_split(df, test_size = .3, random_state = 42)\n    \n    return train[FEATS], train[\"Survived\"], test[FEATS], test[\"Survived\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepareInputs(df: \"pd.DataFrame\"):\n    \"\"\"Preprocess\n    \n    Args:\n        df: raw dataframe\n    \n    Return:\n        df: processed dataframe\n    \"\"\"\n    \n    # 1: Inpute missing values\n    df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].mean())\n    df[\"Fare\"] = df[\"Fare\"].fillna(df[\"Fare\"].mean())\n    \n    # 2: One hot encoding\n    df[\"Cabin_category\"] = [str(cabin_category)[0] for cabin_category in df[\"Cabin\"]]\n    \n    categorical_cols = [\n        \"Pclass\", \"Sex\", \"SibSp\",\n        \"Parch\", \"Embarked\", \n        \"Cabin_category\"\n                        ]\n    \n    for col in categorical_cols:\n        dummies = pd.get_dummies(df[col], \n                                 drop_first = True,\n                                 prefix = col\n                                )\n        df = pd.concat([df, dummies], 1)\n        \n    df = df.drop(categorical_cols, 1)\n    \n    df[\"Ticket_A/5\"] = np.where(df[\"Age\"] == \"A/5\", 1, 0)\n    df[\"Ticket_C.A\"] = np.where(df[\"Age\"] == \"C.A.\", 1, 0)\n    df[\"Ticket_SC/PARIS\"] = np.where(df[\"Age\"] == \"SC/PARIS\", 1, 0)\n    df = df.drop([\"Cabin\", \"Ticket\"], 1)\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardise the data sets\ndef standardiseNumericalFeats(df):\n    \"\"\"Standardise the numerical features\n    \n    Returns:\n        Standardised dataframe\n    \"\"\"\n\n    numerical_cols = [\n        \"Age\", \"Fare\"\n    ]\n\n    for col in numerical_cols:\n        scaler = StandardScaler()\n\n        df[col] = scaler.fit_transform(df[[col]])\n        \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tabNetPretrain(X_train):\n    \"\"\"Pretrain TabNet model\n    \n    Return:\n        TabNet pretrainer obj\n    \"\"\"\n    tabnet_params = dict(n_d=8, n_a=8, n_steps=3, gamma=1.3,\n                             n_independent=2, n_shared=2,\n                             seed=42, lambda_sparse=1e-3,\n                             optimizer_fn=torch.optim.Adam,\n                             optimizer_params=dict(lr=2e-2,\n                                                   weight_decay=1e-5\n                                                  ),\n                             mask_type=\"entmax\",\n                             scheduler_params=dict(max_lr=0.05,\n                                                   steps_per_epoch=int(X_train.shape[0] / 256),\n                                                   epochs=200,\n                                                   is_batch_level=True\n                                                  ),\n                             scheduler_fn=torch.optim.lr_scheduler.OneCycleLR,\n                             verbose=10\n                        )\n\n    pretrainer = TabNetPretrainer(**tabnet_params)\n\n    pretrainer.fit(\n        X_train=X_train.to_numpy(),\n        eval_set=[X_train.to_numpy()],\n        max_epochs = 100,\n        patience = 15, \n        batch_size = 256, \n        virtual_batch_size = 128,\n        num_workers = 1, \n        drop_last = True)\n    \n    return pretrainer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trainTabNetModel(X_train, y_train, X_test, y_test, pretrainer):\n    \"\"\"Train TabNet model\n    \n    Args:\n        pretrainer: pretrained model. If not using this, use None\n        \n    Return:\n        TabNet model obj\n    \"\"\"\n    \n    tabNet_model = TabNetClassifier(\n                                   n_d=8,\n                                   n_a=8,\n                                   n_steps=4,\n                                   gamma=1.3,\n                                   n_independent=4,\n                                   n_shared=5,\n                                   seed=42,\n                                   optimizer_fn = torch.optim.Adam,\n                                   scheduler_params = {\"milestones\": [150,250,300,350,400,450],'gamma':0.2},\n                                   scheduler_fn=torch.optim.lr_scheduler.MultiStepLR\n                                  )\n\n    tabNet_model.fit(\n        X_train = X_train.to_numpy(),\n        y_train = y_train.to_numpy(),\n        eval_set=[(X_train.to_numpy(), y_train.to_numpy()),\n                  (X_test.to_numpy(), y_test.to_numpy())],\n        eval_metric=['accuracy'],\n        max_epochs = 100,\n        batch_size = 256,\n        patience = 15,\n        from_unsupervised = pretrainer\n        )\n    \n    return tabNet_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions\ndef makePredictions(X_test, tabNet_model) -> \"pd.DataFrame\":\n    \"\"\"Make predictions\n    \n    Return:\n        Predictions\n    \"\"\"\n    \n    return tabNet_model.predict_proba(X_test.to_numpy())[:,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluation\ndef evaluate(y_test, y_tabNet_pred) -> None:\n    \"\"\"Evaluate the predictions\n    \n    Process:\n        Print accuracy score\n    \"\"\"\n    \n    print(\"The accuracy score of TabNet model is \" +\n          str(round(accuracy_score(y_test, np.where(y_tabNet_pred > .42, 1, 0)), 4))\n         )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FEATS = [\n        \"Pclass\", \"Sex\", \"Age\",\n        \"SibSp\", \"Parch\", \"Ticket\",\n        \"Fare\", \"Cabin\", \"Embarked\"\n    ]\n\nprint(\"Reading the data\")\ndf = pd.read_csv(\"../input/tabular-playground-series-apr-2021/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-apr-2021/test.csv\")\nsubmission = pd.read_csv(\"../input/tabular-playground-series-apr-2021/sample_submission.csv\")\n\nprint(\"Preprocessing the data\")\nX_train, y_train, X_validation, y_validation = splitData(df, FEATS)\nX_train, X_validation = prepareInputs(X_train), prepareInputs(X_validation)\nX_train, X_validation = standardiseNumericalFeats(X_train), standardiseNumericalFeats(X_validation)\n\nprint(\"The ratio of lapse class in training set is \" +\n      str(round(y_train.sum()/len(y_train) * 100, 2)) +\n      \"%\"\n     )\n\nprint(\"The ratio of lapse class in validation set is \" +\n      str(round(y_validation.sum()/len(y_validation) * 100, 2)) +\n      \"%\"\n     )\n\nprint(\"Pretrain TabNet model\")\npretrainer = tabNetPretrain(X_train)\n\nprint(\"Training TabNet model\")\ntabNet_model = trainTabNetModel(X_train, y_train, X_validation, y_validation, pretrainer)\n\nprint(\"Making validation predictions\")\ny_tabNet_pred = makePredictions(X_validation, tabNet_model)\n\nprint(\"Evaluation of the model\")\nevaluate(y_validation, y_tabNet_pred)\n\nprint(\"Making predictions\")\ntest = prepareInputs(test[FEATS])\ntest = standardiseNumericalFeats(test)\n\nsubmission[\"Survived\"] = makePredictions(test, tabNet_model)\nsubmission[\"Survived\"] = np.where(submission[\"Survived\"] > .42, 1, 0)\n\nsubmission.to_csv(\"submission.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature importance","metadata":{}},{"cell_type":"code","source":"# TabNet model\nimportance_tabNet = pd.DataFrame(tabNet_model.feature_importances_,index=X_train.columns).sort_values(0, ascending = False)\nimportance_tabNet.columns = [\"importance\"]\nimportance_tabNet","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction distribution","metadata":{}},{"cell_type":"code","source":"plt.hist(y_tabNet_pred, bins = 100)\nplt.title(\"Prediction distribution of pretrained TabNet\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}