{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem definition\n\nThe dataset is used for this competition is synthetic but based on a real dataset (in this case, the actual Titanic data!) and generated using a CTGAN.\n\nData description: \n\n| Variable        | Definition           | Key  |\n|---------------|:-------------|------:|\n|survival |\tSurvival | 0 = No, 1 = Yes |\n|pclass |\tTicket class | 1 = 1st, 2 = 2nd, 3 = 3rd |\n|sex |\tSex\t ||\n|Age |\tAge in years\t ||\n|sibsp |\t# of siblings / spouses aboard the Titanic\t ||\n|parch |\t# of parents / children aboard the Titanic\t ||\n|ticket |\tTicket number\t ||\n|fare |\tPassenger fare\t ||\n|cabin |\tCabin number\t| |\n|embarked |\tPort of Embarkation\t| C = Cherbourg, Q = Queenstown, S = Southampton |\n\n<br>\n\nWhere `survival` will be our target variable! üéØ\n\n<br>\n\nCheck out: \n\n  ‚ûú [TPS-Apr2021 EDA Profiling + RF Pipeline Baseline](https://www.kaggle.com/gomes555/tps-apr2021-eda-profiling-rf-pipeline-baseline)\n\n  ‚ûú [Tuning of a Lightgbm with Bayesian Optimization using the `tidymodels` framework in R](https://www.kaggle.com/gomes555/tps-apr2021-r-eda-lightgbm-bayesopt)\n\n  ‚ûú [AutoML (lgbm + catboost) with mljar](https://www.kaggle.com/gomes555/tps-apr2021-autoboost-mljar)\n  \n  ‚ûú [Feature Selection with RFE + Boruta](https://www.kaggle.com/gomes555/tps-apr2021-feature-selection-rfe-boruta)\n  \n  ‚ûú [Simple CatBoost + Preprocess](https://www.kaggle.com/gomes555/tps-apr2021-simple-catboost)\n  \n  ‚ûú [CatBoost + Pseudo + MovingThreshold](https://www.kaggle.com/gomes555/tps-apr2021-catboost-pseudo-movingthreshold)\n  \n  ‚ûú [Catboost + combination of techniques + Optuna](https://www.kaggle.com/gomes555/tps-apr2021-catboost-optuna)\n  \nStrongly inspired by:\n\n  ‚ûú [https://www.kaggle.com/hiro5299834/tps-apr-2021-voting-pseudo-labeling](https://www.kaggle.com/hiro5299834/tps-apr-2021-voting-pseudo-labeling)\n  \n  ‚ûú [https://www.kaggle.com/belov38/catboost-lb](https://www.kaggle.com/belov38/catboost-lb)\n  \n  ‚ûú [https://www.kaggle.com/remekkinas/ensemble-learning-meta-classifier-for-stacking/output\n](https://www.kaggle.com/remekkinas/ensemble-learning-meta-classifier-for-stacking/output)\n  \n<br>\n\n<p align=\"right\"><span style=\"color:firebrick\">Dont forget the upvote if you liked the notebook! ‚úåÔ∏è </p>","metadata":{"papermill":{"duration":0.019328,"end_time":"2021-04-12T00:37:52.284424","exception":false,"start_time":"2021-04-12T00:37:52.265096","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom catboost import CatBoostClassifier\nimport category_encoders as ce\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score","metadata":{"papermill":{"duration":1.73304,"end_time":"2021-04-12T00:37:54.03572","exception":false,"start_time":"2021-04-12T00:37:52.30268","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"../input/tabular-playground-series-apr-2021/\"\ntrain = pd.read_csv(path+'train.csv', index_col=0)\ntest = pd.read_csv(path+'test.csv', index_col=0)\npseudo_label = pd.read_csv('../input/tps-apr-2021-pseudo-labeling-voting-ensemble/voting_submission.csv', index_col=0)\nsubmission = pd.read_csv(path+'sample_submission.csv')","metadata":{"_execution_state":"idle","_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","papermill":{"duration":0.710878,"end_time":"2021-04-12T00:37:54.765337","exception":false,"start_time":"2021-04-12T00:37:54.054459","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pseudo-label from https://www.kaggle.com/hiro5299834/tps-apr-2021-voting-pseudo-labeling\ntest['Survived'] = [x for x in pseudo_label.Survived]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calcule SameFirstName\n\ntrain['FirstName'] = train['Name'].apply(lambda x:x.split(', ')[0])\ntrain['n'] = 1\ngb = train.groupby('FirstName')\ndf_names = gb['n'].sum()\ntrain['SameFirstName'] = train['FirstName'].apply(lambda x:df_names[x])\n\ntest['FirstName'] = test['Name'].apply(lambda x:x.split(', ')[0])\ntest['n'] = 1\ngb = test.groupby('FirstName')\ndf_names = gb['n'].sum()\ntest['SameFirstName'] = test['FirstName'].apply(lambda x:df_names[x])\n\n# To preprocess\n\ndata = pd.concat([train, test], axis=0)\n\n# Before fill missing\ndata['AnyMissing'] = np.where(data.isnull().any(axis=1) == True, 1, 0)\n\n# Family\ndata['FamilySize'] = data['SibSp'] + data['Parch'] + 1\ndata['IsAlone'] = np.where(data['FamilySize'] <= 1, 1, 0)\n\n# Cabin\ndata['Has_Cabin'] = data[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ndata['Cabin'] = data['Cabin'].fillna('X').map(lambda x: x[0].strip())\ncabin_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5,\n             'F': 6, 'G': 7, 'T': 1, 'X': 8}\ndata['Cabin'] = data['Cabin'].str[0].fillna('X').replace(cabin_map)\n\n# Embarked\n#map_Embarked = train.Embarked.mode().item()\ndata['Embarked'] = data['Embarked'].fillna(\"No\")\nconditions = [\n    (data['Embarked']==\"S\"),\n    (data['Embarked']==\"Q\"),\n    (data['Embarked']==\"C\"),\n    (data['Embarked']==\"No\")\n]\nchoices = [0, 1, 2, -1]\ndata[\"Embarked\"] = np.select(conditions, choices)\ndata['Embarked'] = data['Embarked'].astype(int)\n\n# Name\ndata['SecondName'] = data.Name.str.split(', ', 1, expand=True)[1] # to try\ndata['IsFirstNameDublicated'] = np.where(data.FirstName.duplicated(), 1, 0)\n\n# Fare\ndata['Fare'] = data['Fare'].fillna(train['Fare'].median())\n# train['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n# [(0.679, 10.04] < (10.04, 24.46] < (24.46, 33.5] < (33.5, 744.66]]\n# From original Titanic:\nconditions = [\n    (data['Fare'] <= 7.91),\n    ((data['Fare'] > 7.91) & (data['Fare'] <= 14.454)),\n    ((data['Fare'] > 14.454) & (data['Fare'] <= 31)),\n    (data['Fare'] > 31)\n]\n\nchoices = [0, 1, 2, 3]\ndata[\"Fare\"] = np.select(conditions, choices)\ndata['Fare'] = data['Fare'].astype(int)\n\n# Fix Ticket\n# data['TicketNum'] = data.Ticket.str.extract(r'(\\d+)').\\\n#                     astype('float64', copy=False) # to_try\ndata['Ticket'] = data.Ticket.str.replace('\\.','', regex=True).\\\n                    str.replace('(\\d+)', '', regex=True).\\\n                    str.replace(' ', '', regex=True).\\\n                    replace(r'^\\s*$', 'X', regex=True).\\\n                    fillna('X')\n\n#data['Ticket'] = data['Ticket'].astype('category').cat.codes # to_try\n\n# Age \nconditions = [\n    ((data.Sex==\"female\")&(data.Pclass==1)&(data.Age.isnull())),\n    ((data.Sex==\"male\")&(data.Pclass==1)&(data.Age.isnull())),\n    ((data.Sex==\"female\")&(data.Pclass==2)&(data.Age.isnull())),\n    ((data.Sex==\"male\")&(data.Pclass==2)&(data.Age.isnull())),\n    ((data.Sex==\"female\")&(data.Pclass==3)&(data.Age.isnull())),\n    ((data.Sex==\"male\")&(data.Pclass==3)&(data.Age.isnull()))\n]\nchoices = data[['Age', 'Pclass', 'Sex']].\\\n            dropna().\\\n            groupby(['Pclass', 'Sex']).\\\n            mean()['Age']\n\ndata[\"Age\"] = np.select(conditions, choices)\n\nconditions = [\n    (data['Age'].le(16)),\n    (data['Age'].gt(16) & data['Age'].le(32)),\n    (data['Age'].gt(32) & data['Age'].le(48)),\n    (data['Age'].gt(48) & data['Age'].le(64)),\n    (data['Age'].gt(64))\n]\nchoices = [0, 1, 2, 3, 4]\n\ndata[\"Age\"] = np.select(conditions, choices)\n\n# Sex\ndata['Sex'] = np.where(data['Sex']=='male', 1, 0)\n\n# Drop columns\ndata = data.drop(['Name', 'n'], axis = 1)\n\n# Transform object to category\n#for col in data.columns[data.dtypes=='object'].tolist():\n#    data.loc[:,col] = data.loc[:,col].astype('category')","metadata":{"papermill":{"duration":4.670843,"end_time":"2021-04-12T00:37:59.454807","exception":false,"start_time":"2021-04-12T00:37:54.783964","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting into train and test\ntrain = data.iloc[:train.shape[0]]\ntest = data.iloc[train.shape[0]:].drop(columns=['Survived'])","metadata":{"papermill":{"duration":0.04076,"end_time":"2021-04-12T00:37:59.513699","exception":false,"start_time":"2021-04-12T00:37:59.472939","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(3)","metadata":{"papermill":{"duration":0.0597,"end_time":"2021-04-12T00:37:59.591773","exception":false,"start_time":"2021-04-12T00:37:59.532073","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lab_cols = ['Pclass','Age', 'Ticket', 'Fare', 'Cabin', 'Embarked']\ntarget = 'Survived'\n\nfeatures_selected = ['Pclass', 'Sex', 'Age','Embarked','Parch','SibSp','Fare','Cabin','Ticket','SameFirstName']\n\nX = data.drop(target, axis=1)\nX = X[features_selected]\ny = data[target]\n\ntest = test[features_selected]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def kfold_prediction(X, y, X_test, K, od_wait = 500):\n\n    yp = pd.DataFrame()\n    trs = []\n    acc_trs = []\n    \n    kf = StratifiedKFold(n_splits=K, shuffle=True, random_state=314)\n    \n    for i, (train_idx, test_idx) in enumerate(kf.split(X, y)):\n        print(f\"\\n FOLD {i} ...\")\n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[test_idx]\n        y_val = y.iloc[test_idx]\n        \n        # https://catboost.ai/docs/concepts/parameter-tuning.html\n        params = {'iterations': 10000,\n                  'use_best_model':True ,\n                  'eval_metric': 'AUC', # 'Accuracy'\n                  'loss_function':'Logloss',\n                  'od_type':'Iter',\n                  'od_wait':od_wait,\n                  'depth': 6, # [4, 10]\n                  'l2_leaf_reg': 3,\n                  # random_strength ??\n                  'bootstrap_type': 'Bayesian',\n                  'bagging_temperature': 2,\n                  'max_bin': 254,\n                  'grow_policy': 'SymmetricTree',\n                  'cat_features': lab_cols,\n                  'verbose': od_wait,\n                  'random_seed': 314\n         }\n        \n        #params = {'loss_function':'Logloss',\n        #          'eval_metric': 'AUC', # 'Accuracy'\n        #          'od_wait':od_wait,\n        #          'od_type':'Iter', \n        #          'n_estimators': 10000,\n        #          'cat_features': lab_cols,\n        #          'verbose': od_wait,\n        #          'random_seed': 314\n        # }\n        \n        clf = CatBoostClassifier(**params)\n        \n        model_fit = clf.fit(X_train,y_train,\n                            eval_set=[(X_train, y_train), (X_val, y_val)],\n                            use_best_model=True,\n                            plot=False)\n        \n        yp_val = model_fit.predict_proba(X_val)[:, 1]\n        acc = accuracy_score(y_val, np.where(yp_val>0.5, 1, 0))\n        print(f\"- Accuracy before : {acc} ...\")\n        \n        # Moving threshold\n        thresholds = np.arange(0.0, 1.0, 0.01)\n        accuracy_scores = []\n        for thresh in thresholds:\n            accuracy_scores.append(\n                accuracy_score(y_val, [1 if m>thresh else 0 for m in yp_val]))\n\n        accuracies = np.array(accuracy_scores)\n        max_accuracy = accuracies.max() \n        max_accuracy_threshold =  thresholds[accuracies.argmax()] \n        trs = trs + [max_accuracy_threshold]\n        \n        print(\"- Max accuracy threshold: \"+str(max_accuracy_threshold))\n        \n        acc = accuracy_score(y_val, \n                             np.where(yp_val>max_accuracy_threshold, 1, 0)) \n        acc_trs = acc_trs + [acc]\n        print(f\"- Accuracy after: {acc} !\")\n        \n        yp_test = model_fit.predict_proba(X_test)[:, 1]\n        yp_fold = pd.DataFrame({\n            'fold'+str(i): np.where(yp_test>max_accuracy_threshold, 1, 0)})\n        \n        yp = pd.concat([yp, yp_fold], axis=1)\n    \n    return yp, trs, acc_trs","metadata":{"papermill":{"duration":0.039766,"end_time":"2021-04-12T00:37:59.785631","exception":false,"start_time":"2021-04-12T00:37:59.745865","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yp, trs, acc = kfold_prediction(X, y, test, 5)","metadata":{"papermill":{"duration":559.575463,"end_time":"2021-04-12T00:47:19.380181","exception":false,"start_time":"2021-04-12T00:37:59.804718","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Model with train + pseudo train')\nprint(\"Final mean and std accuracy: \", np.mean(acc), round(np.std(acc), 5))\nprint(\"Final mean and std accuracy with Threshold: \", np.mean(trs), round(np.std(trs), 5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vote(r, columns):\n    \"\"\"https://www.kaggle.com/belov38/catboost-lb/\"\"\"\n    ones = 0\n    zeros = 0\n    for i in columns:\n        if r[i]==0:\n            zeros+=1\n        else:\n            ones+=1\n    if ones>zeros:\n        return 1\n    else:\n        return 0","metadata":{"papermill":{"duration":0.053984,"end_time":"2021-04-12T00:51:54.133755","exception":false,"start_time":"2021-04-12T00:51:54.079771","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_pseudo = pd.DataFrame({\n    'PassengerId': test.index,\n    'Survived':yp.apply(lambda x:vote(x, yp.columns.tolist()),axis=1)\n})\n\nsubmission_pseudo.to_csv('submission_pseudo_test.csv', index = False) # best 0.80398 LB","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}