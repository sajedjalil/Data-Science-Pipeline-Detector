{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Table of Contents\n<a id=\"table-of-contents\"></a>\n* [1. Introduction](#1)\n* [2. Preparation](#2)\n    * [2.1. Load packages](#2.1)\n    * [2.2. Load dataset](#2.2)\n    * [2.3. Data pre-processing](#2.3)\n* [3. Baseline Model](#3)\n    * [3.1. Catboost](#3.1)\n    * [3.2. XGBoost](#3.2)\n    * [3.3. LGBM](#3.3)\n    * [3.4. Hard Voting](#3.4)\n* [4. Features Engineering](#4)\n    * [4.1. Family](#4.1)\n    * [4.2. Without family](#4.2)\n    * [4.3. First name and last name](#4.3)\n    * [4.4. Ticket](#4.4)\n    * [4.5. Age - binning](#4.5)\n    * [4.6. Fare - binning](#4.6)\n* [5. Baseline Model Post Features Engineering](#5)\n    * [5.1. Catboost](#5.1)\n    * [5.2. XGBoost](#5.2)\n    * [5.3. LGBM](#5.3)\n    * [5.4. Hard Voting](#5.4)\n* [6. Psuedo Labeling](#6)\n    * [6.1 Preparation](#6.1)\n    * [6.2. Features Engineering](#6.2)\n    * [6.3. Train Model & Prediction](#6.3)\n    * [6.4. Submission](#6.4)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false}},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"1\"></a>\n# 1. Introduction\n\nThe notebook will try to explore models and feature engineering performance in predicting whether or not a passenger survived the sinking of the Synthanic (a synthetic, much larger dataset based on the actual Titanic dataset). The score is the percentage of passengers that are correctly predicted, known as accuracy.","metadata":{}},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"2\"></a>\n# 2. Preparation\n\nSteps that will be performed:\n\n* Load packages for performing label encoding, cross validation, modeling and accuracy measurement.\n* Combine train and test dataset, the purpose is to tackle missing categories when performing label encoding and to fill missing value on continuous features.\n* Label encode all the categorical features and fill missing value in continous features.\n* Split back preprocessed combine dataset into train and test dataset.\n\n<a id=\"2.1\"></a>\n## 2.1. Load packages\nLoad packages for performing label encoding, cross validation, modeling and accuracy measurement.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"2.2\"></a>\n## 2.2. Load dataset\n\nLoad `train`, `test` and`submission` dataset and combine `train` and `test` dataset into `combine` dataset for performing data pre-processing.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-apr-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\nsubmission = pd.read_csv('../input/tabular-playground-series-apr-2021/sample_submission.csv')\ncombine = pd.concat([train, test], axis=0)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"2.3\"></a>\n## 2.3. Data pre-processing\n\nData pre-processing that are used:\n\n* **Cabin**\n    * Take the first letter from the string.\n    * Fill missing values with `NA` category. \n    * Use label encoding to convert them into numbers.\n\n* **Embarked**\n    * Fill missing values with `NA` category.\n    * Use label encoding to convert them into numbers.\n    \n* **Sex**\n    * Use label encoding to convert them into numbers.\n    \n* **Fare**\n    * Fill missing values with `Fare` mean.\n    \n* **Age**\n    * Fill missing values with `Age` mean.\n    \n* **Name**\n    * Will be treated as categorical feature.\n    \n* **Ticket**\n    * Will be treated as categorical feature.\n    \n* **PassengerId**\n    * Will be taken out from the model.","metadata":{}},{"cell_type":"code","source":"combine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine = combine.drop(['PassengerId'], axis=1)\n\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'Ticket']\ncont_features = [col for col in combine.columns if col not in cat_features + ['Survived']]\nfeatures = cat_features + cont_features\n\nlabel_encoder = LabelEncoder()\nfor col in cat_features:\n    combine[col] = label_encoder.fit_transform(combine[col])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"3\"></a>\n# 3. Baseline Model\n\nThis section will evaluate the performance of `Catboost`, `XGBoost` and `LGBM` using preprocessed train dataset without any hyperparameters tuning. `Hard Voting` will be used to ensemble the models. `Hard Voting` calculation is based on the most voting from three models, for example: if `Catboost` vote 1, `XGboost` vote 1 and `LGBM` vote 0, then the final result is 1.\n\n**Observations:** \n* `Catboost` gives the best performance even higher than the `Hard Voting` ensemble which is expected to be performed better than individual model. It seems the others model drag down `Catboost` performance.\n* `XGBoost` is the second best performance followed by `LGBM`.","metadata":{}},{"cell_type":"code","source":"train = combine.iloc[:100000, :]\ntest = combine.iloc[100000:, :]\ntest = test.drop('Survived', axis=1)\nmodel_results = pd.DataFrame()\nfolds = 5","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"3.1\"></a>\n## 3.1. Catboost\n`Catboost` has the highest `accuracy` score of `0.78428` compared to others model even higher than hard voting ensemble.","metadata":{}},{"cell_type":"code","source":"train_oof = np.zeros((100000,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train[features], train['Survived'])):\n    X_train, X_valid = train.iloc[train_idx], train.iloc[valid_idx]\n    y_train = X_train['Survived']\n    y_valid = X_valid['Survived']\n    X_train = X_train.drop('Survived', axis=1)\n    X_valid = X_valid.drop('Survived', axis=1)\n\n    model = CatBoostClassifier(\n        verbose=0,\n        eval_metric=\"Accuracy\",\n        random_state=42,\n        cat_features=cat_features\n    )\n\n    model =  model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)])\n    temp_oof = model.predict(X_valid)\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} Accuracy: ', accuracy_score(y_valid, temp_oof))\n    \nprint(f'OOF Accuracy: ', accuracy_score(train['Survived'], train_oof))\nmodel_results['CatBoost'] = train_oof","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"3.2\"></a>\n## 3.2. XGBoost\n`XGBoost` has the second best accuracy with `0.78201`.","metadata":{}},{"cell_type":"code","source":"train_oof = np.zeros((100000,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train[features], train['Survived'])):\n    X_train, X_valid = train.iloc[train_idx], train.iloc[valid_idx]\n    y_train = X_train['Survived']\n    y_valid = X_valid['Survived']\n    X_train = X_train.drop('Survived', axis=1)\n    X_valid = X_valid.drop('Survived', axis=1)\n\n    model = XGBClassifier(\n        random_state=42,\n        use_label_encoder=False,\n        eval_metric=\"logloss\"\n    )\n\n    model =  model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=0)\n    temp_oof = model.predict(X_valid)\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} Accuracy: ', accuracy_score(y_valid, temp_oof))\n    \nprint(f'OOF Accuracy: ', accuracy_score(train['Survived'], train_oof))\nmodel_results['XGBoost'] = train_oof","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"3.3\"></a>\n## 3.3. LGBM\n`LGBM` is in the last position with `0.78061` accuracy.","metadata":{}},{"cell_type":"code","source":"cat_features_index = []\nfor col in cat_features:\n    cat_features_index.append(train.columns.get_loc(col))\n\ntrain_oof = np.zeros((100000,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train[features], train['Survived'])):\n    X_train, X_valid = train.iloc[train_idx], train.iloc[valid_idx]\n    y_train = X_train['Survived']\n    y_valid = X_valid['Survived']\n    X_train = X_train.drop('Survived', axis=1)\n    X_valid = X_valid.drop('Survived', axis=1)\n\n    model = LGBMClassifier(\n        verbose=0,\n        metric=\"Accuracy\",\n        random_state=42,\n        cat_feature=cat_features_index,\n        force_row_wise=True\n    )\n\n    model =  model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=0)\n    temp_oof = model.predict(X_valid)\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} Accuracy: ', accuracy_score(y_valid, temp_oof))\n    \nprint(f'OOF Accuracy: ', accuracy_score(train['Survived'], train_oof))\nmodel_results['LGBM'] = train_oof","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"3.4\"></a>\n## 3.4. Hard voting\n`Hard Voting` ensemble accuracy of `0.78154` higher than individual accuracy of `XGBoost` and `LGBM`.","metadata":{}},{"cell_type":"code","source":"train_oof = np.zeros((100000,))\ntrain_oof = np.where(model_results.sum(axis=1) > 2, 1, 0)\nprint(f'OOF Accuracy: ', accuracy_score(train['Survived'], train_oof))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"4\"></a>\n# 4. Features Engineering\nThis section will explore if there are new feature that can be generated from the dataset and improved the baseline model accuracy. Baseline model that will be used is `LGBM` as it is light and fast. `LGBM` baseline accuracy is `0.78061`, it's expected by adding/removing feature/s will give a better accuracy than the baseline model.","metadata":{}},{"cell_type":"code","source":"def combine_dataset():\n    train = pd.read_csv('../input/tabular-playground-series-apr-2021/train.csv')\n    test = pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\n    return pd.concat([train, test], axis=0)\n\ndef lgbm_oof(combine, cat_features):\n    cont_features = [col for col in combine.columns if col not in cat_features + ['Survived']]\n    features = cat_features + cont_features\n\n    label_encoder = LabelEncoder()\n    for col in cat_features:\n        combine[col] = label_encoder.fit_transform(combine[col])\n    train = combine.iloc[:100000, :]\n    test = combine.iloc[100000:, :]\n    test = test.drop('Survived', axis=1)\n\n    cat_features_index = []\n    for col in cat_features:\n        cat_features_index.append(train.columns.get_loc(col))    \n\n    train_oof = np.zeros((100000,))\n    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n    for fold, (train_idx, valid_idx) in enumerate(skf.split(train[features], train['Survived'])):\n        X_train, X_valid = train.iloc[train_idx], train.iloc[valid_idx]\n        y_train = X_train['Survived']\n        y_valid = X_valid['Survived']\n        X_train = X_train.drop('Survived', axis=1)\n        X_valid = X_valid.drop('Survived', axis=1)\n\n        model = LGBMClassifier(\n            verbose=0,\n            metric=\"Accuracy\",\n            random_state=42,\n            cat_feature=cat_features_index,\n            force_row_wise=True\n        )\n\n        model =  model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)])\n        temp_oof = model.predict(X_valid)\n        train_oof[valid_idx] = temp_oof\n        print(f'Fold {fold} Accuracy: ', accuracy_score(y_valid, temp_oof))\n\n    print(f'OOF Accuracy: ', accuracy_score(train['Survived'], train_oof))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"4.1\"></a>\n## 4.1. Family\nThis is one of the most popular feature engineering for `Titanic` dataset. It added `SibSp` and `Parch` together to create a new feature called `Family`.\n\n**Observations:**\n* It seems that adding `Family` feature doesn't improve the model.\n* Adding `Family` and also keeping `SibSp` and `Parch` has a better accuracy than removing `SibSp` and `Parch`.\n\n### 4.1.1. Add Family - Continuous\n \n* Add `Family` features and also keep `SibSp` and `Parch` to see if `SibSp` and `Parch` features can help imporove the model performance. \n* Adding `Family` features into the data increased the model accuracy to `0.78055`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Family'] = combine['Parch'] + combine['SibSp']\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine = combine.drop(['PassengerId'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'Ticket']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.1.2. Add Family and remove SibSp and Parch - Continuous\n\n* Add `Family` features and remove `SibSp` and `Parch` features to see if `SibSp` and `Parch` is createing noise to the model. \n* Deleting `SibSp` and `Parch` decreased the accuracy to `0.78049`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Family'] = combine['Parch'] + combine['SibSp']\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine = combine.drop(['PassengerId', 'SibSp', 'Parch'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'Ticket']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"4.2\"></a>\n## 4.2. Without family\nContinuing the `Family` feature, a `WithoutFamily` categorical feature can also be derived to indicate if a passenger is traveling with/without family.\n\n**Observations:**\n* Creating a new `WithoutFamily` categorical feature doesn't improve the model, removing `SibSp` and `Parch` features results to a worse model than keeping them.\n\n### 4.2.1. Adding WithoutFamily feature - Categorical\n* Add `WithoutFamily` feature to see if it can improve the prediction. \n* `WithoutFamily` feature decreased the accuracy score to `0.77986`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Family'] = combine['Parch'] + combine['SibSp']\ncombine['WithoutFamily'] = np.where(combine['Family']==0, 1, 0)\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine = combine.drop(['PassengerId', 'Family'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'Ticket', 'WithoutFamily']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2.2. Adding WithoutFamily feature and remove SibSp and Parch - Categorical\nAdd `WithoutFamily` feature  while remove `SibSp` and `Parch` to see if these 2 features add noise to the model. `WithoutFamily` without`SibSp` and `Parch` features decreased accuracy score to `0.7766`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Family'] = combine['Parch'] + combine['SibSp']\ncombine['WithoutFamily'] = np.where(combine['Family']==0, 1, 0)\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine = combine.drop(['PassengerId', 'Family', 'SibSp', 'Parch'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'Ticket', 'WithoutFamily']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"4.3\"></a>\n## 4.3. First name and last name\nThere are 2 general methods that will be explored to generate new feature/s from `Name`:\n* Extracting the first letter from the `first name` and `last name`.\n* Extracting the character length of `first name` and `last name`.\n\n**Observations:**\n* There is only one feature that improved the model accuracy by adding`first name` and `last name` features as continuous **(4.3.6)** but the improvement is considered small compared to the baseline model.\n\n### 4.3.1. Last Name - Categorical\n* Extract `last name` first letter from `Name` feature and convert it into categorical feature using label encoding.\n* Converting the first letter from `last name` and convert it into categorical feature resulted to `0.78005` accuracy which doesn't improve the accuracy","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine = pd.concat([combine, combine['Name'].str.split(',', expand=True)], axis=1)\ncombine = combine.rename(columns={0: 'LastName', 1:'FirstName'})\ncombine['LastName'] = combine[\"LastName\"].str[0]\ncombine = combine.drop(['Name', 'FirstName', 'PassengerId'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Ticket', 'LastName']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3.2. First Name - Categorical\n* Extract `first name` first letter from `Name` feature and convert it into categorical feature using label encoding.\n* Adding a categorical `first name` from the first letter only doesn't improve the model, the accuracy drop to `0.77979`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine = pd.concat([combine, combine['Name'].str.split(',', expand=True)], axis=1)\ncombine = combine.rename(columns={0:'LastName', 1: 'FirstName'})\ncombine['FirstName'] = combine[\"FirstName\"].str[1]\ncombine = combine.drop(['Name', 'LastName', 'PassengerId'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Ticket', 'FirstName']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3.3. First name and last name - Categorical\n* Extract `first name` and `last name` first letter from `Name` feature and convert them into categorical feature using label encoding.\n* Adding both `first name` and `last name` from the first letter only doesn't improve the model, the accuracy drop to `0.77171`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine = pd.concat([combine, combine['Name'].str.split(',', expand=True)], axis=1)\ncombine = combine.rename(columns={0:'LastName', 1: 'FirstName'})\ncombine['FirstName'] = combine[\"FirstName\"].str[1:]\ncombine['LastName'] = combine[\"LastName\"].str[0:]\ncombine = combine.drop(['Name', 'PassengerId'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Ticket', 'FirstName', 'LastName']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3.4. Last Name - Continuous\n* Extract `last name` and calculate its length from `Name` feature.\n* Taking the length of `last name` doesn't improve the accuracy, it landed at `0.78016`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine = pd.concat([combine, combine['Name'].str.split(',', expand=True)], axis=1)\ncombine = combine.rename(columns={0:'LastName', 1: 'FirstName'})\ncombine['LastName'] = combine[\"LastName\"].str.len()\ncombine = combine.drop(['Name', 'FirstName', 'PassengerId'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Ticket']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3.5. First Name - Continuous\n* Extract `first name` and calculate its length from `Name` feature.\n* Using the `first name` as continuous feature doesn't improve the model, accuracy is at `0.78049`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine = pd.concat([combine, combine['Name'].str.split(',', expand=True)], axis=1)\ncombine = combine.rename(columns={0:'LastName', 1: 'FirstName'})\ncombine['FirstName'] = combine[\"FirstName\"].str[1:]\ncombine['FirstName'] = combine['FirstName'].str.len()\ncombine = combine.drop(['Name', 'LastName', 'PassengerId'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Ticket']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3.6. First name and last Name - Continuous\n* Extract `first name` and `last name` and calculate its length from `Name` feature.\n* Adding both `first name` and `last name` as continuous features improved the accuracy to `0.78067`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine = pd.concat([combine, combine['Name'].str.split(',', expand=True)], axis=1)\ncombine = combine.rename(columns={0:'LastName', 1: 'FirstName'})\ncombine['LastName'] = combine[\"LastName\"].str.len()\ncombine['FirstName'] = combine[\"FirstName\"].str[1:]\ncombine['FirstName'] = combine['FirstName'].str.len()\ncombine = combine.drop(['Name', 'PassengerId'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Ticket']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3.7. Sum Length of first name and last name - Continuous\n* Create a new feature based by summing up the length of `first name` and `last name` and delete `Name`, `FirstName` and `LastName` features.\n* Summing up numbers of letters from `first name` and `last name` does not improve the accuracy of `0.78014`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine = pd.concat([combine, combine['Name'].str.split(',', expand=True)], axis=1)\ncombine = combine.rename(columns={0:'LastName', 1: 'FirstName'})\ncombine['LastName'] = combine[\"LastName\"].str.len()\ncombine['FirstName'] = combine[\"FirstName\"].str[1:]\ncombine['FirstName'] = combine['FirstName'].str.len()\ncombine['Name'] = combine['FirstName'] + combine['LastName']\ncombine = combine.drop(['FirstName', 'LastName', 'PassengerId'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Ticket']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3.8. Combine first name and last Name - Continuous\n* Combine the length of `first name` and `last name` and combine them. In example: `Jack, Wilson` will be converted to `4` and `6` then both of it will be combined into `46` and be treated as continuous feature.\n* Combine the `first name` and `last name` (not summing up) features and convert it back to continuous features resulting to a decreased in accuracy to `0.77947`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine = pd.concat([combine, combine['Name'].str.split(',', expand=True)], axis=1)\ncombine = combine.rename(columns={0:'LastName', 1: 'FirstName'})\ncombine['LastName'] = combine[\"LastName\"].str.len()\ncombine['FirstName'] = combine[\"FirstName\"].str[1:]\ncombine['FirstName'] = combine['FirstName'].str.len()\ncombine['Name'] = combine['LastName'].astype(str) + combine['FirstName'].astype(str)\ncombine['Name'] = combine['Name'].astype(float)\ncombine = combine.drop(['FirstName', 'LastName', 'PassengerId'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Ticket']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"4.4\"></a>\n## 4.4. Ticket\nThere are 2 methods that will be used for `Ticket`:\n* Extract the digits from the `Ticket` features and use the digits as a new features called `Ticket Number`.\n* Extract alphabet from the `Ticket` features and use the digits as a new features called `Ticket Code`.\n\n**Observations:**\n* Creating new feature from `Ticket Number` and put its type as continuous resulting the same accuracy as the baseline model.\n* Creating a new feature from `Ticket Code` and put it as categorical feature improve the accuracy.\n* Combining the `Ticket Number` as continuous and `Ticket Code` as categorical features resulting a more improved model accuracy than baseline model. **(4.4.4)**\n* Combining the `Ticket Number` and `Ticket Code` as categorical features improved model accuracy than baseline model but same as combining `Ticket Number` as continuous and `Ticket Code` as categorical features.","metadata":{}},{"cell_type":"markdown","source":"### 4.4.1. Ticket Numbers - Continuous\nExtracting `Ticket` numbers, put it as continuous features and fill the missing value to `0` has a accuracy of `0.78061` which is the same as the baseline model.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].str.extract('(\\d+)')\ncombine['Ticket'] = combine['Ticket'].astype(float)\ncombine['Ticket'] = combine['Ticket'].fillna(0)\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine = combine.drop(['PassengerId'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.4.2. Ticket Numbers - Categorical\nExtracting `Ticket` numbers, put it as categorical features and fill the missing value to `0` improve the accuracy to `0.77891`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].str.extract('(\\d+)')\ncombine['Ticket'] = combine['Ticket'].astype(float)\ncombine['Ticket'] = combine['Ticket'].fillna(0)\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine = combine.drop(['PassengerId'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'Ticket']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.4.3. Ticket Code - Categorical\nExtracting `Ticket` non-numerical string, put it as categorical features and fill the missing value to `NA` improve the accuracy to `0.77886`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].str.replace('[^\\w\\s]','')\ncombine['Ticket'] = combine['Ticket'].str.replace(' ','')\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Ticket'] = combine['Ticket'].replace('(\\d)', '', regex=True)\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine = combine.drop(['PassengerId'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'Ticket']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.4.4. Ticket Code - Categorical & Ticket Number - Continuous\nCombining `Ticket Code` as categorical and `Ticket Number` as continuous improve the accuracy to `0.78194` which is better than individual accuracy and baseline model.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['TicketCode'] = combine['Ticket'].str.replace('[^\\w\\s]','')\ncombine['TicketCode'] = combine['TicketCode'].str.replace(' ','')\ncombine['TicketCode'] = combine['TicketCode'].fillna('NA')\ncombine['TicketCode'] = combine['TicketCode'].replace('(\\d)', '', regex=True)\ncombine['TicketNumber'] = combine['Ticket'].str.extract('(\\d+)')\ncombine['TicketNumber'] = combine['TicketNumber'].astype(float)\ncombine['TicketNumber'] = combine['TicketNumber'].fillna(0)\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine = combine.drop(['Ticket', 'PassengerId'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'TicketCode']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.4.5. Ticket Code - Categorical & Ticket Number - Categorical\nCombining `Ticket Code` and `Ticket Number` as categorical improve the accuracy to `0.78194` which is same as putting the `Ticket Number` as continuous.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['TicketCode'] = combine['Ticket'].str.replace('[^\\w\\s]','')\ncombine['TicketCode'] = combine['TicketCode'].str.replace(' ','')\ncombine['TicketCode'] = combine['TicketCode'].fillna('NA')\ncombine['TicketCode'] = combine['TicketCode'].replace('(\\d)', '', regex=True)\ncombine['TicketNumber'] = combine['Ticket'].str.extract('(\\d+)')\ncombine['TicketNumber'] = combine['TicketNumber'].astype(float)\ncombine['TicketNumber'] = combine['TicketNumber'].fillna(0)\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine = combine.drop(['Ticket', 'PassengerId'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'TicketCode', 'TicketNumber']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"4.5\"></a>\n## 4.5. Age - binning\nThis section will explore several methods for binning `Age` feature:\n* [Human Life Cycle](https://med.libretexts.org/Courses/American_Public_University/APUS%3A_An_Introduction_to_Nutrition_(Byerley)/Text/12%3A_Maternal_Infant_Childhood_and_Adolescent_Nutrition/12.02%3A_The_Human_Life_Cycle)\n* Fixed Interval\n\n**Observations:**\n* Binning `Age` has a signficant impact to the model.\n* Converting `Age` into categorical features by binning it, improve the model performance.\n* Keeping both the continuous and the categorical (binning) `Age` doesn't make the model performed well though it's still better than the baseline model.\n* Converting missing value to new categorical `NA` improve the model than using the mean value.\n* Binning to only 2 categories stil has a significant impact to the accuracy.\n* It seems that `Age` feature has created a noise to the model, removing it improve the baseline model quite significantly.","metadata":{}},{"cell_type":"markdown","source":"### 4.5.1. Human Life Cycle - Categorical\nBinning the age using human life cycle improve the accuracy into `0.7807`, a litte bit better than the baseline model","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine['AgeBin']=pd.cut(combine['Age'],[-np.inf, 2, 4, 9, 14, 19, 31, 51, np.inf], right=False,\n                         labels = ['Infancy', 'Toddler', 'Childhood', 'Puberty', 'Older adolescence', \n                                   'Adulthood', 'Middle age', 'Senior years'])\ncombine = combine.drop(['PassengerId'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'Ticket', 'AgeBin']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.5.2. Human Life Cycle and remove Age - Categorical\nBinning the age using human life cycle and remove the `Age` feature improve the accuracy into `0.78267`, quite a significant improvement compared to the baseline model. I seems the `Age` feature create a noise to the model as it rudandant to `AgeBin`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine['AgeBin'] = pd.cut(combine['Age'],[-np.inf, 2, 4, 9, 14, 19, 31, 51, np.inf], right=False,\n                         labels = ['Infancy', 'Toddler', 'Childhood', 'Puberty', 'Older adolescence', \n                                   'Adulthood', 'Middle age', 'Senior years']).astype(str)\ncombine = combine.drop(['PassengerId', 'Age'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'Ticket', 'AgeBin']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.5.3. Human Life Cycle, remove Age and missing value - Categorical\nThe different between this section compared to **4.5.2** is the way to treat `Age` missing value; in this section, missing value will be tag as `NA` instead of calculating the mean of the `Age`. There is an improvement in the accuracy compared to the baseline and previous section, the accuracy improved to `0.78282`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['AgeBin'] = pd.cut(combine['Age'],[-np.inf, 2, 4, 9, 14, 19, 31, 51, np.inf], right=False,\n                         labels = ['Infancy', 'Toddler', 'Childhood', 'Puberty', 'Older adolescence', \n                                   'Adulthood', 'Middle age', 'Senior years']).astype(str)\ncombine = combine.drop(['PassengerId', 'Age'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'Ticket', 'AgeBin']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.5.4. Fixed Interval 20 - Categorical\nUsing an interval of 20 years, the model accuracy performed a little bit higher compared to the baseline model with accuracy of `0.78065` ","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine['AgeBin']=pd.cut(combine['Age'],[-np.inf, 20, 40, 60, 80, np.inf], right=False)\ncombine = combine.drop(['PassengerId'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'Ticket', 'AgeBin']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.5.5. Fixed Interval 20 and remove Age - Categorical\nOnce again, removing continuous `Age` features improve the model significantly into `0.78264`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine['AgeBin']=pd.cut(combine['Age'],[-np.inf, 20, 40, 60, 80, np.inf], right=False)\ncombine = combine.drop(['PassengerId', 'Age'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'Ticket', 'AgeBin']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.5.6. Fixed Interval 20, remove Age and missing value - Categorical\nThe different between this section compared to **4.5.5** is the way to treat `Age` missing value; in this section, missing value will be tag as `NA` instead of calculating the mean of the `Age`. There is an improvement in the accuracy compared to the baseline and previous section, the accuracy improved to `0.78288`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['AgeBin']=pd.cut(combine['Age'],[-np.inf, 20, 40, 60, 80, np.inf], right=False)\ncombine = combine.drop(['PassengerId', 'Age'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'Ticket', 'AgeBin']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.5.7. Fixed Interval 30, remove Age and missing value - Categorical\nBase on 2 previous try on binning `Age`, this section will directly remove the `Age` continuous and treat the missing value as categorical features. The accuracy using a fixed interval of 30 is `0.78253`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['AgeBin']=pd.cut(combine['Age'],[-np.inf, 30, 60, 90, np.inf], right=False)\ncombine = combine.drop(['PassengerId', 'Age'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'Ticket', 'AgeBin']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.5.8. Fixed Interval 50, remove Age and missing value - Categorical\nSurprisingly enough, creating only 2 categories by binnin at age 50 resulted a high accuracy of `0.78256`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['AgeBin']=pd.cut(combine['Age'],[-np.inf, 50, np.inf], right=False)\ncombine = combine.drop(['PassengerId', 'Age'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'Ticket', 'AgeBin']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.5.9. Remove Age\nIt's suspected that `Age` feature has created a noise to the model, splitting the features only into 2 category still create a high accuracy as can be seen in the previos section **(4.5.8)**. After removing `Age` feature without creating any binning, the accuracy is increased to `0.78275`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine = combine.drop(['PassengerId', 'Age'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'Ticket']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"4.6\"></a>\n## 4.6. Fare - binning\nThis section will explore binning possibility for `Fare` feature using a fixed interval and subjectivity base on the `Fare` distribution.\n\n**Observations:**\n* It may be better to keep the `Fare` feature as a continuous features.\n* Using a fixed interval of 200 doesn't improve the accuracy.\n* Splitting `Fare` feature into 2 category still doesn't improve the accuracy.\n* Subjectively splitting the `Fare` feature also doesn't imporve the model.\n* Removing `Fare` has a little impact to the model accuracy on fixed interval 200 without `Fare` features and interval distribution without `Fare` feature.\n","metadata":{}},{"cell_type":"markdown","source":"### 4.6.1. Fixed Interval 200 - Categorical\nBinning the `Fare` using a 200 fixed interval, decrease the accuracy to `0.77988`.","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['FareBin']=pd.cut(combine['Fare'],[-np.inf, 200, 400, 600, 800, np.inf], right=False)\ncombine = combine.drop(['PassengerId'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'Ticket', 'FareBin']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.6.2. Fixed Interval 200 and remove Fare - Categorical\nBinning the fare using a 200 fixed interval and remove the Fare feature decrease the accuracy to `0.77989`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['FareBin']=pd.cut(combine['Fare'],[-np.inf, 200, 400, 600, 800, np.inf], right=False)\ncombine = combine.drop(['PassengerId', 'Fare'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'Ticket', 'FareBin']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.6.3. Fixed Interval 400 - Categorical\nBinning the fare using a 400 fixed interval which technically split the feature into to 2 category increased the accuracy to `0.78078`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['FareBin']=pd.cut(combine['Fare'],[-np.inf, 400, np.inf], right=False)\ncombine = combine.drop(['PassengerId'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'Ticket', 'FareBin']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.6.4. Fixed Interval 400 and remove Fare - Categorical\nBinning the fare using a 400 fixed interval which technically split the feature into to 2 category decrease the accuracy to `0.77997`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['FareBin']=pd.cut(combine['Fare'],[-np.inf, 400, np.inf], right=False)\ncombine = combine.drop(['PassengerId', 'Fare'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'Ticket', 'FareBin']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.6.5. Interval Distribution - Categorical\nThe split is based on subjectivity at `Fare` distribution which is separated on 50, 100 and 300. The accuracy result is `0.78002` which still below baseline model. ","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['FareBin']=pd.cut(combine['Fare'],[-np.inf, 50, 100, 300, np.inf], right=False)\ncombine = combine.drop(['PassengerId'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'Ticket', 'FareBin']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.6.5. Interval Distribution and remove Fare - Categorical\nThe split is based on subjectivity at `Fare` distribution which is separated on 50, 100 and 300 and also removing the `Fare` features. The accuracy result is `0.78017` which below baseline model. ","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Ticket'] = combine['Ticket'].fillna('NA')\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Age'] = combine['Age'].fillna(np.mean(combine['Age']))\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\ncombine['FareBin']=pd.cut(combine['Fare'],[-np.inf, 50, 100, 300, np.inf], right=False)\ncombine = combine.drop(['PassengerId', 'Fare'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Name', 'Ticket', 'FareBin']\n\nlgbm_oof(combine, cat_features)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"5\"></a>\n# 5. Baseline Model Post Features Engineering\nThis section will evaluate the performance of `Catboost`, `XGBoost` and `LGBM` using features engineering that has been evaluated before using `LGBM`:\n* Convert `first name` and `last name` into its respective length from `Name` feature and remove `Name` feature.\n* Create new feature `TicketCode` and `TicketNumber` from `Ticket` feature and classify `TicketCode` as categorical and `TicketNumber` as continuous.\n* Create a new feature `AgeBin` which come from 20 interval of `Age` and remove `Age` feature.\n\n**Observations:** \n* Using new/modified features improves the accuracy on all models.\n* `Catboost` is still has the highest accuracy of `0.78596` improve from `0.78428`.\n* `XGBoost` performance is getting worse compared to baseline model a decreased from `0.78201` to `0.78109`. This may due to categorical feature treatement in `XGBoost`.\n* `LGBM` accuracy performance improves to `0.78418` from `0.78061`. It has the highest improvement as `LGBM` is used as baseline when doing the feature engineering.\n* Accuracy performance on Hard Voting ensemble using all models is worse than only use 2 models of `Catboost` and `LGBM`.\n* Even after after taking out the `XGBoost` from Hard Voting ensemble, it is still lower than `Catboost` performance but higher than `LGBM`.","metadata":{}},{"cell_type":"code","source":"combine = combine_dataset()\n\ncombine = pd.concat([combine, combine['Name'].str.split(',', expand=True)], axis=1)\ncombine = combine.rename(columns={0:'LastName', 1: 'FirstName'})\ncombine['LastName'] = combine[\"LastName\"].str.len()\ncombine['FirstName'] = combine[\"FirstName\"].str[1:]\ncombine['FirstName'] = combine['FirstName'].str.len()\ncombine['TicketCode'] = combine['Ticket'].str.replace('[^\\w\\s]','')\ncombine['TicketCode'] = combine['TicketCode'].str.replace(' ','')\ncombine['TicketCode'] = combine['TicketCode'].fillna('NA')\ncombine['TicketCode'] = combine['TicketCode'].replace('(\\d)', '', regex=True)\ncombine['TicketNumber'] = combine['Ticket'].str.extract('(\\d+)')\ncombine['TicketNumber'] = combine['TicketNumber'].astype(float)\ncombine['TicketNumber'] = combine['TicketNumber'].fillna(0)\ncombine['AgeBin']=pd.cut(combine['Age'],[-np.inf, 20, 40, 60, 80, np.inf], right=False)\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\n\ncombine = combine.drop(['Name', 'Ticket', 'Age', 'PassengerId'], axis=1)\ncat_features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'TicketCode', 'AgeBin']\n\ncont_features = [col for col in combine.columns if col not in cat_features + ['Survived']]\nfeatures = cat_features + cont_features\n\nlabel_encoder = LabelEncoder()\nfor col in cat_features:\n    combine[col] = label_encoder.fit_transform(combine[col])\ntrain = combine.iloc[:100000, :]\ntest = combine.iloc[100000:, :]\ntest = test.drop('Survived', axis=1)\n\ncat_features_index = []\nfor col in cat_features:\n    cat_features_index.append(train.columns.get_loc(col))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"5.1\"></a>\n## 5.1. Catboost\n`Catboost` has the highest `accuracy` score of `0.78596` compared to others model even higher than hard voting ensemble.","metadata":{}},{"cell_type":"code","source":"train_oof = np.zeros((100000,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train[features], train['Survived'])):\n    X_train, X_valid = train.iloc[train_idx], train.iloc[valid_idx]\n    y_train = X_train['Survived']\n    y_valid = X_valid['Survived']\n    X_train = X_train.drop('Survived', axis=1)\n    X_valid = X_valid.drop('Survived', axis=1)\n\n    model = CatBoostClassifier(\n        verbose=0,\n        eval_metric=\"Accuracy\",\n        random_state=42,\n        cat_features=cat_features\n    )\n\n    model =  model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)])\n    temp_oof = model.predict(X_valid)\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} Accuracy: ', accuracy_score(y_valid, temp_oof))\n    \nprint(f'OOF Accuracy: ', accuracy_score(train['Survived'], train_oof))\nmodel_results['CatBoost'] = train_oof","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"5.2\"></a>\n## 5.2. XGBoost\n`XGBoost` has the second best accuracy with `0.78109` worse than baseline model without feature engineering.","metadata":{}},{"cell_type":"code","source":"train_oof = np.zeros((100000,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train[features], train['Survived'])):\n    X_train, X_valid = train.iloc[train_idx], train.iloc[valid_idx]\n    y_train = X_train['Survived']\n    y_valid = X_valid['Survived']\n    X_train = X_train.drop('Survived', axis=1)\n    X_valid = X_valid.drop('Survived', axis=1)\n\n    model = XGBClassifier(\n        random_state=42,\n        use_label_encoder=False,\n        eval_metric=\"logloss\"\n    )\n\n    model =  model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=0)\n    temp_oof = model.predict(X_valid)\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} Accuracy: ', accuracy_score(y_valid, temp_oof))\n    \nprint(f'OOF Accuracy: ', accuracy_score(train['Survived'], train_oof))\nmodel_results['XGBoost'] = train_oof","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"5.3\"></a>\n## 5.3. LGBM\n`LGBM` has an accuracy of `0.78418` higher than `XGBoost` accuracy post feature engineering.","metadata":{}},{"cell_type":"code","source":"cat_features_index = []\nfor col in cat_features:\n    cat_features_index.append(train.columns.get_loc(col))\n\ntrain_oof = np.zeros((100000,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train[features], train['Survived'])):\n    X_train, X_valid = train.iloc[train_idx], train.iloc[valid_idx]\n    y_train = X_train['Survived']\n    y_valid = X_valid['Survived']\n    X_train = X_train.drop('Survived', axis=1)\n    X_valid = X_valid.drop('Survived', axis=1)\n\n    model = LGBMClassifier(\n        verbose=0,\n        metric=\"Accuracy\",\n        random_state=42,\n        cat_feature=cat_features_index,\n        force_row_wise=True\n    )\n\n    model =  model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=0)\n    temp_oof = model.predict(X_valid)\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} Accuracy: ', accuracy_score(y_valid, temp_oof))\n    \nprint(f'OOF Accuracy: ', accuracy_score(train['Survived'], train_oof))\nmodel_results['LGBM'] = train_oof","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"5.4\"></a>\n## 5.4. Hard voting\n**Observations:**\n* `Hard Voting` has an accuracy of `0.78326` higher than `XGBoost`.\n* Dropping `XGBoost` from the ensemble improve the model accuracy to `0.78541` but still lower than `Catboost`.\n\n### 5.4.1. Hard voting using all models","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"train_oof = np.zeros((100000,))\ntrain_oof = np.where(model_results.sum(axis=1) > 2, 1, 0)\nprint(f'OOF Accuracy: ', accuracy_score(train['Survived'], train_oof))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.4.2. Hard voting without XGBoost","metadata":{}},{"cell_type":"code","source":"model_results = model_results.drop('XGBoost', axis=1)\ntrain_oof = np.zeros((100000,))\ntrain_oof = np.where(model_results.sum(axis=1) > 1, 1, 0)\nprint(f'OOF Accuracy: ', accuracy_score(train['Survived'], train_oof))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"6\"></a>\n# 6. Pseudo-Labeling\n\n**Pseudo-Labeling** is a technique of using unlabeled data (in this case the test dataset) combine with labeled data (in this case train dataset) to create a better model. There are 4 steps on performing pseudo-labeling:\n1. Train model on train data and make a prediction for test data.\n2. Use predictions from stage 1 as `pseudo` labels for test data. \n3. Combined pseudolabeled dataset with train dataset.\n4. Fit a new model on this combined dataset.\n\n**Notes:** Taken from [Pseudolabelling - Tips and tricks](https://www.kaggle.com/c/tabular-playground-series-apr-2021/discussion/231738) by [Alexander Ryzhkov](https://www.kaggle.com/alexryzhkov)\n\nThis section also mainly inspired by these notebooks:\n* Main code is taken from [TPS-Apr2021 Catboost Run Pseudo label](https://www.kaggle.com/gomes555/tps-apr2021-catboost-run-pseudo-label) by [Fellipe Gomes](https://www.kaggle.com/gomes555)\n* First pseudo label result are hard voting from these notebooks:\n    * [TPS Apr 2021 single DecisionTreeModel](https://www.kaggle.com/hiro5299834/tps-apr-2021-single-decisiontreemodel) by [BIZEN](https://www.kaggle.com/hiro5299834)\n    * [TPS-APR-2021-LGBM](https://www.kaggle.com/svyatoslavsokolov/tps-apr-2021-lgbm) by [Svyatoslav Sokolov](https://www.kaggle.com/svyatoslavsokolov)\n    * [Catboost](https://www.kaggle.com/belov38/catboost-lb) by [Ilya Belov](https://www.kaggle.com/belov38)\n\nPlease check out their great notebooks!\n\n[back to top](#table-of-contents)\n<a id=\"6.1\"></a>\n## 6.1. Preparation\nThis section covered `step 1 to 3`. Prediction for unlabeled data (test dataset) is taken from [TPS Apr 2021 single DecisionTreeModel](https://www.kaggle.com/hiro5299834/tps-apr-2021-single-decisiontreemodel) by [BIZEN](https://www.kaggle.com/hiro5299834)","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-apr-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\nsubmission = pd.read_csv('../input/tabular-playground-series-apr-2021/sample_submission.csv')\ndecision_tree = pd.read_csv('../input/tps-apr-2021-single-decisiontreemodel/submission.csv', index_col=0)\nlgbm = pd.read_csv('../input/tps-apr-2021-lgbm/submission.csv', index_col=0)\ncatboost = pd.read_csv('../input/catboost-lb/result.csv', index_col=0)\npseudo_label = pd.DataFrame()\npseudo_label = pd.concat([decision_tree, lgbm, catboost], axis=1)\npseudo_label['final'] = np.where(pseudo_label.sum(axis=1) > 1, 1, 0)\nFOLDS = 5\ntest['Survived'] = [x for x in pseudo_label.final]\ncombine = pd.concat([train, test], axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"6.2\"></a>\n## 6.2. Features Engineering\nFeatures engineering are taken from previous sections.","metadata":{}},{"cell_type":"code","source":"# Cabin\ncombine['Cabin'] = combine[\"Cabin\"].str[0]\ncombine['Cabin'] = combine['Cabin'].fillna('NA')\n\n#Embarked\ncombine['Embarked'] = combine['Embarked'].fillna('NA')\n\n# Fare\ncombine['Fare'] = combine['Fare'].fillna(np.mean(combine['Fare']))\n\n# Ticket\ncombine['TicketCode'] = combine['Ticket'].str.replace('[^\\w\\s]','')\ncombine['TicketCode'] = combine['TicketCode'].str.replace(' ','')\ncombine['TicketCode'] = combine['TicketCode'].fillna('NA')\ncombine['TicketCode'] = combine['TicketCode'].replace('(\\d)', '', regex=True)\ncombine['TicketNumber'] = combine['Ticket'].str.extract('(\\d+)')\ncombine['TicketNumber'] = combine['TicketNumber'].astype(float)\ncombine['TicketNumber'] = combine['TicketNumber'].fillna(0)\n\n# Age \ncombine['AgeBin']=pd.cut(combine['Age'],[-np.inf, 20, 40, 60, 80, np.inf], right=False)\n\n# Preprocess\ncombine = combine.drop(['Name'], axis = 1)\ncat_features = ['Pclass', 'Sex', 'AgeBin', 'Cabin', 'Embarked', 'TicketCode']\nlabel_encoder = LabelEncoder()\nfor col in cat_features:\n    combine[col] = label_encoder.fit_transform(combine[col])\ntarget = 'Survived'\nfeatures = ['Cabin', 'Embarked', 'Pclass', 'Sex', 'AgeBin', 'Parch', 'SibSp', \n            'Fare', 'TicketCode', 'TicketNumber']\n\n# Splitting into train and test\ntrain = combine.iloc[:100000, :]\ntest = combine.iloc[100000:, :]\ntest = test.drop('Survived', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"6.3\"></a>\n## 6.3. Train Model & Prediction\nMost of the code are taken from [TPS-Apr2021 Catboost Run Pseudo label](https://www.kaggle.com/gomes555/tps-apr2021-catboost-run-pseudo-label) by [Fellipe Gomes](https://www.kaggle.com/gomes555). It has also implemented threshold optimization in his code.","metadata":{}},{"cell_type":"code","source":"train_oof = np.zeros((200000,))\ntest_predicts = pd.DataFrame()\n\nskf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=314)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(combine[features], combine['Survived'])):\n    X_train, X_valid = combine.iloc[train_idx], combine.iloc[valid_idx]\n    y_train = X_train['Survived']\n    y_valid = X_valid['Survived']\n    X_train = X_train[features]\n    X_valid = X_valid[features]\n    X_test = test[features]\n\n    params = {'iterations': 10000,\n              'use_best_model':True ,\n              'eval_metric': 'AUC',\n              'loss_function':'Logloss',\n              'od_type':'Iter',\n              'od_wait':500,\n              'depth': 6,\n              'l2_leaf_reg': 3,\n              'bootstrap_type': 'Bayesian',\n              'bagging_temperature': 2,\n              'max_bin': 254,\n              'grow_policy': 'SymmetricTree',\n              'cat_features': cat_features,\n              'verbose': 0,\n              'random_seed': 314}\n\n    model = CatBoostClassifier(**params)\n    model = model.fit(X_train,y_train,\n                eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                use_best_model=True,\n                plot=False)\n    predict = model.predict_proba(X_valid)[:, 1]\n    accuracy = accuracy_score(y_valid, np.where(predict>0.5, 1, 0))\n    print(f'Fold {fold} Base Accuracy:', accuracy)\n    \n    # Threshold optimization\n    thresholds = np.arange(0.0, 1.0, 0.01)\n    accuracies = []\n    for threshold in thresholds:\n        accuracies.append(accuracy_score(y_valid, np.where(predict>threshold, 1, 0)))\n    \n    accuracies = np.array(accuracies)\n    best_accuracy = accuracies.max()\n    best_accuracy_threshold = thresholds[accuracies.argmax()]\n    print(f'Fold {fold} Best Accuracy:', best_accuracy, 'with threshold of', f'{best_accuracy_threshold}')\n    \n    temp_oof = np.where(predict>best_accuracy_threshold, 1, 0)\n    train_oof[valid_idx] = temp_oof\n    \n    test_predict = model.predict_proba(X_test)[:, 1]\n    test_predict = np.where(test_predict>best_accuracy_threshold, 1, 0)\n    test_predicts['Fold '+str(fold)] = test_predict\n    \nprint(f'OOF Accuracy: ', accuracy_score(combine['Survived'], train_oof))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"6.4\"></a>\n## 6.4. Submission\n`Hard voting` is used to get the final prediction, `above 2` will be `1` and `below 2` will be `0`.","metadata":{}},{"cell_type":"code","source":"test_predicts['voting'] = np.where(test_predicts.sum(axis=1) > (FOLDS/2), 1, 0)\nsubmission['Survived'] = test_predicts['voting']\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}