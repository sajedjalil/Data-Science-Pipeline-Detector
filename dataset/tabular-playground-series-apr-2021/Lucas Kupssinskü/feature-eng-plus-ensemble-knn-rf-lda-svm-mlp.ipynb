{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# My first notebook for a competition\n\nThis notebook achieves a score of 0.78449 in the April competition (the first place got 0.81).\nIt far from the best but was fun to participate in the competition.\n\nTLDR: The data file is read, we preprocess the data and input nan values using the mean,\none-hot encode the categorical features and use an ensemble of LDA, Random Forest, kNN, SVM, and MLP classifiers.","metadata":{}},{"cell_type":"code","source":"# Import Libraries\nfrom sklearn.neighbors import VALID_METRICS\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn import svm\nfrom sklearn.neural_network import MLPClassifier\n\n\nfrom sklearn.neighbors import DistanceMetric\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import GridSearchCV\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the datasets\n\ndatasetTrain = pd.read_csv('../input/tabular-playground-series-apr-2021/train.csv')\ndatasetTest = pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\ndatasetTrain.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defines the function that preprocess the data\n\n\ndef preprocessData(df: pd.DataFrame,\n                   columnTransformer: ColumnTransformer = None,\n                   complementarySetForNull: pd.DataFrame = None) -> (np.ndarray, np.ndarray, ColumnTransformer):\n    '''\n    Preprocess the data passed in, fill None values, encodes categorical features.\n\n    Parameters:\n        df (DataFrame): A pandas dataframe that contains the data to be preprocessed\n        columnTransformer (ColumnTransformer): optional column transformer to be used if null it creates a new ct\n        complementarySetForNull (Dataframe): An optional df (test) that is used for inputation of missing values.\n\n    Returns:\n        X (Numpy Array): The feature array already preprocessed\n        Y (Numpy Array): The target value\n        ct (Column Transformer): The column transformer used in the preprocess \n    '''\n    df.drop(labels=['Name', 'Ticket'],\n            axis=1,\n            inplace=True)\n    if complementarySetForNull is not None:\n        # Age fillna with mean age\n        completeDF = pd.concat(\n            [df, complementarySetForNull]).reset_index(drop=True)\n        meanAge = completeDF['Age'].mean()\n        meanFare = completeDF['Fare'].mean()\n        completeDF = None\n\n        df['Age'] = df['Age'].fillna(meanAge)\n        complementarySetForNull['Age'] = complementarySetForNull['Age'].fillna(\n            meanAge)\n\n        df['Fare'] = df['Fare'].fillna(meanFare)\n        complementarySetForNull['Fare'] = complementarySetForNull['Fare'].fillna(\n            meanFare)\n    df = df.assign(AgeGroup=df['Age'].apply(\n        lambda x: x // 10 if x // 10 <= 6 else 6))\n    df = df.assign(hasFamily=(df['SibSp'] + df['Parch']).apply(\n        lambda x: 1 if x > 0 else 0))\n    df = df.assign(familySize=(df['SibSp'] + df['Parch']).apply(\n        lambda x: 0 if pd.isnull(x) else x))\n    df = df.assign(hasCabin=df['Cabin'].apply(\n        lambda x: 0 if pd.isnull(x) else 1))\n    df = df.assign(cabinLetter=df['Cabin'].apply(\n        lambda x: '.' if pd.isnull(x) else str(x)[0]))\n    df['Fare'] = df['Fare'].apply(\n        lambda x: 0 if pd.isnull(x) else x)\n    df = df.assign(farePerFamily=df['Fare']/(df['familySize']+1))\n    X = df[['Pclass', 'Sex', 'Fare', 'Embarked',\n            'AgeGroup', 'hasFamily', 'hasCabin', 'cabinLetter', 'farePerFamily']].values\n    if 'Survived' in df.columns:\n        Y = np.ravel(df['Survived'])\n    else:\n        Y = None\n    if columnTransformer is None:\n        columnTransformer = ColumnTransformer(\n            [('encoder', OneHotEncoder(drop='first'), [0, 1, 3, 4, 7]),\n             ('minMaxScaler', MinMaxScaler(), [2, 8])], remainder='passthrough')\n        X = columnTransformer.fit_transform(X)\n    else:\n        X = columnTransformer.transform(X)\n\n    return X, Y, columnTransformer\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess the train and the test set and frees the memory of regarding\n# the features of the dataset that were not used\n\nX, Y, ct = preprocessData(datasetTrain, complementarySetForNull=datasetTest)\n\nX_test, _, _ = preprocessData(datasetTest, columnTransformer=ct)\npassengerIdTest = datasetTest['PassengerId'].values.reshape(-1, 1)\n\ndatasetTrain = None\ndatasetTest = None\nensembleOfModels = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# If you don't have enough hardware...\nSometimes you just don't have enough hardware available, it was my case in April's competition.\nMy solution was to do a grid search using just a sample of the training set. Usually, you want to use\nall the data, but if you are on a budget station (like me) a sample can do the job.\nUse the SAMPLE_RATIO (0,1] below to control how much of the training sample you will use.\n\nFeel free to change to 1 if you have enough resources.","metadata":{}},{"cell_type":"code","source":"# Since we dont have enough hardware to grid search through the entire data\n# we will take a i.i.d. sample of 10% of our dataset and will use that\n# to grid search and obtain the best hyperparameters for our models\nCV_FOLDS = 4\nSAMPLE_RATIO = 0.1\nsampleIndexes = np.random.choice(len(Y),\n                                 int(len(Y)*SAMPLE_RATIO),\n                                 replace=False)\nX_sample = X[sampleIndexes]\nY_sample = Y[sampleIndexes]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Grid Search\nSome models have hyperparameters, aka values that you have to manually set and that are not subject to optimization during the training phase.\nFor those, the best practice is to search through some combination of parameters (trial and error) and select the parameters that create the\nbest model. In sklearn, we have the GridSearchCV that allows us to do a grid search in a k-fold cross-validation setup, we will do that.","metadata":{}},{"cell_type":"code","source":"# Grid Search Through the Random Forest Classifier\n\ngridParameters = {'n_estimators': [10, 50, 100],\n                  'criterion': ['gini', 'entropy']}\ngsCV = GridSearchCV(RandomForestClassifier(),\n                    gridParameters,\n                    cv=CV_FOLDS,\n                    n_jobs=-1)\n\ngsCV.fit(X_sample, Y_sample)\n\nprint(\n    f'Best Random Forst Classifier:\\n   Score > {gsCV.best_score_}\\n   Params > {gsCV.best_params_}')\nensembleOfModels.append(gsCV.best_estimator_)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Grid Search Through the kNN classifier\n# to use mahalonobis distance we need to pass the keyword parameters\n# V and VI\n# in case we want to know the valid distance metrics\n# we could run => sorted(VALID_METRICS['brute'])\n\n# covParam = np.cov(X.astype(np.float32))\n# invCovParam = np.linalg.pinv(covParam)\n\ngridParameters = [{'algorithm': ['auto'],\n                  'metric': ['minkowski'],\n                   'n_neighbors': [3, 5, 10]}]#,\n                  #{'algorithm': ['brute'],\n                  # 'metric': ['mahalanobis'],\n                  # 'n_neighbors': [5, 10],\n                  # 'metric_params': [{'V': covParam,\n                  #                    'VI': invCovParam}]}]\ngsCV = GridSearchCV(KNeighborsClassifier(),\n                    gridParameters,\n                    cv=CV_FOLDS,\n                    n_jobs=-1)\n\ngsCV.fit(X_sample, Y_sample)\n\nprint(\n    f'Best kNN Classifier:\\n   Score > {gsCV.best_score_}\\n   Params > {gsCV.best_params_}')\nensembleOfModels.append(gsCV.best_estimator_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The LDA models does not have much hyperparameters to tune\n\nmodel = LinearDiscriminantAnalysis()\ncv = cross_validate(model, X_sample, Y_sample, scoring='accuracy', cv=CV_FOLDS)\n\nprint(np.mean(cv['test_score']))\n\nensembleOfModels.append(model)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets grid search through the SVM classifier\n\ngridParameters = {'C': [0.1, 1, 10, 100, 1000],\n                  'gamma': ['auto'],  # [1, 0.1, 0.01, 0.001, 0.0001],\n                  'kernel': ['rbf']}\ngsCV = GridSearchCV(svm.SVC(),\n                    gridParameters,\n                    cv=CV_FOLDS,\n                    n_jobs=-1)\n\ngsCV.fit(X_sample, Y_sample)\n\nprint(\n    f'Best SVM:\\n   Score > {gsCV.best_score_}\\n   Params > {gsCV.best_params_}')\nensembleOfModels.append(gsCV.best_estimator_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets grid search through the MLP classifier\n\ngridParameters = {'hidden_layer_sizes': [(5, 5), (10, 5), (10, 10), (15, 10), (15, 15)],\n                  'activation': ['logistic', 'relu'],\n                  'solver': ['adam'],\n                  'alpha': [0.0001, 0.05, 0.005],\n                  'learning_rate': ['constant', 'adaptive'],\n                  }\ngsCV = GridSearchCV(MLPClassifier(max_iter=2500),\n                    gridParameters,\n                    cv=CV_FOLDS,\n                    n_jobs=-1)\n\ngsCV.fit(X_sample, Y_sample)\n\nprint(\n    f'Best MLP:\\n   Score > {gsCV.best_score_}\\n   Params > {gsCV.best_params_}')\nensembleOfModels.append(gsCV.best_estimator_)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The prediction...\nWe now take the best model for each of the five algorithms we used (LDA, kNN, Random Forest, SVM) and we train them with selected\nhyperparameters using the whole training set. \n\nWe use each of the five classifiers to predict the target variable. If three or more classifiers vote\non a specific outcome that is the outcome of our ensemble classifier.","metadata":{}},{"cell_type":"code","source":"# Prepare data for Kaggle Submission\nensembleOfModels = list(map(lambda m: m.fit(X, Y), ensembleOfModels))\n\n\npredictions = list(map(lambda m: m.predict(X_test), ensembleOfModels))\n\npredictionsEnsemble = predictions[0] + predictions[1] + \\\n    predictions[2] + predictions[3] + predictions[4]\n\ndataForSubmission = pd.DataFrame(np.concatenate((passengerIdTest,\n                                                 predictionsEnsemble.reshape(-1, 1)), axis=1), columns=['PassengerId', 'Survived'])\ndataForSubmission['Survived'] = dataForSubmission['Survived'].apply(\n    lambda x: 1 if x >= 3 else 0)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creates the submission file\ndataForSubmission.to_csv('./submission.csv',\n                         sep=',',\n                         decimal='.',\n                         index=False)\ndataForSubmission.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}