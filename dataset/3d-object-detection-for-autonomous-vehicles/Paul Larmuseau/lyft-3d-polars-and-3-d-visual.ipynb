{"cells":[{"metadata":{},"cell_type":"markdown","source":"# worked abit on a script here\ncopy from\nhttps://www.kaggle.com/lopuhin/lyft-3d-join-all-lidars-annotations-from-scratch\n\ni think some will get inspiration here"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%matplotlib inline\nimport json\nimport os.path\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom pyquaternion import Quaternion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class Table:\n    def __init__(self, data):\n        self.data = data\n        self.index = {x['token']: x for x in data}\n\n\nDATA_ROOT = '/kaggle/input/3d-object-detection-for-autonomous-vehicles/'\n\n\ndef load_table(name, root=os.path.join(DATA_ROOT, 'train_data')):\n    with open(os.path.join(root, name), 'rb') as f:\n        return Table(json.load(f))\n\n    \nscene = load_table('scene.json')\nsample = load_table('sample.json')\nsample_data = load_table('sample_data.json')\nego_pose = load_table('ego_pose.json')\ncalibrated_sensor = load_table('calibrated_sensor.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scene.data[0])\nprint(sample)\nprint(sample_data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"my_scene = scene.data[0]\nmy_scene","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And the first sample from that scene"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.index[my_scene['first_sample_token']],sample.index[sample.index[my_scene['first_sample_token']]['next']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll use ``sample_data`` to fetch lidar images related to this sample. First check what is inside ``sample_data``"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_data.data[0],sample_data.data[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now fetch lidar images related to the sample (note that to make it efficient you'll want to add an index like in official SDK)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_data.data[0]\nlidars = []\nfor x in sample_data.data:\n    if x['sample_token'] == my_scene['first_sample_token'] and 'lidar' in x['filename']:\n        lidars.append(x)\nlidars","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All lidars happen to have the same ego_pose (because they are on the same car?)"},{"metadata":{"trusted":true},"cell_type":"code","source":"{x['ego_pose_token'] for x in lidars}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's load lidar's point data, we'll keep only first 3 columns (point coordinates)"},{"metadata":{"trusted":true},"cell_type":"code","source":"lidars_data = [\n    # here, sorry\n    np.fromfile(os.path.join(DATA_ROOT, x['filename']).replace('/lidar/', '/train_lidar/'), dtype=np.float32)\n    .reshape(-1, 5)[:, :3] for x in lidars]\nlidars_data[0].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3d triangulated surface plot\n\nthats difficult to imagine something here.\ni guess the more negative the higher...\nso reversed that negativity"},{"metadata":{"trusted":true},"cell_type":"code","source":"lid=pd.DataFrame( lidars_data[0] )\nlid['x']=(3*lid[0]).astype('int')\nlid['y']=(3*lid[1]).astype('int')\nlid['x']=(lid['x'])\nlid['y']=(lid['y'])\nlid=lid.groupby(['x','y']).mean()\nlid=lid[lid[2]>-2]\n\nlid=lid.reset_index()\nfig = plt.figure()\n\nimport sys\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nfrom matplotlib import cm\nfrom mpl_toolkits.mplot3d import Axes3D\n\nimport numpy\nfrom numpy.random import randn\nfrom scipy import array, newaxis\n\nax = fig.add_subplot(111, projection='3d')\n\nsurf = ax.plot_trisurf(lid['x'].astype('int'),lid['y'].astype('int'),-(lid[2].astype('int')-0.6), cmap=cm.jet, linewidth=0)\nfig.colorbar(surf)\n\nax.xaxis.set_major_locator(MaxNLocator(5))\nax.yaxis.set_major_locator(MaxNLocator(6))\nax.zaxis.set_major_locator(MaxNLocator(5))\n\nfig.tight_layout()\nax.view_init(70, 40)\nplt.draw()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# from sparse to bitmap\n\n* adding some spectral color, we see now something interesting: the cars share the same color...\n* i guess we see the buildings and we see trees\n* the difficult thing is each object has a **radarshadow**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom scipy.sparse import coo_matrix\ncoo = coo_matrix((lid[2]+2, ((lid['y']-lid['y'].min()).astype('int'),(lid['x']-lid['x'].min()).astype('int'))))\nfrom matplotlib import pyplot as plt\nplt.figure(figsize=(30,30))\nplt.imshow(coo.todense(), interpolation='nearest',cmap=cm.jet)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# polar version\n\ni don't know why i have to think cartesian, the image is probably constructed polar. Lets try to do something with that view\nnot its important imho to treat the data. Just to get the original view"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cart2pol(x,y):\n\n    rho = np.sqrt(x**2 + y**2)\n    phi = np.arctan2(y, x)*2\n    return(rho, phi)\nfrom scipy.sparse import coo_matrix\n\nif True:\n    simg=lidars_data[0]\n    \n    X=pd.DataFrame([])\n    \n    X[1]=lidars_data[0][:,0] #coo_matrix(simg).row\n    X[0]=lidars_data[0][:,1] #coo_matrix(simg).col#.reshape(-1, 1)\n    \n    P=pd.DataFrame([])\n    #P['r'],P['c']=cart2pol(X[0]-np.min(X[0]),X[1]-np.min(X[1]))\n    P['r'],P['c']=cart2pol(X[0],X[1])\n    P['d']=lidars_data[0][:,2]\n    P['r']=(P['r']*3).astype('int').values\n    P['c']=(P['c']*100).astype('int').values\n    #Psp=P.to_sparse()\n    print(P.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the polar graphs shows some benefits\nfirst a line is a circle around the car\nthe first line is the closest circle\ncars are equally good visible, not as squares , but again as other colour\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(P.groupby(['r','c']).mean().reset_index().pivot(index='c',columns='r',values='d'))\nplt.figure(figsize=(30,30))\nplt.imshow(P.groupby(['r','c']).mean().reset_index().pivot(index='r',columns='c',values='d'), interpolation='nearest',cmap=cm.jet)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# visualize the distance lines touching the ground\n\npeace of cake to see the cars hÃ©\n\nyou can see the road isn't flat imho it goes down in one direction...\n\nthe middle of 0 and the ground 0.5 average, is somewhere 0.25\n\n**seeing al the objects : are all the points below 0.25**\n\nevidently i am not sure yet of all matrixes can be simplified like that\nand i am not sure if a car is on a climbing/descending road, what visualisation i get here..; but probably it should not make very difference since a car drives on a surface..."},{"metadata":{"trusted":true},"cell_type":"code","source":"Pol=P.groupby(['r','c']).mean().reset_index().pivot(index='r',columns='c',values='d')\nPol=Pol-0.25\nPol=Pol[Pol>0]\nplt.figure(figsize=(30,30))\nplt.imshow(Pol, interpolation='nearest',cmap=cm.jet)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# and the objects are the other part of the matrix\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Pol=P.groupby(['r','c']).mean().reset_index().pivot(index='r',columns='c',values='d')\nPol=Pol-0.25\nPol=Pol[Pol<0]\nplt.figure(figsize=(30,30))\nplt.imshow(Pol, interpolation='nearest',cmap=cm.jet)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the bitmap is less clear"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncoo = coo_matrix(( (lidars_data[0][:,2]-lidars_data[0][:,2].max()), ( (P['r']-P['r'].min()).astype('int'),(P['c']-P['c'].min()).astype('int'))))\n\nfrom matplotlib import pyplot as plt\nplt.figure(figsize=(30,30))\nplt.imshow(coo.todense(), interpolation='nearest',cmap=cm.jet)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* svd is not so much informative at first sight\n* you could use it to denoise, but lidar doesn't generate much noise i presume\n* or enhance contrast\n* but i don't get much out here"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD,SparsePCA\nsvd = TruncatedSVD(n_components=30, n_iter=20, random_state=42)\nfrom sklearn.preprocessing import QuantileTransformer\nPol=P.groupby(['r','c']).mean().reset_index().pivot(index='r',columns='c',values='d')\nPol=Pol-0.25\nPol=Pol[Pol<0]\nPol=Pol[Pol>-1]\nrng = np.random.RandomState(0)\nqt = QuantileTransformer(n_quantiles=10, random_state=0)\nPolQ=qt.fit_transform(Pol.fillna(+1)-1.5) \nprint(Pol.shape)\n\nu=svd.fit_transform(PolQ)\nv=svd.components_\ns=svd.singular_values_\nplt.imshow( u  , interpolation='nearest',cmap=cm.jet)\nplt.show()\n\nplt.imshow( v , interpolation='nearest',cmap=cm.jet)\nplt.show()\nplt.figure(figsize=(30,30))\n\nplt.imshow((u/s).dot(v) , interpolation='nearest',cmap=cm.jet)\nplt.show()\nplt.figure(figsize=(30,30))\n\nplt.imshow(Pol-(u/s).dot(v) , interpolation='nearest',cmap=cm.jet)\nplt.show()\n\nplt.plot( s )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[calibrated_sensor.index[x['calibrated_sensor_token']] for x in lidars]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some helpers"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def rotate_points(points, rotation, inverse=False):\n    assert points.shape[1] == 3\n    q = Quaternion(rotation)\n    if inverse:\n        q = q.inverse\n    return np.dot(q.rotation_matrix, points.T).T\n    \ndef apply_pose(points, cs):\n    \"\"\" Translate (lidar) points to vehicle coordinates, given a calibrated sensor.\n    \"\"\"\n    points = rotate_points(points, cs['rotation'])\n    points = points + np.array(cs['translation'])\n    return points\n\ndef inverse_apply_pose(points, cs):\n    \"\"\" Reverse of apply_pose (we'll need it later).\n    \"\"\"\n    points = points - np.array(cs['translation'])\n    points = rotate_points(points, np.array(cs['rotation']), inverse=True)\n    return points","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And let's vizualize all lidars. First let's see what happens if we don't use poses from the lidars\n\n* you have to calibrate all lidars\n* the colours reveal that each radar shows something, but the best is the topradar\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def viz_all_lidars(lidars, lidars_data, clip=50, skip_apply_pose=False):\n    all_points = []\n    all_colors = []\n    Lid=pd.DataFrame([])\n    for color, points, lidar in zip([[1, 0, 0, 0.5], [0, 1, 0, 0.5], [0, 0, 1, 0.5]], lidars_data, lidars):\n        Temp=pd.DataFrame([])\n        cs = calibrated_sensor.index[lidar['calibrated_sensor_token']]\n        if not skip_apply_pose:\n            points = apply_pose(points, cs)\n        for xi in range(0,3):\n            Temp[xi]=points[:,xi]*3\n            Temp[xi]=Temp[xi].astype('int')\n        all_points.append(points)\n        all_colors.append(np.array([color] * len(points)))\n        Lid=Lid.append(Temp)\n    all_points = np.concatenate(all_points)\n    all_colors = np.concatenate(all_colors)\n    perm = np.random.permutation(len(all_points))\n    all_points = all_points[perm]\n    all_colors = all_colors[perm]\n    #all_colors = np.round( all_points[perm,2]/3 )\n\n    plt.figure(figsize=(20, 20))\n    plt.axis('equal')\n    plt.grid()\n    plt.scatter(np.clip(all_points[:, 0], -clip, clip), np.clip(all_points[:, 1], -clip, clip), all_points[:, 2],c=all_colors)\n    \n    return Lid\n\nLid = viz_all_lidars(lidars, lidars_data, clip=40, skip_apply_pose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nLid2=Lid.groupby([0,1]).mean().reset_index()\nLid2=Lid2.pivot(index=0,columns=1,values=2)\nprint(Lid2.iloc[450].mean())\n\nLid2=Lid2-1.5   # floor sits at -1.5\nLid2=Lid2[Lid2>-1.5]\nLid2=Lid2[Lid2<12]  #if you want to see the trucks it has to be higher then 3\nprint(Lid2.iloc[450].mean())\nplt.figure(figsize=(30,30))\nplt.imshow(Lid2, interpolation='nearest',cmap=cm.jet)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# lets try some 3D...\n\nfound some sources here \nhttps://github.com/navoshta/KITTI-Dataset/blob/master/kitti-dataset.ipynb\n\nand look what we have now... thats something that makes things more clear for me..\n**you really see the shape of the cars.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ncolors = {\n    'Car': 'b',\n    'Tram': 'r',\n    'Cyclist': 'g',\n    'Van': 'c',\n    'Truck': 'm',\n    'Pedestrian': 'y',\n    'Sitter': 'k'\n}\naxes_limits = [\n    [-20, 80], # X axis range\n    [-20, 20], # Y axis range\n    [-3, 10]   # Z axis range\n]\naxes_str = ['X', 'Y', 'Z']\n\npoint_size = 0.01 * (1. / 0.2)\n\ndef draw_box(pyplot_axis, vertices, axes=[0, 1, 2], color='black'):\n    \"\"\"\n    Draws a bounding 3D box in a pyplot axis.\n    \n    Parameters\n    ----------\n    pyplot_axis : Pyplot axis to draw in.\n    vertices    : Array 8 box vertices containing x, y, z coordinates.\n    axes        : Axes to use. Defaults to `[0, 1, 2]`, e.g. x, y and z axes.\n    color       : Drawing color. Defaults to `black`.\n    \"\"\"\n    vertices = vertices[axes, :]\n    connections = [\n        [0, 1], [1, 2], [2, 3], [3, 0],  # Lower plane parallel to Z=0 plane\n        [4, 5], [5, 6], [6, 7], [7, 4],  # Upper plane parallel to Z=0 plane\n        [0, 4], [1, 5], [2, 6], [3, 7]  # Connections between upper and lower planes\n    ]\n    for connection in connections:\n        pyplot_axis.plot(*vertices[:, connection], c=color, lw=0.5)\n        \n        \ndef draw_point_cloud(ax, title, axes=[0, 1, 2], xlim3d=None, ylim3d=None, zlim3d=None):\n        \"\"\"\n        Convenient method for drawing various point cloud projections as a part of frame statistics.\n        \"\"\"\n\n        ax.scatter(*np.transpose(velo_frame[:, axes]), s=point_size, c=velo_frame[:, 2], cmap='gray')\n        ax.set_title(title)\n        ax.set_xlabel('{} axis'.format(axes_str[axes[0]]))\n        ax.set_ylabel('{} axis'.format(axes_str[axes[1]]))\n        if len(axes) > 2:\n            ax.set_xlim3d(*axes_limits[axes[0]])\n            ax.set_ylim3d(*axes_limits[axes[1]])\n            ax.set_zlim3d(*axes_limits[axes[2]])\n            ax.set_zlabel('{} axis'.format(axes_str[axes[2]]))\n        else:\n            ax.set_xlim(*axes_limits[axes[0]])\n            ax.set_ylim(*axes_limits[axes[1]])\n        # User specified limits\n        if xlim3d!=None:\n            ax.set_xlim3d(xlim3d)\n        if ylim3d!=None:\n            ax.set_ylim3d(ylim3d)\n        if zlim3d!=None:\n            ax.set_zlim3d(zlim3d)\n            \n        #for t_rects, t_type in zip(tracklet_rects[frame], tracklet_types[frame]):\n        #    draw_box(ax, t_rects, axes=axes, color=colors[t_type])\n\nvelo_frame=lidars_data[1]            \nf2 = plt.figure(figsize=(15, 8))\nax2 = f2.add_subplot(111, projection='3d')    \ndraw_point_cloud(ax2, 'Lyft scan', xlim3d=(-30,30))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"velo_frame[:,2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def colorize(velo_fr2,schift):\n        velo_fr2=velo_fr2+schift\n        velo_col=[]\n        velo_col=['g']\n        for xi in range(1,len(velo_fr2)):\n            if velo_fr2[xi]>0 and velo_fr2[xi]<3.25:  # ground level searched manually, actually the surface is not flat... i should create a surface function\n                if velo_fr2[xi]>velo_fr2[xi-1]+0.001:  #increasing Z is red\n                    velo_col.append('r')\n                elif velo_fr2[xi]<velo_fr2[xi-1]-0.001: # decreasing Z is mauve\n                    velo_col.append('m')\n                else:\n                    velo_col.append('b')\n            elif velo_fr2[xi]>=3.25 :\n                velo_col.append('g') #probably trees\n            else:\n                velo_col.append('y')\n\n        return velo_col\n    \ndef draw_point_cloud_color(ax, title, axes=[0, 1, 2], xlim3d=None, ylim3d=None, zlim3d=None):\n        \"\"\"\n        Convenient method for drawing various point cloud projections as a part of frame statistics.\n        \"\"\"\n            \n        ax.scatter(*np.transpose(velo_frame[:, axes]), s=point_size, c=colorize(velo_frame[:,2],2), cmap='gray')\n        ax.set_title(title)\n        ax.set_xlabel('{} axis'.format(axes_str[axes[0]]))\n        ax.set_ylabel('{} axis'.format(axes_str[axes[1]]))\n        if len(axes) > 2:\n            ax.set_xlim3d(*axes_limits[axes[0]])\n            ax.set_ylim3d(*axes_limits[axes[1]])\n            ax.set_zlim3d(*axes_limits[axes[2]])\n            ax.set_zlabel('{} axis'.format(axes_str[axes[2]]))\n        else:\n            ax.set_xlim(*axes_limits[axes[0]])\n            ax.set_ylim(*axes_limits[axes[1]])\n        # User specified limits\n        if xlim3d!=None:\n            ax.set_xlim3d(xlim3d)\n        if ylim3d!=None:\n            ax.set_ylim3d(ylim3d)\n        if zlim3d!=None:\n            ax.set_zlim3d(zlim3d)\n            \n        #for t_rects, t_type in zip(tracklet_rects[frame], tracklet_types[frame]):\n        #    draw_box(ax, t_rects, axes=axes, color=colors[t_type])\n\nvelo_frame=lidars_data[1]            \nf2 = plt.figure(figsize=(15, 8))\nax2 = f2.add_subplot(111, projection='3d')    \ndraw_point_cloud_color(ax2, 'Lyft scan', xlim3d=(-30,30))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# polar version in 3d"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=pd.DataFrame([])\n    \nX[1]=lidars_data[1][:,0] #coo_matrix(simg).row\nX[0]=lidars_data[1][:,1] #coo_matrix(simg).col#.reshape(-1, 1)\n    \nP=pd.DataFrame([])\nP['r'],P['c']=cart2pol(X[0],X[1])\nP['d']=lidars_data[1][:,2]\npoint_size = 1\n\nvelo_frame=P.values           \nf2 = plt.figure(figsize=(15, 8))\nax2 = f2.add_subplot(111, projection='3d')    \ndraw_point_cloud_color(ax2, 'Lyft polar 3d view ', xlim3d=(40,80))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"markdown","source":"def get_annotations(token):\n    annotations = np.array(train_df.loc[token].PredictionString.split()).reshape(-1, 8)\n    return {\n        'point': annotations[:, :3].astype(np.float32),\n        'wlh': annotations[:, 3:6].astype(np.float32),\n        'rotation': annotations[:, 6].astype(np.float32),\n        'cls': np.array(annotations[:, 7]),\n    }\n\nget_annotations(my_scene['first_sample_token']).keys()"},{"metadata":{"trusted":true},"cell_type":"code","source":"def viz_all_lidars_color(lidars, lidars_data, clip=50, skip_apply_pose=False):\n    all_points = []\n    all_colors = []\n    Lid=pd.DataFrame([])\n\n    for color, points, lidar in zip([[1, 0, 0, 0.5], [0, 1, 0, 0.5], [0, 0, 1, 0.5]], lidars_data, lidars):\n        Temp=pd.DataFrame([])\n        cs = calibrated_sensor.index[lidar['calibrated_sensor_token']]\n        if not skip_apply_pose:\n            points = apply_pose(points, cs)\n        for xi in range(0,3):\n            Temp[xi]=points[:,xi]*3\n            Temp[xi]=Temp[xi].astype('int')\n        all_points.append(points)\n        #all_colors.append(np.array([color] * len(points)))\n        Lid=Lid.append(Temp)\n    all_points = np.concatenate(all_points)\n    #all_colors = np.concatenate(all_colors)\n    all_colors =colorize(all_points[:,2],-0.21)  #since data are calibrated i have to shift\n    #print(len(all_points),len(all_colors))\n    \n    #perm = np.random.permutation(len(all_points))\n    #all_points = all_points[perm]\n    #all_colors = all_colors[perm]\n    #all_colors = colorize( all_points[perm:,2]/3 ) \n\n    plt.figure(figsize=(20, 20))\n    plt.axis('equal')\n    plt.grid()\n    plt.scatter(np.clip(all_points[:, 0], -clip, clip), np.clip(all_points[:, 1], -clip, clip), all_points[:, 2],c=all_colors)\n    \n    return Lid\n\nLid = viz_all_lidars_color(lidars, lidars_data, clip=40, skip_apply_pose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_scene = scene.data[1]\nprint(my_scene,sample_data.data[1])\nlidars = []\nfor x in sample_data.data:\n    if x['sample_token'] == my_scene['first_sample_token'] and 'lidar' in x['filename']:\n        lidars.append(x)\nlidars","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"ego_pose.index[lidars[0]['ego_pose_token']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def viz_annotation_centers(token, lidars, clip=50):\n    # translate annotation points to the car frame\n    ego_pose_token, = {x['ego_pose_token'] for x in lidars}\n    ep = ego_pose.index[ego_pose_token]\n    annotations = get_annotations(token)\n    car_points = annotations['point'][annotations['cls'] == 'car']\n    car_points = inverse_apply_pose(car_points, ep)\n    \n    plt.scatter(np.clip(car_points[:, 0], -clip, clip),\n                np.clip(car_points[:, 1], -clip, clip),\n                s=70,\n                color='purple')\n    \nviz_all_lidars(lidars, lidars_data, clip=50)\nviz_annotation_centers(my_scene['first_sample_token'], lidars, clip=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"viz_all_lidars(lidars, lidars_data, clip=20)\nviz_annotation_centers(my_scene['first_sample_token'], lidars, clip=20)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}