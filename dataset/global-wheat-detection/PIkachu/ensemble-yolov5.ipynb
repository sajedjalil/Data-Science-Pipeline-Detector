{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --no-deps '../input/timm-package/timm-0.1.26-py3-none-any.whl' > /dev/null\n!pip install --no-deps '../input/pycocotools/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl' > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys\nsys.path.insert(0, \"../input/timm-efficientdet-pytorch\")\nsys.path.insert(0, \"../input/omegaconf\")\nsys.path.insert(0, \"../input/weightedboxesfusion\")\nsys.path.insert(0, \"../input/yolov5tta/\")\nimport os\nfrom ensemble_boxes import *\nimport torch\nimport random\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom torch.utils.data import Dataset,DataLoader\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport cv2\nimport gc\nimport torch\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchEval\nfrom effdet.efficientdet import HeadNet\nfrom sklearn.model_selection import StratifiedKFold\nfrom skopt import gp_minimize, forest_minimize\nfrom skopt.utils import use_named_args\nfrom skopt.plots import plot_objective, plot_evaluations, plot_convergence, plot_regret\nfrom skopt.space import Categorical, Integer, Real\n\nSEED = 42\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Yolov5","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport argparse\n\nfrom utils.datasets import *\nfrom utils.utils import *\n\n\ndef detect(save_img=False):\n    weights, imgsz = opt.weights,opt.img_size\n    source = '../input/global-wheat-detection/test/'\n    \n    # Initialize\n    device = torch_utils.select_device(opt.device)\n    half = False\n    # Load model\n    models = []\n    for w in weights:\n        models.append(torch.load(w, map_location=device)['model'].to(device).float().eval())\n\n\n    dataset = LoadImages(source, img_size=1024)\n\n    # Get names and colors\n\n    # Run inference\n    t0 = time.time()\n    img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n    all_path=[]\n    all_bboxex =[]\n    all_score =[]\n    for path, img, im0s, vid_cap in dataset:\n        img = torch.from_numpy(img).to(device)\n        img = img.half() if half else img.float()  # uint8 to fp16/32\n        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n        if img.ndimension() == 3:\n            img = img.unsqueeze(0)\n\n        # Inference\n        t1 = torch_utils.time_synchronized()\n        bboxes_2 = []\n        score_2 = []\n        for model in models:\n            pred = model(img, augment=opt.augment)[0]\n            pred = non_max_suppression(pred, 0.4, opt.iou_thres,merge=True, classes=None, agnostic=False)\n            t2 = torch_utils.time_synchronized()\n\n            bboxes = []\n            score = []\n            # Process detections\n            for i, det in enumerate(pred):  # detections per image\n                p, s, im0 = path, '', im0s\n                gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  #  normalization gain whwh\n                if det is not None and len(det):\n                    det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n                    for c in det[:, -1].unique():\n                        n = (det[:, -1] == c).sum()  # detections per class\n\n                    for *xyxy, conf, cls in det:\n                        if True:  # Write to file\n                            xywh = torch.tensor(xyxy).view(-1).numpy()  # normalized xywh\n#                             xywh[2] = xywh[2]-xywh[0]\n#                             xywh[3] = xywh[3]-xywh[1]\n                            bboxes.append(xywh)\n                            score.append(conf)\n            bboxes_2.append(bboxes)\n            score_2.append(score)\n        all_path.append(path)\n        all_score.append(score_2)\n        all_bboxex.append(bboxes_2)\n    return all_path,all_score,all_bboxex\n\n\n\nclass opt:\n    weights = ['../input/foldogw/best.pt','../input/fold1gwnew/best.pt','../input/fold2gwnew//best.pt',\n                           '../input/yolofold330epochs/weights/best_yolov5x_fold3.pt',\n                                       '../input/fold3wheat30/weights/best_yolov5x_fold4.pt']\n    img_size = 1024\n    conf_thres = 0.1\n    iou_thres = 0.94\n    augment = True\n    device = '0'\n    classes=None\n    agnostic_nms = True\n        \nopt.img_size = check_img_size(opt.img_size)\n\n\nwith torch.no_grad():\n    res = detect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_wbf_yolo(boxes,scores, image_size=1024, iou_thr=0.4, skip_box_thr=0.34, weights=None):\n    labels0 = [np.ones(len(scores[idx])) for idx in range(len(scores))]\n    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels0, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    return boxes, scores, labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Yolov5 Preds","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_path,all_score,all_bboxex = res\nyolov5preds = {}\nfor row in range(len(all_path)):\n    preds = {}\n    image_id = all_path[row].split(\"/\")[-1].split(\".\")[0]\n    boxes = all_bboxex[row]\n    scores = all_score[row]\n    boxes, scores, labels = run_wbf_yolo(boxes,scores)\n    yolov5preds[image_id] = [boxes,scores,labels]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EffDet","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_net(checkpoint_path):\n    config = get_efficientdet_config('tf_efficientdet_d5')\n    net = EfficientDet(config, pretrained_backbone=False)\n\n    config.num_classes = 1\n    config.image_size=512\n    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n\n    checkpoint = torch.load(checkpoint_path)\n    net.load_state_dict(checkpoint['model_state_dict'])\n\n    del checkpoint\n    gc.collect()\n\n    net = DetBenchEval(net, config)\n    net.eval();\n    return net.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_iou_thr = 0.432\nbest_skip_box_thr = 0.397","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [\n        load_net('../input/kernel5c5dc38533/effdet5-cutmix-augmix-bboxaug-0/last-checkpoint.bin'),\n        load_net('../input/fold-1-global-wheat/effdet5-cutmix-augmix-bboxaug-1/last-checkpoint.bin'),\n        load_net('../input/fold-2-global-wheat/effdet5-cutmix-augmix-bboxaug-2/last-checkpoint.bin'),\n        load_net('../input/fold-3-bbox-aug-50/effdet5-cutmix-augmix-bboxaug-3/last-checkpoint.bin'),\n        load_net('../input/fold-4-bbox-aug-50/effdet5-cutmix-augmix-bboxaug-4/last-checkpoint.bin'),\n        load_net('../input/fold-0/effdet5-cutmix-augmix-0/last-checkpoint.bin'),\n        load_net('../input/fold-1/effdet5-cutmix-augmix-1/last-checkpoint.bin'),\n        load_net('../input/fold-2-augmix/effdet5-cutmix-augmix-2/last-checkpoint.bin'),\n        load_net('../input/fold-4/effdet5-cutmix-augmix-4/last-checkpoint.bin')\n    ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_ROOT_PATH = '../input/global-wheat-detection/test'\n\nclass TestDatasetRetriever(Dataset):\n\n    def __init__(self, image_ids, transforms=None):\n        super().__init__()\n        self.image_ids = image_ids\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        image = cv2.imread(f'{DATA_ROOT_PATH}/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        if self.transforms:\n            sample = {'image': image}\n            sample = self.transforms(**sample)\n            image = sample['image']\n        return image, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n\ndef get_valid_transforms():\n    return A.Compose(\n        [\n            A.Resize(height=512, width=512, p=1.0),\n            ToTensorV2(p=1.0),\n        ], \n        p=1.0, \n    )\n\ndataset = TestDatasetRetriever(\n    image_ids=np.array([path.split('/')[-1][:-4] for path in glob.glob(f'{DATA_ROOT_PATH}/*.jpg')]),\n    transforms=get_valid_transforms()\n)\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ndata_loader = DataLoader(\n    dataset,\n    batch_size=2,\n    shuffle=False,\n    num_workers=4,\n    drop_last=False,\n    collate_fn=collate_fn\n)\n\ndef make_predictions(\n    images, \n    score_threshold=0.25,\n):\n    images = torch.stack(images).cuda().float()\n    predictions = []\n    for fold_number, net in enumerate(models):\n        with torch.no_grad():\n            det = net(images, torch.tensor([1]*images.shape[0]).float().cuda())\n            result = []\n            for i in range(images.shape[0]):\n                boxes = det[i].detach().cpu().numpy()[:,:4]    \n                scores = det[i].detach().cpu().numpy()[:,4]\n                indexes = np.where(scores > score_threshold)[0]\n                boxes = boxes[indexes]\n                boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n                boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n                result.append({\n                    'boxes': boxes[indexes],\n                    'scores': scores[indexes],\n                })\n            predictions.append(result)\n    return predictions\n\n\ndef run_wbf(predictions, image_index, image_size=512, iou_thr=best_iou_thr, skip_box_thr=best_skip_box_thr, weights=None):\n    boxes = [(prediction[image_index]['boxes']/(image_size-1)).tolist()  for prediction in predictions]\n    scores = [prediction[image_index]['scores'].tolist()  for prediction in predictions]\n    labels = [np.ones(prediction[image_index]['scores'].shape[0]).tolist() for prediction in predictions]\n    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    boxes = boxes*(image_size-1)\n    return boxes, scores, labels\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EffDet Preds","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"effdetpreds = {}\nfor images, image_ids in data_loader:\n    predictions = make_predictions(images)\n    for i, image in enumerate(images):\n        boxes, scores, labels = run_wbf(predictions, image_index=i)\n        boxes = (boxes*2)\n        image_id = image_ids[i]\n        effdetpreds[image_id] =  [boxes,scores,labels]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_prediction_string(boxes, scores):\n    pred_strings = []\n    for j in zip(scores, boxes):\n        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n    return \" \".join(pred_strings)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = effdetpreds.copy()\nx = yolov5preds.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_last_wbf(yolo,eff,weights=[2,1],iou_thr=0.5,skip_box_thr=0.397):\n    box1,scores1,labels1 = yolo\n    box2,scores2,labels2 = eff\n    box1 = box1/1023\n    box2 = box2/1023\n    boxes, scores, labels = weighted_boxes_fusion([box1,box2], [scores1,scores2],\n                                                [labels1,labels2],weights=[2,1],\n                                                  iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    return boxes,scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iou_thr = 0.5\nskip_box_thr = 0.0001\nresults = []\nfor row in range(len(all_path)):\n    image_id = all_path[row].split(\"/\")[-1].split(\".\")[0]\n    boxes,scores = run_last_wbf(yolov5preds[image_id],effdetpreds[image_id])\n    boxes = (boxes*1023).astype(np.int32).clip(min=0, max=1023)\n    boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n    boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n    result = {'image_id': image_id,'PredictionString': format_prediction_string(boxes, scores)}\n    results.append(result)\n\ntest_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.to_csv('submission.csv', index=False)\ntest_df.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}