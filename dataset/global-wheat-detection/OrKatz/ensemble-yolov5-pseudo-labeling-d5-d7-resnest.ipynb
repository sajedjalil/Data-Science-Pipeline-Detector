{"cells":[{"metadata":{},"cell_type":"markdown","source":"# YOLOv5 Pseudo Labeling\n\nAccording to the results of [this notebook](https://www.kaggle.com/nvnnghia/fasterrcnn-pseudo-labeling) FaterRCNN seems to work well with Pseudo Labeling.\nIn this notebook I am going to test Pseudo labeling technique on Yolov5.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom tqdm.auto import tqdm\nimport shutil as sh","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Getting yolov5 repo ","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#!git clone https://github.com/ultralytics/yolov5\n#!mv yolov5/* ./\n\n!cp -r ../input/yolov5-pseudo-labeling/* .","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --no-deps '../input/weightedboxesfusion/' > /dev/null","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Convert train data to yolov5 format\nBased on [this notebook](https://www.kaggle.com/orkatz2/yolov5-train)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"R_fold = 1\ndef convertTrainLabel():\n    df = pd.read_csv('../input/global-wheat-detection/train.csv')\n    bboxs = np.stack(df['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\n    for i, column in enumerate(['x', 'y', 'w', 'h']):\n        df[column] = bboxs[:,i]\n    df.drop(columns=['bbox'], inplace=True)\n    df['x_center'] = df['x'] + df['w']/2\n    df['y_center'] = df['y'] + df['h']/2\n    df['classes'] = 0\n    from tqdm.auto import tqdm\n    import shutil as sh\n    df = df[['image_id','x', 'y', 'w', 'h','x_center','y_center','classes']]\n    \n    index = list(set(df.image_id))\n    \n    source = 'train'\n    if True:\n        for fold in [0]:\n            val_index = index[len(index)*R_fold//5:len(index)*(R_fold+1)//5]\n            for name,mini in tqdm(df.groupby('image_id')):\n                if name in val_index:\n                    path2save = 'val2017/'\n                else:\n                    path2save = 'train2017/'\n                if not os.path.exists('convertor/fold{}/labels/'.format(fold)+path2save):\n                    os.makedirs('convertor/fold{}/labels/'.format(fold)+path2save)\n                with open('convertor/fold{}/labels/'.format(fold)+path2save+name+\".txt\", 'w+') as f:\n                    row = mini[['classes','x_center','y_center','w','h']].astype(float).values\n                    row = row/1024\n                    row = row.astype(str)\n                    for j in range(len(row)):\n                        text = ' '.join(row[j])\n                        f.write(text)\n                        f.write(\"\\n\")\n                if not os.path.exists('convertor/fold{}/images/{}'.format(fold,path2save)):\n                    os.makedirs('convertor/fold{}/images/{}'.format(fold,path2save))\n                sh.copy(\"../input/global-wheat-detection/{}/{}.jpg\".format(source,name),'convertor/fold{}/images/{}/{}.jpg'.format(fold,path2save,name))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some useful functions\nTTA, WBF, etc","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from ensemble_boxes import *\ndef run_wbf(boxes, scores, image_size=1023, iou_thr=0.5, skip_box_thr=0.7, weights=None):\n    #boxes = [prediction[image_index]['boxes'].data.cpu().numpy()/(image_size-1) for prediction in predictions]\n    #scores = [prediction[image_index]['scores'].data.cpu().numpy() for prediction in predictions]\n    labels = [np.zeros(score.shape[0]) for score in scores]\n    boxes = [box/(image_size) for box in boxes]\n    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    #boxes, scores, labels = nms(boxes, scores, labels, weights=[1,1,1,1,1], iou_thr=0.5)\n    boxes = boxes*(image_size)\n    return boxes, scores, labels\n\ndef TTAImage(image, index):\n    image1 = image.copy()\n    if index==0: \n        rotated_image = cv2.rotate(image1, cv2.ROTATE_90_CLOCKWISE)\n        return rotated_image\n    elif index==1:\n        rotated_image2 = cv2.rotate(image1, cv2.ROTATE_90_CLOCKWISE)\n        rotated_image2 = cv2.rotate(rotated_image2, cv2.ROTATE_90_CLOCKWISE)\n        return rotated_image2\n    elif index==2:\n        rotated_image3 = cv2.rotate(image1, cv2.ROTATE_90_CLOCKWISE)\n        rotated_image3 = cv2.rotate(rotated_image3, cv2.ROTATE_90_CLOCKWISE)\n        rotated_image3 = cv2.rotate(rotated_image3, cv2.ROTATE_90_CLOCKWISE)\n        return rotated_image3\n    elif index == 3:\n        return image1\n    \ndef rotBoxes90(boxes, im_w, im_h):\n    ret_boxes =[]\n    for box in boxes:\n        x1, y1, x2, y2 = box\n        x1, y1, x2, y2 = x1-im_w//2, im_h//2 - y1, x2-im_w//2, im_h//2 - y2\n        x1, y1, x2, y2 = y1, -x1, y2, -x2\n        x1, y1, x2, y2 = int(x1+im_w//2), int(im_h//2 - y1), int(x2+im_w//2), int(im_h//2 - y2)\n        x1a, y1a, x2a, y2a = min(x1, x2), min(y1, y2), max(x1, x2), max(y1, y2)\n        ret_boxes.append([x1a, y1a, x2a, y2a])\n    return np.array(ret_boxes)\n\ndef detect1Image(im0, imgsz, model, device, conf_thres, iou_thres):\n    img = letterbox(im0, new_shape=imgsz)[0]\n    # Convert\n    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n    img = np.ascontiguousarray(img)\n\n\n    img = torch.from_numpy(img).to(device)\n    img =  img.float()  # uint8 to fp16/32\n    img /= 255.0   \n    if img.ndimension() == 3:\n        img = img.unsqueeze(0)\n\n    # Inference\n    pred = model(img, augment=False)[0]\n\n    # Apply NMS\n    pred = non_max_suppression(pred, conf_thres, iou_thres)\n\n    boxes = []\n    scores = []\n    for i, det in enumerate(pred):  # detections per image\n        # save_path = 'draw/' + image_id + '.jpg'\n        if det is not None and len(det):\n            # Rescale boxes from img_size to im0 size\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n\n            # Write results\n            for *xyxy, conf, cls in det:\n                boxes.append([int(xyxy[0]), int(xyxy[1]), int(xyxy[2]), int(xyxy[3])])\n                scores.append(conf)\n\n    return np.array(boxes), np.array(scores) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make pseudo labels for Yolov5","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from utils.datasets import *\nfrom utils.utils import *\n\ndef makePseudolabel(weights = '../input/yolov5pth/weightsbest_yolov5x_fold3.pt'):\n    source = '../input/global-wheat-detection/test/'\n#     weights = '../input/yolov5pth/weightsbest_yolov5x_fold3.pt'\n    imgsz = 1024\n    conf_thres = 0.5\n    iou_thres = 0.6\n    is_TTA = True\n    \n    imagenames =  os.listdir(source)\n    \n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n    # Load model\n    model = torch.load(weights, map_location=device)['model'].float()  # load to FP32\n    model.to(device).eval()\n    \n    dataset = LoadImages(source, img_size=imgsz)\n\n    path2save = 'train2017/'\n    if not os.path.exists('convertor/fold0/labels/'+path2save):\n        os.makedirs('convertor/fold0/labels/'+path2save)\n    if not os.path.exists('convertor/fold0/images/{}'.format(path2save)):\n        os.makedirs('convertor/fold0/images/{}'.format(path2save))\n            \n    for name in tqdm(imagenames):\n        image_id = name.split('.')[0]\n        im01 = cv2.imread('%s/%s.jpg'%(source,image_id))  # BGR\n#         im01 = cv2.resize(im01,(1024,1024))\n        if im01.shape[0]!=1024 or im01.shape[1]!=1024:\n            continue\n        assert im01 is not None, 'Image Not Found '\n        # Padded resize\n        im_w, im_h = im01.shape[:2]\n        if is_TTA:\n            enboxes = []\n            enscores = []\n            for i in range(4):\n                im0 = TTAImage(im01, i)\n                boxes, scores = detect1Image(im0, imgsz, model, device, conf_thres, iou_thres)\n                for _ in range(3-i):\n                    boxes = rotBoxes90(boxes, im_w, im_h)\n                    \n                enboxes.append(boxes)\n                enscores.append(scores) \n\n            boxes, scores, labels = run_wbf(enboxes, enscores, image_size = im_w, iou_thr=0.6, skip_box_thr=0.43)\n            boxes = boxes.astype(np.int32).clip(min=0, max=im_w)\n        else:\n            boxes, scores = detect1Image(im01, imgsz, model, device, conf_thres, iou_thres)\n\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n        \n        boxes = boxes[scores >= 0.1].astype(np.int32)\n        scores = scores[scores >=float(0.1)]\n        \n        lineo = ''\n        for box in boxes:\n            x1, y1, w, h = box\n            xc, yc, w, h = (x1+w/2)/1024, (y1+h/2)/1024, w/1024, h/1024\n            lineo += '0 %f %f %f %f\\n'%(xc, yc, w, h)\n            \n        fileo = open('convertor/fold0/labels/'+path2save+image_id+\".txt\", 'w+')\n        fileo.write(lineo)\n        fileo.close()\n        sh.copy(\"../input/global-wheat-detection/test/{}.jpg\".format(image_id),'convertor/fold0/images/{}/{}.jpg'.format(path2save,image_id))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"convertTrainLabel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"makePseudolabel()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Retrain yolov5 with pseudo data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if len(os.listdir('../input/global-wheat-detection/test/'))<11:\n    pass\n#     !python train.py --img 1024 --batch 4 --epochs 1 --data ../input/configyolo5/wheat0.yaml --cfg ../input/newcon/yolov5x3.yaml --weights ../input/yolov5pth/weightsbest_yolov5x_fold3.pt  \nelse:\n    !python train.py --img 1024 --batch 4 --epochs 10 --data ../input/configyolo5/wheat0.yaml --cfg ../input/newcon/yolov5x3.yaml --weights ../input/yolov5pth/weightsbest_yolov5x_fold3.pt    \n!rm -rf convertor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if not os.path.exists('weights/best.pt'):\n#     weights = '../input/yolov5pth/best_yolov5x_fold3.pt'\n# else:\n#     convertTrainLabel()\n#     makePseudolabel(weights = 'weights/best.pt')\n#     if len(os.listdir('../input/global-wheat-detection/test/'))<11:\n#         !python train.py --img 1024 --batch 4 --epochs 1 --data ../input/configyolo5/wheat0.yaml --cfg ../input/newcon/yolov5x3.yaml --weights weights/best.pt \n#     else:\n#         !python train.py --img 1024 --batch 4 --epochs 4 --data ../input/configyolo5/wheat0.yaml --cfg ../input/newcon/yolov5x3.yaml --weights weights/best.pt  \n#     !rm -rf convertor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_prediction_string(boxes, scores):\n    pred_strings = []\n    for j in zip(scores, boxes):\n        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n\n    return \" \".join(pred_strings)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# !cp ../input/yolo5tta4/yolo.py ./models/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from utils.datasets import *\nfrom utils.utils import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect1Image_aug(im0, imgsz, model, device, conf_thres, iou_thres):\n    img = letterbox(im0, new_shape=imgsz)[0]\n    # Convert\n    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n    img = np.ascontiguousarray(img)\n\n\n    img = torch.from_numpy(img).to(device)\n    img =  img.float()  # uint8 to fp16/32\n    img /= 255.0   \n    if img.ndimension() == 3:\n        img = img.unsqueeze(0)\n\n    # Inference\n    pred = model(img, augment=True)[0]\n\n    # Apply NMS\n    pred = non_max_suppression(pred, conf_thres, iou_thres)\n\n    boxes = []\n    scores = []\n    for i, det in enumerate(pred):  # detections per image\n        # save_path = 'draw/' + image_id + '.jpg'\n        if det is not None and len(det):\n            # Rescale boxes from img_size to im0 size\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n\n            # Write results\n            for *xyxy, conf, cls in det:\n                boxes.append([int(xyxy[0]), int(xyxy[1]), int(xyxy[2]), int(xyxy[3])])\n                scores.append(conf)\n\n    return np.array(boxes), np.array(scores) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clip_coords2(boxes, img_shape):\n    # Clip bounding xyxy bounding boxes to image shape (height, width)\n    boxes[:, 0].clamp_(0, img_shape[1])  # x1\n    boxes[:, 1].clamp_(0, img_shape[0])  # y1\n    boxes[:, 2].clamp_(0, img_shape[1])  # x2\n    boxes[:, 3].clamp_(0, img_shape[0])  # y2\n    \ndef scale_coords2(coords,factorx,factory, img0_shape):\n    coords[:, 0::2] *= factorx  # x padding\n    coords[:, 1::2] *= factory  # y padding\n    clip_coords2(coords, img0_shape)\n    return coords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --no-deps '../input/timm-package/timm-0.1.26-py3-none-any.whl' > /dev/null\n!pip install --no-deps '../input/pycocotools/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl' > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install /kaggle/input/orkatzfdata/yacs-0.1.7-py3-none-any.whl\n!mkdir fvcore\n!cp -R '/kaggle/input/orkatzfdata/fvcore-0.1.dev200407/fvcore-0.1.dev200407/' ./fvcore\n!pip install fvcore/fvcore-0.1.dev200407/.\n!mkdir detectron2-ResNeSt\n!cp -R /kaggle/input/orkatzfdata/detectron2-ResNeSt/* ./detectron2-ResNeSt/\n!pip install detectron2-ResNeSt/.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.insert(0, \"../input/timm-efficientdet-pytorch\")\nsys.path.insert(0, \"../input/omegaconf\")\nimport os\nimport os\n# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\nos.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\nfrom ensemble_boxes import *\nimport torch\nimport random\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom torch.utils.data import Dataset,DataLoader\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport cv2\nimport gc\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchEval\nfrom effdet.efficientdet import HeadNet\nfrom sklearn.model_selection import StratifiedKFold\nfrom skopt import gp_minimize, forest_minimize\nfrom skopt.utils import use_named_args\nfrom skopt.plots import plot_objective, plot_evaluations, plot_convergence, plot_regret\nfrom skopt.space import Categorical, Integer, Real","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_net(checkpoint_path):\n    config = get_efficientdet_config('tf_efficientdet_d5')\n    net = EfficientDet(config, pretrained_backbone=False)\n\n    config.num_classes = 1\n    config.image_size = 512\n    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n\n    checkpoint = torch.load(checkpoint_path)\n    net.load_state_dict(checkpoint)\n\n    del checkpoint\n    gc.collect()\n\n    net = DetBenchEval(net, config)\n    net.eval();\n    return net.cuda()\n\ndef load_net7(checkpoint_path):\n    config = get_efficientdet_config('tf_efficientdet_d7')\n    net = EfficientDet(config, pretrained_backbone=False)\n\n    config.num_classes = 1\n    config.image_size = 1024\n    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n    net = torch.nn.DataParallel(net)\n    checkpoint = torch.load(checkpoint_path)\n    net.load_state_dict(checkpoint['model_state_dict'])\n\n    del checkpoint\n    gc.collect()\n\n    net = DetBenchEval(net, config)\n    net.eval();\n    return net.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models2 = [load_net7('../input/tempb7/best-checkpoint-015epoch.bin'),\n           load_net7('../input/tempb7/best-checkpoint-020epoch.bin'),\n           load_net7('../input/tempb7/best-checkpoint-022epoch.bin'),]\nmodels = [\n        load_net('../input/effdetbestpth/best-fold0-augmix.pth'),\n        load_net('../input/effdetbestpth/best-fold3.pth'),\n        load_net('../input/effdetbestpth/best-fold4.pth'),\n        load_net('../input/effdetbestpth/best-fold1-augmix.pth'),\n        load_net('../input/effdetbestpth/best-fold2-augmix.pth'),\n        load_net('../input/effdetbestpth/fold0-best.bin')\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from detectron2.config import get_cfg\nimport numpy as np\nimport cv2\nimport random\n\n# import some common detectron2 utilities\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog\ncfg = get_cfg()\n\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_cascade_rcnn_ResNeSt_101_FPN_syncbn_range-scale_1x.yaml\"))\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # \n\n\ncfg.MODEL.WEIGHTS = os.path.join('/kaggle/input/best-inrae-1/', \"model_final.pth\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.47  # set the testing threshold for this model\ncfg.DATASETS.TEST = (\"m5_val\", )\npredictor1 = DefaultPredictor(cfg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models3 =[predictor1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_ROOT_PATH = '../input/global-wheat-detection/test'\n\nclass TestDatasetRetriever(Dataset):\n\n    def __init__(self, image_ids, transforms=None,transforms2=None):\n        super().__init__()\n        self.image_ids = image_ids\n        self.transforms = transforms\n        self.transforms2 = transforms2\n\n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        image = cv2.imread(f'{DATA_ROOT_PATH}/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        if self.transforms:\n            sample = {'image': image}\n            sample = self.transforms(**sample)\n            image0 = sample['image']\n        if self.transforms2:\n            sample = {'image': image}\n            sample = self.transforms2(**sample)\n            image1 = sample['image']\n        return image0,image1, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n\ndef get_valid_transforms():\n    return A.Compose(\n        [\n            A.Resize(height=512, width=512, p=1.0),\n            ToTensorV2(p=1.0),\n        ], \n        p=1.0, \n    )\n\ndef get_valid_transforms2():\n    return A.Compose(\n        [\n            A.Resize(height=1024, width=1024, p=1.0),\n            ToTensorV2(p=1.0),\n        ], \n        p=1.0, \n    )\n\ndataset = TestDatasetRetriever(\n    image_ids=np.array([path.split('/')[-1][:-4] for path in glob(f'{DATA_ROOT_PATH}/*.jpg')]),\n    transforms=get_valid_transforms(),transforms2=get_valid_transforms2()\n)\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ndata_loader = DataLoader(\n    dataset,\n    batch_size=1,\n    shuffle=False,\n    num_workers=4,\n    drop_last=False,\n    collate_fn=collate_fn\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_predictions(\n    images, images1,image_ids,\n    score_threshold=0.25,\n):\n    images = images.cuda().float()\n    images1 = images1.cuda().float()\n    image_id = image_ids\n    rh,rw,_ = cv2.imread(f'{DATA_ROOT_PATH}/{image_id}.jpg').shape\n    Hscale512 = rh/512\n    Wscale512 = rw/512\n    \n    Hscale1024 = rh/1024\n    Wscale1024 = rw/1024\n    predictions = []\n    for fold_number, net in enumerate(models):\n        with torch.no_grad():\n            det = net(images, torch.tensor([1]*images.shape[0]).float().cuda())\n            result = []\n            for i in range(images.shape[0]):\n                boxes = det[i].detach().cpu().numpy()[:,:4]    \n                scores = det[i].detach().cpu().numpy()[:,4]\n                indexes = np.where(scores > score_threshold)[0]\n#                 boxes = boxes[indexes]\n                boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n                boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n                \n                boxes[:, 0::2] *= Wscale512\n                boxes[:, 1::2] *= Hscale512\n                result.append({\n                    'boxes': boxes[indexes],\n                    'scores': scores[indexes],\n                })\n            predictions.append(result)\n    \n    for fold_number, net in enumerate(models2):\n        with torch.no_grad():\n            det = net(images1, torch.tensor([1]*images1.shape[0]).float().cuda())\n            result = []\n            for i in range(images.shape[0]):\n                boxes = det[i].detach().cpu().numpy()[:,:4]    \n                scores = det[i].detach().cpu().numpy()[:,4]\n                indexes = np.where(scores > score_threshold)[0]\n#                 boxes = boxes[indexes]\n                boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n                boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n                boxes[:, 0::2] *= Wscale1024\n                boxes[:, 1::2] *= Hscale1024\n                result.append({\n                    'boxes': boxes[indexes],\n                    'scores': scores[indexes],\n                })\n            predictions.append(result)\n    for fold_number, net in enumerate(models3):\n        image_id = image_ids\n        result =[]\n        with torch.no_grad():\n            im = cv2.imread(f'{DATA_ROOT_PATH}/{image_id}.jpg', cv2.IMREAD_COLOR)\n            outputs = predictor1(im)\n            out = outputs[\"instances\"].to(\"cpu\")\n            scores = out.get_fields()['scores'].numpy()\n            boxes = out.get_fields()['pred_boxes'].tensor.numpy().astype(int)\n            labels= out.get_fields()['scores'].numpy()\n            result.append({\n                    'boxes': boxes,\n                    'scores': scores,\n                })\n            predictions.append(result)\n            predictions.append(result)\n            \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clip_coords3(boxes, img_shape):\n    # Clip bounding xyxy bounding boxes to image shape (height, width)\n    boxes[:, 0].clamp_(0, img_shape[1])  # x1\n    boxes[:, 1].clamp_(0, img_shape[0])  # y1\n    boxes[:, 2].clamp_(0, img_shape[1])  # x2\n    boxes[:, 3].clamp_(0, img_shape[0])  # y2\n    return boxes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_wbf2(predictions, image_index, image_size=1024, iou_thr=0.34, skip_box_thr=0.33, weights=None):\n    boxes = [(prediction[image_index]['boxes']/(image_size-1)).tolist()  for prediction in predictions]#+ [(prediction[image_index]['boxes']/(image_size-1)).tolist()  for prediction in predictions1]\n    scores = [prediction[image_index]['scores'].tolist()  for prediction in predictions]#+[prediction[image_index]['scores'].tolist()  for prediction in predictions1]\n    labels = [np.ones(prediction[image_index]['scores'].shape[0]).tolist() for prediction in predictions]#+[np.ones(prediction[image_index]['scores'].shape[0]).tolist() for prediction in predictions1]\n    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    boxes = boxes*(image_size-1)\n    return boxes, scores, labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect():\n    transforms = get_valid_transforms()\n    transforms2 = get_valid_transforms2()\n    source = '../input/global-wheat-detection/test/'\n    weights = 'weights/best.pt'\n    weights800 = '../input/yolo800/best_yolov5x_fold0_800.pt'\n    if not os.path.exists(weights):\n        weights = '../input/yolov5pth/weightsbest_yolov5x_fold3.pt'\n    imgsz = 1024\n    conf_thres = 0.34\n    iou_thres = 0.45\n    is_TTA = True\n    \n    imagenames =  os.listdir(source)\n    \n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n    # Load model\n    model = torch.load(weights, map_location=device)['model'].float()  # load to FP32\n    model.to(device).eval()\n    \n    model800 = torch.load(weights800, map_location=device)['model'].float()  # load to FP32\n    model800.to(device).eval()\n    \n    dataset = LoadImages(source, img_size=imgsz)\n\n    results = []\n    fig, ax = plt.subplots(5, 2, figsize=(30, 70))\n    count = 0\n    # img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n    #for path, img, im0s, _ in dataset:\n    for name in tqdm(imagenames):\n        image_id = name.split('.')[0]\n        im01 = cv2.imread('%s/%s.jpg'%(source,image_id))  # BGR\n        image = cv2.cvtColor(im01, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n\n        sample = {'image': image}\n        sample = transforms(**sample)\n        image0 = sample['image'] #512\n\n        sample = {'image': image}\n        sample = transforms2(**sample)\n        image1 = sample['image'] #1024\n        \n        if image1.ndimension() == 3:\n            image1 = image1.unsqueeze(0)\n            image0 = image0.unsqueeze(0)\n        assert im01 is not None, 'Image Not Found '\n        # Padded resize\n        im_w, im_h = im01.shape[:2]\n        if is_TTA:\n            enboxes = []\n            enscores = []\n            for i in range(4):\n                im0 = TTAImage(im01, i)\n                boxes, scores = detect1Image(im0, imgsz, model, device, conf_thres, iou_thres)\n                for _ in range(3-i):\n                    boxes = rotBoxes90(boxes, im_w, im_h)\n                    \n                if 1: #i<3:\n                    enboxes.append(boxes)\n                    enscores.append(scores) \n                    \n            boxes, scores = detect1Image(im01, imgsz, model, device, conf_thres, iou_thres)\n            enboxes.append(boxes)\n            enscores.append(scores)\n            enboxes.append(boxes)\n            enscores.append(scores)\n            \n#             boxes, scores = detect1Image_aug(im01, 800, model800, device, 0.1, 0.94)\n#             boxes, scores, labels = run_wbf([boxes], [scores], image_size = 800, iou_thr=0.4, skip_box_thr=0.3)\n#             enboxes.append(boxes)\n#             enscores.append(scores)\n\n\n            \n            predictions = make_predictions(image0,image1,image_id)\n            boxes, scores, labels = run_wbf2(predictions, image_index=0)\n            \n            enboxes.append(boxes)\n            enscores.append(scores)\n            enboxes.append(boxes)\n            enscores.append(scores)\n            \n            \n            boxes, scores, labels = run_wbf(enboxes, enscores, image_size = im_w, iou_thr=0.48, skip_box_thr=0.575)\n            boxes = boxes.astype(np.int32).clip(min=0, max=im_w)\n        else:\n            boxes, scores = detect1Image(im01, imgsz, model, device, conf_thres, iou_thres)\n\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n        \n        boxes = boxes[scores >= 0.05].astype(np.int32)\n        scores = scores[scores >=float(0.05)]\n        if count<10:\n            #sample = image.permute(1,2,0).cpu().numpy()\n            for box, score in zip(boxes,scores):\n                cv2.rectangle(im0,\n                              (box[0], box[1]),\n                              (box[2]+box[0], box[3]+box[1]),\n                              (220, 0, 0), 2)\n                cv2.putText(im0, '%.2f'%(score), (box[0]+np.random.randint(20), box[1]), cv2.FONT_HERSHEY_SIMPLEX ,  \n                   0.5, (255,255,255), 2, cv2.LINE_AA)\n            ax[count%5][count//5].imshow(im0)\n            count+=1\n            \n        result = {\n            'image_id': image_id,\n            'PredictionString': format_prediction_string(boxes, scores)\n        }\n\n        results.append(result)\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = detect()\ntest_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\ntest_df.to_csv('submission.csv', index=False)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls weights/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}