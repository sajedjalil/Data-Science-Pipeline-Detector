{"cells":[{"metadata":{},"cell_type":"markdown","source":"# version 47 \n### PB=0.708 LB=0.7505\n### multi sacle tta -> pseudo labeling -> OOF -> multi sacle tta","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\ntorch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\nPSEUDO = len(glob.glob(\"../input/global-wheat-detection/test/*\"))>10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python -m pip install --upgrade ../input/package/pip-20.1.1-py2.py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ../input/nvidiaapex/.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"!pip install --no-deps '../input/timm-package/timm-0.1.26-py3-none-any.whl' > /dev/null\n!pip install --no-deps '../input/pycocotools/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl' > /dev/null\n!pip install \"../input/eff-omegaconf/omegaconf-2.0.0-py3-none-any.whl\" > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cp -r ../input/train-effdet/* ./","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import sys\n# sys.path.insert(0, \"../input/train-effdet\")\n# sys.path.insert(0, \"../input/omegaconf\")\nsys.path.insert(0, \"../input/weightedboxesfusion\")\n\nimport ensemble_boxes\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom torch.utils.data import Dataset,DataLoader\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport cv2\nimport gc\nfrom matplotlib import pyplot as plt\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchEval\nfrom effdet.efficientdet import HeadNet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_valid_transforms():\n    return A.Compose([\n            A.Resize(height=1024, width=1024, p=1.0),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_ROOT_PATH = '../input/global-wheat-detection/test'\n\nclass DatasetRetriever(Dataset):\n\n    def __init__(self, image_ids, transforms=None):\n        super().__init__()\n        self.image_ids = image_ids\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        image = cv2.imread(f'{DATA_ROOT_PATH}/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        shape = image.shape\n        if self.transforms:\n            sample = {'image': image}\n            sample = self.transforms(**sample)\n            image = sample['image']\n        return shape,image, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = DatasetRetriever(\n    image_ids=np.array([path.split('/')[-1][:-4] for path in glob(f'{DATA_ROOT_PATH}/*.jpg')]),\n    transforms=get_valid_transforms()\n)\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ndata_loader = DataLoader(\n    dataset,\n    batch_size=1,\n    shuffle=False,\n    num_workers=2,\n    drop_last=False,\n    collate_fn=collate_fn\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom effdet.anchors import Anchors, AnchorLabeler, generate_detections, MAX_DETECTION_POINTS\nfrom effdet.bench import _post_process\nimport torch.nn.functional as F\nimport math\nimport numpy as np\n\nclass DetBenchEvalMultiScale(nn.Module):\n    def __init__(self, model, config,multiscale=[]):\n        super(DetBenchEvalMultiScale, self).__init__()\n        self.config = config\n        self.model = model\n        self.multiscale = multiscale\n        self.anchors = []\n        for i,m in enumerate(multiscale):\n            self.anchors.append(Anchors(\n                config.min_level, config.max_level,\n                config.num_scales, config.aspect_ratios,\n                config.anchor_scale, int(config.image_size*m)).cuda())\n        self.size = config.image_size\n\n    def forward(self, x, image_scales,scale):\n        if scale not in self.multiscale:\n            print(\"scale not in 0.5,0.625,0.75,0.875,1.0,1.125,1.25,1.375,1.5\")\n            return None\n        s = np.where(np.array(self.multiscale)==scale)[0][0]\n        class_out, box_out = self.model(x)\n        class_out, box_out, indices, classes = _post_process(self.config, class_out, box_out)\n\n        batch_detections = []\n        # FIXME we may be able to do this as a batch with some tensor reshaping/indexing, PR welcome\n        for i in range(x.shape[0]):\n            detections = generate_detections(\n                class_out[i], box_out[i], self.anchors[s].boxes, indices[i], classes[i], image_scales[i])\n            batch_detections.append(detections)\n        return torch.stack(batch_detections, dim=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_net(checkpoint_path):\n    config = get_efficientdet_config('tf_efficientdet_d7')\n    net = EfficientDet(config, pretrained_backbone=False)\n\n    config.num_classes = 1\n    config.image_size=1024\n    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n    net = torch.nn.DataParallel(net)\n    checkpoint = torch.load(checkpoint_path)\n    net.load_state_dict(checkpoint['ema_state_dict'])\n\n    del checkpoint\n    gc.collect()\n\n    net = DetBenchEvalMultiScale(net, config,[0.875,1.0,1.125,1.25])\n    net.eval();\n    return net.cuda()\n\nnet = load_net('../input/effd7pth/best-checkpoint-027epoch.bin')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn.functional as F\nimport math\ndef scale_img(img, ratio=1.0, same_shape=False):  # img(16,3,256,416), r=ratio\n    # scales img(bs,3,y,x) by ratio\n    h, w = img.shape[2:]\n    s = (int(h * ratio), int(w * ratio))  # new size\n    img = F.interpolate(img, size=s, mode='bilinear', align_corners=False)  # resize\n    if not same_shape:  # pad/crop img\n        gs = 32  # (pixels) grid size\n        h, w = [math.ceil(x * ratio / gs) * gs for x in (h, w)]\n    return F.pad(img, [0, w - s[1], 0, h - s[0]], value=0.447)  # value = imagenet mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_tta_predictions(images, score_threshold=0.4024215973594928):\n    with torch.no_grad():\n        x = torch.stack(images).float().cuda()\n        predictions = []\n        img_size = x.shape[-2:]  # height, width\n        s = [1.0,1.0,1.0,1.25,1.125]  # scales\n        y = []\n        all_x =[]\n        center = []\n        for i, xi in enumerate((x,\n                                x.flip(3), # only flip-lr\n                                x.flip(2),\n                                \n                                scale_img(x,s[3]),\n#                                 scale_img(x.flip(3),s[4]),\n#                                 scale_img(x.flip(2),s[5]),\n                                \n                                scale_img(x,s[4]),\n#                                 scale_img(x.flip(3),s[7]),\n#                                 scale_img(x.flip(2),s[8])\n                                )):\n            y.append(net(xi, torch.tensor([1]*xi.shape[0]).float().cuda(),s[i]))\n\n            \n        y[1][..., 0] = img_size[1] - y[1][..., 0] - y[1][..., 2] # flip lr\n        y[2][..., 1] = img_size[0] - y[2][..., 1] - y[2][..., 3]  # flip ud\n        \n        y[3][..., :4]/=s[3]\n        y[4][..., :4]/=s[4]\n#         y[4][..., 0] = img_size[1] - y[4][..., 0] - y[4][..., 2] # flip lr\n        \n\n#         y[5][..., :4]/=s[5]\n#         y[5][..., 1] = img_size[0] - y[5][..., 1] - y[5][..., 3]  # flip ud\n        \n        \n        \n#         y[6][..., :4]/=s[6]\n#         y[7][..., :4]/=s[7]\n#         y[7][..., 0] = img_size[1] - y[7][..., 0] - y[7][..., 2] # flip lr\n#         y[8][..., :4]/=s[8]\n#         y[8][..., 1] = img_size[0] - y[8][..., 1] - y[8][..., 3]  # flip ud\n        \n\n\n        \n        \n        y = np.array(y)\n        boxes_all = []\n        scores_all=[]\n        for j in range(len(y)):\n            boxes = y[j][0].cpu().numpy()[:,:4].copy() \n            scores = y[j][0].cpu().numpy()[:,4].copy()\n            indexes = np.where(scores > score_threshold)[0]\n            boxes = boxes[indexes]\n            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n            boxes_all.append(boxes/1024)\n            scores_all.append(scores[indexes])\n    return boxes_all,scores_all,y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_wbf(boxes,scores, image_index, image_size=1024, iou_thr=0.40813015995026714, skip_box_thr=0.4133694759009949, weights=[0.6,0.2,0.2],name='nms'):\n    labels = [np.ones([row.shape[0]]) for row in scores]\n    if name=='wbf':\n        boxes, scores, labels = ensemble_boxes.ensemble_boxes_wbf.weighted_boxes_fusion(boxes, scores, labels, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    else:\n        boxes, scores, labels = ensemble_boxes.nms(boxes, scores, labels, weights=None, iou_thr=iou_thr)\n#     boxes, scores, labels = ensemble_boxes.soft_nms(boxes, scores, labels, weights=None, iou_thr=iou_thr,thresh=skip_box_thr,sigma=0.5)\n\n    boxes = boxes*(image_size-1)\n    return boxes, scores, labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def norm(img):\n    img = img.astype(float)\n    img-=img.min()\n    img/=img.max()\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=[20,20])\nfor j, (shape,images, image_ids) in enumerate(data_loader):\n    h,w,_ = shape[0]\n    w_factor = w/1024\n    h_factor = h/1024\n\n    boxes_all,scores_all,y = make_tta_predictions(images,score_threshold= 0.4024215973594928)\n    i = 0\n    sample = norm(images[i].permute(1,2,0).cpu().numpy())\n\n    boxes, scores, labels = run_wbf(boxes_all,scores_all, image_index=i,name='wbf',iou_thr=0.43312889428044965, skip_box_thr= 0.393358169307333,\n                                    weights=[0.4,0.05,0.05,0.25,0.25])\n    boxes = boxes.round().clip(min=0, max=1023)\n    boxes[:, [0,2]]*=w_factor\n    boxes[:, [1,3]]*=h_factor\n    boxes = boxes.astype(np.int32)\n#     indexes = np.where(scores > 0.1)[0]\n#     boxes = boxes[indexes]\n    plt.subplot(3,3,j+1)\n    for box in boxes:\n        cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), (1, 1, 1), 3)\n\n    plt.imshow(sample)\n    if j==8:\n        break\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_prediction_string(boxes, scores):\n    pred_strings = []\n    for j in zip(scores, boxes):\n        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n    return \" \".join(pred_strings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nflag = True\ncsv_pseudo = []\nfor shape,images, image_ids in data_loader:\n    boxes_all,scores_all,y = make_tta_predictions(images,score_threshold=0.4024215973594928)\n    for i, image in enumerate(images):\n        h,w,_ = shape[i]\n        w_factor = float(w)/1024.0\n        h_factor = float(h)/1024.0\n        try:\n            boxes, scores, labels = run_wbf(boxes_all,scores_all, image_index=i,name='wbf',iou_thr=0.40813015995026714, skip_box_thr=0.4133694759009949,\n                                            weights=[0.4,0.05,0.05,0.25,0.25])\n        except Exception as e:\n            boxes, scores, labels = run_wbf(boxes_all,scores_all, image_index=i,name='wbf',iou_thr=0.395, skip_box_thr=0.393)\n            print(e)\n#         indexes = np.where(scores > 0.1)[0]\n#         boxes = boxes[indexes]\n        boxes = boxes.round().clip(min=0, max=1023)\n        boxes[:, [0,2]]*=w_factor\n        boxes[:, [1,3]]*=h_factor\n        image_id = image_ids[i]\n        boxes=boxes.astype(np.int32)\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n        df = pd.DataFrame(boxes,columns=['x','y','w','h'])\n        df['image_id'] = image_id\n        df['source'] = 'test'\n        csv_pseudo.append(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pseudo = pd.concat(csv_pseudo)[['image_id','x','y','w','h','source']]\ndf_pseudo.to_csv(\"/kaggle/working/pseudo.csv\",index=False)\ndf_pseudo.to_csv(\"/kaggle/working/train_spike_kaggle.csv\",index=False)\ndf_pseudo.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample = plt.imread(\"../input/global-wheat-detection/test/2fd875eaa.jpg\")\n# bbox = df_pseudo[df_pseudo.image_id=='2fd875eaa'][['x','y','w','h']].values\n# for box in bbox:\n#         cv2.rectangle(sample, (box[0], box[1]), (box[0]+box[2], box[1]+box[3]), (1, 1, 1), 3)\n# plt.imshow(sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir data\n!mkdir data/train\n!cp -r ../input/global-wheat-detection/train/* ./data/train\n!cp -r ../input/global-wheat-detection/test/* ./data/train\n!cp -r ../input/global-wheat-detection/train.csv ./","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import train_utils\nimport datasets_utils\nimport sys\nfrom tqdm.auto import tqdm\nimport torch\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\n#import matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom glob import glob\nimport math\nimport torchvision\nimport argparse\nimport pprint\n\nclass Config:\n    root_data = '/kaggle/working/data/train/'\n    csv = '/kaggle/working/train.csv'\n    pseudo_csv = '/kaggle/working/pseudo.csv'\n    fold_number = 2\n    num_workers = 4\n    batch_size = 2\n    grad_step = 1\n    n_epochs = 2\n    optimizer = torch.optim.SGD #torch.optim.AdamW\n    lr = 0.002\n    SchedulerClass = torch.optim.lr_scheduler.CosineAnnealingLR\n    scheduler_params = dict(\n        T_max=800,\n        )\n    verbose = True\n    verbose_step = 1\n    TrainMultiScale = [1.0]\n    print(TrainMultiScale)\n    net_name = 'tf_efficientdet_d7'\n    checkpoint_name = '../efficientdet/tf_efficientdet_d7_53-6d1d7a95.pth'\n    CocoFormat=False\nclass data_config:\n    real = 0.3\n    mosaic = 0.0\n    cutmix = 0.00\n    stylized = 0.0\n    scale = 0.2\n    hsv = 0.2\n\nif __name__==\"__main__\":\n    class opt:\n        fold=2\n        epochs=2\n        resume = '../input/effd7pth/best-checkpoint-027epoch.bin'\n        train=0 if not PSEUDO else 1\n    config = Config()\n    config.fold_number = opt.fold\n    config.resume = opt.resume\n    folder = 'effdet7_pseudo'\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n    config.folder = folder\n    config.n_epochs = opt.epochs\n    if not config.CocoFormat:\n        marking,df_folds,spike = train_utils.get_k_fols(config)\n        pseudo_csv = pd.read_csv(config.pseudo_csv)\n        index_pseudo = list(set(pseudo_csv.image_id))\n        train_csv = pd.concat([marking,pseudo_csv])\n        image_ids=list(df_folds[df_folds['fold'] != config.fold_number].index.values)\n        image_ids = image_ids+index_pseudo+index_pseudo\n        if opt.train==0:\n            image_ids = image_ids[0:10]\n        \n        train_dataset = datasets_utils.train_wheat(image_ids=np.array(image_ids),\n                                    marking=train_csv,\n                                    data_config = data_config,\n                                    transforms=datasets_utils.get_train_transforms(),\n                                    test=False,\n                                    TRAIN_ROOT_PATH=config.root_data)\n\n        validation_dataset = datasets_utils.DatasetRetrieverTest(image_ids=df_folds[df_folds['fold'] == config.fold_number].index.values,\n                                                  marking=marking,\n                                                  transforms=datasets_utils.get_valid_transforms(),\n                                                  test=True,\n                                                  TRAIN_ROOT_PATH=config.root_data)\n\n\n    train_loader = torch.utils.data.DataLoader(\n                                                    train_dataset,\n                                                    batch_size=config.batch_size,\n                                                    sampler=RandomSampler(train_dataset),\n                                                    pin_memory=False,\n                                                    drop_last=True,\n                                                    num_workers=config.num_workers,\n                                                    collate_fn=datasets_utils.collate_fn\n                                                   )\n    validation_loader = torch.utils.data.DataLoader(\n                                                    validation_dataset,\n                                                    batch_size=1,\n                                                    shuffle=False,\n                                                    num_workers=2,\n                                                    drop_last=False,\n                                                    collate_fn=datasets_utils.collate_fn\n                                                    )\n    if len(config.TrainMultiScale)==1:\n        net = train_utils.get_net(type_net=config.net_name,checkpoint_name=config.checkpoint_name,resume = config.resume)\n    else:\n        print(config.TrainMultiScale)\n        net = train_utils.get_net_multiscle(type_net=config.net_name,checkpoint_name=config.checkpoint_name,resume = config.resume,multiScale=config.TrainMultiScale)\n    net.cuda()\n    fitter = train_utils.Fitter(model=net, config=config)\n    fitter.fit(train_loader, validation_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()\nnet = load_net('/kaggle/working/effdet7_pseudo/last-checkpoint.bin')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom skopt import gp_minimize, forest_minimize\nfrom skopt.utils import use_named_args\nfrom skopt.plots import plot_objective, plot_evaluations, plot_convergence, plot_regret\nfrom skopt.space import Categorical, Integer, Real\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_tta_predictions_val(net,images, score_threshold=0.0):\n    with torch.no_grad():\n        x = torch.stack(images).float().cuda()\n        predictions = []\n        img_size = x.shape[-2:]  # height, width\n        s = [0.83, 0.67]  # scales\n        y = []\n        all_x =[]\n        for i, xi in enumerate((x,\n                                x.flip(3), # only flip-lr\n                                x.flip(2),\n#                                 x.flip(2).flip(3),  # only flip-up\n                                )):\n            xp = xi\n            all_x.append(xp)\n            y.append(net(xp, torch.tensor([1]*xp.shape[0]).float().cuda(),1.0))\n            \n        y[1][..., 0] = img_size[1] - y[1][..., 0] - y[1][..., 2] # flip lr\n        y[2][..., 1] = img_size[0] - y[2][..., 1] - y[2][..., 3]  # flip ud\n#         y[3][..., 1] = img_size[0] - y[3][..., 1] - y[3][..., 3]  # flip ud\n#         y[3][..., 0] = img_size[1] - y[3][..., 0] - y[3][..., 2] # flip lr\n        \n        \n        y = np.array(y)\n        boxes_all = []\n        scores_all=[]\n        for j in range(len(y)):\n            boxes = y[j][0].cpu().numpy()[:,:4].copy() \n            scores = y[j][0].cpu().numpy()[:,4].copy()\n            indexes = np.where(scores > score_threshold)[0]\n            boxes = boxes[indexes]\n\n            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n            boxes = boxes.round().clip(min=0, max=1023)\n            boxes_all.append(boxes/1024)\n            scores_all.append(scores[indexes])\n    return boxes_all,scores_all,y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if True:\n    all_predictions = []\n    for images, targets, image_ids in tqdm(validation_loader, total=len(validation_loader)):\n        with torch.no_grad():\n            fold_predictions = {}\n            for fold_number in range(2,3):\n#                 all_bbox,all_score,_ = make_tta_predictions(net,images,0.0)\n                all_bbox,all_score,_ = make_tta_predictions(images,0.0)\n\n            for i in range(1):\n                image_predictions = {\n                    'image_id': image_ids[i],\n                    'gt_boxes': (targets[i]['boxes'].cpu().numpy()).clip(min=0, max=1023).astype(int),\n                }\n                for fold_number in range(2,3):\n                    image_predictions[f'pred_boxes_fold{fold_number}'] = all_bbox\n                    image_predictions[f'scores_fold{fold_number}'] = all_score\n\n                all_predictions.append(image_predictions)\n        if not PSEUDO:\n            break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from test_utils import *\ndef calculate_final_score(\n    all_predictions,\n    iou_thr,\n    skip_box_thr=0.0,\n    score_threshold = 0.2,\n    method='nms',\n    sigma=0.5,weights=None\n):\n    final_scores = []\n\n    for i in tqdm(range(len(all_predictions))):\n        gt_boxes = all_predictions[i]['gt_boxes'].copy().astype(float)/1024\n        image_id = all_predictions[i]['image_id']\n        folds_boxes, folds_scores, folds_labels = [], [], []\n        for fold_number in range(2,3):\n            folds_boxes = all_predictions[i][f'pred_boxes_fold{fold_number}'].copy()[0:3]\n            folds_scores = all_predictions[i][f'scores_fold{fold_number}'].copy()[0:3]\n#             folds_labels = [np.ones([row.shape[0]]) for row in folds_scores]\n        folds_boxes_new = []\n        folds_scores_new = []\n        for bb in range(len(folds_boxes)):\n            s = folds_scores[bb].copy()\n            b = folds_boxes[bb].copy()\n            indexes = np.where(s > score_threshold)[0]\n            new_bbox = b[indexes]\n            new_score = s[indexes]\n            folds_boxes_new.append(new_bbox)\n            folds_scores_new.append(new_score)\n        folds_boxes = folds_boxes_new\n        folds_scores = folds_scores_new\n        folds_labels = [np.ones([row.shape[0]]) for row in folds_scores_new]\n        if method == 'weighted_boxes_fusion':\n            boxes, scores, labels = weighted_boxes_fusion(folds_boxes, folds_scores, folds_labels, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n        elif method == 'nms':\n            try:\n                boxes, scores, labels = nms(folds_boxes, folds_scores, folds_labels, weights=weights, iou_thr=iou_thr)\n            except:\n                boxes, scores, labels = weighted_boxes_fusion(folds_boxes, folds_scores, folds_labels, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n        elif method == 'soft_nms':\n            boxes, scores, labels = soft_nms(folds_boxes, folds_scores, folds_labels, weights=weights, iou_thr=iou_thr, thresh=skip_box_thr, sigma=sigma)\n        elif method == 'non_maximum_weighted':\n            boxes, scores, labels = non_maximum_weighted(folds_boxes, folds_scores, folds_labels, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n        else:\n            raise\n        image_precision = calculate_image_precision(gt_boxes, boxes, thresholds=iou_thresholds, form='pascal_voc')\n        final_scores.append(image_precision)\n\n    return np.mean(final_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from ensemble_boxes import *\nprint('[WBF]: ', calculate_final_score(\n        all_predictions, \n        iou_thr=0.43312889428044965,\n        skip_box_thr=0.393358169307333,\n        score_threshold = 0.4024215973594928,\n        method='weighted_boxes_fusion',weights=None\n    ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def log(text):\n    with open('opt.log', 'a+') as logger:\n        logger.write(f'{text}\\n')\n\ndef optimize(space, all_predictions, method, n_calls=10):\n    @use_named_args(space)\n    def score(**params):\n        print('-'*5 + f'{method}' + '-'*5)\n        print(params)\n        final_score = calculate_final_score(all_predictions, method=method, **params)\n        print(f'final_score = {final_score}')\n        print('-'*10)\n        return -final_score\n\n    return gp_minimize(func=score, dimensions=space, n_calls=n_calls)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"space = [\n    Real(0.3, 0.75, name='iou_thr'),\n    Real(0.2, 0.5, name='score_threshold'),\n    Real(0.3, 0.6, name='skip_box_thr'),\n#     Real(0.4, 0.9, name='x1'),\n#     Real(0.1, 0.5, name='x2'),\n#     Real(0.1, 0.5, name='x3'),\n#     Real(0.25, 0.55, name='th1'),\n#     Real(0.25, 0.55, name='th2'),\n#     Real(0.25, 0.55, name='th3'),\n]\n\nif True:\n    opt_result = optimize(\n        space, \n        all_predictions,\n        method='weighted_boxes_fusion',\n        n_calls=10 if not PSEUDO else 50,\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_iou_thr = opt_result.x[0]\nbest_score_thr = opt_result.x[1]\nbest_skip_box_thr = opt_result.x[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=[20,20])\nfor j, (shape,images, image_ids) in enumerate(data_loader):\n    h,w,_ = shape[0]\n    w_factor = w/1024\n    h_factor = h/1024\n\n    boxes_all,scores_all,y = make_tta_predictions(images,score_threshold=best_score_thr)\n    i = 0\n    sample = norm(images[i].permute(1,2,0).cpu().numpy())\n\n    boxes, scores, labels = run_wbf(boxes_all,scores_all, image_index=i,name='wbf',iou_thr=best_iou_thr, skip_box_thr=best_skip_box_thr,weights=[0.4,0.05,0.05,0.25,0.25])\n    boxes = boxes.round().clip(min=0, max=1023)\n    boxes[:, [0,2]]*=w_factor\n    boxes[:, [1,3]]*=h_factor\n    boxes = boxes.astype(np.int32)\n    plt.subplot(3,3,j+1)\n    for box in boxes:\n        cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), (1, 1, 1), 3)\n\n    plt.imshow(sample)\n    if j==8:\n        break\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nresults = []\nflag = True\nfor shape,images, image_ids in data_loader:\n    boxes_all,scores_all,y = make_tta_predictions(images,score_threshold=best_score_thr)\n    for i, image in enumerate(images):\n        h,w,_ = shape[i]\n        w_factor = float(w)/1024.0\n        h_factor = float(h)/1024.0\n        try:\n            boxes, scores, labels = run_wbf(boxes_all,scores_all, image_index=i,name='wbf',iou_thr=best_iou_thr, skip_box_thr=best_skip_box_thr,\n                                    weights=[0.4,0.05,0.05,0.25,0.25])\n        except Exception as e:\n            print(e)\n            try:\n                boxes, scores, labels = run_wbf(boxes_all,scores_all, image_index=i,name='wbf',iou_thr=0.43, skip_box_thr=0.4)\n            except Exception as e:\n                print(e)\n                boxes, scores = boxes[0]*1023, scores[0]\n            \n\n        boxes = boxes.round().clip(min=0, max=1023)\n        boxes[:, [0,2]]*=w_factor\n        boxes[:, [1,3]]*=h_factor\n        image_id = image_ids[i]\n        boxes=boxes.astype(np.int32)\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n        if flag:\n            plt.figure(figsize=[10,10])\n            sample = norm(image.permute(1,2,0).cpu().numpy())\n            for box in boxes:\n                cv2.rectangle(sample, (box[0], box[1]), (box[0]+box[2], box[1]+box[3]), (1, 0, 0), 1)\n            plt.imshow(sample)\n            plt.show()\n            flag=False\n        \n        result = {\n            'image_id': image_id,\n            'PredictionString': format_prediction_string(boxes, scores)\n        }\n        results.append(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\ntest_df.to_csv('submission.csv', index=False)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}