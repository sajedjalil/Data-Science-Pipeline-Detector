{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport cv2\nfrom tqdm.auto import tqdm\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/global-wheat-detection/train.csv')\nbboxs = np.stack(df['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\nfor i, column in enumerate(['x', 'y', 'w', 'h']):\n    df[column] = bboxs[:,i]\ndf.drop(columns=['bbox'], inplace=True)\ndf = df[['image_id','x', 'y', 'w', 'h']]\nindex = list(set(df.image_id))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir train\n!mkdir val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!git clone https://github.com/eriklindernoren/PyTorch-GAN/\n%cd PyTorch-GAN/implementations/pix2pix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport math\nimport itertools\nimport time\nimport datetime\nimport sys\n\nimport torchvision.transforms as transforms\nfrom torchvision.utils import save_image\n\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torch.autograd import Variable\n\nfrom models import *\nfrom datasets import *\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class opt:\n    epoch = 0\n    n_epochs = 10 #change to 50 for train\n    dataset_name = 'test1'\n    batch_size = 8\n    lr = 0.0002\n    b1 = 0.5\n    b2 = 0.999\n    decay_epoch = 100\n    n_cpu = 4\n    img_height = 256\n    img_width = 256\n    channels = 3\n    sample_interval = 100\n    checkpoint_interval = 338","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.makedirs(\"images/%s\" % opt.dataset_name, exist_ok=True)\nos.makedirs(\"saved_models/%s\" % opt.dataset_name, exist_ok=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cuda = True if torch.cuda.is_available() else False\n\n# Loss functions\ncriterion_GAN = torch.nn.MSELoss()\ncriterion_pixelwise = torch.nn.L1Loss()\n\n# Loss weight of L1 pixel-wise loss between translated image and real image\nlambda_pixel = 100\n\n# Calculate output of image discriminator (PatchGAN)\npatch = (1, opt.img_height // 2 ** 4, opt.img_width // 2 ** 4)\n\n# Initialize generator and discriminator\ngenerator = GeneratorUNet()\ndiscriminator = Discriminator()\n\nif cuda:\n    generator = generator.cuda()\n    discriminator = discriminator.cuda()\n    criterion_GAN.cuda()\n    criterion_pixelwise.cuda()\n\nif opt.epoch != 0:\n    # Load pretrained models\n    generator.load_state_dict(torch.load(\"saved_models/%s/generator_%d.pth\" % (opt.dataset_name, opt.epoch)))\n    discriminator.load_state_dict(torch.load(\"saved_models/%s/discriminator_%d.pth\" % (opt.dataset_name, opt.epoch)))\nelse:\n    # Initialize weights\n    generator.apply(weights_init_normal)\n    discriminator.apply(weights_init_normal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\nimport random\nimport os\nimport numpy as np\n\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom matplotlib import pyplot as plt\n\nclass ImageDataset_color(Dataset):\n    def __init__(self, root, transforms_=None, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n    def __getitem__(self, index):\n\n        img_A = cv2.imread(self.files[index % len(self.files)])\n        img_A = cv2.cvtColor(img_A,cv2.COLOR_BGR2RGB)\n        img_B = cv2.cvtColor(cv2.cvtColor(img_A,cv2.COLOR_RGB2GRAY),cv2.COLOR_GRAY2RGB)\n        img_A = Image.fromarray(np.array(img_A), \"RGB\")\n        img_B = Image.fromarray(np.array(img_B), \"RGB\")\n        if np.random.random() < 0.5:\n            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n\n        return {\"A\": img_A, \"B\": img_B}\n\n    def __len__(self):\n        return len(self.files)\n    \nclass ImageDataset_edge(Dataset):\n    def __init__(self, root, transforms_=None, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n    def __getitem__(self, index):\n\n        img_A = cv2.imread(self.files[index % len(self.files)])\n        gray = cv2.cvtColor(img_A, cv2.COLOR_BGR2GRAY)\n        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n        img_B = cv2.Canny(blurred,50,150)\n        img_B = cv2.cvtColor(img_B,cv2.COLOR_GRAY2RGB)\n        img_A = cv2.cvtColor(img_A,cv2.COLOR_BGR2RGB)\n        img_A = Image.fromarray(np.array(img_A), \"RGB\")\n        img_B = Image.fromarray(np.array(img_B), \"RGB\")\n        if np.random.random() < 0.5:\n            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n\n        return {\"A\": img_A, \"B\": img_B}\n\n    def __len__(self):\n        return len(self.files)\n    \nclass ImageDataset_edge(Dataset):\n    def __init__(self, root, transforms_=None, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n    def __getitem__(self, index):\n\n        img_A = cv2.imread(self.files[index % len(self.files)])\n        gray = cv2.cvtColor(img_A, cv2.COLOR_BGR2GRAY)\n        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n        img_B = cv2.Canny(blurred,50,150)\n        img_B = cv2.cvtColor(img_B,cv2.COLOR_GRAY2RGB)\n        img_A = cv2.cvtColor(img_A,cv2.COLOR_BGR2RGB)\n        img_A = Image.fromarray(np.array(img_A), \"RGB\")\n        img_B = Image.fromarray(np.array(img_B), \"RGB\")\n        if np.random.random() < 0.5:\n            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n\n        return {\"A\": img_A, \"B\": img_B}\n\n    def __len__(self):\n        return len(self.files)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls kagglet/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Configure dataloaders\ntransforms_ = [\n    transforms.Resize((opt.img_height, opt.img_width), Image.BICUBIC),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n]\n\ndataloader = DataLoader(\n    ImageDataset_color(\"/kaggle/input/global-wheat-detection/\", transforms_=transforms_),\n    batch_size=opt.batch_size,\n    shuffle=True,\n    num_workers=opt.n_cpu,\n)\n\nval_dataloader = DataLoader(\n    ImageDataset_color(\"/kaggle/input/global-wheat-detection/\", transforms_=transforms_, mode=\"test\"),\n    batch_size=10,\n    shuffle=True,\n    num_workers=1,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sample_images(batches_done):\n    \"\"\"Saves a generated sample from the validation set\"\"\"\n    imgs = next(iter(val_dataloader))\n    real_A = Variable(imgs[\"B\"].type(Tensor))\n    real_B = Variable(imgs[\"A\"].type(Tensor))\n    fake_B = generator(real_A)\n    img_sample = torch.cat((real_A.data, fake_B.data, real_B.data), -2).cpu().numpy().astype(np.float32)\n    img_sample -=img_sample.min()\n    img_sample/=img_sample.max()\n    img_sample = img_sample.transpose(0,2,3,1)\n    plt.figure(figsize=[10,20])\n    for row in range(3):\n        plt.subplot(1,3,row+1)\n        plt.imshow(img_sample[row])\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Run color train","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"prev_time = time.time()\n\nfor epoch in range(opt.epoch, opt.n_epochs):\n    for i, batch in enumerate(dataloader):\n\n        # Model inputs\n        real_A = Variable(batch[\"B\"].type(Tensor))\n        real_B = Variable(batch[\"A\"].type(Tensor))\n\n        # Adversarial ground truths\n        valid = Variable(Tensor(np.ones((real_A.size(0), *patch))), requires_grad=False)\n        fake = Variable(Tensor(np.zeros((real_A.size(0), *patch))), requires_grad=False)\n\n        # ------------------\n        #  Train Generators\n        # ------------------\n\n        optimizer_G.zero_grad()\n\n        # GAN loss\n        fake_B = generator(real_A)\n        pred_fake = discriminator(fake_B, real_A)\n        loss_GAN = criterion_GAN(pred_fake, valid)\n        # Pixel-wise loss\n        loss_pixel = criterion_pixelwise(fake_B, real_B)\n\n        # Total loss\n        loss_G = loss_GAN + lambda_pixel * loss_pixel\n\n        loss_G.backward()\n\n        optimizer_G.step()\n\n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n\n        optimizer_D.zero_grad()\n\n        # Real loss\n        pred_real = discriminator(real_B, real_A)\n        loss_real = criterion_GAN(pred_real, valid)\n\n        # Fake loss\n        pred_fake = discriminator(fake_B.detach(), real_A)\n        loss_fake = criterion_GAN(pred_fake, fake)\n\n        # Total loss\n        loss_D = 0.5 * (loss_real + loss_fake)\n\n        loss_D.backward()\n        optimizer_D.step()\n\n        # --------------\n        #  Log Progress\n        # --------------\n\n        # Determine approximate time left\n        batches_done = epoch * len(dataloader) + i\n        batches_left = opt.n_epochs * len(dataloader) - batches_done\n        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n        prev_time = time.time()\n\n        # Print log\n        sys.stdout.write(\n            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, pixel: %f, adv: %f] ETA: %s\"\n            % (\n                epoch,\n                opt.n_epochs,\n                i,\n                len(dataloader),\n                loss_D.item(),\n                loss_G.item(),\n                loss_pixel.item(),\n                loss_GAN.item(),\n                time_left,\n            )\n        )\n\n        # If at sample interval save image\n        if batches_done % opt.sample_interval == 0:\n            sample_images(batches_done)\n\n    if opt.checkpoint_interval != -1 and epoch % opt.checkpoint_interval == 0:\n        # Save model checkpoints\n        torch.save(generator.state_dict(), \"saved_models/%s/generator_%d.pth\" % (opt.dataset_name, epoch))\n        torch.save(discriminator.state_dict(), \"saved_models/%s/discriminator_%d.pth\" % (opt.dataset_name, epoch))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_images(batches_done)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Configure dataloaders\ntransforms_ = [\n    transforms.Resize((opt.img_height, opt.img_width), Image.BICUBIC),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n]\n\ndataloader = DataLoader(\n    ImageDataset_edge(\"/kaggle/input/global-wheat-detection/\", transforms_=transforms_),\n    batch_size=opt.batch_size,\n    shuffle=True,\n    num_workers=opt.n_cpu,\n)\n\nval_dataloader = DataLoader(\n    ImageDataset_edge(\"/kaggle/input/global-wheat-detection/\", transforms_=transforms_, mode=\"test\"),\n    batch_size=10,\n    shuffle=True,\n    num_workers=1,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prev_time = time.time()\n\nfor epoch in range(opt.epoch, opt.n_epochs):\n    for i, batch in enumerate(dataloader):\n\n        # Model inputs\n        real_A = Variable(batch[\"B\"].type(Tensor))\n        real_B = Variable(batch[\"A\"].type(Tensor))\n\n        # Adversarial ground truths\n        valid = Variable(Tensor(np.ones((real_A.size(0), *patch))), requires_grad=False)\n        fake = Variable(Tensor(np.zeros((real_A.size(0), *patch))), requires_grad=False)\n\n        # ------------------\n        #  Train Generators\n        # ------------------\n\n        optimizer_G.zero_grad()\n\n        # GAN loss\n        fake_B = generator(real_A)\n        pred_fake = discriminator(fake_B, real_A)\n        loss_GAN = criterion_GAN(pred_fake, valid)\n        # Pixel-wise loss\n        loss_pixel = criterion_pixelwise(fake_B, real_B)\n\n        # Total loss\n        loss_G = loss_GAN + lambda_pixel * loss_pixel\n\n        loss_G.backward()\n\n        optimizer_G.step()\n\n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n\n        optimizer_D.zero_grad()\n\n        # Real loss\n        pred_real = discriminator(real_B, real_A)\n        loss_real = criterion_GAN(pred_real, valid)\n\n        # Fake loss\n        pred_fake = discriminator(fake_B.detach(), real_A)\n        loss_fake = criterion_GAN(pred_fake, fake)\n\n        # Total loss\n        loss_D = 0.5 * (loss_real + loss_fake)\n\n        loss_D.backward()\n        optimizer_D.step()\n\n        # --------------\n        #  Log Progress\n        # --------------\n\n        # Determine approximate time left\n        batches_done = epoch * len(dataloader) + i\n        batches_left = opt.n_epochs * len(dataloader) - batches_done\n        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n        prev_time = time.time()\n\n        # Print log\n        sys.stdout.write(\n            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, pixel: %f, adv: %f] ETA: %s\"\n            % (\n                epoch,\n                opt.n_epochs,\n                i,\n                len(dataloader),\n                loss_D.item(),\n                loss_G.item(),\n                loss_pixel.item(),\n                loss_GAN.item(),\n                time_left,\n            )\n        )\n\n        # If at sample interval save image\n        if batches_done % opt.sample_interval == 0:\n            sample_images(batches_done)\n\n    if opt.checkpoint_interval != -1 and epoch % opt.checkpoint_interval == 0:\n        # Save model checkpoints\n        torch.save(generator.state_dict(), \"saved_models/%s/generator_%d.pth\" % (opt.dataset_name, epoch))\n        torch.save(discriminator.state_dict(), \"saved_models/%s/discriminator_%d.pth\" % (opt.dataset_name, epoch))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_images(batches_done)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf /kaggle/working/*","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# To be continued ...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}