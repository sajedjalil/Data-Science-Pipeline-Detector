{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is a framework that I have been working on. It commits, so it works to an extent; however, it receives a submission scoring error. The framework runs multiple models, averages predictions depending on overlap, and runs a CNN on the final outcome. If you use, please cite or ask to team up.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Import Statements","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cv2\nimport os\nimport re\nfrom PIL import Image\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport torch\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\nfrom matplotlib import pyplot as plt\nimport os\nimport sys\nsys.path.insert(0, \"../input/weightedboxesfusion\")\nfrom ensemble_boxes import *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# File Locations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"DIR_INPUT = '/kaggle/input/global-wheat-detection'\nDIR_TRAIN = f'{DIR_INPUT}/train'\nDIR_TEST = f'{DIR_INPUT}/test'\nDIR_WEIGHTS = '/kaggle/input/first-group-of-models/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Apply Test Function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def applytest(model,epoch_number):\n    DIR_INPUT = '/kaggle/input/global-wheat-detection'\n    DIR_TEST = f'{DIR_INPUT}/test'\n    \n    test_df = os.listdir(DIR_TEST)\n    \n    for i,img in enumerate(test_df):\n        test_df[i]=test_df[i][:-4]\n    \n    class WheatTestDataset(Dataset):\n    \n        def __init__(self, dataframe, image_dir, transforms=None):\n            super().__init__()\n    \n            self.image_ids = dataframe\n            self.df = dataframe\n            self.image_dir = image_dir\n            self.transforms = transforms\n    \n        def __getitem__(self, index: int):\n    \n            \n            DIR_INPUT = '/kaggle/input/global-wheat-detection'\n            DIR_TEST = f'{DIR_INPUT}/test'\n            image_id = os.listdir(DIR_TEST)\n            image_id = image_id[index]\n            #image_id = self.image_ids[index]\n            #records = self.df[self.df['image_id'] == image_id]\n    \n            image = cv2.imread(DIR_TEST+'/'+image_id, cv2.IMREAD_COLOR)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n            image /= 255.0\n    \n            if self.transforms:\n                sample = {\n                    'image': image,\n                }\n                sample = self.transforms(**sample)\n                image = sample['image']\n    \n            return image, image_id\n    \n        def __len__(self) -> int:\n            return len(self.image_ids)\n        \n    # Albumentations\n    def get_test_transform():\n        return A.Compose([\n            # A.Resize(512, 512),\n            ToTensorV2(p=1.0)\n        ])\n    \n    \n    \n    x = model.to(device)\n    \n    def collate_fn(batch):\n        return tuple(zip(*batch))\n    \n    test_dataset = WheatTestDataset(test_df, DIR_TEST, get_test_transform())\n    \n    test_data_loader = DataLoader(\n        test_dataset,\n        batch_size=1,\n        shuffle=False,\n        num_workers=0,\n        drop_last=False,\n        collate_fn=collate_fn\n    )\n    \n    def format_prediction_string(boxes, scores):\n        pred_strings = []\n        for j in zip(scores, boxes):\n            pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n    \n        return \" \".join(pred_strings)\n    \n    detection_threshold = 0.5\n    results = []\n    \n    allboxes = []\n    allimage_ids = []\n    allscores = []\n    for images, image_ids in test_data_loader:\n        #Batch Size = 1, so only one image per iteration in this loop\n        print(image_ids)\n        images = list(image.to(device) for image in images)\n        outputs = model(images)\n        \n            \n        #We want the boxes, how to store the boxes?\n        #boxes is just a list of boxes for each images, so this is just a list of one\n        sample = images[0].permute(1,2,0).cpu().numpy()\n        boxes = outputs[0]['boxes'].data.cpu().numpy()\n        scores = outputs[0]['scores'].data.cpu().numpy()\n        \n        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n        scores = scores[scores >= detection_threshold]\n        allimage_ids.append(image_ids)\n        allboxes.append(boxes)\n        allscores.append(scores)\n    \n    return allboxes, allimage_ids, allscores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Apply Test to All Test Images","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"path = DIR_WEIGHTS\nmodel_dir = os.listdir(path)\n\nboxes = []\nids = []\nscores = []\nfor i,model_weights in enumerate(model_dir):\n    if i >= 0:\n        #load a model; pre-trained on COCO\n        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)\n            \n        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n            \n        num_classes = 2  # 1 class (wheat) + background\n            \n        # get number of input features for the classifier\n        in_features = model.roi_heads.box_predictor.cls_score.in_features\n            \n        # replace the pre-trained head with a new one\n        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n            \n        # Load the trained weights\n        model.load_state_dict(torch.load(path+model_weights, map_location=torch.device('cpu')))\n        model.eval()\n        tboxes, tids, tscores = applytest(model,i)\n        boxes.append(tboxes)\n        ids.append(tids)\n        scores.append(tscores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bounding Box Functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def overlapped(r1,r2):\n    overlap = 0\n    #Top Right\n    \n    if r1[0]<=r2[2] and r1[0]>=r2[0]:\n        if r1[3]>=r2[3] and r1[3]<=r2[1]:\n            overlap = 1\n    \n    #Top Left\n    if r1[2]>=r2[0] and r1[2]<=r2[2]:\n        if r1[3]<=r2[1] and r1[3]>=r2[3]:\n            overlap = 1\n    \n    #Bottom Right\n    if r2[2]<=r1[2] and r2[2]>=r1[0]:\n        if r2[3]<=r1[1] and r2[3]>=r1[3]:\n            overlap = 1\n    \n    #Bottom Left\n    if r2[0]<=r1[2] and r2[0]>=r1[0]:\n        if r2[3] <= r1[1] and r2[1] >= r1[3]:\n            overlap = 1\n    \n    #Other\n    if r1[0]>=r2[0] and r1[0]<=r2[2]:\n        if r2[1]<=r1[1] and r2[1]>=r1[3]:\n            overlap = 1\n            \n    #Other\n    if r1[0]>=r2[0] and r1[0]<=r2[2]:\n        if r1[1]<=r2[1] and r1[1]>=r2[3]:\n            overlap = 1\n    \n    return overlap\n\ndef rec_overlap(r1,r2):\n    p1x = max(r1[0],r2[0])\n    p1y = min(r1[1],r2[1])\n    p2x = min(r1[2],r2[2])\n    p2y = max(r1[3],r2[3])\n    \n    r3 = list([p1x,p1y,p2x,p2y])\n    overlap = abs(p1x-p2x)*abs(p1y-p2y)\n    \n    area = abs(r1[0]-r1[2])*abs(r1[1]-r1[3])\n    poverlap = overlap/area\n    \n    return poverlap, r3\n\n\ndef groupboxes(boxes):\n    #1040 = size of image\n    boxes = np.asarray([boxes[:,0], 1040 - boxes[:,1], boxes[:,2], 1040 - boxes[:,3]])\n    boxes = boxes.transpose()\n    n = boxes.shape[0]\n    areas = np.zeros([n,n])\n\n    for i in range(0,n):\n        control = 1\n        for j in range(0,n):\n            r0 = list([boxes[i,0],boxes[i,1],boxes[i,2],boxes[i,3]])\n            r1 = list([boxes[j,0],boxes[j,1],boxes[j,2],boxes[j,3]])\n            [p,r2] = rec_overlap(r0,r1)\n    \n            overlap = overlapped(r0,r1)\n            \n            if p <= 1 and overlap == 1 and p != 0:\n                if i >= 0:\n                    areas[i,j]=p\n\n    group = np.zeros([n,1])\n    group_count = 1\n\n    for i in range(0,n):\n        for j in range(0,n):\n            if areas[i,j] > .5:\n                if areas [j,i] > .5:\n                    if group[i] == 0:\n                        if group[j] == 0:\n                            group[i] = group_count\n                            group[j] = group_count\n                            group_count = group_count + 1\n                            continue\n                        \n                        group[i] = group[j]\n                        continue\n                    \n                    if group[j] == 0:\n                        group[j] = group[i]\n                        continue\n                    \n                    group[group==group[j]]=group[i]\n    \n    return areas,group\n                \ndef average_boxes(boxes,group,scores):\n    u = np.unique(group)\n    ab = []\n    ts = []\n    for i in range(0,u.shape[0]):\n        tgroup,temp = np.where(group==u[i])\n        gboxes = boxes[tgroup.astype(int),:]\n        tscores = scores[tgroup.astype(int)]\n        tscores = np.mean(tscores)\n        \n        p1 = np.mean(gboxes[:,0])\n        p2 = np.mean(gboxes[:,1])\n        p3 = np.mean(gboxes[:,2])\n        p4 = np.mean(gboxes[:,3])\n        p4 = p4-p2\n        p3 = p3-p1\n        \n        average_box = list([p1,p2,p3,p4])\n        ab.append(average_box)\n        ts.append(tscores)\n    \n    ab = np.asarray(ab)\n    ts = np.asarray(ts)\n    return ab,ts\n\ndef run_wbf(boxes,scores,labels,image_size=1024, iou_thr=0.55, skip_box_thr=0.7, weights=None):\n#     boxes =boxes/(image_size-1)\n#    labels0 = [np.ones(len(scores[idx])) for idx in range(scores.shape[0])]\n    labels0 = labels\n    boxes, scores, labelst = weighted_boxes_fusion(boxes, scores, labels0, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n#     boxes = boxes*(image_size-1)\n#     boxes = boxes\n    return boxes, scores, labelst","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Formating Prediction Strings","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_prediction_string(boxes, scores):\n        pred_strings = []\n        for j in zip(scores, boxes):\n            pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n    \n        return \" \".join(pred_strings)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Apply CNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from random import random\nfrom tensorflow.keras.models import load_model\nsaved_model = load_model(\"../input/wheatcnn/mymodel_30k.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pylab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def applypadding(img):\n    h = img.shape[0]\n    w = img.shape[1]\n    x = round(random()*(224 - h))\n    y = round(random()*(224 - w))\n    \n    imgtemp = np.zeros((224,224,3))\n    try:\n        imgtemp[x:x+h,y:y+w,:] = img\n    except:\n        try:\n            imgtemp[0:224,y:y+w,:] = img[0:224,:,:]\n        except:\n            try:\n                imgtemp[x:x+h,0:224,:] = img[:,0:224,:]\n            except:\n                imgtemp[0:224,0:224,:] = img[0:224,0:224,:]\n    return imgtemp\n\ndef applycnn(img_sample,model,bb,scores):\n    imgs = []\n    print(bb.shape)\n    count = 0\n    for box in bb:\n        count += 1\n        temp_img = img_sample[box[1]:box[1]+box[3],box[0]:box[0]+box[2],:]\n        temp_img = applypadding(temp_img)\n        imgs.append(temp_img)\n    \n    pylab.imshow(temp_img)\n    temp_imgs = imgs\n    imgs = np.asarray(imgs)\n    outputs = saved_model.predict(imgs)\n    \n    #for i,output in enumerate(outputs):\n        #t = Image.fromarray(temp_imgs[i].astype(np.uint8))\n        #t.save('/kaggle/working/'+str(output)+'.png')\n    return outputs\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Averaging Bounding Boxes/Scores","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimgs = os.listdir(DIR_TEST)\nresults = []\nab_s = []\nts_s = []\nab_nocnn = []\nts_nocnn = []\nfor i,img in enumerate(imgs):\n    print(i)\n    tboxes = []\n    tscores = []\n    \n    #j = number of models,i = image\n    for j in range(0,len(boxes)):\n        tboxes.append(boxes[j][i])\n        tscores.append(scores[j][i])\n    \n    #Concatenates list of boxes and scores extracted from multiple models\n    bboxes = np.vstack(tboxes)\n    iscores = np.hstack(tscores)\n    labels = np.ones(iscores.shape[0])\n    \n    bboxes = [bboxes.tolist()]\n    iscores = [iscores.tolist()]\n    labels = [labels.tolist()]\n    \n    ab,ts,labels = run_wbf(bboxes,iscores,labels)\n    #areas,group = groupboxes(bboxes)\n    #ab,ts = average_boxes(bboxes,group,iscores)\n    \n    ab = ab.astype(int)\n    \n    for j in range(0,ab.shape[0]):\n        ab[j][2] = ab[j][2]-ab[j][0]\n        ab[j][3] = ab[j][3]-ab[j][1]\n    \n    ab_nocnn.append(ab)\n    ts_nocnn.append(ts)\n    check = 1\n    if check == 1:\n        try:\n            sample = cv2.imread(DIR_TEST+'/'+img)\n            pylab.imshow(sample)\n            #Next line was not shadowed last time\n            outputs = applycnn(sample,saved_model,ab,ts)\n            count = 0\n            for i,output in enumerate(outputs):\n                if output < .2:\n                    ab = np.delete(ab,count,axis=0)\n                    ts = np.delete(ts,count,axis=0)\n                else:\n                    count = count + 1\n        except:\n            print('Did not work')\n\n    \n    ab_s.append(ab)\n    ts_s.append(ts)\n    \n    result = {\n        'image_id': img[:-4],\n        'PredictionString': format_prediction_string(ab,ts)\n    }\n    results.append(result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test Images","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,9):\n    print(\"-------------CNN---------------\")\n    \n    sample = cv2.imread(DIR_TEST+'/'+imgs[9-i], cv2.IMREAD_COLOR)\n    sample = cv2.cvtColor(sample, cv2.COLOR_BGR2RGB)\n    boxes = ab_s[9-i]\n    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n    for box in boxes:\n        cv2.rectangle(sample,\n                      (box[0], box[1]),\n                      (box[0]+box[2], box[1]+box[3]),\n                      (220, 0, 0), 2)\n\n    ax.set_axis_off()\n    ax.imshow(sample)\n\n    print(\"-------------No CNN---------------\")\n    \n    sample = cv2.imread(DIR_TEST+'/'+imgs[9-i], cv2.IMREAD_COLOR)\n    sample = cv2.cvtColor(sample, cv2.COLOR_BGR2RGB)\n    boxes = ab_nocnn[9-i]\n    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n    for box in boxes:\n        cv2.rectangle(sample,\n                      (box[0], box[1]),\n                      (box[0]+box[2], box[1]+box[3]),\n                      (220, 0, 0), 2)\n\n    ax.set_axis_off()\n    ax.imshow(sample)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submit","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.DataFrame(results, columns = ['image_id', 'PredictionString'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}