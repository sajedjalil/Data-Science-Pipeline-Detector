{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Really good training pipeline for pytorch EfficientDet \n\nHi everyone!\n\nMy name is Alex Shonenkov, I am DL/NLP/CV/TS research engineer. Especially I am in Love with NLP & DL.\n\nRecently I have created kernel for this competition about Weighted Boxes Fusion:\n- [WBF approach for ensemble](https://www.kaggle.com/shonenkov/wbf-approach-for-ensemble)\n\n\nI hope it is useful for you, my friends! If you didn't read this kernel, don't forget to do it! :)\n\n\nToday I would like to share really good training pipeline for this competition using SOTA [EfficientDet: Scalable and Efficient Object Detection](https://arxiv.org/pdf/1911.09070.pdf)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Main Idea\n \nI read [all public kernels about EfficientDet in kaggle community](https://www.kaggle.com/search?q=efficientdet+in%3Anotebooks) and understand that kaggle don't have really good working public kernels with good score. Why? You can see below picture about COCO AP for different architectures, I think everyone should be able to use such a strong tools EfficientDet for own research, lets do it!\n\n<img src='https://miro.medium.com/max/2400/0*ApAKUWtseHcvRV2U.png' width=400>   \n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Dependencies and imports","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install gluoncv mxnet pycocotools\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!git clone https://github.com/rwightman/efficientdet-pytorch.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install albumentations===0.4.5 timm>=0.1.22 omegaconf>=2.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import sys\nsys.path.insert(0, \"efficientdet-pytorch\")\n\nimport json\nimport zipfile\nimport gluoncv as gcv\nimport torch\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom glob import glob\n\nSEED = 42\nAREA_SMALL = 56 * 56\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"marking = pd.read_csv('../input/global-wheat-detection/train.csv')\nmarking.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"marking = pd.read_csv('../input/global-wheat-detection/train.csv')\nbboxs = np.stack(marking['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\nfor i, column in enumerate(['x', 'y', 'w', 'h']):\n    marking[column] = bboxs[:,i]\n\nmarking['area'] = marking.w * marking.h\nmarking['class'] = 1\nmarking['size'] = (marking.area > AREA_SMALL).astype(int)\nmarking['source_path'] = '../input/global-wheat-detection/train'\nmarking.drop(columns=['bbox'], inplace=True)\nmarking.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"About data splitting you can read [here](https://www.kaggle.com/shonenkov/wbf-approach-for-ensemble):","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":" assert (marking[['image_id', 'source']].groupby('image_id')['source'].nunique() == 1).all() # only one distinct source by image_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"marking.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\ndf_folds = marking[['image_id']].copy()\ndf_folds.loc[:, 'bbox_count'] = 1\ndf_folds = df_folds.groupby('image_id').count()\ndf_folds.loc[:, 'source'] = marking[['image_id', 'source', 'class']].groupby('image_id').min()['source']\ndf_folds.loc[:, 'class'] = marking[['image_id', 'class']].groupby('image_id').min()['class']\ndf_folds.loc[:, 'stratify_group'] = np.char.add(df_folds['class'].apply(lambda x: f'{x}_').values.astype(str), np.char.add(\n    df_folds['source'].values.astype(str),\n    df_folds['bbox_count'].apply(lambda x: f'_{x // 15}').values.astype(str)\n))\ndf_folds.loc[:, 'fold'] = 0\n\nfor fold_number, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds['stratify_group'])):\n    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_folds['stratify_group'].nunique(), marking['image_id'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_folds.groupby('fold')['bbox_count'].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Albumentations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_SIZE = 1024\n\ndef get_train_transforms():\n    return A.Compose(\n        [A.ShiftScaleRotate(scale_limit=(-0.5, 0.5), rotate_limit=0, shift_limit=0., p=0.5, border_mode=0),\n            A.RandomRotate90(p=0.5),\n            A.Resize(IMG_SIZE, IMG_SIZE, p=1),\n            A.VerticalFlip(p=0.5),\n            A.HorizontalFlip(p=0.5),\n            A.OneOf([\n                    A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n                                         val_shift_limit=0.2, p=0.9),\n                    A.RandomBrightnessContrast(brightness_limit=0.2, \n                                               contrast_limit=0.2, p=0.9),\n                    A.RandomGamma(p=0.9),\n            ],p=0.25),\n            A.OneOf([\n                A.IAASharpen(alpha=(0.1, 0.3), p=0.5),\n                A.CLAHE(p=0.8),\n                #A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n                #A.GaussianBlur(blur_limit=3, p=0.5),\n                A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=0.3),\n            ], p=0.0),\n            ToTensorV2(p=1.0),\n        ], \n        p=1.0, \n        bbox_params=A.BboxParams(\n            format='pascal_voc',\n            min_area=0, \n            min_visibility=0,\n            label_fields=['labels']\n        )\n    )\n\n\ndef get_valid_transforms():\n    return A.Compose(\n        [\n            A.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0),\n            ToTensorV2(p=1.0),\n        ], \n        p=1.0, \n        bbox_params=A.BboxParams(\n            format='pascal_voc',\n            min_area=0, \n            min_visibility=0,\n            label_fields=['labels']\n        )\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGENET_DEFAULT_MEAN = [x * 255 for x in (0.485, 0.456, 0.406)]\nIMAGENET_DEFAULT_STD = [x * 255 for x in (0.229, 0.224, 0.225)]\n\n\nclass DatasetRetriever(Dataset):\n\n    def __init__(self, marking, image_ids, transforms=None, test=False):\n        super().__init__()\n\n        self.image_ids = image_ids\n        self.marking = marking\n        self.transforms = transforms\n        self.test = test\n\n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        \n        r = random.random()\n        if self.test or r < 0.80:\n            image, boxes, labels = self.load_image_and_boxes(index)\n        elif r < 0.90:\n            image, boxes, labels = self.load_cutmix_image_and_boxes(index)\n        else: \n            image, boxes, labels = self.load_mixup_iamge_and_boxes(index)\n\n\n        assert len(boxes) == len(labels)\n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = torch.tensor(labels.astype(np.uint8))\n        target['image_id'] = torch.tensor([index])\n        target['img_size'] = torch.tensor([(IMG_SIZE, IMG_SIZE)])\n        target['img_scale'] = torch.tensor([1.])\n        image = image.astype(np.uint8)\n\n        if self.transforms:\n            for i in range(10):\n                sample = self.transforms(**{\n                    'image': image,\n                    'bboxes': target['boxes'],\n                    'labels': labels\n                })\n                \n                if len(sample['bboxes']) > 0:\n                    image = sample['image']\n                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  #yxyx: be warning\n                    target['labels'] = torch.tensor(sample['labels'])\n                    break\n        \n        assert len(target['boxes']) == len(target['labels'])\n        image = image.float() / 255.0\n\n        return image, target, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n\n    def load_image_and_boxes(self, index):\n        image_id = self.image_ids[index]\n        records = self.marking[self.marking['image_id'] == image_id]\n        source_path = records['source_path'].iloc[0]\n        image = cv2.imread(f'{source_path}/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.uint8)\n        #image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0,0) , 10), -4 ,128)\n        #image = (image - IMAGENET_DEFAULT_MEAN) / IMAGENET_DEFAULT_STD\n        boxes = records[['x', 'y', 'w', 'h']].values\n        labels = records['class'].values\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n        return image, boxes, labels\n\n    def load_mixup_iamge_and_boxes(self, index):\n        image, boxes, labels = self.load_image_and_boxes(index)\n        r_image, r_boxes, r_labels = self.load_image_and_boxes(random.randint(0, self.image_ids.shape[0] - 1))\n        mixup_image = (image + r_image) / 2\n        mixup_boxes = np.concatenate([boxes, r_boxes], axis=0)\n        mixup_labels = np.concatenate([labels, r_labels], axis=0)\n        return mixup_image, mixup_boxes, mixup_labels\n        \n    \n    def load_cutmix_image_and_boxes(self, index, imsize=1024):\n        \"\"\" \n        This implementation of cutmix author:  https://www.kaggle.com/nvnnghia \n        Refactoring and adaptation: https://www.kaggle.com/shonenkov\n        \"\"\"\n        w, h = imsize, imsize\n        s = imsize // 2\n    \n        xc, yc = [int(random.uniform(imsize * 0.25, imsize * 0.75)) for _ in range(2)]  # center x, y\n        indexes = [index] + [random.randint(0, self.image_ids.shape[0] - 1) for _ in range(3)]\n\n        result_image = np.full((imsize, imsize, 3), 1, dtype=np.float32)\n        result_boxes = []\n        result_labels = []\n        \n        for i, index in enumerate(indexes):\n            image, boxes, labels = self.load_image_and_boxes(index)\n            if i == 0:\n                x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n                x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n            elif i == 1:  # top right\n                x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n                x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n            elif i == 2:  # bottom left\n                x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n                x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, max(xc, w), min(y2a - y1a, h)\n            elif i == 3:  # bottom right\n                x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n                x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n            result_image[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n            padw = x1a - x1b\n            padh = y1a - y1b\n\n            boxes[:, 0] += padw\n            boxes[:, 1] += padh\n            boxes[:, 2] += padw\n            boxes[:, 3] += padh\n            \n\n            result_boxes.append(boxes)\n            result_labels.append(labels)\n\n        result_boxes = np.concatenate(result_boxes, 0)\n        result_labels = np.concatenate(result_labels, 0)\n        np.clip(result_boxes[:, 0:], 0, 2 * s, out=result_boxes[:, 0:])\n        result_boxes = result_boxes.astype(np.int32)\n        condition = np.where((result_boxes[:,2]-result_boxes[:,0])*(result_boxes[:,3]-result_boxes[:,1]) > 0)\n        result_boxes = result_boxes[condition]\n        result_labels = result_labels[condition]\n        return result_image, result_boxes, result_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fold_number = 1\n\ntrain_dataset = DatasetRetriever(\n    image_ids=df_folds[df_folds['fold'] != fold_number].index.values,\n    marking=marking,\n    transforms=get_train_transforms(),\n    test=False,\n)\n\nvalidation_dataset = DatasetRetriever(\n    image_ids=df_folds[df_folds['fold'] == fold_number].index.values,\n    marking=marking,\n    transforms=get_valid_transforms(),\n    test=True,\n)\n\nimage, target, image_id = train_dataset[9]\nboxes = target['boxes'].cpu().numpy().astype(np.int32)\nnumpy_image = image.permute(1,2,0).cpu().numpy()\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\nfor box in boxes:\n    cv2.rectangle(numpy_image, (box[1], box[0]), (box[3],  box[2]), (0, 1, 0), 2)\n    \n\nax.set_axis_off()\nax.imshow(numpy_image)\n\n\nprint(target['labels'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fitter","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nclass Fitter:\n    \n    def __init__(self, model, device, config):\n        self.config = config\n        self.epoch = 0\n\n        self.base_dir = f'./{config.folder}'\n        if not os.path.exists(self.base_dir):\n            os.makedirs(self.base_dir)\n        \n        self.log_path = f'{self.base_dir}/log.txt'\n        self.best_summary_loss = 10**5\n        self.best_summary_score = 0.0\n\n        self.model = model\n        self.device = device\n\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ] \n\n        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n        self.log(f'Fitter prepared. Device is {self.device}')\n\n    def fit(self, train_loader, validation_loader):\n        for e in range(self.config.n_epochs):\n            if self.config.verbose:\n                lr = self.optimizer.param_groups[0]['lr']\n                timestamp = datetime.utcnow().isoformat()\n                self.log(f'\\n{timestamp}\\nLR: {lr}')\n\n            t = time.time()\n            summary_loss = self.train_one_epoch(train_loader)\n\n            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n                        \n            self.save(f'{self.base_dir}/last-checkpoint.bin')\n\n            t = time.time()\n            summary_loss, summary_score = self.validation(validation_loader)\n            self.log(f'[RESULT]: Validation. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, summary_score: {summary_score.avg:.5f}, time: {(time.time() - t):.5f}')\n            \n            if summary_score.avg > self.best_summary_score:\n                self.best_summary_score = summary_score.avg\n                self.model.eval()\n                self.save(f'{self.base_dir}/best-checkpoint.bin')\n                for path in sorted(glob(f'{self.base_dir}/best-checkpoint.bin'))[:-3]:\n                    os.remove(path)\n\n            if self.config.validation_scheduler:\n                self.scheduler.step(metrics=summary_score.avg)\n\n            self.epoch += 1\n\n    def _adapt_target(targets2):\n        targets2 = {}\n        targets2['bbox'] = [target['boxes'].to(self.device).float() for target in targets] # variable number of instances, so the entire structure can be forced to tensor\n        targets2['cls'] = [target['labels'].to(self.device).float() for target in targets]\n        targets2['image_id'] = torch.tensor([target['image_id'] for target in targets]).to(self.device).float()\n        targets2['img_scale'] = torch.tensor([target['img_scale'] for target in targets]).to(self.device).float()\n        targets2['img_size'] = torch.tensor([(IMG_SIZE, IMG_SIZE) for target in targets]).to(self.device).float()\n        return targets2\n\n    def validation(self, val_loader):\n        self.model.eval()\n        summary_loss = AverageMeter()\n        summary_score = AverageMeter()\n        t = time.time()\n        metrics = [gcv.utils.metrics.VOCMApMetric(iou_thresh=iou) for iou in [0.5, 0.55, 0.6, 0.65, 0.7, 0.75]]\n        \n        for step, (images, targets, image_ids) in enumerate(val_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Val Step {step}/{len(val_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}, ' + \\\n                        f'score: {summary_score.avg:.5f} ', end='\\r'\n                    )\n            with torch.no_grad():\n                images = torch.stack(images)\n                batch_size = images.shape[0]\n                images = images.to(self.device).float()\n                targets2 = {}\n                targets2['bbox'] = [target['boxes'].to(self.device).float() for target in targets] # variable number of instances, so the entire structure can be forced to tensor\n                targets2['cls'] = [target['labels'].to(self.device).float() for target in targets]\n                targets2['image_id'] = torch.tensor([target['image_id'] for target in targets]).to(self.device).float()\n                targets2['img_scale'] = torch.tensor([target['img_scale'] for target in targets]).to(self.device).float()\n                targets2['img_size'] = torch.tensor([(IMG_SIZE, IMG_SIZE) for target in targets]).to(self.device).float()\n                output = self.model(images, targets2)\n                loss = output['loss']\n                det = output['detections']\n                summary_loss.update(loss.detach().item(), batch_size)\n\n                #update(pred_bboxes, pred_labels, pred_scores, gt_bboxes, gt_labels, gt_difficults=None)\n               \n                for i in range(0, len(det)):\n                  pred_scores=det[i, :, 4].cpu().unsqueeze_(0).numpy()\n                  condition=(pred_scores > 0.25)[0]\n                  gt_boxes=targets2['bbox'][i].cpu().unsqueeze_(0).numpy()[...,[1,0,3,2]] #move to PASCAL VOC from yxyx format\n                  gt_labels=targets2['cls'][i].cpu().unsqueeze_(0).numpy()\n                  pred_bboxes=det[i, :, 0:4].cpu().unsqueeze_(0).numpy()[:, condition, :] # move from (x,y,w,h) to (x,y,x,y)\n                  pred_bboxes[..., 2] = pred_bboxes[..., 0] + pred_bboxes[..., 2]\n                  pred_bboxes[..., 3] = pred_bboxes[..., 1] + pred_bboxes[..., 3]\n                  pred_labels=det[i, :, 5].cpu().unsqueeze_(0).numpy()[:, condition]\n                  pred_scores=pred_scores[:, condition]\n                  \n                  for metric in metrics:\n                    metric.update(\n                      pred_bboxes=pred_bboxes,\n                      pred_labels=pred_labels,\n                      pred_scores=pred_scores,\n                      gt_bboxes=gt_boxes,\n                      gt_labels=gt_labels)\n\n                summary_score.update(np.mean([metric.get()[1] for metric in metrics]), batch_size)\n\n        return summary_loss, summary_score\n\n    def train_one_epoch(self, train_loader):\n        self.model.train()\n        summary_loss = AverageMeter()\n        t = time.time()\n        for step, (images, targets, image_ids) in enumerate(train_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Train Step {step}/{len(train_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n            \n            images = torch.stack(images)\n            images = images.to(self.device).float()\n            batch_size = images.shape[0]\n            targets2 = {}\n            targets2['bbox'] = [target['boxes'].to(self.device).float() for target in targets] # variable number of instances, so the entire structure can be forced to tensor\n            targets2['cls'] = [target['labels'].to(self.device).float() for target in targets]\n            targets2['image_id'] = torch.tensor([target['image_id'] for target in targets]).to(self.device).float()\n            targets2['img_scale'] = torch.tensor([target['img_scale'] for target in targets]).to(self.device).float()\n            targets2['img_size'] = torch.tensor([(IMG_SIZE, IMG_SIZE) for target in targets]).to(self.device).float()\n            \n            output = self.model(images, targets2)\n            loss = output['loss'] / self.config.grad_accumulation_steps\n            loss.backward()\n\n            summary_loss.update(loss.detach().item(), batch_size)\n\n            # Gradient accumulation\n            if (step + 1) % self.config.grad_accumulation_steps == 0:\n                self.optimizer.step()\n                self.optimizer.zero_grad()\n                if self.config.step_scheduler:\n                    self.scheduler.step()\n\n        return summary_loss\n    \n    def save(self, path):\n        self.model.eval()\n        torch.save({\n            'model_state_dict': self.model.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'best_summary_loss': self.best_summary_loss,\n            'best_summary_score': self.summary_score,\n            'epoch': self.epoch\n        }, path)\n\n    def load(self, path):\n        checkpoint = torch.load(path)\n        self.model.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        self.best_summary_loss = checkpoint['best_summary_loss']\n        self.best_summary_score = checkpoint['best_summary_score']\n        self.epoch = checkpoint['epoch'] + 1\n        \n    def log(self, message):\n        if self.config.verbose:\n            print(message)\n        with open(self.log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainGlobalConfig:\n    num_workers = 2\n    batch_size = 1\n    n_epochs = 75 # n_epochs = 40\n    lr = 0.0002\n    grad_accumulation_steps = 8\n\n    folder = f'effdet5-cutmix-augmix-1024-fold{fold_number}'\n\n    # -------------------\n    verbose = True\n    verbose_step = 50\n    # -------------------\n\n    # --------------------\n    step_scheduler = False  # do scheduler.step after optimizer.step\n    validation_scheduler = True  # do scheduler.step after validation stage loss\n\n#     SchedulerClass = torch.optim.lr_scheduler.OneCycleLR\n#     scheduler_params = dict(\n#         max_lr=0.001,\n#         epochs=n_epochs,\n#         steps_per_epoch=int(len(train_dataset) / batch_size),\n#         pct_start=0.1,\n#         anneal_strategy='cos', \n#         final_div_factor=10**5\n#     )\n    \n    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n    scheduler_params = dict(\n        mode='max',\n        factor=0.5,\n        patience=2,\n        verbose=False, \n        threshold=0.0001,\n        threshold_mode='abs',\n        cooldown=0, \n        min_lr=1e-8,\n        eps=1e-08\n    )\n    # --------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\n\ndef run_training(train_dataset, valid_dataset, path=''):\n    device = torch.device('cuda:0')\n    net.to(device)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TrainGlobalConfig.batch_size,\n        sampler=RandomSampler(train_dataset),\n        pin_memory=False,\n        drop_last=True,\n        num_workers=TrainGlobalConfig.num_workers,\n        collate_fn=collate_fn,\n    )\n    val_loader = torch.utils.data.DataLoader(\n        valid_dataset, \n        batch_size=TrainGlobalConfig.batch_size,\n        num_workers=TrainGlobalConfig.num_workers,\n        shuffle=False,\n        sampler=SequentialSampler(validation_dataset),\n        pin_memory=False,\n        collate_fn=collate_fn,\n    )\n\n    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n    if path:\n        fitter.load(path=path)\n    fitter.fit(train_loader, val_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\nfrom effdet.efficientdet import HeadNet\n\ndef get_net(config='', checkpoint='', n_classes=1):\n    config = get_efficientdet_config(config)\n    net = EfficientDet(config, pretrained_backbone=False)\n    config.num_classes = n_classes\n    config.image_size = IMG_SIZE\n    head = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n    if checkpoint:\n      metadata = torch.load(checkpoint)\n      if os.path.splitext(checkpoint)[1] == '.pth':\n        net.load_state_dict(metadata)\n        net.class_net = head\n      else:\n        net.class_net = head\n        net.load_state_dict(metadata['model_state_dict'])\n\n    return DetBenchTrain(net, config)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net = get_net('tf_efficientdet_d5', '../input/efficientdet/efficientdet_d5-ef44aea8.pth', 1)\nrun_training(train_dataset, validation_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Re-start training with this command\n\ncheckpoint_path = 'CHANGE ME' # set to a .bin path\n#net = get_net('tf_efficientdet_d5', '', 1)\n#run_training(train_dataset, validation_dataset, path='')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Thank you for reading my kernel!\n\nSo, I have prepared good training SOTA-model baseline for you, my friends! I have used n_epochs = 40 and have got best checkpoint single model that gives 0.7176 LB. You can see [here](https://www.kaggle.com/shonenkov/inference-efficientdet) inference kernel.\n\nJust recently I have started publishing my works, if you like this format of notebooks I would like continue to make kernels.\n\n@weimin Surely! I have got best for fold0:\n\n    25 epoch, summary_loss: 0.35816 (this kernel)\n    72 epoch, summary_loss: 0.35194 (using mixup)\n    \n    \nDetail:\n\n\n\nFitter prepared. Device is cuda:0\n\n2020-05-26T18:26:03.260417\nLR: 0.0002\n[RESULT]: Train. Epoch: 0, summary_loss: 1.82308, time: 598.64807\n[RESULT]: Val. Epoch: 0, summary_loss: 0.47286, time: 44.89923\n\n2020-05-26T18:36:48.215521\nLR: 0.0002\n[RESULT]: Train. Epoch: 1, summary_loss: 0.49716, time: 610.29675\n[RESULT]: Val. Epoch: 1, summary_loss: 0.41123, time: 54.23833\n\n2020-05-26T18:47:54.432743\nLR: 0.0002\n[RESULT]: Train. Epoch: 2, summary_loss: 0.46978, time: 642.39121\n[RESULT]: Val. Epoch: 2, summary_loss: 0.40317, time: 44.18681\n\n\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}