{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"ec556a2d-bf3b-4a56-3927-a48d696ca623"},"source":"**Cluster Visualization - Assets based on Structural NAN values**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c638ca75-42a5-5603-243f-406666063ac8"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\nimport matplotlib.pyplot as plt\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"93b5675c-65c5-9be8-86a5-632af2fe21ca"},"outputs":[],"source":"#load the data. It comes in train.h5 so we need to us HDFStore\ndf = pd.HDFStore(\"../input/train.h5\", \"r\").get(\"train\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4a39f361-566b-8212-fff5-69c89b8957b2"},"outputs":[],"source":"# I make vectors that hold the ratio of NAN values for a given id in a given column\nunique_ids = pd.unique(df.id)\nlen(unique_ids)\nNaN_vectors = np.zeros(shape=(len(unique_ids), df.shape[1]))\n\nfor i, i_id in enumerate(unique_ids):\n    data_sub = df[df.id ==i_id]\n    NaN_vectors[i,:] = np.sum(data_sub.isnull(),axis=0) /float(data_sub.shape[0])\n    \nNaN_vectors"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dc7d4e71-dbd5-2dc6-faee-731217bfd54e"},"outputs":[],"source":"# get all the NaN vectors in which every collumn for that ID is NaN. What we are looking for \n#is collumns in which the features fundamentally do not exist. \nbin_NaN = 1*(NaN_vectors==1)\nprint(\"Still has the shape of {} by {}\".format(bin_NaN.shape[0],bin_NaN.shape[1]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9a1ce23d-d7e3-bb93-0adb-acab37dd764c"},"outputs":[],"source":"# we now have a vector of things that are either 1 where nothing exists in teh column or zero something\n#exists Now we take a covariance over these bins to see which ones move togehter. we are looking only based on columns\nbin_cov=np.corrcoef(bin_NaN.T)\nbin_cov.shape[1]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"77c087e6-49ba-be75-1e66-1aad3a441d3b"},"outputs":[],"source":"# plot bin_cov\nplt.matshow(bin_cov)\n\n#if you think abou what this shows it is show the probability that when an entire column is missing what\n# is the probability that another column will be completely missing. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e350bc14-5d70-8a1c-1ded-e792368884e5"},"outputs":[],"source":"# In this graph i make the matrix sparse by considering only things that have perfect correlation. This\n# gives us insight into the relationship of the pairs.\nplt.matshow(bin_cov == 1) "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a3979287-a84e-706a-2341-f1af89557559"},"outputs":[],"source":"# What we are doing here is looking at all the column pairs that when they are missing  they are always\n# missing together. ie they have a corralation of 1. We also get the count. it stands to reason that\n# if this happens in only one or two id's out of 1400 then perhaps it is a statistical anomoly or could be \n# reflective of a non structural issue. This is actually very enlightening and we see there are 60\n# some odd pairs that satisfy this criteria. More importantly is that it happens for lots of tickers.\n# Maybe we have soemthing here.\nbin_NaN\nedges = []\ncount =np.dot(bin_NaN.T,bin_NaN)\nfor i in range(bin_cov.shape[0]):\n    for j in range(bin_cov.shape[0]-i):\n        if i!=i+j and bin_cov[i,i+j]==1:\n            edges.append([i,i+j,count[i,i+j]])\nprint(edges)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e037fdd2-db33-0421-8da1-b22aaca639b5"},"outputs":[],"source":"#lets see how many unique counts there are. it looks like a few of these counts happen multiple times.\n# this is interesting and could imply some structural issue.\nucount = [i[2] for i in edges]\nprint(np.unique(ucount))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a8741a6-7562-d688-5191-4716d11f14d6"},"outputs":[],"source":"print('rows: {}'.format(bin_NaN.shape[0]))\nprint('cols: {}'.format(len(edges)))\n\n# the idea here is that we create a feature vector. We look at all the ids which have all their data\n# missing in a certain collumn. above we found that if all the data is missing in a certain collumn it\n# would be missing in another collumn as well. so we look at all these pairs (shown as edges) and we \n# then create a matrix of id x edges. We then put a 0 or a 1 in the collumn to indicate that the pair of \n# data is missing or not. This serves as a feature. I will then go on to cluster over these features. \n\nnan_features = np.zeros((bin_NaN.shape[0],len(edges)))\nfor i in range(bin_NaN.shape[0]):\n    for j, edge in enumerate(edges):\n        nan_features[i,j] = 1*(bin_NaN[i,edge[0]] & bin_NaN[i,edge[1]])\n\nprint('this is just a check that indexing is correct: {}'.format(np.sum(nan_features,axis=1).shape[0]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2183b235-fadb-0ceb-7609-c8464d4ec51b"},"outputs":[],"source":"# we take a look at the silouette score as we increase the number of clusters to understand the optimal\n#number of clusters. We see here that it continues to increase which we would expect. I chose to cut it\n# off around 12\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n#Range for k\nkmin = 2\nkmax = 25\nsil_scores = []\n\n#Compute silouhette scoeres\nfor k in range(kmin,kmax):\n    km = KMeans(n_clusters=k, n_init=20).fit(nan_features)\n    sil_scores.append(silhouette_score(nan_features, km.labels_))\n\n#Plot\nplt.plot(range(kmin,kmax), sil_scores)\nplt.title('KMeans Results')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Silhouette Score')\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"283ae6e1-bf98-6176-b70c-6e80bcedd645"},"outputs":[],"source":"# DBSCAN The only thing that is important from here is the labels. \n# the way that DB scan works is that you give it eps and min_samples and it\n# finds core groups. eps is the distance cut off and min is how many elements\n# at minimum you need to define a cluster\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\n\nX = nan_features\n\ndb = DBSCAN(eps=0.3, min_samples=10).fit(X)\ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n\nprint('Estimated number of clusters: %d' % n_clusters_)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"14828a1a-6fa7-d3f3-c608-b7a0dd2ec028"},"outputs":[],"source":"#COLOR MAPPING - we run a kmeans cluster but we need to decide how many\n#clusters we want to use. This is another way for us to cluster. Here we use \nk=12\nkm = KMeans(n_clusters=k, n_init=20).fit(nan_features)\ncolors=km.labels_"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"60f8b819-9a21-b6ed-75e7-d88ddbf768bc"},"outputs":[],"source":"#now we try to visualize the data of these features.\n# WOW LOOK AT THESE RESULTS. THAT IS BEAUTIFUL!!\n\nfrom sklearn.manifold import TSNE\nfrom time import time\n\nn_iter = 5000\n\nfor i in [2, 5, 30, 50, 100]:\n    t0 = time()\n    model = TSNE(n_components=2, n_iter = n_iter,random_state=0, perplexity =i)\n    np.set_printoptions(suppress=True)\n    Y = model.fit_transform(nan_features)\n    t1 =time()\n\n    print( \"t-SNE: %.2g sec\" % (t1 -t0))\n    plt.scatter(Y[:, 0], Y[:, 1], c= colors)\n    plt.title('t-SNE with perplexity = {}'.format(i))\n    plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d0f4dab0-42a5-eec9-d45e-4bf45c89b0b4"},"outputs":[],"source":"# Now i do the same in 3d to try to better understand these clusters\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\nn_iter = 5000\n\nfor i in [2, 5, 30, 50, 100]:\n    fig = plt.figure(1, figsize=(8, 6))\n    ax = Axes3D(fig, elev=-150, azim=110)\n    t0 = time()\n    model = TSNE(n_components=3, random_state=0, perplexity=i, n_iter=n_iter)\n    np.set_printoptions(suppress=True)\n\n    Y = model.fit_transform(nan_features)\n    t1 =time()\n\n    print( \"t-SNE: %.2g sec\" % (t1 -t0))\n\n    ax.scatter(Y[:, 0], Y[:, 1], Y[:, 2],c=db.labels_,\n               cmap=plt.cm.Paired)\n    ax.set_title(\"3D T-SNE - Perplexity = {}\".format(i))\n    ax.set_xlabel(\"1st dim\")\n    ax.w_xaxis.set_ticklabels([])\n    ax.set_ylabel(\"2nd dim\")\n    ax.w_yaxis.set_ticklabels([])\n    ax.set_zlabel(\"3rd dim\")\n    ax.w_zaxis.set_ticklabels([])\n    plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ec508dc1-d7c8-6436-d795-32c2e0170c3f"},"outputs":[],"source":"# I will use PCA now to plot\nfrom sklearn import decomposition\n\n# I chose this number pretty much at random, you can change it. using 22 features to describe\n# something with 110 x variables still seems high.\nn_eigens = 22\n# Creating PCA object\npca = decomposition.PCA(n_components=n_eigens, svd_solver ='randomized', whiten=True)\nX_pca =pca.fit_transform(nan_features)\nX_pca"},{"cell_type":"markdown","metadata":{"_cell_guid":"7f64c663-722a-20de-91c7-99ee2395094b"},"source":"**2D -PCA Plot**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2259ef23-3f4c-d860-7351-08dd953f31db"},"outputs":[],"source":"# This is a 2D pca plot... mehhhh\nplt.scatter(X_pca[:,0],X_pca[:,1],c=colors)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4eddb2fd-aba3-a869-df32-96064669205f"},"outputs":[],"source":"# To getter a better understanding of interaction of the dimensions\n# plot the first three PCA dimensions\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(1, figsize=(8, 6))\nax = Axes3D(fig, elev=-150, azim=110)\nX_reduced = decomposition.PCA(n_components=3).fit_transform(nan_features)\nax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2],c=colors,\n           cmap=plt.cm.Paired)\nax.set_title(\"First three PCA directions\")\nax.set_xlabel(\"1st eigenvector\")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"2nd eigenvector\")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"3rd eigenvector\")\nax.w_zaxis.set_ticklabels([])\n\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"40b8eb67-aa03-f95c-e414-0c5c265d2788"},"outputs":[],"source":"# here we plot a graph that looks at how many PCA components explain the variation\nn_eigens=10\nX_reduced = decomposition.PCA(n_components=n_eigens).fit(nan_features)\nwith plt.style.context('fivethirtyeight'):\n    plt.figure(figsize=(8, 5));\n    plt.title('Explained Variance Ratio over Component');\n    plt.plot(X_reduced.explained_variance_ratio_);"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fca591e6-61bc-660e-cb9f-5d9939c9dc16"},"outputs":[],"source":"with plt.style.context('fivethirtyeight'):\n    plt.figure(figsize=(8, 5));\n    plt.title('Cumulative Explained Variance over EigenFace');\n    plt.plot(X_reduced.explained_variance_ratio_.cumsum());"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"998a9d89-6bc3-3adb-199f-97a05893c9dd"},"outputs":[],"source":"print('PCA captures {:.2f} percent of the variance in the dataset'.format(X_reduced.explained_variance_ratio_.sum() * 100))\nprint('PCA components have dimensions {} by {}'.format(*X_reduced.components_.shape))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"71b6d0be-fa15-0c53-bcd9-b9abf329504f"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}