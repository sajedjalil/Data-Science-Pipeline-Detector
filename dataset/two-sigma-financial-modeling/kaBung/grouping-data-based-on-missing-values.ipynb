{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"cb01d80b-128d-ff15-d994-0f1c40351689"},"source":"There was a suggestion floating around to group the data into clusters of ids based on missing values. Here is an exploration as well as a feasibility study of the suggestion"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"baba1e2c-e2cd-a118-cc48-a97541d53d11"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport kagglegym\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8775c740-fb79-1eb7-1aa6-3394c877279d"},"outputs":[],"source":"env = kagglegym.make()\nobservation = env.reset()\ntrain = observation.train\nprint(\"Train has {} rows\".format(len(train)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a579c760-b865-bf9b-5375-039af98118e5"},"outputs":[],"source":"train.describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6738ed6b-4e97-23fa-b173-11ff2587049a"},"outputs":[],"source":"unique_id = train[\"id\"].unique()\nprint(\"There are {} unique ids\".format(len(unique_id)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"034e0e20-85ea-72bc-6214-e19284c9b243"},"outputs":[],"source":"train.groupby(\"id\").count()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b0826f70-5bac-05d8-a3ea-eecc81d3f85d"},"outputs":[],"source":"count = 0\nmissing_value = np.zeros((len(unique_id), train.shape[1]))\nfor item in unique_id:\n    tmp_id = train[train[\"id\"]==item]\n    tmp_id = tmp_id.fillna(0)\n    tmp_id = tmp_id.values\n    missing_value[count] = np.sum(tmp_id, 0) == 0\n    count = count + 1"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8eb192f3-d0e3-ac2e-56a5-d979c7c993ff"},"outputs":[],"source":"missing_value # binary matrix indicating (1s) completely missing features for each id"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"acc0aa28-1ea9-c494-66b9-8f067287be3e"},"outputs":[],"source":"missing_value.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"64d4b90e-24c9-82f4-734d-576ccb38d7f2"},"outputs":[],"source":"unique_missing_value = np.vstack(set(map(tuple, missing_value)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"07ccc2be-e982-4ff2-1af3-aae677ee0054"},"outputs":[],"source":"print(\"So the ids can be clustered into {} groups, based on completely missing features\".format(unique_missing_value.shape[0]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fdc02365-b7b2-f2ec-c5c7-49fccb6af4b7"},"outputs":[],"source":"# create a frequency table of ids in each group\nfreq_missing_value = []\nfor row in missing_value:\n    for index, row_unique in enumerate(unique_missing_value):\n        if np.array_equal(row, row_unique):\n            freq_missing_value.append(index)\n            \ngroup, id_count = np.unique(freq_missing_value, return_counts=True)            \n\nplt.plot(id_count)\nplt.xlabel(\"group\")\nplt.ylabel(\"frequency\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"0552f7f1-84b4-3192-42ed-963fc523b3b9"},"source":"The above plot shows that there are some groups where the frequency of ids can be up to 25, but the majority of the groups are filled with only one id.\n\nTo dive deeper, a frequency table of the counts in each group is created."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0d15ea16-ff24-4f6c-526d-2e68094191e8"},"outputs":[],"source":"supergroup, group_count = np.unique(id_count, return_counts=True)\n\nprint(np.asarray((supergroup, group_count)).T)\n\nplt.plot(supergroup, group_count, '-o', label=\"Original noisy data\")\nplt.xlabel(\"supergroup\")\nplt.ylabel(\"frequency\")\nplt.legend()\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"c1895195-9f6f-a51b-6237-e452708bf234"},"source":"It appears that the supergroup is structured according to the power law. Applying an exponential fit to the data leads to the below plots."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"52bfda64-448d-9549-0bbc-2cb21025c408"},"outputs":[],"source":"group_count = group_count*10 # rescale data to zoom into the long tail\n\nfrom scipy.optimize import curve_fit\ndef func(x, a, b, c):\n    return a * np.power(x,-b) + c\n\npopt, pcov = curve_fit(func, supergroup, group_count, maxfev=2000)\n\nprint(\"Fit parameters are\", popt) \nprint(\"Error on parameters are\", np.sqrt(np.diag(pcov)))\n\nplt.plot(supergroup, group_count, 'ko', label=\"Original noisy data\")\nplt.plot(supergroup, func(supergroup, *popt), 'r-', label=\"Exponential fit\")\nplt.xlabel(\"supergroup\")\nplt.ylabel(\"frequency\")\nplt.legend()\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2aab2d0b-6ead-6c21-ba52-8bed96315d8f"},"outputs":[],"source":"plt.loglog(supergroup, group_count, 'ko', label=\"Original noisy data\")\nplt.loglog(supergroup, func(supergroup, *popt), 'r-', label=\"Fitted Curve\")\nplt.xlabel(\"supergroup\")\nplt.ylabel(\"frequency\")\nplt.legend()\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"1b676e0a-d42e-8b35-6601-61b58d349811"},"source":"The above plot shows that an exponential curve fits the data fairly well."},{"cell_type":"markdown","metadata":{"_cell_guid":"c076ca9e-7782-8f8d-1e7d-a6e7b1aea541"},"source":"## Conclusion:\nThere are 1096 unique ids. They can be clustered into 784 groups based on completely missing features. About 700 groups are composed of a single id, which means that such a clustering is of no help.\n\nThe groups seem to be following a power law, like in any unconstrained system. The system here is obviously the financial market, and the ids seem to represent the market players like the companies or their stocks."},{"cell_type":"markdown","metadata":{"_cell_guid":"9427d446-8692-8fb7-1bc4-92a95f471ab7"},"source":"NB: Though the conclusions are practically not very useful, thought it might be a good idea to share the insights. Cheers!"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}