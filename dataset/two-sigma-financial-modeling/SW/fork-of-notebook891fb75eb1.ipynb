{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"4e105f87-a68c-8804-6106-3788916360f6"},"source":"Some thoughts\n\n0) By scrolling down it's easy to see that at this stage I didn't get good results. [Disclaimer: Also, I know that I haven't performed sufficient tests of data before going for linear regression... And I know, I got a bit lost.]\n\n1) Plenty of data is missing, but let's see whether for certain ids we can get full(er?... at least) dataset. In other words: maybe for all available set data is missing, but if we take each second/third/fourth id + data we'll get something valuable.\n\n2) Trying predictions with all these features is pointless (correlations), it's reasonable to choose some more valuable features\n\nAd.1)\nLet's start and see how many entries do we have."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"096ecc8b-aafe-eccf-b78e-43af66a332e0"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nwith pd.HDFStore('../input/train.h5') as train:\n    df = train.get('train')\nprint(df.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"60e1283e-f372-e83a-c302-7a76df6e5121"},"source":"Quite a many. Now, let's see how many different ID's and timestamps do we have, e.g. let's check whether ids and timestamps are unique and are they missing anytime in data set at all."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a33d164a-2966-6d88-f428-f187efe8b72f"},"outputs":[],"source":"print(df.count(axis=0))\nplt.plot(df.y)"},{"cell_type":"markdown","metadata":{"_cell_guid":"2922e30b-4ab8-1e47-86bb-d4659e7e1f69"},"source":"My general findings after going through dataset are:\n - I think these are returns on a portfolio built out of instruments with given id (in time, -> timestamps)\n - Range is circa between -5% to 10% which seems feasible.\n - For given id of instruments we have a given set of features and timestamps. We can use these features over time to predict y_id (y for a given id) --> I should try dealing with time series!"},{"cell_type":"markdown","metadata":{"_cell_guid":"02c430e7-c417-b21f-8877-cc9b5f776843"},"source":"I make a hypothesis that's it's possible to predict y by constructing y_general depending on y_ids. Y_ids, returns for a given instruments, are dependent on certain features for them.  I focus below on trying linear regression for a one given Y_id (I ignore timestamps, basically; I think now it might be better to try using time series)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6bc44a87-e60f-40ff-dfbe-737fb0f4abf2"},"outputs":[],"source":"'''\nCheck how many unique id's do we have.\n'''\nlen(df.id.unique())\n'''\nSoo we have 1424 different IDs. Legit. Now we'll make an array of them for future use.\n'''\nid_list = df.id.unique()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9ca74c49-7998-0ce7-1fb6-8aedb581da60"},"outputs":[],"source":"id_df = df.loc[df['id'] == id_list[70]]\n'''\nprind(id_df)\nFrom printing frame we see that for a given id we have a complete list of timestamps\n'''\nprint(id_df.count(axis=0))"},{"cell_type":"markdown","metadata":{"_cell_guid":"66db9f13-5c7f-ed72-89de-990fd875cf0f"},"source":"We can sort our dataframe by an ID. Then we will see complete list of consecutive timestamps and also we can notice that for a given ID, certain number of features have full data set, while some are completely absent (equal 0). We can say then that for this given instrument (with a given id) we can skip certain feature since it apparently had no value for our predictions.\n\nFor the purpose of experiment :) let's use only these features whose total nonNaN number equals number of ids/timestamps."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e238865d-d2e2-db4b-fce7-8395f146822f"},"outputs":[],"source":"new_id_df = id_df.dropna(axis=1, how='any')\n# print(new2_id_df.count(axis=0))\n\nY_train = new_id_df.y\nY_train = Y_train.reset_index()\nY_train = Y_train.iloc[:,1]\nprint(Y_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"25dab19d-2bc0-66ef-17a7-15fe1fc9a44e"},"outputs":[],"source":"print(new_id_df)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4282fa6e-c206-43c6-72cd-2cebe4f5d025"},"outputs":[],"source":"print(new_id_df.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"bd139d02-5170-0bf9-5c4b-5393130d43e3"},"source":"At this stage I think it makes sense to check correlations between the features."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e2ca3719-9f4e-4178-fcc3-8487a6f13fff"},"outputs":[],"source":"'''\nCorrelations: \n'''\n\nc = new_id_df.corr().abs()\n\ns = c.unstack()\nso = s.order(ascending=False)\nff = pd.DataFrame(so, columns=['coeff'])\nff = pd.DataFrame(so, columns=['coeff'])\nff = ff.dropna()\nff = ff.tail(20)\nff = ff.iloc[::2]\nprint(ff)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03512f29-3e98-1fbb-4dee-eae631c45be5"},"outputs":[],"source":"raw_features_list = []\nfor i in range (0, len(ff)):\n    if any('y' in code for code in ff.iloc[[i]].index.tolist()):\n        pass\n    else:\n        for j in range (0,1):\n            raw_features_list.extend(ff.iloc[[i]].index.tolist()[j])\nfeatures_list = list(set(raw_features_list))\nprint(features_list)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b6c1c1c0-ae14-0b35-e876-57cd5effeee1"},"outputs":[],"source":"# So now we have much smaller list of features. Let's X set [we created Y beforehand]\nX_train = new_id_df[features_list]\nX_train = X_train.reset_index()\nX_train = X_train.iloc[:,1:]\nprint(X_train)\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"18fb6afb-6e3e-95f0-134f-4d93bdc1fd00"},"outputs":[],"source":"from sklearn import linear_model\n\n# Split the data into training/testing sets\nX_train = X_train[:-(int(len(X_train)/2))]\nX_train_test = X_train[-(int(len(X_train)/2)):]\n\n# Split the targets into training/testing sets\nY_train = Y_train[:-(int(len(Y_train)/2))]\nY_train_test = Y_train[-(int(len(Y_train)/2)):]\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"82fb61df-dac6-c1fb-9034-d49b417356c5"},"outputs":[],"source":"# Create linear regression object\nregr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(X_train, Y_train)\n\n# The coefficients\nprint('Coefficients: \\n', regr.coef_)\n# The mean squared error\nprint(\"Mean squared error: %.2f\"\n      % np.mean((regr.predict(X_train_test) - Y_train_test) ** 2))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % regr.score(X_train_test, Y_train_test))\n\n# Plot outputs\nplt.plot(X_train_test, regr.predict(X_train_test), color='blue',\n         linewidth=3)\n\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"7b254235-c1d1-6ea8-1883-921e99c3d6c6"},"source":"Variance score 0.03. OK it's not the best way. Attempts to be continued."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fad0d2e7-2aee-6f5a-4e22-399412706e68"},"outputs":[],"source":"plt.plot(Y_train_test)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}