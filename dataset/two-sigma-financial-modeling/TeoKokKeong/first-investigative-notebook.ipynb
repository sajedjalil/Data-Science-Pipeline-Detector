{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"c73847c6-da86-6911-92b0-a24011bde994"},"source":"This is my first notebook at Kaggle, it is really to get myself started on something."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b4e24c2b-b700-f5b2-8f15-5240a1877d60"},"outputs":[],"source":"\nimport numpy as np\nimport pandas as pd \nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"88f53b1f-cfd9-e86d-edf9-2dfe4141e91d"},"source":"To load Kaggle enivornment"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da75fe2b-a641-3532-d56e-3ec212117822"},"outputs":[],"source":"import kagglegym\n# Create environment\nenv = kagglegym.make()\n# Get first observation\nobservation = env.reset()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9c50706e-e465-6fa6-55c0-11ec0fec846d"},"outputs":[],"source":"with pd.HDFStore('../input/train.h5') as train:\n    df = train.get('train')\nprint(df.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"681de0d9-e2fd-ea55-75c7-3e8a09179c12"},"source":"Read in the value from train.h5 ile."},{"cell_type":"markdown","metadata":{"_cell_guid":"bdd498db-0641-bc70-adc7-101880817b90"},"source":"Lets take a look at the data to have a feel of what we are actually having on hand."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"835c0c89-ed8d-5893-bd21-55f689bbcbba"},"outputs":[],"source":"print(df.shape)\ndf.head()\n\n#newdata=df.ix[:,2:110]\n#newdata=newdata.dropna()"},{"cell_type":"markdown","metadata":{"_cell_guid":"8982d04e-e3e7-e66b-ab88-81706e9b44f6"},"source":"Lets look at the column list"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f36e3918-e268-3ce6-a1ad-474e661d619d"},"outputs":[],"source":"for col in df.columns:\n    print(col)"},{"cell_type":"markdown","metadata":{"_cell_guid":"f3c373b8-5e22-2c21-663f-da4d7c582eb7"},"source":"get rid of rows with NAN in any column"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a9f0b731-bdc5-9ebf-c2a0-4a31f023960a"},"outputs":[],"source":"newdata=df.dropna()\nprint(newdata.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"420122d2-ce1e-64e5-455e-8899a6cf7c05"},"source":"observe the value"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cee7024e-48b1-46dc-4e5d-b2e9ac4c6e81"},"outputs":[],"source":"newdata[\"y\"].hist(bins=100)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a41c0f65-bdd3-b770-761c-30bec549633e"},"source":"observe another column"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"edc54d11-2dad-42b1-9c99-2e3d9301fd9c"},"outputs":[],"source":"\nnewdata[\"fundamental_11\"].hist(bins=100)"},{"cell_type":"markdown","metadata":{"_cell_guid":"8ff7e11b-cc15-b0ea-a9e3-97d6721a5c0e"},"source":"all seem to be zero mean, that would mean that the data had went through some scaling pre-process"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f6cfbba5-07c4-b3ec-3b03-12a1be4626e6"},"outputs":[],"source":"newdata[\"fundamental_12\"].hist(bins=100)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ff7036b5-be39-d2db-7122-23ce9f6792ea"},"outputs":[],"source":"print(newdata.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"80945b02-d904-a7d3-c30e-eb984600d42a"},"outputs":[],"source":"\nmoments = df[['id', 'y']].groupby('id').agg([np.mean, np.std, stats.kurtosis, stats.skew]).reset_index()\nmoments.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1f2075bb-aef4-c343-6918-70fae9983aaa"},"outputs":[],"source":"\nmoments = df[['timestamp','id']].groupby('id').agg([np.mean, np.std, stats.kurtosis, stats.skew]).reset_index()\nmoments.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ba3b2bfd-08a8-7982-b8b7-9bc1b8b6e531"},"outputs":[],"source":"nansum = df.isnull().sum()/len(df)\nprint(nansum)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6edbd87c-5aba-eb66-e0e5-826381467fbf"},"outputs":[],"source":"print(nansum.shape)\nplt.bar(np.arange(0,len(nansum),1),nansum)"},{"cell_type":"markdown","metadata":{"_cell_guid":"151a4ffc-41e9-430b-6941-ad6da66e2890"},"source":"**\n\nwould continue to add more code or explaination.\n------------------------------------------------\n\n**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"34a87e78-f22d-330a-cfa0-09f2d5bcb278"},"outputs":[],"source":"import tensorflow as tf\nimport math\nFEATURE_SIZE=108   #total number of features also is the number of input to the network\nbatch_size=32 \nLEARNING_RATE=0.01\nhidden1_units=FEATURE_SIZE\nhidden2_units=32\nd = tf.placeholder(tf.float32, shape=(batch_size,FEATURE_SIZE))                                              \ny = tf.placeholder(tf.float32, shape=(batch_size,1))\n#hidden layer 1 that interface with the input\nwith tf.name_scope('hidden1'):\n    weights = tf.Variable(tf.truncated_normal([FEATURE_SIZE, hidden1_units],\n                        stddev=1.0 / math.sqrt(float(FEATURE_SIZE))),name='weights')\n    biases = tf.Variable(tf.zeros([hidden1_units]),name='biases')\n    hidden1 = tf.nn.relu(tf.matmul(d, weights) + biases)\n#hidden layer 2 \nwith tf.name_scope('hidden2'):\n    weights = tf.Variable(tf.truncated_normal([FEATURE_SIZE, hidden2_units],\n                        stddev=1.0 / math.sqrt(float(FEATURE_SIZE))),name='weights')\n    biases = tf.Variable(tf.zeros([hidden2_units]),name='biases')\n    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n#softmax output\nwith tf.name_scope('softmax_linear'):\n    weights = tf.Variable(tf.truncated_normal([hidden2_units, 1],\n                            stddev=1.0 / math.sqrt(float(hidden2_units))),name='weights')\n    biases = tf.Variable(tf.zeros([1]),name='biases')\n    logits = tf.matmul(hidden2, weights) + biases    \n\n#y = tf.to_int64(y)\n#cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y, name='xentropy')\n#loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e8f0cab1-8423-9b4c-33df-c120a02f5ee1"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}