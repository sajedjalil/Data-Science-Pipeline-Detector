{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"064bbf4d-c858-1ad6-36ae-9cd334b09d83"},"source":"About Overfitting\n-----------------\n\nthis note is used to check the cumulative R value from  https://www.kaggle.com/sudalairajkumar/two-sigma-financial-modeling/am-i-over-fitting.\n\n(1)And it works well in detecting my bad performance of mycode (use four variables ,this model behaves well in the last half of train set with a public score 0.017265299516796905 but in test set with -0.0350178.https://www.kaggle.com/richardmore/two-sigma-financial-modeling/model1-5/run/506392)\nand it seems that the overfitting is a type of time overfitting( works extremely well in very few timestamp, and give a higher magnitude prediction)\n\n (2)And the below zero phenomenon of many model including ridge1 from https://www.kaggle.com/ymcdull/two-sigma-financial-modeling/ridge-lb-0-0100659/\n in early timestamp may be the low volatility in the early timestamp\n\n(3) Most of our model just give a very low magnitude prediction compared with the actual value .(about 1/50)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"face6fa6-36ae-934f-17e5-f08fbd5b0621"},"outputs":[],"source":"# Import all the necessary packages \nimport kagglegym\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge\nimport math\nimport matplotlib.pyplot as plt\n\n%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"018e401e-3666-f9ae-39c8-804e4fea7950"},"outputs":[],"source":"# Read the full data set stored as HDF5 file\nfull_df = pd.read_hdf('../input/train.h5')\n# A custom function to compute the R score\ndef get_reward(y_true, y_fit):\n    R2 = 1 - np.sum((y_true - y_fit)**2) / np.sum((y_true - np.mean(y_true))**2)\n    R = np.sign(R2) * math.sqrt(abs(R2))\n    return(R)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eaab9fc2-8f42-4cfd-218f-38c703dcb736"},"outputs":[],"source":"# Some pre-processing as seen from most of the public scripts.\n# The \"environment\" is our interface for code competitions\nenv = kagglegym.make()\n\n# We get our initial observation by calling \"reset\"\nobservation = env.reset()\ntarget_var = 'y'\n\n# Get the train dataframe\ntrain = observation.train\nmean_values = train.median(axis=0)\ntrain.fillna(mean_values, inplace=True)\n\n# Observed with histograns:\nlow_y_cut = -0.086093\nhigh_y_cut = 0.093497\n\ny_is_above_cut = (train.y > high_y_cut)\ny_is_below_cut = (train.y < low_y_cut)\ny_is_within_cut = (~y_is_above_cut & ~y_is_below_cut)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"512c1515-747d-1500-4a9a-3e00a478c2a4"},"outputs":[],"source":"### https://www.kaggle.com/ymcdull/two-sigma-financial-modeling/ridge-lb-0-0100659/run/545100\n# The \"environment\" is our interface for code competitions\nenv = kagglegym.make()\n\n# We get our initial observation by calling \"reset\"\nobservation = env.reset()\n\n# cols_to_use for ridge model\n#cols_to_use = ['technical_30', 'technical_20', 'fundamental_11']\ncols_to_use = ['technical_20']\n\n# model build\nmodel = Ridge()\nmodel.fit(np.array(train.loc[y_is_within_cut, cols_to_use].values), train.loc[y_is_within_cut, target_var])\n\n# getting the y mean dict for averaging\nymean_dict = dict(train.groupby([\"id\"])[\"y\"].mean())\n\n# weighted average of model & mean\ndef get_weighted_y(series):\n    id, y = series[\"id\"], series[\"y\"]\n    return 0.95 * y + 0.05 * ymean_dict[id] if id in ymean_dict else y\n\ny_actual_list = []\ny_pred_list = []\nr1_overall_reward_list = []\nts_list = []\nr1_lms = [] # mean squares between pred_y and acctual_y on each timestamp \nr1_lms_pre = [] # mean predicted return on each timestamp\nr1_lms_acc = [] # mean acctual return on each timestamp\nwhile True:\n    timestamp = observation.features[\"timestamp\"][0]\n    actual_y = list(full_df[full_df[\"timestamp\"] == timestamp][\"y\"].values)\n    observation.features.fillna(mean_values, inplace=True)\n    test_x = np.array(observation.features[cols_to_use].values)\n    observation.target.y = model.predict(test_x).clip(low_y_cut, high_y_cut)\n    \n    ## weighted y using average value\n    observation.target.y = observation.target.apply(get_weighted_y, axis = 1)\n    target = observation.target\n    observation, reward, done, info = env.step(target)\n    \n    if timestamp % 100 == 0:\n        print(\"Timestamp #{}\".format(timestamp))\n    \n    pred_y = list(target.y.values)\n    y_actual_list.extend(actual_y)\n    y_pred_list.extend(pred_y)\n    r1_lms.append(np.average( (np.array(actual_y)-np.array(pred_y) )**2))\n    tmp = np.average( np.abs(np.array(actual_y))  ) \n    r1_lms_acc.append(  tmp  )\n    tmp = np.average( np.abs( np.array(pred_y)  )   ) \n    r1_lms_pre.append(  tmp )\n    \n    \n    overall_reward = get_reward(np.array(y_actual_list), np.array(y_pred_list))\n    r1_overall_reward_list.append(overall_reward)\n    ts_list.append(timestamp)\n    if done:\n        break\n    \nprint(info)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"640afe23-3c63-4fd7-0ac9-e50ac0cb3374"},"outputs":[],"source":"plt.plot(ts_list,r1_lms)\nplt.title(\"mean square between pred_y and acctual_y on each timestamp\")\nplt.show()\nplt.plot(ts_list,r1_lms_acc)\nplt.title(\"mean absolute value of acctual_y on each timestamp\")\nplt.show()\nplt.plot(ts_list,r1_lms_pre)\nplt.title(\"mean absolute value of pred_y on each timestamp\")\nplt.show()\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"2f3464d8-48b7-ebb9-66ed-46460fecc463"},"source":"it seems that our ridge model have a similar shape with the actual but the magnitude is too small compared with the actual . Most of the predicted values are around Zero. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"963e1f1e-e79d-b229-43b9-a0967bddb96c"},"outputs":[],"source":"fig = plt.figure(figsize=(12, 6))\nplt.plot(ts_list, r1_overall_reward_list, c='blue')\nplt.plot(ts_list, [0]*len(ts_list), c='red')\nplt.title(\"Cumulative R value change for Univariate Ridge (technical_20)\")\nplt.ylim([-0.04,0.04])\nplt.xlim([850, 1850])\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"9bc8d11e-254b-f019-a119-a5de99c7e9b7"},"source":"my code, which perform good in local test but worse in the public LB. can this method shed some light on why this model is overfitting."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6e035dec-feb2-3f11-91ac-170ca45c9719"},"outputs":[],"source":"from sklearn import linear_model as lm\ncols_to_use = ['technical_30', 'technical_20', 'technical_40', 'technical_19']\n# Get first observation\nenv = kagglegym.make()\nobservation = env.reset()\ntrain = observation.train\nmean_values = train.mean(axis=0)\ntrain.fillna(mean_values, inplace=True)\n\nmodel = lm.LinearRegression()\nmodel.fit(np.array(train[cols_to_use]), train.y.values)\n\ny_pred_list = []\nmycode_overall_reward_list = []\ny_actual_list = []\nmycode_lms = []\nmycode_lms_acc = []\nmycode_lms_pre = []\n\nwhile True:\n    # code for the statistics\n    timestamp = observation.features[\"timestamp\"][0]\n    actual_y = list(full_df[full_df[\"timestamp\"] == timestamp][\"y\"].values)\n    \n    observation.features.fillna(mean_values, inplace=True)\n    test_x = np.array(observation.features[cols_to_use])\n    observation.target.y = model.predict(test_x)\n    target = observation.target\n    if timestamp % 100 == 0:\n        print(\"Timestamp #{}\".format(timestamp))\n    # code for the statistics    \n    pred_y = list(target.y.values)    \n    y_pred_list.extend(pred_y)\n    y_actual_list.extend(actual_y)\n    mycode_lms.append(np.average( (np.array(actual_y)-np.array(pred_y) )**2))\n    \n    tmp = np.average( np.abs(np.array(actual_y))  ) \n    mycode_lms_acc.append(  tmp  )\n    tmp = np.average( np.abs( np.array(pred_y)  )   ) \n    mycode_lms_pre.append(  tmp )\n    \n    overall_reward = get_reward(np.array(y_actual_list), np.array(y_pred_list))\n    mycode_overall_reward_list.append(overall_reward)\n    \n    observation, reward, done, info = env.step(target)\n    if done:\n        break\nprint(info)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c4007886-eb05-3a78-8d29-767481e07b45"},"outputs":[],"source":"\nplt.plot(ts_list,np.array(mycode_lms_acc)/50+0.001,color=\"y\",label=\"transformed acutal\")\nplt.plot(ts_list,mycode_lms_pre,label=\"mycode\",color='r')\nplt.plot(ts_list,r1_lms_pre,label=\"r1\",color='g')\nplt.legend(loc='upper right')\nplt.title(\"the magnitude of pred_y from r1 model, pred_y from mycode,transformed actual_y on each timestamp\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"289fc597-bbb7-714d-e18f-acaa53d3954a"},"source":"it seems that the the magnitude of r1 is smaller than mycode."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2de3c637-5dd2-60dc-2954-0a980f5042e4"},"outputs":[],"source":"fig, ax = plt.subplots(figsize=(12, 6))\nax.plot(ts_list, mycode_overall_reward_list, c='green', label='mycode')\nax.plot(ts_list, r1_overall_reward_list, c='blue', label='ridge-1')\nax.plot(ts_list, [0]*len(ts_list), c='red', label='zero line')\nax.legend(loc='lower right')\nax.set_ylim([-0.04,0.04])\nax.set_xlim([850, 1850])\nplt.title(\"Cumulative R value change for ridge1 and mycode\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"8284e17a-b805-97dd-a6ee-e84d2f729c0d"},"source":"\n\n\n----------\n\n\nthis code behaves well in the last half of train set with a public score 0.017265299516796905 but in test set with -0.0350178. and it seems this cumulative R value do a good job in differentiating the overfitting problem.\n\nBut what's wrong about my code? It ends with a scores  0.01726, which should be sightly improvement but in public LB -0.035. From this figure just above, the performance of my code is so volatile , even in cumulative R curve. Note that  around timestamp 1580, there is a big positive R and follows a big negative R. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"60eec15b-092b-3b4f-c633-cde3aad28909"},"outputs":[],"source":"# just for get the statistics....\nobservation = env.reset()\ny_actual_list = []\ncumu_average_return = []\ncumu_std_return = []\naverage_return = []\nstd_return = [] \n\nwhile True:\n    # code for the statistics\n    timestamp = observation.features[\"timestamp\"][0]\n    actual_y = list(full_df[full_df[\"timestamp\"] == timestamp][\"y\"].values)\n    target = observation.target\n    if timestamp % 100 == 0:\n        print(\"Timestamp #{}\".format(timestamp))\n    # code for the statistics    \n    y_actual_list.extend(actual_y)\n    average_return.append(np.average(actual_y))\n    std_return .append(np.std(actual_y))\n    cumu_average_return.append(np.average(y_actual_list))\n    cumu_std_return.append(np.std(y_actual_list))\n    observation, reward, done, info = env.step(target)\n    if done:\n        break"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fc923443-a9bf-061e-890e-b7438f7215e3"},"outputs":[],"source":"plt.plot(ts_list, average_return, c='red', label='average_rtn')\nplt.show()\nplt.plot(ts_list, std_return, c='yellow', label='average_rtn')\nplt.show()\nplt.plot(ts_list, cumu_average_return, c='red', label='average_rtn')\nplt.show()\nplt.plot(ts_list, cumu_std_return, c='yellow', label='average_rtn')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"ca4b46a2-6231-6233-bd4a-e741e99a1ecd"},"source":"from the fig above , we know that the std is lower in the early stage, so according to the R formula, the R will be lower.  this may be a reason why the early cumulative R curve of many model is below zero.\n\nHere, my code didn't work well in the cumulative curve but ends with a good score. This is so called overfitting, but from the cumulative curve, we can know that this overfitting is a type of time overfitting. \nMaybe my code(use four variables) behaves extremely well in special time  but poor in other timestamp. "},{"cell_type":"markdown","metadata":{"_cell_guid":"2ca630f9-c868-15cf-2feb-fdb9c1499255"},"source":"how about define another evaluation measure?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c127dc9a-d089-b0e6-fc68-3f84d682f09a"},"outputs":[],"source":"\nr1_lms_sqrt =[np.sqrt(i) for i in r1_lms] \nmycode_lms_sqrt =[np.sqrt(i) for i in mycode_lms] \n\nfig, ax = plt.subplots(figsize=(12, 6))\nax.plot(ts_list, mycode_lms_sqrt, c='green', label='mycode')\nax.plot(ts_list, r1_lms_sqrt, c='blue', label='ridge-1')\nplt.title(\"the new  for ridge1 and mycode\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"4896c9e2-49e3-71f1-052d-bdedbaea87cb"},"source":"these two curve are overlapping..... This may be why we use R measure."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"810cb2b3-27db-55ec-bed4-cc914d605cd7"},"outputs":[],"source":"plt.plot(ts_list,mycode_lms_pre)\nplt.show()\nplt.plot(ts_list,mycode_lms_acc)\nplt.show()\nplt.plot(ts_list,r1_lms_pre)\nplt.show()\nplt.plot(ts_list,r1_lms_acc)\nplt.show()"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}