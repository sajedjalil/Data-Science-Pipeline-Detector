{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"21aea7ea-c393-c133-84d2-797921a7b99e"},"source":"# RNN Model Production and Exploration Notebook"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"615f83f5-71c6-33d5-85fd-83040b7661b4"},"outputs":[],"source":"# import modules\nimport kagglegym\nimport numpy as np\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, LSTM\nfrom keras.regularizers import l2\nfrom keras.optimizers import SGD\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn import preprocessing as pp\nfrom sklearn.metrics import r2_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom time import time\n\n%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b82ab515-7617-db13-a88f-82827deeade4"},"outputs":[],"source":"# Start environment\nenv = kagglegym.make()\nobservation = env.reset()\ntrain = observation.train"},{"cell_type":"markdown","metadata":{"_cell_guid":"31d32e64-5dc0-44c8-25be-f03f2ca9b8cc"},"source":"Lets print our dataset head to see how it looks like:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b4ee3bb4-3f66-828c-531e-4a2aff59e0d2"},"outputs":[],"source":"observation.train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f948e52c-e920-1245-d5e9-369d5e5780dd"},"outputs":[],"source":"# Data preprocessing\n\n# https://www.kaggle.com/bguberfain/two-sigma-financial-modeling/univariate-model-with-clip/run/482189/code\n# Clipped target value range to use\nlow_y_cut = -0.086093\nhigh_y_cut = 0.093497\n\ny_is_above_cut = (train.y > high_y_cut)\ny_is_below_cut = (train.y < low_y_cut)\ny_is_within_cut = (~y_is_above_cut & ~y_is_below_cut)\n\n# Select the features to use\nexcl = ['id', 'sample', 'y', 'timestamp']\n#feature_vars = [c for c in train.columns if c not in excl]\nfeatures_to_use = ['technical_30', 'technical_20', 'technical_19', 'fundamental_11']\ntarget_var = ['y']\n\nfeatures = train.loc[y_is_within_cut, features_to_use]\nX_train = features.values\n\ntargets = train.loc[y_is_within_cut, target_var]\ny_train = targets.values\n\nim = pp.Imputer(strategy='median')\nX_train = im.fit_transform(X_train)\nX_scaler = pp.RobustScaler()\nX_train = X_scaler.fit_transform(X_train)\ny_scaler = pp.RobustScaler()\ny_train = y_scaler.fit_transform(y_train.reshape([-1,1]))\n\nX_train = pd.DataFrame(X_train, columns=features_to_use)\ny_train = pd.DataFrame(y_train, columns=target_var)\npreprocess_dict = {'fillna': im, 'X_scaler': X_scaler, 'y_scaler': y_scaler}\n\ndel y_is_above_cut, y_is_below_cut, excl, targets, features"},{"cell_type":"markdown","metadata":{"_cell_guid":"2c70aac9-f19c-c030-cc8f-cb31a5346159"},"source":"Lets take a peek in our dataset head again."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dec25d94-9813-00eb-aa54-46d3830a2091"},"outputs":[],"source":"X_train.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"7d9263d9-3d87-90ce-6a1e-20c6528bee56"},"source":"Right! Now we have scaled values and without NaN values. Better this way!\nNow we can start to build our models. This time we gonna try some deep neural network arquitetures and see how it performs. Let's get started!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f296f4b-3001-cfc0-df3f-8555152b9c96"},"outputs":[],"source":"def dnn(shape,timesteps,l2_coef,drop_coef):\n    model = Sequential()\n    model.add(LSTM(shape[1], input_shape=(timesteps, shape[0])))\n    model.add(Dense(shape[2], input_dim=shape[0], init='he_uniform', W_regularizer=l2(l2_coef)))\n    model.add(Activation('relu'))\n    model.add(Dropout(drop_coef))\n    model.add(Dense(shape[3], init='he_uniform', W_regularizer=l2(l2_coef)))\n\n    optm = SGD(lr=0.01, momentum=0.9, decay=0.0, nesterov=True)\n    model.compile(loss='mse',\n                  optimizer=optm,\n                  metrics=None,\n                  verbose=2)\n    return model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"48ee4ef3-eb7c-046a-b251-e942a0c5e05b"},"outputs":[],"source":"print(\"Training model\")\nt0 = time()\ntimesteps = 1\nX_train_ts = pad_sequences(np.reshape(X_train.values, (X_train.values.shape[0], timesteps, X_train.values.shape[1])))\nprint(X_train_ts.shape)\nmodel1 = dnn(shape=[4,8,64,1],timesteps=timesteps,l2_coef=0.0001,drop_coef=0.7)\nmodel1.fit(X_train_ts, y_train.values,\n          nb_epoch=33,\n          batch_size=X_train.values.shape[0],\n          verbose=2\n          );\nprint(\"Done! Training time:\", time() - t0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"de091747-97d7-62f6-51e6-abd9f37ed277"},"outputs":[],"source":"print(\"Evaluating model on training set\")\nt0 = time()\nm1_loss = model1.evaluate(X_train_ts, y_train.values, batch_size=X_train.values.shape[0], verbose=0)\nprint(\"Done! Eval time:\",time() - t0)\nprint(\"Mean squared error for train dataset:\",m1_loss)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"367d3ff3-1e94-45b9-7f3c-db8f7aa20996"},"outputs":[],"source":"print(\"Predicting target on training dataset\")\nt0 = time()\nm1_preds = model1.predict(X_train_ts, batch_size=X_train.values.shape[0], verbose=0)\nscore = r2_score(y_train, m1_preds)\nprint(\"Done! Prediction time:\",time() - t0)\nprint(\"R2 score for train dataset\",score)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"40f1cfe4-fe62-4b7a-518f-19624786790f"},"outputs":[],"source":"# Predict-step-predict routine ################################################################################\ndef gen_preds(model, preprocess_dict, features_to_use, print_info=True):\n    env = kagglegym.make()\n    # We get our initial observation by calling \"reset\"\n    observation = env.reset()\n\n    im = preprocess_dict['fillna']\n    X_scaler = preprocess_dict['X_scaler']\n    y_scaler = preprocess_dict['y_scaler']\n    \n    reward = 0.0\n    reward_log = []\n    timestamps_log = []\n    pos_count = 0\n    neg_count = 0\n\n    total_pos = []\n    total_neg = []\n\n    print(\"Predicting\")\n    t0= time()\n    while True:\n    #    observation.features.fillna(mean_values, inplace=True)\n\n        # Predict with model\n        features_dnn = im.transform(observation.features.loc[:,features_to_use].values)\n        features_dnn = X_scaler.transform(features_dnn)\n        \n        features_dnn_ts = pad_sequences(np.reshape(features_dnn,\n                                                   (features_dnn.shape[0], timesteps, features_dnn.shape[1])))\n\n        y_dnn = model.predict(features_dnn_ts,batch_size=features_dnn.shape[0],\n                              verbose=0).clip(low_y_cut, high_y_cut)\n\n        # Fill target df with predictions \n        observation.target.y = y_scaler.inverse_transform(y_dnn)\n\n        observation.target.fillna(0, inplace=True)\n        target = observation.target\n        timestamp = observation.features[\"timestamp\"][0]\n        \n        observation, reward, done, info = env.step(target)\n\n        timestamps_log.append(timestamp)\n        reward_log.append(reward)\n\n        if (reward < 0):\n            neg_count += 1\n        else:\n            pos_count += 1\n\n        total_pos.append(pos_count)\n        total_neg.append(neg_count)\n        \n        if timestamp % 100 == 0:\n            if print_info:\n                print(\"Timestamp #{}\".format(timestamp))\n                print(\"Step reward:\", reward)\n                print(\"Mean reward:\", np.mean(reward_log[-timestamp:]))\n                print(\"Positive rewards count: {0}, Negative rewards count: {1}\".format(pos_count, neg_count))\n                print(\"Positive reward %:\", pos_count / (pos_count + neg_count) * 100)\n\n            pos_count = 0\n            neg_count = 0\n\n        if done:\n            break\n    print(\"Done: %.1fs\" % (time() - t0))\n    print(\"Total reward sum:\", np.sum(reward_log))\n    print(\"Final reward mean:\", np.mean(reward_log))\n    print(\"Total positive rewards count: {0}, Total negative rewards count: {1}\".format(np.sum(total_pos),\n                                                                                        np.sum(total_neg)))\n    print(\"Final positive reward %:\", np.sum(total_pos) / (np.sum(total_pos) + np.sum(total_neg)) * 100)\n    print(info)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c679d322-8aa3-2654-fd71-d225f4ea2489"},"outputs":[],"source":"gen_preds(model1, preprocess_dict, features_to_use)"},{"cell_type":"markdown","metadata":{"_cell_guid":"9db8a68a-fe2a-964b-ec8f-60e4c0bef6b2"},"source":"### Analysing Training Results"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"33880a48-6d4e-5314-6a69-f8bb341480b7"},"outputs":[],"source":"market_df = observation.train[['timestamp', 'y']].groupby('timestamp').agg([np.mean, np.std]).reset_index()\ny_mean = np.array(market_df['y']['mean'])\nt = market_df['timestamp']\n\nprint(\"Predicting target on training dataset\")\nt0 = time()\nm1_preds = model1.predict(X_train_ts, batch_size=X_train_ts.shape[0], verbose=0)\nscore = r2_score(y_train, m1_preds)\nprint(\"Done! Prediction time:\",time() - t0)\nprint(\"R2 score for train dataset\",score)\ncum_ret = np.log(1+y_mean).cumsum()\npred_ret = pd.DataFrame(np.vstack((observation.train.timestamp.loc[y_is_within_cut], m1_preds[:,0])).T,\n                        columns=['timestamp','y']).groupby('timestamp').agg([np.mean, np.std]).reset_index()\ncum_pred1 = np.log(1+pred_ret['y']['mean']).cumsum()\n\nfig, ax = plt.subplots(figsize=(12,7))\nax.set_xlabel(\"Timestamp\");\nax.set_title(\"Cumulative target signal and predictions over time\");\nsns.tsplot(cum_ret,t,ax=ax,color='b');\nsns.tsplot(cum_pred1,t,ax=ax,color='r');\nax.set_ylabel('Target / Prediction');\n\nfig, ax = plt.subplots(figsize=(12,7))\nax.set_title(\"Target Variable Distribution. (True vs Prediction)\");\n#plt.ylim([0, 100000])\nsns.distplot(observation.train.y ,ax=ax, color='b', kde=False, bins=100);\nsns.distplot(m1_preds ,ax=ax, color='r', kde=False, bins=100);\nax.set_ylabel('Target / Prediction');"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}