{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"cfbd9d83-42e8-4a3d-6fc2-30f2976c12f7"},"source":"This notebook is an assistant visualization script for production and exploration of some ensemble models."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"136634b6-4607-f321-2757-306b58af6504"},"outputs":[],"source":"#XGBoost prediction script\n#importing modules\n\nimport kagglegym\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom time import time"},{"cell_type":"markdown","metadata":{"_cell_guid":"b8223572-b98a-ed3a-5ffe-db1e6298cc5e"},"source":"Setting up the environment."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"408dc43c-fea3-1b80-42ed-1f98ea17aa33"},"outputs":[],"source":"#Making environment ################################################################################################################################################################\n\n# The \"environment\" is our interface for code competitions\nenv = kagglegym.make()\n\n# We get our initial observation by calling \"reset\"\nobservation = env.reset()\n\ntrain = observation.train\n# Note that the first observation we get has a \"train\" dataframe\nprint(\"Train has {} rows\".format(len(observation.train)))\n\n# The \"target\" dataframe is a template for what we need to predict:\nprint(\"Target column names: {}\".format(\", \".join(['\"{}\"'.format(col) for col in list(observation.target.columns)])))"},{"cell_type":"markdown","metadata":{"_cell_guid":"6a2aebf8-5733-221a-3cab-e79de65b7751"},"source":"Some preprocessing of the data. Clipping the y signal helps, as shown in the community kernels."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b4e22e66-621d-ea54-903d-30e5454600ce"},"outputs":[],"source":"# Feature enginnering and preprocessing ############################################################################################################################################\n\n# https://www.kaggle.com/bguberfain/two-sigma-financial-modeling/univariate-model-with-clip/run/482189/code\n# Clipped target value range to use\nlow_y_cut = -0.08\nhigh_y_cut = 0.08\n\ny_is_above_cut = (train.y > high_y_cut)\ny_is_below_cut = (train.y < low_y_cut)\ny_is_within_cut = (~y_is_above_cut & ~y_is_below_cut)\n\n# Select the features to use\nexcl = ['id', 'sample', 'y', 'timestamp']\n#feature_vars = [c for c in train.columns if c not in excl]\ntarget_var = 'y'\n\ntargets = train.loc[y_is_within_cut, target_var]\ny_train = targets.values\n\ndel y_is_above_cut, y_is_below_cut, excl, target_var, targets"},{"cell_type":"markdown","metadata":{"_cell_guid":"582ea5e6-f00c-aa48-676d-9e9ea26195ad"},"source":"Here we are using some ensemble models from XGBoost. Using a linear models ensemble plus a trees regressor ensemble to try to manage overfitting."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"996ad30f-3ed3-ade2-17d5-34b24a256789"},"outputs":[],"source":"# Model training routine ###########################################################################################################################################################\n\n# Univariate linear models, first layer <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n\nfeatures_ulm = ['technical_20', 'technical_19', 'technical_27', 'technical_30', 'technical_2', 'technical_36']\n\nfeatures_ulm_train = train.loc[y_is_within_cut, features_ulm]\nfeature_ulm_names = features_ulm_train.columns\nX_ulm = features_ulm_train.values\n\n# Train dataset\nxglin_train = xgb.DMatrix(X_ulm, label=y_train, feature_names=feature_ulm_names)\n\n# XGb model params\nparams_xglin = {'booster'         :'gblinear',\n                'objective'       :'reg:linear',\n                'eta'             : 0.1,\n                'max_depth'       : 4,\n                'subsample'       : 0.9,\n                'min_child_weight': 1000,\n                'seed'            : 42,\n                'base_score'      : 0\n                }\n\nprint (\"Training linear models\")\nt0 = time()\nbslin = xgb.train(params_xglin, xglin_train, 10)\nprint(\"Done: %.1fs\" % (time() - t0))\n\n# Boosted trees ensemble, first layer <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n\n# https://www.kaggle.com/fernandocanteruccio/two-sigma-financial-modeling/xgboost-feature-importance-analysis\nfeatures_bt = ['technical_35', 'fundamental_37', 'technical_20', 'technical_36', 'fundamental_36', 'fundamental_53',\n                'fundamental_35', 'fundamental_11', 'fundamental_50', 'fundamental_34']\n\nfeatures_bt_train = train.loc[y_is_within_cut, features_bt]\nfeature_bt_names = features_bt_train.columns\nX_bt = features_bt_train.values\n\n# Train dataset\nxgtrees_train = xgb.DMatrix(X_bt, label=y_train, feature_names=feature_bt_names)\n\n# XGb model params\nparams_xgtrees = {'objective'       :'reg:linear',\n                  'eta'             : 0.1,\n                  'max_depth'       : 4,\n                  'subsample'       : 0.9,\n                  'min_child_weight': 1000,\n                  'seed'            : 42,\n                  'base_score'      : 0\n                   }\n\nprint (\"Training boosted trees\")\nt0 = time()\nbst = xgb.train(params_xgtrees, xgtrees_train, 10)\nprint(\"Done: %.1fs\" % (time() - t0))"},{"cell_type":"markdown","metadata":{"_cell_guid":"dad0e9e5-697f-cfae-0c3d-947901d4e593"},"source":"For this simple exploration model, the final ensemble prediction is just the unweighted mean of the first layer models predictions."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0c6a2aa0-9900-1147-b008-92946a507371"},"outputs":[],"source":"# Predict-step-predict routine ####################################################################################################################################################\ndef gen_predictions(update_threshold, print_info=True):\n    \n    global bslin, bst\n    \n    env = kagglegym.make()\n\n    # We get our initial observation by calling \"reset\"\n    observation = env.reset()\n\n    train = observation.train\n\n    params_xglin.update({'process_type': 'update',\n                         'updater'     : 'refresh',\n                         'refresh_leaf': False})\n\n    params_xgtrees.update({'process_type': 'update',\n                           'updater'     : 'refresh',\n                           'refresh_leaf': False})\n\n    # init aux vars\n    reward = 0.0\n    reward_log = []\n    timestamps_log = []\n    pos_count = 0\n    neg_count = 0\n\n    total_pos = []\n    total_neg = []\n\n    print(\"Predicting\")\n    t0= time()\n    while True:\n    #    observation.features.fillna(mean_values, inplace=True)\n\n        # Predict with univariate linear models\n        features_ulm_pred = observation.features.loc[:,features_ulm].values\n        X_ulm_pred = xgb.DMatrix(features_ulm_pred, feature_names=feature_ulm_names)\n\n        y_ulm_pred = bslin.predict(X_ulm_pred).clip(low_y_cut, high_y_cut)\n\n        # Predict with boosted trees\n        features_bt_pred = observation.features.loc[:,features_bt].values\n        X_bt_pred = xgb.DMatrix(features_bt_pred, feature_names=feature_bt_names)\n\n        y_bt_pred = bst.predict(X_bt_pred).clip(low_y_cut, high_y_cut)\n\n        # Average the predictions\n        averaged_out = np.mean(np.vstack((y_ulm_pred, y_bt_pred)), axis=0)\n\n        # Fill target df with predictions \n        observation.target.y = averaged_out\n\n        observation.target.fillna(0, inplace=True)\n        target = observation.target\n        timestamp = observation.features[\"timestamp\"][0]\n        obs_old = observation\n        observation, reward, done, info = env.step(target)\n\n        if update_threshold is not None:\n            if (reward > update_threshold):\n                # update boosted trees model\n                xgtrees_update = xgb.DMatrix(obs_old.features.loc[:,features_bt].values, averaged_out, feature_names=feature_bt_names)\n\n                bst = xgb.train(params_xgtrees, xgtrees_update, 10, xgb_model=bst)\n\n                # update boosted linear model \n                xglin_update = xgb.DMatrix(obs_old.features.loc[:,features_ulm].values, averaged_out, feature_names=feature_ulm_names)\n\n                bslin = xgb.train(params_xglin, xglin_update, 10, xgb_model=bslin)\n\n        \n        timestamps_log.append(timestamp)\n        reward_log.append(reward)\n\n        if (reward < 0):\n            neg_count += 1\n        else:\n            pos_count += 1\n\n        total_pos.append(pos_count)\n        total_neg.append(neg_count)\n        \n        if timestamp % 100 == 0:\n            if print_info:\n                print(\"Timestamp #{}\".format(timestamp))\n                print(\"Step reward:\", reward)\n                print(\"Mean reward:\", np.mean(reward_log[-timestamp:]))\n                print(\"Positive rewards count: {0}, Negative rewards count: {1}\".format(pos_count, neg_count))\n                print(\"Positive reward %:\", pos_count / (pos_count + neg_count) * 100)\n\n            pos_count = 0\n            neg_count = 0\n\n        if done:\n            break\n    print(\"Done: %.1fs\" % (time() - t0))\n    print(\"Total reward sum:\", np.sum(reward_log))\n    print(\"Final reward mean:\", np.mean(reward_log))\n    print(\"Total positive rewards count: {0}, Total negative rewards count: {1}\".format(np.sum(total_pos), np.sum(total_neg)))\n    print(\"Final positive reward %:\", np.sum(total_pos) / (np.sum(total_pos) + np.sum(total_neg)) * 100)\n    print(info)\n\n    return reward_log, timestamps_log, info['public_score']\n\nreward_log, timestamps_log, score = gen_predictions(None)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1c174a41-b755-2f47-518a-56e1a709e6bb"},"source":"With the model predictions in hands, lets plot some rewards distributions."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d0b83462-c340-5194-bc26-4e80ff17390e"},"outputs":[],"source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\");"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ccffb5c2-60c4-b8dc-4145-7e7a207f9bb5"},"outputs":[],"source":"fig, ax = plt.subplots(figsize=(12,7))\nax.set_title(\"Rewards distribution\");\nsns.distplot(reward_log, kde=True);\nprint(\"Rewards count:\",np.array(reward_log).shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d13b5ddb-f965-daba-2013-185b26635206"},"source":"As seen in the histogram, we are getting most of the reward signals around -0.1. Lets find out how it changes over time."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3e703f66-e4f3-2727-e9e8-084c040076aa"},"outputs":[],"source":"def moving_average(a, n=3) :\n    ret = np.cumsum(a, dtype=float)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1:] / n\n\nma_window = 33\n\nfig, ax = plt.subplots(figsize=(12,7))\nax.set_xlabel(\"Timestamp\");\nax.set_title(\"Reward signal over time\");\nsns.tsplot(reward_log,timestamps_log,ax=ax,color='b');\nsns.tsplot(np.hstack((np.zeros(ma_window-1),moving_average(reward_log, ma_window)))\n           ,timestamps_log,ax=ax,color='r');\nax.set_ylabel('Reward');"},{"cell_type":"markdown","metadata":{"_cell_guid":"d62a64d8-a6c7-654e-ded5-bf309276cd3b"},"source":"No real trend here. Lets try to refresh the models trees during the prediction fase and see what happens with the reward signal."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2655e1e0-c9e5-f0ce-562d-475415b008cf"},"outputs":[],"source":"reward_log_2, timestamps_log, score_2 = gen_predictions(0.01,print_info=False)\nprint(\"Percent change:\", (score_2 - score) / score * 100)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e120ffd6-ef40-c06d-bc9e-1b478b28dafe"},"outputs":[],"source":"fig, ax = plt.subplots(figsize=(12,7))\nax.set_title(\"Rewards distribution\");\nsns.distplot(reward_log, kde=True, ax=ax, label='Without update',color='b');\nsns.distplot(reward_log_2, kde=True, ax=ax, label='With update',color='g');\nplt.legend();"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fc3fa294-3880-f5d8-e00a-78b6049a54f2"},"outputs":[],"source":"fig, ax = plt.subplots(figsize=(12,7))\nax.set_xlabel(\"Timestamp\");\nax.set_title(\"Averaged reward signal over time (window = 33)\");\nsns.tsplot(np.hstack((np.zeros(ma_window-1),moving_average(reward_log, ma_window)))\n           ,timestamps_log,ax=ax,color='b');\nsns.tsplot(np.hstack((np.zeros(ma_window-1),moving_average(reward_log_2, ma_window)))\n           ,timestamps_log,ax=ax,color='g');\nax.set_ylabel('Reward');\nax.set_ylim([-0.23, -0.08]);"},{"cell_type":"markdown","metadata":{"_cell_guid":"813f0227-4a7f-080c-8ea0-0388530c948c"},"source":"Not much of a difference on the rewards log, yet the public score jumped. This is an ~15% increase in the score. What if we do more agressive updates?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bf2d0fcd-1802-d61e-5ac2-d09b71b8b10c"},"outputs":[],"source":"reward_log_3, timestamps_log, score_3 = gen_predictions(-0.03,print_info=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"90b61a04-5364-3d4a-2115-cf219269a7b0"},"outputs":[],"source":"fig, ax = plt.subplots(figsize=(12,7))\nax.set_title(\"Rewards distribution\");\nsns.distplot(reward_log, kde=True, ax=ax, label='Without update',color='b');\nsns.distplot(reward_log_2, kde=True, ax=ax, label='With update',color='g');\nsns.distplot(reward_log_3, kde=True, ax=ax, label='More agressive update',color='r');\nplt.legend();\nprint(\"Percent change:\", (score_3 - score) / score * 100)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7026394b-dc79-66cf-be19-69ef2c629515"},"outputs":[],"source":"fig, ax = plt.subplots(figsize=(12,7))\nax.set_xlabel(\"Timestamp\");\nax.set_title(\"Averaged reward signal over time (window = 33)\");\nsns.tsplot(np.hstack((np.zeros(ma_window-1),moving_average(reward_log, ma_window)))\n           ,timestamps_log,ax=ax,color='b');\nsns.tsplot(np.hstack((np.zeros(ma_window-1),moving_average(reward_log_2, ma_window)))\n           ,timestamps_log,ax=ax,color='g');\nsns.tsplot(np.hstack((np.zeros(ma_window-1),moving_average(reward_log_3, ma_window)))\n           ,timestamps_log,ax=ax,color='r');\nax.set_ylabel('Averaged Reward');\nax.set_ylim([-0.23, -0.08]);"},{"cell_type":"markdown","metadata":{"_cell_guid":"173e3e80-4275-ccaf-4756-c1d015095395"},"source":"Again, not much of a chance but the score increased about ~15.4% from the static model. With so many params to tune, it will be nice to do some grid-search with cross-validation. Also, it will be probably better it the ensemble predictions is weighted by some model instead of simple averaging."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}