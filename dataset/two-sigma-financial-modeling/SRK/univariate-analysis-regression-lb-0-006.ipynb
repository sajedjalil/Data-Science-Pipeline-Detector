{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"79bc1341-0357-bd3b-f0fd-923783e8cb2f"},"source":"Let us do some univariate analysis in this notebook and build simple regression models."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"970e3273-0596-1ae2-f41b-8bd4f383e11f"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import linear_model as lm\nimport kagglegym\n\n%matplotlib inline"},{"cell_type":"markdown","metadata":{"_cell_guid":"8ab0f76f-6e12-71b6-64d2-f42ec4d88afa"},"source":"Read the train file from Kaggle gym."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f2400f0b-6036-8bcc-8b40-d5b121dc9e67"},"outputs":[],"source":"# Create environment\nenv = kagglegym.make()\n\n# Get first observation\nobservation = env.reset()\n\n# Get the train dataframe\ntrain = observation.train"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ac653533-ef0e-ff32-141e-d29fad827240"},"outputs":[],"source":"train.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e5f46aaa-dc6b-b166-5ea5-af9f49566724"},"outputs":[],"source":"mean_values = train.mean(axis=0)\ntrain.fillna(mean_values, inplace=True)\ntrain.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"815e3a2e-d838-9248-14f7-bbdd17b66595"},"source":"**Correlation coefficient plot:**\n\nLet us look at the correlation of each of the variables with the target variables to get some important variables to be used for our next steps."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3746958b-a312-87f6-9658-b1dbf3e83f36"},"outputs":[],"source":"# Now let us look at the correlation coefficient of each of these variables #\nx_cols = [col for col in train.columns if col not in ['id','timestamp','y']]\n\nlabels = []\nvalues = []\nfor col in x_cols:\n    labels.append(col)\n    values.append(np.corrcoef(train[col].values, train.y.values)[0,1])\n    \nind = np.arange(len(labels))\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,40))\nrects = ax.barh(ind, np.array(values), color='y')\nax.set_yticks(ind+((width)/2.))\nax.set_yticklabels(labels, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation coefficient\")\n#autolabel(rects)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"d890c2d3-62d9-0d65-d17c-d701b4d46ef4"},"source":"As expected, the correlation coefficient values are very low and the maximum value is around 0.016 (in both positive and negative) as seen from the plot above.\n\nLet us take the top 4 variables from the plot above and do some more analysis on them alone.\n\n - technical_30\n - technical_20\n - fundamental_11\n - technical_19\n\nAs a first step, let us get the correlation coefficient in between these variables. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fe0d9c9c-ffc2-96e7-00ce-00575dfd36fb"},"outputs":[],"source":"cols_to_use = ['technical_30', 'technical_20', 'fundamental_11', 'technical_19']\n\ntemp_df = train[cols_to_use]\ncorrmat = temp_df.corr(method='spearman')\nf, ax = plt.subplots(figsize=(8, 8))\n\n# Draw the heatmap using seaborn\nsns.heatmap(corrmat, vmax=.8, square=True)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"43f48b6b-3e5b-68ee-153b-4f8e52d220c3"},"source":"There is some negative correlation between 'technical_30' and 'technical_20'. \n\nAs the next step, let us build simple linear regression models using these variables alone and see how they perform.\n\nLet us first build our models."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"462135e2-fe24-ac8d-4461-b064cbce4ad3"},"outputs":[],"source":"models_dict = {}\nfor col in cols_to_use:\n    model = lm.LinearRegression()\n    model.fit(np.array(train[col].values).reshape(-1,1), train.y.values)\n    models_dict[col] = model"},{"cell_type":"markdown","metadata":{"_cell_guid":"f47f3427-d931-1fce-4f30-f10885d7e297"},"source":"So we have built 4 univariate models using the train data.\n\n**Technical_30:**\n\nSo we will start predicting with the model using 'technical_30' variable."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"175e00f9-b912-a0ca-9ef4-e1938432fc41"},"outputs":[],"source":"col = 'technical_30'\nmodel = models_dict[col]\nwhile True:\n    observation.features.fillna(mean_values, inplace=True)\n    test_x = np.array(observation.features[col].values).reshape(-1,1)\n    observation.target.y = model.predict(test_x)\n    #observation.target.fillna(0, inplace=True)\n    target = observation.target\n    timestamp = observation.features[\"timestamp\"][0]\n    if timestamp % 100 == 0:\n        print(\"Timestamp #{}\".format(timestamp))\n        \n    observation, reward, done, info = env.step(target)\n    if done:\n        break\ninfo"},{"cell_type":"markdown","metadata":{"_cell_guid":"a580bb5f-6c54-0845-ef0d-6133eaef7b7a"},"source":"We are getting a public score of 0.011 using this variable.\n\n**Technical_20:**\n\nNow let us predict the test using our second univariate model which we have built."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8398a98e-e624-24ba-21b6-4408eaab4757"},"outputs":[],"source":"# Get first observation\nenv = kagglegym.make()\nobservation = env.reset()\n\ncol = 'technical_20'\nmodel = models_dict[col]\nwhile True:\n    observation.features.fillna(mean_values, inplace=True)\n    test_x = np.array(observation.features[col].values).reshape(-1,1)\n    observation.target.y = model.predict(test_x)\n    #observation.target.fillna(0, inplace=True)\n    target = observation.target\n    timestamp = observation.features[\"timestamp\"][0]\n    if timestamp % 100 == 0:\n        print(\"Timestamp #{}\".format(timestamp))\n        \n    observation, reward, done, info = env.step(target)\n    if done:\n        break\ninfo"},{"cell_type":"markdown","metadata":{"_cell_guid":"55d95eff-4ab2-a119-2117-9e08a6777d35"},"source":"Using 'technical_20' as input variable, we are getting a public score of 0.0169 which is slightly better than the previous one.\n\nSubmitting this model to the LB gave me a score of 0.006. I have exported the above script into a kernel and it can be accessed [here][1].  \n\nLet us do the same for our last two variables as well.\n\n**Fundamental_11:**\n\n\n  [1]: https://www.kaggle.com/sudalairajkumar/two-sigma-financial-modeling/univariate-model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d6f91212-8fe3-762c-d4ab-8c648e24dbfa"},"outputs":[],"source":"# Get first observation\nenv = kagglegym.make()\nobservation = env.reset()\n\ncol = 'fundamental_11'\nmodel = models_dict[col]\nwhile True:\n    observation.features.fillna(mean_values, inplace=True)\n    test_x = np.array(observation.features[col].values).reshape(-1,1)\n    observation.target.y = model.predict(test_x)\n    #observation.target.fillna(0, inplace=True)\n    target = observation.target\n    timestamp = observation.features[\"timestamp\"][0]\n    if timestamp % 100 == 0:\n        print(\"Timestamp #{}\".format(timestamp))\n        \n    observation, reward, done, info = env.step(target)\n    if done:\n        break\ninfo"},{"cell_type":"markdown","metadata":{"_cell_guid":"dae84c5d-3e9c-899f-ca37-4fdb373b74be"},"source":"**Technical_19:**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e6531aaf-dadf-77dd-c4f4-bec0e1eef347"},"outputs":[],"source":"# Get first observation\nenv = kagglegym.make()\nobservation = env.reset()\n\ncol = 'technical_19'\nmodel = models_dict[col]\nwhile True:\n    observation.features.fillna(mean_values, inplace=True)\n    test_x = np.array(observation.features[col].values).reshape(-1,1)\n    observation.target.y = model.predict(test_x)\n    #observation.target.fillna(0, inplace=True)\n    target = observation.target\n    timestamp = observation.features[\"timestamp\"][0]\n    if timestamp % 100 == 0:\n        print(\"Timestamp #{}\".format(timestamp))\n        \n    observation, reward, done, info = env.step(target)\n    if done:\n        break\ninfo"},{"cell_type":"markdown","metadata":{"_cell_guid":"c93b7f0a-20b1-9c0a-5ea2-2299eed59bd6"},"source":"**Regression using all 4 variables:**\n\nNow let us build multiple regression model using all these 4 variables."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c5d587be-c27d-4ad8-7c14-274aa89c86db"},"outputs":[],"source":"cols_to_use = ['technical_30', 'technical_20', 'fundamental_11', 'technical_19']\n\n# Get first observation\nenv = kagglegym.make()\nobservation = env.reset()\ntrain = observation.train\ntrain.fillna(mean_values, inplace=True)\n\nmodel = lm.LinearRegression()\nmodel.fit(np.array(train[cols_to_use]), train.y.values)\n\nwhile True:\n    observation.features.fillna(mean_values, inplace=True)\n    test_x = np.array(observation.features[cols_to_use])\n    observation.target.y = model.predict(test_x)\n    target = observation.target\n    timestamp = observation.features[\"timestamp\"][0]\n    if timestamp % 100 == 0:\n        print(\"Timestamp #{}\".format(timestamp))\n        \n    observation, reward, done, info = env.step(target)\n    if done:\n        break\ninfo"},{"cell_type":"markdown","metadata":{"_cell_guid":"21dffde0-614b-65da-255d-595786961ec0"},"source":"This multiple regression gave a score of 0.019 which is better than all univariate models. So probably submitting this model might give a better LB score.\n\n**Model with Clipping:**\n\nAs we can see from this [script][1] which gives the best public LB score of 0.00911, clipping the 'y' values help. \n\nSo let us dig a little deeper to see why the public LB score increased from 0.006 to 0.009 when we clip the 'y' values.\n\n  [1]: https://www.kaggle.com/bguberfain/two-sigma-financial-modeling/univariate-model-with-clip/run/482189/code"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0461ba59-efd2-f31c-3f2a-02ff17dc5433"},"outputs":[],"source":"print(\"Max y value in train : \",train.y.max())\nprint(\"Min y value in train : \",train.y.min())\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"d0d7c4ec-31ac-fe95-0f29-6ec4b626c32d"},"source":"Let us now do the clipping and see the number of rows that will be discarded from the training. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"213232bf-3053-56f5-d424-e113ff2a5ee4"},"outputs":[],"source":"low_y_cut = -0.086093\nhigh_y_cut = 0.093497\n\ny_is_above_cut = (train.y > high_y_cut)\ny_is_below_cut = (train.y < low_y_cut)\ny_is_within_cut = (~y_is_above_cut & ~y_is_below_cut)\ny_is_within_cut.value_counts()"},{"cell_type":"markdown","metadata":{"_cell_guid":"214c5a7d-b94b-dc62-f7b1-7e81fe4b1599"},"source":"So there are 9418 rows in the training set that lie between (-0.086093 and -0.0860941) and (0.093497 and 0.0934978) in the training set. So many values in such a small range.\n\nAs we can see from [anokas script][1], the distribution of 'y' values have two small spikes at both the ends. Probably values which are higher than these values are clipped in the training data and so not using these rows in our model building might be a good idea.\n\n\n\nNow let us re-train our model (using technical_20) by excluding these rows from the training.\n\n\n  [1]: https://www.kaggle.com/anokas/two-sigma-financial-modeling/two-sigma-time-travel-eda"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cc707a6b-c33e-da83-e631-2861b3659eec"},"outputs":[],"source":"# Get first observation\nenv = kagglegym.make()\nobservation = env.reset()\n\ncol = 'technical_20'\nmodel = lm.LinearRegression()\nmodel.fit(np.array(train.loc[y_is_within_cut, col].values).reshape(-1,1), train.loc[y_is_within_cut, 'y'])\n\nwhile True:\n    observation.features.fillna(mean_values, inplace=True)\n    test_x = np.array(observation.features[col].values).reshape(-1,1)\n    observation.target.y = model.predict(test_x).clip(low_y_cut, high_y_cut)\n    #observation.target.fillna(0, inplace=True)\n    target = observation.target\n    timestamp = observation.features[\"timestamp\"][0]\n    if timestamp % 100 == 0:\n        print(\"Timestamp #{}\".format(timestamp))\n        \n    observation, reward, done, info = env.step(target)\n    if done:\n        break\ninfo"},{"cell_type":"markdown","metadata":{"_cell_guid":"ef4c89b0-66d5-3fd4-3760-2dcf2995ca9b"},"source":"So we got almost same public score of 0.0169 with clip.\n\nBut on the leaderboard, we are getting some improvement in the score from 0.006 to 0.009. \n\nHope this gives a good starting point for building models. Happy Kaggling under new environment.!"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}