{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"20912b0b-4f74-a799-cc81-73bb528f5c15"},"source":"From the univariate analysis notebook, \n\nhttps://www.kaggle.com/sudalairajkumar/two-sigma-financial-modeling/univariate-analysis-regression-lb-0-006/notebook\n\nit seems we are getting a good score if we use the variable 'technical_20' and build a model. However if we use the top 4 variables and build a model, we are getting a negative public score though our local validation score improved to 0.0195. So how do we tackle this?\n\nAlso from the kagglegym api, we get the rewards (R value) for individual timestamps and finally for the whole test data set. These individual rewards are very hard to make sense since it is highly influenced by the mean target value of the given timestamp. \n\nSo I thought probably **looking at the cumulative R value till the given timestamp** might help a little more (as mean target values are averaged over all the points till the given timestamp) to build a more robust model and here is an attempt to understand the performance of some of the top public scripts.\n\nKindly upvote if you find this useful and / or share your comments. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7ada3de0-9a24-776f-072c-6382712ebb53"},"outputs":[],"source":"# Import all the necessary packages \nimport kagglegym\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge\nimport math\nimport matplotlib.pyplot as plt\n\n%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"51af1788-a62c-cec9-0847-dc6ce26a09f4"},"outputs":[],"source":"# Read the full data set stored as HDF5 file\nfull_df = pd.read_hdf('../input/train.h5')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6209a6ea-e558-1d3f-d054-e89dfdf35076"},"outputs":[],"source":"# A custom function to compute the R score\ndef get_reward(y_true, y_fit):\n    R2 = 1 - np.sum((y_true - y_fit)**2) / np.sum((y_true - np.mean(y_true))**2)\n    R = np.sign(R2) * math.sqrt(abs(R2))\n    return(R)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b28ca150-e60a-05df-e10a-98e43d31efaf"},"outputs":[],"source":"# Some pre-processing as seen from most of the public scripts.\n# The \"environment\" is our interface for code competitions\nenv = kagglegym.make()\n\n# We get our initial observation by calling \"reset\"\nobservation = env.reset()\ntarget_var = 'y'\n\n# Get the train dataframe\ntrain = observation.train\nmean_values = train.median(axis=0)\ntrain.fillna(mean_values, inplace=True)\n\n# Observed with histograns:\nlow_y_cut = -0.086093\nhigh_y_cut = 0.093497\n\ny_is_above_cut = (train.y > high_y_cut)\ny_is_below_cut = (train.y < low_y_cut)\ny_is_within_cut = (~y_is_above_cut & ~y_is_below_cut)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d685b2d3-2a74-bb62-5449-d727b2db0862"},"source":"**Ridge Regression:**\n\nThe base script can be seen here\n\nhttps://www.kaggle.com/ymcdull/two-sigma-financial-modeling/ridge-lb-0-0100659/\n\nWe shall first run a univariate ridge regression using 'technical_20' variable alone and see how the cumulative R value changes over time. \n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e5517572-05e6-1f13-ccb5-5913a3c45995"},"outputs":[],"source":"### https://www.kaggle.com/ymcdull/two-sigma-financial-modeling/ridge-lb-0-0100659/run/545100\n# The \"environment\" is our interface for code competitions\nenv = kagglegym.make()\n\n# We get our initial observation by calling \"reset\"\nobservation = env.reset()\n\n# cols_to_use for ridge model\n#cols_to_use = ['technical_30', 'technical_20', 'fundamental_11']\ncols_to_use = ['technical_20']\n\n# model build\nmodel = Ridge()\nmodel.fit(np.array(train.loc[y_is_within_cut, cols_to_use].values), train.loc[y_is_within_cut, target_var])\n\n# getting the y mean dict for averaging\nymean_dict = dict(train.groupby([\"id\"])[\"y\"].mean())\n\n# weighted average of model & mean\ndef get_weighted_y(series):\n    id, y = series[\"id\"], series[\"y\"]\n    return 0.95 * y + 0.05 * ymean_dict[id] if id in ymean_dict else y\n\ny_actual_list = []\ny_pred_list = []\nr1_overall_reward_list = []\nts_list = []\nwhile True:\n    timestamp = observation.features[\"timestamp\"][0]\n    actual_y = list(full_df[full_df[\"timestamp\"] == timestamp][\"y\"].values)\n    observation.features.fillna(mean_values, inplace=True)\n    test_x = np.array(observation.features[cols_to_use].values)\n    observation.target.y = model.predict(test_x).clip(low_y_cut, high_y_cut)\n    \n    ## weighted y using average value\n    observation.target.y = observation.target.apply(get_weighted_y, axis = 1)\n    target = observation.target\n    observation, reward, done, info = env.step(target)\n    \n    if timestamp % 100 == 0:\n        print(\"Timestamp #{}\".format(timestamp))\n    \n    pred_y = list(target.y.values)\n    y_actual_list.extend(actual_y)\n    y_pred_list.extend(pred_y)\n    overall_reward = get_reward(np.array(y_actual_list), np.array(y_pred_list))\n    r1_overall_reward_list.append(overall_reward)\n    ts_list.append(timestamp)\n    if done:\n        break\n    \nprint(info)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c01dc288-7c28-82a0-3157-b29cdad7ab22"},"source":"We are getting a final R score of 0.01751 using this model. Now let us look at how the cumulative R value changes over time by plotting it along with zero-line."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"17407ebd-300f-660d-b487-638b0d0f287d"},"outputs":[],"source":"fig = plt.figure(figsize=(12, 6))\nplt.plot(ts_list, r1_overall_reward_list, c='blue')\nplt.plot(ts_list, [0]*len(ts_list), c='red')\nplt.title(\"Cumulative R value change for Univariate Ridge (technical_20)\")\nplt.ylim([-0.04,0.04])\nplt.xlim([850, 1850])\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"0ded26ed-d1b1-591f-fbb3-427bfc8b4435"},"source":"As we could clearly see, the cumulative R values are not positive throughout. \n\nEven after initial ~400 timestamps (meaning we have considerable amount of data), the cumulative R value went below the zero mark (I think this is due to the nature of the data we are dealing with). And finally the cumulative R values went above zero mark and stayed there till the end. \n\nSo there is a possibility of R value ending up less than 0 in the private test set.! \n\nNow let us use two variables and build the ridge model.\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da0fbc7f-22a3-f0f2-9208-7a8d91fdf1b0"},"outputs":[],"source":"### https://www.kaggle.com/ymcdull/two-sigma-financial-modeling/ridge-lb-0-0100659/run/545100\n# The \"environment\" is our interface for code competitions\nenv = kagglegym.make()\n\n# We get our initial observation by calling \"reset\"\nobservation = env.reset()\n\n# cols_to_use for ridge model\n#cols_to_use = ['technical_30', 'technical_20', 'fundamental_11']\ncols_to_use = ['technical_30', 'technical_20']\n\n# model build\nmodel = Ridge()\nmodel.fit(np.array(train.loc[y_is_within_cut, cols_to_use].values), train.loc[y_is_within_cut, target_var])\n\n# getting the y mean dict for averaging\nymean_dict = dict(train.groupby([\"id\"])[\"y\"].mean())\n\n# weighted average of model & mean\ndef get_weighted_y(series):\n    id, y = series[\"id\"], series[\"y\"]\n    return 0.95 * y + 0.05 * ymean_dict[id] if id in ymean_dict else y\n\ny_actual_list = []\ny_pred_list = []\nr2_overall_reward_list = []\nts_list = []\nwhile True:\n    timestamp = observation.features[\"timestamp\"][0]\n    actual_y = list(full_df[full_df[\"timestamp\"] == timestamp][\"y\"].values)\n    observation.features.fillna(mean_values, inplace=True)\n    test_x = np.array(observation.features[cols_to_use].values)\n    observation.target.y = model.predict(test_x).clip(low_y_cut, high_y_cut)\n    \n    ## weighted y using average value\n    observation.target.y = observation.target.apply(get_weighted_y, axis = 1)\n    target = observation.target\n    observation, reward, done, info = env.step(target)\n    \n    if timestamp % 100 == 0:\n        print(\"Timestamp #{}\".format(timestamp))\n    \n    pred_y = list(target.y.values)\n    y_actual_list.extend(actual_y)\n    y_pred_list.extend(pred_y)\n    overall_reward = get_reward(np.array(y_actual_list), np.array(y_pred_list))\n    r2_overall_reward_list.append(overall_reward)\n    ts_list.append(timestamp)\n    if done:\n        break\n    \nprint(info)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d0086e17-f5a4-27a1-ab18-09d9b3dde7cc"},"source":"The final R value of 0.019 is better than the previous one of 0.017\n\nWe can plot both the previous model and the current model to see how the cumulative R value changes for both the models."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"35afe1af-6725-25d6-b3f7-9d036d236f4b"},"outputs":[],"source":"fig, ax = plt.subplots(figsize=(12, 6))\nax.plot(ts_list, r2_overall_reward_list, c='green', label='ridge-2')\nax.plot(ts_list, r1_overall_reward_list, c='blue', label='ridge-1')\nax.plot(ts_list, [0]*len(ts_list), c='red', label='zero line')\nax.legend(loc='lower right')\nax.set_ylim([-0.04,0.04])\nax.set_xlim([850, 1850])\nplt.title(\"Cumulative R value change for Bivariate Ridge\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"dcd6231d-48f7-d694-eb12-b01a02e12786"},"source":"There is not much of a change in the trend for both the models as we can see. \n\nIn most places, when the R value is above zero mark, bivariate ridge gave better scores than univariate one. On the contrary, when the scores are less than 0, univariate gave better scores than bivariate. \n\nNow let us add the third variable 'fundamental_11' to our ridge model (public LB score 0.010) and see the results. \nhttps://www.kaggle.com/ymcdull/two-sigma-financial-modeling/ridge-lb-0-0100659/run/545656"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"51e09de0-8a2d-1c7d-f40a-acaf8ee3e1fd"},"outputs":[],"source":"### https://www.kaggle.com/ymcdull/two-sigma-financial-modeling/ridge-lb-0-0100659/run/545100\n# The \"environment\" is our interface for code competitions\nenv = kagglegym.make()\n\n# We get our initial observation by calling \"reset\"\nobservation = env.reset()\n\n# cols_to_use for ridge model\ncols_to_use = ['technical_30', 'technical_20', 'fundamental_11']\n\n# model build\nmodel = Ridge()\nmodel.fit(np.array(train.loc[y_is_within_cut, cols_to_use].values), train.loc[y_is_within_cut, target_var])\n\n# getting the y mean dict for averaging\nymean_dict = dict(train.groupby([\"id\"])[\"y\"].mean())\n\n# weighted average of model & mean\ndef get_weighted_y(series):\n    id, y = series[\"id\"], series[\"y\"]\n    return 0.95 * y + 0.05 * ymean_dict[id] if id in ymean_dict else y\n\ny_actual_list = []\ny_pred_list = []\nr3_overall_reward_list = []\nts_list = []\nwhile True:\n    timestamp = observation.features[\"timestamp\"][0]\n    actual_y = list(full_df[full_df[\"timestamp\"] == timestamp][\"y\"].values)\n    observation.features.fillna(mean_values, inplace=True)\n    test_x = np.array(observation.features[cols_to_use].values)\n    observation.target.y = model.predict(test_x).clip(low_y_cut, high_y_cut)\n    \n    ## weighted y using average value\n    observation.target.y = observation.target.apply(get_weighted_y, axis = 1)\n    target = observation.target\n    observation, reward, done, info = env.step(target)\n    \n    if timestamp % 100 == 0:\n        print(\"Timestamp #{}\".format(timestamp))\n    \n    pred_y = list(target.y.values)\n    y_actual_list.extend(actual_y)\n    y_pred_list.extend(pred_y)\n    overall_reward = get_reward(np.array(y_actual_list), np.array(y_pred_list))\n    r3_overall_reward_list.append(overall_reward)\n    ts_list.append(timestamp)\n    if done:\n        break\n    \nprint(info)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0f8bf0e7-7e54-3396-1368-99aca0a178f6"},"source":"There is a very slight improvement in the validation R score from 0.01940 0.01946.\n\nNow let us look at the cumulative R score plots of both the models as well."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"282699a4-1ccf-82f4-75a0-0a372254dfbb"},"outputs":[],"source":"fig, ax = plt.subplots(figsize=(12, 6))\nax.plot(ts_list, r3_overall_reward_list, c='brown', label='ridge-3')\nax.plot(ts_list, r2_overall_reward_list, c='green', label='ridge-2')\nax.plot(ts_list, [0]*len(ts_list), c='red', label='zero line')\nax.legend(loc='lower right')\nax.set_ylim([-0.04,0.04])\nax.set_xlim([850, 1850])\nplt.title(\"Cumulative R value change for Trivariate Ridge\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"290b68e5-44a8-64b4-d600-53b7326afc67"},"source":"As we could see, both these lines go together most of the times with trivariate score marginally below bivariate line in some cases. \n\nFrom the above three models, we can see that the trend remains same for all three models (of course all belong to same model family). \n\nThanks to @the1owl, we got a great script which combines Extra trees with ridge to get a good score in the public LB \nhttps://www.kaggle.com/the1owl/two-sigma-financial-modeling/initial-script\n\nLet us build the extra trees model and see how the cumulative R value changes. We shall use 25 trees instead of 100 just for the sake of run time.\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d93a6a0a-35d4-bbe7-dfb0-a96a052bedf9"},"outputs":[],"source":"env = kagglegym.make()\no = env.reset()\n\nexcl = [env.ID_COL_NAME, env.SAMPLE_COL_NAME, env.TARGET_COL_NAME, env.TIME_COL_NAME]\ncol = [c for c in o.train.columns if c not in excl]\n\ntrain = o.train[col]\nd_mean= train.median(axis=0)\n\ntrain = o.train[col]\nn = train.isnull().sum(axis=1)\nfor c in train.columns:\n    train[c + '_nan_'] = pd.isnull(train[c])\n    d_mean[c + '_nan_'] = 0\ntrain = train.fillna(d_mean)\ntrain['znull'] = n\n\nprint(\"Building ET..\")\nmodel_et = ExtraTreesRegressor(n_estimators=25, max_depth=4, n_jobs=-1, random_state=123, verbose=0)\nmodel_et.fit(train, o.train['y'])\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"41b5f01e-c8b2-e6fc-88bf-cc31b42b8f4d"},"outputs":[],"source":"env = kagglegym.make()\no = env.reset()\n\n#https://www.kaggle.com/ymcdull/two-sigma-financial-modeling/ridge-lb-0-0100659\nymean_dict = dict(o.train.groupby([\"id\"])[\"y\"].median())\ndef get_weighted_y(series):\n    id, y = series[\"id\"], series[\"y\"]\n    return 0.95 * y + 0.05 * ymean_dict[id] if id in ymean_dict else y\n\ny_actual_list = []\ny_pred_list = []\net_overall_reward_list = []\nts_list = []\nwhile True:\n    timestamp = o.features[\"timestamp\"][0]\n    actual_y = list(full_df[full_df[\"timestamp\"] == timestamp][\"y\"].values)\n    \n    test = o.features[col]\n    n = test.isnull().sum(axis=1)\n    for c in test.columns:\n        test[c + '_nan_'] = pd.isnull(test[c])\n    test = test.fillna(d_mean)\n    test['znull'] = n\n    \n    pred = o.target\n    pred['y'] = model_et.predict(test).clip(low_y_cut, high_y_cut) \n    pred['y'] = pred.apply(get_weighted_y, axis = 1)\n    o, reward, done, info = env.step(pred[['id','y']])\n    \n    pred_y = list(pred.y.values)\n    y_actual_list.extend(actual_y)\n    y_pred_list.extend(pred_y)\n    overall_reward = get_reward(np.array(y_actual_list), np.array(y_pred_list))\n    et_overall_reward_list.append(overall_reward)\n    ts_list.append(timestamp)\n    \n    if timestamp % 100 == 0:\n        print(\"Timestamp #{}\".format(timestamp))\n    \n    if done:\n        print(info)\n        break"},{"cell_type":"markdown","metadata":{"_cell_guid":"ee2eb4f8-3e4c-70f2-5356-e717c910a75a"},"source":"Let us look at how the R value of the Extra trees model along with univariate ridge model (since univariate ridge is used in best public script)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"55c1b4e9-b408-9813-99f7-b7d3a32911ae"},"outputs":[],"source":"fig, ax = plt.subplots(figsize=(12, 6))\nax.plot(ts_list, et_overall_reward_list, c='blue', label='Extra Trees')\nax.plot(ts_list, r1_overall_reward_list, c='green', label='Ridge-1')\nax.plot(ts_list, [0]*len(ts_list), c='red', label='zero line')\nax.legend(loc='lower right')\nax.set_ylim([-0.04,0.04])\nax.set_xlim([850, 1850])\nplt.title(\"Cumulative R value change for Extra Trees\")\n\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"9e31730a-871d-5e71-4151-35dfd9ebfc36"},"source":"The macro trend remains almost similar. R value of both the models dip below 0 in the intermediate time stamps and is above zero towards the end.\n\nBut we can also clearly notice that there is some variability in the Extra Tress model when compared with the ridge model. I think this helps in the model prediction when both are combined together. I also think this is the reason we are getting an improvement in the public LB score. \n\n**Inferences:**\n\nIf we could build a model which predicts good for the sub-zero R value timestamps (my hunch says it is hard though) and combine it with our existing best performing public models, it may be a good way to hedge the risk of overfitting.\n\nCombining many models together might also be helpful to combat the overfitting risk (as it is proved by the best public script as well).\n\nHappy Kaggling. Thank you.!"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}