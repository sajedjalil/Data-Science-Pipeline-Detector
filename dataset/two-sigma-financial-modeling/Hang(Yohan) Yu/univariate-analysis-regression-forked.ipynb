{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"79bc1341-0357-bd3b-f0fd-923783e8cb2f"},"source":"Let us do some univariate analysis in this notebook and build simple regression models."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"970e3273-0596-1ae2-f41b-8bd4f383e11f"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import linear_model as lm\nimport kagglegym\n\n%matplotlib inline"},{"cell_type":"markdown","metadata":{"_cell_guid":"8ab0f76f-6e12-71b6-64d2-f42ec4d88afa"},"source":"Read the train file from Kaggle gym."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f2400f0b-6036-8bcc-8b40-d5b121dc9e67"},"outputs":[],"source":"# Create environment\nenv = kagglegym.make()\n\n# Get first observation\nobservation = env.reset()\n\n# Get the train dataframe\ntrain = observation.train"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ac653533-ef0e-ff32-141e-d29fad827240"},"outputs":[],"source":"train.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e5f46aaa-dc6b-b166-5ea5-af9f49566724"},"outputs":[],"source":"mean_values = train.mean(axis=0)\ntrain.fillna(mean_values, inplace=True)\ntrain.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"815e3a2e-d838-9248-14f7-bbdd17b66595"},"source":"**Correlation coefficient plot:**\n\nLet us look at the correlation of each of the variables with the target variables to get some important variables to be used for our next steps."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3746958b-a312-87f6-9658-b1dbf3e83f36"},"outputs":[],"source":"# Now let us look at the correlation coefficient of each of these variables #\nx_cols = [col for col in train.columns if col not in ['id','timestamp','y']]\n\nlabels = []\nvalues = []\nfor col in x_cols:\n    labels.append(col)\n    values.append(np.corrcoef(train[col].values, train.y.values)[0,1])\n    \nind = np.arange(len(labels))\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,40))\nrects = ax.barh(ind, np.array(values), color='y')\nax.set_yticks(ind+((width)/2.))\nax.set_yticklabels(labels, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation coefficient\")\n#autolabel(rects)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"d890c2d3-62d9-0d65-d17c-d701b4d46ef4"},"source":"As expected, the correlation coefficient values are very low and the maximum value is around 0.016 (in both positive and negative) as seen from the plot above.\n\nLet us take the top 4 variables from the plot above and do some more analysis on them alone.\n\n - technical_30\n - technical_20\n - fundamental_11\n - technical_19\n\nAs a first step, let us get the correlation coefficient in between these variables. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f42836ef-d4c6-09c7-324b-37ffd6bf296d"},"outputs":[],"source":"a = [abs(x) for x in values]\n#np.sort(a)[::-1]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"69bdfbce-b6b6-8f7b-af83-410c2b2380c5"},"outputs":[],"source":"cols_to_use = []\nfor  i in np.argsort(a)[::-1][:20]:\n    cols_to_use.append(labels[i])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"031f724b-b3fc-245e-c94d-7a176c2b45b1"},"outputs":[],"source":"cols_to_use"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fe0d9c9c-ffc2-96e7-00ce-00575dfd36fb"},"outputs":[],"source":"#cols_to_use = ['technical_30', 'technical_20', 'fundamental_11', 'technical_19']\n\ntemp_df = train[cols_to_use]\ncorrmat = temp_df.corr()\nf, ax = plt.subplots(figsize=(8, 8))\n\n# Draw the heatmap using seaborn\nsns.heatmap(corrmat, vmax=.8, square=True)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"43f48b6b-3e5b-68ee-153b-4f8e52d220c3"},"source":"There is some negative correlation between 'technical_30' and 'technical_20'. \n\nAs the next step, let us build simple linear regression models using these variables alone and see how they perform.\n\nLet us first build our models."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"462135e2-fe24-ac8d-4461-b064cbce4ad3"},"outputs":[],"source":"models_dict = {}\nfor col in cols_to_use:\n    model = lm.LinearRegression()\n    model.fit(np.array(train[col].values).reshape(-1,1), train.y.values)\n    models_dict[col] = model"},{"cell_type":"markdown","metadata":{"_cell_guid":"f47f3427-d931-1fce-4f30-f10885d7e297"},"source":"So we have built 4 univariate models using the train data.\n\n**Technical_30:**\n\nSo we will start predicting with the model using 'technical_30' variable."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"175e00f9-b912-a0ca-9ef4-e1938432fc41"},"outputs":[],"source":"col = 'technical_30'\nmodel = models_dict[col]\nwhile True:\n    observation.features.fillna(mean_values, inplace=True)\n    test_x = np.array(observation.features[col].values).reshape(-1,1)\n    observation.target.y = model.predict(test_x)\n    #observation.target.fillna(0, inplace=True)\n    target = observation.target\n    timestamp = observation.features[\"timestamp\"][0]\n    if timestamp % 100 == 0:\n        print(\"Timestamp #{}\".format(timestamp))\n        \n    observation, reward, done, info = env.step(target)\n    if done:\n        break\ninfo"},{"cell_type":"markdown","metadata":{"_cell_guid":"a580bb5f-6c54-0845-ef0d-6133eaef7b7a"},"source":"We are getting a public score of 0.011 using this variable.\n\n**Technical_20:**\n\nNow let us predict the test using our second univariate model which we have built."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8398a98e-e624-24ba-21b6-4408eaab4757"},"outputs":[],"source":"# Get first observation\nobservation = env.reset()\n\ncol = 'technical_20'\nmodel = models_dict[col]\nwhile True:\n    observation.features.fillna(mean_values, inplace=True)\n    test_x = np.array(observation.features[col].values).reshape(-1,1)\n    observation.target.y = model.predict(test_x)\n    #observation.target.fillna(0, inplace=True)\n    target = observation.target\n    timestamp = observation.features[\"timestamp\"][0]\n    if timestamp % 100 == 0:\n        print(\"Timestamp #{}\".format(timestamp))\n        \n    observation, reward, done, info = env.step(target)\n    if done:\n        break\ninfo"},{"cell_type":"markdown","metadata":{"_cell_guid":"55d95eff-4ab2-a119-2117-9e08a6777d35"},"source":"Using 'technical_20' as input variable, we are getting a public score of 0.014 which is slightly better than the previous one.\n\nSubmitting this model to the LB gave me a score of 0.006. I have exported the above script into a kernel and it can be accessed [here][1].  \n\nLet us do the same for our last two variables as well.\n\n**Fundamental_11:**\n\n\n  [1]: https://www.kaggle.com/sudalairajkumar/two-sigma-financial-modeling/univariate-model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d6f91212-8fe3-762c-d4ab-8c648e24dbfa"},"outputs":[],"source":"# Get first observation\nobservation = env.reset()\n\ncol = 'fundamental_11'\nmodel = models_dict[col]\nwhile True:\n    observation.features.fillna(mean_values, inplace=True)\n    test_x = np.array(observation.features[col].values).reshape(-1,1)\n    observation.target.y = model.predict(test_x)\n    #observation.target.fillna(0, inplace=True)\n    target = observation.target\n    timestamp = observation.features[\"timestamp\"][0]\n    if timestamp % 100 == 0:\n        print(\"Timestamp #{}\".format(timestamp))\n        \n    observation, reward, done, info = env.step(target)\n    if done:\n        break\ninfo"},{"cell_type":"markdown","metadata":{"_cell_guid":"dae84c5d-3e9c-899f-ca37-4fdb373b74be"},"source":"**Technical_19:**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e6531aaf-dadf-77dd-c4f4-bec0e1eef347"},"outputs":[],"source":"# Get first observation\nobservation = env.reset()\n\ncol = 'technical_19'\nmodel = models_dict[col]\nwhile True:\n    observation.features.fillna(mean_values, inplace=True)\n    test_x = np.array(observation.features[col].values).reshape(-1,1)\n    observation.target.y = model.predict(test_x)\n    #observation.target.fillna(0, inplace=True)\n    target = observation.target\n    timestamp = observation.features[\"timestamp\"][0]\n    if timestamp % 100 == 0:\n        print(\"Timestamp #{}\".format(timestamp))\n        \n    observation, reward, done, info = env.step(target)\n    if done:\n        break\ninfo"},{"cell_type":"markdown","metadata":{"_cell_guid":"c93b7f0a-20b1-9c0a-5ea2-2299eed59bd6"},"source":"**Regression using all 4 variables:**\n\nNow let us build multiple regression model using all these 4 variables."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c5d587be-c27d-4ad8-7c14-274aa89c86db"},"outputs":[],"source":"cols_to_use = ['technical_30', 'technical_20', 'fundamental_11', 'technical_19']\n\n# Get first observation\nenv = kagglegym.make()\nobservation = env.reset()\ntrain = observation.train\ntrain.fillna(mean_values, inplace=True)\n\nmodel = lm.LinearRegression()\nmodel.fit(np.array(train[cols_to_use]), train.y.values)\n\nwhile True:\n    observation.features.fillna(mean_values, inplace=True)\n    test_x = np.array(observation.features[cols_to_use])\n    observation.target.y = model.predict(test_x)\n    target = observation.target\n    timestamp = observation.features[\"timestamp\"][0]\n    if timestamp % 100 == 0:\n        print(\"Timestamp #{}\".format(timestamp))\n        \n    observation, reward, done, info = env.step(target)\n    if done:\n        break\ninfo"},{"cell_type":"markdown","metadata":{"_cell_guid":"21dffde0-614b-65da-255d-595786961ec0"},"source":"This multiple regression gave a score of 0.019 which is better than all univariate models. So probably submitting this model might give a better LB score.\n\nHope this gives a good starting point for building models. Happy Kaggling under new environment.!"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}