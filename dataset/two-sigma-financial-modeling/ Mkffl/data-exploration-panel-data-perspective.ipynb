{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"5e08bce8-482e-9032-bed7-5e8845af7f93"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"30aa16f6-2087-0859-bf5f-fb0f6b1dadde"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_columns', 500)\npd.options.display.max_rows = 1000"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a4f2d60d-3722-21be-33d3-845f12ef66ee"},"outputs":[],"source":"hdf = pd.read_hdf('../input/train.h5')\nprint(len(hdf[\"id\"].unique()))\nhdf.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"886bf1c9-504a-1a3f-2d13-13a0a90d7154"},"source":"Note: Plotting the 1,424 ids may result in messy charts and never-ending facetgrids that take for ever to load. Best to sample a few ids used consistently throughout the notebook, and run the analyses for a few samples before making final conclusions."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c716a077-109a-ad95-edbf-c1a41090d9f1"},"outputs":[],"source":"n_samples = 200\ndf_sample = pd.DataFrame({\"sample_id\": pd.Series(hdf[\"id\"].unique()).sample(n_samples) })\ndf_sample = df_sample.reset_index(drop=True).reset_index(drop=False)\ndf_sample[\"group\"] = df_sample[\"index\"].apply(lambda x: int((x+1)/10)) # Create groups of 10 sample ids.\nhdf = pd.merge(left = hdf, right = df_sample[[\"sample_id\", \"group\"]], left_on=\"id\", right_on=\"sample_id\", how = \"left\")\ndf_sample.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d943337f-90ba-736e-b6ea-f8c058664a1b"},"outputs":[],"source":"# Look at \"y\" for a random subset of n_samples ids\nhdf_plot = hdf.loc[~hdf[\"sample_id\"].isnull(), [\"sample_id\", \"timestamp\", \"y\"]]\ngrid = sns.FacetGrid( hdf_plot, col = \"sample_id\", col_wrap=5)\ngrid = (grid.map(plt.plot, \"timestamp\", \"y\")).add_legend()\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"91c94129-7ce2-1634-5061-031631b1c748"},"outputs":[],"source":"# Look at all feature columns for 10 sampled ids\nsample_id_group = 1\ncols_included = list(hdf)[1:]\nhdf_melt = hdf.loc[ (~hdf[\"sample_id\"].isnull()) & (hdf[\"group\"] == sample_id_group), cols_included]\nhdf_melt = pd.melt (hdf_melt, id_vars=[\"timestamp\",\"sample_id\"])\nhdf_melt.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5ae87c1b-422b-5775-cad9-2fad07e2ad1e"},"outputs":[],"source":"grid = sns.FacetGrid( hdf_melt, col = \"variable\", col_wrap=5, hue  =\"sample_id\", ylim = [-2, 2]) # useful to clip values as suggested by other people\ngrid = (grid.map(plt.plot, \"timestamp\", \"value\")).add_legend()\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"290c338c-3d60-e75b-5089-9cc56a0593f2"},"source":"Comments:\n\na) on \"y\"\n- seems to oscillate around the zero mean, with a stationary pattern at least until time period c.1500 \n- differs across ids in terms of scale\n- differs across ids in terms of time periods available and general pattern. For example some ids show large volatility around time=1500 (for ex id n. 2140) while other don't (for example id n. 1419)\n\nb) on feature columns\n- Many features like fundamental_1 are clearly non stationary at least for some ids\n- Technical_x features have some sort of weird sinusoidal shape with a binary scale. For example technical_43 seems to be either 0 or -2\n- Within one feature, ids may differ greatly in terms of scale\n- zooming in on shorter time periods shows some sort of seasonality for some features (not done here). De-trending could be a good idea\n- Inconsistent time periods available across features for a given id. Note: some kernels have dealt with it by simply filling in missing values with medial or mean. Not sure if that's a good idea\n\nMany kernels related to this competition involve fitting linear regressions on all the instruments (ids) and all the timestamps (time periods). Whether the models used are simple, ridge or LASSO regressions, they would perform best under a number of assumptions, including that all id-period observations are independent and identically distributed (iid), which may not apply to this dataset.\n\nOne reason why id-period observations may not be iid is because of specific, unobserved id-level characteristics that a regression model trained across all id-periods in a \"pooled\" fashion would not be able to capture. The problem caused by unobserved differences (also called unobserved heterogeneity) may be avoided by training separate models for each id. \n\nBut modeling ids separately would not leverage the relationships that are constant across all ids, which a model spanning all id-period observations can capture. Also some ids have very little time periods available across any features, which means they could benefit from what can be learnt using other ids. \n\nSo a solution could be to ensemble id-specific regressions with panel data regressions that account for unobserved heterogeneity. An example of an ensembling-based solution could be a weighted score of two models, one that is id specific and one that spans several ids. The weight of the latter model would be high for Ids with little time periods available.\n\nIn the following cells I look at the problemm from a panel data perspective, i.e. considering the availability and distribution of time periods by id. The ultimate questions is \"Is there any reason why I should restricted my regression to a subset of ids, for example because their features span a consistent number of time range or because they have data available across q consistent subset of columns?\"."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ea964747-25e3-7230-643e-eeb52632b577"},"outputs":[],"source":"# Set an index with two dimensions, id and timestamp. Useful for subsequent computations\nhdf = hdf.set_index([\"id\", \"timestamp\"])\nhdf = hdf.sort_index(level = 1).sort_index(level = 0)\n\n# Number of periods available by id\nt=\"Number of periods available by id\"\nres = hdf.groupby(level = 0)[\"y\"].apply(lambda x: len(x[~x.isnull()])) # Note: some ids have null rows that would be counted if not removed\nres.plot(kind = 'hist', bins = 100, title = t)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"68c27f80-5cc7-8254-39da-f0fa6ef040db"},"source":"Over 500 ids have the maximum number of available observations for y (1812). That leaves c.900 ids with some gaps. If that's because they were created after timestamp=0 that's fine - there will still be a continuous set of observations. But if it's because specific time ranges are missing, that could be a problem. I am specifically worried that some ids may  have missing observations for time periods 1500-last. Volatility tends to increase at this point in time and predictions likely apply to timestamps beyond 1812 so it's important to accurately model relationships for this time period. Therefore, worth looking at the maximum time period available by id - is max timestamp mostly within 1500-1812?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5c318a8f-33fc-1066-b4b4-6ed937cb336a"},"outputs":[],"source":"res = hdf.groupby(level = 0)[\"y\"].apply(lambda x: x.index.max()[1] )\nres.plot(kind = 'hist', bins = 100, title = \"Maximum time period for 'y' column by id\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"00940661-5bc1-0205-551a-5e3e0b74841e"},"source":"About 1100 ids have their last observation of y in time period = 1812, which is the maximum timestamp. That's reassuring. Let's look at the c.300 other ids"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9ab86a89-a82f-901f-7258-7e68adba85f8"},"outputs":[],"source":"# How about ids that end before 1812, do many of them end early on?\nres[res < 1812].plot(kind = 'hist', bins = 4)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"1ca783c9-3fda-6803-dbb6-eedbc3e43f94"},"source":"About 250 ids end before timestamp 1400. As suggested previously, these will most benefit from what can be learnt using ids with recent time periods.\n\nNext plot looks at the number of empty columns. If most ids have only a few columns available then we would have to see if those columns tend to be the same across ids and hope that it's the case, otherwise a model that spans multiple ids will be no better than a collection of id-specifif models"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c2985f15-9e6a-f128-8d7c-82e2afdee0a6"},"outputs":[],"source":"def number_empty_columns(df):\n    df_nulls = df.apply(lambda col: (pd.isnull(col)*1))\n    df_nulls = df_nulls.sum(axis = 0)\n    count = ((df_nulls == 0)*1).sum()\n    return count\n\nres = hdf.groupby(level = 0).apply(number_empty_columns)\nres.plot(kind = 'hist', bins = 100, title = \"Number of empty columns by id\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"fcfb074a-10a2-8443-2815-987df9d08eb8"},"source":"Close to 700 ids have only a few columns completely missing. Yet, quite a few ids miss 60 feature columns or more. Next plot shows the same historgram but with cumulative % to get a better sense of how many ids miss a large number of feature columns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6136290f-7a43-805a-d4f6-56aa98f092e4"},"outputs":[],"source":"res.hist(cumulative=True, normed=1, bins=10)\nplt.suptitle( \"Number of empty columns by id - cumulative % frequency\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"57f6d523-eb30-1f13-2383-16a62c40856a"},"source":"40% of ids miss 60 features or more. c.20% miss 80 features of more. That's quite high and would require more investigation. For example, would be useful to know if among this group that includes about 20% of all ids, features available tend to overlap across ids (not done here). \n\nOn the bright side, 40% of ids have 60 or less empty columns, which is not too bad. It would be even better if the non null columns had a decent number of time periods available vs just a few. So the next step takes a look at the average number of time periods available for non-empty columns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"68889895-7cc1-bbd7-1644-e8da53511649"},"outputs":[],"source":"def average_peridods_by_col(df):\n    df_nulls = df.apply(lambda col: (pd.isnull(col)*1)) # returns 1 if value is null, otherwise 0\n    df_nulls = df_nulls.sum(axis = 0) #return count of non null values by column\n    mapping = np.where(df_nulls > 0, '>0',  '0')\n    mean = df_nulls.groupby(mapping).mean() # returns average number of time periods by category\n    mean = mean['>0']\n    return mean\n\nres2 = hdf.groupby(level=0).apply(average_peridods_by_col)\nres2.plot(kind = 'hist', bins = 10, title = \"Average number of time periods for non empty columns\" )\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"de1a83e1-1e76-0040-1908-4a43743bfe46"},"source":"Quite a few ids have less than 100 time periods available on average per non null column. 100 periods is not lot considering the total period covered (1812 timestamps). \n\nNote that this could be due to those ids with large number of empty columns, a group that we may want to treat separatly. Let's look at the same plot by segmenting between ids that have less than 60 empty columns vs other ids. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bb922d4f-652f-e58e-789e-2d9d9fc8e502"},"outputs":[],"source":"target_ids = res[res<60].index\nmapping = np.in1d(res2.index, target_ids) # return True if id has less than 60 empty columns\nmapping = np.where(mapping, \"target\", \"not target\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f0e8633a-1415-97e3-2fd5-3da956c3f06d"},"outputs":[],"source":"fig, axes = plt.subplots(nrows=1, ncols=2)\nfig.suptitle(\"Average number of time periods for non empty columns\", fontsize=14)\ntitle1, title2  = \"Less than 60 empty columns\",  \"More than 60 empty columns\"\nres2[res2.index.isin(target_ids)].plot(kind = 'hist', bins = 10, ax=axes[0], sharex =True,sharey =True, title=title1, xlim = (0,1400))\nres2[~res2.index.isin(target_ids)].plot(kind = 'hist', bins = 10, ax=axes[1], sharex =True,sharey =True,title=title2, xlim = (0,1400))\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"97213d1a-50ad-d321-a993-e44d605d6158"},"source":"Same conclusion applies when we restrict to the group of ids with less than 60 empty columns: most have on average less than 100 time periods available.\n\nThe implication is that some column features may just cover a narrow time range, which relates to the previous concern that the most recent period with high volatility cannot be properly modelled. \n\nAlso, I wonder to what extent that can be harmful for a model accounting for unobserved heterogeneity. For example, a fixed effects model can be implemented by demeaning the data at the id level - the so-called \"within model\". If for a given feature column, only a few time periods are available, the mean value could potentially be far off the actual value we would get across the 0-1812 period."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"296026ad-f7b0-0eff-93fb-4b6bd782b58a","collapsed":true},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}