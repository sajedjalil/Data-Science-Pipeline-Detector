{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"3d2aab97-5d4b-9736-b2dd-b66ef429189e"},"source":"# Two-Sigma Financial Modelling Challenge"},{"cell_type":"markdown","metadata":{"_cell_guid":"f2664d86-b2aa-c472-16f6-f3bc8d0fa593"},"source":"## 0. Problem setting"},{"cell_type":"markdown","metadata":{"_cell_guid":"dbb024b8-ef23-d830-b32e-96e911e11e12"},"source":"In this competition the task is the prediction of the economic outcomes of a portfolio of financial instruments managed by Two Sigma Investments. The dataset contains anonymized features pertaining to a time-varying value for a financial instrument, identified by the variable *id*. Time is represented by the *timestamp* feature and the variable to predict is *y*. This is a **time series forecasting problem**.\n\nThe competition uses the Kernels environment and the competition data API:\n - the API is designed to prevent accessing data beyond the timestamp for which we are predicting and informs about which *ids* require predictions at which timestamps\n - the API also provides a \"reward\" for each timestamp, in the form of an average R value over the predicted values for the previous day. It's possible to use this reward to do reinforcement-style learning\n - the code should expect and handle missing values\n\nThe training is partitioned in such a way that:\n - the first half (split by time) is provided as a training set at the start of a run\n - the seconf half is streamed through the API, as though it is a holdout set\n\nThe submissions are evaluated on the **R value** between the predicted and actual values. The R value is similar to the *$R^2$* value, also called the **coefficient of determination**. *$R^2$* can be calculated as:\n\n$$$\nR^2 = 1 - \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i (y_i - \\mu)^2}.\n$$$\n\nTo calculate R, we then use:\n\n$$\nR = sign\\left( R^2 \\right) \\sqrt{\\left|R^2\\right|},\n$$\n\nwhere *yy* is the actual value, *μ* is the mean of the actual values, and *ŷ* is the predicted value. Negative R values are clipped at -1, i.e. the score you see will be max(−1,R)max(−1,R)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3b2728cf-b9e0-f018-922d-e0f2480cd068"},"outputs":[],"source":"# Pandas / Numpy for data loading and preparation\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nimport sklearn as sk\nfrom scipy.sparse import csr_matrix, hstack\n\n# Charts / graphs\nimport matplotlib.pyplot as plt\nfrom __future__ import division\nfrom mpl_toolkits.basemap import Basemap\nfrom matplotlib.colors import LogNorm\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n\n# General purpose classes / utilities\nfrom copy import deepcopy\nimport json\nimport timeit\nimport datetime\nimport time\nimport os\nfrom ast import literal_eval\nimport csv\nimport ast\nimport itertools\n\n# NLTK\nimport nltk.corpus\nfrom nltk import SnowballStemmer\n\n# Scipy\nfrom scipy.stats import skew, boxcox\n\n# Scikit-learn\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_selection import SelectPercentile, f_classif, chi2\n\nfrom sklearn.svm import SVC\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import mean_absolute_error\n\n# Geopy for geographic clustering\nfrom geopy.distance import great_circle\n\n# Keras framework for neural network training\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.constraints import maxnorm\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Sequential\nfrom keras.utils import np_utils\nfrom keras.models import model_from_json\nfrom keras.utils.np_utils import to_categorical\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n# XGBoost framework\nimport xgboost as xgb\n\n%matplotlib inline "},{"cell_type":"markdown","metadata":{"_cell_guid":"388c9376-acc8-1158-6f42-9d6904305076"},"source":"# 1. Data Load"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3e899085-eef8-8e9d-a2bd-94268187bcd1"},"outputs":[],"source":"# Location of files and basic identifiers\nID = 'id'\nTIMESTAMP = 'timestamp'\nTARGET = 'y'\nSEED = 0\nDATA_DIR = \"../input\"\nTRAIN = \"{0}/train.h5\".format(DATA_DIR)\nTEST = \"{0}/train.h5.csv\".format(DATA_DIR)\nSUBMISSION = \"{0}/sample_submission.csv\".format(DATA_DIR)\n\n# read data\nwith pd.HDFStore(TRAIN, \"r\") as train:\n    df = train.get(\"train\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"3ecc82b4-7715-eacc-74e2-fc5df352ad39"},"source":"## 2. Exploratory Data Analysis"},{"cell_type":"markdown","metadata":{"_cell_guid":"6a3f086b-eac6-fd8c-5b75-bd80758b6510"},"source":"### 2.1 Basic stats"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9c25d1c3-da0d-4fa0-0fd4-11612ba14607"},"outputs":[],"source":"# print all rows and columns\npd.set_option('display.max_columns', None)\n\n# basic stats\nprint('The training set contains:')\nprint('{} records'.format(df.shape[0]))\nprint('{} features'.format(df.shape[1]))\n\n# check colums types and values\nfeatures = list(df.columns)\nderived_features = [x for x in features if x.find('derived') != -1]\nfundamental_features = [x for x in features if x.find('fundamental') != -1]\ntechnical_features = [x for x in features if x.find('technical') != -1]\n\nprint ('\\nFeature FAMILIES:')\nprint ('- {} derived features'.format(len(derived_features)))\nprint ('- {} fundamental features'.format(len(fundamental_features)))\nprint ('- {} technical features'.format(len(technical_features)))\nprint('\\nFeature TYPES:')\nprint('{}'.format(df.dtypes.value_counts()))\n\nprint('\\n{} distinct time series each with an average of {} datapoints over a total time span of {} periods'.format(df.id.nunique(), \n                                                                             int(np.round(df.shape[0]/df.id.nunique(),0)),\n                                                                             len(df.timestamp.unique())))"},{"cell_type":"markdown","metadata":{"_cell_guid":"7ceadad9-e001-88f5-53ae-26acf2834581"},"source":"So we have a market portfolio of 1424 securities of variable composition as some of them are evidently acquired / sold during the observation period. We can define the market portfolio of all the assets together and calculate the market return over the observation period:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f3ff6e4c-c80d-cd59-d3f1-6a419ac30f39"},"outputs":[],"source":"market = df[['timestamp', 'y']].groupby('timestamp').agg([np.mean, np.std, len]).reset_index()\nt      = market['timestamp']\ny_mean = np.array(market['y']['mean'])\ny_std  = np.array(market['y']['std'])\nn      = np.array(market['y']['len'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fa1785b8-6b89-9bb5-dbfc-9e6ec6d839f5"},"outputs":[],"source":"plt.figure()\nplt.plot(t, y_std, '-')\nplt.xlabel('timestamp')\nplt.ylabel('std of y')\nplt.title('Portfolio VOLATILITY over time')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"196a98ee-a279-7aef-1a18-da1ebb093266"},"outputs":[],"source":"plt.figure()\nplt.plot(t, y_mean, '-')\nplt.xlabel('timestamp')\nplt.ylabel('mean of y')\nplt.title('Portfolio AVERAGE VALUE over time')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f21c794c-b1ca-ba12-e5a4-57470c45b8c9"},"outputs":[],"source":"plt.figure()\nplt.plot(t, y_std, '-')\nplt.xlabel('timestamp')\nplt.ylabel('std of y')\nplt.title('Portfolio VOLATILITY over time')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"288fb8c7-6ca4-7cc9-db29-f6267df3bef1"},"outputs":[],"source":"plt.figure()\nplt.plot(t, n, '-')\nplt.xlabel('timestamp')\nplt.ylabel('portfolio size')\nplt.title('Portfolio ASSET COUNT over time')"},{"cell_type":"markdown","metadata":{"_cell_guid":"6358883a-e9aa-58a0-2fe4-7145741edf1c"},"source":"## 3. Feature Engineering"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a15fdadb-700e-b56d-6394-68919f218bb6"},"outputs":[],"source":"import kagglegym\nenv = kagglegym.make()\nobservation = env.reset()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cf7d8078-8459-2cec-1868-87fa34525184"},"outputs":[],"source":"# Get the train dataframe\ntrain = observation.train\nmean_values = train.mean(axis=0)\n# median_values = train.median(axis=0)\ntrain.fillna(mean_values, inplace=True)\n\n\ncols_to_use = ['technical_30', 'technical_20', 'fundamental_11']\n# Observed with histograns:\nlow_y_cut = -0.086093\nhigh_y_cut = 0.093497\n\ny_is_above_cut = (train.y > high_y_cut)\ny_is_below_cut = (train.y < low_y_cut)\ny_is_within_cut = (~y_is_above_cut & ~y_is_below_cut)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d736ceb6-07e5-16b3-c39d-8dc164d5b5df"},"source":"## 3. Model 1: Ridge regression"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2044916f-8179-7b9f-da61-2d1ce487f180"},"outputs":[],"source":"def get_weighted_y(series):\n    id, y = series[\"id\"], series[\"y\"]\n    # return 0.95 * y + 0.05 * ymean_dict[id] if id in ymean_dict else y\n    return 0.95 * y + 0.05 * ymedian_dict[id] if id in ymedian_dict else y"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"05fbc908-1484-a06a-3dbf-444ba4e1214b"},"outputs":[],"source":"model = Ridge()\nmodel.fit(np.array(train.loc[y_is_within_cut, cols_to_use].values), train.loc[y_is_within_cut, TARGET])\n\n# ymean_dict = dict(train.groupby([\"id\"])[\"y\"].mean())\nymedian_dict = dict(train.groupby([\"id\"])[\"y\"].median())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7fe91f5f-d2b5-ebaa-d273-d61da65907ee"},"outputs":[],"source":"while True:\n    \n    # make prediction\n    observation.features.fillna(mean_values, inplace=True)\n    test_x = np.array(observation.features[cols_to_use].values)\n    observation.target.y = model.predict(test_x).clip(low_y_cut, high_y_cut)\n    # weighted y using average value\n    observation.target.y = observation.target.apply(get_weighted_y, axis = 1)\n    \n    # Execute step\n    target = observation.target\n    timestamp = observation.features[\"timestamp\"][0]\n    if timestamp % 100 == 0:\n        print(\"Timestamp #{}\".format(timestamp))\n\n    observation, reward, done, info = env.step(target)\n    if done:        \n        break"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"263224eb-9542-2cc1-c78f-316db305964b"},"outputs":[],"source":"print(info)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}