{"nbformat":4,"cells":[{"cell_type":"markdown","metadata":{"_uuid":"994f3f3a75eedcb5ce15bba6498b189efca380e0","_cell_guid":"abd9030e-32fa-4c72-a3f9-e6f040feee96"},"source":"<div class=\"clearfix\" style=\"padding: 10px; padding-left: 0px\">\n<a href=\"https://www.kaggle.com/c/nips-2017-non-targeted-adversarial-attack\"><img src=\"https://raw.githubusercontent.com/tensorflow/cleverhans/master/assets/logo.png\" width=\"150px\" style=\"display: inline-block; margin-top: 65px;\"></img></a>\n<a href=\"http://linkedin.com/in/corypruce/\"><img src=\"https://sep.yimg.com/ay/fridgedoor/caution-jazz-hands-magnet-4.gif\" width=\"150px\" class=\"pull-right\" style=\"display: inline-block; margin: 0px;\" ></img></a>\n</div>\n# Battle Jazz GANs!!\n\nFor output see https://github.com/Cpruce/Notebooks/blob/master/BattleJazzGANs.ipynb"},{"cell_type":"code","metadata":{"_uuid":"a72f5160c328331e84bfc0decd177921d5e1381c","collapsed":true,"_cell_guid":"10a578a0-f3fd-41ca-a73b-214cc65523d4"},"execution_count":null,"source":"import os\nimport sys\nimport numpy as np\nimport torch\nimport torchvision\nimport torch.utils.data as data\nfrom scipy.misc import imsave\nimport matplotlib\nimport matplotlib.pyplot as plt \nimport os.path\nimport pandas as pd\nimport torchvision.transforms as transforms\n#from inception_v3 import *\n\nfrom PIL import Image\n\nfrom torch import autograd\nfrom torch.autograd.gradcheck import zero_gradients\n#from helpers import *\n\nimport hashlib\nimport io\n\n%matplotlib inline","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"b9b3c37c9facb5646f64543e6152725c3baf26c2","_cell_guid":"5cd50c23-e1b3-4df6-addd-c772002cd784"},"source":"# What's cool about Generative Adversarial Networks?!\n\n<br>\n**The obvious**: a person today can train a generative model easily, with the goal of fooling the best discriminators. In this notebook, I'll do a short demonstration of this scenario, using InceptionV3 pretrained on ImageNet.  \n<br>\n**The not-so-obvious**: the generative model that is produced becomes a pretty good creator of high-dimensional data from the underlying distribution [3]. Generative models will be an extremely useful tool in the near-future where acquiring a large dataset is infeasible. For example, a dataset of a large variety of styles can be produced for image recognition tasks or states, actions, and rewards produced for reinforcement learning. Perhaps generative models will be able to help simulate complex physical events we haven't been able to before. \n<br><br>\n**The obviously cool**: combining deep learning and game theory/reinforcement learning in a highly applicable fashion. Inception of Machine Learning Security, highlighting the infancy of modern-day models and techniques. Spam bots can possibly generate successfully masquerading emails. Fake information can maybe be produced en masse. Moreover, since the generator(s) in GANs learn the joint probability distribution, they have the ability to make \"guesses\" at missing labeled data aka *semi-supervised learning*.\n<br><br><br>\nThe following is a configuration which will allow for a fast convergence of both the non-targeted and the targeted attack using rwightman's Fast Gradient port of the Tensorflow example. I am also working on publishing the non-targeted Virtual Adversarial Training (VAT) attack as an addition.\n\n\nThanks for foundations and inspiration: <br>\nhttps://arxiv.org/pdf/1701.00160.pdf<br>\nhttps://www.kaggle.com/c/nips-2017-targeted-adversarial-attack/discussion/37614<br>\nhttps://www.kaggle.com/allunia/example-attacking-logistic-regression<br>\nhttps://www.kaggle.com/benhamner/adversarial-learning-challenges-getting-started<br>\nhttp://pytorch.org/docs/0.2.0/_modules/torchvision/models/inception.html<br>\nhttps://github.com/znxlwm/pytorch-generative-model-collections/blob/master/<br>\nhttps://github.com/charlesjansen/Deep-Learning-face_generation/blob/master/dlnd_face_generation.ipynb<br>\n"},{"cell_type":"code","metadata":{"_uuid":"386cf5d7655c41196ea6e8891260647cde606638","collapsed":true,"_cell_guid":"9ab3a725-7501-4422-8ef7-aaa6d9e7284b"},"execution_count":null,"source":"IMG_EXTENSIONS = ['.png', '.jpg']\n\nclass LeNormalize(object):\n    \"\"\"Normalize to -1..1 in Google Inception style\n    \"\"\"\n    def __call__(self, tensor):\n        for t in tensor:\n            t.sub_(0.5).mul_(2.0)\n        return tensor\n\n\ndef default_inception_transform(img_size):\n    tf = transforms.Compose([\n        transforms.Scale(img_size),\n        transforms.CenterCrop(img_size),\n        transforms.ToTensor(),\n        LeNormalize(),\n    ])\n    return tf\n\n\ndef find_inputs(folder, filename_to_target=None, types=IMG_EXTENSIONS):\n    inputs = []\n    for root, _, files in os.walk(folder, topdown=False):\n        for rel_filename in files:\n            base, ext = os.path.splitext(rel_filename)\n            if ext.lower() in types:\n                abs_filename = os.path.join(root, rel_filename)\n                target = filename_to_target[rel_filename.split('.')[0]] if filename_to_target else 0\n                inputs.append((abs_filename, target))\n    return inputs\n\n\nclass Dataset(data.Dataset):\n\n    def __init__(\n            self,\n            root,\n            target_file='../input/nips-2017-adversarial-learning-development-set/images.csv',\n            transform=None):\n        \n        if target_file:\n            target_file_path = target_file #os.path.join(root, target_file)\n            target_df = pd.read_csv(target_file_path)#, header=None)\n            target_df[\"TargetClass\"] = target_df[\"TargetClass\"].apply(int)\n            #print(target_df[\"ImageId\"], target_df[\"TargetClass\"])\n            f_to_t = dict(zip(target_df[\"ImageId\"], target_df[\"TargetClass\"] - 1))  # -1 for 0-999 class ids\n        else:\n            f_to_t = dict()\n\n        imgs = find_inputs(root, filename_to_target=f_to_t)\n        if len(imgs) == 0:\n            raise(RuntimeError(\"Found 0 images in subfolders of: \" + root + \"\\n\"\n                               \"Supported image extensions are: \" + \",\".join(IMG_EXTENSIONS)))\n\n        self.root = root\n        self.imgs = imgs\n        self.transform = transform\n\n    def __getitem__(self, index):\n        path, target = self.imgs[index]\n        img = Image.open(path).convert('RGB')\n        if self.transform is not None:\n            img = self.transform(img)\n        if target is None:\n            target = torch.zeros(1).long()\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)\n\n    def set_transform(self, transform):\n        self.transform = transform\n\n    def filenames(self, indices=[], basename=False):\n        if indices:\n            if basename:\n                return [os.path.basename(self.imgs[i][0]) for i in indices]\n            else:\n                return [self.imgs[i][0] for i in indices]\n        else:\n            if basename:\n                return [os.path.basename(x[0]) for x in self.imgs]\n            else:\n                return [x[0] for x in self.imgs]\n\nclass OneShotDataset(data.Dataset):\n\n    def __init__(\n            self,\n            filename,\n            transform=None):\n\n        self.filename = filename\n        self.transform = transform\n\n    def __getitem__(self, index):\n        path = self.filename\n        target = None\n        img = Image.open(path).convert('RGB')\n        if self.transform is not None:\n            img = self.transform(img)\n        if target is None:\n            target = torch.zeros(1).long()\n        return img, target\n\n    def __len__(self):\n        return 1","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"c871010275facfc0739be81bc69759b521421d07","collapsed":true,"_cell_guid":"b234e446-d160-47cb-8935-079fe036cb13"},"execution_count":null,"source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\n\n\n__all__ = ['Inception3', 'inception_v3']\n\n\nmodel_urls = {\n    # Inception v3 ported from TensorFlow\n    'inception_v3_google': 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth',\n}\n\n\ndef inception_v3(pretrained=False, **kwargs):\n    r\"\"\"Inception v3 model architecture from\n    `\"Rethinking the Inception Architecture for Computer Vision\" <http://arxiv.org/abs/1512.00567>`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    if pretrained:\n        if 'transform_input' not in kwargs:\n            kwargs['transform_input'] = True\n        model = Inception3(**kwargs)\n        model.load_state_dict(model_zoo.load_url(model_urls['inception_v3_google']))\n        return model\n\n    return Inception3(**kwargs)\n\n\n\nclass Inception3(nn.Module):\n\n    def __init__(self, num_classes=1000, aux_logits=True, transform_input=False):\n        super(Inception3, self).__init__()\n        self.aux_logits = aux_logits\n        self.transform_input = transform_input\n        self.Conv2d_1a_3x3 = BasicConv2d(3, 32, kernel_size=3, stride=2)\n        self.Conv2d_2a_3x3 = BasicConv2d(32, 32, kernel_size=3)\n        self.Conv2d_2b_3x3 = BasicConv2d(32, 64, kernel_size=3, padding=1)\n        self.Conv2d_3b_1x1 = BasicConv2d(64, 80, kernel_size=1)\n        self.Conv2d_4a_3x3 = BasicConv2d(80, 192, kernel_size=3)\n        self.Mixed_5b = InceptionA(192, pool_features=32)\n        self.Mixed_5c = InceptionA(256, pool_features=64)\n        self.Mixed_5d = InceptionA(288, pool_features=64)\n        self.Mixed_6a = InceptionB(288)\n        self.Mixed_6b = InceptionC(768, channels_7x7=128)\n        self.Mixed_6c = InceptionC(768, channels_7x7=160)\n        self.Mixed_6d = InceptionC(768, channels_7x7=160)\n        self.Mixed_6e = InceptionC(768, channels_7x7=192)\n        if aux_logits:\n            self.AuxLogits = InceptionAux(768, num_classes)\n        self.Mixed_7a = InceptionD(768)\n        self.Mixed_7b = InceptionE(1280)\n        self.Mixed_7c = InceptionE(2048)\n        self.fc = nn.Linear(2048, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                import scipy.stats as stats\n                stddev = m.stddev if hasattr(m, 'stddev') else 0.1\n                X = stats.truncnorm(-2, 2, scale=stddev)\n                values = torch.Tensor(X.rvs(m.weight.data.numel()))\n                m.weight.data.copy_(values)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        if self.transform_input:\n            x = x.clone()\n            x[:, 0] = x[:, 0] * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n            x[:, 1] = x[:, 1] * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n            x[:, 2] = x[:, 2] * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n        # 299 x 299 x 3\n        x = self.Conv2d_1a_3x3(x)\n        # 149 x 149 x 32\n        x = self.Conv2d_2a_3x3(x)\n        # 147 x 147 x 32\n        x = self.Conv2d_2b_3x3(x)\n        # 147 x 147 x 64\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # 73 x 73 x 64\n        x = self.Conv2d_3b_1x1(x)\n        # 73 x 73 x 80\n        x = self.Conv2d_4a_3x3(x)\n        # 71 x 71 x 192\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # 35 x 35 x 192\n        x = self.Mixed_5b(x)\n        # 35 x 35 x 256\n        x = self.Mixed_5c(x)\n        # 35 x 35 x 288\n        x = self.Mixed_5d(x)\n        # 35 x 35 x 288\n        x = self.Mixed_6a(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6b(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6c(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6d(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6e(x)\n        # 17 x 17 x 768\n        if self.training and self.aux_logits:\n            aux = self.AuxLogits(x)\n        # 17 x 17 x 768\n        x = self.Mixed_7a(x)\n        # 8 x 8 x 1280\n        x = self.Mixed_7b(x)\n        # 8 x 8 x 2048\n        x = self.Mixed_7c(x)\n        # 8 x 8 x 2048\n        x = F.avg_pool2d(x, kernel_size=8)\n        # 1 x 1 x 2048\n        x = F.dropout(x, training=self.training)\n        # 1 x 1 x 2048\n        x = x.view(x.size(0), -1)\n        # 2048\n        x = self.fc(x)\n        # 1000 (num_classes)\n        if self.training and self.aux_logits:\n            return x, aux\n        return x\n\n\nclass InceptionA(nn.Module):\n\n    def __init__(self, in_channels, pool_features):\n        super(InceptionA, self).__init__()\n        self.branch1x1 = BasicConv2d(in_channels, 64, kernel_size=1)\n\n        self.branch5x5_1 = BasicConv2d(in_channels, 48, kernel_size=1)\n        self.branch5x5_2 = BasicConv2d(48, 64, kernel_size=5, padding=2)\n\n        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, padding=1)\n\n        self.branch_pool = BasicConv2d(in_channels, pool_features, kernel_size=1)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch5x5 = self.branch5x5_1(x)\n        branch5x5 = self.branch5x5_2(branch5x5)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionB(nn.Module):\n\n    def __init__(self, in_channels):\n        super(InceptionB, self).__init__()\n        self.branch3x3 = BasicConv2d(in_channels, 384, kernel_size=3, stride=2)\n\n        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        branch3x3 = self.branch3x3(x)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n\n        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n\n        outputs = [branch3x3, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionC(nn.Module):\n\n    def __init__(self, in_channels, channels_7x7):\n        super(InceptionC, self).__init__()\n        self.branch1x1 = BasicConv2d(in_channels, 192, kernel_size=1)\n\n        c7 = channels_7x7\n        self.branch7x7_1 = BasicConv2d(in_channels, c7, kernel_size=1)\n        self.branch7x7_2 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n        self.branch7x7_3 = BasicConv2d(c7, 192, kernel_size=(7, 1), padding=(3, 0))\n\n        self.branch7x7dbl_1 = BasicConv2d(in_channels, c7, kernel_size=1)\n        self.branch7x7dbl_2 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7dbl_3 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n        self.branch7x7dbl_4 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7dbl_5 = BasicConv2d(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n\n        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch7x7 = self.branch7x7_1(x)\n        branch7x7 = self.branch7x7_2(branch7x7)\n        branch7x7 = self.branch7x7_3(branch7x7)\n\n        branch7x7dbl = self.branch7x7dbl_1(x)\n        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionD(nn.Module):\n\n    def __init__(self, in_channels):\n        super(InceptionD, self).__init__()\n        self.branch3x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)\n        self.branch3x3_2 = BasicConv2d(192, 320, kernel_size=3, stride=2)\n\n        self.branch7x7x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)\n        self.branch7x7x3_2 = BasicConv2d(192, 192, kernel_size=(1, 7), padding=(0, 3))\n        self.branch7x7x3_3 = BasicConv2d(192, 192, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7x3_4 = BasicConv2d(192, 192, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = self.branch3x3_2(branch3x3)\n\n        branch7x7x3 = self.branch7x7x3_1(x)\n        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)\n        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)\n        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)\n\n        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n        outputs = [branch3x3, branch7x7x3, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionE(nn.Module):\n\n    def __init__(self, in_channels):\n        super(InceptionE, self).__init__()\n        self.branch1x1 = BasicConv2d(in_channels, 320, kernel_size=1)\n\n        self.branch3x3_1 = BasicConv2d(in_channels, 384, kernel_size=1)\n        self.branch3x3_2a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n        self.branch3x3_2b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n\n        self.branch3x3dbl_1 = BasicConv2d(in_channels, 448, kernel_size=1)\n        self.branch3x3dbl_2 = BasicConv2d(448, 384, kernel_size=3, padding=1)\n        self.branch3x3dbl_3a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n        self.branch3x3dbl_3b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n\n        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = [\n            self.branch3x3_2a(branch3x3),\n            self.branch3x3_2b(branch3x3),\n        ]\n        branch3x3 = torch.cat(branch3x3, 1)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = [\n            self.branch3x3dbl_3a(branch3x3dbl),\n            self.branch3x3dbl_3b(branch3x3dbl),\n        ]\n        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionAux(nn.Module):\n\n    def __init__(self, in_channels, num_classes):\n        super(InceptionAux, self).__init__()\n        self.conv0 = BasicConv2d(in_channels, 128, kernel_size=1)\n        self.conv1 = BasicConv2d(128, 768, kernel_size=5)\n        self.conv1.stddev = 0.01\n        self.fc = nn.Linear(768, num_classes)\n        self.fc.stddev = 0.001\n\n    def forward(self, x):\n        # 17 x 17 x 768\n        x = F.avg_pool2d(x, kernel_size=5, stride=3)\n        # 5 x 5 x 768\n        x = self.conv0(x)\n        # 5 x 5 x 128\n        x = self.conv1(x)\n        # 1 x 1 x 768\n        x = x.view(x.size(0), -1)\n        # 768\n        x = self.fc(x)\n        # 1000\n        return x\n\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return F.relu(x, inplace=True)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"da0a01596ffd4a7331099d49c7ca73746630bbfe","_cell_guid":"28df7295-9dde-4e99-8c84-319595c356ac"},"source":"The below is a variation of rwightman's port of the base Iterative-Fast Gradient Sign Method (I-FGSM) for non-targeted attacks: $$x^{adv} = x + \\epsilon * sign(\\nabla_{x}(J(x,y_{true}))) $$ <br> and the Iterative-Targeted Fast Gradient Sign Method (I-TFGSM) for targeted attacks: $$x^{adv} = x - \\epsilon * sign(\\nabla_{x}(J(x,y_{fake})))$$ <br> which leverage the $\\ell_{\\inf}-norm$ solely for input image $x$. Each are computed $num\\_steps$ times.[1] <br>\n\nOne notable difference between my choice of $y_{fake}$ and [1]'s pick is that my selection of attack targets are randomly selected, where as [1] seems to pick the *least-likely* from the original image softmax output."},{"cell_type":"code","metadata":{"_uuid":"2b8aa537d608c8c467c0e5d0e220600a59db164f","collapsed":true,"_cell_guid":"cdf93507-175e-4db6-b8d1-a2ea801275ff"},"execution_count":null,"source":"class AttackIterative:\n\n    def __init__(\n            self,\n            max_epsilon=16, norm=float('inf'), step_alpha=None, \n            num_steps=None, cuda=True, debug=False):\n\n        self.eps = 2.0 * max_epsilon / 255.0\n        self.num_steps = num_steps or 10\n        self.norm = norm\n        if not step_alpha:\n            if norm == float('inf'):\n                self.step_alpha = self.eps / self.num_steps\n            else:\n                # Different scaling required for L2 and L1 norms to get anywhere\n                if norm == 1:\n                    self.step_alpha = 500.0  # L1 needs a lot of (arbitrary) love\n                else:\n                    self.step_alpha = 1.0\n        else:\n            self.step_alpha = step_alpha\n        self.loss_fn = torch.nn.CrossEntropyLoss()\n        if cuda:\n            self.loss_fn = self.loss_fn.cuda()\n        self.debug = debug\n\n    def non_target_attack(self, non_target_model, x, targets, batch_idx=0):\n        input_var = autograd.Variable(x, requires_grad=True)\n        targets_var = autograd.Variable(targets)\n        eps = self.eps\n        step_alpha = self.step_alpha\n\n        step = 0\n        while step < self.num_steps:\n            zero_gradients(input_var)\n            output = non_target_model(input_var)\n            if not step:\n                # for non-targeted, we'll move away from most likely\n                targets_var.data = output.data.max(1)[1]\n            loss = self.loss_fn(output, targets_var)\n            loss.backward()\n\n            # normalize and scale gradient\n            if self.norm == 2:\n                normed_grad = step_alpha * input_var.grad.data / l2_norm(input_var.grad.data)\n            elif self.norm == 1:\n                normed_grad = step_alpha * input_var.grad.data / l1_norm(input_var.grad.data)\n            else:\n                # infinity-norm\n                normed_grad = step_alpha * torch.sign(input_var.grad.data)\n\n            # perturb current input image by normalized and scaled gradient\n            step_adv = input_var.data + normed_grad\n\n            # calculate total adversarial perturbation from original image and clip to epsilon constraints\n            total_adv = step_adv - x\n            total_adv = torch.clamp(total_adv, -eps, eps)\n            \n            if self.debug:\n                print('Non-Targeted --', 'batch:', batch_idx, 'step:', step, total_adv.mean(), total_adv.min(), total_adv.max())\n                sys.stdout.flush()\n\n            # apply total adversarial perturbation to original image and clip to valid pixel range\n            input_adv = x + total_adv\n            input_adv = torch.clamp(input_adv, -1.0, 1.0)\n            input_var.data = input_adv\n            step += 1\n\n        return input_adv.permute(0, 2, 3, 1).cpu().numpy()\n    \n    def target_attack(self, target_model, x, targets, batch_idx=0):\n        input_var = autograd.Variable(x, requires_grad=True)\n        targets_var = autograd.Variable(targets)\n        eps = self.eps\n        step_alpha = self.step_alpha\n\n        step = 0\n        while step < self.num_steps:\n            zero_gradients(input_var)\n            output = target_model(input_var)\n            loss = self.loss_fn(output, targets_var)\n            loss.backward()\n\n            # normalize and scale gradient\n            if self.norm == 2:\n                normed_grad = step_alpha * input_var.grad.data / l2_norm(input_var.grad.data)\n            elif self.norm == 1:\n                normed_grad = step_alpha * input_var.grad.data / l1_norm(input_var.grad.data)\n            else:\n                # infinity-norm\n                normed_grad = step_alpha * torch.sign(input_var.grad.data)\n\n            # perturb current input image by normalized and scaled gradient\n            step_adv = input_var.data - normed_grad\n\n            # calculate total adversarial perturbation from original image and clip to epsilon constraints\n            total_adv = step_adv - x\n            total_adv = torch.clamp(total_adv, -eps, eps)\n            \n            if self.debug:\n                print('Targeted --', 'batch:', batch_idx, 'step:', step, total_adv.mean(), total_adv.min(), total_adv.max())\n                sys.stdout.flush()\n\n            # apply total adversarial perturbation to original image and clip to valid pixel range\n            input_adv = x + total_adv\n            input_adv = torch.clamp(input_adv, -1.0, 1.0)\n            input_var.data = input_adv\n            step += 1\n\n        return input_adv.permute(0, 2, 3, 1).cpu().numpy()\n        \n    def run(self, non_target_model, target_model, x, true_targets, fake_targets, batch_idx=0):\n        non_target_pred = self.non_target_attack(non_target_model, x, true_targets, batch_idx=0)\n        target_pred = self.target_attack(target_model, x, fake_targets, batch_idx=0)\n        return (non_target_pred, target_pred)","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"86b29adf8c7934d5414e1f76693780490ef9c233","collapsed":true,"_cell_guid":"8e71d1bc-ba2c-4fd6-9ddf-43d70190eb2a"},"execution_count":null,"source":"def make_prediction(model, output_file):\n    dataset = OneShotDataset(\n            output_file,\n            transform=default_inception_transform(args[\"img_size\"]))\n    loader = data.DataLoader(\n        dataset,\n        batch_size=1)\n    # one shot\n    for _batch_idx, (tensor, _target) in enumerate(loader): \n        input_var = autograd.Variable(tensor, requires_grad=True)\n        zero_gradients(input_var)\n        output = model(input_var)\n        _, preds = torch.max(output.data, 1)\n    return preds","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"7405ee62e8bddf16ceb788efb9e3aaa6ea9ee6fb","collapsed":true,"_cell_guid":"2e8c5d24-16b2-40ad-a958-6ea5578fef0d"},"execution_count":null,"source":"def make_md5(img_file):\n    m = hashlib.md5()\n    img = Image.open(img_file)\n    with io.BytesIO() as memf:\n        img.save(memf, 'PNG')\n        data = memf.getvalue()\n        m.update(data)\n    return m.hexdigest()\n\ndef display_attacks(attacks, fake_targets, cols):\n    f, axs = plt.subplots(nrows=len(attacks),ncols=3,figsize=(15,22))\n    cat_df = pd.read_csv('../input/nips-2017-adversarial-learning-development-set/categories.csv')\n    \n    for ax, col in zip(axs[0], cols):\n        ax.annotate(col, xy=(0.5, 1.1), xytext=(0, 1),\n                xycoords='axes fraction', textcoords='offset points',\n                size='large', ha='center', va='baseline')\n    \n    for i, row in enumerate(axs):\n        for j, col in enumerate(row):\n            img, label, md5 = attacks[i][j]\n            col.imshow(img)\n            target = cat_df.iloc[label-1][1].split(\",\")[0]\n            title = 'Labeled: '+target\n            if j == 2:\n                fake_target = cat_df.iloc[fake_targets[i]-1][1].split(\",\")[0]\n                title += '. Target: '+fake_target\n            col.set_title(title)\n            col.annotate('MD5: '+md5, xy=(0.5, -0.12), xytext=(0, 1),\n                xycoords='axes fraction', textcoords='offset points',\n                size='large', ha='center', va='baseline')\n    plt.show()","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"cb6ff3047b9b073301274fa1cf212143c5daee3d","collapsed":true,"_cell_guid":"96ab67d4-1f67-4544-864c-37505f0e77ff"},"execution_count":null,"source":"import torchvision.models\nimport matplotlib.image as mpimg\ndef run_iterative_attack(args, attack):\n    assert args[\"input_dir\"]\n\n    dataset = Dataset(\n        args[\"input_dir\"],\n        transform=default_inception_transform(args[\"img_size\"]))\n\n    loader = data.DataLoader(\n        dataset,\n        batch_size=args[\"batch_size\"],\n        shuffle=False)\n\n    # train from scratch\n    non_target_gen_model = inception_v3(pretrained=False, transform_input=True)\n    target_gen_model     = inception_v3(pretrained=False, transform_input=True)\n    if args[\"cuda\"]:\n        non_target_gen_model = non_target_gen_model.cuda()\n        target_gen_model     = target_gen_model.cuda()\n\n    # pick up feature hierarchy from checkpoint\n    if args[\"checkpoint_path\"] is not None and os.path.isfile(args[\"checkpoint_path\"]):\n        checkpoint = torch.load(args[\"checkpoint_path\"])\n        if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n            non_target_gen_model.load_state_dict(checkpoint['state_dict'])\n            target_gen_model.load_state_dict(checkpoint['state_dict'])\n        else:\n            non_target_gen_model.load_state_dict(checkpoint)\n            target_gen_model.load_state_dict(checkpoint)\n    else:\n        print(\"Error: No checkpoint found at %s.\" % args[\"checkpoint_path\"])\n\n    non_target_gen_model.eval()\n    target_gen_model.eval()\n    \n    # pretrained discriminatory model\n    dis_model = inception_v3(pretrained=True)\n    dis_model.eval()\n    \n    attacks = []                        \n    # run both non-targeted and targeted attacks\n    for batch_idx, (input, true_targets) in enumerate(loader):    \n        start_index = args[\"batch_size\"] * batch_idx\n        indices = list(range(start_index, start_index + input.size(0)))\n\n        # spawn 4 random classes. For one batch (size=4), unlikely fake==true (4/1000=1/250)\n        fake_targets = np.random.randint(1, 1001 + 1, size=4)\n        fake_targets = torch.from_numpy(fake_targets)\n        \n        if args[\"cuda\"]:\n            input = input.cuda()\n            true_targets = true_targets.cuda()\n            fake_targets = fake_targets.cuda()\n\n        (non_target_adv, target_adv) = attack.run(non_target_gen_model, target_gen_model, \n                                                  input, true_targets, fake_targets, batch_idx)\n        \n        for i, (filename, non_target_o, target_o) in enumerate(\n            zip(dataset.filenames(indices, basename=True), non_target_adv, target_adv)):\n            # get and save non-targeted adversary image, label, and hash\n            non_target_img = (non_target_o + 1.0) * 0.5\n            non_target_output_file = os.path.join(args[\"output_dir\"], \"non_target_\" + filename)\n            imsave(non_target_output_file, non_target_img, format='png')     \n            non_target_hash  = make_md5(non_target_output_file)\n            non_target_label = make_prediction(dis_model, non_target_output_file)[0]\n            \n            # get and save targeted adversary image, label, and hash\n            target_img = (target_o + 1.0) * 0.5\n            target_output_file = os.path.join(args[\"output_dir\"], \"target_\" + filename)\n            imsave(target_output_file, target_img, format='png')     \n            target_hash  = make_md5(target_output_file)\n            target_label = make_prediction(dis_model, target_output_file)[0]\n            \n            # get original image, label, and hash\n            og_img = mpimg.imread(args[\"input_dir\"]+filename)\n            og_file = os.path.join(args[\"input_dir\"], filename)\n            og_label = make_prediction(dis_model, og_file)[0]\n            og_hash  = make_md5(og_file)\n            attacks.append(((og_img, og_label, og_hash), \n                            (non_target_img, non_target_label, non_target_hash), \n                            (target_img, target_label, target_hash)))\n              \n        # only one batch for the notebook\n        display_attacks(attacks, fake_targets, cols=[\"Original\",\"Non-Targeted\", \"Targeted\"])\n        break ","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"131df9b5a50eca5f2e66b6c28dfef58f9a8d6c4c","collapsed":true,"_cell_guid":"3e1e8ffa-47af-4cfb-9e0c-7db86bf1c532"},"execution_count":null,"source":"args = {}\nargs[\"targeted\"]=True\nargs[\"input_dir\"]='../input/nips-2017-adversarial-learning-development-set/images'\nargs[\"max_epsilon\"]=5 # serves the purpose of the demo\nargs[\"norm\"]=0 \nargs[\"step_alpha\"]=0.01\nargs[\"num_steps\"]=10 \nargs[\"debug\"]=False\nargs[\"img_size\"]=299\nargs[\"batch_size\"]=4\nargs[\"checkpoint_path\"]='pytorch-nips2017-attack-example-master/inception_v3_google-1a9a5a14.pth' # Need this file...\nargs[\"no_gpu\"]=False\nargs[\"output_dir\"]=\"output/\"\nargs[\"cuda\"]=False # quick run on laptop","outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"_uuid":"ef46978a20be9b8495e62ab7c59e1f6a8015ab0c","collapsed":true,"_cell_guid":"fc9ef3ec-67ee-460a-bc9d-d9782fa796ab"},"execution_count":null,"source":"attack = AttackIterative(\n        max_epsilon=args[\"max_epsilon\"],\n        norm=args[\"norm\"],\n        step_alpha=args[\"step_alpha\"],\n        num_steps=args[\"num_steps\"],\n        cuda=args[\"cuda\"],\n        debug=args[\"debug\"])\n\n#print(torchvision.models.__dict__)\nrun_iterative_attack(args, attack)\n\n\n# see https://github.com/Cpruce/Notebooks/blob/master/BattleJazzGANs.ipynb","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"19e0a1190efde8072899baec7993433764466bc4","collapsed":true,"_cell_guid":"b9190160-17a8-4633-a737-a3d224d503eb"},"source":"[1] TramÃ¨r, Florian, et al. \"Ensemble Adversarial Training: Attacks and Defenses.\" arXiv preprint arXiv:1705.07204 (2017). <br>\n[2] Miyato, Takeru, et al. \"Distributional smoothing with virtual adversarial training.\" arXiv preprint arXiv:1507.00677 (2015).<br>\n[3] Goodfellow, Ian. \"NIPS 2016 tutorial: Generative adversarial networks.\" arXiv preprint arXiv:1701.00160 (2016)."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"file_extension":".py","mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","version":"3.6.1","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python"}},"nbformat_minor":1}