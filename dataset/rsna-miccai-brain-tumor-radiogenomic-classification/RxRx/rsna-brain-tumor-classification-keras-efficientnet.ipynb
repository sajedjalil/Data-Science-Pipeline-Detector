{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"EPOCHS = 300\nNUM_RETRIES = 10 \nMODALITIES = ['FLAIR'] # ['T1w'] # ['T1wCE'] # ['T2w'] \nBATCH_SIZE = 64\nDATA_HEIGHT = 256 \nDATA_WIDTH = 256 \nDATA_DEPTH = 32 # 16 \nDATA_CHANNELS = 4 \nINPUT_HEIGHT = 120 # 256 # 50 # 64 # \nINPUT_WIDTH = 120 # 256 # 50 # 64 # \nINPUT_CHANNELS = len(MODALITIES) \nINPUT_DEPTH = DATA_DEPTH \nLEARNING_RATE = 1e-4 \nMODEL_TYPE = 'EfficientNet3D_B0S'\n\nFEATURE_MAPPING = True \nDEBUG = True \nLR_REDUCE = True \nBEST_WEIGHTS = True\n\nROOT_DIR = '/kaggle/input'\n\nWEIGHTS_DIR = f'{ROOT_DIR}/rsna-brain-tumor-classification-external-weights'","metadata":{"execution":{"iopub.status.busy":"2021-10-04T13:19:08.783637Z","iopub.execute_input":"2021-10-04T13:19:08.784032Z","iopub.status.idle":"2021-10-04T13:19:08.811982Z","shell.execute_reply.started":"2021-10-04T13:19:08.783944Z","shell.execute_reply":"2021-10-04T13:19:08.811315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, glob, numpy as np\ndef get_best_weight(weights_dir, model_type):\n  aucs, wts = [], []\n  for f in glob.glob(os.path.join(weights_dir, f'{model_type}*')):\n    if '.h5' in f:\n      aucs.append(float(f.replace('.h5','').split('-')[-1]))\n      wts.append(f)\n  if len(aucs)>0 and len(aucs)==len(wts):\n    return os.path.join(weights_dir, wts[np.argmax(aucs)])","metadata":{"papermill":{"duration":0.030627,"end_time":"2021-09-06T12:31:28.576472","exception":false,"start_time":"2021-09-06T12:31:28.545845","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-04T13:19:08.823219Z","iopub.execute_input":"2021-10-04T13:19:08.82345Z","iopub.status.idle":"2021-10-04T13:19:08.831204Z","shell.execute_reply.started":"2021-10-04T13:19:08.823424Z","shell.execute_reply":"2021-10-04T13:19:08.830544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if BEST_WEIGHTS:\n  WEIGHTS_FILE = get_best_weight(WEIGHTS_DIR, MODEL_TYPE)\nelse:\n  WEIGHTS_FILE = f'{WEIGHTS_DIR}/{MODEL_TYPE}.h5'","metadata":{"papermill":{"duration":0.028164,"end_time":"2021-09-06T12:31:28.626216","exception":false,"start_time":"2021-09-06T12:31:28.598052","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-04T13:19:08.854263Z","iopub.execute_input":"2021-10-04T13:19:08.854723Z","iopub.status.idle":"2021-10-04T13:19:08.871051Z","shell.execute_reply.started":"2021-10-04T13:19:08.854676Z","shell.execute_reply":"2021-10-04T13:19:08.870409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if os.path.exists(WEIGHTS_FILE):\n  !cp {WEIGHTS_FILE} ./\n  print(f'Copied best weights file to training kernel: {WEIGHTS_FILE} ...')","metadata":{"papermill":{"duration":2.556657,"end_time":"2021-09-06T12:31:31.203576","exception":false,"start_time":"2021-09-06T12:31:28.646919","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-04T13:19:08.884177Z","iopub.execute_input":"2021-10-04T13:19:08.884421Z","iopub.status.idle":"2021-10-04T13:19:10.052908Z","shell.execute_reply.started":"2021-10-04T13:19:08.884395Z","shell.execute_reply":"2021-10-04T13:19:10.051731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, re, cv2, math, glob, string, collections, numpy as np, pandas as pd, \\\n       matplotlib.pyplot as plt, tensorflow as tf, tensorflow_addons as tfa\n\nfrom tqdm import tqdm\nfrom six.moves import xrange\nfrom tensorflow.keras import layers\nfrom kaggle_datasets import KaggleDatasets","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":6.501947,"end_time":"2021-09-06T12:31:37.781835","exception":false,"start_time":"2021-09-06T12:31:31.279888","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-04T13:19:10.055199Z","iopub.execute_input":"2021-10-04T13:19:10.055443Z","iopub.status.idle":"2021-10-04T13:19:15.971428Z","shell.execute_reply.started":"2021-10-04T13:19:10.055414Z","shell.execute_reply":"2021-10-04T13:19:15.970488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTO = tf.data.AUTOTUNE\n\ntry:\n  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n  print('Device:', tpu.master())\n  tf.config.experimental_connect_to_cluster(tpu)\n  tf.tpu.experimental.initialize_tpu_system(tpu)\n  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n  TPU = True\nexcept:\n  strategy = tf.distribute.get_strategy()\n  TPU = False\nprint('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"papermill":{"duration":5.630035,"end_time":"2021-09-06T12:31:43.432744","exception":false,"start_time":"2021-09-06T12:31:37.802709","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-04T13:19:15.973059Z","iopub.execute_input":"2021-10-04T13:19:15.973388Z","iopub.status.idle":"2021-10-04T13:19:20.96663Z","shell.execute_reply.started":"2021-10-04T13:19:15.97335Z","shell.execute_reply":"2021-10-04T13:19:20.96597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GCS_BUCKETS = ['rsna-brain-tumor-classification-raw-tfrecords',  \n               'rsna-brain-tumor-classification-resample-tfrecords']    \\\n              if INPUT_DEPTH == 32 else                                 \\\n              ['rsna-brain-tumor-raw-tfrecords-x16',\n               'rsna-brain-tumor-resampled-tfrecords-x16']\nGCS_PATHS= [KaggleDatasets().get_gcs_path(GB) for GB in GCS_BUCKETS]\n[print(f'Reading tfrecords from GCS bucket {GCS_BUCKETS[i]}: {GP} ...') \\\n       for i, GP in enumerate(GCS_PATHS)]","metadata":{"papermill":{"duration":0.882966,"end_time":"2021-09-06T12:31:44.337246","exception":false,"start_time":"2021-09-06T12:31:43.45428","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-04T13:19:20.968918Z","iopub.execute_input":"2021-10-04T13:19:20.969199Z","iopub.status.idle":"2021-10-04T13:19:22.016278Z","shell.execute_reply.started":"2021-10-04T13:19:20.969163Z","shell.execute_reply":"2021-10-04T13:19:22.015512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def deserialize_example(serialized_string):\n  image_feature_description = {'image': tf.io.FixedLenFeature([], tf.string),\n                               'MGMT_value': tf.io.FixedLenFeature([], tf.float32)}\n  parsed_record = tf.io.parse_single_example(serialized_string, image_feature_description)\n  image = tf.io.decode_raw(parsed_record['image'], tf.float32)\n  image = tf.reshape(image,[DATA_HEIGHT, DATA_WIDTH, DATA_DEPTH, DATA_CHANNELS])\n    \n  splitted_modalities = tf.split(tf.cast(image, tf.float32), DATA_CHANNELS, axis=-1)  \n  splitted_modalities = [tf.squeeze(i, axis=-1) for i in splitted_modalities] \n    \n  flair_augment_img = []\n  t1w_augment_img = []\n  t1wce_augment_img = []\n  t2w_augment_img = []\n    \n  for j, modality in enumerate(splitted_modalities):\n    splitted_frames = tf.split(tf.cast(modality, tf.float32), modality.shape[-1], axis=-1)\n    for i, img in enumerate(splitted_frames):\n      img = tf.image.resize(img, [INPUT_HEIGHT, INPUT_WIDTH])  \n      if j == 0:\n        flair_augment_img.append(img)\n      elif j == 1: \n        t1w_augment_img.append(img)\n      elif j == 2:\n        t1wce_augment_img.append(img)\n      elif j == 3:\n        t2w_augment_img.append(img)\n  image = []\n  if 'FLAIR' in MODALITIES:\n    image.append(flair_augment_img)\n  if 'T1w' in MODALITIES:\n    image.append(t1w_augment_img)\n  if 'T1wCE' in MODALITIES:\n    image.append(t1wce_augment_img)\n  if 'T2w' in MODALITIES:\n    image.append(t2w_augment_img)\n  image = tf.transpose(image)\n  image = tf.reshape(image, [INPUT_HEIGHT, INPUT_WIDTH, INPUT_DEPTH, INPUT_CHANNELS])          \n  label = parsed_record['MGMT_value']\n  return image, label","metadata":{"papermill":{"duration":0.032522,"end_time":"2021-09-06T12:31:44.392509","exception":false,"start_time":"2021-09-06T12:31:44.359987","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-04T13:19:22.017611Z","iopub.execute_input":"2021-10-04T13:19:22.01792Z","iopub.status.idle":"2021-10-04T13:19:22.031182Z","shell.execute_reply.started":"2021-10-04T13:19:22.017882Z","shell.execute_reply":"2021-10-04T13:19:22.030359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_gcs_files = [tf.io.gfile.glob(os.path.join(GCS_PATH,\n                    'brain_tumor_classification_*train*.tfrec')) for GCS_PATH in GCS_PATHS]\nval_gcs_files = [tf.io.gfile.glob(os.path.join(GCS_PATH,\n                    'brain_tumor_classification_*val*.tfrec')) for GCS_PATH in GCS_PATHS]                                    \ntrain_set = tf.data.TFRecordDataset(train_gcs_files, compression_type='GZIP', \n    num_parallel_reads=AUTO).map(deserialize_example).batch(BATCH_SIZE).prefetch(AUTO)\nval_set = tf.data.TFRecordDataset(val_gcs_files, compression_type='GZIP', \n    num_parallel_reads=AUTO).map(deserialize_example).batch(BATCH_SIZE).prefetch(AUTO)","metadata":{"papermill":{"duration":0.46868,"end_time":"2021-09-06T12:31:44.883283","exception":false,"start_time":"2021-09-06T12:31:44.414603","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-04T13:19:22.032339Z","iopub.execute_input":"2021-10-04T13:19:22.032558Z","iopub.status.idle":"2021-10-04T13:19:24.150818Z","shell.execute_reply.started":"2021-10-04T13:19:22.032534Z","shell.execute_reply":"2021-10-04T13:19:24.150005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = train_set.take(1)\nfor x, y in d:\n  image, label = x, y\nimg_id = np.random.randint(0, BATCH_SIZE)\nchannel = np.random.randint(0, INPUT_CHANNELS)\n\nplt.figure(figsize=(20,10), facecolor=(0,0,0))\ncols = INPUT_DEPTH//4\nrows = 4\n\nplt.axis(\"off\")\nfor layer_idx in range(INPUT_DEPTH):\n  ax = plt.subplot(rows,cols,layer_idx+1)\n  ax.imshow(np.squeeze(image[img_id,:,:,layer_idx,channel]), cmap='gray')\n  ax.axis(\"off\")\n  ax.set_title(str(layer_idx+1),color='r',y=-0.01)\n    \nplt.suptitle(f\"Batch image no.: {img_id}, MRI modality: {MODALITIES[channel]}, \\\n             Shape: {image[img_id].shape}\", color=\"w\")\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.show()","metadata":{"papermill":{"duration":13.998539,"end_time":"2021-09-06T12:31:58.903957","exception":false,"start_time":"2021-09-06T12:31:44.905418","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-04T13:19:24.151926Z","iopub.execute_input":"2021-10-04T13:19:24.152757Z","iopub.status.idle":"2021-10-04T13:19:37.553261Z","shell.execute_reply.started":"2021-10-04T13:19:24.15272Z","shell.execute_reply":"2021-10-04T13:19:37.55243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.close()","metadata":{"papermill":{"duration":0.049525,"end_time":"2021-09-06T12:31:58.996465","exception":false,"start_time":"2021-09-06T12:31:58.94694","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-04T13:19:37.554251Z","iopub.execute_input":"2021-10-04T13:19:37.554631Z","iopub.status.idle":"2021-10-04T13:19:37.558704Z","shell.execute_reply.started":"2021-10-04T13:19:37.554603Z","shell.execute_reply":"2021-10-04T13:19:37.557832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_legacy_interface(allowed_positional_args=None,\n                              conversions=None,\n                              preprocessor=None,\n                              value_conversions=None):\n    allowed_positional_args = allowed_positional_args or []\n    conversions = conversions or []\n    value_conversions = value_conversions or []\n\n    def legacy_support(func):\n        @six.wraps(func)\n        def wrapper(*args, **kwargs):\n            layer_name = args[0].__class__.__name__\n            if preprocessor:\n                args, kwargs, converted = preprocessor(args, kwargs)\n            else:\n                converted = []\n            if len(args) > len(allowed_positional_args) + 1:\n                raise TypeError('Layer `' + layer_name +\n                                '` can accept only ' +\n                                str(len(allowed_positional_args)) +\n                                ' positional arguments (' +\n                                str(allowed_positional_args) + '), but '\n                                'you passed the following '\n                                'positional arguments: ' +\n                                str(args[1:]))\n            for key in value_conversions:\n                if key in kwargs:\n                    old_value = kwargs[key]\n                    if old_value in value_conversions[key]:\n                        kwargs[key] = value_conversions[key][old_value]\n            for old_name, new_name in conversions:\n                if old_name in kwargs:\n                    value = kwargs.pop(old_name)\n                    kwargs[new_name] = value\n                    converted.append((new_name, old_name))\n            if converted:\n                signature = '`' + layer_name + '('\n                for value in args[1:]:\n                    if isinstance(value, six.string_types):\n                        signature += '\"' + value + '\"'\n                    else:\n                        signature += str(value)\n                    signature += ', '\n                for i, (name, value) in enumerate(kwargs.items()):\n                    signature += name + '='\n                    if isinstance(value, six.string_types):\n                        signature += '\"' + value + '\"'\n                    else:\n                        signature += str(value)\n                    if i < len(kwargs) - 1:\n                        signature += ', '\n                signature += ')`'\n                warnings.warn('Update your `' + layer_name +\n                              '` layer call to the Keras 2 API: ' + signature)\n            return func(*args, **kwargs)\n        return wrapper\n    return legacy_support\n\n\ndef conv3d_args_preprocessor(args, kwargs):\n    if len(args) > 5:\n        raise TypeError('Layer can receive at most 4 positional arguments.')\n    if len(args) == 5:\n        if isinstance(args[2], int) and isinstance(args[3], int) and isinstance(args[4], int):\n            kernel_size = (args[2], args[3], args[4])\n            args = [args[0], args[1], kernel_size]\n    elif len(args) == 4 and isinstance(args[3], int):\n        if isinstance(args[2], int) and isinstance(args[3], int):\n            new_keywords = ['padding', 'strides', 'data_format']\n            for kwd in new_keywords:\n                if kwd in kwargs:\n                    raise ValueError(\n                        'It seems that you are using the Keras 2 '\n                        'and you are passing both `kernel_size` and `strides` '\n                        'as integer positional arguments. For safety reasons, '\n                        'this is disallowed. Pass `strides` '\n                        'as a keyword argument instead.')\n        if 'kernel_dim3' in kwargs:\n            kernel_size = (args[2], args[3], kwargs.pop('kernel_dim3'))\n            args = [args[0], args[1], kernel_size]\n    elif len(args) == 3:\n        if 'kernel_dim2' in kwargs and 'kernel_dim3' in kwargs:\n            kernel_size = (args[2],\n                           kwargs.pop('kernel_dim2'),\n                           kwargs.pop('kernel_dim3'))\n            args = [args[0], args[1], kernel_size]\n    elif len(args) == 2:\n        if 'kernel_dim1' in kwargs and 'kernel_dim2' in kwargs and 'kernel_dim3' in kwargs:\n            kernel_size = (kwargs.pop('kernel_dim1'),\n                           kwargs.pop('kernel_dim2'),\n                           kwargs.pop('kernel_dim3'))\n            args = [args[0], args[1], kernel_size]\n    return args, kwargs, [('kernel_size', 'kernel_dim*')]\n\n\ndef _preprocess_padding(padding):\n    if padding == 'same':\n        padding = 'SAME'\n    elif padding == 'valid':\n        padding = 'VALID'\n    else:\n        raise ValueError('Invalid padding: ' + str(padding))\n    return padding\n\n\ndef dtype(x):\n    return x.dtype.base_dtype.name\n\n\ndef _has_nchw_support():\n    return True\n\n\ndef _preprocess_conv3d_input(x, data_format):\n    if (dtype(x) == 'float64' and\n            StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):\n        x = tf.cast(x, 'float32')\n    tf_data_format = 'NDHWC'\n    return x, tf_data_format","metadata":{"execution":{"iopub.status.busy":"2021-10-04T13:19:37.560166Z","iopub.execute_input":"2021-10-04T13:19:37.560719Z","iopub.status.idle":"2021-10-04T13:19:37.593785Z","shell.execute_reply.started":"2021-10-04T13:19:37.560681Z","shell.execute_reply":"2021-10-04T13:19:37.593029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def depthwise_conv3d_args_preprocessor(args, kwargs):\n    converted = []\n\n    if 'init' in kwargs:\n        init = kwargs.pop('init')\n        kwargs['depthwise_initializer'] = init\n        converted.append(('init', 'depthwise_initializer'))\n\n    args, kwargs, _converted = conv3d_args_preprocessor(args, kwargs)\n    return args, kwargs, converted + _converted\n\n    legacy_depthwise_conv3d_support = generate_legacy_interface(\n    allowed_positional_args=['filters', 'kernel_size'],\n    conversions=[('nb_filter', 'filters'),\n                 ('subsample', 'strides'),\n                 ('border_mode', 'padding'),\n                 ('dim_ordering', 'data_format'),\n                 ('b_regularizer', 'bias_regularizer'),\n                 ('b_constraint', 'bias_constraint'),\n                 ('bias', 'use_bias')],\n    value_conversions={'dim_ordering': {'tf': 'channels_last',\n                                        'th': 'channels_first',\n                                        'default': None}},\n    preprocessor=depthwise_conv3d_args_preprocessor)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T13:19:37.596205Z","iopub.execute_input":"2021-10-04T13:19:37.596567Z","iopub.status.idle":"2021-10-04T13:19:37.609549Z","shell.execute_reply.started":"2021-10-04T13:19:37.596538Z","shell.execute_reply":"2021-10-04T13:19:37.608957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(_KERAS_BACKEND, _KERAS_LAYERS, \n _KERAS_MODELS, _KERAS_UTILS) = (tf.keras.backend, tf.keras.layers, \n                                 tf.keras.models, tf.keras.utils)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T13:19:37.610508Z","iopub.execute_input":"2021-10-04T13:19:37.610815Z","iopub.status.idle":"2021-10-04T13:19:37.628692Z","shell.execute_reply.started":"2021-10-04T13:19:37.61079Z","shell.execute_reply":"2021-10-04T13:19:37.628062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_submodules_from_kwargs(kwargs):\n  backend = kwargs.get('backend', _KERAS_BACKEND)\n  layers  = kwargs.get('layers', _KERAS_LAYERS)\n  models  = kwargs.get('models', _KERAS_MODELS)\n  utils   = kwargs.get('utils', _KERAS_UTILS)\n  for key in kwargs.keys():\n    if key not in ['backend', 'layers', 'models', 'utils']:\n      raise TypeError('Invalid keyword argument: %s', key)\n  return backend, layers, models, utils","metadata":{"execution":{"iopub.status.busy":"2021-10-04T13:19:37.629556Z","iopub.execute_input":"2021-10-04T13:19:37.630269Z","iopub.status.idle":"2021-10-04T13:19:37.64006Z","shell.execute_reply.started":"2021-10-04T13:19:37.630241Z","shell.execute_reply":"2021-10-04T13:19:37.639157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DepthwiseConv3D(tf.keras.layers.Conv3D):\n    def __init__(self,\n                 kernel_size,\n                 strides=(1, 1, 1),\n                 padding='valid',\n                 depth_multiplier=1,\n                 groups=None,\n                 data_format=None,\n                 activation=None,\n                 use_bias=True,\n                 depthwise_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n                 dilation_rate = (1, 1, 1),\n                 depthwise_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 depthwise_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super(DepthwiseConv3D, self).__init__(\n            filters=None,\n            kernel_size=kernel_size,\n            strides=strides,\n            padding=padding,\n            data_format=data_format,\n            activation=activation,\n            use_bias=use_bias,\n            bias_regularizer=bias_regularizer,\n            dilation_rate=dilation_rate,\n            activity_regularizer=activity_regularizer,\n            bias_constraint=bias_constraint,\n            **kwargs)\n        self.depth_multiplier = depth_multiplier\n        self.groups = groups\n        self.depthwise_initializer = tf.keras.initializers.get(depthwise_initializer)\n        self.depthwise_regularizer = tf.keras.regularizers.get(depthwise_regularizer)\n        self.depthwise_constraint = tf.keras.constraints.get(depthwise_constraint)\n        self.bias_initializer = tf.keras.initializers.get(bias_initializer)\n        self.dilation_rate = dilation_rate\n        self._padding = _preprocess_padding(self.padding)\n        self._strides = (1,) + self.strides + (1,)\n        self._data_format = \"NDHWC\"\n        self.input_dim = None\n\n    def build(self, input_shape):\n        if len(input_shape) < 5:\n            raise ValueError('Inputs to `DepthwiseConv3D` should have rank 5. '\n                             'Received input shape:', str(input_shape))\n        if self.data_format == 'channels_first':\n            channel_axis = 1\n        else:\n            channel_axis = -1\n        if input_shape[channel_axis] is None:\n            raise ValueError('The channel dimension of the inputs to '\n                             '`DepthwiseConv3D` '\n                             'should be defined. Found `None`.')\n        self.input_dim = int(input_shape[channel_axis])\n\n        if self.groups is None:\n            self.groups = self.input_dim\n\n        if self.groups > self.input_dim:\n            raise ValueError('The number of groups cannot exceed the number of channels')\n\n        if self.input_dim % self.groups != 0:\n            raise ValueError('Warning! The channels dimension is not divisible by the group size chosen')\n\n        depthwise_kernel_shape = (self.kernel_size[0],\n                                  self.kernel_size[1],\n                                  self.kernel_size[2],\n                                  self.input_dim,\n                                  self.depth_multiplier)\n\n        self.depthwise_kernel = self.add_weight(\n            shape=depthwise_kernel_shape,\n            initializer=self.depthwise_initializer,\n            name='depthwise_kernel',\n            regularizer=self.depthwise_regularizer,\n            constraint=self.depthwise_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.groups * self.depth_multiplier,),\n                                        initializer=self.bias_initializer,\n                                        name='bias',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.input_spec = tf.keras.layers.InputSpec(ndim=5, axes={channel_axis: self.input_dim})\n        self.built = True\n\n    def call(self, inputs, training=None):\n        inputs = _preprocess_conv3d_input(inputs, self.data_format)\n\n        if self.data_format == 'channels_last':\n            dilation = (1,) + self.dilation_rate + (1,)\n        else:\n            dilation = self.dilation_rate + (1,) + (1,)\n\n        if self._data_format == 'NCDHW':\n            outputs = tf.concat(\n                [tf.nn.conv3d(inputs[0][:, i:i+self.input_dim//self.groups, :, :, :], self.depthwise_kernel[:, :, :, i:i+self.input_dim//self.groups, :],\n                    strides=self._strides,\n                    padding=self._padding,\n                    dilations=dilation,\n                    data_format=self._data_format) for i in range(0, self.input_dim, self.input_dim//self.groups)], axis=1)\n\n        else:\n            outputs = tf.concat(\n                [tf.nn.conv3d(inputs[0][:, :, :, :, i:i+self.input_dim//self.groups], self.depthwise_kernel[:, :, :, i:i+self.input_dim//self.groups, :],\n                    strides=self._strides,\n                    padding=self._padding,\n                    dilations=dilation,\n                    data_format=self._data_format) for i in range(0, self.input_dim, self.input_dim//self.groups)], axis=-1)\n\n        if self.bias is not None:\n            outputs = K.bias_add(\n                outputs,\n                self.bias,\n                data_format=self.data_format)\n\n        if self.activation is not None:\n            return self.activation(outputs)\n\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        if self.data_format == 'channels_first':\n            depth = input_shape[2]\n            rows = input_shape[3]\n            cols = input_shape[4]\n            out_filters = self.groups * self.depth_multiplier\n        elif self.data_format == 'channels_last':\n            depth = input_shape[1]\n            rows = input_shape[2]\n            cols = input_shape[3]\n            out_filters = self.groups * self.depth_multiplier\n\n        depth = conv_utils.conv_output_length(depth, self.kernel_size[0],\n                                             self.padding,\n                                             self.strides[0])\n\n        rows = conv_utils.conv_output_length(rows, self.kernel_size[1],\n                                             self.padding,\n                                             self.strides[1])\n\n        cols = conv_utils.conv_output_length(cols, self.kernel_size[2],\n                                             self.padding,\n                                             self.strides[2])\n\n        if self.data_format == 'channels_first':\n            return (input_shape[0], out_filters, depth, rows, cols)\n\n        elif self.data_format == 'channels_last':\n            return (input_shape[0], depth, rows, cols, out_filters)\n\n    def get_config(self):\n        config = super(DepthwiseConv3D, self).get_config()\n        config.pop('filters')\n        config.pop('kernel_initializer')\n        config.pop('kernel_regularizer')\n        config.pop('kernel_constraint')\n        config['depth_multiplier'] = self.depth_multiplier\n        config['depthwise_initializer'] = tf.keras.initializers.serialize(self.depthwise_initializer)\n        config['depthwise_regularizer'] = tf.keras.regularizers.serialize(self.depthwise_regularizer)\n        config['depthwise_constraint'] = tf.keras.constraints.serialize(self.depthwise_constraint)\n        return config","metadata":{"execution":{"iopub.status.busy":"2021-10-04T13:19:37.641555Z","iopub.execute_input":"2021-10-04T13:19:37.642263Z","iopub.status.idle":"2021-10-04T13:19:37.677735Z","shell.execute_reply.started":"2021-10-04T13:19:37.642229Z","shell.execute_reply":"2021-10-04T13:19:37.676847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DepthwiseConvolution3D = DepthwiseConv3D\n\n(backend, layers, models, keras_utils) = (tf.keras.backend, tf.keras.layers, \n                                          tf.keras.models, tf.keras.utils)\n\n# Code of this model implementation is mostly written by\n# Bj√∂rn Barz ([@Callidior](https://github.com/Callidior))\n\nBlockArgs = collections.namedtuple('BlockArgs', [\n    'kernel_size', 'num_repeat', 'input_filters', 'output_filters',\n    'expand_ratio', 'id_skip', 'strides', 'se_ratio'\n])\nBlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n\nDEFAULT_BLOCKS_ARGS = [\n    BlockArgs(kernel_size=3, num_repeat=1, input_filters=32, output_filters=16,\n              expand_ratio=1, id_skip=True, strides=[1, 1, 1], se_ratio=0.25),\n    BlockArgs(kernel_size=3, num_repeat=2, input_filters=16, output_filters=24,\n              expand_ratio=6, id_skip=True, strides=[2, 2, 2], se_ratio=0.25),\n    BlockArgs(kernel_size=5, num_repeat=2, input_filters=24, output_filters=40,\n              expand_ratio=6, id_skip=True, strides=[2, 2, 2], se_ratio=0.25),\n    BlockArgs(kernel_size=3, num_repeat=3, input_filters=40, output_filters=80,\n              expand_ratio=6, id_skip=True, strides=[2, 2, 2], se_ratio=0.25),\n    BlockArgs(kernel_size=5, num_repeat=3, input_filters=80, output_filters=112,\n              expand_ratio=6, id_skip=True, strides=[1, 1, 1], se_ratio=0.25),\n    BlockArgs(kernel_size=5, num_repeat=4, input_filters=112, output_filters=192,\n              expand_ratio=6, id_skip=True, strides=[2, 2, 2], se_ratio=0.25),\n    BlockArgs(kernel_size=3, num_repeat=1, input_filters=192, output_filters=320,\n              expand_ratio=6, id_skip=True, strides=[1, 1, 1], se_ratio=0.25)\n]\n\nCONV_KERNEL_INITIALIZER = {'class_name': 'VarianceScaling',\n  'config': {'scale': 2.0, 'mode': 'fan_out',\n             'distribution': 'normal'}}\n\nDENSE_KERNEL_INITIALIZER = {'class_name': 'VarianceScaling',\n             'config': {'scale': 1. / 3., 'mode': 'fan_out',\n                        'distribution': 'uniform'}}\n\n\ndef preprocess_input(x, **kwargs):\n    kwargs = {k: v for k, v in kwargs.items() if k in ['backend', 'layers', 'models', 'utils']}\n    return _preprocess_input(x, mode='tensorflow', **kwargs)\n\n\ndef get_swish(**kwargs):\n    (backend, layers, models, keras_utils) = (tf.keras.backend, tf.keras.layers, \n                                              tf.keras.models, tf.keras.utils)\n    def swish(x):\n        return tf.nn.swish(x)\n    return swish\n\ndef get_dropout(**kwargs):\n    (backend, layers, models, keras_utils) = (tf.keras.backend, tf.keras.layers, \n                                              tf.keras.models, tf.keras.utils)\n\n    class FixedDropout(layers.Dropout):\n        def _get_noise_shape(self, inputs):\n            if self.noise_shape is None:\n                return self.noise_shape\n\n            symbolic_shape = backend.shape(inputs)\n            noise_shape = [symbolic_shape[axis] if shape is None else shape\n                           for axis, shape in enumerate(self.noise_shape)]\n            return tuple(noise_shape)\n\n    return FixedDropout\n\n\ndef round_filters(filters, width_coefficient, depth_divisor):\n    filters *= width_coefficient\n    new_filters = int(filters + depth_divisor / 2) // depth_divisor * depth_divisor\n    new_filters = max(depth_divisor, new_filters)\n    if new_filters < 0.9 * filters:\n        new_filters += depth_divisor\n    return int(new_filters)\n\n\ndef round_repeats(repeats, depth_coefficient):\n    return int(math.ceil(depth_coefficient * repeats))\n\n\ndef mb_conv_block(inputs, block_args, activation, drop_rate=None, prefix='', ):\n    has_se = (block_args.se_ratio is not None) and (0 < block_args.se_ratio <= 1)\n    bn_axis = 4 #if backend.image_data_format() == 'channels_last' else 1\n\n    # workaround over non working dropout with None in noise_shape in tf.keras\n    Dropout = get_dropout(\n        backend=backend,\n        layers=layers,\n        models=models,\n        utils=keras_utils\n    )\n\n    # Expansion phase\n    filters = block_args.input_filters * block_args.expand_ratio\n    if block_args.expand_ratio != 1:\n        x = tf.keras.layers.Conv3D(filters, 1,\n                          padding='same',\n                          use_bias=False,\n                          kernel_initializer=CONV_KERNEL_INITIALIZER,\n                          name=prefix + 'expand_conv')(inputs)\n        x = tf.keras.layers.BatchNormalization(axis=bn_axis, name=prefix + 'expand_bn')(x)\n        x = tf.keras.layers.Activation(activation, name=prefix + 'expand_activation')(x)\n    else:\n        x = inputs\n\n    # Depthwise Convolution\n    x = DepthwiseConv3D(block_args.kernel_size,\n                               strides=block_args.strides,\n                               padding='same',\n                               use_bias=False,\n                               depthwise_initializer=CONV_KERNEL_INITIALIZER,\n                               name=prefix + 'dwconv')(x)\n    x = tf.keras.layers.BatchNormalization(axis=bn_axis, name=prefix + 'bn')(x)\n    x = tf.keras.layers.Activation(activation, name=prefix + 'activation')(x)\n\n    # Squeeze and Excitation phase\n    if has_se:\n        num_reduced_filters = max(1, int(\n            block_args.input_filters * block_args.se_ratio\n        ))\n        se_tensor = tf.keras.layers.GlobalAveragePooling3D(name=prefix + 'se_squeeze')(x)\n\n        target_shape = (1, 1, 1, filters) if backend.image_data_format() == 'channels_last' else (filters, 1, 1, 1)\n        se_tensor = tf.keras.layers.Reshape(target_shape, name=prefix + 'se_reshape')(se_tensor)\n        se_tensor = tf.keras.layers.Conv3D(num_reduced_filters, 1,\n                                  activation=activation,\n                                  padding='same',\n                                  use_bias=True,\n                                  kernel_initializer=CONV_KERNEL_INITIALIZER,\n                                  name=prefix + 'se_reduce')(se_tensor)\n        se_tensor = tf.keras.layers.Conv3D(filters, 1,\n                                  activation='sigmoid',\n                                  padding='same',\n                                  use_bias=True,\n                                  kernel_initializer=CONV_KERNEL_INITIALIZER,\n                                  name=prefix + 'se_expand')(se_tensor)\n        if backend.backend() == 'theano':\n            # For the Theano backend, we have to explicitly make\n            # the excitation weights broadcastable.\n            pattern = ([True, True, True, True, False] if backend.image_data_format() == 'channels_last'\n                       else [True, False, True, True, True])\n            se_tensor = layers.Lambda(\n                lambda x: backend.pattern_broadcast(x, pattern),\n                name=prefix + 'se_broadcast')(se_tensor)\n        x = layers.multiply([x, se_tensor], name=prefix + 'se_excite')\n\n    # Output phase\n    x = layers.Conv3D(block_args.output_filters, 1,\n                      padding='same',\n                      use_bias=False,\n                      kernel_initializer=CONV_KERNEL_INITIALIZER,\n                      name=prefix + 'project_conv')(x)\n    x = layers.BatchNormalization(axis=bn_axis, name=prefix + 'project_bn')(x)\n    if block_args.id_skip and all(\n            s == 1 for s in block_args.strides\n    ) and block_args.input_filters == block_args.output_filters:\n        if drop_rate and (drop_rate > 0):\n            x = Dropout(drop_rate,\n                        noise_shape=(None, 1, 1, 1, 1),\n                        name=prefix + 'drop')(x)\n        x = layers.add([x, inputs], name=prefix + 'add')\n\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-10-04T13:19:37.679174Z","iopub.execute_input":"2021-10-04T13:19:37.679689Z","iopub.status.idle":"2021-10-04T13:19:37.7186Z","shell.execute_reply.started":"2021-10-04T13:19:37.679658Z","shell.execute_reply":"2021-10-04T13:19:37.717666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def EfficientNet3D(width_coefficient, depth_coefficient, default_resolution,\n                   dropout_rate=0.2, drop_connect_rate=0.2, depth_divisor=8,\n                   blocks_args=DEFAULT_BLOCKS_ARGS, include_top=False,\n                   model_name='efficientnet3d', weights=None, input_tensor=None, \n                   input_shape=None, pooling=None, classes=1000, **kwargs):\n    \n    global backend, layers, models, keras_utils\n    backend, layers, models, keras_utils = get_submodules_from_kwargs(kwargs)\n\n    if weights is not None and not os.path.exists(weights):\n        raise ValueError('The `weights` argument should be '\n                         'a valid path to the weights file ...')\n\n    if input_tensor is None:\n        img_input = layers.Input(shape=input_shape)\n    else:\n        img_input = input_tensor\n\n    bn_axis = 4 if backend.image_data_format() == 'channels_last' else 1\n    activation = get_swish(**kwargs)\n\n    # Build stem\n    x = img_input\n    x = layers.Conv3D(round_filters(32, width_coefficient, depth_divisor), 3,\n                      strides=(2, 2, 2),\n                      padding='same',\n                      use_bias=False,\n                      kernel_initializer=CONV_KERNEL_INITIALIZER,\n                      name='stem_conv')(x)\n    x = layers.BatchNormalization(axis=bn_axis, name='stem_bn')(x)\n    x = layers.Activation(activation, name='stem_activation')(x)\n\n    # Build blocks\n    num_blocks_total = sum(block_args.num_repeat for block_args in blocks_args)\n    block_num = 0\n    for idx, block_args in enumerate(blocks_args):\n        assert block_args.num_repeat > 0\n        # Update block input and output filters based on depth multiplier.\n        block_args = block_args._replace(\n            input_filters=round_filters(block_args.input_filters,\n                                        width_coefficient, depth_divisor),\n            output_filters=round_filters(block_args.output_filters,\n                                         width_coefficient, depth_divisor),\n            num_repeat=round_repeats(block_args.num_repeat, depth_coefficient))\n\n        # The first block needs to take care of stride and filter size increase.\n        drop_rate = drop_connect_rate * float(block_num) / num_blocks_total\n        x = mb_conv_block(x, block_args,\n                          activation=activation,\n                          drop_rate=drop_rate,\n                          prefix='block{}a_'.format(idx + 1))\n        block_num += 1\n        if block_args.num_repeat > 1:\n            # pylint: disable=protected-access\n            block_args = block_args._replace(\n                input_filters=block_args.output_filters, strides=[1, 1, 1])\n            # pylint: enable=protected-access\n            for bidx in xrange(block_args.num_repeat - 1):\n                drop_rate = drop_connect_rate * float(block_num) / num_blocks_total\n                block_prefix = 'block{}{}_'.format(\n                    idx + 1,\n                    string.ascii_lowercase[bidx + 1]\n                )\n                x = mb_conv_block(x, block_args,\n                                  activation=activation,\n                                  drop_rate=drop_rate,\n                                  prefix=block_prefix)\n                block_num += 1\n\n    # Build top\n    x = layers.Conv3D(round_filters(1280, width_coefficient, depth_divisor), 1,\n                      padding='same',\n                      use_bias=False,\n                      kernel_initializer=CONV_KERNEL_INITIALIZER,\n                      name='top_conv')(x)\n    x = layers.BatchNormalization(axis=bn_axis, name='top_bn')(x)\n    x = layers.Activation(activation, name='top_activation')(x)\n    if include_top:\n        x = layers.GlobalAveragePooling3D(name='avg_pool')(x)\n        if dropout_rate and dropout_rate > 0:\n            x = layers.Dropout(dropout_rate, name='top_dropout')(x)\n        x = layers.Dense(classes,\n                         activation='softmax',\n                         kernel_initializer=DENSE_KERNEL_INITIALIZER,\n                         name='probs')(x)\n    else:\n        if pooling == 'avg':\n            x = layers.GlobalAveragePooling3D(name='avg_pool')(x)\n        elif pooling == 'max':\n            x = layers.GlobalMaxPooling3D(name='max_pool')(x)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = keras_utils.get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n\n    # Create model.\n    model = models.Model(inputs, x, name=model_name)\n\n    if weights is not None and os.path.exists(weights):\n      model.load_weights(weights)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-10-04T13:19:37.719906Z","iopub.execute_input":"2021-10-04T13:19:37.720144Z","iopub.status.idle":"2021-10-04T13:19:37.742127Z","shell.execute_reply.started":"2021-10-04T13:19:37.720117Z","shell.execute_reply":"2021-10-04T13:19:37.741217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def EfficientNet3D_B0(include_top=False, input_tensor=None, input_shape=None,\n                      weights=None, pooling=None, classes=1000, **kwargs):\n    return EfficientNet3D(1.0, 1.0, 224, 0.2, model_name='efficientnet3d-b0',\n                          include_top=include_top, weights=None,\n                          input_tensor=input_tensor, input_shape=input_shape,\n                          pooling=pooling, classes=classes, **kwargs)\n\ndef EfficientNet3D_B1(include_top=False, input_tensor=None, input_shape=None,\n                      pooling=None, classes=1000, **kwargs):\n    return EfficientNet3D(1.0, 1.1, 240, 0.2, model_name='efficientnet3d-b1',\n                          include_top=include_top, weights=weights,\n                          input_tensor=input_tensor, input_shape=input_shape,\n                          pooling=pooling, classes=classes, **kwargs)\n\ndef EfficientNet3D_B2(include_top=False, input_tensor=None, input_shape=None,\n                      pooling=None, classes=1000, **kwargs):\n    return EfficientNet3D(1.1, 1.2, 260, 0.3, model_name='efficientnet3d-b2',\n                          include_top=include_top, weights=weights,\n                          input_tensor=input_tensor, input_shape=input_shape,\n                          pooling=pooling, classes=classes, **kwargs)\n\ndef EfficientNet3D_B3(include_top=False, input_tensor=None, input_shape=None,\n                      pooling=None, classes=1000, **kwargs):\n    return EfficientNet3D(1.2, 1.4, 300, 0.3, model_name='efficientnet3d-b3',\n                          include_top=include_top, weights=weights,\n                          input_tensor=input_tensor, input_shape=input_shape,\n                          pooling=pooling, classes=classes, **kwargs)\n\ndef EfficientNet3D_B4(include_top=False, input_tensor=None, input_shape=None,\n                      pooling=None, classes=1000, **kwargs):\n    return EfficientNet3D(1.4, 1.8, 380, 0.4, model_name='efficientnet3d-b4',\n                          include_top=include_top, weights=weights,\n                          input_tensor=input_tensor, input_shape=input_shape,\n                          pooling=pooling, classes=classes, **kwargs)\n\ndef EfficientNet3D_B5(include_top=False, input_tensor=None, input_shape=None,\n                      pooling=None, classes=1000, **kwargs):\n    return EfficientNet3D(1.6, 2.2, 456, 0.4, model_name='efficientnet3d-b5',\n                          include_top=include_top, weights=weights,\n                          input_tensor=input_tensor, input_shape=input_shape,\n                          pooling=pooling, classes=classes, **kwargs)\n\ndef EfficientNet3D_B6(include_top=False, input_tensor=None, input_shape=None,\n                      pooling=None, classes=1000, **kwargs):\n    return EfficientNet3D(1.8, 2.6, 528, 0.5, model_name='efficientnet3d-b6',\n                          include_top=include_top, weights=weights,\n                          input_tensor=input_tensor, input_shape=input_shape,\n                          pooling=pooling, classes=classes, **kwargs)\n\ndef EfficientNet3D_B7(include_top=False, input_tensor=None, input_shape=None,\n                      pooling=None, classes=1000, **kwargs):\n    return EfficientNet3D(2.0, 3.1, 600, 0.5, model_name='efficientnet3d-b7',\n                          include_top=include_top, weights=weights,\n                          input_tensor=input_tensor, input_shape=input_shape,\n                          pooling=pooling, classes=classes, **kwargs)\n\ndef EfficientNet3D_L2(include_top=False, input_tensor=None, input_shape=None,\n                      pooling=None, classes=1000, **kwargs):\n    return EfficientNet3D(4.3, 5.3, 800, 0.5, model_name='efficientnet3d-l2',\n                          include_top=include_top, weights=weights,\n                          input_tensor=input_tensor, input_shape=input_shape,\n                          pooling=pooling, classes=classes, **kwargs)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T13:19:37.743712Z","iopub.execute_input":"2021-10-04T13:19:37.744212Z","iopub.status.idle":"2021-10-04T13:19:37.764137Z","shell.execute_reply.started":"2021-10-04T13:19:37.744183Z","shell.execute_reply":"2021-10-04T13:19:37.763085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_efn3D(input_height, input_width, input_depth, input_channels,\n              model_type='EfficientNet3D_B0', feature_mapping=True):\n  input_tensor = tf.keras.Input((input_height, input_width, \n                        input_depth, input_channels), name='input3D')\n  if feature_mapping:\n    feat3d_map = tf.keras.layers.Conv3D(3, (3,3,3), strides=(1, 1, 1), \n                     padding='same', name='feature3D', use_bias=True)(input_tensor)\n  feat_ext = EfficientNet3D(0.45, 0.45, input_height, 0.2, model_name='efficientnet3d-b0',\n                            input_shape=(input_height, input_width, input_depth, 3 \\\n                            if feature_mapping else input_channels), \n                            include_top=False, weights=None, pooling=None)\n  output = feat_ext(feat3d_map if feature_mapping else input_tensor)\n  output = tf.keras.layers.Dropout(0.5)(output)\n  output = tf.keras.layers.GlobalAveragePooling3D()(output)\n  output = tf.keras.layers.Dropout(0.5)(output)\n  output = tf.keras.layers.Dense(1, activation='sigmoid')(output)\n  return tf.keras.Model(input_tensor, output)","metadata":{"papermill":{"duration":0.05421,"end_time":"2021-09-06T12:31:59.092957","exception":false,"start_time":"2021-09-06T12:31:59.038747","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-04T13:19:37.765749Z","iopub.execute_input":"2021-10-04T13:19:37.766091Z","iopub.status.idle":"2021-10-04T13:19:37.779471Z","shell.execute_reply.started":"2021-10-04T13:19:37.766052Z","shell.execute_reply":"2021-10-04T13:19:37.778747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n  model = get_efn3D(INPUT_HEIGHT, INPUT_WIDTH, INPUT_DEPTH, INPUT_CHANNELS,\n                    model_type=MODEL_TYPE, feature_mapping=FEATURE_MAPPING)\n\n  model.compile(loss='binary_crossentropy', \n                optimizer=tf.keras.optimizers.Nadam(lr=LEARNING_RATE),\n                metrics=['accuracy', 'AUC'])\n    \n  ckpt_cb = tf.keras.callbacks.ModelCheckpoint(\n    filepath=f'{MODEL_TYPE}'+'.{epoch:02d}-{val_auc:.4f}.h5', \n    monitor='val_auc', mode='max', save_best_only=True, verbose=1)\n\n  es_cb = tf.keras.callbacks.EarlyStopping(\n    monitor='val_auc', min_delta=0, patience=5, \n    verbose=1, mode='max', restore_best_weights=True)\n    \n  lr_reduce_cb = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.1, patience=3, verbose=1,\n    mode='min', min_delta=0.0001, cooldown=0, min_lr=1e-16) \n\n  def get_lr_callback(batch_size=8):\n    lr_start   = 0.000005\n    lr_max     = 0.00000125 * batch_size\n    lr_min     = 0.000001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n    def lrfn(epoch):\n      if epoch < lr_ramp_ep:\n        lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n      elif epoch < lr_ramp_ep + lr_sus_ep:\n        lr = lr_max\n      else:\n        lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n      return lr\n    return tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)","metadata":{"papermill":{"duration":0.914936,"end_time":"2021-09-06T12:32:00.151357","exception":false,"start_time":"2021-09-06T12:31:59.236421","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-04T13:19:37.78072Z","iopub.execute_input":"2021-10-04T13:19:37.780958Z","iopub.status.idle":"2021-10-04T13:20:18.623746Z","shell.execute_reply.started":"2021-10-04T13:19:37.78091Z","shell.execute_reply":"2021-10-04T13:20:18.622921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary(); tf.keras.backend.clear_session()\nif DEBUG:\n  for x, y in d:\n    print(x.shape, y.shape)\n  if TPU:\n    model.fit(x, y)\n  else:\n    model.predict(x, verbose=1)","metadata":{"papermill":{"duration":0.064881,"end_time":"2021-09-06T12:32:00.258444","exception":false,"start_time":"2021-09-06T12:32:00.193563","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-04T13:20:18.625087Z","iopub.execute_input":"2021-10-04T13:20:18.625328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if WEIGHTS_FILE is not None and os.path.exists(WEIGHTS_FILE):\n  model.load_weights(WEIGHTS_FILE)\n  print(f'Loaded weights from: {WEIGHTS_FILE} ...')","metadata":{"papermill":{"duration":1.355527,"end_time":"2021-09-06T12:32:01.6583","exception":false,"start_time":"2021-09-06T12:32:00.302773","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in tqdm(range(NUM_RETRIES)):\n  history = model.fit(train_set, validation_data=val_set, epochs=EPOCHS,\n              callbacks=[ckpt_cb, es_cb, lr_reduce_cb  \\\n                if LR_REDUCE else get_lr_callback(BATCH_SIZE)])","metadata":{"papermill":{"duration":13358.627011,"end_time":"2021-09-06T16:18:31.006761","exception":false,"start_time":"2021-09-06T12:35:52.37975","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":3.222295,"end_time":"2021-09-06T16:18:37.495643","exception":false,"start_time":"2021-09-06T16:18:34.273348","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}