{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --quiet attrdict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from attrdict import AttrDict\nfrom tqdm import tqdm\nimport os\nimport glob \nimport numpy as np\nfrom PIL import Image\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\nimport math\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg = AttrDict(\n    metadata=AttrDict(\n        images_path=\"/kaggle/input/rsna-miccai-png\",\n        labels_path='/kaggle/input/training-labels/train_labels.csv',\n        val_mod=3,        # 1/3 of training images are kept for validation\n        limit_first_k=None, # Load only 10 images of train/val/test\n    ),\n    dataset=AttrDict(\n        input_keys=['T1wCE'],\n        img_size=128,     # Resize images to smaller size\n    ),\n    dataloader=AttrDict(\n        batch_size=1,\n        num_workers=8,\n    )\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_metadata(images_path: str, labels_path: str, val_mod=3, limit_first_k=None):\n    result = {'train': [], 'val': [], 'test': []}\n    \n    scan_id_to_label = {}\n    with open(labels_path, 'r') as f:\n        for i, line in enumerate(f):\n            if i > 0:\n                idx, label = line.split(',')\n                scan_id_to_label[idx.strip()] = int(label.strip())\n    \n    for items_key in ['train', 'test']:\n        all_files = list(os.listdir(f\"{images_path}/{items_key}\"))\n        if limit_first_k:\n            all_files = all_files[:limit_first_k]\n            \n        for scan_id in tqdm(all_files):\n            scan_slices = {}\n\n            for filepath in glob.glob(f\"{images_path}/{items_key}/{scan_id}/*/*.png\"):\n                kind = filepath.split('/')[-2]\n                slices = scan_slices.get(kind, [])\n                slice_id = filepath.split('/')[-1].split('-')[-1].split('.')[0]\n                slices.append((slice_id, filepath))\n                scan_slices[kind] = slices\n            \n            for key in scan_slices:\n                slices = scan_slices[key]\n                slices.sort()\n                scan_slices[key] = [path for _, path in slices]\n            \n            key = items_key\n            if hash(scan_id) % val_mod == 0 and items_key == 'train':\n                key = 'val'\n                \n            result[key].append({\n                'scan': scan_id,\n                'label': scan_id_to_label.get(scan_id),\n                **scan_slices,\n            })\n    return AttrDict(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metadata = load_metadata(**cfg.metadata)\nprint(f\"{len(metadata.train)} train | {len(metadata.val)} val | {len(metadata.test)} test\")\n# print(metadata.train[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset3d(torch.utils.data.Dataset):\n    def __init__(self, metadata, input_keys, img_size=None):\n        super().__init__()\n        \n        self.metadata = metadata\n        self.input_keys = input_keys\n        self.img_size = img_size\n\n        # self.load()\n\n    def load(self, idx, prop):\n        img_size = self.img_size\n        filenames = self.metadata[idx].get(prop)\n        \n        result = []\n        for filename in filenames:\n            img = np.array(Image.open(filename))\n            img = (img / 255 - 0.5) * 2\n            img = cv.resize(img, (img_size, img_size),\n                            interpolation=cv.INTER_NEAREST)\n            result.append(img)\n        return np.array(result)\n\n    def __len__(self):\n        return len(self.metadata)\n    \n    def __getitem__(self, idx):\n        img_size = self.img_size\n        \n        result = {}\n        result['label'] = self.metadata[idx]['label']  \n        result['scan'] = self.metadata[idx]['scan']\n        for key in self.input_keys:\n            result[key] = self.load(idx, key)\n        \n        return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = AttrDict(\n  train=Dataset3d(metadata.train, **cfg.dataset),\n  val=Dataset3d(metadata.val, **cfg.dataset),\n  test=Dataset3d(metadata.test, **cfg.dataset),\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# item = dataset.train[2]\n# plt.figure(figsize=(5 * len(cfg.dataset.input_keys), 5))\n# print(f\"Scan: {item['scan']} (label: {item['label']})\")\n# for i, key in enumerate(cfg.dataset.input_keys):\n#     print(f\"  {key}: {item[key].shape}\")\n#     plt.subplot(1, len(cfg.dataset.input_keys), i + 1)\n#     plt.imshow(item[key][10])\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader = AttrDict(\n    train=torch.utils.data.DataLoader(dataset.train, shuffle=True, **cfg.dataloader),\n    val=torch.utils.data.DataLoader(dataset.train, shuffle=False, **cfg.dataloader),\n    test=torch.utils.data.DataLoader(dataset.train, shuffle=False, **cfg.dataloader),\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for batch in dataloader.train:\n#     print(batch)\n#     break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(len(metadata.train[2]['FLAIR']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader.train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### T1\nFat is depicted in white and water in black.<br/>\nThe shape of the brain can be clearly seen, and morphological abnormalities are easy to detect (Atrophy, tumors, etc.)<br/>\n\n### T2\nWater is painted white.<br/>\nLesions appear white. Suitable for lesion evaluation.<br/>\n\n### FLAIR\nIn T2, the spinal fluid (water) is white and the lesion is also white, so you have to look for the white in the white, which is difficult to understand.<br/>\nFLAIR can be roughly thought of as T2, in which the water is also black, making it easier to find the lesion.<br/>\n","metadata":{}},{"cell_type":"markdown","source":"### Observatie\n - T1w este T1 weighted pre-contrast\n - T2wCE este T1 weighted post-contrast\n - T2w este T2 weighted\n - FLAIR = Fluid Attenuated Inversion Recovery\n - fiecare folder contine tipuri diferite de RMN (contrastul difera)\n - NU exista o regula de orientare a scanarilor (de ex. T1w contine rmn in plan sagital, dar si in plan coronal sau orizontal)\n - Plan Sagital = stanga-dreapta\n - Plan Coronal = fata-spate\n - Plan Orizontal = sus-jos\n<br/>\n<br/>\n\n[Link](https://case.edu/med/neurology/NR/MRI%20Basics.htm) explicatii la ce inseamna T1w, T2w, FLAIR.<br/>","metadata":{}},{"cell_type":"markdown","source":"## #1 try - One folder only","metadata":{}},{"cell_type":"code","source":"# plt.figure(figsize=(6 * len(cfg.dataset.input_keys),6))\n# for i in range(4):\n#     plt.subplot(1, len(cfg.dataset.input_keys), i + 1)\n#     plt.imshow(dataset.train[i]['T2w'][20])\n# plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# patient = 0\n# len(dataset.train[patient]['FLAIR'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_images(dataset, patient: int, folder: str, train=True):\n    images = []\n    if train == True:\n        for img in dataset.train[patient][folder]:\n            # Exclude the blank images\n            if np.max(img)!=0:\n                images.append(img)\n            else:\n                pass\n    else:\n        for img in dataset.test[patient][folder]:\n            # Exclude the blank images\n            if np.max(img)!=0:\n                images.append(img)\n            else:\n                pass\n    \n    return images","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# patient = 1\n\n# images = get_images(dataset, patient, 'T2w')\n# print('Nr of images:', len(images))\n\n# fig = plt.figure(figsize=(50,50))\n\n# c = 1\n# for image in images:\n#     ax = fig.add_subplot(len(images)//10+1, 10, c)\n#     ax.imshow(image, cmap='gray')\n#     c+=1\n    \n#     plt.axis('off')\n    \n# fig.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(metadata.train[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label = 'T1wCE'\nmax_images, min_images = len(metadata.train[0][label]), len(metadata.train[0][label])\nfor i in metadata.train[1:]:\n    if len(i[label]) > max_images:\n        max_images = len(i[label])\n    if len(i[label]) < min_images:\n        min_images = len(i[label])\n\nprint(f'Min: {min_images}\\nMax: {max_images}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch in dataloader.train:\n    print(batch)\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dataset.train[0]['T1wCE'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\n# TODO: n/15\n\nlabel = 'T1wCE'\nsmall_dataset = []\nfor d in tqdm(dataloader.train):\n    small_dataset.append(d[label][:15])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\n# TODO: n/15\n\nlabel = 'T1wCE'\nsmall_dataset = []\nfor d in tqdm(dataloader.train):\n#     T2w_dataset.append(d['T1w'][:15])\n    nr_photos = len(d[label])\n    if nr_photos > 15:\n        m = nr_photos / 15\n        if round(m) == math.floor(m): # nu facem padding\n            for i in range(math.floor(m)):\n                small_dataset.append(d[label][i::round(m)]) # trebuie completat si pentru Y\n        else: # facem padding\n            # add (15 - nr_photos % 15) of zero(128)\n            np.append(d[label],np.zeros(((15 - nr_photos % 15), 128, 128)), axis=0)\n            m = (nr_photos + (15 - nr_photos % 15))/15\n            for i in range(m):\n                small_dataset.append(d[label][i::m])\n                      \n    else:\n          small_dataset.append(d[label])\n\n#     if d['T1w'] is None:\n#         continue\n#     else:\n#         T2w_dataset.append(d['T1w'][:15])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"small_dataset[0][0][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Convnet (nn.Module):\n    def __init__(self):\n        super(Convnet, self).__init__()\n        # 128 -> 124 -> 62\n        # 15 -> 11 ->\n        self.conv1 = nn.Sequential(\n            nn.Conv3d(\n                in_channels=1,\n                out_channels=16,\n                kernel_size=5,\n                stride=1,\n                padding=0,\n            ),\n            nn.ReLU(),\n            nn.MaxPool3d(kernel_size=2)\n        )\n        # 62 -> 58 -> 29\n        self.conv2 = nn.Sequential(\n            nn.Conv3d(\n                in_channels=16,\n                out_channels=32,\n                kernel_size=5,\n                stride=1,\n                padding=0,\n            ),\n            nn.ReLU(),\n            nn.MaxPool3d(kernel_size=2)\n        )\n        self.out = nn.Linear(32 * 29 * 29, 1)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        \n        x = x.view(x.size(0), -1)\n        output = self.out(x)\n        return output, x # return x for visualization","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import optim\n\ncnn = Convnet()\n\nloss_func = nn.CrossEntropyLoss()\noptimizer = optim.Adam(cnn.parameters(), lr = 0.001)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ndef get_model(width=128, height=128, depth=15):\n    inputs = keras.Input((width, height, depth, 1))\n    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.GlobalAveragePooling3D()(x)\n    x = layers.Dense(units=512, activation=\"relu\")(x)\n    x = layers.Dropout(0.3)(x)\n\n    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n\n    model = keras.Model(inputs, outputs)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model(128,128,15)\n\nmodel.fit(\n    train[folder][pacient],\n    validation_data=validation_dataset,\n    epochs=10,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def minMaxNormalize(volume):\n    # values between 0 and 1\n    min = 0\n    max = 255\n    volume[volume < min] = min\n    volume[volume > max] = max\n    volume = (volume - min) / (max - min)\n    volume = volume.astype(\"float32\")\n    return volume","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(2):\nrunning_loss = 0.0\nfor i, data in enumerate(trainloader, 0):\n  inputs, labels = data\n  optimizer.zero_grad()\n\n  # forward + backward + optimize\n  outputs = net(inputs)\n  loss = criterion(outputs, labels)\n  loss.backward()\n  optimizer.step()\n\n  # print statistics\n  running_loss += loss.item()\n  if i % 2000 == 1999:    # print every 2000 mini-batches\n      print('[%d, %5d] loss: %.3f' %\n            (epoch + 1, i + 1, running_loss / 2000))\n      running_loss = 0.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}