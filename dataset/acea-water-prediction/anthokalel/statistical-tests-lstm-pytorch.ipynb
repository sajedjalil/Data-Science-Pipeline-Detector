{"cells":[{"metadata":{},"cell_type":"markdown","source":"#  üèû Statistical tests & LSTM(PyTorch) üìà\n\n(you can visit my last notebook on marketing : https://www.kaggle.com/anthokalel/basket-analysis-for-dog-food)"},{"metadata":{},"cell_type":"markdown","source":"![](https://media.nationalgeographic.org/assets/photos/000/191/19125.jpg)"},{"metadata":{},"cell_type":"markdown","source":"* [Introduction](#intro)\n* [Exploratory Data Analysis](#section-one)\n    - [Importing librairies and load data](#section_one_1)\n    - [Description of the dataset](#section_one_2)\n* [Study of MultiVariate Time Series](#section-two)\n    - [How much the time series influence each other ? (Granger's causality test)](#section_two_1)\n    - [How the time series are correlated ? Pearson's test](#section_two_3)\n* [LSTM  model and feature importance (PyTorch)](#section_three)\n    - [LSTM model](#section_three_1)\n    - [Feature importance](#section_three_2)\n    - [Results for Lake Level](#section_three_3)\n    - [Results for Flow Rate](#section_three_4)\n* [Generalisation to other datasets](#generalization)\n    - [Aquifer Auser](#auser)\n    - [Aquifer Doganella](#doganella)\n    - [Aquifer Luco](#luco)\n    - [Aquifer Petrignano](#petrignano)\n    - [River Arno](#arno)\n    - [Water Spring Amiata](#amiata)\n    - [Water Spring Lupa](#lupa)\n    - [Water Spring Madonna di Canneto](#madonna)\n* [Conclusion](#conclusion)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"intro\"></a>\n## Introduction\n\n"},{"metadata":{},"cell_type":"markdown","source":"Hi, welcome to my notebook !\n\n\nThe goal of this notebook is to develop a generic method to forecast the time series of the dataset. Then, use this model to know which features have an importance on the model.\nThis notebook is focused on the Lake Bilancino dataset to develop the method. Then, this will be generealized to other datasets.\n\n\nGood reading !"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n### I - Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"\nWhat is the Bilancino Lake ?\n\nBilancino lake is an artificial lake located in the municipality of Barberino di Mugello (about 50 km from Florence). It is used to refill the Arno river during the summer months. Indeed, during the winter months, the lake is filled up and then, during the summer months, the water of the lake is poured into the Arno river."},{"metadata":{},"cell_type":"markdown","source":"<a id =section_one_1></a>\n#### Importing librairies and load data"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom torch import nn\nimport torch\nfrom statsmodels.tsa.stattools import grangercausalitytests\nimport plotly.express as px\nimport numpy as np\nimport holoviews as hv\nfrom holoviews import opts\nfrom sklearn.metrics import mean_squared_error\nfrom torch.utils.data import Dataset\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader \nimport plotly.express as px\nimport plotly.graph_objects as go\n\nhv.extension('bokeh')\n\nLake_Bilancino = pd.read_csv(\"../input/acea-water-prediction/Lake_Bilancino.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Lake_Bilancino","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset is composed of time  series from from 03/06/2002 to 30/06/2020 month by month :\n* 5 Rainfall variables ;\n* Temperature variables ;\n* Lake Level (target) ;\n* Flow Rate (target)."},{"metadata":{},"cell_type":"markdown","source":"We can see many NaN values at the beginning of the dataset for variables. \n\nI don't want to use method to replace these values because I don't want my model use false data. The best is to keep them like that.\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section_one_2\"></a>\n#### Description of the dataset"},{"metadata":{},"cell_type":"markdown","source":"Let's have a statistical description of our dataset to know it more."},{"metadata":{"trusted":true},"cell_type":"code","source":"Lake_Bilancino.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rainfall indicates the quantity of rain falling, expressed in millimeters (mm) in the area X.\n\nHere are the box plots of rainfall : "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def box_plot(df, id_begin = 1, id_last = 5):\n    fig = go.Figure()\n    for column in df.columns[id_begin:id_last]:\n\n        fig.add_trace(go.Box(y=df[column], name = column))\n        fig.update_traces(boxpoints='outliers', \n                          jitter= 0)\n\n    return fig.show()\n\n\nbox_plot(Lake_Bilancino, id_last = 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rainfall values are most of them but time to there are extreme values that can influence the model for the forecast of Lake Level and Flow Rate."},{"metadata":{},"cell_type":"markdown","source":"The temperature indicates the temperature, expressed in ¬∞C, detected by the thermometric station Le Croci."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = px.line(Lake_Bilancino, x=Lake_Bilancino['Date'], y=Lake_Bilancino['Temperature_Le_Croci'], \n              title='Temperature_Le_Croci')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"box_plot(Lake_Bilancino, id_begin = 6, id_last = 7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Temperature is on average 15 degree celcius. Temperature is very homogeneous around 15¬∞C with extremum in Winter with -5¬∞c and almost 35¬∞c in Summer. \n"},{"metadata":{},"cell_type":"markdown","source":"The Lake Level indicates the river level, expressed in meters (m)\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = px.line(Lake_Bilancino, x=Lake_Bilancino['Date'], y=Lake_Bilancino['Lake_Level'], \n              title='Lake_Level')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Lake Level has their minimum in the middle of Automn after Summer and their maximum in the months of April and May. It has sinusoidal variations. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"box_plot(Lake_Bilancino, id_begin = 7, id_last = 8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the values are around 250m but there are also many values above. "},{"metadata":{},"cell_type":"markdown","source":"The Flow Rate indicates the lake's flow rate, expressed in cubic meters per seconds (mc/s) :"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = px.line(Lake_Bilancino, x=Lake_Bilancino['Date'], y=Lake_Bilancino['Flow_Rate'], \n              title='Flow_Rate')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plot(Lake_Bilancino, id_begin = 8, id_last = 9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Most of the value of rainfall time series are 0. The question is : Do the non-null values have an influence on the behavior of Lake Level and Flow Rate ? Probably, because rainfall is filling Lakes. But, we will analyse this statiscally ; \n* Temperature is very homogeneous around 15¬∞c. I think this is the result of the variation between winter and summer ; \n* Lake Level is almost 250 but there are a few non insignificant values under 248.\n* There are any trend for Flow Rate, there are time to time some pics and it varies greatly."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section_two\"></a>\n### Study of Multivariate Time Series\n\nNow, we are going to understand deeper the behaviour of our time series and how they depend and influence each other in order to establish the proper model. \n\nRemember, we want to create a robust model to predict Lake Level and Flow Rate in function of the last data. A Multivariate Time Series will analyse how much the time series depend each other. It adds information to have a forecasting more robust and more precise.\n\nWe want to find the variables we will use in our future model. We don't want to use variables that are not significant in the forecasting because it can skew our model.\n\n<a id=\"section_two_1\"></a>\n#### How much the time series influence each other ? (Granger's causality test)\n\nIn order to know how much the time series influence each other, we are going to use Granger's causality test. This test is use in order to know whose are the variables that a VAR model should use. It means to know variables in which there are linear relationship. \n\nThe null hypothesis is that the coefficients of past values in the regression equals zero, it means that X past values of time series does not influence the Y time series.\nIf the p-value obtained from the test is lesser than the significance level of 0.05, we can reject the null hypothesis and suppose that there are linear relationships between variables. \n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def granger_causality_tests(df, maxlag = 12):\n\n    \"\"\"Check Granger Causality of all possible combinations of the Time series.\n    The rows are the response variable, columns are predictors. The values in the table \n    are the P-Values. P-Values lesser than the significance level (0.05), implies \n    the Null Hypothesis that the coefficients of the corresponding past values is \n    zero, that is, the X does not cause Y can be rejected.\n\n    data      : pandas dataframe containing the time series variables\n    variables : list containing names of the time series variables.\n    \"\"\"\n    \n    variables = df.columns\n    granger_pvalue_matrix = pd.DataFrame(np.zeros((len(variables), len(variables))))\n\n    for i, variable_c in enumerate(variables):\n        for j, variable_r in enumerate(variables):\n            min_p_value = np.min([grangercausalitytests(df[[variable_c, variable_r]].dropna(), \n                     maxlag = maxlag, verbose = False)[i][0]['ssr_chi2test'][1] for i in range(1, maxlag+1)])\n            granger_pvalue_matrix.loc[i, j] = min_p_value\n\n    granger_pvalue_matrix.columns = [var + '_x' for var in variables]\n    granger_pvalue_matrix.index = [var + '_y' for var in variables]\n    return granger_pvalue_matrix\n\ndef draw_granger_pvalue_matrix(granger_pvalue_matrix):\n    fig = go.Figure(data=go.Heatmap(z = granger_pvalue_matrix, \n                                    x = granger_pvalue_matrix.columns, \n                                    y = granger_pvalue_matrix.index, \n                                    colorscale = [[0, 'green'], [0.05, 'green'],\n                                                  [0.05, 'red'], [1, 'red']\n                                                  ]))\n    fig.update_layout(title=\"Minimum of p_values for a maxlag of 12 of a Granger test\")\n    return fig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"granger_pvalue_matrix = granger_causality_tests(Lake_Bilancino.iloc[578:, 1:].dropna())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"draw_granger_pvalue_matrix(granger_pvalue_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Variables with substring \"_x\" are predictions and with substring \"_y\" are responses. In green, we can see p-values < 0.05. In red, p-values >= 0.05.\n\nHow to interpret this ?\n\nFor example the p-value Flow_Rate_x causes Lake_Level_y is 8.24 x 10^-53. So we can reject the null hypothesis and conclude that Flow Rate causes Lake Level. \n\nIt seems that it is more difficult for rainfall variables to have cause relations. It means that rainfall in a place does not affect rainfall in an other place."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section_two_3\"></a>\n#### How the time series are correlated ? Pearson's test"},{"metadata":{},"cell_type":"markdown","source":"The last test will be to check correlation between time series. \n\nThe method that I will use will be to differentiate in percentage the time series and check their linear correlation with Pearson correlation."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def draw_correlation_matrix(df, method = 'pearson'):\n    pct_change_list = []\n    for column in df.columns[1:]:\n        pct_change_list.append(df[column].pct_change().replace([np.inf, -np.inf], np.nan).dropna())\n    pct_change_array = np.zeros((len(df.columns[1:]), len(df.columns[1:])))\n    for i, pct_change_i in enumerate(pct_change_list):\n        for j, pct_change_j in enumerate(pct_change_list):\n            pct_change_array[i, j] = pct_change_i.corr(pct_change_j, method)\n    fig = px.imshow(pct_change_array,\n                    x=df.columns[1:],\n                    y=df.columns[1:]\n                   )\n    fig.update_xaxes(title = \"Correlation matrix\", side=\"top\")\n    return fig.show()\ndraw_correlation_matrix(Lake_Bilancino.iloc[:, 1:], method = \"pearson\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We observe a great correlation between rainfall variables ;\n* There is also correlation between Lake Level and Flow Rate ;\n* Temperature seems to not affect other variables."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section_three\"></a>\n### LSTM model and feature importance (PyTorch)"},{"metadata":{},"cell_type":"markdown","source":"Our goal is to explain how exogonous variables (temperature and rainfall variables) have an influence on the forecasting or not.\n\nI have trained two LSTM models : one with the exogonous variables and the other without exogonous variables.\n\n"},{"metadata":{},"cell_type":"markdown","source":"\nHere we normalize our data to have a better fit in our model and we split it into three parts : \n- train dataset ;\n- validation test (to avoid overfitting) ;\n- test dataset (to check the variance of our model) ;"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def scale_and_split(df):\n    scaler = StandardScaler()\n    transformer = scaler.fit_transform(df.iloc[:, 1:])\n    df_scaled = pd.DataFrame(transformer, columns = df.columns[1:])\n    train, val = train_test_split(df_scaled, test_size=0.33, shuffle = False)\n    val, test = train_test_split(val, test_size=0.5, shuffle = False)\n    return train, val, test, scaler\n\ndef prepare_inputs_outputs(train, val, test, seq_len, inputs, output):\n    X_train, y_train = train[:len(train)-seq_len][inputs], train[[output]]\n    X_val, y_val = val[:len(val)-seq_len][inputs], val[[output]]\n    X_test, y_test = test[:len(test)-seq_len][inputs], test[[output]]\n    return X_train, y_train, X_val, y_val, X_test, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"parameters = {'inputs':['Lake_Level', 'Flow_Rate'],\n              'outputs':['Lake_Level', 'Flow_Rate'],\n              'seq_len': 30,\n              'batch_size_train':32,\n              'batch_size_val':8,\n              'batch_size_test':8,\n              'lr':0.0005,\n              'epochs':20\n             }\n\ntrain, val, test, scaler = scale_and_split(Lake_Bilancino.iloc[578:]) \nX_train, y_train, X_val, y_val, X_test, y_test = prepare_inputs_outputs(train, val, test, \n                                                                        parameters['seq_len'], \n                                                                        parameters['inputs'], \n                                                                        parameters['outputs'][0])\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=X_train.index, y=X_train['Lake_Level'],\n                    mode='lines',\n                    name='train data'))\nfig.add_trace(go.Scatter(x=X_val.index, y=X_val['Lake_Level'],\n                    mode='lines',\n                    name='validation data'))\nfig.add_trace(go.Scatter(x=X_test.index, y=X_test['Lake_Level'],\n                    mode='lines', \n                    name='test data'))\nfig.update_layout(title = 'Lake level cutted into train, validation and test dataset (normalized)')\n\nfig.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The forecast is based on the 30 previous hours."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from torch.utils.data import Dataset\nclass TimeSeries(Dataset):\n    def __init__(self, X, y, seq_len = 30):\n        self.X = torch.tensor(np.array(X) ,dtype=torch.float32)\n        self.y = torch.tensor(np.array(y) ,dtype=torch.float32)\n        #self.column_input = column_input\n        #self.column_output = column_output\n        self.seq_len = seq_len\n        \n    def __getitem__(self,idx):\n        return self.X[idx:idx+self.seq_len], self.y[idx+self.seq_len]\n\n    def __len__(self):\n        return len(self.X) - (self.seq_len-1)\n    \n    \ndef get_dataloader(X, y, batch_size):\n    return DataLoader(TimeSeries(X, y), shuffle=False, batch_size=32, drop_last = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"if torch.cuda.is_available():  \n    device = \"cuda:0\" \nelse:  \n    device = \"cpu\"  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section_three_1\"></a>\n#### LSTM model\n\n![Visualization of a LSTM unit](https://www.researchgate.net/profile/Xiaofeng_Yuan4/publication/331421650/figure/fig2/AS:771405641695233@1560928845927/The-structure-of-the-LSTM-unit.png)\n\nI am using the recurrent neural network LSTM to make our forecasting. It is a non linear model that can allow us to predict next values in function of past values, even farther values in time.\n\nThe $x_t$ is of dimension *features x timestep* and is fed into the LSTM network.\nThen the LSTM compute these values : \n<math>\n\\begin{align}\nf_t &= \\sigma_g(W_{f} x_t + U_{f} h_{t-1} + b_f) \\\\\ni_t &= \\sigma_g(W_{i} x_t + U_{i} h_{t-1} + b_i) \\\\\no_t &= \\sigma_g(W_{o} x_t + U_{o} h_{t-1} + b_o) \\\\\n\\tilde{c}_t &= \\sigma_c(W_{c} x_t + U_{c} h_{t-1} + b_c) \\\\\nc_t &= f_t \\circ c_{t-1} + i_t \\circ \\tilde{c}_t \\\\\nh_t &= o_t \\circ \\sigma_h(c_t)\n\\end{align}\n</math>\n\n\n* Batch size : 32, it means $x_t$ is fed into the network 32 times before the back propagation algorithm.\n* Epochs : sometimes 20, sometimes, it depends on the data.\n* Loss : MSE Loss (for regression)\n* Optimizer : Adam Optimizer\n\nAdvantages : \n* Non Linear model ;\n* Many parameters ;in \\mathbb{N}$\n* LSTM handle better vanishing and exploding gradients than RNN ;\n* Can memorize previous precious informations in the previous step.\n\nInconvenience : \n* It is a black box model, it's difficult to see and visualize why the model made this prediction and on which features it is based on.\n\n<a id=\"section_three_2\"></a>\n#### Feature importance\n\nFortunately, I have found a paper on [Medium](https://towardsdatascience.com/feature-importance-with-time-series-and-recurrent-neural-network-27346d500b9c) where the author found a simple way to describe how NN made their prediction thanks to derivative importance. \n\nGiven $f$ the function representative of our model, $y_t$ the output and $(x_{t-1}^1, ..., x_{t-p}^m)$ the input variables where $p = 30$ (time steps), $m$ is the number of input features, $W$ the weights, the model can b√© written in the following way : \n$y_t = f(x_{t-1}^1, ..., x_{t-p}^m, W)$\n\nMathematically, the derivative measures the sensivity to change of the function $f$ value with respect to its argument $x$. \n\nThe intuition in the derivative importance is to compute this derivative $f$ w.r.t. to model inputs \n\n<math>\n\\begin{bmatrix}\nx_{t-p}^1 & \\cdots &  x_{t-p}^m\\\\\n\\vdots & \\ddots & \\vdots \\\\\n x_{t-1}^1 & \\cdots &  x_{t-1}^m \n\\end{bmatrix}\n</math>\n\nSo for each $x_t$ feeded into the LSTM network, you can compute the matrix :\n\n<math>\n\\begin{bmatrix}\n\\dfrac{\\partial y_t}{\\partial x_{t-p}^1} & \\cdots & \\dfrac{\\partial y_t}{\\partial x_{t-p}^m}\\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\dfrac{\\partial y_t}{\\partial x_{t-1}^1} & \\cdots & \\dfrac{\\partial y_t}{\\partial x_{t-1}^m} \n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\dfrac{\\partial f}{\\partial x_{t-p}^1} & \\cdots & \\dfrac{\\partial f}{\\partial x_{t-p}^m}\\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\dfrac{\\partial f}{\\partial x_{t-1}^1} & \\cdots & \\dfrac{\\partial f}{\\partial x_{t-1}^m} \n\\end{bmatrix}\n</math>\n    \nIt gives us how much the input values have an influence on the output of our model.\n\nThen, we make a mean over the time step dimension ($p$) and again a mean when all the inputs have been feeded into the network to get the derivative importance and so : feature importance.\n\n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class MTS_RNN(nn.Module):\n    def __init__(self, input_size=8, hidden_layer_size=20, output_size=2):\n        super().__init__()\n        self.hidden_layer_size = hidden_layer_size\n        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n        self.linear = nn.Linear(hidden_layer_size, output_size)\n\n    def forward(self, x):\n        output, _ = self.lstm(x)\n        predictions = self.linear(output[:, -1])\n        return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def train_model(model, train_loader, val_loader, loss_function, optimizer, epochs):\n\n\n    train_val_history = {'train': [],\n                        'val': []}\n    for i in range(epochs):\n        for j, data in enumerate(train_loader):\n            inputs, labels = data[0].to(device), data[1].to(device)\n            optimizer.zero_grad()\n\n            y_pred = model(inputs)\n\n            single_loss = torch.sqrt(loss_function(y_pred, labels))\n\n            single_loss.backward()\n\n            optimizer.step()\n\n        train_val_history['train'].append(single_loss.item())\n        #print(\"Training data : \" + f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n\n        for j, data in enumerate(val_loader):\n            inputs, labels = data[0].to(device), data[1].to(device)\n            optimizer.zero_grad()\n\n            y_pred = model(inputs)\n\n            single_loss = torch.sqrt(loss_function(y_pred, labels))\n\n        train_val_history['val'].append(single_loss.item())\n        #print(\"Validation data : \" + f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n\n    #print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')\n    return model, train_val_history\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\ndef draw_training_loss(train_val_history, epochs):\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=np.arange(epochs), y=np.array(train_val_history['train']),\n                        mode='lines',\n                        name = 'train'))\n    fig.add_trace(go.Scatter(x=np.arange(epochs), y=np.array(train_val_history['val']),\n                        mode='lines',\n                        name = 'validation'))\n    fig.update_layout(title=\"Training and validation loss\",\n                      xaxis_title=\"Epochs\",\n                      yaxis_title=\"MSE loss\",)\n\n    return fig.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def predict_and_draw(df, test_loader, output, scaler, model, y_test, loss_function, draw = False):\n\n    test_output = []\n    test_output_grad = []\n    for j, data in enumerate(test_loader):\n        inputs, labels = data[0].to(device), data[1].to(device)\n        inputs.requires_grad = True\n        y_pred = model(inputs)\n        single_loss = loss_function(y_pred, labels)\n\n        test_output.append(y_pred)\n        \n        single_loss.backward()\n \n        test_output_grad.append(torch.mean(inputs.grad, axis = 0))\n        \n    gradient_input_mean = np.sort(np.absolute(torch.mean(torch.cat(test_output_grad), axis = 0)\n                                      .detach().numpy()))\n    \n    indice_outputs = [i - 1 for i, x in enumerate(df.columns.isin(output)) if x]\n    mean = [mean for i, mean in enumerate(scaler.mean_) if i in indice_outputs]\n    var = [var for i, var in enumerate(scaler.var_) if i in indice_outputs]\n    \n    predictions = pd.DataFrame(torch.cat(test_output).detach().numpy(), columns = output)\n    \n    predictions = predictions * var + mean\n    y = y_test[parameters['seq_len']:] * var + mean\n    \n    def draw(predictions, y, output):\n        fig = make_subplots(rows=2, cols=1, \n                           subplot_titles=(\"Prediction on test dataset for \" + output ,\n                                           \"Feature importance by derivarive importance for \" + output))\n        fig.add_trace(go.Scatter(x=np.arange(len(predictions)), y=np.array(predictions[output]),\n                            mode='lines',\n                            name = 'predictions'), row = 1, col = 1)\n        fig.add_trace(go.Scatter(x=np.arange(len(predictions)), y=np.array(y[output]),\n                            mode='lines',\n                            name = 'ground truth'), row = 1, col = 1)\n        \n        fig.add_trace(go.Bar(x=df.columns[1:], y = gradient_input_mean, \n                            name = 'gradient of lstm w.r.t. features'), row = 2, col = 1)\n\n        return fig.show()\n\n    if draw:\n        draw(predictions, y, output[0])\n \n    return y, predictions\n        \n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def train_predict_and_draw(parameters, df, output):\n    train, val, test, scaler = scale_and_split(df) \n    X_train, y_train, X_val, y_val, X_test, y_test = prepare_inputs_outputs(train, val, test, \n                                                                            parameters['seq_len'], \n                                                                            parameters['inputs'], \n                                                                            output)\n\n    train_loader = get_dataloader(X_train, y_train, parameters['batch_size_train'])\n    val_loader = get_dataloader(X_val, y_val, parameters['batch_size_val'])\n    test_loader = get_dataloader(X_test, y_test, parameters['batch_size_test'])\n    model = MTS_RNN(input_size=len(parameters['inputs']), hidden_layer_size=20, output_size=1).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=parameters['lr'])\n    loss_function = nn.MSELoss()\n\n    model_LSTM, train_val_history = train_model(model, \n                                                train_loader, \n                                                val_loader,\n                                                loss_function, \n                                                optimizer, \n                                                parameters['epochs'])\n\n    draw_training_loss(train_val_history, parameters['epochs'])\n\n    y, predictions = predict_and_draw(df, test_loader, [output], scaler, model, y_test, loss_function, \n                                      draw = True)\n    return y, predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section_three_3\"></a>\n#### Result for Lake Level"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"parameters = {'inputs':Lake_Bilancino.columns[1:],\n              'outputs':['Lake_Level', 'Flow_Rate'],\n            'seq_len': 30,\n              'batch_size_train':32,\n              'batch_size_val':8,\n              'batch_size_test':8,\n              'lr':0.002,\n              'epochs':20\n             }\n\ny, predictions = train_predict_and_draw(parameters, Lake_Bilancino.iloc[578:], parameters['outputs'][0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The RMSE of the model is : **"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The MSE of the model is : **"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions, squared = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The metrics are low, which mean that the model is good for future predictions.\n\nGenerally, the LSTM model is well fitted the ground truth but locally there are some peaks.\n\nAccording to this model, the flow rate but also the Rainfall in Le Croci and Cavallina have an influence on the prediction. I have checked where are these places and there are located near the Lake. The other cities like Saint Piero or Mangona are farther, this is why there have less influence on the prediction of lake level."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section_three_4\"></a>\n#### Result for Flow Rate"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"parameters = {'inputs':Lake_Bilancino.columns[1:],\n              'outputs':['Lake_Level', 'Flow_Rate'],\n            'seq_len': 30,\n              'batch_size_train':32,\n              'batch_size_val':8,\n              'batch_size_test':8,\n              'lr':0.0001,\n              'epochs':40\n             }\n\ny, predictions = train_predict_and_draw(parameters, Lake_Bilancino.iloc[578:], parameters['outputs'][1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"**The RMSE of the model is : **"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The MSE of the model is : **"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions, squared = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The metrics are higher than the flow rate. It is because of many photos that the model is able to predict but not adjust to the correct value. I think it would have taken more data for this.\n\nThere are many peaks in the flow rate  but the model make good forecastings where these peaks appears. More generally, the model is fitted with the test dataset.\n\nAccording to this model, farther the city is from the lake, the less it has an influence on flow rate. Which is logic, the water of rainfall from distant cities make less time to reach the lake."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"generalization\"></a>\n### Generalization to other datasets\n\n<a id=\"auser\"> </a>\n#### Aquifer Auser\n\nThis waterbody consists of two subsystems, called NORTH and SOUTH, where the former partly influences the behavior of the latter. Indeed, the north subsystem is a water table (or unconfined) aquifer while the south subsystem is an artesian (or confined) groundwater.\n\nThe levels of the NORTH sector are represented by the values of the SAL, PAG, CoS and DIEC wells, while the levels of the SOUTH sector by the LT2 well.\n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Aquifer_Auser = pd.read_csv('../input/acea-water-prediction/Aquifer_Auser.csv')\nAquifer_Auser = Aquifer_Auser.interpolate(method = 'linear').fillna(0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"parameters = {'inputs':Aquifer_Auser.columns[1:],\n              'outputs':['Depth_to_Groundwater_SAL', 'Depth_to_Groundwater_CoS', \n                         'Depth_to_Groundwater_LT2'],\n              'seq_len': 30,\n              'batch_size_train':32,\n              'batch_size_val':8,\n              'batch_size_test':8,\n              'lr':0.0001,\n              'epochs':40\n             }\ny, predictions = train_predict_and_draw(parameters, Aquifer_Auser, parameters['outputs'][0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The RMSE for this model is : **"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The MSE for this model is : **"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions, squared = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see in the prediction that it fits well with the general behaviour of the curve but there are many peaks in the prediction that make rise the metrics. \n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"parameters = {'inputs':Aquifer_Auser.columns[1:],\n              'outputs':['Depth_to_Groundwater_SAL', 'Depth_to_Groundwater_CoS', \n                         'Depth_to_Groundwater_LT2'],\n              'seq_len': 30,\n              'batch_size_train':32,\n              'batch_size_val':8,\n              'batch_size_test':8,\n              'lr':0.001,\n              'epochs':30\n             }\ny, predictions = train_predict_and_draw(parameters, Aquifer_Auser, parameters['outputs'][1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The RMSE of the model is : **"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The MSE of the model is : **"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions, squared = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"parameters = {'inputs':Aquifer_Auser.columns[1:],\n              'outputs':['Depth_to_Groundwater_SAL', 'Depth_to_Groundwater_CoS', \n                         'Depth_to_Groundwater_LT2'],\n              'seq_len': 30,\n              'batch_size_train':32,\n              'batch_size_val':8,\n              'batch_size_test':8,\n              'lr':0.0001,\n              'epochs':30\n             }\ny, predictions = train_predict_and_draw(parameters, Aquifer_Auser, parameters['outputs'][2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The RMSE of the model is : **"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The model of the RMSE is : **"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions, squared = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In these three forecasting, the Depth to groundwater is very influened by hydrometry and volume. The Rainfall have a lesser influence on its behaviour. \n\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"doganella\"></a>\n#### Aquifer Doganella\n\n\nThe wells field Doganella is fed by two underground aquifers not fed by rivers or lakes but fed by meteoric infiltration. The upper aquifer is a water table with a thickness of about 30m. The lower aquifer is a semi-confined artesian aquifer with a thickness of 50m and is located inside lavas and tufa products. These aquifers are accessed through wells called Well 1, ..., Well 9. Approximately 80% of the drainage volumes come from the artesian aquifer. The aquifer levels are influenced by the following parameters: rainfall, humidity, subsoil, temperatures and drainage volumes."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Aquifer_Doganella = pd.read_csv('../input/acea-water-prediction/Aquifer_Doganella.csv')\nAquifer_Doganella = Aquifer_Doganella.interpolate(method = 'linear').fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"parameters = {'inputs': Aquifer_Doganella.columns[1:],\n              'outputs':['Depth_to_Groundwater_Pozzo_1', \n                         'Depth_to_Groundwater_Pozzo_2', \n                         'Depth_to_Groundwater_Pozzo_3',\n                         'Depth_to_Groundwater_Pozzo_4', \n                         'Depth_to_Groundwater_Pozzo_5', \n                         'Depth_to_Groundwater_Pozzo_6',\n                         'Depth_to_Groundwater_Pozzo_7', \n                         'Depth_to_Groundwater_Pozzo_8', \n                         'Depth_to_Groundwater_Pozzo_9'],\n              'seq_len': 30,\n              'batch_size_train':32,\n              'batch_size_val':8,\n              'batch_size_test':8,\n              'lr':0.001,\n              'epochs':30\n             }\n\nfor output in parameters['outputs']:\n    y, predictions = train_predict_and_draw(parameters, Aquifer_Doganella, output)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The RMSE of the model is :** "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The MSE of the model is : **"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions, squared = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model is not fitting well with the groundtruth. I think there are too much variables to make him fit. But we can see that the set of these 9 models to predict the depth to groundwater is using always the same variables : the temperature and the volume have a huge impact on it."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"luco\"> </a>\n#### Aquifer Luco\n\nThe Luco wells field is fed by an underground aquifer. This aquifer not fed by rivers or lakes but by meteoric infiltration at the extremes of the impermeable sedimentary layers. Such aquifer is accessed through wells called Well 1, Well 3 and Well 4 and is influenced by the following parameters: rainfall, depth to groundwater, temperature and drainage volumes."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Aquifer_Luco = pd.read_csv('../input/acea-water-prediction/Aquifer_Luco.csv')\nAquifer_Luco = Aquifer_Luco.interpolate(method = 'linear').fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"parameters = {'inputs': Aquifer_Luco.columns[1:],\n              'outputs':['Depth_to_Groundwater_Podere_Casetta'],\n              'seq_len': 30,\n              'batch_size_train':1,\n              'batch_size_val':8,\n              'batch_size_test':8,\n              'lr':0.001,\n              'epochs':30\n             }\ny, predictions = train_predict_and_draw(parameters, Aquifer_Luco, 'Depth_to_Groundwater_Podere_Casetta')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The RMSE of the model is : **"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The MSE of the model is : "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions, squared = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The prediction fits with the trend of the ground truth but again, many peaks come to rise the error metrics.\n\nWe can see that the depth to groundwater is also caused by volume and temperature like we saw in the Aquifer Doganella."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"petrignano\"></a>\n#### Aquifer Petrignano\n\nThe wells field of the alluvial plain between Ospedalicchio di Bastia Umbra and Petrignano is fed by three underground aquifers separated by low permeability septa. The aquifer can be considered a water table groundwater and is also fed by the Chiascio river. The groundwater levels are influenced by the following parameters: rainfall, depth to groundwater, temperatures and drainage volumes, level of the Chiascio river."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Aquifer_Petrignano = pd.read_csv('../input/acea-water-prediction/Aquifer_Petrignano.csv')\nAquifer_Petrignano = Aquifer_Petrignano.interpolate(method = 'linear').fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"parameters = {'inputs': Aquifer_Petrignano.columns[1:],\n              'outputs':['Depth_to_Groundwater_P24', 'Depth_to_Groundwater_P25'],\n              'seq_len': 30,\n              'batch_size_train':1,\n              'batch_size_val':8,\n              'batch_size_test':8,\n              'lr':0.001,\n              'epochs':30\n             }\nfor output in parameters['outputs']:\n    y, predictions = train_predict_and_draw(parameters, Aquifer_Petrignano, output)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The RMSE of the model is : **"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The MSE of the model is : **"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions, squared = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model fits well with the ground truth. The error metrics are low.\n\nAccording to the model, we can see that the Depth to groundwater is caused mainly by the groundwater level (hydrometry) in Chiasco and also by the volume. In this prediction, the temperature is less a variable to predict the depth to groundwater than previous datasets."},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"<a id=\"arno\"></a>\n#### River Arno\n\nArno is the second largest river in peninsular Italy and the main waterway in Tuscany and it has a relatively torrential regime, due to the nature of the surrounding soils (marl and impermeable clays). Arno results to be the main source of water supply of the metropolitan area of Florence-Prato-Pistoia. The availability of water for this waterbody is evaluated by checking the hydrometric level of the river at the section of Nave di Rosano."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"River_Arno = pd.read_csv('../input/acea-water-prediction/River_Arno.csv')\nRiver_Arno = River_Arno.interpolate(method = 'linear').fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"parameters = {'inputs': River_Arno.columns[1:],\n              'outputs':['Hydrometry_Nave_di_Rosano'],\n              'seq_len': 30,\n              'batch_size_train':1,\n              'batch_size_val':8,\n              'batch_size_test':8,\n              'lr':0.001,\n              'epochs':30\n             }\ny, predictions = train_predict_and_draw(parameters, River_Arno, 'Hydrometry_Nave_di_Rosano')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The RMSE of the model is :** "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The MSE of the model is : ******"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions, squared = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model fits very well with the ground truth. The error metrics are very low.\n\nAccording to the model, the groundwater level (hydrometry) depends of the volume but also of some rainfall. There are some rainfalls more important than other. "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"amiata\"></a>\n#### Water Spring Amiata\n\nThe Amiata waterbody is composed of a volcanic aquifer not fed by rivers or lakes but fed by meteoric infiltration. This aquifer is accessed through Ermicciolo, Arbure, Bugnano and Galleria Alta water springs. The levels and volumes of the four sources are influenced by the parameters: rainfall, depth to groundwater, hydrometry, temperatures and drainage volumes."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Water_Spring_Amiata = pd.read_csv('../input/acea-water-prediction/Water_Spring_Amiata.csv')\nWater_Spring_Amiata = Water_Spring_Amiata.interpolate(method = 'linear').fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"parameters = {'inputs': Water_Spring_Amiata.columns[1:],\n              'outputs':['Flow_Rate_Bugnano', 'Flow_Rate_Arbure', 'Flow_Rate_Ermicciolo', \n                        'Flow_Rate_Galleria_Alta'],\n              'seq_len': 30,\n              'batch_size_train':1,\n              'batch_size_val':8,\n              'batch_size_test':8,\n              'lr':0.001,\n              'epochs':30\n             }\nfor output in parameters['outputs']:\n    y, predictions = train_predict_and_draw(parameters, Water_Spring_Amiata, output)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The RMSE of the model is : **"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** The MSE of the model is : **"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions, squared = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model is not fitting well with the ground truth.\n\nAccording to this model, the other flow rates depend of the galleria alta flow rate. The temperature is also a variable that make us the predict the flow rate."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"lupa\"></a>\n#### Water Spring Lupa"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Water_Spring_Lupa = pd.read_csv('../input/acea-water-prediction/Water_Spring_Lupa.csv')\nWater_Spring_Lupa = Water_Spring_Lupa.interpolate(method = 'linear').fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"parameters = {'inputs': Water_Spring_Lupa.columns[1:],\n              'outputs':['Flow_Rate_Lupa'],\n              'seq_len': 30,\n              'batch_size_train':1,\n              'batch_size_val':8,\n              'batch_size_test':8,\n              'lr':0.001,\n              'epochs':30\n             }\nfor output in parameters['outputs']:\n    y, predictions = train_predict_and_draw(parameters, Water_Spring_Lupa, output)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The RMSE of the model is : **"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The MSE of the model is : **"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions, squared = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model fits well with the ground truth. It even predict the last linear rise of the flow rate.\n\nAccording to this model, the flow rate Lupa depends only on itself and not on Terni rainfall."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"madonna\"></a>\n#### Water Spring Madonna di Canneto"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Water_Spring_Madonna_di_Canneto = pd.read_csv('../input/acea-water-prediction/Water_Spring_Madonna_di_Canneto.csv')\nWater_Spring_Madonna_di_Canneto = Water_Spring_Madonna_di_Canneto.interpolate(method = 'linear').fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"parameters = {'inputs': Water_Spring_Madonna_di_Canneto.columns[1:],\n              'outputs':['Flow_Rate_Madonna_di_Canneto'],\n              'seq_len': 30,\n              'batch_size_train':1,\n              'batch_size_val':8,\n              'batch_size_test':8,\n              'lr':0.001,\n              'epochs':30\n             }\nfor output in parameters['outputs']:\n    y, predictions = train_predict_and_draw(parameters, Water_Spring_Madonna_di_Canneto, output)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The RMSE of the model is : **"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The MSE of the model is : **"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mean_squared_error(y[:len(predictions)], predictions, squared = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model fits very well on the ground truth. But there are always some peaks that make higher the error metrics.\n\nAccording to this model, the flow rate depends mainly on itself but also a little bit on temperature."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"conclusion\"> </a>\n### Conclusion\n\nThe LSTM is a good model to fit these time series. Some of them have peaks and trend, which can be diffcult sometimes to forecast but the model detect them and forecast them.\nDerivative importance gives us good informations about how the LSTM make the forecast according to inputs. This technique is very useful when we deal with neural networks which are black boxes and we don't know how the neural network make their prediction. \n\nThanks for having read this notebook, I hope you enjoyed and maybe have learned new things on water behaviour in nature !\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}