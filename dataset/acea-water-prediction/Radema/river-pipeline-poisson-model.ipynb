{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom scipy.stats import boxcox\n\n# plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport plotly.express as px\n#pd.options.plotting.backend = \"plotly\"\n\n# settings\nplt.style.use('seaborn')\nplt.rcParams[\"figure.figsize\"] = (16, 8)\n\nimport tensorflow as tf\n\npd.options.display.max_columns = 500\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# References\n\nHere you can find very useful resources and notebooks that I've read and used to build this notebook:\n\n1. [Intro to Time Series Forecasting by Leonie](hhttps://www.kaggle.com/iamleonie/intro-to-time-series-forecasting)\n2. [EDA: Quenching the Thirst for insights by Leonie](https://www.kaggle.com/iamleonie/eda-quenching-the-thirst-for-insights) \n3. [Acea - EDA: Deep Water by Luca31394](https://www.kaggle.com/luca31394/acea-eda-deep-water)\n4. [(Arno) - EDA + Data Enrichment](https://www.kaggle.com/radema/arno-eda-data-enrichment)\n5. [How to Develop Convolutional Neural Network Models for Time Series Forecasting by Machine Learning Mastery](https://machinelearningmastery.com/how-to-develop-convolutional-neural-network-models-for-time-series-forecasting/?fbclid=IwAR0tzQcUWavNeMXnor2AwUCMZzse6SkMqY5_m9uZ81_gjP2b7HB2jBrU03Q)\n6. [How to use xgboost for Time Series Forecasting by Machine Learning Mastery](https://machinelearningmastery.com/xgboost-for-time-series-forecasting/?fbclid=IwAR0mL6rC6fpWtlw8JpL9WwyLPKETz589aleaLfkoXbMJiouZID5ekDZkvgE)"},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nIn this notebook I'll try to approach the Acea Smart Water Analytics competion. \nThe Acea Group is one of the leading Italian multiutility operators. The company manages and develops water and electricity networks and environmental services. I'm a bit familiar with Acea and the names in the datasets since I live in Italy. \n\\\nIn this notebook, I don't plan to address all the waterbodies but I plan to outline an precise pipeline (see their schema below):\n* to analyze the different datasets grouped by waterbody type\n* clean the data\n* build new features\n* train and validate forecasting model(s)\n\\\n\\\n![image](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F6195295%2Fcca952eecc1e49c54317daf97ca2cca7%2FAcea-Input.png?generation=1606932492951317&alt=media)\n\\\n\\\nI'll try to keep it as simple as possible. In particular I will start with simple EDAs, simple models and then try to add complexity. "},{"metadata":{},"cell_type":"markdown","source":"# Retrieve source datasets\n\nLet's start retrieving the datasets. To differentatie the different sources by waterbody type, I've decided to organize the datasets in dictionaries."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from datetime import datetime\ncustom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y')\n\ndf = {}\nbodytype = {'Aquifer','Water','Lake','River'}\nRiver = {}\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        if filename[-4:]=='.csv':\n            if filename.startswith('River'):\n                River[filename[:-4].replace('River_','')] = pd.read_csv(\n                    os.path.join(dirname, filename)\n                    ,parse_dates = ['Date'], date_parser = custom_date_parser\n                )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the following, you can find the details about the different waterbodies:\n* River Arno: the Arno is the second largest river in peninsular Italy and the main waterway in Tuscany and it  has a relatively torrential regime, due to the nature of the surrounding soils (marl and impermeable clays). Output = Hydrometry\n    \n### River Arno\n\nWe start with the (maybe) simplest one: river Arno. Indeed, we have just one river and we need to forecast a single attribute (hydrometer). "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"nulls = River['Arno'].groupby(\n    River['Arno'].Date.dt.year\n                             ).agg({'Date':'count',\n                                  'Rainfall_Le_Croci':'count','Rainfall_Cavallina':'count', 'Rainfall_S_Agata':'count',\n                                   'Rainfall_Mangona':'count', 'Rainfall_S_Piero':'count', 'Rainfall_Vernio':'count',\n                                   'Rainfall_Stia':'count', 'Rainfall_Consuma':'count', 'Rainfall_Incisa':'count',\n                                   'Rainfall_Montevarchi':'count', 'Rainfall_S_Savino':'count', 'Rainfall_Laterina':'count',\n                                   'Rainfall_Bibbiena':'count', 'Rainfall_Camaldoli':'count', 'Temperature_Firenze':'count',\n                                   'Hydrometry_Nave_di_Rosano':'count'\n                                  }).transpose()\nsns.heatmap(nulls)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the previous pivot table, we see that:\n* for year 1998, we have just the Hydrometry. All the metrics are null => I decide to drop this year\n* from 1999 to 2003, we have values on temparature but not Rainfall(s) => for the moment, I want to drop these years\n* several attributes for specific location are available only in a precise time range.\n\nThe last point is interesting. I can hypothesize the following explanations:\n* the weather reports from these locations are outdated, so they are not used anymore for forecasting\n* the weather reports are missing in this export, so we could try to retrieve these reports from open datasets\n\nFor the moment, since there're several years missing and the rainfalls are probably correlated, I prefer to focus on the following:\n* Rainfall_Le_Croci\t\n* Rainfall_Cavallina\n* Rainfall_S_Agata\n* Rainfall_Mangona\n* Rainfall_S_Piero\n* Rainfall_Vernio\n\nWe have missing Temperature values (almost 2 years). However, I expect this attribute to have a strong seasonality. Let's see more deeply...\n "},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_time_features(df):\n    data = df.copy()\n    #data.reset_index(inplace = True)\n    data['year'] = data.Date.dt.year\n    data['month'] = data.Date.dt.month\n    data['day_in_year'] = data.Date.dt.dayofyear\n    data['week_in_year'] = data.Date.dt.isocalendar().week.astype(int)\n    data['year sin'] = np.sin(2*np.pi*data['day_in_year']/365.25)\n    data['year cos'] = np.cos(2*np.pi*data['day_in_year']/365.25)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"River['Arno'] = get_time_features(River['Arno'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"columns = [\n    'Date',\n    'Rainfall_Le_Croci','Rainfall_Cavallina',\n    'Rainfall_S_Agata','Rainfall_Mangona','Rainfall_S_Piero',\n#    'Rainfall',\n    'Rainfall_Vernio','Rainfall_Incisa', 'Rainfall_Bibbiena',\n    'Temperature_Firenze',\n    'Hydrometry_Nave_di_Rosano'\n]\nfeatures = [\n    'Rainfall_Le_Croci','Rainfall_Cavallina',\n    'Rainfall_S_Agata','Rainfall_Mangona','Rainfall_S_Piero',\n#    'Rainfall',\n    'Rainfall_Vernio','Rainfall_Incisa', 'Rainfall_Bibbiena',\n    'Temperature_Firenze'\n]\n\nlabels = [\n    'Hydrometry_Nave_di_Rosano'\n]\n#River['Arno']['Rainfall'] = River['Arno'][['Rainfall_Le_Croci','Rainfall_Cavallina',\n#    'Rainfall_S_Agata','Rainfall_Mangona','Rainfall_S_Piero']].sum(axis = 1)\n\ndata = River['Arno'][River['Arno'].year>2003][columns]\n\n\nprint(data.describe().transpose())\nprint('\\n')\nfor column in columns[1:]:\n    data.plot.line(x='Date',y=column,figsize = (10,4),title = column.replace('_',' '))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## (Temperature) Enrich datasets with external datasources\n\nAt some point in my analysis, I had the following idea: what if there's an open dataset on the Region [website](http://sir.toscana.it/idrometria-pub) about this data?\nAfter a bit of research, I found them (at least I think so)! Be aware this data should be validated and evetually cleaned, however I want just to show you how we can use this external dataset to enrich ours. This method is much more simpler than build an LSTM model to...forecast missing data. "},{"metadata":{},"cell_type":"markdown","source":"To do: optimize the following cell"},{"metadata":{"trusted":true},"cell_type":"code","source":"temperature = pd.read_csv(\"http://sir.toscana.it/archivio/download.php?IDST=termo_csv&IDS=TOS01001095\", \n                          sep =';',skiprows=18,parse_dates=True, infer_datetime_format =True, dayfirst=True).rename(\n    columns = {'gg/mm/aaaa':'Date', 'Max [°C]':'Max', 'Min [°C]':'Min'}\n)\ntemperature.Date = pd.to_datetime(temperature.Date,format='%d/%m/%Y')\n\ntemperature['data_int'] = temperature.Date.dt.strftime('%Y%m%d').astype(int)\ndata['data_int'] = data.Date.dt.strftime('%Y%m%d').astype(int)\ntemperature['Avg'] = temperature.Max*0.65+temperature.Min*0.35\n\ndata = data.merge(temperature, on ='data_int', how = 'left')\ndata.Temperature_Firenze.fillna(data.Avg, inplace= True)\ndata.drop(['Avg','Min','Max','data_int','Date_y'], inplace=True,axis = 1)\ndata.rename(columns={'Date_x':'Date'}, inplace=True)\nprint(data.groupby(data.Date.dt.year).agg({'Date':'count', 'Temperature_Firenze':'count'\n                                  }).transpose())\ndata.reset_index().plot.line('index','Temperature_Firenze')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def replace_implausible_zeros(col):\n    temp_col = data[[col]].copy()\n    temp_col['key'] = (temp_col[col] != temp_col[col].shift(1)).astype(int).cumsum()\n    key_dict = temp_col.groupby(['key']).agg({col : 'mean', 'key':'count'})\n    key_dict = key_dict[(key_dict[col]==0) & (key_dict.key >2)]\n    for key in key_dict.index:\n        temp_col[col] = np.where(temp_col['key'] == key, np.nan, temp_col[col]) \n    return temp_col[col]\n\nparams = {'num_leaves': 40,\n          'objective': 'regression_l1',\n          'max_depth': 4,\n          'learning_rate': 0.001,\n          \"metric\": 'mae',\n          \"verbosity\": -1,\n          'verbose': -1,\n          'seed' : 42\n         }\n\ndef predict_missing_temperature(data,target, labels):\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_absolute_error\n    import lightgbm as lgb\n    \"\"\"\n    f, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 6))\n    ax.set_title(f'Temperatures for {target}', fontsize=16)\n\n    sns.lineplot(x=waterbodies_df.Date_dt, y=waterbodies_df[target], label=target, )\n    sns.scatterplot(x=waterbodies_df.Date_dt, y=waterbodies_df[target].isna().apply(lambda x: 15 if x else np.nan), color='red', linewidth=0, label='To predict' )\n    ax.set_xlim([date(2000, 1, 1), date(2020, 6, 30)])\n    plt.show()\n    \"\"\"\n    features = [feature for feature in data.columns if feature not in labels+['Date']]\n\n    test_df = data.copy()\n    train_df = data[data[target].notna()]\n\n    features = [c for c in features if c != target]\n\n    X = train_df[features]\n    y = train_df[[target]]\n    X_test = test_df[features]\n\n    y_preds = np.zeros(X_test.shape[0])\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    dtrain = lgb.Dataset(X_train, y_train, params= {'verbose': -1})\n    dvalid = lgb.Dataset(X_valid, y_valid, params= {'verbose': -1})\n\n    # For analysis set 'verbose_eval' to 200, false\n    clf = lgb.train(params, dtrain, 10000, valid_sets = [dtrain, dvalid], verbose_eval=False,  early_stopping_rounds=100)\n\n    y_pred_valid = clf.predict(X_valid)\n    y_preds = clf.predict(X_test)\n    \n    f, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 4))\n    ax.set_title(f'Rolling Mean Temperatures for {target} \\n(Rolling Window: 28 Days, MAE: {mean_absolute_error(y_valid.rolling(28).mean()[28:], pd.Series(y_pred_valid).rolling(28).mean()[28:])})', fontsize=16)\n    old = data[target].copy().replace({np.nan : np.inf})\n    data[target] = np.where(data[target].isna(), pd.Series(y_preds), data[target])\n    sns.lineplot(x=data.Date, y=data[target].rolling(28).mean(), label='Imputed', color='darkorange')\n    sns.lineplot(x=data.Date, y=old, label='Original', color='dodgerblue')\n    ax.set_xlabel('Date', fontsize=14)\n    ax.set_xlim([data.Date.to_list()[0],\n                 data.Date.to_list()[-1]]\n                 )\n    plt.show()\n    #return pd.Series(y_preds)\n\n# Sort temperature columns according to amount of missing values.\n# Fill NaN values for features with least missing values first\n\n#example_col = 'Temperature_Velletri'\n#predict_missing_temperature(example_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for col in features:\n    data[col] = replace_implausible_zeros(col)\nsorted_cols = data[features].isna().sum(axis=0).sort_values().index\n\nfor feature in sorted_cols:\n    predict_missing_temperature(data, feature , labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"nulls = data.groupby(\n    data.Date.dt.year\n                             ).agg('count'\n                                  ).transpose()\nsns.heatmap(nulls)\nprint('\\n')\nfor column in columns[1:]:\n    data.plot.line(x='Date',y=column,figsize = (10,4),title = column.replace('_',' '))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Add seasonal components\nfrom statsmodels.tsa import seasonal\n\n\ndecomposed = seasonal.seasonal_decompose(\n    data['Temperature_Firenze'], period = 366, extrapolate_trend = 'freq'\n)\ndecomposed.plot()\ndata['Temperature_Trend'] = decomposed.trend\ndata['Temperature_Season'] = decomposed.seasonal\ndata['Temperature_Resid'] = decomposed.resid\n\nfeatures = ['Temperature_Trend','Temperature_Season','Temperature_Resid'] + features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Engineering (XGBoost)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"start = data.Date.min()\nend  = data.Date.max()\ndata.set_index('Date',inplace=True)\n\nindex_col = pd.date_range(start = start, end = end, freq = 'D')\ndata = data.reindex(index_col)\n\ndata['Hydrometry_Nave_di_Rosano'][data.Hydrometry_Nave_di_Rosano==0]=np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data.reset_index().plot.line(x='index',y='Hydrometry_Nave_di_Rosano',figsize = (20,9),title = column.replace('_',' '))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#train = data[data.Hydrometry_Nave_di_Rosano.isna()==False]\ntrain = pd.concat([data[labels]\n           #,train[features].shift(0).add_suffix('_actual')\n           ,data[features+labels].shift(1).add_suffix('_shifted_1')\n           #,train[features+labels].shift(2).add_suffix('_shifted_2')\n          ]\n          ,axis = 1).dropna()#['Hydrometry_Nave_di_Rosano']\ntrain['Hydrometry_Nave_di_Rosano'] = 1+  np.log(train.Hydrometry_Nave_di_Rosano)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train = get_time_features(train.reset_index().rename(columns={'index':'Date'}))\nval = train[train.year>=2019]\ntrain = train[train.year < 2019]\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgboost\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold, train_test_split, GridSearchCV\n\n\n# define model\nmodel = xgb.XGBRegressor()\nmodel_features = ['Temperature_Season_shifted_1'\n    ,'Rainfall_Le_Croci_shifted_1'\n    ,'Rainfall_Cavallina_shifted_1'\n    ,'Rainfall_S_Agata_shifted_1'\n    ,'Rainfall_Mangona_shifted_1'\n    ,'Rainfall_S_Piero_shifted_1'\n    #,'Rainfall_Vernio_shifted_1'\n    #,'Rainfall_Incisa_shifted_1'\n    #,'Rainfall_Bibbiena_shifted_1'\n    ,'Hydrometry_Nave_di_Rosano_shifted_1'\n    ,'year sin'\n    ,'year cos'\n           ]\nlabel = ['Hydrometry_Nave_di_Rosano']\nX,Y = train[model_features],train[label]\n\n#X_train, X_test,Y_train, Y_test =  train_test_split(X,Y, train_size = 0.8, shuffle = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_tweedie_deviance\n\nprint(\"Parameter optimization\")\nxgb_model = xgb.XGBRegressor(n_jobs=1)\nclf = GridSearchCV(xgb_model,scoring = 'neg_mean_poisson_deviance',\n                   param_grid = {'max_depth': [2, 4, 6],\n                                 'n_estimators': [50, 100, 200], \n                                 'alpha': [0,0.01,0.001]\n                                 ,'objective': ['reg:tweedie']\n                                 ,'tweedie_variance_power':[1]\n                                }, verbose=1, n_jobs=1, cv = KFold(n_splits=7, shuffle=False))\nclf.fit(X.values, Y.values)\nprint(clf.best_score_)\nprint(clf.best_params_)\nparam = clf.best_params_\n\nkf = KFold(n_splits=2, shuffle=False)\nfor train_index, test_index in kf.split(X):\n    xgb_model = xgb.XGBRegressor(**clf.best_params_, n_jobs=1).fit(X.iloc[train_index,:].values, Y.iloc[train_index,:].values)\n    predictions = xgb_model.predict(X.iloc[test_index,:].values)\n    actuals = Y.iloc[test_index,:].values\n    print('MSE:\\n')\n    print(mean_squared_error(np.exp(actuals-1), np.exp(predictions-1)))\n    print('MAE:\\n')\n    print(mean_absolute_error(np.exp(actuals-1), np.exp(predictions-1)))\n    print('MAPE:\\n')\n    print(mean_tweedie_deviance(np.exp(actuals-1), np.exp(predictions-1),power = 1))\n    \nxgb.plot_importance(xgb_model)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_features = ['Temperature_Season_shifted_1'\n    ,'Rainfall_Le_Croci_shifted_1'\n    ,'Rainfall_Cavallina_shifted_1'\n    ,'Rainfall_S_Agata_shifted_1'\n    ,'Rainfall_Mangona_shifted_1'\n    ,'Rainfall_S_Piero_shifted_1'\n    #,'Rainfall_Vernio_shifted_1'\n    #,'Rainfall_Incisa_shifted_1'\n    #,'Rainfall_Bibbiena_shifted_1'\n    ,'Hydrometry_Nave_di_Rosano_shifted_1'\n    ,'year sin'\n    ,'year cos'\n           ]\n\n# costruisco il dataset da quello reale\ndf = pd.concat([data[labels]\n           #,train[features].shift(0).add_suffix('_actual')\n           ,data[features+labels].shift(1).add_suffix('_shifted_1')\n           #,train[features+labels].shift(2).add_suffix('_shifted_2')\n          ]\n          ,axis = 1)\ndf['Hydrometry_Nave_di_Rosano'] = 1+  np.log(df.Hydrometry_Nave_di_Rosano)\ndf = get_time_features(df.reset_index().rename(columns={'index':'Date'}))\n#droppo la prima riga che avrà ,molti null\ndf = df.iloc[1:]\ndf['predicted'] = 0\nfor i,row in df.iterrows():\n    if row['Hydrometry_Nave_di_Rosano_shifted_1'] == np.nan:\n        row['Hydrometry_Nave_di_Rosano_shifted_1'] = df.iloc[i-1,'predicted']\n    x = np.asarray(pd.DataFrame(row).T[model_features])\n    df.loc[i,'predicted'] = xgb_model.predict(x)\n    \n# scorrendo iterativamente dalla prima riga a quella successiva proseguo come segue\n## se Hydrometry shift 1 è null allora faccio la prediction tramite la il valore precedentemente predetto\ndf['Imputed'] = df.Hydrometry_Nave_di_Rosano.fillna(df.predicted)\ndf.plot.line(x = 'Date', y = ['predicted','Hydrometry_Nave_di_Rosano'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\n\nfig1 = ax.bar(\n    x = range(len(xgb_model.feature_importances_)),\n    height = xgb_model.feature_importances_, \n    tick_label = model_features\n)\nax.set_ylabel('Importances')\nax.set_title('Feature Importances')\nplt.xticks(rotation = 45)\n\nfig.tight_layout()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Model Engineering (LSTM) (Removed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"data['Hydrometry_Nave_di_Rosano'].interpolate(method='spline',order = 5,inplace=True)\ndata.interpolate(method='linear',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"data.reset_index().plot.line(x='index',y='Temperature_Firenze',figsize = (20,9),title = column.replace('_',' '))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"\ndecomposed = seasonal.seasonal_decompose(\n    data['Temperature_Firenze'], period = 366, extrapolate_trend = 'freq'\n)\ndecomposed.plot()\ndata['Temperature_Trend'] = decomposed.trend\ndata['Temperature_Season'] = decomposed.seasonal\ndata['Temperature_Resid'] = decomposed.resid\n#data['Rainfall_Seasonal'] = d.seasonal\n#data['Rainfall_Resid'] = d.resid\n\n\nf, ax = plt.subplots(nrows=4, ncols=1, figsize=(15, 12))\nf.suptitle('Seasonal Components of Features', fontsize=16)\nsns.lineplot(x=data.reset_index().index, y=data.Hydrometry_Nave_di_Rosano, ax=ax[0], color='dodgerblue', label='Hydrometry')\nax[0].set_ylabel(ylabel='Hydrometry', fontsize=14)\n\nsns.lineplot(x=data.reset_index().index, y=data.Temperature_Season, ax=ax[1], color='dodgerblue', label='Seasonal')\nax[1].set_ylabel(ylabel='Temperature', fontsize=14)\n\nsns.lineplot(x=data.reset_index().index, y=data.Temperature_Trend, ax=ax[2], color='dodgerblue', label='Trend')\nax[2].set_ylabel(ylabel='Temperature', fontsize=14)\n\nsns.lineplot(x=data.reset_index().index, y=data.Rainfall_Le_Croci, ax=ax[3], color='lightblue')\nax[3].set_ylabel(ylabel='Rainfall', fontsize=14)\n\n\nfor i in range(4):\n    ax[i].set_xlim([list(data.reset_index().index)[0],list(data.reset_index().index)[-1]])\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"df = get_time_features(data.reset_index().rename(columns = {'index':'Date'}))\nfeatures = ['Hydrometry_Nave_di_Rosano',#'Rainfall_Seasonal','Rainfall_Resid',\n            'Rainfall_Le_Croci', 'Rainfall_Cavallina', 'Rainfall_S_Agata',\n            'Rainfall_Mangona', 'Rainfall_S_Piero',\n            'Temperature_Season',\n           'year sin','year cos'\n           ]\ndf = df[df.year<2020][features]\ndf['Hydrometry_Nave_di_Rosano'] = 1 + np.log(df['Hydrometry_Nave_di_Rosano'])\n\n#df.set_index('Date', inplace=True)\n\nprint(df.columns)\ncolumn_indices = {name: i for i, name in enumerate(df.columns)}\nnum_features = len(column_indices)\nn = len(df)\ntrain_df = df[0:int(n*0.7)]\nval_df = df[int(n*0.7):int(n*0.9)]\ntest_df = df[int(n*0.9):]\n\nrainfall_columns = [\n    'Rainfall_Le_Croci', 'Rainfall_Cavallina', 'Rainfall_S_Agata',\n       'Rainfall_Mangona', 'Rainfall_S_Piero',\n    'Hydrometry_Nave_di_Rosano',\n    #'Rainfall_Resid',\n]\n\nnum_features = df.shape[1]\n\n#train_df[rainfall_columns] = train_df[rainfall_columns].apply(lambda x: np.power(x,1/4))\n#val_df[rainfall_columns] = val_df[rainfall_columns].apply(lambda x: np.power(x,1/4))\n#test_df[rainfall_columns] = test_df[rainfall_columns].apply(lambda x: np.power(x,1/4))\n\nfor feature in rainfall_columns:\n    train_df[feature] = boxcox(train_df[feature] + 0.5,-1)\n    val_df[feature] = boxcox(val_df[feature] + 0.5,-1)\n    test_df[feature] = boxcox(test_df[feature] + 0.5,-1)\n\ntrain_mean = train_df.mean()\ntrain_std= train_df.std()\n\n\n\ntrain_df = (train_df - train_mean) / train_std\nval_df = (val_df - train_mean) / train_std\ntest_df = (test_df - train_mean) / train_std\n\ndf_std = (df - train_mean) / train_std\ndf_std = df_std.melt(var_name='Column', value_name='Normalized')\nplt.figure(figsize=(12, 6))\nax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n_ = ax.set_xticklabels(df.keys(), rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"## Forecasting\n\nIn this section we focus on building a forecasting model for our scenario. I've decided to use Tensorflow (because I'm studying it). In particular, I've largely used and experimented what is written in the [official tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series). The following functions and classes are retrieved from this tutorial. \n\nI want to build a single step model to predict what happens netxt and a multi-step models, i.e. given data about past events, I want to predict a sequence of future values.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"class WindowGenerator():\n    def __init__(self, input_width, label_width, shift,\n                 train_df=train_df, val_df=val_df, test_df=test_df,\n                 label_columns=None):\n        # Store the raw data.\n        self.train_df = train_df\n        self.val_df = val_df\n        self.test_df = test_df\n\n        # Work out the label column indices.\n        self.label_columns = label_columns\n        if label_columns is not None:\n            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n        self.column_indices = {name: i for i, name in enumerate(train_df.columns)}\n\n        # Work out the window parameters.\n        self.input_width = input_width\n        self.label_width = label_width\n        self.shift = shift\n\n        self.total_window_size = input_width + shift\n\n        self.input_slice = slice(0, input_width)\n        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n        self.label_start = self.total_window_size - self.label_width\n        self.labels_slice = slice(self.label_start, None)\n        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n    def __repr__(self):\n        return '\\n'.join([\n            f'Total window size: {self.total_window_size}',\n            f'Input indices: {self.input_indices}',\n            f'Label indices: {self.label_indices}',\n            f'Label column name(s): {self.label_columns}'])\n    \ndef split_window(self, features):\n    inputs = features[:, self.input_slice, :]\n    labels = features[:, self.labels_slice, :]\n    if self.label_columns is not None:\n        labels = tf.stack(\n            [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n            axis=-1)\n\n      # Slicing doesn't preserve static shape information, so set the shapes\n      # manually. This way the `tf.data.Datasets` are easier to inspect.\n    inputs.set_shape([None, self.input_width, None])\n    labels.set_shape([None, self.label_width, None])\n    \n    return inputs, labels\n    \nWindowGenerator.split_window = split_window\n\ndef plot(self, model=None, plot_col='Hydrometry_Nave_di_Rosano', max_subplots=3):\n    inputs, labels = self.example\n    plt.figure(figsize=(12, 8))\n    plot_col_index = self.column_indices[plot_col]\n    max_n = min(max_subplots, len(inputs))\n    for n in range(max_n):\n        plt.subplot(3, 1, n+1)\n        plt.ylabel(f'{plot_col} [normed]')\n        plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n                 label='Inputs', marker='.', zorder=-10)\n        \n        if self.label_columns:\n            label_col_index = self.label_columns_indices.get(plot_col, None)\n        else:\n            label_col_index = plot_col_index\n\n        if label_col_index is None:\n            continue\n\n        plt.scatter(self.label_indices, labels[n, :, label_col_index],\n                    edgecolors='k', label='Labels', c='#2ca02c', s=64)\n        if model is not None:\n            predictions = model(inputs)\n            plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n                        marker='X', edgecolors='k', label='Predictions',\n                        c='#ff7f0e', s=64)\n\n        if n == 0:\n            plt.legend()\n\n    plt.xlabel('Time')\n\nWindowGenerator.plot = plot\n\ndef make_dataset(self, data):\n    data = np.array(data, dtype=np.float32)\n    ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n        data=data,\n        targets=None,\n        sequence_length=self.total_window_size,\n        sequence_stride=1,\n        shuffle=True,\n        batch_size=128)\n\n    ds = ds.map(self.split_window)\n\n    return ds\n\nWindowGenerator.make_dataset = make_dataset\n\n@property\ndef train(self):\n    return self.make_dataset(self.train_df)\n\n@property\ndef val(self):\n    return self.make_dataset(self.val_df)\n\n@property\ndef test(self):\n    return self.make_dataset(self.test_df)\n\n@property\ndef example(self):\n    \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n    result = getattr(self, '_example', None)\n    if result is None:\n        # No example batch was found, so get one from the `.train` dataset\n        result = next(iter(self.train))\n        # And cache it for next time\n        self._example = result\n    return result\n\nWindowGenerator.train = train\nWindowGenerator.val = val\nWindowGenerator.test = test\nWindowGenerator.example = example\n\nMAX_EPOCHS = 50\n\ndef compile_and_fit(model, window, learning_rate = 0.01, epochs = MAX_EPOCHS, patience=2):\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                    patience=patience,\n                                                    mode='min')\n\n    model.compile(loss=tf.losses.MAE\n                  ,\n                optimizer=tf.optimizers.Adam(learning_rate = learning_rate),\n                metrics=[tf.metrics.MeanAbsoluteError()\n                         ,tf.metrics.MeanAbsolutePercentageError()\n                         ,tf.metrics.Poisson()\n                        ])\n\n    history = model.fit(window.train, epochs=epochs,\n                      validation_data=window.val,\n                      callbacks=[early_stopping])\n    \n    x = pd.DataFrame(history.history)\n    x.plot.line(y=['loss','val_loss'],figsize = (5,3))\n    x.plot.line(y=['mean_absolute_error','val_mean_absolute_error'],figsize = (5,3))\n    x.plot.line(y=['mean_absolute_percentage_error','val_mean_absolute_percentage_error'],figsize = (5,3))\n    x.plot.line(y=['poisson','val_poisson'],figsize = (5,3))\n    \n    del x\n    \n    return history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"class Baseline(tf.keras.Model):\n    def __init__(self, label_index=None):\n        super().__init__()\n        self.label_index = label_index\n    def call(self, inputs):\n        if self.label_index is None:\n            return inputs\n        result = inputs[:, :, self.label_index]\n        return result[:, :, tf.newaxis]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"single_step_window = WindowGenerator(\n    input_width=3, label_width=3, shift=1,\n    label_columns=['Hydrometry_Nave_di_Rosano'])\n#single_step_window\n\nbaseline = Baseline(label_index=column_indices['Hydrometry_Nave_di_Rosano'])\n\nbaseline.compile(loss=tf.losses.MAE,\n                 metrics=[tf.metrics.MeanAbsoluteError(), tf.metrics.MeanAbsolutePercentageError()])\n\nval_performance = {}\nperformance = {}\nval_performance['Baseline'] = baseline.evaluate(single_step_window.val)\nperformance['Baseline'] = baseline.evaluate(single_step_window.test, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"wide_window = WindowGenerator(\n    input_width=1, label_width=1, shift=1,\n    label_columns=['Hydrometry_Nave_di_Rosano'])\n\nlstm_model = tf.keras.models.Sequential([\n    # Shape [batch, time, features] => [batch, time, lstm_units]\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    # Shape => [batch, time, features]\n    tf.keras.layers.Dense(units=1)\n])\n\nhistory = compile_and_fit(lstm_model, wide_window, learning_rate = 0.001)\n\nval_performance['LSTM'] = lstm_model.evaluate(wide_window.val)\nperformance['LSTM'] = lstm_model.evaluate(wide_window.test, verbose=0)\n\nwide_window.plot(lstm_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"\nx = np.arange(len(performance))\nwidth = 0.3\nmetric_name = 'mean_absolute_error'\nmetric_index = lstm_model.metrics_names.index('mean_absolute_error')\nval_mae = [v[metric_index] for v in val_performance.values()]\ntest_mae = [v[metric_index] for v in performance.values()]\n\nplt.ylabel('mean_absolute_error [Hydrometry, normalized]')\nplt.bar(x - 0.17, val_mae, width, label='Validation')\nplt.bar(x + 0.17, test_mae, width, label='Test')\nplt.xticks(ticks=x, labels=performance.keys(),\n           rotation=45)\n_ = plt.legend()\n\n\nfor name, value in performance.items():\n    print(f'{name:12s}: {value[1]:0.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"plt.bar(x = range(0,lstm_model.layers[0].weights[0].numpy().shape[0]),\n        height = np.average(lstm_model.layers[0].weights[0].numpy(),axis = 1), \n        yerr = np.var(lstm_model.layers[0].weights[0].numpy(),axis = 1),\n       align='center', ecolor='black', capsize=10)\naxis = plt.gca()\naxis.set_xticks(range(len(train_df.columns)))\n_ = axis.set_xticklabels(train_df.columns, rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}