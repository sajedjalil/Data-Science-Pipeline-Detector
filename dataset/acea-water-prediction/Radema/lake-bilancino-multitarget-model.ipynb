{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom scipy.stats import boxcox\n\n# plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport plotly.express as px\n#pd.options.plotting.backend = \"plotly\"\n\n# settings\nplt.style.use('seaborn')\nplt.rcParams[\"figure.figsize\"] = (16, 8)\n\nimport tensorflow as tf\n\npd.options.display.max_columns = 500\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nIn this notebook I'll try to approach the Acea Smart Water Analytics competion. \nThe Acea Group is one of the leading Italian multiutility operators. The company manages and develops water and electricity networks and environmental services. I'm a bit familiar with Acea and the names in the datasets since I live in Italy. \n\nIn this notebook, I don't plan to address all the waterbodies but I plan to outline an precise pipeline (see their schema below):\n* to analyze the different datasets grouped by waterbody type\n* clean the data\n* build new features\n* train and validate forecasting model(s)\n\\\n\\\n![image](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F6195295%2Fcca952eecc1e49c54317daf97ca2cca7%2FAcea-Input.png?generation=1606932492951317&alt=media)\n\\\n\\\nI'll try to keep it as simple as possible. In particular I will start with simple EDAs, simple models and then try to add complexity. \n\n"},{"metadata":{},"cell_type":"markdown","source":"# Retrieve source datasets\n\nIn notebook I want to focus on a specific waterbody: Lake Bilancino.\n\\\nIn the following, I plan to test and experiment building a multi target model. Indeed, the related dataset has two attributes that need to be predicted:\n* the Flow Rate\n* the Lake Level\n\nIn the next cells, I will implement a rather simple data pipeline to clean data and build features; then, I will build a multioutput model. "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from datetime import datetime\ncustom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y')\n\ndf = {}\nbodytype = {'Aquifer','Water','Lake','River'}\nLake = {}\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        if filename[-4:]=='.csv':\n            if filename.startswith('Lake'):\n                Lake[filename[:-4].replace('Lake_','')] = pd.read_csv(\n                    os.path.join(dirname, filename)\n                   ,parse_dates = ['Date'],date_parser = custom_date_parser\n                )\nlake_df = Lake['Bilancino']\ndel Lake","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"lake_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"features =[\n    'Rainfall_S_Piero', 'Rainfall_Mangona', 'Rainfall_S_Agata',\n       'Rainfall_Cavallina', 'Rainfall_Le_Croci', 'Temperature_Le_Croci'\n] \nlabels = [\n    'Lake_Level', 'Flow_Rate'\n]\ndata = lake_df.copy()\n\nprint(data.describe().transpose())\nprint('\\n')\nfor column in features + labels:\n    data.plot.line(x='Date',y=column,figsize = (20,9),title = column.replace('_',' '))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n\nIn this section, you can find the code related to data exploration, data cleansing and feature engineering.\n\nLet's start building time features relate to column date (format: dd/mm/yyyy), in particular:\n* year (integer)\n* month (integer)\n* day_in_year (integer from 1 to 366)\n* week_in_year (integer from 1 to 55)\n* year sin (float), defined as the sin(2 * pi * day_in_year / 365.25)\n* year cos (float), defined as the cos(2 * pi * day_in_year / 365.25)\n\nyear sin/cos are particullary useful to easily catch seasons. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# aggiustare indice con datetime\ndef get_time_features(df):\n    data = df.copy()\n    data['year'] = data.Date.dt.year\n    data['month'] = data.Date.dt.month\n    data['day_in_year'] = data.Date.dt.dayofyear\n    data['week_in_year'] = data.Date.dt.isocalendar().week.astype(int)\n    data['year sin'] = np.sin(2*np.pi*data['day_in_year']/365.25)\n    data['year cos'] = np.cos(2*np.pi*data['day_in_year']/365.25)\n    return data\n\ndata = get_time_features(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a clear visualization o missing values and attributes quality."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# controllare null \nnulls = data.groupby(\n    'year'\n).agg('count').transpose()\nsns.heatmap(nulls)\ndel nulls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that :\n* there are missing values in 2002 and 2003\n* we have only half (first six month) the values for 2020\n\nI decided to restrict this analysis to the data for years in 2004 and 2019."},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"data = data[(data.year > 2003)&(data.year<2020)]\ndata.Temperature_Le_Croci[data.Temperature_Le_Croci.isna()]=6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"markdown","source":"## Temperature Seasonality\n\nIn the following we decompose the Temperature decomposition. In the following cells, we visualize:\n* autocorrelation plot to the detect possible period values\n* Temperature Trend, Seasonality and Residual components"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.plotting.autocorrelation_plot(data.Temperature_Le_Croci)\nplt.show()\n\n# controllare seasonality, trend e resid degli attributi\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndecomposed = seasonal_decompose(\n    data['Temperature_Le_Croci'], \n    period = 365,\n    extrapolate_trend = 'freq'\n)\n\ndecomposed.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data['Temperature_Trend'] = decomposed.trend\ndata['Temperature_Season'] = decomposed.seasonal\ndata['Temperature_Resid'] = decomposed.resid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# sistemare flow rate\n\n#data['Flow_Rate'] = np.abs(data['Flow_Rate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pd.plotting.scatter_matrix(data[features + ['Temperature_Trend','Temperature_Season','Temperature_Resid','year sin','year cos'] + labels ], figsize = (20,20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Targets Visualization\n\nWith the following visualizations, we focus on the target variables and their scaled versions. These are useful to decide which model we can opt for each variable."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2, 2)\n\nax[0,0].set_title('Lake Level')\n\nax[0,0].hist(data.Lake_Level, bins = 50, density = True)\nax[0,0].set_ylabel('Lake_Level')\n\nax[1,0].hist(boxcox(1- data.Lake_Level + data.Lake_Level.max(),0), bins = 50, density = True)\nax[0,0].set_ylabel('Scaled Lake_Level')\n\n\nax[0,1].set_title('Flow Rate')\n\nax[0,1].hist(data.Flow_Rate, bins = 50, density = True)\nax[0,1].set_ylabel('Flow_Rate')\n\nax[1,1].hist(1 + np.log(1 + data.Flow_Rate), bins = 50, density = True)\nax[1,1].set_ylabel('Scaled Flow Rate')\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def split_data(df, frac=0.8):\n    data = df.copy()\n    data.sort_index(inplace = True)\n    n = len(data)\n    return data[:int(n*frac)],data[int(n*frac):]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from scipy.stats import boxcox\n\ntrain, val = split_data(data,0.8)\n\ntrain.set_index('Date',inplace = True)\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain_df = (train - train_mean)/train_std\nmaxima = max(train_df.Lake_Level)\n\ntrain_df.Lake_Level =  boxcox(1- train_df.Lake_Level + maxima,0)\ntrain_df.Flow_Rate = 1 + np.log(1 + train_df.Flow_Rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#pd.plotting.scatter_matrix(train_df[features + ['Temperature_Trend','Temperature_Season','Temperature_Resid','year sin','year cos'] + labels ], figsize = (20,20))\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"num_days = 1\nsns.heatmap(pd.concat(\n    [train_df[labels],\n     train_df[features + ['Temperature_Trend','Temperature_Season','Temperature_Resid','year sin','year cos']  ].shift(num_days).add_suffix('_shifted')\n    #,train_df[labels ].shift(30).add_suffix('_shifted')\n    ]\n    ,axis = 1).dropna().corr()\n            ,annot = True)\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We build the dataset as follows:\n* target variables : Flow_Rate and Lake_Level\n* Rainfall features shifted by 1 (day): Rainfall_S_Piero_shifted, Rainfall_Mangona_shifted, Rainfall_S_Agata_shifted, Rainfall_Cavallina_shifted, Rainfall_Le_Croci_shifted\n* Lake_Level and Flow_Rate shifted by 30 (days)\n* Temperature features shifted by 1 (day): Temperature_Season_shifted, Temperature_Trend_shifted\n* Time features: year sin, year cos\n\\\n\\\nI decided to opt for Lake_Level and Flow_Rate shifted by 30 because I assume that this is an information available in \"production\"."},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"model_features = ['Rainfall_S_Piero_shifted', 'Rainfall_Mangona_shifted', 'Rainfall_S_Agata_shifted',\n       'Rainfall_Cavallina_shifted', 'Rainfall_Le_Croci_shifted',\n       #'Lake_Level_shifted', 'Flow_Rate_shifted', \n                  'year sin_shifted', 'year cos_shifted', 'Temperature_Trend_shifted',\n       'Temperature_Season_shifted']\ntrain_df = pd.concat([train_df[labels],\n                      train_df[features + ['Temperature_Trend','Temperature_Season','Temperature_Resid','year sin','year cos']].shift(1).add_suffix('_shifted'),\n                     # train_df[labels ].shift(30).add_suffix('_shifted')\n                     ],axis = 1).dropna().iloc[1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multitarget Model (XGBRegressor)\n\nI found three different possibilities to build a multioutput model:\n* use [MultiOutputRegressor from scikit.multioutput](https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputRegressor.html) but I couldn't opt for different estimators\n* use a multistep Tensorflow model for forecasting (see [here](https://www.tensorflow.org/tutorials/structured_data/time_series))\n* build from scratch, so that I can adopt different models to adapt better to target distributions"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV, KFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_tweedie_deviance\n\nmodel = {\n    'Lake_Level': {\n        'estimator':xgb.XGBRegressor(objective = 'reg:squarederror',n_jobs = 1),\n        'param_grid' : {'max_depth': [2,3, 4],\n                                 'n_estimators': [50, 100], \n                                 'alpha': [0,0.01,0.001]\n                                },\n        'scoring':'neg_mean_absolute_error'\n    },\n    'Flow_Rate' : {\n        'estimator':xgb.XGBRegressor(objective = 'reg:tweedie',n_jobs = 1),\n        'param_grid': {'max_depth': [2,3,4],\n                                 'n_estimators': [50, 100], \n                                 'alpha': [0,0.01,0.001]\n                                 ,'objective': ['reg:tweedie']\n                                 ,'tweedie_variance_power':[1,2]\n                                },\n        'scoring':'neg_mean_gamma_deviance'\n    }\n    \n}\n\n\nX,Y = train_df[model_features],train_df[labels]\n\n\nfor label in labels:\n    clf = GridSearchCV(\n        model[label]['estimator'], \n        scoring = model[label]['scoring'],\n        param_grid = model[label]['param_grid'], \n        verbose=1, n_jobs=1, cv = KFold(n_splits=7, shuffle=False)\n    )\n    clf.fit(X.values, Y[label].values)\n    print('\\nGridSearchCV for '+ label)\n    print('\\nBest Score:')\n    print(clf.best_score_)\n    print('\\nBest Parameters:')\n    print(clf.best_params_)\n    model[label]['clf'] = clf\n    \n    kf = KFold(n_splits=2, shuffle=False)\n    for train_index, test_index in kf.split(X):\n        reg = model[label]['clf'].best_estimator_\n        predictions = reg.predict(X.iloc[test_index,:].values)\n        actuals = Y[label].iloc[test_index].values\n        print('\\n'+label + ' MSE:')\n        print(mean_squared_error(actuals, predictions))\n        print('\\n'+label+' MAE:')\n        print(mean_absolute_error(actuals, predictions))\n        print('\\n'+label+' Poisson Deviance (p = 1):')\n        print(mean_tweedie_deviance(1 + actuals, 1 + predictions,power = 1))\n        print('\\n'+label+'Gamma Deviance (p = 2):')\n        print(mean_tweedie_deviance(1 + actuals, 1 + predictions,power = 2))\n    fig, ax = plt.subplots()\n\n    fig1 = ax.bar(\n        x = range(len(reg.feature_importances_)),\n        height = reg.feature_importances_, \n        tick_label = model_features\n    )\n    ax.set_ylabel('Importances')\n    ax.set_title('Model '+label+': Features Importances')\n    plt.xticks(rotation = 45)\n    fig.tight_layout()\n\n    plt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# costruisco il dataset da quello reale\ndf = data.copy()\ndf.set_index('Date',inplace = True)\ndf = df[features + ['Temperature_Trend','Temperature_Season','Temperature_Resid','year sin','year cos'] + labels]\n#normalize dataset with previous parameters\ndf = (df - train_mean)/train_std\n\nminima = abs((-train_df.Lake_Level).min())\n\ndf.Lake_Level =  boxcox(1- df.Lake_Level + maxima,0)\ndf.Flow_Rate = 1 + np.log(1 + df.Flow_Rate)\n\n#shift 1\ndf = pd.concat([df[labels]\n                ,df[features + ['Temperature_Trend','Temperature_Season','Temperature_Resid','year sin','year cos']].shift(1).add_suffix('_shifted')\n                #,df[labels].shift(30).add_suffix('_shifted')\n          ]\n          ,axis = 1).iloc[30:]\n\n\n#droppo la prima riga che avrÃ  ,molti null\ndf['Lake_Level_predicted'] = 0\ndf['Flow_Rate_predicted'] = 0\n\nfor i,row in df.iterrows():\n    #if row['Lake_Level_shifted'] == np.nan:\n    #    row['Lake_Level_shifted'] = df.iloc[i-1,'Lake_Level_predicted']\n    #if row['Flow_Rate_shifted'] == np.nan:\n    #    row['Flow_Rate_shifted'] = df.iloc[i-1,'Flow_Rate_predicted']\n        \n    x = np.asarray(pd.DataFrame(row).T[model_features])\n    df.loc[i,'Lake_Level_predicted'] = model['Lake_Level']['clf'].best_estimator_.predict(x)\n    df.loc[i,'Flow_Rate_predicted'] = model['Flow_Rate']['clf'].best_estimator_.predict(x)\n    \n# scorrendo iterativamente dalla prima riga a quella successiva proseguo come segue\n## se Hydrometry shift 1 Ã¨ null allora faccio la prediction tramite la il valore precedentemente predetto\nfrom scipy.special import inv_boxcox\n\ndf.Lake_Level = (-inv_boxcox(df.Lake_Level,0) + 1 + maxima)*train_std.Lake_Level + train_mean.Lake_Level\ndf.Lake_Level_predicted = (-inv_boxcox(df.Lake_Level_predicted,0) + 1 + maxima)*train_std.Lake_Level + train_mean.Lake_Level\n#df.Lake_Level = (-df.Lake_Level + minima)*train_std.Lake_Level + train_mean.Lake_Level \n#df.Lake_Level_predicted = (-df.Lake_Level_predicted + minima)*train_std.Lake_Level + train_mean.Lake_Level \ndf.Flow_Rate = (np.exp(df.Flow_Rate - 1) - 1)*train_std.Flow_Rate + train_mean.Flow_Rate\ndf.Flow_Rate_predicted = (np.exp(df.Flow_Rate_predicted - 1) - 1)*train_std.Flow_Rate + train_mean.Flow_Rate\n\ndf['Lake_Level_Imputed'] = df.Lake_Level.fillna(df.Lake_Level_predicted)\ndf['Flow_Rate_Imputed'] = df.Flow_Rate.fillna(df.Flow_Rate_predicted)\n \ndf.reset_index().plot.line(x = 'Date', y = ['Lake_Level_predicted','Lake_Level'])\ndf.reset_index().plot.line(x = 'Date', y = ['Flow_Rate_predicted','Flow_Rate'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Work in progress\nTo do:\n* try with a (tensorflow?) model with multiple times steps \n* Poisson Regressor with Tensorflow?\n* Probabilistic ML model?"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}