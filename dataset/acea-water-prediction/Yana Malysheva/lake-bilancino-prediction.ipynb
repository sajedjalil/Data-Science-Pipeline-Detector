{"cells":[{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport datetime\n\nfrom collections import defaultdict\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.metrics import explained_variance_score, mean_squared_error, mean_absolute_error\n\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Integer\n\nfrom graphviz import Digraph\nfrom IPython.display import SVG\n\nimport warnings\n\nfrom lightgbm import LGBMRegressor","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# fix the date in a dataframe (pandas does not read it in correctly by default), set it as index\ndef fix_date(df):\n    df['Date'] = pd.to_datetime(df['Date'], format=\"%d/%m/%Y\")\n    df.set_index('Date', drop=False, inplace=True)\n    df['dayofyear'] = df['Date'].dt.dayofyear","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# The main rainfall effect model\n\ndef rainfall_effect(\n    # data parameters\n    rain_series, # pd series where index is date and value is amount of rain which fell that day\n    start_date = None,\n    end_date = None,\n    # simulation parameters\n    fraction_retained = 0.9, # fraction of water retained each day (vs. fraction that is carried away elsewhere - pooling, transpiration, etc.)\n    first_day_flow = 0.004, # what fraction of the rain takes effect on the first day\n    funnel_start_width = 0.0, # when 0, funnel is cone-shaped; when large, funnel is closer to cyllinder-shaped.\n    time_gap = 0, # integer(days) - how long does it take even the first water to reach the area of interest\n):\n    # calculate default start and end date\n    if start_date is None:\n        start_date = rain_series.first_valid_index()\n    if end_date is None:\n        end_date = rain_series.last_valid_index()\n    \n    # calculate flow speed per \"funnel unit\"\n    first_day_area = funnel_start_width+1.0\n    flow_speed = first_day_flow/first_day_area\n    \n    # total rain \"taking effect\" on a given day\n    rain_effect = defaultdict(int)\n    \n    # process rain coming in each day\n    current_date = start_date\n    while current_date <= end_date:\n        rain = 0\n        if current_date in rain_series.index:\n            rain = rain_series[current_date]\n        \n        # start with explicit 0 for each input date      \n        if current_date not in rain_effect:\n            rain_effect[current_date] = 0\n            \n        # iterate through upcoming days and calculate effect of rain which reaches the body *on that day*\n        retained_remaining = rain # this variable keeps track of the effect of retention/drainage (but ignores actual outflow)\n        # effectively,it is used to infer how much water is left which originated on a given \"daily level\" of the funnel.\n        total_water_remaining = rain # actual amount of rain remaining \"un-claimed\"\n        current_area_factor = first_day_area\n        rain_effect_date = current_date + datetime.timedelta(int(time_gap))\n        while retained_remaining >= 0.01 and total_water_remaining >= 0.01 and rain_effect_date <= end_date:\n            water_out = current_area_factor*flow_speed*retained_remaining\n            water_out = min(water_out, total_water_remaining)\n        \n            # update totals\n            rain_effect[rain_effect_date] += water_out\n            total_water_remaining -= water_out\n            \n            # update running state variables\n            retained_remaining *= fraction_retained\n            total_water_remaining *= fraction_retained\n            current_area_factor += 2\n            rain_effect_date += datetime.timedelta(1)\n            \n        current_date += datetime.timedelta(1)\n    \n    return rain_effect","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# error metric: global error variance\ndef global_error_var(correct, pred):\n    return (correct-pred).var(ddof=0)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def local_error_var(correct, pred):\n    return (correct-pred).rolling(10).var(ddof=0).mean()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Helper functions to be able to define more elaborate priors for gp_optimize than just \"uniform\" and \"loguniform\"\n \n# reverse a log-uniform prior so that bigger values are more likely \ndef make_inverse_loguniform_prior(name, lower=None, upper=None):\n    \n    if lower is None:\n        lower = 0 + np.finfo(float).eps\n    if upper is None:\n        upper = 1 - np.finfo(float).eps\n        \n    def convert(x):\n        return upper-x\n    \n    dimension = Real(0 + np.finfo(float).eps, upper-lower, name=name, prior='log-uniform')\n    \n    return convert, dimension\n\n# set up a dimension such that the resulting converted variable will have a logistic distribution \n# with given s, mu, and lower/upper bounds.\ndef make_logit_prior(name, s = 1, m = 0, lower=None, upper=None):\n\n    # (0+np.finfo(float).eps)\n    \n    # x should be between 0 and 1\n    def convert_using_logit(x):\n        return m+np.log(x/(1-x))*s\n    \n    lower_x = 0 + np.finfo(float).eps\n    if lower is not None:\n        lower += np.finfo(float).eps\n        lower_x = 1/(1+np.exp((m-lower)/s))\n    upper_x = 1 - np.finfo(float).eps\n    if upper is not None:\n        upper -= np.finfo(float).eps\n        upper_x = 1/(1+np.exp((m-upper)/s))\n    \n    dimension = Real(lower_x, upper_x, name=name)\n    \n    return convert_using_logit, dimension","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# wrap rain effect calculation into a format which can be plugged into gp_optimize, with custom priors.\ndef make_rain_func_and_dimensions(\n    rain_series, \n    retained_prior = None,\n    flow_prior = None,\n    width_prior = None,\n    lag_prior = None,\n    verbose=True\n):\n    # TODO: take  in priors?..\n    \n    # How to convert given input variable (generate defaults if not provided in parameters)\n    if retained_prior is None:\n        # retained_conv, retained_dim = make_inverse_loguniform_dim('fraction_retained')\n        retained_conv, retained_dim = make_logit_prior('fraction_retained', s=0.3, m=0.9, lower=0.0, upper=1.0)\n    else:\n        retained_conv, retained_dim = retained_prior\n        \n    if flow_prior is None:\n        # we set the lower bound to 0.0001 for practical reasons: lower values don't make much of a cumulative effect, \n        # but they do take a long time to compute because the effect of each day's rainfall is spread out over many more days.\n        flow_conv, flow_dim = make_logit_prior('first_day_flow', s=0.05, m=0.01, lower=0.0001, upper=1.0) #m=0.1, s=0.1?\n    else:\n        flow_conv, flow_dim = flow_prior\n        \n    if width_prior is None:\n        width_conv, width_dim = make_logit_prior('funnel_start_width', s=40.0, m=180.0, lower=0.0) # m=270?\n    else:\n        width_conv, width_dim = width_prior\n    \n    if lag_prior is None:\n        lag_conv = lambda x: x\n        lag_dim = Integer(0,10, name='time_gap')\n    else:\n        lag_conv, lag_dim = lag_prior\n    \n    conversions = [retained_conv, flow_conv, width_conv, lag_conv]\n    dimensions = [retained_dim, flow_dim, width_dim, lag_dim]\n    \n    def convert(x):\n        return [conversions[i](x_i) for (i, x_i) in enumerate(x)]\n    \n    def calc_rain_from_vector(x):\n        fraction_retained, first_day_flow, funnel_start_width, time_gap = convert(x)\n        \n        if verbose:\n            print('inputs:', fraction_retained, first_day_flow, funnel_start_width, time_gap)\n            \n        return pd.Series(rainfall_effect(\n            # data parameters\n            rain_series,\n            # simulation parameters\n            fraction_retained = fraction_retained,\n            first_day_flow = first_day_flow,\n            funnel_start_width = funnel_start_width,\n            time_gap=time_gap\n        ))\n\n    return calc_rain_from_vector, dimensions, convert","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# use Baysian optimization with linear regresssion to fit ML model of rain effects, plus any set of linear-effect parameters.\ndef fit_rain_effects(\n    # inputs\n    rains_df, \n    ground_truth, \n    make_rain_func=make_rain_func_and_dimensions,\n    additional_fields=None, # additional fields to throw into linear regression\n    # options\n    error_func = global_error_var,\n    spinup=30, # TODO: use?..\n    verbose=True,\n    nrandom=20,\n    ntotal=100,\n    x0=None, # optional input point(s) to try for gp_minimize; e.g. best overall results of previous runs\n):\n    calcs = []\n    rain_names = []\n    converts = []\n    all_dims = []\n    dims_per_rain = 0\n    for rain_name, rain_series in rains_df.iteritems():\n        calc, dims, convert = make_rain_func(rain_series, verbose=verbose)\n        dims_per_rain = len(dims)\n        calcs.append(calc)\n        converts.append(convert)\n        rain_names.append(rain_name)\n        all_dims += dims\n        \n    reg_fields =rain_names\n    if additional_fields is not None:\n        reg_fields += list(additional_fields.columns)\n    \n    optimal_error = None\n    optimal_linreg = None\n    optimal_n = None\n    \n    def calculate_error(x):\n        prediction_frame = ground_truth.to_frame(name='ground_truth')\n        \n        for i, rain_calc in enumerate(calcs):\n            if(verbose):\n                print(rain_names[i])\n            rain_result = rain_calc(x[dims_per_rain*i:dims_per_rain*(i+1)])\n            prediction_frame[rain_names[i]] = rain_result\n            prediction_frame.loc[(prediction_frame['ground_truth'].notnull()) & (prediction_frame[rain_names[i]].isnull()), rain_names[i]] = 0\n\n        if additional_fields is not None:\n            prediction_frame[list(additional_fields.columns)] = additional_fields\n                \n        without_nulls = prediction_frame.dropna().copy()\n        \n        reg = LinearRegression().fit(without_nulls[reg_fields], without_nulls['ground_truth'])\n        if verbose:\n            print('rescale parameters:', reg.coef_, reg.intercept_)\n\n        without_nulls['pred'] = reg.predict(without_nulls[reg_fields])\n        \n        error = error_func(without_nulls['ground_truth'], without_nulls['pred'])\n        if verbose:\n            print('error value:', error)\n            print()\n            \n        nonlocal optimal_error, optimal_linreg, optimal_n\n        if optimal_error is None or error < optimal_error:\n            optimal_error = error\n            optimal_linreg = reg\n            optimal_n = len(without_nulls)\n        \n        return error\n        \n    res = gp_minimize(\n        calculate_error,  # function to minimize\n        all_dims,             # dimension configuration\n        acq_func=\"gp_hedge\",    # acquisition function (PI = optimize probability of reducing error; 'gp_hedge' - guess/vary)\n        n_calls=ntotal,      # number of evaluations of f\n        n_random_starts=nrandom, # first n calls are random (avoid local minima) \n        x0=x0, # input points to definitely try\n    )  \n    \n    additional_names = []\n    if additional_fields is not None:\n        additional_names = list(additional_fields.columns)\n    \n    return generate_prediction_function(rain_names, additional_names, res, converts, optimal_linreg, optimal_n)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Calculate BIC from error variance (making the gaussian assumption)\ndef get_bic(errvar, n, k):\n    return n*np.log(errvar) + k*np.log(n)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# given the outputs of a rain effect model fit, return a function which will generate the predictions based on that model.\ndef generate_prediction_function(rain_names, additional_field_names, fit_result, conversions, optimal_linreg, training_n, verbose=True):\n    \n    # get converted parameters for rain effect calculations\n    dims_per_rain = len(fit_result.x)//len(conversions)\n    all_rain_params = []\n    for i, convert in enumerate(conversions):\n        params = convert(fit_result.x[dims_per_rain*i:dims_per_rain*(i+1)])\n        all_rain_params.append(params)\n    \n    if verbose:\n        for name, params in zip(rain_names, all_rain_params):\n            print(f'Parameters for {name}: {params}')\n        print('Scaling:')\n        for name, coef in zip(rain_names+additional_field_names, optimal_linreg.coef_):\n            print(f'  {name}: {coef}')\n        print(f'Translation parameter: {optimal_linreg.intercept_}')\n        print(f'raw gp_minimize parameters: {fit_result.x}')\n        print(f'error value: {fit_result.fun}')\n        print(f'BIC (assuming error metric is error variance): {get_bic(fit_result.fun, training_n, len(fit_result.x)+len(optimal_linreg.coef_)+1)}')\n    \n    # function to generate prediction from trained parameters\n    def predict_from_rain(rain_fields, additional_fields=None):\n        pred_df = pd.DataFrame(index=rain_fields.index)\n        \n        for rain_name, rain_params in zip(rain_names, all_rain_params): \n            rain_series = rain_fields[rain_name]\n            fraction_retained, first_day_flow, funnel_start_width, time_gap = rain_params\n            rain_pred = pd.Series(rainfall_effect(\n                rain_series,\n                fraction_retained = fraction_retained, \n                first_day_flow = first_day_flow,\n                funnel_start_width = funnel_start_width,\n                time_gap = time_gap,\n            ))\n            pred_df[rain_name] = rain_pred\n            \n        if additional_fields is not None:\n            pred_df[list(additional_fields.columns)] = additional_fields\n            \n        pred_df.dropna(inplace=True)\n        return pd.Series(optimal_linreg.predict(pred_df), index=pred_df.index)\n            \n    return predict_from_rain\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def get_expected_inputs(input_data, dayofyear):\n    week_rolling_mean = input_data.rolling(7, center=True).mean()\n    expectations = defaultdict(list)\n    variances = defaultdict(list)\n    for d in range(365):\n        nearby_days = np.arange(d-3, d+3)%365+1\n        near_data = week_rolling_mean[dayofyear.isin(nearby_days)]\n        for field in week_rolling_mean.columns:\n            expectations[field].append(near_data[field].mean())\n            variances[field].append(near_data[field].var())\n    \n    for field in week_rolling_mean.columns:\n        expectations[field].append(expectations[field][-1])# hack to do  *something* about leap years.\n        variances[field].append(variances[field][-1])\n    return expectations, variances","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def gen_inputs_with_expectations(input_data, days_to_predict, expected_means):\n    expected_data = input_data.append(pd.DataFrame(index=days_to_predict))\n    expected_data['dayofyear'] = expected_data.index.dayofyear\n    for field in input_data.columns:\n        exp_df = pd.DataFrame(expected_means[field], index=range(1, 367), columns=[field+'_expected'])\n        expected_data = expected_data.join(exp_df, on='dayofyear')\n        expected_data[field] = expected_data[field].where(~expected_data.index.isin(list(days_to_predict)), expected_data[field+'_expected'])\n    return expected_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hello\n\nSo I rushed my competition submission (made the last changes literally 3 minutes before the deadline...) so I kind of bungled the implementation of my lake prediction. But I wanted to prove to myself that my model was viable, and it seems like it is if I actually do the math right and combine the components correctly.\n\nSo here it is. If you're curious about what my general model is doing (the `fit_rain_effects` function), briefly: it's a model of how rainfall on a particular day takes effect over the subsequent days, which has four physically-meaningful parameters and the emerget property of generating an $\\frac{x}{e^x}$ distribution. Plus some linear regression with other arbitrary parameters thrown in on top. All fit to the data using Bayesian optimization.\n\nOh yes, also, when forecasting, I use a sort of smoothed average of the rain and temperature one would expect to see on that day of year as inputs to the model.\n\nIf you want more detail, you'll have to read the code or wait until I figure out what the actual rules are about publishing submissions. Sorry. Hopefully the bits I do expain are still interesting too."},{"metadata":{},"cell_type":"markdown","source":"## Bilancino"},{"metadata":{},"cell_type":"markdown","source":"### Data Overview"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"bilancino=pd.read_csv('../input/acea-water-prediction/Lake_Bilancino.csv')\nfix_date(bilancino)\n\nrain_fields = ['Rainfall_S_Piero', 'Rainfall_Mangona', 'Rainfall_S_Agata', 'Rainfall_Cavallina', 'Rainfall_Le_Croci']\n\n# The mean flow out of the lake (Flow_Rate) yesterday\nbilancino['flow_mean_yesterday'] = bilancino['Flow_Rate'].rolling(2).mean()\n\nbilancino['delta_level'] = bilancino['Lake_Level'].diff()\n\n\nbilancino['temp_30'] = bilancino['Temperature_Le_Croci'].rolling(30).mean()\nbilancino['temp_120'] = bilancino['Temperature_Le_Croci'].rolling(90).mean()\nbilancino['temp_180'] = bilancino['Temperature_Le_Croci'].rolling(180).mean()\n\n\nbilancino.columns","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(4, figsize=(15,10))\n\nfor r in rain_fields:\n    bilancino[r].rolling(120).sum().plot(ax=axes[0])\n    \nbilancino['Temperature_Le_Croci'].plot(ax=axes[1])\n\nbilancino['Lake_Level'].plot(ax=axes[2])\n\nbilancino['Flow_Rate'].plot(ax=axes[3])\n    \nfor ax in axes:\n    ax.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cutoff = datetime.date(2016,1,1)\nb_train = bilancino[:train_cutoff].copy()\nb_test = bilancino[train_cutoff+datetime.timedelta(1):]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Dependencies and model"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"lake_graph = Digraph(graph_attr={'ranksep':'1'})\n\nlake_graph.node('R', 'Rainfall')\nlake_graph.node('T', 'Temperature')\nlake_graph.node('F', 'Flow out of dam', shape='octagon', color='blue')\nlake_graph.node('L', 'Lake Level', shape='octagon', color='green')\n\nlake_graph.edge('R', 'L', color='green')\nlake_graph.edge('T', 'L', color='green')\nlake_graph.edge('F', 'L', color='green')\nlake_graph.edge('L', 'F', color='blue')\n\nlake_graph","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Bilancino lake is an interesting case, because its behavior depends on a man-made and human-operated structure - the dam that created the lake.\n\nThe level of water in the lake depends on the flow out of the dam, but the flow out of the dam also depends on the lake level. Specifically, the lake level determines the amount of pressure created, and therefore the strengt of the flow.\n\nAccording to the challenge description, water is let out of the dam quickly at certain times, and allowed to collect at other times.\n\nFurther, the data indicates that the flow spikes drastically when the lake level goes over a certain point - This seems to be the dam's spillway being activated. According to [this site](http://cmcgruppo.com/cmc/en/project/bilancino-dam/), the spillway has an automatic flap gate. This means that the gate opens wider when pressure increases, which makes the interaction even more complicated."},{"metadata":{},"cell_type":"markdown","source":"#### Modeling change in lake level\n\nIn my previous experimentation, I found that two of the rainfall fields contain most of the information: `Rainfall_Mangona` and `Rainfall_Cavallina`. This makes sense: `Rainfall_Cavallina` is the closest location to the lake iteslf, and `Rainfall_Mangona` is located over the Sieve river, before the rver flows into the lake. Therefore, `Rainfall_Mangona` captures information about water which enters the lake via the river.\n\nThe new component here is how flow rate affects lake level: the amount of water that leaves the lake via the dam (or more precisely, *the amount that left yesterday*) is proportional to the subsequent reduction in lake level. So unlike the temerature parameters, **The parameters of flow in the linear regression actually have a direct physical meaning**. Specifically, the scaling coefficient tells us how to convert between flow rate and (change in) lake level."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"b_pred_func=fit_rain_effects(\n    rains_df = b_train[['Rainfall_Mangona', 'Rainfall_Cavallina']], \n    ground_truth = b_train['delta_level'], \n    additional_fields = b_train[['flow_mean_yesterday', 'Temperature_Le_Croci','temp_30', 'temp_120', 'temp_180']],\n    nrandom=3,\n    ntotal=10,\n    verbose=False, \n    x0=[\n        [0.15233691562216556, 0.8854084902188695, 0.9999999999999998, 1, 0.13666937295698817, 0.9979837155391087, 0.14202766104040032, 0],\n        [0.0474258731775668, 0.9999999974825013, 0.570952205261587, 1, 0.3850443032078931, 0.45805440829961064, 0.545042159616547, 0],\n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model predicts **lake level** based on **temperature, rainfall, and flow out of the lake**.\n\nBut when forecasting, we won't actually know the flow out of the lake - it's one of the variables we need to predict! So instead of using the returned prediction function with flow as input, we can use just move the flow effect to the other side of the equation, and predict the **cumulative change in water level as a result of weather inputs, including the water that subsequently flows out of the dam**\n\n$$\\text{Lake_Level} = C+\\text{Mangona_effect}*S_\\text{RM}+\\text{Cavallina_effect}*S_\\text{RM}+\n(\\sum{\\text{Temp_var_i}*S_i}) - \\text{Flow_Rate}*S_F\n$$\n\n$$\\text{Lake_Level} +\\text{Flow_Rate}*S_F =  C+\\text{Mangona_effect}*S_\\text{RM}+\\text{Cavallina_effect}*S_\\text{RM}+\n(\\sum{\\text{Temp_var_i}*S_i})\n$$\n\n(For clarity about whether water is flowing out or in, I inverted the $S_F$ paraameter from what it is in the linear regression - so it is around 0.007 instead of -0.007)"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"flow_lake_conversion = 0.007661467849044978\n\n# predict cumulative level delta + flow effect based on model of lake level\n# (using parameters trained in the model above)\ndef predict_cum_delta(input_df):\n    Rainfall_Mangona_prediction = pd.Series(rainfall_effect(\n        input_df['Rainfall_Mangona'],\n        fraction_retained = 2.220446049250313e-16, \n        first_day_flow = 1.0, \n        funnel_start_width = 191.42948720854787, \n        time_gap = 1,\n    ))\n\n    Rainfall_Cavallina_prediction = pd.Series(rainfall_effect(\n        input_df['Rainfall_Cavallina'],\n        fraction_retained = 0.7595424520378065, \n        first_day_flow = 0.0015911180194193453, \n        funnel_start_width = 187.22633570423756, \n        time_gap = 0,\n    ))\n    pred_cum_delta = 0.005862430953008586*Rainfall_Mangona_prediction+\\\n    1.4547492793031174*Rainfall_Cavallina_prediction+\\\n    0.0039183372980673425*input_df['Temperature_Le_Croci']+\\\n    -0.0015752030857040245*input_df['temp_30']+\\\n    -0.008107915323914416*input_df['temp_120']+\\\n    0.0048819421510388206*input_df['temp_180']+\\\n    -0.010936677349591556\n    return pred_cum_delta, Rainfall_Mangona_prediction, Rainfall_Cavallina_prediction\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cum_delta, rm, rc = predict_cum_delta(bilancino)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can then compare predictions to reality by adding the true delta-level and flow rate parameter, with the flow rate scaled according to the model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"true_cum_delta =(bilancino['delta_level']+bilancino['flow_mean_yesterday']*flow_lake_conversion)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax1 = plt.subplots(figsize=(15, 3))\ntrue_cum_delta.plot(label='True cumulative change')\ncum_delta[:train_cutoff].plot(alpha=0.7, label='Predicted cumulative change(training data)')\ncum_delta[train_cutoff:].plot(alpha=0.7, label='Predicted cumulative change(test data)', color='red')\nplt.legend()\nplt.xlim(datetime.date(2010,1,1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And then we can calculate a **cumulative sum** of the true and predicted deltas to see what would happen if water kept coming in the lake, but would magically never leave via the dam:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax1 = plt.subplots(figsize=(15, 3))\n\ntrue_cum_delta[cum_delta.first_valid_index():].cumsum().plot(label='True cumulative level')\ncum_delta.cumsum().plot(label='Predicted cumulative level')\nplt.legend()\nplt.xlim(datetime.date(2004,1,1))\nplt.title('Cumulative lake level predicted based on actual rain/temp data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Modeling flow out of lake\n\nAs I mentioned above, the flow out of the lake is a complex process which depends on several factors, including human behavior.\n\nI tentatively separated it into three typess of flow:\n- \"normal\" flow\n- \"drain\" flow, when the water is being drained from the lake through the dam's intake process\n- \"spillway\" flow, when the lake level is high enough that the spillway is active.\n\nMy manual guess about when each happens is shown below:\n(lake level is on one axis, in dashed lines, and flow rate is on anoter, in a solid line)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# label specific types of flow\n\n# dates when \"intake\" (from the lake into the river) seems to happen\nstart_intake = 180# pd.to_datetime(datetime.date(2008, 7, 1)).dayofyear\nend_intake = pd.to_datetime(datetime.date(2008, 11, 1)).dayofyear\n\nbilancino.loc[(bilancino['dayofyear'] >= start_intake) & (bilancino['dayofyear'] <= end_intake), 'flow_type']='intake'\n\n# spillway is active\n# bilancino.loc[bilancino['Flow_Rate'] > 7.8, 'flow_type']='spillway'\nbilancino.loc[bilancino['Lake_Level'] > 251.5, 'flow_type']='spillway'\n\nfig, ax1 = plt.subplots(figsize=(15, 5))\nax2 = ax1.twinx()\nbilancino['Lake_Level'].plot(ax=ax1, color='orange', linestyle='dashed')\nbilancino.where(bilancino['flow_type']=='intake')['Lake_Level'].plot(ax=ax1, color='blue', linestyle='dashed')\nbilancino.where(bilancino['flow_type']=='spillway')['Lake_Level'].plot(ax=ax1, color='red', linestyle='dashed')\nbilancino['Flow_Rate'].plot(ax=ax2, color='orange', label='normal flow')\nbilancino.where(bilancino['flow_type']=='intake')['Flow_Rate'].plot(ax=ax2, color='blue', label='draining')\nbilancino.where(bilancino['flow_type']=='spillway')['Flow_Rate'].plot(ax=ax2, color='red', label='spillway')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However, I found it very hard to fit parameters to expeted flow rates in each case. There seems to be a lot of non-linearity in the relationships; additionally, the relationship potentially changes in the last two years - the scatterplot below shows the relationship between flow rate and lake level; we can clearly see two greenish-yellow lines that do not conform to the pattern. these represent spillway flow after 2018 - it appears that the spillway started activating eariler. Again, human behavior makes things less predictable.\n\nFor this reason, I chose to cut off the training dataset at 2016, and focus on prediciting 2017. We can also see what happens to the subsequent years, and whether this shift changes things."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(bilancino['Lake_Level'],bilancino['Flow_Rate'], marker='x',c=bilancino['Date'].dt.year)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because this is a complex and piecewise relationship, decision trees, and specifically boosted forests (aka LGBM) are a good fit.\n\nI generated some derived features for the LGBM which capture my beliefs about the important pieces of this puzzle:\n- my best guess about when draining typically starts, ends, and how it ramps up (just from eyeballing the data)\n- stats about the last 60 days of lake level - because I suspect there is some inertia in the system, possibly due to the flap gate."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"lgbm_input = bilancino[['Lake_Level']].copy() #, 'dayofyear'\n\n # from lake level analysis above\n\nlgbm_input['effective_level'] = bilancino['Lake_Level']+bilancino['flow_mean_yesterday']*flow_lake_conversion\nlgbm_input.drop('Lake_Level',axis=1, inplace=True)\nlgbm_input['rolling_min'] = lgbm_input['effective_level'].rolling(60).min()\nlgbm_input['rolling_max'] = lgbm_input['effective_level'].rolling(60).max()\nlgbm_input['rolling_mean'] = lgbm_input['effective_level'].rolling(60).mean()\n\nintake_start = 160 # Nth day in year\nrampup_end = 220\nintake_end = 360 # end_intake  # stop \"intake\" drain\nintake_rampup = bilancino['dayofyear']\nintake_rampup = (intake_rampup-intake_start)/(rampup_end-intake_start)\nintake_rampup[(bilancino['dayofyear']<intake_start) | (bilancino['dayofyear']>intake_end)] = 0\nintake_rampup[(bilancino['dayofyear']>rampup_end) & (bilancino['dayofyear']<=intake_end)] = 1\n\nlgbm_input['intake_rampup'] = intake_rampup\n\nlgbm_test_cutoff = datetime.date(2017, 1, 1)\nlgbm_input = lgbm_input.loc[:lgbm_test_cutoff].copy()\nlgbm_flow_rate = bilancino.loc[:lgbm_test_cutoff, 'Flow_Rate']\n\n\nX_train = lgbm_input.loc[:train_cutoff]\ny_train = lgbm_flow_rate[:train_cutoff]\nX_test = lgbm_input.loc[train_cutoff:]\ny_test = lgbm_flow_rate[train_cutoff:]\n\nreg = LGBMRegressor().fit(X_train, y_train)\n\nfig, ax1 = plt.subplots(figsize=(15, 5))\n\nbilancino['Flow_Rate'].plot(ax=ax1)\nax1.plot(X_train.index,reg.predict(X_train), label='LGBM prediction(training data)')\nax1.plot(X_test.index,reg.predict(X_test), label='LGBM prediction(test data)')\nplt.xlim(None, lgbm_test_cutoff)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pred = pd.Series(reg.predict(X_train), index=X_train.index)\ntest_pred = pd.Series(reg.predict(X_test), index=X_test.index)\n\nprint('train error variance:',(train_pred-bilancino['Flow_Rate']).var(),'\\ntest error variance:', (test_pred-bilancino['Flow_Rate']).var())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Forecasting"},{"metadata":{},"cell_type":"markdown","source":"Forecasting is tricky because of the feedback loop between flow and lake level.\n\nFor this reason, I am using a custom procedure to integrate the cumulative-delta predictions and the LGBM's flow predictions:\n1. predict **cumulative delta-level** for all dates\n2. For each date in the dataset (in order), repeat the following:\n    - take the previous date's lake level (real for day 1, predicted for the rest of the time period)\n    - add the delta-level generated in step 1\n    - use this \"effective lake level\" as input to the LGBM (it was actually trained on these values)\n    - Take the flow predicted by the LGBM, and calculate the reduction in lake level (by multiplying by the conversion factor known from the rain-effect model); subtract the result from the \"effective lake level\" to get the actual lake level"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"exp_means, exp_vars = get_expected_inputs(\n    b_train[['Rainfall_Mangona', 'Rainfall_Cavallina', 'Temperature_Le_Croci']], b_train['dayofyear'])\nexp_inputs = gen_inputs_with_expectations(\n    b_train[['Rainfall_Mangona', 'Rainfall_Cavallina', 'Temperature_Le_Croci']],\n    b_test.index,\n    exp_means\n)\n\nexp_inputs['temp_30'] = exp_inputs['Temperature_Le_Croci'].rolling(30).mean()\nexp_inputs['temp_120'] = exp_inputs['Temperature_Le_Croci'].rolling(90).mean()\nexp_inputs['temp_180'] = exp_inputs['Temperature_Le_Croci'].rolling(180).mean()\n\nexp_inputs.loc[train_cutoff:,'flow_mean_yesterday'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_cum_delta, rm, rc=predict_cum_delta(exp_inputs)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax1 = plt.subplots(figsize=(15, 3))\n\ntrue_cum_delta[cum_delta.first_valid_index():].cumsum().plot(label='True cumulative level')\npred_cum_delta.cumsum()[:train_cutoff].plot(label='Predicted cumulative level(train data)')\npred_cum_delta.cumsum()[train_cutoff:].plot(label='Predicted cumulative level(expected rain/temp)')\nplt.legend()\nplt.xlim(datetime.date(2004,1,1))\nplt.title('Cumulative lake level predicted based on expected rain/temp')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"current_lake_level = b_train.iloc[-1]['Lake_Level'] # start with last lake level in the training dataset\nlevel_history = list(b_train.iloc[-60:]['Lake_Level']) # history so we can always get the min/max/mean of the last 60\n\nflow_predictions = {}\nlevel_predictions = {}\n\nfor date in b_test.index:\n    # calculate LGBM inputs\n    effective_lake_level = current_lake_level+pred_cum_delta[date]\n    min_60 = min(level_history[-60:])\n    max_60 = max(level_history[-60:])\n    mean_60 = np.mean(level_history[-60:])\n    intake_rampup_factor=0\n    if rampup_end < date.dayofyear < intake_end:\n        intake_rampup_factor = 1\n    elif intake_start < date.dayofyear:\n        intake_rampup_factor = (date.dayofyear-intake_start)/(rampup_end-intake_start)\n    \n    # predict\n    pred_flow = reg.predict([[\n        effective_lake_level,\n        min_60,\n        max_60,\n        mean_60,\n        intake_rampup_factor]])[0]\n    pred_level = effective_lake_level -(pred_flow*flow_lake_conversion)\n    \n    level_predictions[date] = pred_level\n    flow_predictions[date] = pred_flow\n    \n    # update state\n    level_history.append(pred_level)\n    current_lake_level = pred_level\n    \n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax1 = plt.subplots(figsize=(15, 3))\n\nb_test['Lake_Level'].plot()\npd.Series(level_predictions).plot(label='Predicted lake level')\nplt.legend()\nplt.title('Lake Level predictions')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Lake Level:')\nprint('MAE(all years):', mean_absolute_error(b_test['Lake_Level'],pd.Series(level_predictions)))\nprint('MAE(one year):', mean_absolute_error(b_test.loc[:datetime.date(2017,1,1),'Lake_Level'],pd.Series(level_predictions).loc[:datetime.date(2017,1,1)]))\nprint('RMSE(all years):', mean_squared_error(b_test['Lake_Level'],pd.Series(level_predictions), squared=False))\nprint('RMSE(one year):', mean_squared_error(b_test.loc[:datetime.date(2017,1,1),'Lake_Level'],pd.Series(level_predictions).loc[:datetime.date(2017,1,1)], squared=False))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax1 = plt.subplots(figsize=(15, 3))\n\nb_test['Flow_Rate'].plot()\npd.Series(flow_predictions).plot(legend='Predicted dam flow rate')\n\nplt.title('Flow Rate predictions')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Flow Rate:')\nprint('MAE(all years):', mean_absolute_error(b_test['Flow_Rate'],pd.Series(flow_predictions)))\nprint('MAE(one year):', mean_absolute_error(b_test.loc[:datetime.date(2017,1,1),'Flow_Rate'],pd.Series(flow_predictions).loc[:datetime.date(2017,1,1)]))\nprint('RMSE(all years):', mean_squared_error(b_test['Flow_Rate'],pd.Series(flow_predictions), squared=False))\nprint('RMSE(one year):', mean_squared_error(b_test.loc[:datetime.date(2017,1,1),'Flow_Rate'],pd.Series(flow_predictions).loc[:datetime.date(2017,1,1)], squared=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the lake level predictions capture the general pattern relatively well, but are a lot smoother than reality, especially right around the rainy winter season. This is because of my choice of input for the forecasting part of the  model: historical data averaged over a week and then averaged again. Because the lake + dam system is sensitive to spikes in the rain, and because rain data tends to be so spiky, taking the average doesn't capture the behavior as well as it does for other water bodies.\n\nIn fact, we can see that the predicted flow never goes into the \"spillway\" behavior - precisely because spillways are *designed* in large part for mitigating sudden spikes in rain.\n\nWe also see that the LGBM tends to produce much more gradual declines in flow than in reality, whereas in the real world, the flow almost seems to switch between several different levels.\n\nHaving more information about the dam and its operation might help make a more precise model. For example, just having the dates and ramp-up procedure for water intake might help isolate that effect, and analyze the other effects in more detail."},{"metadata":{},"cell_type":"markdown","source":"### Using real rain/temperature\nIn order to see the effect of using averaged expected inputs, I re-ran the prediction with the **actual** rain and temperature values. "},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"pred_cum_delta, rm, rc = predict_cum_delta(bilancino)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"current_lake_level = b_train.iloc[-1]['Lake_Level'] # start with last lake level in the training dataset\nlevel_history = list(b_train.iloc[-60:]['Lake_Level']) # history so we can always get the min/max/mean of the last 60\n\nflow_predictions = {}\nlevel_predictions = {}\n\nfor date in b_test.index:\n    # calculate LGBM inputs\n    effective_lake_level = current_lake_level+pred_cum_delta[date]\n    min_60 = min(level_history[-60:])\n    max_60 = max(level_history[-60:])\n    mean_60 = np.mean(level_history[-60:])\n    intake_rampup_factor=0\n    if rampup_end < date.dayofyear < intake_end:\n        intake_rampup_factor = 1\n    elif intake_start < date.dayofyear:\n        intake_rampup_factor = (date.dayofyear-intake_start)/(rampup_end-intake_start)\n    \n    # predict\n    pred_flow = reg.predict([[\n        effective_lake_level,\n        min_60,\n        max_60,\n        mean_60,\n        intake_rampup_factor]])[0]\n    pred_level = effective_lake_level -(pred_flow*flow_lake_conversion)\n    \n    level_predictions[date] = pred_level\n    flow_predictions[date] = pred_flow\n    \n    # update state\n    level_history.append(pred_level)\n    current_lake_level = pred_level\n    \n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax1 = plt.subplots(figsize=(15, 3))\n\nb_test['Lake_Level'].plot()\npd.Series(level_predictions).plot(label='Predicted lake level')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax1 = plt.subplots(figsize=(15, 5))\n\nb_test['Flow_Rate'].plot()\nbilancino.where(bilancino['flow_type']=='spillway').loc[train_cutoff:,'Flow_Rate'].plot(color='red', label='Flow Rate (spillway)')\npd.Series(flow_predictions).plot(alpha=0.8, label='Predicted flow rate')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The shape of the predictions is much closer to reality now, and the flow predictions even produce plausible spillway spikes in mostly the right places. \n\nThere are still a couple of interesting discrepancies, mostly with the flow spillway spikes. In the graph above, I highlighted in red the parts of flow which should correspond with the spillway being active (the same as the red parts on the flow + lake level graph at the beginning of the \"modeling flow\" section). These are sections where the lake level is above a certain level, above which the spillway seems to activate according to the trainign data. The vertical part of the hockey stick on the flow vs. level scatter plot.\n\nThe LGBM's decision on when to spike the flow matches my lake level-based prediction pretty well; in fact, there are  a couple of places  where my red highlighting and the LGBM agree, even though the LGBM did not \"know\" about my heuristic, but the data disagrees:\n\n- Most prominently, right around the start of 2018 and also the end of 2019, there are spikes in real data which neither the LGBM nor my Lake Level based heuristic anticipated. These correspond to the outliers I pointed out on the scatter plot: In these years, the spillway seems to be behaving differently than in the training data.\n- There are also a couple of small places where the opposite happens: the spillway ought to be active according to my heuristic, and the LGBM predicts a relatively high flow, but in reality the flow does not spike. The most noteable one is around early 2017, where the LGBM created a very narrow spike exactly where my heuristic highlighted a very tiny red spot.\n\nBut overall, having realistic rain predictions allows the model to match the real behavior much closer. For this particular water body, it might make sense to come up with a different way of capturing expected rain which doesn't rely on smoothing; for exampe, use the known means and variances of rain on a given day to actually generate random rain data. Though that would still not capture patters where rainy days are often clumped together, producing an extended spike of inflowing water. And of course, any such approximation wouldn't be able to predict *exactly when* the spikes would happen without making actual weather predictions."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}