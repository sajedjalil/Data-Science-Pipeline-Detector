{"cells":[{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport datetime\n\nfrom collections import defaultdict\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.metrics import explained_variance_score, mean_squared_error, mean_absolute_error\n\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Integer\n\nfrom graphviz import Digraph\nfrom IPython.display import SVG\n\nimport warnings\n\nfrom lightgbm import LGBMRegressor","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# fix the date in a dataframe (pandas does not read it in correctly by default), set it as index\ndef fix_date(df):\n    df['Date'] = pd.to_datetime(df['Date'], format=\"%d/%m/%Y\")\n    df.set_index('Date', drop=False, inplace=True)\n    df['dayofyear'] = df['Date'].dt.dayofyear","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# The main rainfall effect model\n\ndef rainfall_effect(\n    # data parameters\n    rain_series, # pd series where index is date and value is amount of rain which fell that day\n    start_date = None,\n    end_date = None,\n    # simulation parameters\n    fraction_retained = 0.9, # fraction of water retained each day (vs. fraction that is carried away elsewhere - pooling, transpiration, etc.)\n    first_day_flow = 0.004, # what fraction of the rain takes effect on the first day\n    funnel_start_width = 0.0, # when 0, funnel is cone-shaped; when large, funnel is closer to cyllinder-shaped.\n    time_gap = 0, # integer(days) - how long does it take even the first water to reach the area of interest\n):\n    # calculate default start and end date\n    if start_date is None:\n        start_date = rain_series.first_valid_index()\n    if end_date is None:\n        end_date = rain_series.last_valid_index()\n    \n    # calculate flow speed per \"funnel unit\"\n    first_day_area = funnel_start_width+1.0\n    flow_speed = first_day_flow/first_day_area\n    \n    # total rain \"taking effect\" on a given day\n    rain_effect = defaultdict(int)\n    \n    # process rain coming in each day\n    current_date = start_date\n    while current_date <= end_date:\n        rain = 0\n        if current_date in rain_series.index:\n            rain = rain_series[current_date]\n        \n        # start with explicit 0 for each input date      \n        if current_date not in rain_effect:\n            rain_effect[current_date] = 0\n            \n        # iterate through upcoming days and calculate effect of rain which reaches the body *on that day*\n        retained_remaining = rain # this variable keeps track of the effect of retention/drainage (but ignores actual outflow)\n        # effectively,it is used to infer how much water is left which originated on a given \"daily level\" of the funnel.\n        total_water_remaining = rain # actual amount of rain remaining \"un-claimed\"\n        current_area_factor = first_day_area\n        rain_effect_date = current_date + datetime.timedelta(int(time_gap))\n        while retained_remaining >= 0.01 and total_water_remaining >= 0.01 and rain_effect_date <= end_date:\n            water_out = current_area_factor*flow_speed*retained_remaining\n            water_out = min(water_out, total_water_remaining)\n        \n            # update totals\n            rain_effect[rain_effect_date] += water_out\n            total_water_remaining -= water_out\n            \n            # update running state variables\n            retained_remaining *= fraction_retained\n            total_water_remaining *= fraction_retained\n            current_area_factor += 2\n            rain_effect_date += datetime.timedelta(1)\n            \n        current_date += datetime.timedelta(1)\n    \n    return rain_effect","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# error metric: global error variance\ndef global_error_var(correct, pred):\n    return (correct-pred).var(ddof=0)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Helper functions to be able to define more elaborate priors for gp_optimize than just \"uniform\" and \"loguniform\"\n \n# reverse a log-uniform prior so that bigger values are more likely \ndef make_inverse_loguniform_prior(name, lower=None, upper=None):\n    \n    if lower is None:\n        lower = 0 + np.finfo(float).eps\n    if upper is None:\n        upper = 1 - np.finfo(float).eps\n        \n    def convert(x):\n        return upper-x\n    \n    dimension = Real(0 + np.finfo(float).eps, upper-lower, name=name, prior='log-uniform')\n    \n    return convert, dimension\n\n# set up a dimension such that the resulting converted variable will have a logistic distribution \n# with given s, mu, and lower/upper bounds.\ndef make_logit_prior(name, s = 1, m = 0, lower=None, upper=None):\n\n    # (0+np.finfo(float).eps)\n    \n    # x should be between 0 and 1\n    def convert_using_logit(x):\n        return m+np.log(x/(1-x))*s\n    \n    lower_x = 0 + np.finfo(float).eps\n    if lower is not None:\n        lower += np.finfo(float).eps\n        lower_x = 1/(1+np.exp((m-lower)/s))\n    upper_x = 1 - np.finfo(float).eps\n    if upper is not None:\n        upper -= np.finfo(float).eps\n        upper_x = 1/(1+np.exp((m-upper)/s))\n    \n    dimension = Real(lower_x, upper_x, name=name)\n    \n    return convert_using_logit, dimension","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# wrap rain effect calculation into a format which can be plugged into gp_optimize, with custom priors.\ndef make_rain_func_and_dimensions(\n    rain_series, \n    retained_prior = None,\n    flow_prior = None,\n    width_prior = None,\n    lag_prior = None,\n    verbose=True\n):\n    # TODO: take  in priors?..\n    \n    # How to convert given input variable (generate defaults if not provided in parameters)\n    if retained_prior is None:\n        # retained_conv, retained_dim = make_inverse_loguniform_dim('fraction_retained')\n        retained_conv, retained_dim = make_logit_prior('fraction_retained', s=0.3, m=0.9, lower=0.0, upper=1.0)\n    else:\n        retained_conv, retained_dim = retained_prior\n        \n    if flow_prior is None:\n        # we set the lower bound to 0.0001 for practical reasons: lower values don't make much of a cumulative effect, \n        # but they do take a long time to compute because the effect of each day's rainfall is spread out over many more days.\n        flow_conv, flow_dim = make_logit_prior('first_day_flow', s=0.05, m=0.01, lower=0.0001, upper=1.0) #m=0.1, s=0.1?\n    else:\n        flow_conv, flow_dim = flow_prior\n        \n    if width_prior is None:\n        width_conv, width_dim = make_logit_prior('funnel_start_width', s=40.0, m=180.0, lower=0.0) # m=270?\n    else:\n        width_conv, width_dim = width_prior\n    \n    if lag_prior is None:\n        lag_conv = lambda x: x\n        lag_dim = Integer(0,10, name='time_gap')\n    else:\n        lag_conv, lag_dim = lag_prior\n    \n    conversions = [retained_conv, flow_conv, width_conv, lag_conv]\n    dimensions = [retained_dim, flow_dim, width_dim, lag_dim]\n    \n    def convert(x):\n        return [conversions[i](x_i) for (i, x_i) in enumerate(x)]\n    \n    def calc_rain_from_vector(x):\n        fraction_retained, first_day_flow, funnel_start_width, time_gap = convert(x)\n        \n        if verbose:\n            print('inputs:', fraction_retained, first_day_flow, funnel_start_width, time_gap)\n            \n        return pd.Series(rainfall_effect(\n            # data parameters\n            rain_series,\n            # simulation parameters\n            fraction_retained = fraction_retained,\n            first_day_flow = first_day_flow,\n            funnel_start_width = funnel_start_width,\n            time_gap=time_gap\n        ))\n\n    return calc_rain_from_vector, dimensions, convert","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# use Baysian optimization with linear regresssion to fit ML model of rain effects, plus any set of linear-effect parameters.\ndef fit_rain_effects(\n    # inputs\n    rains_df, \n    ground_truth, \n    make_rain_func=make_rain_func_and_dimensions,\n    additional_fields=None, # additional fields to throw into linear regression\n    # options\n    error_func = global_error_var,\n    spinup=30, # TODO: use?..\n    verbose=True,\n    nrandom=20,\n    ntotal=100,\n    x0=None, # optional input point(s) to try for gp_minimize; e.g. best overall results of previous runs\n):\n    calcs = []\n    rain_names = []\n    converts = []\n    all_dims = []\n    dims_per_rain = 0\n    for rain_name, rain_series in rains_df.iteritems():\n        calc, dims, convert = make_rain_func(rain_series, verbose=verbose)\n        dims_per_rain = len(dims)\n        calcs.append(calc)\n        converts.append(convert)\n        rain_names.append(rain_name)\n        all_dims += dims\n        \n    reg_fields =rain_names\n    if additional_fields is not None:\n        reg_fields += list(additional_fields.columns)\n    \n    optimal_error = None\n    optimal_linreg = None\n    optimal_n = None\n    \n    def calculate_error(x):\n        prediction_frame = ground_truth.to_frame(name='ground_truth')\n        \n        for i, rain_calc in enumerate(calcs):\n            if(verbose):\n                print(rain_names[i])\n            rain_result = rain_calc(x[dims_per_rain*i:dims_per_rain*(i+1)])\n            prediction_frame[rain_names[i]] = rain_result\n            prediction_frame.loc[(prediction_frame['ground_truth'].notnull()) & (prediction_frame[rain_names[i]].isnull()), rain_names[i]] = 0\n\n        if additional_fields is not None:\n            prediction_frame[list(additional_fields.columns)] = additional_fields\n                \n        without_nulls = prediction_frame.dropna().copy()\n        \n        reg = LinearRegression().fit(without_nulls[reg_fields], without_nulls['ground_truth'])\n        if verbose:\n            print('rescale parameters:', reg.coef_, reg.intercept_)\n\n        without_nulls['pred'] = reg.predict(without_nulls[reg_fields])\n        \n        error = error_func(without_nulls['ground_truth'], without_nulls['pred'])\n        if verbose:\n            print('error value:', error)\n            print()\n            \n        nonlocal optimal_error, optimal_linreg, optimal_n\n        if optimal_error is None or error < optimal_error:\n            optimal_error = error\n            optimal_linreg = reg\n            optimal_n = len(without_nulls)\n        \n        return error\n        \n    res = gp_minimize(\n        calculate_error,  # function to minimize\n        all_dims,             # dimension configuration\n        acq_func=\"gp_hedge\",    # acquisition function (PI = optimize probability of reducing error; 'gp_hedge' - guess/vary)\n        n_calls=ntotal,      # number of evaluations of f\n        n_random_starts=nrandom, # first n calls are random (avoid local minima) \n        x0=x0, # input points to definitely try\n    )  \n    \n    additional_names = []\n    if additional_fields is not None:\n        additional_names = list(additional_fields.columns)\n    \n    return generate_prediction_function(rain_names, additional_names, res, converts, optimal_linreg, optimal_n)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Calculate BIC from error variance (making the gaussian assumption)\ndef get_bic(errvar, n, k):\n    return n*np.log(errvar) + k*np.log(n)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# given the outputs of a rain effect model fit, return a function which will generate the predictions based on that model.\ndef generate_prediction_function(rain_names, additional_field_names, fit_result, conversions, optimal_linreg, training_n, verbose=True):\n    \n    # get converted parameters for rain effect calculations\n    dims_per_rain = len(fit_result.x)//len(conversions)\n    all_rain_params = []\n    for i, convert in enumerate(conversions):\n        params = convert(fit_result.x[dims_per_rain*i:dims_per_rain*(i+1)])\n        all_rain_params.append(params)\n    \n    if verbose:\n        for name, params in zip(rain_names, all_rain_params):\n            print(f'Parameters for {name}: {params}')\n        print('Scaling:')\n        for name, coef in zip(rain_names+additional_field_names, optimal_linreg.coef_):\n            print(f'  {name}: {coef}')\n        print(f'Translation parameter: {optimal_linreg.intercept_}')\n        print(f'raw gp_minimize parameters: {fit_result.x}')\n        print(f'error value: {fit_result.fun}')\n        print(f'BIC (assuming error metric is error variance): {get_bic(fit_result.fun, training_n, len(fit_result.x)+len(optimal_linreg.coef_)+1)}')\n    \n    # function to generate prediction from trained parameters\n    def predict_from_rain(rain_fields, additional_fields=None):\n        pred_df = pd.DataFrame(index=rain_fields.index)\n        \n        for rain_name, rain_params in zip(rain_names, all_rain_params): \n            rain_series = rain_fields[rain_name]\n            fraction_retained, first_day_flow, funnel_start_width, time_gap = rain_params\n            rain_pred = pd.Series(rainfall_effect(\n                rain_series,\n                fraction_retained = fraction_retained, \n                first_day_flow = first_day_flow,\n                funnel_start_width = funnel_start_width,\n                time_gap = time_gap,\n            ))\n            pred_df[rain_name] = rain_pred\n            \n        if additional_fields is not None:\n            pred_df[list(additional_fields.columns)] = additional_fields\n            \n        pred_df.dropna(inplace=True)\n        return pd.Series(optimal_linreg.predict(pred_df), index=pred_df.index)\n            \n    return predict_from_rain\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Overview\n\nMy approach to this problems aims to balance physics-based methods with numerical(statistical?) methods to arrive at a Machine Learning model which captures important physical characteristics of the system, but still takes a data-driven approach to [confirming/demonstrating/finding evidence for effect of the physical properties] actually forecasting and making predictions.\n\nIn my view, the model I describe here has several specific advantages:\n- Checks and balances: reasoning about the physics of a system can validate the choice of a particular ML model, and the fit of the ML model can validate the physical assumptions.\n- A physically-plausible model of the way rainfall affects a water body, with an emergent distribution that's supported by the data.\n    - this model can produce a more nuanced and realistic result than simply generating an exponential-decay feature from the rainfall and using it in a black-box ML model. Additionally, the parameters of this model are tuned as part of the model fit, instead of having to tune the decay factor as part of hyperparameter optimization (or just eyeballing/hand-picking it).\n    - it also allows for interpretation of the model parameters \n- No imputation of data needed, except in some cases it may be helpful to impute rain - otherwisse, the method is robust to missing data.\n- A way of quantifying uncertainty and simulating specific scenarios (e.g. a dry year)\n- Never uses target variables in prediction\n- In many cases, can make sensible predictions for up to a year or more\n- A rigorous and mathematical method of parameter/model selection using the Bayesian Information Criterion\n"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#  This submission aims to balance physics-based methods with numerical methods by establishing a physics-based relationship model between the fields and then using ML to find the best parameters for the model.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Model\n\nConceptually, my approach is built on one overall model of how the known features of a water body interact. This model is then  adapted for each type of desired output, as well as specific properties of each particular water body."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"flow = Digraph(graph_attr={'ranksep':'1'})#, 'rankdir':'LR'})\nflow.node('R', 'Rainfall', penwidth='3')\nflow.node('T', 'Temperature and/or seasonality', penwidth='2')\nflow.node('O', 'Latent factors and\\n factors specific to water body', shape='rect', penwidth='2')\n#flow.node('O', 'Latent factors \\n not captured in dataset', shape='rect', penwidth='2')\nflow.node('F', 'Flow rate\\n(flowing water body)', shape='octagon')\nflow.node('DD', 'Depth and/or \\nChange in depth\\n(standing water body)', shape='octagon')\n\nflow.edge('R', 'F')\nflow.edge('R', 'DD')\nflow.edge('O', 'F')\nflow.edge('O', 'DD')\nflow.edge('T', 'F')\nflow.edge('T', 'DD')\nflow.edge('F', 'DD',  constraint='false')\nflow.edge('DD', 'F',  constraint='false')\n\nflow","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The figure above shows the general framework of how different features interact in the model. It is fairly straightforward, though there are a couple of notable points:\n- The **Rainfall** tends to be the most important input feature, since it is the main way that water enters the system.\n- The model primarily seeks to predict the **change in depth** of a stationary water body, rather than attempting to predict the depth directly. The depth itself can then be derived from the change in depth by a simple cumulative sum operation. This means that the model is always reasoning about how water travels over time, which simplifies it significantly.\n- There tends to be a **circular dependency** between the flow rate and the depth (or change in depth). For example, a spring's flow rate depends partially on the amount of water currently present in its source aquifer; but the aquifer's water level also changes due to discharging some of the water through the spring. A lot of the time, it is beneficia to model these two directions separately.\n- Temperature and season-based effects are often conflated, and temperature is used as a proxy for seasonality, without necessarily committing to a physical model of that particular effect. This is because temperature-based and season-based physical effects can be hard to reason about without having access to lots of different esoteric data (e.g. humidity, vegetation types and quality, human factors, and any number of other properties of the location where the water body is based). Indeed, it is often easy to find potential justification for a temperature-based effect, then \"turn the graph upside down\" and find an equally plausible explanation for the opposite effect.\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Rainfall Effect\nBecause rainfall tends to be the most important feature, one of the main building blocks of the overall model is a model of **the effect of rainfall on a particular water body**, specifically, how the effect of rain which fell on a particular day is distributed over time. In this model, the effect of a single day's rain on some subsequent day $t$ is distributed as \n$\\frac{t}{e^t}$:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"x = np.arange(0,8,0.1)\ny = x/np.exp(x)\nplt.plot(x,y)\nplt.tick_params(axis='x',labelbottom=False)\nplt.tick_params(axis='y',labelleft=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This means that the effect has a linear ramp-up and an exponential decay; this kind of ramp-up and decay can frequently be observed in the data when there's a large amount of rain in one day, followed by a dry period.\n\nIn practice, this is modeled as the rain coming down some watershed area into the affected water body, and the $\\frac{t}{e^t}$ distribution is an **emergent property** of the model (not a hard-coded distribution).  This allows for the model to be based on four physically interpretable parameters, namely:\n- fraction of remaining water retained each day (vs. water lost to transpiration, pooling, diversion, etc.)\n- the fraction of total rainfall which affects the water body *on the very first day*\n- the shape of the watershed \"funnel\" area, expressed as the \"width\" of the base of the watershed (compared to the rest of the watershed. Small \"width\" means that the watershed is more cone-shaped (rain from a wide area collects down to a single point), while large \"width\"means that the watershed is more cyllinder-shaped (rain tends to go down in a single direction, ending in a wide range of specific locations; e.g. all along some length of a river)\n- a \"time lag\" factor which makes it so that water takes some number of days to get from the base of the watershed to the actual affected water body (e.g. to seep down into an aquifer after falling on the ground)\n\nThe fraction of water lost each day creates the exponential decay effect, and the funnel/watershed model creates the linear ramp-up. The funnel shape affects how steep the exponential decay is: the effect is more drawn out with a wider base. The time lag, of course, delays the entire effect.\n\nMachine learning techniques are used to find the best-fitting values for each of these four parameters, for any given water body and dependent variable."},{"metadata":{},"cell_type":"markdown","source":"## Workflow\nThe overall workflow of the model is the same for each water body:\n1. Establish dependencies between features and fit model\n    - In some cases, it is necessary to establish which dependencies matter based on the model fit. In those cases, the [Baysian Information Criterion](https://en.wikipedia.org/wiki/Bayesian_information_criterion) (BIC) of several models are used to select the most appropriate model.\n2. Forecast dependent variable based on trends of the input variables\n3. Optionally, estimate uncertainty of the forecast: what is the *range* of possibilities, and how widely can they vary?\n\nI will use the first water body (the Petrignano aquifer) to illustrate this basic process in detail, and then explain variations and elaborations as they come up in other water bodies."},{"metadata":{},"cell_type":"markdown","source":"# Aquifers (and detailed model description)"},{"metadata":{},"cell_type":"markdown","source":"## Petrignano"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Load data\npetrignano = pd.read_csv('../input/acea-water-prediction/Aquifer_Petrignano.csv')\nfix_date(petrignano)\n\ndepth_fields = ['Depth_to_Groundwater_P24',\n       'Depth_to_Groundwater_P25']\ntemp_fields = ['Temperature_Bastia_Umbra',\n       'Temperature_Petrignano']\n\n# Drop data that's set to exactly 0 (in cases where this doesn't make sense)\npetrignano.loc[petrignano['Volume_C10_Petrignano']==0, 'Volume_C10_Petrignano'] = np.nan\npetrignano.loc[petrignano['Hydrometry_Fiume_Chiascio_Petrignano']==0, 'Hydrometry_Fiume_Chiascio_Petrignano'] = np.nan\nfor t in temp_fields:\n    petrignano.loc[petrignano[t]==0, t] = np.nan\n\n# Additional useful features\npetrignano['delta_depth'] = petrignano['Depth_to_Groundwater_P24'].diff()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# plot summary\nfig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5, figsize=(15, 10), sharex = True)\nfig.suptitle('Petrignano Data Overview', fontsize=16)\n\nax1.set_title('Depths')\nfor d in depth_fields:\n    petrignano[d].plot(ax=ax1)\n\nax2.set_title('Rainfall - rolling average over 120 days')\npetrignano['Rainfall_Bastia_Umbra'].rolling(120).mean().plot(ax=ax2)\n\nax3.set_title('Volume (water withdrawn from well) - rolling average over 30 days')\npetrignano['Volume_C10_Petrignano'].rolling(30, center=True).mean().plot(ax=ax3)\n\nax4.set_title('Temperatures')\nfor t in temp_fields:\n    petrignano[t].plot(ax=ax4)\n\nax5.set_title('Hydrometry (river flow)')\npetrignano['Hydrometry_Fiume_Chiascio_Petrignano'].plot(ax=ax5)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Dependencies and Model\n\nThe graph below shows the modeled dependencies between features. Dotted lines represent dependencies that were not used in the model, even though they are probably present in real life. Each color represents a separately trained component model for predicting a particular feature from its inputs.\n\n\n*Side note - why Volume Withdrawn is unused:* Conceptually, the volume of water witdrawn from a well both affects the groundwater depth (it depletes groundwater) and depends on it (the operators may be unwilling and/or unable to withdraw a lot of water if there is not much left). However, in practice, the volume of water withdrawn from well C10 was not used in the final model because it consistently made the model perform worse. Looking at the graphs above, we can see that the volume withdrawn correlates directly with the total depth in both of the other wells: when there is more water in the aquifer, more water tends to be withdrawn, and vice versa. If the water withdrawn had a large effect on groundwater depth, we would expect the opposite correlation: when more water is withdrawn, there is *less* water remaining. Since we don't see this correlation (and since the model doesn't benefit from the volume data), we can conjecture that the water is being withdrawn sustainably from the well so as to not deplete the aquifer, and/or that well C10 is quite far away from the wells with depth data. Indeed, [this paper](https://www.researchgate.net/publication/26812692_The_Sustainable_Pumping_Rate_Concept_Lessons_from_a_Case_Study_in_Central_Italy) suggests that the aquifer was exploited heavily around 1998-2004, but after that presumably efforts were made to use it more sustainably."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"petr_graph = Digraph(graph_attr={'ranksep':'1'})\nvol_depth = Digraph(graph_attr={'rank':'same'})\n\npetr_graph.node('R', 'Rainfall')\npetr_graph.node('T', 'Temperature')\nvol_depth.node('D', 'Change in Depth', shape='octagon', color='green')\npetr_graph.node('H', 'River Hydrometry', color='blue')\nvol_depth.node('V', 'Volume Withdrawn', style='dotted')\n\npetr_graph.edge('R', 'H', color='blue')\npetr_graph.edge('R', 'D', color='green')\npetr_graph.edge('T', 'D', color='green')\npetr_graph.edge('T', 'H', color='blue')\npetr_graph.edge('H', 'D', color='green')\nvol_depth.edge('D', 'V', style='dotted')\nvol_depth.edge('V', 'D', style='dotted')\n\n\npetr_graph.subgraph(vol_depth)\npetr_graph","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Rainfall + Temperature -> River Hydrometry\n\nThe Hydrometry data for the Chiascio river exhibits a clear exponential-decay pattern, with the peaks mapping to days with heavy rainfall. So the Rainfall Effect model makes sense here. \n\nFor illustration purposes, the graph below shows a slice of the hydrometry and raifall data alongside one semi-plausible, hand-picked set of parameters for a rainfall effect prediction. \n\nI chose the parameters based on observations about the hydrometry data:\n- the rain starts taking effect immediately, so there is no time gap\n- the rain is falling into a river, so the watershed's \"funnel base\" should be fairly wide\n- the rainfall effect tends to ramp up very quickly an decay over about a month, which dictated the `fraction_retained` and `first_day_flow` parameters.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"rain_prediction = pd.Series(rainfall_effect(\n    petrignano['Rainfall_Bastia_Umbra'],\n    fraction_retained = 0.9, # fraction of water retained each day (vs. fraction that is carried away elsewhere - pooling, transpiration, etc.)\n    first_day_flow = 0.002, # what fraction of the total rain takes effect on the first day\n    funnel_start_width = 180.0, # when 0, funnel is cone-shaped; when large, funnel is closer to cyllinder-shaped.\n    time_gap = 0, # integer(days) - how long does it take even the first water to reach the area of interest\n))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"example_slice = petrignano.loc[datetime.date(2018,6,1):]\nfig, ax1 = plt.subplots( figsize=(15, 7))\nax2 = ax1.twinx()\nax2.tick_params(axis='y',labelright=False)\nax3 = ax1.twinx()\n\nh = ax1.plot(example_slice.index,example_slice['Hydrometry_Fiume_Chiascio_Petrignano'], color='green')\nr = ax2.bar(example_slice.index, example_slice['Rainfall_Bastia_Umbra'])\np = ax3.plot(rain_prediction.index, rain_prediction, color='red', linestyle='dashed')\n\nwith warnings.catch_warnings(): # \"mixed positional and keyword arguments\" warning, but matplotlib only allows this kind of usage\n    warnings.simplefilter(\"ignore\")\n    fig.legend([h,r,p], labels=['Hydrometry_Fiume_Chiascio_Petrignano', 'Rainfall_Bastia_Umbra', 'rainfall effect prediction (manual parameter guess)'])\nplt.xlim(datetime.date(2018,6,1), datetime.date(2019,7,1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from the graph above that the general shape of the prediction matches the general shape of the hydrometry data pretty well, though there are places where the rainfall-based prediction produces spikes that are too large or too small compared to the actual effect. This illustrates that single-point rainfall data can sometimes lack the precision necessary to make extremly accurate predictions about the effect of rain on an entire area. However, in practice, the model tends to be pretty robust to the resulting noise, and the fit can be quite good anyway. \n\n**Note** that the rainfall effect prediction and the hydrometry data are shown on separate axes, because they are being measured in different units. Converting from millimeters of rain to the output units is **not** part of the Rainfall Effect model. Instead, the conversion parameters will be found using linear regression *after* a candidate Rainfall Effect output is calculted. Fitting parameters of a complex model is expensive, but scaling with linear regression is cheap. In fact, we can see that even matplotlib's default strategy for scaling each axis is enough to be able to compare the data sets without explicitly finding conversion factors.\n"},{"metadata":{},"cell_type":"markdown","source":"Fitting the parameters of the Rainfall Effect model is achieved through [Bayesian Optimization](https://en.wikipedia.org/wiki/Bayesian_optimization). This is a technique which minimizes a potentially expensive-to-evaluate function with unknown properties (in our case, the error resulting from a given set of Rainfall Effect parameters). It does this by evaluating the function at strategic points, in order to gain the most information to refine the posterior belief about the function overall. \n\nOne benefit of this technique, and the Bayesian approach to probability in general, is that it's possible (and in fact, necessary) to specify some prior beliefs about the input parameters. These priors will be used as a starting point for the model. Good priors will make the model converge better and faster. A bad set of priors can slow down the fitting processs, but will not *prevent* the model from trying a particular value unless we explicitly assume that this value is impossible (e.g. negative rainfall doesn't make sense).\n\nFor this model, I wrote a wrapper around [skopt's gp_minimize](https://scikit-optimize.github.io/stable/modules/generated/skopt.gp_minimize.html) function in order to be able to specify bell-shaped priors: the mean of the distribution is then my best guess of what the parameter should be, and a bell \"width\" parameter controls how sure I am (narrower = more sure). These priors generally remain at one default setting, but their shape allows for more effective optimization. In particular, the default skopt behavior is to specify hard limits on the possible values of a parameter. But a bell-shaped function can theoretically extend to infinity (though there are in practice numeric limitations), while still keeping the region of *likely* priors manageable.\n\nAt each step of the optimization, given a set of parameters suggested by gp_minimize, the wrapper:\n- computes the rain effect with those parameterss\n- uses linear regression to scale these rain effects, *plus a set of optional linear-effect features*, to the target variable\n- computes the error metric and returns it back to gp_optmize\n\nThe default error metric used is **error variance**. Optimizing for error variance is useful in a number of ways:\n- Error variance is needed for [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion)-based model selection, so optimizing for it helps ensure the BIC accurately represents the model quality. \n- Error variance is useful for quantifying uncertainty on the training data set, which in turn can be used to estimate uncertainty on the forecast data.\n- This metric is very closely related to the Mean Squared Error, so optimizing for it will also optimize the RMSE, which is one of the requested metrics in the data challenge.  "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"last_date = petrignano.iloc[-1].name \nnum_days_in_test = 365\ntrain_cutoff = last_date - datetime.timedelta(num_days_in_test-1)\nprint(f'Splitting train and test data on: {train_cutoff}')\nprint(f'({num_days_in_test} days in test dataset)')\n\ntrain_data = petrignano.loc[:train_cutoff].copy()\ntest_data = petrignano.loc[train_cutoff:]\n\n# For fitting hydrometry, take only relevant columns and drop all rows with any nulls\n# This makes the BIC comparisons more apples-to-apples, because then N (number of input data points) is the same across models,\ntrain_hydrometry_data = train_data[['Rainfall_Bastia_Umbra','Temperature_Bastia_Umbra','Temperature_Petrignano', 'Hydrometry_Fiume_Chiascio_Petrignano']].dropna().copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Fitting hydrometry from rainfall alone:')\npred_func=fit_rain_effects(\n    rains_df = train_hydrometry_data[['Rainfall_Bastia_Umbra']], \n    ground_truth = train_hydrometry_data['Hydrometry_Fiume_Chiascio_Petrignano'], \n    # limit the number of iterations so that notebook runtime is reasonable\n    # (this number of iterations is not actually practical without the x0 hint below)\n    nrandom=3,\n    ntotal=10,\n    verbose=False, # switch this to True(default) to see the results for each iteration\n    # x0 gives hints about good input values directly to gp_minimize \n    # (these are from previous training rounds, usually generated with the same training data set)\n    # This is unlikely to cause train-test leaks even in cases where the underlying data sets were different, \n    # since the quality(error metric) of the hint is calculated using the current training data,\n    # So the hint is accepted or rejected based solely on how well the parameters fit the training data.\n    # If the suggested parameters do have a good fit with the training data, they would have been found eventually \n    # throgh the optimization process, given enough time.\n    # If they don't, then they will be rejected and better values will befound (also given enough time).\n    # Anyway, in order to properly refit the model to some other input (e.g. a different train/test split),\n    # you can get rid of the x0 parameter  and change nrandom and ntotal to something large, e.g. 20 and 300.\n    # This may take a while, I suggest turning on the verbosity.\n    x0=[\n        [0.5526420950379467, 0.47164771912757525, 0.9189958623656255, 0],\n        [0.5539522165888526, 0.4506610850117819, 0.9376069254850813, 0],\n    ],\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can interpret the generated model parameters above as follows:\n\nThe model predicts how rain which fell on some watershed area, which can be accurately captured by the mm of rain measurement taken from an instrument at Bastia Umbra, flows down that watershed and into the Chiascio river.\n\n- *0.9634054888302401*: about 96.3% of the remaining water is preserved each day. In other words, about 3.7% of the water that fell as rainfall gets dissipated or diverted each day as it travels toward the river.\n- *0.0043234544645566145*: The first day after a rainfall, about 0.4% of the total water reaches the river\n- *277.15125538997574*: The \"base\" of the watershed - the area from which the rain reaches the river in the first day - measures 277 of some abstract area unit; each subsequent \"level\" of the watershed - the area from which the rain takes `n` days to reach the river - has area `277+2*n` (*This abstract area unit can and does change with each run of the simulation; it only captures the relationships between the levels, and how the total watershed area is distributed among the levels*) \n- *0*: There is no time lag - the water from the rain starts reaching the river immediately.\n- *Scaling factor 1.974471914324171*: conversion factor: if the total volumen of water which falls on the entire watershed area to generate a 1 mm reading was instantly dumped into the river, it would increase the river level by about 1.97 m in that moment \n- *Translation parameter 2.0381578930345676*:  about 2 meters of the river level can be considered a \"pre-existing baseline\", in that they are not generated by the rainfall in this particular watershed area.\n\nThe graph below shows the generated model predictions (note that unlike my manual guess, the generated predictions *are* in fact plotted on the same axis as the ground truth data, since they have been rescaled as part of the model):"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"example_slice = petrignano.loc[datetime.date(2018,6,1):]\nrain_prediction = pred_func(train_hydrometry_data[['Rainfall_Bastia_Umbra']])\n\nfig, ax1 = plt.subplots( figsize=(15, 7))\nax2 = ax1.twinx()\nax2.tick_params(axis='y',labelright=False)\n#ax3 = ax1.twinx()\n\nh = ax1.plot(example_slice.index,example_slice['Hydrometry_Fiume_Chiascio_Petrignano'], color='green')\nr = ax2.bar(example_slice.index, example_slice['Rainfall_Bastia_Umbra'])\np = ax1.plot(rain_prediction.index, rain_prediction, color='red', linestyle='dashed')\n\nwith warnings.catch_warnings(): # \"mixed positional and keyword arguments\" warning, but matplotlib only allows this kind of usage\n    warnings.simplefilter(\"ignore\")\n    fig.legend([h,r,p], labels=['Hydrometry_Fiume_Chiascio_Petrignano', 'Rainfall_Bastia_Umbra', 'rainfall effect prediction (parameters fit with Bayesian optimization)'])\nplt.xlim(datetime.date(2018,6,1), datetime.date(2019,7,1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The generated parameters seem to produce a decent fit, thoug it's possible that the model underestimates how quickly the effect of the rain should decay; and the magnitude of the rain-related spikes is not alwaays right.\n\nOf course, the quality of the model fit doesn't immediately *guarantee* that the physically interpretable parameters are close to the physical reality. In fact, the same (or extremely similar) exponential-decay rainfall effect curves can often be generated with sets of signinficantly different parameters. So there can be several physical explanations to the same observed effect.\n\nIn particular, the **scaling factor**  - how millimeters of rain translate to meters of river level - can vary widely between candidate solutions with similar fit. That scaling factor implicitly defines the *area* of the watershed from which the rain in question enters the river - a millimeter of rain falling across one squared meter is a very different amount of water than a millimeter of rain falling across a squared kilometer area. But the watershed area is not only completely unknown, but also ambiguous (which rain can we count as \"close enough\" to the measurement point to be part of \"the same\" watershed area?..)\n\nOn the other hand, the **translation parameter** stays quite similar throughout the model fit process. (we can observe this if we turn on verbosity in the model so that the intermediate results are shown). So we can be pretty sure about the amount of water in the river that's not accounted for solely by the rainfall."},{"metadata":{},"cell_type":"markdown","source":"But temperature and/or seasonality may also have an effect on the river's water level. In fact, as we can see from the graph below, there seems to be some correlation between temperature and the prediction error of the rainfall-only model. (I am graphing just one of the available temperature variables, since they both look about the same.)\n\nIn this case, the correlation implies that when temperatures are lower, the prediction tends to be too low: the actual river level is higher than the level predicted from rain alone. As I mentioned in the overview, there could be any number of plausible but conflicting explanations for how temperature (and, by proxy, seasons) affect the various water bodies. For example, one explanation could be that when the ground is cold, it tends to absorb less water, and therefore less water is lost, and/or the water is less delayed as it comes down the watershed. Similarly, it could be that in warmer weather, there tend to be many more plants which absorb the water as it makes its way down, and so more water is lost to transpiration. Alternatively, perhaps the river simply has a different \"baseline flow\" in different seasons."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"error = pred_func(train_data[['Rainfall_Bastia_Umbra']]) - train_data['Hydrometry_Fiume_Chiascio_Petrignano'] \nfig, ax1 = plt.subplots( figsize=(15, 5))\nax2 = ax1.twinx()\n\ne = ax1.plot(error.index, error)\nt = ax2.plot(train_data.index,train_data['Temperature_Petrignano'], color='red')\n# t = ax2.plot(train_data.index,train_data['Temperature_Petrignano'], color='orange')\n\nwith warnings.catch_warnings(): # \"mixed positional and keyword arguments\" warning, but matplotlib only allows this kind of usage\n    warnings.simplefilter(\"ignore\")\n    fig.legend([e,t], labels=['Prediction error(rainfall only model)', 'Temperature_Bastia_Umbra'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since temperature-based variations are hard to explain conclusively, we can just use a simple model to try to take the effect into account without trying to model an explanation which can be arbitrarily far from reality. \n\nThe simplest thing to do is to just incorporate the temperature data as an additional parameter in the linear regresssion portion of the model fit. Additionally, since most plausible explanations about seasonality effects involve reasoning about whether it *has been* warm or cold *recently* (and thus whether the ground has gotten cold, or the plants have sprouted, etc.), I use a **rolling average** of the temperature over the past 30 days. This also helps  smooth out some of the noise, which can avoid overfitting.\n\n30 days was chosen arbitrarily, by eyeballing some graphs. In principle, given unlimited time (for both development and training the model), one could also incorporate this parameter into the bayesian optimization flow and find the best fit. But the improvement would likely be marginal."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Fitting hydrometry from rainfall and one temperature feature:')\npred_func_onerain=fit_rain_effects(\n    rains_df = train_hydrometry_data[['Rainfall_Bastia_Umbra']], \n    ground_truth = train_hydrometry_data['Hydrometry_Fiume_Chiascio_Petrignano'], \n    additional_fields = train_hydrometry_data[['Temperature_Bastia_Umbra']].rolling(30).mean(),\n    nrandom=3,\n    ntotal=10,\n    verbose=False,\n    x0=[\n        [0.5526420950379467, 0.47164771912757525, 0.9189958623656255, 0],\n        [0.5539522165888526, 0.4506610850117819, 0.9376069254850813, 0],\n    ],\n)\n# -10759.23167816104","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, the prediction is better when we incorporate temperature into the model! The error has gone down from about 0.074 to about 0.05. But whenever the fit of a model improves due to increasing the model's complexity, it's natural to ask: are we overfitting? \n\nThis is a question which can be answered using the Bayesian information criterion (BIC). The BIC quantifies the tradeoff between comlexity and fit. Specifically, it uses **the likelihood of seeing the data actually observed , if the model was correct** to quantify a model's fit, and the **number of parameters the model has** to quantify its complexity. (*Side note: since neural network-based and boosting-based ML methods use **tons** of parameters - hundreds or thousands - it can be harder to apply BIC in those cases. That is why cross validation tends to be a more popular method for model selection in those cases, though it has its own imprecisions and pitfalls. On the other hand, with BIC, we can reason about model selection and overfitting by **only** examining the training data.*)\n\nThere's a slight problem with trying to apply the BIC to just any model: Since the BIC reasons about the *likelihood* of seeing a particular outcome given the model, the underlying asssumption is that the model can tell us the likelihood of something. In other words, the model implies a *probability distribution* of observing particular outcomes. But our model just predicts the outcome it thinks is most likely, and doesn't commit to any probability distribution which would tell us how  likely it actually is. \n\nThe fix for this is to make a reasonable assumption about what the probability distribution would be, given how the model prediction's errors  are distributed. Namely, if we assume that the errors are iid (independent and identically distributed) and have a normal distribution, the BIC can be calculated as:\n$$\nn\\ln(v)+k\\ln(n)\n$$\n\nwhere $v$ is the error variance of our model - the very metric we were optimizing for!\n\nThis is, in fact, the BIC that is output after fitting the model. Smaller BIC (i.e. negative BIC with larger absolute value) is better, so **comparing the BICs suggests that the model *with* temperature is better**: the BIC is smaller, so the gains in fit are  justified by the additional complexity.\n\n(the graph below suggests that the normally-distributed error assumption is justified. The iid assumption is harder to justify with just graphs.)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"error = pred_func_onerain(train_data[['Rainfall_Bastia_Umbra']], train_data[['Temperature_Bastia_Umbra']].rolling(30).mean()) - train_data['Hydrometry_Fiume_Chiascio_Petrignano'] \nerror.hist(bins=30)\nplt.title('error distribution(looks normal-ish)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But wait, we actually have two temperature features available. Would it make sense to try to use both of them? Or would it be redundant, since the two temperatures match each other quite closely?\n\nWe can use the BIC to answer that, too:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Fitting hydrometry from rainfall and both temperature features:')\npred_func_twotemp=fit_rain_effects(\n    rains_df = train_hydrometry_data[['Rainfall_Bastia_Umbra']], \n    ground_truth = train_hydrometry_data['Hydrometry_Fiume_Chiascio_Petrignano'], \n    additional_fields = train_hydrometry_data[['Temperature_Bastia_Umbra', 'Temperature_Petrignano']].rolling(30).mean(),\n    nrandom=3,\n    ntotal=10,\n    verbose=False,\n    x0=[\n        [0.5526420950379467, 0.47164771912757525, 0.9189958623656255, 0],\n        [0.5539522165888526, 0.4506610850117819, 0.9376069254850813, 0],\n    ],\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The fit does improve very slightly, but the BIC suggests that the improvement is not justified by the increased complexity. So, we will use `Rainfall_Bastia_Umbra` and `Temperature_Bastia_Umbra` as inputs to predict `Hydrometry_Fiume_Chiascio_Petrignano`.\n\n(*Why `Temperature_Bastia_Umbra` and not `Temperature_Petrignano`? It turns out it doesn't matter much which one to use. The resulting error and BIC are actually identical, but I omitted the code verifying this for brevity and clarity.*)"},{"metadata":{},"cell_type":"markdown","source":"The predictions of the final river flow model are shown below. In principle, it may be possible to add further parameters, for example, to try to account for the occasional really large spikes by adding a second copy of the rain data (physically, this would represent a primary and secondary effect from the same rain falling on the same area: some rain rolls down the ground surface almost immediately, while other rain seeps into the ground and then rolls down more slowly). I tried this out, but it did not improve the BIC. Regardless, the fit of this model will work for our purposes."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"example_slice = petrignano.loc[datetime.date(2018,6,1):]\nrain_prediction = pred_func_onerain(train_data[['Rainfall_Bastia_Umbra']], train_data[['Temperature_Petrignano']].rolling(30).mean()) \n\nfig, ax1 = plt.subplots( figsize=(15, 7))\nax2 = ax1.twinx()\nax2.tick_params(axis='y',labelright=False)\n# ax3 = ax1.twinx()\n\nh = ax1.plot(example_slice.index,example_slice['Hydrometry_Fiume_Chiascio_Petrignano'], color='green')\nr = ax2.bar(example_slice.index, example_slice['Rainfall_Bastia_Umbra'])\np = ax1.plot(rain_prediction.index, rain_prediction, color='red', linestyle='dashed')\n\nwith warnings.catch_warnings(): # \"mixed positional and keyword arguments\" warning, but matplotlib only allows this kind of usage\n    warnings.simplefilter(\"ignore\")\n    fig.legend([h,r,p], labels=['Hydrometry_Fiume_Chiascio_Petrignano', 'Rainfall_Bastia_Umbra', 'rainfall effect prediction (parameters fit with Bayesian optimization)'])\nplt.xlim(datetime.date(2018,6,1), datetime.date(2019,7,1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### River Hydrometry + Rainfall + Temperature -> (change in) Depth to Groundwater\n\nSeparately from the hydrometry model, we will train models to predict the daily change in depth-to-groundwater based on the river flow, rainfall, and temperature.\n\nConceptually, we will try to model that:\n- depth *increases* in proportion with the river flow, because the river brings in water (this phenomenon is confirmed in [this paper](https://www.researchgate.net/publication/26812692_The_Sustainable_Pumping_Rate_Concept_Lessons_from_a_Case_Study_in_Central_Italy)) \n- depth also *increases* when rain falls (with the rain taking effect as in the Rainfall Effect model)\n- depth *decreases* at a constant rate, as water seeps out, leaves with the river on the other end of the aquifer, evaporates, etc.\n- depth may be affected by temperature and/or seasonality, e.g. because the characteristics of earth in the aquifer change with temperature.\n\nBelow is the fit of the standard rain-effect model with all the parameters included."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"train_depth_data = train_data[[\n    'Rainfall_Bastia_Umbra',\n    'Temperature_Bastia_Umbra','Temperature_Petrignano', \n    'Hydrometry_Fiume_Chiascio_Petrignano', \n    'Depth_to_Groundwater_P24', 'Depth_to_Groundwater_P25'\n]].copy()\n\nfor d in depth_fields:\n    train_depth_data['delta_'+d] = train_depth_data[d].diff() # delta depth: current depth minus previous day's depth\n    \nfor t in temp_fields:\n    train_depth_data[t+'_rolling'] = train_depth_data[t].rolling(30).mean()\n    \ntrain_depth_data = train_depth_data.dropna()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Fitting change-in-depth from rainfall, hydrometry, and temperature:')\npred_depth_func=fit_rain_effects(\n    rains_df = train_depth_data[['Rainfall_Bastia_Umbra']], \n    ground_truth = train_depth_data['delta_Depth_to_Groundwater_P24'], \n    additional_fields = train_depth_data[['Temperature_Bastia_Umbra_rolling', 'Temperature_Petrignano_rolling', 'Hydrometry_Fiume_Chiascio_Petrignano']],\n    nrandom=3,\n    ntotal=10,\n    verbose=False,\n    x0=[\n        [0.0474258731775668, 0.6367973783355014, 0.9079965422001715, 0],\n        [0.0474258731775668, 0.9991795384988705, 0.17941634117347088, 0],\n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interestingly, the model prefers to minimize the amount of rain water retained from day-to-day as the water is on its way down some watershed into the aquifer (the first input parameter for the Rainfall_Bastia_Umbra effect model). This essentially means that when rain falls, **all rainwater either enters the aquifer or dissipates elsewhere on the very first day**. In other words, all rainfall primarily takes effect on the very same day as it falls, proportional to the amount of rain that fell. But that's just a linear effect which we could model directly with linear regression, instead of using the complex but more general rainfall effect model:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f'Fitting delta-depth using linear regression(all inputs)')\n\nlinreg_X = train_depth_data[[\n    'Rainfall_Bastia_Umbra',\n    'Temperature_Bastia_Umbra_rolling', 'Temperature_Petrignano_rolling',\n    'Hydrometry_Fiume_Chiascio_Petrignano']]\n\nlinreg_delta_depth = LinearRegression().fit(linreg_X, train_depth_data['delta_Depth_to_Groundwater_P24'])\n\nlinreg_pred = pd.Series(linreg_delta_depth.predict(linreg_X), index=train_depth_data.index)\nlinreg_error_var = (linreg_pred-train_depth_data['delta_Depth_to_Groundwater_P24']).var()\nlinreg_bic = get_bic(linreg_error_var, len(train_depth_data), 4)\nprint(f'error variance: {linreg_error_var}')\nprint(f'BIC: {linreg_bic}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that with linear regresssion, the error variance is a tiny bit worse, but the BIC is a bit better. This is because the linear model is less complex: it only fits one additional parameter to incorporate the effect of rain, while the Rainfall Effect model needs 4 additional parameters.\n\nRemoving one of the temperature variables again makes the error metric slightly worse, but the BIC slightly better:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(f'Fitting delta-depth using linear regression (just one temperature)')\n\nlinreg_X = train_depth_data[[\n    'Rainfall_Bastia_Umbra',\n    'Temperature_Bastia_Umbra_rolling',\n    'Hydrometry_Fiume_Chiascio_Petrignano']]\n\nlinreg_delta_depth = LinearRegression().fit(linreg_X, train_depth_data['delta_Depth_to_Groundwater_P24'])\n\nlinreg_pred = pd.Series(linreg_delta_depth.predict(linreg_X), index=train_depth_data.index)\nlinreg_error_var = (linreg_pred-train_depth_data['delta_Depth_to_Groundwater_P24']).var()\nlinreg_bic = get_bic(linreg_error_var, len(train_depth_data), 3)\nprint(f'error variance: {linreg_error_var}')\nprint(f'BIC: {linreg_bic}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plain linear regression is also MUCH faster to fit, so it's a lot easier to work with. Actually, with such a fast-to-fit model, we can revisit the use of average temperature over 30 days by trying a series of feasible values for the temperature average:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"errvars = []\nfor temp_rolling in range(1,60):\n    train_depth_data = train_data[[\n        'Rainfall_Bastia_Umbra',\n        'Temperature_Bastia_Umbra','Temperature_Petrignano', \n        'Hydrometry_Fiume_Chiascio_Petrignano', \n        'Depth_to_Groundwater_P24', 'Depth_to_Groundwater_P25'\n    ]].copy()\n\n    for d in depth_fields:\n        train_depth_data['delta_'+d] = train_depth_data[d].diff() # delta depth: current depth minus previous day's depth\n\n    for t in temp_fields:\n        train_depth_data[t+'_rolling'] = train_depth_data[t].rolling(temp_rolling).mean()\n\n    train_depth_data = train_depth_data.dropna()\n\n    linreg_X = train_depth_data[[\n        'Rainfall_Bastia_Umbra',\n        'Temperature_Bastia_Umbra_rolling',\n        'Hydrometry_Fiume_Chiascio_Petrignano']]\n\n    linreg_delta_depth = LinearRegression().fit(linreg_X, train_depth_data['delta_Depth_to_Groundwater_P24'])\n\n    linreg_pred = pd.Series(linreg_delta_depth.predict(linreg_X), index=train_depth_data.index)\n    linreg_error_var = (linreg_pred-train_depth_data['delta_Depth_to_Groundwater_P24']).var()\n    errvars.append(linreg_error_var)\n    \nplt.plot(range(1,60),errvars)\nplt.xlabel('average temperature over n days')\nplt.ylabel('error variance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears that at least for the aquifer depth, using near-instant temperature is actually a better idea than the 30-day average. So, let's adjust our model a final time. Technically, this means that we have now added one more parameter to the model for BIC calculation purposes, since we got our new rolling average width from fitting the model to the data.\n\n(I will keep the 30 day rolling average for the river flow prediction, since the mechanism of the effect of temperature could well be completely different for that model)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('delta_Depth_to_Groundwater_P24')\nprint('fitting new version of linear regression with a smaller rolling temperature span')\ntrain_depth_data = train_data[[\n    'Rainfall_Bastia_Umbra',\n    'Temperature_Bastia_Umbra','Temperature_Petrignano', \n    'Hydrometry_Fiume_Chiascio_Petrignano', \n    'Depth_to_Groundwater_P24', 'Depth_to_Groundwater_P25'\n]].copy()\n\nfor d in depth_fields:\n    train_depth_data['delta_'+d] = train_depth_data[d].diff() # delta depth: current depth minus previous day's depth\n\nfor t in temp_fields:\n    train_depth_data[t+'_rolling'] = train_depth_data[t].rolling(2).mean()\n\ntrain_depth_data = train_depth_data.dropna()\n\nlinreg_X = train_depth_data[[\n    'Rainfall_Bastia_Umbra',\n    'Temperature_Bastia_Umbra_rolling',\n    'Hydrometry_Fiume_Chiascio_Petrignano']]\n\nlinreg_delta_depth_p24 = LinearRegression().fit(linreg_X, train_depth_data['delta_Depth_to_Groundwater_P24'])\n\nlinreg_pred = pd.Series(linreg_delta_depth_p24.predict(linreg_X), index=train_depth_data.index, name='delta depth prediction')\nlinreg_error_var = (linreg_pred-train_depth_data['delta_Depth_to_Groundwater_P24']).var()\nlinreg_bic = get_bic(linreg_error_var, len(train_depth_data), 4)\nprint(f'error variance: {linreg_error_var}')\nprint(f'BIC: {linreg_bic}')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.subplots(figsize = (15,5))\nlinreg_pred.rolling(30).mean().plot()\ntrain_depth_data['delta_Depth_to_Groundwater_P24'].rolling(30).mean().plot()\nplt.legend()\nplt.title('real vs. predicted delta_Depth_to_Groundwater_P24 (rolling mean, 30 days)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Linear regression for delta_Depth_to_Groundwater_P25')\n\nlinreg_X = train_depth_data[[\n    'Rainfall_Bastia_Umbra',\n    'Temperature_Bastia_Umbra_rolling',\n    'Hydrometry_Fiume_Chiascio_Petrignano']]\n\nlinreg_delta_depth_p25 = LinearRegression().fit(linreg_X, train_depth_data['delta_Depth_to_Groundwater_P25'])\n\nlinreg_pred = pd.Series(linreg_delta_depth_p25.predict(linreg_X), index=train_depth_data.index, name='delta depth prediction')\nlinreg_error_var = (linreg_pred-train_depth_data['delta_Depth_to_Groundwater_P24']).var()\nlinreg_bic = get_bic(linreg_error_var, len(train_depth_data), 4)\nprint(f'error variance: {linreg_error_var}')\nprint(f'BIC: {linreg_bic}')\n\nplt.subplots(figsize = (15,5))\nlinreg_pred.rolling(30).mean().plot()\ntrain_depth_data['delta_Depth_to_Groundwater_P25'].rolling(30).mean().plot()\nplt.legend()\nplt.title('real vs. predicted delta_Depth_to_Groundwater_P25 (rolling mean, 30 days)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the depth of the two wells is extremely similar, I have not gone through a separate model selection process for the second well, though I did of course refit the model to the actual data.\n\nThe graphs above show the predicted and actual delta-depth for the two wells. I graphed the rolling average over 30 days, because otherwise the data is too noisy to read.\n\nFor both wells, we can see that the real data has larger fluctuations, but the model still captures the trend pretty well. Some of these fluctuations can actually be captured by incorporating the data about volume pumped from well C10. However,that would mean that we have to forecast this volume data as part of the forecasting, and I did not want to attemt that kind of logic just for a small potential improvement in the fit. This could be an area for further research."},{"metadata":{},"cell_type":"markdown","source":"### 2. Forecasting\n\nNow we have models that explain how change-in-depth can be predicted from rainfall and temperature data (via the intermediate prediction of river flow). But we need to forecast the depth of aquifer **without knowing what the rainfall and temperature will be in the future**.\n\nTo do this, I use a very simple model of each input variable: for each day of the year, I aggregate the weekly averages of what the rainfall and temperature was like on that day. This gives a general expectation of what the rain and temperature might be like on that day. I then use these expected values as input to the model to generate a forecast for any number of days in the year."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train_data['week_Rainfall_Bastia_Umbra'] = train_data['Rainfall_Bastia_Umbra'].rolling(7, center=True).mean()\ntrain_data['week_Temperature_Bastia_Umbra'] = train_data['Temperature_Bastia_Umbra'].rolling(7, center=True).mean()\n\nexpectation_data = {\n    'week_Rainfall_Bastia_Umbra':[],\n    'week_Temperature_Bastia_Umbra':[],\n}\nvar_data = {\n    'week_Rainfall_Bastia_Umbra':[],\n    'week_Temperature_Bastia_Umbra':[],\n}\nfor field in expectation_data.keys():\n    for d in range(365):\n        nearby_days = np.arange(d-3, d+3)%365+1\n        near_data = train_data[train_data['dayofyear'].isin(nearby_days)]\n        expectation_data[field].append(near_data[field].mean())\n        var_data[field].append(near_data[field].var())\n        \n        \nfig, axes = plt.subplots(2, figsize=(10,10))\nfor field, ax in zip(expectation_data.keys(), axes):\n    ax.plot(range(365), expectation_data[field])\n    ax.set_title(f'Expected {field} on given day of year')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to make accurate predictions into the future, we combine known rain/temp data from the past with the expected values from the future into one series.\n\nIn particular, this allows us to incorporate the effect of recent rains into the predictions for the immediate future, instead of starting with the assumtion that it hasn't rained before the prediction period.\n\nOne interesting nuance is that even though the expected rainfall and temerature will be the same on a given day of year, regardless of the year itself, the model may well evolve in a way that predicts different values on those days, since the output of the model depends on the cumulative past, not just the current day's input."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"petrignano_with_exp = petrignano.join(pd.DataFrame(expectation_data['week_Rainfall_Bastia_Umbra'], index=range(1, 366), columns=['rain_for_pred']), on='dayofyear')\npetrignano_with_exp['rain_for_pred'] = petrignano_with_exp['rain_for_pred'].where(petrignano.index >= train_cutoff, petrignano['Rainfall_Bastia_Umbra'])\n\npetrignano_with_exp = petrignano_with_exp.join(pd.DataFrame(expectation_data['week_Temperature_Bastia_Umbra'], index=range(1, 366), columns=['temp_for_pred']), on='dayofyear')\npetrignano_with_exp['temp_for_pred'] = petrignano_with_exp['temp_for_pred'].where(petrignano.index >= train_cutoff, petrignano['Temperature_Bastia_Umbra'])\n\nfig, (ax1,ax2)  = plt.subplots(2, figsize=(15,6), sharex=True)\nax1.set_title('actual(training data set) and expected(test data set) rainfall')\npetrignano_with_exp.loc[:train_cutoff,'rain_for_pred'].plot(ax=ax1)\npetrignano_with_exp.loc[train_cutoff:,'rain_for_pred'].plot(ax=ax1)\nax2.set_title('actual(training data set) and expected(test data set) temperature')\npetrignano_with_exp.loc[:train_cutoff,'temp_for_pred'].plot(ax=ax2)\npetrignano_with_exp.loc[train_cutoff:,'temp_for_pred'].plot(ax=ax2)\nplt.xlim(datetime.date(2010,1,1), None)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"river_prediction_input = petrignano_with_exp[['rain_for_pred', 'temp_for_pred']].rename(columns={\n    'rain_for_pred':'Rainfall_Bastia_Umbra',\n    'temp_for_pred':'Temperature_Bastia_Umbra'\n})\nriver_prediction = pred_func_onerain(river_prediction_input[['Rainfall_Bastia_Umbra']], river_prediction_input[['Temperature_Bastia_Umbra']].rolling(30).mean())\n\npetrignano_with_exp['hydrometry_for_pred'] = river_prediction\n\nplt.subplots(figsize=(15,5))\npetrignano['Hydrometry_Fiume_Chiascio_Petrignano'].plot()\nriver_prediction[:train_cutoff].plot(label='predicted hydrometry (training input)')\nriver_prediction[train_cutoff:].plot(label='predicted hydrometry (test input - expected values)')\nplt.legend()\nplt.title('Intermediate step: Hydrometry forecasting')\nplt.xlim(datetime.date(2018,7,1), None)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the river flow prediction matches reality fairly reasonably, even though we used the expected values of rain and temperature as input parameters."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"depth_prediction_input = petrignano_with_exp[['rain_for_pred', 'temp_for_pred','hydrometry_for_pred']].rename(columns={\n    'rain_for_pred': 'Rainfall_Bastia_Umbra',\n    'temp_for_pred': 'Temperature_Bastia_Umbra',\n    'hydrometry_for_pred': 'Hydrometry_Fiume_Chiascio_Petrignano',\n})\n# reset the hydrometry input to use actual training data instead of predictions based on training data (TODO: maybe not?.. maybe variance)\n#depth_prediction_input['Hydrometry_Fiume_Chiascio_Petrignano'] = depth_prediction_input['Hydrometry_Fiume_Chiascio_Petrignano'].where(depth_prediction_input.index >= train_cutoff,petrignano['Hydrometry_Fiume_Chiascio_Petrignano'] )\n\n# predict both delta-depths\ndelta_depth_p24 = pd.Series(linreg_delta_depth_p24.predict(depth_prediction_input.dropna()), index=depth_prediction_input.dropna().index)\ndelta_depth_p25 = pd.Series(linreg_delta_depth_p25.predict(depth_prediction_input.dropna()), index=depth_prediction_input.dropna().index)\n\n# get depth predictions from last known depth in the training data and sequence of delta-depth predictions\nlast_train_depth24 = train_data.iloc[-1]['Depth_to_Groundwater_P24']\ndepth_pred_p24 = delta_depth_p24[train_cutoff:].cumsum()+last_train_depth24\nlast_train_depth25 = train_data.iloc[-1]['Depth_to_Groundwater_P25']\ndepth_pred_p25 = delta_depth_p25[train_cutoff:].cumsum()+last_train_depth25\n\nfig, (ax1, ax2) = plt.subplots(2,figsize=(15,5), sharex=True)\npetrignano.loc[datetime.date(2018,7,1):,'Depth_to_Groundwater_P24'].plot(ax=ax1)\ndepth_pred_p24.plot(ax=ax1, label='prediction')\nax1.set_title('Depth_to_Groundwater_P24')\nax1.legend()\npetrignano.loc[datetime.date(2018,7,1):,'Depth_to_Groundwater_P25'].plot(ax=ax2)\ndepth_pred_p25.plot(ax=ax2, label='prediction')\nax2.set_title('Depth_to_Groundwater_P25')\nax2.legend()\nplt.xlim(datetime.date(2018,7,1), None)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the predictions follow the general trend of the ground truth quite closely, and don't deviate even after a year. The model cannot predict fluctuations in depth that happen on a small timescale. Based on our model of how depth depends on the input fields, we know that these fluctuations are very likely the result of very recent inputs into the system, such as how much it rained last week, or how much water was withdrawn in the immediate past. Based on that, I would not expect to be able to predict such fluctuations accurately in the long term regardless of the underlying model.\n\nThe requested error metrics(on a daily timescale) are shown below."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#mean_absolute_error(petrignano.loc[train_cutoff:,'Depth_to_Groundwater_P24'],depth_pred_p24)\n#mean_squared_error, \nerror_calc_df = petrignano.loc[train_cutoff:,['Depth_to_Groundwater_P24', 'Depth_to_Groundwater_P25']].copy()\nerror_calc_df['pred_24'] = depth_pred_p24\nerror_calc_df['pred_25'] = depth_pred_p25\nerror_calc_df.dropna(inplace=True)\nmae_24 = mean_absolute_error(error_calc_df['Depth_to_Groundwater_P24'],error_calc_df['pred_24'])\nrmse_24 = mean_squared_error(error_calc_df['Depth_to_Groundwater_P24'],error_calc_df['pred_24'], squared=False)\nmae_25 = mean_absolute_error(error_calc_df['Depth_to_Groundwater_P25'],error_calc_df['pred_25'])\nrmse_25 = mean_squared_error(error_calc_df['Depth_to_Groundwater_P25'],error_calc_df['pred_25'], squared=False)\nprint('Error metrics for predictions (daily)\\n')\nprint('Depth_to_Groundwater_P24:')\nprint(f'  MAE:{mae_24}')\nprint(f'  RMSE:{rmse_24}')\nprint('Depth_to_Groundwater_P25:')\nprint(f'  MAE:{mae_25}')\nprint(f'  RMSE:{rmse_25}')\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"monthly_pred = error_calc_df.resample('M').mean()\nmae_24 = mean_absolute_error(monthly_pred['Depth_to_Groundwater_P24'],monthly_pred['pred_24'])\nrmse_24 = mean_squared_error(monthly_pred['Depth_to_Groundwater_P24'],monthly_pred['pred_24'], squared=False)\nmae_25 = mean_absolute_error(monthly_pred['Depth_to_Groundwater_P25'],monthly_pred['pred_25'])\nrmse_25 = mean_squared_error(monthly_pred['Depth_to_Groundwater_P25'],monthly_pred['pred_25'], squared=False)\nprint('Error metrics for predictions (averaged monthly)\\n')\nprint('Depth_to_Groundwater_P24:')\nprint(f'  MAE:{mae_24}')\nprint(f'  RMSE:{rmse_24}')\nprint('Depth_to_Groundwater_P25:')\nprint(f'  MAE:{mae_25}')\nprint(f'  RMSE:{rmse_25}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Quantifying uncertainty\n\n\nThe model forecasts a single prediction based on the historic (smoothed) averages of rain and temperature values. But it's very unlikely that this exact prediction will come true, or indeed that the temperature and rain values in the future will match the historic averages exactly. \n\nWe can predict a *distribution* of possible outcomes based on two observed distributions (for simplicity, I'm assuming all distributions will be iid normal):\n- The distribution of the smoothed rain and temperature values which we averaged to generate the expected rain and temperature predictions. This is the uncertainty created by unknown future rain and temperature parameters.\n- The distribution of errors in the final delta-depth prediction over *known training values*. This is the uncertainty created by an inexact model fit to exact inputs\n\n\nWe can use these values in a Monte Carlo approach to estimating uncertanty:\n1. Generate many different predictions based on randomized temperature and rain inputs - the temperature and rain will be drawn from random distributions based on the expected mean and variance.\n2. Compute the  mean and variance of the resulting predictions\n3. Factor in the variance of the model over the training data - since we are assuming normal distributions, we can simply add variances together."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def generate_randomized_prediction():\n    def generate_randomized_input(row):\n        dayofyear = int(row['dayofyear'])\n        dayofyear = min(dayofyear, 365) # pretend leap days don't exist\n        random_rain = np.random.normal(\n            expectation_data['week_Rainfall_Bastia_Umbra'][dayofyear-1], \n            np.sqrt(var_data['week_Temperature_Bastia_Umbra'][dayofyear-1])\n        )\n        random_rain = max(0, random_rain) # no negative rain!\n        random_temp = np.random.normal(\n            expectation_data['week_Temperature_Bastia_Umbra'][dayofyear-1], \n            var_data['week_Temperature_Bastia_Umbra'][dayofyear-1]\n        )\n        return [random_rain,random_temp]\n\n    randomized_prediction_input = depth_prediction_input.copy()\n    randomized_prediction_input['dayofyear'] = petrignano_with_exp['dayofyear']\n    randomized_vals = randomized_prediction_input.loc[train_cutoff:].apply(generate_randomized_input, axis=1, result_type='expand')\n    randomized_prediction_input.loc[train_cutoff:, 'Rainfall_Bastia_Umbra'] = randomized_vals[0]\n    randomized_prediction_input.loc[train_cutoff:, 'Temperature_Bastia_Umbra'] = randomized_vals[1]\n\n    river_prediction = pred_func_onerain(randomized_prediction_input[['Rainfall_Bastia_Umbra']], randomized_prediction_input[['Temperature_Bastia_Umbra']].rolling(30).mean())\n\n    randomized_prediction_input['Hydrometry_Fiume_Chiascio_Petrignano'] = river_prediction\n    randomized_prediction_input = randomized_prediction_input.drop('dayofyear', axis=1).dropna()\n\n    randomized_delta_depth_p25 = pd.Series(linreg_delta_depth_p25.predict(randomized_prediction_input), index=randomized_prediction_input.index)\n\n    randomized_depth_pred_p25 = randomized_delta_depth_p25[train_cutoff:].cumsum()+last_train_depth25\n    return randomized_depth_pred_p25","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"\nrandomized_depth_pred_p25 = generate_randomized_prediction()\nplt.subplots(figsize=(15,2.5))\npetrignano.loc[datetime.date(2018,7,1):,'Depth_to_Groundwater_P25'].plot()\nrandomized_depth_pred_p25.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The figure above shows a sample randomized prediction.\n\nThe figure below shows the mean and two-standard-deviation-wide span of the predicted distribution.\nIt was generated using 50 simulation rounds, which is probably not enough in practice to get consistent results; but a realistc simulation is a bit too time-consuming to run in a notebook that's trying to cover a lot of ground."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"training_variance = (delta_depth_p25[:train_cutoff]-train_depth_data['delta_Depth_to_Groundwater_P25']).var()\n\nrandom_predictions = [generate_randomized_prediction() for i in range(50)] # change 50 to run a different number of simulations\n\nplt.subplots(figsize=(15,2.5))\nrandom_mean = pd.DataFrame(random_predictions).mean()\nrandom_variance = pd.DataFrame(random_predictions).var()\nrandom_2std = np.sqrt(random_variance+training_variance)*2\npetrignano.loc[datetime.date(2018,7,1):,'Depth_to_Groundwater_P25'].plot()\nrandom_mean.plot()\nplt.fill_between(random_mean.index, random_mean-random_2std, random_mean+random_2std, alpha=0.5)\nplt.title('mean expected depth prediction with two-standard-deviation error area(well P25)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This general technique for predicting uncertainty can be adapted for more nuanced analyses, as necesssary.\n\nFor example, what would happen if we had a dry year, where the rain tends to be one standard deviation below the average?"},{"metadata":{},"cell_type":"markdown","source":"## Auser"},{"metadata":{},"cell_type":"markdown","source":"### Data Overview"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"auser = pd.read_csv('../input/acea-water-prediction/Aquifer_Auser.csv')\nfix_date(auser)\n\n# limit to specific period with consistently good data\nauser = auser.loc[datetime.date(2010,1,1): datetime.date(2020,1,1)]\n\nrain_fields = ['Rainfall_Gallicano', 'Rainfall_Pontetetto',\n       'Rainfall_Monte_Serra', 'Rainfall_Orentano', 'Rainfall_Borgo_a_Mozzano',\n       'Rainfall_Piaggione', 'Rainfall_Calavorno', 'Rainfall_Croce_Arcana',\n       'Rainfall_Tereglio_Coreglia_Antelminelli',\n       'Rainfall_Fabbriche_di_Vallico']\ndepth_fields = [ 'Depth_to_Groundwater_LT2',\n       'Depth_to_Groundwater_SAL', 'Depth_to_Groundwater_PAG',\n       'Depth_to_Groundwater_CoS', 'Depth_to_Groundwater_DIEC']\ntemp_fields = ['Temperature_Orentano', 'Temperature_Monte_Serra',\n       'Temperature_Ponte_a_Moriano', 'Temperature_Lucca_Orto_Botanico']\nvolume_fields = ['Volume_POL', 'Volume_CC1', 'Volume_CC2', 'Volume_CSA', 'Volume_CSAL']\nhydro_fields = ['Hydrometry_Monte_S_Quirico', 'Hydrometry_Piaggione']\n\n# get rid of bad data: exactly 0 where it doesn't make sense\nfor t in temp_fields:\n    auser.loc[auser[t]==0, t]=np.nan\nfor v in volume_fields:\n    auser.loc[auser[v]==0, v]=np.nan\n    \n\n# resample volume as monthly since the data are clearly at monthly granularity\nauser_resampled = pd.DataFrame()\nfor v in volume_fields:\n    auser_resampled[v]=auser[v].resample('M').mean()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(5, figsize=(15,10), sharex=True)\n\naxes[0].set_title('Rainfall (rolling 120 days)')\nfor r in rain_fields:\n    auser[r].rolling(120).sum().plot(ax=axes[0])\naxes[0].legend()\n\naxes[1].set_title('Depth')\nfor d in depth_fields:\n    auser[d].plot(ax=axes[1])\naxes[1].legend()\n\naxes[2].set_title('Temperature(rolling 30 days)')\nfor t in temp_fields:\n    auser[t].rolling(30).sum().plot(ax=axes[2])\naxes[2].legend()\n\n\naxes[3].set_title('Volume withdrawn')\nfor v in volume_fields:\n    auser_resampled[v].plot(ax=axes[3])\naxes[3].legend()\n\naxes[4].set_title('Hydrometry (river flow?)')\nfor h in hydro_fields:\n    auser[h].plot(ax=axes[4])\naxes[4].legend()\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cutoff = datetime.date(2018,1,1)\nauser_train = auser[:train_cutoff].copy()\nauser_test = auser[train_cutoff+datetime.timedelta(1):]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Dependencies and Model\n\nI used the same overall model as Petrignano (shown below)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"petr_graph = Digraph(graph_attr={'ranksep':'1'})\nvol_depth = Digraph(graph_attr={'rank':'same'})\n\npetr_graph.node('R', 'Rainfall')\npetr_graph.node('T', 'Temperature')\nvol_depth.node('D', 'Change in Depth', shape='octagon', color='green')\npetr_graph.node('H', 'River Hydrometry', color='blue')\nvol_depth.node('V', 'Volume Withdrawn', style='dotted')\n\npetr_graph.edge('R', 'H', color='blue')\npetr_graph.edge('R', 'D', color='green')\npetr_graph.edge('T', 'D', color='green')\npetr_graph.edge('T', 'H', color='blue')\npetr_graph.edge('H', 'D', color='green')\nvol_depth.edge('D', 'V', style='dotted')\nvol_depth.edge('V', 'D', style='dotted')\n\n\npetr_graph.subgraph(vol_depth)\npetr_graph","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Modeling hydrometry\n\nThis model is quite similar to the Petrignano one.\n\nFor each hydrometry field, I tried using each rain field as the basis for the hydrometry model. `Rainfall_Fabbriche_di_Vallico` seems to be the best fit for both hydrometry fields.\n\nFor temperature inputs, I added several versions of rolling averages to the model  at once, to capture different aspects of seassonality. I then tried(offline, not in this notebook) removing some of the rolling averages to see if the BIC improved, but it did not, so it seems that all of these levels are important. \n"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"auser_train['temp_30'] = auser_train['Temperature_Lucca_Orto_Botanico'].rolling(30).mean()\nauser_train['temp_120'] = auser_train['Temperature_Lucca_Orto_Botanico'].rolling(90).mean()\nauser_train['temp_180'] = auser_train['Temperature_Lucca_Orto_Botanico'].rolling(180).mean()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# find single best rain fit for predicting each hydrometry field\nriver_preds = {}\nfor river in hydro_fields:\n    min_error = None\n    min_rain = None\n    for rain in rain_fields:\n        pred_func=fit_rain_effects(\n            rains_df = auser_train[[rain]], \n            ground_truth = auser_train[river], \n            additional_fields = auser_train[['Temperature_Lucca_Orto_Botanico', 'temp_30', 'temp_120', 'temp_180']],\n            nrandom=3,\n            ntotal=20,\n            verbose=False, \n            x0=[\n                [0.47802002829947315, 0.5011833749157925, 0.2943005663489582, 0],\n                [0.4427317029133336, 0.5388455770005512, 0.9999999999999998, 0],\n                [0.4535401381253068, 0.4836752048445926, 0.9999999999999998, 0],\n                [0.48557892850101986, 0.5230183486520695, 0.4361481698207067, 0],\n                [0.4919269314582723, 0.47413855100416424, 0.48034420622498836, 0],\n                [0.5032878715390603, 0.4506610850117819, 0.9999999999999998, 0],\n                [0.4427317029133336, 0.5388455770005512, 0.9999999999999998, 0],\n                [0.48137685236410005, 0.4506610850117819, 0.9999999999999998, 0],\n                [0.519754609419978, 0.4506610850117819, 0.8823067663440267, 0],\n                [0.46288084149002906, 0.5332854256618504, 0.9999999999999998, 0],\n                [0.456639047570966, 0.4506610850117819, 0.836729302031876, 0],\n                [0.426807932210006, 0.4506610850117819, 0.9999999999999998, 0],\n                [0.37089547099277526, 0.4506610850117819, 0.8705303183695998, 0],\n            ]\n        )\n        pred = pred_func(auser_train[[rain]],  auser_train[['Temperature_Lucca_Orto_Botanico', 'temp_30', 'temp_120', 'temp_180']])\n        err = (pred-auser_train[river]).var(ddof=0)\n        if min_error is None or err < min_error:\n            min_error = err\n            min_rain = rain\n            river_preds[river] = pred_func\n        print()\n\n    print(f'For {river}, the single best rain field is {min_rain} (error variance {min_error})')\n    print()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, (ax1,ax2) = plt.subplots(2,figsize=(15,5))\nfig.suptitle('Hydrometry: actual vs. predicted', fontsize=16)\n\npred = river_preds['Hydrometry_Monte_S_Quirico'](auser_train[['Rainfall_Fabbriche_di_Vallico']], auser_train[['Temperature_Lucca_Orto_Botanico', 'temp_30', 'temp_120', 'temp_180']])\nauser_train['Hydrometry_Monte_S_Quirico'].plot(ax=ax1)\npred.plot(label='predicted', ax=ax1)\nax1.legend()\npred2 = river_preds['Hydrometry_Piaggione'](auser_train[['Rainfall_Fabbriche_di_Vallico']], auser_train[['Temperature_Lucca_Orto_Botanico', 'temp_30', 'temp_120', 'temp_180']])\nauser_train['Hydrometry_Piaggione'].plot(ax=ax2)\npred2.plot(label='predicted', ax=ax2)\nax2.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Modeling change in depth\n\nHere, again, I used the same approach as for the Petrignano aquifer and used the BIC to guide my selection of parameters. Adding and removing various rain and temperature inputs did not have a large effect on the BIC of the output, so I picked a relatively simple option with relatively high BIC across the board.\n\nInterestingly, even though this is a two-aquifer system where the artisinal aquifer's depth should ostensibly depend on the depth of the other aquifer, fitting all depths directly to the hydrometry data works really well; in fact, the fit is best for the well in the artisinal aquifer. This could mean that the rivers affect the artisinal aquifer directly, or that the effect of the parameters on the other aquifer gets \"passed through\" to the artisinal aquifer nearly instantly (or some combination of these explanations)."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"for d in depth_fields:\n    auser_train['delta_'+d]=auser_train[d].diff()\n    \ndeltadepth_train = auser_train[\n    ['dayofyear', 'Rainfall_Fabbriche_di_Vallico','Hydrometry_Monte_S_Quirico', 'Hydrometry_Piaggione', 'Temperature_Lucca_Orto_Botanico', 'temp_30', 'temp_120', 'temp_180',]+\n    ['delta_'+d for d in depth_fields]\n].dropna().copy()\n\nlinreg_x = deltadepth_train[['Rainfall_Fabbriche_di_Vallico','Hydrometry_Monte_S_Quirico', 'Hydrometry_Piaggione', 'Temperature_Lucca_Orto_Botanico', 'temp_30' ]]\n\nprint('inputs:', list(linreg_x.columns))\n\ndelta_depth_fits = {}\nfor d in depth_fields:\n    dd_fit = LinearRegression().fit(\n        linreg_x,\n        deltadepth_train['delta_'+d]\n    )\n    pred = dd_fit.predict(linreg_x)\n    pred = pd.Series(pred, index=deltadepth_train.index)\n\n    errvar = (pred-deltadepth_train['delta_'+d]).var(ddof=0)\n    bic = get_bic(errvar, len(deltadepth_train), linreg_x.shape[1]+1)\n    print(d, errvar, bic)\n    delta_depth_fits[d] = dd_fit\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Forecasting"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def get_expected_inputs(input_data, dayofyear):\n    week_rolling_mean = input_data.rolling(7, center=True).mean()\n    expectations = defaultdict(list)\n    variances = defaultdict(list)\n    for d in range(365):\n        nearby_days = np.arange(d-3, d+3)%365+1\n        near_data = week_rolling_mean[dayofyear.isin(nearby_days)]\n        for field in week_rolling_mean.columns:\n            expectations[field].append(near_data[field].mean())\n            variances[field].append(near_data[field].var())\n    return expectations, variances","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def gen_inputs_with_expectations(input_data, days_to_predict, expected_means):\n    expected_data = input_data.append(pd.DataFrame(index=days_to_predict))\n    expected_data['dayofyear'] = expected_data.index.dayofyear\n    for field in input_data.columns:\n        exp_df = pd.DataFrame(expected_means[field], index=range(1, 366), columns=[field+'_expected'])\n        expected_data = expected_data.join(exp_df, on='dayofyear')\n        expected_data[field] = expected_data[field].where(~expected_data[field].isnull(), expected_data[field+'_expected'])\n        # expected_data[field] = expected_data[field].where(~expected_data.index.isin(list(days_to_predict)), expected_data[field+'_expected'])\n    return expected_data\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"exp_means, exp_vars = get_expected_inputs(auser_train[['Rainfall_Fabbriche_di_Vallico', 'Temperature_Lucca_Orto_Botanico']], auser_train['dayofyear'])\nexp_inputs = gen_inputs_with_expectations(\n    auser_train[['Rainfall_Fabbriche_di_Vallico', 'Temperature_Lucca_Orto_Botanico']],\n    auser_test.index,\n    exp_means\n)\nexp_inputs['temp_30'] = exp_inputs['Temperature_Lucca_Orto_Botanico'].rolling(30).mean()\nexp_inputs['temp_120'] = exp_inputs['Temperature_Lucca_Orto_Botanico'].rolling(90).mean()\nexp_inputs['temp_180'] = exp_inputs['Temperature_Lucca_Orto_Botanico'].rolling(180).mean()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(len(hydro_fields), figsize=(15,7), sharex=True)\n\nfor h,ax in zip(hydro_fields, axes):\n    exp_inputs[h] = river_preds[h](exp_inputs[['Rainfall_Fabbriche_di_Vallico']], exp_inputs[['Temperature_Lucca_Orto_Botanico', 'temp_30', 'temp_120', 'temp_180']])\n    exp_inputs.loc[:train_cutoff,h].plot(ax=ax)\n    exp_inputs.loc[train_cutoff:,h].plot(ax=ax)\n    auser[h].plot(ax=ax)\n    ax.set_title(h)\n    ax.set_xlim(datetime.date(2018,2,1), None)\n    ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"linreg_x = exp_inputs[['Rainfall_Fabbriche_di_Vallico','Hydrometry_Monte_S_Quirico', 'Hydrometry_Piaggione', 'Temperature_Lucca_Orto_Botanico', 'temp_30' ]].dropna()\n\nfig, axes = plt.subplots(len(depth_fields), figsize=(15,10), sharex=True)\n\npredictions = {}\nfor d,ax in zip(depth_fields, axes):\n    dd_fit = delta_depth_fits[d]\n    pred_delta = dd_fit.predict(linreg_x)\n    pred_delta = pd.Series(pred_delta, index=linreg_x.index)\n    pred_delta = pred_delta[train_cutoff+datetime.timedelta(1):]\n    d_c = auser_train.iloc[-1][d]\n    pred = pred_delta.cumsum()+d_c\n    predictions[d] = pred\n    ax.plot(pred)\n    auser[d].plot(ax=ax)\n    ax.set_title(d)\n    ax.set_xlim(train_cutoff, None)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"\nfor d in depth_fields:\n    print(d)\n    pred_df=None\n    pred_df = auser[[d]].copy()\n    pred_df['pred'] = predictions[d]\n    pred_df.dropna(inplace=True)\n    print('  MAE(daily, all years):', mean_absolute_error(pred_df[d],pred_df['pred']))\n    print('  MAE(daily, one year):', mean_absolute_error(pred_df.loc[:datetime.date(2019,1,1),d],pred_df.loc[:datetime.date(2019,1,1),'pred']))\n    print('  RMSE(daily, all years):', mean_squared_error(pred_df[d],pred_df['pred'], squared=False))\n    print('  RMSE(daily, one year):', mean_squared_error(pred_df.loc[:datetime.date(2019,1,1),d],pred_df.loc[:datetime.date(2019,1,1),'pred'], squared=False))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the predictions start to deviate from reality after one year, but the general trend still tends to be in the right direction.\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Doganella and Luco\n\nOmitted due to time constraints. Should be similar in concept, but without rivers (so only one layer of prediction)."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"petr_graph = Digraph(graph_attr={'ranksep':'1'})\nvol_depth = Digraph(graph_attr={'rank':'same'})\n\npetr_graph.node('R', 'Rainfall')\npetr_graph.node('T', 'Temperature')\nvol_depth.node('D', 'Change in Depth', shape='octagon', color='green')\nvol_depth.node('V', 'Volume Withdrawn', style='dotted')\n\npetr_graph.edge('R', 'D', color='green')\npetr_graph.edge('T', 'D', color='green')\nvol_depth.edge('D', 'V', style='dotted')\nvol_depth.edge('V', 'D', style='dotted')\n\n\npetr_graph.subgraph(vol_depth)\npetr_graph","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Rivers\n## Arno"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"arno_graph = Digraph(graph_attr={'ranksep':'1'})\n\narno_graph.node('R', 'Rainfall')\narno_graph.node('T', 'Temperature')\narno_graph.node('D', 'River Hydrometry', shape='octagon', color='green')\n\narno_graph.edge('R', 'D', color='green')\narno_graph.edge('T', 'D', color='green')\n\n\narno_graph","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Overview"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"arno = pd.read_csv('../input/acea-water-prediction/River_Arno.csv')\nfix_date(arno)\n\n# fix bad data: zeroed-out Hydrometry\narno.loc[arno['Hydrometry_Nave_di_Rosano']==0, 'Hydrometry_Nave_di_Rosano']  = np.nan\n\n# rain near lake (same fields as in lake data; probably can use output flow instead)\nlake_rain = [\n    'Rainfall_Le_Croci',\n    'Rainfall_Cavallina',\n    'Rainfall_S_Agata',\n    'Rainfall_Mangona',\n    'Rainfall_S_Piero',\n    'Rainfall_Vernio',\n]\n\n# rain along Arno river itself\nriver_rain = [\n    'Rainfall_Stia',\n    'Rainfall_Consuma',\n    'Rainfall_Incisa', # closest\n    'Rainfall_Montevarchi',\n    'Rainfall_S_Savino',\n    'Rainfall_Laterina',\n    'Rainfall_Bibbiena',\n    'Rainfall_Camaldoli',\n]\n\n\narno['temp_30'] = arno['Temperature_Firenze'].rolling(30).mean()\narno['temp_120'] = arno['Temperature_Firenze'].rolling(90).mean()\narno['temp_180'] = arno['Temperature_Firenze'].rolling(180).mean()\n\nrain_fields = lake_rain+river_rain\n\narno = arno.loc[:arno['Rainfall_Incisa'].last_valid_index()].copy()\n\narno.columns","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(4, figsize=(15,10))\n\naxes[0].set_title('Rainfall (Lake Bilancino area)')\nfor r in lake_rain:\n    arno[r].rolling(120).sum().plot(ax=axes[0])\n    \naxes[1].set_title('Rainfall (along river)')\nfor r in river_rain:\n    arno[r].rolling(120).sum().plot(ax=axes[1])\n\naxes[2].set_title('Temperature')\narno['Temperature_Firenze'].plot(ax=axes[2])\n\naxes[3].set_title('Hydrometry')\narno['Hydrometry_Nave_di_Rosano'].plot(ax=axes[3])\n    \nfor ax in axes: \n    ax.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset has a lot of rain fields, but many of them actually are closer to the Bilancino lake and the Sieve river, and thus would only affect the Arno river through the outflow from Bilancino. I explored how the Bilanino flow output correlates to the Arno river (offline), and it seems the answer is \"not well\". This doesn't meaan that the lake doesn't affect arno; just that the magnitude of the effect can't be derived from knowing what happenss to the lake (for example, the effect could be mostly constant compared to the effect of rain near the river.)\n\nFor this reason, I'll only be using the \"river rainfall\" fields in my model."},{"metadata":{},"cell_type":"markdown","source":"### 1. Dependencies and model\n\nThis is a single-layer model where the rainfall from several areas is combined with temperature data to predict the river's hydrometry metric.\n\nThe additional interesting feature of a long river is that using several rain fields, with different parameters, may be appropriate here. Unfortunately, there are only about 3.5 years  of data which actually contain all the important rain fields.(out of over 20 years of data total). To estimate whether it makes more sense to limit the dataset to 3.5 years or to only use one rain field, I first looked at how the fit is affected by limiting the data."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Rainfall_Incisa, all available data:')\npred_func=fit_rain_effects(\n    rains_df = arno[['Rainfall_Incisa']], \n    ground_truth = arno['Hydrometry_Nave_di_Rosano'], \n    additional_fields = arno[['Temperature_Firenze','temp_30', 'temp_120', 'temp_180']],\n    nrandom=3,\n    ntotal=20,\n    verbose=False, \n    x0=[\n        [0.4302223831313535, 0.4506610850117819, 0.7521851546976578, 0],\n        [0.4313650849509147, 0.5012229150866508, 0.9788660353174939, 0],\n        [0.3496183131045193, 0.5784179384080085, 0.874943754380722, 0],\n    ]\n)\n\nprint()\nprint('Rainfall_Incisa, up to 2007-7-6:')\n\narno_slice = arno.loc[:datetime.date(2007,7,6)]\npred_func=fit_rain_effects(\n    rains_df = arno_slice[['Rainfall_Incisa']], \n    ground_truth = arno_slice['Hydrometry_Nave_di_Rosano'], \n    additional_fields = arno_slice[['Temperature_Firenze','temp_30', 'temp_120', 'temp_180']],\n    nrandom=3,\n    ntotal=20,\n    verbose=False, \n    x0=[\n        [0.4302223831313535, 0.4506610850117819, 0.7521851546976578, 0],\n        [0.4313650849509147, 0.5012229150866508, 0.9788660353174939, 0],\n        [0.3496183131045193, 0.5784179384080085, 0.874943754380722, 0],\n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The fact that the error is signinficantly smaller with less data does suggest that there's overfitting.\n\nAnd although our simplifed BIC formula with Gaussian assumptions does not completely apply, because it also asssumes equal input size for apples-to-apples-comparison, the BIC is also a *lot* worse for the smaller data set (which is to be expected).\n\nImputing the rain data would have been possible, but would lose the benefit of having several independent measurements of rain.\n\nGiven that, I decided to work with just one rain field - the one near-river field which has the most data available, Rainfall_Incisa.\n\nI did, however, fit **two copies** of this rain field to two rain effect models: one came out to represent a more immediate effect, while the other one had a slower and delayed effect. One feasible explanation is tha the quicker effect is from water running down the surface, while the slower effect is from water that seeped into the ground before eventually propagating into the river."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"train_cutoff = datetime.date(2014,1,1)\narno_train = arno[:train_cutoff].copy()\narno_test = arno[train_cutoff+datetime.timedelta(1):]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"arno_train['rain_copy'] = arno_train['Rainfall_Incisa']\narno_pred_func=fit_rain_effects(\n    rains_df = arno_train[['Rainfall_Incisa', 'rain_copy']], \n    ground_truth = arno_train['Hydrometry_Nave_di_Rosano'], \n    additional_fields = arno_train[['Temperature_Firenze','temp_30', 'temp_120', 'temp_180']],\n    nrandom=3,\n    ntotal=10,\n    verbose=False, \n    x0=[\n        [0.33084597196099713, 0.7646045780132397, 0.5477330014465549, 0, 0.534142242067082, 0.5492149158336986, 0.9893596672770165, 5]\n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Forecasting"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"exp_means, exp_vars = get_expected_inputs(\n    arno_train[['Rainfall_Incisa', 'Temperature_Firenze']], arno_train['dayofyear'])\nexp_inputs = gen_inputs_with_expectations(\n    arno_train[['Rainfall_Incisa', 'Temperature_Firenze']],\n    arno_test.index,\n    exp_means\n)\n\nexp_inputs['rain_copy'] = exp_inputs['Rainfall_Incisa']\nexp_inputs['temp_30'] = exp_inputs['Temperature_Firenze'].rolling(30).mean()\nexp_inputs['temp_120'] = exp_inputs['Temperature_Firenze'].rolling(90).mean()\nexp_inputs['temp_180'] = exp_inputs['Temperature_Firenze'].rolling(180).mean()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# exp_inputs=exp_inputs.loc[datetime.date(2014,1,2):exp_inputs['Rainfall_Incisa'].last_valid_index()]\npred_flow = arno_pred_func(exp_inputs[['Rainfall_Incisa', 'rain_copy']], exp_inputs[['Temperature_Firenze','temp_30', 'temp_120', 'temp_180']])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(15,5))\nplt.plot(pred_flow, label='predicted')\nplt.plot(arno['Hydrometry_Nave_di_Rosano'], label='Hydrometry_Nave_di_Rosano')\nplt.legend()\nplt.xlim(datetime.date(2014,1,1),exp_inputs['Rainfall_Incisa'].last_valid_index())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df = arno[['Hydrometry_Nave_di_Rosano']].copy()\npred_df['pred'] = pred_flow\npred_df.dropna(inplace=True)\nd='Hydrometry_Nave_di_Rosano'\nprint('  MAE(daily, two years):', mean_absolute_error(pred_df[d],pred_df['pred']))\nprint('  MAE(daily, one year):', mean_absolute_error(pred_df.loc[:datetime.date(2015,1,1),d],pred_df.loc[:datetime.date(2015,1,1),'pred']))\nprint('  RMSE(daily, two years):', mean_squared_error(pred_df[d],pred_df['pred'], squared=False))\nprint('  RMSE(daily, one year):', mean_squared_error(pred_df.loc[:datetime.date(2015,1,1),d],pred_df.loc[:datetime.date(2015,1,1),'pred'], squared=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interestingly, the error metrics ar only slightly worse across both years than for the first year, even though on the graph the first year's trend seems to match the actual flow much more closely."},{"metadata":{},"cell_type":"markdown","source":"# Lakes\n## Bilancino\n"},{"metadata":{},"cell_type":"markdown","source":"*Note*: The implementation below is buggy, and therefore does not produce great results. [This correction](https://www.kaggle.com/yanamal/lake-bilancino-prediction#Bilancino) demonstrates that the general approach is still viable, and works well when implemented correctly. Unfortunately, it took me a couple of days after the deadline to fix all the bugs.\n\n(and then I never heard back despite repeatedly asking whether it's appropriate to insert notes like this into subsequent versions of this notebook, so now I'm just guessing)"},{"metadata":{},"cell_type":"markdown","source":"### Data Overview"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"bilancino=pd.read_csv('../input/acea-water-prediction/Lake_Bilancino.csv')\nfix_date(bilancino)\n\nrain_fields = ['Rainfall_S_Piero', 'Rainfall_Mangona', 'Rainfall_S_Agata', 'Rainfall_Cavallina', 'Rainfall_Le_Croci']\n\n# The mean flow out of the lake (Flow_Rate) yesterday\nbilancino['flow_mean_yesterday'] = bilancino['Flow_Rate'].rolling(2).mean()\n\nbilancino['delta_level'] = bilancino['Lake_Level'].diff()\n\n\nbilancino['temp_30'] = bilancino['Temperature_Le_Croci'].rolling(30).mean()\nbilancino['temp_120'] = bilancino['Temperature_Le_Croci'].rolling(90).mean()\nbilancino['temp_180'] = bilancino['Temperature_Le_Croci'].rolling(180).mean()\n\n\nbilancino.columns","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(4, figsize=(15,10))\n\nfor r in rain_fields:\n    bilancino[r].rolling(120).sum().plot(ax=axes[0])\n    \nbilancino['Temperature_Le_Croci'].plot(ax=axes[1])\n\nbilancino['Lake_Level'].plot(ax=axes[2])\n\nbilancino['Flow_Rate'].plot(ax=axes[3])\n    \nfor ax in axes:\n    ax.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cutoff = datetime.date(2016,1,1)\nb_train = bilancino[:train_cutoff].copy()\nb_test = bilancino[train_cutoff+datetime.timedelta(1):]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Dependencies and model"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"lake_graph = Digraph(graph_attr={'ranksep':'1'})\n\nlake_graph.node('R', 'Rainfall')\nlake_graph.node('T', 'Temperature')\nlake_graph.node('F', 'Flow out of dam', shape='octagon', color='blue')\nlake_graph.node('L', 'Lake Level', shape='octagon', color='green')\n\nlake_graph.edge('R', 'L', color='green')\nlake_graph.edge('T', 'L', color='green')\nlake_graph.edge('F', 'L', color='green')\nlake_graph.edge('L', 'F', color='blue')\n\nlake_graph","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Bilancino lake is an interessting cases, becaues its behavior depends on a man-made and human-operated structure - the dam that created the lake.\n\nThe level of water in the lake depends on the flow out of the dam, but the flow out of the dam also depends on the lake level - specifically, the lake level determines the amount of pressure created, and therefore the strengt of the flow.\n\nAccording to the challenge description, water is let out of the dam quickly at certain times, and allowed to collect at other times.\n\nFurther, the data indicates that the flow spikes drastically when the lake level goes over a certain point - This seems to be the dam's spillwaay being activated. According to [this site](http://cmcgruppo.com/cmc/en/project/bilancino-dam/), the spillway has an automatic flap gate. This means that the gate opens wider when pressure increases, which makess the interaxtion even more complicated."},{"metadata":{},"cell_type":"markdown","source":"#### Modeling change in lake level\n\nIn my previous experimentation, I found that two of the rainfall fields contain most of the information: `Rainfall_Mangona` and `Rainfall_Cavallina`. This makes sense:`Rainfall_Cavallina` is the closest location to the lake iteslf, and `Rainfall_Mangona` is located over the Sieve river, before the rver flows into the lake. Therefore, it captures information about water which enters the lake via the river.\n\nThe new component here is how flow rate affects lake level: the amount of water that leaves the lake via the dam (or more precisely, *the amount that left yesterday* is proportional to the subsequent reduction in lake level. So unlike the temerature parameters, **The parameters of flow in the linear regression actually have a direct physical meaning**. specifically, the scaling coefficient tells us how to convert between flow rate and (change in) lake level."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"b_pred_func=fit_rain_effects(\n    rains_df = b_train[['Rainfall_Mangona', 'Rainfall_Cavallina']], \n    ground_truth = b_train['delta_level'], \n    additional_fields = b_train[['flow_mean_yesterday', 'Temperature_Le_Croci','temp_30', 'temp_120', 'temp_180']],\n    nrandom=3,\n    ntotal=10,\n    verbose=False, \n    x0=[\n        [0.15233691562216556, 0.8854084902188695, 0.9999999999999998, 1, 0.13666937295698817, 0.9979837155391087, 0.14202766104040032, 0],\n        [0.0474258731775668, 0.9999999974825013, 0.570952205261587, 1, 0.3850443032078931, 0.45805440829961064, 0.545042159616547, 0],\n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Modeling flow out of lake\n\nAs I mentioned above, the flow out of the lake is a complex process which depends on several factors, including human behavior.\n\nI tentatively separated it into three typess of flow:\n- \"normal\" flow\n- \"drain\" flow, when the water is being drained from the lake through the dam's intake process\n- \"spillway\" flow, when the lake level is high enough that the spillway is active.\n\nMy manual guess about when each happens is shown below:\n(lake level is on one axis, in dashed lines, and flow rate is on anoter, in a solid line)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# label specific types of flow\n\n# dates when \"intake\" (from the lake into the river) seems to happen\nstart_intake = 180# pd.to_datetime(datetime.date(2008, 7, 1)).dayofyear\nend_intake = pd.to_datetime(datetime.date(2008, 11, 1)).dayofyear\n\nbilancino.loc[(bilancino['dayofyear'] >= start_intake) & (bilancino['dayofyear'] <= end_intake), 'flow_type']='intake'\n\n# spillway is active\n# bilancino.loc[bilancino['Flow_Rate'] > 7.8, 'flow_type']='spillway'\nbilancino.loc[bilancino['Lake_Level'] > 251.5, 'flow_type']='spillway'\n\nfig, ax1 = plt.subplots(figsize=(15, 5))\nax2 = ax1.twinx()\nbilancino['Lake_Level'].plot(ax=ax1, color='orange', linestyle='dashed')\nbilancino.where(bilancino['flow_type']=='intake')['Lake_Level'].plot(ax=ax1, color='blue', linestyle='dashed')\nbilancino.where(bilancino['flow_type']=='spillway')['Lake_Level'].plot(ax=ax1, color='red', linestyle='dashed')\nbilancino['Flow_Rate'].plot(ax=ax2, color='orange', label='normal flow')\nbilancino.where(bilancino['flow_type']=='intake')['Flow_Rate'].plot(ax=ax2, color='blue', label='draining')\nbilancino.where(bilancino['flow_type']=='spillway')['Flow_Rate'].plot(ax=ax2, color='red', label='spillway')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However, I found it very hard to fit parameters to expeted flow rates in each case. There seems to be a lot of non-linearity in the relationships; additionally, the relationship potentially changes in the last two years - the scatterplot below shows the relationship between flow rate and lake level; we can clearly see two greenish-yellow lines that do not conform to the pattern. these represent spillway flow after 2018 - it appears that the spillway started activating eariler. Again, human behavior makes things less predictable.\n\nFor this reason, I chose to cut off the training dataset at 2016, and focus on prediciting 2017. We can also see what happens to the subsequent years, and whether this shift changes things."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(bilancino['Lake_Level'],bilancino['Flow_Rate'], marker='x',c=bilancino['Date'].dt.year)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because this is a complex and piecewise relationship, decision trees, and specifically boosted forests (aka LGBM) are a good fit.\n\nI generated some derived features for the LGBM which capture my beliefs about the important pieces of this puzzle:\n- my best guess as to how the flow should behave when the spillway is active (I used a similar analysis to finding the optimal temperature averaging window for Petrignano's delta-level)\n- my best guess about when draining typically starts, ends, and how it ramps up (just from eyeballing the data)\n- stats about the last 60 days of lake level - because I suspect there is some inertia in the system."},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_input = bilancino[['Lake_Level']].copy() #, 'dayofyear'\n\nflow_lake_conversion = 0.007661467849044978 # from lake level analysis above\n\nlgbm_input['effect'] = bilancino['Lake_Level']+(bilancino['flow_mean_yesterday']*flow_lake_conversion).cumsum()\nlgbm_input.drop('Lake_Level',axis=1, inplace=True)\nlgbm_input['rolling_min'] = lgbm_input['effect'].rolling(60).min()\nlgbm_input['rolling_max'] = lgbm_input['effect'].rolling(60).max()\nlgbm_input['rolling_mean'] = lgbm_input['effect'].rolling(60).mean()\n\ninlet_start = 160 # Nth day in year\nrampup_end = 220\ninlet_end = 360# end_intake  # stop \"inlet\" drain\ninlet_rampup = bilancino['dayofyear']\ninlet_rampup = (inlet_rampup-inlet_start)/(rampup_end-inlet_start)\ninlet_rampup[(bilancino['dayofyear']<inlet_start) | (bilancino['dayofyear']>inlet_end)] = 0\ninlet_rampup[(bilancino['dayofyear']>rampup_end) & (bilancino['dayofyear']<=inlet_end)] = 1\n\nlgbm_input['inlet_rampup'] = inlet_rampup\n\nlgbm_test_cutoff = datetime.date(2017, 1, 1)\nlgbm_input = lgbm_input.loc[:lgbm_test_cutoff].copy()\nlgbm_flow_rate = bilancino.loc[:lgbm_test_cutoff, 'Flow_Rate']\n\n\nX_train = lgbm_input.loc[:train_cutoff]\ny_train = lgbm_flow_rate[:train_cutoff]\nX_test = lgbm_input.loc[train_cutoff:]\ny_test = lgbm_flow_rate[train_cutoff:]\n\nreg = LGBMRegressor().fit(X_train, y_train)\n\nfig, ax1 = plt.subplots(figsize=(15, 5))\n\nbilancino['Flow_Rate'].plot(ax=ax1)\nax1.plot(X_train.index,reg.predict(X_train))\nax1.plot(X_test.index,reg.predict(X_test))\nplt.xlim(None, lgbm_test_cutoff)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pred = pd.Series(reg.predict(X_train), index=X_train.index)\ntest_pred = pd.Series(reg.predict(X_test), index=X_test.index)\n\nprint('train error variance:',(train_pred-bilancino['Flow_Rate']).var(),'\\ntest error variance:', (test_pred-bilancino['Flow_Rate']).var())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Forecasting"},{"metadata":{},"cell_type":"markdown","source":"Forecasting is tricky because of the feedbaack look between flow and lake level.\n\nbelow, I am forecasting slightly custom values to account for the oddities:\n\n1. The *cumulative lake level* which would exist if the dam did not have any drain - to do this, I am doing a custom prediction using all the parameters generated by the rain effect model, **but putting yesterday's flow on the other side of the equation**. I am comparing that to the actual Lake_Level **plus flow, also scaled  according to the parameter from the trained model**\n\n2. I am using this cumulative level as input for the LGBM to predict flow out - actually, that's how the LGBM was trained.\n\n3. I am using the predicted flow and cumulative level to try to get the actual level back.\n\nThe result deviates from reality quickly. The LGBM may be partially to blame - while all of thte other models were written explicitly to respect the physics of the situation, the LGBM is unaware of them. Having more information abot the dam may allow for a model that avoids such black-box systems.\n\n\n\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"exp_means, exp_vars = get_expected_inputs(\n    b_train[['Rainfall_Mangona', 'Rainfall_Cavallina', 'Temperature_Le_Croci']], b_train['dayofyear'])\nexp_inputs = gen_inputs_with_expectations(\n    b_train[['Rainfall_Mangona', 'Rainfall_Cavallina', 'Temperature_Le_Croci']],\n    b_test.index,\n    exp_means\n)\n\nexp_inputs['temp_30'] = exp_inputs['Temperature_Le_Croci'].rolling(30).mean()\nexp_inputs['temp_120'] = exp_inputs['Temperature_Le_Croci'].rolling(90).mean()\nexp_inputs['temp_180'] = exp_inputs['Temperature_Le_Croci'].rolling(180).mean()\n\nexp_inputs.loc[train_cutoff:,'flow_mean_yesterday'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nParameters for Rainfall_Mangona: [2.220446049250313e-16, 1.0000000011740589, 191.42948720854787, 1]\nParameters for Rainfall_Cavallina: [0.7595424520378065, 0.0015911180194193453, 187.22633570423756, 0]\nScaling:\n  Rainfall_Mangona: 0.005862430953008586\n  Rainfall_Cavallina: 1.4547492793031174\n  flow_mean_yesterday: -0.0077987173442005675\n  Temperature_Le_Croci: 0.0039183372980673425\n  temp_30: -0.0015752030857040245\n  temp_120: -0.008107915323914416\n  temp_180: 0.0048819421510388206\nTranslation parameter: -0.010936677349591556\nraw gp_minimize parameters: [0.0474258731775668, 0.9999999974825013, 0.570952205261587, 1, 0.3850443032078931, 0.45805440829961064, 0.545042159616547, 0]\nerror value: 0.004762807981376231\nBIC (assuming error metric is error variance): -22344.942068760232\n'''\n# manually apply parameters to put flow_mean_yesterday (and its factor) on the other side of the equation\n\nRainfall_Mangona_prediction = pd.Series(rainfall_effect(\n    exp_inputs['Rainfall_Mangona'],\n    fraction_retained = 2.220446049250313e-16, \n    first_day_flow = 1.0, \n    funnel_start_width = 191.42948720854787, \n    time_gap = 1,\n))\n\nRainfall_Cavallina_prediction = pd.Series(rainfall_effect(\n    exp_inputs['Rainfall_Cavallina'],\n    fraction_retained = 0.7595424520378065, \n    first_day_flow = 0.0015911180194193453, \n    funnel_start_width = 187.22633570423756, \n    time_gap = 0,\n))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_cum_delta = 0.005862430953008586*Rainfall_Mangona_prediction+\\\n1.4547492793031174*Rainfall_Cavallina_prediction+\\\n0.0039183372980673425*exp_inputs['Temperature_Le_Croci']+\\\n-0.0015752030857040245*exp_inputs['temp_30']+\\\n-0.008107915323914416*exp_inputs['temp_120']+\\\n0.0048819421510388206*exp_inputs['temp_180']+\\\n-0.010936677349591556","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"cumulative level"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_cum_level = pred_cum_delta[train_cutoff:].cumsum()+b_train.iloc[-1]['Lake_Level']\npred_cum_level.plot()\nb_train['Lake_Level'].plot()\nplt.xlim(datetime.date(2015, 1, 1), lgbm_test_cutoff)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"flow prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = X_test.copy()\nX_test['effect'] = pred_cum_level\nX_test['rolling_min'] = X_test['effect'].rolling(60).min()\nX_test['rolling_max'] = X_test['effect'].rolling(60).max()\nX_test['rolling_mean'] = X_test['effect'].rolling(60).mean()\n\nflow_pred = pd.Series(reg.predict(X_test), index=X_test.index,name='cumlev' )\n\nfig, ax1 = plt.subplots(figsize=(15, 5))\n\nbilancino['Flow_Rate'].plot(ax=ax1)\nax1.plot(X_train.index,reg.predict(X_train))\nax1.plot(X_test.index,reg.predict(X_test))\nplt.xlim(datetime.date(2015, 1, 1), lgbm_test_cutoff)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = pred_cum_level.to_frame( name='cumlev')\npred['flow'] = flow_pred\npred['lev'] = pred['cumlev']-(pred['flow']*0.0077987173442005675).cumsum()\n(pred['lev']).plot()\n\nbilancino['Lake_Level'].plot()\nplt.xlim(datetime.date(2015, 1, 1), lgbm_test_cutoff)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Streams\nomitted due to time constraints (but would work in a very ssimilar way to rivers, or in the case of Amiata, to a river flowing from a built-in aquifer - calculate the aquifer depth and use linear regression on depths to get flow (because flow is based on water pressure in the aquifer)"},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}