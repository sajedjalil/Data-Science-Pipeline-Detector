{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Notebook Contents\n\nHi, here I share with you my Data Exploration for the ACEA Smart Water Analytics challenge.\n\n\nContenuti: \n\n- analisi di tutti i file relativi alla challenge (con dimensione in bytes, numero di colonne e numero di righe)\n\n- estensione temporale di ogni dataset \n\n- analisi degli NA\n\n- mappa (e magari correlazione con meteo) \n\n\n\nInserire il link di ogni sezione! \n\n\n### Props to:\n\n- https://www.kaggle.com/maunish/jsmp-super-cool-eda-lgbm-baseline/comments\n\n- https://www.kaggle.com/iamleonie/eda-quenching-the-thirst-for-insights"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.options.display.max_columns = 30\nimport os\nimport re\nfrom colorama import Fore, Back, Style\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib\nfrom matplotlib import pyplot as plt\nplt.rcParams.update({'figure.max_open_warning': 0})\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')\n\nroot_path = '/kaggle/input/acea-water-prediction'\ndata_files = [i for i in os.listdir(root_path) if re.match(\".+\\.csv$\", i)]\ndata_files.sort()\ndata_names = [i.replace('.csv', '') for i in data_files]\ndata_files = list(map(lambda x: os.path.join(root_path, x), data_files))\n\nwaterbody_type = [re.match(\"water_spring|aquifer|river|lake\", i.lower())[0] for i in data_names]\n\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL\n\ncolor_dict = {'aquifer': r_, 'water_spring': g_, 'lake': c_, 'river': b_}\n\ndef get_pattern_count(pattern_list, cols): \n    counts = []\n    for j in pattern_list: \n        counts.append(len([i for i in cols if re.match(j, i)]))\n    return counts\n\ndef get_df_basic_information(df, waterbody_type, df_name): \n    \n    n_rows, n_columns = df.shape\n    \n    mb_size = round(df.memory_usage(deep=True).sum()/1000000., 3)\n    \n    print(\"\"\"{0}{1}\\n\n          N rows: {2}\\tN columns: {3}\\n\n          Memory Usage: {4} Mb\\n\\n\\n\"\"\".format(color_dict[waterbody_type], df_name,\n                                           n_rows, n_columns, mb_size))\n    \ndef chunks(l, n):\n    \"\"\" Yield n successive chunks from l.\n    \"\"\"\n    newn = int(len(l) / n)\n    for i in range(0, n-1):\n        yield l[i*newn:i*newn+newn]\n    yield l[n*newn-newn:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Challenge Description\n\n_This competition uses nine different datasets, completely independent and not linked to each other. Each dataset can represent a different kind of waterbody. As each waterbody is different from the other, the related features as well are different from each other. So, if for instance we consider a water spring we notice that its features are different from the lake’s one. This is correct and reflects the behavior and characteristics of each waterbody. The Acea Group deals with four different type of waterbodies: water spring (for which three datasets are provided), lake (for which a dataset is provided), river (for which a dataset is provided) and aquifers (for which four datasets are provided)._"},{"metadata":{},"cell_type":"markdown","source":"So the dataset we have at our disposal are: "},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(waterbody_type)):\n    print(\"{0}{1}\\n\".format(color_dict[waterbody_type[i]], data_names[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We can see there are 4 aquifers, one lake, one river and 3 water springs, as expected. \n\nThe challenge descriptions states: _As each waterbody is different from the other, the related features as well are different from each other_. \n\nLet's start by checking each dataframe shape. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dict = dict(zip(data_names, list(map(lambda x: pd.read_csv(x, sep = \";\") if 'Auser' in x else pd.read_csv(x), data_files))))\n\nfor name in data_names:\n    df_dict[name] = df_dict[name].loc[~df_dict[name]['Date'].isna()]\n    df_dict[name]['Date'] = pd.to_datetime(df_dict[name]['Date'], format = \"%d/%m/%Y\").dt.date\n    df_dict[name].sort_values('Date', ignore_index = True, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for j in range(len(data_names)): \n    \n    source_name = data_names[j]\n    get_df_basic_information(df_dict[source_name], waterbody_type[j],source_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each source has its own number of rows and columns..."},{"metadata":{},"cell_type":"markdown","source":"Possibili idee: \n\n- ho 9 dataset, vorrei vedere per ognuno di questi quali colonne sono presenti (0-1) (Una sorta di confusion matrix?) \n\n- se il numero unico di colonne è troppo elevato, potrebbe essere interessante andare a considerare dei sottogruppi\n\n- naturalmente ci potrebbe essere un'analisi di correlazione e crosscorrelazione (che potrebbero dare vita a notebook separati) \n\n- un'altra sarebbe quella di fare un'analisi geografica nelle distanze tra target e altre colonne o tra dataset diversi (potrebbe dar vita a un notebook separato)\n\n- analisi per singola fonte?"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_pattern = ['Date', 'Depth_to_Groundwater', 'Flow_Rate', 'Hydrometry', 'Lake_Level', 'Rainfall', 'Temperature', 'Volume']\n\nfeature_matrix = np.zeros((len(data_names), len(features_pattern)))\n\nfor k in range(len(data_names)): \n    name_df = data_names[k]\n    df_columns = df_dict[name_df].columns.tolist()\n    feature_matrix[k, :] = get_pattern_count(features_pattern, df_columns)\n    \nfeature_matrix = pd.DataFrame(feature_matrix, columns = features_pattern, index = data_names)\nfeature_matrix['total_columns'] = list(map(lambda x: x.shape[1], df_dict.values()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.color_palette(\"rocket\")\nfig, ax = plt.subplots(1, 1, figsize = (17, 11))\n\ncolors = sns.color_palette('rocket', 15)\nlevels = [1, 2, 3, 4, 5, 7, 9, 11, 14, 17, 20, 23, 26, 29, 32]\ncmap, norm = matplotlib.colors.from_levels_and_colors(levels, colors, extend=\"max\")\n\nsns.heatmap(feature_matrix, annot=True, cmap=cmap, ax = ax, norm=norm)\nax.xaxis.set_ticks_position('top')\nax.set(xlabel='Column Type', ylabel='Dataset')\nplt.title('Number of columns per type')\nplt.xticks(rotation=290)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize = (17,11))\n\nnorm_feature_matrix = feature_matrix.copy()\nnorm_feature_matrix.iloc[:, :-1] = round(norm_feature_matrix.iloc[:, :-1].div(norm_feature_matrix.iloc[:, -1], axis = 0), 3)\n\nsns.heatmap(norm_feature_matrix.drop('total_columns', 1), annot=True, ax = ax, cmap = sns.color_palette('rocket'))\nax.xaxis.set_ticks_position('top')\nax.set(xlabel='Column Type', ylabel='Dataset')\nplt.title('Number of columns per type')\nplt.xticks(rotation=290)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### TimeSpan\n\nHere I compare each dataset timespan (minimum and maximum date) for target columns. "},{"metadata":{"trusted":true},"cell_type":"code","source":"target_dict = {'Aquifer_Auser' : ['Depth_to_Groundwater_LT2', 'Depth_to_Groundwater_SAL', 'Depth_to_Groundwater_CoS'],\n               'Aquifer_Doganella' : ['Depth_to_Groundwater_Pozzo_1','Depth_to_Groundwater_Pozzo_2','Depth_to_Groundwater_Pozzo_3',\n                                      'Depth_to_Groundwater_Pozzo_4','Depth_to_Groundwater_Pozzo_5','Depth_to_Groundwater_Pozzo_6',\n                                      'Depth_to_Groundwater_Pozzo_7','Depth_to_Groundwater_Pozzo_8','Depth_to_Groundwater_Pozzo_9'],\n               'Aquifer_Luco' : ['Depth_to_Groundwater_Podere_Casetta'],\n               'Aquifer_Petrignano' : ['Depth_to_Groundwater_P24','Depth_to_Groundwater_P25'],\n               'Lake_Bilancino': ['Lake_Level','Flow_Rate'],\n               'River_Arno': ['Hydrometry_Nave_di_Rosano'],\n               'Water_Spring_Amiata': ['Flow_Rate_Bugnano','Flow_Rate_Arbure','Flow_Rate_Ermicciolo','Flow_Rate_Galleria_Alta'],\n               'Water_Spring_Lupa': ['Flow_Rate_Lupa'],\n               'Water_Spring_Madonna_di_Canneto': ['Flow_Rate_Madonna_di_Canneto']}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for j in range(len(data_names)):\n    \n    df_name = data_names[j]\n    target_cols = target_dict[df_name]\n    \n    df = df_dict[df_name][['Date'] + target_cols]\n    \n    print(df.dropna().Date.min(), df.loc[df[target_cols].isna().sum(axis = 1) < len(target_cols)-1].Date.min())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, quando è NaN un target lo sono tutti "},{"metadata":{"trusted":true},"cell_type":"code","source":"cmap_plot = plt.get_cmap('jet_r')\n\nfor j in range(len(data_names)):\n    \n    data_name = data_names[j]\n    \n    target_cols = target_dict[data_name]    \n    target_cols_len = len(target_cols)\n\n    df = (df_dict[data_name][['Date']+target_cols].melt(id_vars='Date', value_vars=target_cols))\n    df['value'] = df['value'].astype(float)\n    \n    fig = px.line(data_frame=df, x = 'Date', y = 'value', color = 'variable', title = data_name, labels = 'variable')\n    \n    fig.update_xaxes(tickangle=45,\n                 tickmode = 'array',\n                 tickvals = [df['Date'].min(), df['Date'].max()])\n    \n    fig.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Targets distribution over time for each Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"cmap_plot = plt.get_cmap('jet_r')\n\nfor j in range(len(data_names)):\n    \n    data_name = data_names[j]\n    \n    target_cols = target_dict[data_name]    \n    target_cols_len = len(target_cols)\n    \n    fig, ax = plt.subplots(1, 1, figsize = (14, 7))\n    for i, target in enumerate(target_cols):\n        if target_cols_len > 4:\n            color = cmap_plot(float(i)/target_cols_len)\n            df_dict[data_name][[target, 'Date']].plot(x = 'Date', ax = ax, c = color, lw = 3)\n        else:\n            df_dict[data_name][[target, 'Date']].plot(x = 'Date', ax = ax, lw = 3)\n        plt.legend(title='targets', bbox_to_anchor=(1.05, 1), loc='upper left')\n    ax.set_title(data_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Targets distribution for each dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"cmap_plot = plt.get_cmap('jet_r')\n\nfor j in range(len(data_names)):\n    \n    data_name = data_names[j]\n    \n    target_cols = target_dict[data_name]    \n    target_cols_len = len(target_cols)\n    \n    df = df_dict[data_name]\n    \n    fig, ax = plt.subplots(1, 1, figsize = (12, 6))\n    for i, target in enumerate(target_cols):\n        sns.kdeplot(df[target], shade=True, alpha=0.5, ax = ax)\n        \n    plt.legend(title='targets', bbox_to_anchor=(1.05, 1), loc='upper left')\n    ax.set_title(data_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"?plt.subplots","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def distribution1_mod(feature, color, df, data_name):\n    fig, axes = plt.subplots(1, 2, figsize=(11, 7))\n    fig.suptitle(data_name+\" \"+feature)\n    ax = axes.ravel()\n    sns.distplot(df[feature],color=color,ax=ax[0])\n    ax[0].set(xlabel='density')\n    sns.violinplot(df[feature], ax=ax[1])\n    ax[0].set(xlabel='violin')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cmap_plot = plt.get_cmap('jet_r')\n\nfor j in range(len(data_names)):\n    \n    data_name = data_names[j]\n    \n    target_cols = target_dict[data_name]    \n    target_cols_len = len(target_cols)\n    \n    df = df_dict[data_name]\n    \n    for i, target in enumerate(target_cols):\n        distribution1_mod(target,'red', df, data_name)\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### NaNs\n\nC'è un discorso legato ai NaN ma anche uno legato alle date mancanti. \n\nPer step:\n\n- numero di NaN nelle target cols per ogni dataset \n\n- filling dei NaNs ? "},{"metadata":{},"cell_type":"markdown","source":"#### (Auto/Cross) Correlation Analysis\n\n- Correlation for each dataset\n\n- Correlation just for targets (rectangular matrix)\n\n- Correlation between all variables of all datasets\n\n- AutoCorrelation for both Target and Predictor variables\n\n- CrossCorrelation between Target/Targets and Target/Predictors\n\n- CrossCorrelation between all columns of all datasets"},{"metadata":{},"cell_type":"markdown","source":"Correlation Matrix for each Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"for j in range(len(data_names)):\n    \n    data_name = data_names[j]\n    df = df_dict[data_name]\n    \n    corr_matrix = round(df.drop('Date', axis = 1).corr(), 2)\n    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n    \n    fig, ax = plt.subplots(1, 1, figsize = (16, 10))\n    colors = sns.color_palette('rocket', 21)\n    levels = np.linspace(-1, 1, 21)\n    cmap_plot, norm = matplotlib.colors.from_levels_and_colors(levels, colors, extend=\"max\")\n    sns.heatmap(corr_matrix, mask=mask, annot=True, ax = ax, cmap = cmap_plot, norm = norm, annot_kws={\"size\": 9})\n    ax.xaxis.set_ticks_position('top')\n    ax.xaxis.label.set_size(14)\n    plt.title('Correlation Matrix for {}'.format(data_name))\n    plt.xticks(rotation=280)\n    fig.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation Matrix for each dataset, target variables\n\n\n- plot both real values and absolute ones"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax.title.set_fontsize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for j in range(len(data_names)):\n    \n    data_name = data_names[j]\n    df = df_dict[data_name]\n    \n    corr_matrix = round(df.drop('Date', axis = 1).corr(), 2)\n    corr_matrix = corr_matrix[target_dict[data_name]]\n    \n    fig, axes = plt.subplots(1, 2, figsize = (14, 8))\n    plt.suptitle(data_name)\n    ax = axes.ravel()[0]\n    colors = sns.color_palette('rocket', 11)\n    levels = np.linspace(-1, 1, 11)\n    cmap_plot, norm = matplotlib.colors.from_levels_and_colors(levels, colors, extend=\"max\")\n    sns.heatmap(corr_matrix, annot=True, ax = ax, cmap = cmap_plot, norm = norm, annot_kws={\"size\": 9})\n    ax.xaxis.set_ticks_position('top')\n    ax.xaxis.label.set_size(10)\n    ax.tick_params(axis='both', which='major', labelsize=10)\n    ax.tick_params(axis='both', which='minor', labelsize=8)\n    ax.title.set_text('Correlation Matrix')\n    ax.title.set_fontsize(12)\n\n    ax = axes.ravel()[1]\n    colors = sns.color_palette('rocket', 11)\n    levels = np.linspace(0, 1, 11)\n    cmap_plot, norm = matplotlib.colors.from_levels_and_colors(levels, colors, extend=\"max\")\n    sns.heatmap(abs(corr_matrix), annot=True, ax = ax, cmap = cmap_plot, norm = norm, annot_kws={\"size\": 9})\n    ax.xaxis.set_ticks_position('top')\n    ax.tick_params(axis='both', which='major', labelsize=10)\n    ax.tick_params(axis='both', which='minor', labelsize=8)\n    ax.set_yticks([])\n\n    ax.title.set_text('Absolute Correlation Matrix')\n    ax.title.set_fontsize(12)\n    \n    for ax in fig.axes:\n        matplotlib.pyplot.sca(ax)\n        plt.xticks(rotation=280)\n    fig.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation Matrix for each dataset, but let's consider just absolute values > 0.75"},{"metadata":{"trusted":true},"cell_type":"code","source":"for j in range(len(data_names)):\n    \n    data_name = data_names[j]\n    df = df_dict[data_name]\n    \n    corr_matrix = round(df.drop('Date', axis = 1).corr(), 2)\n    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n    \n    mask[(abs(corr_matrix) < 0.75) & (mask == False)] = True\n    \n    fig, ax = plt.subplots(1, 1, figsize = (12, 8))\n    colors = sns.color_palette('rocket', 11)\n    levels = np.linspace(-1, 1, 11)\n    cmap_plot, norm = matplotlib.colors.from_levels_and_colors(levels, colors, extend=\"max\")\n    sns.heatmap(corr_matrix, mask=mask, annot=True, ax = ax, cmap = cmap_plot, norm = norm, annot_kws={\"size\": 9})\n    ax.xaxis.set_ticks_position('top')\n    ax.xaxis.label.set_size(14)\n    plt.title('Correlation Matrix for {}'.format(data_name))\n    plt.xticks(rotation=280)\n    fig.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for j in range(len(data_names)):\n    \n    data_name = data_names[j]\n    df = df_dict[data_name]\n    \n    corr_matrix = round(df.drop('Date', axis = 1).corr(), 2)\n    corr_matrix = corr_matrix[target_dict[data_name]]\n    \n    mask = np.zeros_like(corr_matrix, dtype=bool)\n    \n    mask[(abs(corr_matrix) < 0.75) & (mask == False)] = True\n    \n    fig, ax = plt.subplots(1, 1, figsize = (12, 8))\n    colors = sns.color_palette('rocket', 11)\n    levels = np.linspace(-1, 1, 11)\n    cmap_plot, norm = matplotlib.colors.from_levels_and_colors(levels, colors, extend=\"max\")\n    sns.heatmap(corr_matrix, mask=mask, annot=True, ax = ax, cmap = cmap_plot, norm = norm, annot_kws={\"size\": 9})\n    ax.xaxis.set_ticks_position('top')\n    ax.xaxis.label.set_size(14)\n    plt.title('Correlation Matrix for {}'.format(data_name))\n    plt.xticks(rotation=280)\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Autocorrelation"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_labels = ax.get_yticklabels()\nfor j in y_labels:\n    print(j)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"?sns.barplot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_chunks = chunks(range(len(data_names)), 3)\nchunk_len = 3\n\nfor chunk in data_chunks:\n    fig, axes = plt.subplots(1, 3, figsize = (20, 12))\n    fig.suptitle('Autocorrelation for each features and dataset')\n    axes_raveled = axes.ravel()\n    for k in range(len(chunk)):\n        \n        j = chunk[k]\n        data_name = data_names[j]\n        df = df_dict[data_name].sort_values('Date', ignore_index = True).drop('Date', 1)\n        \n        autocorr_dataframe = (pd.DataFrame(df.apply(lambda x: x.autocorr(), 0))\n                             .reset_index().rename(columns = {'index': 'feature', 0: 'autocorrelation'})\n                             .sort_values('autocorrelation', ascending = False))\n        \n        ax = axes_raveled[k]\n    \n        sns.barplot(x='autocorrelation', y='feature', data=(autocorr_dataframe), ax = ax, palette = 'jet_r')\n        y_labels = autocorr_dataframe.feature.tolist()\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])\n        ax.title.set_text(data_name)\n        ax.title.set_fontsize(12)\n        t=0\n        for p in ax.patches:\n            width = p.get_width() \n            if width < 0.01:# get bar length\n                ax.text(width,       # set the text at 1 unit right of the bar\n                p.get_y() + p.get_height() / 2, # get Y coordinate + X coordinate / 2\n                '{:1.4f}'.format(width), # set variable to display, 2 decimals\n                ha = 'left',   # horizontal alignment\n                va = 'center')  # vertical alignment\n            else:\n                ax.text(width/4, \n                    # set the text at 1 unit right of the bar\n                p.get_y() + p.get_height() / 2, # get Y coordinate + X coordinate / 2\n                '{} {:1.4f}'.format(y_labels[t], width), # set variable to display, 2 decimals\n                ha = 'left',   # horizontal alignment\n                va = 'center',\n                color = 'black',\n                fontsize = 11)\n            t+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}