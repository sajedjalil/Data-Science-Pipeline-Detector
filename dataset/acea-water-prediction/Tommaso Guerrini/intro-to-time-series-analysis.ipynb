{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id=\"top\"></a>\n# Notebook Contents\n\nIn this notebook I try to share the kind of analysis I perform to analyze Time Series Data. \n\nThe notebook is divided in the following sections and subsections: \n\n[**1. Correlation Analysis**](#correlation)<br>\n    1.1 [*Example Correlation Matrix for Water Spring Amiata*](#correlation_amiata)<br>\n    1.2 [*Example Correlation Matrix for Water Spring Amiata just targets*](#correlation_amiata_targets)<br>\n    1.3 [*Correlation Matrix for all Acea datasets*](#correlation_all)<br>\n    1.4 [*Correlation Matrix for all Acea datasets just targets*](#correlation_all_targets)<br>\n    1.5 [*What to do with it*](#use_correlation)<br>\n    \n    \n[**2. AutoCorrelation Analysis**](#autocorrelation) <br>\n    2.1 [*Stationarity Test*](#stationarity_test)<br>\n    2.2 [*ACF and PACF plots stationary vs non stationary*](#acf_pacf_comparison)<br>\n    2.3 [*ACF and PACF plots all Acea datasets*](#acf_pacf_all)<br>\n    2.4 [*ACF and PACF plots all Acea datasets just targets*](#acf_pacf_all_targets)<br>\n    2.5 [*What to do with it*](#use_acf_pacf)<br>\n\n## **CURRENTLY UNDER CONSTRUCTION**\n\n[**3. CrossCorrelation Analysis**](#crosscorrelation)<br> \n    3.1 [*Most crosscorrelated features/lags*](#most_cross)<br>\n    3.2 [*What to do with it*](#use_cross)<br>\n    \n[**4. Spurious Correlations**](#spurious)<br>\n    4.1 [*Revisiting Correlations*](#correlation_redo)<br>\n    4.2 [*What to do with it*](#use_spurious)<br>\n\n- **Causality**(maybe later)\n\n<div class=\"row\">\n  <div class=\"column\">\n    <img src=\"https://www.gruppo.acea.it/content/dam/acea-corporate/acea-foundation/immagini/al-servizio-delle-persone/hub/acea-acqua-760x425.jpg\" align=\"left\" style=\"width:50%\">\n  </div>\n  <div class=\"column\">\n    <img src=\"https://i.imgur.com/IxrgRGl.png\" style=\"width:50%\" align=\"right\">\n  </div>\n</div>\n\n<br>\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.options.display.max_columns = 30\nimport os\nimport re\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.graphics.tsaplots import plot_pacf, plot_acf\nimport seaborn as sns\nimport tqdm\nimport itertools\nimport matplotlib\nfrom matplotlib import pyplot as plt\nplt.rcParams.update({'figure.max_open_warning': 0})\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')\n\nroot_path = '/kaggle/input/acea-water-prediction'\ndata_files = [i for i in os.listdir(root_path) if re.match(\".+\\.csv$\", i)]\ndata_files.sort()\ndata_names = [i.replace('.csv', '') for i in data_files]\ndata_files = list(map(lambda x: os.path.join(root_path, x), data_files))\n\nwaterbody_type = [re.match(\"water_spring|aquifer|river|lake\", i.lower())[0] for i in data_names]\n\ndef get_df_basic_information(df, waterbody_type, df_name): \n    \n    n_rows, n_columns = df.shape\n    \n    mb_size = round(df.memory_usage(deep=True).sum()/1000000., 3)\n    \n    print(\"\"\"{0}{1}\\n\n          N rows: {2}\\tN columns: {3}\\n\n          Memory Usage: {4} Mb\\n\\n\\n\"\"\".format(color_dict[waterbody_type], df_name,\n                                           n_rows, n_columns, mb_size))\n    \ndef crosscorr(datax, datay, lag=0):\n    \"\"\" Lag-N cross correlation. \n    Parameters\n    ----------\n    lag : int, default 0\n    datax, datay : pandas.Series objects of equal length\n\n    Returns\n    ----------\n    crosscorr : float\n    \"\"\"\n    return datax.corr(datay.shift(lag))\n    \ndef chunks_old(l, n):\n    \"\"\" Yield n successive chunks from l.\n    \"\"\"\n    newn = int(len(l) / n)\n    for i in range(0, n-1):\n        yield l[i*newn:i*newn+newn]\n    yield l[n*newn-newn:]\n    \ndef chunks(lst, n):\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n    for i in range(0, len(lst), n):\n        yield lst[i:i + n]\n    \ndf_dict = dict(zip(data_names, list(map(lambda x:pd.read_csv(x), data_files))))\n\nfor name in data_names:\n    df_dict[name] = df_dict[name].loc[~df_dict[name]['Date'].isna()]\n    df_dict[name]['Date'] = pd.to_datetime(df_dict[name]['Date'], format = \"%d/%m/%Y\").dt.date\n    df_dict[name].sort_values('Date', ignore_index = True, inplace = True)\n    \ntarget_dict = {'Aquifer_Auser' : ['Depth_to_Groundwater_LT2', 'Depth_to_Groundwater_SAL', 'Depth_to_Groundwater_CoS'],\n               'Aquifer_Doganella' : ['Depth_to_Groundwater_Pozzo_1','Depth_to_Groundwater_Pozzo_2','Depth_to_Groundwater_Pozzo_3',\n                                      'Depth_to_Groundwater_Pozzo_4','Depth_to_Groundwater_Pozzo_5','Depth_to_Groundwater_Pozzo_6',\n                                      'Depth_to_Groundwater_Pozzo_7','Depth_to_Groundwater_Pozzo_8','Depth_to_Groundwater_Pozzo_9'],\n               'Aquifer_Luco' : ['Depth_to_Groundwater_Podere_Casetta'],\n               'Aquifer_Petrignano' : ['Depth_to_Groundwater_P24','Depth_to_Groundwater_P25'],\n               'Lake_Bilancino': ['Lake_Level','Flow_Rate'],\n               'River_Arno': ['Hydrometry_Nave_di_Rosano'],\n               'Water_Spring_Amiata': ['Flow_Rate_Bugnano','Flow_Rate_Arbure','Flow_Rate_Ermicciolo','Flow_Rate_Galleria_Alta'],\n               'Water_Spring_Lupa': ['Flow_Rate_Lupa'],\n               'Water_Spring_Madonna_di_Canneto': ['Flow_Rate_Madonna_di_Canneto']}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"correlation\"></a>\n\n## 1. Correlation Analysis"},{"metadata":{},"cell_type":"markdown","source":"### Correlation Definition\n\nOne of the pillars of any EDA is to look for correlation between variables, i.e. the *Pearson correlation coefficient*. The Pearson correlation coefficient between two random variables $X$ and $Y$ is defined as: \n\n${\\displaystyle \\rho _{X,Y}={\\frac {\\operatorname {\\mathbb {E} } [(X-\\mu _{X})(Y-\\mu _{Y})]}{\\sigma _{X}\\sigma _{Y}}}}$ , further details [here](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#Definition)\n\nActually what we use is the *Sample Pearson Correlation Coefficient*, defined as: \n\n${\\displaystyle r_{xy}={\\frac {\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})(y_{i}-{\\bar {y}})}{{\\sqrt {\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})^{2}}}{\\sqrt {\\sum _{i=1}^{n}(y_{i}-{\\bar {y}})^{2}}}}}}$, where $\\mathbf{x},\\mathbf{y}$ are non other than two columns (of dimension $n$) of your dataframe ($\\bar{x}$,$\\bar{y}$ being their sample means).\n\nThis coefficient takes values in $[-1,1]$ and we could sum it up like [this](https://machinelearningmastery.com/how-to-use-correlation-to-understand-the-relationship-between-variables/#:~:text=The%20statistical%20relationship%20between%20two,the%20other%20variables'%20values%20decrease.): \n\n- **Positive** Correlation: both variables change in the *same direction*.\n- **Neutral** Correlation: *no relationship in the change* of the variables.\n- **Negative** Correlation: variables change in *opposite directions*.\n\nNotice also that it's symmetric and $r_{xx}=1$"},{"metadata":{},"cell_type":"markdown","source":"### Correlation on Acea datasets\n\nLet's now focus on Acea datasets and calculate the correlation between each dataset columns. \nGiven the fact that it's symmetric we will plot just the lower triangular matrix and avoid plotting the diagunal since it's all 1s. \n\nI will show just some plots to avoid clogging the notebook, I hide the others so that you can see them simply my unhiding them. \n\nThe subsections are: \n\n[*Example Correlation Matrix for Water Spring Amiata*](#correlation_amiata) <br>\n\n[*Example Correlation Matrix for Water Spring Amiata just targets*](#correlation_amiata_targets) <br>\n\n[*Correlation Matrix for all Acea datasets*](#correlation_all) <br>\n\n[*Correlation Matrix for all Acea datasets just targets*](#correlation_all_targets) <br>\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"correlation_amiata\"></a>\n#### Example Correlation Matrix for Water Spring Amiata"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Change this flag if you want to see all correlation values, not just those whose absolute value exceeds THRESHOLD\n\nPLOT_JUST_HIGH_VALS = True\nTHRESHOLD = 0.7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = sns.color_palette('coolwarm', 21)\nlevels = np.linspace(-1, 1, 21)\ncmap_plot, norm = matplotlib.colors.from_levels_and_colors(levels, colors, extend=\"max\")\n\ndata_name = 'Water_Spring_Amiata'\ndf = df_dict[data_name]\nn_cols = df.shape[1]\n\ncorr_matrix = round(df.drop('Date', axis = 1).corr(), 2)\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n\nif PLOT_JUST_HIGH_VALS:\n    mask[(abs(corr_matrix) < THRESHOLD) & (mask == False)] = True\n        \nfig, ax = plt.subplots(1, 1, figsize = (16, 10))\n\nsns.heatmap(corr_matrix, mask=mask, annot=True, ax = ax, \n            vmin = -1, vmax = 1, \n            cmap = cmap_plot, norm = norm, annot_kws={\"size\": 9})\nax.hlines(range(0, n_cols+1), *ax.get_xlim(), lw=1, linestyles = 'dashed')\nax.vlines(range(0, n_cols+1), *ax.get_xlim(), lw=1, linestyles = 'dashed')\nax.xaxis.set_ticks_position('top')\nax.xaxis.label.set_size(11)\nax.tick_params(axis='both', which='major', labelsize=10)\nplt.title('{}'.format(data_name.upper()))\nplt.xticks(rotation=285)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***In this case we have rainfalls and temperatures positively correlated (since probably the areas are near each other), while he have depth to groundwater variables strongly negatively correlated with Flow_Rate_Galleria_Alta.***\n\n<a id=\"correlation_amiata_targets\"></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"data_name = 'Water_Spring_Amiata'\ndf = df_dict[data_name]\nn_cols = df.shape[1]\n\ncorr_matrix = round(df.drop('Date', axis = 1).corr(), 2)\nmask = np.zeros_like(corr_matrix, dtype=bool)\nif PLOT_JUST_HIGH_VALS:\n    mask[(abs(corr_matrix) < THRESHOLD) & (mask == False)] = True\n    for j in range(mask.shape[1]):\n        mask[j,j] = True\n    n_cols = df.shape[1]\n    \ntarget_cols = target_dict[data_name]\ndf_cols = corr_matrix.columns.tolist()\nindices = [df_cols.index(i) for i in target_cols]\ncorr_matrix = corr_matrix[target_cols]\nmask = mask[:, indices]\n\nfig, ax = plt.subplots(1, 1, figsize = (16, 10))\n\nsns.heatmap(corr_matrix, mask=mask, annot=True, ax = ax, \n            vmin = -1, vmax = 1, \n            cmap = cmap_plot, norm = norm, annot_kws={\"size\": 9})\nax.hlines(range(0, n_cols+1), *ax.get_xlim(), lw=1, linestyles = 'dashed')\nax.vlines(range(0, n_cols+1), *ax.get_ylim(), lw=1, linestyles = 'dashed')\nax.xaxis.set_ticks_position('top')\nax.xaxis.label.set_size(11)\nax.tick_params(axis='both', which='major', labelsize=10)\nplt.title('{} just targets'.format(data_name.upper()))\nplt.xticks(rotation=285)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unhide to see how correlated features look over time. "},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\ndata_name = 'Water_Spring_Amiata'\ndf = df_dict[data_name]\ndf = df.loc[df.Date.astype(str) > '2018-01-01']\ndf_std = df.copy()\ndf_std.iloc[:, 1:] = sc.fit_transform(df.iloc[:, 1:])\ndf_std.iloc[:, 1:] = df_std.iloc[:, 1:].rolling(15).mean()\ndf = df_std.copy()\n\nfig, axes = plt.subplots(1, 2, figsize = (20, 12))\nax = axes.ravel()\ndf[['Depth_to_Groundwater_S_Fiora_8', 'Date']].plot(x = 'Date', ax = ax[0], lw = 3)\ndf[['Flow_Rate_Galleria_Alta', 'Date']].plot(x = 'Date', ax = ax[0], lw = 1)\nax[0].set(title = 'Negative Correlation')\ndf[['Flow_Rate_Arbure', 'Date']].plot(x = 'Date', ax = ax[1], lw = 1)\ndf[['Flow_Rate_Bugnano', 'Date']].plot(x = 'Date', ax = ax[1], lw = 1)\nax[1].set(title = 'Positive Correlation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Of course we are interested in the correlation among all variables, also between pairs of non targets ([collinearity](https://en.wikipedia.org/wiki/Multicollinearity#Definition) may be an issue for some linear-based models). "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"correlation_all\"></a>\nUnhide the following to show correlation matrices for all datasets in Acea. "},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"colors = sns.color_palette('coolwarm', 21)\nlevels = np.linspace(-1, 1, 21)\ncmap_plot, norm = matplotlib.colors.from_levels_and_colors(levels, colors, extend=\"max\")\n\nfor j in range(len(data_names)):\n    \n    data_name = data_names[j]\n    df = df_dict[data_name]\n    n_cols = df.shape[1]\n    \n    corr_matrix = round(df.drop('Date', axis = 1).corr(), 2)\n    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n    \n    if PLOT_JUST_HIGH_VALS:\n        mask[(abs(corr_matrix) < THRESHOLD) & (mask == False)] = True\n    \n    fig, ax = plt.subplots(1, 1, figsize = (16, 10))\n    \n    sns.heatmap(corr_matrix, mask=mask, annot=True, ax = ax, \n                vmin = -1, vmax = 1, \n                cmap = cmap_plot, norm = norm, annot_kws={\"size\": 9})\n    ax.hlines(range(0, n_cols+1), *ax.get_xlim(), lw=1, linestyles = 'dashed')\n    ax.vlines(range(0, n_cols+1), *ax.get_xlim(), lw=1, linestyles = 'dashed')\n    ax.xaxis.set_ticks_position('top')\n    ax.tick_params(axis='both', which='major', labelsize=10)\n    ax.xaxis.label.set_size(14)\n    plt.title('{}'.format(data_name.upper()))\n    plt.xticks(rotation=280)\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"correlation_all_targets\"></a>\nUnhide the following to show correlation matrices for all datasets in Acea, focusing on targets.\n\nSome plots are empty, that means that there are no features whose absolute correlation is higher than the threshold defined above. "},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"for i, data_name in enumerate(data_names):\n    \n    df = df_dict[data_name]\n    n_cols = df.shape[1]\n    corr_matrix = round(df.drop('Date', axis = 1).corr(), 2)\n    mask = np.zeros_like(corr_matrix, dtype=bool)\n    if PLOT_JUST_HIGH_VALS:\n        mask[(abs(corr_matrix) < THRESHOLD) & (mask == False)] = True\n        for j in range(mask.shape[1]):\n            mask[j,j] = True\n        n_cols = df.shape[1]\n\n    target_cols = target_dict[data_name]\n    df_cols = corr_matrix.columns.tolist()\n    indices = [df_cols.index(i) for i in target_cols]\n    corr_matrix = corr_matrix[target_cols]\n    mask = mask[:, indices]\n\n    fig, ax = plt.subplots(1, 1, figsize = (16, 10))\n\n    sns.heatmap(corr_matrix, mask=mask, annot=True, ax = ax, \n                vmin = -1, vmax = 1, \n                cmap = cmap_plot, norm = norm, annot_kws={\"size\": 9})\n    ax.hlines(range(0, n_cols+1), *ax.get_xlim(), lw=1, linestyles = 'dashed')\n    ax.vlines(range(0, n_cols+1), *ax.get_ylim(), lw=1, linestyles = 'dashed')\n    ax.xaxis.set_ticks_position('top')\n    ax.xaxis.label.set_size(11)\n    ax.tick_params(axis='both', which='major', labelsize=10)\n    plt.title('{} just targets'.format(data_name.upper()))\n    plt.xticks(rotation=285)\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"use_correlation\"></a>\n#### What to do with it\n\nUnderstanding which features are correlated with the target ones may help in selecting them in a model. If collinearity is an issue (for instance in linear models), working with principal components or using some regularization term (L1, so that features are dropped) may help."},{"metadata":{},"cell_type":"markdown","source":"[return to top](#top)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"autocorrelation\"></a>\n\n## 2. Autocorrelation Analysis"},{"metadata":{},"cell_type":"markdown","source":"### Autocorrelation Definition\n\nIn time series analysis we can't limit ourselves just at looking for correlation between two variables, for several reasons: \n\n- you may not have predictor variables, just target ones\n\n- you usually don't have accurate information into the future, where you want to make predictions (you may want to predict lake level in a week from now but you don't have the actual data for rainfalls and temperatures)\n\n- correlation compares two variables at the same timestamp, what we would like to do is to see whether the same variable is related to itself at a different point in time so that knowing past values can help us infer future ones\n\n\n**Stationarity**\n\nFor any autocorrelation/autoregression **linear** (f.i. ARIMA) technique we need our time series to be **stationary**. \n\nI consider it an important concept, even if **we don't necessarily need stationarity for modeling**.\n\nA loose definition of stationarity is the following:\n\n> *A stationary time series is one whose statistical properties such as mean, variance, autocorrelation do not change over time ([reference](https://people.duke.edu/~rnau/411diff.htm))*. \n\n[Here](https://en.wikipedia.org/wiki/Stationary_process#Example_1) you can find a more precise definition (and useful examples to understand it).\n\nSpeaking in layman's terms: if our time series has some trend (f.i. grows over time) or seasonality (changes periodically, f.i. with a period of a year) we will need to remove these *components* to make the series stationary. \n\nHere you see an example of both: \n\n<div class=\"row\">\n  <div class=\"column\">\n    <img src=\"https://i.imgur.com/UoNcPu9.png\" align=\"left\" style=\"width:50%\">\n  </div>\n  <div class=\"column\">\n    <img src=\"https://i.imgur.com/sHH220M.png\" style=\"width:50%\" align=\"right\">\n  </div>\n</div>\n<br>\nThrough `statsmodels.tsa.seasonal.seasonal_decompose` \nwe can also split a time series into trend, seasonality and noise components.\n\n<div class=\"row\">\n    <div class=\"column\">\n    <img src=\"https://i.imgur.com/0gURfWG.png\" align=\"center\" style=\"width:50%\">\n  </div>\n  </div>\n<br>\n\n**Autocorrelation**\n\nLet me be more engineer than strict mathematician: consider autocorrelation as a *time-dependent Pearson correlation coefficient* that we defined above. \n\n\nSo:\n\n${\\displaystyle \\rho _{XX}(\\tau )={\\frac {\\operatorname {E} \\left[(X_{t}-\\mu ){\\overline {(X_{t+\\tau }-\\mu )}}\\right]}{\\sigma ^{2}}}}$, go [here](https://en.wikipedia.org/wiki/Autocorrelation#Definition_for_wide-sense_stationary_stochastic_process) for more details.\n\n\n\n<br>\n\n**Partial Autocorrelation**\n\nThe partial autocorrelation at lag k is the correlation that results after removing the effect of any correlations due to the terms at shorter lags.\n\n\n<div class=\"row\">\n  <div class=\"column\">\n    <img src=\"https://i.imgur.com/8kDMmZh.png\" align=\"left\" style=\"width:50%\">\n  </div>\n  <div class=\"column\">\n    <img src=\"https://i.imgur.com/fCVUETa.png\" style=\"width:50%\" align=\"right\">\n  </div>\n</div>\n\n<br> \n\nYou can see that since the time series is very autocorrelated at lag 1 this high correlation is dragged to the other lags. By plotting the partial autocorrelation function we remove this drag and focus on the 'real' correlated lags (notice however that in this case we probably have a non stationary time series, rather a positively trending one). \n\n# Open Question: \n\nIn this notebook I don't perform any *AR* or *MA* modeling, but I still stress the importance of stationarity since it is a pillar concept in time series analysis. \n\n> My question is the following: given a $y_t$ non stationary time series and $\\hat{y_t}$ its stationary counterpart (f.i. obtained through differencing) is looking at $y_t$ PACF plot the same as looking at $\\hat{y_t}$ ACF plot in terms of which lag results as most important? \n\nI know that mathematically the two are non equivalent, but I was wondering whether there were examples of the two being different. \n\n\n\nSubsections are: \n\n- [*Stationarity Test*](#stationarity_test)<br>\n- [*ACF and PACF plots stationary vs non stationary*](#acf_pacf_comparison)<br>\n- [*ACF and PACF plots all Acea datasets*](#acf_pacf_all)<br>\n- [*ACF and PACF plots all Acea datasets just targets*](#acf_pacf_all_targets)<br>\n- [*What to do with it*](#use_acf_pacf)<br>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"stationarity_test\"></a>\n\n### Stationarity test\n\nTo check if our features are stationary we will run the [Augmented Dickey Fuller test](https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test). \n\n> *The test is trying to reject the null hypothesis that a unit root exists and the data is non-stationary. If the null hypothesis is rejected, then the alternate can be considered valid (e.g., the data is stationary).* https://pythondata.com/stationary-data-tests-for-time-series-forecasting/\n\n**A low p value rejects the null hypothesis i.e. we can consider the time series stationary**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Example on Aquifer Auser dataset\n\nHere I'll run the Augmented Dickey Fuller test on the *Aquifer Auser* dataset. \n\nSince there are plenty of Nans present (which must be filled to perform the test), I will consider just data from 2017 and fill nans with both *mean* and *preceding value* to see whether test results are consistent. \n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"RECALCULATE = False # I have already calculated them \n\nif RECALCULATE: \n    data_name = 'Aquifer_Auser'\n    df = df_dict[data_name]\n    df['Year'] = pd.to_datetime(df.Date).dt.year\n    df = df.loc[df.Date.astype(str) >= '2017-01-01']\n\n    MAX_LAG = 365\n\n    adf_list = []\n    cols_to_check = [i for i in df.columns if i not in ['Date', 'Year']]\n\n    for col in tqdm.tqdm(cols_to_check):\n        adf_test_ffill = adfuller(df[col].fillna(method = 'ffill'), maxlag=MAX_LAG)\n        col_mean = df[col].mean()\n        adf_test_mean = adfuller(df[col].fillna(col_mean), maxlag=MAX_LAG)\n        adf_list.append(pd.DataFrame({'feature': [col], 'p_value_mean': [adf_test_mean[1]], 'p_value_ffill': [adf_test_ffill[1]]}))\n\n    adf_df = pd.concat(adf_list, axis = 0)\n    adf_df.iloc[:, 1:] = round(adf_df.iloc[:, 1:], 6)\nelse:\n    data_name = 'Aquifer_Auser'\n    df = df_dict[data_name]\n    df['Year'] = pd.to_datetime(df.Date).dt.year\n    df = df.loc[df.Date.astype(str) >= '2017-01-01']\n    adf_df = pd.read_pickle('/kaggle/input/adf-calculated/adf_test_auser.pickle')\n    \ndisplay(adf_df.sort_values(['p_value_mean', 'p_value_ffill'], ignore_index = True, ascending = False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Low p-values correspond to stationary time series. We could expected rainfalls to be stationary, or at least more stationary than temperatures which have a clear seasonal pattern. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 1, figsize = (11, 7))\nax = axes.ravel()\n\ndf[['Temperature_Monte_Serra', 'Date']].set_index('Date').plot(lw = 2, linestyle = \"-.\",ax = ax[0], title = 'Temperature_Monte_Serra (Non stationary Time Series)')\n\ndf[['Rainfall_Monte_Serra', 'Date']].set_index('Date').plot(lw = 2,linestyle = \"-.\", ax = ax[1], title = 'Rainfall_Monte_Serra (stationary Time Series)', color = 'red', sharex = True)\n\nfig.suptitle('example of Stationary vs non Stationary time series')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Make a Time Series stationary\n\nThe canonical way of making a time series stationary is to take finite differences of the original time series until we achieve stationarity. Basically:\n\n$y'_{i}=y_{i}-y_{i-1}$ $\\forall i \\in [1, n]$\n\nThe same holds for removing *seasonality*. Let's say that our original time series has a seasonality pattern of period $m$ = 365. Then: \n\n$y'_{i}=y_{i}-y_{i-m}$ $\\forall i \\in [1, n]$\n\nCheck [this](https://otexts.com/fpp2/stationarity.html) for further and more accurate explanations.\n\nLet's see an example."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"data_name = 'Aquifer_Auser'\ndf = df_dict[data_name]\ntime_series = df[['Temperature_Monte_Serra', 'Date']].loc[df.Date.astype(str) > '2016-01-01'].set_index('Date')\n\ntime_series_stationary = time_series.diff(1)\n\ntime_series_stationary_mean = time_series_stationary.mean()\nadf_test_diff = adfuller(time_series_stationary.fillna(time_series_stationary_mean), maxlag=365)[1]\n\nprint('Former P Value:\\t{}\\nNew P Value:\\t{}'.format(0.157, round(adf_test_diff, 2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By taking the difference the *Temperature_Monte_Serra* adf test p value changed from 0.16 (null hypothesis of non stationarity holds) to 0.0 (the series is now stationary). \n\nOne could also argue why I didn't take the seasonal difference: well, it could be another way of doing this. By taking the one lag difference we are checking whether increases in temperature from one day to the next have a seasonal behaviour and this suffices to make it stationary. \n\nWe can also see it visually:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 1, figsize = (11, 7))\nax = axes.ravel()\n\ntime_series.plot(lw = 3, linestyle = \"-.\",ax = ax[0], title = 'Temperature_Monte_Serra (Non stationary Time Series)')\n\ntime_series_stationary.plot(lw = 3,linestyle = \"-.\", ax = ax[1], \n                            title = 'Differenced Temperature_Monte_Serra (stationary Time Series)', color = 'red', sharex = True)\n\nfig.suptitle('example of Stationary vs non Stationary time series')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"acf_pacf_auser\"></a>\n\n### ACF and PACF plots\n\nThere are different ways of calculating autocorrelation in python: Pandas handles nans, but does not provide a partial autocorrelation function, so I will use statsmodels (and fillna before). "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"acf_pacf_comparison\"></a>\n\n#### Stationary vs Non Stationary comparison\n\nI will compare the acf/pacf plots between stationary vs non stationary time series."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"data_name = 'Aquifer_Auser'\ndf = df_dict[data_name]\ndf = df.loc[(df.Date.astype(str) >= '2016-01-01') & (df.Date.astype(str) <= '2020-06-01')]\ncols = ['Depth_to_Groundwater_CoS', 'Rainfall_Borgo_a_Mozzano']\ncolors = [(0.31883238319215684, 0.4266050511215686, 0.8598574482039216), \n          (0.810615674827451, 0.26879706171764706, 0.23542761153333333)]\nfig, axes = plt.subplots(len(cols), 3, figsize = (20, 12))\nax = axes.ravel()\nfor enum, col in enumerate(cols):\n    (df[[col, 'Date']].loc[(df.Date.astype(str) >= '2017-01-01')].set_index('Date')\n    .plot(lw = 3, ax = ax[enum*3], title = col, color = colors[enum], legend = False))\n    ax[enum*3].set_xlabel(' ')\n    ax[enum*3].xaxis.set_ticks(['2017-01', '2018-01', '2019-01', '2020-01'])\n    myFmt = matplotlib.dates.DateFormatter(\"%Y-%m\")\n    ax[enum*3].xaxis.set_major_formatter(myFmt)\n\n    acf_stat = acf(df[col].fillna(method = 'ffill'), nlags = 365)\n    pd.Series(acf_stat[1:]).plot(ax = ax[enum*3+1], color = colors[enum],\n                                 title = col+' acf', linestyle = '--', alpha = None, lw = 2, ylim=(-1,1))\n\n    pacf_stat = pacf(df[col].fillna(method = 'ffill'), nlags = 365, method = 'ols')\n    pd.Series(pacf_stat[1:]).plot(ax = ax[enum*3+2], title = col+' pacf', alpha = None, lw = 2, \n                                  linestyle = '-.', ylim=(-1,1), color = colors[enum],)\nplt.suptitle('Stationary vs Non Stationary acf/pacf comparison')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see the non stationary time series (*Depth_to_Groundwater_CoS*) has a stronger autocorrelation over all lags. We can nicely spot the annual seasonality peaking at lags ~180-185 and ~365. \n\nInstead, the stationary time series (*Rainfall_Borgo_a_Mozzano*) has no particular trend nor seasonality and that is shown in the acf and pacf plots. "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"acf_pacf_all\"></a>\n\n#### ACF and PACF plots all Acea datasets\n\nunhide to see"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"for data_name in data_names:\n    df = df_dict[data_name]\n    df = df.loc[(df.Date.astype(str) >= '2016-01-01') & (df.Date.astype(str) <= '2020-06-01')]\n    cols_to_consider = [i for i in df.columns if i not in ['Date', 'Year']]\n    colors = sns.color_palette(\"coolwarm_r\", len(cols_to_consider))\n    data_chunks = list(chunks(cols_to_consider, 3))\n    for enum_chunk, cols in enumerate(list(data_chunks)):\n        if len(cols)==3:\n            fig, axes = plt.subplots(3, 3, figsize = (20, 12))\n        else:\n            fig, axes = plt.subplots(len(cols), 3, figsize = (20, 12))\n        ax = axes.ravel()\n        for enum, col in enumerate(cols):\n            (df[[col, 'Date']].loc[(df.Date.astype(str) >= '2017-01-01')].set_index('Date')\n            .plot(lw = 3, ax = ax[enum*3], title = col, color = colors[3*enum_chunk+enum], legend = False, sharex=True))\n            ax[enum*3].set_xlabel(' ')\n            ax[enum*3].xaxis.set_ticks(['2017-01', '2018-01', '2019-01', '2020-01'])\n            myFmt = matplotlib.dates.DateFormatter(\"%Y-%m\")\n            ax[enum*3].xaxis.set_major_formatter(myFmt)\n            try:\n                acf_stat = acf(df[col].fillna(method = 'ffill'), nlags = 365)\n                pd.Series(acf_stat[1:]).plot(ax = ax[enum*3+1], color = colors[3*enum_chunk+enum],\n                                         title = col+' acf', linestyle = '--', alpha = None, lw = 2, ylim=(-1,1),\n                                        sharex=True)\n            except:\n                print('Failed to Calculate acf')\n            try:\n                pacf_stat = pacf(df[col].fillna(method = 'ffill'), nlags = 365, method = 'ols')\n                pd.Series(pacf_stat[1:]).plot(ax = ax[enum*3+2], title = col+' pacf', alpha = None, lw = 2, \n                                          linestyle = '-.', ylim=(-1,1), color = colors[3*enum_chunk+enum],\n                                          sharex=True)\n            except:\n                print('Failed to Calculate acf')\n        plt.suptitle(data_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"acf_pacf_all_targets\"></a>\n\n#### ACF and PACF plots all Acea datasets just targets\n\nunhide to see"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"for data_name in data_names:\n    target_cols = target_dict[data_name]\n    df = df_dict[data_name][target_cols + ['Date']]\n    df = df.loc[(df.Date.astype(str) >= '2016-01-01') & (df.Date.astype(str) <= '2020-06-01')]\n    cols_to_consider = [i for i in df.columns if i not in ['Date', 'Year']]\n    colors = sns.color_palette(\"coolwarm_r\", len(cols_to_consider))\n    data_chunks = list(chunks(cols_to_consider, 3))\n    for enum_chunk, cols in enumerate(list(data_chunks)):\n        if len(cols)==3:\n            fig, axes = plt.subplots(3, 3, figsize = (20, 12))\n        else:\n            fig, axes = plt.subplots(len(cols), 3, figsize = (20, 12))\n        ax = axes.ravel()\n        for enum, col in enumerate(cols):\n            (df[[col, 'Date']].loc[(df.Date.astype(str) >= '2017-01-01')].set_index('Date')\n            .plot(lw = 3, ax = ax[enum*3], title = col, color = colors[3*enum_chunk+enum], legend = False, sharex=True))\n            ax[enum*3].set_xlabel(' ')\n            ax[enum*3].xaxis.set_ticks(['2017-01', '2018-01', '2019-01', '2020-01'])\n            myFmt = matplotlib.dates.DateFormatter(\"%Y-%m\")\n            ax[enum*3].xaxis.set_major_formatter(myFmt)\n            try:\n                acf_stat = acf(df[col].fillna(method = 'ffill'), nlags = 365)\n                pd.Series(acf_stat[1:]).plot(ax = ax[enum*3+1], color = colors[3*enum_chunk+enum],\n                                         title = col+' acf', linestyle = '--', alpha = None, lw = 2, ylim=(-1,1),\n                                        sharex=True)\n            except:\n                print('Failed to Calculate acf')\n            try:\n                pacf_stat = pacf(df[col].fillna(method = 'ffill'), nlags = 365, method = 'ols')\n                pd.Series(pacf_stat[1:]).plot(ax = ax[enum*3+2], title = col+' pacf', alpha = None, lw = 2, \n                                          linestyle = '-.', ylim=(-1,1), color = colors[3*enum_chunk+enum],\n                                          sharex=True)\n            except:\n                print('Failed to Calculate acf')\n        plt.suptitle(data_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"use_acf_pacf\"></a>\n\n##### What to do with it\n\nInspecting the autocorrelation of a time series helps us finding which lags maybe used in a forecasting model. \nOf course there maybe more automatic ways than visually inspecting our data. "},{"metadata":{},"cell_type":"markdown","source":"##### Other (maybe) useful plots"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"data_chunks = chunks(range(len(data_names)), 3)\nchunk_len = 3\n\nfor chunk in data_chunks:\n    fig, axes = plt.subplots(1, 3, figsize = (20, 12))\n    fig.suptitle('Autocorrelation lag 1 for each feature and dataset')\n    axes_raveled = axes.ravel()\n    for k in range(len(chunk)):\n        \n        j = chunk[k]\n        data_name = data_names[j]\n        df = df_dict[data_name].sort_values('Date', ignore_index = True).drop(['Date', 'Year'], \n                                                                              axis = 1, errors = 'ignore')\n        \n        autocorr_dataframe = (pd.DataFrame(df.apply(lambda x: x.autocorr(), 0))\n                             .reset_index().rename(columns = {'index': 'feature', 0: 'autocorrelation'})\n                             .sort_values('autocorrelation', ascending = False))\n        \n        ax = axes_raveled[k]\n    \n        sns.barplot(x='autocorrelation', y='feature', data=(autocorr_dataframe), ax = ax, palette = 'jet_r')\n        y_labels = autocorr_dataframe.feature.tolist()\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])\n        ax.title.set_text(data_name)\n        ax.title.set_fontsize(12)\n        t=0\n        for p in ax.patches:\n            width = p.get_width() \n            if width < 0.01:# get bar length\n                ax.text(width,       # set the text at 1 unit right of the bar\n                p.get_y() + p.get_height() / 2, # get Y coordinate + X coordinate / 2\n                '{:1.4f}'.format(width), # set variable to display, 2 decimals\n                ha = 'left',   # horizontal alignment\n                va = 'center')  # vertical alignment\n            else:\n                ax.text(width/4, \n                    # set the text at 1 unit right of the bar\n                p.get_y() + p.get_height() / 2, # get Y coordinate + X coordinate / 2\n                '{} {:1.4f}'.format(y_labels[t], width), # set variable to display, 2 decimals\n                ha = 'left',   # horizontal alignment\n                va = 'center',\n                color = 'black',\n                fontsize = 12)\n            t+=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[return to top](#top)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"crosscorrelation\"></a>\n\n## 3. Crosscorrelation analysis\n\nOnce again, we won't dwelve too much into technical details here, remembering that our aim is to interpret the crosscorrelation coefficient as the Pearson correlation coefficient between two different features at different lags. We use this definition of crosscorrelation coefficient:\n\n${\\displaystyle \\rho _{XY}(\\tau )={\\frac {\\operatorname {K} _{XY}(\\tau )}{\\sigma _{X}\\sigma _{Y}}}={\\frac {\\operatorname {E} [\\left(X_{t}-\\mu _{X}\\right){\\overline {\\left(Y_{t+\\tau }-\\mu _{Y}\\right)}}]}{\\sigma _{X}\\sigma _{Y}}}}$, go [here](https://en.wikipedia.org/wiki/Cross-correlation#Cross-correlation_function) for more details.\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"most_cross\"></a>\n\n### Most crosscorrelated features/lags\n\nAs a first we will look for crosscorrelated features for lags 1-30 (a month) and see the most correlated combinations. \n\n**N.B.** Of course, many of the time series are non stationary and are very autocorrelated: this will result in many lags having a high crosscorrelation value. I will address this later. "},{"metadata":{"trusted":true},"cell_type":"code","source":"CROSS_THRESHOLD = 0.6\nTOTAL_LAGS = 30","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"cross_corr_dict = {}\n\nfor data_name in data_names:\n    df = df_dict[data_name]\n    total_lags = range(1, TOTAL_LAGS)\n    cross_corr = {}\n    features = list(set(df.columns) - set(['Date', 'Year']))\n    combinations = list(itertools.product(features, features))\n\n    for j in total_lags:\n        cross_corr[j] = []\n        for k in tqdm.tqdm(combinations):\n            cross_corr[j].append(crosscorr(df[k[0]], df[k[1]], lag = j))\n\n    cross_corr = pd.DataFrame(cross_corr)\n    cross_corr.columns = ['cross_correlation_lag_{}'.format(i) for i in range(1, TOTAL_LAGS)]\n\n    cross_correlations = (pd.concat([pd.DataFrame(combinations).rename(columns = {0: 'first_feature', 1: 'second_feature'}),\n                                     pd.DataFrame(cross_corr)], 1)\n                          )\n\n    cross_correlations_melt = (pd.melt(cross_correlations, id_vars=['first_feature', 'second_feature'], \n                               value_vars=['cross_correlation_lag_{}'.format(i) for i in range(1, TOTAL_LAGS)],\n                               var_name = 'lag',\n                               value_name = 'cross_correlation')\n                              .assign(lag=lambda x: x.lag.str.replace('cross_correlation_lag_', \"\")))\n    \n    \n    def sort_features(x, y):\n        \n        return tuple(sorted([x,y]))\n    \n    cross_correlations_melt[['pair_of_features']] = (cross_correlations_melt.apply(lambda x:sort_features(x.first_feature,\n                                                                                                              x.second_feature), 1))\n\n    cross_correlations_melt['first_feature'] = cross_correlations_melt['pair_of_features'].apply(lambda x: x[0])\n    cross_correlations_melt['second_feature'] = cross_correlations_melt['pair_of_features'].apply(lambda x: x[1])\n    \n    cross_correlations_melt['pair_of_features'] = (cross_correlations_melt['first_feature'].str.replace(\"feature_\", \"\") + \n                                              \"_\"  + cross_correlations_melt['second_feature'].str.replace(\"feature_\", \"\") + \"_lag\" +\n                                                   cross_correlations_melt['lag']\n                                                  ).astype(str)\n    \n    cross_correlations_melt = cross_correlations_melt.drop_duplicates(['pair_of_features'], ignore_index=True)\n    \n    cross_corr_dict[data_name] = (cross_correlations_melt\n                                  .loc[(abs(cross_correlations_melt.cross_correlation) > CROSS_THRESHOLD) & \n                                      (cross_correlations_melt.first_feature!= cross_correlations_melt.second_feature)]\n                                  .reset_index(drop = True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### CrossCorrelated Features for each dataset\n\nFor each dataset let's see the top 5 ***most and least*** correlated features (with corresponding lag). "},{"metadata":{"trusted":true},"cell_type":"code","source":"for data_name in data_names:\n    if len(cross_corr_dict[data_name]) == 0:\n        display('{} empty'.format(data_name))\n    else:\n        display(pd.concat([cross_corr_dict[data_name]\n            .sort_values('cross_correlation', ascending = False)\n            .drop('pair_of_features', 1)\n            .head(5),cross_corr_dict[data_name]\n            .sort_values('cross_correlation', ascending = False)\n            .drop('pair_of_features', 1)\n            .tail(5)], axis = 0, ignore_index=True\n           ).assign(dataset=data_name))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"example\"></a>\n#### An example of crosscorrelated pair of features"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_cross = df_dict['Aquifer_Auser']\ncols = ['Temperature_Lucca_Orto_Botanico', 'Temperature_Monte_Serra']\ndf_cross['Temperature_Monte_Serra_lag2'] = df_cross['Temperature_Monte_Serra'].shift(2)\n\ndf_cross['Volume_POL_lag29'] = df_cross['Volume_POL'].shift(29)\ndf_cross = df_cross.loc[df_cross.Date.astype(str)>='2017-01-01'].set_index('Date')\n\nfig, axes = plt.subplots(2, 2, figsize = (14, 10))\nax = axes.ravel()\n\ndf_cross[['Temperature_Lucca_Orto_Botanico', 'Temperature_Monte_Serra_lag2']].plot(ax = ax[0], lw = 2, \n                                                 linestyle = \"-.\")\n\n(df_cross.loc[(df_cross.index.astype(str)>='2019-01-01') & (df_cross.index.astype(str)<='2019-06-01')]\n            [['Temperature_Lucca_Orto_Botanico', 'Temperature_Monte_Serra_lag2']].plot(ax = ax[1], lw = 2, \n                                                 linestyle = \"-.\"))\n\n\n(df_cross[['Volume_CSA', 'Volume_POL_lag29']].plot(ax = ax[2], lw = 2, \n                                                 linestyle = \"-.\", sharex=True,))\n\n(df_cross[(df_cross.index.astype(str)>='2019-01-01') & (df_cross.index.astype(str)<='2019-06-01')]\n[['Volume_CSA', 'Volume_POL_lag29']].plot(ax = ax[3], lw = 2, sharex=True,\n                                                 linestyle = \"-.\"))\n\nfor j in range(4):\n    ax[j].get_legend().remove()\n\nfig.suptitle('Positive (Above) vs Negative (Below) crosscorrelation (Auser)')\n\nax[2].xaxis.set_ticks(['2017-01', '2018-01', '2019-01', '2020-01'])\nmyFmt = matplotlib.dates.DateFormatter(\"%Y-%m\")\nax[enum*3].xaxis.set_major_formatter(myFmt)\n\nax[1].legend(loc=\"upper right\", bbox_to_anchor=(1.3,1.1))\nax[3].legend(loc=\"upper right\", bbox_to_anchor=(1.3,1.1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<a id=\"spurious\"></a>\n    \n## 4. Spurious Relationships\n\nI would define a spurious correlation as the presence of a linear relationship between two variables due to either pure coincidence or the presence of a certain 3rd unseen factor ([here](https://en.wikipedia.org/wiki/Spurious_relationship) a more in depth definition).\n\nThis is clearly a **qualitative definition**: there's no clear mathematically definition of coincidence or what represente a 3rd factor, yet I find it useful in some cases.\n\n\nOne of the most known sources for spurious correlations is the https://www.tylervigen.com/spurious-correlations, where you can find plots like the following: \n\n<img src=\"https://i.imgur.com/L7nu3gj.png\" align=\"center\" style=\"width:80%\">\n\n\nOf course random correlations are not of interest, we may be more interested in spurious correlations where there is the presence of a 3rd unseen factor: \n\n<img src=\"https://i.stack.imgur.com/inojA.png\" align=\"center\" style=\"width:60%\">\n"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"correlation_redo\"></a>\n\n#### Revisiting Correlations\n\nHere I go through some of the (auto/cross)correlations found above and revisit them after disclosing the 3rd omitting factor. "},{"metadata":{},"cell_type":"markdown","source":"##### Spurious correlation with seasonality: positive and negative examples"},{"metadata":{},"cell_type":"markdown","source":"Let's go back to [this](#example) example, focusing on ***Temperature_Lucca_Orto_Botanico*** vs ***Temperature_Monte_Serra_lag2*** positive crosscorrelation."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_cross = df_dict['Aquifer_Auser']\ncols = ['Temperature_Lucca_Orto_Botanico', 'Temperature_Monte_Serra']\ndf_cross['Temperature_Monte_Serra_lag2'] = df_cross['Temperature_Monte_Serra'].shift(2)\n\ndf_cross = df_cross.loc[df_cross.Date.astype(str)>='2017-01-01'].set_index('Date')\n\nfig, ax = plt.subplots(1, 1, figsize = (11, 7))\n\ndf_cross[['Temperature_Lucca_Orto_Botanico', 'Temperature_Monte_Serra_lag2']].plot(ax = ax, lw = 2, \n                                                 linestyle = \"-.\")\n\n\n#ax.get_legend().remove()\n\nfig.suptitle('Pearson Correlation: {}'.format(round(crosscorr(df_cross['Temperature_Lucca_Orto_Botanico'],\n                                                        df_cross['Temperature_Monte_Serra_lag2']), 3)))\n\nax.xaxis.set_ticks(['2017-01', '2018-01', '2019-01', '2020-01'])\nmyFmt = matplotlib.dates.DateFormatter(\"%Y-%m\")\nax.xaxis.set_major_formatter(myFmt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's pretty clear we have a seasonality pattern here, let's check it with `statsmodels.tsa.seasonal.seasonal_decompose`\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\n\ndf_cross.index = pd.to_datetime(df_cross.index)\n\nts_1 = seasonal_decompose(df_cross['Temperature_Lucca_Orto_Botanico'], period =365)\nts_2 = seasonal_decompose(df_cross['Temperature_Monte_Serra_lag2'], period =365)\n\nfig, axes = plt.subplots(2, 1, figsize = (11, 7))\nax = axes.ravel()\ndf_dec_1 = pd.DataFrame(pd.concat([ts_1.trend, ts_1.seasonal, ts_1.resid], axis = 1))\ndf_dec_1 = (df_dec_1.loc[(df_dec_1.index.astype(str)>='2017-07-01') & (df_dec_1.index.astype(str)<='2020-01-01')])\n(df_dec_1.plot( ax = ax[0], linewidth = 2, linestyle = \"-.\"))\nax[0].title.set_text('Decomposition of Temperature_Lucca_Orto_Botanico time series')\n\ndf_dec_2 = pd.DataFrame(pd.concat([ts_2.trend, ts_2.seasonal, ts_2.resid], axis = 1))\ndf_dec_2 = (df_dec_2.loc[(df_dec_2.index.astype(str)>='2017-07-01') & (df_dec_2.index.astype(str)<='2020-01-01')])\n(df_dec_2.plot( ax = ax[1], linewidth = 2, linestyle = \"-.\", sharex = True))\nax[1].title.set_text('Decomposition of Temperature_Monte_Serra_lag2 time series')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can definitely see a seasonal component dictating both time series behaviours! Let's see the correlation of both with both their seasonal components. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from scipy.stats import pearsonr\nts1_season1 = round(pearsonr(ts_1.observed, ts_1.seasonal)[0], 3)\nts1_season2 = round(pearsonr(ts_1.observed, ts_2.seasonal)[0], 3)\nts2_season2 = round(pearsonr(ts_2.observed, ts_2.seasonal)[0], 3)\nts2_season1 = round(pearsonr(ts_2.observed, ts_1.seasonal)[0], 3)\nseason2_season1 = round(pearsonr(ts_2.seasonal, ts_1.seasonal)[0], 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<table style=\"width:100%\" align=\"center\">\n  <tr>\n    <th>Time Series 1</th>\n    <th>Time Series 2</th>\n    <th>Correlation</th>\n  </tr>\n  <tr>\n    <td>Temperature_Lucca_Orto_Botanico</td>\n    <td>Temperature_Lucca_Orto_Botanico Seasonal Component</td>\n    <td>0.94</td>\n  </tr>\n  <tr>\n    <td>Temperature_Lucca_Orto_Botanico</td>\n    <td>Temperature_Monte_Serra_lag2 Seasonal Component</td>\n    <td>0.88</td>\n  </tr>\n  <tr>\n    <td>Temperature_Monte_Serra_lag2</td>\n    <td>Temperature_Monte_Serra_lag2 Seasonal Component</td>\n    <td>0.89</td>\n  </tr>\n  <tr>\n    <td>Temperature_Monte_Serra_lag2</td>\n    <td>Temperature_Lucca_Orto_Botanico Seasonal Component</td>\n    <td>0.88</td>\n  </tr>\n  <tr>\n    <td>Temperature_Lucca_Orto_Botanico Seasonal Component</td>\n    <td>Temperature_Monte_Serra_lag2 Seasonal Component</td>\n    <td>0.94</td>\n  </tr>\n</table>"},{"metadata":{},"cell_type":"markdown","source":"As we can see there's a strong correlation with the seasonal component for each of the time series. \n\nAlready here we could say that the **$3^{rd}$** omitting factor is definitely **seasonality**. To definitely check this we could see how residuals+trend correlate between the two time series (this is an intuition of mine which could definitely be wrong: a more proper method would be to make the 2 time series stationary and compare them then). "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"ts1_no_season = (ts_1.resid + ts_1.trend).dropna()\nts2_no_season = (ts_2.resid + ts_2.trend).dropna()\n\ndf_no_season = pd.concat([ts1_no_season, ts2_no_season], axis = 1)\ndf_no_season.columns = ['Temperature_Lucca_Orto_Botanico', 'Temperature_Monte_Serra_lag2']\n\nfig, ax = plt.subplots(1, 1, figsize = (11, 7))\n\ndf_no_season[['Temperature_Lucca_Orto_Botanico', 'Temperature_Monte_Serra_lag2']].plot(ax = ax, lw = 2, \n                                                 linestyle = \"--\")\n\n\n#ax.get_legend().remove()\n\nfig.suptitle('Pearson Correlation after dropping seasonality (omitting factor): {}'.format(round(crosscorr(df_no_season['Temperature_Lucca_Orto_Botanico'],\n                                                        df_no_season['Temperature_Monte_Serra_lag2']), 3)))\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from scipy.stats import pearsonr\nts1_no_season = (ts_1.resid + ts_1.trend).dropna()\nts2_no_season = (ts_2.resid + ts_2.trend).dropna()\n\nts1_ts2_noseason = round(pearsonr(ts1_no_season, ts2_no_season)[0], 3)\nprint(ts1_ts2_noseason)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<table style=\"width:100%\" align=\"center\">\n  <tr>\n    <th>Time Series 1</th>\n    <th>Time Series 2</th>\n    <th>Correlation</th>\n  </tr>\n  <tr>\n    <td>Temperature_Lucca_Orto_Botanico no Seasonal Component</td>\n    <td>Temperature_Monte_Serra_lag2 no Seasonal Component</td>\n    <td>0.497</td>\n  </tr>"},{"metadata":{},"cell_type":"markdown","source":"Correlation drops to 0.497 with no seasonal component, definitely less than the original time series."},{"metadata":{},"cell_type":"markdown","source":"[return to top](#top)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}