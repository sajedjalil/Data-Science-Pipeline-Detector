{"cells":[{"metadata":{},"cell_type":"markdown","source":"# overview of aqua systems and first interpretation"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2020-12-16T22:54:25.084576Z","iopub.status.busy":"2020-12-16T22:54:25.083904Z","iopub.status.idle":"2020-12-16T22:54:25.09501Z","shell.execute_reply":"2020-12-16T22:54:25.095651Z"},"papermill":{"duration":0.030359,"end_time":"2020-12-16T22:54:25.095839","exception":false,"start_time":"2020-12-16T22:54:25.06548","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2020-12-16T22:54:25.132718Z","iopub.status.busy":"2020-12-16T22:54:25.132053Z","iopub.status.idle":"2020-12-16T22:54:25.391323Z","shell.execute_reply":"2020-12-16T22:54:25.390697Z"},"papermill":{"duration":0.281913,"end_time":"2020-12-16T22:54:25.391433","exception":false,"start_time":"2020-12-16T22:54:25.10952","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Import\naq_auser = pd.read_csv(\"../input/acea-water-prediction/Aquifer_Auser.csv\")\naq_doganella = pd.read_csv(\"../input/acea-water-prediction/Aquifer_Doganella.csv\")\naq_luco = pd.read_csv(\"../input/acea-water-prediction/Aquifer_Luco.csv\")\naq_petrignago = pd.read_csv(\"../input/acea-water-prediction/Aquifer_Petrignano.csv\")\nlakeBilancino = pd.read_csv(\"../input/acea-water-prediction/Lake_Bilancino.csv\")\nriverArno = pd.read_csv(\"../input/acea-water-prediction/River_Arno.csv\")\nws_Amiata = pd.read_csv(\"../input/acea-water-prediction/Water_Spring_Amiata.csv\")\nws_Lupa = pd.read_csv(\"../input/acea-water-prediction/Water_Spring_Lupa.csv\")\nws_Madonna = pd.read_csv(\"../input/acea-water-prediction/Water_Spring_Madonna_di_Canneto.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-16T22:54:25.435848Z","iopub.status.busy":"2020-12-16T22:54:25.427113Z","iopub.status.idle":"2020-12-16T22:54:25.475717Z","shell.execute_reply":"2020-12-16T22:54:25.475132Z"},"papermill":{"duration":0.071161,"end_time":"2020-12-16T22:54:25.475846","exception":false,"start_time":"2020-12-16T22:54:25.404685","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"riverArno","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.013956,"end_time":"2020-12-16T22:54:25.50429","exception":false,"start_time":"2020-12-16T22:54:25.490334","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# prepare data\n- split date\n- copy data with timelag 3 days"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-16T22:54:25.583159Z","iopub.status.busy":"2020-12-16T22:54:25.58213Z","iopub.status.idle":"2020-12-16T22:54:25.674757Z","shell.execute_reply":"2020-12-16T22:54:25.675508Z"},"papermill":{"duration":0.157137,"end_time":"2020-12-16T22:54:25.675694","exception":false,"start_time":"2020-12-16T22:54:25.518557","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def datumsplit(data,datumvar,isnafilter,diff,pref):\n\n\n    #drop empty data, loosing that info leaves us with the available data\n    data=data[data[isnafilter].isna()==False].reset_index()    \n    #split date in components for regression, this way one can discover seasonal, week, year effects\n    column_1=pd.to_datetime(data[datumvar],format='%d/%m/%Y')\n    temp=pd.DataFrame({pref+\"year\": column_1.dt.year,\n                  pref+\"month\": column_1.dt.month,\n                  pref+\"day\": column_1.dt.day,\n                  #\"hour\": column_1.dt.hour,\n                  #pref+\"dayofyear\": column_1.dt.dayofyear,\n                  pref+\"week\": column_1.dt.week,\n                  pref+\"weekofyear\": column_1.dt.weekofyear,\n                  pref+\"dayofweek\": column_1.dt.dayofweek,\n                  pref+\"weekday\": column_1.dt.weekday,\n                  pref+\"quarter\": column_1.dt.quarter,\n                 })\n\n\n    #drop date since you cannot regress a date\n    datalabel=data[isnafilter]\n    data=data.drop([datumvar,isnafilter],axis=1)\n    #move the information of (diff=1) yesterday one day foreward and add  the columns\n    data2=data[diff:]\n    for ki in data.columns:\n        data2[ki+'_lag'+str(diff)]=data[:-diff][ki].values\n    #add splitted date\n    for ki in temp.columns:\n        data2[ki]=temp[diff:][ki].values  \n    #add moving average to dataset (moving average filters noise)\n    temp=data.rolling(diff*5,).mean()\n    for ki in data.columns:\n        data2[ki+'_MA5*'+str(diff)]=temp[:-diff][ki].values\n    data2[isnafilter]=datalabel[diff:]\n    print(data.shape,data2.shape)\n    return data2\n\ntrain=datumsplit(riverArno,'Date','Hydrometry_Nave_di_Rosano',1,'ext_')\ntrain","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.014805,"end_time":"2020-12-16T22:54:25.706525","exception":false,"start_time":"2020-12-16T22:54:25.69172","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# ARIMA Timeseries analysis\n- there is a month effect\n- there is a 1-5 day lag effect autoc\n- autocorrelation is nonstationary, needs difference per day to do an ARIMA forecast\n- indeed the difference of the seasonal gives the better forecast \n- a week effect in the forecast seems to me rather intuitive, since the weekly structure of our habits, but the monthly effect is for me rather counterintuitive. I wonder what habit could be monthly...\n- ARIMA does not use any other information than the production itself, and just uses the 'monthly' and '5day' peak effets to forecast the consumption until 14days ahead. Its not such a strong forecaster but as such you can always use it\n"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-16T22:54:25.74598Z","iopub.status.busy":"2020-12-16T22:54:25.745108Z","iopub.status.idle":"2020-12-16T22:54:27.465022Z","shell.execute_reply":"2020-12-16T22:54:27.465539Z"},"papermill":{"duration":1.743976,"end_time":"2020-12-16T22:54:27.465687","exception":false,"start_time":"2020-12-16T22:54:25.721711","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"    from statsmodels.tsa.statespace.sarimax import SARIMAX\n    from statsmodels.graphics.tsaplots import plot_acf\n    from statsmodels.graphics.tsaplots import plot_pacf\n    from statsmodels.tsa.stattools import adfuller\n    from sklearn.metrics import mean_squared_error\n    from math import sqrt\n    \n\n    label='Hydrometry_Nave_di_Rosano'\n\n    isnafilter='Rainfall_Le_Croci'\n    # Find  by Date/Time not na\n    train=riverArno[riverArno[isnafilter].isna()==False]\n\n    day_df = train[['Date',label]]\n    # setting Date/Time as index\n    day_df.index = pd.DatetimeIndex(day_df.Date)\n    # Resampling to daily trips\n    day_df = day_df.resample('1D').apply(np.sum)\n\n    day_df.plot()\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-16T22:54:27.502187Z","iopub.status.busy":"2020-12-16T22:54:27.501194Z","iopub.status.idle":"2020-12-16T22:54:28.43861Z","shell.execute_reply":"2020-12-16T22:54:28.439137Z"},"papermill":{"duration":0.957261,"end_time":"2020-12-16T22:54:28.439285","exception":false,"start_time":"2020-12-16T22:54:27.482024","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"##Checking trend and autocorrelation\ndef initial_plots(time_series, num_lag):\n    import matplotlib.pyplot as plt\n\n    #Original timeseries plot\n    plt.figure(1)\n    plt.plot(time_series)\n    plt.title('Original data across time')\n    plt.figure(2)\n    plot_acf(time_series, lags = num_lag)\n    plt.title('Autocorrelation plot')\n    plot_pacf(time_series, lags = num_lag)\n    plt.title('Partial autocorrelation plot')\n    \n    plt.show()\n\n    \n#Augmented Dickey-Fuller test for stationarity\n#checking p-value\nprint('p-value: {}'.format(adfuller(day_df)[1]))\n\n#plotting\ninitial_plots(day_df, 45)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.019228,"end_time":"2020-12-16T22:54:28.478685","exception":false,"start_time":"2020-12-16T22:54:28.459457","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#storing differenced series\ndiff_series = day_df.diff(periods=1)\n\n#Augmented Dickey-Fuller test for stationarity\n#checking p-value\nprint('p-value: {}'.format(adfuller(diff_series.dropna())[1]))\n\n\ninitial_plots(diff_series.dropna(), 45)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"# lets try an LGBM regression\n\n- its a robust and fast decisiontree regressor\n- that can find a regression fit with unbalanced data, omitting the outliers\n-\n"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-16T22:54:28.521773Z","iopub.status.busy":"2020-12-16T22:54:28.52079Z","iopub.status.idle":"2020-12-16T22:54:28.621751Z","shell.execute_reply":"2020-12-16T22:54:28.622284Z"},"papermill":{"duration":0.124201,"end_time":"2020-12-16T22:54:28.62242","exception":false,"start_time":"2020-12-16T22:54:28.498219","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# check if you don't create a selffulfilling prophecy\n- this means you have to take care that you don't forecast with data you could impossibly know at the day 5, you can not know what is the temperature of day 6, and if the moving average has even a slight knowledge of the day ahaid, this narrows the error of the forecast, but  your forecast is wrong..\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"    if True:\n        !pip install dabl\n        import dabl\n        #data = dabl.clean(train, verbose=1)\n        #dabl.plot(data.drop('Hydrometry_Nave_di_Rosano',axis=1), data['Hydrometry_Nave_di_Rosano'])\n        #model = dabl.SimpleClassifier(random_state=0).fit(data.fillna(0), target_col=\"Hydrometry_Nave_di_Rosano\")","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-16T22:54:28.668421Z","iopub.status.busy":"2020-12-16T22:54:28.667423Z","iopub.status.idle":"2020-12-16T22:54:28.672133Z","shell.execute_reply":"2020-12-16T22:54:28.671472Z"},"papermill":{"duration":0.028938,"end_time":"2020-12-16T22:54:28.672242","exception":false,"start_time":"2020-12-16T22:54:28.643304","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#train[['index','Rainfall_Le_Croci','Rainfall_Le_Croci_1']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# lets create an aquaforecast function\n- its a forecast that works as long as you have all variables available\n- althoug beware : temperature, waterlevels can mere be an estimate of historical values so maybe we need to drop all those values and fill in historical values for everything"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-16T22:54:28.834895Z","iopub.status.busy":"2020-12-16T22:54:28.813565Z","iopub.status.idle":"2020-12-16T22:54:32.615318Z","shell.execute_reply":"2020-12-16T22:54:32.614256Z"},"papermill":{"duration":3.837933,"end_time":"2020-12-16T22:54:32.615446","exception":false,"start_time":"2020-12-16T22:54:28.777513","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"\ndef aquaforecast(aqua,label,indxvar,dropvalue,diff,dayahead):\n    # prepare data with datesplit function see above\n    train=datumsplit(aqua,'Date',dropvalue,diff,'ext_')\n    # split train test with number of day ahead\n    if True:\n        #!pip install dabl\n        #import dabl\n        train = dabl.clean(train, verbose=1)\n        dabl.plot(train.drop(label,axis=1), train[label])\n        #model = dabl.SimpleClassifier(random_state=0).fit(data.fillna(0), target_col=\"Hydrometry_Nave_di_Rosano\") \n        \n    test=train[-dayahead:]\n    train=train[:-dayahead] \n    print('Availbable columns to regress',aqua.columns)\n    param = {'num_leaves': 200,   # increasing or decreasing can influence the decisiontree and the result, there is optimum possible beware overfit\n             'min_data_in_leaf': 50, # increasing or decreasing can influence the decisiontree and the result, there is optimum possible beware overfit\n             'objective':'regression', #regress\n             'max_depth': -1,\n             'learning_rate': 0.1,\n             \"boosting\": \"gbdt\", #gbdt,dart  #gradient boost \n             \"feature_fraction\": 0.8,\n             \"bagging_freq\": 1,\n             \"bagging_fraction\": 0.8 ,\n             \"bagging_seed\": 11,\n             \"metric\": 'mse', #'rmse',#auc  #use mean square error\n             \"num_classes\": 1,#classific\n             \"lambda_l1\": 0.1,\n\n             \"random_state\": 133,\n             \"verbosity\": -1}\n\n    max_iter = 10\n    from sklearn import metrics\n    import lightgbm as lgb\n    from sklearn.model_selection import KFold\n    import warnings\n    import gc\n    import time\n    import sys\n    import datetime\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    from tqdm import tqdm\n    from sklearn.metrics import mean_squared_error\n    from sklearn.metrics import mean_squared_error\n\n    target = train[[label]]\n    # drop column to forecast\n    train=train.drop([label],axis=1)\n\n    folds = KFold(n_splits=5, shuffle=True, random_state=15)\n    oof = np.zeros((len(train)))\n\n    features = [c for c in train.columns if c not in ['ID']]\n    predictions = np.zeros(len(test))\n    start = time.time()\n    feature_importance_df = pd.DataFrame()\n    start_time= time.time()\n    score = [0 for _ in range(folds.n_splits)]\n    # folding takes care of 'unbalanced data', is always a strong idea to use\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n        print(\"fold nÂ°{}\".format(fold_))\n        trn_data = lgb.Dataset(train.iloc[trn_idx][features],\n                               label=target.iloc[trn_idx]#.fillna('')#,\n                               \n                              )\n        val_data = lgb.Dataset(train.iloc[val_idx][features],\n                               label=target.iloc[val_idx]#.fillna('')#,\n                               \n                              )\n        print(train.shape,target.shape,target.iloc[val_idx].shape,train.iloc[val_idx][features].shape,val_data,trn_data)\n        num_round = 20000\n        clf = lgb.train(param,\n                        trn_data,\n                        num_round,\n                        valid_sets = [trn_data, val_data],\n                        verbose_eval=1000,\n                        early_stopping_rounds = 100)\n\n        oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = features\n        fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n        fold_importance_df[\"fold\"] = fold_ + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n        # we perform predictions by chunks\n        initial_idx = 0\n        chunk_size = 1000000\n        current_pred = np.zeros(len(test))\n        while initial_idx < test.shape[0]:\n            final_idx = min(initial_idx + chunk_size, test.shape[0])\n            idx = range(initial_idx, final_idx)\n            current_pred[idx] = clf.predict(test.iloc[idx][features], num_iteration=clf.best_iteration)\n            initial_idx = final_idx\n        predictions += current_pred / min(folds.n_splits, max_iter)\n        plt.scatter(x=0,y=0)\n        plt.scatter(x=test[label], y=current_pred, marker='.', alpha=1,c=np.abs(test[label].values-current_pred))\n        plt.scatter(x=[np.mean(test[label])], y=[np.mean(current_pred)], marker='o', color='red')\n        plt.xlabel('Real test'); plt.ylabel('Pred. test')\n        plt.show()\n\n        print(\"time elapsed: {:<5.2}s\".format((time.time() - start_time) / 3600))\n        #score[fold_] = metrics.roc_auc_score(target.iloc[val_idx], oof[val_idx])\n        #mse = mean_squared_error(target.iloc[val_idx], oof[val_idx])\n        #print(mse)\n\n        if fold_ == max_iter - 1: break\n\n    if (folds.n_splits == max_iter):\n        print(\"CV score: {:<8.5f}\".format(metrics.roc_auc_score(target, oof)))\n    else:\n         print(\"CV score: {:<8.5f}\".format(sum(score) / max_iter))\n\n    sub_df = pd.DataFrame({indxvar: test[indxvar].values})\n    sub_df[label] =predictions\n    sub_df[:10]\n    sub_df.to_csv(\"submit.csv\", index=False)\n    test[label],predictions\n    mse=mean_squared_error(predictions, test[label].fillna(0).values)\n    print( 'mse',mse, 'rmse',sqrt(mse) )\n    \n    lgb.plot_importance(\n        clf, \n        max_num_features=20, \n        importance_type='gain', \n        figsize=(12,8));\n    return","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# riverbed Arno\n\n\nthis system is autocorrelated as all forecasts, lag en moving average is always the best forecast, but has as drawback that you feed in during the 400day period information you can not know on forehand. You could know the forecasted data though, which gives a huge autocorrelation. This means the autocorrelation means that the best forecast for next day or even this week is perfectly narrowly possible, but 400days ahead you can't know the fluctuations in the system on forehand. So you simply expect the outflow to be what is was the day before.\n\n\nfirst regression is with dropout of Hydrometry_Nave_di_Rosano label Moving average and Lagged information, what gives a rather good forecast of the riversystem. The best forecast is the 'week effet', temperature, rainfall Moving Average and month effect as expected.\nBut when one adds the 'lagged information' of the riverbed, so you can use this forecast for week-ahead but not much further\n"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-16T22:54:32.679185Z","iopub.status.busy":"2020-12-16T22:54:32.678442Z","iopub.status.idle":"2020-12-16T22:54:32.686338Z","shell.execute_reply":"2020-12-16T22:54:32.685784Z"},"papermill":{"duration":0.043455,"end_time":"2020-12-16T22:54:32.686454","exception":false,"start_time":"2020-12-16T22:54:32.642999","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"aquaforecast(riverArno,'Hydrometry_Nave_di_Rosano' ,\"index\",'Hydrometry_Nave_di_Rosano',1,400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aquaforecast(riverArno,'Hydrometry_Nave_di_Rosano' ,\"index\",'Rainfall_Le_Croci',1,400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# aqua petrignago\n\nhere there are two variables Depth to groudwater p24,p25\nevidently again you drop out the other variable to get a forecast without autocorrelation effect\nthe test / prediction graph shows the great variability of the forecast. Although this fuzziness, the forecast is still good\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"aquaforecast(aq_petrignago.drop('Depth_to_Groundwater_P25',axis=1),'Depth_to_Groundwater_P24' ,\"index\",'Depth_to_Groundwater_P24',1,400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aquaforecast(aq_petrignago,'Depth_to_Groundwater_P24' ,\"index\",'Depth_to_Groundwater_P24',1,400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Aqua Lupa\n- a flat prediction, there seems no information avaibable other then 'rainfall...\n- here i could try to add weather and rain data from 'italy' in general to see if the errorate becomes smaller, for the time being its an unsolved problem\n- exept if we use the 'autocorrelation power by forgetting to omit the P25, the forecast has higher forecast error, although the parameter is most influential"},{"metadata":{"trusted":true},"cell_type":"code","source":"aquaforecast(ws_Lupa,'Flow_Rate_Lupa' ,\"index\",'Flow_Rate_Lupa',1,400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ws_Lupa['Dummy']=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aquaforecast(ws_Lupa,'Flow_Rate_Lupa' ,\"index\",'Dummy',1,400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# lake Bilancino\n\n- again a relative flat and uncorrelated forecast, if we drop the Lake level\n- keeping the variable Lake lavel in the prediction, narrows the error. You could keep this variable to forecast, since a lake level is something very visible, and you can trust any prediction of that lake level stays within a deviation limit (fe 1meter) of the forecast. \n- imho the flatness means there is no overdraining of the lake, and the source is not stressed to his limits"},{"metadata":{"trusted":true},"cell_type":"code","source":"aquaforecast(lakeBilancino.drop('Lake_Level',axis=1),'Flow_Rate' ,\"index\",'Flow_Rate',1,400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aquaforecast(lakeBilancino,'Flow_Rate' ,\"index\",'Flow_Rate',1,400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# water system Madonna\n\n- again an unpredictable system..., this means imho there is not yet a limit in that system"},{"metadata":{"trusted":true},"cell_type":"code","source":"aquaforecast(ws_Madonna,'Flow_Rate_Madonna_di_Canneto' ,\"index\",'Flow_Rate_Madonna_di_Canneto',1,400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ws_Madonna['Dummy']=1\naquaforecast(ws_Madonna,'Flow_Rate_Madonna_di_Canneto' ,\"index\",'Dummy',1,400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Aqua Auser\n- a nice forecast is possible\n- beware for the outlayers like 'zero depth' values...\n- its not clear if we forecast SAL depth, if the PAG is a variable we should drop too, lets drop all depth levels, the forecast remains narrow\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"aquaforecast(aq_auser,'Depth_to_Groundwater_SAL' ,\"index\",'Depth_to_Groundwater_SAL',1,400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aquaforecast(aq_auser.drop(['Depth_to_Groundwater_LT2', 'Depth_to_Groundwater_PAG',\n       'Depth_to_Groundwater_CoS', 'Depth_to_Groundwater_DIEC'],axis=1),'Depth_to_Groundwater_SAL' ,\"index\",'Depth_to_Groundwater_SAL',1,400)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}