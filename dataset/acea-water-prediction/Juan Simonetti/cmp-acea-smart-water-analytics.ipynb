{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Important note\nThis notebook will only cover **River Arno**, leaving the other 8 waterbodies (datasets) out of scope."},{"metadata":{},"cell_type":"markdown","source":"# Challenge overview\n\nThe Acea Group is one of the leading Italian multiutility operators. Listed on the Italian Stock Exchange since 1999, the company manages and develops water and electricity networks and environmental services. Acea is the foremost Italian operator in the water services sector supplying 9 million inhabitants in Lazio, Tuscany, Umbria, Molise, Campania.\n\nThis competition uses nine different datasets, completely independent and not linked to each other. Each dataset can represent a different kind of waterbody. As each waterbody is different from the other, the related features as well are different from each other. So, if for instance we consider a water spring we notice that its features are different from the lakeâ€™s one. This is correct and reflects the behavior and characteristics of each waterbody. The Acea Group deals with four different type of waterbodies: water spring (for which three datasets are provided), lake (for which a dataset is provided), river (for which a dataset is provided) and aquifers (for which four datasets are provided).\n\nThe desired outcome of this challenge is a notebook that can generate four mathematical models, one for each category of waterbody (acquifers, water springs, river, lake) that might be applicable to each single waterbody.\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F6195295%2Fcca952eecc1e49c54317daf97ca2cca7%2FAcea-Input.png?generation=1606932492951317&alt=media)\n\nEach waterbody has its own different features to be predicted. The table below shows the expected feature to forecast for each waterbody.\n\n![](https://storage.cloud.google.com/kaggle-media/competitions/Acea/Screen%20Shot%202020-12-02%20at%2012.40.17%20PM.png)"},{"metadata":{},"cell_type":"markdown","source":"# Reading files"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sns\nplt.rcParams['figure.dpi'] = 300\nimport matplotlib.dates as mdates\nimport missingno as msno\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\n\n# ignoring warnings\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfiles = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n        if '.csv' in filename:\n            files +=list([filename])\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\naq_auser = pd.read_csv(\"../input/acea-water-prediction/Aquifer_Auser.csv\", index_col = 'Date')\naq_doganella = pd.read_csv(\"../input/acea-water-prediction/Aquifer_Doganella.csv\", index_col = 'Date')\naq_luco = pd.read_csv(\"../input/acea-water-prediction/Aquifer_Luco.csv\", index_col = 'Date')\naq_petrignano = pd.read_csv(\"../input/acea-water-prediction/Aquifer_Petrignano.csv\", index_col = 'Date')\nlk_bilancino = pd.read_csv(\"../input/acea-water-prediction/Lake_Bilancino.csv\", index_col = 'Date')\nrv_arno = pd.read_csv(\"../input/acea-water-prediction/River_Arno.csv\", index_col = 'Date')\nws_amiata = pd.read_csv(\"../input/acea-water-prediction/Water_Spring_Amiata.csv\", index_col = 'Date')\nws_lupa = pd.read_csv(\"../input/acea-water-prediction/Water_Spring_Lupa.csv\", index_col = 'Date')\nws_madonna = pd.read_csv(\"../input/acea-water-prediction/Water_Spring_Madonna_di_Canneto.csv\", index_col = 'Date')\n\ndatasets=[aq_auser,aq_doganella,aq_luco,aq_petrignano,lk_bilancino,rv_arno,ws_amiata,ws_lupa,ws_madonna]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Datasets overview"},{"metadata":{},"cell_type":"markdown","source":"Below table is just to show all the datasets, waterbody types, rows, and columns of the tables. As stated before, I'll only focus on **River Arno**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a brief dataframe to compare the qty of rows and cols of each file.\ndatasets_df = pd.DataFrame(columns=['File_Name'], data=files)\ndatasets_df['Waterbody_type'] = datasets_df.File_Name.apply(lambda x: x.split('_')[0])\ndatasets_df['Qty_Rows'] = datasets_df.File_Name.apply(lambda x: pd.read_csv(f'../input/acea-water-prediction/{x}').shape[0])\ndatasets_df['Qty_Cols'] = datasets_df.File_Name.apply(lambda x: pd.read_csv(f'../input/acea-water-prediction/{x}').shape[1])\ndatasets_df = datasets_df.replace('Water','Water_Spring')\ndatasets_df = datasets_df.sort_values(by=['Waterbody_type','Qty_Rows'], ascending=[True,False]).reset_index(drop=True)\n#datasets_df.style.bar(subset=['Qty_Rows','Qty_Cols'], color='#118DFF')\ndatasets_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Stating each waterbody target\nauser_targets = ['Depth_to_Groundwater_LT2', 'Depth_to_Groundwater_SAL', 'Depth_to_Groundwater_CoS']\ndoganella_targets = ['Depth_to_Groundwater_Pozzo_1','Depth_to_Groundwater_Pozzo_2','Depth_to_Groundwater_Pozzo_3',\n                     'Depth_to_Groundwater_Pozzo_4','Depth_to_Groundwater_Pozzo_5','Depth_to_Groundwater_Pozzo_6',\n                     'Depth_to_Groundwater_Pozzo_7','Depth_to_Groundwater_Pozzo_8','Depth_to_Groundwater_Pozzo_9']\nluco_targets = ['Depth_to_Groundwater_Podere_Casetta']\npetrignano_targets = ['Depth_to_Groundwater_P24', 'Depth_to_Groundwater_P25']\nbilancino_targets = ['Lake_Level', 'Flow_Rate']\narno_targets = ['Hydrometry_Nave_di_Rosano']\namiata_targets = ['Flow_Rate_Bugnano','Flow_Rate_Arbure', \n                  'Flow_Rate_Ermicciolo','Flow_Rate_Galleria_Alta']\nlupa_targets = ['Flow_Rate_Lupa']\nmadonna_targets = ['Flow_Rate_Madonna_di_Canneto']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defining some functions\ndef df_relinfo(df, target_var=[]):\n    x = pd.DataFrame(df.isna().sum().apply(lambda x: x/df.shape[0])).reset_index().rename(columns={'index':'Feature',0:'%Na'})\n    x['Na_qty'] = df.isna().sum().tolist()\n    x['Variable'] = x.Feature.apply(lambda x: 'Target' if x in target_var else 'Predictor')\n    return x.sort_values(by='%Na', ascending = False).reset_index(drop=True).style.bar(subset = ['%Na'], color = '#118DFF')\n\ndef corr_plot(data, top_visible=False, right_visible=False, bottom_visible=True, left_visible=False, ylabel=None, figsize=(15,11), axis_grid='y'):\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title('Correlations (Pearson)', size=15, fontweight='bold')\n    mask = np.triu(np.ones_like(data.corr(), dtype=bool))\n    sns.heatmap(round(data.corr(), 2), mask=mask, cmap='viridis', annot=True)\n    plt.show()\n    \ndef line_plot(data, y, title, color, top_visible=False, right_visible=False, bottom_visible=True, left_visible=False,\n             ylabel=None, figsize=(10,4), axis_grid='y'):\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title, size=15, fontweight='bold')\n    \n    for i in ['top','right','bottom','left']:\n        ax.spines[i].set_color('black')\n    #    ax.spines[i].set_visible(i+'_visible')\n    \n    ax.spines['top'].set_visible(top_visible)\n    ax.spines['right'].set_visible(right_visible)\n    ax.spines['bottom'].set_visible(bottom_visible)\n    ax.spines['left'].set_visible(left_visible)\n    \n    sns.lineplot(x=range(len(data[y])), y=data[y], dashes=False, color=color, linewidth=.5)\n    ax.xaxis.set_major_locator(plt.MaxNLocator(20))\n    \n    ax.set_xticks([])\n    plt.xticks(rotation=90)\n    plt.xlabel('')\n    plt.ylabel(ylabel)\n    ax.grid(axis=axis_grid, alpha=0.9, linestyle='--')\n    plt.show()\n\ndef columns_viz(data, color):\n    for i in range(len(data.columns)):\n        line_plot(data=data, y=data.columns[i], color=color, \n                 title='{} dynamics'.format(data.columns[i]),\n                  bottom_visible=False, figsize=(10,2))\n        \n# some more helper functions\ndef add_month(df):\n    \"\"\"\n    Convert date to a date object, then create the month column\n    \"\"\"\n    df = df.reset_index()\n    df['Date'] = pd.to_datetime(df['Date'])\n    df = df.sort_values(by = 'Date')\n    \n    df['Month'] = pd.DatetimeIndex(df['Date']).month\n    return df\n\ndef add_year(df):\n    \"\"\"\n    add a column for the year\n    \"\"\"\n    df['Year'] = pd.DatetimeIndex(df['Date']).year\n    return df\n\ndef add_seasons(df):\n    \"\"\"\n    This function will add the season (winter, spring, summer, autumn) based on the month\n    Spring: March, April, May\n    Summer: June, July, August\n    Autumn: September, October, November\n    Winter: December, January, February\n    \"\"\"\n    months = df['Month'].unique()\n    df['Season'] = df['Month']\n    for month in months:\n        if month in [12,1,2]:\n            df.loc[lambda df: df['Month'] == month, 'Season'] = '1_Winter'\n        elif month in [3,4,5]:\n            df.loc[lambda df: df['Month'] == month, 'Season'] = '2_Spring'\n        elif month in [6,7,8]:\n            df.loc[lambda df: df['Month'] == month, 'Season'] = '3_Summer'\n        else:\n            df.loc[lambda df: df['Month'] == month, 'Season'] = '4_Autumn'\n    return df\n\ndef do_dates(df):\n    df = add_month(df)\n    df = add_year(df)\n    df = add_seasons(df)\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# River Arno Analysis"},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The earliest date is: \\t', datasets[5].index[0])\nprint('The latest date is: \\t', datasets[5].index[-1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a quick look at the features, missing values and types of variables.\nWe can see 9 predictors with over 44% of missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_relinfo(rv_arno,arno_targets)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now plot the correlations matrix.\nWe can see high negative correlation in Hydrometry with Temperature, meaning the higher the temperature, the lower the target.\nWe can also see a set of rainfalls with lower correlation than others. Coincidentally, the ones with higher correlation are the ones with higher quantity of missing values. This may lead into problems afterwards."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_plot(datasets[5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now plot the daily dynamics for a glimpse"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_viz(datasets[5], '#FF5733')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now plot the monthly dynamics. Scales will be tweaked to make it more visual (log and *10).\nNote how rainfall data is starting in around 2003-2004 and temperature data is until 2017."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adding rainfall Sum, year, month, month_year\ndf = rv_arno[['Hydrometry_Nave_di_Rosano', 'Temperature_Firenze']].reset_index()\ndf['rainfall'] = rv_arno.iloc[:, 0:-2].sum(axis = 1).values\ndf['year'] = pd.to_datetime(df.Date).dt.year\ndf['month'] = pd.to_datetime(df.Date).dt.month\ndf['month_year'] = pd.to_datetime(df.Date).apply(lambda x: x.strftime('%Y/%m'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Monthly dynamics\nr_means = np.log(df.groupby('month_year').Hydrometry_Nave_di_Rosano.mean() * 10).reset_index()\nr_means['month_year'] = pd.to_datetime(r_means['month_year'])\n\nr_rain = np.log(df.groupby('month_year').rainfall.mean()).reset_index()\nr_rain['month_year'] = pd.to_datetime(r_rain['month_year'])\n\nr_temp = np.log(df.groupby('month_year').Temperature_Firenze.mean()).reset_index()\nr_temp['month_year'] = pd.to_datetime(r_temp['month_year'])\n\nfig, ax = plt.subplots(figsize = (15, 5))\nplt.title('Monthly dynamics (Arno River)', size = 15, fontweight = 'bold')\n          \nsns.lineplot(data = r_rain, x = 'month_year', y = 'rainfall',  \n             color = 'gray', label = 'Rainfall', alpha = 0.4)\nplt.xticks(rotation = 45)\nsns.lineplot(data = r_temp, x = 'month_year', y = 'Temperature_Firenze', \n             color = 'green', label = 'Temperature_Firenze', alpha = 0.6)\nplt.xticks(rotation = 45)\nsns.lineplot(data = r_means, x = 'month_year', y = 'Hydrometry_Nave_di_Rosano', \n             color = 'blue', label = 'Hydrometry')\nplt.xticks(rotation = 45)\n    \nfor i in ['top', 'right', 'bottom', 'left']:\n        ax.spines[i].set_visible(False)\n\nax.set_xticks(r_means.month_year[::12])\nax.set_xticklabels(range(1998, 2021, 1))\nax.set_xlabel('')\nax.set_ylabel('')\nax.grid(axis = 'y', linestyle = '--', alpha = 0.9)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now plotting the yearly dynamics"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Yearly dynamics\nr_means_y = np.log(df.groupby('year').Hydrometry_Nave_di_Rosano.mean() * 10).reset_index()\nr_rain_y = np.log(df.groupby('year').rainfall.mean()).reset_index()\nr_temp_y = np.log(df.groupby('year').Temperature_Firenze.mean()).reset_index()\n\nfig, ax = plt.subplots(figsize = (15, 5))\nplt.title('Yearly dynamics (Arno River)', size = 15, fontweight = 'bold')\n          \nsns.lineplot(data = r_rain_y, x = 'year', y = 'rainfall',  \n             color = 'gray', label = 'Rainfall', alpha = 0.4)\nplt.xticks(rotation = 45)\nsns.lineplot(data = r_temp_y, x = 'year', y = 'Temperature_Firenze', \n             color = 'green', label = 'Temperature_Firenze', alpha = 0.6)\nplt.xticks(rotation = 45)\nsns.lineplot(data = r_means_y, x = 'year', y = 'Hydrometry_Nave_di_Rosano', \n             color = 'blue', label = 'Hydrometry')\nplt.xticks(rotation = 45)\n    \nfor i in ['top', 'right', 'bottom', 'left']:\n        ax.spines[i].set_visible(False)\n\nax.set_xticks(r_means_y.year)\nax.set_xlabel('')\nax.set_ylabel('')\nax.grid(axis = 'y', linestyle = '--', alpha = 0.9)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now plot missing values from Missingno which allow us to see also the distribution of the missing values.\nWe can see that all Rainfalls started to be recorded at the same time, which was later than Temperature and Hydrometry.\nWe can also see that Temperature was not recorded until last day."},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.matrix(rv_arno)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Due to high number of missing values in values (over 44%), I will drop the following 9 Rainfalls: 'Rainfall_Vernio','Rainfall_Stia', 'Rainfall_Consuma', 'Rainfall_Incisa', 'Rainfall_Montevarchi', 'Rainfall_S_Savino', 'Rainfall_Laterina', 'Rainfall_Bibbiena', 'Rainfall_Camaldoli'.\n\nDo not forget that these Rainfalls have a higher correlation that the ones that will remain.\n\nI will now group create Seasonal columns particularly to group each of the rainfalls and try to fill the missing values with the mean within the season. I believe this approach is better than the mean of the year. The same approach can be used for the temperature. \nMoreover, a column Rainfall_Mean."},{"metadata":{"trusted":true},"cell_type":"code","source":"rv_arno_wrk = do_dates(rv_arno).drop(columns=['Rainfall_Vernio','Rainfall_Stia','Rainfall_Consuma', \\\n    'Rainfall_Incisa', 'Rainfall_Montevarchi', 'Rainfall_S_Savino', 'Rainfall_Laterina', 'Rainfall_Bibbiena', 'Rainfall_Camaldoli'])\nrv_arno_wrk['Rainfall_Mean'] = rv_arno_wrk.iloc[:, 1:6].mean(axis = 1).values\nrv_arno_wrk","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this plot we can see that Rainfalls varies considerably through seasons."},{"metadata":{"trusted":true},"cell_type":"code","source":"test = rv_arno_wrk.groupby('Season').mean().drop(columns=['Year','Month','Temperature_Firenze','Hydrometry_Nave_di_Rosano'])\n\nfig, ax = plt.subplots(figsize = (15, 5))\nsns.lineplot(data=test, dashes=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I decided to drop all values before 2004 because of the quantity of missing values (might not be the best approach but I prefer this one rather than replace NaNs in each rainfall before 2003 with data from after 2004).\\\nI will also replace few missing values in Hydrometry with ffill since they are only three and not consecutive. Also the type of variable suggest me that extending the measure for one more day might be better than using some average. Quantity of records impacted: 3.\\\nI found 187 rows with Hydrometry = 0 which sounds like a data collection issue for me. I will also replace this values with the ffill method."},{"metadata":{"trusted":true},"cell_type":"code","source":"# I will delete data before 2004 because its having all missing values on the Rainfall variables.\n# I will fill the NaN values in Target with ffill. Affecting only 3 rows.\n# I will replace 0 values in Target with ffill as well. Affecting 187 rows, but mainly not consecutive.\nrv_arno_wrk = rv_arno_wrk[rv_arno_wrk.Date>'2004-01-01']\nrv_arno_wrk['Hydrometry_Nave_di_Rosano'].fillna(method='ffill', inplace=True)\nrv_arno_wrk['Hydrometry_Nave_di_Rosano'].replace(to_replace=0, method='ffill', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now dive into Temperature's missing values. There are 1082 missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"rv_arno_wrk['Temperature_Firenze'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plot below, we can see a high correlation between the temperature and the month (as we can intuitively expect). Based on this, I will replace all missing values in the temperatures with the mean of temperatures of that month, across all years. "},{"metadata":{"trusted":true},"cell_type":"code","source":"rv_arno_tmp = rv_arno_wrk.groupby(['Year','Month']).mean().drop(columns=['Hydrometry_Nave_di_Rosano','Rainfall_Le_Croci', 'Rainfall_Cavallina', 'Rainfall_S_Agata',\n       'Rainfall_Mangona', 'Rainfall_S_Piero','Rainfall_Mean']).reset_index()\nrv_arno_tmp =rv_arno_tmp.pivot('Month','Year','Temperature_Firenze')\nfig, ax = plt.subplots(figsize = (15, 5))\nplt.title('Average Temperature per Month and Year', size = 15, fontweight = 'bold')\nsns.lineplot(data=rv_arno_tmp);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rv_arno_tmp_mean = rv_arno_wrk.groupby('Month').mean().drop(columns=['Year','Hydrometry_Nave_di_Rosano','Rainfall_Le_Croci', 'Rainfall_Cavallina', 'Rainfall_S_Agata',\n       'Rainfall_Mangona', 'Rainfall_S_Piero','Rainfall_Mean'])\nrv_arno_tmp_mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for month in range(1,13):\n    rv_arno_wrk.loc[lambda x: (x['Month']==month) & (x['Temperature_Firenze'].isnull()), 'Temperature_Firenze'] = rv_arno_tmp_mean.loc[month,'Temperature_Firenze']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check if we still have to work some feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.matrix(rv_arno_wrk)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, I'll plot the Monthly Dynamics again just to see how it looks after the Featuring Engineering."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adding rainfall Sum, year, month, month_year\ndf = rv_arno_wrk[['Date','Rainfall_Mean','Hydrometry_Nave_di_Rosano', 'Temperature_Firenze']]#.reset_index()\n#df['rainfall'] = rv_arno.iloc[:, 0:-2].sum(axis = 1).values\ndf['year'] = pd.to_datetime(df.Date).dt.year\ndf['month'] = pd.to_datetime(df.Date).dt.month\ndf['month_year'] = pd.to_datetime(df.Date).apply(lambda x: x.strftime('%Y/%m'))\n\n# Monthly dynamics\nr_means = np.log(df.groupby('month_year').Hydrometry_Nave_di_Rosano.mean() * 10).reset_index()\nr_means['month_year'] = pd.to_datetime(r_means['month_year'])\n\nr_rain = np.log(df.groupby('month_year').Rainfall_Mean.mean() *10).reset_index()\nr_rain['month_year'] = pd.to_datetime(r_rain['month_year'])\n\nr_temp = np.log(df.groupby('month_year').Temperature_Firenze.mean()).reset_index()\nr_temp['month_year'] = pd.to_datetime(r_temp['month_year'])\n\nfig, ax = plt.subplots(figsize = (15, 5))\nplt.title('Monthly dynamics (Arno River)', size = 15, fontweight = 'bold')\n          \nsns.lineplot(data = r_rain, x = 'month_year', y = 'Rainfall_Mean',  \n             color = 'gray', label = 'Rainfall', alpha = 0.4)\nplt.xticks(rotation = 45)\nsns.lineplot(data = r_temp, x = 'month_year', y = 'Temperature_Firenze', \n             color = 'green', label = 'Temperature_Firenze', alpha = 0.6)\nplt.xticks(rotation = 45)\nsns.lineplot(data = r_means, x = 'month_year', y = 'Hydrometry_Nave_di_Rosano', \n             color = 'blue', label = 'Hydrometry')\nplt.xticks(rotation = 45)\n    \nfor i in ['top', 'right', 'bottom', 'left']:\n        ax.spines[i].set_visible(False)\n\nax.set_xlabel('')\nax.set_ylabel('')\nax.grid(axis = 'y', linestyle = '--', alpha = 0.9)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rv_arno_wrk_model = rv_arno_wrk.set_index('Date')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now plot the Correlations matrix again to see how it looks."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_plot(rv_arno_wrk_model.drop(columns=['Year','Month']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model prediction"},{"metadata":{},"cell_type":"markdown","source":"I will split the data on 70% for training and 30% for testing and apply XGB because it's powerful and popular but other models could also be applied.\nSince I'm applying XGB, I'm not normalizing the data (not required). Parameters chosen after some experimentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = rv_arno_wrk_model['Hydrometry_Nave_di_Rosano']\nX = rv_arno_wrk_model.drop(['Hydrometry_Nave_di_Rosano','Season','Month','Year'], axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, shuffle = False)\n\nparams = {'n_estimators': 100,\n          'max_depth': 4,\n          'subsample': 0.7,\n          'learning_rate': 0.04,\n          'random_state': 0}\n\nmodel = XGBRegressor(**params)\n\nmodel.fit(X_train, y_train,)\n\ny_pred = model.predict(X_test)\nprint('MAE value: %.4f'%mean_absolute_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"The Mean Absolute Error of the predicted values is 0.3607, meaning that on average the model have an error of 36cm."},{"metadata":{},"cell_type":"markdown","source":"I will now plot the Feature Importances. Where we can see that Temperature is the most important."},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_imp_viz(model, train_data, bias = 0.01):\n    imp = pd.DataFrame({'importance': model.feature_importances_,\n                        'features': train_data.columns}).sort_values('importance', \n                                                                     ascending = False)\n    fig, ax = plt.subplots(figsize = (10, 4))\n    plt.title('Feature importances', size = 15, fontweight = 'bold')\n\n    sns.barplot(x = imp.importance, y = imp.features, edgecolor = 'black',\n                palette = reversed(sns.color_palette(\"viridis\", len(imp.features))))\n\n    for i in ['top', 'right']:\n            ax.spines[i].set_visible(None)\n\n    rects = ax.patches\n    labels = imp.importance\n    for rect, label in zip(rects, labels):\n        x_value = rect.get_width() + bias\n        y_value = rect.get_y() + rect.get_height() / 2\n\n        ax.text(x_value, y_value, round(label, 3), fontsize = 9, color = 'black',\n                 ha = 'center', va = 'center')\n    ax.set_xlabel('Importance', fontweight = 'bold')\n    ax.set_ylabel('Features', fontweight = 'bold')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_imp_viz(model, X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now plot the Hydrometry Real vs Predicted."},{"metadata":{"trusted":true},"cell_type":"code","source":"def predicted_viz(y_test, y_pred, param, name):\n    rm = y_test.reset_index()\n    rm['month_year'] = pd.to_datetime(rm.Date).apply(lambda x: x.strftime('%Y/%m'))\n    rm_means = rm.groupby('month_year')[param].mean().reset_index()\n    rm_means['month_year'] = pd.to_datetime(rm_means['month_year'])\n\n    pm = pd.DataFrame({'Date': y_test.index, param: y_pred})\n    pm['month_year'] = pd.to_datetime(pm.Date).apply(lambda x: x.strftime('%Y/%m'))\n    pm_means = pm.groupby('month_year')[param].mean().reset_index()\n    pm_means['month_year'] = pd.to_datetime(pm_means['month_year'])\n\n    fig, ax = plt.subplots(figsize = (15, 5))\n    plt.title('{} prediction ({})'.format(param, name), size = 15, \n              fontweight = 'bold')\n\n    sns.lineplot(data = rm_means, x = 'month_year', y = param, \n                 color = 'blue', label = 'Real {}'.format(param), alpha = 1)\n    sns.lineplot(data = pm_means, x = 'month_year', y = param, \n                 color = 'red', label = 'Pred {}'.format(param), alpha = 0.5)\n\n    for i in ['top', 'right', 'bottom', 'left']:\n            ax.spines[i].set_visible(False)\n\n    ax.set_xticks(rm_means.month_year[::12])\n    ax.set_xlabel('')\n    ax.set_ylabel('')\n    ax.grid(axis = 'y', linestyle = '--', alpha = 0.9)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_viz(y_test, y_pred, 'Hydrometry_Nave_di_Rosano', 'Arno River')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def resid_viz(y_test, y_pred):\n    resid = abs(y_test - y_pred)\n    fig, ax = plt.subplots(figsize = (10, 5))\n    plt.title('Residuals', size = 15, fontweight = 'bold')\n\n    sns.scatterplot(x = y_test, y = resid, color = 'red', \n                    edgecolor = 'black', alpha = 0.7)\n\n    for i in ['top', 'right']:\n            ax.spines[i].set_visible(False)\n\n    ax.set_xlabel('Real values', fontweight = 'bold')\n    ax.set_ylabel('Resiaduals', fontweight = 'bold')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resid_viz(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Residual distribution is a powerful tool for assessing the quality of a model. The linear dependence, which is most pronounced for high hydrometry values, proves that our model does not consider all the dependencies. Perhaps if all predictors were used (this is not possible due to missing values), the model would do much better."},{"metadata":{},"cell_type":"markdown","source":"# Further steps"},{"metadata":{},"cell_type":"markdown","source":"* Try differnt algorithms,\n* Analyze the remainig 8 waterbodies,\n* Try new parameters for XGB,\n* Improve data selection and preprocessing."},{"metadata":{},"cell_type":"markdown","source":"# Inspiration/Credits/Sources\nhttps://www.kaggle.com/tomwarrens/intro-to-time-series-analysis \\\nhttps://www.kaggle.com/marcomarchetti/acea-smart-water-eda#7-Conclusions \\\nhttps://www.kaggle.com/maksymshkliarevskyi/acea-smart-water-eda-prediction/execution \\\nhttps://www.kaggle.com/iamleonie/intro-to-time-series-forecasting \\\nhttps://www.kaggle.com/lucena1990/acea-smater-water-water-availability-data#Exploratory-Data-Analysis \\\nhttps://www.kaggle.com/kevinnolasco/random-forests-and-early-stopping-to-predict-water \\\nKirill Eremenko, Hadelin de Ponteves, Super Data Science"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}