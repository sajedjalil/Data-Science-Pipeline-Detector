{"cells":[{"metadata":{},"cell_type":"markdown","source":"Predicting values 30 (n_future) days from a given time by looking at 60 (n_past) days of past data:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\n\n#mpl.rcParams['figure.figsize'] = (10, 8)\nmpl.rcParams['axes.grid'] = False\npd.set_option('display.max_columns', None)  \npd.set_option('display.max_rows', None)  \n\ndata_dir = '../input/acea-water-prediction'\nfname_Auser = os.path.join(data_dir, 'Aquifer_Auser.csv')\nfname_Doganella = os.path.join(data_dir, 'Aquifer_Doganella.csv')\nfname_Luco = os.path.join(data_dir, 'Aquifer_Luco.csv')\nfname_Petrignano = os.path.join(data_dir, 'Aquifer_Petrignano.csv')\nfname_Bilancino = os.path.join(data_dir, 'Lake_Bilancino.csv')\nfname_Arno = os.path.join(data_dir, 'River_Arno.csv')\nfname_Amiata = os.path.join(data_dir, 'Water_Spring_Amiata.csv')\nfname_Lupa = os.path.join(data_dir, 'Water_Spring_Lupa.csv')\nfname_Madonna_di_Canneto = os.path.join(data_dir, 'Water_Spring_Madonna_di_Canneto.csv')\n\n# reading files\ndef file_read(fname):\n    df = pd.read_csv(fname, parse_dates={'date': ['Date']},\n                  date_parser=lambda x: pd.to_datetime(x, format=\"%d/%m/%Y\"))\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Auser=file_read(fname_Auser)\ndf_Doganella=file_read(fname_Doganella)\ndf_Luco=file_read(fname_Luco)\ndf_Petrignano=file_read(fname_Petrignano)\ndf_Bilancino=file_read(fname_Bilancino)\ndf_Arno=file_read(fname_Arno)\ndf_Lupa=file_read(fname_Lupa)\ndf_Madonna_di_Canneto=file_read(fname_Madonna_di_Canneto)\ndf_Amiata=file_read(fname_Amiata)\n\n\nlabel_cols_Auser=['Depth_to_Groundwater_SAL','Depth_to_Groundwater_CoS','Depth_to_Groundwater_LT2']\nlabel_cols_Doganella=['Depth_to_Groundwater_Pozzo_1','Depth_to_Groundwater_Pozzo_2','Depth_to_Groundwater_Pozzo_3','Depth_to_Groundwater_Pozzo_4','Depth_to_Groundwater_Pozzo_5','Depth_to_Groundwater_Pozzo_6','Depth_to_Groundwater_Pozzo_7','Depth_to_Groundwater_Pozzo_8','Depth_to_Groundwater_Pozzo_9']\nlabel_cols_Petrignano=['Depth_to_Groundwater_P24','Depth_to_Groundwater_P25']\nlabel_cols_Luco =['Depth_to_Groundwater_Podere_Casetta']\n\nlabel_cols_Madonna_di_Canneto=['Flow_Rate']\nlabel_cols_Lupa=['Flow_Rate']\nlabel_cols_Amiata=['Flow_Rate_Bugnano','Flow_Rate_Arbure',' Flow_Rate_Ermicciolo','Flow_Rate_Galleria_Alta']\n\nlabel_cols_Bilancino=['Lake_Level','Flow_Rate']\n\nlabel_cols_Arno=['Hydrometry_Nave_di_Rosano']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def df_prep_xgb(df_waterbody,label_cols,max_lag=90,cols_to_drop=None):\n    df=df_waterbody.copy()\n    #drop the required columns in df\n    if cols_to_drop is not None:\n            df=df.drop(cols_to_drop, axis = 1)\n\n    #filtering the largest chunk of dataset with no missing values of target columns \n    #after interpolation of 14 days\n    df= df.interpolate(method ='linear', limit_direction ='both', limit = 14)\n    df['mv_count']=df[label_cols].isnull().sum(axis=1)\n    df['slice_index']=0\n    j=1\n    for i in range(len(df)):\n        if df.loc[i,'mv_count']==0:\n            df.loc[i,'slice_index']=j\n        else: \n                 j=j+1\n    my_list=[]\n    for val, cnt in df['slice_index'].value_counts().iteritems():\n        if (cnt>=365) & (val!=0):\n                my_list.append(val)\n\n    boolean_series = df.slice_index.isin(my_list)\n    df = df[boolean_series]\n    maxSliceNum=df['slice_index'].value_counts()[:1].index.tolist()[0]\n    df=df[df['slice_index']==maxSliceNum]\n    df=df.drop(['mv_count','slice_index'], axis = 1)\n    #make 'Date' the index of df\n    df.set_index(\"date\", inplace = True)  \n    #count number of zeroes in each column\n    num_zeroes={}\n    for var in df.columns: \n        num_zeroes[var]=len(df)-(df[var].astype(bool).sum())\n    #add features with lag \n    for var in df.columns:    \n           for t in range(1, max_lag+1):\n                    df[var+'_lag'+str(t)] = df[var].shift(t)\n    #add time series features \n    df['Date']=df.index\n    df['month']=df['Date'].dt.month \n    df['day']=df['Date'].dt.day\n    df['Week_Number'] = df['Date'].dt.isocalendar().week\n    df['Week_Number']=df['Week_Number'].astype(float)\n    #df['Week_Number']=pd.Series(np.array(df['Week_Number'], dtype=int))\n    df['Week_Number'] = pd.to_numeric(df['Week_Number'],downcast='integer',errors='ignore')\n    df['dayofweek_name']=df['Date'].dt.day_name()\n    # Add a new column named 'weekend' \n    df['weekend']=[1 if ((x =='Saturday')| (x=='Sunday')) else 0 for x in df['dayofweek_name']] \n    df=df.drop(['dayofweek_name','Date'], axis = 1)                \n    #making the n target variables the first n columns in the dataframe\n    df_label_cols=df[label_cols]\n    df_label_cols_not=df.drop(label_cols, axis = 1) \n    df=df_label_cols.join(df_label_cols_not)\n    df.drop(df.head(max_lag).index, inplace=True)\n\n    return df,num_zeroes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_prep_Bilancino,num_zeroes_Bilancino=df_prep_xgb(df_Bilancino,label_cols_Bilancino,max_lag=60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_prep_Bilancino.shape\n#num_zeroes_Bilancino","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def X_y_train_test(ts_waterbody,feature_columns,label_columns,n_future):\n            df=ts_waterbody.copy()\n            label_cols=label_columns.copy()\n            feature_cols=feature_columns.copy()\n            label_cols.append('date')\n            feature_cols.append('date')\n            df['date']=df.index\n            df=df.reset_index(drop=True)\n            X=df.head(len(df)-n_future)\n            y=df.tail(len(df)-n_future)\n            X=X.reindex(columns=feature_cols)\n            y=y.reindex(columns=label_cols)\n            #indices_X=[range(0,(len(df)-n_future))]\n            #indices_y=[range(n_future,len(df))]\n            #X=df.loc[df.index.intersection(indices_X),feature_cols]\n            #y=df.loc[df.index.intersection(indices_y),label_cols]\n            X.set_index(\"date\", inplace = True) \n            y.set_index(\"date\", inplace = True)             \n            n_train=int(0.8*len(X))\n            n_test=len(X)-n_train\n            X_train=X.head(n_train)\n            y_train=y.head(n_train)\n            X_test=X.tail(n_test)\n            y_test=y.tail(n_test)\n\n            return X_train,y_train,X_test,y_test\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_Bilancino,y_train_Bilancino,X_test_Bilancino,y_test_Bilancino=X_y_train_test(df_prep_Bilancino,df_prep_Bilancino.columns.to_list(),label_cols_Bilancino,30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"shape of X_train_Bilancino: {}\".format(X_train_Bilancino.shape))\nprint(\"shape of y_train_Bilancino: {}\".format(y_train_Bilancino.shape))\nprint(\"shape of X_test_Bilancino: {}\".format(X_test_Bilancino.shape))\nprint(\"shape of y_test_Bilancino: {}\".format(y_test_Bilancino.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_absolute_percentage_error_2(y_true, y_pred): \n    my_list=[]\n    for i in range(len(y_true)):\n\n        if (y_true[i]==0) & (y_pred[i]==0):\n                #print(\"y_true[i] in loop if 1:\",y_true[i])\n                #print(\"y_pred[i] in loop if 1:\",y_pred[i])\n                my_list.append(0)\n        if (y_true[i]==0) & (y_pred[i]!=0):\n                #print(\"y_true[i] in loop if 2:\",y_true[i])\n                #print(\"y_pred[i] in loop if 2:\",y_pred[i])            \n                y_true[i]=0.00001\n                my_list.append((y_true[i] - y_pred[i]) / y_true[i])\n        if (y_true[i]!=0) & (y_pred[i]!=0):\n                #print(\"y_true[i] in loop if 3:\",y_true[i])\n                #print(\"y_pred[i] in loop if 3:\",y_pred[i])  \n                my_list.append((y_true[i] - y_pred[i]) / y_true[i])\n        if (y_true[i]!=0) & (y_pred[i]==0):\n                #print(\"y_true[i] in loop if 3:\",y_true[i])\n                #print(\"y_pred[i] in loop if 3:\",y_pred[i])  \n                my_list.append((y_true[i] - y_pred[i]) / y_true[i])\n    return np.mean(np.abs(np.array(my_list))) * 100\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import make_regression\nfrom xgboost import XGBRegressor\n#from sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.feature_selection import SelectFromModel\nfrom numpy import sort\n\ndef feature_selection(X_train_waterboby,y_train_waterboby,X_test_waterboby,y_test_waterboby):\n    imp_features={}\n    X_train=X_train_waterboby.copy()\n    y_train=y_train_waterboby.copy()\n    X_test=X_test_waterboby.copy()\n    y_test=y_test_waterboby.copy()\n    for i in range(len(y_train.columns)):\n        model = XGBRegressor()\n        # fit the model\n        model.fit(X_train, y_train.loc[:,y_train.columns[i]])\n        # get importance\n        #importance = model.feature_importances_\n        results=pd.DataFrame()\n        results['columns']=X_train.columns\n        \n        #results['columns']=X_train.columns\n        results['importances'] = model.feature_importances_\n        \n        results.sort_values(by='importances',ascending=True,inplace=True)\n        results=results.reset_index(drop=True)\n        results['mape']=0\n        results['threshold']=0\n        #results=results[:num_feature]\n\n        thresholds = sort(model.feature_importances_)\n        j=0\n        for thresh in thresholds:\n         # select features using threshold\n                selection = SelectFromModel(model, threshold=thresh, prefit=True)\n                select_X_train = selection.transform(X_train)\n         # train model\n                selection_model = XGBRegressor()\n                selection_model.fit(select_X_train, y_train.loc[:,y_train.columns[i]])\n         # eval model\n                select_X_test = selection.transform(X_test)\n                y_pred = selection_model.predict(select_X_test)\n                mape = mean_absolute_percentage_error_2(np.array(y_test.loc[:,y_test.columns[i]]), np.array(y_pred))\n                \n                results.loc[j,'mape']=mape\n                results.loc[j,'threshold']=thresh\n                j=j+1\n        results=results.tail(len(results)-(results['mape'].idxmin(axis = 0)))\n        imp_features[y_train.columns[i]]=results\n     \n    return imp_features\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp_features_Bilancino=feature_selection(X_train_Bilancino,y_train_Bilancino,X_test_Bilancino,y_test_Bilancino)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_lag_table(my_dict,target):\n    my_dict[target]['if_lag']=my_dict[target]['columns'].apply(lambda x : 1 if x.find('lag') != -1 else 0 )\n    my_dict[target]['lag_num']=my_dict[target][my_dict[target]['if_lag']==1]['columns'].apply(lambda x : x.split('_lag')[1])\n    my_dict[target]['lag_num']=my_dict[target]['lag_num'].fillna(0)\n    my_dict[target]['features']=my_dict[target]['columns']\n    my_dict[target]['features']=my_dict[target]['features'].apply(lambda x : x.split('_lag')[0] if x.find('lag') != -1 else x )\n    df_1=my_dict[target].loc[:,['features','lag_num']]\n    df_2=df_1.groupby('features').apply(lambda x: x['lag_num'].unique())\n    df_3=pd.DataFrame()\n    df_3['features']=df_2.index \n    df_3['lag']=df_2.values\n    return df_3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_i={}\nfor i in range(len(label_cols_Bilancino)):\n            df_i[label_cols_Bilancino[i]]=feature_lag_table(imp_features_Bilancino,label_cols_Bilancino[i])\n            file_name_csv=label_cols_Bilancino[i]+\"_features.csv\"\n            df_i[label_cols_Bilancino[i]].to_csv(file_name_csv,index=False) \n\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_cols_Bilancino[1]    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_i[label_cols_Bilancino[1]]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_cols_Bilancino[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_i[label_cols_Bilancino[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_test(df_prep_waterbody,imp_feature_dict,target_columns,n_future,figsize=(14,10)):\n    number_of_subplots=len(target_columns)\n    fig, axes = plt.subplots(nrows=number_of_subplots, ncols=1, figsize=figsize)\n    plt.subplots_adjust(hspace = 0.8) \n    for i in range(len(target_columns)):\n            df_features=imp_feature_dict[target_columns[i]]\n            selected_columns=df_features['columns'].to_list()\n            out_col=[target_columns[i]]\n            X_train,y_train,X_test,y_test=X_y_train_test(df_prep_waterbody,selected_columns,out_col,n_future)\n            datelist_y_test=y_test.index\n            print(\"shape of X_train: {}\".format(X_train.shape))\n            print(\"shape of y_train: {}\".format(y_train.shape))\n            print(\"shape of X_test: {}\".format(X_test.shape))\n            print(\"shape of y_test: {}\".format(y_test.shape))\n            model = XGBRegressor()\n            model.fit(X_train, y_train)\n            # eval model\n            y_pred = model.predict(X_test)\n            mape = mean_absolute_percentage_error_2(np.array(y_test), np.array(y_pred))\n            axes[i].xaxis.set_tick_params(rotation=45)\n            axes[i].plot(datelist_y_test, y_test, 'b')\n            axes[i].plot(datelist_y_test, y_pred, 'r')\n            axes[i].legend(['actual', 'prediction'], loc='upper left')\n            axes[i].set_title(target_columns[i]+\"  MAPE=\"+str(mape))   \n            \n    return axes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_test(df_prep_Bilancino,imp_features_Bilancino,label_cols_Bilancino,30,figsize=(14,8))\nplt.savefig('predict_test_Bilancino.jpeg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_importance_plot(imp_feature_dict,target_columns,num_feature,figsize=(14,10)):\n    number_of_subplots=len(target_columns)\n    fig, axes = plt.subplots(nrows=number_of_subplots, ncols=1, figsize=figsize)\n    plt.subplots_adjust(hspace = 1.2) \n    \n    for i in range(len(target_columns)):\n            df_features=imp_feature_dict[target_columns[i]]\n            selected_columns=df_features['columns'].to_list()\n            \n            feat_imp=df_features['importances'].to_list()\n            axes[i].xaxis.set_tick_params(rotation=20)\n            axes[i].bar(selected_columns[-num_feature:], feat_imp[-num_feature:], color ='maroon',width=0.2)\n            axes[i].set_title(target_columns[i])   \n            \n    return axes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance_plot(imp_features_Bilancino,label_cols_Bilancino,10,figsize=(14,8))\nplt.savefig('feature_importance_Bilancino.jpeg')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}