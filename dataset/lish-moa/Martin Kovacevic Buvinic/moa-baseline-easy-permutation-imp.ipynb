{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Comments\n\nThanks to https://www.kaggle.com/simakov/keras-multilabel-neural-network-v1-2\n    \nFeature selection is very important, we could make an iterative process were we drop a feature and re-train our model again and check if the score improves or not. This is a very slow process, we have 874 features so if we want to apply the previous technique we need to train 874 models!. To resolve this we can use permutation importance were the main idea is to just train one model with all the features, and then predict 874 times were each time one feature is changed to random noise (an easy way to do that is to just shuffle the same feature, this also will mantain the same distribution for that feature). We then compare each score with our baseline score, if the score improves we can drop that feature (random noise is better than the original feature), on the other hand if the score get worst we could say that the feature makes our model better"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow_addons as tfa\nfrom sklearn.metrics import log_loss\nfrom tqdm.notebook import tqdm\nimport random\nimport os\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Basic training configurations\n# Number of folds for KFold validation strategy\nFOLDS = 5\n# Number of epochs to train each model\nEPOCHS = 80\n# Batch size\nBATCH_SIZE = 128\n# Learning rate\nLR = 0.001\n# Verbosity\nVERBOSE = 2\n# Seed for deterministic results\nSEED = 123\n\n# Function to seed everything\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mapping_and_filter(train, train_targets, test):\n    cp_type = {'trt_cp': 0, 'ctl_vehicle': 1}\n    cp_dose = {'D1': 0, 'D2': 1}\n    for df in [train, test]:\n        df['cp_type'] = df['cp_type'].map(cp_type)\n        df['cp_dose'] = df['cp_dose'].map(cp_dose)\n    train_targets = train_targets[train['cp_type'] == 0].reset_index(drop = True)\n    train = train[train['cp_type'] == 0].reset_index(drop = True)\n    train_targets.drop(['sig_id'], inplace = True, axis = 1)\n    return train, train_targets, test\n\n\ndef scaling(train, test):\n    features = train.columns[2:]\n    scaler = StandardScaler()\n    train[features] = scaler.fit_transform(train[features])\n    test[features] = scaler.transform(test[features])\n    return train, test, features\n\n\n# Function to calculate the mean los loss of the targets\ndef mean_log_loss(y_true, y_pred):\n    metrics = []\n    for target in range(len(train_targets.columns)):\n        metrics.append(log_loss(y_true[:, target], y_pred[:, target]))\n    return np.mean(metrics)\n\n# Function to create our dnn\ndef create_model(shape):\n    inp = tf.keras.layers.Input(shape = (shape))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation = 'relu'))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.5)(x)\n    x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(1048, activation = 'relu'))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.5)(x)\n    out = tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, activation = 'sigmoid'))(x)\n    model = tf.keras.models.Model(inputs = inp, outputs = out)\n    opt = tf.optimizers.Adam()\n    opt = tfa.optimizers.Lookahead(opt, sync_period = 10)\n    model.compile(optimizer = opt, \n                  loss = 'binary_crossentropy')\n    return model\n\n\n# Function to train our dnn\ndef train_and_evaluate(train, train_targets, test, features, perm_imp = False):\n    models = []\n    trn_indices, val_indices = [], []\n    oof_pred = np.zeros((train.shape[0], 206))\n    test_pred = np.zeros((test.shape[0], 206))\n    for fold, (trn_ind, val_ind) in enumerate(MultilabelStratifiedKFold(n_splits = FOLDS, \n                                                                        random_state = SEED, \n                                                                        shuffle = True)\\\n                                              .split(train_targets, train_targets)):\n        print(f'Training fold {fold + 1}')\n        K.clear_session()\n        model = create_model(len(features))\n        early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss',\n                                                          mode = 'min',\n                                                          patience = 10,\n                                                          restore_best_weights = True,\n                                                          verbose = 1)\n        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss',\n                                                         mode = 'min',\n                                                         factor = 0.3,\n                                                         patience = 3)\n\n        x_train, x_val = train[features].values[trn_ind], train[features].values[val_ind]\n        y_train, y_val = train_targets.values[trn_ind], train_targets.values[val_ind]\n\n        model.fit(x_train, y_train,\n                  validation_data = (x_val, y_val),\n                  epochs = EPOCHS, \n                  batch_size = BATCH_SIZE,\n                  callbacks = [early_stopping, reduce_lr],\n                  verbose = VERBOSE)\n\n        oof_pred[val_ind] = model.predict(x_val)\n        test_pred += model.predict(test[features].values) / FOLDS\n        \n        models.append(model)\n        trn_indices.append(trn_ind)\n        val_indices.append(val_ind)\n        \n        print('-'*50)\n        print('\\n')\n\n\n    oof_score = mean_log_loss(train_targets.values, oof_pred)\n    print(f'Our out of folds mean log loss score is {oof_score}')\n    \n    if perm_imp:\n        return models, trn_indices, val_indices, oof_score\n    else:\n        return test_pred\n    \n    \n# Function to perform permutation importance (feature selection)\ndef permutation_importance(train, train_targets, test, features):\n    # Get the base score of our model\n    models, trn_indices, val_indices, oof_score = train_and_evaluate(train, train_targets, test, features, perm_imp = True)\n    scores = np.zeros(len(features))\n    # Iterate over each feature\n    for num, feature in enumerate(tqdm(features)):\n        train_ = train.copy()\n        # Shuffle the data for the given feature\n        train_[feature] = train_[feature].sample(frac = 1.0).reset_index(drop = True)\n        oof_pred = np.zeros((train_.shape[0], 206))\n        # Predict each validation fold with the corresponding model\n        for model, trn_ind, val_ind in zip(models, trn_indices, val_indices):\n            oof_pred[val_ind] = model.predict(train_[features].values[val_ind])\n        score = mean_log_loss(train_targets.values, oof_pred)\n        # If the result is positive the feature is useless, if it is negative is usefull\n        scores[num] = oof_score - score\n        \n    perm_dataframe = pd.DataFrame({'features': features, 'score': scores})\n    perm_dataframe = perm_dataframe[perm_dataframe['score'] < 0]\n    new_features = list(perm_dataframe['features'])\n    print(f'We have select {len(new_features)} features')\n    return new_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntest = pd.read_csv('../input/lish-moa/test_features.csv')\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n\ntrain, train_targets, test = mapping_and_filter(train, train_targets, test)\n\ntrain, test, features = scaling(train, test)\n\nnew_features = permutation_importance(train, train_targets, test, features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = train_and_evaluate(train, train_targets, test, new_features, perm_imp = False)\nsample_submission.loc[:, train_targets.columns] = test_preds\nsample_submission.loc[test['cp_type'] == 1, train_targets.columns] = 0\nsample_submission.to_csv('submission.csv', index = False)\nsample_submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}