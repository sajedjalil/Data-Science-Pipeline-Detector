{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nfrom matplotlib.ticker import MultipleLocator\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import VarianceThreshold\nimport sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master/')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_features = pd.read_csv(\"/kaggle/input/lish-moa/train_features.csv\")\ntrain_targets_scored = pd.read_csv(\"/kaggle/input/lish-moa/train_targets_scored.csv\")\ntrain_targets_nonscored = pd.read_csv(\"/kaggle/input/lish-moa/train_targets_nonscored.csv\")\ntest_features = pd.read_csv(\"/kaggle/input/lish-moa/test_features.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/lish-moa/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_stats(df, columns, prefix):\n    df[prefix + '_mean'] = columns.mean(axis=1)\n    df[prefix + '_sum'] = columns.sum(axis=1)\n    df[prefix + '_std'] = columns.std(axis=1)\n    df[prefix + '_kurt'] = columns.kurtosis(axis = 1)\n    df[prefix + '_skew'] = columns.skew(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full = train_features.merge(train_targets_scored, on='sig_id')\ntrain_full = train_full[train_full.cp_type != 'ctl_vehicle']\ntrain_full = train_full.drop(columns=['sig_id', 'cp_type'])\ntrain_full['cp_dose'] = pd.get_dummies(train_full['cp_dose'], drop_first=True)\n\nX_train = train_full.iloc[:, :874]\ny_train = train_full.iloc[:, 874:]\n\nX_test = test_features\nX_test = X_test[X_test.cp_type != 'ctl_vehicle']\nX_test = X_test.drop(columns=['sig_id', 'cp_type'])\nX_test['cp_dose'] = pd.get_dummies(X_test['cp_dose'], drop_first=True)\n\nadd_stats(X_train, X_train.iloc[:, 2:774], 'g')\nadd_stats(X_train, X_train.iloc[:, 774:874], 'c')\nadd_stats(X_train, X_train.iloc[:, 2:874], 'gc')\n                    \nadd_stats(X_test, X_test.iloc[:, 2:774], 'g')\nadd_stats(X_test, X_test.iloc[:, 774:874], 'c')\nadd_stats(X_test, X_test.iloc[:, 2:874], 'gc')\n\nX_full = pd.concat([X_train, X_test])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def pca_analyse(columns, num_comp, scaler=None):\n    pca = PCA(n_components=num_comp)\n    if scaler is not None:\n        columns = scaler.fit_transform(columns)\n    pca.fit(columns)\n    return np.cumsum(pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PCA for g- features"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_nonscaled = pca_analyse(X_full.iloc[:, 2:774], 700)\npca_stand = pca_analyse(X_full.iloc[:, 2:774], 700, StandardScaler())\npca_quant = pca_analyse(X_full.iloc[:, 2:774], 700, QuantileTransformer(output_distribution=\"normal\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(1, 1, 1)\n\nplt.plot(pca_nonscaled, label='nonscaled')\nplt.plot(pca_stand, label='stand')\nplt.plot(pca_quant, label='quant')\nax.yaxis.set_major_locator(MultipleLocator(0.05))\nax.grid(which='major')\nax.set_xlabel(\"Число компонент\")\nax.set_ylabel(\"Доля объясненной дисперсии\")\nax.set_title(\"g- признаки\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Будем использовать QuantileTransformer с num_comp=600, чтобы доля объясненной дисперсии равнялась 0.95"},{"metadata":{},"cell_type":"markdown","source":"PCA for с- features"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_nonscaled = pca_analyse(X_full.iloc[:, 774:874], 100)\npca_stand = pca_analyse(X_full.iloc[:, 774:874], 100, StandardScaler())\npca_quant = pca_analyse(X_full.iloc[:, 774:874], 100, QuantileTransformer(output_distribution=\"normal\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(1, 1, 1)\n\nplt.plot(pca_nonscaled, label='nonscaled')\nplt.plot(pca_stand, label='stand')\nplt.plot(pca_quant, label='quant')\nax.yaxis.set_major_locator(MultipleLocator(0.05))\nax.grid(which='major')\nax.set_xlabel(\"Число компонент\")\nax.set_ylabel(\"Доля объясненной дисперсии\")\nax.set_title(\"с- признаки\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Будем использовать QuantileTransformer с num_comp=80, чтобы доля объясненной дисперсии равнялась 0.95"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_pca(df, columns, num_comp, prefix, scaler):\n    pca = PCA(n_components=num_comp)\n    if scaler is not None:\n        columns = scaler.fit_transform(columns)\n    comps = pca.fit_transform(columns)\n    pca_df = pd.DataFrame(comps, \n                          columns=[prefix + f\"_pca_{i}\".format(i) for i in range(num_comp)])\n    return pd.concat((df.reset_index(drop=True), pca_df.reset_index(drop=True)), axis=1)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def variance_threshold(df, columns, thr=0.85):\n    vt = VarianceThreshold(threshold=thr)\n    var = vt.fit(columns).variances_\n    \n    drop_cols = columns.columns[var < thr]\n    return df.drop(columns=drop_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full = train_features.merge(train_targets_scored, on='sig_id')\n\nX_full = add_pca(X_full, X_full.iloc[:, 2:774], 600, 'g', \n                 QuantileTransformer(output_distribution=\"normal\"))\nX_full = add_pca(X_full, X_full.iloc[:, 774:874], 80, 'c', \n                 QuantileTransformer(output_distribution=\"normal\"))\n\nX_full = variance_threshold(X_full, X_full.drop(columns=['cp_time', 'cp_dose']), thr=0.9)\n\nX_train = X_full[:X_train.shape[0]]\nX_test = X_full[-X_test.shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoaDataset(Dataset):\n    def __init__(self, X, y=None):\n        self.X = X\n        self.y = y\n        self.device = device\n    \n    def __getitem__(self, idx):\n        if self.y is None:\n            return torch.tensor(self.X[idx], dtype=torch.float)\n        \n        return torch.tensor(self.X[idx], dtype=torch.float),  \\\n                   torch.tensor(self.y[idx], dtype=torch.float)\n    \n    def __len__(self):\n        return self.X.shape[0]\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.2)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n        \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def valid_epoch(model, criterion, dataloader, device):\n    model.eval()\n    \n    with torch.no_grad():\n        total_loss = 0.\n        total_num = 0\n        for X, y in dataloader:\n            X = X.to(device)\n            y = y.to(device)\n            outputs = model(X)\n            loss = criterion(outputs, y)\n            total_loss += loss.item() * X.shape[0]\n            total_num += X.shape[0]\n        \n        return total_loss / total_num\n\ndef train_epoch(model, criterion, optimizer, scheduler, train_dataloader, valid_dataloader, device):\n    model.train()\n    total_loss = 0.\n    total_num = 0\n    for X, y in train_dataloader:\n        X = X.to(device)\n        y = y.to(device)\n        optimizer.zero_grad()\n        \n        outputs = model(X)\n        loss = criterion(outputs, y)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        total_loss += loss.detach().item() * X.shape[0]\n        total_num += X.shape[0]\n        \n    train_loss = total_loss / total_num\n        \n    val_loss = valid_epoch(model, criterion, val_dataloader, device)\n    return train_loss, val_loss\n        \ndef predict(model, dataset, device):\n    model.eval()\n    result = []\n    with torch.no_grad():\n        for elem in dataset:\n            elem = elem.unsqueeze(0).to(device)\n            output = model(elem)\n            result.append(torch.sigmoid(output).cpu().numpy())\n    \n    return np.concatenate(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = 'cuda'\ntrain_loss_list = []\nval_loss_list = []\npred_list = []\ntest_dataset = MoaDataset(X_test.values)\nnum_features = X_train.shape[1]\nnum_targets = y_train.shape[1]\n\ncriterion = torch.nn.BCEWithLogitsLoss()\nseed_list = np.arange(7)\n\nfor seed in seed_list:\n    seed_everything(seed)\n    mskf = MultilabelStratifiedKFold(n_splits=7, shuffle=True, random_state=seed)\n    for fold_ind, (train_idx, val_idx) in enumerate(mskf.split(X_train, y_train)):\n        curr_train_loss_list = []\n        curr_val_loss_list = []\n\n        train_dataset = MoaDataset(X_train.values[train_idx], y_train.values[train_idx])\n        train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=1)\n\n        val_dataset = MoaDataset(X_train.values[val_idx], y_train.values[val_idx])\n        val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=True, num_workers=1)\n\n        model = Model(num_features, num_targets, 1024).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n        # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=3, verbose=True)\n        # scheduler = optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=5, gamma=0.1)\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                                  max_lr=1e-2, epochs=25, steps_per_epoch=len(train_dataloader))\n\n\n        for epoch_num in range(25):\n            train_loss, val_loss = train_epoch(model, criterion, optimizer, scheduler, \n                                               train_dataloader, val_dataloader, device)\n\n            curr_train_loss_list.append(train_loss)\n            curr_val_loss_list.append(val_loss)\n\n            print('seed: ', seed, ' fold: ', fold_ind, ' epoch_num: ', epoch_num, ' train loss: ', train_loss)\n            print('seed: ', seed, ' fold: ', fold_ind, ' epoch_num: ', epoch_num, ' val loss: ', val_loss)\n\n        train_loss_list.append(curr_train_loss_list)\n        val_loss_list.append(curr_val_loss_list)\n\n        pred_list.append(predict(model, test_dataset, device))\n        \n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"curr_train_loss: \", sum([elem[-1] for elem in train_loss_list]) / len(train_loss_list))\nprint(\"curr_val_loss: \", sum([elem[-1] for elem in val_loss_list]) / len(val_loss_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = np.zeros((sample_submission.shape[0], y_train.shape[1]))\ny_test[test_features.cp_type != 'ctl_vehicle'] = sum(pred_list) / len(pred_list)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(y_test, columns=train_targets_scored.columns[1:])\nsubmission['sig_id'] = test_features['sig_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}