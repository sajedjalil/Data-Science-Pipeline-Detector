{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Import usual stuff\nimport matplotlib.pyplot as plt\nfrom functools import partial\nimport pandas as pd\nimport random as rn\nimport numpy as np\nimport pickle\nimport copy\nimport os\n\n# Import preprocessing and metrics functions\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import hamming_loss, accuracy_score, f1_score, log_loss\nfrom skmultilearn.model_selection import iterative_train_test_split, IterativeStratification\n\n\n# Baseline models\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom skmultilearn.adapt import MLkNN\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\n\n# Import modeling and tuning functions\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport kerastuner as kt\n\nfrom scipy.sparse.csr import csr_matrix\nfrom scipy.sparse.lil import lil_matrix\n\n# Lets seed everything\ntf.random.set_seed(123)\nos.environ['PYTHONHASHSEED'] = '123'\nnp.random.seed(123)\nrn.seed(123)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Mechanisms of Action\n\nMechanisms of Action (MoA) is a multilabel classification problem in which objective is to predict the MoA target from a set of gene and cell response.\n\nThe objective of this notebook is to do some experimentations over the Mechanisms of Action data with the objective of obtain a comparision over different models and use the kmowledge as a baseline for future modeling.\n\nFirst, there will be a short EDA to obtain insights about the problems that will have the models.\n\nThe models tested here will be:\n* Random Forest.\n* Logistic Regresssion.\n* KNN.\n* Neural network.\n* Convolutional NN (Mixed representation).\n* Tabnet.\n* CatBoost.\n\nAfter that, the objective will be improve it as much as possible triying different preprocessing techniques. These techniques will be:\n* PCA.\n* Power Transform.\n* Feature selection (TODO).\n* MLSMOTE (TODO).\n\nFinally the best model hyperparameters will be tuned and obtain the final prediction."},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/lish-moa//train_features.csv')\nlabels = pd.read_csv('/kaggle/input/lish-moa//train_targets_scored.csv')\ntest = pd.read_csv('/kaggle/input/lish-moa//test_features.csv')\n\ntrain = train.set_index('sig_id')\nlabels = labels.set_index('sig_id')\ntest = test.set_index('sig_id')\n\nprint(f'Train shape {train.shape}; number of labels {labels.shape[1]}; test shape {test.shape} ({train.shape[0]//test.shape[0]}%)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset consist on 23814 examples for training with 875 features and 206 classes. The test dataset consist on the 5% of the training data approximately with 3982 examples."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"number_of_genetic_features = len(list(filter(lambda col: 'g-'in col, train.columns)))\nnumber_of_cell_features = len(list(filter(lambda col: 'c-'in col, train.columns)))\nprint(f'Number of genetic columns: {number_of_genetic_features} Number of cell columns: {number_of_cell_features}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[[f'g-{rn.randint(0, 772)}' for i in range(6)]].hist(figsize=(20,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[[f'c-{rn.randint(0, 100)}' for i in range(6)]].hist(figsize=(20,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['cp_time'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['cp_type'].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The histograms show that the genetic and cell distributions are mostly normal existing some samples around the 10 and -10 values. In other kernels like this () explain more in detail their behaiviour, this can be considered outliers. In the experimentation stage would be interesting see how removing this samples will affect the performance of the model.\nThe cp_time feature consist in 3 values (24, 48 and 72) with no imbalance.\nThe cp_type consist in two values (tt_cp and ctl_vehicle), the data is veri unbalanced and also the ctl_vahicle category has a problem in it that we will see."},{"metadata":{"trusted":true},"cell_type":"code","source":"ctl_vahicle_index = train[train['cp_type']=='ctl_vehicle'].index\nnumber_classes_ctl_vahicle = labels.loc[ctl_vahicle_index,:].sum().sum()\nprint(f'The number of non zero classes with cp_type=ctl_vahicle is {number_classes_ctl_vahicle}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So every time the cp_type is ctl_vahicle there will be no scored class, so every time the cp_type is this value we can say that all classes are zero. Although, the cp_type only have another value (trt_cp) so if we drop the rows with ctl_vehicle there will be only one possible value and it will not give any useful information so the column can be dropped too."},{"metadata":{"trusted":true},"cell_type":"code","source":"index_to_remove = train.index[train.cp_type == 'ctl_vehicle']\ntrain = train.drop(index_to_remove).drop('cp_type', axis=1)\nlabels = labels.drop(index_to_remove)\nindex_to_remove_test = test.index[test.cp_type == 'ctl_vehicle']\ntest = test.drop(index_to_remove_test).drop('cp_type', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"number_of_rows_by_class = labels.sum(axis=0).sort_values()\nprint('======== Classes with less rows ========')\nprint(number_of_rows_by_class[:10])\nprint('======== Classes with more rows ========')\nprint(number_of_rows_by_class[-10:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we have a problem, the data is very unbalanced, there are some classes that have only 1 example like atp-sensitive_potassium_channel_antagonist and erbb2_inhibitor. This will introduce error in our model because if we split it with some of this two examples out the model will fail for sure. \nnfkb_inhibitor seems to be the most common class with 832."},{"metadata":{"trusted":true},"cell_type":"code","source":"number_of_classes_distribution = labels.sum(axis=1).value_counts()\nprint('============== Number of classes in every row distribution ==============')\nnumber_of_classes_distribution","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see the distribution of number of classes that a row can have. We can see that the most common case is when an example have only 1 class followed by 0 classes. So dont have class is very common in this dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_7_classes = labels.loc[labels.sum(axis=1)[labels.sum(axis=1)==7].index, :]\nrows_with_7_labels = labels_7_classes.sum()[labels_7_classes.sum()>0]\nprint('============== Number of rows with 7 classes ==============')\nrows_with_7_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('============== Number of rows of the classes independently ==============')\nnumber_of_rows_by_class[rows_with_7_labels.index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the strangest case with 7 labels only happen with the classes:\n* apoptosis_stimulant\n* bcl_inhibitor ikk_inhibitor\n* nfkb_inhibitor\n* nitric_oxide_production_inhibitor\n* nrf2_activator \n* ppar_receptor_agonist  \n\nAlthough the class nitric_oxide_production_inhibitor only appears 12 times so the probability of having 7 classes is 50 in this case."},{"metadata":{},"cell_type":"markdown","source":"Finally we are gonna define the function that will read the data. Note that this function encodes the categorical data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data():\n    train = pd.read_csv('/kaggle/input/lish-moa//train_features.csv')\n    labels = pd.read_csv('/kaggle/input/lish-moa//train_targets_scored.csv')\n    test = pd.read_csv('/kaggle/input/lish-moa//test_features.csv')\n    \n    train = train.set_index('sig_id')\n    labels = labels.set_index('sig_id')\n    test = test.set_index('sig_id')\n    \n    # Removed because all classes are always zero when cp_type = 'ctl_vehicle\n    index_to_remove = train.index[train.cp_type == 'ctl_vehicle']\n    train = train.drop(index_to_remove).drop('cp_type', axis=1)\n    labels = labels.drop(index_to_remove)\n    index_to_remove_test = test.index[test.cp_type == 'ctl_vehicle']\n    test = test.drop(index_to_remove_test).drop('cp_type', axis=1)\n    \n    # Binarize column cp_dose\n    label_bin = LabelBinarizer()\n    train['cp_dose'] = label_bin.fit_transform(train['cp_dose']).squeeze()    \n    test['cp_dose'] = label_bin.transform(test['cp_dose']).squeeze()\n    \n    # Encode cp_time in three categories\n    train['cp_time'] = train['cp_time'].astype('category')\n    test['cp_time'] = test['cp_time'].astype('category')\n    train = pd.get_dummies(train)\n    test = pd.get_dummies(test)\n        \n    return train, labels, test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Experimentation setup"},{"metadata":{},"cell_type":"markdown","source":"### Dataset"},{"metadata":{},"cell_type":"markdown","source":"The fist thing ww will do is to create a short version of the data because the objective is to make the training as fast as possible to get the results and finally train with all the data with the best model and transformations."},{"metadata":{"trusted":true},"cell_type":"code","source":"train, labels, test = read_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"special_rows_index = labels[(labels['atp-sensitive_potassium_channel_antagonist']>0) | (labels['erbb2_inhibitor']>0)].index\nspecial_rows_features = train.loc[special_rows_index].reset_index(drop=True)\nspecial_rows_labels = labels.loc[special_rows_index].reset_index(drop=True)\ntrain = train.drop(special_rows_index)\nlabels = labels.drop(special_rows_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, Y_train, X_valid, Y_valid = iterative_train_test_split(train.values, labels.values, .8)\n\ntrain_short = pd.DataFrame(X_train, columns=train.columns)\nlabels_short = pd.DataFrame(Y_train, columns=labels.columns)\n\n# Ensure that the special cases are included\ntrain_short = pd.concat([train_short, special_rows_features])\nlabels_short = pd.concat([labels_short, special_rows_labels])\n\ntrain_short.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that with the partition we removed the atp-sensitive_potassium_channel_antagonist and the erbb2_inhibitor so the experiments will not be trained in that classes never and we wont be able to evaluate the model in that classes."},{"metadata":{},"cell_type":"markdown","source":"### Experiment environment"},{"metadata":{},"cell_type":"markdown","source":"Once we have our data we need to develop a environment in which we can evaluate the different experiments with the most similar scenario. We will indentify two main objects: ModelConfig and ExperimentConfig. A ModelConfig can have 1 or ExperimentConfig, the ModelConfig will determine the model and their hyperparameters while ExperimentConfig will determine the configuration of the experiment."},{"metadata":{},"cell_type":"markdown","source":"Lets define the main function: run_experiments"},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_experiment(data, configurations):\n  \n    results = []\n    metrics = None\n    \n    experiment_config = configurations['experiment_config']\n    model_config = configurations['model_config']\n    name = experiment_config['name']\n\n    print(f'========================= Runing experiment for {name} =========================')\n    \n    # Apply the data transformations\n    data, pipeline, experiment_config = preprocess(data, experiment_config)\n    \n    # The data can change the number of features due to PCA\n    if 'n_features' in experiment_config:\n        model_config['n_features'] = experiment_config['n_features']\n    \n    # Obtain the model \n    model = get_model(model_config)\n    \n    # Train the model\n    current_metrics, model = train_model(model, data, experiment_config=experiment_config)\n\n    # Update the metrics\n    metrics = update_metrics(name, current_metrics, metrics)\n\n    # The model with their transformatios will be returned\n    results.append((model, pipeline))\n        \n    return metrics, results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **model = get_model(model_config)**\n\nLets start defining the get model function. This function will create the model based on the the configuration."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model(model_config):\n    \n    model_type = model_config['type']\n    model = None\n    \n    if model_type=='nn-mixed':\n        model = mixednn(model_config)\n    elif model_type=='nn-dense':\n        model = densenn(model_config)\n    elif model_type=='rf':\n        model = RandomForestClassifier(model_config['max_depth'], random_state=123, n_jobs=-1)\n    elif model_type=='lr':\n        model = OneVsRestClassifier(LogisticRegression(random_state=123, max_iter=model_config['max_iter'], solver=model_config['solver'], C=model_config['C']))\n    elif model_type=='xgb':\n        model = OneVsRestClassifier(XGBClassifier(max_depth=model_config['max_depth'], learning_rate=model_config['learning_rate'], n_estimators=model_config['n_estimators']\n                                                  , objective='binary:logistic', booster=model_config['booster'],n_jobs=-1))\n    elif model_type=='catboost':\n        model = OneVsRestClassifier(CatBoostClassifier(iterations=model_config['iterations'],random_state=123, logging_level='Silent'))\n    elif model_type=='knn':\n        model = MLkNN(k=model_config['k'])\n    elif model_type=='stabnet':\n        model = stabnet(model_config)\n    else:\n        raise Exception('No model provided')\n    \n    return model\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The neural networks will take more spce so I put them in different functions. There is two versions, a simple fully-connected neural network and a neural network that uses convolutions over the image representation over the genetic information and the cell information in images of 10x10 and 194x4 respectively."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Credits to -> https://github.com/titu1994/tf-TabNet\n# I just modified the activation to be a sigmoid\n\ndef register_keras_custom_object(cls):\n    tf.keras.utils.get_custom_objects()[cls.__name__] = cls\n    return cls\n\n\ndef glu(x, n_units=None):\n    \"\"\"Generalized linear unit nonlinear activation.\"\"\"\n    if n_units is None:\n        n_units = tf.shape(x)[-1] // 2\n\n    return x[..., :n_units] * tf.nn.sigmoid(x[..., n_units:])\n\n\n\"\"\"\nCode replicated from https://github.com/tensorflow/addons/blob/master/tensorflow_addons/activations/sparsemax.py\n\"\"\"\n\n\n@register_keras_custom_object\n@tf.function\ndef sparsemax(logits, axis):\n    \"\"\"Sparsemax activation function [1].\n    For each batch `i` and class `j` we have\n      $$sparsemax[i, j] = max(logits[i, j] - tau(logits[i, :]), 0)$$\n    [1]: https://arxiv.org/abs/1602.02068\n    Args:\n        logits: Input tensor.\n        axis: Integer, axis along which the sparsemax operation is applied.\n    Returns:\n        Tensor, output of sparsemax transformation. Has the same type and\n        shape as `logits`.\n    Raises:\n        ValueError: In case `dim(logits) == 1`.\n    \"\"\"\n    logits = tf.convert_to_tensor(logits, name=\"logits\")\n\n    # We need its original shape for shape inference.\n    shape = logits.get_shape()\n    rank = shape.rank\n    is_last_axis = (axis == -1) or (axis == rank - 1)\n\n    if is_last_axis:\n        output = _compute_2d_sparsemax(logits)\n        output.set_shape(shape)\n        return output\n\n    # If dim is not the last dimension, we have to do a transpose so that we can\n    # still perform softmax on its last dimension.\n\n    # Swap logits' dimension of dim and its last dimension.\n    rank_op = tf.rank(logits)\n    axis_norm = axis % rank\n    logits = _swap_axis(logits, axis_norm, tf.math.subtract(rank_op, 1))\n\n    # Do the actual softmax on its last dimension.\n    output = _compute_2d_sparsemax(logits)\n    output = _swap_axis(output, axis_norm, tf.math.subtract(rank_op, 1))\n\n    # Make shape inference work since transpose may erase its static shape.\n    output.set_shape(shape)\n    return output\n\n\ndef _swap_axis(logits, dim_index, last_index, **kwargs):\n    return tf.transpose(\n        logits,\n        tf.concat(\n            [\n                tf.range(dim_index),\n                [last_index],\n                tf.range(dim_index + 1, last_index),\n                [dim_index],\n            ],\n            0,\n        ),\n        **kwargs,\n    )\n\n\ndef _compute_2d_sparsemax(logits):\n    \"\"\"Performs the sparsemax operation when axis=-1.\"\"\"\n    shape_op = tf.shape(logits)\n    obs = tf.math.reduce_prod(shape_op[:-1])\n    dims = shape_op[-1]\n\n    # In the paper, they call the logits z.\n    # The mean(logits) can be substracted from logits to make the algorithm\n    # more numerically stable. the instability in this algorithm comes mostly\n    # from the z_cumsum. Substacting the mean will cause z_cumsum to be close\n    # to zero. However, in practise the numerical instability issues are very\n    # minor and substacting the mean causes extra issues with inf and nan\n    # input.\n    # Reshape to [obs, dims] as it is almost free and means the remanining\n    # code doesn't need to worry about the rank.\n    z = tf.reshape(logits, [obs, dims])\n\n    # sort z\n    z_sorted, _ = tf.nn.top_k(z, k=dims)\n\n    # calculate k(z)\n    z_cumsum = tf.math.cumsum(z_sorted, axis=-1)\n    k = tf.range(1, tf.cast(dims, logits.dtype) + 1, dtype=logits.dtype)\n    z_check = 1 + k * z_sorted > z_cumsum\n    # because the z_check vector is always [1,1,...1,0,0,...0] finding the\n    # (index + 1) of the last `1` is the same as just summing the number of 1.\n    k_z = tf.math.reduce_sum(tf.cast(z_check, tf.int32), axis=-1)\n\n    # calculate tau(z)\n    # If there are inf values or all values are -inf, the k_z will be zero,\n    # this is mathematically invalid and will also cause the gather_nd to fail.\n    # Prevent this issue for now by setting k_z = 1 if k_z = 0, this is then\n    # fixed later (see p_safe) by returning p = nan. This results in the same\n    # behavior as softmax.\n    k_z_safe = tf.math.maximum(k_z, 1)\n    indices = tf.stack([tf.range(0, obs), tf.reshape(k_z_safe, [-1]) - 1], axis=1)\n    tau_sum = tf.gather_nd(z_cumsum, indices)\n    tau_z = (tau_sum - 1) / tf.cast(k_z, logits.dtype)\n\n    # calculate p\n    p = tf.math.maximum(tf.cast(0, logits.dtype), z - tf.expand_dims(tau_z, -1))\n    # If k_z = 0 or if z = nan, then the input is invalid\n    p_safe = tf.where(\n        tf.expand_dims(\n            tf.math.logical_or(tf.math.equal(k_z, 0), tf.math.is_nan(z_cumsum[:, -1])),\n            axis=-1,\n        ),\n        tf.fill([obs, dims], tf.cast(float(\"nan\"), logits.dtype)),\n        p,\n    )\n\n    # Reshape back to original size\n    p_safe = tf.reshape(p_safe, shape_op)\n    return p_safe\n\n\n\"\"\"\nCode replicated from https://github.com/tensorflow/addons/blob/master/tensorflow_addons/layers/normalizations.py\n\"\"\"\n\n\n@register_keras_custom_object\nclass GroupNormalization(tf.keras.layers.Layer):\n    \"\"\"Group normalization layer.\n    Group Normalization divides the channels into groups and computes\n    within each group the mean and variance for normalization.\n    Empirically, its accuracy is more stable than batch norm in a wide\n    range of small batch sizes, if learning rate is adjusted linearly\n    with batch sizes.\n    Relation to Layer Normalization:\n    If the number of groups is set to 1, then this operation becomes identical\n    to Layer Normalization.\n    Relation to Instance Normalization:\n    If the number of groups is set to the\n    input dimension (number of groups is equal\n    to number of channels), then this operation becomes\n    identical to Instance Normalization.\n    Arguments\n        groups: Integer, the number of groups for Group Normalization.\n            Can be in the range [1, N] where N is the input dimension.\n            The input dimension must be divisible by the number of groups.\n        axis: Integer, the axis that should be normalized.\n        epsilon: Small float added to variance to avoid dividing by zero.\n        center: If True, add offset of `beta` to normalized tensor.\n            If False, `beta` is ignored.\n        scale: If True, multiply by `gamma`.\n            If False, `gamma` is not used.\n        beta_initializer: Initializer for the beta weight.\n        gamma_initializer: Initializer for the gamma weight.\n        beta_regularizer: Optional regularizer for the beta weight.\n        gamma_regularizer: Optional regularizer for the gamma weight.\n        beta_constraint: Optional constraint for the beta weight.\n        gamma_constraint: Optional constraint for the gamma weight.\n    Input shape\n        Arbitrary. Use the keyword argument `input_shape`\n        (tuple of integers, does not include the samples axis)\n        when using this layer as the first layer in a model.\n    Output shape\n        Same shape as input.\n    References\n        - [Group Normalization](https://arxiv.org/abs/1803.08494)\n    \"\"\"\n\n    def __init__(\n            self,\n            groups: int = 2,\n            axis: int = -1,\n            epsilon: float = 1e-3,\n            center: bool = True,\n            scale: bool = True,\n            beta_initializer=\"zeros\",\n            gamma_initializer=\"ones\",\n            beta_regularizer=None,\n            gamma_regularizer=None,\n            beta_constraint=None,\n            gamma_constraint=None,\n            **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.supports_masking = True\n        self.groups = groups\n        self.axis = axis\n        self.epsilon = epsilon\n        self.center = center\n        self.scale = scale\n        self.beta_initializer = tf.keras.initializers.get(beta_initializer)\n        self.gamma_initializer = tf.keras.initializers.get(gamma_initializer)\n        self.beta_regularizer = tf.keras.regularizers.get(beta_regularizer)\n        self.gamma_regularizer = tf.keras.regularizers.get(gamma_regularizer)\n        self.beta_constraint = tf.keras.constraints.get(beta_constraint)\n        self.gamma_constraint = tf.keras.constraints.get(gamma_constraint)\n        self._check_axis()\n\n    def build(self, input_shape):\n\n        self._check_if_input_shape_is_none(input_shape)\n        self._set_number_of_groups_for_instance_norm(input_shape)\n        self._check_size_of_dimensions(input_shape)\n        self._create_input_spec(input_shape)\n\n        self._add_gamma_weight(input_shape)\n        self._add_beta_weight(input_shape)\n        self.built = True\n        super().build(input_shape)\n\n    def call(self, inputs, training=None):\n        # Training=none is just for compat with batchnorm signature call\n        input_shape = tf.keras.backend.int_shape(inputs)\n        tensor_input_shape = tf.shape(inputs)\n\n        reshaped_inputs, group_shape = self._reshape_into_groups(\n            inputs, input_shape, tensor_input_shape\n        )\n\n        normalized_inputs = self._apply_normalization(reshaped_inputs, input_shape)\n\n        outputs = tf.reshape(normalized_inputs, tensor_input_shape)\n\n        return outputs\n\n    def get_config(self):\n        config = {\n            \"groups\": self.groups,\n            \"axis\": self.axis,\n            \"epsilon\": self.epsilon,\n            \"center\": self.center,\n            \"scale\": self.scale,\n            \"beta_initializer\": tf.keras.initializers.serialize(self.beta_initializer),\n            \"gamma_initializer\": tf.keras.initializers.serialize(\n                self.gamma_initializer\n            ),\n            \"beta_regularizer\": tf.keras.regularizers.serialize(self.beta_regularizer),\n            \"gamma_regularizer\": tf.keras.regularizers.serialize(\n                self.gamma_regularizer\n            ),\n            \"beta_constraint\": tf.keras.constraints.serialize(self.beta_constraint),\n            \"gamma_constraint\": tf.keras.constraints.serialize(self.gamma_constraint),\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def _reshape_into_groups(self, inputs, input_shape, tensor_input_shape):\n\n        group_shape = [tensor_input_shape[i] for i in range(len(input_shape))]\n        group_shape[self.axis] = input_shape[self.axis] // self.groups\n        group_shape.insert(self.axis, self.groups)\n        group_shape = tf.stack(group_shape)\n        reshaped_inputs = tf.reshape(inputs, group_shape)\n        return reshaped_inputs, group_shape\n\n    def _apply_normalization(self, reshaped_inputs, input_shape):\n\n        group_shape = tf.keras.backend.int_shape(reshaped_inputs)\n        group_reduction_axes = list(range(1, len(group_shape)))\n        axis = -2 if self.axis == -1 else self.axis - 1\n        group_reduction_axes.pop(axis)\n\n        mean, variance = tf.nn.moments(\n            reshaped_inputs, group_reduction_axes, keepdims=True\n        )\n\n        gamma, beta = self._get_reshaped_weights(input_shape)\n        normalized_inputs = tf.nn.batch_normalization(\n            reshaped_inputs,\n            mean=mean,\n            variance=variance,\n            scale=gamma,\n            offset=beta,\n            variance_epsilon=self.epsilon,\n        )\n        return normalized_inputs\n\n    def _get_reshaped_weights(self, input_shape):\n        broadcast_shape = self._create_broadcast_shape(input_shape)\n        gamma = None\n        beta = None\n        if self.scale:\n            gamma = tf.reshape(self.gamma, broadcast_shape)\n\n        if self.center:\n            beta = tf.reshape(self.beta, broadcast_shape)\n        return gamma, beta\n\n    def _check_if_input_shape_is_none(self, input_shape):\n        dim = input_shape[self.axis]\n        if dim is None:\n            raise ValueError(\n                \"Axis \" + str(self.axis) + \" of \"\n                                           \"input tensor should have a defined dimension \"\n                                           \"but the layer received an input with shape \" + str(input_shape) + \".\"\n            )\n\n    def _set_number_of_groups_for_instance_norm(self, input_shape):\n        dim = input_shape[self.axis]\n\n        if self.groups == -1:\n            self.groups = dim\n\n    def _check_size_of_dimensions(self, input_shape):\n\n        dim = input_shape[self.axis]\n        if dim < self.groups:\n            raise ValueError(\n                \"Number of groups (\" + str(self.groups) + \") cannot be \"\n                                                          \"more than the number of channels (\" + str(dim) + \").\"\n            )\n\n        if dim % self.groups != 0:\n            raise ValueError(\n                \"Number of groups (\" + str(self.groups) + \") must be a \"\n                                                          \"multiple of the number of channels (\" + str(dim) + \").\"\n            )\n\n    def _check_axis(self):\n\n        if self.axis == 0:\n            raise ValueError(\n                \"You are trying to normalize your batch axis. Do you want to \"\n                \"use tf.layer.batch_normalization instead\"\n            )\n\n    def _create_input_spec(self, input_shape):\n\n        dim = input_shape[self.axis]\n        self.input_spec = tf.keras.layers.InputSpec(\n            ndim=len(input_shape), axes={self.axis: dim}\n        )\n\n    def _add_gamma_weight(self, input_shape):\n\n        dim = input_shape[self.axis]\n        shape = (dim,)\n\n        if self.scale:\n            self.gamma = self.add_weight(\n                shape=shape,\n                name=\"gamma\",\n                initializer=self.gamma_initializer,\n                regularizer=self.gamma_regularizer,\n                constraint=self.gamma_constraint,\n            )\n        else:\n            self.gamma = None\n\n    def _add_beta_weight(self, input_shape):\n\n        dim = input_shape[self.axis]\n        shape = (dim,)\n\n        if self.center:\n            self.beta = self.add_weight(\n                shape=shape,\n                name=\"beta\",\n                initializer=self.beta_initializer,\n                regularizer=self.beta_regularizer,\n                constraint=self.beta_constraint,\n            )\n        else:\n            self.beta = None\n\n    def _create_broadcast_shape(self, input_shape):\n        broadcast_shape = [1] * len(input_shape)\n        broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n        broadcast_shape.insert(self.axis, self.groups)\n        return broadcast_shape\n    \nclass TransformBlock(tf.keras.Model):\n\n    def __init__(self, features,\n                 norm_type,\n                 momentum=0.9,\n                 virtual_batch_size=None,\n                 groups=2,\n                 block_name='',\n                 **kwargs):\n        super(TransformBlock, self).__init__(**kwargs)\n\n        self.features = features\n        self.norm_type = norm_type\n        self.momentum = momentum\n        self.groups = groups\n        self.virtual_batch_size = virtual_batch_size\n\n        self.transform = tf.keras.layers.Dense(self.features, use_bias=False, name=f'transformblock_dense_{block_name}')\n\n        if norm_type == 'batch':\n            self.bn = tf.keras.layers.BatchNormalization(axis=-1, momentum=momentum,\n                                                         virtual_batch_size=virtual_batch_size,\n                                                         name=f'transformblock_bn_{block_name}')\n\n        else:\n            self.bn = GroupNormalization(axis=-1, groups=self.groups, name=f'transformblock_gn_{block_name}')\n\n    def call(self, inputs, training=None):\n        x = self.transform(inputs)\n        x = self.bn(x, training=training)\n        return x\n\n\nclass TabNet(tf.keras.Model):\n\n    def __init__(self, feature_columns,\n                 feature_dim=64,\n                 output_dim=64,\n                 num_features=None,\n                 num_decision_steps=5,\n                 relaxation_factor=1.5,\n                 sparsity_coefficient=1e-5,\n                 norm_type='group',\n                 batch_momentum=0.98,\n                 virtual_batch_size=None,\n                 num_groups=2,\n                 epsilon=1e-5,\n                 **kwargs):\n        \"\"\"\n        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n        # Hyper Parameter Tuning (Excerpt from the paper)\n        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n        selection:\n            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n            overfitting and yield poor generalization.\n            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n            Nsteps value favors for a larger γ.\n            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n            much smaller than the batch size.\n            - Initially large learning rate is important, which should be gradually decayed until convergence.\n        Args:\n            feature_columns: The Tensorflow feature columns for the dataset.\n            feature_dim (N_a): Dimensionality of the hidden representation in feature\n                transformation block. Each layer first maps the representation to a\n                2*feature_dim-dimensional output and half of it is used to determine the\n                nonlinearity of the GLU activation where the other half is used as an\n                input to GLU, and eventually feature_dim-dimensional output is\n                transferred to the next layer.\n            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n                later mapped to the final classification or regression output.\n            num_features: The number of input features (i.e the number of columns for\n                tabular data assuming each feature is represented with 1 dimension).\n            num_decision_steps(N_steps): Number of sequential decision steps.\n            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n                feature at different decision steps. When it is 1, a feature is enforced\n                to be used only at one decision step and as it increases, more\n                flexibility is provided to use a feature at multiple decision steps.\n            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n                Sparsity may provide a favorable inductive bias for convergence to\n                higher accuracy for some datasets where most of the input features are redundant.\n            norm_type: Type of normalization to perform for the model. Can be either\n                'batch' or 'group'. 'group' is the default.\n            batch_momentum: Momentum in ghost batch normalization.\n            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n                overall batch size should be an integer multiple of virtual_batch_size.\n            num_groups: Number of groups used for group normalization.\n            epsilon: A small number for numerical stability of the entropy calculations.\n        \"\"\"\n        super(TabNet, self).__init__(**kwargs)\n\n        # Input checks\n        if feature_columns is not None:\n            if type(feature_columns) not in (list, tuple):\n                raise ValueError(\"`feature_columns` must be a list or a tuple.\")\n\n            if len(feature_columns) == 0:\n                raise ValueError(\"`feature_columns` must be contain at least 1 tf.feature_column !\")\n\n            if num_features is None:\n                num_features = len(feature_columns)\n            else:\n                num_features = int(num_features)\n\n        else:\n            if num_features is None:\n                raise ValueError(\"If `feature_columns` is None, then `num_features` cannot be None.\")\n\n        if num_decision_steps < 1:\n            raise ValueError(\"Num decision steps must be greater than 0.\")\n\n        if feature_dim <= output_dim:\n            raise ValueError(\"To compute `features_for_coef`, feature_dim must be larger than output dim\")\n\n        feature_dim = int(feature_dim)\n        output_dim = int(output_dim)\n        num_decision_steps = int(num_decision_steps)\n        relaxation_factor = float(relaxation_factor)\n        sparsity_coefficient = float(sparsity_coefficient)\n        batch_momentum = float(batch_momentum)\n        num_groups = max(1, int(num_groups))\n        epsilon = float(epsilon)\n\n        if relaxation_factor < 0.:\n            raise ValueError(\"`relaxation_factor` cannot be negative !\")\n\n        if sparsity_coefficient < 0.:\n            raise ValueError(\"`sparsity_coefficient` cannot be negative !\")\n\n        if virtual_batch_size is not None:\n            virtual_batch_size = int(virtual_batch_size)\n\n        if norm_type not in ['batch', 'group']:\n            raise ValueError(\"`norm_type` must be either `batch` or `group`\")\n\n        self.feature_columns = feature_columns\n        self.num_features = num_features\n        self.feature_dim = feature_dim\n        self.output_dim = output_dim\n\n        self.num_decision_steps = num_decision_steps\n        self.relaxation_factor = relaxation_factor\n        self.sparsity_coefficient = sparsity_coefficient\n        self.norm_type = norm_type\n        self.batch_momentum = batch_momentum\n        self.virtual_batch_size = virtual_batch_size\n        self.num_groups = num_groups\n        self.epsilon = epsilon\n\n        if num_decision_steps > 1:\n            features_for_coeff = feature_dim - output_dim\n            print(f\"[TabNet]: {features_for_coeff} features will be used for decision steps.\")\n\n        if self.feature_columns is not None:\n            self.input_features = tf.keras.layers.DenseFeatures(feature_columns, trainable=True)\n\n            if self.norm_type == 'batch':\n                self.input_bn = tf.keras.layers.BatchNormalization(axis=-1, momentum=batch_momentum, name='input_bn')\n            else:\n                self.input_bn = GroupNormalization(axis=-1, groups=self.num_groups, name='input_gn')\n\n        else:\n            self.input_features = None\n            self.input_bn = None\n\n        self.transform_f1 = TransformBlock(2 * self.feature_dim, self.norm_type,\n                                           self.batch_momentum, self.virtual_batch_size, self.num_groups,\n                                           block_name='f1')\n\n        self.transform_f2 = TransformBlock(2 * self.feature_dim, self.norm_type,\n                                           self.batch_momentum, self.virtual_batch_size, self.num_groups,\n                                           block_name='f2')\n\n        self.transform_f3_list = [\n            TransformBlock(2 * self.feature_dim, self.norm_type,\n                           self.batch_momentum, self.virtual_batch_size, self.num_groups, block_name=f'f3_{i}')\n            for i in range(self.num_decision_steps)\n        ]\n\n        self.transform_f4_list = [\n            TransformBlock(2 * self.feature_dim, self.norm_type,\n                           self.batch_momentum, self.virtual_batch_size, self.num_groups, block_name=f'f4_{i}')\n            for i in range(self.num_decision_steps)\n        ]\n\n        self.transform_coef_list = [\n            TransformBlock(self.num_features, self.norm_type,\n                           self.batch_momentum, self.virtual_batch_size, self.num_groups, block_name=f'coef_{i}')\n            for i in range(self.num_decision_steps - 1)\n        ]\n\n        self._step_feature_selection_masks = None\n        self._step_aggregate_feature_selection_mask = None\n\n    def call(self, inputs, training=None):\n        if self.input_features is not None:\n            features = self.input_features(inputs)\n            features = self.input_bn(features, training=training)\n\n        else:\n            features = inputs\n\n        batch_size = tf.shape(features)[0]\n        self._step_feature_selection_masks = []\n        self._step_aggregate_feature_selection_mask = None\n\n        # Initializes decision-step dependent variables.\n        output_aggregated = tf.zeros([batch_size, self.output_dim])\n        masked_features = features\n        mask_values = tf.zeros([batch_size, self.num_features])\n        aggregated_mask_values = tf.zeros([batch_size, self.num_features])\n        complementary_aggregated_mask_values = tf.ones(\n            [batch_size, self.num_features])\n\n        total_entropy = 0.0\n        entropy_loss = 0.\n\n        for ni in range(self.num_decision_steps):\n            # Feature transformer with two shared and two decision step dependent\n            # blocks is used below.=\n            transform_f1 = self.transform_f1(masked_features, training=training)\n            transform_f1 = glu(transform_f1, self.feature_dim)\n\n            transform_f2 = self.transform_f2(transform_f1, training=training)\n            transform_f2 = (glu(transform_f2, self.feature_dim) +\n                            transform_f1) * tf.math.sqrt(0.5)\n\n            transform_f3 = self.transform_f3_list[ni](transform_f2, training=training)\n            transform_f3 = (glu(transform_f3, self.feature_dim) +\n                            transform_f2) * tf.math.sqrt(0.5)\n\n            transform_f4 = self.transform_f4_list[ni](transform_f3, training=training)\n            transform_f4 = (glu(transform_f4, self.feature_dim) +\n                            transform_f3) * tf.math.sqrt(0.5)\n\n            if (ni > 0 or self.num_decision_steps == 1):\n                decision_out = tf.nn.relu(transform_f4[:, :self.output_dim])\n\n                # Decision aggregation.\n                output_aggregated += decision_out\n\n                # Aggregated masks are used for visualization of the\n                # feature importance attributes.\n                scale_agg = tf.reduce_sum(decision_out, axis=1, keepdims=True)\n\n                if self.num_decision_steps > 1:\n                    scale_agg = scale_agg / tf.cast(self.num_decision_steps - 1, tf.float32)\n\n                aggregated_mask_values += mask_values * scale_agg\n\n            features_for_coef = transform_f4[:, self.output_dim:]\n\n            if ni < (self.num_decision_steps - 1):\n                # Determines the feature masks via linear and nonlinear\n                # transformations, taking into account of aggregated feature use.\n                mask_values = self.transform_coef_list[ni](features_for_coef, training=training)\n                mask_values *= complementary_aggregated_mask_values\n                mask_values = sparsemax(mask_values, axis=-1)\n\n                # Relaxation factor controls the amount of reuse of features between\n                # different decision blocks and updated with the values of\n                # coefficients.\n                complementary_aggregated_mask_values *= (\n                        self.relaxation_factor - mask_values)\n\n                # Entropy is used to penalize the amount of sparsity in feature\n                # selection.\n                total_entropy += tf.reduce_mean(\n                    tf.reduce_sum(\n                        -mask_values * tf.math.log(mask_values + self.epsilon), axis=1)) / (\n                                     tf.cast(self.num_decision_steps - 1, tf.float32))\n\n                # Add entropy loss\n                entropy_loss = total_entropy\n\n                # Feature selection.\n                masked_features = tf.multiply(mask_values, features)\n\n                # Visualization of the feature selection mask at decision step ni\n                # tf.summary.image(\n                #     \"Mask for step\" + str(ni),\n                #     tf.expand_dims(tf.expand_dims(mask_values, 0), 3),\n                #     max_outputs=1)\n                mask_at_step_i = tf.expand_dims(tf.expand_dims(mask_values, 0), 3)\n                self._step_feature_selection_masks.append(mask_at_step_i)\n\n            else:\n                # This branch is needed for correct compilation by tf.autograph\n                entropy_loss = 0.\n\n        # Adds the loss automatically\n        self.add_loss(self.sparsity_coefficient * entropy_loss)\n\n        # Visualization of the aggregated feature importances\n        # tf.summary.image(\n        #     \"Aggregated mask\",\n        #     tf.expand_dims(tf.expand_dims(aggregated_mask_values, 0), 3),\n        #     max_outputs=1)\n\n        agg_mask = tf.expand_dims(tf.expand_dims(aggregated_mask_values, 0), 3)\n        self._step_aggregate_feature_selection_mask = agg_mask\n\n        return output_aggregated\n\n    @property\n    def feature_selection_masks(self):\n        return self._step_feature_selection_masks\n\n    @property\n    def aggregate_feature_selection_mask(self):\n        return self._step_aggregate_feature_selection_mask\n    \nclass StackedTabNet(tf.keras.Model):\n\n    def __init__(self, feature_columns,\n                 num_layers=1,\n                 feature_dim=64,\n                 output_dim=64,\n                 num_features=None,\n                 num_decision_steps=5,\n                 relaxation_factor=1.5,\n                 sparsity_coefficient=1e-5,\n                 norm_type='group',\n                 batch_momentum=0.98,\n                 virtual_batch_size=None,\n                 num_groups=2,\n                 epsilon=1e-5,\n                 **kwargs):\n        \"\"\"\n        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n        Stacked variant of the TabNet model, which stacks multiple TabNets into a singular model.\n        # Hyper Parameter Tuning (Excerpt from the paper)\n        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n        selection:\n            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n            overfitting and yield poor generalization.\n            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n            Nsteps value favors for a larger γ.\n            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n            much smaller than the batch size.\n            - Initially large learning rate is important, which should be gradually decayed until convergence.\n        Args:\n            feature_columns: The Tensorflow feature columns for the dataset.\n            num_layers: Number of TabNets to stack together.\n            feature_dim (N_a): Dimensionality of the hidden representation in feature\n                transformation block. Each layer first maps the representation to a\n                2*feature_dim-dimensional output and half of it is used to determine the\n                nonlinearity of the GLU activation where the other half is used as an\n                input to GLU, and eventually feature_dim-dimensional output is\n                transferred to the next layer. Can be either a single int, or a list of\n                integers. If a list, must be of same length as the number of layers.\n            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n                later mapped to the final classification or regression output.\n                Can be either a single int, or a list of\n                integers. If a list, must be of same length as the number of layers.\n            num_features: The number of input features (i.e the number of columns for\n                tabular data assuming each feature is represented with 1 dimension).\n            num_decision_steps(N_steps): Number of sequential decision steps.\n            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n                feature at different decision steps. When it is 1, a feature is enforced\n                to be used only at one decision step and as it increases, more\n                flexibility is provided to use a feature at multiple decision steps.\n            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n                Sparsity may provide a favorable inductive bias for convergence to\n                higher accuracy for some datasets where most of the input features are redundant.\n            norm_type: Type of normalization to perform for the model. Can be either\n                'batch' or 'group'. 'group' is the default.\n            batch_momentum: Momentum in ghost batch normalization.\n            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n                overall batch size should be an integer multiple of virtual_batch_size.\n            num_groups: Number of groups used for group normalization.\n            epsilon: A small number for numerical stability of the entropy calculations.\n        \"\"\"\n        super(StackedTabNet, self).__init__(**kwargs)\n\n        if num_layers < 1:\n            raise ValueError(\"`num_layers` cannot be less than 1\")\n\n        if type(feature_dim) not in [list, tuple]:\n            feature_dim = [feature_dim] * num_layers\n\n        if type(output_dim) not in [list, tuple]:\n            output_dim = [output_dim] * num_layers\n\n        if len(feature_dim) != num_layers:\n            raise ValueError(\"`feature_dim` must be a list of length `num_layers`\")\n\n        if len(output_dim) != num_layers:\n            raise ValueError(\"`output_dim` must be a list of length `num_layers`\")\n\n        self.num_layers = num_layers\n\n        layers = []\n        layers.append(TabNet(feature_columns=feature_columns,\n                             num_features=num_features,\n                             feature_dim=feature_dim[0],\n                             output_dim=output_dim[0],\n                             num_decision_steps=num_decision_steps,\n                             relaxation_factor=relaxation_factor,\n                             sparsity_coefficient=sparsity_coefficient,\n                             norm_type=norm_type,\n                             batch_momentum=batch_momentum,\n                             virtual_batch_size=virtual_batch_size,\n                             num_groups=num_groups,\n                             epsilon=epsilon))\n\n        for layer_idx in range(1, num_layers):\n            layers.append(TabNet(feature_columns=None,\n                                 num_features=output_dim[layer_idx - 1],\n                                 feature_dim=feature_dim[layer_idx],\n                                 output_dim=output_dim[layer_idx],\n                                 num_decision_steps=num_decision_steps,\n                                 relaxation_factor=relaxation_factor,\n                                 sparsity_coefficient=sparsity_coefficient,\n                                 norm_type=norm_type,\n                                 batch_momentum=batch_momentum,\n                                 virtual_batch_size=virtual_batch_size,\n                                 num_groups=num_groups,\n                                 epsilon=epsilon))\n\n        self.tabnet_layers = layers\n\n    def call(self, inputs, training=None):\n        x = self.tabnet_layers[0](inputs, training=training)\n\n        for layer_idx in range(1, self.num_layers):\n            x = self.tabnet_layers[layer_idx](x, training=training)\n\n        return x\n\n    @property\n    def tabnets(self):\n        return self.tabnet_layers\n\n    @property\n    def feature_selection_masks(self):\n        return [tabnet.feature_selection_masks\n                for tabnet in self.tabnet_layers]\n\n    @property\n    def aggregate_feature_selection_mask(self):\n        return [tabnet.aggregate_feature_selection_mask\n                for tabnet in self.tabnet_layers]\n\n    \nclass StackedTabNetClassifier(tf.keras.Model):\n\n    def __init__(self, feature_columns,\n                 num_classes,\n                 num_layers=1,\n                 feature_dim=64,\n                 output_dim=64,\n                 num_features=None,\n                 num_decision_steps=5,\n                 relaxation_factor=1.5,\n                 sparsity_coefficient=1e-5,\n                 norm_type='group',\n                 batch_momentum=0.98,\n                 virtual_batch_size=None,\n                 num_groups=2,\n                 epsilon=1e-5,\n                 **kwargs):\n        \"\"\"\n        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n        Stacked variant of the TabNet model, which stacks multiple TabNets into a singular model.\n        # Hyper Parameter Tuning (Excerpt from the paper)\n        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n        selection:\n            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n            overfitting and yield poor generalization.\n            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n            Nsteps value favors for a larger γ.\n            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n            much smaller than the batch size.\n            - Initially large learning rate is important, which should be gradually decayed until convergence.\n        Args:\n            feature_columns: The Tensorflow feature columns for the dataset.\n            num_classes: Number of classes.\n            num_layers: Number of TabNets to stack together.\n            feature_dim (N_a): Dimensionality of the hidden representation in feature\n                transformation block. Each layer first maps the representation to a\n                2*feature_dim-dimensional output and half of it is used to determine the\n                nonlinearity of the GLU activation where the other half is used as an\n                input to GLU, and eventually feature_dim-dimensional output is\n                transferred to the next layer. Can be either a single int, or a list of\n                integers. If a list, must be of same length as the number of layers.\n            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n                later mapped to the final classification or regression output.\n                Can be either a single int, or a list of\n                integers. If a list, must be of same length as the number of layers.\n            num_features: The number of input features (i.e the number of columns for\n                tabular data assuming each feature is represented with 1 dimension).\n            num_decision_steps(N_steps): Number of sequential decision steps.\n            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n                feature at different decision steps. When it is 1, a feature is enforced\n                to be used only at one decision step and as it increases, more\n                flexibility is provided to use a feature at multiple decision steps.\n            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n                Sparsity may provide a favorable inductive bias for convergence to\n                higher accuracy for some datasets where most of the input features are redundant.\n            norm_type: Type of normalization to perform for the model. Can be either\n                'batch' or 'group'. 'group' is the default.\n            batch_momentum: Momentum in ghost batch normalization.\n            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n                overall batch size should be an integer multiple of virtual_batch_size.\n            num_groups: Number of groups used for group normalization.\n            epsilon: A small number for numerical stability of the entropy calculations.\n        \"\"\"\n        super(StackedTabNetClassifier, self).__init__(**kwargs)\n\n        self.num_classes = num_classes\n\n        self.stacked_tabnet = StackedTabNet(feature_columns=feature_columns,\n                                            num_layers=num_layers,\n                                            feature_dim=feature_dim,\n                                            output_dim=output_dim,\n                                            num_features=num_features,\n                                            num_decision_steps=num_decision_steps,\n                                            relaxation_factor=relaxation_factor,\n                                            sparsity_coefficient=sparsity_coefficient,\n                                            norm_type=norm_type,\n                                            batch_momentum=batch_momentum,\n                                            virtual_batch_size=virtual_batch_size,\n                                            num_groups=num_groups,\n                                            epsilon=epsilon)\n\n        self.clf = tf.keras.layers.Dense(num_classes, activation='sigmoid', use_bias=False)\n\n    def call(self, inputs, training=None):\n        self.activations = self.stacked_tabnet(inputs, training=training)\n        out = self.clf(self.activations)\n\n        return out\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def stabnet(model_config):\n    feature_columns = []\n    for col_name in model_config['feature_columns']:\n        feature_columns.append(tf.feature_column.numeric_column(col_name))\n    model = StackedTabNetClassifier(num_classes=model_config['n_labels'],  feature_columns=feature_columns, num_layers=model_config['n_layers'], num_features=model_config['n_features'], feature_dim=model_config['feature_dim'], output_dim=model_config['output_dim'])\n    \n    optimizer = tfa.optimizers.RectifiedAdam()\n    if model_config['use_loookahead']:\n        optimizer = tfa.optimizers.Lookahead(optimizer, sync_period=10)\n    \n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    #model.build((None, model_config['n_features']))\n    \n    return model\n\ndef densenn(model_config):\n    model = tf.keras.Sequential()\n    \n    layers = model_config['n_layers']\n    use_weight_normalization =  model_config['use_weight_normalization']\n    use_loookahead = model_config['use_loookahead']\n    units = model_config['units']\n    use_batch_norm = model_config['use_batch_norm']\n    use_mish = model_config['use_mish']\n    drop_rate= model_config['drop_rate']\n    n_features = model_config['n_features']\n    n_labels = model_config['n_labels']\n    \n    for l in range(layers):\n        \n        if use_batch_norm:\n             model.add(tf.keras.layers.BatchNormalization())\n                \n        dense = tf.keras.layers.Dense(units=units)\n        \n        if use_weight_normalization:\n             dense = tfa.layers.WeightNormalization(dense)\n        model.add(dense)\n        \n        if use_mish:\n            model.add(tf.keras.layers.Activation(tfa.activations.mish))\n        else:\n            model.add(tf.keras.layers.Activation('relu'))\n            \n        model.add(tf.keras.layers.Dropout(drop_rate))\n    \n    dense = tf.keras.layers.Dense(n_labels, activation='sigmoid')\n    if use_weight_normalization:\n         dense = tfa.layers.WeightNormalization(dense)\n            \n    model.add(dense)\n    #learning_rate = hp.Float('learning_rate', min_value=3e-4, max_value=3e-3)\n    \n    optimizer = tfa.optimizers.RectifiedAdam()\n    if use_loookahead:\n        optimizer = tfa.optimizers.Lookahead(optimizer, sync_period=10)\n    \n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    model.build((None, n_features))\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def mixednn(model_config):\n    input_cell = tf.keras.layers.Input((10,10,1))\n    input_gen = tf.keras.layers.Input((193,4,1))\n    input_cats = tf.keras.layers.Input((4,))\n    \n    \"\"\"Parameters\"\"\"\n    \n    layers_cell = model_config['layers_cell']\n    layers_gen = model_config['layers_gen']\n    layers_final = model_config['layers_final']\n    use_mish = model_config['use_mish']\n    \n    use_weight_normalization =  model_config['use_weight_normalization']\n    use_loookahead = model_config['use_loookahead']\n    num_units = model_config['units']\n    use_batch_norm = model_config['use_batch_norm']\n    drop_rate= model_config['drop_rate']\n    n_features = model_config['n_features']\n    n_labels = model_config['n_labels']\n    mask_size = model_config['mask_size']\n    \n    \"\"\"Cell layer\"\"\"\n    for lc in range(layers_cell):\n        \n        conv_layer = tf.keras.layers.Conv2D(num_units, (mask_size, mask_size), activation='linear', padding='same')\n        if use_weight_normalization:\n            output_cell = tfa.layers.WeightNormalization(conv_layer)(input_cell)\n        else:\n            output_cell = conv_layer(input_cell)\n        \n        if use_batch_norm:\n            output_cell = tf.keras.layers.BatchNormalization()(output_cell)\n            \n        if use_mish:\n            output_cell = tf.keras.layers.Activation(tfa.activations.mish)(output_cell)\n        else:\n            output_cell = tf.keras.layers.Activation('relu')(output_cell)\n            \n        output_cell = tf.keras.layers.Dropout(drop_rate)(output_cell)\n    \n    \"\"\"Gen layer\"\"\"\n    for gc in range(layers_gen):\n\n        conv_layer = tf.keras.layers.Conv2D(num_units, (mask_size, mask_size), activation='linear', padding='same')\n        \n        if use_weight_normalization:\n             output_gen = tfa.layers.WeightNormalization(conv_layer)(input_gen)\n        else:\n            output_gen = conv_layer(input_gen)\n        \n        if use_batch_norm:\n            output_gen = tf.keras.layers.BatchNormalization()(output_gen)\n        \n        if use_mish:\n            output_gen = tf.keras.layers.Activation(tfa.activations.mish)(output_gen)\n        else:\n            output_gen = tf.keras.layers.Activation('relu')(output_gen)\n            \n        output_gen = tf.keras.layers.Dropout(drop_rate)(output_gen)\n\n    output_cell = tf.keras.layers.Flatten()(output_cell)\n    output_gen = tf.keras.layers.Flatten()(output_gen)\n\n    output = tf.keras.layers.Concatenate()([output_cell, output_gen, input_cats])\n    \n    \"\"\"Final layer\"\"\"\n    for lf in range(layers_final):\n        \n        dense = tf.keras.layers.Dense(num_units, activation='linear')(output)\n        if use_weight_normalization:\n             output = tfa.layers.WeightNormalization(dense)(output)\n        else:\n            output = dense(output)\n            \n        if use_batch_norm:\n            output = tf.keras.layers.BatchNormalization()(output)\n            \n        if use_mish:\n            output = tf.keras.layers.Activation(tfa.activations.mish)(output)\n        else:\n            output = tf.keras.layers.Activation('relu')(output)\n            \n        output = tf.keras.layers.Dropout(drop_rate)(output)\n    \n    output = tf.keras.layers.Dense(n_labels, activation='sigmoid')(output)\n\n    model = tf.keras.Model(inputs=[input_cell, input_gen, input_cats], outputs=[output])\n\n    optimizer = tfa.optimizers.RectifiedAdam()\n    if use_loookahead:\n        optimizer = tfa.optimizers.Lookahead(optimizer, sync_period=10)\n    bce = tf.keras.losses.BinaryCrossentropy() \n\n    model.compile(optimizer=optimizer,\n              loss=bce,\n              metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets test it!"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_configs = [\n        {'type': 'stabnet', 'n_layers': 1, 'feature_dim': 512, 'output_dim': 256,'use_loookahead': True, 'feature_columns': list(train_short.columns),  'n_features': train_short.shape[1],'n_labels': labels_short.shape[1]},\n    {'type': 'nn-mixed', 'layers_cell': 1, 'layers_gen': 1, 'layers_final': 0, 'units': 32, 'drop_rate': 0.3, \n     'use_weight_normalization': False, 'use_loookahead': False, 'use_batch_norm': False, 'n_features': train_short.shape[1],\n     'n_labels': labels_short.shape[1], 'mask_size':3, 'use_mish':False},\n      {'type': 'nn-dense', 'n_layers': 2, 'units': 256, 'drop_rate': 0.3, \n     'use_weight_normalization': False, 'use_loookahead': False, 'use_batch_norm': False, 'n_features': train_short.shape[1],\n     'n_labels': labels_short.shape[1], 'use_mish':False},\n    {'type': 'rf', 'max_depth': 20},\n    {'type': 'lr', 'max_iter': 100, 'solver': 'lbfgs', 'C': 1},\n    {'type': 'xgb', 'max_depth': 20, 'learning_rate': 0.1, 'n_estimators':  50, 'booster':'gbtree'},\n    {'type': 'catboost', 'iterations': 10},\n    {'type': 'knn', 'k': 3}\n]\nmodels = [get_model(config) for config in model_configs]\nmodels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### data, pipeline = preprocess(data, experiment_config)\n\nThe next function will be the preprocess function, this function will apply the transformations to the data described in experiment_config."},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(data, experiment_config, transformations=[], prefit=False, is_testing=False):\n    \n    pipeline = get_pipeline(experiment_config)        \n    \n    train_data = data[:2]\n    if not is_testing:\n        valid_data = data[2:]\n    \n    fitted_transformations = []\n    for idx, (name, transformation) in enumerate(pipeline):\n        \n        if len(transformations)>0:\n            transformer = transformations[idx]\n        else:\n            transformer = None\n            \n        train_data, transformer, experiment_config = transformation(train_data, experiment_config, prefit, transformer=transformer)\n        if not is_testing and name != 'do_mlsmote':\n            valid_data, _, _ = transformation(valid_data, experiment_config, prefit=True, transformer=transformer)\n        \n        fitted_transformations.append(transformer)\n    \n    data_processed = []\n    data_processed.extend(train_data)\n    if not is_testing:\n        data_processed.extend(valid_data)\n        \n    return data_processed, fitted_transformations, experiment_config","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will define the get_pipeline method that instantiate the transformations. Each transformation will take the (features, labels) pair and will transform it."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_pipeline(experiment_config):\n    \n    pipeline = []\n    \n    if experiment_config.get('do_mlsmote', False):\n        pass\n    if experiment_config.get('do_power_transform', False):\n        pipeline.append(('do_power_transform',power_transform))\n    if experiment_config.get('do_pca', False):\n        pipeline.append(('do_pca',pca_transform))\n    if experiment_config.get('do_scale', False):\n        pipeline.append(('do_scale', scaler_transform)) \n    if experiment_config.get('do_mixed', False):\n        pipeline.append(('do_mixed',mixed_representation))\n    if experiment_config.get('do_tf_dataset', False):\n        pipeline.append(('do_tf_dataset',make_tf_dataset))\n        \n    return pipeline\n\ndef scaler_transform(data, experiment_config, prefit=False, transformer=None):\n    features, labels = data\n    if prefit:\n        new_features = transformer.transform(features)\n    else:\n        transformer = MinMaxScaler()\n        new_features = transformer.fit_transform(features)\n    \n    return [new_features, labels], transformer, experiment_config\n\ndef power_transform(data, experiment_config, prefit=False, transformer=None):\n    features, labels = data\n    if prefit:\n        new_features = transformer.transform(features)\n    else:\n        transformer = PowerTransformer()\n        new_features = transformer.fit_transform(features)\n    \n    return [new_features, labels], transformer, experiment_config\n\ndef pca_transform(data, experiment_config, prefit=False, transformer=None):\n    features, labels = data\n    if prefit:\n        new_features = transformer.transform(features)\n    else:\n        transformer = PCA(n_components=experiment_config['min_variance'], random_state=123)\n        new_features = transformer.fit_transform(features)\n        experiment_config['n_features'] = new_features.shape[1]\n    \n    return [new_features, labels], transformer, experiment_config\n\ndef transform_map(x, y, experiment_config):\n    features = tf.unstack(x)\n    labels = y\n    \n    x = dict(zip(experiment_config['columns'], features))\n    y = labels\n    return x, y\n\ndef make_tf_dataset(data, experiment_config, prefit=False, transformer=None):\n    features, labels = data\n    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n    \n    if not prefit: # Not shuffle the validation data\n        dataset = dataset.shuffle(labels.shape[0], seed=123)\n    \n    if experiment_config.get('ds_map', False):\n        dataset = dataset.map(lambda x, y: transform_map(x, y, experiment_config))\n        \n    dataset = dataset.batch(experiment_config['batch_size'])#.prefetch(experiment_config['batch_size'])\n    \n    return [dataset], transformer, experiment_config\n\ndef mixed_representation(data, experiment_config, prefit=False, transformer=None):\n    features, labels = data\n    \n    cell_data_train = features[:, experiment_config['cell_cols_mask']].reshape(-1, 10,10)\n    gen_data_train = features[:, experiment_config['gen_cols_mask']].reshape(-1, 193,4)\n    cat_data_train = features[:, experiment_config['cat_cols_mask']]\n    new_features = {'input1':cell_data_train, 'input2': gen_data_train, 'input3':cat_data_train}\n    \n    return [new_features, labels], transformer, experiment_config","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [train_short.values, labels_short.values]\nexperiment_config = {'do_power_transform': False, 'do_pca': False, 'do_scale': True, 'do_tf_dataset': False, 'min_variance': 0.9}\ndata_np = iterative_train_test_split(train.values, labels.values, .2)\ndata, pipeline, experiment_config = preprocess(data_np, experiment_config)\nprint(data_np[0])\nprint(data[0].min())\nprint(data[0].max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [train_short.values, labels_short.values]\n\ncell_cols_mask = ['c-' in col for col in train_short.columns]\ngen_cols_mask = ['g-' in col for col in train_short.columns]\ncat_cols_mask = [('g-' not in col and 'c-' not in col) for col in train_short.columns]\n\nexperiment_config = {'do_power_transform': False, 'do_pca': False, 'do_scale': True, 'do_tf_dataset': True, 'min_variance': 0.9, 'batch_size': 128, 'do_mixed': True,\n                    'cell_cols_mask': cell_cols_mask, 'gen_cols_mask': gen_cols_mask, 'cat_cols_mask': cat_cols_mask}\ndata, pipeline,experiment_config = preprocess([X_train, Y_train, X_valid, Y_valid], experiment_config)\nprint(data)\nx, y = data\nfor x, y in data[0]:\n    print(x['input1'].shape)\n    print(y.shape)\n    break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### current_metrics, model = train_model(model, data, experiment_config=experiment_config)\n\nNow its turn of the function that will train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, data, experiment_config, has_valid=True):\n    \n    metrics = pd.DataFrame(columns = ['log_loss', 'hamming_loss', 'accuracy'])\n    \n    best_model = None\n\n    if experiment_config.get('do_tf_dataset', False) or experiment_config.get('is_tf', False):\n        if has_valid:\n            train_dataset, valid_dataset  = data\n            model.fit(train_dataset, validation_data=valid_dataset, callbacks=experiment_config['callbacks'], epochs=experiment_config['epochs'], verbose=0)\n\n            Y_valid = np.concatenate(np.array(list(valid_dataset))[:,1], axis=0)\n            predictions = model.predict(valid_dataset)\n        else:\n            train_dataset = data[0]\n            model.fit(train_dataset, callbacks=experiment_config['callbacks'], epochs=experiment_config['epochs'], verbose=0) \n\n    else:\n        if has_valid:\n            X_train, Y_train, X_valid, Y_valid = data\n        else:\n            X_train, Y_train = data\n            \n        model.fit(X_train, Y_train)\n        \n        if has_valid:\n            predictions = model.predict_proba(X_valid)\n\n            if type(predictions) == csr_matrix or type(predictions) == lil_matrix:\n                predictions = predictions.toarray()\n            elif len(np.array(predictions).shape)>2:\n                predictions = np.array(predictions)[:,:,1].T \n                \n    if has_valid:\n        metrics = calculate_metrics(Y_valid, np.array(predictions))\n\n    return metrics, model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  metrics = update_metrics(name, current_metrics, metrics)\n\nFinally the metrics functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_loss_score(actual, predicted,  eps=1e-15):\n        \n    p1 = actual * np.log(predicted+eps)\n    p0 = (1-actual) * np.log(1-predicted+eps)\n    loss = p0 + p1\n\n    return -loss.mean()\n\ndef log_loss_multi(y_true, y_pred):\n    n_labels = y_true.shape[1]\n    results = np.zeros(n_labels)\n    for i in range(n_labels):\n        true = np.argwhere(y_true[:,i] > 0.01)\n        print(true)\n        results[i] = log_loss(y_true[:,i], y_pred[:,i])\n    return results.mean()\n        \n\ndef calculate_metrics(real, predictions):\n    index = 0\n    metrics = {'log_loss': 0, 'hamming_loss': 0, 'accuracy': 0}\n    n_labels = real.shape[1]\n    \n    predictions = np.clip(np.array(predictions), 1e-15, 1-1e-15)\n    predictions_minus_1 = np.clip(np.array(1-predictions), 1e-15, 1-1e-15)\n    metrics['log_loss'] = -(real*np.log(predictions)+ (1-real)*np.log(predictions_minus_1)).mean() #log_loss_multi(real, predictions)\n    metrics['hamming_loss'] = hamming_loss(real, predictions.astype(int))\n    metrics['accuracy'] = accuracy_score(real, predictions.astype(int))\n    \n    metrics = pd.DataFrame(metrics, index=[index])\n    index += 1\n    return metrics\n\ndef update_metrics(name, current_metrics, metrics=None):\n    if metrics is None:\n        metrics = pd.DataFrame(columns = ['id', 'log_loss', 'hamming_loss', 'accuracy'])\n        \n    current_metrics['id'] = name\n    metrics = pd.concat([metrics, current_metrics])\n    \n    return metrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally some testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"experiment_config = {'name':'Dense-NN','is_tf': True, 'do_scale': True, 'do_tf_dataset': True,  'batch_size': 128, 'callbacks': None, 'epochs': 30,  'do_pca': False,  'min_variance': 0.8}\n\n\ndata_np = iterative_train_test_split(train_short.values, labels_short.values, .2)\ndata, pipeline, experiment_config = preprocess(data_np, experiment_config)\n\nmodel_config =  {'type': 'nn-dense', 'n_layers': 5, 'units': 512, 'drop_rate': 0.2, \n     'use_weight_normalization': True, 'use_loookahead': True, 'use_batch_norm': True, 'n_features': train.shape[1], #experiment_config['n_features'],\n     'n_labels': labels_short.shape[1], 'use_mish':True}\n\nmodel = get_model(model_config)\ntrain_dataset, valid_dataset  = data\n\nmetrics, model = train_model(model, data, experiment_config)\nmetrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model experimentation"},{"metadata":{},"cell_type":"markdown","source":"Let make the experimentation with different models and see the results for each one"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cell_cols_mask = ['c-' in col for col in train.columns]\ngen_cols_mask = ['g-' in col for col in train.columns]\ncat_cols_mask = [('g-' not in col and 'c-' not in col) for col in train.columns]\n\nexperiment_configs = [\n                    {'name':'Tabnet','is_tf': True, 'do_scale': True, 'do_tf_dataset': True,  'batch_size': 128, 'callbacks': None, 'epochs': 30, 'ds_map': True,'columns': list(train_short.columns)},\n                    {'name':'NN-Mixed', 'is_tf': True, 'do_scale': True, 'do_tf_dataset': True, 'batch_size': 128, 'callbacks': None, 'epochs': 30, 'do_mixed': True,\n                     'cell_cols_mask': cell_cols_mask, 'gen_cols_mask': gen_cols_mask, 'cat_cols_mask': cat_cols_mask}, \n    \n                     {'name':'Dense-NN','is_tf': True, 'do_scale': True, 'do_tf_dataset': True,  'batch_size': 128, 'callbacks': None, 'epochs': 30},\n                     \n                    {'name':'KNN','do_scale': True},\n                     {'name':'RandomForest'},\n                     {'name':'LogisticRegression','do_scale': True,}, {'name':'XGB'}, {'name':'Catboost'}\n                     \n                    ]\n\ncell_cols_mask = ['c-' in col for col in train_short.columns]\ngen_cols_mask = ['g-' in col for col in train_short.columns]\ncat_cols_mask = [('g-' not in col and 'c-' not in col) for col in train_short.columns]\n\nmodel_configs = [\n    {'type': 'stabnet', 'n_layers': 1, 'feature_dim': 512, 'output_dim': 256,'use_loookahead': True, 'feature_columns': list(train_short.columns),  'n_features': train_short.shape[1],'n_labels': labels_short.shape[1]},\n    {'type': 'nn-mixed', 'layers_cell': 1, 'layers_gen': 1, 'layers_final': 0, 'units': 32, 'drop_rate': 0.3, \n     'use_weight_normalization': False, 'use_loookahead': True, 'use_batch_norm': False, 'n_features': train_short.shape[1],\n     'n_labels': labels_short.shape[1], 'mask_size':3, 'use_mish':True},\n    {'type': 'nn-dense', 'n_layers': 2, 'units': 256, 'drop_rate': 0.3, \n     'use_weight_normalization': False, 'use_loookahead': True, 'use_batch_norm': False, 'n_features': train_short.shape[1],\n     'n_labels': labels_short.shape[1], 'use_mish':True},\n    \n    {'type': 'knn', 'k': 3},\n    { 'type': 'rf', 'max_depth': 20, 'n_estimators':  100},\n    {'type': 'lr', 'max_iter': 200, 'solver': 'lbfgs', 'C': 1},\n    {'type': 'xgb', 'max_depth': 20, 'learning_rate': 0.1, 'n_estimators':  100, 'booster':'gbtree'},\n    {'type': 'catboost', 'iterations': 100}\n]\n\nconfigurations = zip(model_configs, experiment_config)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\n\nmetrics = None\n\nconfigurations = zip(model_configs, experiment_configs)\nfor model_config, experiment_config in configurations:     \n    config = {'model_config': model_config, 'experiment_config': experiment_config} \n    data = iterative_train_test_split(train_short.values, labels_short.values, .2)\n    model = get_model(model_config)\n    experiment_metrics, _ = run_experiment(data, config)\n    metrics = update_metrics(experiment_config['name'], experiment_metrics, metrics)\nmetrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A simple logistic regression seems to be the best option, lets improve it."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"experiment_configs = [\n                     {'name':'LogisticRegression-Base','do_scale': True},\n                     {'name':'LogisticRegression-Power','do_scale': True, 'do_power_transform': True},\n                     {'name':'LogisticRegression-PCA','do_scale': True, 'do_pca': True,  'min_variance': 0.8},\n                    {'name':'LogisticRegression-Power+PCA','do_scale': True, 'do_pca': True, 'do_power_transform': True, 'min_variance': 0.8}\n                    ]\n\nmodel_configs = [\n    {'type': 'lr', 'max_iter': 500, 'solver': 'lbfgs', 'C': 1},\n    {'type': 'lr', 'max_iter': 500, 'solver': 'lbfgs', 'C': 1},\n    {'type': 'lr', 'max_iter': 500, 'solver': 'lbfgs', 'C': 1},\n    {'type': 'lr', 'max_iter': 500, 'solver': 'lbfgs', 'C': 1}\n]\n\nconfigurations = zip(model_configs, experiment_config)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\n\nmetrics_lr = None\n\nconfigurations = zip(model_configs, experiment_configs)\nfor model_config, experiment_config in configurations:\n    config = {'model_config': model_config, 'experiment_config': experiment_config} \n    data = iterative_train_test_split(train_short.values, labels_short.values, .2)\n    model = get_model(model_config)\n    print(model)\n    experiment_metrics, _ = run_experiment(data, config)\n    metrics_lr = update_metrics(experiment_config['name'], experiment_metrics, metrics_lr)\nmetrics_lr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets try improve the neural network option because it has more hyperparameters to tune and compare it to logistic regression with all the dataset. The tabnet will not be tested even being an neurall network because it has almost the worst losgloss."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\n\nearly_stop = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', min_lr = 1e-7,  factor=0.1, patience=3, verbose=1)\ncallbacks = [early_stop, reduce_lr]\n\nexperiment_configs = [\n                     {'name':'LogisticRegression-Base','do_scale': True},\n                        \n                     {'name':'Dense-NN-original','is_tf': True, 'do_scale': True, 'do_tf_dataset': True,  'batch_size': 128, 'callbacks': callbacks, 'epochs': 30},\n                     {'name':'Dense-NN-lookahead','is_tf': True, 'do_scale': True, 'do_tf_dataset': True,  'batch_size': 128, 'callbacks': callbacks, 'epochs': 30},\n                     {'name':'Dense-NN-batchnorm','is_tf': True, 'do_scale': True, 'do_tf_dataset': True,  'batch_size': 128, 'callbacks': callbacks, 'epochs': 30},\n                     {'name':'Dense-NN-weightnorm','is_tf': True, 'do_scale': True, 'do_tf_dataset': True,  'batch_size': 128, 'callbacks': callbacks, 'epochs': 30},\n                     {'name':'Dense-NN-mish','is_tf': True, 'do_scale': True, 'do_tf_dataset': True,  'batch_size': 128, 'callbacks': callbacks, 'epochs': 30},\n                     {'name':'Dense-NN-all','is_tf': True, 'do_scale': True, 'do_tf_dataset': True,  'batch_size': 128, 'callbacks': callbacks, 'epochs': 30},\n                     {'name':'Dense-NN-PCA','is_tf': True, 'do_scale': True, 'do_tf_dataset': True, 'do_pca': True, 'batch_size': 128, 'callbacks': callbacks, 'epochs': 30, 'min_variance': 0.8},\n                     {'name':'Dense-NN-Power','is_tf': True, 'do_scale': True, 'do_tf_dataset': True, 'do_power_transform': True, 'batch_size': 128, 'callbacks': callbacks, 'epochs': 30},\n                     {'name':'Dense-NN-PCA+lookahead+mish','is_tf': True, 'do_scale': True, 'do_tf_dataset': True,  'batch_size': 128, 'callbacks': callbacks, \n                          'epochs': 30, 'do_pca': True, 'do_power_transform': False, 'min_variance': 0.8 }, \n                     {'name':'Dense-NN-Power+lookahead+mish','is_tf': True, 'do_scale': True, 'do_tf_dataset': True,  'batch_size': 128, 'callbacks': callbacks, \n                          'epochs': 30, 'do_pca': False, 'do_power_transform': True, 'min_variance': 0.8 }, \n                     {'name':'Dense-NN-improved','is_tf': True, 'do_scale': True, 'do_tf_dataset': True,  'batch_size': 128, 'callbacks': callbacks, \n                          'epochs': 30, 'do_pca': True, 'do_power_transform': True, 'min_variance': 0.8 },\n                    \n                    {'name':'NN-Mixed-original', 'is_tf': True, 'do_scale': True, 'do_tf_dataset': True, 'batch_size': 128, 'callbacks': None, 'epochs': 30, 'do_mixed': True,\n                                     'cell_cols_mask': cell_cols_mask, 'gen_cols_mask': gen_cols_mask, 'cat_cols_mask': cat_cols_mask},\n                    {'name':'NN-Mixed-lookahead', 'is_tf': True, 'do_scale': True, 'do_tf_dataset': True, 'batch_size': 128, 'callbacks': None, 'epochs': 30, 'do_mixed': True,\n                                     'cell_cols_mask': cell_cols_mask, 'gen_cols_mask': gen_cols_mask, 'cat_cols_mask': cat_cols_mask},\n                    {'name':'NN-Mixed-batchnorm', 'is_tf': True, 'do_scale': True, 'do_tf_dataset': True, 'batch_size': 128, 'callbacks': None, 'epochs': 30, 'do_mixed': True,\n                                     'cell_cols_mask': cell_cols_mask, 'gen_cols_mask': gen_cols_mask, 'cat_cols_mask': cat_cols_mask},\n                    {'name':'NN-Mixed-weightnorm', 'is_tf': True, 'do_scale': True, 'do_tf_dataset': True, 'batch_size': 128, 'callbacks': None, 'epochs': 30, 'do_mixed': True,\n                                     'cell_cols_mask': cell_cols_mask, 'gen_cols_mask': gen_cols_mask, 'cat_cols_mask': cat_cols_mask},\n                    {'name':'NN-Mixed-mish', 'is_tf': True, 'do_scale': True, 'do_tf_dataset': True, 'batch_size': 128, 'callbacks': None, 'epochs': 30, 'do_mixed': True,\n                                     'cell_cols_mask': cell_cols_mask, 'gen_cols_mask': gen_cols_mask, 'cat_cols_mask': cat_cols_mask},\n                    {'name':'NN-Mixed-all', 'is_tf': True, 'do_scale': True, 'do_tf_dataset': True, 'batch_size': 128, 'callbacks': None, 'epochs': 30, 'do_mixed': True,\n                                     'cell_cols_mask': cell_cols_mask, 'gen_cols_mask': gen_cols_mask, 'cat_cols_mask': cat_cols_mask},\n                    {'name':'NN-Mixed-Power', 'is_tf': True, 'do_scale': True, 'do_tf_dataset': True, 'batch_size': 128, 'callbacks': None, 'epochs': 30, 'do_mixed': True,\n                                     'cell_cols_mask': cell_cols_mask, 'gen_cols_mask': gen_cols_mask, 'cat_cols_mask': cat_cols_mask, 'do_power_transform': True,}\n     \n                    ]\n\n\nmodel_configs = [\n        {'type': 'lr', 'max_iter': 500, 'solver': 'lbfgs', 'C': 1},\n        \n    {'type': 'nn-dense', 'n_layers': 2, 'units': 256, 'drop_rate': 0.3, \n     'use_weight_normalization': False, 'use_loookahead': True, 'use_batch_norm': False, 'n_features': train_short.shape[1],\n     'n_labels': labels_short.shape[1], 'use_mish':False},\n    {'type': 'nn-dense', 'n_layers': 2, 'units': 256, 'drop_rate': 0.3, \n     'use_weight_normalization': False, 'use_loookahead': True, 'use_batch_norm': False, 'n_features': train_short.shape[1],\n     'n_labels': labels_short.shape[1], 'use_mish':False},\n    {'type': 'nn-dense', 'n_layers': 2, 'units': 256, 'drop_rate': 0.3, \n     'use_weight_normalization': False, 'use_loookahead': False, 'use_batch_norm': True, 'n_features': train_short.shape[1],\n     'n_labels': labels_short.shape[1], 'use_mish':False},\n    {'type': 'nn-dense', 'n_layers': 2, 'units': 256, 'drop_rate': 0.3, \n     'use_weight_normalization': True, 'use_loookahead': False, 'use_batch_norm': False, 'n_features': train_short.shape[1],\n     'n_labels': labels_short.shape[1], 'use_mish':False},\n    {'type': 'nn-dense', 'n_layers': 2, 'units': 256, 'drop_rate': 0.3, \n     'use_weight_normalization': False, 'use_loookahead': False, 'use_batch_norm': False, 'n_features': train_short.shape[1],\n     'n_labels': labels_short.shape[1], 'use_mish':True},\n    {'type': 'nn-dense', 'n_layers': 2, 'units': 256, 'drop_rate': 0.3, \n     'use_weight_normalization': True, 'use_loookahead': True, 'use_batch_norm': True, 'n_features': train_short.shape[1],\n     'n_labels': labels_short.shape[1], 'use_mish':True},\n    {'type': 'nn-dense', 'n_layers': 2, 'units': 256, 'drop_rate': 0.3, \n     'use_weight_normalization': False, 'use_loookahead': False, 'use_batch_norm': False, 'n_features': train_short.shape[1],\n     'n_labels': labels_short.shape[1], 'use_mish':False},\n    {'type': 'nn-dense', 'n_layers': 2, 'units': 256, 'drop_rate': 0.3, \n     'use_weight_normalization': False, 'use_loookahead': False, 'use_batch_norm': False, 'n_features': train_short.shape[1],\n     'n_labels': labels_short.shape[1], 'use_mish':False},\n    {'type': 'nn-dense', 'n_layers': 2, 'units': 256, 'drop_rate': 0.3, \n     'use_weight_normalization': False, 'use_loookahead': True, 'use_batch_norm': False, 'n_features': train_short.shape[1],\n     'n_labels': labels_short.shape[1], 'use_mish':True},\n    {'type': 'nn-dense', 'n_layers': 2, 'units': 256, 'drop_rate': 0.3, \n     'use_weight_normalization': False, 'use_loookahead': True, 'use_batch_norm': False, 'n_features': train_short.shape[1],\n     'n_labels': labels_short.shape[1], 'use_mish':True},\n    {'type': 'nn-dense', 'n_layers': 2, 'units': 256, 'drop_rate': 0.3, \n     'use_weight_normalization': False, 'use_loookahead': True, 'use_batch_norm': False, 'n_features': train_short.shape[1],\n     'n_labels': labels_short.shape[1], 'use_mish':True},\n    \n    {'type': 'nn-mixed', 'layers_cell': 1, 'layers_gen': 1, 'layers_final': 0, 'units': 32, 'drop_rate': 0.3, \n     'use_weight_normalization': False, 'use_loookahead': False, 'use_batch_norm': False, 'n_features': train_short.shape[1],\n     'n_labels': labels_short.shape[1], 'mask_size':3, 'use_mish':False},\n    {'type': 'nn-mixed', 'layers_cell': 1, 'layers_gen': 1, 'layers_final': 0, 'units': 32, 'drop_rate': 0.3, \n     'use_weight_normalization': False, 'use_loookahead': True, 'use_batch_norm': False, 'n_features': train_short.shape[1],\n     'n_labels': labels_short.shape[1], 'mask_size':3, 'use_mish':False},\n    {'type': 'nn-mixed', 'layers_cell': 1, 'layers_gen': 1, 'layers_final': 0, 'units': 32, 'drop_rate': 0.3, \n     'use_weight_normalization': False, 'use_loookahead': False, 'use_batch_norm': True, 'n_features': train_short.shape[1],\n     'n_labels': labels_short.shape[1], 'mask_size':3, 'use_mish':False},\n    {'type': 'nn-mixed', 'layers_cell': 1, 'layers_gen': 1, 'layers_final': 0, 'units': 32, 'drop_rate': 0.3, \n     'use_weight_normalization': True, 'use_loookahead': False, 'use_batch_norm': False, 'n_features': train_short.shape[1],\n     'n_labels': labels_short.shape[1], 'mask_size':3, 'use_mish':False},\n    {'type': 'nn-mixed', 'layers_cell': 1, 'layers_gen': 1, 'layers_final': 0, 'units': 32, 'drop_rate': 0.3, \n     'use_weight_normalization': False, 'use_loookahead': False, 'use_batch_norm': False, 'n_features': train_short.shape[1],\n     'n_labels': labels_short.shape[1], 'mask_size':3, 'use_mish':True},\n    {'type': 'nn-mixed', 'layers_cell': 1, 'layers_gen': 1, 'layers_final': 0, 'units': 32, 'drop_rate': 0.3, \n     'use_weight_normalization': True, 'use_loookahead': True, 'use_batch_norm': True, 'n_features': train_short.shape[1],\n     'n_labels': labels_short.shape[1], 'mask_size':3, 'use_mish':True},\n    {'type': 'nn-mixed', 'layers_cell': 1, 'layers_gen': 1, 'layers_final': 0, 'units': 32, 'drop_rate': 0.3, \n     'use_weight_normalization': False, 'use_loookahead': False, 'use_batch_norm': False, 'n_features': train_short.shape[1],\n     'n_labels': labels_short.shape[1], 'mask_size':3, 'use_mish':False}\n]\n   \n\nconfigurations = zip(model_configs, experiment_config)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\n\nmetrics_nn = None\n\nconfigurations = zip(model_configs, experiment_configs)\nfor model_config, experiment_config in configurations:\n    config = {'model_config': model_config, 'experiment_config': experiment_config} \n    data = iterative_train_test_split(train.values, labels.values, .2)\n    model = get_model(model_config)\n    experiment_metrics, _ = run_experiment(data, config)\n    metrics_nn = update_metrics(experiment_config['name'], experiment_metrics, metrics_nn)\nmetrics_nn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dense-nn its near to the logistic regression so lets tune the hyperparameters, even if some configurations seems to be not work well, we wil try to optimize anyway just in case."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def dense_nn_kt(hp, n_features, n_labels):\n    model = tf.keras.Sequential()\n    layers = hp.Int('layers',min_value=1,max_value=10,step=1)\n    use_weight_normalization = hp.Boolean('use_weight_norm', default=False)\n    use_loookahead = hp.Boolean('use_loookahead', default=False)\n    use_mish = hp.Boolean('use_mish', default=False)\n    \n    for l in range(layers):\n        use_batch_norm = True #hp.Boolean(f'use_batch_norm{l}', default=False)\n        dense = tf.keras.layers.Dense(units=hp.Int(f'units_{l}',min_value=32,max_value=1024,step=32))\n        if use_weight_normalization:\n             dense = tfa.layers.WeightNormalization(dense)\n        model.add(dense)\n        if use_batch_norm:\n             model.add(tf.keras.layers.BatchNormalization())\n                \n        if use_mish:\n            model.add(tf.keras.layers.Activation(tfa.activations.mish))\n        else:\n            model.add(tf.keras.layers.Activation('relu'))\n        \n        drop_rate= hp.Float(f'dropout_{l}',min_value=0.0,max_value=0.5,default=0.25,step=0.05 )\n        model.add(tf.keras.layers.Dropout(drop_rate))\n        \n    model.add(tf.keras.layers.Dense(n_labels, activation='sigmoid'))\n    \n    learning_rate = hp.Float('learning_rate', min_value=3e-4, max_value=3e-2)\n    sync_period = hp.Int('sync_period',min_value=5,max_value=30,step=1)\n    \n    optimizer = optimizer = tfa.optimizers.RectifiedAdam(learning_rate)\n    if use_loookahead:\n        optimizer = tfa.optimizers.Lookahead(optimizer, sync_period=sync_period)\n    \n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will run the optimization of the DenseNN using KerasTuner."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"early_stop = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', min_lr = 1e-7,  factor=0.1, patience=3, verbose=1)\ncallbacks = [early_stop, reduce_lr]\n\nexperiment_config =  {'name':'Dense-NN-batchnorm','is_tf': True, 'do_scale': True, 'do_tf_dataset': True,  'batch_size': 128, 'callbacks': callbacks, 'epochs': 30}\n\n\n\ntuner = kt.Hyperband(partial(dense_nn_kt,n_features=train.shape[1], n_labels=labels.shape[1]), objective = 'val_loss', max_epochs = 100, factor = 10, seed=123,\n                            directory=f'./nn_tuner', project_name='nn_tuner')\n\ndata_np = iterative_train_test_split(train.values, labels.values, .2)\ndata, pipeline, experiment_config = preprocess(data_np, experiment_config)\n\ntrain_dataset, valid_dataset = data\n\ntuner.search(train_dataset, validation_data=valid_dataset, callbacks=callbacks, verbose=0)\nmodel = tuner.get_best_models()[0]\n\nY_valid = np.concatenate(np.array(list(valid_dataset))[:,1], axis=0)\npredictions = model.predict(valid_dataset)\ncalculate_metrics(Y_valid, predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally we will make the predictions with out best two methods training 5 times with different datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"tries = 5\nearly_stop = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', min_lr = 1e-7,  factor=0.1, patience=3, verbose=0)\ncallbacks = [early_stop, reduce_lr]\n\nexperiment_config_nn = {'name':'Dense-NN-batchnorm','is_tf': True, 'do_scale': True, 'do_tf_dataset': True,  'batch_size': 128, 'callbacks': callbacks, 'epochs': 30}\nexperiment_config_lr =  {'name':'LogisticRegression-Base','do_scale': True}\n\nmodel_config_lr={'type': 'lr', 'max_iter': 500, 'solver': 'lbfgs', 'C': 1}\nmodel_config_nn = {'type': 'nn-dense', 'n_layers': 2, 'units': 256, 'drop_rate': 0.3, 'use_weight_normalization': False, \n                    'use_loookahead': False, 'use_batch_norm': True, 'n_features': train_short.shape[1],'n_labels': labels_short.shape[1], 'use_mish':False}\n                                        \n_, transformations, _ = preprocess([train.values, labels.values], experiment_config_lr, is_testing=True)\ndata_test, _, _ = preprocess([test.values, None], experiment_config_lr, transformations=transformations, prefit=True, is_testing=True)\n\npredictions = np.zeros((test.shape[0], labels.shape[1]))\nfor i in range(tries):\n    \n    data_np = iterative_train_test_split(train.values, labels.values, .2)\n    data_lr, _, experiment_config_lr = preprocess(data_np, experiment_config_lr)\n    data_nn, _, experiment_config_nn = preprocess(data_np, experiment_config_nn)\n    model_nn = dense_nn_kt(tuner.get_best_hyperparameters()[0], n_features=train.shape[1], n_labels=labels.shape[1])\n    model_lr = get_model(model_config_lr)\n    #model_nn = get_model(model_config_nn)\n\n    _, model_lr = train_model(model_lr, data_lr, experiment_config_lr)\n    _, model_nn = train_model(model_nn, data_nn, experiment_config_nn)\n\n    test_dataset = data_test[0]\n    predictions += (model_lr.predict(test_dataset) + model_nn.predict(test_dataset))/2\n    \npredictions = predictions/tries","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally lets make the submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission =  pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\nsubmission = submission.set_index('sig_id')\nsubmission.loc[:,:] = np.zeros(submission.shape)\nsubmission.loc[test.index,:] = predictions\n\nsubmission.to_csv('submission.csv')\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"TODO:\n* Test MLSMOTE.\n* Test feature selection.\n* Improve documentation.\n* Test other multilabel techniques (Classification chain,....).\n* Study errors."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}