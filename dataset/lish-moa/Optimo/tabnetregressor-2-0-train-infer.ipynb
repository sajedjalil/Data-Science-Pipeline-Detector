{"cells":[{"metadata":{},"cell_type":"markdown","source":"## About this notebook\n\n\nThere has been some great notebooks shared using pytorch-tabnet like this one:\n\nhttps://www.kaggle.com/ludovick/introduction-to-tabnet-kfold-10-inference\n    \nHowever they seem overcomplicated to me and can be a bit scary at first sight.\n\nPytorch-tabnet has been created to be easy to use, so it defeats the original purpose. \n\nAll in all this notebook does essentially the same as the above mentionned notebook, but in a much simpler way.\n\n\n### New features have come with pytorch-tabnet 2.0 so I wanted to make a simple example on how to use them for MOA competition.\n\n\n## Why TabNetRegressor?\n\nIt would probably make more sense to use TabNetMultiTaskClassifier since this is what this competition is about.\n\nThe main pain point is that tasks are very sparse in this competition, so it's hard to make a cross validation whit non trivail folds.\n\nBy trivial folds (I could have called them absurd folds) I mean either:\n- only 0s in the training fold\n- only 0s in the validation fold\n\nSince TabNetClassifier automatically infer the dimensions based on the train set, you'll end up with some errors as the libary won't allow you to do something trivial.\n\nHowever, all the tasks here are binary hence we can easily use TabNetRegressor which won't have any probleme with trivial folds.\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet\n!pip install /kaggle/input/iterative-stratification/iterative-stratification-master/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import StratifiedKFold\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nimport numpy as np\nimport pandas as pd \n\nimport os\nimport random\nimport sys\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nfrom tqdm import tqdm\nfrom sklearn.metrics import log_loss\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data and minimal preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_path = \"../input/lish-moa/\"\ntrain = pd.read_csv(data_path+'train_features.csv')\ntrain.drop(columns=[\"sig_id\"], inplace=True)\n\ntrain_targets_scored = pd.read_csv(data_path+'train_targets_scored.csv')\ntrain_targets_scored.drop(columns=[\"sig_id\"], inplace=True)\n\ntest = pd.read_csv(data_path+'test_features.csv')\ntest.drop(columns=[\"sig_id\"], inplace=True)\n\nsubmission = pd.read_csv(data_path+'sample_submission.csv')\n\nremove_vehicle = False\n\nif remove_vehicle:\n    kept_index = train['cp_type']=='trt_cp'\n    train = train.loc[kept_index].reset_index(drop=True)\n    train_targets_scored = train_targets_scored.loc[kept_index].reset_index(drop=True)\n\ntrain[\"cp_type\"] = (train[\"cp_type\"]==\"trt_cp\") + 0\ntrain[\"cp_dose\"] = (train[\"cp_dose\"]==\"D1\") + 0\n\ntest[\"cp_type\"] = (test[\"cp_type\"]==\"trt_cp\") + 0\ntest[\"cp_dose\"] = (test[\"cp_dose\"]==\"D1\") + 0\n\nX_test = test.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model parameters \n\nHappy tuning! ;)"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_EPOCH=200\ntabnet_params = dict(n_d=24, n_a=24, n_steps=1, gamma=1.3,\n                     lambda_sparse=0, optimizer_fn=torch.optim.Adam,\n                     optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n                     mask_type='entmax',\n                     scheduler_params=dict(mode=\"min\",\n                                           patience=5,\n                                           min_lr=1e-5,\n                                           factor=0.9,),\n                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                     verbose=10,\n                     )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define custom metric for valdidation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\nfrom pytorch_tabnet.metrics import Metric\nfrom sklearn.metrics import roc_auc_score, log_loss\n\nclass LogitsLogLoss(Metric):\n    \"\"\"\n    LogLoss with sigmoid applied\n    \"\"\"\n\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        \"\"\"\n        Compute LogLoss of predictions.\n\n        Parameters\n        ----------\n        y_true: np.ndarray\n            Target matrix or vector\n        y_score: np.ndarray\n            Score matrix or vector\n\n        Returns\n        -------\n            float\n            LogLoss of predictions vs targets.\n        \"\"\"\n        logits = 1 / (1 + np.exp(-y_pred))\n        aux = (1-y_true)*np.log(1-logits+1e-15) + y_true*np.log(logits+1e-15)\n        return np.mean(-aux)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_auc_all= []\ntest_cv_preds = []\n\nNB_SPLITS = 10\nmskf = MultilabelStratifiedKFold(n_splits=NB_SPLITS, random_state=0, shuffle=True)\noof_preds = []\noof_targets = []\nscores = []\nscores_auc = []\nfor fold_nb, (train_idx, val_idx) in enumerate(mskf.split(train, train_targets_scored)):\n    print(\"FOLDS : \", fold_nb)\n\n    ## model\n    X_train, y_train = train.values[train_idx, :], train_targets_scored.values[train_idx, :]\n    X_val, y_val = train.values[val_idx, :], train_targets_scored.values[val_idx, :]\n    model = TabNetRegressor(**tabnet_params)\n\n    model.fit(X_train=X_train,\n              y_train=y_train,\n              eval_set=[(X_val, y_val)],\n              eval_name = [\"val\"],\n              eval_metric = [\"logits_ll\"],\n              max_epochs=MAX_EPOCH,\n              patience=20, batch_size=1024, virtual_batch_size=128,\n              num_workers=1, drop_last=False,\n              # use binary cross entropy as this is not a regression problem\n              loss_fn=torch.nn.functional.binary_cross_entropy_with_logits)\n\n    preds_val = model.predict(X_val)\n    # Apply sigmoid to the predictions\n    preds =  1 / (1 + np.exp(-preds_val))\n    score = np.min(model.history[\"val_logits_ll\"])\n#     name = cfg.save_name + f\"_fold{fold_nb}\"\n#     model.save_model(name)\n    ## save oof to compute the CV later\n    oof_preds.append(preds_val)\n    oof_targets.append(y_val)\n    scores.append(score)\n\n    # preds on test\n    preds_test = model.predict(X_test)\n    test_cv_preds.append(1 / (1 + np.exp(-preds_test)))\n\noof_preds_all = np.concatenate(oof_preds)\noof_targets_all = np.concatenate(oof_targets)\ntest_preds_all = np.stack(test_cv_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aucs = []\nfor task_id in range(oof_preds_all.shape[1]):\n    aucs.append(roc_auc_score(y_true=oof_targets_all[:, task_id],\n                              y_score=oof_preds_all[:, task_id]))\nprint(f\"Overall AUC : {np.mean(aucs)}\")\nprint(f\"Average CV : {np.mean(scores)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_feat = [col for col in submission.columns if col not in [\"sig_id\"]]\nsubmission[all_feat] = test_preds_all.mean(axis=0)\n# set control to 0\nsubmission.loc[test['cp_type']==0, submission.columns[1:]] = 0\nsubmission.to_csv('submission.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}