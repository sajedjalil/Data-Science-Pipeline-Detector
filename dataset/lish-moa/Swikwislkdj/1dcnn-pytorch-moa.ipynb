{"cells":[{"metadata":{},"cell_type":"markdown","source":"## If you are looking for a <span style=\"color:GOLD\">team member, do consider me too !"},{"metadata":{},"cell_type":"markdown","source":"\nReferences :\n1. @abhishek and @artgor 's Parallel Programming video https://www.youtube.com/watch?v=VRVit0-0AXE\n2. @yasufuminakama 's Amazying Notebook https://www.kaggle.com/yasufuminakama/moa-pytorch-nn-starter \n\n# `If you consider forking my kernel, remember to turn off the internet after giving an` **<span style=\"color:GREEN\">UPVOTE**"},{"metadata":{},"cell_type":"markdown","source":"## Update V.14\n1. Added features from PCA to existing ones [improves CV score]\n\n## Update V.11\n1. Added feature selection using `VarianceThreshold` method of sklearn [improves CV score]\n\n## Update:\n1. Model updated\n2. Increased Seeds\n"},{"metadata":{},"cell_type":"markdown","source":"# If you like it, Do Upvote :)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input/lish-moa')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored.sum()[1:].sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features['cp_type'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCA features + Existing features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# GENES\nn_comp = 50\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CELLS\nn_comp = 15\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# feature Selection using Variance Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\n\n\nvar_thresh = VarianceThreshold(threshold=0.5)\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\ntrain_features\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[train_targets_scored.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Binning"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# for col in GENES:\n#     train.loc[:, f'{col}_bin'] = pd.cut(train[col], bins=3, labels=False)\n#     test.loc[:, f'{col}_bin'] = pd.cut(test[col], bins=3, labels=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Distribution plots"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# plt.figure(figsize=(16,16))\n# sns.set_style(\"whitegrid\")\n\n# gene_choice = np.random.choice(len(GENES), 16)\n# for i, col in enumerate(gene_choice):\n#     plt.subplot(4, 4, i+1)\n#     plt.hist(train_features.loc[:, GENES[col]],bins=100, color='orange')\n#     plt.title(GENES[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# [Naive] Outlier Removal"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\n# train_ = train.copy() [Didn't wanted to actually normalize, so created a copy and normalized that for further calculation]\n# for col in GENES:\n    \n# #     train_[col] = (train[col]-np.mean(train[col])) / (np.std(train[col]))\n    \n#     mean = train_[col].mean()\n#     std = train_[col].std()\n\n#     std_r = mean + 4*std\n#     std_l = mean - 4*std\n\n#     drop = train_[col][(train_[col]>std_r) | (train_[col]<std_l)].index.values\n\n# train = train.drop(drop).reset_index(drop=True)\n# # folds = folds.drop(drop).reset_index(drop=True)\n# target = target.drop(drop).reset_index(drop=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCA"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# n_comp = 50\n\n# data = pd.concat([pd.DataFrame(train[CELLS]), pd.DataFrame(test[CELLS])])\n# data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\n# train2 = data2[:train.shape[0]]; test2 = data2[train.shape[0]:]\n\n# train2 = pd.DataFrame(train2, columns=[f'c-{i}' for i in range(n_comp)])\n# test2 = pd.DataFrame(test2, columns=[f'c-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n# train = train.drop(columns=drop_cols)\n# test = test.drop(columns=drop_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CV folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = train.copy()\n\nmskf = MultilabelStratifiedKFold(n_splits=5)\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)\nfolds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(folds.shape)\nprint(test.shape)\nprint(target.shape)\nprint(sample_submission.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset Classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n#         print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n   \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        cha_1 = 256\n        cha_2 = 512\n        cha_3 = 512\n\n        cha_1_reshape = int(hidden_size/cha_1)\n        cha_po_1 = int(hidden_size/cha_1/2)\n        cha_po_2 = int(hidden_size/cha_1/2/2) * cha_3\n\n        self.cha_1 = cha_1\n        self.cha_2 = cha_2\n        self.cha_3 = cha_3\n        self.cha_1_reshape = cha_1_reshape\n        self.cha_po_1 = cha_po_1\n        self.cha_po_2 = cha_po_2\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.1)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n\n        self.batch_norm_c1 = nn.BatchNorm1d(cha_1)\n        self.dropout_c1 = nn.Dropout(0.1)\n        self.conv1 = nn.utils.weight_norm(nn.Conv1d(cha_1,cha_2, kernel_size = 5, stride = 1, padding=2,  bias=False),dim=None)\n\n        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = cha_po_1)\n\n        self.batch_norm_c2 = nn.BatchNorm1d(cha_2)\n        self.dropout_c2 = nn.Dropout(0.1)\n        self.conv2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n\n        self.batch_norm_c2_1 = nn.BatchNorm1d(cha_2)\n        self.dropout_c2_1 = nn.Dropout(0.3)\n        self.conv2_1 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n\n        self.batch_norm_c2_2 = nn.BatchNorm1d(cha_2)\n        self.dropout_c2_2 = nn.Dropout(0.2)\n        self.conv2_2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_3, kernel_size = 5, stride = 1, padding=2, bias=True),dim=None)\n\n        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n\n        self.flt = nn.Flatten()\n\n        self.batch_norm3 = nn.BatchNorm1d(cha_po_2)\n        self.dropout3 = nn.Dropout(0.2)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(cha_po_2, num_targets))\n\n    def forward(self, x):\n\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.celu(self.dense1(x), alpha=0.06)\n\n        x = x.reshape(x.shape[0],self.cha_1,\n                      self.cha_1_reshape)\n\n        x = self.batch_norm_c1(x)\n        x = self.dropout_c1(x)\n        x = F.relu(self.conv1(x))\n\n        x = self.ave_po_c1(x)\n\n        x = self.batch_norm_c2(x)\n        x = self.dropout_c2(x)\n        x = F.relu(self.conv2(x))\n        x_s = x\n\n        x = self.batch_norm_c2_1(x)\n        x = self.dropout_c2_1(x)\n        x = F.relu(self.conv2_1(x))\n\n        x = self.batch_norm_c2_2(x)\n        x = self.dropout_c2_2(x)\n        x = F.relu(self.conv2_2(x))\n        x =  x * x_s\n\n        x = self.max_po_c2(x)\n\n        x = self.flt(x)\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing steps"},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_data(data):\n    \n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n#     data.loc[:, 'cp_time'] = data.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})\n#     data.loc[:, 'cp_dose'] = data.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n\n# --------------------- Normalize ---------------------\n#     for col in GENES:\n#         data[col] = (data[col]-np.mean(data[col])) / (np.std(data[col]))\n    \n#     for col in CELLS:\n#         data[col] = (data[col]-np.mean(data[col])) / (np.std(data[col]))\n    \n#--------------------- Removing Skewness ---------------------\n#     for col in GENES + CELLS:\n#         if(abs(data[col].skew()) > 0.75):\n            \n#             if(data[col].skew() < 0): # neg-skewness\n#                 data[col] = data[col].max() - data[col] + 1\n#                 data[col] = np.sqrt(data[col])\n            \n#             else:\n#                 data[col] = np.sqrt(data[col])\n    \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 5\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=4096*4\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Single fold training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.01, div_factor=1e3, max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,factor=0.1,patience=1,verbose=True)\n    #scheduler = optim.lr_scheduler.CyclicLR(optimizer=optimizer, base_lr = 1e-4, max_lr=1e-2,step_size_up=100,cycle_momentum=False,mode=\"triangular2\")\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_fn, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        #scheduler.step(valid_loss) #delete\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ / NFOLDS\n        oof += oof_\n        \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = Model(\n#     num_features=num_features,\n#     num_targets=num_targets,\n#     hidden_size=hidden_size,)\n\n# optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n# scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.001, div_factor=1e3, max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=137)\n# #scheduler = optim.lr_scheduler.CyclicLR(optimizer=optimizer, base_lr = 1e-4, max_lr=1e-2,step_size_up=100,cycle_momentum=False,mode=\"triangular2\")\n\n# lrs = []\n\n\n# for i in range(25*137):\n#     optimizer.step()\n#     lrs.append(optimizer.param_groups[0][\"lr\"])\n# #     print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"])\n#     scheduler.step()\n\n# plt.plot(lrs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Averaging on multiple SEEDS\nEPOCHS=25\n\nSEED = [0, 1, 2]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# test['atp-sensitive_potassium_channel_antagonist'] = 0.0\n# test['erbb2_inhibitor'] = 0.0\n\n# train['atp-sensitive_potassium_channel_antagonist'] = 0.0\n# train['erbb2_inhibitor'] = 0.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets_scored","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(target_cols)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ / target.shape[1]\n    \nprint(\"CV log_loss: \", score)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Things that can improve your CV even further:\n1. Increasing SEEDS\n2. Feature Selection over GENES/CELLS columns\n3. Model Hyperparameter Tuning\n4. Removing Skewness from GENES/CELLS columns [Comment below if it helps]\n5. PCA........................................[Comment below if it helps]\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}