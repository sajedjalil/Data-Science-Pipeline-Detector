{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Exploration of oversampling strategies\n\nIt's known by now that the distribution of classes is highly unbalanced.\nThis version tries to come up with a strategy that employs smote oversampling\nof the minority classes in order to achieve lower log-loss. It employs label\npowerset transformation and attempts to find a sweetspot regarding which\nclasses to oversample."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import libraries\nimport numpy as np\nimport pandas as pd\n\n# Load the competition data (train features and train labels)\nx_train = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ny_train = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\n\nprint(f'Train features shape: {x_train.shape}')\nprint(f'Train target shape: {y_train.shape}')\n\n# The competition authors mentioned that only treatment typed samples have\n# labels. We can quickly verify that\n\nctrl_ids = x_train[x_train.cp_type == 'ctl_vehicle'].sig_id\nctrl_label_count = y_train[y_train.sig_id.isin(ctrl_ids)].values[:, 1:].sum()\nprint(f'Control type samples label count: {ctrl_label_count}')\nx_train = x_train[~x_train.sig_id.isin(ctrl_ids)]\ny_train = y_train[~y_train.sig_id.isin(ctrl_ids)]\n\n# Onehot-encode the categorical features\n# type_cat = pd.get_dummies(x_train.cp_type)\ndose_cat = pd.get_dummies(x_train.cp_dose)\ntime_cat = pd.get_dummies(x_train.cp_time)\nx_train = pd.concat([x_train, dose_cat, time_cat], axis=1)\n\n# Select the columns of the train dataset.\n# The train dataset will consist of the numeric features plus the one-hot encoded\n# representations of dose and time.\nfeature_columns = [i for i in x_train.columns if i not in ['sig_id', 'cp_type', 'cp_time', 'cp_dose']]\ntarget_classes = [i for i in y_train.columns if i not in ['sig_id']]\n\n# Filter the data based on the selected columns\nX = x_train[feature_columns].values\ny = y_train[target_classes].values\nprint(f'Train dataset shape: {X.shape}')\nprint(f'Train labels shape: {y.shape}')\n\n# Calculate the sparcity of labels\ntotal_positive_labels = np.sum(y)\ntotal_labels = y.flatten().shape[0]\nprint(f'Positive labels: {total_positive_labels}')\nprint(f'Total labels: {total_labels}')\nprint(f'Sparsity ratio: {total_positive_labels / total_labels:.4f}')\n\n# Perform analysis on the representation of classes between samples\n# We select the unique combination of feature classes in a 2D array\n# and the frequency if each unique combination in a 1D array\nunique_rows, unique_counts = np.unique(y, return_counts=True, axis=0)\n\n# Construct a dataframe from the above extraction for easier manipulation\n# Dataframe inndex will be the integer label for each unique combination,\n# Row is each unique representation and class count its frequency count\nclass_df = pd.DataFrame({\n    'row': [i for i in unique_rows],\n    'class_count': list(unique_counts)\n})\n\n# Sort the dataframe in descending popularity. The impact of the sorting\n# to end performance should be investigated, but this way when we filter out\n# unpopular classes we do not have gaps in class numbers, can be problematic\n# for certain sklearn methods\nclass_df.sort_values(by='class_count', ascending=False, inplace=True)\nclass_df.reset_index(drop=True, inplace=True)\n\n# Construct dictionaries to map from a unique representation to a class number\n# and vice versa\nrow_to_class = {}\nclass_to_row = {}\nfor i, df_row in class_df.iterrows():\n    row_to_class[tuple(df_row.row)] = i\n    class_to_row[i] = df_row.row\n\n# Map the train labels to their respective class number. This way we can filter out\n# train samples that belong to unpopular classes but also perform StratifedKFold.\n# We can also use this new representation of classes to transform our multi-label\n# problem to a multi-class problem which are generally easier to slove.\n\n# But there are issues with that:\n# If there are too many unique representations we end up with too many labels.\n# Usually heavy class imbalance\n# We cannot predict class combinations whose representation does not exist during\n# training\ny_classes = np.array([row_to_class[tuple(i)] for i in y])\nprint(f'Target class shape: {y_classes.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# Plot the distribution of unique label combinations. We can see that\n# The popularity reduces rapidly over different combinations with the majority\n# Of samples belonging to just a handful of combinations.\nclass_df.class_count.plot(figsize=(13,6), logy=True, title='Histogram of label combinations')\nplt.xlabel('Label combination no')\nplt.ylabel('Log Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Find which classes are the least popular.\noutlier_classes = class_df[class_df.class_count < 6].index\n\n# Find the index of the train labels where there are no unpopular classes\nfiltered_idx = [i for i,x in enumerate(y_classes) if x not in outlier_classes]\n\n# Filter the train set without having unpopular classes\nX = X[filtered_idx]\ny = y[filtered_idx]\ny_classes = y_classes[filtered_idx]\nprint(f'Outlier classes: {outlier_classes}')\nprint(f'Shape of filtered features: {X.shape}')\nprint(f'Shape of filtered classes: {y_classes.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cell_features = [i for i in x_train.columns if str(i)[:2] == 'c-']\ngene_features = [i for i in x_train.columns if str(i)[:2] == 'g-']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import Model, layers, optimizers, callbacks\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, KBinsDiscretizer, MaxAbsScaler\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.feature_selection import VarianceThreshold\ntf.config.optimizer.set_jit(True)\n\n\n# Load the test set and preprocess it to the same format with\n# the training set\ntest_features = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\nctrl_idxs = test_features[test_features.cp_type != 'trt_cp' ].index\n\n# test_type = pd.get_dummies(test_features.cp_type)\ntest_time = pd.get_dummies(test_features.cp_time)\ntest_dose = pd.get_dummies(test_features.cp_dose)\n\ntest_features = pd.concat([test_features, test_dose, test_time], axis=1)\ntest_features = test_features[feature_columns].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/multilabel-skf')\nfrom ml_stratifiers import MultilabelStratifiedKFold\nfrom imblearn.over_sampling import SMOTENC\n\n\ndef make_nn(layer_size, num_features):\n    # Simple neural network with one hidden layer. Nothing special\n    # Last layer is shape 1xnum_classes with sigmoid activation\n    # and the loss is binary crossentropy because we have to predict\n    # multiple labels for each sample.\n    # This means our labels are not mutually exclusive and we cannot use\n    # softmax activation and categorical crossentropy loss\n    optimizer = optimizers.Adam()\n    loss = tf.keras.losses.BinaryCrossentropy()\n\n    model = tf.keras.Sequential([\n        layers.Input(shape=(num_features,)),\n        layers.BatchNormalization(),\n        layers.Dropout(.3),\n        tfa.layers.WeightNormalization(layers.Dense(layer_size // 2,\n                                                    activation='elu')),\n        layers.Dropout(.3),\n        layers.BatchNormalization(),\n        tfa.layers.WeightNormalization(layers.Dense(layer_size,\n                                                    activation='elu')),\n        layers.Dropout(.5),\n        layers.BatchNormalization(),\n        tfa.layers.WeightNormalization(layers.Dense(layer_size,\n                                                    activation='elu')),\n        layers.Dropout(.5),\n        layers.BatchNormalization(),\n        tfa.layers.WeightNormalization(layers.Dense(y.shape[1], activation='sigmoid'))\n    ])\n    \n    model.compile(optimizer=optimizer, loss=loss)\n    \n    return model\n\n\n\nearly_stopping = callbacks.EarlyStopping(patience=10, restore_best_weights=True)\nreduce_lr = callbacks.ReduceLROnPlateau(patience=3, mode='min', monitor='val_loss')\n\n# Assert predictions shape matches the number of models\nparam_list = [42, 99, 4]\n\noof_score = []\nvalidation_preds = np.zeros((X.shape[0], y.shape[1]))\nsubmission_preds = np.zeros((test_features.shape[0], y.shape[1]))\n\n# Loop over combinations of models using fold cross validation. For each\n# fold make predictions of the test set.\n\nfor j, d in enumerate(param_list):\n    kf = MultilabelStratifiedKFold(n_splits=10, shuffle=True, random_state=d)\n    for i, (train_index, test_index) in enumerate(kf.split(X, y)):\n        \n        # Define oversampling strategy\n        # Because there is huge class imbalance\n        # Creating too many synthetic samples\n        # Could be problematic\n        X_smote = X[train_index]\n        y_smote = y_classes[train_index]\n        \n        # Find the distribution of label power-sets within the fold\n        fold_unique_y, fold_unique_counts = np.unique(y_smote, return_counts=True)\n        \n        # Define oversampling by touching only minority classes\n        # with few enough samples but not too few\n        resample_dict = {}\n        for k, c_name in enumerate(fold_unique_y):\n            if fold_unique_counts[k] > 150:\n                oversample_count = fold_unique_counts[k]\n            elif fold_unique_counts[k] < 100:\n                oversample_count = fold_unique_counts[k]\n            else:\n                oversample_count = 150\n            \n            resample_dict[c_name] = oversample_count\n            \n        cat_idxs = np.array([872, 873, 874, 875, 876])\n        smote = SMOTENC(cat_idxs, sampling_strategy=resample_dict)\n        \n        X_train, y_cat_train = smote.fit_resample(X_smote, y_smote)\n        \n        y_train = np.array([class_to_row[i] for i in y_cat_train])\n        \n        X_test = X[test_index]\n        y_test = y[test_index]\n\n        ckp_filepath = f'weights.{j:02d}-{i:02d}.hdf5'\n        model_checkpoint = callbacks.ModelCheckpoint(filepath=ckp_filepath,\n                                                     save_weights_only=True,\n                                                     monitor='val_loss',\n                                                     mode='min',\n                                                     save_best_only=True)\n        tf.keras.backend.clear_session()\n        model_instance = make_nn(1024, X_train.shape[1])\n        oof_h = model_instance.fit(X_train, y_train,\n                                   epochs=50, batch_size=64,\n                                   validation_data=(X_test, y_test),\n                                   callbacks=[early_stopping,\n                                              reduce_lr,\n                                              model_checkpoint],\n                                   verbose=0)\n        model_instance.load_weights(ckp_filepath)\n               \n        # Predict the hold out set\n        fold_score = min(oof_h.history['val_loss'])\n        \n        # Keep track of the validation score\n        print(f'Finished training seed {j} fold {i}. OOF log loss: {fold_score:.5f}')\n        oof_score.append(fold_score)\n        \n        # Predict the validation set\n        valid_preds = model_instance.predict(X_test)\n        validation_preds[test_index] += valid_preds\n        \n        # Predict the test set\n        test_preds = model_instance.predict(test_features)\n        submission_preds += test_preds\n        \n        \n# Average the predictions\nvalidation_preds /= (j + 1)\nsubmission_preds /= (i + 1) * (j + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the average validation score across all folds\nprint(f'Average fold log loss: {np.mean(oof_score):.5} +- {np.std(oof_score):.5}')\n\nbce = tf.keras.losses.BinaryCrossentropy()\nvalid_loss = bce(y, validation_preds).numpy()\nprint(f'Validation log loss: {valid_loss:.5}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill in the predictions to the submission csv and save it\nsubmission = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\nsubmission.iloc[:, 1:] = submission_preds\nsubmission.loc[ctrl_idxs, submission.columns[1:]] = 0\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}