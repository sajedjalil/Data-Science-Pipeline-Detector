{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom sklearn.metrics import log_loss\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\ns_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Log_loss metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"def average_log_loss(y_true, y_pred):\n    print(y_true.shape, y_pred.shape)\n    num_samples, num_outputs = y_true.shape\n    loss = 0.00\n    for i in range(num_outputs):\n        loss += log_loss(y_true[:, i], y_pred[:, i], labels=[0, 1])\n    loss /= num_outputs\n    return loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(df):\n    df = df.drop(columns=['sig_id'])\n    df.cp_dose = df.cp_dose.map({'D1': -1, 'D2': 1})\n    df.cp_time = df.cp_time.map({24: -1, 48: 0, 72: 1})\n    df.cp_type = df.cp_type.map({'trt_cp': -1, 'ctl_vehicle': 1})\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = preprocess(train_features)\ntrain_targets_scored = train_targets_scored.drop(columns=['sig_id'])\ntest_features = preprocess(test_features)\n\ntargets_np = train_targets_scored.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scaler and transform"},{"metadata":{"trusted":true},"cell_type":"code","source":"g_cols = [col for col in train_features.columns if col.startswith('g-')]\nc_cols = [col for col in train_features.columns if col.startswith('c-')]\ncp_cols = [col for col in train_features.columns if col.startswith('cp_')]\n\ndef scaler_and_PCA(pca_num_components, train, test):\n    data = np.concatenate((train, test), axis=0)\n    n = train.shape[0]\n    \n    # variance threshold\n    selector = VarianceThreshold(threshold=0.8)\n    data = selector.fit_transform(data)\n    \n    # scale\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(data)\n\n    # PCA\n    pca = PCA(pca_num_components)\n    pca_data = pca.fit_transform(scaled_data)\n\n    train_trans = pca_data[:n, :]\n    test_trans = pca_data[n:, :]\n\n    return train_trans, test_trans\n\n\n# For columns \"g-\"\ntrain_X_g = train_features[g_cols].to_numpy()\ntest_X_g = test_features[g_cols].to_numpy()\ntrain_X_g, test_X_g = scaler_and_PCA(80, train_X_g, test_X_g)\n\n# For columns \"c-\"\ntrain_X_c = train_features[c_cols].to_numpy()\ntest_X_c = test_features[c_cols].to_numpy()\ntrain_X_c, test_X_c = scaler_and_PCA(20, train_X_c, test_X_c)\n\nfeatures_np = np.concatenate((train_features[cp_cols].to_numpy(), train_X_g, train_X_c), axis=1)\ntest_np = np.concatenate((test_features[cp_cols].to_numpy(), test_X_g, test_X_c), axis=1)\nprint('Shape after scaler and PCA', features_np.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cross Validation Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = None\nbest_loss = 999999999999999999\nkf = KFold(n_splits=5)\nj = 1\nfor train_indices, val_indices in kf.split(features_np):\n    X_train, Y_train = features_np[train_indices, :], targets_np[train_indices, :]\n    X_val, Y_val = features_np[val_indices, :], targets_np[val_indices, :]\n\n    all_categories = list(train_targets_scored.columns)\n    model_dict = {}\n    print('FIT')\n    for i in tqdm(range(206)):\n        if Y_train[:, i].max() == 0:\n            # use last model\n            model_dict[all_categories[i]] = logistic_model\n        else:\n            logistic_model = LogisticRegression(C=0.1, max_iter=1000, class_weight={0: 0.4, 1: 0.6})\n            logistic_model.fit(X_train, Y_train[:, i])\n            # saving model\n            model_dict[all_categories[i]] = logistic_model\n    print('PREDICT')\n    Y_pred = np.zeros(Y_val.shape)\n    i = 0\n    for category in tqdm(all_categories):\n        Y_pred[:, i] = np.copy(model_dict[category].predict_proba(X_val)[:, 1])\n        i += 1\n    print('VALIDATE')\n    cur_loss = average_log_loss(Y_val, Y_pred)\n    print('Log_loss', j, cur_loss)\n    if cur_loss < best_loss:\n        best_model = model_dict\n        best_loss = cur_loss\n    j += 1\n\nprint('Best loss is:', best_loss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predict result"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_res = s_submission.drop(columns=['sig_id']).to_numpy()\ni = 0\nall_categories = list(train_targets_scored.columns)\nprint('PREDICT RESULT')\nfor category in tqdm(all_categories):\n    Y_res[:, i] = np.copy(best_model[category].predict_proba(test_np)[:, 1])\n    i += 1\n# POSTPROCESS\nfor i in range(test_np.shape[0]):\n    if test_np[i][0] == 1:\n        Y_res[i, :] = np.zeros(Y_res.shape[1])\ns_res = pd.DataFrame(Y_res, columns=all_categories)\ns_res = pd.concat([s_submission['sig_id'], s_res], axis=1)\ns_res.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}