{"cells":[{"metadata":{"papermill":{"duration":0.12073,"end_time":"2020-10-06T17:09:07.70244","exception":false,"start_time":"2020-10-06T17:09:07.58171","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# MoA or Not MoA : that is an easier question?"},{"metadata":{"papermill":{"duration":0.117106,"end_time":"2020-10-06T17:09:07.938076","exception":false,"start_time":"2020-10-06T17:09:07.82097","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Overview\nThis notebook messes around with the data sets from the [MoA Prediction](https://www.kaggle.com/c/lish-moa) competition.  For simplicity a simple classifier is implemented to assign sig_ids as \"MoA\" or \"not MoA\".\nIn v15+ the classifier is implemented as 3 separate XGB models: one for each value of cp_time. The models use the same hyper-parameters and the same reduced set of features (v19+: c-ave, c-std, c-5%, c-95%, c-38, c-65, c-70, c-48, g-95%, g-hif, and 11 specifically selected g values).\n\n(v15) The classifier, with threshold=0.45, labels ~ 2000 of the ~ 22000 non-control train sig_ids as being \"not MoA\" with 89% precision, and, changing the threshold to 0.32, we find ~ 6000 sig_ids are determined as \"MoA\" with 95% precision.  These still leave about 14000 non-control sig_ids whose status is ambiguously determined, although 61% of them are actually \"MoA\". \n\nOf course the features and ML used here are simple, even so it suggests that there may not be enough information in the data to determine \"MoA\" or \"not MoA\" for many of the sig_ids.\n\n#### The Notebook's LB Score\nThe notebook's initial score of 0.02363 (v3) comes from the simplest model of setting the controls' targets to 0 and the non-control sig_ids' targets are set to their average as seen in the non-control training set, e.g., the nfkb_inhibitor target is set to 0.037908.\n\nOne motivation for wanting to determine the not-MoA sig_ids is that then all of their target values could be set (closer) to 0 and the score would improve. With so few cleanly determined non-MoAs the improvement possible seems at/below the 5th decimal digit - so much for that idea ;-)\n\n**In (v19+)** the classifier is switched to have y=1 correspond to MoA>0. Rows predicted to have MoA>0 have all their target values increased by a calculated factor (e.g., x1.3) and the other rows have their targets decrease by another factor (e.g., x0.95). <br>\n**In v20--31**, instead of using all of the targets, the classifying and factors adjustments are restricted to a subset of the targets, i.e.: the \"tSNE-9\" easy-to-detect ones (v21), the \"big 2\" (v22), the high-MoA ones w/o the big 2 (v24), the cdk_inhibitor itself (v26), the ones whose g-vectors seem 'detectable' (v28), etc. See details in the <a href=\"#DiaryRecent\">recent Diary entries</a> and in the <a href=\"#OutputKaggle\">Output Kaggle Predictions</a> section.<br><br>\n**In v32** all the targets are used again, with results not so different from v15: about 3000 ids are classified as not MoA and perhaps 6000 as MoA, leaving 13000 that are not well classified, as seen by the overlap in the confusion-dots plot."},{"metadata":{"papermill":{"duration":0.118554,"end_time":"2020-10-06T17:09:08.17384","exception":false,"start_time":"2020-10-06T17:09:08.055286","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## <a id=\"Index\">Index</a>\n<a href=\"#Diary\">Diary and Score history</a><br>\n . . .   --> <a href=\"#DiaryRecent\">most recent</a><br>\n\n<a href=\"#Preliminaries\">Preliminaries</a><br>\n<a href=\"#DataProcessing\">Reading and Processing csv Data Files</a><br>\n\n<a href=\"#TargetSummary\">Looking at the Targets</a><br>\n<a href=\"#FeatureSummary\">Looking at the Features</a><br>\n\n<a href=\"#gVectors\">g-Vectors for each Target MoA</a><br>\n<a href=\"#tSNEfeatures\">t-SNE on Features shows some Target clusters</a><br>\n\n<br>\n<a href=\"#MachineLearning\">Machine Learning</a><br>\n<a href=\"#HyperSearch\">Hyper-Parameter Search</a><br>\n<a href=\"#FeatureImportance\">Feature Importance</a><br>\n<a href=\"#ROC\">Model Quality and ROC</a><br>\n\n<br>\n<a href=\"#OutputKaggle\">Output Kaggle Predictions</a><br>\n<a href=\"#TheEnd\">The End</a><br>\n<br>"},{"metadata":{"papermill":{"duration":0.117975,"end_time":"2020-10-06T17:09:08.409194","exception":false,"start_time":"2020-10-06T17:09:08.291219","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## <a id=\"Diary\">Diary and Scores History</a>\nBack to <a href=\"#Index\">Index</a> <br>\n\n**(vN) LB-score** <-- These entries mark each commit and its LB score (if submitted). <br>\n(v0) Read in the data and looked at the Target values: they are very sparse... Most common one has fraction 3.5%. Many ids, 40%, have 0 active targets; 51% have 1; 6% have 2; and about 3% for 3 and more targets active in an id. Compared with a random distribution there are more 2s and 3+s than expected, i.e., there's some anti-correlation between targets being active.<br>\n**(v1,2) 0.02398** Expected a score something like 0.021, OK. (v2: Had to disable internet.)<br>\n11Sep2020: Started looking through the Discussion posts, from oldest to most recent posted, things I learned: `cp_type=='ctl_vehicle'` are [controls](https://www.kaggle.com/c/lish-moa/discussion/180304) and have no active targets; `cp_time, cp_dose` have [6 possible combinations](https://www.kaggle.com/c/lish-moa/discussion/180588), so the same drug (may?must?) appear 6 times in ids; a very nice [EDA notebook](https://www.kaggle.com/datafan07/mechanisms-of-action-what-do-we-have-here) includes scatter-matrix plot in PC space, color-coded by cp_type: shows the controls have less feature variation.<br>\n12Sep2020: Separated out the Treatment ids and modified the fraction calculation to use these (no controls) to estimate the active fractions. Also set the Test control ids to have 0s for their predictions.<br>\n**(v3) 0.02363** Expected 0.02045.<br>\nNo obvious change in target fractions when selecting on cp_time or cp_dose, though a [cp_time effect](https://www.kaggle.com/c/lish-moa/discussion/180981) was seen by others. Posted 0,1,2,3+ actual and expected percentages to a [discussion](https://www.kaggle.com/c/lish-moa/discussion/180500).<br>\n13Sep2020: Added df_aug_feats with additional useful columns, e.g., numMoA. Looked at the c features and it does seem they have similar variation [as reported](https://www.kaggle.com/c/lish-moa/discussion/181798); with this in mind, create two c-summary features: c-ave and c-std. A c-std vs c-ave plot colored by numMoA has structure. Add the c-ave,std to the test features too.<br>\n14Sep2020: Added g-ave and g-std features too. g-ave seems not useful, mostly near 0. Also plotted g-std vs c-ave and c-std vs g-std; added coloring by cp_time which has patterns, coloring by cp_dose seems intermixed with no patterns. These suggest that the features: c-ave, c-std, g-std, and cp_time could be used in predicting numMoA. Use an xgb classifier to decide if numMoA=0 (y=1) based on the features: cp_time, c-ave, c-std, g-ave, g-std.<br>\n(v4) (not submitted, should be the same as v3) <br>\nIf we identify an id as having MoA=0 then we can half, say, all 206 of its targets predictions improving its summed score contribution (by 206 x 0.005 ish ~ 1.0); if we were wrong then the one (or two) active the target(s) in that id will increase their contribution to the summed score by 0.69. So the number we can select/detect with a high precision (> 66%, say) is measure of how well we're doing.<br>\n**(v5) 0.02363** Train: 233+102, 69.5% precision, ROC 0.633. Test: 35 assigned MoA=0. <br>\n**(v6) 0.02363** Train: 407+149, 73.2% precision, ROC 0.642. Test: 72 assigned MoA=0.<br>\nChanged xgb params: min_child_weight=1(was 4), colsample_bytree=0.8(was 0.4).   <br>\n15Sep2020: The C-std vs C-ave plot of the MoA=0 ML-selected ids in (v6) seems like it is doing as well as could be expected from the few features used.<br>\nLook into using the (772!) g-values, to identify specific targets? Made average g-vectors from ids with a common MoA to see if any fixed pattern results. Seems kind of variable: some MoAs have many significant g-averaged values, others have just a few.  Add two more features: fraction of gs > 2 (g_hif) and fraction < -2 (g_lof) in an id; they generally track each other. Make and look at their ratio, g_hilof: very close to 1. Do ML adding only g_hif to previous features; makes just about no change to the ML performance.<br>\n(v7,8) 0.02363  Many ids with MoA=1 have c-ave, c-std of (0,0.5), the same as controls - so with these simple features we have no way to decide if MoA=1 or 0 (or even if they are control.) But there is a line-segment region of mostly MoA=0 ids in the c-std va c-ave space: what's special about these?<br>\n18Sep2020: Started creating the average g-vectors of the ids having a given MoA target and the g-vector for controls. <br>\n**(v9) 0.02363** No change, just saving this as a version for the record. <br>\n19Sep2020: Assembled a dataframe (i.e., matrix) of the average g-vectors for each of the MoAs (targets). Using the g-vector for each MoA, we can get a ranking of which MoAs might be most predictable, the top 24 (out of 0 to 205) are: <br>\n163, 136, 110, 139, 46, 142, 194, 103, 133, 63, 127, 65, 169, 34, 199, 153, 47, 35, 112, 12, 166, 36, 175, 82.<br>\nIn a similar way, 22 of the g features were selected that have the highest variation across the MoAs:<br>\ng-392, g-100, g-158, g-50, g-231, g-91, g-744, g-75, g-37, g-175, g-257, g-178, g-672, g-38, g-489, g-332, g-411, g-65, g-761, g-723, g-58, g-131.  Including these as features for the ML classifier gives some improvement but there are still a lot sig_ids with high MoA-or-not ambiguity.<br>\n**(v10) 0.02363** Train: 481+111, 81.3% precision, ROC 0.699 Test: 65 assigned MoA=0.<br>\nImproved the XGB hyper-parameters, max_depth=8, learning_rate=0.05, etc.:<br>\n**(v11) 0.02365** Train: 1504+85, 94.6% precision, ROC 0.849. Test: 149 assigned MoA=0.<br>\n22Sep2020: Add g-5% and g-95% and run with the better hyper-parameters, but don't use the gs_to_use features (these encourage overfitting?). Note the XGB on Kaggle is different from my local version, numbers are updated for the Kaggle-version result.<br>\n**(v12) 0.02364** Train: 866+137, 86.3% precision, ROC 0.758. Test: 123 assigned MoA=0.<br>\nWhat could be a better submission for (v13) than setting all targets to 2E-15: all of the target=1 values will contribute an error score of 33.85 for an average score of ~ 4.1E-5 for each target=1 in test. So, using the score lets us determine the number of targets in the test set (yes, it's test knowledge that we should NOT use in predictions.) If proportional number of 1s in test (expect about 2800 of them), then expect a score around: 0.11500, that should make a splash on the leader board ;-)<br>\n**(v13) 0.12811** (expected ~ 0.11500 when all targets = 2E-15.) So, sum(Test MoA) ~ 3125, which is about 11% more than expected.<br>\nNext, same as v12 (i.e., no gs_to_use features) but with changed model params (based on local GSCV: min_child=1, max_depth=6, colbytree=0.9, learningrate=0.03, n_ests=100)<br>\n**(v14) 0.02363** Train: 401+94, 81.0% precision, ROC 0.679. Test: 50 assigned MoA=0.<br>\n(Small oops: n_ests was 100 for the v14 run, but should have been 120.)<br>\n24Sep2020: Add in c-5% and c-95% features; replace g-hilof with g-hilopc. Look at correlations between the 22 gs_to_use: looks like a subset of 11 are most unique, so reduce gs_to_use to that subset: g-392, g-100, g-158, g-91, g-231, g-175, g-178, g-75, g-65, g-332, g-50. Look at their histograms over the 206 average target vectors.<br>\n25Sep2020: Selecting and fitting each cp_time's data separately looks like it gives better results (the numerical/categorical cp_time feature was not being used well by XGB.) Use three separate models for the 3 cp_time values and combine their results for the final submission...<br>\n**(v15) 0.02367** Train: 1651+185, 89.9% precision, ROC 0.821. Test: 229 assigned MoA=0.<br>\n(Run on my setup: Train: 1715+214, 88.9% precision, ROC 0.832. Test: 272 assigned MoA=0.) Well, the score increased :(  <br>\nOK, try more 'regularization' by setting min_child_weight = 2 and max_depth = 5, and use threshold = 0.47 for more certainty...:<br>\n**(v16) 0.02363** Train: 600+66, 90.1% precision, ROC 0.767. Test: 73 assigned MoA=0.<br>\n(Run on my setup: Train: 584+66, 89.8% precision, ROC 0.770. Test: 76 assigned MoA=0.) There are too few MoA=0 sig_ids that can be clearly identified and that makes a too-tiny effect on the score.<br>\n27Sep2020: Based on the [t-SNE discussion post](https://www.kaggle.com/c/lish-moa/discussion/186919) and\n[Notebook](https://www.kaggle.com/nelsonewert/using-t-sne-to-identify-clusters-in-the-data), I ran t-SNE on the training data to see what kind of grouping appears, color-coded output by: controls, MoA=0,1, and specific target MoA. There are some all-MoA \"islands\" in the plot and these are each dominated by a particular target,\nsee [t-SNE section below.](#tSNEfeatures), so expect these targets could be well identified. <br>\n1-Oct2020: Selected 3 c- features to include in ML as well...<br>\n**(v17) 0.02364** Train: 594+59, 90.9% precision, ROC 0.776. Test: 82 assigned MoA=0.<br>\n(Run on my setup: Train: 582+65, 89.9% precision, ROC 0.768. Test: 84 assigned MoA=0.) Feels like it is over-fitting, remove many of the general g- features leaving just g-hif and g-95% plus the 11 selected specific gs (along with the 4 general cs and, now, 4 specific c features: 21 features in all.)<br>\n**(v18) 0.02363** Train: 538+57, 90.4% precision, ROC 0.763. Test: 64 assigned MoA=0.<br>\n(Run on my setup: Train: 531+50, 91.4% precision, ROC 0.771. Test: 57 assigned MoA=0.)<br>\n<br>\n . . . 2-Oct2020: Slight \"ah-ha\" moment: I focussed on the MoA=0 ones because then I'd know that *all* of their targets should be near 0, so all of them in the row could be reduced.  I'd thought that knowing MoA=1 for a row is not so useful because we wouldn't know *which* of the MoAs is the one to 'set'. However, since this is a probability metric we can instead increase all the targets in the MoA=1 rows by some amount and reduce targets in all the other rows by some amount. So I've switched to having y=1 mean MoA >= 1 for that row. Note that the ROC stays the same when switching the 'sign' of the binary classification.<br>\n**(v19) 0.02355** Train: 3161+70, 97.8% precision, ROC 0.763. Test: 486-18 assigned MoA=1.<br>\n(Run on my setup: Train: 3149+61, 98.1% precision, ROC 0.771. Test: 493-20 assigned MoA=1.)<br>\n . . . Since it seems some targets are more detectable than others, limiting the MoA>0 classifier to a subset of targets improves its performance. The classification is then used to adjust the probabilities for just the targets in the subset. <br>\nUsing **the subset of 9 \"t-SNE island\" targets**, fewer MoA>0 are detected but larger probability corrections are made for that subset of targets - improving the score a bit more? -yes. <br> \n**(v20) 0.02284** Train: 1782+30, 98.3% precision, ROC 0.960. Test: 262-1 assigned MoA=1. <br>\n(Run on my setup: Train: 1789+28, 98.4% precision, ROC 0.962. Test: 263-1 assigned MoA=1.)<br>\n . . . In v20, adjustments to the prediction probabilities were made only to the targets in the target subset;<br> now also make appropriate changes to the not-in-subset target values. (The ML is the same.)<br> \n**(v21) 0.02271** Train: 1782+30, 98.3% precision, ROC 0.960. Test: 262-1 assigned MoA=1. <br>\n(Run on my setup: Train: 1789+28, 98.4% precision, ROC 0.962. Test: 263-1 assigned MoA=1.)<br>\n . . . 4-Oct2020: As an extreme version of the subset method, try **the target subset: nfkb_inhibitor, proteasome_inhibitor**; these are both very common, detectable, and often occur together, set y=1 for numSub>1 to find where both are set (there are 718 of these in the training set). This is fewer than the previous subset, but they are set much closer to 1 (less error score) than in a diluted 9-target set. Let's see...<br>\n**(v22) 0.02228** Train:  712+5, 99.3% precision, ROC 0.999. Test: 138-0 assigned y=1. <br>\n(Run on my setup: Train:  712+5, 99.3% precision, ROC 0.999. Test: 138-0 assigned y=1.)<br>\n . . . Made some t-SNE summary values and plots trying to identify targets with low spread in t-SNE space, not so convincing... As another subset demo use **the subset \"4: low t-SNE rms\"** these are higher-counts ones in the set of 9 and have a different location in c-std--c-ave space than the \"big 2\" used in (v22).<br>\n**(v23) 0.02331** Train:  820+25, 97.0% precision, ROC 0.974. Test: 91-0 assigned y=1. <br>\n(Run on my setup: Train:  823+27, 96.8% precision, ROC 0.974. Test: 89-0 assigned y=1.)<br>\n . . . 6-Oct2020: Another subset: use **22 targets with an average MoA rate > 0.01**, but exclude the \"big 2\" targets since they are unique (they occur together and are more common.) Show this subset highlighted in one of the t-SNE plots too.<br>\n**(v24) 0.02348** Train:  1125+30, 97.4% precision, ROC 0.778. Test: 136-1 assigned y=1. <br>\n<a id=\"DiaryRecent\">Recent activity:</a><br>\n . . . 10Oct2020: Put the y_yhat_plots() routine in an external file and import it. Use just **1 target, nfkb_inhibitor**... <br>\n**(v25) 0.02310** Train: 716+2, 99.7% precision, ROC 0.974. Test: 137-0 assigned y=1. <br>\n(Run on my setup: Train: 716+2, 99.7% precision, ROC 0.974. Test: 138-0 assigned y=1.)<br>\n . . . Some cleaning up, and set threshold for ~ 97% precision for the single-MoA classifying.  Do the **cdk_inhibitor** by itself as another example:<br>\n**(v26) 0.02349** Train: 283+8, 97.2% precision, ROC 0.979. Test:  31-0 assigned y=1. <br>\n(Run on my setup: Train: 283+8, 97.2% precision, ROC 0.979. Test:  31-0 assigned y=1.)<br>\n . . . Looking at other individual targets, each of the \"tSNE-9\" are very detectable (good Recall with 97% Precision) whereas other targets (even ones that have a high number of MoAs, or are expected to be 'detectable') are not clearly detected.<br>\nTo add some 'regularization' fit the models on 4x the X,y with added random noise in c- and g- features, use std = 0.5.<br>\n . . . The subset of **16 targets that have a > 0.01 (but are not in the tSNE-9)** have very little signal in the features. Use all 22 of the initially selected \"gs_to_use\" and set a 95% precision (threshold 0.311(me), 0.319(kaggle)):<br>\n**(v27) 0.02358** Train: 872+41, 19% Recall @ 95.5% Precision, ROC 0.865. Test: 100-12 assigned y=1. <br>\n(Run on my setup: Train: 954+49, 21% Recall @ 95.1% Precision, ROC 0.856. Test: 104-8 assigned y=1.)<br>\n . . . The subset of **7 g-detectable w/o tSNE-9** have g-vectors with signal, do them at 95% precision:<br>\n**(v28) 0.02354** Train: 417+22, 56% Recall @ 94.9% Precision, ROC 0.972. Test:  50-0 assigned y=1. <br>\n(Run on my setup: Train: 417+22, 56% Recall @ 94.9% Precision, ROC 0.972. Test:  50-0 assigned y=1.)<br>\n . . . Feels like the 22 gs_to_use don't have much information on the previous 16(v27) and 7(v28) target sets. Look at the g-vectors just for these 23 targets and find the ones with most variation across them and add those g-vectors to the features (10 more addded.)  Re-do v28 with these extra gs-added:<br>\n**(v29) 0.02353** Train: 485+23, 65% Recall @ 95.5% Precision, ROC 0.978. Test:  56-0 assigned y=1. <br>\n(Run on my setup: Train: 484+25, 65% Recall @ 95.1% Precision, ROC 0.980. Test:  59-1 assigned y=1.)<br>\n. . . Use different hyper-parameters: min_child=2, maxdepth=10, colsample=0.8; go back to the 11 gs_to_use (instead of 22) and include the 10 more 'do-better' features:<br>\n **(v30) 0.02363** Train: 667+35, 89% Recall @ 95% Precision, ROC 0.998. Test:  97-2 assigned y=1. <br>\n   . . . Well, that is overfitting... use child=4, depth=8, colsample=0.7:<br>\n**(v31) 0.02354** Train: 505+26, 68% Recall @ 95% Precision, ROC 0.988. Test:  64-2 assigned y=1. <br>\n<BR>\n . . . OK, go back to using all targets to classify as MoA or notMoA<br>\n**(v32) 0.023??** Train: 5768+65, 40% Recall @ 99% Precision, ROC 0.913. Test: 852-80 assigned y=1. <br>\n . . . **HALT**"},{"metadata":{"papermill":{"duration":0.117408,"end_time":"2020-10-06T17:09:08.644241","exception":false,"start_time":"2020-10-06T17:09:08.526833","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### To-do:\n\n- Make a local test set and score it rather than submitting to competition to score.\n- Re-check the hyper-parameters now that we've included: new, larger feature set, and the x4 X,y expansion.\n- Make the ROC plot for a subset of all the sig_ids to help diagnose where errors are coming from.\n"},{"metadata":{"papermill":{"duration":0.116382,"end_time":"2020-10-06T17:09:08.877307","exception":false,"start_time":"2020-10-06T17:09:08.760925","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## <a id=\"Preliminaries\">Preliminaries</a>\nBack to <a href=\"#Index\">Index</a> <br>"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:09:09.116671Z","iopub.status.busy":"2020-10-06T17:09:09.115671Z","iopub.status.idle":"2020-10-06T17:09:09.118922Z","shell.execute_reply":"2020-10-06T17:09:09.118158Z"},"papermill":{"duration":0.125234,"end_time":"2020-10-06T17:09:09.119086","exception":false,"start_time":"2020-10-06T17:09:08.993852","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Some key parameters/variables to control the notebook operation\nLOCATION_KAGGLE = True\nSHOW_EDA = True\n\n# used to label any output files\nversion_str = \"v32\"\n\n# Can write up to 5GB to the current directory (/kaggle/working/)\nout_dir = \".\"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2020-10-06T17:09:09.36308Z","iopub.status.busy":"2020-10-06T17:09:09.362242Z","iopub.status.idle":"2020-10-06T17:09:10.783816Z","shell.execute_reply":"2020-10-06T17:09:10.783042Z"},"papermill":{"duration":1.546007,"end_time":"2020-10-06T17:09:10.783938","exception":false,"start_time":"2020-10-06T17:09:09.237931","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Usual suspects to use\nimport numpy as np # linear algebra\nfrom numpy import random\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n# t-SNE\nfrom sklearn.manifold import TSNE\n\nfrom time import time\n\nimport os","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:09:11.02571Z","iopub.status.busy":"2020-10-06T17:09:11.024934Z","iopub.status.idle":"2020-10-06T17:09:11.028524Z","shell.execute_reply":"2020-10-06T17:09:11.027758Z"},"papermill":{"duration":0.126974,"end_time":"2020-10-06T17:09:11.028651","exception":false,"start_time":"2020-10-06T17:09:10.901677","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# The seed is set once here at beginning of notebook.\nRANDOM_SEED = 360\n# Uncomment this to get a time-based random value, 0 to 1023\n##RANDOM_SEED = int(time()) % 2**10\n# in either case initialize the seed\nnp.random.seed(RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The y_yhat_plots() routine"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the y_yhat_plots() routine to shown how the prediction is doing.\n# This routine is taken from the file chirp_roc_lib.py in the github repo at: \n#   https://github.com/dan3dewey/chirp-to-ROC\n# Some small modifications have been made here.\nimport roc_plots\nfrom roc_plots import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In case changes are made and we want to reload it while the kernel is running:\nimport importlib\nimportlib.reload(roc_plots)\nfrom roc_plots import *","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.119192,"end_time":"2020-10-06T17:09:11.266351","exception":false,"start_time":"2020-10-06T17:09:11.147159","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## <a id=\"DataProcessing\">Reading and Processing csv Data Files</a>\nBack to <a href=\"#Index\">Index</a>"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:09:11.50843Z","iopub.status.busy":"2020-10-06T17:09:11.507456Z","iopub.status.idle":"2020-10-06T17:09:11.513841Z","shell.execute_reply":"2020-10-06T17:09:11.512977Z"},"papermill":{"duration":0.129404,"end_time":"2020-10-06T17:09:11.513994","exception":false,"start_time":"2020-10-06T17:09:11.38459","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Show available data files (Kaggle provided code):\nfor dirname, _, filenames in os.walk('../input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:09:11.759454Z","iopub.status.busy":"2020-10-06T17:09:11.758305Z","iopub.status.idle":"2020-10-06T17:09:11.761087Z","shell.execute_reply":"2020-10-06T17:09:11.7618Z"},"papermill":{"duration":0.128613,"end_time":"2020-10-06T17:09:11.761952","exception":false,"start_time":"2020-10-06T17:09:11.633339","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Where are the data files\n# Data dir\nif LOCATION_KAGGLE:\n    dat_dir ='../input/lish-moa/'\n    # CSV file names - features and targets are separate\n    train_feats = \"train_features.csv\"\n    train_targs = \"train_targets_scored.csv\"\n    test_feats = \"test_features.csv\"\n    test_targs = \"sample_submission.csv\"\nelse:\n    dat_dir =\"../input/\"\n    # CSV file names - features and targets are separate\n    train_feats = \"train_features.csv.zip\"\n    train_targs = \"train_targets_scored.csv.zip\"\n    test_feats = \"test_features.csv.zip\"\n    test_targs = \"sample_submission.csv.zip\"","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.119132,"end_time":"2020-10-06T17:09:12.031624","exception":false,"start_time":"2020-10-06T17:09:11.912492","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Read the files and do basic feature defining and processing\nMuch of the feature processing/adjusting is done in this one code cell to prevent it getting out of sync."},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:09:12.383589Z","iopub.status.busy":"2020-10-06T17:09:12.3625Z","iopub.status.idle":"2020-10-06T17:12:19.381886Z","shell.execute_reply":"2020-10-06T17:12:19.380962Z"},"papermill":{"duration":187.193478,"end_time":"2020-10-06T17:12:19.382095","exception":false,"start_time":"2020-10-06T17:09:12.188617","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Read the files and do All the basic feature processing\n# time it\nt_preproc = time()\n\n# Read in the train and test data\n\n\n# = = = = =\n# Train\ndf_train_feats = pd.read_csv(dat_dir+train_feats)\ndf_train_targs = pd.read_csv(dat_dir+train_targs)\n\n\n# = = = = =\n# Test\ndf_test_feats = pd.read_csv(dat_dir+test_feats)\ndf_test_targs = pd.read_csv(dat_dir+test_targs)\n\nprint(\"{:.2f} seconds -- read in data files\\n\".format(time() - t_preproc))\n \n\n# Separate out the Controls\n#\n# cp_type feature has only 2 values: 'trt_cp' 'ctl_vehicle'\n#   trt_cp      means a treatment is applied\n#   ctl_vehicle means it was a control\n# There are 1866 controls in Train, and 358 in Test.\ntrain_ctls = df_train_feats.cp_type == 'ctl_vehicle'\ntest_ctls = df_test_feats.cp_type == 'ctl_vehicle'\n#\n# Get dfs of just the control features:\ndf_train_ctls = df_train_feats[train_ctls].copy()\ndf_test_ctls = df_test_feats[test_ctls].copy()\n#\n# Check the sum of all the train-control Targets - should be 0.\nprint(\"Train control targets sum:\",\n      sum(df_train_targs[train_ctls].drop(columns=['sig_id']).sum()))\n#\n# Create Treatment-only dfs of feats and targs:\ndf_treat_feats = df_train_feats[~train_ctls].copy()\ndf_treat_targs = df_train_targs[~train_ctls].copy()\n#\n# For now, don't need to make treatment-only df for Test:\n# We have the test controls, df_test_ctls, to compare with train values.\n# Probably do not need separate Test-treament df,\n# since we'll just make predictions on all Test ids and\n# then set the Test-control targets to 0.\n\n\n# Check the sum of all the treatment targets - should be 16844.\nprint(\"Treatment targets sum:\",\n      sum(df_treat_targs.drop(columns=['sig_id']).sum()))\n\n\n# Create some other columns, etc.\n# Normally additional features would be added to the df train (or df treat, here.)\n# To keep the df_treat_feats 'clean' for easy stat.s,\n# instead make a df_aug_feats from treat and add other columns to it:\ndf_aug_feats = df_treat_feats.copy()\n\n\n# All Targets --> numMoA\n# --- Add a numMoA column (number of MoA set in each sig_id row) ---\n# Of course this leaks target information, so don't use for prediction ;-)\n# Instead, make a binary target from numMoA, e.g., y=1 when numMoA > 0\n#\n# Use this==0 as the target for \"MoA or not-MoA\" ML:\ndf_aug_feats['numMoA'] = df_treat_targs.drop(['sig_id'],axis=1).sum(axis=1)\n\n\n# Subset of Targets --> numSub\n# --- Add a numSub column (similar to numMoA but for a SUBSET of the targets.)\n#\n# These 9 targets were identified as \"islands\" in the t-SNE output, \n# so expect they can be well 'learned':\n##targ_subset = ['proteasome_inhibitor', 'nfkb_inhibitor', 'glucocorticoid_receptor_agonist',\n##               'raf_inhibitor', 'cdk_inhibitor', 'hmgcr_inhibitor', \n##               'egfr_inhibitor', 'hsp_inhibitor', 'tubulin_inhibitor']\n##targ_subset_name = \"9: t-SNE islands\"\n##df_aug_feats['numSub'] = df_treat_targs[targ_subset].sum(axis=1)\n#\n# These 2 targets are detectable and mostly occur together,\n# numSub>0 is the OR of them, sumSub>1 is the AND.\n##targ_subset = ['proteasome_inhibitor', 'nfkb_inhibitor']\n##targ_subset_name = \"2: proteasome & nfkb\"\n##df_aug_feats['numSub'] = df_treat_targs[targ_subset].sum(axis=1)\n#\n# The tSNE-9 without the \"big 2\":\ntarg_subset = ['glucocorticoid_receptor_agonist',\n               'raf_inhibitor', 'cdk_inhibitor', 'hmgcr_inhibitor', \n               'egfr_inhibitor', 'hsp_inhibitor', 'tubulin_inhibitor']\ntarg_subset_name = \"7: t-SNE-9 w/o big 2\"\ndf_aug_feats['numSub'] = df_treat_targs[targ_subset].sum(axis=1)\n#\n# Try these 4 together:\n##targ_subset = ['glucocorticoid_receptor_agonist',\n##               'raf_inhibitor', 'cdk_inhibitor', 'hmgcr_inhibitor']\n##targ_subset_name = \"4: low t-SNE rms\"\n##df_aug_feats['numSub'] = df_treat_targs[targ_subset].sum(axis=1)\n#\n# All the targets with a > 0.01\n#    *except* for 'proteasome_inhibitor', 'nfkb_inhibitor' because they are unique -\n#             they are most common and appear together usually.\n##targ_subset = ['acetylcholine_receptor_antagonist', 'adrenergic_receptor_agonist',\n##               'adrenergic_receptor_antagonist', 'calcium_channel_blocker',\n##               'cdk_inhibitor', 'cyclooxygenase_inhibitor',\n##               'dna_inhibitor', 'dopamine_receptor_antagonist',\n##               'egfr_inhibitor', 'flt3_inhibitor',\n##               'glucocorticoid_receptor_agonist', 'glutamate_receptor_antagonist',\n##               'histamine_receptor_antagonist', 'hmgcr_inhibitor',\n##               'kit_inhibitor',\n##               'pdgfr_inhibitor', 'phosphodiesterase_inhibitor',\n##               'raf_inhibitor',\n##               'serotonin_receptor_agonist', 'serotonin_receptor_antagonist',\n##               'sodium_channel_inhibitor', 'tubulin_inhibitor']\n##targ_subset_name = \"22: a above 0.01\"\n##df_aug_feats['numSub'] = df_treat_targs[targ_subset].sum(axis=1)\n#\n# All above 0.010 except for the tSNE-9:     * Not very detectable *\n##targ_subset = ['acetylcholine_receptor_antagonist', 'adrenergic_receptor_agonist',\n##               'adrenergic_receptor_antagonist', 'calcium_channel_blocker', 'cyclooxygenase_inhibitor',\n##               'dna_inhibitor', 'dopamine_receptor_antagonist', 'flt3_inhibitor',\n##               'glutamate_receptor_antagonist', 'histamine_receptor_antagonist', 'kit_inhibitor',\n##               'pdgfr_inhibitor', 'phosphodiesterase_inhibitor', 'serotonin_receptor_agonist',\n##               'serotonin_receptor_antagonist', 'sodium_channel_inhibitor']\n##targ_subset_name = \"16: >0.010, w/o tSNE-9\"\n##df_aug_feats['numSub'] = df_treat_targs[targ_subset].sum(axis=1)\n#\n# These are the targets with 'highly detectable' g-vectors (unless listed in tSNE-9 already)\n#    'topoisomerase_inhibitor', 'hdac_inhibitor', 'mtor_inhibitor', 'mek_inhibitor',\n#    'pi3k_inhibitor', 'protein_synthesis_inhibitor', 'atpase_inhibitor'\n##targ_subset = ['topoisomerase_inhibitor', 'hdac_inhibitor', 'mtor_inhibitor', 'mek_inhibitor',\n##        'pi3k_inhibitor', 'protein_synthesis_inhibitor', 'atpase_inhibitor']\n##targ_subset_name = \"7: detectable\"\n##df_aug_feats['numSub'] = df_treat_targs[targ_subset].sum(axis=1)\n#\n# OR\n#\n# A single target ;-)     These lines are repeated below in Machine Learning to\n#                         easily switch between single targets.\n##targ_subset = ['histamine_receptor_antagonist']\n##targ_subset_name = \"1: hista\"\n##df_aug_feats['numSub'] = df_treat_targs[targ_subset].sum(axis=1)\n#\n\n# --- The average and std of the c-0 to c-99 features ---\nprint(\"\\n{:.2f} s -- Adding Train c-ave, c-std...\".format(time() - t_preproc))\n\n# TRAIN: Add the mean and std over the c values, for each id:\nccols = list(range(776, 776+100))\nn_treats = len(df_treat_feats)\nc_aves = np.zeros(n_treats)\nc_stds = np.zeros(n_treats)\n# 5th and 95th percentile values:\nc_5pc = np.zeros(n_treats)\nc_95pc = np.zeros(n_treats)\nn_cs = len(ccols); i5pc = int(0.05*n_cs) ; i95pc = int(0.95*n_cs)\nfor irow in range(n_treats):\n    these_cs = df_treat_feats.iloc[irow, ccols].values\n    these_cs.sort()\n    \n    c_aves[irow] = these_cs.mean()\n    c_stds[irow] = these_cs.std()\n    c_5pc[irow] = these_cs[i5pc]\n    c_95pc[irow] = these_cs[i95pc]\n# and put them in the augmented df:\ndf_aug_feats['c-ave'] = c_aves\ndf_aug_feats['c-std'] = c_stds\ndf_aug_feats['c-5%'] = c_5pc\ndf_aug_feats['c-95%'] = c_95pc\n\nprint(\"\\n{:.2f} s -- Adding Test c-ave, c-std...\".format(time() - t_preproc))\n\n# TEST: generate these and add them to the df_test_feats:\nccols = list(range(776, 776+100))\nn_treats = len(df_test_feats)\nc_aves = np.zeros(n_treats)\nc_stds = np.zeros(n_treats)\n# 5th and 95th percentile values:\nc_5pc = np.zeros(n_treats)\nc_95pc = np.zeros(n_treats)\nn_cs = len(ccols); i5pc = int(0.05*n_cs) ; i95pc = int(0.95*n_cs)\nfor irow in range(n_treats):\n    these_cs = df_test_feats.iloc[irow, ccols].values\n    these_cs.sort()\n    \n    c_aves[irow] = these_cs.mean()\n    c_stds[irow] = these_cs.std()\n    c_5pc[irow] = these_cs[i5pc]\n    c_95pc[irow] = these_cs[i95pc]\n# and put them in the test df:\ndf_test_feats['c-ave'] = c_aves\ndf_test_feats['c-std'] = c_stds\ndf_test_feats['c-5%'] = c_5pc\ndf_test_feats['c-95%'] = c_95pc\n\n\n# --- The average and std of the g-0 to g-771 features ---\nprint(\"\\n{:.2f} s -- Adding Train g-ave, g-std...\".format(time() - t_preproc))\n\n# TRAIN: Add the mean and std over the *** g values ***, for each id:\ngcols = list(range(4, 4+772))\nn_treats = len(df_treat_feats)\ng_aves = np.zeros(n_treats)\ng_stds = np.zeros(n_treats)\n# fractions above 2 and below -2:\ng_hif = np.zeros(n_treats)\ng_lof = np.zeros(n_treats)\n# 5th and 95th percentile values:\ng_5pc = np.zeros(n_treats)\ng_95pc = np.zeros(n_treats)\nn_gs = len(gcols); i5pc = int(0.05*n_gs) ; i95pc = int(0.95*n_gs)\nfor irow in range(n_treats):\n    these_gs = df_treat_feats.iloc[irow, gcols].values\n    these_gs.sort()\n\n    g_aves[irow] = these_gs.mean()\n    g_stds[irow] = these_gs.std()\n    g_hif[irow] = sum(1.0*(these_gs > 2.0))/n_gs\n    g_lof[irow] = sum(1.0*(these_gs < -2.0))/n_gs\n    g_5pc[irow] = these_gs[i5pc]\n    g_95pc[irow] = these_gs[i95pc]\n# and put them in the augmented df:\ndf_aug_feats['g-ave'] = g_aves\ndf_aug_feats['g-std'] = g_stds\ndf_aug_feats['g-hif'] = g_hif\ndf_aug_feats['g-lof'] = g_lof\ndf_aug_feats['g-hilof'] = g_hif/(g_lof + 1.3e-3)  # use hilopc instead\ndf_aug_feats['g-5%'] = g_5pc\ndf_aug_feats['g-95%'] = g_95pc\ndf_aug_feats['g-hilopc'] = g_95pc/(g_5pc + 1.3e-3)\n\nprint(\"\\n{:.2f} s -- Adding Test g-ave, g-std...\".format(time() - t_preproc))\n\n# TEST: Add the mean and std over the *** g values ***, for each id:\ngcols = list(range(4, 4+772))\nn_treats = len(df_test_feats)   # repurposing n_treats ;-)\ng_aves = np.zeros(n_treats)\ng_stds = np.zeros(n_treats)\n# fractions above 2 and below -2:\ng_hif = np.zeros(n_treats)\ng_lof = np.zeros(n_treats)\n# 5th and 95th percentile values:\ng_5pc = np.zeros(n_treats)\ng_95pc = np.zeros(n_treats)\nn_gs = len(gcols); i5pc = int(0.05*n_gs) ; i95pc = int(0.95*n_gs)\nfor irow in range(n_treats):\n    these_gs = df_test_feats.iloc[irow, gcols].values\n    these_gs.sort()\n    g_aves[irow] = these_gs.mean()\n    g_stds[irow] = these_gs.std()\n    g_hif[irow] = sum(1.0*(these_gs > 2.0))/n_gs\n    g_lof[irow] = sum(1.0*(these_gs < -2.0))/n_gs\n    g_5pc[irow] = these_gs[i5pc]\n    g_95pc[irow] = these_gs[i95pc]\n# and put them in the test df:\ndf_test_feats['g-ave'] = g_aves\ndf_test_feats['g-std'] = g_stds\ndf_test_feats['g-hif'] = g_hif\ndf_test_feats['g-lof'] = g_lof\ndf_test_feats['g-hilof'] = g_hif/(g_lof + 1.3e-3)  # use hilopc instead\ndf_test_feats['g-5%'] = g_5pc\ndf_test_feats['g-95%'] = g_95pc\ndf_test_feats['g-hilopc'] = g_95pc/(g_5pc + 1.3e-3)\n\nprint(\"\\n{:.2f} seconds -- added basic new feature columns\".format(time() - t_preproc))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.17955,"end_time":"2020-10-06T17:12:19.723243","exception":false,"start_time":"2020-10-06T17:12:19.543693","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Check for any NaNs in the Train,Test data"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:19.976211Z","iopub.status.busy":"2020-10-06T17:12:19.975121Z","iopub.status.idle":"2020-10-06T17:12:19.97871Z","shell.execute_reply":"2020-10-06T17:12:19.97797Z"},"papermill":{"duration":0.134526,"end_time":"2020-10-06T17:12:19.978835","exception":false,"start_time":"2020-10-06T17:12:19.844309","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Check for NaN's in the data\n# Go through the columns one at a time\n\n# Do it for feats and targs:\n# feats: (train, test)\n#   All OK - no NaNs found.\n# targs: (train only)\n#   All OK - no NaNs found.\n\nif False:\n    n_train = len(df_train_feats)\n    n_test = len(df_test_feats)\n    print(\"\\nChecking for NaNs:\\n\")\n    all_ok = True\n    #\n    nanpc_train = 0; nanpc_test = 0\n    #  do it for feats  and  targs\n    for col in df_train_targs.columns:\n        nona_train = len(df_train_targs[col].dropna(axis=0))\n        nanpc_train = 100.0*(n_train-nona_train)/n_train\n        ##nona_test = len(df_test_feats[col].dropna(axis=0))\n        ##nanpc_test = 100.0*(n_test-nona_test)/n_test\n        # Only show it if there are NaNs:\n        if (nanpc_train + nanpc_test > 0.0):\n            print(\"{:.3f}%  {} OK out of {}\".format(nanpc_train, nona_train, n_train), \"  \"+col)\n            ##print(\"{:.3f}%  {} OK out of {}\".format(nanpc_test, nona_test, n_test), \"  \"+col)\n            all_ok = False\n    if all_ok:\n        print(\"   All OK - no NaNs found.\\n\")\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.121375,"end_time":"2020-10-06T17:12:20.22078","exception":false,"start_time":"2020-10-06T17:12:20.099405","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## <a id=\"TargetSummary\">Looking at the Targets</a>\nBack to <a href=\"#Index\">Index</a>\n\nThere are 206 target values given for each of the 23814 (21948 treatment) rows.<br>\nThe MoA target values are all binary: 0 or 1; think of these as \"not active\" and \"active\"."},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:20.482664Z","iopub.status.busy":"2020-10-06T17:12:20.481386Z","iopub.status.idle":"2020-10-06T17:12:20.517569Z","shell.execute_reply":"2020-10-06T17:12:20.518173Z"},"papermill":{"duration":0.173863,"end_time":"2020-10-06T17:12:20.518333","exception":false,"start_time":"2020-10-06T17:12:20.34447","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Use just the treatment rows:\ndf_treat_targs","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:20.772631Z","iopub.status.busy":"2020-10-06T17:12:20.771656Z","iopub.status.idle":"2020-10-06T17:12:20.803704Z","shell.execute_reply":"2020-10-06T17:12:20.802852Z"},"papermill":{"duration":0.162208,"end_time":"2020-10-06T17:12:20.803858","exception":false,"start_time":"2020-10-06T17:12:20.64165","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Can (re-)do the following analyses with a downselected set of ids,\n# with the down selection based on feature/target criteria.\n\n# This will select all  21948 treatment sig_ids\nselect = df_treat_feats.cp_time > 0\nselect_str = \"All treatment ids\"\n\n\n# No obvious difference when selecting on cp_time or cp_dose.\n\n# Select only the longest time  7180 rows\n##select = df_treat_feats.cp_time == 72\n##select_str = \"All treatment ids w/ t=72\"\n\n# Select only the shortest time   7166 rows\n##select = df_treat_feats.cp_time == 24\n##select_str = \"All treatment ids w/ t=24\"\n\n# Select only the LOW dose, D2   10752 rows\n##select = df_treat_feats.cp_dose == \"D2\"\n##select_str = \"All treatment ids w/ D2\"\n\n# Select only the HIGH dose, D1  11196  rows\n##select = df_treat_feats.cp_dose == \"D1\"\n##select_str = \"All treatment ids w/ D1\"\n\n\n# Can select based on the numMoA value using df_aug_feats - not sure what it means ;-)\n##select = df_aug_feats.numMoA <= 1\n##select_str = \"Treatment ids with numMoA = 0 or 1\"\n\n##select = df_aug_feats.numMoA > 1\n##select_str = \"Treatment ids with numMoA >= 2\"","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:21.063264Z","iopub.status.busy":"2020-10-06T17:12:21.062127Z","iopub.status.idle":"2020-10-06T17:12:21.688544Z","shell.execute_reply":"2020-10-06T17:12:21.687792Z"},"papermill":{"duration":0.760351,"end_time":"2020-10-06T17:12:21.688666","exception":false,"start_time":"2020-10-06T17:12:20.928315","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# What fraction of ids are active for each target?\nn_rows = len(df_treat_targs[select])\n\n# Calculate the sum in each column:\ntarg_col_sums = df_treat_targs[select].drop(['sig_id'],axis=1).sum(axis=0)\nn_targs = len(targ_col_sums)\nprint(\"Number of targets:\",n_targs,\"   Sum of all:\",sum(targ_col_sums))\n\nplt.hist(np.array(targ_col_sums),bins=100)\nplt.title(\"Histogram of the target (col) sums\")\nplt.xlabel(\"Sum of the column = number of active ids for target\")\nplt.ylabel(\"Number of targets\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:21.950322Z","iopub.status.busy":"2020-10-06T17:12:21.947809Z","iopub.status.idle":"2020-10-06T17:12:21.958333Z","shell.execute_reply":"2020-10-06T17:12:21.957426Z"},"papermill":{"duration":0.144689,"end_time":"2020-10-06T17:12:21.958532","exception":false,"start_time":"2020-10-06T17:12:21.813843","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Average active value of each target\naves_targs = targ_col_sums/n_rows\n\n# List the highest ones\nprint(aves_targs.sort_values(ascending=False)[0:30])\nprint(\"\\nStarting from the lowest:\\n\")\nprint(aves_targs.sort_values(ascending=True)[0:10])","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:22.218404Z","iopub.status.busy":"2020-10-06T17:12:22.217529Z","iopub.status.idle":"2020-10-06T17:12:22.221868Z","shell.execute_reply":"2020-10-06T17:12:22.221065Z"},"papermill":{"duration":0.136561,"end_time":"2020-10-06T17:12:22.221993","exception":false,"start_time":"2020-10-06T17:12:22.085432","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# All targets with a > 0.01:\na_above_010 = list(aves_targs[aves_targs > 0.010].index)\n\nprint(a_above_010)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:22.483706Z","iopub.status.busy":"2020-10-06T17:12:22.482908Z","iopub.status.idle":"2020-10-06T17:12:22.929608Z","shell.execute_reply":"2020-10-06T17:12:22.928731Z"},"papermill":{"duration":0.579424,"end_time":"2020-10-06T17:12:22.929755","exception":false,"start_time":"2020-10-06T17:12:22.350331","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Plot all 206 average active values, sorted\nplt.plot(np.array(aves_targs.sort_values(ascending=True)))\nplt.title(select_str)\nplt.ylabel(\"$a$ value for target\")\nplt.xlabel(\"Targets (sorted)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:23.194897Z","iopub.status.busy":"2020-10-06T17:12:23.194107Z","iopub.status.idle":"2020-10-06T17:12:23.535915Z","shell.execute_reply":"2020-10-06T17:12:23.536464Z"},"papermill":{"duration":0.479506,"end_time":"2020-10-06T17:12:23.536629","exception":false,"start_time":"2020-10-06T17:12:23.057123","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# How many of the 206 Targets are set, aka active, in each id ?\n\n# Calculate the sum in each row:\ntarg_row_sums = df_treat_targs[select].drop(['sig_id'],axis=1).sum(axis=1)\nprint(\"Number of rows:\",n_rows,\"   Sum of all:\",sum(targ_row_sums))\n\n# Note: this same calculation was used for the numMoA augmented feature;\n# so, this line would make the same plot:\n#  plt.hist(np.array(df_aug_feats.numMoA.values),bins=50)\n\nplt.hist(np.array(targ_row_sums),bins=50)\nplt.title(\"Histogram of the id (row) sums\")\nplt.xlabel(\"Sum of the row = number of active targets\")\nplt.ylabel(\"Number of ids\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:23.799523Z","iopub.status.busy":"2020-10-06T17:12:23.79851Z","iopub.status.idle":"2020-10-06T17:12:23.804883Z","shell.execute_reply":"2020-10-06T17:12:23.804247Z"},"papermill":{"duration":0.141678,"end_time":"2020-10-06T17:12:23.805041","exception":false,"start_time":"2020-10-06T17:12:23.663363","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Get the numbers of ids that have 0, 1, 2, 3+ active targets:\nnum_ids_vs_active = np.histogram(np.array(targ_row_sums),\n                                 bins=[-0.5,0.5,1.5,2.5,9.5])[0]\n# The fraction of rows with 0, 1, 2, 3+ active targets in them:\nfrac_vs_active = num_ids_vs_active / n_rows\nfor inum, counts in enumerate(num_ids_vs_active):\n    print(inum, counts, frac_vs_active[inum])","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:24.073067Z","iopub.status.busy":"2020-10-06T17:12:24.071917Z","iopub.status.idle":"2020-10-06T17:12:24.079245Z","shell.execute_reply":"2020-10-06T17:12:24.078372Z"},"papermill":{"duration":0.147219,"end_time":"2020-10-06T17:12:24.079381","exception":false,"start_time":"2020-10-06T17:12:23.932162","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Are the target active fractions roughly independent?\n# Assuming independence we can calculate the expected fraction vs active number.\n\n# Probabilty of getting 0 of the targets active:\nprob0 = 1.0\nfor this_ave in aves_targs:\n    prob0 = prob0 * (1 - this_ave)\nprint(\"0 active:   Actual frac = \",frac_vs_active[0],\n        \"   Expected frac = \",prob0)\n# Probability of exactly 1 active:\nprob1 = 0.0\n# add prob of each one being active and all the others not:\nfor this_ave in aves_targs:\n    prob1 = prob1 + this_ave * prob0/(1-this_ave)\nprint(\"1 active:   Actual frac = \",frac_vs_active[1],\n        \"   Expected frac = \",prob1)\n# Probability of 2 active (slight approximation?)\nprob2 = 0.0\nfor this_ave in aves_targs:\n    prob2 = prob2 + this_ave * (prob1 - this_ave * prob0/(1-this_ave))\n# divide by 2 since double counted:\nprob2 = prob2 / 2\nprint(\"2 active:   Actual frac = \",frac_vs_active[2],\n        \"   Expected frac = \",prob2)\n\nprob3etc = 1.0 - prob0 - prob1 - prob2\nprint(\"3+ active:   Actual frac = \",frac_vs_active[3],\n        \"   Expected frac = \",prob3etc)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.127999,"end_time":"2020-10-06T17:12:24.336092","exception":false,"start_time":"2020-10-06T17:12:24.208093","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Because there are very few sig_ids with 3-or-more active MoAs,<br>\nit's tempting to think about mapping the multi-label problem to a multi-class one,<br>\ne.g., using the [Label Powerset in section 4.1.3](https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/)\nSee also [Ephrem Admasu's response Updated September 9, 2019](https://www.quora.com/What-is-the-best-way-to-convert-multi-label-classification-dataset-into-multi-class-classification-dataset)\n\nWhat are the unique pairs that appear among the 1538 numMoA==2 sig_ids ?"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:24.616585Z","iopub.status.busy":"2020-10-06T17:12:24.615762Z","iopub.status.idle":"2020-10-06T17:12:24.822951Z","shell.execute_reply":"2020-10-06T17:12:24.822153Z"},"papermill":{"duration":0.358648,"end_time":"2020-10-06T17:12:24.823097","exception":false,"start_time":"2020-10-06T17:12:24.464449","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Get all the rows that have 2 MoAs set\ndf_just_targs = df_treat_targs.drop(['sig_id'],axis=1)\ndf_two_moas = df_just_targs[df_aug_feats['numMoA'] == 2]\n\n# array of values: 1,2,3,4,...,206\none_to_206 = 1 + np.array(list(range(206)))\n\nall_pairids = []\nfor irow in range(len(df_two_moas)):\n    two_moas = np.sort(df_two_moas.iloc[irow].values * one_to_206)[[-2,-1]]\n    i_combined = 1000*two_moas[0] + two_moas[1]\n    all_pairids.append(i_combined)\nser_pairids = pd.Series(all_pairids)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:25.08738Z","iopub.status.busy":"2020-10-06T17:12:25.086381Z","iopub.status.idle":"2020-10-06T17:12:25.091312Z","shell.execute_reply":"2020-10-06T17:12:25.091882Z"},"papermill":{"duration":0.141355,"end_time":"2020-10-06T17:12:25.09205","exception":false,"start_time":"2020-10-06T17:12:24.950695","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"pair_counts = ser_pairids.value_counts()\nprint(\"Unique number of pairs:\",len(pair_counts))\n\npair_counts[0:15]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:25.356732Z","iopub.status.busy":"2020-10-06T17:12:25.355676Z","iopub.status.idle":"2020-10-06T17:12:25.36008Z","shell.execute_reply":"2020-10-06T17:12:25.359224Z"},"papermill":{"duration":0.139722,"end_time":"2020-10-06T17:12:25.36022","exception":false,"start_time":"2020-10-06T17:12:25.220498","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Each of the target numbers in the combined pair id are from 1 to 206.\n# To get their names subtract 1:\nprint(df_just_targs.columns[137-1], \",\", df_just_targs.columns[164-1])","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.12986,"end_time":"2020-10-06T17:12:25.619714","exception":false,"start_time":"2020-10-06T17:12:25.489854","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Expected \"guessing\" scores for each target\nThe best constant probability to guess for each target is that target's average, *a*.<br>\nIn that case, we expect an average score for that target of $~-(a~ln(a) + (1-a)~ln(1-a))$"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:25.90596Z","iopub.status.busy":"2020-10-06T17:12:25.897723Z","iopub.status.idle":"2020-10-06T17:12:26.093222Z","shell.execute_reply":"2020-10-06T17:12:26.093915Z"},"papermill":{"duration":0.344146,"end_time":"2020-10-06T17:12:26.094277","exception":false,"start_time":"2020-10-06T17:12:25.750131","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Calculate the individual scores_targs from the a values\nscores_targs = 0.0*aves_targs\nfor itarg in range(len(aves_targs)):\n    this_ave = max(aves_targs[itarg], 1e-5)\n    scores_targs[itarg]= -1.0*(this_ave*np.log(this_ave) +\n                               (1-this_ave)*np.log(1-this_ave))\n\n# Plot all 206 average score values, sorted\nplt.plot(np.array(scores_targs.sort_values(ascending=True)))\nplt.title(select_str)\nplt.ylabel(\"ave score for target\")\nplt.xlabel(\"Targets (sorted)\")\nplt.show()\n\n# Calculate the average of those over all the aves_targs (= a) values:\nprint(\"Expect a score around\", sum(scores_targs)/len(aves_targs))\n# If the control ids are included with target=0 then the score decreases a bit:\nexpect_score = (len(df_treat_targs)/len(df_train_targs))*sum(scores_targs)/len(aves_targs)\nprint(\"Expected score, corrected for controls, is\",expect_score)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:26.368416Z","iopub.status.busy":"2020-10-06T17:12:26.367314Z","iopub.status.idle":"2020-10-06T17:12:26.371688Z","shell.execute_reply":"2020-10-06T17:12:26.37092Z"},"papermill":{"duration":0.144676,"end_time":"2020-10-06T17:12:26.371826","exception":false,"start_time":"2020-10-06T17:12:26.22715","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"scores_targs.sort_values(ascending=False)[0:15]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:26.653769Z","iopub.status.busy":"2020-10-06T17:12:26.644213Z","iopub.status.idle":"2020-10-06T17:12:26.815893Z","shell.execute_reply":"2020-10-06T17:12:26.815152Z"},"papermill":{"duration":0.313208,"end_time":"2020-10-06T17:12:26.816045","exception":false,"start_time":"2020-10-06T17:12:26.502837","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# It's tempting to think that the more present MoAs (the higher a = aves_targs values)\n# might be easier to 'learn' and make accurate predictions for.\n# What score would we get if\n# i) we just \"guessed\", e.g., used aves_targs values for the lower k targets, and\n# ii) we had 100% accuracy for all targets above k.\n\n# We can calculate and plot that vs the cutoff k value:\nsorted_scores = np.array(scores_targs.sort_values(ascending=True))\ncumave_scores = 0.0*sorted_scores\nscore_sum = 0.0\nfor isc, this_sc in enumerate(sorted_scores):\n    score_sum += this_sc\n    cumave_scores[isc] = score_sum/len(cumave_scores)\n\n# Plot expected score vs the a cutoff value\nplt.plot(cumave_scores)\nplt.title(select_str)\nplt.ylabel(\"Score if all higher targets are known\")\nplt.xlabel(\"k, number of lower targets guessed\")\nplt.show()\n\n# The plot and the array cumave_scores[k-1] has:\n# score of 0 when k=0: that's when we know all the targets values.\n# score of ~ 0.22 when k=206: that's when we are guessing all target values.\n#                             (before applying the treatment vs total correction.)\n# score of 0.0100 when k=167: that's when we are guessing targets 1 through 167\n#                             and know exactly the 39 targets from 168 to 206\n#                             (these target numbers are as sorted in increasing a order.)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.131981,"end_time":"2020-10-06T17:12:27.080452","exception":false,"start_time":"2020-10-06T17:12:26.948471","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## <a id=\"FeatureSummary\">Looking at the Features</a>\nBack to <a href=\"#Index\">Index</a>\n"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:27.354218Z","iopub.status.busy":"2020-10-06T17:12:27.353429Z","iopub.status.idle":"2020-10-06T17:12:27.427933Z","shell.execute_reply":"2020-10-06T17:12:27.428614Z"},"papermill":{"duration":0.214494,"end_time":"2020-10-06T17:12:27.428779","exception":false,"start_time":"2020-10-06T17:12:27.214285","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"df_test_feats[~test_ctls]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:27.705594Z","iopub.status.busy":"2020-10-06T17:12:27.704803Z","iopub.status.idle":"2020-10-06T17:12:27.754487Z","shell.execute_reply":"2020-10-06T17:12:27.755093Z"},"papermill":{"duration":0.19237,"end_time":"2020-10-06T17:12:27.755266","exception":false,"start_time":"2020-10-06T17:12:27.562896","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"## Look at the df_treat_feats with the additional features added\ndf_aug_feats","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.133417,"end_time":"2020-10-06T17:12:28.022416","exception":false,"start_time":"2020-10-06T17:12:27.888999","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### The c features"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:28.306401Z","iopub.status.busy":"2020-10-06T17:12:28.305494Z","iopub.status.idle":"2020-10-06T17:12:28.493242Z","shell.execute_reply":"2020-10-06T17:12:28.492645Z"},"papermill":{"duration":0.337153,"end_time":"2020-10-06T17:12:28.493378","exception":false,"start_time":"2020-10-06T17:12:28.156225","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# The features in df_treat_feats.columns are:\n#iloc\n#   0  sig_id  - anonymized values (I believe)\n#   1  cp_type - determines controls\n#   2  cp_time - 3 values: 24, 48, 72\n#   3  cp_dose - 2 values D1, D2\n#\n#   4  g-0  to -  ~ Gaussian-with-outliers, 772 of them in all\n# 775  g-771\n#\n# 776  c-0  to - ?, 100 of them in all\n# 875  c-99\n\n# Look at the values using iloc[ irow, icol ]\n\n# Pick a single feature (COLUMN), show the feature value for a bunch of the ids' values\nicol = 800+10\ncol_str = df_treat_feats.columns[icol]\n# Show some controls and treatment ones\nplt.plot(df_treat_feats.iloc[1000:1500, icol].values,'r.')\nplt.plot(df_train_ctls.iloc[0:500, icol].values,'y.',alpha=0.5)\n\nplt.title(col_str+\" : Treated (red) and  Control (yellow)\")\nplt.ylabel(\"Value of the feature\")\nplt.xlabel(\"A bunch of sig_ids\")\nplt.show()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:28.786199Z","iopub.status.busy":"2020-10-06T17:12:28.784905Z","iopub.status.idle":"2020-10-06T17:12:28.970259Z","shell.execute_reply":"2020-10-06T17:12:28.969586Z"},"papermill":{"duration":0.34127,"end_time":"2020-10-06T17:12:28.970395","exception":false,"start_time":"2020-10-06T17:12:28.629125","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# The features in df_treat_feats.columns are:\n#iloc\n#   0  sig_id  - anonymized values (I believe)\n#   1  cp_type - determines controls\n#   2  cp_time - 3 values: 24, 48, 72\n#   3  cp_dose - 2 values D1, D2\n#\n#   4  g-0  to -  ~ Gaussian-with-outliers, 772 of them in all\n# 775  g-771\n#\n# 776  c-0  to - ?, 100 of them in all\n# 875  c-99\n\n# Look at the values using iloc[ irow, icol ]\n\n# Single sig_id (ROW), show a bunch of the target values\n\n# Looking at the c values for a single id\n# Wow: check out irow = 1199 !\n#      less dramatic but cool: 1197\n# --> The c values do all seem to move together...\n\n# Pick a single sig_id\nirow = 1197\n\n# Look at all 100 of the c features for that sig_id\nicol = 776   # c starts here\ncol_str = df_treat_feats.columns[icol]\nicols = list(range(icol, icol+100))\n# Show some controls and treatment ones\nplt.plot(df_treat_feats.iloc[irow, icols].values,'r.')\nplt.plot(df_train_ctls.iloc[irow, icols].values,'y.',alpha=0.5)\n\nplt.title(df_treat_feats.iloc[irow,0]+\" : Treated (red) and  Control (yellow)\")\nplt.ylabel(\"Value of the feature\")\nplt.xlabel(col_str+\" and following target values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.136888,"end_time":"2020-10-06T17:12:29.243523","exception":false,"start_time":"2020-10-06T17:12:29.106635","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### C- and g- : ave, std, 5%-tile, 95%-tile and various color-coded scatter plots\nFor now the 100 c features for an id are digested to their average, standard deviation, 5%-tile and 95%-tile -- c-ave, c-std, c-5%, c-95%; this makes some sense since the c values seem correlated.  These c-ave, etc. features were added to df_aug_feats (above, right after the data are read in.)   For completeness, g-ave, g-std, g-5%, g-95% were also created and and used to create various 2D color-coded scatter plots."},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:29.53441Z","iopub.status.busy":"2020-10-06T17:12:29.533595Z","iopub.status.idle":"2020-10-06T17:12:34.941043Z","shell.execute_reply":"2020-10-06T17:12:34.941605Z"},"papermill":{"duration":5.559247,"end_time":"2020-10-06T17:12:34.941752","exception":false,"start_time":"2020-10-06T17:12:29.382505","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# The features c-ave and c-std were added to the augmented df\n# Plot c-std vs c-ave.\n# Choose how to color the points:\n#  by numMoA -- looks useful to help determine the MoA\ncolors = 1.0*np.array(df_aug_feats.numMoA > 0) + 1.0*np.array(df_aug_feats.numMoA > 1)\n#  by cp_dose -- no obvious pattern, just intermixed.\n##colors = 1.0*np.array(df_aug_feats.cp_dose == 'D1')\n#  by cp_time -- clear pattern: higher times are further along the swoosh...\n##colors = 1.0*np.array(df_aug_feats.cp_time)\n#\ndf_aug_feats.plot(x='c-ave',y='c-std',kind='scatter', figsize=(12,7),\n                 c=colors, colormap='jet', alpha=0.25, marker='o',s=50,\n                 title='C-std vs C-ave for all non-control sig_ids'+\n                  '   Colored by number of MoAs (0, 1, 2+)')\nplt.savefig(\"C-std_vs_C-ave_MoA-color_\"+version_str+\".png\")\nplt.show()\n\n# This is very cool! Looks like c-ave and c-std have some ability to\n# help decide if numMoA is 0, 1, 2+.\n\n#  * * *  Note that there are many ids with MoA=1\n#         that overlap with the \"control blob\" at (0,0.5).  * * *\n\n\n\n#  by cp_time -- clear pattern: higher times are further along the swoosh...\ncolors = 1.0*np.array(df_aug_feats.cp_time)\n#\ndf_aug_feats.plot(x='c-ave',y='c-std',kind='scatter', figsize=(12,7),\n                 c=colors, colormap='jet', alpha=0.25, marker='o',s=50,\n                 title='C-std vs C-ave for all non-control sig_ids'+\n                  '   Colored by cp_time (24, 48, 72)')\nplt.savefig(\"C-std_vs_C-ave_time-color_\"+version_str+\".png\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:35.335424Z","iopub.status.busy":"2020-10-06T17:12:35.244533Z","iopub.status.idle":"2020-10-06T17:12:40.450525Z","shell.execute_reply":"2020-10-06T17:12:40.449725Z"},"papermill":{"duration":5.362199,"end_time":"2020-10-06T17:12:40.450664","exception":false,"start_time":"2020-10-06T17:12:35.088465","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#  - - C CONTROLS - -\n# Take a look at the mean and std over the c values of the Train controls -\n# The controls look very different: \n# They are concentrated at 0.0,0.5 blob, don't have the parabola swoosh band,\n# and do have a hint of the straight not-MoA line.\nif True:\n    ccols = list(range(776, 776+100))\n    n_treats = len(df_train_ctls)\n    c_aves = np.zeros(n_treats)\n    c_stds = np.zeros(n_treats)\n    for irow in range(n_treats):\n        c_aves[irow] = df_train_ctls.iloc[irow, ccols].values.mean()\n        c_stds[irow] = df_train_ctls.iloc[irow, ccols].values.std()\n    \n    ##lt.plot(c_aves,'b.')\n    ##plt.show()\n\n    ##plt.plot(c_stds,'b.')\n    ##plt.show()\n\n    plt.plot(c_aves,c_stds,'b.')\n    plt.title(\"Very different from the Treatment ids\")\n    plt.ylabel(\"c-std for Controls\")\n    plt.ylabel(\"c-ave for Controls\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:40.761322Z","iopub.status.busy":"2020-10-06T17:12:40.760536Z","iopub.status.idle":"2020-10-06T17:12:41.499802Z","shell.execute_reply":"2020-10-06T17:12:41.499181Z"},"papermill":{"duration":0.896868,"end_time":"2020-10-06T17:12:41.49994","exception":false,"start_time":"2020-10-06T17:12:40.603072","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# TEST: Plot c-std vs c-ave -- no MoA colors, but can show controls.\n\n# 0 - control, 1 - not control\ncolors = 1.4 * 1.0*np.array(df_test_feats.cp_type != 'ctl_vehicle')\ncolors[0]=2.0\n\ndf_test_feats.plot(x='c-ave',y='c-std',kind='scatter', figsize=(12,7),\n                 c=colors, colormap='jet', alpha=0.15, marker='o',s=20,\n                 title='C-std vs C-ave for all TEST sig_ids'+\n                  '   Colored by Control (blue) or not-control (orange)')\nplt.show()\n\n\n# The test values look similarly distributed...","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:41.826148Z","iopub.status.busy":"2020-10-06T17:12:41.825365Z","iopub.status.idle":"2020-10-06T17:12:44.796Z","shell.execute_reply":"2020-10-06T17:12:44.796605Z"},"papermill":{"duration":3.133493,"end_time":"2020-10-06T17:12:44.796765","exception":false,"start_time":"2020-10-06T17:12:41.663272","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# The features c-5% and c-95% were added to the augmented df\n# Plot c-5% vs c-95%.\n# Choose how to color the points:\n#  by numMoA -- looks useful to help determine the MoA\ncolors = 1.0*np.array(df_aug_feats.numMoA > 0) + 1.0*np.array(df_aug_feats.numMoA > 1)\n#  by cp_dose -- no obvious pattern, just intermixed.\n##colors = 1.0*np.array(df_aug_feats.cp_dose == 'D1')\n#  by cp_time -- clear pattern: higher times are further along the swoosh...\n##colors = 1.0*np.array(df_aug_feats.cp_time)\n#\ndf_aug_feats.plot(x='c-95%',y='c-5%',kind='scatter', figsize=(12,7),\n                 c=colors, colormap='jet', alpha=0.25, marker='o',s=50,\n                 title='C-5% vs C-95% for all non-control sig_ids'+\n                  '   Colored by number of MoAs (0, 1, 2+)')\nplt.show()\n\n#  by cp_time -- clear pattern: higher times are further along the swoosh...\ncolors = 1.0*np.array(df_aug_feats.cp_time)\n#\ndf_aug_feats.plot(x='c-95%',y='c-5%',kind='scatter', figsize=(12,7),\n                 c=colors, colormap='jet', alpha=0.25, marker='o',s=50,\n                 title='C-5% vs C-95% for all non-control sig_ids'+\n                  '   Colored by number of MoAs (0, 1, 2+)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.156307,"end_time":"2020-10-06T17:12:45.109609","exception":false,"start_time":"2020-10-06T17:12:44.953302","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:45.43785Z","iopub.status.busy":"2020-10-06T17:12:45.437083Z","iopub.status.idle":"2020-10-06T17:12:48.314835Z","shell.execute_reply":"2020-10-06T17:12:48.31543Z"},"papermill":{"duration":3.047719,"end_time":"2020-10-06T17:12:48.3156","exception":false,"start_time":"2020-10-06T17:12:45.267881","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# The features g-ave and g-std were added to the augmented df\n# Plot g-std vs c-ave.  * used C-ave <-- g-ave not very useful, mostlys ~ 0.\n# Choose how to color the points:\n#  by numMoA -- looks useful to help determine the MoA\ncolors = 1.0*np.array(df_aug_feats.numMoA > 0) + 1.0*np.array(df_aug_feats.numMoA > 1)\n#  by cp_dose -- no obvious pattern, just intermixed.\n##colors = 1.0*np.array(df_aug_feats.cp_dose == 'D1')\n#  by cp_time -- clear pattern when C-ave is used on x axis.\n##colors = 1.0*np.array(df_aug_feats.cp_time)\n#\ndf_aug_feats.plot(x='c-ave',y='g-std',kind='scatter', figsize=(12,7),\n                 c=colors, colormap='jet', alpha=0.25, marker='o',s=50,\n                 title='g-std vs C-ave for all non-control sig_ids'+\n                  '   Colored by number of MoAs (0, 1, 2+)')\nplt.show()\n\n# As kind of expected: the g-ave tends to remain around 0,\n# so,use C-ave instead of g-ave for the plot.\n# The g-std is larger especially for numMoA = 2+ ids\n\n\n#  by cp_time -- clear pattern when C-ave is used on x axis.\ncolors = 1.0*np.array(df_aug_feats.cp_time)\n#\ndf_aug_feats.plot(x='c-ave',y='g-std',kind='scatter', figsize=(12,7),\n                 c=colors, colormap='jet', alpha=0.25, marker='o',s=50,\n                 title='g-std vs C-ave for all non-control sig_ids'+\n                  '   Colored by cp_time (24, 48, 72)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:48.658447Z","iopub.status.busy":"2020-10-06T17:12:48.657625Z","iopub.status.idle":"2020-10-06T17:12:49.376342Z","shell.execute_reply":"2020-10-06T17:12:49.375636Z"},"papermill":{"duration":0.894598,"end_time":"2020-10-06T17:12:49.376466","exception":false,"start_time":"2020-10-06T17:12:48.481868","status":"completed"},"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"# TEST: Plot g-std vs C-ave -- but no MoA colors.\n\n# 0 - control, 1 - not control\ncolors = 1.4 * 1.0*np.array(df_test_feats.cp_type != 'ctl_vehicle')\ncolors[0]=2.0\n\ndf_test_feats.plot(x='c-ave',y='g-std',kind='scatter', figsize=(12,7),\n                 c=colors, colormap='jet', alpha=0.25, marker='o',s=20,\n                 title='g-std vs C-ave for all TEST sig_ids'+\n                  '   Colored by Control (blue) or not-control (orange)')\n\nplt.show()\n\n# The test values look similarly distributed...","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:49.729566Z","iopub.status.busy":"2020-10-06T17:12:49.72798Z","iopub.status.idle":"2020-10-06T17:12:52.647098Z","shell.execute_reply":"2020-10-06T17:12:52.647757Z"},"papermill":{"duration":3.102338,"end_time":"2020-10-06T17:12:52.647917","exception":false,"start_time":"2020-10-06T17:12:49.545579","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Look at C-std vs g-std\n#\n# Choose how to color the points:\n#  by numMoA -- looks useful to help determine the MoA\ncolors = 1.0*np.array(df_aug_feats.numMoA > 0) + 1.0*np.array(df_aug_feats.numMoA > 1)\n#  by cp_dose -- no obvious pattern, just intermixed.\n##colors = 1.0*np.array(df_aug_feats.cp_dose == 'D1')\n#  by cp_time -- clear patterns in plot\n##colors = 1.0*np.array(df_aug_feats.cp_time)\n#\ndf_aug_feats.plot(x='g-std',y='c-std',kind='scatter', figsize=(12,7),\n                 c=colors, colormap='jet', alpha=0.25, marker='o',s=50,\n                 title='C-std vs g-std for all non-control sig_ids'+\n                  '   Colored by number of MoAs (0, 1, 2+)')\nplt.show()\n\n\n\n#  by cp_time -- clear patterns in plot\ncolors = 1.0*np.array(df_aug_feats.cp_time)\n#\ndf_aug_feats.plot(x='g-std',y='c-std',kind='scatter', figsize=(12,7),\n                 c=colors, colormap='jet', alpha=0.25, marker='o',s=50,\n                 title='C-std vs g-std for all non-control sig_ids'+\n                  '   Colored by cp_time (24, 48, 72)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:53.016653Z","iopub.status.busy":"2020-10-06T17:12:53.015107Z","iopub.status.idle":"2020-10-06T17:12:54.454082Z","shell.execute_reply":"2020-10-06T17:12:54.454716Z"},"papermill":{"duration":1.62699,"end_time":"2020-10-06T17:12:54.454882","exception":false,"start_time":"2020-10-06T17:12:52.827892","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Look at g-lof vs g-hif\n#\n# Choose how to color the points:\n#  by numMoA -- looks useful to help determine the MoA\ncolors = 1.0*np.array(df_aug_feats.numMoA > 0) + 1.0*np.array(df_aug_feats.numMoA > 1)\n#  by cp_dose -- no obvious pattern, just intermixed.\n##colors = 1.0*np.array(df_aug_feats.cp_dose == 'D1')\n#  by cp_time -- clear patterns in plot\n##colors = 1.0*np.array(df_aug_feats.cp_time)\n#\ndf_aug_feats.plot(x='g-hif',y='g-lof',kind='scatter', figsize=(12,7),\n                 c=colors, colormap='jet', alpha=0.25, marker='o',s=50,\n                 title='g-lof vs g-hif for all non-control sig_ids'+\n                  '   Colored by number of MoAs (0, 1, 2+)')\nplt.show()\n\n# These basically track each other, mostly.","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:54.83343Z","iopub.status.busy":"2020-10-06T17:12:54.832629Z","iopub.status.idle":"2020-10-06T17:12:56.288214Z","shell.execute_reply":"2020-10-06T17:12:56.288788Z"},"papermill":{"duration":1.647842,"end_time":"2020-10-06T17:12:56.288949","exception":false,"start_time":"2020-10-06T17:12:54.641107","status":"completed"},"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"# Look at g-5% vs g-95%\n#\n# Choose how to color the points:\n#  by numMoA -- looks useful to help determine the MoA\ncolors = 1.0*np.array(df_aug_feats.numMoA > 0) + 1.0*np.array(df_aug_feats.numMoA > 1)\n#  by cp_dose -- no obvious pattern, just intermixed.\n##colors = 1.0*np.array(df_aug_feats.cp_dose == 'D1')\n#  by cp_time -- clear patterns in plot\n##colors = 1.0*np.array(df_aug_feats.cp_time)\n#\ndf_aug_feats.plot(x='g-95%',y='g-5%',kind='scatter', figsize=(12,7),\n                 c=colors, colormap='jet', alpha=0.25, marker='o',s=50,\n                 title='g-5% vs g-95% for all non-control sig_ids'+\n                  '   Colored by number of MoAs (0, 1, 2+)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:56.668477Z","iopub.status.busy":"2020-10-06T17:12:56.667721Z","iopub.status.idle":"2020-10-06T17:12:58.087161Z","shell.execute_reply":"2020-10-06T17:12:58.087781Z"},"papermill":{"duration":1.612476,"end_time":"2020-10-06T17:12:58.087931","exception":false,"start_time":"2020-10-06T17:12:56.475455","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Look at g-95% vs g-hif\n#\n# Choose how to color the points:\n#  by numMoA -- looks useful to help determine the MoA\ncolors = 1.0*np.array(df_aug_feats.numMoA > 0) + 1.0*np.array(df_aug_feats.numMoA > 1)\n#  by cp_dose -- no obvious pattern, just intermixed.\n##colors = 1.0*np.array(df_aug_feats.cp_dose == 'D1')\n#  by cp_time -- clear patterns in plot\n##colors = 1.0*np.array(df_aug_feats.cp_time)\n#\ndf_aug_feats.plot(x='g-hif',y='g-95%',kind='scatter', figsize=(12,7),\n                 c=colors, colormap='jet', alpha=0.25, marker='o',s=50,\n                 title='g-95% vs g-hif for all non-control sig_ids'+\n                  '   Colored by number of MoAs (0, 1, 2+)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:12:58.476208Z","iopub.status.busy":"2020-10-06T17:12:58.475474Z","iopub.status.idle":"2020-10-06T17:12:59.997724Z","shell.execute_reply":"2020-10-06T17:12:59.998355Z"},"papermill":{"duration":1.720465,"end_time":"2020-10-06T17:12:59.998512","exception":false,"start_time":"2020-10-06T17:12:58.278047","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Look at g-5% vs g-lof\n#\n# Choose how to color the points:\n#  by numMoA -- looks useful to help determine the MoA\ncolors = 1.0*np.array(df_aug_feats.numMoA > 0) + 1.0*np.array(df_aug_feats.numMoA > 1)\n#  by cp_dose -- no obvious pattern, just intermixed.\n##colors = 1.0*np.array(df_aug_feats.cp_dose == 'D1')\n#  by cp_time -- clear patterns in plot\n##colors = 1.0*np.array(df_aug_feats.cp_time)\n#\ndf_aug_feats.plot(x='g-lof',y='g-5%',kind='scatter', figsize=(12,7),\n                 c=colors, colormap='jet', alpha=0.25, marker='o',s=50,\n                 title='g-5% vs g-lof for all non-control sig_ids'+\n                  '   Colored by number of MoAs (0, 1, 2+)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:13:00.395898Z","iopub.status.busy":"2020-10-06T17:13:00.395146Z","iopub.status.idle":"2020-10-06T17:13:03.223494Z","shell.execute_reply":"2020-10-06T17:13:03.224211Z"},"papermill":{"duration":3.034007,"end_time":"2020-10-06T17:13:03.224379","exception":false,"start_time":"2020-10-06T17:13:00.190372","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Look at g-hif vs C-ave\n#\n# Choose how to color the points:\n#  by numMoA -- looks useful to help determine the MoA\ncolors = 1.0*np.array(df_aug_feats.numMoA > 0) + 1.0*np.array(df_aug_feats.numMoA > 1)\n#  by cp_dose -- no obvious pattern, just intermixed.\n##colors = 1.0*np.array(df_aug_feats.cp_dose == 'D1')\n#  by cp_time -- clear patterns in plot\n##colors = 1.0*np.array(df_aug_feats.cp_time)\n#\ndf_aug_feats.plot(x='c-ave',y='g-hif',kind='scatter', figsize=(12,7),\n                 c=colors, colormap='jet', alpha=0.25, marker='o',s=50,\n                 title='g-hif vs C-ave for all non-control sig_ids'+\n                  '   Colored by number of MoAs (0, 1, 2+)')\nplt.show()\n\n\n\n#  by cp_time -- clear patterns in plot\ncolors = 1.0*np.array(df_aug_feats.cp_time)\n#\ndf_aug_feats.plot(x='c-ave',y='g-hif',kind='scatter', figsize=(12,7),\n                 c=colors, colormap='jet', alpha=0.25, marker='o',s=50,\n                 title='g-hif vs C-ave for all non-control sig_ids'+\n                  '   Colored by cp_time (24, 48, 72)')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:13:03.68759Z","iopub.status.busy":"2020-10-06T17:13:03.682178Z","iopub.status.idle":"2020-10-06T17:13:06.529401Z","shell.execute_reply":"2020-10-06T17:13:06.529971Z"},"papermill":{"duration":3.103224,"end_time":"2020-10-06T17:13:06.530152","exception":false,"start_time":"2020-10-06T17:13:03.426928","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Look at g-95% vs C-ave\n#\n# Choose how to color the points:\n#  by numMoA -- looks useful to help determine the MoA\ncolors = 1.0*np.array(df_aug_feats.numMoA > 0) + 1.0*np.array(df_aug_feats.numMoA > 1)\n#  by cp_dose -- no obvious pattern, just intermixed.\n##colors = 1.0*np.array(df_aug_feats.cp_dose == 'D1')\n#  by cp_time -- clear patterns in plot\n##colors = 1.0*np.array(df_aug_feats.cp_time)\n#\ndf_aug_feats.plot(x='c-ave',y='g-95%',kind='scatter', figsize=(12,7),\n                 c=colors, colormap='jet', alpha=0.25, marker='o',s=50,\n                 title='g-95% vs C-ave for all non-control sig_ids'+\n                  '   Colored by number of MoAs (0, 1, 2+)')\nplt.show()\n\n\ncolors = 1.0*np.array(df_aug_feats.cp_time)\n#\ndf_aug_feats.plot(x='c-ave',y='g-95%',kind='scatter', figsize=(12,7),\n                 c=colors, colormap='jet', alpha=0.25, marker='o',s=50,\n                 title='g-95% vs C-ave for all non-control sig_ids'+\n                  '   Colored by cp_time (24, 48, 72)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:13:06.967415Z","iopub.status.busy":"2020-10-06T17:13:06.966611Z","iopub.status.idle":"2020-10-06T17:13:09.552791Z","shell.execute_reply":"2020-10-06T17:13:09.553441Z"},"papermill":{"duration":2.811127,"end_time":"2020-10-06T17:13:09.553608","exception":false,"start_time":"2020-10-06T17:13:06.742481","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# One more strange 'feature': g-hilopc = g-95%/g-5%\n#                     and/or  g-hilof = g-hif/g-lof\n\n# Of the two, hilof may be 'better', information-wise,\n# though both are unconvincing in the scatter plots.\n# Leave them out of features.\n\n# Choose how to color the points:\n#  by numMoA -- looks useful to help determine the MoA\ncolors = 1.0*np.array(df_aug_feats.numMoA > 0) + 1.0*np.array(df_aug_feats.numMoA > 1)\n#  by cp_dose -- no obvious pattern, just intermixed.\n##colors = 1.0*np.array(df_aug_feats.cp_dose == 'D1')\n#  by cp_time -- clear patterns in plot\n##colors = 1.0*np.array(df_aug_feats.cp_time)\n#\ndf_aug_feats.plot(x='c-ave',y='g-hilof',kind='scatter', figsize=(12,7),\n                 c=colors, colormap='jet', alpha=0.25, marker='o',s=20,\n                 title='g-hilof vs C-ave for all non-control sig_ids'+\n                  '   Colored by number of MoAs (0, 1, 2+)', ylim=(0,2))\n\n\n#  by cp_time -- clear patterns in plot\ncolors = 1.0*np.array(df_aug_feats.cp_time)\n#\ndf_aug_feats.plot(x='c-ave',y='g-hilof',kind='scatter', figsize=(12,7),\n                 c=colors, colormap='jet', alpha=0.25, marker='o',s=20,\n                 title='g-hilof vs C-ave for all non-control sig_ids'+\n                  '   Colored by cp_time (24, 48, 72)', ylim=(0,2))\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.22083,"end_time":"2020-10-06T17:13:10.004559","exception":false,"start_time":"2020-10-06T17:13:09.783729","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### The g features"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:13:10.463396Z","iopub.status.busy":"2020-10-06T17:13:10.457337Z","iopub.status.idle":"2020-10-06T17:13:10.652464Z","shell.execute_reply":"2020-10-06T17:13:10.65314Z"},"papermill":{"duration":0.426858,"end_time":"2020-10-06T17:13:10.653308","exception":false,"start_time":"2020-10-06T17:13:10.22645","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# The features in df_treat_feats.columns are:\n#iloc\n#   0  sig_id  - anonymized values (I believe)\n#   1  cp_type - determines controls\n#   2  cp_time - 3 values: 24, 48, 72\n#   3  cp_dose - 2 values D1, D2\n#\n#   4  g-0  to -  ~ Gaussian-with-outliers, 772 of them in all\n# 775  g-771\n#\n# 776  c-0  to - ?, 100 of them in all\n# 875  c-99\n\n# Look at the values using iloc[ irow, icol ]\n\n# Pick a single feature (COLUMN), and plot the feature values for a bunch of the ids.\nicol = 4+600\ncol_str = df_treat_feats.columns[icol]\n# Show some controls and treatment ones\nplt.plot(df_treat_feats.iloc[1000:1500, icol].values,'r.')\nplt.plot(df_train_ctls.iloc[0:500, icol].values,'y.',alpha=0.5)\n\nplt.title(col_str+\" : Treated (red) and  Control (yellow)\")\nplt.ylabel(\"Value of the feature\")\nplt.xlabel(\"A bunch of sig_ids\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:13:11.122826Z","iopub.status.busy":"2020-10-06T17:13:11.121881Z","iopub.status.idle":"2020-10-06T17:13:11.335738Z","shell.execute_reply":"2020-10-06T17:13:11.335167Z"},"papermill":{"duration":0.453883,"end_time":"2020-10-06T17:13:11.335862","exception":false,"start_time":"2020-10-06T17:13:10.881979","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"cell_type":"code","source":"# The features in df_treat_feats.columns are:\n#iloc\n#   0  sig_id  - anonymized values (I believe)\n#   1  cp_type - determines controls\n#   2  cp_time - 3 values: 24, 48, 72\n#   3  cp_dose - 2 values D1, D2\n#\n#   4  g-0  to -  ~ Gaussian-with-outliers, 772 of them in all\n# 775  g-771\n#\n# 776  c-0  to - ?, 100 of them in all\n# 875  c-99\n\n# Look at the values using iloc[ irow, icol ]\n\n# Single sig_id (ROW), show a bunch of the target values\n\n# Looking at the g values for a single id\nirow = 201\n\nicol = 4  # 4 is start of the g features\ncol_str = df_treat_feats.columns[icol]\nicols = list(range(icol, icol+772))\n# Show some controls and treatment ones\nplt.plot(df_treat_feats.iloc[irow, icols].values,'r.')\nplt.plot(df_train_ctls.iloc[irow, icols].values,'y.',alpha=0.5)\n\nplt.title(df_treat_feats.iloc[irow,0]+\" : Treated (red) and  Control (yellow)\")\nplt.ylabel(\"Value of the feature\")\nplt.xlabel(col_str+\" and following target values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:13:11.80175Z","iopub.status.busy":"2020-10-06T17:13:11.800593Z","iopub.status.idle":"2020-10-06T17:13:11.812992Z","shell.execute_reply":"2020-10-06T17:13:11.811959Z"},"papermill":{"duration":0.253281,"end_time":"2020-10-06T17:13:11.813208","exception":false,"start_time":"2020-10-06T17:13:11.559927","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# The g-0 to g-771 column names are:\ngcol_strs = df_train_feats.columns[4:4+772]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:13:12.355707Z","iopub.status.busy":"2020-10-06T17:13:12.338811Z","iopub.status.idle":"2020-10-06T17:13:14.307599Z","shell.execute_reply":"2020-10-06T17:13:14.306942Z"},"papermill":{"duration":2.220202,"end_time":"2020-10-06T17:13:14.307723","exception":false,"start_time":"2020-10-06T17:13:12.087521","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Form and look at the average g-vectors for the CONTROLS\nsel_dose = 'All'    # No selection on dose\nsel_time = 244872   # 24, 48, 72, or 244872 for all\ntarg_str = 'Controls'\nif sel_time > 100:\n    cp_select = ((df_train_feats.cp_type == 'ctl_vehicle') &\n             (df_train_feats.cp_time < sel_time))\nelse:\n        cp_select = ((df_train_feats.cp_type == 'ctl_vehicle') &\n             (df_train_feats.cp_time == sel_time))\n        \n# Get the statistics on the g columns\ndf_g_stats = df_train_feats.loc[cp_select,gcol_strs].describe()\n# transpose it so that the gs are in rows and the stats in columns\ndf_g_stats = df_g_stats.T\n# add a z-score column\nsqrt_count = np.sqrt(df_g_stats.loc['g-0','count'])\ndf_g_stats['z-score'] = df_g_stats['mean'] / (df_g_stats['std'] / sqrt_count)\n\nprint(\"\\nThere are\",df_g_stats.loc['g-0','count'],\" ids with dose=\"+sel_dose+\n          \", time=\"+str(sel_time)+\", and MoA is\",targ_str)\n\n\n# Save this g-vector of mean control values,\n# it will be subtracted from target g-vectors to reduce the control pattern.\ncontrol_means = df_g_stats['mean']","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:13:14.814685Z","iopub.status.busy":"2020-10-06T17:13:14.811105Z","iopub.status.idle":"2020-10-06T17:13:15.195678Z","shell.execute_reply":"2020-10-06T17:13:15.194952Z"},"papermill":{"duration":0.633484,"end_time":"2020-10-06T17:13:15.195825","exception":false,"start_time":"2020-10-06T17:13:14.562341","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Show the mean control vector values and their stds \ndf_g_stats[['mean']].plot(kind='line',style='.b')\nplt.title(\"g-vector values: {:.0f} ids,\".format(df_g_stats.loc['g-0','count']) +\n          \" dose=\"+sel_dose+\", time=\"+str(sel_time)+\", MoA=\"+targ_str)\nplt.show()\n\ndf_g_stats[['std']].plot(kind='line',style='.b')\nplt.title(\"stds of the g-vector\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:13:15.684371Z","iopub.status.busy":"2020-10-06T17:13:15.683573Z","iopub.status.idle":"2020-10-06T17:13:17.539511Z","shell.execute_reply":"2020-10-06T17:13:17.540327Z"},"papermill":{"duration":2.108092,"end_time":"2020-10-06T17:13:17.54053","exception":false,"start_time":"2020-10-06T17:13:15.432438","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Look at a selected MoA=1 target:\nif True:\n    # Pick the target:\n    ##targ_str = 'serotonin_receptor_agonist'\n    ##targ_str = 'serotonin_receptor_antagonist'\n    ##targ_str = 'calcium_channel_blocker'\n    ##targ_str = 'vegfr_inhibitor'\n    ##targ_str = 'cdk_inhibitor'\n    targ_str = 'nfkb_inhibitor'\n    \n    # dose and time to use:\n    sel_dose = 'All'  # No selection on dose\n    sel_time = 244872\n\n    if sel_time > 100:\n        cp_select = (df_train_targs[targ_str] > 0) & (df_train_feats.cp_time < sel_time)\n    else:\n        cp_select = (df_train_targs[targ_str] > 0) & (df_train_feats.cp_time == sel_time)\nelse:\n    # For comparison can also look at (some of) the controls\n    sel_dose = 'All'  # No selection on dose\n    sel_time = 48\n    targ_str = 'Controls'\n    cp_select = ((df_train_feats.cp_type == 'ctl_vehicle') &\n             (df_train_feats.cp_time == sel_time))\n\n\ndf_g_stats = df_train_feats.loc[cp_select,gcol_strs].describe()\n# transpose it so that the gs are in rows and the stats in columns\ndf_g_stats = df_g_stats.T\n# add a z-score column\nsqrt_count = np.sqrt(df_g_stats.loc['g-0','count'])\n# Subtract off the control means\ndf_g_stats['mean-ctl'] = df_g_stats['mean'] - control_means\ndf_g_stats['z-score'] = (df_g_stats['mean-ctl'] / (df_g_stats['std'] / sqrt_count))\n# and an abs(z-score):\ndf_g_stats['z-abs'] = df_g_stats['z-score'].abs()\n\nprint(\"\\nThere are\",df_g_stats.loc['g-0','count'],\" ids with dose=\"+sel_dose+\n          \", time=\"+str(sel_time)+\", and MoA is\",targ_str)\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:13:18.015109Z","iopub.status.busy":"2020-10-06T17:13:18.011734Z","iopub.status.idle":"2020-10-06T17:13:18.426026Z","shell.execute_reply":"2020-10-06T17:13:18.425289Z"},"papermill":{"duration":0.657358,"end_time":"2020-10-06T17:13:18.426158","exception":false,"start_time":"2020-10-06T17:13:17.7688","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"##df_g_stats[['mean']].plot(kind='line',style='.b')\n##plt.show()\n\ndf_g_stats[['mean-ctl']].plot(kind='line',style='.g')\nplt.title(\"g-vector values: {:.0f} ids,\".format(df_g_stats.loc['g-0','count']) +\n          \" dose=\"+sel_dose+\", time=\"+str(sel_time)+\", MoA=\"+targ_str)\nplt.show()\n\ndf_g_stats[['std']].plot(kind='line',style='.b')\nplt.title(\"stds of the g-vector\")\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.229569,"end_time":"2020-10-06T17:13:18.894514","exception":false,"start_time":"2020-10-06T17:13:18.664945","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## <a id=\"gVectors\">g-Vectors for each Target MoA</a>\nBack to <a href=\"#Index\">Index</a>\n\n\nThe 772 g values for a sample are its g-vector.  Here we calculate the average g-vector for all the ids that have a given MoA=1. "},{"metadata":{"papermill":{"duration":0.237474,"end_time":"2020-10-06T17:13:19.362142","exception":false,"start_time":"2020-10-06T17:13:19.124668","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Assemble a dataframe of the 206 average g-vectors"},{"metadata":{"papermill":{"duration":0.234537,"end_time":"2020-10-06T17:13:19.836212","exception":false,"start_time":"2020-10-06T17:13:19.601675","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:13:20.334809Z","iopub.status.busy":"2020-10-06T17:13:20.329539Z","iopub.status.idle":"2020-10-06T17:19:44.848118Z","shell.execute_reply":"2020-10-06T17:19:44.847321Z"},"papermill":{"duration":384.777952,"end_time":"2020-10-06T17:19:44.848252","exception":false,"start_time":"2020-10-06T17:13:20.0703","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"if LOCATION_KAGGLE:\n    # Define the columns of the g-vectors dataframe\n    all_cols = ['targ_str','count','median_std']\n    # add on the g columns\n    all_cols = all_cols + list(gcol_strs)\n\n    # Setup a dataframe\n    df_g_vectors = pd.DataFrame(columns=all_cols)\n\n    print(df_g_vectors)\n\n\n    # Go through the MoA targets\n\n    all_targ_strs = list(df_treat_targs.columns[1:])\n\n    # dose and time to use:\n    sel_dose = 'All'  # No selection on dose\n    sel_time = 244872\n    \n    for targ_str in all_targ_strs:\n\n        if sel_time > 100:\n            cp_select = (df_train_targs[targ_str] > 0) & (df_train_feats.cp_time < sel_time)\n        else:\n            cp_select = (df_train_targs[targ_str] > 0) & (df_train_feats.cp_time == sel_time)\n\n        df_g_stats = df_train_feats.loc[cp_select,gcol_strs].describe()\n        # transpose it so that the gs are in rows and the stats in columns\n        df_g_stats = df_g_stats.T\n        # add a z-score column\n        sqrt_count = np.sqrt(df_g_stats.loc['g-0','count'])\n        # Subtract off the control means\n        df_g_stats['mean-ctl'] = df_g_stats['mean'] - control_means\n\n        ##print(\"Averaging\",df_g_stats.loc['g-0','count'],\" ids with MoA = \",targ_str)\n\n        # Add this g-vector to the dataframe\n        row_values = [targ_str, df_g_stats.loc['g-0','count'], df_g_stats['std'].median()]\n        for gval in list(df_g_stats['mean-ctl']):\n            row_values.append(gval)\n        df_g_vectors = df_g_vectors.append(pd.DataFrame([row_values],\n                            columns=df_g_vectors.columns),ignore_index=True)\n\n    # save the dataframe\n    df_g_vectors.to_csv(\"g_vectors_\"+sel_dose+str(sel_time)+\"_\"+\n                    version_str+\".csv\",index=False,float_format='%.3f')\n    \nelse:\n    # Read in a previously saved file:\n    df_g_vectors = pd.read_csv(\"./g_vectors_All244872_v10_SAVE.csv\")","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:19:45.318548Z","iopub.status.busy":"2020-10-06T17:19:45.316452Z","iopub.status.idle":"2020-10-06T17:19:45.35636Z","shell.execute_reply":"2020-10-06T17:19:45.355627Z"},"papermill":{"duration":0.277509,"end_time":"2020-10-06T17:19:45.356482","exception":false,"start_time":"2020-10-06T17:19:45.078973","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"cell_type":"code","source":"# Each row has the average g-values for one of the MoAs.\ndf_g_vectors","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:19:45.835711Z","iopub.status.busy":"2020-10-06T17:19:45.834935Z","iopub.status.idle":"2020-10-06T17:19:46.019755Z","shell.execute_reply":"2020-10-06T17:19:46.018999Z"},"papermill":{"duration":0.430306,"end_time":"2020-10-06T17:19:46.019877","exception":false,"start_time":"2020-10-06T17:19:45.589571","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Show that it has the same values for the example shown before:\ntarg_str = 'nfkb_inhibitor'\n# select the desired row\nirow = sum(df_g_vectors.index * (df_g_vectors.targ_str == targ_str))\ndf_g_vectors.iloc[irow,3:3+772].plot(kind='line',style='.g')\nplt.title(\"g-vector for MoA = \"+targ_str)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.233175,"end_time":"2020-10-06T17:19:46.486723","exception":false,"start_time":"2020-10-06T17:19:46.253548","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Which MoAs (targets) have the most detectable g-vectors?"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:19:46.965401Z","iopub.status.busy":"2020-10-06T17:19:46.962597Z","iopub.status.idle":"2020-10-06T17:19:47.48645Z","shell.execute_reply":"2020-10-06T17:19:47.485679Z"},"papermill":{"duration":0.767706,"end_time":"2020-10-06T17:19:47.486573","exception":false,"start_time":"2020-10-06T17:19:46.718867","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Get statistics for each target MoA row \ndf_target_stats = df_g_vectors.drop(columns=\n        ['targ_str','count','median_std']).T.describe(percentiles=[0.05,0.5,0.95]).T\ndf_target_stats","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:19:47.971683Z","iopub.status.busy":"2020-10-06T17:19:47.96526Z","iopub.status.idle":"2020-10-06T17:19:48.137591Z","shell.execute_reply":"2020-10-06T17:19:48.136846Z"},"papermill":{"duration":0.41655,"end_time":"2020-10-06T17:19:48.13771","exception":false,"start_time":"2020-10-06T17:19:47.72116","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Show the ~ max, min g-values for the MoA targets\ndf_target_stats.plot('95%','5%',kind='scatter')\nplt.show()\n\n# This linear shape comes from having similar magnitudes\n# for the most positive and for the most negative of the g-values.","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:19:48.62413Z","iopub.status.busy":"2020-10-06T17:19:48.617142Z","iopub.status.idle":"2020-10-06T17:19:48.774982Z","shell.execute_reply":"2020-10-06T17:19:48.774242Z"},"papermill":{"duration":0.400964,"end_time":"2020-10-06T17:19:48.77513","exception":false,"start_time":"2020-10-06T17:19:48.374166","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# The max - min :\ntarg_max_min_sort = (df_target_stats['95%'] - \n                   df_target_stats['5%']).sort_values(ascending=False)\nplt.plot(targ_max_min_sort.values,'.b')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:19:49.25972Z","iopub.status.busy":"2020-10-06T17:19:49.258783Z","iopub.status.idle":"2020-10-06T17:19:49.263496Z","shell.execute_reply":"2020-10-06T17:19:49.262737Z"},"papermill":{"duration":0.251133,"end_time":"2020-10-06T17:19:49.263634","exception":false,"start_time":"2020-10-06T17:19:49.012501","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Select the ones that are above 2.4(24) or 3.0(13)\n# as the ones that are most 'predictable':\nn_predict = 24\ntargs_to_predict = targ_max_min_sort[0:n_predict]\ntargs_to_predict = list(targs_to_predict.index)\n\n# show them\ntarg_max_min_sort[0:n_predict]\n##targs_to_predict","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:19:49.760231Z","iopub.status.busy":"2020-10-06T17:19:49.759207Z","iopub.status.idle":"2020-10-06T17:19:49.763948Z","shell.execute_reply":"2020-10-06T17:19:49.764545Z"},"papermill":{"duration":0.26047,"end_time":"2020-10-06T17:19:49.764724","exception":false,"start_time":"2020-10-06T17:19:49.504254","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Show their names with the number of sig_ids they label\ndf_targ_to_predict = df_g_vectors.loc[targs_to_predict,['count','targ_str']]\n\n# Show the ones with 70 or more counts:\nlist(df_targ_to_predict[df_targ_to_predict['count'] > 69].targ_str)\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:19:50.318429Z","iopub.status.busy":"2020-10-06T17:19:50.317476Z","iopub.status.idle":"2020-10-06T17:19:50.321054Z","shell.execute_reply":"2020-10-06T17:19:50.321855Z"},"papermill":{"duration":0.289511,"end_time":"2020-10-06T17:19:50.322094","exception":false,"start_time":"2020-10-06T17:19:50.032583","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Use results above for \"guessing\" estimate of score, and repeat the calculation, but\n# subtract off the errors for the targs_to_predict ones, assuming we 'know' them:\nprint(\"If we can correctly predict all {} targets above,\\n\".format(len(targs_to_predict)),\n      \"then the expected score (incl controls) is:\\n\",\n      ((len(df_treat_targs)/len(df_train_targs)) * \n         (sum(scores_targs) - sum(scores_targs[targs_to_predict])) / len(aves_targs)))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.235892,"end_time":"2020-10-06T17:19:50.795237","exception":false,"start_time":"2020-10-06T17:19:50.559345","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Which gs (features) have the most variation across the Target MoAs?"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:19:51.284997Z","iopub.status.busy":"2020-10-06T17:19:51.282726Z","iopub.status.idle":"2020-10-06T17:19:53.116469Z","shell.execute_reply":"2020-10-06T17:19:53.117033Z"},"papermill":{"duration":2.084601,"end_time":"2020-10-06T17:19:53.117198","exception":false,"start_time":"2020-10-06T17:19:51.032597","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Get statistics for each single g component (column) over the 206 MoAs\ndf_vector_stats = df_g_vectors.describe(percentiles=\n                    [0.05,0.5,0.95]).drop(columns=['count','median_std']).T\n# KLUDGE: Same stats but over all samples (instead of one per MoA)\n##df_vector_stats = df_aug_feats[gcol_strs].describe(percentiles=\n##                    [0.05,0.5,0.95]).T\n\ndf_vector_stats","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:19:53.660772Z","iopub.status.busy":"2020-10-06T17:19:53.659891Z","iopub.status.idle":"2020-10-06T17:19:54.256179Z","shell.execute_reply":"2020-10-06T17:19:54.255522Z"},"papermill":{"duration":0.851559,"end_time":"2020-10-06T17:19:54.256305","exception":false,"start_time":"2020-10-06T17:19:53.404746","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"df_vector_stats.hist('95%',bins=50)\nplt.show()\n\ndf_vector_stats.hist('5%',bins=50)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:19:54.759516Z","iopub.status.busy":"2020-10-06T17:19:54.758699Z","iopub.status.idle":"2020-10-06T17:19:54.9279Z","shell.execute_reply":"2020-10-06T17:19:54.927198Z"},"papermill":{"duration":0.429875,"end_time":"2020-10-06T17:19:54.928041","exception":false,"start_time":"2020-10-06T17:19:54.498166","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"df_vector_stats.plot('95%','5%',kind='scatter')\nplt.show()\n\n# This plot shows that an individual g features tends to\n# either go positive or go negative, from the control range around (0.5, -0.5).\n# There are 3 that seem to be more symmetric, two near (-1.7,1) and one (1.5,-1.2).","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:19:55.430526Z","iopub.status.busy":"2020-10-06T17:19:55.423809Z","iopub.status.idle":"2020-10-06T17:19:55.691362Z","shell.execute_reply":"2020-10-06T17:19:55.69063Z"},"papermill":{"duration":0.519591,"end_time":"2020-10-06T17:19:55.691483","exception":false,"start_time":"2020-10-06T17:19:55.171892","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"gs_max_min_sort = (df_vector_stats['95%'] - \n                   df_vector_stats['5%']).sort_values(ascending=False)\ngs_max_min_sort.plot(style='.')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:19:56.185728Z","iopub.status.busy":"2020-10-06T17:19:56.184716Z","iopub.status.idle":"2020-10-06T17:19:56.189577Z","shell.execute_reply":"2020-10-06T17:19:56.188778Z"},"papermill":{"duration":0.256542,"end_time":"2020-10-06T17:19:56.189708","exception":false,"start_time":"2020-10-06T17:19:55.933166","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Most 'sensitive' are the ones above 2.4 :\n# Put their names in a list:\ngs_to_use_22 = gs_max_min_sort[0:22]\ngs_to_use_22 = list(gs_to_use_22.index)\n\n# show them\ngs_max_min_sort[0:22]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:19:57.225832Z","iopub.status.busy":"2020-10-06T17:19:57.224619Z","iopub.status.idle":"2020-10-06T17:19:57.229829Z","shell.execute_reply":"2020-10-06T17:19:57.230386Z"},"papermill":{"duration":0.287729,"end_time":"2020-10-06T17:19:57.230555","exception":false,"start_time":"2020-10-06T17:19:56.942826","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Dataframe of the \"gs to use\" in columns and targets (206) as rows.\ndf_g_vectors[gs_to_use_22]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:19:57.732119Z","iopub.status.busy":"2020-10-06T17:19:57.731142Z","iopub.status.idle":"2020-10-06T17:19:57.735031Z","shell.execute_reply":"2020-10-06T17:19:57.734298Z"},"papermill":{"duration":0.259103,"end_time":"2020-10-06T17:19:57.735172","exception":false,"start_time":"2020-10-06T17:19:57.476069","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Get the correlations between these selected gs\n# use abs to better show highly correlated from little correlation\ngs_corr = np.abs(df_g_vectors[gs_to_use_22].corr())","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:19:58.243512Z","iopub.status.busy":"2020-10-06T17:19:58.2421Z","iopub.status.idle":"2020-10-06T17:19:58.661988Z","shell.execute_reply":"2020-10-06T17:19:58.661317Z"},"papermill":{"duration":0.682321,"end_time":"2020-10-06T17:19:58.662136","exception":false,"start_time":"2020-10-06T17:19:57.979815","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# plot the heatmap\nsns.heatmap(gs_corr,\n        xticklabels=gs_corr.columns,\n        yticklabels=gs_corr.columns)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:19:59.165504Z","iopub.status.busy":"2020-10-06T17:19:59.164307Z","iopub.status.idle":"2020-10-06T17:19:59.451799Z","shell.execute_reply":"2020-10-06T17:19:59.450971Z"},"papermill":{"duration":0.543663,"end_time":"2020-10-06T17:19:59.451928","exception":false,"start_time":"2020-10-06T17:19:58.908265","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Manually pick an uncorrelated subset:\n#   g-231, g-175, g-178 are very uncorrelated with others !?\n#   g-75 and g-65 standout, and g-332 too, though less so.\n#   The first 3: g-392, g-100, g-158 and g-91\n#     also have low-ish correlation with most others.\n#   Finally, g-50 has high correlation with most of the rest.\n\nif True:\n    # So use this subset:\n    gs_to_use = ['g-392', 'g-100', 'g-158', 'g-91',\n             'g-231', 'g-175', 'g-178',\n             'g-75', 'g-65', 'g-332',  'g-50']\n\n    gs_corr = np.abs(df_g_vectors[gs_to_use].corr())\n    sns.heatmap(gs_corr,\n        xticklabels=gs_corr.columns,\n        yticklabels=gs_corr.columns)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:20:00.025603Z","iopub.status.busy":"2020-10-06T17:20:00.02262Z","iopub.status.idle":"2020-10-06T17:20:03.600623Z","shell.execute_reply":"2020-10-06T17:20:03.599932Z"},"papermill":{"duration":3.867293,"end_time":"2020-10-06T17:20:03.600742","exception":false,"start_time":"2020-10-06T17:19:59.733449","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Show the histograms of each of the selected g feature over the 206 taargets\n# (Want to use a log scale for the y-axis.)\ndf_g_vectors[gs_to_use].hist(figsize=(10,8),sharex=True,sharey=True,layout=(3,4),bins=20)\nplt.show()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:20:04.10835Z","iopub.status.busy":"2020-10-06T17:20:04.106842Z","iopub.status.idle":"2020-10-06T17:20:04.536501Z","shell.execute_reply":"2020-10-06T17:20:04.535901Z"},"papermill":{"duration":0.68739,"end_time":"2020-10-06T17:20:04.536636","exception":false,"start_time":"2020-10-06T17:20:03.849246","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"cell_type":"code","source":"if True:\n    # Show the heatmap for all the g- features that will be used:\n    gs_corr = np.abs(df_aug_feats[['g-hif','g-95%'] + gs_to_use].corr())\n    sns.heatmap(gs_corr,\n        xticklabels=gs_corr.columns,\n        yticklabels=gs_corr.columns)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:20:05.049036Z","iopub.status.busy":"2020-10-06T17:20:05.047496Z","iopub.status.idle":"2020-10-06T17:20:06.295659Z","shell.execute_reply":"2020-10-06T17:20:06.296232Z"},"papermill":{"duration":1.509695,"end_time":"2020-10-06T17:20:06.296386","exception":false,"start_time":"2020-10-06T17:20:04.786691","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"if True:\n    # Scatter plot of a specific g feature vs g-hif  over all sig_ids\n\n    colors = 1.0*np.array(df_aug_feats.numMoA > 0) + 1.0*np.array(df_aug_feats.numMoA > 1)\n    #  by cp_time -- clear patterns in plot\n    ##colors = 1.0*np.array(df_aug_feats.cp_time)\n\n    df_aug_feats.plot('g-hif','g-175',kind='scatter',c=colors,figsize=(9,6),\n                 colormap='jet', alpha=0.25, marker='o',s=20)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Which g-vectors have variation over the 23 targets we'd like to detect\n\nThe \"tSNE-9\" set of targets are very detectible with the gs_to_use_22 but\ntwo other sets of targets are not well detected with just those, the sets are:\ni) the targets with a > 0.01, and ii) expected to be highly detectable (with > 69 counts.)\nLook for any g-vectors that are sensitive to these..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 23 Targets we want to detect better:\n\n# All above 0.010 except for the tSNE-9:     * Not very detectable *\ntarg_do_better = ['acetylcholine_receptor_antagonist', 'adrenergic_receptor_agonist',\n               'adrenergic_receptor_antagonist', 'calcium_channel_blocker', 'cyclooxygenase_inhibitor',\n               'dna_inhibitor', 'dopamine_receptor_antagonist', 'flt3_inhibitor',\n               'glutamate_receptor_antagonist', 'histamine_receptor_antagonist', 'kit_inhibitor',\n               'pdgfr_inhibitor', 'phosphodiesterase_inhibitor', 'serotonin_receptor_agonist',\n               'serotonin_receptor_antagonist', 'sodium_channel_inhibitor']\n##targ_subset_name = \"16: >0.010, w/o tSNE-9\"\n\n# These are the targets with 'highly detectable' g-vectors (unless listed in tSNE-9 already)\ntarg_do_better = (targ_do_better + \n        ['topoisomerase_inhibitor', 'hdac_inhibitor', 'mtor_inhibitor', 'mek_inhibitor',\n        'pi3k_inhibitor', 'protein_synthesis_inhibitor', 'atpase_inhibitor'])\n##targ_subset_name = \"7: detectable\"\n\nlen(targ_do_better)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get statistics for each single g component (column) over the 23 DO BETTER MoAs\ndf_dobetter_vects = df_g_vectors.set_index('targ_str').copy()\ndf_dobetter_vects = df_dobetter_vects.loc[targ_do_better]\ndf_dobetter_stats = df_dobetter_vects.describe(percentiles=\n                    [0.15,0.5,0.85]).drop(columns=['count','median_std']).T\n\ndf_dobetter_stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dobetter_stats.hist('85%',bins=50)\nplt.show()\n\ndf_dobetter_stats.hist('15%',bins=50)\nplt.show()\n\ndf_dobetter_stats.hist('std',bins=50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dobetter_stats.plot('85%','15%',kind='scatter')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dbgs_max_min = (df_dobetter_stats['85%'] - df_dobetter_stats['15%'])\ndf_dobetter_stats['85%-15%'] = dbgs_max_min\n\ndf_dobetter_stats.plot('std','85%-15%',kind='scatter')\nplt.show()\n\ndbgs_max_min_sort = dbgs_max_min.sort_values(ascending=False)\ndbgs_max_min_sort.plot(style='.')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most 'sensitive' are the ones above 1.75:\n# Put their names in a list:\ndbgs_num = 28\ndbgs_to_use = dbgs_max_min_sort[0:dbgs_num]\ndbgs_to_use_28 = list(dbgs_to_use.index)\n\n# show them\ndbgs_max_min_sort[0:dbgs_num]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop any gs that are already in the gs_to_use_22 list:\ndbgs_to_add = dbgs_to_use_28.copy()\nfor this_g in gs_to_use_22:\n    try:\n        dbgs_to_add.remove(this_g)\n    except:\n        pass\n\nprint(\"\\nFound\",len(dbgs_to_add),\"g-vectors to add to the features:\\n\")\nprint(dbgs_to_add)\n\n# Using 15%, 85%   shares: 146, 201, 228, 72\n# ['g-146', 'g-201', 'g-406', 'g-208', 'g-215', 'g-386', 'g-228', 'g-529', 'g-298', 'g-72']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look at the correlations between these\nif True:\n    dbgs_corr = np.abs(df_g_vectors[dbgs_to_add].corr())\n    sns.heatmap(dbgs_corr,\n        xticklabels=dbgs_corr.columns,\n        yticklabels=dbgs_corr.columns)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.253423,"end_time":"2020-10-06T17:20:06.810895","exception":false,"start_time":"2020-10-06T17:20:06.557472","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Which c- features have the most variation across the treatment data?"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:20:07.325705Z","iopub.status.busy":"2020-10-06T17:20:07.324685Z","iopub.status.idle":"2020-10-06T17:20:07.328481Z","shell.execute_reply":"2020-10-06T17:20:07.327754Z"},"papermill":{"duration":0.264365,"end_time":"2020-10-06T17:20:07.3286","exception":false,"start_time":"2020-10-06T17:20:07.064235","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Similar to the above for g features BUT did not make MoA average c vectors - use all the data.\n# C columns:\n# 776  c-0  to - ?, 100 of them in all\n# 875  c-99\nccol_strs = df_aug_feats.columns[776:875+1]\n##ccol_strs","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:20:07.853042Z","iopub.status.busy":"2020-10-06T17:20:07.852215Z","iopub.status.idle":"2020-10-06T17:20:08.171686Z","shell.execute_reply":"2020-10-06T17:20:08.170913Z"},"papermill":{"duration":0.589175,"end_time":"2020-10-06T17:20:08.17181","exception":false,"start_time":"2020-10-06T17:20:07.582635","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# KLUDGE: Same stats but over all samples (instead of one per MoA)\ndf_ccol_stats = df_aug_feats[ccol_strs].describe(percentiles=\n                    [0.05,0.5,0.95]).T\n\n##df_ccol_stats","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:20:08.695335Z","iopub.status.busy":"2020-10-06T17:20:08.689087Z","iopub.status.idle":"2020-10-06T17:20:08.872989Z","shell.execute_reply":"2020-10-06T17:20:08.872215Z"},"papermill":{"duration":0.446354,"end_time":"2020-10-06T17:20:08.873135","exception":false,"start_time":"2020-10-06T17:20:08.426781","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"df_ccol_stats.plot('95%','5%',kind='scatter')\nplt.show()\n\n# There is very little variation in the 95% values (survival)\n# it's the 5% (lowest) values that vary across the cs.","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:20:09.405607Z","iopub.status.busy":"2020-10-06T17:20:09.401641Z","iopub.status.idle":"2020-10-06T17:20:09.541854Z","shell.execute_reply":"2020-10-06T17:20:09.541129Z"},"papermill":{"duration":0.413814,"end_time":"2020-10-06T17:20:09.54198","exception":false,"start_time":"2020-10-06T17:20:09.128166","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"cs_max_min_sort = (df_ccol_stats['95%'] - \n                   df_ccol_stats['5%']).sort_values(ascending=False)\ncs_max_min_sort.plot(style='.')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:20:10.072472Z","iopub.status.busy":"2020-10-06T17:20:10.071508Z","iopub.status.idle":"2020-10-06T17:20:10.076076Z","shell.execute_reply":"2020-10-06T17:20:10.075292Z"},"papermill":{"duration":0.273649,"end_time":"2020-10-06T17:20:10.076208","exception":false,"start_time":"2020-10-06T17:20:09.802559","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Most 'sensitive' are the ones above 6.8 :\n# Put their names in a list:\ncs_to_use = cs_max_min_sort[0:18]\ncs_to_use = list(cs_to_use.index)\n\n# show them\ncs_max_min_sort[0:18]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:20:10.604296Z","iopub.status.busy":"2020-10-06T17:20:10.603227Z","iopub.status.idle":"2020-10-06T17:20:10.62387Z","shell.execute_reply":"2020-10-06T17:20:10.623117Z"},"papermill":{"duration":0.284199,"end_time":"2020-10-06T17:20:10.624022","exception":false,"start_time":"2020-10-06T17:20:10.339823","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Get the correlations between these selected cs\n# use abs to better show highly correlated from little correlation\ncs_corr = np.abs(df_aug_feats[cs_to_use].corr())","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:20:11.212938Z","iopub.status.busy":"2020-10-06T17:20:11.208941Z","iopub.status.idle":"2020-10-06T17:20:11.587346Z","shell.execute_reply":"2020-10-06T17:20:11.586548Z"},"papermill":{"duration":0.674735,"end_time":"2020-10-06T17:20:11.587476","exception":false,"start_time":"2020-10-06T17:20:10.912741","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# plot the heatmap\nsns.heatmap(cs_corr,\n        xticklabels=cs_corr.columns,\n        yticklabels=cs_corr.columns)\n\n# As noted the c- features are very correlated...","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:20:12.171802Z","iopub.status.busy":"2020-10-06T17:20:12.170319Z","iopub.status.idle":"2020-10-06T17:20:12.447764Z","shell.execute_reply":"2020-10-06T17:20:12.446951Z"},"papermill":{"duration":0.579805,"end_time":"2020-10-06T17:20:12.447895","exception":false,"start_time":"2020-10-06T17:20:11.86809","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Manually pick a few for a least-correlated subset:\nif True:\n    # So use this subset (including c-38 that seems correlated with most):\n    cs_to_use = ['c-38','c-65','c-70','c-48']\n\n    # Show the correlations and include the other c- features too:\n    cs_corr = np.abs(df_aug_feats[['c-ave','c-std','c-5%','c-95%'] + \n                                  cs_to_use].corr())\n    sns.heatmap(cs_corr,\n        xticklabels=cs_corr.columns,\n        yticklabels=cs_corr.columns)\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:20:12.977788Z","iopub.status.busy":"2020-10-06T17:20:12.976295Z","iopub.status.idle":"2020-10-06T17:20:14.184128Z","shell.execute_reply":"2020-10-06T17:20:14.184859Z"},"papermill":{"duration":1.474606,"end_time":"2020-10-06T17:20:14.185056","exception":false,"start_time":"2020-10-06T17:20:12.71045","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Scatter plot of a specific c feature vs c-ave\n\ncolors = 1.0*np.array(df_aug_feats.numMoA > 0) + 1.0*np.array(df_aug_feats.numMoA > 1)\n#  by cp_time -- clear patterns in plot\n##colors = 1.0*np.array(df_aug_feats.cp_time)\n\ndf_aug_feats.plot('c-ave','c-65',kind='scatter',c=colors,figsize=(9,6),\n                 colormap='jet', alpha=0.25, marker='o',s=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.262987,"end_time":"2020-10-06T17:20:14.715538","exception":false,"start_time":"2020-10-06T17:20:14.452551","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## <a id=\"tSNEfeatures\">t-SNE on Features shows some Target clusters</a>\nBack to <a href=\"#Index\">Index</a>\n\nThis was motivated by Nelson Ewert's [Discussion post](https://www.kaggle.com/c/lish-moa/discussion/186919) and the code\nin his [Notebook](https://www.kaggle.com/nelsonewert/using-t-sne-to-identify-clusters-in-the-data)"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:20:15.270862Z","iopub.status.busy":"2020-10-06T17:20:15.269811Z","iopub.status.idle":"2020-10-06T17:20:15.273143Z","shell.execute_reply":"2020-10-06T17:20:15.272428Z"},"papermill":{"duration":0.295032,"end_time":"2020-10-06T17:20:15.273262","exception":false,"start_time":"2020-10-06T17:20:14.97823","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# t-SNE results\n#\n# Doing t-SNE on the train features (below, using the 11 features in gs_to_use)\n# identified some clear \"islands\" of active MoA sig_ids.\n# Each island is dominated by a single target MoA.\n# This suggests that it should be possible to predict these particular MoAs accurately.\n# These are the island-targets I identified:  (the scores are from, e.g., scores_targs['hsp_inhibitor'])\n#                    cts = counts in a specific island region (not all for the target)\n#                    * = also in the most-detectable MoA list above.\n# cts  score         MoA target\n# 718  0.1453   *   proteasome_inhibitor  AND  <-- these two mostly appear together\n# 718  0.1612   *      nfkb_inhibitor          <--\n# 236  0.0655       glucocorticoid_receptor_agonist\n# 185  0.0567   *   raf_inhibitor\n# 145  0.0799   *   cdk_inhibitor\n# 142  0.0689       hmgcr_inhibitor  81+61\n#  92  0.0792       egfr_inhibitor\n#  66  0.0274   *   hsp_inhibitor\n#  49  0.0754   *   tubulin_inhibitor\n#\n# These were all put in a subset list, repeated here:\ntSNE_9subset = ['proteasome_inhibitor', 'nfkb_inhibitor',  'glucocorticoid_receptor_agonist',\n               'raf_inhibitor', 'cdk_inhibitor', 'hmgcr_inhibitor', \n               'egfr_inhibitor', 'hsp_inhibitor', 'tubulin_inhibitor']\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:20:15.807603Z","iopub.status.busy":"2020-10-06T17:20:15.806693Z","iopub.status.idle":"2020-10-06T17:20:15.810443Z","shell.execute_reply":"2020-10-06T17:20:15.811268Z"},"papermill":{"duration":0.275966,"end_time":"2020-10-06T17:20:15.811476","exception":false,"start_time":"2020-10-06T17:20:15.53551","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Use results above for \"guessing\" estimate of score, and repeat the calculation, but\n# subtract off the errors for the *** targ_subset *** ones, assuming we 'know' them:\nprint(\"If we can correctly predict all {} of the t-SNE targets above,\\n\".format(len(tSNE_9subset)),\n      \"then the expected score (incl controls) is:\\n\",\n      ((len(df_treat_targs)/len(df_train_targs)) * \n         (sum(scores_targs) - sum(scores_targs[tSNE_9subset])) / len(aves_targs)))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:20:16.375704Z","iopub.status.busy":"2020-10-06T17:20:16.374667Z","iopub.status.idle":"2020-10-06T17:20:16.37814Z","shell.execute_reply":"2020-10-06T17:20:16.377424Z"},"papermill":{"duration":0.28853,"end_time":"2020-10-06T17:20:16.37826","exception":false,"start_time":"2020-10-06T17:20:16.08973","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Calculating the rms variation of a target's t-SNE points around their average -\n# a rough idea of clustering?\n#\n# The targets with high counts (>200) and lowish rms (<58)\n#                             targ_str  count       0-ave      1-ave        rms\n# these two:\n# 136                   nfkb_inhibitor  832.0  112.883379  -0.535171  54.513346\n# 163             proteasome_inhibitor  726.0  129.556319  -0.973011  25.720070\n# and these four are also listed above\n# 63                     cdk_inhibitor  340.0   85.844863 -14.761353  43.245544\n# 96   glucocorticoid_receptor_agonist  266.0   50.430224  74.849734  35.988496\n# 109                  hmgcr_inhibitor  283.0   39.967208 -55.079256  46.488723\n# 169                    raf_inhibitor  223.0   36.522937 -81.275786  48.158390\ntSNE_4subset = ['glucocorticoid_receptor_agonist',\n               'raf_inhibitor', 'cdk_inhibitor', 'hmgcr_inhibitor']","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:20:16.916987Z","iopub.status.busy":"2020-10-06T17:20:16.916167Z","iopub.status.idle":"2020-10-06T17:20:16.919585Z","shell.execute_reply":"2020-10-06T17:20:16.920207Z"},"papermill":{"duration":0.277467,"end_time":"2020-10-06T17:20:16.920364","exception":false,"start_time":"2020-10-06T17:20:16.642897","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Use results above for \"guessing\" estimate of score, and repeat the calculation, but\n# subtract off the errors for the *** targ_subset *** ones, assuming we 'know' them:\nprint(\"If we can correctly predict all {} of the t-SNE targets above,\\n\".format(len(tSNE_4subset)),\n      \"then the expected score (incl controls) is:\\n\",\n      ((len(df_treat_targs)/len(df_train_targs)) * \n         (sum(scores_targs) - sum(scores_targs[tSNE_4subset])) / len(aves_targs)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:20:17.47632Z","iopub.status.busy":"2020-10-06T17:20:17.475571Z","iopub.status.idle":"2020-10-06T17:20:17.479177Z","shell.execute_reply":"2020-10-06T17:20:17.478411Z"},"papermill":{"duration":0.275168,"end_time":"2020-10-06T17:20:17.479301","exception":false,"start_time":"2020-10-06T17:20:17.204133","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Doing the t-SNE\n# Can choose to do it or not, always do it when submitting to Kaggle:\nDO_TSNE = False","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:20:18.022869Z","iopub.status.busy":"2020-10-06T17:20:18.021678Z","iopub.status.idle":"2020-10-06T17:27:06.877725Z","shell.execute_reply":"2020-10-06T17:27:06.87849Z"},"papermill":{"duration":409.132909,"end_time":"2020-10-06T17:27:06.878673","exception":false,"start_time":"2020-10-06T17:20:17.745764","status":"completed"},"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"if DO_TSNE or LOCATION_KAGGLE:\n    # Select the train data with the features to do t-SNE on  *** Include the cs_to_use too ***\n    # All sig_ids:\n    train_data_TSNE = df_train_feats[gs_to_use + cs_to_use].copy()\n    print(train_data_TSNE.columns)\n    \n    # Set basic parameters\n    this_TSNE = TSNE(perplexity=30.0, learning_rate=200.0, init='pca', \n                 n_iter=2000, n_iter_without_progress=150, min_grad_norm=1e-6,\n                 verbose=1, random_state=17, n_jobs=-2)\n\n    # Do the t-SNE on all, or use a subset of the data\n    ##n_samples = 5000\n    n_samples = len(train_data_TSNE)   # Use All\n\n    t_start = time()\n    # Do the fit & transform\n    train_TSNE_out = this_TSNE.fit_transform(train_data_TSNE.iloc[0:n_samples])\n    train_TSNE_out = pd.DataFrame(train_TSNE_out)\n    print(\" t-SNE(Train)   Total time:\",time()-t_start,\"   Iterations:\", this_TSNE.n_iter_)\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:27:07.436405Z","iopub.status.busy":"2020-10-06T17:27:07.435636Z","iopub.status.idle":"2020-10-06T17:27:12.688281Z","shell.execute_reply":"2020-10-06T17:27:12.689258Z"},"papermill":{"duration":5.543199,"end_time":"2020-10-06T17:27:12.689494","exception":false,"start_time":"2020-10-06T17:27:07.146295","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"if DO_TSNE or LOCATION_KAGGLE:\n    # Make a dataframe for some t-SNE summary info by target\n    # Start with the first 2 columns of df_g_vectors\n    df_tSNE_summary = df_g_vectors[['targ_str','count']].copy()\n    n_targs = len(df_tSNE_summary)\n    tsne0_ave = np.zeros(n_targs)\n    tsne1_ave = np.zeros(n_targs)\n    tsne_rms = np.zeros(n_targs)\n\n    for itarg, this_targ in enumerate(df_tSNE_summary['targ_str']):\n        # Find all the sig_ids for this target\n        targ_sel = (df_train_targs[this_targ] > 0)\n        num_ids = sum(targ_sel)\n        tsne0_ave[itarg] = sum(train_TSNE_out[0] * 1.0*targ_sel)/num_ids\n        tsne1_ave[itarg] = sum(train_TSNE_out[1] * 1.0*targ_sel)/num_ids\n        tsne_rms[itarg] = np.sqrt(sum( (1.0*targ_sel)*((train_TSNE_out[0]-tsne0_ave[itarg])**2 +\n                                        (train_TSNE_out[1]-tsne1_ave[itarg])**2))/num_ids)\n        # print stuff and cut it short for testing...\n        ##print(itarg,this_targ,num_ids,tsne0_ave[itarg],tsne1_ave[itarg],tsne_rms[itarg])\n        ##if itarg > 9:\n        ##    break\n    \n    # Put them in the df\n    df_tSNE_summary['0-ave'] = tsne0_ave\n    df_tSNE_summary['1-ave'] = tsne1_ave\n    df_tSNE_summary['rms'] = tsne_rms\n\n    # Show info about them\n    df_tSNE_summary.hist('rms',bins=90)\n    plt.title(\"Histogram of the t-SNE rms for the targets\")\n    plt.show()\n\n    df_tSNE_summary.plot('0-ave','1-ave',c='rms',cmap='jet',kind='scatter',s=10)\n    plt.title(\"Location of the t-SNE averages for the targets\")\n    plt.show()\n\n    df_tSNE_summary.plot('rms','count',c='rms',cmap='jet',kind='scatter',s=10)\n    plt.title(\"Counts vs t-SNE rms for the targets\")\n    plt.show()\n\n    # List high-counts, lowish rms ones\n    print(df_tSNE_summary[( (df_tSNE_summary['rms'] < 58) & (df_tSNE_summary['count'] > 200) )])","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:27:13.265868Z","iopub.status.busy":"2020-10-06T17:27:13.264921Z","iopub.status.idle":"2020-10-06T17:27:18.389288Z","shell.execute_reply":"2020-10-06T17:27:18.389881Z"},"papermill":{"duration":5.421443,"end_time":"2020-10-06T17:27:18.390061","exception":false,"start_time":"2020-10-06T17:27:12.968618","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"if DO_TSNE or LOCATION_KAGGLE:\n    # Show where Controls are\n    colors = 1.0*(df_train_feats['cp_type'] != 'ctl_vehicle')\n    plt.figure(figsize = [10.4, 8])\n    plt.scatter(train_TSNE_out[0], train_TSNE_out[1], \n            cmap='prism', c=colors[0:n_samples], alpha=0.3, s=5)\n    plt.title(\"t-SNE Output with color-coding: Controls (red), non-Controls (green)\")\n    plt.xlabel(\"t-SNE[0]\"); plt.ylabel(\"t-SNE[1]\")\n    plt.savefig('tSNE_controls_coloring.jpg')\n    plt.show()\n    \n    # Show where any active MoA is\n    numMoA = df_train_targs.sum(axis=1)\n    colors = 1.0*(numMoA > 0)\n    ##print(\"Sum of MoA colors = \",sum(colors), sum(colors > 0))\n    plt.figure(figsize = [10.4, 8])\n    plt.scatter(train_TSNE_out[0], train_TSNE_out[1], \n            cmap='prism', c=colors[0:n_samples], alpha=0.3, s=5)\n    plt.title(\"t-SNE Output with color-coding: numMoA=0 (red), numMoA >= 1 (green)\")\n    plt.xlabel(\"t-SNE[0]\"); plt.ylabel(\"t-SNE[1]\")\n    plt.savefig('tSNE_MoA_coloring.jpg')\n    \n    # Show where a selected target's MoAs are set in tSNE space\n    #\n    # Both nfkb_inhibitor and proteasome_inhibitor are set:\n    ##colors = 1.0*((df_train_targs['nfkb_inhibitor'] > 0) &\n    ##            (df_train_targs['proteasome_inhibitor'] > 0))\n    # Other 'island's:\n    ##colors = 1.0*(df_train_targs['cdk_inhibitor'] > 0) \n    ##colors = 1.0*(df_train_targs['raf_inhibitor'] > 0)\n    ##colors = 1.0*(df_train_targs['glucocorticoid_receptor_agonist'] > 0)\n    # Or any that are in the targ_subset\n    colors = 1.0*(df_train_targs[targ_subset].sum(axis=1) > 0)\n    #        \n    ##print(\"Sum of MoA colors = \",sum(colors), sum(colors > 0))\n    plt.figure(figsize = [10.4, 8])\n    plt.scatter(train_TSNE_out[0], train_TSNE_out[1], \n            cmap='prism', c=colors[0:n_samples], alpha=0.3, s=5)\n    plt.title(\"t-SNE Output with color-coding:  not selected(red), Target Subset (green)\")\n    plt.xlabel(\"t-SNE[0]\"); plt.ylabel(\"t-SNE[1]\")\n    plt.savefig('tSNE_targetsubset_coloring.jpg')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:27:19.016505Z","iopub.status.busy":"2020-10-06T17:27:19.012966Z","iopub.status.idle":"2020-10-06T17:28:09.233493Z","shell.execute_reply":"2020-10-06T17:28:09.234083Z"},"papermill":{"duration":50.539768,"end_time":"2020-10-06T17:28:09.234237","exception":false,"start_time":"2020-10-06T17:27:18.694469","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"if DO_TSNE or LOCATION_KAGGLE:   \n    # Do the test too:  doesn't have as many samples...    *** Include the cs_to_use too ***\n    test_data_TSNE = df_test_feats[gs_to_use + cs_to_use].copy()\n    print(test_data_TSNE.columns)\n    this_TSNE = TSNE(perplexity=30.0, learning_rate=200.0, init='pca', \n                 n_iter=2000, n_iter_without_progress=150, min_grad_norm=1e-6,\n                 verbose=1, random_state=17, n_jobs=-2)\n    n_samples = len(test_data_TSNE)   # Use All\n    t_start = time()\n    test_TSNE_out = this_TSNE.fit_transform(test_data_TSNE.iloc[0:n_samples])\n    test_TSNE_out = pd.DataFrame(test_TSNE_out)\n    print(\" t-SNE(Test)   Total time:\",time()-t_start,\"   Iterations:\", this_TSNE.n_iter_)\n\n    # Show where TEST Controls are\n    colors = 1.0*(df_test_feats['cp_type'] != 'ctl_vehicle')\n    plt.figure(figsize = [10.4, 8])\n    plt.scatter(test_TSNE_out[0], test_TSNE_out[1], \n            cmap='prism', c=colors[0:n_samples], alpha=0.3, s=5)\n    plt.title(\"t-SNE -TEST- Output with color-coding: Controls (red), non-Controls (green)\")\n    plt.xlabel(\"t-SNE[0]\"); plt.ylabel(\"t-SNE[1]\")\n    plt.savefig('tSNE_controls-TEST_coloring.jpg')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.310241,"end_time":"2020-10-06T17:28:09.855822","exception":false,"start_time":"2020-10-06T17:28:09.545581","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### End of looking at Targets and Features\nBack to <a href=\"#Index\">Index</a>"},{"metadata":{"papermill":{"duration":0.304549,"end_time":"2020-10-06T17:28:11.756869","exception":false,"start_time":"2020-10-06T17:28:11.45232","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## _ _ _ _ _ _ _ _ _ _"},{"metadata":{"papermill":{"duration":0.308445,"end_time":"2020-10-06T17:28:12.455133","exception":false,"start_time":"2020-10-06T17:28:12.146688","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## <a id=\"MachineLearning\">Machine Learning</a>\nBack to <a href=\"#Index\">Index</a>\n\nSetup a classifier to decide MoA or not MoA,\nusing the general structure from my [\"Titanic Confusion-Dots Plot\"](https://www.kaggle.com/dan3dewey/titanic-confusion-dots-plot) notebook.\n\n(up through v18) Note that the y=1 case is \"notMoA\" -- we're trying to identify sig_ids that we can be sure have no MoA targets selected, the precision, TP/(TP+FP) = \"what fraction of claimed notMoA=1 are correctly identified\", measures how well we're doing; this can also be seen in the \"confusion dots\" plot. Conversely, setting a lower threshold and looking at the \"negative precision\", TN/(TN+FN), shows how well we're doing at detecting sig_ids that are likely to have one or more MoAs active.\n\n(V19 etc.) The y=1 was changed to mean numMoA > 0; in this case the precision is a measure of \"what fraction of claimed MoA>1 are correctly identified.\""},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:28:13.094793Z","iopub.status.busy":"2020-10-06T17:28:13.093995Z","iopub.status.idle":"2020-10-06T17:28:13.978552Z","shell.execute_reply":"2020-10-06T17:28:13.979175Z"},"papermill":{"duration":1.212942,"end_time":"2020-10-06T17:28:13.979363","exception":false,"start_time":"2020-10-06T17:28:12.766421","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# eXtreme Gradient Boost classifier\n\nfrom xgboost import XGBClassifier\n\n# Other ML things we'll use:\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import make_scorer\n\nfrom sklearn.model_selection import GridSearchCV\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.309344,"end_time":"2020-10-06T17:28:14.595321","exception":false,"start_time":"2020-10-06T17:28:14.285977","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Select the Features"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:28:15.252053Z","iopub.status.busy":"2020-10-06T17:28:15.248717Z","iopub.status.idle":"2020-10-06T17:28:15.257681Z","shell.execute_reply":"2020-10-06T17:28:15.256883Z"},"papermill":{"duration":0.338733,"end_time":"2020-10-06T17:28:15.257845","exception":false,"start_time":"2020-10-06T17:28:14.919112","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Select and fill the features\n\n# Drop some of the g features based on correlations and scatter plots\n# Use all the gs_to_use_22 and add in the dbgs_to_add ones:\n##features = (['c-ave', 'c-std', 'c-5%', 'c-95%'] + cs_to_use +\n##           ['g-95%', 'g-hif'] + gs_to_use_22 + dbgs_to_add)\n\n# Use the 11 gs_to_use and add in the 10 dbgs_to_add ones:\nfeatures = (['c-ave', 'c-std', 'c-5%', 'c-95%'] + cs_to_use +\n           ['g-95%', 'g-hif'] + gs_to_use + dbgs_to_add)\n\nprint(\"\\nLength of cs_to_use:\",len(cs_to_use), cs_to_use)\nprint(\"\\nLength of gs_to_use:\",len(gs_to_use), gs_to_use_22)\nprint(\"\\nTotal number of features used:\",len(features))\nprint(features)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Select Target: numMoA or numSub\nAnd create the 3 sets of X,y s, one for each cp_time."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Can select/update the desired target here:    OR NOT\n# A single target ;-)\n\n# Choose from the \"tSNE 9\":\n#       'proteasome_inhibitor', 'nfkb_inhibitor', 'glucocorticoid_receptor_agonist',\n#       'raf_inhibitor', 'cdk_inhibitor', 'hmgcr_inhibitor', \n#       'egfr_inhibitor', 'hsp_inhibitor', 'tubulin_inhibitor'\n##targ_subset = ['nfkb_inhibitor'];  targ_subset_name = \"1: nfkb\"\n##targ_subset = ['proteasome_inhibitor'];  targ_subset_name = \"1: prot\"\n##targ_subset = ['glucocorticoid_receptor_agonist'];  targ_subset_name = \"1: gluc\"\n##targ_subset = ['raf_inhibitor'];  targ_subset_name = \"1: raf_\"\n##targ_subset = ['cdk_inhibitor'];  targ_subset_name = \"1: cdk_\"\n##targ_subset = ['hmgcr_inhibitor'];  targ_subset_name = \"1: hmgcr\"\n##targ_subset = ['egfr_inhibitor'];  targ_subset_name = \"1: egfr\"\n##targ_subset = ['hsp_inhibitor'];  targ_subset_name = \"1: hsp_\"\n##targ_subset = ['tubulin_inhibitor'];  targ_subset_name = \"1: tubu\"\n\n# These are the targets with 'highly detectable' g-vectors (unless listed in tSNE-9 already)\n#    'topoisomerase_inhibitor', 'hdac_inhibitor', 'mtor_inhibitor', 'mek_inhibitor',\n#    'pi3k_inhibitor', 'protein_synthesis_inhibitor', 'atpase_inhibitor'\n##targ_subset = ['topoisomerase_inhibitor'];  targ_subset_name = \"1: topoi\"\n##targ_subset = ['hdac_inhibitor'];  targ_subset_name = \"1: hdac_\"\n##targ_subset = ['mtor_inhibitor'];  targ_subset_name = \"1: mtor_\"\n##targ_subset = ['mek_inhibitor'];  targ_subset_name = \"1: mek_i\"\n##targ_subset = ['pi3k_inhibitor'];  targ_subset_name = \"1: pi3k_\"\n##targ_subset = ['protein_synthesis_inhibitor'];  targ_subset_name = \"1: prote\"\n##targ_subset = ['atpase_inhibitor'];  targ_subset_name = \"1: atpas\"\n\n# Add other ones that are above 0.012:  (not necessarily 'detectable', though)\n#      'acetylcholine_receptor_antagonist', 'adrenergic_receptor_agonist', 'adrenergic_receptor_antagonist',\n#      'calcium_channel_blocker', 'cyclooxygenase_inhibitor', 'dna_inhibitor', 'dopamine_receptor_antagonist',\n#      'flt3_inhibitor', 'glutamate_receptor_antagonist', 'histamine_receptor_antagonist',\n#      'kit_inhibitor', 'pdgfr_inhibitor',\n#      'phosphodiesterase_inhibitor', 'serotonin_receptor_antagonist', 'sodium_channel_inhibitor'\n##targ_subset = ['acetylcholine_receptor_antagonist'];  targ_subset_name = \"1: acety\"\n##targ_subset = ['adrenergic_receptor_agonist'];  targ_subset_name = \"1: adren_ago\"\n##targ_subset = ['adrenergic_receptor_antagonist'];  targ_subset_name = \"1: adren_ant\"\n##targ_subset = ['calcium_channel_blocker'];  targ_subset_name = \"1: calci\"\n##\n##targ_subset = ['histamine_receptor_antagonist'];  targ_subset_name = \"1: hista\"\n## . . .\n##targ_subset = ['sodium_channel_inhibitor'];  targ_subset_name = \"1: sodiu\"\n\n\n\n# and fill the numSub from this target subset:\n##df_aug_feats['numSub'] = df_treat_targs[targ_subset].sum(axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:28:15.91803Z","iopub.status.busy":"2020-10-06T17:28:15.916908Z","iopub.status.idle":"2020-10-06T17:28:16.04884Z","shell.execute_reply":"2020-10-06T17:28:16.049643Z"},"papermill":{"duration":0.484503,"end_time":"2020-10-06T17:28:16.049841","exception":false,"start_time":"2020-10-06T17:28:15.565338","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Fill the X,y and Xkag,y_kag\n\n# The Target is set here also:\n# - Usually y=1 when numMoA is either =0 or >0\n# - Another option is to use numSub instead:\n#     this is the sum of MoAs of a subset of targets,\n#     in the variable: targ_subset and \n#        given a name: targ_subset_name.\n# Defined subsets are (the number is how many targets are in it):\n#      \"9: t-SNE islands\"\n#      \"2: proteasome & nfkb\"\n\nUSE_TARG_SUBSET = False\n\n# Make 3 sets of X,y, selecting on cp_time:\n\n# (v19+) The classifier target is   *** numMoA > 0 ***\n# So y=1 means -->  numMoA > 0\n\n# Select the three sets of sig_ids based on cp_time\n\nselect_train_24 = df_aug_feats['cp_time'] == 24\nX24 = df_aug_feats.loc[ select_train_24, features ].copy()\n\nselect_train_48 = df_aug_feats['cp_time'] == 48\nX48 = df_aug_feats.loc[ select_train_48, features ].copy()\n\nselect_train_72 = df_aug_feats['cp_time'] == 72\nX72 = df_aug_feats.loc[ select_train_72, features ].copy()\n\n# Their targets\nif USE_TARG_SUBSET:\n    # Usually use > 0, i.e, if MoA total is 1 or more.\n    if (\"2: pro\" in targ_subset_name):\n        # But, for the \"2: prot...\" subset use > 1 to find where both are active (very common.)\n        y24 = 1.0*(df_aug_feats.loc[select_train_24, 'numSub'].values > 1)\n        y48 = 1.0*(df_aug_feats.loc[select_train_48, 'numSub'].values > 1)\n        y72 = 1.0*(df_aug_feats.loc[select_train_72, 'numSub'].values > 1)\n    else:\n        y24 = 1.0*(df_aug_feats.loc[select_train_24, 'numSub'].values > 0)\n        y48 = 1.0*(df_aug_feats.loc[select_train_48, 'numSub'].values > 0)\n        y72 = 1.0*(df_aug_feats.loc[select_train_72, 'numSub'].values > 0)\n    \nelse:\n    # Not using a subset, so use numMoA as the target\n    y24 = 1.0*(df_aug_feats.loc[select_train_24, 'numMoA'].values > 0)\n    y48 = 1.0*(df_aug_feats.loc[select_train_48, 'numMoA'].values > 0)\n    y72 = 1.0*(df_aug_feats.loc[select_train_72, 'numMoA'].values > 0)\n    \n    \nif USE_TARG_SUBSET:\n    print(\"\\n  *** A subset of targets is used:  \"+targ_subset_name,\"  ***\")\n    \nprint(\"\\nThe X24, y24 have lengths of {} and {}.\\n\".format(len(X24),len(y24)))\nprint(\"The X48, y48 have lengths of {} and {}.\\n\".format(len(X48),len(y48)))\nprint(\"The X72, y72 have lengths of {} and {}.\\n\".format(len(X72),len(y72)))\n\n\n# To-be-predicted features and (dummy) target\n# select on cp_time:\nselect_kag_24 = df_test_feats['cp_time'] == 24\nXkag24 = df_test_feats.loc[select_kag_24, features].copy()\ny_kag24 = np.zeros(len(Xkag24))\nprint(\"\\nThe Xkag24, y_kag24 have lengths of {} and {}.\\n\".format(len(Xkag24),len(y_kag24)))\n\nselect_kag_48 = df_test_feats['cp_time'] == 48\nXkag48 = df_test_feats.loc[select_kag_48, features].copy()\ny_kag48 = np.zeros(len(Xkag48))\nprint(\"The Xkag48, y_kag48 have lengths of {} and {}.\\n\".format(len(Xkag48),len(y_kag48)))\n\nselect_kag_72 = df_test_feats['cp_time'] == 72\nXkag72 = df_test_feats.loc[select_kag_72, features].copy()\ny_kag72 = np.zeros(len(Xkag72))\nprint(\"The Xkag72, y_kag72 have lengths of {} and {}.\\n\".format(len(Xkag72),len(y_kag72)))\n\n# The features don't need to be scaled.\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Optionally do this (if using a target subset)\n# Disable because there are some many features ;-)\nif False and USE_TARG_SUBSET:\n    # For some targets and some features,\n    # Go through the targets and for each target\n    #   find all the sig_ids that have that target active and cp_time=48,\n    #   and calculate the statistics of the feature values.\n    #   For each of the features,\n    #     plot all of that feature's values that are in the sig_ids selected.\n    #\n    # Use df_treat_targs and df_aug_feats to have access to everything...\n\n    # select the target(s)\n    ##targs_to_plot = ['nfkb_inhibitor','cdk_inhibitor']\n    # Just show the last one\n    targs_to_plot = [targ_subset[-1]]\n\n    # These can be any features (not just gs),\n    # some of the generally higher-importance ones\n    ##gs_to_plot = ['g-hif','g-75','g-100','g-392','c-ave','c-95%']\n    gs_to_plot = features\n\n    # Loop over the targets\n    for targ_str in targs_to_plot:\n        # Get the sig_ids with this target active and cp_time is 48\n        si_select = (df_treat_targs[targ_str] > 0) & (df_aug_feats['cp_time'] == 48)\n        this_targ_gs = df_aug_feats.loc[si_select, gs_to_plot]\n        print(\"\\n\\n\"+targ_str+\":\\n\")\n        df_g_stats = this_targ_gs.describe()\n        df_g_stats = df_g_stats.T.drop(columns=['min','max'])\n        print(df_g_stats,\"\\n\")\n        # Make the plots for each g\n        for this_g in gs_to_plot:\n            # get the values\n            gvals = this_targ_gs[this_g].values\n            # sort them to see common levels (plateaus) - cute but not intuitive to view\n            ##gvals.sort()\n            plt.plot(gvals,'.b')\n            plt.title(\"Values of \"+this_g+\" for all sig_ids with \"+\n                 targ_str+\" = 1\")\n            plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create 'Jumbo' versions of each X and y that are\n# 4 times larger and the Xs include added random noise.\n  \nif True:\n    # Create 'Jumbo' versions of each X and y that are\n    # 4 times larger and the Xs include added random noise.\n    # \n    blurr_cols = cs_to_use + gs_to_use\n    blurr_std = 0.5\n    \n    # Make the Xs\n    X24J = X24.copy().append(X24, ignore_index=True).append(X24, \n            ignore_index=True).append(X24, ignore_index=True) \n    lenX = len(X24J)\n    for this_col in blurr_cols:\n        X24J[this_col] = X24J[this_col] + blurr_std * random.standard_normal(lenX)\n        \n    X48J = X48.copy().append(X48, ignore_index=True).append(X48, \n            ignore_index=True).append(X48, ignore_index=True)\n    lenX = len(X48J)\n    for this_col in blurr_cols:\n        X48J[this_col] = X48J[this_col] + blurr_std * random.standard_normal(lenX)\n        \n    X72J = X72.copy().append(X72, ignore_index=True).append(X72, \n            ignore_index=True).append(X72, ignore_index=True)\n    lenX = len(X72J)\n    for this_col in blurr_cols:\n        X72J[this_col] = X72J[this_col] + blurr_std * random.standard_normal(lenX)\n\n        \n    # Assemble the ys:\n    y24J = np.concatenate([y24,y24,y24,y24])\n    y48J = np.concatenate([y48,y48,y48,y48])\n    y72J = np.concatenate([y72,y72,y72,y72])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Choose a ML classifier to use: XGB"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:28:16.678405Z","iopub.status.busy":"2020-10-06T17:28:16.677615Z","iopub.status.idle":"2020-10-06T17:28:16.680876Z","shell.execute_reply":"2020-10-06T17:28:16.680189Z"},"papermill":{"duration":0.322341,"end_time":"2020-10-06T17:28:16.681022","exception":false,"start_time":"2020-10-06T17:28:16.358681","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Thefollowing is from 40% of the way down on the page:\n#   https://xgboost.readthedocs.io/en/latest/python/python_api.html\n\n# XGBClassifier(\n# max_depth=3, learning_rate=0.1, n_estimators=100,\n# verbosity=1, objective='binary:logistic', booster='gbtree',\n# tree_method='auto', n_jobs=1, gpu_id=-1,\n# gamma=0, min_child_weight=1, max_delta_step=0,\n# subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1,\n# reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5,\n# random_state=0, missing=None)\n\n# get_params output, in alpha order:\n\n#{      'base_score': 0.50,\n# 'booster': 'gbtree',\n# 'colsample_bylevel': 1,\n# 'colsample_bynode': 1,\n# 'colsample_bytree': 1,\n#                           'gamma': 0,\n#                           'learning_rate': 0.1,    # xgb's eta\n# 'max_delta_step': 0,\n#                           'max_depth': 1,\n# 'min_child_weight': 1,\n# 'missing': None,\n#                           'n_estimators': 100,\n# 'n_jobs': 1,\n# 'nthread': None,\n#       'objective': 'binary:logistic',\n# 'random_state': 0,\n# 'reg_alpha': 0,    # xgb's alpha\n#       'reg_lambda': 1,   # xgb's lambda\n# 'scale_pos_weight': 1,\n# 'seed': None,\n# 'silent': None,\n#       'subsample': 1,\n#       'verbosity': 1}\n\n#   Used in (v11 with gs_to_use features as well)\n#        \"max_depth\"        : 8,\n#        \"learning_rate\"    : 0.05,\n#        \"n_estimators\"     : 80,\n#        \"min_child_weight\" : 3,\n#        \"gamma\"            : 1.5,\n#        \"colsample_bytree\" : 0.70,\n#        \"subsample\"        : 1.0,\n#        \"reg_lambda\"       : 1.0,\n\n#   Used in (v14 - NO gs_to_use features)\n#        \"max_depth\"        : 6,\n#        \"learning_rate\"    : 0.03,\n#        \"n_estimators\"     : 120,     # oopse: used 100 in v14, should be 120.\n#        \"min_child_weight\" : 1,\n#        \"gamma\"            : 1.5,\n#        \"colsample_bytree\" : 0.90,\n#        \"subsample\"        : 1.0,\n#        \"reg_lambda\"       : 1.0,\n\nxgb_params = {\n        \"max_depth\"        : 8,   #\n        \"learning_rate\"    : 0.06,\n        \"n_estimators\"     : 60,\n        \"min_child_weight\" : 4,    #\n        \"gamma\"            : 1.5,\n        \"colsample_bytree\" : 0.70, #\n        \"subsample\"        : 1.0,\n        \"reg_lambda\"       : 2.0,\n    #\n        \"objective\": \"binary:logistic\",\n        \"base_score\" : 0.50,\n        \"verbosity\" : 1\n     }\n\n# Setup hyper-parameter grid for the model:      *** For X48 fitting ***\nxgb_param_grid = [\n    {\n        \"max_depth\"        : [5,6,7,8,9,10,11,12],                      # 10 is good\n        ##\"learning_rate\"    : [0.04, 0.05, 0.06, 0.07, 0.08],  # 0.06\n        ##\"n_estimators\"       : [40, 50, 60, 70, 80],          # 60 is good\n        \"min_child_weight\"   : [1, 2, 3, 4, 6],                 # use 2\n        ##\"gamma\"            : [0.5, 1.0, 1.5, 2.0, 4.0],       # 1.5 is good\n        \"colsample_bytree\" : [0.5, 0.70, 0.9],                  # 0.80 is good\n        ##\"subsample\"        : [0.8, 1.0],                      # keep value of 1\n        ##\"reg_lambda\"       : [0.2, 0.50, 1.0, 2.0, 5.0],      # 2 is good\n     }\n]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:28:17.327535Z","iopub.status.busy":"2020-10-06T17:28:17.326444Z","iopub.status.idle":"2020-10-06T17:28:17.329748Z","shell.execute_reply":"2020-10-06T17:28:17.329187Z"},"papermill":{"duration":0.323746,"end_time":"2020-10-06T17:28:17.329873","exception":false,"start_time":"2020-10-06T17:28:17.006127","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"model_name = 'xgb'\nmodel_base = XGBClassifier(**xgb_params)\nparam_grid = xgb_param_grid","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Do the fits, use x4 with added noise "},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:28:17.966056Z","iopub.status.busy":"2020-10-06T17:28:17.965294Z","iopub.status.idle":"2020-10-06T17:28:19.515554Z","shell.execute_reply":"2020-10-06T17:28:19.520792Z"},"papermill":{"duration":1.878822,"end_time":"2020-10-06T17:28:19.521045","exception":false,"start_time":"2020-10-06T17:28:17.642223","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Doing this fit here lets us skip over the Hyper-Parameter Search and continue on.\n\nif True:\n    # Use the 'Jumbo' versions of each X and y that are\n    # 4 times larger and the Xs include added random noise.\n\n    best_fit_mod24 = XGBClassifier(**xgb_params).fit(X24J,y24J)\n    # Show these parameters\n    print(best_fit_mod24.get_params())\n    \n    best_fit_mod48 = XGBClassifier(**xgb_params).fit(X48J,y48J)\n    # Show these parameters\n    print(best_fit_mod48.get_params())\n    \n    best_fit_mod72 = XGBClassifier(**xgb_params).fit(X72J,y72J)\n    # Show these parameters\n    print(best_fit_mod72.get_params())\n\n    # Also define cv_folds, gscv_stats incase the following is skipped:\n    cv_folds = 4\n    gscv_stats = []\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.342432,"end_time":"2020-10-06T17:28:20.212695","exception":false,"start_time":"2020-10-06T17:28:19.870263","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n### Continue with the <a id=\"HyperSearch\">Hyper-Parameter Search</a> that follows\n#### Or skip the whole Hyper-Parameter section, go to <a href=\"#FeatureImportance\">Feature Importance</a> <br>\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"},{"metadata":{"papermill":{"duration":0.310632,"end_time":"2020-10-06T17:28:21.470402","exception":false,"start_time":"2020-10-06T17:28:21.15977","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n###  Done with <a href=\"#HyperSearch\">Hyper-parameters</a> above\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"},{"metadata":{"papermill":{"duration":0.304929,"end_time":"2020-10-06T17:28:22.105437","exception":false,"start_time":"2020-10-06T17:28:21.800508","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## <a id=\"FeatureImportance\">Feature Importance</a>\nBack to <a href=\"#Index\">Index</a>"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:28:22.735765Z","iopub.status.busy":"2020-10-06T17:28:22.734701Z","iopub.status.idle":"2020-10-06T17:28:22.742669Z","shell.execute_reply":"2020-10-06T17:28:22.743332Z"},"papermill":{"duration":0.331688,"end_time":"2020-10-06T17:28:22.743508","exception":false,"start_time":"2020-10-06T17:28:22.41182","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Choose one model to show Feature Importance\nbest_fit_model = best_fit_mod48\nX = X48.copy()\ny = y48.copy()\n\n# Show the model parameters:\nbest_fit_model.get_params()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:28:23.379181Z","iopub.status.busy":"2020-10-06T17:28:23.378394Z","iopub.status.idle":"2020-10-06T17:28:23.827374Z","shell.execute_reply":"2020-10-06T17:28:23.826767Z"},"papermill":{"duration":0.776189,"end_time":"2020-10-06T17:28:23.827512","exception":false,"start_time":"2020-10-06T17:28:23.051323","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Feature importance\n\nif model_name in ['lgr','dtc','rfc','gbc','mlp','xgb']:\n    # Plot feature importance\n    # Get feature importance\n    if model_name == 'mlp':\n        # For mlp regressor create a quasi-importance from the weights.\n        # \"The ith element in the list represents the weight matrix corresponding to layer i.\"\n        # Input layer weights\n        ##len(best_regressor.coefs_[0])\n        # sum of abs() of input weights for each feature\n        feature_importance = np.array([sum(np.abs(wgts)) for wgts in best_fit_model.coefs_[0] ])\n    elif model_name == 'lgr':\n        # For Logisitic Regression use the coeff.s to approximate an importance\n        coeffs = best_fit_model.coef_[0]\n        feature_importance = 0.0 * coeffs\n        print(\" Feature        Import.      coeff.    max from mean\")\n        for icol, col in enumerate(X.columns):\n            col_mean = X[col].mean()\n            col_max_from_mean = np.max(np.abs(X[col] - col_mean))\n            feature_importance[icol] = abs(coeffs[icol]/col_max_from_mean)\n            print(\"{:10}: {:10.3f}, {:10.3f}, {:10.2f}\".format(col, feature_importance[icol], coeffs[icol], col_max_from_mean))\n    else:\n        # tree models have feature importance directly available:\n        feature_importance = best_fit_model.feature_importances_\n        \n    # make importances relative to max importance\n    max_import = feature_importance.max()\n    feature_importance = 100.0 * (feature_importance / max_import)\n    sorted_idx = np.argsort(feature_importance)\n    pos = np.arange(sorted_idx.shape[0]) + 0.5\n\n    plt.figure(figsize=(8, 15))\n    ##plt.subplot(1, 2, 2)\n    plt.barh(pos, feature_importance[sorted_idx], align='center')\n    plt.yticks(pos, X.columns[sorted_idx])\n    plt.xlabel(model_name.upper()+' -- Relative Importance')\n    plt.title('           '+model_name.upper()+\n              ' -- Variable Importance                  max --> {:.3f} '.format(max_import))\n\n    plt.savefig(model_name.upper()+\"_importance_\"+version_str+\".png\")\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:28:24.461521Z","iopub.status.busy":"2020-10-06T17:28:24.460714Z","iopub.status.idle":"2020-10-06T17:28:24.524224Z","shell.execute_reply":"2020-10-06T17:28:24.524912Z"},"papermill":{"duration":0.38521,"end_time":"2020-10-06T17:28:24.525112","exception":false,"start_time":"2020-10-06T17:28:24.139902","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Nominal accuracy of each model fit on the original training data\n\nprint(\"\")\nall_train_score = accuracy_score(y24, best_fit_mod24.predict(X24))\nprint(\"24: Nominal (thresh.=0.5) best-fit \"+\n      \"All-Train accuracy: {:.2f} %\\n\".format(100.0*all_train_score))\n\nprint(\"\")\nall_train_score = accuracy_score(y48, best_fit_mod48.predict(X48))\nprint(\"48: Nominal (thresh.=0.5) best-fit \"+\n      \"All-Train accuracy: {:.2f} %\\n\".format(100.0*all_train_score))\n\nprint(\"\")\nall_train_score = accuracy_score(y72, best_fit_model.predict(X72))\nprint(\"72: Nominal (thresh.=0.5) best-fit \"+\n      \"All-Train accuracy: {:.2f} %\\n\".format(100.0*all_train_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if USE_TARG_SUBSET:\n    print(targ_subset_name)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:28:25.21628Z","iopub.status.busy":"2020-10-06T17:28:25.21307Z","iopub.status.idle":"2020-10-06T17:28:25.280451Z","shell.execute_reply":"2020-10-06T17:28:25.281274Z"},"papermill":{"duration":0.402308,"end_time":"2020-10-06T17:28:25.281476","exception":false,"start_time":"2020-10-06T17:28:24.879168","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Make the model probability predictions on the Training and Test (Kaggle) data\n\n# The 'soft' probabilty values, go from 0 to 1\nyh24 = best_fit_mod24.predict_proba(X24)\nyh24 = yh24[:,1]\nyh48 = best_fit_mod48.predict_proba(X48)\nyh48 = yh48[:,1]\nyh72 = best_fit_mod72.predict_proba(X72)\nyh72 = yh72[:,1]\n\n# Make the Kaggle set predictions too\nyh_kag24 = best_fit_mod24.predict_proba(Xkag24)\nyh_kag24 = yh_kag24[:,1]\nyh_kag48 = best_fit_mod48.predict_proba(Xkag48)\nyh_kag48 = yh_kag48[:,1]\nyh_kag72 = best_fit_mod72.predict_proba(Xkag72)\nyh_kag72 = yh_kag72[:,1]\n\n\n# yh and yh_kag are the model probability predictions.\n# Convert to discrete 0,1 using a threshold:\n#\n# Select the threshold based on balance between FP and FN,\n# e.g. desired Precision, etc.\n# Use the same for all cp_time values?\n#\nif USE_TARG_SUBSET:\n    #\n    # Threshold to use:  * Iteratively set this to get the desired precision, see below. *\n    #\n    yh_threshold = 0.236   # set for 95% precision\n    #\n    #\n    # lower it for the \"4: low...\" subset\n    if \"4: low\" in targ_subset_name:\n        yh_threshold = 0.55\n    # lower it for the \"22: a above 0.01\" subset\n    if \"22: a\" in targ_subset_name:\n        yh_threshold = 0.60\n    #\nelse:\n    # All targets used, for the y=1 is MoA>0 case\n    yh_threshold = 0.693   # v32 Set for 99% precision  \n\n\n# Apply the threshold to get binary predictions\n# Training:\nyp24 = 1.0*(yh24 > yh_threshold)\nyp48 = 1.0*(yh48 > yh_threshold)\nyp72 = 1.0*(yh72 > yh_threshold)\n# Test (Kaggle):\nyp_kag24 = 1.0*(yh_kag24 > yh_threshold)\nyp_kag48 = 1.0*(yh_kag48 > yh_threshold)\nyp_kag72 = 1.0*(yh_kag72 > yh_threshold)\n\n\nprint(\"\")\nave_train_score = accuracy_score(y24, yp24)\nprint(\"24: Using a threshold of {} gives an \".format(yh_threshold)+\n      \"accuracy: {:.2f} %\\n\".format(100.0*ave_train_score))\nave_train_score = accuracy_score(y48, yp48)\nprint(\"48: Using a threshold of {} gives an \".format(yh_threshold)+\n      \"accuracy: {:.2f} %\\n\".format(100.0*ave_train_score))\nave_train_score = accuracy_score(y72, yp72)\nprint(\"72: Using a threshold of {} gives an \".format(yh_threshold)+\n      \"accuracy: {:.2f} %\\n\".format(100.0*ave_train_score))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:28:25.952906Z","iopub.status.busy":"2020-10-06T17:28:25.952127Z","iopub.status.idle":"2020-10-06T17:28:25.95992Z","shell.execute_reply":"2020-10-06T17:28:25.959182Z"},"papermill":{"duration":0.33585,"end_time":"2020-10-06T17:28:25.960068","exception":false,"start_time":"2020-10-06T17:28:25.624218","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Combine all of these into complete: X, y, yh ?  \n# Or look at one by itself...\n# Define the 8 variables to use:\n\nif True:\n    # Combine all together\n    \n    # Train, all Xs\n    X = df_aug_feats[features].copy()\n    \n    # ys for Train, start with all -1:\n    y = np.zeros(len(X)) - 1\n    yh = np.zeros(len(X)) - 1\n    yp = np.zeros(len(X)) - 1\n    # Use the select_train_24, etc to load the separate ys into y:\n    y[select_train_24] = y24\n    y[select_train_48] = y48\n    y[select_train_72] = y72\n    \n    yh[select_train_24] = yh24\n    yh[select_train_48] = yh48\n    yh[select_train_72] = yh72\n\n    yp[select_train_24] = yp24\n    yp[select_train_48] = yp48\n    yp[select_train_72] = yp72\n    \n\n    # Test, all Xs\n    Xkag = df_test_feats[features].copy()\n    \n    # ys for Test, start with all -1:\n    y_kag = np.zeros(len(Xkag)) - 1\n    yh_kag = np.zeros(len(Xkag)) - 1\n    yp_kag = np.zeros(len(Xkag)) - 1\n    # Use the select_kag_24, etc to load the separate ys into y:\n    y_kag[select_kag_24] = y_kag24\n    y_kag[select_kag_48] = y_kag48\n    y_kag[select_kag_72] = y_kag72\n    \n    yh_kag[select_kag_24] = yh_kag24\n    yh_kag[select_kag_48] = yh_kag48\n    yh_kag[select_kag_72] = yh_kag72\n\n    yp_kag[select_kag_24] = yp_kag24\n    yp_kag[select_kag_48] = yp_kag48\n    yp_kag[select_kag_72] = yp_kag72\n    \nelse:\n    # Use a particular one of the 3 cp_times\n    X = X48\n    y = y48\n    yh = yh48\n    yp = yp48\n    \n    Xkag = Xkag48\n    y_kag = y_kag48\n    yh_kag = yh_kag48\n    yp_kag = yp_kag48\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.309664,"end_time":"2020-10-06T17:28:26.581257","exception":false,"start_time":"2020-10-06T17:28:26.271593","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## <a id=\"ROC\">Model Quality and ROC</a>\nBack to <a href=\"#Index\">Index</a>"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:28:27.241615Z","iopub.status.busy":"2020-10-06T17:28:27.240682Z","iopub.status.idle":"2020-10-06T17:28:35.141302Z","shell.execute_reply":"2020-10-06T17:28:35.14069Z"},"papermill":{"duration":8.235122,"end_time":"2020-10-06T17:28:35.141432","exception":false,"start_time":"2020-10-06T17:28:26.90631","status":"completed"},"scrolled":false,"tags":[],"trusted":true},"cell_type":"code","source":"# See how the prediction, yh, compares with the known y values:\n\nroc_area, ysframe = y_yhat_plots(y, yh, title=\"y and y_score\", y_thresh=yh_threshold,\n                       plots_prefix=model_name.upper()+\"_\"+version_str,\n                                return_ysframe_too=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the yh_threshold needed for a given precision.\n# Can then go to 3rd code cell back, set the threshold to desired value, and repeat from there.\n#\nprec_thresh = ysframe[ysframe['Precis'] >  0.99 ].Thresh.min()\n# Set to 3 decimals and increase by 0.001:\nprec_thresh = int(1.0 + 1000.0*prec_thresh)/1000.0\nprec_thresh","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the log-loss if it is a single-target classification\nif (USE_TARG_SUBSET and (\"1: \" in targ_subset_name)):\n    y_ave = sum(y)/len(y)\n    log_loss_dumb = log_loss(y, y_ave + np.zeros(len(y)))\n    log_loss_y_yh = log_loss(y, yh)\n    # These parameters are based on what's used below for a single target:\n    adj1 = 0.98\n    adj0 = max(0.02*y_ave, (sum(y) - sum(yp))/(len(y) - sum(yp)))\n    yh_adj = yp*(adj1-adj0) + adj0\n    log_loss_y_yhadj = log_loss(y, yh_adj)\n    \n    print('\\nFor the single target \"'+targ_subset_name+'\" :')\n    print('\\n  The dumb-guess log-loss, yh = ave(y), would be',log_loss_dumb)\n    print(\"\\n  The ML's yh gives a log-loss of\",log_loss_y_yh)\n    print('\\n  The yh from factors (below) gives a log-loss of',log_loss_y_yhadj,\")\")\n    print('\\n  The score improvement, given the '+str(n_targs)+' targets, is:',\n             int(1.e5*(log_loss_dumb - log_loss_y_yhadj)/n_targs),\"x10^-5   (using the factors value.)\")","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:28:35.781899Z","iopub.status.busy":"2020-10-06T17:28:35.78086Z","iopub.status.idle":"2020-10-06T17:28:35.784459Z","shell.execute_reply":"2020-10-06T17:28:35.783712Z"},"papermill":{"duration":0.323375,"end_time":"2020-10-06T17:28:35.784582","exception":false,"start_time":"2020-10-06T17:28:35.461207","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Keeping track of some single-target performances            Test - Control\n# Precision set to 97% or 95%\n\n#  The tSNE 9:\n#      'proteasome_inhibitor', 'nfkb_inhibitor', 'glucocorticoid_receptor_agonist',\n#      'raf_inhibitor', 'cdk_inhibitor', 'hmgcr_inhibitor',\n#      'egfr_inhibitor', 'hsp_inhibitor', 'tubulin_inhibitor'\n#\n# For the single target \"1: nfkb\" :  The score improvement, given the 206 targets, is: 61 x10^-5\n# For the single target \"1: prot\" :  The score improvement, given the 206 targets, is: 67 x10^-5\n# For the single target \"1: gluc\" :  The score improvement, given the 206 targets, is: 26 x10^-5\n# For the single target \"1: raf_\" :  The score improvement, given the 206 targets, is: 22 x10^-5\n# For the single target \"1: cdk_\" :  The score improvement, given the 206 targets, is: 29 x10^-5     31 - 0\n# For the single target \"1: hmgcr\" : The score improvement, given the 206 targets, is: 26 x10^-5\n# For the single target \"1: egfr\" :  The score improvement, given the 206 targets, is: 30 x10^-5     26 - 0\n# For the single target \"1: hsp_\" :  The score improvement, given the 206 targets, is:  8 x10^-5\n# For the single target \"1: tubu\" :  The score improvement, given the 206 targets, is: 24 x10^-5\n\n#         0.02363\n# Total - 0.00262\n# ~ 0.02110\n\n# [16] Ones y_ave > 0.010, but not in tSNE-9   * not very encouraging *  Need different features?\n#     'acetylcholine_receptor_antagonist', 'adrenergic_receptor_agonist', 'adrenergic_receptor_antagonist',\n#     'calcium_channel_blocker', 'cyclooxygenase_inhibitor', 'dna_inhibitor', 'dopamine_receptor_antagonist',\n#     'flt3_inhibitor', 'glutamate_receptor_antagonist', 'histamine_receptor_antagonist',\n#     'kit_inhibitor', 'pdgfr_inhibitor',\n#     'phosphodiesterase_inhibitor', 'serotonin_receptor_antagonist', 'sodium_channel_inhibitor'\n#\n# For the single target \"1: acety\" :  The score improvement, given the 206 targets, is: 3 x10^-5\n# For the single target \"1: adren_ago\" :  The score improvement, given the 206 targets, is: 4 x10^-5  1 - 1  :(\n# For the single target \"1: adren_ant\" :  The score improvement, given the 206 targets, is: 3 x10^-5\n# For the single target \"1: calci\"  :  The score improvement, given the 206 targets, is: 1 x10^-5\n# . . .\n# For the single target \"1: hista\" :  The score improvement, given the 206 targets, is: 13 x10^-5     15 - 3\n# . . .\n# For the single target \"1: sodiu\" :  The score improvement, given the 206 targets, is: 0 x10^-5\n\n\n# [7] Ones that have 'detectable g-vectors' with > 69 counts\n# For the single target \"1: topoi\" :  The score improvement, given the 206 targets, is:  9 x10^-5\n# For the single target \"1: hdac_\" :  The score improvement, given the 206 targets, is:  6 x10^-5\n# For the single target \"1: mtor_\" :  The score improvement, given the 206 targets, is: 13 x10^-5     15 - 0\n# For the single target \"1: mek_i\" :  The score improvement, given the 206 targets, is:  4 x10^-5      0 - 0 \n# For the single target \"1: pi3k_\" :  The score improvement, given the 206 targets, is:  7 x10^-5      0 - 0\n# For the single target \"1: prote\" :  The score improvement, given the 206 targets, is:  2 x10^-5      3 - 0\n# For the single target \"1: atpas\" :  The score improvement, given the 206 targets, is:  2 x10^-5      4 - 0","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:28:36.420607Z","iopub.status.busy":"2020-10-06T17:28:36.419801Z","iopub.status.idle":"2020-10-06T17:28:38.557677Z","shell.execute_reply":"2020-10-06T17:28:38.558251Z"},"papermill":{"duration":2.460648,"end_time":"2020-10-06T17:28:38.558401","exception":false,"start_time":"2020-10-06T17:28:36.097753","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Plot c-std vs c-ave and show the ML-selected ids\n# Color by the yp=1 (i.e., MoA=0) ones; use color scheme similar to numMoA\ncolors = (1-yp); colors[0]=2.0\n\nX.plot(x='c-ave',y='c-std',kind='scatter', figsize=(12,7),\n                 c=colors, colormap='jet', alpha=0.25, marker='o',s=50,\n                 title='Train:  C-std vs C-ave for all non-control sig_ids'+\n                  '   Colored by ML MoA>0 (blue, yp=1)')\nplt.savefig(\"C-std_vs_C-ave_ML-color_\"+version_str+\".png\")\nplt.show()\n\n# The line-segment ids are the clearest non-MoA ones.","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:28:39.206369Z","iopub.status.busy":"2020-10-06T17:28:39.205268Z","iopub.status.idle":"2020-10-06T17:28:40.344694Z","shell.execute_reply":"2020-10-06T17:28:40.34392Z"},"papermill":{"duration":1.46697,"end_time":"2020-10-06T17:28:40.344818","exception":false,"start_time":"2020-10-06T17:28:38.877848","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Show the yh distribution by known notMoA status (similar to confusion dots output)\n\n# Temporarily ... Add 'soft' predictions and ys to the X dataframe:\nX['yh_preds'] = yh\nX['y_actual'] = y\n\nX.hist('yh_preds', by='y_actual', bins=100, sharex=True, sharey=True, layout=(5,1), figsize=(14,9))\nplt.show()\n\n# Remove the added columns:\nX = X.drop(['y_actual','yh_preds'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.318536,"end_time":"2020-10-06T17:28:40.983642","exception":false,"start_time":"2020-10-06T17:28:40.665106","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Looking at the Test predictions"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:28:41.631325Z","iopub.status.busy":"2020-10-06T17:28:41.630499Z","iopub.status.idle":"2020-10-06T17:28:41.639181Z","shell.execute_reply":"2020-10-06T17:28:41.638406Z"},"papermill":{"duration":0.335873,"end_time":"2020-10-06T17:28:41.639339","exception":false,"start_time":"2020-10-06T17:28:41.303466","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"print(\"There are\",len(y_kag), \"predictions corresponding to the\",len(Xkag), \"test ids;  \",\n         sum(test_ctls),\"of them are controls.\")\nprint(\"The number of y=1 (i.e., MoA>0) predicted values is  \",int(sum(yp_kag)),\n         \"  (threshold =\",yh_threshold,\")\")","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:28:42.291524Z","iopub.status.busy":"2020-10-06T17:28:42.290758Z","iopub.status.idle":"2020-10-06T17:28:43.225902Z","shell.execute_reply":"2020-10-06T17:28:43.224972Z"},"papermill":{"duration":1.266864,"end_time":"2020-10-06T17:28:43.226098","exception":false,"start_time":"2020-10-06T17:28:41.959234","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Show the yh distribution for Test\n\n# Temporarily ... Add 'soft' predictions to the Test features dataframe:\nXkag['yh_kag'] = yh_kag\n\n# Including the controls:\nXkag.hist('yh_kag', bins=100, sharex=True, sharey=True, layout=(5,1), figsize=(14,9))\nplt.show()\n\n# Just the controls:\nXkag[test_ctls].hist('yh_kag', bins=100, sharex=True, sharey=True, layout=(5,1), figsize=(14,9))\nplt.show()\n\n# Find the number of Controls that are incorrectly predicted as y=1.\n# Count the number above the yh_threshold:\nctls_above = sum(Xkag[test_ctls].yh_kag > yh_threshold)\nprint(\"There are\", ctls_above, \"controls predicted as MoA.\\n\")\n\n# Remove the added columns:\nXkag = Xkag.drop(['yh_kag'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.320993,"end_time":"2020-10-06T17:28:43.882935","exception":false,"start_time":"2020-10-06T17:28:43.561942","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## <a id=\"OutputKaggle\">Output Kaggle Predictions</a>\nBack to <a href=\"#Index\">Index</a>"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:28:44.533267Z","iopub.status.busy":"2020-10-06T17:28:44.532513Z","iopub.status.idle":"2020-10-06T17:28:44.684806Z","shell.execute_reply":"2020-10-06T17:28:44.684093Z"},"papermill":{"duration":0.482349,"end_time":"2020-10-06T17:28:44.684922","exception":false,"start_time":"2020-10-06T17:28:44.202573","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Use the submission example as the start of submission\n# Read it in fresh just in case...\ndf_test_targs = pd.read_csv(dat_dir+test_targs)\ndf_test_targs","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:28:45.339322Z","iopub.status.busy":"2020-10-06T17:28:45.337837Z","iopub.status.idle":"2020-10-06T17:28:45.372518Z","shell.execute_reply":"2020-10-06T17:28:45.371837Z"},"papermill":{"duration":0.365367,"end_time":"2020-10-06T17:28:45.372679","exception":false,"start_time":"2020-10-06T17:28:45.007312","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Fill the test_targs with the predicted values.\n# Here we start with the average MoAs-per-row value for each target (column):\n# these are the aves_targs() values determined from training.\nfor icol, this_col in enumerate(aves_targs.index):\n    df_test_targs[this_col] = aves_targs.values[icol] \n    \n# To monitor changes, show the sum over all target values\nprint(\"Sum of targets:\",sum(df_test_targs.drop(columns='sig_id').sum()))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:28:46.162553Z","iopub.status.busy":"2020-10-06T17:28:46.159186Z","iopub.status.idle":"2020-10-06T17:28:58.927673Z","shell.execute_reply":"2020-10-06T17:28:58.92669Z"},"papermill":{"duration":13.228951,"end_time":"2020-10-06T17:28:58.927859","exception":false,"start_time":"2020-10-06T17:28:45.698908","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Go through the row index values of the test CONTROLS       * Inefficient code *\n# and set all of their target values to 0.\nfor irow in df_test_targs[test_ctls].index:\n    df_test_targs.iloc[irow,1:] = np.zeros(206)\n\nprint(\"\\nThe number of test Controls is\",sum(test_ctls),\n      \", out of a total of\",len(df_test_targs),\"test rows.\")\n\n# To monitor changes, show the sum over all target values:\npredicted_moas = sum(df_test_targs.drop(columns='sig_id').sum())\nprint(\"\\n\",\"Sum of targets:\",predicted_moas,\" <-- this is the predicted MoA sum\\n\")\n\n# With the controls now set to zero, this sum is the predicted total number of MoAs.\n# Note that the \"actual\" number of test MoAs (determined sneakily in v13)\n# is about 3125, or about 12.4% higher.\n# (We could scale the predictions by 1.124 to get a better score, but that would be 'wrong'.)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.324579,"end_time":"2020-10-06T17:28:59.580526","exception":false,"start_time":"2020-10-06T17:28:59.255947","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Make use of the classification to adjust the target values\n\nIn v18 and earlier, the rows identified as MoA=0 had their target values\nreduced (divided by 2 to limit the penalty for false positives.)\n\nIn v19+ we will increase the target predictions for the MoA>0 detected rows,\nand reduce the target predictions for the other rows.\n"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:29:00.292358Z","iopub.status.busy":"2020-10-06T17:29:00.291541Z","iopub.status.idle":"2020-10-06T17:29:00.29518Z","shell.execute_reply":"2020-10-06T17:29:00.294459Z"},"papermill":{"duration":0.371356,"end_time":"2020-10-06T17:29:00.295299","exception":false,"start_time":"2020-10-06T17:28:59.923943","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Calculation when all targets are used   (i.e., the case where the subset is all targets)\nif USE_TARG_SUBSET == False:\n    # Values used to determine the target scaling factors:\n    #   M = total number of non-control rows\n    #   N = total number of predicted MoA values.\n    #   m1 = number of predicted MoA>1 rows <-- we'll subtract the number of controls above threshold.\n    # Initially, the sum of the predictions in each row is equal to N/M,\n    # which from the train data is: 16844/21948 ~ 0.767.\n    # If a y=1 row has one MoA (though could be 2...),\n    # then the summed predictions should be increased from N/M to 1.0,\n    # a factor of 1 / (N/M) = M/N.\n    # The other M-m1 rows have N-m1 MoAs in them, for an average of (N-m1)/(M-m1) MoAs/row.\n    # The scaling factor for these rows can also be made as (N-m1)/(M-m1) / (N/M).\n    # Summarizing, the scaling factors are:\n    #  factor(MoA>0) = M/N\n    #  factor(MoA=0) = (M/N)*(N-m1)/(M-m1)\n    # Calculating them:\n    m_rows = len(df_test_targs) - sum(test_ctls)\n    n_moas = predicted_moas\n    m1 = sum(yp_kag) - ctls_above\n    print(\"\\nUsing M, N, and m1 values of:\", m_rows, \",\", n_moas, \",\", m1,\n              \"(corrected for\",ctls_above,\"controls)\\n\")\n\n    factor_1 = m_rows/n_moas\n    factor_0 = (m_rows/n_moas)*(n_moas-m1)/(m_rows-m1)\n\n    print(\"The factors for Moa>0 and MoA=0 are:\", factor_1, \",\", factor_0,\"\\n\")\n\n    # Create a (column) vector of the correction factors based on yp_kag:\n    scale_factors = yp_kag*factor_1 + (1-yp_kag)*factor_0\n\n    # Go through the df and multiply each column by the scale_factors:\n    for this_col in df_test_targs.columns[1:]:\n        df_test_targs[this_col] = scale_factors * df_test_targs[this_col]\n    \n    # To monitor changes, show the sum over all target values:\n    print(sum(df_test_targs.drop(columns='sig_id').sum()))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:29:00.980179Z","iopub.status.busy":"2020-10-06T17:29:00.97903Z","iopub.status.idle":"2020-10-06T17:29:01.064051Z","shell.execute_reply":"2020-10-06T17:29:01.063295Z"},"papermill":{"duration":0.439405,"end_time":"2020-10-06T17:29:01.064211","exception":false,"start_time":"2020-10-06T17:29:00.624806","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Calculating factors when a SUBSET of targets is used to select MoA>0 rows\nif USE_TARG_SUBSET == True:\n    print(\"\\n The [\",targ_subset_name,\"] subset of\",len(targ_subset),\n          \"Targets was used to classify rows as MoA>0:\\n\\n\", \n          targ_subset,\"\\n\")\n\n    # Values used to determine the target scaling factors:\n    print(\"\\nUseful values:\\n\")\n    \n    #   M = total number of non-control rows\n    m_rows = len(df_test_targs) - sum(test_ctls)\n    print(\"M =\", m_rows,\"  Number of non-control rows\")\n    #   m1 = number of detected MoA>0 rows <-- we subtract the number of controls above threshold.\n    m1 = sum(yp_kag) - ctls_above\n    print(\"m1 =\", m1, \"  Number of detected MoA>0 rows (corrected for\",ctls_above,\"controls)\\n\")\n    \n    #   N = prediction for the sum of all MoA values. (from previous calc. above)\n    n_moas = predicted_moas\n    print(\"N = \", n_moas, \"  Predicted sum of all MoA values\")\n    #   Nsub = predicted sum of MoA values in the target subset.\n    nsub_moas = m_rows * df_test_targs.loc[0,targ_subset].sum()\n    print(\"Nsub = \", nsub_moas,\"  Predicted sum of MoAs in the target subset\\n\")\n\n    print(\"   Initial dumb-guess values in the target columns:\")\n    print(df_test_targs.loc[0,targ_subset],\"\\n\\n\")\n    \n    # The current prediction for MoA-per-row in the subset is Nsub/M.\n    # The rows identified as MoA>0 will instead have at least 1.0 MoA-per-row in the subset,\n    # so the current prediction for these rows should be multiplied by the factor:\n    factor_1 = 1.0 / (nsub_moas/m_rows)\n\n    # Because m1 rows have been detected with MoA>0,\n    # the remaining M-m1 rows will have a reduced MoA total of Nsub-m1 (or less).\n    # So the scale factor for these rows in the target columns is\n    factor_0 = (nsub_moas - m1)/(m_rows - m1) / (nsub_moas/m_rows)\n\n    # For the case of \"2: prot...\" and selecting numSub>1 (i.e. detecting 2 or more MoAs)\n    # we expect 2 active MoA in each of the m1 rows; to allow for false positives, etc, instead\n    # we increase the factor_1 by 1.7 and in the factor_0 equation 1.7*m1 moas are removed:\n    if (\"2: pro\" in targ_subset_name):\n        factor_1 = 1.7 / (nsub_moas/m_rows)\n        factor_0 = (nsub_moas - 1.7*m1)/(m_rows - m1) / (nsub_moas/m_rows)\n        # guard against negative values\n        factor_0 = max(factor_0, 0.05)\n    \n    # For the case of \"1: ...\" there is a single target,\n    # so expect exactly 1 active MoA in each of the m1 rows - scale it close to 1.0 \n    if (\"1: \" == targ_subset_name[0:3]):\n        factor_1 = 0.98 / (nsub_moas/m_rows)\n        # Reduce the predictions in the non-m1 rows proportional to the number of m1s found.\n        factor_0 = (nsub_moas - m1)/(m_rows - m1) / (nsub_moas/m_rows)\n        # Guard against going negative, i.e., when m1 is larger than the total expected,\n        # don't go lower than 5% of the dumb-guess rate. (Happens only with proteasome ?)\n        factor_0 = max(factor_0, 0.05)\n    \n    print(\"In the Subset of columns, the factors for the m1 and not-m1 rows are:\\n\",\n          factor_1, \",\", factor_0,\"\\n\")\n    \n    # Apply these factors to the SUBSET columns\n    # Create a (column) vector of the correction factors based on yp_kag:\n    scale_factors = yp_kag*factor_1 + (1-yp_kag)*factor_0\n    \n    # Go through the df and multiply each column IN THE SUBSET by the scale_factors:\n    for this_col in targ_subset:\n        df_test_targs[this_col] = scale_factors * df_test_targs[this_col]\n    \n    \n    # There are also corrections we can apply to the non-SUBSET columns,\n    # both in the m1 rows (reducing their predictions)\n    # and in the non-m1 rows (a small increase in the predictions).\n    # Leave these out for now, v20, to check the main subset-column effect.\n    \n    # Non-subset ones in the MoA>0 rows\n    # For the MoA>0 rows, it's certain that the number of detected MoAs (1+ per row)\n    # is more than the expected number, 0.767 per row. Tempting to set them near 0,\n    # instead lets reduce the the non-subset predictions a bunch:\n    nonsub_1 = 0.33\n    \n    # Non-subset ones in the MoA=0 rows (i.e., non-m1)\n    # These will increase slightly to match what was lost in the nonsub m1 rows\n    nonsub_0 = 1.0+(1.0-nonsub_1)*m1/(m_rows - m1)\n\n    # For the case of \"2: prot...\" and selecting numSub>1 (i.e. detecting 2 or more MoAs)\n    # we decrease nonsub_1 to a much smaller value and nonsub_0 changes appropriately:\n    if (\"2: pro\" in targ_subset_name):\n        nonsub_1 = 0.03\n        nonsub_0 = 1.0+(1.0-nonsub_1)*m1/(m_rows - m1)  \n        \n    print(\"In the Non-subset columns, the factors for the m1 and not-m1 rows are:\\n\",\n          nonsub_1, \",\", nonsub_0,\"\\n\")\n    \n    # Apply these factors to the NOT-in-the-SUBSET columns\n    # Create a (column) vector of the correction factors based on yp_kag:\n    scale_factors = yp_kag*nonsub_1 + (1-yp_kag)*nonsub_0\n    \n    # Go through the df and multiply each column NOT-in-the-SUBSET by the scale_factors:\n    col_list = list(df_test_targs.columns[1:])\n    for sub_col in targ_subset:\n        col_list.remove(sub_col)\n    #\n    for this_col in col_list:\n        df_test_targs[this_col] = scale_factors * df_test_targs[this_col]\n\n    \n    # To monitor changes, show the sum over all target values:\n    print(\"Sum of targets:\",sum(df_test_targs.drop(columns='sig_id').sum()))\n    # It stays the same because we've just redistributed the MoA values within the subset.","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:29:01.728544Z","iopub.status.busy":"2020-10-06T17:29:01.727416Z","iopub.status.idle":"2020-10-06T17:29:01.767122Z","shell.execute_reply":"2020-10-06T17:29:01.767776Z"},"papermill":{"duration":0.375096,"end_time":"2020-10-06T17:29:01.767944","exception":false,"start_time":"2020-10-06T17:29:01.392848","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Show the df\ndf_test_targs","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-16T12:39:38.757833Z","iopub.status.busy":"2020-09-16T12:39:38.742298Z","iopub.status.idle":"2020-09-16T12:39:38.780277Z","shell.execute_reply":"2020-09-16T12:39:38.77966Z"},"papermill":{"duration":0.328851,"end_time":"2020-10-06T17:29:02.465161","exception":false,"start_time":"2020-10-06T17:29:02.13631","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:29:03.128023Z","iopub.status.busy":"2020-10-06T17:29:03.126435Z","iopub.status.idle":"2020-10-06T17:29:05.243588Z","shell.execute_reply":"2020-10-06T17:29:05.242884Z"},"papermill":{"duration":2.450781,"end_time":"2020-10-06T17:29:05.243745","exception":false,"start_time":"2020-10-06T17:29:02.792964","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Save the result as the submission\ndf_test_targs.to_csv(\"submission.csv\",index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:29:05.899908Z","iopub.status.busy":"2020-10-06T17:29:05.899171Z","iopub.status.idle":"2020-10-06T17:29:05.904161Z","shell.execute_reply":"2020-10-06T17:29:05.903544Z"},"papermill":{"duration":0.33527,"end_time":"2020-10-06T17:29:05.904298","exception":false,"start_time":"2020-10-06T17:29:05.569028","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# that's all.\n##!head -10 submission.csv","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:29:06.570918Z","iopub.status.busy":"2020-10-06T17:29:06.569827Z","iopub.status.idle":"2020-10-06T17:29:06.572776Z","shell.execute_reply":"2020-10-06T17:29:06.573314Z"},"papermill":{"duration":0.336304,"end_time":"2020-10-06T17:29:06.573474","exception":false,"start_time":"2020-10-06T17:29:06.23717","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"##!tail -10 submission.csv","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-06T17:29:07.227888Z","iopub.status.busy":"2020-10-06T17:29:07.226961Z","iopub.status.idle":"2020-10-06T17:29:07.230597Z","shell.execute_reply":"2020-10-06T17:29:07.231304Z"},"papermill":{"duration":0.333825,"end_time":"2020-10-06T17:29:07.231467","exception":false,"start_time":"2020-10-06T17:29:06.897642","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# show/confirm the random seed value\nprint(\"Used RANDOM_SEED = {}\".format(RANDOM_SEED))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.326054,"end_time":"2020-10-06T17:29:07.882795","exception":false,"start_time":"2020-10-06T17:29:07.556741","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## <a id=\"TheEnd\">The End</a>\nBack to <a href=\"#Index\">Index</a>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}