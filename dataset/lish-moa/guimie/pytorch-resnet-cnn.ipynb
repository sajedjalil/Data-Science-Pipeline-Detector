{"cells":[{"metadata":{},"cell_type":"markdown","source":"参考：\n\nhttps://www.kaggle.com/iamabhishekdas/moa-prediction-pytorch-cnn\n\nhttps://www.kaggle.com/kushal1506/moa-pytorch-feature-engineering-0-01846\n\n可以参考：[PyTorch Starter (U-Net ResNet)](https://www.kaggle.com/ateplyuk/pytorch-starter-u-net-resnet/data)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/iterative-stratification/iterative_stratification-0.1.6-py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport random\nimport csv\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils import data\nfrom torchvision import models\n\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import log_loss\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\nfrom matplotlib import pyplot as plt\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features0 = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\ntest_features0 = pd.read_csv('../input/lish-moa/test_features.csv')\nsubmission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RankGauss\n\nGENES = [col for col in train_features0.columns if col.startswith('g-')]\nCELLS = [col for col in train_features0.columns if col.startswith('c-')]\n\nfor col in (GENES + CELLS):\n\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features0[col].values)\n    vec_len_test = len(test_features0[col].values)\n    raw_vec = train_features0[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features0[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features0[col] = transformer.transform(test_features0[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fe_stats(train, test, extra=True):\n    \n    features_g = [col for col in train.columns if 'g-' in col]\n    features_c = [col for col in train.columns if 'c-' in col]\n\n    for df in [train, test]:\n        df['g_sum'] = df[features_g].sum(axis=1)\n        df['g_mean'] = df[features_g].mean(axis=1)\n        df['g_std'] = df[features_g].std(axis=1)\n        df['g_kurt'] = df[features_g].kurtosis(axis=1)\n        df['g_skew'] = df[features_g].skew(axis=1)\n        df['g_std'] = df[features_g].std(axis=1)\n        df['c_sum'] = df[features_c].sum(axis=1)\n        df['c_mean'] = df[features_c].mean(axis=1)\n        df['c_std'] = df[features_c].std(axis=1)\n        df['c_kurt'] = df[features_c].kurtosis(axis=1)\n        df['c_skew'] = df[features_c].skew(axis=1)\n        df['c_std'] = df[features_c].std(axis=1)\n        df['gc_sum'] = df[features_g + features_c].sum(axis=1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis=1)\n        df['gc_std'] = df[features_g + features_c].std(axis=1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis=1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis=1)\n        df['gc_std'] = df[features_g + features_c].std(axis=1)\n        \n        # \n        if extra:\n            df['g_sum-c_sum'] = df['g_sum'] - df['c_sum']\n            df['g_mean-c_mean'] = df['g_mean'] - df['c_mean']\n            df['g_std-c_std'] = df['g_std'] - df['c_std']\n            df['g_kurt-c_kurt'] = df['g_kurt'] - df['c_kurt']\n            df['g_skew-c_skew'] = df['g_skew'] - df['c_skew']\n            \n            df['g_sum*c_sum'] = df['g_sum'] * df['c_sum']\n            df['g_mean*c_mean'] = df['g_mean'] * df['c_mean']\n            df['g_std*c_std'] = df['g_std'] * df['c_std']\n            df['g_kurt*c_kurt'] = df['g_kurt'] * df['c_kurt']\n            df['g_skew*c_skew'] = df['g_skew'] * df['c_skew']\n            \n            df['g_sum/c_sum'] = df['g_sum'] / df['c_sum']\n            df['g_mean/c_mean'] = df['g_mean'] / df['c_mean']\n            df['g_std/c_std'] = df['g_std'] / df['c_std']\n            df['g_kurt/c_kurt'] = df['g_kurt'] / df['c_kurt']\n            df['g_skew/c_skew'] = df['g_skew'] / df['c_skew']\n        \n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fe_pca(train, test, n_components_g=50, n_components_c=10, SEED=123):\n    \n    features_g = [col for col in train.columns if 'g-' in col]\n    features_c = [col for col in train.columns if 'c-' in col]\n    \n    def create_pca(train, test, features, kind='g', n_components=n_components_g):\n        \n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        \n        # data = pd.concat([train_, test_], axis=0)\n        pca = PCA(n_components=n_components,  random_state=SEED)\n        # data = pca.fit_transform(data)\n        data1 = pca.fit_transform(train_)\n        data2 = pca.transform(test_)\n        data = np.concatenate((data1, data2), axis=0)\n\n        \n        columns = [f'pca_{kind}{i + 1}' for i in range(n_components)]\n        data = pd.DataFrame(data, columns=columns)\n        train_ = data.iloc[:train.shape[0]]\n        test_ = data.iloc[train.shape[0]:].reset_index(drop=True)\n        train = pd.concat([train.reset_index(drop=True),\n                           train_.reset_index(drop=True)], axis=1)\n        test = pd.concat([test.reset_index(drop=True),\n                          test_.reset_index(drop=True)], axis=1)\n        return train, test\n    \n    train, test = create_pca(train, test, features_g, kind='g', n_components=n_components_g)\n    train, test = create_pca(train, test, features_c, kind='c', n_components=n_components_c)\n    \n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\ndef fe_cluster(train, test, n_clusters_g = 15, n_clusters_c = 5, SEED = 123):\n    \n    features_g = [col for col in train.columns if 'g-' in col]\n    features_c = [col for col in train.columns if 'c-' in col]\n    \n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        \n        # StandardScaler\n        scaler = preprocessing.StandardScaler()\n        train_.iloc[:, :] = scaler.fit_transform(train_)\n        test_.iloc[:, :] = scaler.transform(test_)\n\n        #data = pd.concat([train_, test_], axis = 0)\n        #kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n        #train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n        #test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n        \n        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(train_)\n        train[f'clusters_{kind}'] = kmeans.predict(train_.values)\n        test[f'clusters_{kind}'] = kmeans.predict(test_.values)\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\n\ndef fe_feature_selection(train_features0, test_features0, threshold=0.8):\n    var_thresh = VarianceThreshold(threshold)  #<-- Update\n    data = train_features0.append(test_features0)\n    data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n    cols = test_features0.iloc[:, 4:].columns[var_thresh.get_support()] # 获取保留下来的列名\n    \n    train_features0_transformed = pd.DataFrame(data_transformed[ : train_features0.shape[0]], columns=cols)\n    test_features0_transformed = pd.DataFrame(data_transformed[-test_features0.shape[0] : ], columns=cols)\n    \n    \n    train_features0 = pd.DataFrame(train_features0[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n        columns=['sig_id','cp_type','cp_time','cp_dose'])\n    train_features0 = pd.concat([train_features0, train_features0_transformed], axis=1)\n\n\n    test_features0 = pd.DataFrame(test_features0[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n        columns=['sig_id','cp_type','cp_time','cp_dose'])\n    test_features0 = pd.concat([test_features0, test_features0_transformed], axis=1)\n\n    return train_features0, test_features0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## data"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoATrainDataset():\n\n    def __init__(self, train_data, train_labels, feature_cols):\n\n        super(MoATrainDataset).__init__()\n        pad_size_l = int((32*32-len(feature_cols))//2)\n        if (32*32-len(feature_cols))%2==0:\n            pad_size_r = pad_size_l\n        else:\n            pad_size_r = pad_size_l + 1\n        self.X = np.pad(train_data, ((0,0),(pad_size_l, pad_size_r)), 'constant', \n                        constant_values=(0)).reshape(-1,1,32,32)\n        \n        self.X = torch.from_numpy(self.X).float()\n        self.Y = torch.from_numpy(train_labels).float()\n        \n            \n    def __getitem__(self, index):\n        \n        image = self.X[index]\n        label= self.Y[index]\n        \n        dct = {'x': image,\n              'y': label}\n\n        return dct\n        \n    def __len__(self):\n        return len(self.X)\n    \n\n\nclass MoATestDataset():    \n\n    def __init__(self, test_data, feature_cols):\n\n        super(MoATestDataset).__init__()\n        pad_size_l = int((32*32-len(feature_cols))/2)\n        if (32*32-len(feature_cols))%2==0:\n            pad_size_r = pad_size_l\n        else:\n            pad_size_r = pad_size_l + 1\n        self.X = np.pad(test_data, ((0,0),(pad_size_l, pad_size_r)), 'constant', \n                        constant_values=(0)).reshape(-1,1,32,32)    \n        \n        self.X = torch.from_numpy(self.X).float()\n        \n            \n    def __getitem__(self, index):\n        \n        image = self.X[index]\n        dct = {'x': image}\n        return dct\n        \n    def __len__(self):\n        return len(self.X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train() #  set the model to training mode\n    final_loss = 0  # Initialise final loss to zero\n    \n    for data in dataloader:\n        optimizer.zero_grad() #every time we use the gradients to update the parameters, we need to zero the gradients afterwards\n        inputs, targets = data['x'].to(device), data['y'].to(device) #Sending data to GPU(cuda) if gpu is available otherwise CPU\n        outputs = model(inputs) #output \n        loss = loss_fn(outputs, targets) #loss function\n        loss.backward() #compute gradients(work its way BACKWARDS from the specified loss)\n        optimizer.step()  #gradient optimisation\n        scheduler.step() \n        \n        final_loss += loss.item() #Final loss\n        \n    final_loss /= len(dataloader) #average loss\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval() #  set the model to evaluation/validation mode\n    final_loss = 0 # Initialise validation final loss to zero\n    valid_preds = [] #Empty list for appending prediction\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device) #Sending data to GPU(cuda) if gpu is available otherwise CPU\n        outputs = model(inputs) #output\n        loss = loss_fn(outputs, targets) #loss calculation\n        \n        final_loss += loss.item() #final validation loss\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy()) # get CPU tensor as numpy array # cannot get GPU tensor as numpy array directly\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds) #concatenating predictions under valid_preds\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():  # need to use NO_GRAD to keep the update out of the gradient computation\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy()) \n        \n    preds = np.concatenate(preds)\n    \n    return preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## model"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nProperly implemented ResNet-s for CIFAR10 as described in paper [1].\nThe implementation and structure of this file is hugely influenced by [2]\nwhich is implemented for ImageNet and doesn't have option A for identity.\nMoreover, most of the implementations on the web is copy-paste from\ntorchvision's resnet and has wrong number of params.\nProper ResNet-s for CIFAR10 (for fair comparision and etc.) has following\nnumber of layers and parameters:\nname      | layers | params\nResNet20  |    20  | 0.27M\nResNet32  |    32  | 0.46M\nResNet44  |    44  | 0.66M\nResNet56  |    56  | 0.85M\nResNet110 |   110  |  1.7M\nResNet1202|  1202  | 19.4m\nwhich this implementation indeed has.\nReference:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n[2] https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\nIf you use this implementation in you work, please don't forget to mention the\nauthor, Yerlan Idelbayev.\n'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\n\nfrom torch.autograd import Variable\n\n__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n\ndef _weights_init(m):\n    classname = m.__class__.__name__\n    #print(classname)\n    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n        init.kaiming_normal_(m.weight)\n\nclass LambdaLayer(nn.Module):\n    def __init__(self, lambd):\n        super(LambdaLayer, self).__init__()\n        self.lambd = lambd\n\n    def forward(self, x):\n        return self.lambd(x)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1, option='A'):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != planes:\n            if option == 'A':\n                \"\"\"\n                For CIFAR10 ResNet paper uses option A.\n                \"\"\"\n                self.shortcut = LambdaLayer(lambda x:\n                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n            elif option == 'B':\n                self.shortcut = nn.Sequential(\n                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n                     nn.BatchNorm2d(self.expansion * planes)\n                )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_planes = 32 #16\n\n        #self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n        #self.bn1 = nn.BatchNorm2d(16)\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.layer1 = self._make_layer(block, 32, num_blocks[0], stride=1)\n        #self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n        self.linear = nn.Linear(64, num_classes)\n\n        self.apply(_weights_init)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        #out = self.layer2(out)\n        out = self.layer3(out)\n        out = F.avg_pool2d(out, out.size()[3])\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef resnet20(num_classes):\n    return ResNet(BasicBlock, [3, 3, 3], num_classes)\n\n\ndef resnet32(num_classes):\n    return ResNet(BasicBlock, [5, 5, 5], num_classes)\n\n\ndef resnet44(num_classes):\n    return ResNet(BasicBlock, [7, 7, 7], num_classes)\n\n\ndef resnet56(num_classes):\n    return ResNet(BasicBlock, [9, 9, 9], num_classes)\n\n\ndef resnet110(num_classes):\n    return ResNet(BasicBlock, [18, 18, 18], num_classes)\n\n\ndef resnet1202(num_classes):\n    return ResNet(BasicBlock, [200, 200, 200], num_classes)\n\n\ndef test(net):\n    import numpy as np\n    total_params = 0\n\n    for x in filter(lambda p: p.requires_grad, net.parameters()):\n        total_params += np.prod(x.data.numpy().shape)\n    print(\"Total number of params\", total_params)\n    print(\"Total layers\", len(list(filter(lambda p: p.requires_grad and len(p.data.size())>1, net.parameters()))))\n\n\n# if __name__ == \"__main__\":\n#     for net_name in __all__:\n#         if net_name.startswith('resnet'):\n#             print(net_name)\n#             test(globals()[net_name]())\n#             print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Label Smoothing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Smoothing Regularization(LSR) is a trick to overcome overfitting and \n# reduce the ability of the model to adapt.\n# it is similar to CrossEntropyLoss in Tensorflow.\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(fold, seed):\n    \n    seed_everything(seed) #seeding\n    \n    #train = process_data(folds) #converting train set categorical columns\n    #test_ = process_data(test) #converting test set categorical columns\n    \n    \n    trn_idx = folds[folds['kfold'] != fold].index\n    val_idx = folds[folds['kfold'] == fold].index\n    \n    #cross validation (splitting randomly train and validation set )\n    train_df = folds[folds['kfold'] != fold].reset_index(drop=True) \n    valid_df = folds[folds['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoATrainDataset(x_train, y_train, feature_cols) \n    valid_dataset = MoATrainDataset(x_valid, y_valid, feature_cols)\n    # train batch optimization with specified \n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True) \n    #specified batch size, mini-batch (of size BATCH_SIZE), one epoch has N/BATCH_SIZE updates\n    #optimization with random batches of size BATCH_SIZE for validation set\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False) \n    \n    \n    model = resnet32(num_classes=206)\n    model.to(device)\n    \n    #using adam optimizer for optimization\n    optimizer = torch.optim.Adam(model.parameters(), \n                                 lr=LEARNING_RATE, \n                                 weight_decay=WEIGHT_DECAY)\n    #learning rate scheduler, fit one cycle\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, \n                                              pct_start=0.1, \n                                              div_factor=1e3, \n                                              max_lr=1e-2, \n                                              epochs=EPOCHS, \n                                              steps_per_epoch=len(trainloader)) \n    #Binary Cross entropy loss\n    loss_fn = nn.BCEWithLogitsLoss() \n    loss_tr = SmoothBCEwLogits(smoothing =0.0015) \n    \n    # early stopping to prevent overfitting and computational time\n    early_stopping_steps = EARLY_STOPPING_STEPS  \n    early_step = 0 \n   \n    oof = np.zeros((len(folds), len(target_cols)))\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, device) #training loss\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, device) #validation loss\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    x_test = test_features[feature_cols].values\n    testdataset = MoATestDataset(x_test, feature_cols)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = resnet32(num_classes=206)\n    model.to(device)\n    \n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(device)\n    \n    predictions = np.zeros((len(test_features), len(target_cols)))\n    predictions = inference_fn(model, testloader, device)\n    \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(folds), len(target_cols)))\n    predictions = np.zeros((len(test_features), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ / NFOLDS\n        oof += oof_\n        \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## setting"},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS =50\nBATCH_SIZE = 256 #128\nLEARNING_RATE = 0.001\nWEIGHT_DECAY = 1e-5\nNFOLDS = 7\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\nADD_PCA = True\nADD_STATS = False\nADD_CLUSTER = False\nIS_FE_SELECT = True\nPCA_G_FEATS = 50\nPCA_C_FEATS = 10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MultilabelStratifiedKFold"},{"metadata":{"trusted":true},"cell_type":"code","source":"# kfold\nif ADD_STATS:\n    train_features0, test_features0 = fe_stats(train_features0, test_features0, extra=True)\nif ADD_PCA:\n    train_features0, test_features0 = fe_pca(train_features0, test_features0, \n                                           n_components_g=PCA_G_FEATS, \n                                           n_components_c=PCA_C_FEATS, \n                                           SEED=123)\nif ADD_CLUSTER:\n    train_features0, test_features0 = fe_cluster(train_features0, test_features0)\nif IS_FE_SELECT:\n    train_features0, test_features0 = fe_feature_selection(train_features0, test_features0, threshold=0.8)\n    \nprint(train_features0.shape)\n\n\n# 裁剪\ntrain_features = train_features0.iloc[:,4:]\ntest_features = test_features0.iloc[:,4:]\n\ntarget_cols = train_targets_scored.drop('sig_id', axis=1).columns.values.tolist()\nprint(train_features.shape)\nprint(len(target_cols))\ntrain_data = train_features.copy()\ntrain_labels = train_targets_scored.iloc[:,1:]\n\nfolds = train_data.copy()\nmlsk = MultilabelStratifiedKFold(n_splits=7)\nfor f, (t_idx, v_idx) in enumerate(mlsk.split(X=train_data, y=train_labels)):\n    folds.loc[v_idx, 'kfold'] = int(f)\nfolds['kfold'] = folds['kfold'].astype(int)\nfolds.head()\n\n# # StandardScaler\n# scaler = preprocessing.StandardScaler()\n# folds.iloc[:, :-1] = scaler.fit_transform(train_features)\n# test_features.iloc[:, :] = scaler.transform(test_features)\n\n\n\n# Max-Min Scaler\nscaler = preprocessing.MinMaxScaler()\nfolds.iloc[:, :-1] = scaler.fit_transform(train_features)\ntest_features.iloc[:, :] = scaler.transform(test_features)\n\nfolds = pd.concat([folds, train_labels], axis=1)\n\n\n        \nfeature_cols = [c for c in test_features.columns]\nprint(len(feature_cols))\nfolds.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Averaging on multiple SEEDS\n\nSEED = [0]#, 1, 2]\noof = np.zeros((len(folds), len(target_cols)))\npredictions = np.zeros((len(test_features), len(target_cols)))\n\nfor seed in SEED:\n    \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features0[target_cols] = oof\ntest_features0[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train_features0[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ / len(target_cols) #target.shape[1]\n    \nprint(\"CV log_loss: \", score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = submission.drop(columns=target_cols).merge(test_features0[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}