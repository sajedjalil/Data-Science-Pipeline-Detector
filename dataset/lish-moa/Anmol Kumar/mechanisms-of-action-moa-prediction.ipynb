{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install /kaggle/input/whl-datasets/iterative_stratification-0.1.6-py3-none-any.whl\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport seaborn as sns\nfrom numpy import mean, std\nimport seaborn as sns\nfrom matplotlib import *\nfrom matplotlib import pyplot as plt\nimport matplotlib.patches as mpatches\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nimport tensorflow_addons as tfa\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\nfrom tqdm.notebook import tqdm\n\nimport warnings\nwarnings.simplefilter(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"trainFeatures = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ntestFeatures = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\ntrainTargetsS = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ntrainTargetsNS = pd.read_csv('/kaggle/input/lish-moa/train_targets_nonscored.csv')\nsubmission = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\n\nfor file in (trainFeatures, testFeatures, trainTargetsS, trainTargetsNS):\n    file.columns = file.columns.str.lower().str.strip().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n    \nprint('Train features shape: ', trainFeatures.shape)\nprint('Test features shape: ', testFeatures.shape)\nprint('Train Targets (Scored) shape: ', trainTargetsS.shape)\nprint('Train Targets (Non Scored) shape: ', trainTargetsNS.shape)\n\nprint('------- Train Features view -------')\ntrainFeatures.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('------- Train Targets (Scored) view -------')\ntrainTargetsS.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('------- Train Targets (Non Scored) view -------')\ntrainTargetsNS.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The Sample Treatment is heavily skewed:\n\nprint('Compound Treatment', round(trainFeatures['cp_type'].value_counts()[0]/len(trainFeatures) * 100, 2), '% of the dataset')\nprint('Control Perturbation Treatment', round(trainFeatures['cp_type'].value_counts()[1]/len(trainFeatures) * 100, 2), '% of the dataset')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot('cp_type', data = trainFeatures, palette = colors)\nplt.title('Sample Treatment Distribution \\n (trt_cp: Compound Treatment || clt_vehicle: Control Perturbation Treatment)', fontsize = 14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The Treatment Dose is almost equally distributed:\n\nprint('High Dose', round(trainFeatures['cp_dose'].value_counts()[0]/len(trainFeatures) * 100, 2), '% of the dataset')\nprint('Low Dose', round(trainFeatures['cp_dose'].value_counts()[1]/len(trainFeatures) * 100, 2), '% of the dataset')\n\ncolors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot('cp_dose', data = trainFeatures, palette = colors)\nplt.title('Treatment Dose Distribution \\n (High Dose || Low Dose)', fontsize = 14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The Treatment Duration is almost equally distributed:\n\nduration = pd.DataFrame(trainFeatures['cp_time'].value_counts()).reset_index()\n\nprint('24 hrs. of treatment doses', round(duration.loc[0][1]/len(trainFeatures) * 100, 2), '% of the dataset')\nprint('48 hrs. of treatment doses', round(duration.loc[1][1]/len(trainFeatures) * 100, 2), '% of the dataset')\nprint('72 hrs. of treatment doses', round(duration.loc[2][1]/len(trainFeatures) * 100, 2), '% of the dataset')\n\ncolors = [\"#0101DF\", \"#DF0101\", \"#008000\"]\n\nsns.countplot('cp_time', data = trainFeatures, palette = colors)\nplt.title('Treatment Duration Distribution \\n (24 || 48 || 72 units of hours)', fontsize = 14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 4, figsize = (22, 4))\n\ngene3 = trainFeatures['g-3'].values\ngene2 = trainFeatures['g-2'].values\ngene1 = trainFeatures['g-1'].values\ngene0 = trainFeatures['g-0'].values\n\nsns.distplot(gene3, ax = ax[0], color = 'r')\nax[0].set_title('Distribution of gene3', fontsize = 14)\nax[0].set_xlim([min(gene3), max(gene3)])\n\nsns.distplot(gene2, ax = ax[1], color = 'b')\nax[1].set_title('Distribution of gene2', fontsize = 14)\nax[1].set_xlim([min(gene2), max(gene2)])\n\nsns.distplot(gene1, ax = ax[2], color = 'g')\nax[2].set_title('Distribution of gene1', fontsize = 14)\nax[2].set_xlim([min(gene1), max(gene1)])\n\nsns.distplot(gene0, ax = ax[3], color = 'y')\nax[3].set_title('Distribution of gene0', fontsize = 14)\nax[3].set_xlim([min(gene0), max(gene0)])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation**: Gene variables' means have a close to normal distribution, with very slight skewness in most of the gene variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Skewness = pd.DataFrame(trainFeatures.skew()).reset_index()\nSkewness.columns = ['column', 'skewness']\nSkewness = Skewness.sort_values('skewness')\nSkewness.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cell_cols = [col for col in trainFeatures if col.startswith('c-')]\ngene_cols = [col for col in trainFeatures if col.startswith('g-')]\n\ncellMeans = trainFeatures[cell_cols].mean()\ncellMeans = pd.DataFrame(cellMeans).reset_index()\ncellMeans.columns = ['column', 'mean']\ngeneMeans = trainFeatures[gene_cols].mean()\ngeneMeans = pd.DataFrame(geneMeans).reset_index()\ngeneMeans.columns = ['column', 'mean']\n\n\nfig, ax = plt.subplots(1, 2, figsize = (22, 4))\n\ncell = cellMeans['mean'].values\ngene = geneMeans['mean'].values\n\nsns.distplot(cell, ax = ax[0], color = 'r')\nax[0].set_title('Distribution of means of cell variables', fontsize = 14)\nax[0].set_xlim([min(cell), max(cell)])\n\nsns.distplot(gene, ax = ax[1], color = 'b')\nax[1].set_title('Distribution of means of gene variables', fontsize = 14)\nax[1].set_xlim([min(gene), max(gene)])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cellMins = trainFeatures[cell_cols].max()\ncellMins = pd.DataFrame(cellMins).reset_index()\ncellMins.columns = ['column', 'MAX']\ngeneMins = trainFeatures[gene_cols].max()\ngeneMins = pd.DataFrame(geneMins).reset_index()\ngeneMins.columns = ['column', 'MAX']\n\n\nfig, ax = plt.subplots(1, 2, figsize = (22, 4))\n\ncell = cellMins['MAX'].values\ngene = geneMins['MAX'].values\n\nsns.distplot(cell, ax = ax[0], color = 'r')\nax[0].set_title('Distribution of maximums of cell variables', fontsize = 14)\nax[0].set_xlim([min(cell), max(cell)])\n\nsns.distplot(gene, ax = ax[1], color = 'b')\nax[1].set_title('Distribution of maximums of gene variables', fontsize = 14)\nax[1].set_xlim([min(gene), max(gene)])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation**: \n- The means of cells and genes have close to normal distributions. \n- The max of cells are centered around 3.5 - 4 and 10 that of genes","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Visualising different types of target columns:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20, 6))\ntargetTypes = []\nfor column in trainTargetsS.columns:\n    try:\n        targetTypes.append(column.rsplit('_', 1)[1])\n    except:\n        targetTypes.append(column.rsplit('_', 1)[0])\ntargetTypes = list(dict.fromkeys(targetTypes))\n\ntargetTypes.remove('id')\ntargetTypes.remove('b')\n\ntargets = {}\nfor i in targetTypes:\n    targets[i] = 0\n\nfor column in trainTargetsS.columns:\n    try:\n        col = column.rsplit('_', 1)[1]\n        if col not in ['id', 'b']:\n            targets[col] += 1\n        else: pass\n                \n    except:\n        col = column.rsplit('_', 1)[0]\n        if col not in ['id', 'b']:\n            targets[col] += 1\n        else: pass\ntargets = pd.DataFrame.from_dict(targets, orient = 'index')\ntargets = targets.reset_index()\ntargets.columns = ['target_type', 'types']\nsns.barplot(x = 'target_type', y = 'types', data = targets)\nplt.xticks(rotation = 90)\nplt.title('Types of Target variables')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Below are the most common target classes:\n\n- Inhibotor\n- Agonist\n- Antagonist\n- Activator\n- Analgesic","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = trainFeatures.corr()\ncorrelationsM = correlations.abs()\nkot = correlations[(correlationsM >= .9) & (correlationsM != 1)]\nkot = kot.dropna(axis = 1, how = 'all')\nkot = kot.dropna(axis = 0, how = 'all')\nplt.figure(figsize = (20, 20))\nsns.heatmap(kot, cmap = \"Greens\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(df):\n    df = df.copy()\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del df['sig_id']\n    return df\n\ntrain = preprocess(trainFeatures)\ntest = preprocess(testFeatures)\n\ndel trainTargetsS['sig_id']\n\ntrainTargetsS = trainTargetsS.loc[train['cp_type'] == 0].reset_index(drop = True)\ntrain = train.loc[train['cp_type'] == 0].reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model based upon: https://www.kaggle.com/simakov/keras-multilabel-neural-network-v1-2/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(num_columns):\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(num_columns),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.2),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1048, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, activation=\"sigmoid\"))\n    ])\n    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period = 10),\n                  loss = 'binary_crossentropy',\n                  )\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 42\n\nfrom typing import Tuple, List, Callable, Any\n\nfrom sklearn.utils import check_random_state  # type: ignore\n\n### from eli5\ndef iter_shuffled(X, columns_to_shuffle=None, pre_shuffle=False,\n                  random_state=None):\n    rng = check_random_state(random_state)\n\n    if columns_to_shuffle is None:\n        columns_to_shuffle = range(X.shape[1])\n\n    if pre_shuffle:\n        X_shuffled = X.copy()\n        rng.shuffle(X_shuffled)\n\n    X_res = X.copy()\n    for columns in tqdm(columns_to_shuffle):\n        if pre_shuffle:\n            X_res[:, columns] = X_shuffled[:, columns]\n        else:\n            rng.shuffle(X_res[:, columns])\n        yield X_res\n        X_res[:, columns] = X[:, columns]\n\n\n\ndef get_score_importances(\n        score_func,  # type: Callable[[Any, Any], float]\n        X,\n        y,\n        n_iter=5,  # type: int\n        columns_to_shuffle=None,\n        random_state=None\n    ):\n    rng = check_random_state(random_state)\n    base_score = score_func(X, y)\n    scores_decreases = []\n    for i in range(n_iter):\n        scores_shuffled = _get_scores_shufled(\n            score_func, X, y, columns_to_shuffle=columns_to_shuffle,\n            random_state=rng, base_score=base_score\n        )\n        scores_decreases.append(scores_shuffled)\n\n    return base_score, scores_decreases\n\n\n\ndef _get_scores_shufled(score_func, X, y, base_score, columns_to_shuffle=None,\n                        random_state=None):\n    Xs = iter_shuffled(X, columns_to_shuffle, random_state=random_state)\n    res = []\n    for X_shuffled in Xs:\n        res.append(-score_func(X_shuffled, y) + base_score)\n    return res\n\ndef metric(y_true, y_pred):\n    metrics = []\n    for i in range(y_pred.shape[1]):\n        if y_true[:, i].sum() > 1:\n            metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float)))\n    return np.mean(metrics)   \n\nperm_imp = np.zeros(train.shape[1])\nall_res = []\nfor n, (tr, te) in enumerate(KFold(n_splits=7, random_state=0, shuffle=True).split(trainTargetsS)):\n    print(f'Fold {n}')\n\n    model = create_model(len(train.columns))\n    checkpoint_path = f'repeat:{seed}_Fold:{n}.hdf5'\n    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n    cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True,\n                                     save_weights_only = True, mode = 'min')\n    model.fit(train.values[tr],\n                  trainTargetsS.values[tr],\n                  validation_data=(train.values[te], trainTargetsS.values[te]),\n                  epochs=35, batch_size=128,\n                  callbacks=[reduce_lr_loss, cb_checkpt], verbose=2\n                 )\n        \n    model.load_weights(checkpoint_path)\n        \n    def _score(X, y):\n        pred = model.predict(X)\n        return metric(y, pred)\n\n    base_score, local_imp = get_score_importances(_score, train.values[te], trainTargetsS.values[te], n_iter=1, random_state=0)\n    all_res.append(local_imp)\n    perm_imp += np.mean(local_imp, axis=0)\n    print('')\n    \ntop_feats = np.argwhere(perm_imp < 0).flatten()\ntop_feats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def metric(y_true, y_pred):\n    metrics = []\n    for _target in trainTargetsS.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels = [0,1]))\n    return np.mean(metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_STARTS = 7\ntf.random.set_seed(42)\n\nres = trainTargetsS.copy()\nsubmission.loc[:, trainTargetsS.columns] = 0\nres.loc[:, trainTargetsS.columns] = 0\n\nfor seed in range(N_STARTS):\n    for n, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits=7, random_state=seed, shuffle=True).split(trainTargetsS, trainTargetsS)):\n        print(f'Fold {n}')\n    \n        model = create_model(len(top_feats))\n        checkpoint_path = f'repeat:{seed}_Fold:{n}.hdf5'\n        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n        cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True,\n                                     save_weights_only = True, mode = 'min')\n        model.fit(train.values[tr][:, top_feats],\n                  trainTargetsS.values[tr],\n                  validation_data=(train.values[te][:, top_feats], trainTargetsS.values[te]),\n                  epochs=35, batch_size=128,\n                  callbacks=[reduce_lr_loss, cb_checkpt], verbose=2\n                 )\n        \n        model.load_weights(checkpoint_path)\n        test_predict = model.predict(test.values[:, top_feats])\n        val_predict = model.predict(train.values[te][:, top_feats])\n        \n        submission.loc[:, trainTargetsS.columns] += test_predict\n        res.loc[te, trainTargetsS.columns] += val_predict\n        print('')\n    \nsubmission.loc[:, trainTargetsS.columns] /= ((n+1) * N_STARTS)\nres.loc[:, trainTargetsS.columns] /= N_STARTS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'OOF Metric: {metric(trainTargetsS, res)}')\nsubmission.loc[test['cp_type'] == 1, trainTargetsS.columns] = 0\nsubmission.to_csv('submission.csv', index = False)\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}