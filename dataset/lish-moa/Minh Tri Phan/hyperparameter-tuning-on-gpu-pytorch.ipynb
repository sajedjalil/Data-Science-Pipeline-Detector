{"cells":[{"metadata":{"papermill":{"duration":0.057533,"end_time":"2020-09-25T15:27:56.424606","exception":false,"start_time":"2020-09-25T15:27:56.367073","status":"completed"},"tags":[]},"cell_type":"markdown","source":"* Tune hyperparameters for neural network using Hyperopt\n* This kernel is modified from the work of Yirun Yang's work https://www.kaggle.com/gogo827jz/hyperparameter-tuning-on-gpu, working with Pytorch. \n* If you upvote my work, please also upvote his. Thank you and enjoy kaggling!\n* Add some PCA features\n"},{"metadata":{"execution":{"iopub.execute_input":"2020-09-25T15:27:56.484203Z","iopub.status.busy":"2020-09-25T15:27:56.483117Z","iopub.status.idle":"2020-09-25T15:27:57.624117Z","shell.execute_reply":"2020-09-25T15:27:57.623251Z"},"papermill":{"duration":1.168931,"end_time":"2020-09-25T15:27:57.624252","exception":false,"start_time":"2020-09-25T15:27:56.455321","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2020-09-25T15:27:57.657639Z","iopub.status.busy":"2020-09-25T15:27:57.656783Z","iopub.status.idle":"2020-09-25T15:27:59.669906Z","shell.execute_reply":"2020-09-25T15:27:59.668607Z"},"papermill":{"duration":2.033955,"end_time":"2020-09-25T15:27:59.670049","exception":false,"start_time":"2020-09-25T15:27:57.636094","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport gc\nimport numpy as np\nimport pandas as pd\nimport json\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, TensorDataset, DataLoader\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\n\nfrom hyperopt import hp, fmin, atpe, tpe, Trials\nfrom hyperopt.pyll.base import scope\nfrom tqdm.notebook import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/tabnetdevelop/tabnet-develop')\nfrom pytorch_tabnet.tab_model import TabNetRegressor","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2020-09-25T15:27:59.701164Z","iopub.status.busy":"2020-09-25T15:27:59.699876Z","iopub.status.idle":"2020-09-25T15:28:06.470278Z","shell.execute_reply":"2020-09-25T15:28:06.469466Z"},"papermill":{"duration":6.788707,"end_time":"2020-09-25T15:28:06.470416","exception":false,"start_time":"2020-09-25T15:27:59.681709","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\n\nss = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-25T15:28:06.516988Z","iopub.status.busy":"2020-09-25T15:28:06.515969Z","iopub.status.idle":"2020-09-25T15:28:06.550125Z","shell.execute_reply":"2020-09-25T15:28:06.550877Z"},"papermill":{"duration":0.066061,"end_time":"2020-09-25T15:28:06.551082","exception":false,"start_time":"2020-09-25T15:28:06.485021","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def preprocess(df):\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del df['sig_id']\n    return df\n\ntrain = preprocess(train_features)\ntest = preprocess(test_features)\n\ndel train_targets['sig_id']\n\ntrain_targets = train_targets.loc[train['cp_type']==0].reset_index(drop=True)\ntrain = train.loc[train['cp_type']==0].reset_index(drop=True)\n\nfeatures = list(train_features.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\n\nfor col in (GENES + CELLS):\n\n    transformer = QuantileTransformer(n_quantiles = 100, random_state = 0, output_distribution=\"normal\")\n    vec_len = len(train[col].values)\n    vec_len_test = len(test[col].values)\n    raw_vec = train[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test[col] = transformer.transform(test[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-25T15:28:06.597461Z","iopub.status.busy":"2020-09-25T15:28:06.591527Z","iopub.status.idle":"2020-09-25T15:28:06.639104Z","shell.execute_reply":"2020-09-25T15:28:06.639767Z"},"papermill":{"duration":0.072291,"end_time":"2020-09-25T15:28:06.639933","exception":false,"start_time":"2020-09-25T15:28:06.567642","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def pca(xtrain, features):\n    train_features = pd.DataFrame(xtrain, columns = features)\n    gene_features = [i for i, x in enumerate(features) if x[:2] == 'g-']\n    cell_features = [i for i, x in enumerate(features) if x[:2] == 'c-']\n\n    # For generic features\n    pca_g = PCA(n_components = len(gene_features))\n    g_pca = pca_g.fit(train_features.iloc[:,gene_features])\n\n    # For cellular features\n    pca_c = PCA(n_components = len(cell_features))\n    c_pca = pca_c.fit(train_features.iloc[:,cell_features])\n\n    # Now, we care only on 95% of total variance\n    cut_g = np.where(np.cumsum(pca_g.explained_variance_ratio_) < 0.80)[0][-1]\n    cut_c = np.where(np.cumsum(pca_c.explained_variance_ratio_) < 0.90)[0][-1]\n    \n    return pca_g, pca_c, gene_features, cell_features, cut_g, cut_c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the json file of important features\nwith open('../input/t-test-pca-rfe-logistic-regression/main_predictors.json') as json_file:\n    important_feats = json.load(json_file)\n    \n# Extract the important features from the train data\ntrain_top_feats = train[important_feats['start_predictors']]\ntest_top_feats = test[important_feats['start_predictors']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_log_loss(y_true, y_pred):\n    metrics = []\n    for i, target in enumerate(targets):\n        metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float), labels = [0,1]))\n    return np.mean(metrics)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-25T15:28:06.800543Z","iopub.status.busy":"2020-09-25T15:28:06.798156Z","iopub.status.idle":"2020-09-25T15:28:06.801433Z","shell.execute_reply":"2020-09-25T15:28:06.802068Z"},"papermill":{"duration":0.041347,"end_time":"2020-09-25T15:28:06.802227","exception":false,"start_time":"2020-09-25T15:28:06.76088","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"## TABNET\n#from pytorch_tabnet.tab_model import TabModel\nimport torch\nimport numpy as np\nfrom scipy.sparse import csc_matrix\nimport time\nfrom abc import abstractmethod\nfrom pytorch_tabnet import tab_network\nfrom pytorch_tabnet.multiclass_utils import unique_labels\nfrom sklearn.metrics import roc_auc_score, mean_squared_error, accuracy_score\nfrom torch.nn.utils import clip_grad_norm_\nfrom pytorch_tabnet.utils import (PredictDataset,\n                                  create_dataloaders,\n                                  create_explain_matrix)\nfrom sklearn.base import BaseEstimator\nfrom torch.utils.data import DataLoader\nfrom copy import deepcopy\nimport io\nimport json\nfrom pathlib import Path\nimport shutil\nimport zipfile\n\n\nclass TabModel(BaseEstimator):\n    def __init__(self, n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1,\n                 n_independent=2, n_shared=2, epsilon=1e-15,  momentum=0.02,\n                 lambda_sparse=1e-3, seed=0,\n                 clip_value=1, verbose=1,\n                 optimizer_fn=torch.optim.Adam,\n                 optimizer_params=dict(lr=2e-2),\n                 scheduler_params=None, scheduler_fn=None,\n                 mask_type=\"sparsemax\",\n                 input_dim=None, output_dim=None,\n                 device_name='auto'):\n        \"\"\" Class for TabNet model\n        Parameters\n        ----------\n            device_name: str\n                'cuda' if running on GPU, 'cpu' if not, 'auto' to autodetect\n        \"\"\"\n\n        self.n_d = n_d\n        self.n_a = n_a\n        self.n_steps = n_steps\n        self.gamma = gamma\n        self.cat_idxs = cat_idxs\n        self.cat_dims = cat_dims\n        self.cat_emb_dim = cat_emb_dim\n        self.n_independent = n_independent\n        self.n_shared = n_shared\n        self.epsilon = epsilon\n        self.momentum = momentum\n        self.lambda_sparse = lambda_sparse\n        self.clip_value = clip_value\n        self.verbose = verbose\n        self.optimizer_fn = optimizer_fn\n        self.optimizer_params = optimizer_params\n        self.device_name = device_name\n        self.scheduler_params = scheduler_params\n        self.scheduler_fn = scheduler_fn\n        self.mask_type = mask_type\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n        self.batch_size = 1024\n\n        self.seed = seed\n        torch.manual_seed(self.seed)\n        # Defining device\n        if device_name == 'auto':\n            if torch.cuda.is_available():\n                device_name = 'cuda'\n            else:\n                device_name = 'cpu'\n        self.device = torch.device(device_name)\n        print(f\"Device used : {self.device}\")\n\n    @abstractmethod\n    def construct_loaders(self, X_train, y_train, X_valid, y_valid,\n                          weights, batch_size, num_workers, drop_last):\n        \"\"\"\n        Returns\n        -------\n        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n            Training and validation dataloaders\n        -------\n        \"\"\"\n        raise NotImplementedError('users must define construct_loaders to use this base class')\n\n    def init_network(\n                     self,\n                     input_dim,\n                     output_dim,\n                     n_d,\n                     n_a,\n                     n_steps,\n                     gamma,\n                     cat_idxs,\n                     cat_dims,\n                     cat_emb_dim,\n                     n_independent,\n                     n_shared,\n                     epsilon,\n                     virtual_batch_size,\n                     momentum,\n                     device_name,\n                     mask_type,\n                     ):\n        self.network = tab_network.TabNet(\n            input_dim,\n            output_dim,\n            n_d=n_d,\n            n_a=n_a,\n            n_steps=n_steps,\n            gamma=gamma,\n            cat_idxs=cat_idxs,\n            cat_dims=cat_dims,\n            cat_emb_dim=cat_emb_dim,\n            n_independent=n_independent,\n            n_shared=n_shared,\n            epsilon=epsilon,\n            virtual_batch_size=virtual_batch_size,\n            momentum=momentum,\n            device_name=device_name,\n            mask_type=mask_type).to(self.device)\n\n        self.reducing_matrix = create_explain_matrix(\n            self.network.input_dim,\n            self.network.cat_emb_dim,\n            self.network.cat_idxs,\n            self.network.post_embed_dim)\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, loss_fn=None,\n            weights=0, max_epochs=100, patience=10, batch_size=1024,\n            virtual_batch_size=128, num_workers=0, drop_last=False):\n        \"\"\"Train a neural network stored in self.network\n        Using train_dataloader for training data and\n        valid_dataloader for validation.\n        Parameters\n        ----------\n            X_train: np.ndarray\n                Train set\n            y_train : np.array\n                Train targets\n            X_train: np.ndarray\n                Train set\n            y_train : np.array\n                Train targets\n            weights : bool or dictionnary\n                0 for no balancing\n                1 for automated balancing\n                dict for custom weights per class\n            max_epochs : int\n                Maximum number of epochs during training\n            patience : int\n                Number of consecutive non improving epoch before early stopping\n            batch_size : int\n                Training batch size\n            virtual_batch_size : int\n                Batch size for Ghost Batch Normalization (virtual_batch_size < batch_size)\n            num_workers : int\n                Number of workers used in torch.utils.data.DataLoader\n            drop_last : bool\n                Whether to drop last batch during training\n        \"\"\"\n        # update model name\n\n        self.update_fit_params(X_train, y_train, X_valid, y_valid, loss_fn,\n                               weights, max_epochs, patience, batch_size,\n                               virtual_batch_size, num_workers, drop_last)\n\n        train_dataloader, valid_dataloader = self.construct_loaders(X_train,\n                                                                    y_train,\n                                                                    X_valid,\n                                                                    y_valid,\n                                                                    self.updated_weights,\n                                                                    self.batch_size,\n                                                                    self.num_workers,\n                                                                    self.drop_last)\n\n        self.init_network(\n            input_dim=self.input_dim,\n            output_dim=self.output_dim,\n            n_d=self.n_d,\n            n_a=self.n_a,\n            n_steps=self.n_steps,\n            gamma=self.gamma,\n            cat_idxs=self.cat_idxs,\n            cat_dims=self.cat_dims,\n            cat_emb_dim=self.cat_emb_dim,\n            n_independent=self.n_independent,\n            n_shared=self.n_shared,\n            epsilon=self.epsilon,\n            virtual_batch_size=self.virtual_batch_size,\n            momentum=self.momentum,\n            device_name=self.device_name,\n            mask_type=self.mask_type\n        )\n\n        self.optimizer = self.optimizer_fn(self.network.parameters(),\n                                           **self.optimizer_params)\n\n        if self.scheduler_fn:\n            self.scheduler = self.scheduler_fn(self.optimizer, **self.scheduler_params)\n        else:\n            self.scheduler = None\n\n        self.losses_train = []\n        self.losses_valid = []\n        self.learning_rates = []\n        self.metrics_train = []\n        self.metrics_valid = []\n\n        if self.verbose > 0:\n            print(\"Will train until validation stopping metric\",\n                  f\"hasn't improved in {self.patience} rounds.\")\n            msg_epoch = f'| EPOCH |  train  |   valid  | total time (s)'\n            print('---------------------------------------')\n            print(msg_epoch)\n\n        total_time = 0\n        while (self.epoch < self.max_epochs and\n               self.patience_counter < self.patience):\n            starting_time = time.time()\n            # updates learning rate history\n            self.learning_rates.append(self.optimizer.param_groups[-1][\"lr\"])\n\n            fit_metrics = self.fit_epoch(train_dataloader, valid_dataloader)\n\n            # leaving it here, may be used for callbacks later\n            self.losses_train.append(fit_metrics['train']['loss_avg'])\n            self.losses_valid.append(fit_metrics['valid']['total_loss'])\n            self.metrics_train.append(fit_metrics['train']['stopping_loss'])\n            self.metrics_valid.append(fit_metrics['valid']['stopping_loss'])\n\n            stopping_loss = fit_metrics['valid']['stopping_loss']\n            if stopping_loss < self.best_cost:\n                self.best_cost = stopping_loss\n                self.patience_counter = 0\n                # Saving model\n                self.best_network = deepcopy(self.network)\n                has_improved = True\n            else:\n                self.patience_counter += 1\n                has_improved=False\n            self.epoch += 1\n            total_time += time.time() - starting_time\n            if self.verbose > 0:\n                if self.epoch % self.verbose == 0:\n                    separator = \"|\"\n                    msg_epoch = f\"| {self.epoch:<5} | \"\n                    msg_epoch += f\" {fit_metrics['train']['stopping_loss']:.5f}\"\n                    msg_epoch += f' {separator:<2} '\n                    msg_epoch += f\" {fit_metrics['valid']['stopping_loss']:.5f}\"\n                    msg_epoch += f' {separator:<2} '\n                    msg_epoch += f\" {np.round(total_time, 1):<10}\"\n                    msg_epoch += f\" {has_improved}\"\n                    print(msg_epoch)\n\n        if self.verbose > 0:\n            if self.patience_counter == self.patience:\n                print(f\"Early stopping occured at epoch {self.epoch}\")\n            print(f\"Training done in {total_time:.3f} seconds.\")\n            print('---------------------------------------')\n\n        self.history = {\"train\": {\"loss\": self.losses_train,\n                                  \"metric\": self.metrics_train,\n                                  \"lr\": self.learning_rates},\n                        \"valid\": {\"loss\": self.losses_valid,\n                                  \"metric\": self.metrics_valid}}\n        # load best models post training\n        self.load_best_model()\n\n        # compute feature importance once the best model is defined\n        self._compute_feature_importances(train_dataloader)\n\n    def save_model(self, path):\n        \"\"\"\n        Saving model with two distinct files.\n        \"\"\"\n        saved_params = {}\n        for key, val in self.get_params().items():\n            if isinstance(val, type):\n                # Don't save torch specific params\n                continue\n            else:\n                saved_params[key] = val\n\n        # Create folder\n        Path(path).mkdir(parents=True, exist_ok=True)\n\n        # Save models params\n        with open(Path(path).joinpath(\"model_params.json\"), \"w\", encoding=\"utf8\") as f:\n            json.dump(saved_params, f)\n\n        # Save state_dict\n        torch.save(self.network.state_dict(), Path(path).joinpath(\"network.pt\"))\n        shutil.make_archive(path, 'zip', path)\n        shutil.rmtree(path)\n        print(f\"Successfully saved model at {path}.zip\")\n        return f\"{path}.zip\"\n\n    def load_model(self, filepath):\n\n        try:\n            with zipfile.ZipFile(filepath) as z:\n                with z.open(\"model_params.json\") as f:\n                    loaded_params = json.load(f)\n                with z.open(\"network.pt\") as f:\n                    try:\n                        saved_state_dict = torch.load(f)\n                    except io.UnsupportedOperation:\n                        # In Python <3.7, the returned file object is not seekable (which at least\n                        # some versions of PyTorch require) - so we'll try buffering it in to a\n                        # BytesIO instead:\n                        saved_state_dict = torch.load(io.BytesIO(f.read()))\n        except KeyError:\n            raise KeyError(\"Your zip file is missing at least one component\")\n\n        self.__init__(**loaded_params)\n\n        self.init_network(\n            input_dim=self.input_dim,\n            output_dim=self.output_dim,\n            n_d=self.n_d,\n            n_a=self.n_a,\n            n_steps=self.n_steps,\n            gamma=self.gamma,\n            cat_idxs=self.cat_idxs,\n            cat_dims=self.cat_dims,\n            cat_emb_dim=self.cat_emb_dim,\n            n_independent=self.n_independent,\n            n_shared=self.n_shared,\n            epsilon=self.epsilon,\n            virtual_batch_size=1024,\n            momentum=self.momentum,\n            device_name=self.device_name,\n            mask_type=self.mask_type\n        )\n        self.network.load_state_dict(saved_state_dict)\n        self.network.eval()\n        return\n\n    def fit_epoch(self, train_dataloader, valid_dataloader):\n        \"\"\"\n        Evaluates and updates network for one epoch.\n        Parameters\n        ----------\n            train_dataloader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with train set\n            valid_dataloader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with valid set\n        \"\"\"\n        train_metrics = self.train_epoch(train_dataloader)\n        valid_metrics = self.predict_epoch(valid_dataloader)\n\n        fit_metrics = {'train': train_metrics,\n                       'valid': valid_metrics}\n\n        return fit_metrics\n\n    @abstractmethod\n    def train_epoch(self, train_loader):\n        \"\"\"\n        Trains one epoch of the network in self.network\n        Parameters\n        ----------\n            train_loader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with train set\n        \"\"\"\n        raise NotImplementedError('users must define train_epoch to use this base class')\n\n    @abstractmethod\n    def train_batch(self, data, targets):\n        \"\"\"\n        Trains one batch of data\n        Parameters\n        ----------\n            data: a :tensor: `torch.tensor`\n                Input data\n            target: a :tensor: `torch.tensor`\n                Target data\n        \"\"\"\n        raise NotImplementedError('users must define train_batch to use this base class')\n\n    @abstractmethod\n    def predict_epoch(self, loader):\n        \"\"\"\n        Validates one epoch of the network in self.network\n        Parameters\n        ----------\n            loader: a :class: `torch.utils.data.Dataloader`\n                    DataLoader with validation set\n        \"\"\"\n        raise NotImplementedError('users must define predict_epoch to use this base class')\n\n    @abstractmethod\n    def predict_batch(self, data, targets):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            batch_outs: dict\n        \"\"\"\n        raise NotImplementedError('users must define predict_batch to use this base class')\n\n    def load_best_model(self):\n        if self.best_network is not None:\n            self.network = self.best_network\n\n    @abstractmethod\n    def predict(self, X):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            predictions: np.array\n                Predictions of the regression problem or the last class\n        \"\"\"\n        raise NotImplementedError('users must define predict to use this base class')\n\n    def explain(self, X):\n        \"\"\"\n        Return local explanation\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            M_explain: matrix\n                Importance per sample, per columns.\n            masks: matrix\n                Sparse matrix showing attention masks used by network.\n        \"\"\"\n        self.network.eval()\n\n        dataloader = DataLoader(PredictDataset(X),\n                                batch_size=self.batch_size, shuffle=False)\n\n        for batch_nb, data in enumerate(dataloader):\n            data = data.to(self.device).float()\n\n            M_explain, masks = self.network.forward_masks(data)\n            for key, value in masks.items():\n                masks[key] = csc_matrix.dot(value.cpu().detach().numpy(),\n                                            self.reducing_matrix)\n\n            if batch_nb == 0:\n                res_explain = csc_matrix.dot(M_explain.cpu().detach().numpy(),\n                                             self.reducing_matrix)\n                res_masks = masks\n            else:\n                res_explain = np.vstack([res_explain,\n                                         csc_matrix.dot(M_explain.cpu().detach().numpy(),\n                                                        self.reducing_matrix)])\n                for key, value in masks.items():\n                    res_masks[key] = np.vstack([res_masks[key], value])\n        return res_explain, res_masks\n\n    def _compute_feature_importances(self, loader):\n        self.network.eval()\n        feature_importances_ = np.zeros((self.network.post_embed_dim))\n        for data, targets in loader:\n            data = data.to(self.device).float()\n            M_explain, masks = self.network.forward_masks(data)\n            feature_importances_ += M_explain.sum(dim=0).cpu().detach().numpy()\n\n        feature_importances_ = csc_matrix.dot(feature_importances_,\n                                              self.reducing_matrix)\n        self.feature_importances_ = feature_importances_ / np.sum(feature_importances_)\n        \n        \nclass TabNetRegressor(TabModel):\n\n    def construct_loaders(self, X_train, y_train, X_valid, y_valid, weights,\n                          batch_size, num_workers, drop_last):\n        \"\"\"\n        Returns\n        -------\n        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n            Training and validation dataloaders\n        -------\n        \"\"\"\n        if isinstance(weights, int):\n            if weights == 1:\n                raise ValueError(\"Please provide a list of weights for regression.\")\n        if isinstance(weights, dict):\n            raise ValueError(\"Please provide a list of weights for regression.\")\n\n        train_dataloader, valid_dataloader = create_dataloaders(X_train,\n                                                                y_train,\n                                                                X_valid,\n                                                                y_valid,\n                                                                weights,\n                                                                batch_size,\n                                                                num_workers,\n                                                                drop_last)\n        return train_dataloader, valid_dataloader\n\n    def update_fit_params(self, X_train, y_train, X_valid, y_valid, loss_fn,\n                          weights, max_epochs, patience,\n                          batch_size, virtual_batch_size, num_workers, drop_last):\n\n        if loss_fn is None:\n            self.loss_fn = torch.nn.functional.mse_loss\n        else:\n            self.loss_fn = loss_fn\n\n        assert X_train.shape[1] == X_valid.shape[1], \"Dimension mismatch X_train X_valid\"\n        self.input_dim = X_train.shape[1]\n\n        if len(y_train.shape) == 1:\n            raise ValueError(\"\"\"Please apply reshape(-1, 1) to your targets\n                                if doing single regression.\"\"\")\n        assert y_train.shape[1] == y_valid.shape[1], \"Dimension mismatch y_train y_valid\"\n        self.output_dim = y_train.shape[1]\n\n        self.updated_weights = weights\n\n        self.max_epochs = max_epochs\n        self.patience = patience\n        self.batch_size = batch_size\n        self.virtual_batch_size = virtual_batch_size\n        # Initialize counters and histories.\n        self.patience_counter = 0\n        self.epoch = 0\n        self.best_cost = np.inf\n        self.num_workers = num_workers\n        self.drop_last = drop_last\n\n    def train_epoch(self, train_loader):\n        \"\"\"\n        Trains one epoch of the network in self.network\n        Parameters\n        ----------\n            train_loader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with train set\n        \"\"\"\n\n        self.network.train()\n        y_preds = []\n        ys = []\n        total_loss = 0\n\n        for data, targets in train_loader:\n            batch_outs = self.train_batch(data, targets)\n            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n            total_loss += batch_outs[\"loss\"]\n\n        y_preds = np.vstack(y_preds)\n        ys = np.vstack(ys)\n\n        #stopping_loss = mean_squared_error(y_true=ys, y_pred=y_preds)\n        stopping_loss = mean_log_loss(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  )\n        total_loss = total_loss / len(train_loader)\n        epoch_metrics = {'loss_avg': total_loss,\n                         'stopping_loss': total_loss,\n                         }\n\n        if self.scheduler is not None:\n            self.scheduler.step()\n        return epoch_metrics\n\n    def train_batch(self, data, targets):\n        \"\"\"\n        Trains one batch of data\n        Parameters\n        ----------\n            data: a :tensor: `torch.tensor`\n                Input data\n            target: a :tensor: `torch.tensor`\n                Target data\n        \"\"\"\n        self.network.train()\n        data = data.to(self.device).float()\n\n        targets = targets.to(self.device).float()\n        self.optimizer.zero_grad()\n\n        output, M_loss = self.network(data)\n\n        loss = self.loss_fn(output, targets)\n        \n        loss -= self.lambda_sparse*M_loss\n\n        loss.backward()\n        if self.clip_value:\n            clip_grad_norm_(self.network.parameters(), self.clip_value)\n        self.optimizer.step()\n\n        loss_value = loss.item()\n        batch_outs = {'loss': loss_value,\n                      'y_preds': output,\n                      'y': targets}\n        return batch_outs\n\n    def predict_epoch(self, loader):\n        \"\"\"\n        Validates one epoch of the network in self.network\n        Parameters\n        ----------\n            loader: a :class: `torch.utils.data.Dataloader`\n                    DataLoader with validation set\n        \"\"\"\n        y_preds = []\n        ys = []\n        self.network.eval()\n        total_loss = 0\n\n        for data, targets in loader:\n            batch_outs = self.predict_batch(data, targets)\n            total_loss += batch_outs[\"loss\"]\n            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n\n        y_preds = np.vstack(y_preds)\n        ys = np.vstack(ys)\n\n        stopping_loss = mean_log_loss(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  ) #mean_squared_error(y_true=ys, y_pred=y_preds)\n\n        total_loss = total_loss / len(loader)\n        epoch_metrics = {'total_loss': total_loss,\n                         'stopping_loss': stopping_loss}\n\n        return epoch_metrics\n\n    def predict_batch(self, data, targets):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            batch_outs: dict\n        \"\"\"\n        self.network.eval()\n        data = data.to(self.device).float()\n        targets = targets.to(self.device).float()\n\n        output, M_loss = self.network(data)\n       \n        loss = self.loss_fn(output, targets)\n        #print(self.loss_fn, loss)\n        loss -= self.lambda_sparse*M_loss\n        #print(loss)\n        loss_value = loss.item()\n        batch_outs = {'loss': loss_value,\n                      'y_preds': output,\n                      'y': targets}\n        return batch_outs\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            predictions: np.array\n                Predictions of the regression problem\n        \"\"\"\n        self.network.eval()\n        dataloader = DataLoader(PredictDataset(X),\n                                batch_size=self.batch_size, shuffle=False)\n\n        results = []\n        for batch_nb, data in enumerate(dataloader):\n            data = data.to(self.device).float()\n\n            output, M_loss = self.network(data)\n            predictions = output.cpu().detach().numpy()\n            results.append(predictions)\n        res = np.vstack(results)\n        return res","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-25T15:28:06.844105Z","iopub.status.busy":"2020-09-25T15:28:06.842907Z","iopub.status.idle":"2020-09-25T15:28:06.846486Z","shell.execute_reply":"2020-09-25T15:28:06.845869Z"},"papermill":{"duration":0.029582,"end_time":"2020-09-25T15:28:06.846615","exception":false,"start_time":"2020-09-25T15:28:06.817033","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# dataset class\nclass MoaDataset(Dataset):\n    def __init__(self, df, targets, feats_idx, mode='train'):\n        self.mode = mode\n        self.feats = feats_idx\n        self.data = df[:, feats_idx]\n        if mode=='train':\n            self.targets = targets\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        if self.mode == 'train':\n            return torch.FloatTensor(self.data[idx]), torch.FloatTensor(self.targets[idx])\n        elif self.mode == 'test':\n            return torch.FloatTensor(self.data[idx]), 0","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-25T15:28:07.253634Z","iopub.status.busy":"2020-09-25T15:28:07.252499Z","iopub.status.idle":"2020-09-25T15:28:07.255434Z","shell.execute_reply":"2020-09-25T15:28:07.255974Z"},"papermill":{"duration":0.39275,"end_time":"2020-09-25T15:28:07.256133","exception":false,"start_time":"2020-09-25T15:28:06.863383","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"nfolds = 7\nnepochs = 150\nbatch_size = 1024\nntargets = train_targets.shape[1]\ntargets = [col for col in train_targets.columns]\ncriterion = nn.BCELoss()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-25T15:28:07.317898Z","iopub.status.busy":"2020-09-25T15:28:07.297247Z","iopub.status.idle":"2020-09-25T15:28:07.331092Z","shell.execute_reply":"2020-09-25T15:28:07.331631Z"},"papermill":{"duration":0.061966,"end_time":"2020-09-25T15:28:07.331806","exception":false,"start_time":"2020-09-25T15:28:07.26984","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def optimise(params):\n    # Define the device\n    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n    \n    # Firstly, with these settings of hyperparameters, train the model and store the best model\n    print(params)\n    print('=' * 50)\n    \n    res_nn = train_targets.copy()\n    res_nn.loc[:, train_targets.columns] = 0\n    \n    for n, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits = nfolds, \n                                       random_state = 0, \n                                       shuffle = True).split(train, train_targets)):\n        xtrain, xval = train_top_feats.values[tr], train_top_feats.values[te]\n        ytrain, yval = train_targets.values[tr], train_targets.values[te]\n        \n        model = TabNetRegressor(n_d = params['n'], n_a = params['n'], n_steps = params['n_steps'], gamma = params['gamma'], lambda_sparse = 0, cat_emb_dim = 1,\n                                cat_idxs = [], optimizer_fn = optim.Adam, optimizer_params = dict(lr = params['lr'], weight_decay = params['weight_decay']),\n                                mask_type = params['mask_type'], device_name = device, verbose = 1, scheduler_params = dict(milestones = [50, 100], gamma = 0.5),\n                                scheduler_fn = optim.lr_scheduler.MultiStepLR)\n        \n        model.fit(X_train = xtrain, y_train = ytrain, X_valid = xval, y_valid = yval, max_epochs = nepochs,\n                  patience = 50, batch_size = batch_size, virtual_batch_size = 128, num_workers = 0, drop_last = False,\n                  loss_fn = F.binary_cross_entropy_with_logits)\n        \n        # Inference\n        model.load_best_model()\n        \n        # In the validation set\n        preds = model.predict(xval)\n        res_nn.loc[te, train_targets.columns] += torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n        \n    res_nn.loc[train_features['cp_type'] == 1, train_targets.columns] = 0\n    metrics = []\n    for _target in train_targets.columns:\n        metrics.append(log_loss(train_targets.loc[:, _target], res_nn.loc[:, _target]))\n    print(f'OOF Metric with postprocessing: {np.mean(metrics)}')\n    print('=' * 50)\n    \n    return np.mean(metrics)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Due to the limited capacity of the memory, I can only design a narrow parameter space"},{"metadata":{"execution":{"iopub.execute_input":"2020-09-25T15:28:07.378085Z","iopub.status.busy":"2020-09-25T15:28:07.376207Z","iopub.status.idle":"2020-09-25T17:32:03.139786Z","shell.execute_reply":"2020-09-25T17:32:03.139111Z"},"papermill":{"duration":7435.794459,"end_time":"2020-09-25T17:32:03.139907","exception":false,"start_time":"2020-09-25T15:28:07.345448","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"param_space = {\n    'n': hp.choice('n', [8, 24]),\n    'n_steps': hp.choice('n_steps', [1, 2, 3]),\n    'gamma': hp.uniform('gamma', 1.0, 2.0),\n    'lr': hp.uniform('lr', 0.007, 0.02),\n    'weight_decay': hp.uniform('weight_decay', 1e-6, 1e-5),\n    'mask_type': hp.choice('mask_type', ['sparsemax', 'entmax'])\n}\n\ntrials = Trials()\n\nhopt = fmin(fn = optimise,\n            space = param_space,\n            algo = tpe.suggest, \n            max_evals = 15, \n            timeout = 8.9 * 60 * 60, \n            trials = trials,\n           )","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-25T17:32:10.160489Z","iopub.status.busy":"2020-09-25T17:32:10.159658Z","iopub.status.idle":"2020-09-25T17:32:10.164213Z","shell.execute_reply":"2020-09-25T17:32:10.164794Z"},"papermill":{"duration":3.263615,"end_time":"2020-09-25T17:32:10.164945","exception":false,"start_time":"2020-09-25T17:32:06.90133","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"print(hopt)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}