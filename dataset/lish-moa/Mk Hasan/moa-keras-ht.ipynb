{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nimport time\nimport re\nimport random\nimport datetime\nimport pickle\nimport gc\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport yaml\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom tqdm.notebook import tqdm\n\n# sklearn\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, OneHotEncoder, LabelEncoder, QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA, TruncatedSVD, FactorAnalysis\nfrom sklearn.cluster import KMeans\n\n# Pytorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\n\n# tensorflow\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import layers, regularizers, Sequential, backend, callbacks, optimizers, metrics, losses\n\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', 300)\npd.set_option('display.max_rows', 300)\npd.set_option('display.max_colwidth', 300)\npd.options.display.float_format = '{:.3f}'.format\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport os\nimport sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = \"/kaggle/\"\ndf_Train = pd.read_csv(path+\"input/lish-moa/train_features.csv\")\ndf_Test = pd.read_csv(path+\"input/lish-moa/test_features.csv\")\ndf_Train_target_nonscored = pd.read_csv(\"../input/lish-moa/train_targets_nonscored.csv\")\ndf_Train_target_scored = pd.read_csv(path+\"input/lish-moa/train_targets_scored.csv\")\nsample_submission = pd.read_csv(path+\"input/lish-moa/sample_submission.csv\")\n\n\ntarget_cols = df_Train_target_scored.columns[1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Config():\n    # setting\n    is_debug = False\n    is_kaggle = True\n    is_pretrain = False\n\n    # features\n    do_variancethreshold = False\n    do_kmeans = False\n    do_filter = True\n    do_feature_squared = True\n    do_feature_stats = True\n    do_feature_pca = True\n    do_feature_svd = True\n    do_feature_fa = True\n\n    # constant\n    seed = 42\n    n_gene_comp = 70\n    n_cell_comp = 10\n    n_gene_kmeans_cluster = 30\n    n_cell_kmeans_cluster = 5\n    n_variance_threshold = 0.7\n    scaler = 'Rankgauss' # Standard, Robust, MinMax\n\n    # HyperParameters\n    epochs = 80\n    seed_avg = [0, 101, 202, 303 ,404, 999]\n    nfold = 7\n    verbose = 0\n    lr = 1e-3\n    weight_decay = 1e-5\n    batch_size = 128\n\nconfig = Config()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def HighestCorrelation(corrmat):\n    c = corrmat.abs()\n    shape = c.shape\n    s = c.unstack()\n    so = s.sort_values(kind=\"quicksort\")\n    return so[-(shape[0] + 10):-shape[0]]\n\ndef data_filter(train, test):\n    \"\"\"cp_type = ctl_vehicle\n    \"\"\"\n    train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n    test = test[test['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n    train = train.drop('cp_type', axis=1)\n    test = test.drop('cp_type', axis=1)\n    return train, test\n\ndef one_hot_encoder(df, cols):\n    \"\"\"sklearn.OneHotEncoder.Encoding\n    \"\"\"\n    for col in cols:\n        ohe = OneHotEncoder(sparse=False)\n        ohe_df = pd.DataFrame(ohe.fit_transform(df[[col]])).add_prefix(col + '_ohe_')\n        # 元のDFに結合\n        df = pd.concat([df, ohe_df], axis=1)\n        # oheしたカラムを除外\n        df = df.drop(col, axis=1)\n    return df\n\ndef feature_stats(df):\n    \"\"\" features distibution stats\n    \"\"\"\n    df.loc[:, 'g-sum'] = df[GENES].sum(axis=1)\n    df.loc[:, 'g-mean'] = df[GENES].mean(axis=1)\n    df.loc[:, 'g-std'] = df[GENES].std(axis=1)\n    df.loc[:, 'g-kurt'] = df[GENES].kurtosis(axis=1)\n    df.loc[:, 'g-skew'] = df[GENES].skew(axis=1)\n\n    df.loc[:, 'c-sum'] = df[CELLS].sum(axis=1)\n    df.loc[:, 'c-mean'] = df[CELLS].mean(axis=1)\n    df.loc[:, 'c-std'] = df[CELLS].std(axis=1)\n    df.loc[:, 'c-kurt'] = df[CELLS].kurtosis(axis=1)\n    df.loc[:, 'c-skew'] = df[CELLS].skew(axis=1)\n\n    df.loc[:, 'gc-sum'] = df[GENES + CELLS].sum(axis=1)\n    df.loc[:, 'gc-mean'] = df[GENES + CELLS].mean(axis=1)\n    df.loc[:, 'gc-std'] = df[GENES + CELLS].std(axis=1)\n    df.loc[:, 'gc-kurt'] = df[GENES + CELLS].kurtosis(axis=1)\n    df.loc[:, 'gc-skew'] = df[GENES + CELLS].skew(axis=1)\n    return df\n\n\ndef feature_pca(df, col_list, n_comp, col_type='g', seed=config.seed):\n    \"\"\"PCA for important features\n    \"\"\"\n    pca = (PCA(n_components=n_comp, random_state=seed).fit_transform(df[col_list]))\n    pca_df = pd.DataFrame(pca, columns=[f'{col_type}-pca_{i}' for i in range(n_comp)])\n    df = pd.concat([df, pca_df], axis=1)\n    return df\n\ndef feature_svd(df, col_list, n_comp, col_type='g', seed=config.seed):\n    \"\"\"SVD\n    \"\"\"\n    svd = (TruncatedSVD(n_components=n_comp, random_state=seed).fit_transform(df[col_list]))\n    svd_df = pd.DataFrame(svd, columns=[f'{col_type}-svd_{i}' for i in range(n_comp)])\n    df = pd.concat([df, svd_df], axis=1)\n    return df\n\ndef feature_fa(df, col_list, n_comp, col_type='g', seed=config.seed):\n\n    svd = (FactorAnalysis(n_components=n_comp, random_state=seed).fit_transform(df[col_list]))\n    svd_df = pd.DataFrame(svd, columns=[f'{col_type}-fa_{i}' for i in range(n_comp)])\n    df = pd.concat([df, svd_df], axis=1)\n    return df\n\ndef feature_squared(df, cols_list):\n\n    for feature in cols_list:\n        df.loc[:, f'{feature}_squared'] = df[feature] ** 2\n    return df\n\n\ndef variance_threshold(df, n):\n \n    var_thresh = VarianceThreshold(threshold=n)\n    df = pd.DataFrame(var_thresh.fit_transform(df))\n    return df\n\n\ndef rankgauss(df, cols, seed=config.seed):\n\n    for col in cols:\n        transformer = QuantileTransformer(n_quantiles=100, random_state=seed, output_distribution=\"normal\")\n        vec_len = len(df[col].values)\n        raw_vec = df[col].values.reshape(vec_len, 1)\n        transformer.fit(raw_vec)\n        df[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n        \n    return df\n\n\n\ndef feature_engineering(train_features, test_features):\n\n    global GENES, CELLS\n\n\n    GENES = [col for col in train_features.columns if col.startswith('g-')]\n    CELLS = [col for col in train_features.columns if col.startswith('c-')]\n\n    cat_columns = ['cp_time', 'cp_dose']\n\n    # filter\n    if config.do_filter:\n        print('do filter')\n        train, test = data_filter(train_features, test_features)\n\n    df = pd.concat([train, test])\n    df = df.reset_index(drop=True)\n\n    # Stats feature\n    if config.do_feature_stats:\n        print('do feature_stats')\n        df = feature_stats(df)\n\n    # squared\n    if config.do_feature_squared:\n        print('do feature_squared')\n        df = feature_squared(df, CELLS)\n\n    # PCA feature\n    if config.do_feature_pca:\n        print('do feature_pca')\n        df = feature_pca(df, GENES, n_comp=config.n_gene_comp, col_type='g')\n        df = feature_pca(df, CELLS, n_comp=config.n_cell_comp, col_type='c')\n\n    # SVD feature\n    if config.do_feature_svd:\n        print('do feature_svd')\n        df = feature_svd(df, GENES, n_comp=config.n_gene_comp, col_type='g')\n        df = feature_svd(df, CELLS, n_comp=config.n_cell_comp, col_type='c')\n\n    # FA feature\n    if config.do_feature_fa:\n        print('do feature_fa')\n        df = feature_fa(df, GENES, n_comp=config.n_gene_comp, col_type='g')\n        df = feature_fa(df, CELLS, n_comp=config.n_cell_comp, col_type='c')\n\n    cat_df = df[['sig_id'] + cat_columns]\n    num_df = df.drop(['sig_id'] + cat_columns, axis=1)\n\n    # VarianceThreshold\n    if config.do_variancethreshold:\n        print('do variancethreshold')\n        num_df = variance_threshold(num_df, n=config.n_variance_threshold)\n\n    if config.scaler == 'Rankgauss':\n        print('do Rankgauss')\n        df = rankgauss(df, num_df.columns.tolist())\n\n    elif config.scaler == 'Standard':\n        print('do Standard')\n        sscaler = StandardScaler()\n        num_df.iloc[:, :] = sscaler.fit_transform(num_df)\n\n    elif config.scaler == 'Robust':\n        print('do Robust')\n        rscaler = RobustScaler()\n        num_df.iloc[:, :] = rscaler.fit_transform(num_df)\n\n    elif config.scaler == 'MinMax':\n        print('do MinMax')\n        mmscaler = MinMaxScaler()\n        num_df.iloc[:, :] = mmscaler.fit_transform(num_df)\n\n    #one-hot-encode\n    cat_df = one_hot_encoder(cat_df, cat_columns)\n\n\n    df = pd.concat([cat_df, num_df], axis=1)\n\n    # train & test\n    train = df.iloc[:len(train), :]\n    test = df.iloc[len(train):, :]\n    train = train.reset_index(drop=True)\n    test = test.reset_index(drop=True)\n\n    return train, test\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = feature_engineering(df_Train, df_Test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols = df_Train_target_scored.drop('sig_id', axis=1).columns.values.tolist()  # Predicted value \ntarget_cols_non_scored = df_Train_target_nonscored.drop('sig_id', axis=1).columns.values.tolist()  # pretrain variable column list for\nfeature_cols = [c for c in train.columns if c not in ['sig_id']]  \n\ntrain = train.merge(df_Train_target_scored, on='sig_id')\ntarget = train[df_Train_target_scored.columns]\n\n# pretrain\ntrain_non_scored = train[['sig_id'] + feature_cols].merge(df_Train_target_nonscored, on='sig_id')\ntarget_non_scored = train_non_scored[df_Train_target_nonscored.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=config.seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to calculate the mean log loss of the targets including clipping\ndef mean_log_loss(y_true, y_pred):\n    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    metrics = []\n    for target in range(len(target_cols)):\n        metrics.append(log_loss(y_true[:, target], y_pred[:, target]))\n    return np.mean(metrics)\n\n\ndef create_model_3l(feature_len, target_len):\n    inp = tf.keras.layers.Input(shape = (feature_len))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(0.4914099166744246)(x)\n    x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(1159, activation = 'relu'))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.18817607797795838)(x)\n    x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(960, activation = 'relu'))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.12542057776853896)(x)\n    x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(1811, activation = 'relu'))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.20175242230280122)(x)\n    out = tfa.layers.WeightNormalization(tf.keras.layers.Dense(target_len, activation = 'sigmoid'))(x)\n    model = tf.keras.models.Model(inputs = inp, outputs = out)\n    opt = tf.optimizers.Adam(learning_rate = LEARNING_RATE)\n    opt = tfa.optimizers.Lookahead(opt, sync_period = 10)\n    model.compile(optimizer = opt, \n                  loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.0015),\n                  metrics = tf.keras.metrics.BinaryCrossentropy())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nLEARNING_RATE = config.lr\nWEIGHT_DECAY = config.weight_decay\nBATCH_SIZE = config.batch_size\nEPOCHS = config.epochs\nSEED_AVG = config.seed_avg\nNFOLDS = config.nfold\nVERBOSE = config.verbose\n\nif config.is_debug:\n    EPOCHS = 3\n    SEED_AVG = [0, 101]\n    NFOLDS = 3\n    VERBOSE = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(tr_idx, va_idx, fold, seed):\n\n    # SEED\n    seed_everything(seed)\n\n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    predictions = np.zeros((len(test), target.iloc[:, 1:].shape[1]))\n\n    # train& validation\n    train_df = train.iloc[tr_idx]\n    valid_df = train.iloc[va_idx]\n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n\n    K.clear_session()\n    model = create_model_3l(len(feature_cols), len(target_cols))\n    early_stopping = tf.keras.callbacks.EarlyStopping(\n        monitor = 'val_binary_crossentropy',\n        mode = 'min',\n        patience = 10,\n        restore_best_weights = True,\n        verbose = 2\n    )\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor = 'val_binary_crossentropy',\n        mode = 'min',\n        factor = 0.3,\n        patience = 3,\n        verbose = 2\n    )\n    \n    model.fit(\n        x_train, y_train,\n        validation_data = (x_valid, y_valid),\n        epochs = EPOCHS, \n        batch_size = BATCH_SIZE,\n        callbacks = [early_stopping, reduce_lr],\n        verbose = 2\n    )\n\n    oof[va_idx] = model.predict(x_valid)\n    predictions = model.predict(test[feature_cols].values)\n    \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED_AVG","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n%%time\n\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED_AVG:\n    print(f'============== Training SEED is {seed} ==============')\n    start = time.time()\n\n    oof_fold = np.zeros((len(train), len(target_cols)))\n    predictions_fold = np.zeros((len(test), len(target_cols)))\n\n    # CV\n    mskf = MultilabelStratifiedKFold(n_splits=NFOLDS, random_state=seed, shuffle=True)\n\n    for i_fold, (tr_idx, va_idx) in enumerate(tqdm(mskf.split(X=train, y=target))):\n\n        oof_, pred_ = run_training(tr_idx, va_idx, i_fold, seed)\n\n\n        oof_fold += oof_\n        predictions_fold += pred_ / NFOLDS\n\n    oof_score = mean_log_loss(target.drop('sig_id', axis=1).values, oof_fold)\n    print(f'seed: {seed} fold mean log loss score is {oof_score}')\n\n\n    oof += oof_fold / len(SEED_AVG)\n    predictions += predictions_fold / len(SEED_AVG)\n    \n    elapsed_time = time.time() - start\n    print(f'SEED: {seed} Elapsed_time:{elapsed_time:.4f} sec')\n\nseed_log_loss = mean_log_loss(target.drop('sig_id', axis=1).values, oof)\nprint(f'Our out of folds log loss for our seed blend model is {seed_log_loss}')\n\nprint(f'==================== Training END ====================')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions\nprint(len(target_cols))\n\nvalid_results = df_Train_target_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nprint(df_Train_target_scored.shape, valid_results.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ny_true = df_Train_target_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ / target.shape[1]\n    \nprint(\"CV log_loss: \", score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config.is_kaggle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nif config.is_kaggle:\n    sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n    sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config_str = ''\nfor key, value in Config.__dict__.items():\n    if not key.startswith('__'):\n        config_str += f'{key}: {value}\\n'\n\nconfig_str += 'CV log_loss:' + str(score)\n# 日付を取得\ndata_str = datetime.datetime.now().strftime(\"%m%d%H%M\")\n\nif config.is_kaggle:\n    config_dir = './'\nelse:\n    config_dir = 'score/'\n\nwith open(config_dir + str(round(score, 7)) + '_' + data_str + '_config_score.txt', mode='w') as f:\n    f.write(config_str)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}