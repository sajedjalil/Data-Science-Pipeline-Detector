{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Install package"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"! pip install ../input/moa-pytorch16/torch-1.6.0cu101-cp37-cp37m-linux_x86_64.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install ../input/moaiterative/iterative_stratification-0.1.6-py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[](http://)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import sys\nsys.path.insert(0, \"../input/moa-tabnet-develop/tabnet-develop\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom joblib import dump, load\nimport os\nimport random\nimport sys\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nfrom tqdm import tqdm\nfrom sklearn.metrics import log_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(1984)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsubmission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n\n\nremove_vehicle = True\n\nif remove_vehicle:\n    train_features = train.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\n    train_targets_scored = train_targets_scored.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\n    train_targets_nonscored = train_targets_nonscored.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\nelse:\n    train_features = train\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ratio for each label\n\ndef get_ratio_labels(df):\n    columns = list(df.columns)\n    columns.pop(0)\n    ratios = []\n    toremove = []\n    for c in columns:\n        counts = df[c].value_counts()\n        if len(counts) != 1:\n            ratios.append(counts[0]/counts[1])\n        else:\n            toremove.append(c)\n    print(f\"remove {len(toremove)} columns\")\n    \n    for t in toremove:\n        columns.remove(t)\n    return columns, np.array(ratios).astype(np.int32)\n\ncolumns, ratios = get_ratio_labels(train_targets_scored)\ncolumns_nonscored, ratios_nonscored = get_ratio_labels(train_targets_nonscored)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### autoencoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\n\nfrom sklearn import preprocessing\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef load_data(train_path, cols):\n    # read in from csv\n    if isinstance(train_path, list):\n        df_base = pd.read_csv(train_path[0])\n        for i in range(1, len(train_path)):\n            df_base.append(pd.read_csv(train_path[i]))\n        df_base = df_base[cols]\n    else:\n        df = pd.read_csv(train_path)\n        df_base = df[cols]\n    x = df_base.values.reshape(-1, df_base.shape[1]).astype('float32')\n    # stadardize values\n    standardizer = preprocessing.StandardScaler()\n    x = standardizer.fit_transform(x)    \n    return x, standardizer\n\ndef numpyToTensor(x):\n    x_train = torch.from_numpy(x).to(device)\n    return x_train\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass DataBuilder(Dataset):\n    def __init__(self, path, cols):\n        self.x, self.standardizer = load_data(path, cols)\n        self.x = numpyToTensor(self.x)\n        self.len=self.x.shape[0]\n    def __getitem__(self,index):      \n        return self.x[index]\n    def __len__(self):\n        return self.len","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Autoencoder(nn.Module):\n    def __init__(self,D_in, H, H2, latent_dim):\n        \n        #Encoder\n        super(Autoencoder,self).__init__()\n        self.linear1=nn.Linear(D_in,H)\n        self.lin_bn1 = nn.BatchNorm1d(num_features=H)\n        self.linear2=nn.Linear(H,H2)\n        self.lin_bn2 = nn.BatchNorm1d(num_features=H2)\n        self.linear3=nn.Linear(H2,H2)\n        self.lin_bn3 = nn.BatchNorm1d(num_features=H2)\n        \n#         # Latent vectors mu and sigma\n        self.fc1 = nn.Linear(H2, latent_dim)\n        self.bn1 = nn.BatchNorm1d(num_features=latent_dim)\n        self.fc21 = nn.Linear(latent_dim, latent_dim)\n        self.fc22 = nn.Linear(latent_dim, latent_dim)\n\n#         # Sampling vector\n        self.fc3 = nn.Linear(latent_dim, latent_dim)\n        self.fc_bn3 = nn.BatchNorm1d(latent_dim)\n        self.fc4 = nn.Linear(latent_dim, H2)\n        self.fc_bn4 = nn.BatchNorm1d(H2)\n        \n#         # Decoder\n        self.linear4=nn.Linear(H2,H2)\n        self.lin_bn4 = nn.BatchNorm1d(num_features=H2)\n        self.linear5=nn.Linear(H2,H)\n        self.lin_bn5 = nn.BatchNorm1d(num_features=H)\n        self.linear6=nn.Linear(H,D_in)\n        self.lin_bn6 = nn.BatchNorm1d(num_features=D_in)\n        \n        self.leaky_relu = nn.LeakyReLU()\n        \n    def encode(self, x):\n        lin1 = self.leaky_relu(self.lin_bn1(self.linear1(x)))\n        lin2 = self.leaky_relu(self.lin_bn2(self.linear2(lin1)))\n        lin3 = self.leaky_relu(self.lin_bn3(self.linear3(lin2)))\n\n        fc1 = F.leaky_relu(self.bn1(self.fc1(lin3)))\n\n        r1 = self.fc21(fc1)\n        r2 = self.fc22(fc1)\n        \n        return r1, r2\n    \n    def reparameterize(self, mu, logvar):\n        if self.training:\n            std = logvar.mul(0.5).exp_()\n            eps = Variable(std.data.new(std.size()).normal_())\n            return eps.mul(std).add_(mu)\n        else:\n            return mu\n        \n    def decode(self, z):\n        fc3 = self.leaky_relu(self.fc_bn3(self.fc3(z)))\n        fc4 = self.leaky_relu(self.fc_bn4(self.fc4(fc3)))\n\n        lin4 = self.leaky_relu(self.lin_bn4(self.linear4(fc4)))\n        lin5 = self.leaky_relu(self.lin_bn5(self.linear5(lin4)))\n        return self.lin_bn6(self.linear6(lin5))\n    \n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        # self.decode(z) ist spÃ¤ter recon_batch, mu ist mu und logvar ist logvar\n        return self.decode(z), mu, logvar\n    \nclass customLoss(nn.Module):\n    def __init__(self):\n        super(customLoss, self).__init__()\n        self.mse_loss = nn.MSELoss(reduction=\"sum\")\n    \n    # x_recon ist der im forward im Model erstellte recon_batch, x ist der originale x Batch, mu ist mu und logvar ist logvar \n    def forward(self, x_recon, x, mu, logvar):\n        loss_MSE = self.mse_loss(x_recon, x)\n        loss_KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n        return loss_MSE + loss_KLD\n    \n# takes in a module and applies the specified weight initialization\ndef weights_init_uniform_rule(m):\n    classname = m.__class__.__name__\n    # for every Linear layer in a model..\n    if classname.find('Linear') != -1:\n        # get the number of the inputs\n        n = m.in_features\n        y = 1.0/np.sqrt(n)\n        m.weight.data.uniform_(-y, y)\n        m.bias.data.fill_(0)\n        \ndef train(epoch, dataloader, model, optimizer):\n    loss_mse = customLoss()\n    model.train()\n    train_loss = 0\n    for batch_idx, data in enumerate(dataloader):\n        data = data.to(device)\n        optimizer.zero_grad()\n        recon_batch, mu, logvar = model(data)\n        loss = loss_mse(recon_batch, data, mu, logvar)\n        loss.backward()\n        train_loss += loss.item()\n        optimizer.step()\n\n    if epoch % 100 == 0:        \n        print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(dataloader.dataset)))\n        \n        \ndef encode_features(data_loader, model, optimizer):\n    mu_output = []\n    logvar_output = []\n    with torch.no_grad():\n        for i, (data) in enumerate(data_loader):\n            data = data.to(device)\n            optimizer.zero_grad()\n            recon_batch, mu, logvar = model(data)\n\n            mu_tensor = mu   \n            mu_output.append(mu_tensor)\n            mu_result = torch.cat(mu_output, dim=0)\n\n            logvar_tensor = logvar   \n            logvar_output.append(logvar_tensor)\n            logvar_result = torch.cat(logvar_output, dim=0)\n    return mu_result\n\n\ndef get_encoder_features(TRAIN_DATA_PATH, TEST_DATA_PATH, cols, latent_dim, name, train_flag=True):\n    print('Reading train data...')\n    data_set_train=DataBuilder(TRAIN_DATA_PATH, cols)\n    trainloader=DataLoader(dataset=data_set_train,batch_size=512)\n    \n    print('Reading test data...')\n    data_set_test=DataBuilder(TEST_DATA_PATH, cols)\n    testloader=DataLoader(dataset=data_set_test,batch_size=512)\n    \n    print('Reading data...')\n    data_set = DataBuilder([TRAIN_DATA_PATH, TEST_DATA_PATH], cols)\n    loader=DataLoader(dataset=data_set,batch_size=512)\n    \n    \n    print('Prepare params...')\n    D_in = data_set_train.x.shape[1]\n    H = D_in - (D_in - latent_dim) // 4\n    H2 = D_in - (D_in - latent_dim) // 2\n    model = Autoencoder(D_in, H, H2, latent_dim).to(device)\n    model.apply(weights_init_uniform_rule)\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n    loss_mse = customLoss()\n\n    epochs = 500\n    log_interval = 50\n    val_losses = []\n    train_losses = []\n    \n    print(\"train or load model...\")\n    if train_flag:\n        print(\"train model...\")\n        for epoch in range(1, epochs + 1):\n            train(epoch, loader, model, optimizer)\n        torch.save(model.state_dict(), name)\n    else:\n        print(\"load model...\")\n        model.load_state_dict(torch.load(name))\n        model.eval()\n    \n    print(\"extract features...\")\n    train_encoder = encode_features(trainloader, model, optimizer)\n    test_encoder = encode_features(testloader, model, optimizer)\n    \n    return train_encoder, test_encoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_DATA_PATH = '../input/lish-moa/train_features.csv'\nTEST_DATA_PATH = '../input/lish-moa/test_features.csv'\n\ngene_encode_dim = 50\ncell_encode_dim = 30\n\ngenes_model_name = '../input/moa-tabnet-1830/autoencode_genes_50_30_default'\ncells_model_name = '../input/moa-tabnet-1830/autoencode_cells_50_30_default'\n\ntrain_gene_encoder, test_gene_encoder = get_encoder_features(TRAIN_DATA_PATH, TEST_DATA_PATH, GENES, gene_encode_dim, genes_model_name, \n                                                             train_flag=False)\ntrain_cell_encoder, test_cell_encoder = get_encoder_features(TRAIN_DATA_PATH, TEST_DATA_PATH, CELLS, cell_encode_dim, cells_model_name, \n                                                             train_flag=False)\n\ntrain_gene_encoder_pd = pd.DataFrame(train_gene_encoder.cpu().numpy(), columns=[f'VAE_G_{i}' for i in range(gene_encode_dim)])\ntest_gene_encoder_pd = pd.DataFrame(test_gene_encoder.cpu().numpy(), columns=[f'VAE_G_{i}' for i in range(gene_encode_dim)])\n\ntrain_cell_encoder_pd = pd.DataFrame(train_cell_encoder.cpu().numpy(), columns=[f'VAE_C_{i}' for i in range(cell_encode_dim)])\ntest_cell_encoder_pd = pd.DataFrame(test_cell_encoder.cpu().numpy(), columns=[f'VAE_C_{i}' for i in range(cell_encode_dim)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gene_encoder_pd['sig_id'] = train_features['sig_id']\ntrain_cell_encoder_pd['sig_id'] = train_features['sig_id']\ntest_gene_encoder_pd['sig_id'] = test_features['sig_id']\ntest_cell_encoder_pd['sig_id'] = test_features['sig_id']\n\ntrain_features = train_features.merge(train_gene_encoder_pd, on='sig_id')\ntrain_features = train_features.merge(train_cell_encoder_pd, on='sig_id')\n\ntest_features = test_features.merge(test_gene_encoder_pd, on='sig_id')\ntest_features = test_features.merge(test_cell_encoder_pd, on='sig_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import FastICA\nfrom sklearn.decomposition import FactorAnalysis\nfrom sklearn.cluster import FeatureAgglomeration\nfrom sklearn.decomposition import PCA, KernelPCA\n\n\n# PCA\nn_comp = 80\npca_g = load('../input/moa-tabnet-1830/pca_g.joblib')\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = pca_g.transform(data[GENES])\ntrain2 = data2[:train_features.shape[0]]; \ntest2 = data2[-test_features.shape[0]:]\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\nn_comp = 40\npca_c = load('../input/moa-tabnet-1830/pca_c.joblib')\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = pca_c.transform(data[CELLS])\ntrain2 = data2[:train_features.shape[0]]; \ntest2 = data2[-test_features.shape[0]:]\ntrain2 = pd.DataFrame(train2, columns=[f'pca_1_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_1_C-{i}' for i in range(n_comp)])\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\n\n#FastICA\nn_comp = 80\nprint(\"FastICA on genes\")\ntransformer = load('../input/moa-tabnet-1830/FastICA_g.joblib')\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = transformer.transform(data[GENES])\ntrain2 = data2[:train_features.shape[0]]; \ntest2 = data2[-test_features.shape[0]:]\ntrain2 = pd.DataFrame(train2, columns=[f'FastICA_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'FastICA_G-{i}' for i in range(n_comp)])\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\n\nn_comp = 40\nprint(\"FastICA on cells\")\ntransformer = load('../input/moa-tabnet-1830/FastICA_c.joblib')\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = transformer.transform(data[CELLS])\ntrain2 = data2[:train_features.shape[0]]; \ntest2 = data2[-test_features.shape[0]:]\ntrain2 = pd.DataFrame(train2, columns=[f'FastICA_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'FastICA_C-{i}' for i in range(n_comp)])\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\n\n\n# FactorAnalysis\nn_comp = 80\nprint(\"FactorAnalysis on genes\")\ntransformer = load('../input/moa-tabnet-1830/FactorAnalysis_g.joblib')\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = transformer.transform(data[GENES])\ntrain2 = data2[:train_features.shape[0]]; \ntest2 = data2[-test_features.shape[0]:]\ntrain2 = pd.DataFrame(train2, columns=[f'FactorAnalysis_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'FactorAnalysis_G-{i}' for i in range(n_comp)])\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\n\nn_comp = 40\nprint(\"FactorAnalysis on cells\")\ntransformer = load('../input/moa-tabnet-1830/FactorAnalysis_c.joblib')\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = transformer.transform(data[CELLS])\ntrain2 = data2[:train_features.shape[0]]; \ntest2 = data2[-test_features.shape[0]:]\ntrain2 = pd.DataFrame(train2, columns=[f'FactorAnalysis_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'FactorAnalysis_C-{i}' for i in range(n_comp)])\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\n\n# FeatureAgglomeration\nn_comp = 80\nprint(\"FeatureAgglomeration on genes\")\ntransformer = load('../input/moa-tabnet-1830/FeatureAgglomeration_g.joblib')\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = transformer.transform(data[GENES])\ntrain2 = data2[:train_features.shape[0]]; \ntest2 = data2[-test_features.shape[0]:]\ntrain2 = pd.DataFrame(train2, columns=[f'FeatureAgglomeration_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'FeatureAgglomeration_G-{i}' for i in range(n_comp)])\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\n\nn_comp = 40\nprint(\"FeatureAgglomeration on cells\")\ntransformer = load('../input/moa-tabnet-1830/FeatureAgglomeration_c.joblib')\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = transformer.transform(data[CELLS])\ntrain2 = data2[:train_features.shape[0]]; \ntest2 = data2[-test_features.shape[0]:]\ntrain2 = pd.DataFrame(train2, columns=[f'FeatureAgglomeration_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'FeatureAgglomeration_C-{i}' for i in range(n_comp)])\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stat_feat(df, features_g, features_c):\n    df['g_sum'] = df[features_g].sum(axis = 1)\n    df['g_mean'] = df[features_g].mean(axis = 1)\n    df['g_std'] = df[features_g].std(axis = 1)\n    df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n    df['g_skew'] = df[features_g].skew(axis = 1)\n    df['c_sum'] = df[features_c].sum(axis = 1)\n    df['c_mean'] = df[features_c].mean(axis = 1)\n    df['c_std'] = df[features_c].std(axis = 1)\n    df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n    df['c_skew'] = df[features_c].skew(axis = 1)\n    df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n    df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n    df['gc_std'] = df[features_g + features_c].std(axis = 1)\n    df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n    df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n    return df\n\ntrain_features = stat_feat(train_features, GENES, CELLS)\ntest_features = stat_feat(test_features, GENES, CELLS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class VarianceThreshold:\n    def __init__(self, threshold):\n        self.threshold = threshold\n    def fit(self, df, cont_cols):\n        self.cont_cols = cont_cols\n        self.var = df[cont_cols].var()\n        good_cols = self.var[self.var > self.threshold]\n        self.index = good_cols.index.to_list()\n        self.dropcols = [x for x in cont_cols if x not in self.var[self.var > self.threshold].index.to_list()]\n        self.validcols = [x for x in cont_cols if x in self.var[self.var > self.threshold].index.to_list()]\n    def transform(self, df):\n        return df.drop(self.dropcols, axis=1)\n    def fit_transform(self, df, cont_cols):\n        self.fit(df, cont_cols)\n        return self.transform(df), self.validcols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_cols = [col for col in train_features.columns.to_list() if col not in ['sig_id','cp_type','cp_time','cp_dose']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = 0.2 #x_develop[cont_cols].var().sort_values().quantile(0.01)\nprint('Variance Threshold:', threshold)\nVarThres = VarianceThreshold(threshold)\ntrain_features, cont_cols = VarThres.fit_transform(train_features, cont_cols)\ntest_features = VarThres.transform(test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder, QuantileTransformer\n\nqt = load('../input/moa-tabnet-1830/qt.joblib')\ntrain_features[cont_cols] = qt.fit_transform(train_features[cont_cols])\ntest_features[cont_cols] = qt.transform(test_features[cont_cols])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataloader"},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_data(train, test, col, normalize=True, removed_vehicle=False):\n    \"\"\"\n        the first 3 columns represents categories, the others numericals features\n    \"\"\"\n    mapping = {\"cp_type\":{\"trt_cp\": 0, \"ctl_vehicle\":1},\n               \"cp_time\":{48:0, 72:1, 24:2},\n               \"cp_dose\":{\"D1\":0, \"D2\":1}}\n    \n    if removed_vehicle:\n        categories_tr = np.stack([ train[c].apply(lambda x: mapping[c][x]).values for c in col[1:3]], axis=1)\n        categories_test = np.stack([ test[c].apply(lambda x: mapping[c][x]).values for c in col[1:3]], axis=1)\n    else:\n        categories_tr = np.stack([ train[c].apply(lambda x: mapping[c][x]).values for c in col[:3]], axis=1)\n        categories_test = np.stack([ test[c].apply(lambda x: mapping[c][x]).values for c in col[:3]], axis=1)\n    \n    max_ = 10.\n    min_ = -10.\n   \n    if removed_vehicle:\n        numerical_tr = train[col[3:]].values\n        numerical_test = test[col[3:]].values\n    else:\n        numerical_tr = train[col[3:]].values\n        numerical_test = test[col[3:]].values\n    if normalize:\n        numerical_tr = (numerical_tr-min_)/(max_ - min_)\n        numerical_test = (numerical_test-min_)/(max_ - min_)\n    return categories_tr, categories_test, numerical_tr, numerical_test\ncol_features = list(train_features.columns)[1:]\ncat_tr, cat_test, numerical_tr, numerical_test = transform_data(train_features, test_features, col_features, normalize=False, removed_vehicle=remove_vehicle)\ntargets_tr = train_targets_scored[columns].values.astype(np.float32)\ntargets2_tr = train_targets_nonscored[columns_nonscored].values.astype(np.float32)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utils"},{"metadata":{"trusted":true},"cell_type":"code","source":"           \ndef evals(model, X, y, verbose=True):\n    with torch.no_grad():\n        y_preds = model.predict(X)\n        y_preds = torch.clamp(y_preds, 0.0,1.0).detach().numpy()\n    score = log_loss_multi(y, y_preds)\n    #print(\"Logloss = \", score)\n    return y_preds, score\n\n\ndef inference_fn(model, X ,verbose=True):\n    with torch.no_grad():\n        y_preds = model.predict( X )\n        y_preds = torch.sigmoid(torch.as_tensor(y_preds)).numpy()\n    return y_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_loss_score(actual, predicted,  eps=1e-15):\n\n        \"\"\"\n        :param predicted:   The predicted probabilities as floats between 0-1\n        :param actual:      The binary labels. Either 0 or 1.\n        :param eps:         Log(0) is equal to infinity, so we need to offset our predicted values slightly by eps from 0 or 1\n        :return:            The logarithmic loss between between the predicted probability assigned to the possible outcomes for item i, and the actual outcome.\n        \"\"\"\n\n        \n        p1 = actual * np.log(predicted+eps)\n        p0 = (1-actual) * np.log(1-predicted+eps)\n        loss = p0 + p1\n\n        return -loss.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_loss_multi(y_true, y_pred):\n    M = y_true.shape[1]\n    results = np.zeros(M)\n    for i in range(M):\n        results[i] = log_loss_score(y_true[:,i], y_pred[:,i])\n    return results.mean()\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_targets(targets):\n    ### check if targets are all binary in training set\n    \n    for i in range(targets.shape[1]):\n        if len(np.unique(targets[:,i])) != 2:\n            return False\n    return True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def auc_multi(y_true, y_pred):\n    M = y_true.shape[1]\n    results = np.zeros(M)\n    for i in range(M):\n        try:\n            results[i] = roc_auc_score(y_true[:,i], y_pred[:,i])\n        except:\n            pass\n    return results.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"## TABNET\n\nimport torch\nimport numpy as np\nfrom scipy.sparse import csc_matrix\nimport time\nfrom abc import abstractmethod\nfrom pytorch_tabnet import tab_network\nfrom pytorch_tabnet.multiclass_utils import unique_labels\nfrom sklearn.metrics import roc_auc_score, mean_squared_error, accuracy_score\nfrom torch.nn.utils import clip_grad_norm_\nfrom pytorch_tabnet.utils import (PredictDataset,\n                                  create_dataloaders,\n                                  create_explain_matrix)\nfrom sklearn.base import BaseEstimator\nfrom torch.utils.data import DataLoader\nfrom copy import deepcopy\nimport io\nimport json\nfrom pathlib import Path\nimport shutil\nimport zipfile\n\nclass TabModel(BaseEstimator):\n    def __init__(self, n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1,\n                 n_independent=2, n_shared=2, epsilon=1e-15,  momentum=0.02,\n                 lambda_sparse=1e-3, seed=0,\n                 clip_value=1, verbose=1,\n                 optimizer_fn=torch.optim.Adam,\n                 optimizer_params=dict(lr=2e-2),\n                 scheduler_params=None, scheduler_fn=None,\n                 mask_type=\"sparsemax\",\n                 input_dim=None, output_dim=None,\n                 device_name='auto'):\n        \"\"\" Class for TabNet model\n        Parameters\n        ----------\n            device_name: str\n                'cuda' if running on GPU, 'cpu' if not, 'auto' to autodetect\n        \"\"\"\n\n        self.n_d = n_d\n        self.n_a = n_a\n        self.n_steps = n_steps\n        self.gamma = gamma\n        self.cat_idxs = cat_idxs\n        self.cat_dims = cat_dims\n        self.cat_emb_dim = cat_emb_dim\n        self.n_independent = n_independent\n        self.n_shared = n_shared\n        self.epsilon = epsilon\n        self.momentum = momentum\n        self.lambda_sparse = lambda_sparse\n        self.clip_value = clip_value\n        self.verbose = verbose\n        self.optimizer_fn = optimizer_fn\n        self.optimizer_params = optimizer_params\n        self.device_name = device_name\n        self.scheduler_params = scheduler_params\n        self.scheduler_fn = scheduler_fn\n        self.mask_type = mask_type\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n        self.batch_size = 1024\n\n        self.seed = seed\n        torch.manual_seed(self.seed)\n        # Defining device\n        if device_name == 'auto':\n            if torch.cuda.is_available():\n                device_name = 'cuda'\n            else:\n                device_name = 'cpu'\n        self.device = torch.device(device_name)\n        print(f\"Device used : {self.device}\")\n\n    @abstractmethod\n    def construct_loaders(self, X_train, y_train, X_valid, y_valid,\n                          weights, batch_size, num_workers, drop_last):\n        \"\"\"\n        Returns\n        -------\n        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n            Training and validation dataloaders\n        -------\n        \"\"\"\n        raise NotImplementedError('users must define construct_loaders to use this base class')\n\n    def init_network(\n                     self,\n                     input_dim,\n                     output_dim,\n                     n_d,\n                     n_a,\n                     n_steps,\n                     gamma,\n                     cat_idxs,\n                     cat_dims,\n                     cat_emb_dim,\n                     n_independent,\n                     n_shared,\n                     epsilon,\n                     virtual_batch_size,\n                     momentum,\n                     device_name,\n                     mask_type,\n                     ):\n        self.network = tab_network.TabNet(\n            input_dim,\n            output_dim,\n            n_d=n_d,\n            n_a=n_a,\n            n_steps=n_steps,\n            gamma=gamma,\n            cat_idxs=cat_idxs,\n            cat_dims=cat_dims,\n            cat_emb_dim=cat_emb_dim,\n            n_independent=n_independent,\n            n_shared=n_shared,\n            epsilon=epsilon,\n            virtual_batch_size=virtual_batch_size,\n            momentum=momentum,\n            device_name=device_name,\n            mask_type=mask_type).to(self.device)\n\n        self.reducing_matrix = create_explain_matrix(\n            self.network.input_dim,\n            self.network.cat_emb_dim,\n            self.network.cat_idxs,\n            self.network.post_embed_dim)\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, loss_fn=None,\n            weights=0, max_epochs=100, patience=10, batch_size=1024,\n            virtual_batch_size=128, num_workers=0, drop_last=False):\n        \"\"\"Train a neural network stored in self.network\n        Using train_dataloader for training data and\n        valid_dataloader for validation.\n        Parameters\n        ----------\n            X_train: np.ndarray\n                Train set\n            y_train : np.array\n                Train targets\n            X_train: np.ndarray\n                Train set\n            y_train : np.array\n                Train targets\n            weights : bool or dictionnary\n                0 for no balancing\n                1 for automated balancing\n                dict for custom weights per class\n            max_epochs : int\n                Maximum number of epochs during training\n            patience : int\n                Number of consecutive non improving epoch before early stopping\n            batch_size : int\n                Training batch size\n            virtual_batch_size : int\n                Batch size for Ghost Batch Normalization (virtual_batch_size < batch_size)\n            num_workers : int\n                Number of workers used in torch.utils.data.DataLoader\n            drop_last : bool\n                Whether to drop last batch during training\n        \"\"\"\n        # update model name\n\n        self.update_fit_params(X_train, y_train, X_valid, y_valid, loss_fn,\n                               weights, max_epochs, patience, batch_size,\n                               virtual_batch_size, num_workers, drop_last)\n\n        train_dataloader, valid_dataloader = self.construct_loaders(X_train,\n                                                                    y_train,\n                                                                    X_valid,\n                                                                    y_valid,\n                                                                    self.updated_weights,\n                                                                    self.batch_size,\n                                                                    self.num_workers,\n                                                                    self.drop_last)\n\n        self.init_network(\n            input_dim=self.input_dim,\n            output_dim=self.output_dim,\n            n_d=self.n_d,\n            n_a=self.n_a,\n            n_steps=self.n_steps,\n            gamma=self.gamma,\n            cat_idxs=self.cat_idxs,\n            cat_dims=self.cat_dims,\n            cat_emb_dim=self.cat_emb_dim,\n            n_independent=self.n_independent,\n            n_shared=self.n_shared,\n            epsilon=self.epsilon,\n            virtual_batch_size=self.virtual_batch_size,\n            momentum=self.momentum,\n            device_name=self.device_name,\n            mask_type=self.mask_type\n        )\n\n        self.optimizer = self.optimizer_fn(self.network.parameters(),\n                                           **self.optimizer_params)\n\n        if self.scheduler_fn:\n            self.scheduler = self.scheduler_fn(self.optimizer, **self.scheduler_params)\n        else:\n            self.scheduler = None\n\n        self.losses_train = []\n        self.losses_valid = []\n        self.learning_rates = []\n        self.metrics_train = []\n        self.metrics_valid = []\n\n        if self.verbose > 0:\n            print(\"Will train until validation stopping metric\",\n                  f\"hasn't improved in {self.patience} rounds.\")\n            msg_epoch = f'| EPOCH |  train  |   valid  | total time (s)'\n            print('---------------------------------------')\n            print(msg_epoch)\n\n        total_time = 0\n        while (self.epoch < self.max_epochs and\n               self.patience_counter < self.patience):\n            starting_time = time.time()\n            # updates learning rate history\n            self.learning_rates.append(self.optimizer.param_groups[-1][\"lr\"])\n\n            fit_metrics = self.fit_epoch(train_dataloader, valid_dataloader)\n\n            # leaving it here, may be used for callbacks later\n            self.losses_train.append(fit_metrics['train']['loss_avg'])\n            self.losses_valid.append(fit_metrics['valid']['total_loss'])\n            self.metrics_train.append(fit_metrics['train']['stopping_loss'])\n            self.metrics_valid.append(fit_metrics['valid']['stopping_loss'])\n\n            stopping_loss = fit_metrics['valid']['stopping_loss']\n            if stopping_loss < self.best_cost:\n                self.best_cost = stopping_loss\n                self.patience_counter = 0\n                # Saving model\n                self.best_network = deepcopy(self.network)\n                has_improved = True\n            else:\n                self.patience_counter += 1\n                has_improved=False\n            self.epoch += 1\n            total_time += time.time() - starting_time\n            if self.verbose > 0:\n                if self.epoch % self.verbose == 0:\n                    separator = \"|\"\n                    msg_epoch = f\"| {self.epoch:<5} | \"\n                    msg_epoch += f\" {fit_metrics['train']['stopping_loss']:.5f}\"\n                    msg_epoch += f' {separator:<2} '\n                    msg_epoch += f\" {fit_metrics['valid']['stopping_loss']:.5f}\"\n                    msg_epoch += f' {separator:<2} '\n                    msg_epoch += f\" {np.round(total_time, 1):<10}\"\n                    msg_epoch += f\" {has_improved}\"\n                    print(msg_epoch)\n\n        if self.verbose > 0:\n            if self.patience_counter == self.patience:\n                print(f\"Early stopping occured at epoch {self.epoch}\")\n            print(f\"Training done in {total_time:.3f} seconds.\")\n            print('---------------------------------------')\n\n        self.history = {\"train\": {\"loss\": self.losses_train,\n                                  \"metric\": self.metrics_train,\n                                  \"lr\": self.learning_rates},\n                        \"valid\": {\"loss\": self.losses_valid,\n                                  \"metric\": self.metrics_valid}}\n        # load best models post training\n        self.load_best_model()\n\n        # compute feature importance once the best model is defined\n        self._compute_feature_importances(train_dataloader)\n\n    def save_model(self, path):\n        \"\"\"\n        Saving model with two distinct files.\n        \"\"\"\n        saved_params = {}\n        for key, val in self.get_params().items():\n            if isinstance(val, type):\n                # Don't save torch specific params\n                continue\n            else:\n                saved_params[key] = val\n\n        # Create folder\n        Path(path).mkdir(parents=True, exist_ok=True)\n\n        # Save models params\n        with open(Path(path).joinpath(\"model_params.json\"), \"w\", encoding=\"utf8\") as f:\n            json.dump(saved_params, f)\n\n        # Save state_dict\n        torch.save(self.network.state_dict(), Path(path).joinpath(\"network.pt\"))\n        shutil.make_archive(path, 'zip', path)\n        shutil.rmtree(path)\n        print(f\"Successfully saved model at {path}.zip\")\n        return f\"{path}.zip\"\n\n    def load_model(self, filepath):\n\n        try:\n            try:\n                with zipfile.ZipFile(filepath) as z:\n                    with z.open(\"model_params.json\") as f:\n                        loaded_params = json.load(f)\n                    with z.open(\"network.pt\") as f:\n                        try:\n                            saved_state_dict = torch.load(f)\n                        except io.UnsupportedOperation:\n                            # In Python <3.7, the returned file object is not seekable (which at least\n                            # some versions of PyTorch require) - so we'll try buffering it in to a\n                            # BytesIO instead:\n                            saved_state_dict = torch.load(io.BytesIO(f.read()))\n                            \n            except:\n                with open(os.path.join(filepath, \"model_params.json\")) as f:\n                        loaded_params = json.load(f)\n\n                saved_state_dict = torch.load(os.path.join(filepath, \"network.pt\"), map_location=\"cpu\")\n \n        except KeyError:\n            raise KeyError(\"Your zip file is missing at least one component\")\n\n        #print(loaded_params)\n        if torch.cuda.is_available():\n            device_name = 'cuda'\n        else:\n            device_name = 'cpu'\n        loaded_params[\"device_name\"] = device_name\n        self.__init__(**loaded_params)\n        \n        \n\n        self.init_network(\n            input_dim=self.input_dim,\n            output_dim=self.output_dim,\n            n_d=self.n_d,\n            n_a=self.n_a,\n            n_steps=self.n_steps,\n            gamma=self.gamma,\n            cat_idxs=self.cat_idxs,\n            cat_dims=self.cat_dims,\n            cat_emb_dim=self.cat_emb_dim,\n            n_independent=self.n_independent,\n            n_shared=self.n_shared,\n            epsilon=self.epsilon,\n            virtual_batch_size=1024,\n            momentum=self.momentum,\n            device_name=self.device_name,\n            mask_type=self.mask_type\n        )\n        self.network.load_state_dict(saved_state_dict)\n        self.network.eval()\n        return\n\n    def fit_epoch(self, train_dataloader, valid_dataloader):\n        \"\"\"\n        Evaluates and updates network for one epoch.\n        Parameters\n        ----------\n            train_dataloader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with train set\n            valid_dataloader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with valid set\n        \"\"\"\n        train_metrics = self.train_epoch(train_dataloader)\n        valid_metrics = self.predict_epoch(valid_dataloader)\n\n        fit_metrics = {'train': train_metrics,\n                       'valid': valid_metrics}\n\n        return fit_metrics\n\n    @abstractmethod\n    def train_epoch(self, train_loader):\n        \"\"\"\n        Trains one epoch of the network in self.network\n        Parameters\n        ----------\n            train_loader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with train set\n        \"\"\"\n        raise NotImplementedError('users must define train_epoch to use this base class')\n\n    @abstractmethod\n    def train_batch(self, data, targets):\n        \"\"\"\n        Trains one batch of data\n        Parameters\n        ----------\n            data: a :tensor: `torch.tensor`\n                Input data\n            target: a :tensor: `torch.tensor`\n                Target data\n        \"\"\"\n        raise NotImplementedError('users must define train_batch to use this base class')\n\n    @abstractmethod\n    def predict_epoch(self, loader):\n        \"\"\"\n        Validates one epoch of the network in self.network\n        Parameters\n        ----------\n            loader: a :class: `torch.utils.data.Dataloader`\n                    DataLoader with validation set\n        \"\"\"\n        raise NotImplementedError('users must define predict_epoch to use this base class')\n\n    @abstractmethod\n    def predict_batch(self, data, targets):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            batch_outs: dict\n        \"\"\"\n        raise NotImplementedError('users must define predict_batch to use this base class')\n\n    def load_best_model(self):\n        if self.best_network is not None:\n            self.network = self.best_network\n\n    @abstractmethod\n    def predict(self, X):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            predictions: np.array\n                Predictions of the regression problem or the last class\n        \"\"\"\n        raise NotImplementedError('users must define predict to use this base class')\n\n    def explain(self, X):\n        \"\"\"\n        Return local explanation\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            M_explain: matrix\n                Importance per sample, per columns.\n            masks: matrix\n                Sparse matrix showing attention masks used by network.\n        \"\"\"\n        self.network.eval()\n\n        dataloader = DataLoader(PredictDataset(X),\n                                batch_size=self.batch_size, shuffle=False)\n\n        for batch_nb, data in enumerate(dataloader):\n            data = data.to(self.device).float()\n\n            M_explain, masks = self.network.forward_masks(data)\n            for key, value in masks.items():\n                masks[key] = csc_matrix.dot(value.cpu().detach().numpy(),\n                                            self.reducing_matrix)\n\n            if batch_nb == 0:\n                res_explain = csc_matrix.dot(M_explain.cpu().detach().numpy(),\n                                             self.reducing_matrix)\n                res_masks = masks\n            else:\n                res_explain = np.vstack([res_explain,\n                                         csc_matrix.dot(M_explain.cpu().detach().numpy(),\n                                                        self.reducing_matrix)])\n                for key, value in masks.items():\n                    res_masks[key] = np.vstack([res_masks[key], value])\n        return res_explain, res_masks\n\n    def _compute_feature_importances(self, loader):\n        self.network.eval()\n        feature_importances_ = np.zeros((self.network.post_embed_dim))\n        for data, targets in loader:\n            data = data.to(self.device).float()\n            M_explain, masks = self.network.forward_masks(data)\n            feature_importances_ += M_explain.sum(dim=0).cpu().detach().numpy()\n\n        feature_importances_ = csc_matrix.dot(feature_importances_,\n                                              self.reducing_matrix)\n        self.feature_importances_ = feature_importances_ / np.sum(feature_importances_)\n        \n        \nclass TabNetRegressor(TabModel):\n\n    def construct_loaders(self, X_train, y_train, X_valid, y_valid, weights,\n                          batch_size, num_workers, drop_last):\n        \"\"\"\n        Returns\n        -------\n        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n            Training and validation dataloaders\n        -------\n        \"\"\"\n        if isinstance(weights, int):\n            if weights == 1:\n                raise ValueError(\"Please provide a list of weights for regression.\")\n        if isinstance(weights, dict):\n            raise ValueError(\"Please provide a list of weights for regression.\")\n\n        train_dataloader, valid_dataloader = create_dataloaders(X_train,\n                                                                y_train,\n                                                                X_valid,\n                                                                y_valid,\n                                                                weights,\n                                                                batch_size,\n                                                                num_workers,\n                                                                drop_last)\n        return train_dataloader, valid_dataloader\n\n    def update_fit_params(self, X_train, y_train, X_valid, y_valid, loss_fn,\n                          weights, max_epochs, patience,\n                          batch_size, virtual_batch_size, num_workers, drop_last):\n\n        if loss_fn is None:\n            self.loss_fn = torch.nn.functional.mse_loss\n        else:\n            self.loss_fn = loss_fn\n\n        assert X_train.shape[1] == X_valid.shape[1], \"Dimension mismatch X_train X_valid\"\n        self.input_dim = X_train.shape[1]\n\n        if len(y_train.shape) == 1:\n            raise ValueError(\"\"\"Please apply reshape(-1, 1) to your targets\n                                if doing single regression.\"\"\")\n        assert y_train.shape[1] == y_valid.shape[1], \"Dimension mismatch y_train y_valid\"\n        self.output_dim = y_train.shape[1]\n\n        self.updated_weights = weights\n\n        self.max_epochs = max_epochs\n        self.patience = patience\n        self.batch_size = batch_size\n        self.virtual_batch_size = virtual_batch_size\n        # Initialize counters and histories.\n        self.patience_counter = 0\n        self.epoch = 0\n        self.best_cost = np.inf\n        self.num_workers = num_workers\n        self.drop_last = drop_last\n\n    def train_epoch(self, train_loader):\n        \"\"\"\n        Trains one epoch of the network in self.network\n        Parameters\n        ----------\n            train_loader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with train set\n        \"\"\"\n\n        self.network.train()\n        y_preds = []\n        ys = []\n        total_loss = 0\n\n        for data, targets in train_loader:\n            batch_outs = self.train_batch(data, targets)\n            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n            total_loss += batch_outs[\"loss\"]\n\n        y_preds = np.vstack(y_preds)\n        ys = np.vstack(ys)\n\n        #stopping_loss = mean_squared_error(y_true=ys, y_pred=y_preds)\n        stopping_loss =log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  )\n        total_loss = total_loss / len(train_loader)\n        epoch_metrics = {'loss_avg': total_loss,\n                         'stopping_loss': total_loss,\n                         }\n\n        if self.scheduler is not None:\n            self.scheduler.step()\n        return epoch_metrics\n\n    def train_batch(self, data, targets):\n        \"\"\"\n        Trains one batch of data\n        Parameters\n        ----------\n            data: a :tensor: `torch.tensor`\n                Input data\n            target: a :tensor: `torch.tensor`\n                Target data\n        \"\"\"\n        self.network.train()\n        data = data.to(self.device).float()\n\n        targets = targets.to(self.device).float()\n        self.optimizer.zero_grad()\n\n        output, M_loss = self.network(data)\n\n        loss = self.loss_fn(output, targets)\n        \n        loss -= self.lambda_sparse*M_loss\n\n        loss.backward()\n        if self.clip_value:\n            clip_grad_norm_(self.network.parameters(), self.clip_value)\n        self.optimizer.step()\n\n        loss_value = loss.item()\n        batch_outs = {'loss': loss_value,\n                      'y_preds': output,\n                      'y': targets}\n        return batch_outs\n\n    def predict_epoch(self, loader):\n        \"\"\"\n        Validates one epoch of the network in self.network\n        Parameters\n        ----------\n            loader: a :class: `torch.utils.data.Dataloader`\n                    DataLoader with validation set\n        \"\"\"\n        y_preds = []\n        ys = []\n        self.network.eval()\n        total_loss = 0\n\n        for data, targets in loader:\n            batch_outs = self.predict_batch(data, targets)\n            total_loss += batch_outs[\"loss\"]\n            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n\n        y_preds = np.vstack(y_preds)\n        ys = np.vstack(ys)\n\n        stopping_loss =log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  ) #mean_squared_error(y_true=ys, y_pred=y_preds)\n\n        total_loss = total_loss / len(loader)\n        epoch_metrics = {'total_loss': total_loss,\n                         'stopping_loss': stopping_loss}\n\n        return epoch_metrics\n\n    def predict_batch(self, data, targets):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            batch_outs: dict\n        \"\"\"\n        self.network.eval()\n        data = data.to(self.device).float()\n        targets = targets.to(self.device).float()\n\n        output, M_loss = self.network(data)\n       \n        loss = self.loss_fn(output, targets)\n        #print(self.loss_fn, loss)\n        loss -= self.lambda_sparse*M_loss\n        #print(loss)\n        loss_value = loss.item()\n        batch_outs = {'loss': loss_value,\n                      'y_preds': output,\n                      'y': targets}\n        return batch_outs\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            predictions: np.array\n                Predictions of the regression problem\n        \"\"\"\n        self.network.eval()\n        dataloader = DataLoader(PredictDataset(X),\n                                batch_size=self.batch_size, shuffle=False)\n\n        results = []\n        for batch_nb, data in enumerate(dataloader):\n            data = data.to(self.device).float()\n\n            output, M_loss = self.network(data)\n            predictions = output.cpu().detach().numpy()\n            results.append(predictions)\n        res = np.vstack(results)\n        return res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# script"},{"metadata":{"trusted":true},"cell_type":"code","source":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Config(object):\n    def __init__(self):\n        self.num_class = targets_tr.shape[1]\n        self.verbose=False\n        #\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.SPLITS = 30\n        self.EPOCHS = 200\n        self.num_ensembling = 1\n        self.seed = 42\n        # Parameters model\n        self.cat_emb_dim=[1] * cat_tr.shape[1] #to choose\n        self.cats_idx = list(range(cat_tr.shape[1]))\n        self.cat_dims = [len(np.unique(cat_tr[:, i])) for i in range(cat_tr.shape[1])]\n        self.num_numericals= numerical_tr.shape[1]\n        # save\n        self.save_name = \"../input/moa-tabnet-1830/raw_step1_dae_new/tabnet_raw_step1\"\n        \n        self.strategy = \"KFOLD\" # \ncfg = Config()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = np.concatenate([cat_test, numerical_test], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if cfg.strategy == \"KFOLD\":\n    oof_preds_all = []\n    oof_targets_all = []\n    scores_all =  []\n    scores_auc_all= []\n    preds_test = []\n    idx_all = []\n    for seed in range(cfg.num_ensembling):\n        print(\"## SEED : \", seed)\n        mskf = MultilabelStratifiedKFold(n_splits=cfg.SPLITS, random_state=cfg.seed+seed, shuffle=True)\n        oof_preds = []\n        oof_targets = []\n        scores = []\n        scores_auc = []\n        p = []\n        for j, (train_idx, val_idx) in enumerate(mskf.split(np.zeros(len(cat_tr)), targets_tr)):\n            print(\"FOLDS : \", j)\n            idx_all.append(val_idx)\n\n            ## model\n            X_train, y_train = torch.as_tensor(np.concatenate([cat_tr[train_idx], numerical_tr[train_idx] ], axis=1)), torch.as_tensor(targets_tr[train_idx])\n            X_val, y_val = torch.as_tensor(np.concatenate([cat_tr[val_idx], numerical_tr[val_idx] ], axis=1)), torch.as_tensor(targets_tr[val_idx])\n            model = TabNetRegressor(n_d=24, n_a=64, n_steps=1, gamma=1.3, lambda_sparse=0, cat_dims=cfg.cat_dims, cat_emb_dim=cfg.cat_emb_dim, cat_idxs=cfg.cats_idx, optimizer_fn=torch.optim.Adam,\n                                   optimizer_params=dict(lr=2e-2), mask_type='entmax', device_name=cfg.device, scheduler_params=dict(milestones=[ 50,100,150], gamma=0.9), scheduler_fn=torch.optim.lr_scheduler.MultiStepLR)\n            #'sparsemax'\n            \n            name = cfg.save_name + f\"_fold{j}_{seed}\"\n            model.load_model(name)\n            # preds on val\n            preds = model.predict(X_val)\n            preds = torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n            score = log_loss_multi(y_val, preds)\n            \n            # preds on test\n            temp = model.predict(X_test)\n            p.append(torch.sigmoid(torch.as_tensor(temp)).detach().cpu().numpy())\n            ## save oof to compute the CV later\n            oof_preds.append(preds)\n            oof_targets.append(y_val)\n            scores.append(score)\n            scores_auc.append(auc_multi(y_val,preds))\n            print(f\"validation fold {j} : {score}\")\n        p = np.stack(p)\n        preds_test.append(p)\n        oof_preds_all.append(np.concatenate(oof_preds))\n        oof_targets_all.append(np.concatenate(oof_targets))\n        scores_all.append(np.array(scores))\n        scores_auc_all.append(np.array(scores_auc))\n    preds_test = np.stack(preds_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx_list = []\nfor idx_ in idx_all:\n    for ix in idx_:\n       idx_list.append(ix)\noof_preds_df = pd.DataFrame(oof_preds_all[0])\noof_preds_df['idx'] = idx_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_org = pd.read_csv('../input/lish-moa/train_features.csv')\ntarget_org = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_org = train_org.loc[train_org['cp_type']=='trt_cp'].reset_index(drop=True)\n\ntrain_org['idx'] = train_org.index\noof_final = train_org.merge(oof_preds_df, how='left', on='idx')[list(range(206)) + ['sig_id']]\noof_final = target_org.merge(oof_final, how='left', on='sig_id')[list(range(206))].fillna(0.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = target_org[[col for col in target_org.columns.to_list() if col != 'sig_id']]\nlog_loss_multi(targets.to_numpy(), oof_final.to_numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if cfg.strategy == \"KFOLD\":\n    for i in range(cfg.num_ensembling): \n        print(\"CV score fold : \", log_loss_multi(oof_targets_all[i], oof_preds_all[i]))\n        print(\"auc mean : \", sum(scores_auc_all[i])/len(scores_auc_all[i]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SAVE CSV"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[columns] = preds_test.mean(1).mean(0)\nsubmission.loc[test_features['cp_type']=='ctl_vehicle', submission.columns[1:]] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nsub = submission.loc[test_features.cp_type=='trt_cp']\nx = sub.iloc[:, 1:].values.flatten()\nprint(f'min x is {min(x)} and max x is{max(x)}')\nplt.figure(figsize=(20, 5))\nplt.subplot(1, 2, 1)\nx2 = x[x < 0.01]\nplt.hist(x2, bins=100)\nplt.subplot(1, 2, 2)\nx2 = x[x > 0.9]\nplt.hist(x2, bins=100)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}