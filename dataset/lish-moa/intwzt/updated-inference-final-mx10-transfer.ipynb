{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/moaiterative/iterative_stratification-0.1.6-py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom sklearn.preprocessing import OneHotEncoder, QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import (KNeighborsClassifier, NeighborhoodComponentsAnalysis)\nfrom joblib import dump, load\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras import layers,regularizers,Sequential,backend,callbacks,optimizers,metrics,losses\n\nfrom tensorflow.keras.callbacks import (\n    ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nSEED = 1984\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"x_develop = pd.read_csv('../input/lish-moa/train_features.csv')\ny_develop = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n\nx_test= pd.read_csv('../input/lish-moa/test_features.csv')\nsub = pd.read_csv('../input/lish-moa/sample_submission.csv')\n\nc_cols = x_develop.columns[x_develop.columns.str.startswith('c-')]\ng_cols = x_develop.columns[x_develop.columns.str.startswith('g-')]\ntarget_cols = y_develop.columns[1:]  # All columns except sig_id\nN_TARGETS = len(target_cols)\nN_TARGETS","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"#### Encode Categorical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_df(df):\n    if 'sig_id' in df.columns:\n        df.set_index('sig_id', inplace=True)\n    df['cp_type'] = (df['cp_type'] == 'trt_cp').astype(int)\n    df['cp_dose'] = (df['cp_dose'] == 'D2').astype(int)\n    \n    df = df.join(pd.get_dummies(df['cp_time'], drop_first=False, prefix='cp_time'))\n    df = df.drop('cp_time', axis=1)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_develop = preprocess_df(x_develop)\ny_develop = y_develop.set_index('sig_id')\nx_test = preprocess_df(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\n\nfrom sklearn import preprocessing\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef load_data(train_path, cols):\n    # read in from csv\n    if isinstance(train_path, list):\n        df_base = pd.read_csv(train_path[0])\n        for i in range(1, len(train_path)):\n            df_base.append(pd.read_csv(train_path[i]))\n        df_base = df_base[cols]\n    else:\n        df = pd.read_csv(train_path)\n        df_base = df[cols]\n    x = df_base.values.reshape(-1, df_base.shape[1]).astype('float32')\n    # stadardize values\n    standardizer = preprocessing.StandardScaler()\n    x = standardizer.fit_transform(x)    \n    return x, standardizer\n\ndef numpyToTensor(x):\n    x_train = torch.from_numpy(x).to(device)\n    return x_train\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass DataBuilder(Dataset):\n    def __init__(self, path, cols):\n        self.x, self.standardizer = load_data(path, cols)\n        self.x = numpyToTensor(self.x)\n        self.len=self.x.shape[0]\n    def __getitem__(self,index):      \n        return self.x[index]\n    def __len__(self):\n        return self.len","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Autoencoder(nn.Module):\n    def __init__(self,D_in, H, H2, latent_dim):\n        \n        #Encoder\n        super(Autoencoder,self).__init__()\n        self.linear1=nn.Linear(D_in,H)\n        self.lin_bn1 = nn.BatchNorm1d(num_features=H)\n        self.linear2=nn.Linear(H,H2)\n        self.lin_bn2 = nn.BatchNorm1d(num_features=H2)\n        self.linear3=nn.Linear(H2,H2)\n        self.lin_bn3 = nn.BatchNorm1d(num_features=H2)\n        \n#         # Latent vectors mu and sigma\n        self.fc1 = nn.Linear(H2, latent_dim)\n        self.bn1 = nn.BatchNorm1d(num_features=latent_dim)\n        self.fc21 = nn.Linear(latent_dim, latent_dim)\n        self.fc22 = nn.Linear(latent_dim, latent_dim)\n\n#         # Sampling vector\n        self.fc3 = nn.Linear(latent_dim, latent_dim)\n        self.fc_bn3 = nn.BatchNorm1d(latent_dim)\n        self.fc4 = nn.Linear(latent_dim, H2)\n        self.fc_bn4 = nn.BatchNorm1d(H2)\n        \n#         # Decoder\n        self.linear4=nn.Linear(H2,H2)\n        self.lin_bn4 = nn.BatchNorm1d(num_features=H2)\n        self.linear5=nn.Linear(H2,H)\n        self.lin_bn5 = nn.BatchNorm1d(num_features=H)\n        self.linear6=nn.Linear(H,D_in)\n        self.lin_bn6 = nn.BatchNorm1d(num_features=D_in)\n        \n        self.leaky_relu = nn.LeakyReLU()\n        \n    def encode(self, x):\n        lin1 = self.leaky_relu(self.lin_bn1(self.linear1(x)))\n        lin2 = self.leaky_relu(self.lin_bn2(self.linear2(lin1)))\n        lin3 = self.leaky_relu(self.lin_bn3(self.linear3(lin2)))\n\n        fc1 = F.leaky_relu(self.bn1(self.fc1(lin3)))\n\n        r1 = self.fc21(fc1)\n        r2 = self.fc22(fc1)\n        \n        return r1, r2\n    \n    def reparameterize(self, mu, logvar):\n        if self.training:\n            std = logvar.mul(0.5).exp_()\n            eps = Variable(std.data.new(std.size()).normal_())\n            return eps.mul(std).add_(mu)\n        else:\n            return mu\n        \n    def decode(self, z):\n        fc3 = self.leaky_relu(self.fc_bn3(self.fc3(z)))\n        fc4 = self.leaky_relu(self.fc_bn4(self.fc4(fc3)))\n\n        lin4 = self.leaky_relu(self.lin_bn4(self.linear4(fc4)))\n        lin5 = self.leaky_relu(self.lin_bn5(self.linear5(lin4)))\n        return self.lin_bn6(self.linear6(lin5))\n    \n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        # self.decode(z) ist spÃ¤ter recon_batch, mu ist mu und logvar ist logvar\n        return self.decode(z), mu, logvar\n    \nclass customLoss(nn.Module):\n    def __init__(self):\n        super(customLoss, self).__init__()\n        self.mse_loss = nn.MSELoss(reduction=\"sum\")\n    \n    # x_recon ist der im forward im Model erstellte recon_batch, x ist der originale x Batch, mu ist mu und logvar ist logvar \n    def forward(self, x_recon, x, mu, logvar):\n        loss_MSE = self.mse_loss(x_recon, x)\n        loss_KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n        return loss_MSE + loss_KLD\n    \n# takes in a module and applies the specified weight initialization\ndef weights_init_uniform_rule(m):\n    classname = m.__class__.__name__\n    # for every Linear layer in a model..\n    if classname.find('Linear') != -1:\n        # get the number of the inputs\n        n = m.in_features\n        y = 1.0/np.sqrt(n)\n        m.weight.data.uniform_(-y, y)\n        m.bias.data.fill_(0)\n        \ndef train(epoch, dataloader, model, optimizer):\n    loss_mse = customLoss()\n    model.train()\n    train_loss = 0\n    for batch_idx, data in enumerate(dataloader):\n        data = data.to(device)\n        optimizer.zero_grad()\n        recon_batch, mu, logvar = model(data)\n        loss = loss_mse(recon_batch, data, mu, logvar)\n        loss.backward()\n        train_loss += loss.item()\n        optimizer.step()\n\n    if epoch % 100 == 0:        \n        print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(dataloader.dataset)))\n        \n        \ndef encode_features(data_loader, model, optimizer):\n    mu_output = []\n    logvar_output = []\n    with torch.no_grad():\n        for i, (data) in enumerate(data_loader):\n            data = data.to(device)\n            optimizer.zero_grad()\n            recon_batch, mu, logvar = model(data)\n\n            mu_tensor = mu   \n            mu_output.append(mu_tensor)\n            mu_result = torch.cat(mu_output, dim=0)\n\n            logvar_tensor = logvar   \n            logvar_output.append(logvar_tensor)\n            logvar_result = torch.cat(logvar_output, dim=0)\n    return mu_result\n\n\ndef get_encoder_features(TRAIN_DATA_PATH, TEST_DATA_PATH, cols, latent_dim, name, train_flag=True):\n    print('Reading train data...')\n    data_set_train=DataBuilder(TRAIN_DATA_PATH, cols)\n    trainloader=DataLoader(dataset=data_set_train,batch_size=512)\n    \n    print('Reading test data...')\n    data_set_test=DataBuilder(TEST_DATA_PATH, cols)\n    testloader=DataLoader(dataset=data_set_test,batch_size=512)\n    \n    print('Reading data...')\n    data_set = DataBuilder([TRAIN_DATA_PATH, TEST_DATA_PATH], cols)\n    loader=DataLoader(dataset=data_set,batch_size=512)\n    \n    \n    print('Prepare params...')\n    D_in = data_set_train.x.shape[1]\n    H = D_in - (D_in - latent_dim) // 4\n    H2 = D_in - (D_in - latent_dim) // 2\n    model = Autoencoder(D_in, H, H2, latent_dim).to(device)\n    model.apply(weights_init_uniform_rule)\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n    loss_mse = customLoss()\n\n    epochs = 1000\n    log_interval = 50\n    val_losses = []\n    train_losses = []\n    \n    print(\"train or load model...\")\n    if train_flag:\n        print(\"train model...\")\n        for epoch in range(1, epochs + 1):\n            train(epoch, loader, model, optimizer)\n        torch.save(model.state_dict(), name)\n    else:\n        print(\"load model...\")\n        model.load_state_dict(torch.load(name))\n        model.eval()\n    \n    print(\"extract features...\")\n    train_encoder = encode_features(trainloader, model, optimizer)\n    test_encoder = encode_features(testloader, model, optimizer)\n    \n    return train_encoder, test_encoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_DATA_PATH = '../input/lish-moa/train_features.csv'\nTEST_DATA_PATH = '../input/lish-moa/test_features.csv'\n\ngene_encode_dim = 50\ncell_encode_dim = 30\n\nGENES = g_cols.to_list()\nCELLS = c_cols.to_list()\n\ngenes_model_name = f'../input/update-mx10-model/autoencode_genes_50_30_default'\ncells_model_name = f'../input/update-mx10-model/autoencode_cells_50_30_default'\n\ntrain_gene_encoder, test_gene_encoder = get_encoder_features(TRAIN_DATA_PATH, TEST_DATA_PATH, GENES, gene_encode_dim, genes_model_name, \n                                                             train_flag=False)\ntrain_cell_encoder, test_cell_encoder = get_encoder_features(TRAIN_DATA_PATH, TEST_DATA_PATH, CELLS, cell_encode_dim, cells_model_name, \n                                                             train_flag=False)\n\ntrain_gene_encoder_pd = pd.DataFrame(train_gene_encoder.cpu().numpy(), columns=[f'VAE_G_{i}' for i in range(gene_encode_dim)])\ntest_gene_encoder_pd = pd.DataFrame(test_gene_encoder.cpu().numpy(), columns=[f'VAE_G_{i}' for i in range(gene_encode_dim)])\n\ntrain_cell_encoder_pd = pd.DataFrame(train_cell_encoder.cpu().numpy(), columns=[f'VAE_C_{i}' for i in range(cell_encode_dim)])\ntest_cell_encoder_pd = pd.DataFrame(test_cell_encoder.cpu().numpy(), columns=[f'VAE_C_{i}' for i in range(cell_encode_dim)])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gene_encoder_pd['sig_id'] = list(x_develop.index)\ntrain_cell_encoder_pd['sig_id'] = list(x_develop.index)\ntest_gene_encoder_pd['sig_id'] = list(x_test.index)\ntest_cell_encoder_pd['sig_id'] = list(x_test.index)\n\nx_develop = x_develop.merge(train_gene_encoder_pd, on='sig_id')\nx_develop = x_develop.merge(train_cell_encoder_pd, on='sig_id')\n\nx_test = x_test.merge(test_gene_encoder_pd, on='sig_id')\nx_test = x_test.merge(test_cell_encoder_pd, on='sig_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### PCA Decomposition"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import FactorAnalysis, FastICA\nfrom sklearn.cluster import FeatureAgglomeration\n# g-features\nn_comp = 80\ntransformer = load('../input/update-mx10-model/mx10_pca_g.joblib')\nx_develop_pca = transformer.transform(x_develop[g_cols])\nx_develop_pca = pd.DataFrame(x_develop_pca, columns=[f'pca_g-{i}' for i in range(n_comp)], index=x_develop.index)\nx_develop = pd.concat((x_develop, x_develop_pca), axis=1)\n\nx_test_pca = transformer.transform(x_test[g_cols])\nx_test_pca = pd.DataFrame(x_test_pca, columns=[f'pca_g-{i}' for i in range(n_comp)], index=x_test.index)\nx_test = pd.concat((x_test, x_test_pca), axis=1)\n\n# c-features\nn_comp = 40\ntransformer = load('../input/update-mx10-model/mx10_pca_c.joblib')\nx_develop_pca = transformer.transform(x_develop[c_cols])\nx_develop_pca = pd.DataFrame(x_develop_pca, columns=[f'pca_c-{i}' for i in range(n_comp)], index=x_develop.index)\nx_develop = pd.concat((x_develop, x_develop_pca), axis=1)\n\nx_test_pca = transformer.transform(x_test[c_cols])\nx_test_pca = pd.DataFrame(x_test_pca, columns=[f'pca_c-{i}' for i in range(n_comp)], index=x_test.index)\nx_test = pd.concat((x_test, x_test_pca), axis=1)\n# ------------------------------------------------------------------------\nn_comp = 80\ntransformer = load('../input/update-mx10-model/mx10_FastICA_g.joblib')\nx_develop_pca = transformer.transform(x_develop[g_cols])\nx_develop_pca = pd.DataFrame(x_develop_pca, columns=[f'FastICA_g-{i}' for i in range(n_comp)], index=x_develop.index)\nx_develop = pd.concat((x_develop, x_develop_pca), axis=1)\n\nx_test_pca = transformer.transform(x_test[g_cols])\nx_test_pca = pd.DataFrame(x_test_pca, columns=[f'FastICA_g-{i}' for i in range(n_comp)], index=x_test.index)\nx_test = pd.concat((x_test, x_test_pca), axis=1)\n\n# c-features\nn_comp = 40\ntransformer = load('../input/update-mx10-model/mx10_FastICA_c.joblib')\nx_develop_pca = transformer.transform(x_develop[c_cols])\nx_develop_pca = pd.DataFrame(x_develop_pca, columns=[f'FastICA_c-{i}' for i in range(n_comp)], index=x_develop.index)\nx_develop = pd.concat((x_develop, x_develop_pca), axis=1)\n\nx_test_pca = transformer.transform(x_test[c_cols])\nx_test_pca = pd.DataFrame(x_test_pca, columns=[f'FastICA_c-{i}' for i in range(n_comp)], index=x_test.index)\nx_test = pd.concat((x_test, x_test_pca), axis=1)\n\n# ------------------------------------------------------------------------\nn_comp = 80\ntransformer = load('../input/update-mx10-model/mx10_FactorAnalysis_g.joblib')\nx_develop_pca = transformer.transform(x_develop[g_cols])\nx_develop_pca = pd.DataFrame(x_develop_pca, columns=[f'FactorAnalysis_g-{i}' for i in range(n_comp)], index=x_develop.index)\nx_develop = pd.concat((x_develop, x_develop_pca), axis=1)\n\nx_test_pca = transformer.transform(x_test[g_cols])\nx_test_pca = pd.DataFrame(x_test_pca, columns=[f'FactorAnalysis_g-{i}' for i in range(n_comp)], index=x_test.index)\nx_test = pd.concat((x_test, x_test_pca), axis=1)\n\n\n# c-features\nn_comp = 40\ntransformer = load('../input/update-mx10-model/mx10_FactorAnalysis_c.joblib')\nx_develop_pca = transformer.transform(x_develop[c_cols])\nx_develop_pca = pd.DataFrame(x_develop_pca, columns=[f'FactorAnalysis_c-{i}' for i in range(n_comp)], index=x_develop.index)\nx_develop = pd.concat((x_develop, x_develop_pca), axis=1)\n\nx_test_pca = transformer.transform(x_test[c_cols])\nx_test_pca = pd.DataFrame(x_test_pca, columns=[f'FactorAnalysis_c-{i}' for i in range(n_comp)], index=x_test.index)\nx_test = pd.concat((x_test, x_test_pca), axis=1)\n\n# ------------------------------------------------------------------------\nn_comp = 80\ntransformer = load('../input/update-mx10-model/mx10_FeatureAgglomeration_g.joblib')\nx_develop_pca = transformer.transform(x_develop[g_cols])\nx_develop_pca = pd.DataFrame(x_develop_pca, columns=[f'FeatureAgglomeration_g-{i}' for i in range(n_comp)], index=x_develop.index)\nx_develop = pd.concat((x_develop, x_develop_pca), axis=1)\n\nx_test_pca = transformer.transform(x_test[g_cols])\nx_test_pca = pd.DataFrame(x_test_pca, columns=[f'FeatureAgglomeration_g-{i}' for i in range(n_comp)], index=x_test.index)\nx_test = pd.concat((x_test, x_test_pca), axis=1)\n\n\n# c-features\nn_comp = 40\ntransformer = load('../input/update-mx10-model/mx10_FeatureAgglomeration_c.joblib')\nx_develop_pca = transformer.transform(x_develop[c_cols])\nx_develop_pca = pd.DataFrame(x_develop_pca, columns=[f'FeatureAgglomeration_c-{i}' for i in range(n_comp)], index=x_develop.index)\nx_develop = pd.concat((x_develop, x_develop_pca), axis=1)\n\nx_test_pca = transformer.transform(x_test[c_cols])\nx_test_pca = pd.DataFrame(x_test_pca, columns=[f'FeatureAgglomeration_c-{i}' for i in range(n_comp)], index=x_test.index)\nx_test = pd.concat((x_test, x_test_pca), axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stat_feat(df, features_g, features_c):\n    df['g_sum'] = df[features_g].sum(axis = 1)\n    df['g_mean'] = df[features_g].mean(axis = 1)\n    df['g_std'] = df[features_g].std(axis = 1)\n    df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n    df['g_skew'] = df[features_g].skew(axis = 1)\n    df['c_sum'] = df[features_c].sum(axis = 1)\n    df['c_mean'] = df[features_c].mean(axis = 1)\n    df['c_std'] = df[features_c].std(axis = 1)\n    df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n    df['c_skew'] = df[features_c].skew(axis = 1)\n    df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n    df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n    df['gc_std'] = df[features_g + features_c].std(axis = 1)\n    df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n    df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n    return df, ['g_sum', 'g_mean', 'g_std', 'g_kurt', 'g_skew', \n                'c_sum', 'c_mean', 'c_std', 'c_kurt', 'c_skew', \n                'gc_sum', 'gc_mean', 'gc_std', 'gc_kurt', 'gc_skew']\n\nx_develop, new_feat = stat_feat(x_develop, g_cols.to_list(), c_cols.to_list())\nx_test, new_feat = stat_feat(x_test, g_cols.to_list(), c_cols.to_list())\n\ncont_cols += new_feat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Variance Threshold"},{"metadata":{"trusted":true},"cell_type":"code","source":"class VarianceThreshold:\n    def __init__(self, threshold):\n        self.threshold = threshold\n    def fit(self, df, cont_cols):\n        self.cont_cols = cont_cols\n        self.var = df[cont_cols].var()\n        good_cols = self.var[self.var > self.threshold]\n        self.index = good_cols.index.to_list()\n        self.dropcols = [x for x in cont_cols if x not in self.var[self.var > self.threshold].index.to_list()]\n        self.validcols = [x for x in cont_cols if x in self.var[self.var > self.threshold].index.to_list()]\n    def transform(self, df):\n        return df.drop(self.dropcols, axis=1)\n    def fit_transform(self, df, cont_cols):\n        self.fit(df, cont_cols)\n        return self.transform(df), self.validcols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_cols = [col for col in x_develop.columns.to_list() if col not in ['sig_id','cp_type','cp_time','cp_dose']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = 0.2 #x_develop[cont_cols].var().sort_values().quantile(0.01)\nprint('Variance Threshold:', threshold)\nVarThres = VarianceThreshold(threshold)\nx_develop, cont_cols = VarThres.fit_transform(x_develop, cont_cols)\nx_test = VarThres.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Transform Numerical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"qt = load('../input/update-mx10-model/mx10_qt.joblib')\nx_develop[cont_cols] = qt.fit_transform(x_develop[cont_cols])\nx_test[cont_cols] = qt.transform(x_test[cont_cols])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Group Data Into Folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_folds(df, fold_no, fold_type='mls_kfold', save=False):\n    \"\"\"\n    df: target dataframe\n    \"\"\"\n    if fold_type == 'kfold':\n        kf = KFold(n_splits=fold_no, shuffle=True, random_state=SEED)\n    elif fold_type == 'mls_kfold':\n        kf = MultilabelStratifiedKFold(n_splits=fold_no, random_state=SEED)\n        \n    df['Fold'] = -1\n    df.reset_index(inplace=True)\n    for fold, (t, v) in enumerate(kf.split(df, df)):\n        df.loc[v, 'Fold'] = fold\n    df.set_index('sig_id', inplace=True)\n    if save:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_FOLDS = 30\nfold_type = 'mls_kfold'\ncreate_folds(y_develop, fold_no=N_FOLDS, fold_type=fold_type, save=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define Model Architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model():\n    def __init__(self, input_shape, output_bias=None):\n        self.input_shape = input_shape\n        self.output_bias = output_bias\n        \n    def create_model1(self, trans_model):\n        if self.output_bias is not None:\n            self.output_bias = tf.keras.initializers.Constant(output_bias)\n#         trans_model.trainable = False\n            \n        x = trans_model.layers[7].output\n        \n        outputs = tfa.layers.WeightNormalization(L.Dense(N_TARGETS,\n                                                         activation='sigmoid',\n                                                         bias_initializer=self.output_bias,\n                                                         name=\"scored_dense\"\n                                                        ),\n                                                 name='scored_weight'\n                                                 )(x)\n        model = tf.keras.Model(inputs=trans_model.input, outputs=outputs)\n\n        metrics = [tf.keras.losses.BinaryCrossentropy(name='mean_loss')]\n\n        OPTIMIZER = tfa.optimizers.Lookahead(\n            tfa.optimizers.AdamW(weight_decay=1e-5),\n            sync_period=5)\n        model.compile(optimizer=OPTIMIZER, loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001), metrics=metrics)\n\n        return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_develop.set_index('sig_id', inplace=True)\nx_test.set_index('sig_id', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_pred(x_df, models):\n    if isinstance(models[0], np.ndarray):\n        pred_test = np.repeat([models[0]], len(x_df), axis=0)\n    else:\n        for i in models:\n            if i == 0:\n                pred_test = models[i].predict(x_df)\n            else:\n                pred_test += models[i].predict(x_df)\n\n        pred_test = pred_test/len(models)\n    pred_test = pd.DataFrame(index=x_df.index, columns=target_cols, data=pred_test)\n    pred_test.loc[x_df['cp_type'] == 0] = 0\n    return pred_test\n\n\ndef oof_score(oof: dict):\n    return np.mean(list(oof.values())), np.std(list(oof.values()))\n\n\ndef combine_pred(pred):\n    for i in pred:\n        if i==0:\n            r = pred[i]\n        else:\n            r = np.append(r, pred[i], axis=0)\n    return r\n\n\n\ndef run_cv(xtrain=x_develop, ytrain=y_develop, model=None, N_FOLDS=N_FOLDS, summary=True, debug=False):\n    histories = {x: '' for x in range(N_FOLDS)} \n    models = {x: '' for x in range(N_FOLDS)}\n    results = {x: '' for x in range(N_FOLDS)}\n    oof_bp = {x: [] for x in range(N_FOLDS)}\n    oof_ap = {x: [] for x in range(N_FOLDS)}\n    pred_val_fold = {x: [] for x in range(N_FOLDS)}\n    target_val_fold = {x: [] for x in range(N_FOLDS)}\n    oof_pd = pd.DataFrame()\n    \n    for foldno in np.sort(y_develop['Fold'].unique()):\n        x_train_fold = x_develop[y_develop['Fold'] != foldno]\n        y_train_fold = y_develop[y_develop['Fold'] != foldno].drop('Fold', axis=1)\n        x_val_fold = x_develop[y_develop['Fold'] == foldno]\n        y_val_fold = y_develop[y_develop['Fold'] == foldno].drop('Fold', axis=1)\n        \n            \n        train_sample_size = len(y_train_fold)\n        val_sample_size = len(y_val_fold)\n        print(\" \")\n        print(f\"Fold-%d\" % (foldno))\n        print(\"Train sample size:\", train_sample_size, \", Validation sample size:\", val_sample_size)\n\n        FEATURE_SIZE = x_train_fold.shape[-1]\n\n        # Train Data Pipeline\n        train_ds = tf.data.Dataset.from_tensor_slices((x_train_fold, y_train_fold))\n        # train_ds = train_ds.filter(lambda x, y: tf.reduce_any(y != np.zeros(206)))\n        train_ds = train_ds.shuffle(1024).batch(56)\n\n        # Validation Data Pipeline\n        val_ds = tf.data.Dataset.from_tensor_slices((x_val_fold, y_val_fold))\n        val_ds = val_ds.batch(val_sample_size)\n        \n#         t_model = load_model(f'../input/moafinalmodels/mx10_nonscored/weights-fold{foldno}.h5', compile=False)\n        \n#         # MODEL\n#         models[foldno] = model.create_model1(t_model)\n        \n#         # Train\n#         cb_es = EarlyStopping(monitor='val_mean_loss', patience=5, restore_best_weights=True)\n#         reduce_lr_loss = ReduceLROnPlateau(monitor='val_mean_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n#         histories[foldno] = models[foldno].fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=[cb_es, reduce_lr_loss], verbose=1)\n\n\n        models[foldno] = load_model(f'../input/update-mx10-model/weights-fold{foldno}.h5', compile=False)\n        \n        # Predict Validation Probabilities\n        pred_val_fold[foldno] = models[foldno].predict(x_val_fold)\n        \n        # Calculate OOF (Validation) Results\n        oof_bp[foldno] = tf.keras.losses.binary_crossentropy(y_val_fold, pred_val_fold[foldno]).numpy().mean()\n        print('Out-of-Fold Score: ', oof_bp[foldno])\n        \n        pred_val_fold[foldno][x_val_fold['cp_type'] == 0] == 0\n        oof_ap[foldno] = tf.keras.losses.binary_crossentropy(y_val_fold, pred_val_fold[foldno]).numpy().mean()\n        print('Out-of-Fold Score with post processing: ', oof_ap[foldno])\n        \n        target_val_fold[foldno] = y_val_fold.to_numpy()\n        \n        cur_oof_pd = pd.DataFrame(pred_val_fold[foldno], columns=[f'{i}' for i in range(206)])\n        cur_oof_pd['sig_id'] = y_val_fold.index\n        oof_pd = oof_pd.append(cur_oof_pd)\n        \n\n        # Save Model\n        if SAVE_MODEL:\n            pass\n#             models[foldno].save(f'weights-fold{foldno}.h5')\n            \n    pred_val_fold = combine_pred(pred_val_fold)\n    target_val_fold = combine_pred(target_val_fold)\n    \n    \n    print('\\n')\n    if summary:\n        print('Summary')\n        # Mean out of score before postprocessing\n        print('Mean OOF score: %f +/- %f' % (oof_score(oof_bp)))\n\n        # Mean out of score after postprocessing\n        print('Mean OOF score after postprocessing: %f +/- %f' % (oof_score(oof_ap)))\n    \n    return models, histories, oof_ap, pred_val_fold, pred_val_fold, target_val_fold, oof_pd\n\n\ndef submit(res):\n    sub = res.reset_index()\n    sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RUN THE TRAINING\nEPOCHS = 45\nSAVE_MODEL = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_bias = -np.log(y_develop[y_develop.columns[:-1]].mean(axis=0).to_numpy())\nmodels, histories, oof_ap, pred_val_fold, pred_val_fold, target_val_fold, oof_pd = run_cv(x_develop, y_develop,\n                                                                                          model=Model(x_develop.shape[1], output_bias),\n                                                                                          N_FOLDS=N_FOLDS,\n                                                                                          debug=False)\npred_test = calc_pred(x_test, models) # This is for single model submission\nsubmit(pred_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_loss_score(actual, predicted,  eps=1e-15):\n    p1 = actual * np.log(predicted+eps)\n    p0 = (1-actual) * np.log(1-predicted+eps)\n    loss = p0 + p1\n    return -loss.mean()\n\ndef log_loss_multi(y_true, y_pred):\n    M = y_true.shape[1]\n    results = np.zeros(M)\n    for i in range(M):\n        results[i] = log_loss_score(y_true[:,i], y_pred[:,i])\n    return results.mean()\noof_final = y_develop.merge(oof_pd, how='left', on='sig_id')[[str(col) for col in range(206)]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\nlog_loss_multi(y_true.drop(['sig_id'], axis=1).to_numpy(), oof_final.to_numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save('LBS.npy', oof_final.to_numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame.from_dict(oof_ap, orient='index').hist()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}