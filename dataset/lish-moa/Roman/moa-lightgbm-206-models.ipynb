{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this kernel I am building 206 models (one for each label). Because there is no easier way to make LightGBM work with multilabel/multiclass task. And this competitions is a case because we have 206 targets to predict.\n\n## Versions\n* v2: Label encoding of categorical features. CV: 0.01627, LB: 0.02040\n* v4: One-Hot encoding of categorical features. Metric hackind added - all predictions below threshold set to 0. CV: 0.01622, LB: 0.02209","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport multiprocessing\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport gc\nfrom time import time\nimport datetime\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import log_loss\nwarnings.simplefilter('ignore')\nsns.set()\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"files = ['../input/lish-moa/test_features.csv', \n         '../input/lish-moa/train_targets_scored.csv',\n         '../input/lish-moa/train_features.csv',\n         '../input/lish-moa/train_targets_nonscored.csv',\n         '../input/lish-moa/sample_submission.csv']\n\nwith multiprocessing.Pool() as pool:\n    test, train_target, train, train_nonscored, sub = pool.map(pd.read_csv, files)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One-Hot encoding\nfor feature in ['cp_time', 'cp_type', 'cp_dose']:\n    concat = pd.concat([train[feature], test[feature]], ignore_index=True)\n    dummies = pd.get_dummies(concat, dummy_na=True, dtype=np.uint8, prefix=feature)\n    train = pd.concat([train, dummies.iloc[:train.shape[0]]], axis=1)\n    test = pd.concat([test, dummies.iloc[:test.shape[0]]], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = [col for col in train_target.columns if col != 'sig_id']\nprint('Number of different labels:', len(targets))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [col for col in train.columns if col not in ['sig_id', 'cp_time', 'cp_type', 'cp_dose']]\nprint('Number of features:', len(features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train[features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 491,\n          'min_child_weight': 0.03,\n          'feature_fraction': 0.3,\n          'bagging_fraction': 0.4,\n          'min_data_in_leaf': 106,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.01,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'binary_logloss',\n          \"verbosity\": 0,\n          'reg_alpha': 0.4,\n          'reg_lambda': 0.6,\n          'random_state': 47\n         }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accumulative_loss = 0\nskf = StratifiedKFold(n_splits=3, random_state=47, shuffle=True)\n\nprint('Execution time | Model number | logloss | new logloss | best coeff')\n# 206 different models. One for each label\nfor model, target in enumerate(targets, 1):\n    y = train_target[target]\n    start_time = time()\n    preds = np.zeros(test.shape[0])\n    oof = np.zeros(X.shape[0])\n\n    for trn_idx, test_idx in skf.split(X, y):\n        \n        trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n        val_data = lgb.Dataset(X.iloc[test_idx], label=y.iloc[test_idx])\n        clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=0, early_stopping_rounds=20)\n        oof[test_idx] = clf.predict(X.iloc[test_idx])\n        preds += clf.predict(test[features]) / skf.n_splits\n\n    loss = log_loss(y, oof)\n    \n    # Hacking the metric\n    coeffs = [3, 2, 1.5, 1.4, 1.3, 1.2, 1.1, 1.0, 0.9, 0.8, 0.7]\n    best_coeff = 0\n    best_loss = loss\n    for coeff in coeffs:\n        new_oof = oof.copy()\n        new_oof[new_oof < new_oof.mean() / coeff] = 0\n        new_loss = log_loss(y, new_oof)\n        if new_loss < loss:\n            preds[preds < preds.mean() / coeff] = 0\n            best_coeff = coeff\n            best_loss = new_loss\n    \n    if best_coeff:\n        preds[preds < preds.mean() / best_coeff] = 0\n    # End of metric hacking\n    sub[target] = preds\n\n    accumulative_loss += best_loss\n    print('{}\\t\\t{}\\t{:.5f}\\t\\t{:.5f}\\t\\t{}'.format(str(datetime.timedelta(seconds=time() - start_time))[:7], model, loss, best_loss, best_coeff))\n    del preds, oof, start_time, y, loss, best_loss, new_oof\n    gc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Overall mean loss: {:.5f}'.format(accumulative_loss / 206))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}