{"cells":[{"metadata":{},"cell_type":"markdown","source":"# RAPIDS - Genetic Algorithm KNN - [CV 0.01840]\nThe RAPIDS library is now available in all Kaggle notebooks. Hooray! Simply type `import cuml` or `import cudf` to load the two most popular packages.\n\nRAPIDS is described [here][1]. RAPIDS `cuDF` accelerates dataframe operations using GPU and has a similar api as Pandas. RAPIDS `cuML` accelerates machine learning algorithms using GPU and has a similar api as Scikit-Learn. Since RAPIDS ML algorithms are so fast, we can do things that were never possible like applying genetic algorithms to ML hyperparameter searchs!\n\nThe ML algorithm kNN is very sensitive to the scaling of the feature columns. In this notebook, we find the weights for feature columns (`0 <= w <= 1`) using a genetic algorithm. This is similar to a feature selection algorithm like ridge regression which shrinks the importance of features. Alternatively we could mimic lasso regression if we choose only weights `w = 0` and `w = 1`.\n\n[1]: https://rapids.ai/"},{"metadata":{},"cell_type":"markdown","source":"# Load Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys, warnings\nwarnings.filterwarnings(\"ignore\")\nsys.path.append('../input/iterativestratification')\n\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nfrom sklearn.metrics import log_loss\n\nimport cuml\nprint('RAPIDS',cuml.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/lish-moa/train_features.csv')\nprint('train shape',train.shape)\ntest = pd.read_csv('../input/lish-moa/test_features.csv')\nprint('test shape',test.shape)\ntargets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.cp_dose = train.cp_dose.map({'D1':-1,'D2':1})\ntest.cp_dose = test.cp_dose.map({'D1':-1,'D2':1})\n\ntrain.cp_time = train.cp_time.map({24:-1, 48:0, 72:1})\ntest.cp_time = test.cp_time.map({24:-1, 48:0, 72:1})\n\ntrain.cp_type = train.cp_type.map({'trt_cp':-1, 'ctl_vehicle':1})\ntest.cp_type = test.cp_type.map({'trt_cp':-1, 'ctl_vehicle':1})\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Double Stratified K Fold\nWe will use double stratified folds discussed [here][1]\n  \n[1]: https://www.kaggle.com/c/lish-moa/discussion/195195"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_folds(folds = 5, random_state = 0, stratify = True, scored = None):\n    \n    drug = pd.read_csv('../input/lish-moa/train_drug.csv')\n    if scored is None:\n        scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n    targets = scored.columns[1:]\n    scored = scored.merge(drug, on='sig_id', how='left')\n\n    # LOCATE DRUGS\n    vc = scored.drug_id.value_counts()\n    vc1 = vc.loc[vc<=18].index.sort_values()\n    vc2 = vc.loc[vc>18].index.sort_values()\n\n    # STRATIFY DRUGS 18 OR LESS\n    dct1 = {}; dct2 = {}\n    if stratify:\n        skf = MultilabelStratifiedKFold(n_splits=folds, shuffle=True, random_state=random_state)\n    else:\n        skf = KFold(n_splits=folds, shuffle=True, random_state=random_state)\n    tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n        dd = {k:fold for k in tmp.index[idxV].values}\n        dct1.update(dd)\n    \n    # STRATIFY DRUGS MORE THAN 18\n    if stratify:\n        skf = MultilabelStratifiedKFold(n_splits=folds, shuffle=True, random_state=random_state)\n    else:\n        skf = KFold(n_splits=folds, shuffle=True, random_state=random_state)\n    tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\n    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n        dd = {k:fold for k in tmp.sig_id[idxV].values}\n        dct2.update(dd)\n    \n    # ASSIGN FOLDS\n    scored['fold'] = np.nan\n    scored['fold'] = scored.drug_id.map(dct1)\n    scored.loc[scored.fold.isna(),'fold'] = scored.loc[scored.fold.isna(),'sig_id'].map(dct2)\n    scored.fold = scored.fold.astype('int8')\n    \n    return scored[['sig_id','fold']].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initialize Variables\nOur genetic algorithm works as follows. Each generation, we take the best `sqrt(POPULATION)` sets of weights and then we randomly combine them in pairs to create a new population of size `POPULATION`. We also apply `MUTATE` percentage of mutations (i.e. randomly change some weights). We can think of a set of weights as \"DNA\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"GENERATIONS = 20\nPOPULATION = 100 # must be perfect square\nPARENTS = int( np.sqrt(POPULATION) )\nMUTATE = 0.05\n\n# RANDOMLY CREATE CV\nFOLDS = 5; SEED = 42\nff = make_folds(folds=FOLDS, random_state=SEED, stratify=True, scored=targets)\ntrain['fold'] = ff.fold.values\ntargets['fold'] = ff.fold.values\n\n# INITIALIZE\noof = np.zeros((len(train),206))\ndna = np.random.uniform(0,1,(POPULATION,875))**2.0\ncvs = np.zeros((POPULATION))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Genetic Search"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"%%time\nfor jj in range(GENERATIONS):\n\n    # ALL PREVIOUS POPULATION \"DNA\" AND CV SCORES\n    df = pd.DataFrame(data=dna)\n    df['cv'] = cvs\n    df.sort_values('cv',inplace=True)\n    print('Evolving...')\n\n    # GENERATE AND EVALUATE CHILDREN\n    for k in range(POPULATION):\n        print(k,', ',end='')\n        \n        # GENERATE CHILD\n        if jj!=0:\n            parent1 = k//PARENTS; parent2 = k%PARENTS\n            TMP = np.random.randint(0,2,875)\n            dna[k,] = TMP * df.iloc[parent1,:-1] + (1-TMP) * df.iloc[parent2,:-1]\n            x = np.random.uniform(0,1,875)\n            IDX = np.where(x<MUTATE)[0]\n            dna[k,IDX] = np.random.uniform(0,1,len(IDX))**2.0\n        else:\n            dna[k,] = df.iloc[k,:-1]\n        \n        # KNN WEIGHTS\n        WGT = dna[k,]\n        # WEIGHTS FOR CP_TYPE, CP_TIME, CP_DOSE\n        WGT[0]= 100 ; WGT[1] = 12/2; WGT[2] = 5/2\n\n        # KNN KFOLD VALIDATE\n        for fold in range(FOLDS):    \n            model = cuml.neighbors.KNeighborsClassifier(n_neighbors=1000)\n            model.fit( train.loc[ train.fold!=fold, train.columns[1:-1] ].values * WGT,\n                       targets.loc[targets.fold!=fold, targets.columns[1:-1] ] )\n    \n            pp = model.predict_proba( train.loc[ train.fold==fold, train.columns[1:-1] ].values * WGT )\n            pp = np.stack( [(1 - pp[x][:,0]) for x in range(len(pp))] ).T\n            oof[targets.fold==fold,] = pp\n        \n        cv_score = log_loss( targets.iloc[:,1:-1].values.flatten(), oof.flatten() )\n        cvs[k] = cv_score\n        \n    # DISPLAY BEST \"DNA\"\n    RES = 10\n    idx = np.argmin(cvs)\n    plt.figure(figsize=(20,3))\n    plt.plot(np.arange(872)[::RES],dna[idx,3:][::RES],'o-')\n    plt.title('Best \"DNA\" after generation %i yields CV SCORE = %.5f'%(jj+1,np.min(cvs)),size=16)\n    plt.show()    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Show all population \"DNA\" with CV scores\nEach row in the dataframe below is one set of weights from our evolved population. The first column indicates the cv score and the next 875 columns are the weights that we scale the 875 features with. (The weights can be thought of as \"DNA\" in the context of genetic algorithm). We will save the \"DNA\" to disk."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(data=dna)\ndf['cv'] = cvs\ndf = df[['cv']+list(np.arange(0,875))]\ndf.sort_values('cv',inplace=True)\ndf.to_csv('dna.csv',index=False)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"WGT = df.iloc[0,1:].values\noof = np.zeros((len(train),206))\npreds = np.zeros((len(test),206))\n\nfor fold in range(FOLDS):\n    print('FOLD %i'%(fold+1), ' ', end='')\n    \n    model = cuml.neighbors.KNeighborsClassifier(n_neighbors=1000)\n    model.fit( train.loc[ train.fold!=fold, train.columns[1:-1] ].values * WGT,\n               targets.loc[targets.fold!=fold, targets.columns[1:-1] ] )\n    \n    pp = model.predict_proba( train.loc[ train.fold==fold, train.columns[1:-1] ].values * WGT )\n    pp = np.stack( [(1 - pp[x][:,0]) for x in range(len(pp))] ).T\n    oof[targets.fold==fold,] = pp\n    \n    pp = model.predict_proba( test[test.columns[1:]].values * WGT )\n    pp = np.stack( [(1 - pp[x][:,0]) for x in range(len(pp))] ).T\n    preds += pp/FOLDS    \n    \nprint()\ncv_score = log_loss( targets.iloc[:,1:-1].values.flatten(), oof.flatten() )\nprint('CV SCORE = %.5f'%cv_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clip and Submit\nIf we predict 0 when the true target is 1, we are severly penalized in this competition's metric. Therefore we clip our predictions and observe an increase of CV LB 0.00050! We will save the OOF to disk before clipping."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_oof = targets.copy()\ndf_oof.iloc[:,1:-1] = oof\ndf_oof.to_csv('oof.csv',index=False)\n\noof = np.clip(oof,0.0005,0.999)\noof[train.cp_type==1,] = 0\n\ncv_score = log_loss( targets.iloc[:,1:-1].values.flatten(), oof.flatten() )\nprint('CV SCORE with CLIPPING = %.5f'%cv_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/lish-moa/sample_submission.csv')\nsub.iloc[:,1:] = np.clip(preds,0.0005,0.999)\nsub.loc[test.cp_type==1,sub.columns[1:]] = 0\nsub.to_csv('submission.csv',index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets.iloc[:,1:-1].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.iloc[:,1:].mean()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}