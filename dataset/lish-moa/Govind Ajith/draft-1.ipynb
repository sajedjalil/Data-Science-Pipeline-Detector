{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Mechanisms of Action (MoA) Prediction"},{"metadata":{},"cell_type":"markdown","source":"Predicting multiple targets of the Mechanism of Action (MoA) response(s) of different samples (sig_id), given various inputs such as gene expression data and cell viability data."},{"metadata":{},"cell_type":"markdown","source":"## Some of the important terms used in the headings of the tables are presented here:\n    \n    g - : signifies gene expression data\n    c - : signifies cell expression data\n    cp_type : indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle)\n    NOTE: (samples with control perturbations don't have MoAs)\n    cp_time - treatment duration (24,48,72) Hours\n    cp_dose - Dosage - HIGH or LOW"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the multi label stratified k-fold \n# cross validator\nimport sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\n# Initial random imports\nimport random\nimport os\nimport copy\nimport warnings\n# warnings.filterwarnings('ignore')\n\n# Importing numpy\nimport numpy as np\n\n# Importing pandas\nimport pandas as pd\n\n# Importing matplotlib\nimport matplotlib.pyplot as plt\n\n# Importing seaborn\nimport seaborn as sns\n\n# Importing sklearn\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import VarianceThreshold\n\n# Importing pytorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using GPU if available"},{"metadata":{"trusted":true},"cell_type":"code","source":"# using GPU if available\nif torch.cuda.is_available():\n    device_code = 'cuda'\nelse:\n    device_code = 'cpu'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setting the seed, so that every time the seed is started from the same number\n\ndef set_seed_characteristics(seed=55):\n    # Setting a random seed value\n    \n    random.seed(seed)\n    \n    # for guaranteering the reproducability of numbers by setting seed for NumPy\n    \n    np.random.seed(seed) \n    \n    # for setting the seed for cuda or cpu\n    \n    torch.manual_seed(seed) \n\n    # To ensure that Pytorch doesnt just switch to the fastest possible algorithm but \n    # ensures that it selects a deterministic algorithm\n    \n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading the CSV Files"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_features = pd.read_csv('../input/lish-moa/train_features.csv')\n# Reading the head rows and columns of train features\ntraining_features_head = training_features.head()\n\ntraining_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n# Reading the head rows and columns of train targets scored\ntraining_targets_scored_head = training_targets_scored.head()\n\ntesting_features = pd.read_csv('../input/lish-moa/test_features.csv')\n# Reading the head rows and columns of train targets non-scored\ntesting_features_head = testing_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing the head - training features \ntraining_features_head","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing the head - train targets scored \ntraining_targets_scored_head","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing the head - test features\ntesting_features_head","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset classes, training and testing functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pytorch data loader implementation of MoA dataset\nclass MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        train_tensor_dictionary = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return train_tensor_dictionary\n\n# Pytorch data loader implementation of test dataset\nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        test_tensor_dictionary = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return test_tensor_dictionary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pytorch model for the MoA determination\n\nclass Model(nn.Module):\n    \n    # Instantiaing all the models before utilizing\n    # them later in the forward function.\n    def __init__(self, num_features, num_targets, hidden_size):\n        \n        # super keyword used to access data from the parent\n        # pytorch.nn.Module class\n        super(Model, self).__init__()\n        # Applying batch normalization. This is done to standardize\n        # the input for each mini batches and will help reduce the\n        # number of epochs for which the training is done. This limits\n        # the covariate shift (this is the value by which the hidden\n        # layer values shift around) and allows to learn from a more \n        # stable set of data. Sometimes, it also allows for a\n        # higher learning rate.This is also used for regularization\n        # and helps reduce over fitting. Generally, if batch \n        # normalization is used, you can use a smaller dropout,\n        # which in turn means that lesser layers can be lost \n        # in every step.\n        self.batch_normalization_1 = nn.BatchNorm1d(num_features)        \n        # For regularization purposes the dropout is set\n        # This is done by setting a probablity. Random \n        # neural networks are picked at a probablity, say p\n        # or dropped at a probablity of 1-p. This is essential \n        # to prevent overfitiing of the model and also reduces\n        # the computation time. A fully connected neural network, if\n        # run without dropout will start forming dependancies between\n        # each other and this can lead to over-fitting.\n        self.dropoutlayer_1 = nn.Dropout(0.2)\n        # nn.utils.weight_norm : This is weight normalization. Usually,\n        #                        faster than batch normalization\n        # nn.Linear : Applying linear transform to the incoming data\n        #             and creates a single layer feed forward network.\n        # input size : num_features\n        # output size : hidden_size\n        self.denselayer_1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_normalization_2 = nn.BatchNorm1d(hidden_size)\n        self.dropoutlayer_2 = nn.Dropout(0.2)\n        # input size : hidden_size\n        # output size : hidden_size\n        self.denselayer_2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_normalization_3 = nn.BatchNorm1d(hidden_size)\n        self.dropoutlayer_3 = nn.Dropout(0.1)\n        # input size : hidden_size\n        # output size : hidden_size\n        self.denselayer_3 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_normalization_4 = nn.BatchNorm1d(hidden_size)\n        self.dropoutlayer_4 = nn.Dropout(0.1)\n        # input size : hidden_size\n        # output size : hidden_size\n        self.denselayer_4 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        \n        self.batch_normalization_5 = nn.BatchNorm1d(hidden_size)\n        self.dropoutlayer_5 = nn.Dropout(0.1)\n        # input size : hidden_size\n        # output size : num_targets\n        self.denselayer_5 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    # The forward function basically defines the model\n    def forward(self, forward_x):\n        \n        forward_x = self.batch_normalization_1(forward_x)\n        forward_x = self.dropoutlayer_1(forward_x)\n        forward_x = F.relu(self.denselayer_1(forward_x))\n        \n        forward_x = self.batch_normalization_2(forward_x)\n        forward_x = self.dropoutlayer_2(forward_x)\n        forward_x = F.relu(self.denselayer_2(forward_x))\n        \n        forward_x = self.batch_normalization_3(forward_x)\n        forward_x = self.dropoutlayer_3(forward_x)\n        forward_x = self.denselayer_3(forward_x)\n        \n        forward_x = self.batch_normalization_4(forward_x)\n        forward_x = self.dropoutlayer_4(forward_x)\n        forward_x = self.denselayer_4(forward_x)\n\n        forward_x = self.batch_normalization_5(forward_x)\n        forward_x = self.dropoutlayer_5(forward_x)\n        forward_x = self.denselayer_5(forward_x)\n        \n        return forward_x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to train the model\ndef trainingFunction(model, optimizer, scheduler, lossFunction, trainloader, device_code):\n    model.train()\n    training_loss = 0\n    for training_data in trainloader:\n        optimizer.zero_grad()\n        inputs, targets = training_data['x'].to(device_code), training_data['y'].to(device_code)\n        outputs = model(inputs)\n        loss = lossFunction(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        training_loss += loss.item()    \n    training_loss /= len(trainloader) \n    return training_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to validate the model\ndef validationFunction(model, lossFunction, validationloader, device_code):\n    model.eval()\n    validation_loss = 0\n    validation_predictions = []   \n    for validation_data in validationloader:\n        inputs, targets = validation_data['x'].to(device_code), validation_data['y'].to(device_code)\n        outputs = model(inputs)\n        loss = lossFunction(outputs, targets)\n        validation_loss += loss.item()\n        validation_predictions.append(outputs.sigmoid().detach().cpu().numpy())\n    validation_loss /= len(validationloader)\n    validation_predictions = np.concatenate(validation_predictions)\n    return validation_loss, validation_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding the inference function\ndef inferenceFunction(model, inferenceloader, device_code):\n    model.eval()\n    inferences = [] \n    for data in inferenceloader:\n        inputs = data['x'].to(device_code)\n        with torch.no_grad():\n            outputs = model(inputs)\n        inferences.append(outputs.sigmoid().detach().cpu().numpy())   \n    inferences = np.concatenate(inferences)  \n    return inferences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding dummy inserts to the cp_time and cp_dose columns\n# Usually done to categorical variables\ndef addDummies(data):\n    dummy_data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return dummy_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"set_seed_characteristics(seed=55)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seperating out the Gene expression Column and Cell Viability Column\n\ngene_expression = [g for g in training_features.columns if g.startswith('g-')]\ncell_viability = [c for c in training_features.columns if c.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since our dimensions are really high, we can resort to \n# using PCA for dimensionality reduction, but is still able \n# to capture the characteristics of the data.\n\n# Now, this can be done by choosing a random dimension, and \n# having the same random state as before. By doing this\n# we observe that we do not encounter\n# any 'nan' errors during training.\n\n# Doing PCA for the Gene expression data\n\n# can choose any random number here\nrandom_pca_dimension_genes = 20\n\n# Concatenating the training and test set\ndata = pd.concat([pd.DataFrame(training_features[gene_expression]), pd.DataFrame(testing_features[gene_expression])])\n\n# Performing PCA and converting to a random_pca_dimension_genes number of columns\npca_genes = PCA(n_components = random_pca_dimension_genes, random_state=55)\n\n# Fitting the PCA transform\ndata_pca = pca_genes.fit_transform(data[gene_expression])\n\n# Splitting the training and test columns\ntrain_pca_genes = data_pca[:training_features.shape[0]] \ntest_pca_genes = data_pca[-testing_features.shape[0]:]\n\n# Converting training and testing  into Pandas data frame shape\ntrain_pca_genes = pd.DataFrame(train_pca_genes, columns=[f'pca_G-{i}' for i in range(random_pca_dimension_genes)])\ntest_pca_genes = pd.DataFrame(test_pca_genes, columns=[f'pca_G-{i}' for i in range(random_pca_dimension_genes)])\n\n# Concatenating these back to the original features\ntraining_features = pd.concat((training_features, train_pca_genes), axis=1)\ntesting_features = pd.concat((testing_features, test_pca_genes), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Doing PCA for the Cell Viability Data\n\n# can choose any random number here\nrandom_pca_dimension_cells = 32\n\n# Concatenating the training and test set\ndata = pd.concat([pd.DataFrame(training_features[cell_viability]), pd.DataFrame(testing_features[cell_viability])])\n\n# Performing PCA and converting to a random_pca_dimension_cells number of columns\npca_cells = PCA(n_components = random_pca_dimension_cells, random_state=55)\n\n# Fitting the PCA transform\ndata_pca = pca_cells.fit_transform(data[cell_viability])\n\n# Splitting the training and test columns\ntrain_pca_cells = data_pca[:training_features.shape[0]]\ntest_pca_cells = data_pca[-testing_features.shape[0]:]\n\n# Converting training and testing  into Pandas data frame shape\ntrain_pca_cells = pd.DataFrame(train_pca_cells, columns=[f'pca_C-{i}' for i in range(random_pca_dimension_cells)])\ntest_pca_cells = pd.DataFrame(test_pca_cells, columns=[f'pca_C-{i}' for i in range(random_pca_dimension_cells)])\n\n# Concatenating these back to the original features\ntraining_features = pd.concat((training_features, train_pca_cells), axis=1)\ntesting_features = pd.concat((testing_features, test_pca_cells), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting a desired threshold to calculate the VarianceThreshold.\n# As per the math all the Features with a training-set variance \n# lower than this threshold will be removed.\nvariancethreshold = VarianceThreshold(threshold=0.7)\n\n# Combining training and test features to create a single dataset\ncombined_data = training_features.append(testing_features)\n\n# Fits to the data, before transforming it\ncombined_data_transformed = variancethreshold.fit_transform(combined_data.iloc[:, 4:])\n\n# Extracting the training and the testing data out of the\n# transformed data\ntraining_features_transformed = combined_data_transformed[ : training_features.shape[0]]\ntesting_features_transformed = combined_data_transformed[-testing_features.shape[0] : ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting the training features in a suitable \n# pandas dataset format and numbering the columns\n# after the labels of 'sig_id', 'cp_type', 'cp_time', 'cp_dose'.\ntraining_features = pd.DataFrame(training_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4), columns=['sig_id','cp_type','cp_time','cp_dose'])\ntraining_features = pd.concat([training_features, pd.DataFrame(training_features_transformed)], axis=1)\n\n# Extracting the testing features in a suitable \n# pandas dataset format and numbering the columns\n# after the labels of 'sig_id', 'cp_type', 'cp_time', 'cp_dose'.\n\ntesting_features = pd.DataFrame(testing_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4), columns=['sig_id','cp_type','cp_time','cp_dose'])\ntesting_features = pd.concat([testing_features, pd.DataFrame(testing_features_transformed)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging the columns\n\ntrain = training_features.merge(training_targets_scored, on='sig_id')\n\n# Removing rows with cp_type as ctl_vehicle \n# since control perturbations have no MoAs\n# We are also manually setting the drop type as \n# true because we do not want to include them back \n# as a new column.\n\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\n# Naturally, we have to get rid of them from the test dataset \n# as well\n\ntest = testing_features[testing_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting the columns of the drugs that are sold from\n# the train pandas dataframe\n\ntarget = train[training_targets_scored.columns]\n\n# Now that the ctl_vehicle drugs have been removed, we do not need\n# cp_type. So we can go ahead and remove that columns as well.\n\ntrain = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)\n\n# extracting the columns in the targets \n\ntarget_columns = target.drop('sig_id', axis=1).columns.values.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset preparation complete"},{"metadata":{"trusted":true},"cell_type":"code","source":"# multilabel stratified K Fold import causes a small warning and we do not want\n# to show that in the notebook.\nwarnings.filterwarnings('ignore')\n\nfolds = train.copy()\nnumber_of_folds = 3\n\n# creating a 3 fold multilabel stratified K Fold\nmultilabel_k_fold = MultilabelStratifiedKFold(n_splits = number_of_folds)\n\n# Standard k fold splitting. Here we are splitting into number_of_folds folds\n\nfor fol, (train_folds, validation_folds) in enumerate(multilabel_k_fold.split(X=train, y=target)):\n    folds.loc[validation_folds, 'kfold'] = int(fol)\n    \nfolds['kfold'] = folds['kfold'].astype(int)\n\n# Isolating out the feature columns. This is done by first \n# Isolating the columns that are not present in the target\n# followed by extracting the columns except the sig_id and \n# kfold.\n\nfeature_columns = [c for c in addDummies(folds).columns if c not in target_columns]\nfeature_columns = [c for c in feature_columns if c not in ['kfold','sig_id']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Declaring the HyperParameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 128\nmax_epochs = 16\n# When training neural networks, it is common to use \n# weight decay where after each update, the weights \n# are multiplied by a factor slightly less than 1\nweight_decay = 1e-5\n# deciding the initial learning rate\n# It controls how quickly or slowly a neural\n# network model can learn a model or a problem.\nlr = 1e-3\n# Boolean to decide on stopping early when the \n# validation_loss > best_loss\nbool_early_stop = True\n# steps to execute before early stopping\nsteps_early_stopping= 10\n# number of features corresponding to the columns in the\n# targets\nnum_features=len(feature_columns)\n# number of targets corresponding to the columns in the\n# features\nnum_targets=len(target_columns)\n# in between neural netwrok size\nhidden_size=1024","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Declaring the training functions and performing the training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# to plot validation loss\nvalid_loss_list = []\n# to plot the training loss\ntrain_loss_list = []\n# to plot the best recorded loss\nbest_loss_list = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(fold, seed):\n    # declaring the list as global to plot validation loss\n    global valid_loss_list\n    # declaring the training loss list as global to plot\n    # the training loss\n    global train_loss_list\n    # declaring the best loss list as global to plot the\n    # best losses recorded\n    global best_loss_list\n    \n    # setting the seed to start from the same number as \n    # explained previously\n    set_seed_characteristics(seed)\n    \n    # adding dummy variables to the training set\n    train = addDummies(folds)\n\n    # extracting the validating rows numbers for the\n    # respective k fold values\n    val_idx = train[train['kfold'] == fold].index\n    \n    # Dropping all the rows from the training set\n    # that does not belong to this kth fold\n    train_necessary_rows = train[train['kfold'] != fold].reset_index(drop=True)\n    # Dropping all the rows from the valiadtion set\n    # that does not belong to this kth fold\n    valid_necessary_rows = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    # splitting the x and y values for training set\n    train_features, train_targets  = train_necessary_rows[feature_columns].values, train_necessary_rows[target_columns].values\n    # splitting the x and y values for test set\n    validation_features, validation_targets =  valid_necessary_rows[feature_columns].values, valid_necessary_rows[target_columns].values\n    \n    # Converting the training data to standard pytorch \n    # dataset class format\n    train_dataset = MoADataset(train_features, train_targets)\n    \n    # Converting the validation data to standard pytorch \n    # dataset class format\n    valid_dataset = MoADataset(validation_features, validation_targets)\n    \n    # calling the pytorch data loading utility for the\n    # training set\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    # calling the pytorch data loading utility for the\n    # validation set  \n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    # Declaring the model and can be tuned here\n    # using the hyper parameters\n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    # moving the model to GPU if available,\n    # else will run it on CPU itself\n    model.to(device_code)\n    \n    # A standard optimizer. Adam optimizer is widely used\n    # because it combines the advantages of the Adaptive gradient\n    # algorithm and the root mean square propogation. Basically, it does\n    # not stick to one learning rate and adapts it to the problem. \n    # It is widely known to offer good results really fast.\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    \n    # We use a learning rate scheduler to converge to the lowest\n    # loss faster. This is also seen to provide higher accuracy.\n    # This can be tuned.\n    # Some of the optimizers I tried here are\n    # optim.lr_scheduler.OneCycleLR\n    # optim.lr_scheduler.StepLR\n    \n    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.05, div_factor=1.5e3, \n                                              max_lr=1e-2, epochs=max_epochs, steps_per_epoch=len(trainloader))\n    \n    # after research I saw that the Binary cross\n    # entropy loss with sigmoid layer works well\n    lossFunction = nn.BCEWithLogitsLoss()\n    \n    # stops when the error starts increaseing. Setting the counter\n    # to track this\n    steps_before_early_stop = 0\n    # general out of fold array shape\n    out_of_fold = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    # declaring a very high value as an \n    # initial loss for each kth fold\n    best_loss = np.inf\n    \n    # looping through the epochs\n    for epoch in range(max_epochs):\n        \n        # training the model\n        training_loss = trainingFunction(model, optimizer,scheduler, lossFunction, trainloader, device_code)\n        print('epoch : ',epoch,'>> training_loss : ',training_loss)\n        train_loss_list.append(training_loss)\n        validation_loss, validation_predictions = validationFunction(model, lossFunction, validloader, device_code)\n        print('epoch : ',epoch,'>> validation : ',validation_loss)\n        valid_loss_list.append(validation_loss)\n        \n        # checking if the loss is decreasing\n        if validation_loss < best_loss:\n            best_loss = validation_loss\n            best_loss_list.append(best_loss)\n            # Updating the out of fold predictions\n            out_of_fold[val_idx] = validation_predictions\n            # saving the model and data for this kth fold\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n        \n        # Handling the increasing loss by calling \n        # early stopping\n        elif(bool_early_stop == True):\n            \n            # breaks out of the loop when this happens\n            steps_before_early_stop += 1\n            if (steps_before_early_stop >= steps_early_stopping):\n                break\n    # adding dummy variables to the test set\n    test_ = addDummies(test)       \n    \n    # extracting the x_test\n    x_test = test_[feature_columns].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(num_features=num_features,num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    # uploading the saved data for this kth fold\n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    # again uploading the model to GPU, if available\n    model.to(device_code)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    # evaluates the model\n    predictions = inferenceFunction(model, testloader, device_code)\n    \n    return out_of_fold, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def executeKFold(number_of_folds, seed):\n    # standard size for the out of fold predictions\n    out_of_fold = np.zeros((len(train), len(target_columns)))\n    # same size for all of the predictions\n    predictions = np.zeros((len(test), len(target_columns)))\n    \n    for each_k_fold in range(number_of_folds):\n        print('Fold Number : ', each_k_fold)\n        out_of_fold_, pred_ = run_training(each_k_fold, seed)\n        \n        # adding all the predictions\n        predictions += pred_ / number_of_folds\n        # adding all the out of fold predictions\n        out_of_fold += out_of_fold_\n        print(\"------------------------\")\n        \n    k_th_prediction = predictions\n    return out_of_fold, k_th_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setting a standard seed number\nSEED = [55]\n# general out of fold array shape\nout_of_fold = np.zeros((len(train), len(target_columns)))\n# general predictions array shape\npredictions = np.zeros((len(test), len(target_columns)))\n\n# for seed in SEED:\nout_of_fold_, predictions_ = executeKFold(number_of_folds, SEED[0])\nout_of_fold += out_of_fold_ / len(SEED)\npredictions += predictions_ / len(SEED)\n\ntrain[target_columns] = out_of_fold\ntest[target_columns] = predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluating the logarithmic loss function applied to each drug-MoA annotation pair."},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_df = training_targets_scored.drop(columns=target_columns).merge(train[['sig_id']+target_columns], on='sig_id', how='left').fillna(0)\n# True target values\ntrue_target = training_targets_scored[target_columns].values\n# Predicted target values\npredicted_target = validation_df[target_columns].values\ncross_validation_score = 0\n\n# Now we can calculate the cross entropy loss\n\nfor i in range(len(target_columns)):\n    cross_validation_score_target = log_loss(true_target[:, i], predicted_target[:, i])\n    cross_validation_score += cross_validation_score_target / target.shape[1]  \n\nprint(\" The Cross Validation loss is :>> \", cross_validation_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting the Validation loss for each fold"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(valid_loss_list)\nplt.title('Validation Loss')\nplt.savefig('valid_loss_list.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting the Training loss for each fold"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(train_loss_list)\nplt.title('Training loss')\nplt.savefig('train_loss_list.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting the Best recorded loss for each fold"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(best_loss_list)\nplt.title('Best Recorded loss')\nplt.savefig('best_loss_list.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sample_submission.drop(columns=target_columns).merge(test[['sig_id']+target_columns], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}