{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is my attempt to tackle this problem from neural network side. I will provide simple, clean, from begin to end neural network model which generates all 206 label prediction simultaneously and achieves basic performance. Pythorch is used. There is no feature engineering etc. fancy trick in this walkthrough."},{"metadata":{},"cell_type":"markdown","source":"> **previous changes** using cross validation to replace train/test splits.\n\n> **version 17** Add dropout before lost hidden layer\n\n**this version** increase to 10 fold"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport gc\nimport sys\nfrom sklearn.metrics import log_loss\nfrom scipy.special import expit\nimport sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`MultilabelStratifiedKFold` was from Y.Nakama's data page\n"},{"metadata":{},"cell_type":"markdown","source":"## Data Review\nAs we already saw from compitition introduction, in this competition we need to use gene expression, cell viability, treatment type, treatment time, treatment dose to classify each instance into among 206 MoA categories. That mean each instance can belong to more than one category or even none of the categories.\n\nBelow we read in data first."},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ntrainraw = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ntest = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we exteact the column names of gene and cell features, in case we need to do something to them in the future. "},{"metadata":{"trusted":true},"cell_type":"code","source":"featurename = trainraw.columns.tolist()\ngenefeat = [n for n in featurename if n[:2]=='g-']\ncellfeat = [n for n in featurename if n[:2]=='c-']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize = (15,3))\nax[0].plot(trainraw.loc[0,genefeat].values)\nax[0].set_title('gene expression')\nax[1].plot(trainraw.loc[0,cellfeat].values)\nax[1].set_title('cell viability')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apparently gene expression cover around 800 features and cell activity only have around 100 features.\n\nNow from data description, we know `cp_type`, `cp_dose` are categorical variables with string as their values. We perform one-hot encoding to convert them to numerical."},{"metadata":{"trusted":true},"cell_type":"code","source":"OHE = OneHotEncoder(sparse=False)\nonehotfeat = OHE.fit_transform(trainraw[['cp_type', 'cp_dose']])\nonehotfeat.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainraw['cp_type_0'] = onehotfeat[:,0]\ntrainraw['cp_type_1'] = onehotfeat[:,1]\ntrainraw['cp_dose_0'] = onehotfeat[:,2]\ntrainraw['cp_dose_1'] = onehotfeat[:,3]\ntrainraw.drop(['cp_type', 'cp_dose'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_type = test['cp_type']\nonehotfeat = OHE.transform(test[['cp_type', 'cp_dose']])\nonehotfeat.shape\ntest['cp_type_0'] = onehotfeat[:,0]\ntest['cp_type_1'] = onehotfeat[:,1]\ntest['cp_dose_0'] = onehotfeat[:,2]\ntest['cp_dose_1'] = onehotfeat[:,3]\ntest.drop(['cp_type', 'sig_id', 'cp_dose'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In case the target data and training data are not already match. I perform a merge below, then split target and training data apart."},{"metadata":{"trusted":true},"cell_type":"code","source":"wholeset = targets.merge(trainraw, how='left', on='sig_id')\nwholeset.drop('sig_id', axis=1, inplace=True)\ntargetsname = targets.columns.tolist()\ntargetsname.remove('sig_id')\nwholeset = wholeset[wholeset['cp_type_0']!=1].reset_index(drop=True)\ntargets = wholeset[targetsname]\nwholeset.drop(targetsname, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.xticks(rotation=90)\nplt.bar(targetsname,targets.sum(axis=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A relative close look shows us that the classes are not balanced. This might be something that we can work on to improve our performance later on.\n\nIn this version, I will use mutilabel kfold instead of simple split."},{"metadata":{"trusted":true},"cell_type":"code","source":"#split data \n# train_x, valid_x, train_y, valid_y = train_test_split(wholeset, targets, test_size=0.2, shuffle=True, random_state=111914)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below is a convenient class to pass data to pytorch dataloader."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, X, labels):\n        'Initialization'\n        self.labels = labels\n        self.X = X\n        \n    def __len__(self):\n        'Denotes the total number of samples'\n        return len(self.X)\n\n    def __getitem__(self, index):\n        'Generates one sample of data'\n        # Load data and get label\n        x = self.X[index]\n        y = self.labels[index]\n        return x, y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define NN Architecture \nWe now define a clean NN structure: one input layer, two hidden layer expend and shrink feature, one classification layer. Here in last layer we use linear activation, meaning not using any when it comes to code. It is done to satified the input require of our loss function."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Mynet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(877, 1024),\n#             nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n#             nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 206),\n            )  \n    def forward(self, x):\n        x = self.mlp(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#capture gpu if available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_x, train_y, valid_x, valid_y = train_x.values, train_y.values, valid_x.values, valid_y.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #load data\n# total_size = train_x.shape[0]\n# batch_size = 256\n# batchs = int(np.ceil(total_size/batch_size))\n# valid_loader = torch.utils.data.DataLoader(valid_x.astype(np.float32), batch_size=32, shuffle=False)\n# train_loader = torch.utils.data.DataLoader(train_x.astype(np.float32), batch_size=32, shuffle=False)\n# trainlabeleddata = Dataset(train_x.astype(np.float32),train_y.astype(np.float32))\n# trainlabeleddata_loader = torch.utils.data.DataLoader(trainlabeleddata, batch_size=batch_size, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_loss_metric(y_true, y_pred):\n    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    loss = - np.mean(np.mean(y_true * np.log(y_pred_clip) + (1 - y_true) * np.log(1 - y_pred_clip), axis = 1))\n    return loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training\nWe can train model with our data now. I use early stopping to stop training when loss doesn't decrease for 5 epochs. Multi-Label Cross Entropy Loss With Logits in pytorch are used, it's like a negative log loss with inputs that haven't been \"sigmoided\". Other parameter setting are untuned usual starting setting.\n\nWe also setup a 5 folds cross validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"nfolds = 10\nKfolds = MultilabelStratifiedKFold(n_splits=nfolds, shuffle=True, random_state=234)\ntest_loader = torch.utils.data.DataLoader(test.values.astype(np.float32), batch_size=32, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof = np.zeros(targets.shape)\npredictions = np.zeros((test.shape[0], targets.shape[1]))\nfor f, (t_idx, v_idx) in enumerate(Kfolds.split(X=wholeset, y=targets)):\n    print(f'fold_{f+1}', flush=True)\n    #split data\n    train_x =  wholeset.values[t_idx,:]\n    train_y =  targets.values[t_idx,:] \n    valid_x =  wholeset.values[v_idx,:]\n    valid_y =  targets.values[v_idx,:]\n\n    #load data\n    total_size = train_x.shape[0]\n    batch_size = 256\n    batchs = int(np.ceil(total_size/batch_size))\n    valid_loader = torch.utils.data.DataLoader(valid_x.astype(np.float32), batch_size=32, shuffle=False)\n    train_loader = torch.utils.data.DataLoader(train_x.astype(np.float32), batch_size=32, shuffle=False)\n    trainlabeleddata = Dataset(train_x.astype(np.float32),train_y.astype(np.float32))\n    trainlabeleddata_loader = torch.utils.data.DataLoader(trainlabeleddata, batch_size=batch_size, shuffle=True)\n\n    #initialize model\n    mynet = Mynet()\n    mynet.to(device)\n    lr = 0.001\n    num_epochs = 50\n    loss_function = nn.BCEWithLogitsLoss()\n\n    optimizer = torch.optim.Adamax(mynet.parameters(), lr=lr)\n#     lambdaschecule = lambda x: 0.95\n#     scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lambdaschecule)\n#     scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n#                                               max_lr=1e-2, epochs=num_epochs, steps_per_epoch=len(trainlabeleddata_loader))\n    losses = []\n    vallosses = []\n    acc = []\n    valacc = []\n    gc.collect()\n    torch.cuda.empty_cache() \n    wait = 0\n    best_epoch = 0\n    best = np.Inf  \n    patience = 10\n    for epoch in range(num_epochs):\n        # setup progress bar\n        sys.stdout.write(\"[\")\n        sys.stdout.flush()  \n        mynet.train()\n        for tn in trainlabeleddata_loader:\n            mynet.zero_grad()\n            inputs, labels = tn[0].to(device), tn[1].to(device)\n            output = mynet(inputs)\n            netloss = loss_function(torch.squeeze(output), labels)\n            netloss.backward()\n            optimizer.step()\n#             scheduler.step()\n            sys.stdout.write(\"-\")\n            sys.stdout.flush()\n        sys.stdout.write(\"]\\n\") # this ends the progress bar\n        mynet.eval()\n        predtrain = []\n        for traindata in train_loader:\n            traindata = traindata.to(device)\n            predtrain.append(torch.squeeze(mynet(traindata)).detach().cpu().numpy())\n        predtrain = np.concatenate(predtrain) \n        predvalid = []\n        for validdata in valid_loader:\n            validdata = validdata.to(device)\n            predvalid.append(torch.squeeze(mynet(validdata)).detach().cpu().numpy())\n        predvalid = np.concatenate(predvalid) \n        losses.append(log_loss_metric(train_y, expit(predtrain).astype(np.float64)))\n        vallosses.append(log_loss_metric(valid_y, expit(predvalid).astype(np.float64)))\n        print(f\"Epoch: {epoch}, Loss: {losses[epoch]}, LossVal: {vallosses[epoch]}\")\n        # Early stoping module\n        if np.less(vallosses[epoch], best):\n            best = vallosses[epoch]\n            wait = 0\n            best_epoch = epoch\n            # Record the best weights if current results is better (less).\n            torch.save(mynet.state_dict(), 'checkpoint.pt')\n        else:\n            wait += 1\n            if wait >= patience:\n                print(\"=======================================\")\n                print(\"Restoring model weights from the end of the best epoch.\")\n                mynet.load_state_dict(torch.load('checkpoint.pt'))\n                print(\"Epoch %05d: early stopping\" % (best_epoch))\n                break\n    mynet.eval()\n    predvalid = []\n    for validdata in valid_loader:\n        validdata = validdata.to(device)\n        predvalid.append(torch.squeeze(mynet(validdata)).detach().cpu().numpy())\n    predvalid = np.concatenate(predvalid) \n    oof[v_idx,:] = predvalid\n    predtest = []\n    for testdata in test_loader:\n        testdata = testdata.to(device)\n        predtest.append(torch.squeeze(mynet(testdata)).detach().cpu().numpy())\n    predtest = np.concatenate(predtest) \n    predtest = expit(predtest)\n    predictions += predtest/nfolds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('valodation score: {}'.format(log_loss_metric(targets, expit(oof).astype(np.float64))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.plot(losses)\n# plt.plot(vallosses)\n# plt.legend(['loss_train','loss_valid'])\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Figure above also show our optimal point."},{"metadata":{},"cell_type":"markdown","source":"## Submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/lish-moa/sample_submission.csv')\nsub.iloc[:,1:] = predictions\nsub.loc[test_type=='ctl_vehicle',1:] = 0\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\nThis model can give us a fair score, yet more can be done. For example, what we can do with the unbalanced classes. If we change prediction from all classes to one class a time, will improvement happen? Please share your questions about this post and other inspiring ideas about the data."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}