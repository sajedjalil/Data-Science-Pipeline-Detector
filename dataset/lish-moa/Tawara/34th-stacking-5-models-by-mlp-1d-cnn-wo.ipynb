{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Input"},{"metadata":{"trusted":true},"cell_type":"code","source":"DO_SUBMISSION = True\nDO_TRAIN_FOR_ENSEMBLE = False\nDO_VIRTUAL_SUBMISSION = False\nassert (sum([DO_SUBMISSION, DO_TRAIN_FOR_ENSEMBLE, DO_VIRTUAL_SUBMISSION]) == 1), \"select `ONE` mode\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport time\nimport shutil\nfrom pathlib import Path\nfrom copy import deepcopy\nfrom contextlib import contextmanager\n\nimport random\nimport typing as tp\nfrom collections import OrderedDict\n\n\nimport yaml\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@contextmanager\ndef timer(logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None):\n    if prefix: format_str = str(prefix) + format_str\n    if suffix: format_str = format_str + str(suffix)\n    start = time.time()\n    yield\n    d = time.time() - start\n    out_str = format_str.format(d)\n    if logger:\n        logger.info(out_str)\n    else:\n        print(out_str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_virtual_private(test_feat, smpl_sub, rate_to_pub=4):\n    \n    assert (test_feat.sig_id == smpl_sub.sig_id).all()\n    \n    # public は含まれるので、そのまま入れる.\n    test_feat_list = [test_feat]\n    smpl_sub_list = [smpl_sub]\n    \n    for i in range(1, rate_to_pub):\n        tmp_test_feat = test_feat.copy()\n        tmp_smpl_sub = smpl_sub.copy()\n        \n        # #  id を変更\n        tmp_test_feat.sig_id = tmp_test_feat.sig_id + f\"_{i}\"\n        tmp_smpl_sub.sig_id = tmp_smpl_sub.sig_id + f\"_{i}\"\n        assert (tmp_test_feat.sig_id == tmp_smpl_sub.sig_id).all()\n        \n        # # `c-*` と `g-*` に適当な値を加える. 複製して増やすだけだとすり抜ける場合があったため.\n        tmp_test_feat.iloc[:, 4:] += i * 10\n        assert (tmp_test_feat.iloc[:, 4:] != test_feat.iloc[:, 4:]).all().all()\n        \n        test_feat_list.append(tmp_test_feat)\n        smpl_sub_list.append(tmp_smpl_sub)\n        \n    # # 結合\n    test_feat_concat = pd.concat(test_feat_list, axis=0, ignore_index=True)\n    smpl_sub_concat = pd.concat(smpl_sub_list, axis=0, ignore_index=True)\n    \n    return test_feat_concat, smpl_sub_concat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# pip install"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\n# for nn.py\n!pip install ../input/iterative-stratification/iterative-stratification-master/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\n# for tabnet\n!pip install --no-index --find-links ../input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference Test by Stage1 Models"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\nif DO_SUBMISSION:\n    !python ../input/moa-nn-tabnet-fix5/nn-inference-0.01833.py\n\nelif DO_VIRTUAL_SUBMISSION:\n    !python ../input/moa-nn-tabnet-fix5/virtual-nn-use-train-public-inference.py\n\nelif DO_TRAIN_FOR_ENSEMBLE:\n    pass\nelse:\n    raise ValueError","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\nif DO_SUBMISSION:\n    !python ../input/moa-nn-tabnet-fix5/tabnet-inference-0.01840.py\n\nelif DO_VIRTUAL_SUBMISSION:\n    !python ../input/moa-nn-tabnet-fix5/virtual-tabnet-inference-add-param-n-shared.py\n\nelif DO_TRAIN_FOR_ENSEMBLE:\n    pass\nelse:\n    raise ValueError","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\nif DO_SUBMISSION:\n    !python ../input/moa-takapy-script/tf-rn-transfer-1layerother-selcol100.py  # 0.01862\n\nelif DO_VIRTUAL_SUBMISSION:\n    !python ../input/moa-takapy-script/virtual-tf-rn-transfer-1layerother-selcol100.py\n\nelif DO_TRAIN_FOR_ENSEMBLE:\n    pass\nelse:\n    raise ValueError","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\nif DO_SUBMISSION:\n    !python ../input/moa-tawara-scripts-for-final-submission/moa-for-final-thrnn-seed-cv-0.01836.py\n\nelif DO_VIRTUAL_SUBMISSION:\n    !python ../input/moa-tawara-scripts-for-final-submission/virtual-moa-for-final-thrnn-seed-cv-0.01836.py\n\nelif DO_TRAIN_FOR_ENSEMBLE:\n    pass\nelse:\n    raise ValueError","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"%%time\nif DO_SUBMISSION:\n    !python ../input/moa-tawara-scripts-for-final-submission/moa-for-final-thrnn-drug-seed-cv-0.01841.py\n\nelif DO_VIRTUAL_SUBMISSION:\n    !python ../input/moa-tawara-scripts-for-final-submission/virtual-moa-for-final-thrnn-drug-seed-cv-0.01841.py\n\nelif DO_TRAIN_FOR_ENSEMBLE:\n    pass\nelse:\n    raise ValueError","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load OOF predcitions"},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGET_COL = ['5-alpha_reductase_inhibitor', '11-beta-hsd1_inhibitor', 'acat_inhibitor', 'acetylcholine_receptor_agonist', 'acetylcholine_receptor_antagonist', 'acetylcholinesterase_inhibitor', 'adenosine_receptor_agonist', 'adenosine_receptor_antagonist', 'adenylyl_cyclase_activator', 'adrenergic_receptor_agonist', 'adrenergic_receptor_antagonist', 'akt_inhibitor', 'aldehyde_dehydrogenase_inhibitor', 'alk_inhibitor', 'ampk_activator', 'analgesic', 'androgen_receptor_agonist', 'androgen_receptor_antagonist', 'anesthetic_-_local', 'angiogenesis_inhibitor', 'angiotensin_receptor_antagonist', 'anti-inflammatory', 'antiarrhythmic', 'antibiotic', 'anticonvulsant', 'antifungal', 'antihistamine', 'antimalarial', 'antioxidant', 'antiprotozoal', 'antiviral', 'apoptosis_stimulant', 'aromatase_inhibitor', 'atm_kinase_inhibitor', 'atp-sensitive_potassium_channel_antagonist', 'atp_synthase_inhibitor', 'atpase_inhibitor', 'atr_kinase_inhibitor', 'aurora_kinase_inhibitor', 'autotaxin_inhibitor', 'bacterial_30s_ribosomal_subunit_inhibitor', 'bacterial_50s_ribosomal_subunit_inhibitor', 'bacterial_antifolate', 'bacterial_cell_wall_synthesis_inhibitor', 'bacterial_dna_gyrase_inhibitor', 'bacterial_dna_inhibitor', 'bacterial_membrane_integrity_inhibitor', 'bcl_inhibitor', 'bcr-abl_inhibitor', 'benzodiazepine_receptor_agonist', 'beta_amyloid_inhibitor', 'bromodomain_inhibitor', 'btk_inhibitor', 'calcineurin_inhibitor', 'calcium_channel_blocker', 'cannabinoid_receptor_agonist', 'cannabinoid_receptor_antagonist', 'carbonic_anhydrase_inhibitor', 'casein_kinase_inhibitor', 'caspase_activator', 'catechol_o_methyltransferase_inhibitor', 'cc_chemokine_receptor_antagonist', 'cck_receptor_antagonist', 'cdk_inhibitor', 'chelating_agent', 'chk_inhibitor', 'chloride_channel_blocker', 'cholesterol_inhibitor', 'cholinergic_receptor_antagonist', 'coagulation_factor_inhibitor', 'corticosteroid_agonist', 'cyclooxygenase_inhibitor', 'cytochrome_p450_inhibitor', 'dihydrofolate_reductase_inhibitor', 'dipeptidyl_peptidase_inhibitor', 'diuretic', 'dna_alkylating_agent', 'dna_inhibitor', 'dopamine_receptor_agonist', 'dopamine_receptor_antagonist', 'egfr_inhibitor', 'elastase_inhibitor', 'erbb2_inhibitor', 'estrogen_receptor_agonist', 'estrogen_receptor_antagonist', 'faah_inhibitor', 'farnesyltransferase_inhibitor', 'fatty_acid_receptor_agonist', 'fgfr_inhibitor', 'flt3_inhibitor', 'focal_adhesion_kinase_inhibitor', 'free_radical_scavenger', 'fungal_squalene_epoxidase_inhibitor', 'gaba_receptor_agonist', 'gaba_receptor_antagonist', 'gamma_secretase_inhibitor', 'glucocorticoid_receptor_agonist', 'glutamate_inhibitor', 'glutamate_receptor_agonist', 'glutamate_receptor_antagonist', 'gonadotropin_receptor_agonist', 'gsk_inhibitor', 'hcv_inhibitor', 'hdac_inhibitor', 'histamine_receptor_agonist', 'histamine_receptor_antagonist', 'histone_lysine_demethylase_inhibitor', 'histone_lysine_methyltransferase_inhibitor', 'hiv_inhibitor', 'hmgcr_inhibitor', 'hsp_inhibitor', 'igf-1_inhibitor', 'ikk_inhibitor', 'imidazoline_receptor_agonist', 'immunosuppressant', 'insulin_secretagogue', 'insulin_sensitizer', 'integrin_inhibitor', 'jak_inhibitor', 'kit_inhibitor', 'laxative', 'leukotriene_inhibitor', 'leukotriene_receptor_antagonist', 'lipase_inhibitor', 'lipoxygenase_inhibitor', 'lxr_agonist', 'mdm_inhibitor', 'mek_inhibitor', 'membrane_integrity_inhibitor', 'mineralocorticoid_receptor_antagonist', 'monoacylglycerol_lipase_inhibitor', 'monoamine_oxidase_inhibitor', 'monopolar_spindle_1_kinase_inhibitor', 'mtor_inhibitor', 'mucolytic_agent', 'neuropeptide_receptor_antagonist', 'nfkb_inhibitor', 'nicotinic_receptor_agonist', 'nitric_oxide_donor', 'nitric_oxide_production_inhibitor', 'nitric_oxide_synthase_inhibitor', 'norepinephrine_reuptake_inhibitor', 'nrf2_activator', 'opioid_receptor_agonist', 'opioid_receptor_antagonist', 'orexin_receptor_antagonist', 'p38_mapk_inhibitor', 'p-glycoprotein_inhibitor', 'parp_inhibitor', 'pdgfr_inhibitor', 'pdk_inhibitor', 'phosphodiesterase_inhibitor', 'phospholipase_inhibitor', 'pi3k_inhibitor', 'pkc_inhibitor', 'potassium_channel_activator', 'potassium_channel_antagonist', 'ppar_receptor_agonist', 'ppar_receptor_antagonist', 'progesterone_receptor_agonist', 'progesterone_receptor_antagonist', 'prostaglandin_inhibitor', 'prostanoid_receptor_antagonist', 'proteasome_inhibitor', 'protein_kinase_inhibitor', 'protein_phosphatase_inhibitor', 'protein_synthesis_inhibitor', 'protein_tyrosine_kinase_inhibitor', 'radiopaque_medium', 'raf_inhibitor', 'ras_gtpase_inhibitor', 'retinoid_receptor_agonist', 'retinoid_receptor_antagonist', 'rho_associated_kinase_inhibitor', 'ribonucleoside_reductase_inhibitor', 'rna_polymerase_inhibitor', 'serotonin_receptor_agonist', 'serotonin_receptor_antagonist', 'serotonin_reuptake_inhibitor', 'sigma_receptor_agonist', 'sigma_receptor_antagonist', 'smoothened_receptor_antagonist', 'sodium_channel_inhibitor', 'sphingosine_receptor_agonist', 'src_inhibitor', 'steroid', 'syk_inhibitor', 'tachykinin_antagonist', 'tgf-beta_receptor_inhibitor', 'thrombin_inhibitor', 'thymidylate_synthase_inhibitor', 'tlr_agonist', 'tlr_antagonist', 'tnf_inhibitor', 'topoisomerase_inhibitor', 'transient_receptor_potential_channel_antagonist', 'tropomyosin_receptor_kinase_inhibitor', 'trpv_agonist', 'trpv_antagonist', 'tubulin_inhibitor', 'tyrosine_kinase_inhibitor', 'ubiquitin_specific_protease_inhibitor', 'vegfr_inhibitor', 'vitamin_b', 'vitamin_d_receptor_agonist', 'wnt_inhibitor']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def del_control(df: pd.DataFrame()) -> pd.DataFrame():\n    train = pd.read_csv('../input/lish-moa/train_features.csv')\n    df['cp_type'] = train['cp_type']\n    df = df.loc[df['cp_type']=='trt_cp'].reset_index(drop=True)\n    df = df.drop('cp_type', axis=1)\n    return df\n\ndef drop_sig_id(df: pd.DataFrame()) -> pd.DataFrame():\n    return df.drop('sig_id', axis=1)\n\ndef fix_oof(df: pd.DataFrame()) -> pd.DataFrame():\n    if df.shape[0] != 21948:\n        df = del_control(df)\n    if 'sig_id' in df.columns.tolist():\n        df = drop_sig_id(df)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_list = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sinchir0\n# nn\noof_list.append(fix_oof(\n    pd.read_pickle('../input/nn-use-train-public/oof.pkl')))\n\n# tabnet\noof_list.append(fix_oof(\n    pd.DataFrame(\n        np.load('../input/tabnet-train-public-add-n-shared-1/oof_tabnet.npy'),\n        columns=TARGET_COL))\n)\n\n##################################\n# asteriam\n\n\n##################################\n# takapy\noof_list.append(fix_oof(\n    pd.read_csv(\"../input/moa-takapy-tf-resnet-transfer/oof_prediction_takapy_resnet.csv\")))\n\n##################################\n# Tawara\n# thrnn - seed cv\noof_list.append(fix_oof(\n    pd.read_csv(\"../input/moa-weight-thrnn-seed-cv/oof_prediction.csv\")))\n\n# thrnn - drug seed cv\noof_list.append(fix_oof(\n    pd.read_csv(\"../input/moa-weight-thrnn-drug-seed-cv/oof_prediction.csv\")))\n\n##################################\n# Taro Masuda\n# ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = fix_oof(pd.read_csv('../input/lish-moa/train_targets_scored.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confirm shape\nprint(\"sinchir0 - NN:\", oof_list[0].shape)\nprint(\"local logloss:\", log_loss(y_true.values.ravel(), oof_list[0].values.ravel()))\ndisplay(oof_list[0].head())\n\nprint(\"sinchir0 - TabNet:\", oof_list[1].shape)\nprint(\"local logloss:\", log_loss(y_true.values.ravel(), oof_list[1].values.ravel()))\ndisplay(oof_list[1].head())\n\nprint(\"takapy - ResNet:\", oof_list[2].shape)\nprint(\"local logloss:\", log_loss(y_true.values.ravel(), oof_list[2].values.ravel()))\ndisplay(oof_list[2].head())\n\nprint(\"tawara - ThrNN seed-cv:\", oof_list[3].shape)\nprint(\"local logloss:\", log_loss(y_true.values.ravel(), oof_list[3].values.ravel()))\ndisplay(oof_list[3].head())\n\nprint(\"tawara - ThrNN drug-seed-cv:\", oof_list[4].shape)\nprint(\"local logloss:\", log_loss(y_true.values.ravel(), oof_list[4].values.ravel()))\ndisplay(oof_list[4].head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Arithmetic mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"Blend_oof = oof_list[0].copy()\nBlend_oof[:] = 0.\nfor oof_df in oof_list:\n    Blend_oof += oof_df\n    \nBlend_oof = Blend_oof / len(oof_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ny_true = fix_oof(y_true)\n\ny_pred = Blend_oof.copy()\n\n# score = 0\n# for i in range(len(oof_list[0].columns.tolist())):\n#     score_ = log_loss(y_true.iloc[:, i], y_pred.iloc[:, i])\n#     score += score_ / oof_list[0].shape[1]\n\n# print(\"CV log_loss: \", score)\n\nprint(\"CV log_loss: \", log_loss(y_true.values.ravel(), y_pred.values.ravel()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Output - Submission file"},{"metadata":{"trusted":true},"cell_type":"code","source":"if DO_SUBMISSION or DO_VIRTUAL_SUBMISSION:\n    sub_list = [\n        pd.read_csv('./submission-sinchir0-nn.csv'),\n        pd.read_csv('./submission_sinchir0_tabnet.csv'),\n        pd.read_csv(\"./submission_takapy_tf-resnet.csv\"),\n        pd.read_csv('./submission_tawara_thrnn_seed_cv.csv'),\n        pd.read_csv('./submission_tawara_thrnn_drug_seed_cv.csv'),\n    ]\nelse:\n    sub_list = [\n        pd.read_csv('../input/nn-use-train-public/submission.csv'),\n        pd.read_csv('../input/tabnet-train-public-add-n-shared-1/submission.csv'),\n        pd.read_csv(\"../input/moa-takapy-tf-resnet-transfer/submission.csv\"),\n        pd.read_csv('../input/moa-weight-thrnn-seed-cv/submission.csv'),\n        pd.read_csv('../input/moa-weight-thrnn-drug-seed-cv/submission.csv'),\n    ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check Output"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sinchir0 - NN model\nprint(\"shape:\", sub_list[0].shape)\ndisplay(sub_list[0].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sinchir0 - TabNet model\nprint(\"shape:\", sub_list[1].shape)\ndisplay(sub_list[1].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# takapy - tf resnet\nprint(\"shape:\", sub_list[2].shape)\ndisplay(sub_list[2].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tawara - ThrNN model (seed CV)\nprint(\"shape:\", sub_list[3].shape)\ndisplay(sub_list[3].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tawara - ThrNN model (drug seed CV)\nprint(\"shape:\", sub_list[4].shape)\ndisplay(sub_list[4].head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Magic Function for private"},{"metadata":{"trusted":true},"cell_type":"code","source":"def order_sub(sub) : \n    return sub.sort_values('sig_id').reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_list = [order_sub(sub_df) for sub_df in sub_list]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference OOF and Test by Stacking Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils import data\ntorch.backends.cudnn.benchmark = True\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n\nsys.path.append(\"../input/iterative-stratification/iterative-stratification-master\")\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nsys.path.append(\"../input/pytorch-pfn-extras/pytorch-pfn-extras-0.3.1/\")\nimport pytorch_pfn_extras as ppe\nfrom pytorch_pfn_extras.training import extensions as ppe_extensions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## prepare"},{"metadata":{},"cell_type":"markdown","source":"### definition"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoAStackingDataset(data.Dataset):\n    \n    def __init__(self, feat: np.ndarray, label: np.ndarray = None):\n        \"\"\"\"\"\"\n        self.feat = feat\n        if label is None:\n            self.label = np.full((len(feat), 1), -1)\n        else:\n            self.label = label\n        \n    def __len__(self):\n        \"\"\"\"\"\"\n        return len(self.feat)\n    \n    def __getitem__(self, index: int):\n        \"\"\"\"\"\"\n        return [\n            torch.from_numpy(self.feat[index]).float(),\n            torch.from_numpy(self.label[index]).float()\n        ]\n\n\nclass MoAStackingDataset2d(data.Dataset):\n    \n    def __init__(self, feat: np.ndarray, label: np.ndarray = None):\n        \"\"\"\"\"\"\n        self.feat = feat\n        if label is None:\n            self.label = np.full((len(feat), 1), -1)\n        else:\n            self.label = label\n        self.reset_model_order()\n        \n    def reset_model_order(self):\n        self.model_order = np.arange(self.feat.shape[-1])\n        \n    def shuffle_model_order(self, seed):\n        np.random.seed(seed)\n        self.model_order = np.random.permutation(self.model_order)\n        \n    def __len__(self):\n        \"\"\"\"\"\"\n        return len(self.feat)\n    \n    def __getitem__(self, index: int):\n        \"\"\"\"\"\"\n        return [\n            torch.from_numpy(self.feat[index][:, self.model_order]).float(),\n            torch.from_numpy(self.label[index]).float()\n        ]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def get_activation(activ_name: str=\"relu\"):\n    \"\"\"\"\"\"\n    act_dict = {\n        \"relu\": nn.ReLU(),\n        \"tanh\": nn.Tanh(),\n        \"sigmoid\": nn.Sigmoid(),\n        \"identity\": nn.Identity()}\n    if activ_name in act_dict:\n        return act_dict[activ_name]\n    elif re.match(r\"^htanh\\_\\d{4}$\", activ_name):\n        bound = int(activ_name[-4:]) / 1000\n        return nn.Hardtanh(-bound, bound)\n    else:\n        raise NotImplementedError\n\nclass LBAD(nn.Module):\n    \"\"\"Linear (-> BN) -> Activation (-> Dropout)\"\"\"\n    \n    def __init__(\n        self, in_features: int, out_features: int, drop_rate: float=0.0,\n        use_bn: bool=False, use_wn: bool=False, activ: str=\"relu\"\n    ):\n        \"\"\"\"\"\"\n        super(LBAD, self).__init__()\n        layers = [nn.Linear(in_features, out_features)]\n        if use_wn:\n            layers[0] = nn.utils.weight_norm(layers[0])\n        \n        if use_bn:\n            layers.append(nn.BatchNorm1d(out_features))\n        \n        layers.append(get_activation(activ))\n        \n        if drop_rate > 0:\n            layers.append(nn.Dropout(drop_rate))\n        \n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        return self.layers(x)\n    \n    \nclass BDLA(nn.Module):\n    \"\"\"(BN -> Dropout ->) Linear -> Activation\"\"\"\n    \n    def __init__(\n        self, in_features: int, out_features: int, drop_rate: float=0.0,\n        use_bn: bool=False, use_wn: bool=False, activ: str=\"relu\"\n    ):\n        \"\"\"\"\"\"\n        super(BDLA, self).__init__()\n        layers = []\n        if use_bn:\n            layers.append(nn.BatchNorm1d(in_features))\n            \n        if drop_rate > 0:\n            layers.append(nn.Dropout(drop_rate))\n        \n        layers.append(nn.Linear(in_features, out_features))\n        if use_wn:\n            layers[-1] = nn.utils.weight_norm(layers[-1])\n            \n        layers.append(get_activation(activ))\n        \n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        return self.layers(x)\n    \n\nclass LABD(nn.Module):\n    \"\"\"Linear -> Activation (-> BN -> Dropout) \"\"\"\n    \n    def __init__(\n        self, in_features: int, out_features: int, drop_rate: float=0.0,\n        use_bn: bool=False, use_wn: bool=False, activ: str=\"relu\"\n    ):\n        \"\"\"\"\"\"\n        super(LABD, self).__init__()\n        layers = [nn.Linear(in_features, out_features), get_activation(activ)]\n        \n        if use_wn:\n            layers[0] = nn.utils.weight_norm(layers[0])\n        \n        if use_bn:\n            layers.append(nn.BatchNorm1d(out_features))\n        \n        if drop_rate > 0:\n            layers.append(nn.Dropout(drop_rate))\n        \n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        return self.layers(x)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class MLP(nn.Module):\n    \"\"\"Stacked Dense layers\"\"\"\n    \n    def __init__(\n        self, n_features_list: tp.List[int], use_tail_as_out: bool=False,\n        drop_rate: float=0.0, use_bn: bool=False, use_wn: bool=False,\n        activ:str=\"relu\", block_name: str=\"LBAD\"\n    ):\n        \"\"\"\"\"\"\n        super(MLP, self).__init__()\n        n_layers = len(n_features_list) - 1\n        block_class = {\n            \"LBAD\": LBAD, \"BDLA\": BDLA, \"LABD\": LABD}[block_name]\n        layers = []\n        for i in range(n_layers):\n            in_feats, out_feats = n_features_list[i: i + 2]\n            if i == n_layers - 1 and use_tail_as_out:\n                if block_name in [\"BDLA\"]:\n                    layer = block_class(in_feats, out_feats, drop_rate, use_bn,  use_wn, \"identity\")\n                else:\n                    layer = nn.Linear(in_feats, out_feats)\n                    if use_wn:\n                        layer = nn.utils.weight_norm(layer)\n            else:\n                layer = block_class(in_feats, out_feats, drop_rate, use_bn,  use_wn, activ)\n            layers.append(layer)\n                \n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        return self.layers(x)\n\n\nclass CNNForStacking(nn.Module):\n    \"\"\"\"\"\"\n    \n    def __init__(\n        self, n_models: int,\n        n_channels_list: tp.List[int],\n        kwargs_head: tp.Dict,\n    ):\n        \"\"\"\"\"\"\n        super(CNNForStacking, self).__init__()\n        self.n_conv_layers = len(n_channels_list) - 1\n        for i in range(self.n_conv_layers):\n            in_ch = n_channels_list[i]\n            out_ch = n_channels_list[i + 1]\n            layer = nn.Sequential(\n                nn.Conv1d(in_ch, out_ch, 3, 1, 0),\n                nn.BatchNorm1d(out_ch),\n                nn.ReLU(inplace=True))\n            setattr(self, \"conv{}\".format(i + 1), layer)\n        \n        kwargs_head[\"n_features_list\"][0] = (n_models - 2 * self.n_conv_layers) * n_channels_list[-1]\n        self.head = MLP(**kwargs_head)\n    \n    def forward(self, x: torch.FloatTensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        bs = x.shape[0]\n        h = x  # shape: (bs, n_classes, n_models)\n        for i in range(self.n_conv_layers):\n            h = getattr(self, \"conv{}\".format(i + 1))(h)\n            \n        h = torch.reshape(h, (bs, -1))\n        h = self.head(h)\n        return h","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def set_random_seed(seed: int = 42, deterministic: bool = False):\n    \"\"\"Set seeds\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    if deterministic:\n        torch.backends.cudnn.deterministic = True  # type: ignore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inference_function(settings, model, loader, device):\n    model.to(device)\n    model.eval()\n    pred_list = []\n    with torch.no_grad():\n        for x, t in loader:\n            y = model(x.to(device))\n            pred_list.append(y.sigmoid().detach().cpu().numpy())\n        \n        pred_arr = np.concatenate(pred_list)\n        del pred_list\n    return pred_arr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### create dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.concatenate([df.values for df in oof_list], axis=1)\nprint(X.shape)\n\ny = y_true.values\nprint(y.shape)\n\ntrain_all_dataset = MoAStackingDataset(X, y)\n\nX_test = np.concatenate([\n    sub_df.iloc[:, 1:].values for sub_df in sub_list], axis=1)\nprint(X_test.shape)\ntest_dataset = MoAStackingDataset(X_test, None)\n\nX_2d = np.stack([df.values for df in oof_list], axis=2)\nprint(X_2d.shape)\n\ntrain_all_dataset_2d = MoAStackingDataset2d(X_2d, y)\n\nX_test_2d = np.stack(\n    [sub_df.iloc[:, 1:].values for sub_df in sub_list], axis=2)\nprint(X_test_2d.shape)\ntest_dataset_2d = MoAStackingDataset2d(X_test_2d, None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MLP"},{"metadata":{"trusted":true},"cell_type":"code","source":"settings_str = \"\"\"\nglobals:\n  seed: 1990\n  seeds_for_avg: [\n    1990,\n    42, 0, 1086, 39\n  ]\n  max_epoch: 20\n  n_folds_split: 7\n  patience: 10\n  cuda_visible_devices: 0\n  device: cuda\n  fast_commit: false\n\nloader:\n  train: {\n    batch_size: 128, shuffle: True, num_workers: 2,\n    pin_memory: True, drop_last: True}\n  val: {\n    batch_size: 256, shuffle: False, num_workers: 2,\n    pin_memory: True, drop_last: False}\n\nmodel:\n  name: MLP\n  params:\n    n_features_list: [-1, 1024, 1024, 206]\n    use_tail_as_out: True\n    drop_rate: 0.2\n    use_bn: True\n    use_wn: True\n    block_name: BDLA\n        \nloss:\n  name: MyLSLogLoss\n  params: {k: 2, alpha: 1.0e-03}\n\noptimizer:\n  name: Adam\n  # params: {lr: 1.0e-03}\n  params: {lr: 1.0e-03, weight_decay: 5.0e-06}\n\nscheduler:\n  name: OneCycleLR\n  params: {pct_start: 0.1, div_factor: 1.0e+3, max_lr: 1.0e-02}\n\"\"\"\n\nsettings = yaml.safe_load(settings_str)\nsettings[\"model\"][\"params\"][\"n_features_list\"][0] = len(oof_list) * 206","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(settings[\"globals\"][\"device\"])\ncv_score_list_mlp = []\nstgs_list_mlp = []\n\n# # seed avg\nfor tmp_seed in settings[\"globals\"][\"seeds_for_avg\"]:\n    stgs_list_mlp.append(deepcopy(settings))\n    stgs_list_mlp[-1][\"globals\"][\"seed\"] = tmp_seed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"oof_pred_arr_avg_mlp = np.zeros((len(X), 206))\ntest_pred_arr_avg_mlp = np.zeros((len(X_test), 206))\n\nfor m_id, tmp_stgs in enumerate(stgs_list_mlp):\n    n_folds = tmp_stgs[\"globals\"][\"n_folds_split\"]\n    tmp_seed = tmp_stgs[\"globals\"][\"seed\"]\n    mlskf = MultilabelStratifiedKFold(n_splits=n_folds, shuffle=True, random_state=tmp_seed)\n    train_val_indexs = list(mlskf.split(train_all_dataset.feat, train_all_dataset.label))\n    \n    oof_pred_arr = np.zeros((len(X), 206))\n    test_preds_arr = np.zeros((n_folds, len(X_test), 206))\n    \n    for fold_id in range(n_folds):\n        print(\"[fold: {} - model: {}]\".format(fold_id, m_id))\n        model_path = \"../input/moa-weight-stacking-5model-by-mlp-seed-cv/best_model_fold{}_model{}.pth\".format(fold_id, m_id)\n        model = MLP(**tmp_stgs[\"model\"][\"params\"])\n        model.load_state_dict(torch.load(model_path))\n\n        val_index = train_val_indexs[fold_id][1]\n        val_dataset = data.Subset(train_all_dataset, val_index)\n        val_loader = data.DataLoader(val_dataset, **tmp_stgs[\"loader\"][\"val\"])\n\n        val_pred = inference_function(tmp_stgs, model, val_loader, device)\n        oof_pred_arr[val_index] = val_pred\n\n        del val_dataset; del val_loader; del val_pred\n\n        test_loader = data.DataLoader(test_dataset, **tmp_stgs[\"loader\"][\"val\"])\n        test_pred = inference_function(tmp_stgs, model, test_loader, device)\n        test_preds_arr[fold_id] = test_pred\n\n        del test_loader; del test_pred;\n        gc.collect()\n    \n    oof_pred_arr_avg_mlp += oof_pred_arr\n    test_pred_arr_avg_mlp += test_preds_arr.mean(axis=0)\n    \n    oof_score = log_loss(y.ravel(), oof_pred_arr.ravel())\n\n    cv_score_list_mlp.append([m_id, \"oof\", oof_score])\n    \noof_pred_arr_avg_mlp /= len(stgs_list_mlp)\ntest_pred_arr_avg_mlp /= len(stgs_list_mlp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"pd.DataFrame(cv_score_list_mlp, columns=[\"model\", \"fold\", \"val loss\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(log_loss(y.ravel(), oof_pred_arr_avg_mlp.ravel()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BLEND_mlp = sub_list[0].copy()\nBLEND_mlp.iloc[:, 1:] = test_pred_arr_avg_mlp\n\nprint(\"shape:\", BLEND_mlp.shape)\ndisplay(BLEND_mlp.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1D-CNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"settings_str = \"\"\"\nglobals:\n  seed: 1990\n  seeds_for_avg: [\n    1990,\n    42, 0, 1086, 39\n  ]\n  max_epoch: 20\n  n_folds_split: 7\n  patience: 10\n  cuda_visible_devices: 0\n  device: cuda\n  fast_commit: False\n\nloader:\n  train: {\n    batch_size: 128, shuffle: True, num_workers: 2,\n    pin_memory: True, drop_last: True}\n  val: {\n    batch_size: 256, shuffle: False, num_workers: 2,\n    pin_memory: True, drop_last: False}\n\nmodel:\n  name: CNNforStacking\n  params:\n    n_models: -1\n    n_channels_list: [206, 512, 1024]\n    kwargs_head:\n        n_features_list: [-1, 1024, 206]\n        use_tail_as_out: True\n        drop_rate: 0.2\n        use_bn: True\n        use_wn: True\n        block_name: BDLA\n        \nloss:\n  name: MyLSLogLoss\n  params: {k: 2, alpha: 1.0e-03}\n\noptimizer:\n  name: Adam\n  params: {lr: 1.0e-03, weight_decay: 5.0e-06}\n\nscheduler:\n  name: OneCycleLR\n  params: {pct_start: 0.1, div_factor: 1.0e+3, max_lr: 1.0e-02}\n\"\"\"\n\nsettings = yaml.safe_load(settings_str)\n\nsettings[\"model\"][\"params\"][\"n_models\"] = len(oof_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(settings[\"globals\"][\"device\"])\ncv_score_list_cnn = []\n\nstgs_list_cnn = []\n\n# # seed avg\nfor tmp_seed in settings[\"globals\"][\"seeds_for_avg\"]:\n    stgs_list_cnn.append(deepcopy(settings))\n    stgs_list_cnn[-1][\"globals\"][\"seed\"] = tmp_seed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"oof_pred_arr_avg_cnn = np.zeros((len(X), 206))\ntest_pred_arr_avg_cnn = np.zeros((len(X_test), 206))\n\nfor m_id, tmp_stgs in enumerate(stgs_list_cnn):\n    n_folds = tmp_stgs[\"globals\"][\"n_folds_split\"]\n    tmp_seed = tmp_stgs[\"globals\"][\"seed\"]\n    mlskf = MultilabelStratifiedKFold(n_splits=n_folds, shuffle=True, random_state=tmp_seed)\n    train_val_indexs = list(mlskf.split(train_all_dataset.feat, train_all_dataset.label))\n    \n    oof_pred_arr = np.zeros((len(X), 206))\n    test_preds_arr = np.zeros((n_folds, len(X_test), 206))\n    \n    train_all_dataset_2d.reset_model_order()\n    train_all_dataset_2d.shuffle_model_order(tmp_seed)\n    test_dataset_2d.model_order = train_all_dataset_2d.model_order\n    print(\"[model {}'s order]: {}\".format(m_id, train_all_dataset_2d.model_order))\n    \n    for fold_id in range(n_folds):\n        print(\"[fold: {} - model: {}]\".format(fold_id, m_id))\n        model_path = \"../input/moa-weight-stacking-5model-by-1dcnn-seed-cv/best_model_cnn_fold{}_model{}.pth\".format(fold_id, m_id)\n        model = CNNForStacking(**tmp_stgs[\"model\"][\"params\"])\n        model.load_state_dict(torch.load(model_path))\n\n        val_index = train_val_indexs[fold_id][1]\n        val_dataset = data.Subset(train_all_dataset_2d, val_index)\n        val_loader = data.DataLoader(val_dataset, **tmp_stgs[\"loader\"][\"val\"])\n\n        val_pred = inference_function(tmp_stgs, model, val_loader, device)\n        oof_pred_arr[val_index] = val_pred\n\n        del val_dataset; del val_loader; del val_pred\n\n        test_loader = data.DataLoader(test_dataset_2d, **tmp_stgs[\"loader\"][\"val\"])\n        test_pred = inference_function(tmp_stgs, model, test_loader, device)\n        test_preds_arr[fold_id] = test_pred\n\n        del test_loader; del test_pred;\n        gc.collect()\n    \n    oof_pred_arr_avg_cnn += oof_pred_arr\n    test_pred_arr_avg_cnn += test_preds_arr.mean(axis=0)\n    \n    oof_score = log_loss(y.ravel(), oof_pred_arr.ravel())\n    cv_score_list_cnn.append([m_id, \"oof\",  oof_score])\n    \noof_pred_arr_avg_cnn /= len(stgs_list_cnn)\ntest_pred_arr_avg_cnn /= len(stgs_list_cnn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(cv_score_list_cnn, columns=[\"model\", \"fold\", \"val loss\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(log_loss(y.ravel(), oof_pred_arr_avg_cnn.ravel()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BLEND_cnn = sub_list[0].copy()\nBLEND_cnn.iloc[:, 1:] = test_pred_arr_avg_cnn\n\nprint(\"shape:\", BLEND_cnn.shape)\ndisplay(BLEND_cnn.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### WO"},{"metadata":{"trusted":true},"cell_type":"code","source":"opt_weights = [ 0.1709929 ,  0.22160569,  0.07207321,  0.95431566, -0.42108426]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_pred_arr_avg_wo = np.zeros((len(X),206))\n\nfor w, oof_df in zip(opt_weights, oof_list):\n    oof_pred_arr_avg_wo += oof_df.values * w\n\nprint(log_loss(y.ravel(), oof_pred_arr_avg_wo.ravel()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BLEND_wo = sub_list[0].copy()\nBLEND_wo.iloc[:, 1:] = 0.\n\nfor w, sub_df in zip(opt_weights, sub_list):\n    BLEND_wo.iloc[:,1:] += sub_df.iloc[:,1:] * w","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## averaging stacking models"},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_pred_arr_avg = (oof_pred_arr_avg_mlp + oof_pred_arr_avg_cnn + oof_pred_arr_avg_wo) / 3\n\nprint(log_loss(y.ravel(), oof_pred_arr_avg.ravel()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BLEND = sub_list[0].copy()\nBLEND.iloc[:, 1:] = (BLEND_mlp.iloc[:, 1:] + BLEND_cnn.iloc[:, 1:] + BLEND_wo.iloc[:,1:]) / 3\n\nBLEND.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"if DO_SUBMISSION or DO_TRAIN_FOR_ENSEMBLE:\n    df_test = pd.read_csv(\"../input/lish-moa/test_features.csv\")\n    submission = pd.read_csv(\"../input/lish-moa/sample_submission.csv\")\n\nelif DO_VIRTUAL_SUBMISSION:\n    df_test = pd.read_csv(\"../input/lish-moa/test_features.csv\")\n    submission = pd.read_csv(\"../input/lish-moa/sample_submission.csv\")\n    print(df_test.shape, submission.shape)\n    df_test, submission = generate_virtual_private(df_test, submission)\n    print(df_test.shape, submission.shape)\n\nelse:\n    raise ValueError","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\n\n# df = pd.read_csv(\"/kaggle/input/lish-moa/sample_submission.csv\")\ndf = submission.copy()\n\npublic_id = list(df['sig_id'].values)\n\n# df_test = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\ntest_id = list(df_test['sig_id'].values)\n\nprivate_id = list(set(test_id)-set(public_id))\n\ndf_submit = pd.DataFrame(index = public_id+private_id, columns=TARGET_COL)\ndf_submit.index.name = 'sig_id'\ndf_submit[:] = 0\ndf_predict = BLEND.copy()\ndf_submit.loc[df_predict.sig_id,:] = df_predict[TARGET_COL].values\ndf_submit.loc[df_test[df_test.cp_type =='ctl_vehicle'].sig_id] = 0\ndf_submit.to_csv('submission.csv',index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_submit.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submit.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}