{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport pickle\nimport random\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nsys.path.append('../input/rank-gauss')\nfrom gauss_rank_scaler import GaussRankScaler\nimport tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nfrom warnings import simplefilter\nsimplefilter(action='ignore', category=FutureWarning)\n\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# parameters\ndata_path = '../input/lish-moa/'\nno_ctl = True\nscale = 'rankgauss'\nvariance_threshould = 0.7\ndecompo = 'PCA'\nncompo_genes = 80\nncompo_cells = 10\nencoding = 'dummy'\nbase_seed = 2020","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything(base_seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read data\ntest = pd.read_csv(data_path + 'test_features.csv')\ntrain = pd.read_csv(data_path + 'train_features.csv')\ntargets = pd.read_csv(data_path + 'train_targets_scored.csv')\n\nif no_ctl:\n    # 删掉 cp_type==ctl_vehicle 的样本\n    print('not_ctl')\n    train = train[train['cp_type']!='ctl_vehicle']\n    test = test[test['cp_type']!='ctl_vehicle']\n    targets = targets.iloc[train.index]\n    train.reset_index(drop=True, inplace=True)\n    test.reset_index(drop=True, inplace=True)\n    targets.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 筛掉方差小于 variance_threshould 的特征\ndata_all = pd.concat([train, test], ignore_index=True)\ncols_numeric = [feat for feat in list(data_all.columns) if feat not in ['sig_id', 'cp_type', 'cp_time', 'cp_dose']]\nmask = (data_all[cols_numeric].var() >= variance_threshould).values\ntmp = data_all[cols_numeric].loc[:, mask]\ndata_all = pd.concat([data_all[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)\ncols_numeric = [feat for feat in list(data_all.columns) if feat not in ['sig_id', 'cp_type', 'cp_time', 'cp_dose']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale_minmax(col):\n    return (col - col.min()) / (col.max() - col.min())\n\ndef scale_norm(col):\n    return (col - col.mean()) / col.std()\n\nif scale == 'boxcox':\n    # 通过 BoxCox 正态化\n    print('boxcox')\n    data_all[cols_numeric] = data_all[cols_numeric].apply(scale_minmax, axis=0)\n    trans = []\n    for feat in cols_numeric:\n        trans_var, lambda_var = stats.boxcox(data_all[feat].dropna() + 1)\n        trans.append(scale_minmax(trans_var))\n    data_all[cols_numeric] = np.asarray(trans).T\n    \nelif scale == 'norm':\n    # 通过标准化正态化\n    print('norm')\n    data_all[cols_numeric] = data_all[cols_numeric].apply(scale_norm, axis=0)\n    \nelif scale == 'minmax':\n    # 归一化\n    print('minmax')\n    data_all[cols_numeric] = data_all[cols_numeric].apply(scale_minmax, axis=0)\n    \nelif scale == 'rankgauss':\n    # RankGauss\n    print('rankgauss')\n    scaler = GaussRankScaler()\n    data_all[cols_numeric] = scaler.fit_transform(data_all[cols_numeric])\n    \nelse:\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 降维\nfrom sklearn.decomposition import PCA\n\nif decompo == 'PCA':\n    print('PCA')\n    GENES = [col for col in data_all.columns if col.startswith('g-')]\n    CELLS = [col for col in data_all.columns if col.startswith('c-')]\n\n    pca_genes = PCA(n_components=ncompo_genes, random_state=base_seed).fit_transform(data_all[GENES])\n    pca_cells = PCA(n_components=ncompo_cells, random_state=base_seed).fit_transform(data_all[CELLS])\n    pca_genes = pd.DataFrame(pca_genes, columns=[f'pca_g-{i}' for i in range(ncompo_genes)])\n    pca_cells = pd.DataFrame(pca_cells, columns=[f'pca_c-{i}' for i in range(ncompo_cells)])\n    data_all = pd.concat([data_all, pca_genes, pca_cells], axis=1)\nelse:\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding\nif encoding == 'lb':\n    print('Label Encoding')\n    for feat in ['cp_time', 'cp_dose']:\n        data_all[feat] = LabelEncoder().fit_transform(data_all[feat])\nelif encoding == 'dummy':\n    print('One-hot')\n    data_all = pd.get_dummies(data_all, columns=['cp_time', 'cp_dose'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 特征生成\nGENES = [col for col in data_all.columns if col.startswith('g-')]\nCELLS = [col for col in data_all.columns if col.startswith('c-')]\nfor stats in tqdm.tqdm(['sum', 'mean', 'std', 'kurt', 'skew']):\n    data_all['g_'+stats] = getattr(data_all[GENES], stats)(axis=1)\n    data_all['c_'+stats] = getattr(data_all[CELLS], stats)(axis=1)\n    data_all['gc_'+stats] = getattr(data_all[GENES+CELLS], stats)(axis=1)\n    \n# for cell in CELLS:\n#     data_all[cell + '_squared'] = data_all[cell] ** 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 保存数据\nimport pickle\n\nwith open('data_all.pickle', 'wb') as f:\n    pickle.dump(data_all, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 读取数据\nwith open('data_all.pickle', 'rb') as f:\n    data_all = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 得到 train_df, test_df\nfeatures_todrop = ['sig_id', 'cp_type']\ndata_all.drop(features_todrop, axis=1, inplace=True)\ntry:\n    targets.drop('sig_id', axis=1, inplace=True)\nexcept:\n    pass\n\ntrain_df = data_all[:train.shape[0]]\ntrain_df.reset_index(drop=True, inplace=True)\ntrain_df = pd.concat([train_df, targets], axis=1)\ntest_df = data_all[train_df.shape[0]:]\ntest_df.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hypter-parameters\ndevice = ('cuda' if torch.cuda.is_available() else 'cpu')\n\n# model\nhsize = 1024\ndropratio = 0.2\n\n# train\nbatchsize = 128\nlr = 0.001\nwd = 1e-5\nsmoothing = 0.001\np_min = smoothing\np_max = 1 - smoothing\nnepoch = 20\nearlystop = True\nearlystop_step = 10\n\n# lr_scheduler, options: ['OneCycleLR', 'ReduceLROnPlateau', 'both']\nlr_scheduler = 'OneCycleLR'\n# OneCycleLR\npct_start = 0.1\ndiv_factor = 1e3\n# ReduceLROnPlateau\nfactor=0.5\npatience=3\n\n# kfold\nnseed = 5\nnfold = 7\neval_strategy = 'kfold'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 模型结构\nclass Model(nn.Module):\n    def __init__(self, n_features, n_targets, hidden_size=512, dropratio=0.2):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(n_features)\n        self.dropout1 = nn.Dropout(dropratio)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(n_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(dropratio)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(dropratio)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, n_targets))\n        \n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = self.relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = self.relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# label smmothing\nclass SmoothCrossEntropyLoss(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets, n_classes, smoothing=0.0):\n        assert 0 <= smoothing <= 1\n        with torch.no_grad():\n#             targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n            targets = targets * (1 - smoothing) + torch.ones_like(targets).to(device) * smoothing / n_classes\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothCrossEntropyLoss()._smooth(targets, inputs.shape[1], self.smoothing)\n\n        if self.weight is not None:\n            inputs = inputs * self.weight.unsqueeze(0)\n\n        loss = F.binary_cross_entropy_with_logits(inputs, targets)\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def running_train(X_train, Y_train, X_val, Y_val, dataloader, i_fold=None, seed=None):\n    # prepare for train\n    model = Model(n_features=X_train.shape[1], n_targets=Y_train.shape[1], hidden_size=hsize, dropratio=dropratio).to(device)\n    criterion = SmoothCrossEntropyLoss(smoothing=smoothing)\n    metric = lambda inputs, targets : F.binary_cross_entropy((torch.clamp(torch.sigmoid(inputs), p_min, p_max)), targets)\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, weight_decay=wd)\n    if lr_scheduler == 'OneCycleLR' or lr_scheduler == 'both':\n        scheduler1 = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=pct_start, div_factor=div_factor, \n                                                    max_lr=1e-2, epochs=nepoch, steps_per_epoch=len(dataloader))\n    if lr_scheduler == 'ReduceLROnPlateau' or lr_scheduler == 'both':\n        scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=3)\n    # train\n    min_valmetric = np.inf\n    step = 0\n    for epoch in range(nepoch):\n        train_loss = 0\n        train_metric = 0\n        for i, (X, Y) in enumerate(dataloader):\n            model.train()\n            predictions = model(X.to(device=device))\n            loss = criterion(predictions, Y.to(device=device))\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            if lr_scheduler == 'OneCycleLR' or lr_scheduler == 'both':\n                scheduler1.step()\n\n            train_loss += loss.item()\n            train_metric += metric(predictions, Y.to(device=device))\n\n        train_loss /= len(dataloader)\n        train_metric /= len(dataloader)\n        model.eval()\n        predictions = model(X_val.to(device=device))\n\n        val_loss = criterion(predictions, Y_val.to(device=device))\n        val_metric = metric(predictions, Y_val.to(device=device))\n        if lr_scheduler == 'ReduceLROnPlateau' or lr_scheduler == 'both':\n            scheduler2.step(val_metric)\n        print('Epoch {}/{}, Train Loss={:5f}, Train Metric={:.5f}, Val Loss={:.5f}, Val Metric={:.5f}'.format(\n            epoch + 1, nepoch, train_loss, train_metric, val_loss, val_metric))\n        if val_metric.item() < min_valmetric:\n            min_valmetric = val_metric.item()\n            model_name = 'model_{}_{}.pth'.format(i_fold + 1, seed) if eval_strategy == 'kfold' else 'model_single.pth'\n            torch.save(model.state_dict(), model_name)\n        elif earlystop:\n            step += 1\n            if step > earlystop_step:\n                break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# kfold\ndef train_kfold_model(train_df):\n    X_train_val = torch.from_numpy(train_df.iloc[:, :-targets.shape[1]].values).to(dtype=torch.float32)\n    Y_train_val = torch.from_numpy(train_df.iloc[:, -targets.shape[1]:].values).to(dtype=torch.float32)\n    X_test = torch.from_numpy(test_df.values).to(dtype=torch.float32)\n\n    oof = torch.zeros(X_train_val.shape[0], Y_train_val.shape[1]) # 用于计算 cv_score\n    prediction_test = torch.zeros(X_test.shape[0], Y_train_val.shape[1]) # 用于计算 submission\n\n    for i_seed in range(nseed):\n        seed = random.randint(0, base_seed)\n        seed_everything(seed)\n        print('Seed: {}, {}/{}'.format(seed, i_seed + 1, nseed))\n        mskf = MultilabelStratifiedKFold(n_splits=nfold, random_state=seed, shuffle=True)\n        for i_fold, (train_idx, val_idx) in enumerate(mskf.split(X_train_val, Y_train_val)):\n            print(\"# Fold: {}/{} (seed: {}/{})\".format(i_fold + 1, nfold, i_seed + 1, nseed))\n\n            # dataset and dataloader\n            X_train, Y_train = X_train_val[train_idx], Y_train_val[train_idx]\n            X_val, Y_val = X_train_val[val_idx], Y_train_val[val_idx]\n            dataset = TensorDataset(X_train, Y_train)\n            dataloader = DataLoader(dataset, batch_size=batchsize, shuffle=True)\n\n            # train\n            running_train(X_train, Y_train, X_val, Y_val, dataloader, i_fold=i_fold, seed=seed)\n\n            # predict on oof\n            print('predict on oof...', end='')\n            model = Model(n_features=X_train.shape[1], n_targets=Y_train.shape[1], hidden_size=hsize, dropratio=dropratio).to(device)\n            model.load_state_dict(torch.load('model_{}_{}.pth'.format(i_fold + 1, seed)))\n            model.eval()\n            oof[val_idx] += torch.clamp(torch.sigmoid(model(X_val.to(device=device)).detach().cpu()), p_min, p_max) / nseed\n            print('  done.')\n            # predict on test\n            print('predict on test...', end='')\n            prediction_test += torch.clamp(torch.sigmoid(model(X_test.to(device=device)).detach().cpu()), p_min, p_max) / (nfold * nseed)\n            print('  done.\\n')\n\n    cv_score = F.binary_cross_entropy(oof, Y_train_val)\n    print('{} folds cv_score: {:.5f}'.format(nfold, cv_score))\n    \n    return oof, pd.DataFrame(prediction_test.numpy(), columns=targets.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# single model\ndef train_single_model(train_df):\n    # obtain train, val, test\n    train_df, val_df = train_test_split(train_df, test_size=0.2, shuffle=True, random_state=base_seed)\n\n    X_train = train_df.iloc[:, :-targets.shape[1]].values\n    Y_train = train_df.iloc[:, -targets.shape[1]:].values\n    X_val = val_df.iloc[:, :-targets.shape[1]].values\n    Y_val = val_df.iloc[:, -targets.shape[1]:].values\n    X_test = test_df.values\n\n    # dataset 和 dataloader\n    X_train = torch.from_numpy(X_train).to(dtype=torch.float32)\n    Y_train = torch.from_numpy(Y_train).to(dtype=torch.float32)\n    X_val = torch.from_numpy(X_val).to(dtype=torch.float32)\n    Y_val = torch.from_numpy(Y_val).to(dtype=torch.float32)\n    X_test = torch.from_numpy(X_test.astype(np.float32))\n    dataset = TensorDataset(X_train, Y_train)\n    dataloader = DataLoader(dataset, batch_size=batchsize, shuffle=True)\n    \n    # train\n    running_train(X_train, Y_train, X_val, Y_val, dataloader)\n\n    # predict on test\n    print('predict on test...', end='')\n    model = Model(n_features=X_train.shape[1], n_targets=Y_train.shape[1], hidden_size=hsize, dropratio=dropratio).to(device)\n    model.load_state_dict(torch.load('model_single.pth'))\n    model.eval()\n    predictions = torch.clamp(torch.sigmoid(model(X_test.to(device)).detach().cpu()), p_min, p_max)\n    print('  done.\\n')\n    \n    return pd.DataFrame(predictions.numpy(), columns=targets.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if eval_strategy == 'kfold':\n    oof, test_pred = train_kfold_model(train_df)\nelif eval_strategy == 'single':\n    test_pred = train_single_model(train_df)\nelse:\n    print('eval_strategy should be \\\"kfold\\\" or \\\"single\\\"')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submit\ntest = pd.read_csv(data_path + 'test_features.csv')\nsig_id = test[test['cp_type']!='ctl_vehicle'].sig_id.reset_index(drop=True)\ntest_pred['sig_id'] = sig_id\n\nsub = pd.merge(test[['sig_id']], test_pred, on='sig_id', how='left')\nsub.fillna(0, inplace=True)\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}