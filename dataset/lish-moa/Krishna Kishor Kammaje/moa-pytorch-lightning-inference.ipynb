{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Training from\nhttps://www.kaggle.com/krisho007/moa-pytorch-lightning-kfold"},{"metadata":{},"cell_type":"markdown","source":"### Inference"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Install pytorch-lightning\n!pip install ../input/pytorch-lightening/tensorboard-2.2.0-py3-none-any.whl -q\n!pip install ../input/pytorch-lightening/pytorch_lightning-0.9.0-py3-none-any.whl -q","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"import pandas as pd\nimport pytorch_lightning as pl\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nfrom  torch import nn\nimport torch\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class MoATestDataset(Dataset):\n    \n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return self.features.shape[0]\n        \n    def __getitem__(self, index):\n        return {\n            \"x\": torch.tensor(self.features[index, :], dtype=torch.float)\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"LR = 0.001\nF_DROPOUT = 0.45\nLAYERS = [4096, 2048, 1024, 512]\nLABEL_SMOOTHING = 0.001\n\nclass Model(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super().__init__()\n        layers = []\n        \n        # Intermediate layers\n        in_size = num_features   \n        for i in range(len(LAYERS)):\n            out_size = LAYERS[i]\n            layers.append(torch.nn.Linear(in_size, out_size, bias=False))\n            layers.append(nn.BatchNorm1d(out_size))\n            layers.append(nn.Dropout(F_DROPOUT))\n            layers.append(nn.PReLU())\n            in_size = out_size\n\n        # Final layer\n        layers.append(torch.nn.Linear(in_size, num_targets))    \n        self.model = torch.nn.Sequential(*layers)        \n        \n        # Initialize weights\n        self.model.apply(self._init_weights)\n        \n    def _init_weights(self, m):\n        if type(m) == nn.Linear:\n            nn.init.xavier_uniform_(m.weight)\n            if m.bias != None:\n                m.bias.data.fill_(0.01)\n        \n    def forward(self, x):\n        x = self.model(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class PLitMoAModule(pl.LightningModule):\n    def __init__(self, hparams, model):\n        super(PLitMoAModule, self).__init__()\n        self.hparams = hparams\n        self.model = model\n        self.criterion = nn.BCEWithLogitsLoss()\n        \n    def forward(self, x):\n        return self.model(x)\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.hparams[\"lr\"])\n        scheduler = {\"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, threshold=0.00001, mode='min', verbose=True),\n                      \"interval\": \"epoch\",\n                      \"monitor\": \"val_loss\"}\n        return [optimizer], [scheduler]\n    \n    def training_step(self, batch, batch_index):\n        features = batch['x']\n        targets = batch['y']\n        out = self(features)\n        loss = self.criterion(out, targets)\n        logs = {\"train_loss\" : loss}\n        return {\"loss\": loss, \"log\": logs, \"progress_bar\": logs}\n    \n    def training_epoch_end(self, outputs):\n        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n        logs = {\"train_loss\": avg_loss}\n        return {\"log\": logs, \"progress_bar\": logs}\n            \n    def validation_step(self, batch, batch_index):\n        features = batch['x']\n        targets = batch['y']\n        out = self(features)\n        loss = self.criterion(out, targets)\n        logs = {\"val_loss\" : loss}\n        return {\"loss\": loss, \"log\": logs, \"progress_bar\": logs}\n    \n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n        logs = {\"val_loss\": avg_loss}\n        return {\"log\": logs, \"progress_bar\": logs}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"sample_submission.shape, test_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Convert categorical features into OHE\ntest_features = pd.concat([test_features, pd.get_dummies(test_features['cp_time'], prefix='cp_time')], axis=1)\ntest_features = pd.concat([test_features, pd.get_dummies(test_features['cp_dose'], prefix='cp_dose')], axis=1)\ntest_features = pd.concat([test_features, pd.get_dummies(test_features['cp_type'], prefix='cp_type')], axis=1)\n# Delete original categorical features\ntest_features = test_features.drop(['cp_time', 'cp_dose', 'cp_type'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"batch_size=1024\ntestDataSet = MoATestDataset(test_features.iloc[:, 1:].values)\ntestDataLoader = DataLoader(testDataSet, batch_size=batch_size, num_workers=4, shuffle=False)\nnet = Model(879, 206) # Input Features, Output Targets\n# pylitModel = PLitMoAModule(hparams={\"lr\":1e-3}, model=net)\ntrainer = pl.Trainer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Empty array with as many rows\npredictions = np.zeros((test_features.shape[0], 206))\nmodel_path = '../input/optuna-moa-pytorch-lightning-kfold/models/'\nfor modelFileName in os.listdir(model_path):\n    # Load each K-Fold model\n    model = PLitMoAModule.load_from_checkpoint(checkpoint_path=f\"{model_path}{modelFileName}\", model=net)\n    for index, batch in enumerate(testDataLoader):\n        # Sigmoid is used to convert eact predictions into a range between 0 and 1 (probability)\n        batch_predictions = torch.sigmoid(model(batch['x'])).detach().cpu().numpy()\n        start_index = index*batch_size\n        end_index = index*batch_size + batch_predictions.shape[0]\n        predictions[start_index:end_index] = predictions[start_index:end_index] + batch_predictions\n\n# Average predictions across KFolds\npredictions = np.true_divide(predictions, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.iloc[:, 1:] = predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Post Processing\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/c/lish-moa/discussion/180165 \nvehicle_indices = test_features[test_features[\"cp_type_ctl_vehicle\"]==1].index.tolist()\nsample_submission.iloc[vehicle_indices, 1:] = np.zeros((1, 206))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}