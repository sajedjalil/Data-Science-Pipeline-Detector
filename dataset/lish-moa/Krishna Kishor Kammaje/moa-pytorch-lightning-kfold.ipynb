{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Introduction\nThis is a multi-label classification problem. I.e. you need to predict 206 labels  for each row in the test set.\nSince shallow learning methods like gradient boosting do not work great on multi-label problems, you may need to have one model for each target so you may end up 200+ models. So we are aiming for neural net based single model to predict 206 labels.\n\nThese 206 labels represent the properties (affects) of each drug (row). Each drug can exhibit one or more properties/affects. (i.e. Multi-Label)"},{"metadata":{},"cell_type":"markdown","source":"### Thanks to\nAbhishek and Andrew for their [live coding session](https://www.youtube.com/watch?v=VRVit0-0AXE) from which lots of code here is inspired."},{"metadata":{},"cell_type":"markdown","source":"- Multi-label classification - A row can belong to multiple classes SIMULTANEOUSLY\n- Multi-class classification - Classify a row into ONE of multiple possible classes . (One class at a time)."},{"metadata":{},"cell_type":"markdown","source":"### Hyper Parameters from Optune\nhttps://www.kaggle.com/krisho007/moa-pytorch-lightning-params-tuning-optuna"},{"metadata":{"trusted":true},"cell_type":"code","source":"LR = 0.001462129310551811\n# LR = 0.008\nN_LAYERS = 5\nF_DROPOUT = 0.407\nLAYERS = [987,1206,2390,2498,3449]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LABEL_SMOOTHING = 0.008","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"scrolled":true},"cell_type":"code","source":"# Install required modules only once\nimport sys\nimport subprocess\nimport pkg_resources\n\nrequired = {'iterative-stratification', 'pytorch-lightning'}\ninstalled = {pkg.key for pkg in pkg_resources.working_set}\nmissing = required - installed\n\nif missing:\n    python = sys.executable\n    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nimport torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom ranger_py import Ranger\nfrom mish_activation import *  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":true},"cell_type":"code","source":"test_features = pd.read_csv('../input/lish-moa/test_features.csv')\ntrain_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Though number of rows is not huge, number of features is 875, \n# which is very huge here, unlike in common cases.\ntrain_features.shape, test_features.shape, train_targets_scored.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train_features.cp_type.value_counts(), train_features.cp_time.value_counts(), train_features.cp_dose.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'ctl_vehicle' indicates that these rows are not treated. Meaning targets for these can be set as zero. https://www.kaggle.com/c/lish-moa/discussion/180165\ncp_time indicates How long each row was treated for?\nNext we have 772 gene features (prefixed with g) and 100 cell features (prefixed with c). "},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train_targets_scored.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Column wise sums across all rows\ntrain_targets_scored.sum()[1:].sort_values().head(100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you see there are atleast two columns which have 1 occurance only once. Andrey: \"Better to mark these as all zeros\". Maximum occurance is 832 (out of 20k+) so this is a very much unbalanced dataset, so this is going to be complicated.\n\nLet us look at g-values in a single row."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"gs = train_features[7:8][[col for col in train_features.columns if 'g-' in col]].values.reshape(-1, 1)\nplt.plot(gs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"plt.plot(sorted(gs))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"772 columns in all rows have the same spread as above. Mysterious why values always follow this curve. Even the c- columns also have a similar graph and all rows follow the same pattern. Originally observed by @shadab"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Let us look at spread of each columns\nfigure = plt.figure(figsize=(15,8))\nax1 = figure.add_subplot(4,2,1)\nax2 = figure.add_subplot(4,2,2)\nax3 = figure.add_subplot(4,2,3)\nax4 = figure.add_subplot(4,2,4)\nax5 = figure.add_subplot(4,2,5)\nax6 = figure.add_subplot(4,2,6)\nax7 = figure.add_subplot(4,2,7)\nax8 = figure.add_subplot(4,2,8)\n\nax1.hist(train_features['c-1'])\nax2.hist(train_features['g-1'])\nax3.hist(train_features['c-2'])\nax4.hist(train_features['g-2'])\nax5.hist(train_features['c-25'])\nax6.hist(train_features['g-25'])\nax7.hist(train_features['c-49'])\nax8.hist(train_features['g-49'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since all these seem to be zero centered, it looks like the data was normalized and a noise added later?"},{"metadata":{},"cell_type":"markdown","source":"### Dataset\nDataset is used by the Pytorch framework to read data, and find total length. This is where we convert data into Pytorch Tensors."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class MoADataset(Dataset):\n    \n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return self.features.shape[0]\n        \n    def __getitem__(self, index):\n        return {\n            \"x\": torch.tensor(self.features[index, :], dtype=torch.float),\n            \"y\": torch.tensor(self.targets[index, :], dtype=torch.float)\n        }\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Datamodule"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class MoADataModule(pl.LightningDataModule):\n    def __init__(self, batch_size=1024, fold= 0):\n        super().__init__()\n        self.batch_size = batch_size\n        self.fold = fold\n        \n    def prepare_data(self):\n        # Even in multi-GPU training. this method is called only from a single GPU. \n        # So this method ideal for download, stratification etc. \n        # Startification on multi-label dataset is tricky. \n        # scikit-learn stratified KFold cannot be used. \n        # So we are using interative-stratification\n        if os.path.isfile(\"train_folds.csv\"):\n            return\n        complete_training_data = self._read_data()        \n        self._startify_and_save(complete_training_data)        \n        \n    def _read_data(self):\n        features = pd.read_csv('../input/lish-moa/train_features.csv')\n        # Convert categorical features into OHE\n        features = pd.concat([features, pd.get_dummies(features['cp_time'], prefix='cp_time')], axis=1)\n        features = pd.concat([features, pd.get_dummies(features['cp_dose'], prefix='cp_dose')], axis=1)\n        features = pd.concat([features, pd.get_dummies(features['cp_type'], prefix='cp_type')], axis=1)\n        # Delete original categorical features\n        features = features.drop(['cp_time', 'cp_dose', 'cp_type'], axis=1)\n        targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n        merged = features.merge(targets_scored, how=\"inner\", on=\"sig_id\")\n        return merged\n        \n    def _startify_and_save(self, data):\n        # New column to hold the fold number\n        data.loc[:, \"kfold\"] = -1\n\n        # Shuffle the dataframe\n        data = data.sample(frac=1).reset_index(drop=True)        \n        \n        # 5 Folds\n        mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=False, random_state=None) \n        # trn_ and val_ are indices\n        targets = data.drop(['kfold', 'sig_id'], axis=1)                                                                                       \n        for fold_, (trn_,val_) in enumerate(mskf.split(X=data, y=targets.iloc[:, 879:])): \n            # We are just filling the vaidation indices. \n            # All other data are for training (trn indices are not required)\n            data.loc[val_, \"kfold\"] = fold_\n    \n        # We are saving the result to the disk so that other GPUs can pick it from there. \n        # Rather if we do \"self.startified_data = train_targets_scored\", \n        # other GPUs will not be able to read this \n        data.to_csv(\"train_folds.csv\", index=False)   \n        \n    def setup(self, stage=None):\n        # In multi-GPU training, this method is run on each GPU. \n        # So ideal for each training/valid split\n        data = pd.read_csv(\"train_folds.csv\")\n        \n        training_data = data[data.kfold != self.fold]\n        training_data = training_data.drop(['kfold', 'sig_id'], axis=1)\n        validation_data = data[data.kfold == self.fold]\n        validation_data = validation_data.drop(['kfold', 'sig_id'], axis=1)\n        self.train_dataset = MoADataset(training_data.iloc[:, :879].values, training_data.iloc[:, 879:].values)\n        self.valid_dataset = MoADataset(validation_data.iloc[:, :879].values, validation_data.iloc[:, 879:].values)        \n\n    \n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, self.batch_size, num_workers=4, shuffle=True)\n    \n    def val_dataloader(self):\n        return DataLoader(self.valid_dataset, self.batch_size, num_workers=4, shuffle=False)    \n            \n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NN Model"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super().__init__()\n        layers = []\n        \n        # Intermediate layers\n        in_size = num_features   \n        for i in range(N_LAYERS):\n            out_size = LAYERS[i]\n            layers.append(torch.nn.Linear(in_size, out_size, bias=False))\n            layers.append(nn.BatchNorm1d(out_size))\n            layers.append(nn.Dropout(F_DROPOUT))\n            layers.append(nn.PReLU())\n#             layers.append(nn.BatchNorm1d(in_size))\n#             layers.append(nn.Dropout(F_DROPOUT))    \n#             layers.append(torch.nn.Linear(in_size, out_size))\n#             layers.append(nn.PReLU())\n#             layers.append(Mish())\n            in_size = out_size\n\n        # Final layer\n        layers.append(torch.nn.Linear(in_size, num_targets))    \n        self.model = torch.nn.Sequential(*layers)        \n        \n        # Initialize weights\n        self.model.apply(self._init_weights)\n        \n    def _init_weights(self, m):\n        if type(m) == nn.Linear:\n            nn.init.xavier_uniform_(m.weight)\n            if m.bias != None:\n                m.bias.data.fill_(0.01)\n        \n    def forward(self, x):\n        x = self.model(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lightning Model"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class PLitMoAModule(pl.LightningModule):\n    def __init__(self, hparams, model):\n        super(PLitMoAModule, self).__init__()\n        self.hparams = hparams\n        self.model = model\n        self.criterion = nn.BCEWithLogitsLoss()\n#         self.criterion = nn.BCELoss()\n        \n    def forward(self, x):\n        return self.model(x)\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.hparams[\"lr\"] )\n#         optimizer = Ranger(self.model.parameters(), lr=self.hparams[\"lr\"] )\n        scheduler = {\"scheduler\": \n                     torch.optim.lr_scheduler.ReduceLROnPlateau(\n                        optimizer, patience=2, \n                        threshold=0.00003, \n                        mode='min', verbose=True),\n                    \"interval\": \"epoch\",\n                    \"monitor\": \"val_loss\"}\n        return [optimizer], [scheduler]\n    \n    def training_step(self, batch, batch_index):\n        features = batch['x']\n        targets = batch['y']\n        out = self(features)\n        targets_smooth = targets.float() * (1 - LABEL_SMOOTHING) + 0.5 * LABEL_SMOOTHING\n        loss = self.criterion(out, targets_smooth)\n        logs = {\"train_loss\" : loss}\n        return {\"loss\": loss, \"log\": logs, \"progress_bar\": logs}\n    \n    def training_epoch_end(self, outputs):\n        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n        logs = {\"train_loss\": avg_loss}\n        return {\"log\": logs, \"progress_bar\": logs}\n            \n    def validation_step(self, batch, batch_index):\n        features = batch['x']\n        targets = batch['y']\n        out = self(features)\n        loss = self.criterion(out, targets)\n        logs = {\"val_loss\" : loss}\n        return {\"loss\": loss, \"log\": logs, \"progress_bar\": logs}\n    \n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n        logs = {\"val_loss\": avg_loss}\n        return {\"log\": logs, \"progress_bar\": logs}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Five fold training. \nfor k in range(5):  \n    \n    checkpoint_callback = ModelCheckpoint(\n        filepath='./models/model_{epoch:02d}', \n        monitor='val_loss', verbose=False, \n        save_last=False, save_top_k=1, save_weights_only=False, \n        mode='min', period=1, prefix='')\n    trainer = pl.Trainer(gpus=-1 if torch.cuda.is_available() else None, max_epochs=30, checkpoint_callback=checkpoint_callback)\n    dm = MoADataModule(fold=k)\n    net = Model(879, 206) # Input Features, Output Targets\n    pylitModel = PLitMoAModule(hparams={\"lr\":LR}, model=net)\n    trainer.fit(pylitModel, dm)\n    \n    print(checkpoint_callback.best_model_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inference\nhttps://www.kaggle.com/krisho007/moa-pytorch-lightning-inference"},{"metadata":{},"cell_type":"markdown","source":"### ToDO\n- https://autogluon.mxnet.io/\n- TabNet\n- LabelSmoothing"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}