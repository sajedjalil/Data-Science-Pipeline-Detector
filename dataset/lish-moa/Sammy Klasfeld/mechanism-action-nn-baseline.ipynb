{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Background\nIn the competition, https://www.kaggle.com/headsortails/explorations-of-action-moa-eda, we saw that the data contains:\n* treated and control samples\n* high and low treatments\n* treatment treated for 24h, 48h, and 72h\nGene expression features were randomly sampled and found to be about normally distributed. \n\n# Goal\nBuild a basic neural network using pytorch to train, validate, and test a model to classify the mechanisms of action for each drug\n\n# Libraries\nThe code in the box directly below comes in every kaggle notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next I import the libaries that will be used to build the neural network."},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as opt\nimport torch.utils.data as data\n\nimport tqdm\nimport itertools","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Set up the dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/lish-moa/train_features.csv\")\nprint(train_df.shape)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a quick look at our categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.cp_type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.cp_dose.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"cool, cool. Now we look at the labels for each row in the training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels_df = pd.read_csv(\"/kaggle/input/lish-moa/train_targets_scored.csv\")\nprint(train_labels_df.shape)\ntrain_labels_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can later match the `sig_id` in the rows to get the labels. First, let's set up a validation set to use for training. We randomly choose 5000 of the 23814 rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"val_df = train_df.sample(5000, random_state=100) \nprint(val_df.shape)\nval_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And why not, let's import the testing set."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/lish-moa/test_features.csv\")\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create the Model\nNow the code starts to get more complicated as we dive into pytorch and a feed-forward network!\n\nHere is the reference I used to get acquainted as a beginner:\n* https://www.marktechpost.com/2019/06/30/building-a-feedforward-neural-network-using-pytorch-nn-module/\n\nA feed-forward network simply takes input, feeds the input through several layers, and then finally gives the output. \n\nTo make this network, we create a class `FeedForward_Network` which inherits the class `nn.Module`. An `nn.Module` contains layers, and a method `forward(input)` that returns the output.\n\nIn the `__init__` function we have 5 layers that apply linear transformations as well as a list of dropout parameters. Because we have 5 layets that means we reshape the data 4 times before we return the output. The four shapes here are 700,500,300, and 250. These can be optimized, but the reason we use decreasing numbers is because previous work has shown us that smaller layers can generalize models better.\n\nThe dropout rates can also be optimized. Dropouts are useful to prevent overfitting.\n\nWith each forward pass through the network we run the input through the first layer, apply relu to remove any negative connections (A negative relation is meaningless so removing them will allow for models to learn faster and perform better), dropout some of the sample features, repeat three times with the second, third, and forth layer, apply the fifth layer, and then run the data into a sigmoid function to scale the outputs to be between 0 and 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"class FeedForward_Network(nn.Module):\n    def __init__(self, input_size=100, output_size=100, \n                 dropout=[0.25, 0.25, 0.25, 0.25]):\n        super(FeedForward_Network, self).__init__()\n        \n        self.layer1 = nn.Linear(input_size, 700)\n        self.layer2 = nn.Linear(700, 500)\n        self.layer3 = nn.Linear(500, 300)\n        self.layer4 = nn.Linear(300, 250)\n        self.layer5 = nn.Linear(250, output_size)\n        \n        self.dropout = dropout\n        \n    def forward(self, x):\n        # run linear transformation using layer 1\n        output = self.layer1(x)\n        # remove negative connections\n        output = F.relu(output) \n        # prevent overfitting\n        output = F.dropout(output, p=self.dropout[0]) \n        \n        # run linear transformation using layer 2\n        output = self.layer2(output)\n        # remove negative connections\n        output = F.relu(output)\n        # prevent overfitting\n        output = F.dropout(output, p=self.dropout[1])\n        \n        # run linear transformation using layer 3\n        output = self.layer3(output) \n        # remove negative connections\n        output = F.relu(output)\n        # prevent overfitting\n        output = F.dropout(output, p=self.dropout[2])\n        \n        # run linear transformation using layer 4\n        output = self.layer4(output)\n        # remove negative connections\n        output = F.relu(output)\n        # prevent overfitting\n        output = F.dropout(output, p=self.dropout[3])\n        \n        # run linear transformation using layer 5\n        output = self.layer5(output)\n        # translate output to range between 0-1\n        return torch.sigmoid(output) \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we create a dictionary `data_map` that contains both our training and validation data in tuples. Each tuple contains:\n0. the features of the (training or validation) set\n1. the targets of the (training or validation) set"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_map = {\n    'train':(\n        train_df.query(f\"sig_id not in {val_df.sig_id.tolist()}\"), \n        train_labels_df.query(f\"sig_id not in {val_df.sig_id.tolist()}\")\n    ),\n    'validation':(\n        val_df, \n        train_labels_df.query(f\"sig_id in {val_df.sig_id.tolist()}\")\n    )\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we create a dictionary of dataloaders `dataloader_map` using the the above `data_map` dictionary. For beginners, note that we use \"[dictionary comprehension](https://www.python.org/dev/peps/pep-0274/)\" rather than a for-loop to fill the `dataloader_map`. \n\nThe `dataloader_map` contains a [DataLoader object](https://pytorch.org/docs/stable/data.html) for both both the 'train' and 'validation' set. This is the `DataLoader`constructor signature:\n```\nDataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n           batch_sampler=None, num_workers=0, collate_fn=None,\n           pin_memory=False, drop_last=False, timeout=0,\n           worker_init_fn=None, *, prefetch_factor=2,\n           persistent_workers=False)\n```\nThere are two possible dataset types: interable-style and map-style. Iterable-style datasets are particularly suitable for cases where random reads are either improbable, impossible, or expensive. Since this training data is not being streamed in we do not need this type. Instead we will use a map-style dataset which implements __getitem__() and __len__().\n\nWe set the \"dataset\" in the DataLoader to a TensorDataset object which wraps a dataset with tensors. The TensorDataset class takes tensors that have the same height but nesiccarily the same width. We import two FloatTensors into the TensorDataset. One which contains the features of the (training or validation) set and the other which contains the targets.\n\nBecause we are using a map-style dataset we can control how the data in our DataLoader is loaded. Batch size is a hyperparameter of gradient descent that controls the number of training samples to work through before the model's internal parameters are updated. Here we set this to 256, but it can always be optimized.\n\nEvery time we use the dataloader we want to reshuffle the training data to prevent overfitting (shuffle=True). However, we do not need to shuffle the validation data (shuffle=False)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create the dataloader\ncolumn_drops = [\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\"]\ndataloader_map = {\n    label:data.DataLoader(\n        data.TensorDataset(\n            torch.FloatTensor(data_map[label][0].drop(column_drops, axis=1).values), \n            torch.FloatTensor(data_map[label][1].drop(\"sig_id\", axis=1).values)\n        ), \n        batch_size=256,\n        shuffle=(label == 'train')\n    )\n    for label in data_map\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can finally define the FeedForward network model using the FeedForward_Nerwork class we made above. We set the height of the training features floatTensor and the height of the target values floatTensor to initialize our network."},{"metadata":{},"cell_type":"markdown","source":"For this competition submissions are scored by the log loss:\n\n$$ \\text{score} = - \\frac{1}{M}\\sum_{m=1}^{M} \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_{i,m} \\log(\\hat{y}_{i,m}) + (1 - y_{i,m}) \\log(1 - \\hat{y}_{i,m})\\right] $$\n\nwhere:\n\n* `N` is the number of sig_id observations in the test data (\\(i=1,…,N\\))\n* `M` is the number of scored MoA targets (\\(m=1,…,M\\))\n* $\\hat{y}_{i,m}$ is the predicted probability of a positive MoA response for a sig_id\n* $y_{i,m}$ is the ground truth, 1 for a positive response, 0 otherwise\n* $log()$ is the natural (base e) logarithm"},{"metadata":{},"cell_type":"markdown","source":"Since our data tends to be roughly normally distributed we set our loss function to Poisson negative log loss. "},{"metadata":{"trusted":true},"cell_type":"code","source":"model = FeedForward_Network(\n    input_size=dataloader_map['train'].dataset[0][0].shape[0],\n    output_size=dataloader_map['train'].dataset[0][1].shape[0]\n)\nloss_fn = nn.PoissonNLLLoss()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We set a few more hyper-parameters. \n* The number of epochs is a hyperparameter of gradient descent that controls the number of complete passes through the training dataset. This can be optimized. \n* lr is the learning rate, the rate at which model weights are adjusted. If this number is too low, the model will take forever to run, but if it is too high, the model may overshoot.\n\nWe can try multiple values for these hyper-parameters by adding them to the list. We put these hyper-parameters into a grid so we can test each of the epoch values with each of the learning-rate values."},{"metadata":{"trusted":true},"cell_type":"code","source":"#epochs = [50,100, 200]\n#lr = [1e-3, 1e-5, 1e-7]\nepochs = [50]\nlr = [1e-3]\ngrid = itertools.product(epochs, lr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To optimize the model using each of the epoch values with each of the learning-rate values we perform the code below."},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch_grid, lr_param in grid:\n    optimizer = opt.Adam(model.parameters(), lr=lr_param)\n    \n    # Run the epochs\n    for epoch in range(epoch_grid+1):\n        \n        if epoch > 0:\n            print(epoch)\n            train_loss = []\n            \n            for batch in dataloader_map['train']:\n                # reset optimizer\n                optimizer.zero_grad()\n                \n                # pass data through the model\n                prediction = model(batch[0])\n                loss = loss_fn(prediction, batch[1])\n                \n                # Track the model loss\n                train_loss.append(loss.item())\n                loss.backward()\n                optimizer.step()\n\n            print(f\"train loss: {np.mean(train_loss)}\")\n                \n                \n        # Set model to evaluation\n        model.eval()\n        val_loss = []\n        \n        for batch in dataloader_map['validation']:\n            # Pass data through model\n            prediction = model(batch[0])\n            \n            # Get the loss\n            loss = loss_fn(prediction, batch[1])\n            \n            # Track the loss\n            val_loss.append(loss.item())\n        \n        print(f\"val loss: {np.mean(val_loss)}\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once the model has finished predicting the target values we can submit the predictions by importing them into the \"model_submission.csv\" file."},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_cols = (\n    train_labels_df\n    .drop(\"sig_id\", axis=1)\n    .columns\n)\n\n(\n    pd.DataFrame(\n        model(\n            torch.FloatTensor(\n                test_df\n                .drop(column_drops, axis=1)\n                .values\n            )\n        )\n        .detach()\n        .numpy(),\n        columns=feat_cols\n    )\n    .assign(sig_id=test_df.sig_id.tolist())\n    .to_csv(\"submission.csv\", index=False)\n)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}