{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Description\n* This notebook started as an identical copy of @Vitalii Mokin's great notebook https://www.kaggle.com/vbmokin/moa-pytorch-rankgauss-pca-nn-upgrade-3d-visual/notebook\n* In order to keep the notebook short all the functions are imported from moalib dataset"},{"metadata":{},"cell_type":"markdown","source":"# Upgrade \n* Commit 9: Added features generated by cnn from https://www.kaggle.com/martintosstorff/moa-dimensionalityreduction-for-applying-cnn\n* Commit 11: Removed features generated by cnn as they lead to overfit"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        continue\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/moalib')\nsys.path.append('../input/iterativestratification')\n\nfrom rankgauss import quantile_transform_dataframe\nfrom pca import add_PC_to_dataframe\nfrom featureselection import select_features\nfrom cvfolds import get_folds\nfrom main import run_training\nfrom main import run_predicition\nfrom preprocess import process_data\nfrom models import Model\nfrom seed import seed_everything\nfrom transferlearning import FineTuneScheduler\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import log_loss\nfrom sklearn.feature_selection import VarianceThreshold\nimport torch.nn.functional as F\n#from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n#import_modules()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataloading"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ncontnames = train_features.columns[4:]\n\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Quantile regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"quantile_transform_dataframe(train_features, test_features, GENES + CELLS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_comp_GENES = 463\nn_comp_CELLS = 60\ntrain_features, test_features = add_PC_to_dataframe(train_features, test_features, n_comp_GENES, n_comp_CELLS, GENES, CELLS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"VarianceThreshold_for_FS = 0.9\ntrain, test, target, target_cols, slected_feats = select_features(train_features, test_features, train_targets_scored, VarianceThreshold_for_FS)\n#train, test, target, target_cols = select_original_or_PCA_features(train_features, test_features, train_targets_scored, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#target_cols = [x for x in train_targets_scored.columns if x != 'sig_id']\naux_target_cols = [x for x in train_targets_nonscored.columns if x != 'sig_id']\nall_target_cols = target_cols + aux_target_cols\n\nnum_targets = len(target_cols)\nnum_aux_targets = len(aux_target_cols)\nnum_all_targets = len(all_target_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.merge(train_targets_nonscored, on='sig_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[print(f) for f in train_features.columns[4:][slected_feats]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfolds = get_folds(train, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(folds.shape)\nprint(test.shape)\nprint(target.shape)\nprint(sample_submission.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = [c for c in process_data(folds).columns if c not in all_target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size, Dropout_Model):\n        super(Model, self).__init__()\n        self.hidden_size = [hidden_size]\n        self.dropout_value = [Dropout_Model]\n        \n        self.batch_norm_0 = nn.BatchNorm1d(num_features)\n        self.dense_0 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n\n        self.batch_norm_1 = nn.BatchNorm1d(hidden_size)\n        self.dropout_1 = nn.Dropout(Dropout_Model)\n        self.dense_1 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n\n        self.batch_norm_last = nn.BatchNorm1d(hidden_size)\n        self.dropout_last = nn.Dropout(Dropout_Model)\n        self.dense_last = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n        \n    def recalibrate_layer(self, layer):\n\n        if(torch.isnan(layer.weight_v).sum() > 0):\n            print ('recalibrate layer.weight_v')\n            layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n            layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n\n        if(torch.isnan(layer.weight).sum() > 0):\n            print ('recalibrate layer.weight')\n            layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n            layer.weight += 1e-7\n\n    def forward(self, x):\n        x = self.batch_norm_0(x)\n        self.recalibrate_layer(self.dense_0)\n        x = F.leaky_relu(self.dense_0(x))        \n\n        x = self.batch_norm_1(x)\n        x = self.dropout_1(x)\n        self.recalibrate_layer(self.dense_1)\n        x = F.leaky_relu(self.dense_1(x))\n\n        x = self.batch_norm_last(x)\n        x = self.dropout_last(x)\n        self.recalibrate_layer(self.dense_last)\n        x = self.dense_last(x)\n\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"from seed import seed_everything\nfrom preprocess import process_data\nfrom datasets import MoADataset\nfrom datasets import TestDataset\nfrom labelsmoothing import SmoothBCEwLogits\nfrom training import train_fn\nfrom training import valid_fn\nfrom training import inference_fn\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\n\ndef run_training(fold, seed, folds, model, feature_cols, target_cols, target, BATCH_SIZE, DEVICE, LEARNING_RATE, WEIGHT_DECAY, EPOCHS, EARLY_STOPPING_STEPS, EARLY_STOP):\n    seed_everything(seed)\n\n    train = process_data(folds)\n\n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n\n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n\n    x_train, y_train = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid = valid_df[feature_cols].values, valid_df[target_cols].values\n\n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n    model.to(DEVICE)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3,\n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n\n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing=0.001)\n\n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n\n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n\n    for epoch in range(EPOCHS):\n\n        train_loss = train_fn(model, optimizer, scheduler, loss_tr, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n\n        if valid_loss < best_loss:\n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_{seed}.pth\")\n\n        elif (EARLY_STOP == True):\n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n\n    return oof\n\n\n\ndef run_predicition(fold, model, test, feature_cols, DEVICE, BATCH_SIZE):\n    test_ = process_data(test)\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n\n    model.load_state_dict(torch.load(f\"FOLD{fold}_{seed}.pth\"))\n    model.to(DEVICE)\n\n    predictions = inference_fn(model, testloader, DEVICE)\n\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(fold, seed):\n    seed_everything(seed)\n    \n    train = process_data(folds)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n\n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)   \n    \n    def train_model(model, tag_name, target_cols_now, fine_tune_scheduler=None):       \n        x_train, y_train  = train_df[feature_cols].values, train_df[target_cols_now].values\n        x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols_now].values\n        \n        train_dataset = MoADataset(x_train, y_train)\n        valid_dataset = MoADataset(x_valid, y_valid)\n\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n        \n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY[tag_name])\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n                                                  steps_per_epoch=len(trainloader),\n                                                  pct_start=PCT_START,\n                                                  div_factor=DIV_FACTOR[tag_name], \n                                                  max_lr=MAX_LR[tag_name],\n                                                  epochs=EPOCHS)\n        \n        loss_fn = nn.BCEWithLogitsLoss()\n        loss_tr = SmoothBCEwLogits(smoothing=0.001)\n\n        oof = np.zeros((len(train), len(target_cols_now)))\n        best_loss = np.inf\n        \n        for epoch in range(EPOCHS):\n            if fine_tune_scheduler is not None:\n                fine_tune_scheduler.step(epoch, model)\n\n            train_loss = train_fn(model, optimizer, scheduler, loss_tr, trainloader, DEVICE)\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n            print(f\"SEED: {seed}, FOLD: {fold}, {tag_name}, EPOCH: {epoch}, train_loss: {train_loss:.6f}, valid_loss: {valid_loss:.6f}\")\n\n            if np.isnan(valid_loss):\n                break\n            \n            if valid_loss < best_loss:\n                best_loss = valid_loss\n                oof[val_idx] = valid_preds\n                torch.save(model.state_dict(), f\"{tag_name}_FOLD{fold}_{seed}.pth\")\n\n        return oof\n\n    fine_tune_scheduler = FineTuneScheduler(EPOCHS)\n\n    pretrained_model = Model(\n        num_features=num_features,        \n        num_targets=num_all_targets,\n        hidden_size=hidden_size,\n        Dropout_Model=Dropout_Model)  \n    #pretrained_model = Model(num_features, num_all_targets)\n    pretrained_model.to(DEVICE)\n\n    # Train on scored + nonscored targets\n    train_model(pretrained_model, 'ALL_TARGETS', all_target_cols)\n\n    # Load the pretrained model with the best loss\n    pretrained_model =Model(\n        num_features=num_features,        \n        num_targets=num_all_targets,\n        hidden_size=hidden_size,\n        Dropout_Model=Dropout_Model)  \n    pretrained_model.load_state_dict(torch.load(f\"ALL_TARGETS_FOLD{fold}_{seed}.pth\"))\n    pretrained_model.to(DEVICE)\n\n    model_new = Model(\n        num_features=num_features,        \n        num_targets=num_all_targets,\n        hidden_size=hidden_size,\n        Dropout_Model=Dropout_Model)  \n    model_new.load_state_dict(pretrained_model.state_dict())\n    # Copy model without the top layer\n    final_model = fine_tune_scheduler.copy_without_top(pretrained_model, model_new, num_targets, DEVICE)\n\n    # Fine-tune the model on scored targets only\n    oof = train_model(final_model, 'SCORED_ONLY', target_cols, fine_tune_scheduler)\n    \n    return oof   \n\n\ndef run_predicition(fold, model, test, feature_cols, DEVICE, BATCH_SIZE):\n    test_ = process_data(test)\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n\n    model.load_state_dict(torch.load(f\"SCORED_ONLY_FOLD{fold}_{seed}.pth\"))\n    model.to(DEVICE)\n\n    predictions = inference_fn(model, testloader, DEVICE)\n\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 7\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\nDropout_Model = 0.25\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size = 1550","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 28\nBATCH_SIZE = 128\n\nWEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 3e-6}\nMAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 3e-3}\nDIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2}\nPCT_START = 0.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os.path\ndef has_model(fold): return os.path.isfile(f\"FOLD{fold}_.pth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n   \n    for fold in range(NFOLDS):\n        gc.collect()\n        #model = Model(\n        #num_features=num_features,        \n        #num_targets=num_targets,\n        #hidden_size=hidden_size,\n        #Dropout_Model=Dropout_Model)     \n                      \n        \n        #oof_ = run_training(fold, seed, folds, model, feature_cols, \n        #                    target_cols, target, BATCH_SIZE, DEVICE, LEARNING_RATE,\n        #                    WEIGHT_DECAY, EPOCHS, EARLY_STOPPING_STEPS, EARLY_STOP)\n        \n        oof_ = run_training(fold, seed)\n        \n        modelPred = Model(\n        num_features=num_features,        \n        num_targets=num_targets,\n        hidden_size=hidden_size,\n        Dropout_Model=Dropout_Model)       \n        \n        pred_ = run_predicition(fold, modelPred, test, feature_cols, DEVICE, BATCH_SIZE)\n        \n        predictions += pred_ / NFOLDS\n        oof += oof_\n        \n    return oof, predictions\n# Averaging on multiple SEEDS\n\nSEED = [0,1,2,3,4,5,6]\n\n\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\nfor seed in SEED:  \n    #folds = get_folds(train, target)\n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)  \n    #del folds\n                \n                \n\ntrain[target_cols] = oof\ntest[target_cols] = predictions\n        \nvalid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ / target.shape[1]\n            \n\n            \n#print(\"num_layers: \", num_layers)\nprint(\"hidden_size: \", hidden_size)\nprint(\"CV log_loss: \", score)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train[target_cols] = oof\n#test[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ / target.shape[1]\n    \nprint(\"CV log_loss: \", score)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}