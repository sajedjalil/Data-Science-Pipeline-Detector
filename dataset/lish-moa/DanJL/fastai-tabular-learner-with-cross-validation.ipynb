{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\nIn [this notebook](https://www.kaggle.com/dliend/fastai-tabular-learner-moa-challenge), we used a basic fastai `TabularLearner` to generate predictions for this challenge. We did not attempt substantial hyperparameter tuning (aside from tweaking the learning rate).\n\nIn [subsequent approaches](https://www.kaggle.com/dliend/testing-moa-based-on-grid-search-results), we used grid search and a lot of manual hyperparameter tuning and found that wide, two-layer networks with a fair amount of regularization and small batch sizes performed better than alternatives.\n\nIn this notebook, we attempt to make greater use of the training data available to us by using cross validation. We will be following examples set up on other notebooks, which will be referenced here.\n\nIn particular, we start by following the cross validation example implemented [here](https://www.kaggle.com/robertlangdonvinci/lish-moa-kfold-fastai-tabnet-ensemble)."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define MultilabelStratifiedKFold Class\nSOURCE: https://www.kaggle.com/robertlangdonvinci/lish-moa-kfold-fastai-tabnet-ensemble/data?select=ml_stratifiers.py  (should track down original source)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection._split import _BaseKFold, _RepeatedSplits, \\\n    BaseShuffleSplit, _validate_shuffle_split\nfrom sklearn.utils.validation import _num_samples, check_array\nfrom sklearn.utils.multiclass import type_of_target\nfrom sklearn.utils import check_random_state\n\n\n\ndef IterativeStratification(labels, r, random_state):\n    \"\"\"This function implements the Iterative Stratification algorithm described\n    in the following paper:\n    Sechidis K., Tsoumakas G., Vlahavas I. (2011) On the Stratification of\n    Multi-Label Data. In: Gunopulos D., Hofmann T., Malerba D., Vazirgiannis M.\n    (eds) Machine Learning and Knowledge Discovery in Databases. ECML PKDD\n    2011. Lecture Notes in Computer Science, vol 6913. Springer, Berlin,\n    Heidelberg.\n    \"\"\"\n\n    n_samples = labels.shape[0]\n    test_folds = np.zeros(n_samples, dtype=int)\n\n    # Calculate the desired number of examples at each subset\n    c_folds = r * n_samples\n\n    # Calculate the desired number of examples of each label at each subset\n    c_folds_labels = np.outer(r, labels.sum(axis=0))\n\n    labels_not_processed_mask = np.ones(n_samples, dtype=bool)\n\n    while np.any(labels_not_processed_mask):\n        # Find the label with the fewest (but at least one) remaining examples,\n        # breaking ties randomly\n        num_labels = labels[labels_not_processed_mask].sum(axis=0)\n\n        # Handle case where only all-zero labels are left by distributing\n        # across all folds as evenly as possible (not in original algorithm but\n        # mentioned in the text). (By handling this case separately, some\n        # code redundancy is introduced; however, this approach allows for\n        # decreased execution time when there are a relatively large number\n        # of all-zero labels.)\n        if num_labels.sum() == 0:\n            sample_idxs = np.where(labels_not_processed_mask)[0]\n\n            for sample_idx in sample_idxs:\n                fold_idx = np.where(c_folds == c_folds.max())[0]\n\n                if fold_idx.shape[0] > 1:\n                    fold_idx = fold_idx[random_state.choice(fold_idx.shape[0])]\n\n                test_folds[sample_idx] = fold_idx\n                c_folds[fold_idx] -= 1\n\n            break\n\n        label_idx = np.where(num_labels == num_labels[np.nonzero(num_labels)].min())[0]\n        if label_idx.shape[0] > 1:\n            label_idx = label_idx[random_state.choice(label_idx.shape[0])]\n\n        sample_idxs = np.where(np.logical_and(labels[:, label_idx].flatten(), labels_not_processed_mask))[0]\n\n        for sample_idx in sample_idxs:\n            # Find the subset(s) with the largest number of desired examples\n            # for this label, breaking ties by considering the largest number\n            # of desired examples, breaking further ties randomly\n            label_folds = c_folds_labels[:, label_idx]\n            fold_idx = np.where(label_folds == label_folds.max())[0]\n\n            if fold_idx.shape[0] > 1:\n                temp_fold_idx = np.where(c_folds[fold_idx] ==\n                                         c_folds[fold_idx].max())[0]\n                fold_idx = fold_idx[temp_fold_idx]\n\n                if temp_fold_idx.shape[0] > 1:\n                    fold_idx = fold_idx[random_state.choice(temp_fold_idx.shape[0])]\n\n            test_folds[sample_idx] = fold_idx\n            labels_not_processed_mask[sample_idx] = False\n\n            # Update desired number of examples\n            c_folds_labels[fold_idx, labels[sample_idx]] -= 1\n            c_folds[fold_idx] -= 1\n\n    return test_folds\n\nclass MultilabelStratifiedKFold(_BaseKFold):\n    \"\"\"Multilabel stratified K-Folds cross-validator\n    Provides train/test indices to split multilabel data into train/test sets.\n    This cross-validation object is a variation of KFold that returns\n    stratified folds for multilabel data. The folds are made by preserving\n    the percentage of samples for each label.\n    Parameters\n    ----------\n    n_splits : int, default=3\n        Number of folds. Must be at least 2.\n    shuffle : boolean, optional\n        Whether to shuffle each stratification of the data before splitting\n        into batches.\n    random_state : int, RandomState instance or None, optional, default=None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Unlike StratifiedKFold that only uses random_state\n        when ``shuffle`` == True, this multilabel implementation\n        always uses the random_state since the iterative stratification\n        algorithm breaks ties randomly.\n    Examples\n    --------\n    >>> from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n    >>> import numpy as np\n    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n    >>> mskf = MultilabelStratifiedKFold(n_splits=2, random_state=0)\n    >>> mskf.get_n_splits(X, y)\n    2\n    >>> print(mskf)  # doctest: +NORMALIZE_WHITESPACE\n    MultilabelStratifiedKFold(n_splits=2, random_state=0, shuffle=False)\n    >>> for train_index, test_index in mskf.split(X, y):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [0 3 4 6] TEST: [1 2 5 7]\n    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n    Notes\n    -----\n    Train and test sizes may be slightly different in each fold.\n    See also\n    --------\n    RepeatedMultilabelStratifiedKFold: Repeats Multilabel Stratified K-Fold\n    n times.\n    \"\"\"\n\n    def __init__(self, n_splits=3, shuffle=False, random_state=None):\n        super(MultilabelStratifiedKFold, self).__init__(n_splits, shuffle, random_state)\n\n    def _make_test_folds(self, X, y):\n        y = np.asarray(y, dtype=bool)\n        type_of_target_y = type_of_target(y)\n\n        if type_of_target_y != 'multilabel-indicator':\n            raise ValueError(\n                'Supported target type is: multilabel-indicator. Got {!r} instead.'.format(type_of_target_y))\n\n        num_samples = y.shape[0]\n\n        rng = check_random_state(self.random_state)\n        indices = np.arange(num_samples)\n\n        if self.shuffle:\n            rng.shuffle(indices)\n            y = y[indices]\n\n        r = np.asarray([1 / self.n_splits] * self.n_splits)\n\n        test_folds = IterativeStratification(labels=y, r=r, random_state=rng)\n\n        return test_folds[np.argsort(indices)]\n\n    def _iter_test_masks(self, X=None, y=None, groups=None):\n        test_folds = self._make_test_folds(X, y)\n        for i in range(self.n_splits):\n            yield test_folds == i\n\n    def split(self, X, y, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n            Note that providing ``y`` is sufficient to generate the splits and\n            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n            ``X`` instead of actual training data.\n        y : array-like, shape (n_samples, n_labels)\n            The target variable for supervised learning problems.\n            Multilabel stratification is done based on the y labels.\n        groups : object\n            Always ignored, exists for compatibility.\n        Returns\n        -------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        \"\"\"\n        y = check_array(y, ensure_2d=False, dtype=None)\n        return super(MultilabelStratifiedKFold, self).split(X, y, groups)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.tabular.all import *\npath = Path('../input/lish-moa')\ntrain_features = pd.read_csv(path/'train_features.csv')\ntest_features = pd.read_csv(path/'test_features.csv')\ntrain_targets = pd.read_csv(path/'train_targets_scored.csv')\ntrain_drugs = pd.read_csv(path/'train_drug.csv')\nsample_submission = pd.read_csv(path/'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the documentation:\n\n>Features for the training set. Features `g-` signify gene expression data, and `c-` signify cell viability data. `cp_type` indicates samples treated with a compound (`cp_vehicle`) or with a control perturbation (`ctrl_vehicle`); control perturbations have no MoAs; `cp_time` and `cp_dose` indicate treatment duration (24, 48, 72 hours) and dose (high or low)."},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = ['cp_type', 'cp_time', 'cp_dose']\ncontinuous = [i for i in train_features.columns if i not in ['cp_type', 'cp_time', 'cp_dose', 'sig_id']]\ndep_var = [i for i in train_targets.columns if i != 'sig_id']\ntrain_features[dep_var] = train_targets[dep_var]\ntrain_features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Set up Folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train_features.sample(frac=1.,random_state=41)\ndf['kfold'] = -1\nkf = MultilabelStratifiedKFold(n_splits=10)\ny = df[dep_var].values\nfor fold, (t_,v_) in enumerate(kf.split(X=df,y=y)):\n    df.loc[v_,'kfold'] = fold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"procs = [FillMissing, Categorify, Normalize]\ndef get_data(fold):\n    val_idx = df[df.kfold==fold].index\n    dls = TabularDataLoaders.from_df(df, path=path, y_names=dep_var,\n                                        cat_names = categorical,\n                                        cont_names = continuous,\n                                        procs = procs,\n                                        valid_idx=val_idx,\n                                        bs=64)\n    return dls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fit Model\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache() \ntest_sc = []\nbest_valid = []\n\nfor i in range(10):\n    \n    dls = get_data(i) # Data\n    \n    learn = tabular_learner(dls, layers=[15000, 1600], loss_func = BCEWithLogitsLossFlat(),\n                            model_dir='/kaggle/working/', wd = 4) # Model\n    \n    name = 'best_model_' + str(i) \n    cb = SaveModelCallback(monitor='valid_loss',fname=name ,mode='min') # Callbacks\n    \n    lr = .003\n    learn.fit_one_cycle(19, lr,cbs=cb) # Training\n    \n    learn.load(name) # Load best model\n    losses = np.array(learn.recorder.values)\n    best = np.argmin(losses[:,1])\n    best_valid.append(losses[best,1])\n    \n    test_dl = learn.dls.test_dl(test_features)\n    sub = learn.get_preds(dl=test_dl) # prediction\n    test_sc.append(sub[0].numpy())\n    \n    learn.export('/kaggle/working/'+name+'.pkl') # export model\n    \ntest_sc = np.array(test_sc)\nbest_valid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.argmin(best_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get Predictions for Submission\nWe follow the guide to setting up a test set here: https://forums.fast.ai/t/a-brief-guide-to-test-sets-in-v2-you-can-do-labelled-now-too/57054"},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_prds = test_sc[np.argmin(best_valid)]\nsubmission = sample_submission.copy()\nsubmission[dep_var] = avg_prds\nsubmission.loc[test_features['cp_type']=='ctl_vehicle', dep_var] = 0\nsubmission['atp-sensitive_potassium_channel_antagonist'] = 0 # only appears once\nsubmission['erbb2_inhibitor'] = 0 # only appears once","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}