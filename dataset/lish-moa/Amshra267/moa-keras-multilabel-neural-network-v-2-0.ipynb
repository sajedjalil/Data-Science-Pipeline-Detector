{"cells":[{"metadata":{},"cell_type":"markdown","source":"This kernel is an improvement of that [kernel](https://www.kaggle.com/simakov/keras-multilabel-neural-network-v1-2)\n\n**Changes I done**\n\n1) Improved The Architecture (Neurons, Dropout, Early Stopping)\n\n2) Added Elu as an activation function\n\n3) Learning rate adjusted, k folds and seed averaging to 9 with 100 epochs\n\n4) Using AdamW as an optimizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nimport tensorflow_addons as tfa\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\n\nss = pd.read_csv('../input/lish-moa/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(df):\n    df = df.copy()\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del df['sig_id']\n    return df\n\ntrain = preprocess(train_features)\ntest = preprocess(test_features)\n\ndel train_targets['sig_id']\n\ntrain_targets = train_targets.loc[train['cp_type']==0].reset_index(drop=True)\ntrain = train.loc[train['cp_type']==0].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MODEL"},{"metadata":{"trusted":true},"cell_type":"code","source":"def nn_model(num_columns):\n   \n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(num_columns),\n\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.2),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1048, activation='elu')),\n\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.3),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1048, activation='elu')),\n\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.35),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(510, activation='elu')),\n\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.4),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(510, activation='elu')),\n        \n     tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.4),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(510, activation='elu')),\n    \n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(256, activation='elu')),\n\n    #============ Final Layer =================\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.4),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, activation=\"sigmoid\"))\n    ])\n    model.compile(optimizer=tfa.optimizers.AdamW(lr = 1e-3, weight_decay = 1e-5, clipvalue = 700), \n                  loss='binary_crossentropy', \n                  )\n    return model    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnum_features = ['g-0','g-2','g-3','g-5','g-6','g-7','g-8'\n,'g-9','g-11','g-12','g-13','g-15','g-16','g-17','g-18','g-20','g-21'\n,'g-22','g-24','g-25','g-26','g-27','g-28','g-29','g-30','g-31','g-32'\n,'g-33','g-34','g-36','g-37','g-38','g-39','g-41','g-42','g-43','g-45'\n,'g-47','g-48','g-49','g-50','g-51','g-52','g-53','g-54','g-55','g-56'\n,'g-57','g-58','g-60','g-61','g-62','g-63','g-65','g-66','g-67','g-68'\n,'g-69','g-70','g-71','g-72','g-73','g-75','g-76','g-77','g-78','g-79'\n,'g-80','g-81','g-83','g-84','g-85','g-86','g-87','g-89','g-90','g-91'\n,'g-92','g-93','g-94','g-96','g-97','g-98','g-100','g-101','g-102','g-103'\n,'g-104','g-105','g-106','g-107','g-108','g-109','g-110','g-111','g-112'\n,'g-113','g-114','g-115','g-116','g-117','g-118','g-119','g-120','g-121'\n,'g-122','g-123','g-124','g-125','g-126','g-127','g-129','g-130','g-131'\n,'g-132','g-133','g-134','g-135','g-136','g-137','g-138','g-139','g-140'\n,'g-141','g-142','g-143','g-144','g-146','g-147','g-148','g-149','g-150'\n,'g-151','g-152','g-154','g-156','g-157','g-158','g-160','g-161','g-162'\n,'g-163','g-164','g-165','g-166','g-167','g-169','g-170','g-172','g-173'\n,'g-174','g-175','g-177','g-178','g-179','g-180','g-181','g-183','g-184'\n,'g-185','g-186','g-187','g-188','g-189','g-190','g-192','g-194','g-195'\n,'g-196','g-199','g-200','g-202','g-203','g-205','g-206','g-207','g-208'\n,'g-209','g-210','g-211','g-212','g-215','g-216','g-217','g-218','g-219'\n,'g-221','g-222','g-224','g-225','g-226','g-227','g-228','g-229','g-230'\n,'g-231','g-233','g-235','g-236','g-237','g-238','g-239','g-240','g-241'\n,'g-242','g-243','g-245','g-246','g-247','g-248','g-250','g-251','g-252'\n,'g-253','g-254','g-255','g-256','g-257','g-258','g-260','g-262','g-263'\n,'g-265','g-267','g-268','g-269','g-270','g-272','g-273','g-274','g-276'\n,'g-279','g-280','g-283','g-284','g-285','g-286','g-287','g-291','g-292'\n,'g-293','g-294','g-296','g-297','g-298','g-299','g-300','g-301','g-302'\n,'g-303','g-305','g-306','g-307','g-308','g-309','g-310','g-312','g-313'\n,'g-314','g-317','g-318','g-319','g-321','g-322','g-323','g-324','g-325'\n,'g-326','g-327','g-328','g-329','g-330','g-331','g-332','g-335','g-336'\n,'g-337','g-338','g-340','g-341','g-342','g-343','g-344','g-346','g-347'\n,'g-348','g-349','g-350','g-352','g-353','g-354','g-355','g-356','g-357'\n,'g-358','g-359','g-360','g-361','g-362','g-363','g-365','g-366','g-367'\n,'g-368','g-369','g-371','g-372','g-373','g-374','g-375','g-376','g-377'\n,'g-379','g-380','g-381','g-382','g-383','g-384','g-385','g-386','g-387'\n,'g-388','g-389','g-390','g-391','g-392','g-394','g-395','g-396','g-397'\n,'g-398','g-400','g-402','g-403','g-404','g-405','g-407','g-408','g-409'\n,'g-410','g-411','g-412','g-414','g-415','g-416','g-417','g-418','g-419'\n,'g-420','g-421','g-422','g-423','g-424','g-425','g-426','g-427','g-428'\n,'g-429','g-430','g-431','g-432','g-433','g-434','g-435','g-438','g-439'\n,'g-440','g-441','g-442','g-443','g-444','g-445','g-446','g-447','g-449'\n,'g-450','g-451','g-453','g-454','g-455','g-456','g-457','g-458','g-459'\n,'g-460','g-461','g-462','g-463','g-465','g-466','g-468','g-469','g-470'\n,'g-471','g-472','g-473','g-474','g-475','g-476','g-479','g-480','g-482'\n,'g-483','g-484','g-485','g-486','g-488','g-489','g-491','g-492','g-493'\n,'g-497','g-498','g-499','g-500','g-502','g-503','g-504','g-506','g-507'\n,'g-508','g-509','g-510','g-511','g-513','g-514','g-515','g-516','g-518'\n,'g-520','g-522','g-523','g-524','g-525','g-526','g-527','g-528','g-529'\n,'g-530','g-531','g-533','g-534','g-535','g-536','g-537','g-538','g-539'\n,'g-540','g-541','g-542','g-543','g-544','g-546','g-547','g-550','g-551'\n,'g-552','g-553','g-554','g-555','g-556','g-557','g-558','g-559','g-560'\n,'g-561','g-562','g-563','g-564','g-566','g-567','g-568','g-569','g-570'\n,'g-571','g-572','g-574','g-577','g-578','g-579','g-580','g-583','g-584'\n,'g-587','g-588','g-589','g-590','g-592','g-593','g-594','g-595','g-596'\n,'g-597','g-598','g-599','g-600','g-602','g-604','g-605','g-606','g-608'\n,'g-609','g-610','g-611','g-612','g-613','g-614','g-616','g-619','g-620'\n,'g-622','g-624','g-627','g-628','g-629','g-630','g-631','g-632','g-634'\n,'g-635','g-636','g-639','g-640','g-641','g-642','g-643','g-644','g-646'\n,'g-647','g-648','g-649','g-651','g-652','g-655','g-656','g-657','g-658'\n,'g-659','g-660','g-661','g-663','g-664','g-665','g-666','g-667','g-669'\n,'g-671','g-672','g-673','g-674','g-675','g-677','g-678','g-679','g-681'\n,'g-682','g-683','g-684','g-685','g-686','g-688','g-689','g-691','g-692'\n,'g-693','g-694','g-696','g-697','g-698','g-699','g-700','g-701','g-702'\n,'g-704','g-705','g-706','g-708','g-709','g-710','g-711','g-712','g-713'\n,'g-714','g-720','g-722','g-724','g-725','g-726','g-727','g-728','g-729'\n,'g-731','g-733','g-734','g-735','g-736','g-737','g-738','g-739','g-740'\n,'g-741','g-742','g-743','g-744','g-745','g-746','g-747','g-748','g-749'\n,'g-750','g-751','g-752','g-753','g-755','g-756','g-757','g-758','g-759'\n,'g-760','g-761','g-762','g-763','g-764','g-766','g-767','g-768','g-769'\n,'g-771','c-0','c-5','c-6','c-7','c-8','c-9','c-10','c-12','c-13','c-15'\n,'c-18','c-20','c-22','c-24','c-25','c-26','c-30','c-33','c-34','c-36'\n,'c-37','c-38','c-41','c-44','c-45','c-46','c-47','c-48','c-50','c-51'\n,'c-52','c-54','c-56','c-57','c-58','c-59','c-60','c-62','c-63','c-64'\n,'c-65','c-66','c-67','c-69','c-70','c-71','c-72','c-73','c-75','c-76'\n,'c-77','c-79','c-80','c-81','c-83','c-85','c-86','c-87','c-89','c-92'\n,'c-93','c-95','c-96','c-98','c-99']\n\nprint(len(num_features))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def metric(y_true, y_pred):\n    metrics = []\n    for _target in train_targets.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels=[0,1]))\n    return np.mean(metrics)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"N_STARTS = 9\n\ntf.random.set_seed(43)\n\nres = train_targets.copy()\nss.loc[:, train_targets.columns] = 0\nres.loc[:, train_targets.columns] = 0\n\nfor seed in range(N_STARTS):\n    for n, (tr, te) in enumerate(KFold(n_splits=5, random_state=(seed+1)**2, shuffle=True).split(train_targets)):\n        print(f'Fold {n}')\n    \n        model = nn_model(len(num_features))\n        checkpoint_path = f'repeat:{seed}_Fold:{n}.hdf5'\n        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-5, mode='min')\n        cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True,\n                                     save_weights_only = True, mode = 'min')\n        \n       \n        \n        early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience= 5)\n        \n        model.fit(train[num_features].values[tr],\n                  train_targets.values[tr],\n                  validation_data=(train[num_features].values[te], train_targets.values[te]),\n                  epochs=75, batch_size=128,\n                  steps_per_epoch = len(train.values[tr]) // 128 + 1,\n                  validation_steps = len(train.values[te]) // 128 + 1,\n                  callbacks=[reduce_lr_loss, cb_checkpt, early], verbose=2\n                 )\n        \n        model.load_weights(checkpoint_path)\n        test_predict = model.predict(test[num_features].values)\n        val_predict = model.predict(train[num_features].values[te])\n        \n        ss.loc[:, train_targets.columns] += test_predict\n        res.loc[te, train_targets.columns] += val_predict\n        print('')\n    \nss.loc[:, train_targets.columns] /= ((n+1) * N_STARTS)\nres.loc[:, train_targets.columns] /= N_STARTS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'OOF Metric: {metric(train_targets, res)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.loc[test['cp_type']==1, train_targets.columns] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you find this work helpful please **Upvote**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}