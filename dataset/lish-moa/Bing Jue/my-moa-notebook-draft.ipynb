{"cells":[{"metadata":{"_uuid":"5ab0d09d-2ae5-4a0f-9614-0c47d45950ad","_cell_guid":"b324564b-31ae-4dc7-9edb-ad5d38578564","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\nimport random\nfrom sklearn.model_selection import *\nfrom sklearn.preprocessing import *\nfrom sklearn.ensemble import *\nfrom sklearn.metrics import *\nfrom scipy.stats import pearsonr\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d1ed1aa-6ecb-4117-8abe-01abe2613ce8","_cell_guid":"d67b7333-dcac-48cb-8458-76137328983b","trusted":true},"cell_type":"markdown","source":"We first read the csv data."},{"metadata":{"_uuid":"1346aab7-a46c-4f4f-afbe-107a798e9ea0","_cell_guid":"a501a360-1ea0-4923-9586-721facdd00d8","trusted":true},"cell_type":"code","source":"# read csv data\ndf_features = pd.read_csv('/kaggle/input/lish-moa/train_features.csv',index_col='sig_id')\ndf_test_features = pd.read_csv('/kaggle/input/lish-moa/test_features.csv',index_col='sig_id')\ndf_sample_submission = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv', index_col='sig_id')\ndf_targets = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv',index_col='sig_id')\n\ndf_features_targets = pd.concat([df_features, df_targets])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1791e3cf-b6d1-4100-aeef-2d901bd17414","_cell_guid":"64439671-20e8-4458-ba9c-7c67979cefb5","trusted":true},"cell_type":"code","source":"# look at training data\ndf_features.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85b2d80e-9489-4864-bb68-b24c27db9b7f","_cell_guid":"e5a53ce5-9eb7-4308-a811-053d90fc4aa7","trusted":true},"cell_type":"code","source":"# look at test data\ndf_test_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at columns\ndf_features.columns\n\n# 875 columns [cp_type, cp_time, cp_does, g-0 ... g-771, c-0, c-99]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0299ecbc-bc91-45c1-9d04-afacf8340eaf","_cell_guid":"236d30c4-31ad-4ca4-9e86-9296cbec61b6","trusted":true},"cell_type":"code","source":"fig, (cp_type_bar, cp_dose_bar) = plt.subplots(nrows=1, ncols=2, figsize=[12, 6])\n\n# plot frequency of cp_type\ncp_type_training_count = df_features['cp_type'].value_counts()\ncp_type_test_count = df_test_features['cp_type'].value_counts()\ncp_type_label = cp_type_training_count.index\ncp_type_width = 1.0\ncp_type_bar.bar([0, 3], cp_type_training_count, width=cp_type_width)\ncp_type_bar.bar([1, 4], cp_type_test_count, width=cp_type_width)\ncp_type_bar.set_xticks([0.5, 3.5])\ncp_type_bar.set_xticklabels(cp_type_label)\ncp_type_bar.set_title('Frequency of cp_dose in training')\n\n# plot frequency of cp_dose\ncp_dose_training_count = df_features['cp_dose'].value_counts()\ncp_dose_test_count = df_test_features['cp_dose'].value_counts()\ncp_dose_label = cp_dose_training_count.index\ncp_dose_width = 1.0\ncp_dose_bar.bar([0, 3], cp_dose_training_count, width = cp_dose_width)\ncp_dose_bar.bar([1, 4], cp_dose_test_count, width = cp_dose_width)\ncp_dose_bar.set_xticks([0.5, 3.5])\ncp_dose_bar.set_xticklabels(cp_dose_label)\ncp_dose_bar.set_title('Frequency of cp_dose in training')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nrows, ncols = 3, 3\nfig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=[36,24])\nfig.tight_layout(pad=12.0)\ncmap = plt.cm.get_cmap(\"tab10\")\ncolors = cmap.colors\n# plot pdf for 9 random g- features\nfor i in range(nrows):\n    for j in range(ncols):\n        feature = random.randint(0, 771)\n        axis = ax[i][j]\n        axis.hist(df_features[f'g-{feature}'], bins=100, density=True, color=colors[2 * i + j])\n        axis.set_title(f'pdf for g-{feature}', {'fontsize': 32})\n        axis.set_xlabel(\"Numerical value in training set\", {'fontsize': 18})\n        axis.set_ylabel(\"Probability density\", {'fontsize': 18})\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some slightly skewed data for features in g\nnrows, ncols = 2, 2\nfig, ax = plt.subplots(figsize=[24, 18], nrows=nrows, ncols=ncols)\nskewed_g = [['g-744', 'g-123'], ['g-489', 'g-644']]\ncmap = plt.cm.get_cmap(\"Set2\")\ncolors = cmap.colors\nfor i in range(nrows):\n    for j in range(ncols):\n        axis = ax[i][j]\n        axis.hist(df_features[skewed_g[i][j]], bins=100, density=True, color=colors[2 * i + j])\n        axis.set_title(f'pdf for {skewed_g[i][j]}')\n        axis.set_xlabel(\"Numerical value in training set\", {'fontsize': 18})\n        axis.set_ylabel(\"Probability density\", {'fontsize': 18})\nplt.show()\n#g-744,g-123 g-489, g-644g-23, #g-644, g-413, g-307, g-238","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some slightly skewed data for features in g\nnrows, ncols = 2, 2\nfig, ax = plt.subplots(figsize=[24, 18], nrows=nrows, ncols=ncols)\nskewed_g = [['g-23', 'g-413'], ['g-307', 'g-238']]\ncmap = plt.cm.get_cmap(\"Set2\")\ncolors = cmap.colors\nfor i in range(nrows):\n    for j in range(ncols):\n        axis = ax[i][j]\n        axis.hist(df_features[skewed_g[i][j]], bins=100, density=True, color=colors[2 * i + j])\n        axis.set_title(f'pdf for {skewed_g[i][j]}', {'fontsize': 24})\n        axis.set_xlabel(\"Numerical value in training set\", {'fontsize': 16})\n        axis.set_ylabel(\"Probability density\", {'fontsize': 16})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Peer into frequency count for g-307\ng_307_series = df_features.groupby(pd.cut(df_features['g-307'], 100))['g-307'].count()\ng_307_series.sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_307_nlargest_series = g_307_series.nlargest(10)\nfig, ax = plt.subplots(figsize=[18, 10])\nax.bar(range(10), g_307_nlargest_series)\nax.set_xticks(range(10))\nax.set_xticklabels(g_307_nlargest_series.index)\nax.set_title(\"Intervals with highest frequency in training for g-307\", {'fontsize': 18})\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nrows, ncols = 3, 3\nfig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=[36,24])\nfig.tight_layout(pad=12.0)\ncmap = plt.cm.get_cmap(\"tab10\")\ncolors=cmap.colors\n# plot pdf for 9 random g- features\nfor i in range(nrows):\n    for j in range(ncols):\n        feature = random.randint(0, 99)\n        axis = ax[i][j]\n        axis.hist(df_features[f'c-{feature}'], bins=100, density=True, color=colors[i*2+j])\n        axis.set_title(f'pdf for c-{feature}', {'fontsize': 32})\n        axis.set_xlabel(\"Numerical value in training set\", {'fontsize': 18})\n        axis.set_ylabel(\"Probability density\", {'fontsize': 18})\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RobustScalar transforms the feature vector by subtracting the median and then dividing by the interquartile range (25% - 75%)\ndf_copy = df_features.copy(deep=True)\ndf_copy['cp_type'] = df_copy['cp_type'].apply(lambda x: 1 if x == \"ctl_vehicle\" else 0)\ndf_copy['cp_dose'] = df_copy['cp_dose'].apply(lambda x: 1 if x == \"D2\" else 0)\nscaler = RobustScaler()\nX = df_copy.values\nX = scaler.fit_transform(X)\n\ndf_X = pd.DataFrame(X, columns=df_copy.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# New pdf after applying RobustScalar\nfeature = random.randint(3, 771)\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=[12,10])\nax.hist(df_features[f'g-{feature}'], density=True, bins=100)\nax.hist(df_X[f'g-{feature}'],density=True, bins=100, alpha=0.5, color='red')\nax.legend(['Before', 'After'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# New pdf for 9 random features\nnrows, ncols = 3, 3\n\nfig, ax = plt.subplots(nrows=3, ncols=3, figsize=[36,24])\n\nfor i in range(nrows):\n    for j in range(ncols):\n        axis = ax[i][j]\n        feature = random.randint(0, 875)\n        column_name = df_features.columns[feature]\n        axis.hist(df_features.iloc[:, feature], density=True, bins=100)\n        axis.hist(df_X.iloc[:, feature],density=True, bins=100, alpha=0.5, color='red')\n        axis.set_title(f'pdf for {df_features.columns[feature]}', {'fontsize': 30})\n        axis.legend(['Before', 'After'])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Robust Scalar on slightly skewed data\nnrows, ncols = 2, 2\nfig, ax = plt.subplots(figsize=[24, 18], nrows=nrows, ncols=ncols)\nskewed_g = [['g-23', 'g-413'], ['g-307', 'g-238']]\ncmap = plt.cm.get_cmap(\"Set2\")\ncolors = cmap.colors\nfor i in range(nrows):\n    for j in range(ncols):\n        axis = ax[i][j]\n        axis.hist(df_features[skewed_g[i][j]], bins=100, density=True)\n        axis.hist(df_X[skewed_g[i][j]], bins=100, density=True, color='red', alpha=0.5)\n        axis.set_title(f'pdf for {skewed_g[i][j]}', {'fontsize': 24})\n        axis.set_xlabel(\"Numerical value in training set\", {'fontsize': 16})\n        axis.set_ylabel(\"Probability density\", {'fontsize': 16})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# types of target columns\nfor column in df_targets.columns:\n    print(column)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of targets to predict\nnum_targets_dict = dict()\nfor index, row in df_targets.iterrows():\n    num_targets = np.sum(row)\n    num_targets_dict[num_targets] = 1 + num_targets_dict[num_targets] if num_targets in num_targets_dict else 1\n\n\nfig, ax = plt.subplots(figsize=[12,10])\nax.bar(num_targets_dict.keys(), num_targets_dict.values())\nax.set_xlabel(\"Number of targets\", {'fontsize': 14})\nax.set_title(\"Frequency of test instance for x number of targets\", {'fontsize': 18})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_matrix = np.corrcoef(df_copy, df_targets, False)\n# remove instances where feature is correlated to feature, and target is correlated to target\ncorrelation_features = correlation_matrix[:875,875:] # shape 875 x 206\ndf_correlation = pd.DataFrame(correlation_features, index=df_features.columns, columns=df_targets.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot correlation matrix for 10 features and 10 targets\nfeature = random.randint(0, 865)\ntarget = random.randint(0, 196)\n\ncorrelation_features_submatrix = correlation_features[feature:feature + 10, target: target + 10]\nfig, ax = plt.subplots(figsize=[10,10])\nax.imshow(correlation_features_submatrix)\n\n# Loop over data dimensions and create text annotations.\nfor i in range(10):\n    for j in range(10):\n        text = ax.text(j, i, round(correlation_features_submatrix[i, j], 4),\n                       ha=\"center\", va=\"center\", color=\"w\")\n\nax.set_yticks(range(10))\nax.set_yticklabels([df_features.columns[f] for f in np.arange(feature, feature +11)])\nax.set_xticks(range(10))\nax.set_xticklabels([df_targets.columns[t] for t in np.arange(target, target + 11)])\nplt.xticks(rotation=45, ha='right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_correlation.head()\ndf_correlation['hdac_inhibitor'].nlargest(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38a39059-a518-409d-8803-7d833cb9cf61","_cell_guid":"bed8eba3-8985-42b5-8df7-27fe9479b077","trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"# # %% [code]\n# def change_df(df):\n#     df['cp_type'] = df['cp_type'].apply(lambda x: \"1\" if x == \"ctl_vehicle\" else 0)\n#     df['cp_dose'] = df['cp_dose'].apply(lambda x: \"1\" if x == \"D2\" else 0)\n#     return df\n\n# # transform training instances of 'cp_type' and 'cp_dose' to boolean attribute\n# df_features = change_df(df_features)\n# # transform test instances of 'cp_type' and 'cp_does' to boolean attribute\n# df_test_features = change_df(df_test_features)\n\n# # %% [code]\n# df_features\n\n# # %% [code]\n# from sklearn.model_selection import *\n# from sklearn.preprocessing import *\n# from sklearn.ensemble import *\n# from sklearn.metrics import *\n\n# # %% [code]\n# # change from numpy data frame to array\n# X = df_features.values\n# y = df_targets.values\n# X_test = df_test_features.values\n\n# # %% [code]\n# #scaler = MinMaxScaler(feature_range=(1,2))\n\n# # RobutScalar transforms the feature vector by subtracting the median and then dividing by the interquartile range (25% - 75%)\n# scaler = RobustScaler()\n# X = scaler.fit_transform(X)\n# X_test = scaler.transform(X_test)\n\n# # %% [code]\n# # Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree\n# poly = PolynomialFeatures(2)\n# poly.fit(X)\n\n# # %% [code]\n# import keras\n# import tensorflow as tf\n# import tensorflow_addons as tfa\n# from keras.models import *\n# from keras.layers import *\n# from keras.backend import *\n# from keras.callbacks import *\n# from keras.optimizers import *\n\n# # %% [code]\n# batch_size = 512\n# class FeatureGenerator(keras.utils.Sequence):\n#     'Generates data for Keras'\n#     def __init__(self, X, y, batch_size=32, shuffle=True):\n#         'Initialization'\n#         self.X = X\n#         self.y = y\n#         self.batch_size = batch_size\n#         self.shuffle = shuffle\n#         self.on_epoch_end()\n\n#     def __len__(self):\n#         'Denotes the number of batches per epoch'\n#         return int(np.floor(len(self.X) / self.batch_size)) + 1\n\n#     def __getitem__(self, index):\n#         'Generate one batch of data'\n#         # Generate indexes of the batch\n\n#         indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        \n#         X_batch = self.X[indexes]\n#         y_batch = self.y[indexes]\n        \n#         #X_batch = poly.transform(X_batch)\n#         #X_batch = np.concatenate([X_batch, np.log(X_batch), np.exp(X_batch)], axis=1)\n#         y_batch = y_batch.astype('float')\n#         return X_batch, y_batch\n\n#     def on_epoch_end(self):\n#         'Updates indexes after each epoch'\n#         self.indexes = np.arange(len(self.X))\n#         if self.shuffle == True:\n#             np.random.shuffle(self.indexes)\n\n# # %% [code]\n# def get_model():\n#     model_input = Input(shape=(len(train_generator.__getitem__(0)[0][0]), ))\n\n#     #x = GaussianNoise(0.4)(model_input)\n#     \"\"\"x = Dense(256, activation='swish')(model_input)\n#     x = BatchNormalization()(x)\n#     x = Dropout(0.5)(x)\n\n#     x = expand_dims(x, axis=1)\n#     x_1 = Conv1D(256, 1, padding='same', use_bias=False, kernel_initializer='he_normal', activation='swish')(x)\n#     x_2 = Conv1D(256, 1, padding='same', use_bias=False, kernel_initializer='he_normal', activation='swish')(x)\n#     x_att = Multiply()([x_1, x_2])\n#     x_att = Activation('softmax')(x_att)\n\n#     x_g = Conv1D(256, 1, padding='same', use_bias=False, kernel_initializer='he_normal', activation='swish')(x)\n#     x_y = Multiply()([x_g, x_att])\n\n#     x = Add()([x, x_y])\n#     x = squeeze(x, axis=1)\n\n#     \"\"\"\n#     x = Dense(1024, activation='swish')(model_input)\n#     x = BatchNormalization()(x)\n#     x = Dropout(0.5)(x)\n#     x_1 = x\n\n#     x = Dense(512, activation='swish')(x)\n#     x = BatchNormalization()(x)\n#     x = Dropout(0.5)(x)\n#     x_2 = x\n\n#     x = Concatenate()([x, x_1])\n#     x = Dense(256, activation='swish')(x)\n#     x = BatchNormalization()(x)\n#     x = Dropout(0.5)(x)\n\n#     x = Concatenate()([x, x_2])\n#     model_output = Dense(len(y_train[0]), activation='sigmoid')(x)\n#     model = Model(inputs = model_input,outputs = model_output)\n#     model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n#     model.summary()\n#     return model\n\n# \"\"\"\n# model = Sequential()\n# model.add(GaussianNoise(0.01, input_dim=len(train_generator.__getitem__(0)[0][0])))\n# model.add(Dense(512, activation='swish'))\n# model.add(BatchNormalization())\n# model.add(Dropout(0.5))\n# model.add(Dense(256, activation='swish'))\n# model.add(BatchNormalization())\n# model.add(Dropout(0.5))\n# model.add(Dense(len(y_train[0]), activation='sigmoid'))\n# model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])\n# #tfa.losses.SigmoidFocalCrossEntropy\n# model.summary()\"\"\"\n\n\n# # %% [code]\n# kf = KFold(n_splits=5)\n# models = []\n# losses = []\n# for train_index, valid_index in kf.split(X):\n#     X_train, y_train = X[train_index], y[train_index]\n#     X_valid, y_valid = X[valid_index], y[valid_index]\n#     train_generator = FeatureGenerator(X_train, y_train, batch_size=batch_size)\n#     print(\"HERE\")\n#     print(len(train_generator))\n#     print(\"AFTER\")\n#     valid_generator = FeatureGenerator(X_valid, y_valid, batch_size=batch_size, shuffle=False)\n#     test_generator = FeatureGenerator(X_test, np.zeros(len(X_test)), batch_size=batch_size, shuffle=False)\n    \n#     model = get_model()\n    \n#     reducelr = ReduceLROnPlateau(patience=3,verbose=1)\n#     checkpoint = ModelCheckpoint('/checkpoint', monitor='val_loss', verbose=1, save_best_only=True,save_weights_only=True)\n#     earlystopping = EarlyStopping(patience=10)\n    \n#     #model.fit(train_generator, epochs=200, batch_size=batch_size, validation_data=valid_generator, callbacks=[reducelr, checkpoint, earlystopping])\n    \n#     #models.append(model)\n    \n#     #print(log_loss(y_valid, model.predict(valid_generator)) / len(y_valid[0]))\n#     #losses.append(log_loss(y_valid, model.predict(valid_generator)) / len(y_valid[0]))\n\n# # %% [code]\n# print(losses)\n\n# # %% [code]\n# test_generator = FeatureGenerator(X_test, np.zeros(len(X_test)), batch_size=batch_size, shuffle=False)\n# y_test = np.mean([model.predict(test_generator) for model in models], axis=0)\n\n# # %% [code]\n# df_sample_submission[:] = y_test\n# df_sample_submission\n\n# # %% [code]\n# df_sample_submission.to_csv('submission.csv')\n\n# # %% [code]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}