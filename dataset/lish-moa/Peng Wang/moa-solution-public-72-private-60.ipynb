{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Summary**:\n\nThe purpose of the notebook is to summarize the solution for Team \"Peng Wang + Datong Guo\". The solution we used here is to stack 6 models. These 6 models are mainly NN models,the reason why they are seperated is due to different ways of dealing with dataset. The only Non-NN model used is TabNet. In order to comply with the GPU limit, the solution here is 5538.6 seconds, which includes all the trainning process. In addition, the 7200 seconds is for submission time, which means, if the notebook is commited successfully, there is a chance for notebook timeout error. Around 10% of the notebook should be reserved.\n\nBut if pretrainned models is used, the solution should have more potential. The below solution public score is 0.01823 and the private score is 0.01609."},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"As there are several published notebooks for EDA analysis, we will not discussed all the analysis. Instead, we will discuss two parts, which seems not widely discussed."},{"metadata":{},"cell_type":"markdown","source":"**1.** Quantile Transformer for Genes and Cells, or Quantile Transformer only for Genes?"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import QuantileTransformer\n\ntrain_features = pd.read_csv('../input/lish-moa/train_features.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\n\nGenes_Mean = train_features[GENES + CELLS].mean()\n\nfig, ax = plt.subplots(figsize=(15, 5))\nax.set_xlabel (\"Normal\", fontsize=18)\nax.set_ylabel (\"Mean of Genes + Cells\", fontsize=18);\n\nGenes_Mean.plot(lw=3);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\n\ntransformed_all = train_features\n\nfor col in (GENES + CELLS):\n    transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n    vec_len = len(transformed_all[col].values)\n    raw_vec = transformed_all[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    transformed_all[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n\nGenes_Mean = transformed_all[GENES + CELLS].mean()\n\nfig, ax = plt.subplots(figsize=(15, 5))\nax.set_xlabel (\"Transformed Genes + Cells\", fontsize=18)\nax.set_ylabel (\"Mean of Genes + Cells\", fontsize=18);\n\nGenes_Mean.plot(lw=3);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n\nGENES = [col for col in train_features.columns if col.startswith('g-')]\ntransformed_Genes = train_features\n\nfor col in (GENES):\n    transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n    vec_len = len(transformed_Genes[col].values)\n    raw_vec = transformed_Genes[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    transformed_Genes[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n\nGenes_Mean = transformed_Genes[GENES + CELLS].mean()\n\nfig, ax = plt.subplots(figsize=(15, 5))\nax.set_xlabel (\"Transformed Genes Only\", fontsize=18)\nax.set_ylabel (\"Mean of Genes + Cells\", fontsize=18);\n\nGenes_Mean.plot(lw=3);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**: Compare the normal graph, we think the scale of the genes and cells are different, we decide to only apply the transfers to genes features."},{"metadata":{},"cell_type":"markdown","source":"2. How to handle the variance of the features?\n\nOne common used method is to apply variance threhold across the features.\n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.decomposition import PCA\n\n\ndef loading_files():\n\n  train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n  train_features[\"cp_dose_time\"] = train_features[\"cp_dose\"] + \"_\" + train_features[\"cp_time\"].astype(\"str\")\n  train_features[\"cp_dose_time\"] = train_features[\"cp_dose_time\"].map({\"D1_24\":0 , \"D1_48\":1, \"D1_72\":2, \"D2_24\":3 , \"D2_48\":4, \"D2_72\":5})\n\n  train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n  train_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\n  test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n  test_features[\"cp_dose_time\"] = test_features[\"cp_dose\"] + \"_\" + test_features[\"cp_time\"].astype(\"str\")\n  test_features[\"cp_dose_time\"] = test_features[\"cp_dose_time\"].map({\"D1_24\":0 , \"D1_48\":1, \"D1_72\":2, \"D2_24\":3 , \"D2_48\":4, \"D2_72\":5})\n\n  sample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n\n  return train_features,train_targets_scored,train_targets_nonscored,test_features,sample_submission\n\ndef processing_files(train_features,train_targets_scored,train_targets_nonscored,test_features,sample_submission):\n  \n  GENES = [col for col in train_features.columns if col.startswith('g-')]\n  CELLS = [col for col in test_features.columns if col.startswith('c-')]\n\n  for col in (GENES):\n\n    transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n  \n  from sklearn.cluster import KMeans\n\n  def fe_cluster(train, test, n_clusters_g = 35, n_clusters_c = 5, SEED = 123):\n      \n      features_g = list(train[GENES])\n      features_c = list(train[CELLS])\n      \n      def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n          train_ = train[features].copy()\n          test_ = test[features].copy()\n          data = pd.concat([train_, test_], axis = 0)\n          kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n          train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n          test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n          train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n          test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n          return train, test\n      \n      train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n  \n      return train, test\n\n  train_features ,test_features=fe_cluster(train_features,test_features)\n\n  def fe_stats(train, test):\n    \n    features_c = list(train.columns[776:876])\n    \n    for df in train, test:\n\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        \n    return train, test\n\n  train_features,test_features=fe_stats(train_features,test_features)\n\n  return train_features,test_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features,train_targets_scored,train_targets_nonscored,test_features,sample_submission = loading_files()\ntrain_features,test_features = processing_files(train_features,train_targets_scored,train_targets_nonscored,test_features,sample_submission)\n\n# GENES\n\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\n\nn_comp = 200\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\n#CELLS\nn_comp = 60\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features_variance = pd.DataFrame(train_features[GENES].var())\ntrain_features_variance.columns = ['Vaiance']\ntrain_features_variance = train_features_variance.sort_values('Vaiance',ascending= False)\n\nfig, ax = plt.subplots(figsize=(15, 5))\nax.set_xlabel (\"Genes\", fontsize=18)\nax.set_ylabel (\"Variance of Genes\", fontsize=18);\n\ntrain_features_variance['Vaiance'].plot(lw=3);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train_features[GENES]\na = pd.DataFrame(data.std()/data.mean())\na.columns = ['Ratio']\na = a.sort_values('Ratio',ascending= False)\n\nfig, ax = plt.subplots(figsize=(15, 5))\nax.set_xlabel (\"Genes\", fontsize=18)\nax.set_ylabel (\"Std/Mean Ratio\", fontsize=18);\n\na['Ratio'].plot(lw=3);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**: After comparing the two ways for variance filtering, we think other than giving a threhold for the numbers after PCA, we also apply the models to the two tails in the Ratio graph. In the competition, we only get a chance to apply the part for the positive side. "},{"metadata":{},"cell_type":"markdown","source":"# Generate Pre-Trained Model\n\nIn the dataset, it contains train_targets_nonscored.csv file. This file contains additional binary MoA responses for the training data. Although the data is not predicted nor scored, the file can be used to generate pre-trained model. The file has additional features comparing with the files. K-means is used to generate the number of features needed for the input layer, as we want to explore whether to use the input layer weight and hidden layer weight, or just use the hidden layer weight. By referencing to the public score, as a conclusion, we believe that only replace the hidden layer weight can optimize the public score."},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#Common Structures. As the only difference between Model 1 and Model 2 is the feature engineering, so the common part is stored as functions.\n#The function can also be used in the Model 1 and Model 2, so these functions will not be loaded again in the later section.\n\n#Load \n\nimport numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import QuantileTransformer\n\nimport sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master/iterstrat/')\nfrom ml_stratifiers import MultilabelStratifiedKFold\n\nfrom sklearn.feature_selection import VarianceThreshold\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)\n\ndef loading_files():\n\n  train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n  train_features[\"cp_dose_time\"] = train_features[\"cp_dose\"] + \"_\" + train_features[\"cp_time\"].astype(\"str\")\n  train_features[\"cp_dose_time\"] = train_features[\"cp_dose_time\"].map({\"D1_24\":0 , \"D1_48\":1, \"D1_72\":2, \"D2_24\":3 , \"D2_48\":4, \"D2_72\":5})\n\n  train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n  train_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\n  test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n  test_features[\"cp_dose_time\"] = test_features[\"cp_dose\"] + \"_\" + test_features[\"cp_time\"].astype(\"str\")\n  test_features[\"cp_dose_time\"] = test_features[\"cp_dose_time\"].map({\"D1_24\":0 , \"D1_48\":1, \"D1_72\":2, \"D2_24\":3 , \"D2_48\":4, \"D2_72\":5})\n\n  sample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n\n  return train_features,train_targets_scored,train_targets_nonscored,test_features,sample_submission\n\ndef processing_files(train_features,train_targets_scored,train_targets_nonscored,test_features,sample_submission):\n  \n  GENES = [col for col in train_features.columns if col.startswith('g-')]\n  CELLS = [col for col in train_features.columns if col.startswith('c-')]\n\n  for col in (GENES):\n\n    transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n  \n  from sklearn.cluster import KMeans\n\n  def fe_cluster(train, test, n_clusters_g = 35, n_clusters_c = 5, SEED = 123):\n      \n      features_g = list(train[GENES])\n\n      def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n          train_ = train[features].copy()\n          test_ = test[features].copy()\n          data = pd.concat([train_, test_], axis = 0)\n          kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n          train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n          test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n          train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n          test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n          return train, test\n      \n      train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n \n      return train, test\n\n  train_features ,test_features=fe_cluster(train_features,test_features)\n\n  def fe_stats(train, test):\n    \n    features_c = list(train[CELLS])\n    \n    for df in train, test:\n\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        \n    return train, test\n\n  train_features,test_features=fe_stats(train_features,test_features)\n\n  return train_features,test_features\n\ndef VarianceFilter(num,train_features,test_features):\n  var_thresh = VarianceThreshold(threshold=num)\n  data = train_features.append(test_features)\n  data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\n  train_features_transformed = data_transformed[ : train_features.shape[0]]\n  test_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\n  train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                                columns=['sig_id','cp_type','cp_time','cp_dose'])\n\n  train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\n  test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\n  test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\n  return train_features,test_features\n\ndef data_cleaning(train_features,test_features):\n\n  train = train_features.merge(train_targets_scored, on='sig_id')\n  train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n  test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\n  target = train[train_targets_scored.columns]\n  train = train.drop('cp_type', axis=1)\n  test = test.drop('cp_type', axis=1)\n\n  return train,test,target\n\ndef createfolds(train,number):\n\n  folds = train.copy()\n\n  mskf = MultilabelStratifiedKFold(n_splits=number)\n\n  for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n      folds.loc[v_idx, 'kfold'] = int(f)\n\n  folds['kfold'] = folds['kfold'].astype(int)\n\n  return folds\n\nclass MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n\ndef train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss\n\ndef process_data(data):\n    \n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    \n    return data\n\ndef run_training(fold, seed):\n    \n    #This part is not modified, as we need to generate both pretrained model,so, this function will be loaded again for the second model.\n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=5e-3, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_tr, validloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"NonScored{fold}_.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.load_state_dict(torch.load(f\"NonScored{fold}_.pth\"))\n    model.to(DEVICE)\n\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions\n\ndef run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ / NFOLDS\n        oof += oof_\n        \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Generate Pre-Trained Model for First Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features,train_targets_scored,train_targets_nonscored,test_features,sample_submission = loading_files()\n\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters = 205, random_state = 20).fit(train_targets_nonscored.iloc[:,1:])\nkind = 'x'\ntrain_targets_nonscored[f'clusters_{kind}'] = kmeans.labels_[:train_targets_nonscored.shape[0]]\ntrain_targets_nonscored = pd.get_dummies(train_targets_nonscored, columns = [f'clusters_{kind}'])\nnew_clusters = [col for col in train_targets_nonscored.columns if col.startswith('cl')]\ntrain_targets_scored = train_targets_nonscored[['sig_id'] + new_clusters]\n\n\ntrain_features,test_features = processing_files(train_features,train_targets_scored,train_targets_nonscored,test_features,sample_submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GENES\n\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\n\nn_comp = 200\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\n#CELLS\nn_comp = 60\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features,test_features = VarianceFilter(0,train_features,test_features)\ntrain,test,target = data_cleaning(train_features,test_features)\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\nfolds = createfolds(train,15)\n\nclass Model(nn.Module):\n    \n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.11)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.11)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n  \n        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n        self.dropout4 = nn.Dropout(0.11)\n        self.dense4 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n\n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = self.dense4(x)\n        \n        return x\n    \nfeature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\n\n# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 35\nBATCH_SIZE = 128\nLEARNING_RATE = 0.002\nWEIGHT_DECAY = 1e-5\nNFOLDS = 15\nEARLY_STOPPING_STEPS = 25\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=1024","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Averaging on multiple SEEDS\n\nSEED = [0,1,2]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    print(\"Seed:\",seed)\n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)\n\ntrain[target_cols] = oof\ntest[target_cols] = pd.DataFrame(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ / target.shape[1]\n    \nprint(\"CV log_loss: \", score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Generate Pre-Trained Model for Second Model**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def run_training(fold, seed):\n    \n    #This part is loaded again for different file names, which can be optimized with if function.\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=5e-3, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_tr, validloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"SecondNonScored{fold}_.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.load_state_dict(torch.load(f\"SecondNonScored{fold}_.pth\"))\n    model.to(DEVICE)\n\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features,train_targets_scored,train_targets_nonscored,test_features,sample_submission = loading_files()\n\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters = 205, random_state = 20).fit(train_targets_nonscored.iloc[:,1:])\nkind = 'x'\ntrain_targets_nonscored[f'clusters_{kind}'] = kmeans.labels_[:train_targets_nonscored.shape[0]]\ntrain_targets_nonscored = pd.get_dummies(train_targets_nonscored, columns = [f'clusters_{kind}'])\nnew_clusters = [col for col in train_targets_nonscored.columns if col.startswith('cl')]\ntrain_targets_scored = train_targets_nonscored[['sig_id'] + new_clusters]\n\n\ntrain_features,test_features = processing_files(train_features,train_targets_scored,train_targets_nonscored,test_features,sample_submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GENES\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\n\nn_comp = 70\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\na = pd.DataFrame(data.std()/data.mean())\na.columns = ['Vaiance']\na = a.sort_values('Vaiance',ascending= False)\nlowerthrehold = a.iloc[:,0][round(len(a.iloc[:,0])*0.5)]\nupperthrehold = a.iloc[:,0][round(len(a.iloc[:,0])*0.1)]\ncriteria = (a['Vaiance'] > lowerthrehold) & (a['Vaiance'] < upperthrehold)\nb = a[criteria]\ndata = data[b.index]\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data))\ndata2 = np.append(np.power(data2[0:20],3), data2[10:],axis = 0)\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CELLS\nn_comp = 20\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_features,test_features = VarianceFilter(0,train_features,test_features)\ntrain,test,target = data_cleaning(train_features,test_features)\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\nfolds = createfolds(train,15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    \n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.5)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n        self.dropout4 = nn.Dropout(0.5)\n        self.dense4 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n \n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = self.dense4(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 24\nBATCH_SIZE = 128\nLEARNING_RATE = 2.15e-05\nWEIGHT_DECAY = 1e-5\nNFOLDS = 15\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=1024","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Averaging on multiple SEEDS\n\nSEED = [0,1,2]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    print(\"Seed:\",seed)\n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)\n\ntrain[target_cols] = oof\ntest[target_cols] = pd.DataFrame(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ / target.shape[1]\n    \nprint(\"CV log_loss: \", score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submitted Soluion:"},{"metadata":{},"cell_type":"markdown","source":"# **First Model:** \n(This is a NN model implemented by PyTorch. The Pretrained Model Non Scored folder use the pre-trained Model in the above section)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Common Structures for Model 1 and Model 2\n\n#Load\n\nimport time\nt0 = time.time()\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import QuantileTransformer\n\nimport sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master/iterstrat/')\nfrom ml_stratifiers import MultilabelStratifiedKFold\n\nfrom sklearn.feature_selection import VarianceThreshold\n\ndef seed_everything(seed=34):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=34)\n\ndef loading_files():\n\n  train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n  train_features[\"cp_dose_time\"] = train_features[\"cp_dose\"] + \"_\" + train_features[\"cp_time\"].astype(\"str\")\n  train_features[\"cp_dose_time\"] = train_features[\"cp_dose_time\"].map({\"D1_24\":0 , \"D1_48\":1, \"D1_72\":2, \"D2_24\":3 , \"D2_48\":4, \"D2_72\":5})\n\n  train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n  train_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\n  test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n  test_features[\"cp_dose_time\"] = test_features[\"cp_dose\"] + \"_\" + test_features[\"cp_time\"].astype(\"str\")\n  test_features[\"cp_dose_time\"] = test_features[\"cp_dose_time\"].map({\"D1_24\":0 , \"D1_48\":1, \"D1_72\":2, \"D2_24\":3 , \"D2_48\":4, \"D2_72\":5})\n\n  sample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n\n  return train_features,train_targets_scored,train_targets_nonscored,test_features,sample_submission\n\ndef processing_files(train_features,train_targets_scored,train_targets_nonscored,test_features,sample_submission):\n  \n  GENES = [col for col in train_features.columns if col.startswith('g-')]\n  CELLS = [col for col in test_features.columns if col.startswith('c-')]\n\n  for col in (GENES):\n\n    transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n  \n  from sklearn.cluster import KMeans\n\n  def fe_cluster(train, test, n_clusters_g = 35, n_clusters_c = 5, SEED = 123):\n      \n      features_g = list(train[GENES])\n      features_c = list(train[CELLS])\n      \n      def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n          train_ = train[features].copy()\n          test_ = test[features].copy()\n          data = pd.concat([train_, test_], axis = 0)\n          kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n          train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n          test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n          train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n          test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n          return train, test\n      \n      train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n  \n      return train, test\n\n  train_features ,test_features=fe_cluster(train_features,test_features)\n\n  def fe_stats(train, test):\n    \n    features_c = list(train.columns[776:876])\n    \n    for df in train, test:\n\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        \n    return train, test\n\n  train_features,test_features=fe_stats(train_features,test_features)\n\n  return train_features,test_features\n\ndef VarianceFilter(num,train_features,test_features):\n  var_thresh = VarianceThreshold(threshold=num)\n  data = train_features.append(test_features)\n  data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\n  train_features_transformed = data_transformed[ : train_features.shape[0]]\n  test_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\n  train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                                columns=['sig_id','cp_type','cp_time','cp_dose'])\n\n  train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\n  test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\n  test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\n  return train_features,test_features\n\ndef data_cleaning(train_features,test_features):\n\n  train = train_features.merge(train_targets_scored, on='sig_id')\n  train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n  test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\n  target = train[train_targets_scored.columns]\n  train = train.drop('cp_type', axis=1)\n  test = test.drop('cp_type', axis=1)\n\n  return train,test,target\n\ndef createfolds(train,number):\n\n  folds = train.copy()\n\n  mskf = MultilabelStratifiedKFold(n_splits=number)\n\n  for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n      folds.loc[v_idx, 'kfold'] = int(f)\n\n  folds['kfold'] = folds['kfold'].astype(int)\n\n  return folds\n\nclass MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    \ndef train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n#         print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss\n\ndef process_data(data):\n    \n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    if EPOCHS > 1:\n        print('Loading the Pre-Trained Model Based on Non-Scored File to train')\n        model1 = Model(num_features=num_features,num_targets=num_targets,hidden_size=hidden_size,)\n        model1.load_state_dict(torch.load(f\"../input/pretrainedmodelnonscored/MoANNModel/NonScored{fold}_.pth\"))\n        model = Model(num_features=num_features,num_targets=num_targets,hidden_size=hidden_size,)\n        model.dense2.weight = model1.state_dict()['dense2.weight_v']\n    else:\n        model = Model(num_features=num_features,num_targets=num_targets,hidden_size=hidden_size,)\n\n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=5e-3, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    if EPOCHS > 1:\n      for epoch in range(EPOCHS):\n          \n          train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n          print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n          valid_loss, valid_preds = valid_fn(model, loss_tr, validloader, DEVICE)\n          print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n          \n          if valid_loss < best_loss:\n              \n              best_loss = valid_loss\n              oof[val_idx] = valid_preds\n              torch.save(model.state_dict(), f\"FirstMFOLD{fold}_.pth\")\n          \n          elif(EARLY_STOP == True):\n              \n              early_step += 1\n              if (early_step >= early_stopping_steps):\n                  break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    if EPOCHS > 1:\n        model.load_state_dict(torch.load(f\"FirstMFOLD{fold}_.pth\"))\n    else:\n        print(\"Load Pre-Trained Model to predict:\",\"Seed:\",seed,\"Fold:\",fold)\n        model.load_state_dict(torch.load(f\"../input/modelforsubmissionfinal/ModelForSubmission/FirstMFOLD{fold,seed}_.pth\"))\n  \n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ / NFOLDS\n        oof += oof_\n        \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This part is used for feature selection, due to the time limit, it is not used, but it can be used to reduce the number of features used.\n\ndef decision_tree_selection():\n  train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n  train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n  \n  train = train_features.merge(train_targets_scored, on='sig_id')\n  train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n  target = train[train_targets_scored.columns].iloc[:,1:]\n\n  from sklearn.model_selection import train_test_split\n\n  target = train[train_targets_scored.columns].iloc[:,1:]\n  traindata = train.iloc[:,4:876]\n\n  X_train, X_test, y_train, y_test = train_test_split(traindata, target, test_size=0.3, random_state=42)\n\n  from sklearn.model_selection import cross_val_score\n  from sklearn.tree import DecisionTreeClassifier\n\n  clf = DecisionTreeClassifier(random_state=0)\n  empty = []\n\n  for i in range(X_test.shape[1]-2):\n    a = X_test.iloc[:,i:i+2]\n    score = sum(cross_val_score(clf, a, y_test, cv=2))/2\n    empty.append([list(a.columns),score])\n    print([list(a.columns),score])\n\n  empty1 = pd.DataFrame(empty)\n\n  empty1.columns = ['ParameterNames', 'CV_Values']\n  empty1 = empty1.sort_values('CV_Values',ascending= False)\n  threhold = empty1.iloc[:,1][round(len(empty1.iloc[:,1])*0.2)]\n  filtered = empty1[empty1['CV_Values'] > threhold]\n\n  predictor_list = []\n  for element in filtered['ParameterNames']:\n    predictor_list += element\n\n  start_predicator = list(set(predictor_list))\n\n  return start_predicator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features,train_targets_scored,train_targets_nonscored,test_features,sample_submission = loading_files()\ntrain_features,test_features = processing_files(train_features,train_targets_scored,train_targets_nonscored,test_features,sample_submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GENES\n\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\n\nn_comp = 200\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CELLS\nn_comp = 60\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_features,test_features = VarianceFilter(0.5,train_features,test_features)\ntrain,test,target = data_cleaning(train_features,test_features)\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\nfolds = createfolds(train,15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    \n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.11)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.11)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n  \n        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n        self.dropout4 = nn.Dropout(0.11)\n        self.dense4 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n\n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = self.dense4(x)\n        \n        return x\n\nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 35\nBATCH_SIZE = 128\nLEARNING_RATE = 0.002\nWEIGHT_DECAY = 1e-5\nNFOLDS = 15\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=1024","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Averaging on multiple SEEDS\nimport gc\n\nSEED = [0]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    gc.collect()\n    torch.cuda.empty_cache()\n    print(\"Seed:\",seed)\n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)\n\ntrain[target_cols] = oof\ntest[target_cols] = pd.DataFrame(predictions)\ntest3 = test\n\nt1 = time.time()\ntotal = t1-t0\n\nprint(total)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ / target.shape[1]\n    \nprint(\"CV log_loss: \", score)\n\nt1 = time.time()\ntotal = t1-t0\n\nprint(total)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The result is dumpped as a variable, as the memory is cleaned constantly.\nfrom joblib import dump, load\n\ndump(test3, 'model1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Second Model** \n(which is same structure as the first model, only difference is the way of dealing with preprocessed data.The Pretrained Model Non Scored folder use the pre-trained Model in the above section.)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    if EPOCHS > 1:\n        print('Loading the Pre-Trained Model Based on Non-Scored File to train')\n        model1 = Model(num_features=num_features,num_targets=num_targets,hidden_size=hidden_size,)\n        model1.load_state_dict(torch.load(f\"../input/pretrainedmodelnonscored/MoANNModel/SecondNonScored{fold}_.pth\"))\n        model = Model(num_features=num_features,num_targets=num_targets,hidden_size=hidden_size,)\n        model.dense2.weight = model1.state_dict()['dense2.weight_v']\n    else:\n        model = Model(num_features=num_features,num_targets=num_targets,hidden_size=hidden_size,)\n\n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=5e-3, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    if EPOCHS > 1:\n      for epoch in range(EPOCHS):\n          \n          train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n          print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n          valid_loss, valid_preds = valid_fn(model, loss_tr, validloader, DEVICE)\n          print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n          \n          if valid_loss < best_loss:\n              \n              best_loss = valid_loss\n              oof[val_idx] = valid_preds\n              torch.save(model.state_dict(), f\"SecondMFOLD{fold}_.pth\")\n          \n          elif(EARLY_STOP == True):\n              \n              early_step += 1\n              if (early_step >= early_stopping_steps):\n                  break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    if EPOCHS > 1:\n        model.load_state_dict(torch.load(f\"SecondMFOLD{fold}_.pth\"))\n    else:\n        print(\"Load Pre-Trained Model to predict:\",\"Seed:\",seed,\"Fold:\",fold)\n        model.load_state_dict(torch.load(f\"../input/modelforsubmissionfinal/ModelForSubmission/SecondMFOLD{fold,seed}_.pth\"))\n  \n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_features,train_targets_scored,train_targets_nonscored,test_features,sample_submission = loading_files()\ntrain_features,test_features = processing_files(train_features,train_targets_scored,train_targets_nonscored,test_features,sample_submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GENES\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\n\nn_comp = 70\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\na = pd.DataFrame(data.std()/data.mean())\na.columns = ['Vaiance']\na = a.sort_values('Vaiance',ascending= False)\nlowerthrehold = a.iloc[:,0][round(len(a.iloc[:,0])*0.5)]\nupperthrehold = a.iloc[:,0][round(len(a.iloc[:,0])*0.1)]\ncriteria = (a['Vaiance'] > lowerthrehold) & (a['Vaiance'] < upperthrehold)\nb = a[criteria]\ndata = data[b.index]\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data))\ndata2 = np.append(np.power(data2[0:20],3), data2[10:],axis = 0)\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\n#CELLS\nn_comp = 20\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data))\ndata2 = np.append(np.power(data2[0:20],3), data2[10:],axis = 0)\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_features,test_features = VarianceFilter(0.5,train_features,test_features)\ntrain,test1,target = data_cleaning(train_features,test_features)\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\nfolds = createfolds(train,15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    \n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.5)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n        self.dropout4 = nn.Dropout(0.5)\n        self.dense4 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n \n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = self.dense4(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 24\nBATCH_SIZE = 128\nLEARNING_RATE = 2.15e-05\nWEIGHT_DECAY = 1e-5\nNFOLDS = 10\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=1024","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Averaging on multiple SEEDS\n\nSEED = [0]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    gc.collect()\n    torch.cuda.empty_cache()\n    print(\"Seed:\",seed)\n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)\n\ntrain[target_cols] = oof\ntest1[target_cols] = pd.DataFrame(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dump(test1, 'model2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n\ny_true = train_targets_scored[target_cols].values\ny_pred1 = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred1[:, i])\n    score += score_ / target.shape[1]\n    \nprint(\"CV log_loss: \", score)\n\nt1 = time.time()\ntotal = t1-t0\n\nprint(total)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Third Tabnet Model: \nThis model is non-NN model"},{"metadata":{},"cell_type":"markdown","source":"Notes: when the notebook is created, environmental errors is faced, so, internet is enabled to install the pytorch-tabnet package. But in the competition, the internet is disabled."},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"!pip install pytorch-tabnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master/iterstrat')\nfrom ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import StratifiedKFold\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nimport numpy as np\nimport pandas as pd \nfrom sklearn.metrics import roc_auc_score\n\nimport os\nimport random\nimport sys\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nfrom tqdm import tqdm\nfrom sklearn.metrics import log_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data Loading\n\ndata_path = \"../input/lish-moa/\"\ntrain = pd.read_csv(data_path+'train_features.csv')\n\n#start_predicator = decision_tree_selection()\n#final_predicator = ['sig_id','cp_type','cp_time','cp_dose'] + start_predicator\n\n#train = train[final_predicator]\ntrain.drop(columns=[\"sig_id\"], inplace=True)\n\ntrain_targets_scored = pd.read_csv(data_path+'train_targets_scored.csv')\ntrain_targets_scored.drop(columns=[\"sig_id\"], inplace=True)\n\ntest = pd.read_csv(data_path+'test_features.csv')\n#test = test[final_predicator]\ntest.drop(columns=[\"sig_id\"], inplace=True)\n\nGENES = [col for col in train.columns if col.startswith('g-')]\nCELLS = [col for col in train.columns if col.startswith('c-')]\n\nfor col in (GENES):\n\n  transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n  vec_len = len(train[col].values)\n  vec_len_test = len(test[col].values)\n  raw_vec = train[col].values.reshape(vec_len, 1)\n  transformer.fit(raw_vec)\n\n  train[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n  test[col] = transformer.transform(test[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n\nfrom sklearn.cluster import KMeans\n\ndef fe_cluster(train, test, n_clusters_g = 35, n_clusters_c = 5, SEED = 123):\n      \n    features_g = list(train[GENES])\n    features_c = list(train[CELLS])\n      \n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n        train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n        test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n      \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n      #train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n      \n    return train, test\n\ntrain, test=fe_cluster(train, test)\n\ndef fe_stats(train, test):\n    \n  features_c = list(train[CELLS])\n    \n  for df in train, test:\n\n      df['c_sum'] = df[features_c].sum(axis = 1)\n      df['c_mean'] = df[features_c].mean(axis = 1)\n      df['c_std'] = df[features_c].std(axis = 1)\n      df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n      df['c_skew'] = df[features_c].skew(axis = 1)\n        \n  return train, test\n\ntrain, test=fe_stats(train, test)\n\n\nsubmission = pd.read_csv(data_path+'sample_submission.csv')\n\nremove_vehicle = False\n\nif remove_vehicle:\n    kept_index = train['cp_type']=='trt_cp'\n    train = train.loc[kept_index].reset_index(drop=True)\n    train_targets_scored = train_targets_scored.loc[kept_index].reset_index(drop=True)\n\ntrain[\"cp_type\"] = (train[\"cp_type\"]==\"trt_cp\") + 0\ntrain[\"cp_dose\"] = (train[\"cp_dose\"]==\"D1\") + 0\n\ntest[\"cp_type\"] = (test[\"cp_type\"]==\"trt_cp\") + 0\ntest[\"cp_dose\"] = (test[\"cp_dose\"]==\"D1\") + 0\n\nX_test = test.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_EPOCH = 200\ntabnet_params = dict(n_d=24, n_a=24, n_steps=1, gamma=1.3,\n                     lambda_sparse=0, optimizer_fn=torch.optim.Adam,\n                     optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n                     mask_type='entmax',\n                     scheduler_params=dict(mode=\"min\",\n                                           patience=5,\n                                           min_lr=1e-5,\n                                           factor=0.9,),\n                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                     verbose=10,\n                     )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\nfrom pytorch_tabnet.metrics import Metric\nfrom sklearn.metrics import roc_auc_score, log_loss\n\nclass LogitsLogLoss(Metric):\n    \"\"\"\n    LogLoss with sigmoid applied\n    \"\"\"\n\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        \"\"\"\n        Compute LogLoss of predictions.\n\n        Parameters\n        ----------\n        y_true: np.ndarray\n            Target matrix or vector\n        y_score: np.ndarray\n            Score matrix or vector\n\n        Returns\n        -------\n            float\n            LogLoss of predictions vs targets.\n        \"\"\"\n        logits = 1 / (1 + np.exp(-y_pred))\n        aux = (1-y_true)*np.log(1-logits+1e-15) + y_true*np.log(logits+1e-15)\n        \n        return np.mean(-aux)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_auc_all= []\ntest_cv_preds = []\n\nNB_SPLITS = 7\nmskf = MultilabelStratifiedKFold(n_splits=NB_SPLITS, random_state=0, shuffle=True)\noof_preds = []\noof_targets = []\nscores = []\nscores_auc = []\n\nfor seed in [0]:\n  print('Seed',seed)\n\n  for fold_nb, (train_idx, val_idx) in enumerate(mskf.split(train, train_targets_scored)):\n      print(\"FOLDS : \", fold_nb)\n\n      ## model\n      X_train, y_train = train.values[train_idx, :], train_targets_scored.values[train_idx, :]\n      X_val, y_val = train.values[val_idx, :], train_targets_scored.values[val_idx, :]\n      model = TabNetRegressor(**tabnet_params)\n\n      model.fit(X_train=X_train,\n                y_train=y_train,\n                eval_set=[(X_val, y_val)],\n                eval_name = [\"val\"],\n                eval_metric = [\"logits_ll\"],\n                max_epochs=MAX_EPOCH,\n                patience=20, batch_size=1024, virtual_batch_size=128,\n                num_workers=1, drop_last=False,\n                # use binary cross entropy as this is not a regression problem\n                loss_fn=torch.nn.functional.binary_cross_entropy_with_logits)\n\n      preds_val = model.predict(X_val)\n      # Apply sigmoid to the predictions\n      preds =  1 / (1 + np.exp(-preds_val))\n      score = np.min(model.history[\"val_logits_ll\"])\n  #     name = cfg.save_name + f\"_fold{fold_nb}\"\n  #     model.save_model(name)\n      ## save oof to compute the CV later\n      oof_preds.append(preds_val)\n      oof_targets.append(y_val)\n      scores.append(score)\n\n      # preds on test\n      preds_test = model.predict(X_test)\n      test_cv_preds.append(1 / (1 + np.exp(-preds_test)))\n\noof_preds_all = np.concatenate(oof_preds)\noof_targets_all = np.concatenate(oof_targets)\ntest_preds_all = np.stack(test_cv_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"aucs = [] #roc_auc_score\nfor task_id in range(oof_preds_all.shape[1]):\n    y_true=oof_targets_all[:, task_id]\n    y_score=oof_preds_all[:, task_id]\n    aucs.append(roc_auc_score(y_true,y_score))\nprint(f\"Overall AUC : {np.mean(aucs)}\")\nprint(f\"Average CV : {np.mean(scores)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_feat = [col for col in submission.columns if col not in [\"sig_id\"]]\nsubmission[all_feat] = test_preds_all.mean(axis=0)\n# set control to 0\nsubmission.loc[test['cp_type']==0, submission.columns[1:]] = 0\nTabNetResult = submission\n\nt1 = time.time()\nprint(t1-t0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dump(TabNetResult, 'model3')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fourth Model: \nThis is a NN Model, built by Keras"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import os\nimport random\n\nimport warnings\nwarnings.resetwarnings()\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n\nimport tensorflow as tf\nfrom keras.initializers import Constant\nfrom tensorflow.keras import callbacks, losses, backend\nimport tensorflow_addons as tfa\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import OneHotEncoder, QuantileTransformer, StandardScaler\nfrom statistics import mean, median","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"os.environ['PYTHONHASHSEED'] = '1068'\nnp.random.seed(1068)\ntf.random.set_seed(1068)\nrandom.seed(1068)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def _path(file):\n    return \"../input/lish-moa/\" + file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(_path(\"train_features.csv\"))\ntest = pd.read_csv(_path(\"test_features.csv\"))\n\ntarget = pd.read_csv(_path(\"train_targets_scored.csv\"))\nsample_sub = pd.read_csv(_path(\"sample_submission.csv\"))\n\nGENES = [col for col in train.columns if col.startswith('g-')]\n\nfor col in (GENES):\n  transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n  vec_len = len(train[col].values)\n  vec_len_test = len(test[col].values)\n  raw_vec = train[col].values.reshape(vec_len, 1)\n  transformer.fit(raw_vec)\n\n  train[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n  test[col] = transformer.transform(test[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n\nfrom sklearn.cluster import KMeans\n\ndef fe_cluster(train, test, n_clusters_g = 35, n_clusters_c = 5, SEED = 123):\n      \n    features_g = list(train[GENES])\n    features_c = list(train[CELLS])\n      \n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n        train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n        test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n      \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n      #train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n      \n    return train, test\n\ntrain, test=fe_cluster(train, test)\n\ndef fe_stats(train, test):\n    \n  features_c = list(train.columns[776:876])\n    \n  for df in train, test:\n\n      df['c_sum'] = df[features_c].sum(axis = 1)\n      df['c_mean'] = df[features_c].mean(axis = 1)\n      df['c_std'] = df[features_c].std(axis = 1)\n      df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n      df['c_skew'] = df[features_c].skew(axis = 1)\n        \n  return train, test\n\ntrain, test=fe_stats(train, test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_col = target.drop('sig_id', axis=1).columns\nctl_train = train['cp_type'] == 'ctl_vehicle'\nctl_test = test['cp_type'] == 'ctl_vehicle'\nno_ctl_train = np.logical_not(ctl_train)\nno_ctl_test = np.logical_not(ctl_test)\nsample_sub.loc[:, target_col] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(train, test, target, params):\n    \n    \n    train = train.drop('sig_id', axis=1)\n    test = test.drop('sig_id', axis=1)\n    target = target.drop('sig_id', axis=1)\n    \n    train_len = train.shape[0]\n    test_len = test.shape[0]\n    \n    print('train shape:', train.shape)\n    print('test shape:', test.shape)\n\n    df = pd.concat([train, test], ignore_index=True)\n    \n    GENES = [col for col in df.columns if col.startswith('g-')]\n    CELLS = [col for col in df.columns if col.startswith('c-')]\n    num_cols = GENES + CELLS\n    \n    \n    print('label encoding...')\n    df['cp_type'] = df['cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df['cp_dose'] = df['cp_dose'].map({'D1': 0, 'D2': 1})\n    print('finish label encoding...')\n\n    \n    print('PCA:', params['pca'])\n    if params['pca']:\n        g_n_comp = params['pca']['g_n_comp']\n        c_n_comp = params['pca']['c_n_comp']\n        g_datas = df[GENES]\n        g_datas_transformed = (PCA(n_components=g_n_comp, random_state=1068).fit_transform(g_datas[GENES]))\n        g_datas_transformed = pd.DataFrame(g_datas_transformed, columns=[f'pca_G-{i}' for i in range(g_n_comp)])\n        df = pd.concat((df, g_datas_transformed), axis=1)\n        c_datas = df[CELLS]\n        c_datas_transformed = (PCA(n_components=c_n_comp, random_state=1068).fit_transform(g_datas[GENES]))\n        c_datas_transformed = pd.DataFrame(c_datas_transformed, columns=[f'pca_G-{i}' for i in range(c_n_comp)])\n        df = pd.concat((df, c_datas_transformed), axis=1)\n        print('finish PCA.....')\n        print('train shape:', df[:train_len].shape)\n        print('test shape:', df[train_len:].shape)\n    \n    \n    print('numerical_encoding:', params['numerical_encoding'])\n    if params['numerical_encoding'] == 'standart_scale':\n        scaler = StandardScaler()\n        df[num_cols] = scaler.fit_transform(df[num_cols])\n        print('finish encoding...')\n    elif params['numerical_encoding'] == 'quantiletransform':\n        qt = QuantileTransformer(output_distribution='normal', random_state=1068)\n        df[num_cols] = qt.fit_transform(df[num_cols])\n        print('finish encoding...')\n    \n    \n\n    print('VarianceThreshold:', params['variance_encoding'])\n    if not params['variance_encoding']:\n        train = df[:train_len].values\n        test = df[train_len:].values\n        target = target.values\n    else:\n        print('do variance encoding....')\n        threshold = params['variance_encoding']\n        var_thresh = VarianceThreshold(threshold=threshold)\n        df = var_thresh.fit_transform(df)\n        print('finish variance encoding')\n        print('train shape:', df[:train_len].shape)\n        print('test shape:', df[train_len:].shape)\n        train = df[:train_len]\n        test = df[train_len:]\n        target = target.values\n    \n    \n    print('###################')\n    print('finish all proccess')\n    print('train shape:', train.shape)\n    print('test shape:', test.shape)\n    print('target shape:', target.shape)\n    print('###################')\n    \n    return train, test, target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'pca': False,\n         'numerical_encoding':'quantiletransform',\n         'categorycal_encoding':'label_encoding',\n         'variance_encoding':False\n}\n\ntrain_x, test_x, train_y = preprocess(train, test, target, params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_min = 0.001\np_max = 0.999\n\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -backend.mean(y_true * backend.log(y_pred) + (1 - y_true) * backend.log(1 - y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MLP:\n    def __init__(self, input_shape, activation, output_bias):\n        \n        output_bias = tf.keras.initializers.Constant(output_bias)\n        \n        self.model = tf.keras.Sequential([\n        \n        tf.keras.layers.Input(input_shape),\n        \n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.25),\n        tfa.layers.WeightNormalization(tf.keras.layers.Dense(512, activation=activation)),\n        \n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.60),\n        tfa.layers.WeightNormalization(tf.keras.layers.Dense(256, activation=activation)),\n\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.45),\n        tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, activation='sigmoid', bias_initializer=output_bias))\n        \n        ])\n        \n        \n        optimizer = tfa.optimizers.Lookahead(\n            tfa.optimizers.AdamW(weight_decay=1e-5),\n            sync_period=5\n        )\n        \n        \n        self.model.compile(optimizer=optimizer, \n                           loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0008),\n                           metrics=logloss)\n        \n        \n        \n    def fit(self, train_x,train_y, val_x, val_y, epochs, batch_size, verbose):\n        reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_logloss', factor=0.3, patience=5, mode='min', min_lr=5e-6)\n        early_stopping = callbacks.EarlyStopping(monitor='val_logloss', min_delta=5e-6, patience=5, mode='min',restore_best_weights=True)\n        history = self.model.fit(train_x,\n                                 train_y,\n                                 epochs=epochs,\n                                 validation_data=(val_x, val_y),\n                                 batch_size = batch_size,\n                                 verbose = verbose,\n                                 callbacks = [reduce_lr, early_stopping]\n                                )\n        return history\n    \n    \n    def predict(self,test_x):   \n        pred = self.model.predict(test_x)\n        return pred\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_loss(losses):\n    for loss in losses:\n        print(' -> {:.6f}'.format(loss), end='')\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_loss(history, i, seed):\n    title = 'loss fold {} in seed {}'.format(i, seed)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title(title)\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run(train_x, train_y, test_x, n_seed, n_fold, epoch, debug, verbose, output_bias, activation, plot):\n    start_run = time.time()\n    preds = sample_sub[:]\n    val_losses = []\n    if debug:\n        print('run debug mode...')\n        train_x = train_x[:10]\n        train_y = train_y[:10]\n        # test_x = test_x[:10]   \n        epoch = 2\n        n_fold = 2\n        n_seed = 2\n        plot = True\n        verbose = 1\n    for seed in range(1068, n_seed + 1068):\n        print('###############')\n        print('run_seed:', seed)\n        print('###############')\n        start = time.time()\n        mskf = MultilabelStratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)         \n        for (i, (train_idx, val_idx)) in enumerate(mskf.split(train_x, train_y), 1):\n            model = MLP(train_x.shape[1], activation, output_bias)\n            history = model.fit(train_x[train_idx],\n                                train_y[train_idx], \n                                train_x[val_idx], \n                                train_y[val_idx],\n                                epochs=epoch, \n                                batch_size=128,\n                                verbose=verbose\n                                )\n            \n            if plot:\n                plot_loss(history, i, seed)\n            \n            print('finish run fold', i, 'with', len(history.history['loss']), 'epoch')\n            print('train_loss of last 5 epoch:', end='')\n            show_loss(history.history['loss'][-5:])\n            print('val_loss of last 5 epoch  :', end='')\n            show_loss(history.history['val_loss'][-5:])\n            val_losses.append(history.history['val_loss'][-1])\n            \n            \n            pred = model.predict(test_x)\n            preds.loc[:, target_col] += pred\n        \n        elapsed_time = time.time() - start\n        print (\"time:{0}\".format(elapsed_time) + \"[sec]\")\n    \n    preds.loc[:,target_col] /= n_fold * n_seed\n    \n    print('-------------------------')\n    print('finish train and predict!')\n    print('-------------------------')\n    elapsed_time = time.time() - start_run\n    print ('time: {0} '.format(elapsed_time) + '[sec]')\n    print('loss:', end=' ')\n    med_loss = (max(val_losses) + min(val_losses)) / 2\n    pm_loss = max(val_losses) - med_loss\n    print('{:.6f}  {:.6f} (mean: {:.6f})'.format(med_loss, pm_loss, mean(val_losses)))\n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_seed = 3\nn_fold = 5\noutput_bias = -np.log(train_y.mean(axis=0))\nverbose = 0\ndebug = False\nepoch = 100\nactivation = 'swish'\nplot = False\npreds = run(train_x, train_y, test_x, n_seed, n_fold, epoch, debug, verbose, output_bias, activation, plot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds.iloc[:,1:] = np.clip(preds.iloc[:,1:].values, p_min, p_max)\npreds.loc[ctl_test, target_col] = 0.0\n\nKerasSubmission = preds\n\ndump(KerasSubmission, 'model4')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fifth Model:\nComparing with the first model and second model, the difference is the new files for Drug ID file (the new file is released at the later stage of the competition)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n \nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.decomposition import PCA\n \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n \nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = '../input/lish-moa/'\nos.listdir(data_dir)\n\ntrain_features = pd.read_csv(data_dir + 'train_features.csv')\ntrain_targets_scored = pd.read_csv(data_dir + 'train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv(data_dir + 'train_targets_nonscored.csv')\ntrain_drug = pd.read_csv(data_dir + 'train_drug.csv')\ntest_features = pd.read_csv(data_dir + 'test_features.csv')\nsample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n\nprint('train_features: {}'.format(train_features.shape))\nprint('train_targets_scored: {}'.format(train_targets_scored.shape))\nprint('train_targets_nonscored: {}'.format(train_targets_nonscored.shape))\nprint('train_drug: {}'.format(train_drug.shape))\nprint('test_features: {}'.format(test_features.shape))\nprint('sample_submission: {}'.format(sample_submission.shape))\n\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\n\nprint('GENES: {}'.format(GENES[:10]))\nprint('CELLS: {}'.format(CELLS[:10]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in (GENES):\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n\nfrom sklearn.cluster import KMeans\n\ndef fe_cluster(train, test, n_clusters_g = 35, n_clusters_c = 5, SEED = 123):\n      \n    features_g = list(train[GENES])\n    features_c = list(train[CELLS])\n      \n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n        train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n        test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n      \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n  \n    return train, test\n\ntrain_features ,test_features=fe_cluster(train_features,test_features)\n\ndef fe_stats(train, test):\n    \n    features_c = list(train.columns[776:876])\n    \n    for df in train, test:\n\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        \n    return train, test\n\ntrain_features,test_features=fe_stats(train_features,test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED_VALUE = 42\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=SEED_VALUE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GENES\nn_comp = 600\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=SEED_VALUE).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\nprint('train_features: {}'.format(train_features.shape))\nprint('test_features: {}'.format(test_features.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CELLS\nn_comp = 50\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=SEED_VALUE).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\nprint('train_features: {}'.format(train_features.shape))\nprint('test_features: {}'.format(test_features.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\n\nvar_thresh = VarianceThreshold(0.8)\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\nprint('train_features: {}'.format(train_features.shape))\nprint('test_features: {}'.format(test_features.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train.merge(train_targets_nonscored, on='sig_id')\ntrain = train.merge(train_drug, on='sig_id')\ntrain = train[train['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols = [x for x in train_targets_scored.columns if x != 'sig_id']\naux_target_cols = [x for x in train_targets_nonscored.columns if x != 'sig_id']\nall_target_cols = target_cols + aux_target_cols\n\nnum_targets = len(target_cols)\nnum_aux_targets = len(aux_target_cols)\nnum_all_targets = len(all_target_cols)\n\nprint('num_targets: {}'.format(num_targets))\nprint('num_aux_targets: {}'.format(num_aux_targets))\nprint('num_all_targets: {}'.format(num_all_targets))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)\n        }\n        \n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n\n        return dct","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    return final_loss\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n\n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n            \n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super(Model, self).__init__()\n        self.hidden_size = [1500, 1250, 1000, 750]\n        self.dropout_value = [0.5, 0.35, 0.3, 0.25]\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n        \n        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n        self.dropout2 = nn.Dropout(self.dropout_value[0])\n        self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n\n        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n        self.dropout3 = nn.Dropout(self.dropout_value[1])\n        self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n\n        self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n        self.dropout4 = nn.Dropout(self.dropout_value[2])\n        self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n\n        self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n        self.dropout5 = nn.Dropout(self.dropout_value[3])\n        self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = F.leaky_relu(self.dense3(x))\n\n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = F.leaky_relu(self.dense4(x))\n\n        x = self.batch_norm5(x)\n        x = self.dropout5(x)\n        x = self.dense5(x)\n        return x\n    \nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n            \n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FineTuneScheduler:\n    def __init__(self, epochs):\n        self.epochs = epochs\n        self.epochs_per_step = 0\n        self.frozen_layers = []\n\n    def copy_without_top(self, model, num_features, num_targets, num_targets_new):\n        self.frozen_layers = []\n\n        model_new = Model(num_features, num_targets)\n        model_new.load_state_dict(model.state_dict())\n\n        # Freeze all weights\n        for name, param in model_new.named_parameters():\n            layer_index = name.split('.')[0][-1]\n\n            if layer_index == 5:\n                continue\n\n            param.requires_grad = False\n\n            # Save frozen layer names\n            if layer_index not in self.frozen_layers:\n                self.frozen_layers.append(layer_index)\n\n        self.epochs_per_step = self.epochs // len(self.frozen_layers)\n\n        # Replace the top layers with another ones\n        model_new.batch_norm5 = nn.BatchNorm1d(model_new.hidden_size[3])\n        model_new.dropout5 = nn.Dropout(model_new.dropout_value[3])\n        model_new.dense5 = nn.utils.weight_norm(nn.Linear(model_new.hidden_size[-1], num_targets_new))\n        model_new.to(DEVICE)\n        return model_new\n\n    def step(self, epoch, model):\n        if len(self.frozen_layers) == 0:\n            return\n\n        if epoch % self.epochs_per_step == 0:\n            last_frozen_index = self.frozen_layers[-1]\n            \n            # Unfreeze parameters of the last frozen layer\n            for name, param in model.named_parameters():\n                layer_index = name.split('.')[0][-1]\n\n                if layer_index == last_frozen_index:\n                    param.requires_grad = True\n\n            del self.frozen_layers[-1]  # Remove the last layer as unfrozen","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = [c for c in process_data(train).columns if c not in all_target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold', 'sig_id', 'drug_id']]\nnum_features = len(feature_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 24\nBATCH_SIZE = 128\n\nWEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 3e-6}\nMAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 3e-3}\nDIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2}\nPCT_START = 0.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show model architecture\nmodel = Model(num_features, num_all_targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\ndef make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH):\n    vc = train.drug_id.value_counts()\n    vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n    vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n\n    for seed_id in range(SEEDS):\n        kfold_col = 'kfold_{}'.format(seed_id)\n        \n        # STRATIFY DRUGS 18X OR LESS\n        dct1 = {}\n        dct2 = {}\n\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.groupby('drug_id')[target_cols].mean().loc[vc1]\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        # STRATIFY DRUGS MORE THAN 18X\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        # ASSIGN FOLDS\n        train[kfold_col] = train.drug_id.map(dct1)\n        train.loc[train[kfold_col].isna(), kfold_col] = train.loc[train[kfold_col].isna(), 'sig_id'].map(dct2)\n        train[kfold_col] = train[kfold_col].astype('int8')\n        \n    return train\n\nSEEDS = 7\nNFOLDS = 7\nDRUG_THRESH = 18\n\ntrain = make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(fold_id, seed_id):\n    seed_everything(seed_id)\n    \n    train_ = process_data(train)\n    test_ = process_data(test)\n    \n    kfold_col = f'kfold_{seed_id}'\n    trn_idx = train_[train_[kfold_col] != fold_id].index\n    val_idx = train_[train_[kfold_col] == fold_id].index\n    \n    train_df = train_[train_[kfold_col] != fold_id].reset_index(drop=True)\n    valid_df = train_[train_[kfold_col] == fold_id].reset_index(drop=True)\n    \n    def train_model(model, tag_name, target_cols_now, fine_tune_scheduler=None):\n        x_train, y_train  = train_df[feature_cols].values, train_df[target_cols_now].values\n        x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols_now].values\n        \n        train_dataset = MoADataset(x_train, y_train)\n        valid_dataset = MoADataset(x_valid, y_valid)\n\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n        \n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY[tag_name])\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n                                                  steps_per_epoch=len(trainloader),\n                                                  pct_start=PCT_START,\n                                                  div_factor=DIV_FACTOR[tag_name], \n                                                  max_lr=MAX_LR[tag_name],\n                                                  epochs=EPOCHS)\n        \n        loss_fn = nn.BCEWithLogitsLoss()\n        loss_tr = SmoothBCEwLogits(smoothing=0.001)\n\n        oof = np.zeros((len(train), len(target_cols_now)))\n        best_loss = np.inf\n        \n        if EPOCHS > 1:\n            for epoch in range(EPOCHS):\n                gc.collect()\n                torch.cuda.empty_cache()\n                if fine_tune_scheduler is not None:\n                    fine_tune_scheduler.step(epoch, model)\n\n                train_loss = train_fn(model, optimizer, scheduler, loss_tr, trainloader, DEVICE)\n                valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n                print(f\"SEED: {seed_id}, FOLD: {fold_id}, {tag_name}, EPOCH: {epoch}, train_loss: {train_loss:.6f}, valid_loss: {valid_loss:.6f}\")\n\n                if np.isnan(valid_loss):\n                    break\n\n                if valid_loss < best_loss:\n                    best_loss = valid_loss\n                    oof[val_idx] = valid_preds\n                    torch.save(model.state_dict(), f\"{tag_name}_FOLD{fold_id}_.pth\")\n\n        return oof\n\n    fine_tune_scheduler = FineTuneScheduler(EPOCHS)\n\n    pretrained_model = Model(num_features, num_all_targets)\n    pretrained_model.to(DEVICE)\n\n    # Train on scored + nonscored targets\n    train_model(pretrained_model, 'ALL_TARGETS', all_target_cols)\n\n    # Load the pretrained model with the best loss\n    pretrained_model = Model(num_features, num_all_targets)\n    if EPOCHS > 1:\n        pretrained_model.load_state_dict(torch.load(f\"ALL_TARGETS_FOLD{fold_id}_.pth\"))\n    else:\n        print('Load PreTrained Model,','fold_id:',fold_id)\n        pretrained_model.load_state_dict(torch.load(f\"../input/fifthpretrainedmodel/ALL_TARGETS_FOLD{fold_id}_.pth\"))\n    pretrained_model.to(DEVICE)\n\n    # Copy model without the top layer\n    final_model = fine_tune_scheduler.copy_without_top(pretrained_model, num_features, num_all_targets, num_targets)\n\n    # Fine-tune the model on scored targets only\n    oof = train_model(final_model, 'SCORED_ONLY', target_cols, fine_tune_scheduler)\n\n    # Load the fine-tuned model with the best loss\n    model = Model(num_features, num_targets)\n    if EPOCHS > 1:\n        model.load_state_dict(torch.load(f\"SCORED_ONLY_FOLD{fold_id}_.pth\"))\n    else:\n        print('Load PreTrained Model,','fold_id:',fold_id)\n        model.load_state_dict(torch.load(f\"../input/fifthpretrainedmodel/SCORED_ONLY_FOLD{fold_id}_.pth\"))\n    model.to(DEVICE)\n\n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    predictions = np.zeros((len(test_), num_targets))\n    predictions = inference_fn(model, testloader, DEVICE)\n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed_id):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold_id in range(NFOLDS):\n        oof_, pred_ = run_training(fold_id, seed_id)\n        predictions += pred_ / NFOLDS\n        oof += oof_\n        \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from time import time\n\n# Averaging on multiple SEEDS\nSEED = [0,1]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\ntime_begin = time()\n\nfor seed_id in SEED:\n    oof_, predictions_ = run_k_fold(NFOLDS, seed_id)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)\n\ntime_diff = time() - time_begin\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import timedelta\nstr(timedelta(seconds=time_diff))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\n\nfor i in range(len(target_cols)):\n    score += log_loss(y_true[:, i], y_pred[:, i])\n\nprint(\"CV log_loss: \", score / y_pred.shape[1])\n\nfifthmodelresult = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dump(fifthmodelresult,'model5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sixth Model: \nThe structure of the model is the same as the fifth model. As the code is not fully optimized, so, only the difference with the fifth model is shown."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n \nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.decomposition import PCA\n \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n \nimport warnings\nwarnings.filterwarnings('ignore')\n\ndata_dir = '../input/lish-moa/'\nos.listdir(data_dir)\n\ntrain_features = pd.read_csv(data_dir + 'train_features.csv')\ntrain_targets_scored = pd.read_csv(data_dir + 'train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv(data_dir + 'train_targets_nonscored.csv')\ntrain_drug = pd.read_csv(data_dir + 'train_drug.csv')\ntest_features = pd.read_csv(data_dir + 'test_features.csv')\nsample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n\nprint('train_features: {}'.format(train_features.shape))\nprint('train_targets_scored: {}'.format(train_targets_scored.shape))\nprint('train_targets_nonscored: {}'.format(train_targets_nonscored.shape))\nprint('train_drug: {}'.format(train_drug.shape))\nprint('test_features: {}'.format(test_features.shape))\nprint('sample_submission: {}'.format(sample_submission.shape))\n\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\n\nprint('GENES: {}'.format(GENES[:10]))\nprint('CELLS: {}'.format(CELLS[:10]))\n\nfor col in (GENES):\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n\nfrom sklearn.cluster import KMeans\n\ndef fe_cluster(train, test, n_clusters_g = 35, n_clusters_c = 5, SEED = 123):\n      \n    features_g = list(train[GENES])\n    features_c = list(train[CELLS])\n      \n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n        train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n        test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n      \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n  \n    return train, test\n\ntrain_features ,test_features=fe_cluster(train_features,test_features)\n\ndef fe_stats(train, test):\n    \n    features_c = list(train.columns[776:876])\n    \n    for df in train, test:\n\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        \n    return train, test\n\ntrain_features,test_features=fe_stats(train_features,test_features)\n\nSEED_VALUE = 42\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=SEED_VALUE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GENES\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\n\nn_comp = 70\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\na = pd.DataFrame(data.std()/data.mean())\na.columns = ['Vaiance']\na = a.sort_values('Vaiance',ascending= False)\nlowerthrehold = a.iloc[:,0][round(len(a.iloc[:,0])*0.7)]\nupperthrehold = a.iloc[:,0][round(len(a.iloc[:,0])*0.5)]\ncriteria = (a['Vaiance'] > lowerthrehold) & (a['Vaiance'] < upperthrehold)\nb = a[criteria]\ndata = data[b.index]\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data))\ndata2 = np.append(np.power(data2[0:20],3), data2[10:],axis = 0)\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\nprint('train_features: {}'.format(train_features.shape))\nprint('test_features: {}'.format(test_features.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CELLS\nn_comp = 50\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=SEED_VALUE).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\nprint('train_features: {}'.format(train_features.shape))\nprint('test_features: {}'.format(test_features.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\n\nvar_thresh = VarianceThreshold(0.8)\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\nprint('train_features: {}'.format(train_features.shape))\nprint('test_features: {}'.format(test_features.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train.merge(train_targets_nonscored, on='sig_id')\ntrain = train.merge(train_drug, on='sig_id')\ntrain = train[train['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"target_cols = [x for x in train_targets_scored.columns if x != 'sig_id']\naux_target_cols = [x for x in train_targets_nonscored.columns if x != 'sig_id']\nall_target_cols = target_cols + aux_target_cols\n\nnum_targets = len(target_cols)\nnum_aux_targets = len(aux_target_cols)\nnum_all_targets = len(all_target_cols)\n\nprint('num_targets: {}'.format(num_targets))\nprint('num_aux_targets: {}'.format(num_aux_targets))\nprint('num_all_targets: {}'.format(num_all_targets))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)\n        }\n        \n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n\n        return dct","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    return final_loss\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n\n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    return preds\n\nimport torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n            \n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super(Model, self).__init__()\n        self.hidden_size = [1500, 1250, 1000, 750]\n        self.dropout_value = [0.5, 0.35, 0.3, 0.25]\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n        \n        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n        self.dropout2 = nn.Dropout(self.dropout_value[0])\n        self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n\n        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n        self.dropout3 = nn.Dropout(self.dropout_value[1])\n        self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n\n        self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n        self.dropout4 = nn.Dropout(self.dropout_value[2])\n        self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n\n        self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n        self.dropout5 = nn.Dropout(self.dropout_value[3])\n        self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = F.leaky_relu(self.dense3(x))\n\n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = F.leaky_relu(self.dense4(x))\n\n        x = self.batch_norm5(x)\n        x = self.dropout5(x)\n        x = self.dense5(x)\n        return x\n    \nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n            \n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class FineTuneScheduler:\n    def __init__(self, epochs):\n        self.epochs = epochs\n        self.epochs_per_step = 0\n        self.frozen_layers = []\n\n    def copy_without_top(self, model, num_features, num_targets, num_targets_new):\n        self.frozen_layers = []\n\n        model_new = Model(num_features, num_targets)\n        model_new.load_state_dict(model.state_dict())\n\n        # Freeze all weights\n        for name, param in model_new.named_parameters():\n            layer_index = name.split('.')[0][-1]\n\n            if layer_index == 5:\n                continue\n\n            param.requires_grad = False\n\n            # Save frozen layer names\n            if layer_index not in self.frozen_layers:\n                self.frozen_layers.append(layer_index)\n\n        self.epochs_per_step = self.epochs // len(self.frozen_layers)\n\n        # Replace the top layers with another ones\n        model_new.batch_norm5 = nn.BatchNorm1d(model_new.hidden_size[3])\n        model_new.dropout5 = nn.Dropout(model_new.dropout_value[3])\n        model_new.dense5 = nn.utils.weight_norm(nn.Linear(model_new.hidden_size[-1], num_targets_new))\n        model_new.to(DEVICE)\n        return model_new\n\n    def step(self, epoch, model):\n        if len(self.frozen_layers) == 0:\n            return\n\n        if epoch % self.epochs_per_step == 0:\n            last_frozen_index = self.frozen_layers[-1]\n            \n            # Unfreeze parameters of the last frozen layer\n            for name, param in model.named_parameters():\n                layer_index = name.split('.')[0][-1]\n\n                if layer_index == last_frozen_index:\n                    param.requires_grad = True\n\n            del self.frozen_layers[-1]  # Remove the last layer as unfrozen","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data\n\nfeature_cols = [c for c in process_data(train).columns if c not in all_target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold', 'sig_id', 'drug_id']]\nnum_features = len(feature_cols)\nnum_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 24\nBATCH_SIZE = 128\n\nWEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 3e-6}\nMAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 3e-3}\nDIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2}\nPCT_START = 0.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\ndef make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH):\n    vc = train.drug_id.value_counts()\n    vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n    vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n\n    for seed_id in range(SEEDS):\n        kfold_col = 'kfold_{}'.format(seed_id)\n        \n        # STRATIFY DRUGS 18X OR LESS\n        dct1 = {}\n        dct2 = {}\n\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.groupby('drug_id')[target_cols].mean().loc[vc1]\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        # STRATIFY DRUGS MORE THAN 18X\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        # ASSIGN FOLDS\n        train[kfold_col] = train.drug_id.map(dct1)\n        train.loc[train[kfold_col].isna(), kfold_col] = train.loc[train[kfold_col].isna(), 'sig_id'].map(dct2)\n        train[kfold_col] = train[kfold_col].astype('int8')\n        \n    return train\n\nSEEDS = 7\nNFOLDS = 7\nDRUG_THRESH = 18\n\ntrain = make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def run_training(fold_id, seed_id):\n    seed_everything(seed_id)\n    \n    train_ = process_data(train)\n    test_ = process_data(test)\n    \n    kfold_col = f'kfold_{seed_id}'\n    trn_idx = train_[train_[kfold_col] != fold_id].index\n    val_idx = train_[train_[kfold_col] == fold_id].index\n    \n    train_df = train_[train_[kfold_col] != fold_id].reset_index(drop=True)\n    valid_df = train_[train_[kfold_col] == fold_id].reset_index(drop=True)\n    \n    def train_model(model, tag_name, target_cols_now, fine_tune_scheduler=None):\n        x_train, y_train  = train_df[feature_cols].values, train_df[target_cols_now].values\n        x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols_now].values\n        \n        train_dataset = MoADataset(x_train, y_train)\n        valid_dataset = MoADataset(x_valid, y_valid)\n\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n        \n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY[tag_name])\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n                                                  steps_per_epoch=len(trainloader),\n                                                  pct_start=PCT_START,\n                                                  div_factor=DIV_FACTOR[tag_name], \n                                                  max_lr=MAX_LR[tag_name],\n                                                  epochs=EPOCHS)\n        \n        loss_fn = nn.BCEWithLogitsLoss()\n        loss_tr = SmoothBCEwLogits(smoothing=0.001)\n\n        oof = np.zeros((len(train), len(target_cols_now)))\n        best_loss = np.inf\n        \n        if EPOCHS > 1:\n            for epoch in range(EPOCHS):\n                gc.collect()\n                torch.cuda.empty_cache()\n                if fine_tune_scheduler is not None:\n                    fine_tune_scheduler.step(epoch, model)\n\n                train_loss = train_fn(model, optimizer, scheduler, loss_tr, trainloader, DEVICE)\n                valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n                print(f\"SEED: {seed_id}, FOLD: {fold_id}, {tag_name}, EPOCH: {epoch}, train_loss: {train_loss:.6f}, valid_loss: {valid_loss:.6f}\")\n\n                if np.isnan(valid_loss):\n                    break\n\n                if valid_loss < best_loss:\n                    best_loss = valid_loss\n                    oof[val_idx] = valid_preds\n                    torch.save(model.state_dict(), f\"{tag_name}_FOLD{fold_id}_.pth\")\n\n        return oof\n\n    fine_tune_scheduler = FineTuneScheduler(EPOCHS)\n\n    pretrained_model = Model(num_features, num_all_targets)\n    pretrained_model.to(DEVICE)\n\n    # Train on scored + nonscored targets\n    train_model(pretrained_model, 'ALL_TARGETS', all_target_cols)\n\n    # Load the pretrained model with the best loss\n    pretrained_model = Model(num_features, num_all_targets)\n    if EPOCHS > 1:\n        pretrained_model.load_state_dict(torch.load(f\"ALL_TARGETS_FOLD{fold_id}_.pth\"))\n    else:\n        print('Load PreTrained Model,','fold_id:',fold_id)\n        pretrained_model.load_state_dict(torch.load(f\"../input/sixthpretrainedmodel/ALL_TARGETS_FOLD{fold_id}_.pth\"))\n    pretrained_model.to(DEVICE)\n\n    # Copy model without the top layer\n    final_model = fine_tune_scheduler.copy_without_top(pretrained_model, num_features, num_all_targets, num_targets)\n\n    # Fine-tune the model on scored targets only\n    oof = train_model(final_model, 'SCORED_ONLY', target_cols, fine_tune_scheduler)\n\n    # Load the fine-tuned model with the best loss\n    model = Model(num_features, num_targets)\n    if EPOCHS > 1:\n        model.load_state_dict(torch.load(f\"SCORED_ONLY_FOLD{fold_id}_.pth\"))\n    else:\n        print('Load PreTrained Model,','fold_id:',fold_id)\n        model.load_state_dict(torch.load(f\"../input/sixthpretrainedmodel/SCORED_ONLY_FOLD{fold_id}_.pth\"))\n    model.to(DEVICE)\n\n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    predictions = np.zeros((len(test_), num_targets))\n    predictions = inference_fn(model, testloader, DEVICE)\n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed_id):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold_id in range(NFOLDS):\n        oof_, pred_ = run_training(fold_id, seed_id)\n        predictions += pred_ / NFOLDS\n        oof += oof_\n        \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from time import time\n\n# Averaging on multiple SEEDS\nSEED = [0]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\ntime_begin = time()\n\nfor seed_id in SEED:\n    oof_, predictions_ = run_k_fold(NFOLDS, seed_id)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)\n\ntime_diff = time() - time_begin\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import timedelta\nstr(timedelta(seconds=time_diff))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\n\nfor i in range(len(target_cols)):\n    score += log_loss(y_true[:, i], y_pred[:, i])\n\nprint(\"CV log_loss: \", score / y_pred.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sixthModel = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test3 = load('model1')\ntest1 = load('model2')\nTabNetResult = load('model3')\nKerasSubmission = load('model4')\nfifthmodelresult = load('model5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Combined Them\ntestf = test3[target_cols]*0.9+ test1[target_cols]*0.1\ncombineresults = pd.concat((test1.iloc[:, 0], testf), axis=1)\nsub = sample_submission.drop(columns=target_cols).merge(combineresults[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The weight for each model is based on the public score as the main method to assign coresponding weight."},{"metadata":{"trusted":true},"cell_type":"code","source":"results = (sub.iloc[:, 1:])*(0.5) + (TabNetResult.iloc[:, 1:])*(0.1) + (KerasSubmission.iloc[:, 1:])*(0.1) + ((fifthmodelresult.iloc[:, 1:])*(0.9) + (sixthModel.iloc[:, 1:])*(0.1))*(0.3)\ncombineresults = pd.concat((TabNetResult.iloc[:, 0], results), axis=1)\n\ncombineresults.to_csv('submission.csv',index=False)\n\nmy_sub = 'submission.csv'\n\nimport numpy as np \nimport pandas as pd \nimport os \nTARGET_COL = ['5-alpha_reductase_inhibitor', '11-beta-hsd1_inhibitor', 'acat_inhibitor', 'acetylcholine_receptor_agonist', 'acetylcholine_receptor_antagonist', 'acetylcholinesterase_inhibitor', 'adenosine_receptor_agonist', 'adenosine_receptor_antagonist', 'adenylyl_cyclase_activator', 'adrenergic_receptor_agonist', 'adrenergic_receptor_antagonist', 'akt_inhibitor', 'aldehyde_dehydrogenase_inhibitor', 'alk_inhibitor', 'ampk_activator', 'analgesic', 'androgen_receptor_agonist', 'androgen_receptor_antagonist', 'anesthetic_-_local', 'angiogenesis_inhibitor', 'angiotensin_receptor_antagonist', 'anti-inflammatory', 'antiarrhythmic', 'antibiotic', 'anticonvulsant', 'antifungal', 'antihistamine', 'antimalarial', 'antioxidant', 'antiprotozoal', 'antiviral', 'apoptosis_stimulant', 'aromatase_inhibitor', 'atm_kinase_inhibitor', 'atp-sensitive_potassium_channel_antagonist', 'atp_synthase_inhibitor', 'atpase_inhibitor', 'atr_kinase_inhibitor', 'aurora_kinase_inhibitor', 'autotaxin_inhibitor', 'bacterial_30s_ribosomal_subunit_inhibitor', 'bacterial_50s_ribosomal_subunit_inhibitor', 'bacterial_antifolate', 'bacterial_cell_wall_synthesis_inhibitor', 'bacterial_dna_gyrase_inhibitor', 'bacterial_dna_inhibitor', 'bacterial_membrane_integrity_inhibitor', 'bcl_inhibitor', 'bcr-abl_inhibitor', 'benzodiazepine_receptor_agonist', 'beta_amyloid_inhibitor', 'bromodomain_inhibitor', 'btk_inhibitor', 'calcineurin_inhibitor', 'calcium_channel_blocker', 'cannabinoid_receptor_agonist', 'cannabinoid_receptor_antagonist', 'carbonic_anhydrase_inhibitor', 'casein_kinase_inhibitor', 'caspase_activator', 'catechol_o_methyltransferase_inhibitor', 'cc_chemokine_receptor_antagonist', 'cck_receptor_antagonist', 'cdk_inhibitor', 'chelating_agent', 'chk_inhibitor', 'chloride_channel_blocker', 'cholesterol_inhibitor', 'cholinergic_receptor_antagonist', 'coagulation_factor_inhibitor', 'corticosteroid_agonist', 'cyclooxygenase_inhibitor', 'cytochrome_p450_inhibitor', 'dihydrofolate_reductase_inhibitor', 'dipeptidyl_peptidase_inhibitor', 'diuretic', 'dna_alkylating_agent', 'dna_inhibitor', 'dopamine_receptor_agonist', 'dopamine_receptor_antagonist', 'egfr_inhibitor', 'elastase_inhibitor', 'erbb2_inhibitor', 'estrogen_receptor_agonist', 'estrogen_receptor_antagonist', 'faah_inhibitor', 'farnesyltransferase_inhibitor', 'fatty_acid_receptor_agonist', 'fgfr_inhibitor', 'flt3_inhibitor', 'focal_adhesion_kinase_inhibitor', 'free_radical_scavenger', 'fungal_squalene_epoxidase_inhibitor', 'gaba_receptor_agonist', 'gaba_receptor_antagonist', 'gamma_secretase_inhibitor', 'glucocorticoid_receptor_agonist', 'glutamate_inhibitor', 'glutamate_receptor_agonist', 'glutamate_receptor_antagonist', 'gonadotropin_receptor_agonist', 'gsk_inhibitor', 'hcv_inhibitor', 'hdac_inhibitor', 'histamine_receptor_agonist', 'histamine_receptor_antagonist', 'histone_lysine_demethylase_inhibitor', 'histone_lysine_methyltransferase_inhibitor', 'hiv_inhibitor', 'hmgcr_inhibitor', 'hsp_inhibitor', 'igf-1_inhibitor', 'ikk_inhibitor', 'imidazoline_receptor_agonist', 'immunosuppressant', 'insulin_secretagogue', 'insulin_sensitizer', 'integrin_inhibitor', 'jak_inhibitor', 'kit_inhibitor', 'laxative', 'leukotriene_inhibitor', 'leukotriene_receptor_antagonist', 'lipase_inhibitor', 'lipoxygenase_inhibitor', 'lxr_agonist', 'mdm_inhibitor', 'mek_inhibitor', 'membrane_integrity_inhibitor', 'mineralocorticoid_receptor_antagonist', 'monoacylglycerol_lipase_inhibitor', 'monoamine_oxidase_inhibitor', 'monopolar_spindle_1_kinase_inhibitor', 'mtor_inhibitor', 'mucolytic_agent', 'neuropeptide_receptor_antagonist', 'nfkb_inhibitor', 'nicotinic_receptor_agonist', 'nitric_oxide_donor', 'nitric_oxide_production_inhibitor', 'nitric_oxide_synthase_inhibitor', 'norepinephrine_reuptake_inhibitor', 'nrf2_activator', 'opioid_receptor_agonist', 'opioid_receptor_antagonist', 'orexin_receptor_antagonist', 'p38_mapk_inhibitor', 'p-glycoprotein_inhibitor', 'parp_inhibitor', 'pdgfr_inhibitor', 'pdk_inhibitor', 'phosphodiesterase_inhibitor', 'phospholipase_inhibitor', 'pi3k_inhibitor', 'pkc_inhibitor', 'potassium_channel_activator', 'potassium_channel_antagonist', 'ppar_receptor_agonist', 'ppar_receptor_antagonist', 'progesterone_receptor_agonist', 'progesterone_receptor_antagonist', 'prostaglandin_inhibitor', 'prostanoid_receptor_antagonist', 'proteasome_inhibitor', 'protein_kinase_inhibitor', 'protein_phosphatase_inhibitor', 'protein_synthesis_inhibitor', 'protein_tyrosine_kinase_inhibitor', 'radiopaque_medium', 'raf_inhibitor', 'ras_gtpase_inhibitor', 'retinoid_receptor_agonist', 'retinoid_receptor_antagonist', 'rho_associated_kinase_inhibitor', 'ribonucleoside_reductase_inhibitor', 'rna_polymerase_inhibitor', 'serotonin_receptor_agonist', 'serotonin_receptor_antagonist', 'serotonin_reuptake_inhibitor', 'sigma_receptor_agonist', 'sigma_receptor_antagonist', 'smoothened_receptor_antagonist', 'sodium_channel_inhibitor', 'sphingosine_receptor_agonist', 'src_inhibitor', 'steroid', 'syk_inhibitor', 'tachykinin_antagonist', 'tgf-beta_receptor_inhibitor', 'thrombin_inhibitor', 'thymidylate_synthase_inhibitor', 'tlr_agonist', 'tlr_antagonist', 'tnf_inhibitor', 'topoisomerase_inhibitor', 'transient_receptor_potential_channel_antagonist', 'tropomyosin_receptor_kinase_inhibitor', 'trpv_agonist', 'trpv_antagonist', 'tubulin_inhibitor', 'tyrosine_kinase_inhibitor', 'ubiquitin_specific_protease_inhibitor', 'vegfr_inhibitor', 'vitamin_b', 'vitamin_d_receptor_agonist', 'wnt_inhibitor']\nNUM_TARGET = len(TARGET_COL)#206\ndata_dir = '../input/lish-moa/'\ndf = pd.read_csv(data_dir+'/sample_submission.csv')\npublic_id = list(df['sig_id'].values)\ndf_test = pd.read_csv(data_dir+'/test_features.csv')\ntest_id = list(df_test['sig_id'].values)\nprivate_id = list(set(test_id)-set(public_id))\ndf_submit = pd.DataFrame(index = public_id+private_id, columns=TARGET_COL)\ndf_submit.index.name = 'sig_id'\ndf_submit[:] = 0.1 # assign a val to private test set\ndf_predict = pd.read_csv(my_sub)\ndf_submit.loc[df_predict.sig_id,:] = df_predict[TARGET_COL].values\ndf_submit.loc[df_test[df_test.cp_type=='ctl_vehicle'].sig_id]=0\ndf_submit.to_csv('submission.csv',index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the code is not fully optimized due to the time limitation, as we are trying to compete in other competitions now, but we will definitely clean up the code better later."},{"metadata":{},"cell_type":"markdown","source":"Reference:\n\n1. https://www.kaggle.com/thehemen/pytorch-transfer-learning-with-k-folds-by-drug-ids\n2. https://www.kaggle.com/namanj27/new-baseline-pytorch-moa\n3. To be added...(As we definitely read other notebooks as well, will try to add all of the references later)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}