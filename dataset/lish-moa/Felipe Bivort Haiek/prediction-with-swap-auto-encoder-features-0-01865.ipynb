{"cells":[{"metadata":{},"cell_type":"markdown","source":"For more information on how to se the model and how to generate your own DAE wrights visit this post\n\nhttps://www.kaggle.com/c/lish-moa/discussion/195642\n\nIn this version I am inferring on  groupkfold on the experiment clusters I deduced through clustering here \n\nhttps://www.kaggle.com/felipebihaiek/my-2-cents-on-an-accurate-per-experiment-split\n\nYou may also be interested in continued training with the non-scored targets. In the next kenel I implement a function that initializes a network from a past training with the non scored targets.\n\nhttps://www.kaggle.com/felipebihaiek/torch-continued-from-auxiliary-targets-smoothing\n\n"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2020-09-12T10:35:52.096375Z","iopub.status.busy":"2020-09-12T10:35:52.095606Z","iopub.status.idle":"2020-09-12T10:35:52.885827Z","shell.execute_reply":"2020-09-12T10:35:52.885289Z"},"papermill":{"duration":0.817757,"end_time":"2020-09-12T10:35:52.885954","exception":false,"start_time":"2020-09-12T10:35:52.068197","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nsys.path.append('..')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2020-09-12T10:35:52.94117Z","iopub.status.busy":"2020-09-12T10:35:52.940456Z","iopub.status.idle":"2020-09-12T10:35:54.286013Z","shell.execute_reply":"2020-09-12T10:35:54.285455Z"},"papermill":{"duration":1.375673,"end_time":"2020-09-12T10:35:54.286123","exception":false,"start_time":"2020-09-12T10:35:52.91045","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sn\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.pipeline import make_pipeline,make_union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            \n            # Print current column type\n            #print(\"******************************\")\n            #print(\"Column: \",col)\n            #print(\"dtype before: \",props[col].dtype)\n            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(props[col]).all(): \n                NAlist.append(col)\n                props[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = props[col].fillna(0).astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n            \n            # Print new column type\n           # print(\"dtype after: \",props[col].dtype)\n           # print(\"******************************\")\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n    return props, NAlist\n\ndef train_short_form_loader(feature_file,target_file,extra_target_file=None):\n    '''takes the original target and features and creates a train dataset \n    in col long format'''\n\n\n    train_features = pd.read_csv(feature_file)\n\n    train_targets = pd.read_csv(target_file)\n    train_features,_= reduce_mem_usage(train_features)\n    train_targets,_ = reduce_mem_usage(train_targets)\n\n\n    if extra_target_file is not None:\n        extra_targets = pd.read_csv(extra_target_file)\n        extra_targets,_ = reduce_mem_usage(extra_targets)\n        train_targets = pd.merge(train_targets,extra_targets,on ='sig_id')\n        del extra_targets\n\n    targets = train_targets.columns[1:]\n\n    train_melt=train_targets.merge(train_features,how=\"left\",on=\"sig_id\")\n\n\n    del train_features,train_targets\n\n\n    train_melt.set_index(\"sig_id\",inplace=True)\n\n    #train_melt[\"variable\"]= train_melt[\"variable\"].astype('category')\n    train_melt[\"cp_type\"]= train_melt[\"cp_type\"].astype('category')\n    train_melt[\"cp_dose\"]= train_melt[\"cp_dose\"].astype('category')\n\n    return train_melt , targets.to_list()\n\n\n\ndef test_short_form_loader(feature_file):\n    '''takes the original target and features and creates a train dataset \n    in col long format'''\n\n\n    train_features = pd.read_csv(feature_file)\n\n    #train_targets = pd.read_csv(target_file)\n    train_features,_= reduce_mem_usage(train_features)\n    #train_targets,_ = reduce_mem_usage(train_targets)\n\n    train_melt =  train_features.copy()\n    del train_features\n\n\n    train_melt.set_index(\"sig_id\",inplace=True)\n\n    #train_melt[\"variable\"]= train_melt[\"variable\"].astype('category')\n    train_melt[\"cp_type\"]= train_melt[\"cp_type\"].astype('category')\n    train_melt[\"cp_dose\"]= train_melt[\"cp_dose\"].astype('category')\n\n    return train_melt \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_directory = '../input/lish-moa/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train,target_cols = train_short_form_loader(input_directory +'train_features.csv',input_directory+'train_targets_scored.csv')\ntest = test_short_form_loader(input_directory +\"test_features.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in train.columns if col.startswith('g-')]\nCELLS = [col for col in train.columns if col.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.compose import make_column_transformer,ColumnTransformer","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-12T10:36:00.082793Z","iopub.status.busy":"2020-09-12T10:36:00.082133Z","iopub.status.idle":"2020-09-12T10:36:00.087736Z","shell.execute_reply":"2020-09-12T10:36:00.087244Z"},"papermill":{"duration":0.035085,"end_time":"2020-09-12T10:36:00.087837","exception":false,"start_time":"2020-09-12T10:36:00.052752","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    #os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.02481,"end_time":"2020-09-12T10:36:00.405939","exception":false,"start_time":"2020-09-12T10:36:00.381129","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# feature Selection transformers"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator,TransformerMixin\n\nclass CatIntMapper( BaseEstimator, TransformerMixin ):\n    #Class constructor method that takes in a list of values as its argument\n    def __init__(self ,col,dicti):\n        self.col = col\n        self.dicti = dicti\n        \n    def fit(self, X, y = None):\n        \n        return self\n    #Return self nothing else to do here\n    def fit_transform( self, X, y = None  ):\n        \n        assert  X[self.col].isin(self.dicti.keys()).all() \n        \n       \n        return pd.concat([X.drop(self.col,axis=1),X[self.col].map(self.dicti).astype(int).rename(self.col)],axis=1) \n    \n    def transform( self, X):\n        assert  X[self.col].isin(self.dicti.keys()).all() \n        \n        return pd.concat([X.drop(self.col,axis=1),X[self.col].map(self.dicti).astype(int).rename(self.col)],axis=1) \n\nclass NamedOutTWrapper( BaseEstimator, TransformerMixin ):\n    \n    def __init__(self,transformer,columns,inplace=False,prefix='_' ):\n        \n        self.transformer = transformer\n        self.cols = columns\n        self.inplace =  inplace\n        self.prefix = prefix\n        self.transformer_name = self._get_transformer_name()\n        \n    def fit(self, X, y = None):\n            \n        self.transformer =   self.transformer.fit(X[self.cols] , y )\n            \n        return self\n    #Return self nothing else to do here\n    def fit_transform( self, X, y = None  ):\n        \n       \n        \n        transformed_columns = self.transformer.fit_transform(X[self.cols] , y )\n        out=pd.DataFrame(index=X.index)\n        \n       \n        if self.inplace:\n            out = X[self.cols]\n            out[self.cols] = transformed_columns\n            \n            return pd.concat([X.drop(self.cols,axis=1),out],axis=1)\n        else:\n           \n            for i,values in enumerate(transformed_columns.transpose()):\n            \n                out[ self.transformer_name + self.prefix + str(i)] = values\n        \n       \n        \n            return   pd.concat([X,out],axis=1)\n    \n    def transform( self, X):\n        \n        transformed_columns = self.transformer.transform(X[self.cols]  )\n        \n        out=pd.DataFrame(index=X.index)\n        \n        if self.inplace:\n            out = X[self.cols]\n            out[self.cols] = transformed_columns\n            \n            return pd.concat([X.drop(self.cols,axis=1),out],axis=1)\n        else:\n            for i,values in enumerate(transformed_columns.transpose()):\n\n                out[ self.transformer_name + self.prefix + str(i)] = values\n\n             \n        return   pd.concat([X,out],axis=1)\n            \n    \n    def _get_transformer_name(self):\n        return str(self.transformer.__class__).split('.')[-1][0:-2]\n\n\nclass IdentityTransformer:\n    '''Duummy_tansformer as a filler'''\n    def __init__(self ):\n        pass\n    def fit(self, X, y = None):\n        \n        return self\n    #Return self nothing else to do here\n    def fit_transform( self, X, y = None  ):\n        \n        return  X\n      \n    \n    def transform( self, X):\n       \n        return  X    \n\nclass SuppressControls( BaseEstimator, TransformerMixin ):\n    #Class constructor method that takes in a list of values as its argument\n    def __init__(self ):\n        pass\n    def fit(self, X, y = None):\n        return self\n    #Return self nothing else to do here\n    def fit_transform( self, X, y = None  ):\n        \n      \n        \n        return   X.loc[X['cp_type']!='ctl_vehicle'].drop('cp_type', axis=1) \n    \n    def transform( self, X):\n       \n       \n        return    X.loc[X['cp_type']!='ctl_vehicle'].drop('cp_type', axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.027852,"end_time":"2020-09-12T10:36:02.17222","exception":false,"start_time":"2020-09-12T10:36:02.144368","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# CV folds"},{"metadata":{"execution":{"iopub.execute_input":"2020-09-12T10:36:02.23479Z","iopub.status.busy":"2020-09-12T10:36:02.233591Z","iopub.status.idle":"2020-09-12T10:36:04.852035Z","shell.execute_reply":"2020-09-12T10:36:04.850755Z"},"papermill":{"duration":2.652257,"end_time":"2020-09-12T10:36:04.852139","exception":false,"start_time":"2020-09-12T10:36:02.199882","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def multifold_indexer(train,target_columns,n_splits=10,random_state=12347,**kwargs):\n    folds = train.copy()\n\n    mskf = MultilabelStratifiedKFold(n_splits=n_splits,random_state=random_state,**kwargs)\n    folds[ 'kfold']=0\n    for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=train[target_columns])):\n        folds.iloc[v_idx,-1] = int(f)\n\n    folds['kfold'] = folds['kfold'].astype(int)\n    return folds\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.028651,"end_time":"2020-09-12T10:36:04.977763","exception":false,"start_time":"2020-09-12T10:36:04.949112","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Dataset Classes"},{"metadata":{"execution":{"iopub.execute_input":"2020-09-12T10:36:05.047595Z","iopub.status.busy":"2020-09-12T10:36:05.046831Z","iopub.status.idle":"2020-09-12T10:36:05.049694Z","shell.execute_reply":"2020-09-12T10:36:05.049265Z"},"papermill":{"duration":0.042195,"end_time":"2020-09-12T10:36:05.049792","exception":false,"start_time":"2020-09-12T10:36:05.007597","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \n\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    ","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-12T10:36:05.047595Z","iopub.status.busy":"2020-09-12T10:36:05.046831Z","iopub.status.idle":"2020-09-12T10:36:05.049694Z","shell.execute_reply":"2020-09-12T10:36:05.049265Z"},"papermill":{"duration":0.042195,"end_time":"2020-09-12T10:36:05.049792","exception":false,"start_time":"2020-09-12T10:36:05.007597","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# DAE Model","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-12T10:36:05.047595Z","iopub.status.busy":"2020-09-12T10:36:05.046831Z","iopub.status.idle":"2020-09-12T10:36:05.049694Z","shell.execute_reply":"2020-09-12T10:36:05.049265Z"},"papermill":{"duration":0.042195,"end_time":"2020-09-12T10:36:05.049792","exception":false,"start_time":"2020-09-12T10:36:05.007597","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class DAE_Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size=1100,hidden_size2=1300):\n        super(DAE_Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        #self.dropout1 = nn.Dropout(drop_rate1)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n      #  self.dropout2 = nn.Dropout(drop_rate2)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size2))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size2)\n        #self.dropout3 = nn.Dropout(drop_rate2)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size2, hidden_size))\n        \n      #  self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n      #  self.dropout4 = nn.Dropout(drop_rate3)\n        self.dense4 = nn.utils.weight_norm(nn.Linear(hidden_size, num_features))\n\n        \n    def forward(self, x,mode='DAE'):\n      #  x = self.batch_norm1(x)\n       # x1 = self.dropout1(x1)\n        x1 = F.relu(self.dense1(x))\n        \n            \n        x2 = self.batch_norm2(x1)\n      #  x = self.dropout2(x)\n        x2 = F.relu(self.dense2(x2))\n        \n        x3 = self.batch_norm3(x2)\n      \n        x3 = F.relu(self.dense3(x3))\n        \n        out = self.dense4(x3)\n        \n        if mode == 'DAE':\n            return out\n        else:\n            return x1,x2,x3\n    \n#     def forwardh2(self, x):\n#       #  x = self.batch_norm1(x)\n#        # x1 = self.dropout1(x1)\n#         x = F.relu(self.dense1(x))\n        \n#         return x\n    \n#     def forwardh3(self, x):\n#       #  x = self.batch_norm1(x)\n#        # x1 = self.dropout1(x1)\n#         x = F.relu(self.dense1(x))\n        \n#         return x","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-12T10:36:05.047595Z","iopub.status.busy":"2020-09-12T10:36:05.046831Z","iopub.status.idle":"2020-09-12T10:36:05.049694Z","shell.execute_reply":"2020-09-12T10:36:05.049265Z"},"papermill":{"duration":0.042195,"end_time":"2020-09-12T10:36:05.049792","exception":false,"start_time":"2020-09-12T10:36:05.007597","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.pipeline import make_pipeline,make_union\n\n\n\nmap_controls = CatIntMapper('cp_type',{'ctl_vehicle': 0, 'trt_cp': 1})    \n\nmap_dose = CatIntMapper('cp_dose',{'D1': 1, 'D2': 0})    \nmap_time = CatIntMapper('cp_time',{24: 0, 48: 1, 72: 2})    \n\n\ntrain = pd.read_csv(f'{input_directory}/train_features.csv')\n\nGENES = [col for col in train.columns if col.startswith('g-')]\nCELLS = [col for col in train.columns if col.startswith('c-')]\n\nGENES\n\nRankg_g_tansform =  NamedOutTWrapper( QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\"),columns= GENES+CELLS,inplace=True)\n\nPCA_g_tansform =  NamedOutTWrapper(PCA(20),columns= GENES,prefix ='_g' )\n\nPCA_c_tansform =  NamedOutTWrapper(PCA(20),columns= CELLS,prefix ='_c' )\n\n#transformers_list=[map_controls,map_dose,map_time,PCA_g_tansform,PCA_c_tansform,Rankg_g_tansform]\n\n\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass ColumnDropper( BaseEstimator, TransformerMixin ):\n    #Class Constructor \n    def __init__( self, cols ):\n        self.cols=cols\n    #Return self nothing else to do here    \n    def fit( self, X, y = None ):\n        return self \n    \n    #Method that describes what we need this transformer to do\n    def transform( self, X, y = None ):\n\n        return X.drop(self.cols,axis=1)\n\n\n\nCatDropper =ColumnDropper(cols=['cp_type','cp_time','cp_dose'])\n\ntransformers_list=[map_controls,map_dose,map_time,Rankg_g_tansform,CatDropper]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-12T10:36:05.047595Z","iopub.status.busy":"2020-09-12T10:36:05.046831Z","iopub.status.idle":"2020-09-12T10:36:05.049694Z","shell.execute_reply":"2020-09-12T10:36:05.049265Z"},"papermill":{"duration":0.042195,"end_time":"2020-09-12T10:36:05.049792","exception":false,"start_time":"2020-09-12T10:36:05.007597","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"exp_name = 'test_DAE_0.2_all_together'\n\ndef run_inference(X_train,y_train,X_valid,y_valid,X_test,fold, seed,inference_only=False,**kwargs):\n    seed_everything(seed)\n    if not  inference_only:\n        train_dataset = MoADataset(X_train, y_train)\n        valid_dataset = MoADataset(X_valid, y_valid)\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n    testdataset = TestDataset(X_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = DAE_Model(\n        num_features= X_train.shape[1] ,\n        num_targets=  X_train.shape[1],\n       # hidden_size=hidden_size,\n        **kwargs\n    )\n    \n    \n    model.load_state_dict(torch.load( f\"../input/swap-dae-noise0-2/FOLD{fold}_{exp_name}.pth\",map_location=torch.device('cpu')))#map_location='cuda:0'))#,freeze_first_layer=True)\n    \n    model.to(DEVICE)\n    \n    if not  inference_only:\n        oof = inference_infer_features_fn(model, validloader, DEVICE)    \n    else:\n        oof= 0\n    \n\n    predictions = infer_features_fn(model, testloader, DEVICE)\n    \n    predictions = predictions\n    \n    return oof, predictions\n\ntransformers_list\n\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 1000\nBATCH_SIZE = 640\nLEARNING_RATE = 2e-3\nWEIGHT_DECAY = 1e-8\nNFOLDS = 10\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\nGAMMA=0.5\nFACTOR=0.75\n#num_features=len(feature_cols)\n#num_targets=len(target_cols)\nhidden_size=1100\nhidden_size2=1300\nPATIENCE=10\nTHRESHOLD = 5e-3\n\n\ndef infer_features_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs,mode='get_features')\n        \n#         print(len(outputs))\n        \n#         for i in range(len(outputs)):\n#             print(outputs[i].shape)\n            \n#         print(torch.cat(outputs,axis=1).shape)\n        \n        preds.append(torch.cat(outputs,axis=1).detach().cpu().numpy())\n        \n        \n        \n    preds = np.concatenate(preds)\n    \n    return preds\n   \n ","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-12T10:36:05.047595Z","iopub.status.busy":"2020-09-12T10:36:05.046831Z","iopub.status.idle":"2020-09-12T10:36:05.049694Z","shell.execute_reply":"2020-09-12T10:36:05.049265Z"},"papermill":{"duration":0.042195,"end_time":"2020-09-12T10:36:05.049792","exception":false,"start_time":"2020-09-12T10:36:05.007597","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"   \n\n#SEED = [0,12347,565657,123123,78591]\nSEED = [0]\ntrain,target_cols = train_short_form_loader('../input/lish-moa/train_features.csv','../input/lish-moa/train_targets_scored.csv')\ntest = test_short_form_loader(\"../input/lish-moa/test_features.csv\")\n\ntrain = pd.concat([train,test])\ntrain[target_cols]= train[target_cols].fillna(0)\ntest = train.copy()\n#pipeline_test = make_pipeline(*transformers_list)\n#pipeline_test.fit(train)\n#test = pipeline_test.transform(test)\n    \n\n\n\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    \n    train = multifold_indexer(train,target_cols,n_splits=NFOLDS)\n    \n    #print(test_.head())\n    for fold in range(NFOLDS):\n        \n     \n        #pipeline_val = pk.load(open(f\"../input/tmultiv5rnkgpcag50smth1e3unpropertrafo/preprocessing_SEED{seed}_FOLD{fold}.pth\",'rb'))\n        \n        \n        #trn_idx = train[train['kfold'] != fold].reset_index().index\n        #val_idx = train[train['kfold'] == fold].reset_index().index\n    \n        train_df = train[train['kfold'] != fold]#.reset_index(drop=True)\n        valid_df = train[train['kfold'] == fold]#.reset_index(drop=True)\n        \n       # print(len(train_df))\n        #print(len(valid_df))\n        \n        feature_cols = [col  for col in train_df.columns if not (col in target_cols+['kfold'])]\n        \n        #print(feature_cols)\n        \n        pipeline_val = make_pipeline(*transformers_list)\n        \n        X_train, y_train  = train_df[feature_cols], train_df[target_cols]\n        X_valid, y_valid =  valid_df[feature_cols], valid_df[target_cols].values\n        \n      \n       \n        #X_train = pipeline_val.fit_transform(X_train,y_train)\n        X_train = pipeline_val.fit_transform(X_train)\n        \n        #feature_cols = [col  for col in X_train.columns if not (col in target_cols+['kfold'])]\n        \n        X_train = X_train.values\n        \n        \n        X_valid = pipeline_val.transform(X_valid)\n        \n        \n        \n        \n        valid_index = X_valid.index\n        X_valid = X_valid.values\n        \n        y_train = y_train.values\n        \n       \n        \n        X_test = test[feature_cols]\n            \n        \n        X_test = pipeline_val.transform(X_test).values\n        \n        #X_test = X_test.values\n        \n        \n        pred_ = run_inference(X_train,y_train,X_valid,y_valid,X_test,fold, seed,inference_only=True)    \n        \n        break","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-12T10:36:05.047595Z","iopub.status.busy":"2020-09-12T10:36:05.046831Z","iopub.status.idle":"2020-09-12T10:36:05.049694Z","shell.execute_reply":"2020-09-12T10:36:05.049265Z"},"papermill":{"duration":0.042195,"end_time":"2020-09-12T10:36:05.049792","exception":false,"start_time":"2020-09-12T10:36:05.007597","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"pred_[1].shape","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-12T10:36:05.047595Z","iopub.status.busy":"2020-09-12T10:36:05.046831Z","iopub.status.idle":"2020-09-12T10:36:05.049694Z","shell.execute_reply":"2020-09-12T10:36:05.049265Z"},"papermill":{"duration":0.042195,"end_time":"2020-09-12T10:36:05.049792","exception":false,"start_time":"2020-09-12T10:36:05.007597","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"transformed_features = pd.DataFrame(pred_[1],index=test.index)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.028912,"end_time":"2020-09-12T10:36:05.183929","exception":false,"start_time":"2020-09-12T10:36:05.155017","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# DAE Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DAE_Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size=1100,hidden_size2=1300):\n        super(DAE_Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        #self.dropout1 = nn.Dropout(drop_rate1)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n      #  self.dropout2 = nn.Dropout(drop_rate2)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size2))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size2)\n        #self.dropout3 = nn.Dropout(drop_rate2)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size2, hidden_size))\n        \n      #  self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n      #  self.dropout4 = nn.Dropout(drop_rate3)\n        self.dense4 = nn.utils.weight_norm(nn.Linear(hidden_size, num_features))\n\n        \n    def forward(self, x,mode='DAE'):\n      #  x = self.batch_norm1(x)\n       # x1 = self.dropout1(x1)\n        x1 = F.relu(self.dense1(x))\n        \n            \n        x2 = self.batch_norm2(x1)\n      #  x = self.dropout2(x)\n        x2 = F.relu(self.dense2(x2))\n        \n        x3 = self.batch_norm3(x2)\n      \n        x3 = F.relu(self.dense3(x3))\n        \n        out = self.dense4(x3)\n        \n        if mode == 'DAE':\n            return out\n        else:\n            return x1,x2,x3\n    \n#     def forwardh2(self, x):\n#       #  x = self.batch_norm1(x)\n#        # x1 = self.dropout1(x1)\n#         x = F.relu(self.dense1(x))\n        \n#         return x\n    \n#     def forwardh3(self, x):\n#       #  x = self.batch_norm1(x)\n#        # x1 = self.dropout1(x1)\n#         x = F.relu(self.dense1(x))\n        \n#         return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.pipeline import make_pipeline,make_union\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmap_controls = CatIntMapper('cp_type',{'ctl_vehicle': 0, 'trt_cp': 1})    \n\nmap_dose = CatIntMapper('cp_dose',{'D1': 1, 'D2': 0})    \nmap_time = CatIntMapper('cp_time',{24: 0, 48: 1, 72: 2})    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(f'{input_directory}/train_features.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GENES = [col for col in train.columns if col.startswith('g-')]\nCELLS = [col for col in train.columns if col.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Rankg_g_tansform =  NamedOutTWrapper( QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\"),columns= GENES+CELLS,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PCA_g_tansform =  NamedOutTWrapper(PCA(20),columns= GENES,prefix ='_g' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PCA_c_tansform =  NamedOutTWrapper(PCA(20),columns= CELLS,prefix ='_c' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass ColumnDropper( BaseEstimator, TransformerMixin ):\n    #Class Constructor \n    def __init__( self, cols ):\n        self.cols=cols\n    #Return self nothing else to do here    \n    def fit( self, X, y = None ):\n        return self \n    \n    #Method that describes what we need this transformer to do\n    def transform( self, X, y = None ):\n\n        return X.drop(self.cols,axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nCatDropper =ColumnDropper(cols=['cp_type','cp_time','cp_dose'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformers_list=[map_controls,map_dose,map_time,Rankg_g_tansform,CatDropper]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exp_name = 'test_DAE_0.2_all_together'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_inference(X_train,y_train,X_valid,y_valid,X_test,fold, seed,inference_only=False,**kwargs):\n    seed_everything(seed)\n    if not  inference_only:\n        train_dataset = MoADataset(X_train, y_train)\n        valid_dataset = MoADataset(X_valid, y_valid)\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n    testdataset = TestDataset(X_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = DAE_Model(\n        num_features= X_train.shape[1] ,\n        num_targets=  X_train.shape[1],\n       # hidden_size=hidden_size,\n        **kwargs\n    )\n    \n    \n    model.load_state_dict(torch.load( f\"../input/swap-dae-noise0-2/FOLD{fold}_{exp_name}.pth\",map_location=torch.device('cpu')))#map_location='cuda:0'))#,freeze_first_layer=True)\n    \n    model.to(DEVICE)\n    \n    if not  inference_only:\n        oof = inference_infer_features_fn(model, validloader, DEVICE)    \n    else:\n        oof= 0\n    \n\n    predictions = infer_features_fn(model, testloader, DEVICE)\n    \n    predictions = predictions\n    \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformers_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 1000\nBATCH_SIZE = 640\nLEARNING_RATE = 2e-3\nWEIGHT_DECAY = 1e-8\nNFOLDS = 10\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\nGAMMA=0.5\nFACTOR=0.75\n#num_features=len(feature_cols)\n#num_targets=len(target_cols)\nhidden_size=1100\nhidden_size2=1300\nPATIENCE=10\nTHRESHOLD = 5e-3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef infer_features_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs,mode='get_features')\n        \n#         print(len(outputs))\n        \n#         for i in range(len(outputs)):\n#             print(outputs[i].shape)\n            \n#         print(torch.cat(outputs,axis=1).shape)\n        \n        preds.append(torch.cat(outputs,axis=1).detach().cpu().numpy())\n        \n        \n        \n    preds = np.concatenate(preds)\n    \n    return preds\n   \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SEED = [0,12347,565657,123123,78591]\nSEED = [0]\ntrain,target_cols = train_short_form_loader('../input/lish-moa/train_features.csv','../input/lish-moa/train_targets_scored.csv')\ntest = test_short_form_loader(\"../input/lish-moa/test_features.csv\")\n\ntrain = pd.concat([train,test])\ntrain[target_cols]= train[target_cols].fillna(0)\ntest = train.copy()\n#pipeline_test = make_pipeline(*transformers_list)\n#pipeline_test.fit(train)\n#test = pipeline_test.transform(test)\n    \n\n\n\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    \n    train = multifold_indexer(train,target_cols,n_splits=NFOLDS)\n    \n    #print(test_.head())\n    for fold in range(NFOLDS):\n        \n     \n        #pipeline_val = pk.load(open(f\"../input/tmultiv5rnkgpcag50smth1e3unpropertrafo/preprocessing_SEED{seed}_FOLD{fold}.pth\",'rb'))\n        \n        \n        #trn_idx = train[train['kfold'] != fold].reset_index().index\n        #val_idx = train[train['kfold'] == fold].reset_index().index\n    \n        train_df = train[train['kfold'] != fold]#.reset_index(drop=True)\n        valid_df = train[train['kfold'] == fold]#.reset_index(drop=True)\n        \n       # print(len(train_df))\n        #print(len(valid_df))\n        \n        feature_cols = [col  for col in train_df.columns if not (col in target_cols+['kfold'])]\n        \n        #print(feature_cols)\n        \n        pipeline_val = make_pipeline(*transformers_list)\n        \n        X_train, y_train  = train_df[feature_cols], train_df[target_cols]\n        X_valid, y_valid =  valid_df[feature_cols], valid_df[target_cols].values\n        \n      \n       \n        #X_train = pipeline_val.fit_transform(X_train,y_train)\n        X_train = pipeline_val.fit_transform(X_train)\n        \n        #feature_cols = [col  for col in X_train.columns if not (col in target_cols+['kfold'])]\n        \n        X_train = X_train.values\n        \n        \n        X_valid = pipeline_val.transform(X_valid)\n        \n        \n        \n        \n        valid_index = X_valid.index\n        X_valid = X_valid.values\n        \n        y_train = y_train.values\n        \n       \n        \n        X_test = test[feature_cols]\n            \n        \n        X_test = pipeline_val.transform(X_test).values\n        \n        #X_test = X_test.values\n        \n        \n        pred_ = run_inference(X_train,y_train,X_valid,y_valid,X_test,fold, seed,inference_only=True)    \n        \n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_[1].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformed_features = pd.DataFrame(pred_[1],index=test.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sn.pairplot(transformed_features.iloc[:,0:13])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sn.pairplot(transformed_features.iloc[:,110:130])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformed_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformed_features.columns = [str(i) for i in range(len(transformed_features.columns))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Saving features generated with DAE and analizing the output\n"},{"metadata":{"trusted":true},"cell_type":"code","source":" transformed_features.reset_index().to_feather('./features_0.2_altogether.fth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = transformed_features.corr().abs()\n\nsol = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n                  .stack()\n                  .sort_values(ascending=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(sol > 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multin = sol.loc[sol > 0.5].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multin.get_level_values(0).unique()","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"multin.get_level_values(0).unique().tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set(multin.get_level_values(0).unique().tolist() + multin.get_level_values(1).unique().tolist()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_cors = transformed_features[set(multin.get_level_values(0).unique().tolist() + multin.get_level_values(1).unique().tolist())].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_dims = (20, 40)\nfig, ax = plt.subplots(figsize=fig_dims)\nsn.heatmap(top_cors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparation for Inference model\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    #os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transformer to add auto-encoder features"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DaeAdder( BaseEstimator, TransformerMixin ):\n    def __init__(self,filename):\n        \n        self.filename=filename\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def fit_transform(self,X,y=None):\n        \n        Dae_features = pd.read_feather(self.filename).set_index('sig_id')\n        \n        return X.merge(Dae_features,how='left', on='sig_id')\n        \n        \n    def transform(self,X):\n        \n        Dae_features = pd.read_feather(self.filename).set_index('sig_id')\n        \n        return X.merge(Dae_features,how='left', on='sig_id')\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train,target_cols = train_short_form_loader('../input/lish-moa/train_features.csv','../input/lish-moa/train_targets_scored.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Dae0_2 =DaeAdder(filename='features_0.2_altogether.fth')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = Dae0_2.fit_transform(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nclass SupressControls( BaseEstimator, TransformerMixin ):\n    #Class constructor method that takes in a list of values as its argument\n    def __init__(self ):\n        pass\n    def fit(self, X, y = None):\n        return self\n    #Return self nothing else to do here\n    def fit_transform( self, X, y = None  ):\n        \n        X = X[X['cp_type']!=0]\n        X = X.drop('cp_type', axis=1)\n        return X \n    \n    def transform( self, X):\n        X = X[X['cp_type']!=0]\n        X = X.drop('cp_type', axis=1)\n        return X\n    \nclass CatIntMapper( BaseEstimator, TransformerMixin ):\n    #Class constructor method that takes in a list of values as its argument\n    def __init__(self ,col,dicti):\n        self.col = col\n        self.dicti = dicti\n        \n    def fit(self, X, y = None):\n        \n        return self\n    #Return self nothing else to do here\n    def fit_transform( self, X, y = None  ):\n        \n        assert  X[self.col].isin(self.dicti.keys()).all() \n        \n       \n        return pd.concat([X.drop(self.col,axis=1),X[self.col].map(self.dicti).astype(int).rename(self.col)],axis=1) \n    \n    def transform( self, X):\n        assert  X[self.col].isin(self.dicti.keys()).all() \n        \n        return pd.concat([X.drop(self.col,axis=1),X[self.col].map(self.dicti).astype(int).rename(self.col)],axis=1) \n\nclass NamedOutTWrapper( BaseEstimator, TransformerMixin ):\n    \n    def __init__(self,transformer,columns,inplace=False,prefix='_' ):\n        \n        self.transformer = transformer\n        self.cols = columns\n        self.inplace =  inplace\n        self.prefix = prefix\n        self.transformer_name = self._get_transformer_name()\n        \n    def fit(self, X, y = None):\n            \n        self.transformer =   self.transformer.fit(X[self.cols] , y )\n            \n        return self\n    #Return self nothing else to do here\n    def fit_transform( self, X, y = None  ):\n        \n       \n        \n        transformed_columns = self.transformer.fit_transform(X[self.cols] , y )\n        out=pd.DataFrame(index=X.index)\n        \n       \n        if self.inplace:\n            out = X[self.cols].copy()\n            out[self.cols] = transformed_columns\n            \n            return pd.concat([X.drop(self.cols,axis=1),out],axis=1)\n        else:\n           \n            for i,values in enumerate(transformed_columns.transpose()):\n            \n                out[ self.transformer_name + self.prefix + str(i)] = values\n        \n       \n        \n            return   pd.concat([X,out],axis=1)\n    \n    def transform( self, X):\n        \n        transformed_columns = self.transformer.transform(X[self.cols]  )\n        \n        out=pd.DataFrame(index=X.index)\n        \n        if self.inplace:\n            out = X[self.cols].copy()\n            out[self.cols] = transformed_columns\n            \n            return pd.concat([X.drop(self.cols,axis=1),out],axis=1)\n        else:\n            for i,values in enumerate(transformed_columns.transpose()):\n\n                out[ self.transformer_name + self.prefix + str(i)] = values\n\n             \n        return   pd.concat([X,out],axis=1)\n            \n    \n    def _get_transformer_name(self):\n        return str(self.transformer.__class__).split('.')[-1][0:-2]\n\n\nclass IdentityTransformer:\n    '''Duummy_tansformer as a filler'''\n    def __init__(self ):\n        pass\n    def fit(self, X, y = None):\n        \n        return self\n    #Return self nothing else to do here\n    def fit_transform( self, X, y = None  ):\n        \n        return  X\n      \n    \n    def transform( self, X):\n       \n        return  X    \n\nclass SuppressControls( BaseEstimator, TransformerMixin ):\n    #Class constructor method that takes in a list of values as its argument\n    def __init__(self ):\n        pass\n    def fit(self, X, y = None):\n        return self\n    #Return self nothing else to do here\n    def fit_transform( self, X, y = None  ):\n        \n      \n        \n        return   X.loc[X['cp_type']=='trt_cp'].drop('cp_type', axis=1) \n    \n    def transform( self, X):\n       \n       \n        return    X.loc[X['cp_type']=='trt_cp'].drop('cp_type', axis=1)\n\n\nclass ColumnDropper( BaseEstimator, TransformerMixin ):\n    #Class Constructor \n    def __init__( self, cols ):\n        self.cols=cols\n    #Return self nothing else to do here    \n    def fit( self, X, y = None ):\n        return self \n    \n    #Method that describes what we need this transformer to do\n    def transform( self, X, y = None ):\n\n        return X.drop(self.cols,axis=1)\n\n\nfrom sklearn.base import BaseEstimator,TransformerMixin\n#Custom transformer that breaks dates column into year, month and day into separate columns and\n#converts certain features to binary \nclass VarianceFilter( BaseEstimator, TransformerMixin ):\n    def __init__(self,threshold):\n        self.threshold = threshold\n    def fit(self,X,y=None):\n        mask = X.var()<= self.threshold\n        self.drop_cols = set([ col for val,col in zip(mask,X.columns) if val])\n        self.drop_cols.discard('cp_type')\n        return self\n    def transform(self,X):\n        \n        return X.drop(self.drop_cols,axis=1)\n\n\ndef apply_pipe_together(pipeline,train,test):\n    #@add warning when intesection is not the whole\n    data = pd.concat([train,test])\n\n    data = pipeline.fit_transform(data)\n    \n    train = data.loc[data.index.intersection(train.index)]\n    test = data.loc[data.index.intersection(test.index)]\n    \n    return pipeline,train,test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmap_controls = CatIntMapper('cp_type',{'ctl_vehicle': 0, 'trt_cp': 1})    \n\nmap_dose = CatIntMapper('cp_dose',{'D1': 1, 'D2': 0})    \nmap_time = CatIntMapper('cp_time',{24: 0, 48: 1, 72: 2})    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Groupkfold based on cluster-id"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GroupKFold\ndef groupkfold_indexer(train,target_columns,n_splits=10,random_state=12347,**kwargs):\n    #@alternatively use groushufflesplit to obtein different groups\n    \n    \n    folds = train.copy()\n\n    groups =pd.read_csv('../input/inferreddrugexperimentclustermoa/aggclus4_results.csv',index_col='sig_id')['aggclus1-clusters']\n    \n    gkf = GroupKFold(n_splits=n_splits)\n    folds[ 'kfold']=0\n    for f, (t_idx, v_idx) in enumerate(gkf.split(X=train.sample(len(train),random_state=random_state), y=train[target_columns].sample(len(train),random_state=random_state),groups= groups.loc[train.index].sample(len(train),random_state=random_state) )):\n        folds.iloc[v_idx,-1] = int(f)\n\n    folds['kfold'] = folds['kfold'].astype(int)\n    return folds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size1=388,hidden_size2=512,drop_rate1=0.0,drop_rate2=0.3,drop_rate3=0.3):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(drop_rate1)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size1))\n\n        self.batch_norm2 = nn.BatchNorm1d(hidden_size1)\n        self.dropout2 = nn.Dropout(drop_rate2)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size1, hidden_size2))\n\n        self.batch_norm3 = nn.BatchNorm1d(hidden_size2)\n        self.dropout3 = nn.Dropout(drop_rate3)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size2, num_targets))\n\n\n    def forward(self, x):\n        \n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-12T10:36:05.121448Z","iopub.status.busy":"2020-09-12T10:36:05.120667Z","iopub.status.idle":"2020-09-12T10:36:05.123455Z","shell.execute_reply":"2020-09-12T10:36:05.123057Z"},"papermill":{"duration":0.044641,"end_time":"2020-09-12T10:36:05.123539","exception":false,"start_time":"2020-09-12T10:36:05.078898","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n#         print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        if not  scheduler.__class__ ==  torch.optim.lr_scheduler.ReduceLROnPlateau:\n            scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, scheduler, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    if scheduler.__class__ ==  torch.optim.lr_scheduler.ReduceLROnPlateau:\n        scheduler.step(final_loss)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n   \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(X_train,y_train,X_valid,y_valid,X_test,fold, seed,verbose=False,**kwargs):\n    \n    seed_everything(seed)\n    \n   \n    \n    train_dataset = MoADataset(X_train, y_train)\n    valid_dataset = MoADataset(X_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features= X_train.shape[1] ,\n        num_targets=  y_train.shape[1],hidden_size1=hidden_size1,hidden_size2=hidden_size2,\n       **kwargs\n    )\n    \n    model.to(DEVICE)\n    \n    #initialize_from_past_model(model, f\"../results/original_torch_moa_smoothed_lrplateau_5_folds_AUX_SEED{seed}_FOLD{fold}.pth\")#,freeze_first_layer=True)\n    \n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    #scheduler = optim.lr_scheduler.OneCycleLR(optimizer, pct_start=0.1, div_factor=1e3, \n     #                                         max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=3)\n    \n    loss_val = nn.BCEWithLogitsLoss()\n    #loss_tr =  nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    #todo el guardado de los resultados se puede mover a kfold que si tiene info de los indices\n    #oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    \n    \n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n        if verbose:\n            print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model,scheduler, loss_val, validloader, DEVICE)\n        if verbose:\n            print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof = valid_preds\n        \n        \n        \n       #     torch.save(model.state_dict(), f\"../results/{exp_name}_SEED{seed}_FOLD{fold}.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    torch.save(model.state_dict(), f\"{exp_name}_SEED{seed}_FOLD{fold}.pth\")\n    #--------------------- PREDICTION---------------------\n   \n    testdataset = TestDataset(X_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n#     model = Model(\n#          num_features= X_train.shape[1] ,\n#         num_targets=  y_train.shape[1],\n#         hidden_size=hidden_size,**kwargs\n#     )\n    \n#     model.load_state_dict(torch.load(f\"../results/FOLD{fold}_{exp_name}.pth\"))\n   # model.to(DEVICE)\n    \n    #predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    \n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#params for one cycle schedule\nDEVICE =  torch.device('cuda:0')\nEPOCHS = 70\nBATCH_SIZE = 1024\nLEARNING_RATE = 6e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 7              #<-- Update\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\n#num_features=len(feature_cols)\n#num_targets=len(target_cols)\n\nhidden_size1=2048\n\nhidden_size2=2048","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## defining transformations"},{"metadata":{"trusted":true},"cell_type":"code","source":"transformers_list=[map_dose,map_time,Dae0_2,SuppressControls()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#params ={}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(folds,target_cols,test,transformers_list,NFOLDS, seed,verbose=False,**kwargs):\n    \n    \n    train = folds\n    test_ = test\n    \n    #oof = np.zeros((len(folds), len(target_cols)))\n    oof = train[target_cols].copy()\n    oof = oof*0\n    predictions = pd.DataFrame(0,columns=target_cols,index=test.index)\n    \n    #print(test_.head())\n    for fold in range(NFOLDS):\n        \n        #trn_idx = train[train['kfold'] != fold].reset_index().index\n        #val_idx = train[train['kfold'] == fold].reset_index().index\n    \n        train_df = train[train['kfold'] != fold]#.reset_index(drop=True)\n        valid_df = train[train['kfold'] == fold]#.reset_index(drop=True)\n        \n       # print(len(train_df))\n        #print(len(valid_df))\n        \n        feature_cols = [col  for col in train_df.columns if not (col in target_cols+['kfold'])]\n        \n        #print(feature_cols)\n        \n        pipeline_val = make_pipeline(*transformers_list)\n        \n        X_train, y_train  = train_df[feature_cols], train_df[target_cols]\n        X_valid, y_valid =  valid_df[feature_cols], valid_df[target_cols].values\n        \n      \n       \n        X_train = pipeline_val.fit_transform(X_train,y_train)\n        feature_cols = [col  for col in X_train.columns if not (col in target_cols+['kfold'])]\n        \n        X_train = X_train.values\n        \n        \n        X_valid = pipeline_val.transform(X_valid)\n        valid_index = X_valid.index\n        X_valid = X_valid.values\n        \n        y_train = y_train.values\n        \n        X_test = pipeline_val.transform(test_)\n        test_index = X_test.index\n        X_test = X_test[feature_cols].values\n            \n        oof_, pred_ = run_training(X_train,y_train,X_valid,y_valid,X_test,fold, seed,verbose,**kwargs)\n        \n#         print(X_valid.shape)\n#         print(oof_.shape)\n#         print( oof.loc[valid_index].head())\n        \n        oof.loc[valid_index] = oof_\n        \n        \n        \n        predictions.loc[test_index] += pred_ / NFOLDS\n        \n        \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params={}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## training and final prediction"},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"# Averaging on multiple SEEDS\n#SEED = [0,12347,565657]\n#SEED = [0,12347,565657,123123,78591]\nSEED = [0]\ntrain,target_cols = train_short_form_loader('../input/lish-moa/train_features.csv','../input/lish-moa/train_targets_scored.csv')\ntest = test_short_form_loader(\"../input/lish-moa/test_features.csv\")\n\n\npipeline_test = make_pipeline(*transformers_list)\npipeline_test,train , test = apply_pipe_together(pipeline_test,train,test)\n#pipeline_test.fit(train)\n#test = pipeline_test.transform(test)\n#suppresor = SupressControls()\n#train = suppresor.fit_transform(train)\n#test = suppresor.transform(test)\ntransformers_list=[IdentityTransformer()]\n\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n   \n    folds = groupkfold_indexer(train,target_cols,n_splits=NFOLDS)\n    oof_, predictions_ = run_k_fold(folds,target_cols,test,transformers_list,NFOLDS, seed,verbose=True,**params)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)\n\n#train[target_cols] = oof\ntest[target_cols] = predictions\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#valid_results = train.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n#valid_results\n\ny_true = train[target_cols].values\ny_pred = oof\n\nscore = 0\nfor i in range(len(target_cols)):\n   # print(log_loss(y_true[:, i], y_pred[:, i])/ len(target_cols))\n    score_ = log_loss(y_true[:, i], y_pred.iloc[:, i],labels=[0,1])\n    #if score_ > 0.02:\n     #   print(score_)\n    score +=( score_ / len(target_cols))\n    \nprint(\"CV log_loss: \", score)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train,target_cols = train_short_form_loader('../input/lish-moa/train_features.csv','../input/lish-moa/train_targets_scored.csv')\ny_true = train[target_cols].values\n\ny_pred = train[target_cols].copy()\ny_pred[target_cols] = 0\ny_pred.loc[oof.index] = oof\ny_pred.loc[train.cp_type=='ctl_vehicle'] = 0\n\nscore = 0\nfor i in range(len(target_cols)):\n   # print(log_loss(y_true[:, i], y_pred[:, i])/ len(target_cols))\n    score_ = log_loss(y_true[:, i], y_pred.iloc[:, i],labels=[0,1])\n    #if score_ > 0.02:\n     #   print(score_)\n    score +=( score_ / len(target_cols))\n    \nprint(\"CV log_loss: \", score)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\nsample_submission.set_index('sig_id',inplace=True)\ntest_features.set_index('sig_id',inplace=True)\ntest_features = test_features.loc[sample_submission.index]\n\nsub = sample_submission.drop(columns=target_cols).merge(test[target_cols], on='sig_id', how='left').fillna(0)\n#sub.set_index('sig_id',inplace=True)\nsub.loc[test_features['cp_type']=='ctl_vehicle', target_cols] =0\nsub.to_csv('./submission.csv', index=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}