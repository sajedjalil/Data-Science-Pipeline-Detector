{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Drug Method of Action prediction using tensorflow"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the needed libraries \nimport pandas as pd \nimport numpy as np \n# Import Deep learning library \nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, scale\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_features.drop(['sig_id'], axis=1, inplace=True)\ntrain_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_scored.drop(['sig_id'], axis=1, inplace=True)\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\ntest_features.drop(['sig_id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My Strategy of Analysis & Modeling will be as follows :\n\n\n1. Turn categorical features into numerical (Encoding) \n2. Scaling the data (Standarization) \n3. Split the data to training & validation sets and transform to proper format for modeling with tensorflow \n4. Create model with tensorflow \n5. fit the model to the data \n6. Tuning Hyperparameters of the deep learning model \n7. Make predictions & Submit to kaggle.com"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. Turn categorical features into numerical through one hot encoding \ncat_features = ['cp_type', 'cp_dose']\none_enc = OneHotEncoder(handle_unknown='ignore')\nenc_df = pd.DataFrame(one_enc.fit_transform(train_features[['cp_type', 'cp_dose']]).toarray())\ntrain_features = train_features.join(enc_df)\ntrain_features = train_features.drop(cat_features, axis=1)\ntrain_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.Scaling the data\nfor label, content in train_features.items(): \n    mean = np.mean(content)\n    std = np.std(content)\n    train_features[label] = (content-mean)/std\ntrain_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3. Split the data into training and validation sets & transform to proper format for tensorflow\nnp.random.seed(70)\nx_train, x_valid, y_train, y_valid = train_test_split(train_features, train_targets_scored, test_size=0.1)\nnp.savez('train_data', inputs = x_train, targets=y_train)\nnp.savez('valid_data', inputs = x_valid, targets=y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4. Create model with tensorflow \n# load the saved data \nnpz_train = np.load('train_data.npz',allow_pickle=True )\ntrain_inputs, train_targets = npz_train['inputs'].astype(np.float32), npz_train['targets'].astype(np.int32)\nnpz_valid = np.load('valid_data.npz',allow_pickle=True )\nvalid_inputs, valid_targets = npz_valid['inputs'].astype(np.float32), npz_valid['targets'].astype(np.int32)\n# creating and compiling the model \ninput_size, output_size = train_inputs.shape[1], train_targets.shape[1]\nhidden_layer_size = 800\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n    tf.keras.layers.Dense(output_size, activation='sigmoid')\n])\nopt = tf.keras.optimizers.SGD(learning_rate=0.02)\nmodel.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['accuracy'])\nbatch_size, max_epochs = 128, 100\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience=2) #callback is created to prevent overfitting \n# fitting the model \nmodel.fit(train_inputs, train_targets, batch_size=batch_size, epochs=max_epochs, callbacks=[early_stopping],\n          validation_data=(valid_inputs, valid_targets), verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}