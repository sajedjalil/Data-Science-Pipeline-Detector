{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/pytorch16gpu/torch-1.6.0cu101-cp37-cp37m-linux_x86_64.whl --quiet","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2020-09-06T18:32:08.877939Z","iopub.status.busy":"2020-09-06T18:32:08.877228Z","iopub.status.idle":"2020-09-06T18:32:14.972831Z","shell.execute_reply":"2020-09-06T18:32:14.972235Z"},"papermill":{"duration":6.107558,"end_time":"2020-09-06T18:32:14.972944","exception":false,"start_time":"2020-09-06T18:32:08.865386","status":"completed"},"tags":[],"id":"aI8-T6rjZD5_","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nsys.path.insert(0, \"../input/tabnetdevelop/tabnet-develop\")\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\n\nimport os\nimport gc\nimport math\nimport random\nimport datetime\nimport numpy as np\nimport pandas as pd\nfrom joblib import dump, load\nfrom numba import njit\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as layers\nimport tensorflow_addons as tfa\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import log_loss\nfrom scipy.optimize import minimize\nfrom tqdm.notebook import tqdm\nfrom time import time\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.metrics import roc_auc_score\n\nN_STARTS = 3\nN_SPLITS = 7\nCALCULATE_OOF = False\nCALCULATE_OOF_PL = True\nFINETUNE = True\nPOST_PROCESS = True","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(42)","execution_count":null,"outputs":[]},{"metadata":{"id":"EHkVsCptoNfT"},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2020-09-06T18:32:14.997376Z","iopub.status.busy":"2020-09-06T18:32:14.996643Z","iopub.status.idle":"2020-09-06T18:32:20.482365Z","shell.execute_reply":"2020-09-06T18:32:20.483409Z"},"papermill":{"duration":5.505878,"end_time":"2020-09-06T18:32:20.48361","exception":false,"start_time":"2020-09-06T18:32:14.977732","status":"completed"},"tags":[],"id":"5DxCXDYIZD6C","trusted":true},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\n\nss = pd.read_csv('../input/lish-moa/sample_submission.csv')\n\ncols = [c for c in ss.columns.values if c != 'sig_id']\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def preprocess(df):\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del df['sig_id']\n    return df\n\ndef log_loss_metric(y_true, y_pred):\n    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    return - np.mean(y_true * np.log(y_pred_clip) + (1 - y_true) * np.log(1 - y_pred_clip))\n\ntrain = preprocess(train_features)\ntest = preprocess(test_features)\n\ndel train_targets['sig_id']\ndel train_targets_nonscored['sig_id']\n\n# qt = QuantileTransformer(output_distribution = 'normal', random_state = 42)\n# qt.fit(pd.concat([pd.DataFrame(train[GENES+CELLS]), pd.DataFrame(test[GENES+CELLS])]))\nqt = load('../input/moa-preprocess/qt')\ntrain[GENES+CELLS] = qt.transform(train[GENES+CELLS])\ntest[GENES+CELLS] = qt.transform(test[GENES+CELLS])\n\n# GENES\nn_comp_genes = 600  #<--Update\n\ndata = pd.concat([pd.DataFrame(train[GENES]), pd.DataFrame(test[GENES])])\n# pca_genes = PCA(n_components = n_comp_genes, random_state = 42)\n# data2 = pca_genes.fit_transform(data[GENES])\npca_genes = load('../input/moa-preprocess/pca_genes')\ndata2 = pca_genes.transform(data[GENES])\ntrain2 = data2[:train.shape[0]]; test2 = data2[-test.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns = [f'pca_G-{i}' for i in range(n_comp_genes)])\ntest2 = pd.DataFrame(test2, columns = [f'pca_G-{i}' for i in range(n_comp_genes)])\n\ntrain = pd.concat((train, train2), axis = 1)\ntest = pd.concat((test, test2), axis = 1)\n\n#CELLS\nn_comp_cells = 50  #<--Update\n\ndata = pd.concat([pd.DataFrame(train[CELLS]), pd.DataFrame(test[CELLS])])\n# pca_cells = PCA(n_components = n_comp_cells, random_state = 42)\n# data2 = pca_cells.fit_transform(data[CELLS])\npca_cells = load('../input/moa-preprocess/pca_cells')\ndata2 = pca_cells.transform(data[CELLS])\ntrain2 = data2[:train.shape[0]]; test2 = data2[-test.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns = [f'pca_C-{i}' for i in range(n_comp_cells)])\ntest2 = pd.DataFrame(test2, columns = [f'pca_C-{i}' for i in range(n_comp_cells)])\n\ntrain = pd.concat((train, train2), axis = 1)\ntest = pd.concat((test, test2), axis = 1)\n\ndata = train.append(test)\n# var_thresh = VarianceThreshold(0.8)  #<-- Update\n# data_transformed = var_thresh.fit_transform(data.iloc[:, 3:])\nvar_thresh = load('../input/moa-preprocess/var_thresh')\ndata_transformed = var_thresh.transform(data.iloc[:, 3:])\n\ntrain_transformed = data_transformed[ : train.shape[0]]\ntest_transformed = data_transformed[-test.shape[0] : ]\n\ntrain = pd.DataFrame(train[['cp_type','cp_time','cp_dose']].values.reshape(-1, 3),\\\n                     columns=['cp_type','cp_time','cp_dose'])\n\ntrain = pd.concat([train, pd.DataFrame(train_transformed)], axis=1)\n\ntest = pd.DataFrame(test[['cp_type','cp_time','cp_dose']].values.reshape(-1, 3),\\\n                    columns=['cp_type','cp_time','cp_dose'])\n\ntest = pd.concat([test, pd.DataFrame(test_transformed)], axis=1)\n\nprint(train.shape)\nprint(test.shape)\n\ntrain_targets = train_targets.loc[train['cp_type'] == 0].reset_index(drop = True)\ntrain_targets_nonscored = train_targets_nonscored.loc[train['cp_type'] == 0].reset_index(drop = True)\ntrain = train.loc[train['cp_type'] == 0].reset_index(drop = True)\n\nprint(train.shape)\n\ntop_feats = np.arange(1, train.shape[1])\nprint(top_feats)\n\ncat_tr, cat_test, numerical_tr, numerical_test = train.loc[:, train.columns[1:3]], test.loc[:, test.columns[1:3]], train.loc[:, train.columns[3:]].values, test.loc[:, test.columns[3:]].values\ncat_tr.loc[:, 'cp_time'] = cat_tr.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})\ncat_test.loc[:, 'cp_time'] = cat_test.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})\ncat_tr = cat_tr.values\ncat_test = cat_test.values\ntargets_tr = train_targets[cols].values.astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utils"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def evals(model, X, y, verbose=True):\n    with torch.no_grad():\n        y_preds = model.predict(X)\n        y_preds = torch.clamp(y_preds, 0.0,1.0).detach().numpy()\n    score = log_loss_multi(y, y_preds)\n    #print(\"Logloss = \", score)\n    return y_preds, score\n\n\ndef inference_fn(model, X ,verbose=True):\n    with torch.no_grad():\n        y_preds = model.predict( X )\n        y_preds = torch.sigmoid(torch.as_tensor(y_preds)).numpy()\n    return y_preds\n\ndef log_loss_score(actual, predicted,  eps=1e-15):\n\n        \"\"\"\n        :param predicted:   The predicted probabilities as floats between 0-1\n        :param actual:      The binary labels. Either 0 or 1.\n        :param eps:         Log(0) is equal to infinity, so we need to offset our predicted values slightly by eps from 0 or 1\n        :return:            The logarithmic loss between between the predicted probability assigned to the possible outcomes for item i, and the actual outcome.\n        \"\"\"\n\n        \n        p1 = actual * np.log(predicted+eps)\n        p0 = (1-actual) * np.log(1-predicted+eps)\n        loss = p0 + p1\n\n        return -loss.mean()\ndef log_loss_multi(y_true, y_pred):\n    M = y_true.shape[1]\n    results = np.zeros(M)\n    for i in range(M):\n        results[i] = log_loss_score(y_true[:,i], y_pred[:,i])\n    return results.mean()\ndef check_targets(targets):\n    ### check if targets are all binary in training set\n    \n    for i in range(targets.shape[1]):\n        if len(np.unique(targets[:,i])) != 2:\n            return False\n    return True\ndef auc_multi(y_true, y_pred):\n    M = y_true.shape[1]\n    results = np.zeros(M)\n    for i in range(M):\n        try:\n            results[i] = roc_auc_score(y_true[:,i], y_pred[:,i])\n        except:\n            pass\n    return results.mean()\n\nimport torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets, self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss\n    \nsbcewlogits = SmoothBCEwLogits(smoothing = 0.0008)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TabNet"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"## TABNET\n\nimport torch\nimport numpy as np\nfrom scipy.sparse import csc_matrix\nimport time\nfrom abc import abstractmethod\nfrom pytorch_tabnet import tab_network\nfrom pytorch_tabnet.multiclass_utils import unique_labels\nfrom sklearn.metrics import roc_auc_score, mean_squared_error, accuracy_score\nfrom torch.nn.utils import clip_grad_norm_\nfrom pytorch_tabnet.utils import (PredictDataset,\n                                  create_dataloaders,\n                                  create_explain_matrix)\nfrom sklearn.base import BaseEstimator\nfrom torch.utils.data import DataLoader\nfrom copy import deepcopy\nimport io\nimport json\nfrom pathlib import Path\nimport shutil\nimport zipfile\n\nclass TabModel(BaseEstimator):\n    def __init__(self, n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1,\n                 n_independent=2, n_shared=2, epsilon=1e-15,  momentum=0.02,\n                 lambda_sparse=1e-3, seed=0,\n                 clip_value=1, verbose=1,\n                 optimizer_fn=torch.optim.Adam,\n                 optimizer_params=dict(lr=2e-2),\n                 scheduler_params=None, scheduler_fn=None,\n                 mask_type=\"sparsemax\",\n                 input_dim=None, output_dim=None,\n                 device_name='auto'):\n        \"\"\" Class for TabNet model\n        Parameters\n        ----------\n            device_name: str\n                'cuda' if running on GPU, 'cpu' if not, 'auto' to autodetect\n        \"\"\"\n\n        self.n_d = n_d\n        self.n_a = n_a\n        self.n_steps = n_steps\n        self.gamma = gamma\n        self.cat_idxs = cat_idxs\n        self.cat_dims = cat_dims\n        self.cat_emb_dim = cat_emb_dim\n        self.n_independent = n_independent\n        self.n_shared = n_shared\n        self.epsilon = epsilon\n        self.momentum = momentum\n        self.lambda_sparse = lambda_sparse\n        self.clip_value = clip_value\n        self.verbose = verbose\n        self.optimizer_fn = optimizer_fn\n        self.optimizer_params = optimizer_params\n        self.device_name = device_name\n        self.scheduler_params = scheduler_params\n        self.scheduler_fn = scheduler_fn\n        self.mask_type = mask_type\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n        self.batch_size = 1024\n\n        self.seed = seed\n        torch.manual_seed(self.seed)\n        # Defining device\n        if device_name == 'auto':\n            if torch.cuda.is_available():\n                device_name = 'cuda'\n            else:\n                device_name = 'cpu'\n        self.device = torch.device(device_name)\n        print(f\"Device used : {self.device}\")\n\n    @abstractmethod\n    def construct_loaders(self, X_train, y_train, X_valid, y_valid,\n                          weights, batch_size, num_workers, drop_last):\n        \"\"\"\n        Returns\n        -------\n        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n            Training and validation dataloaders\n        -------\n        \"\"\"\n        raise NotImplementedError('users must define construct_loaders to use this base class')\n\n    def init_network(\n                     self,\n                     input_dim,\n                     output_dim,\n                     n_d,\n                     n_a,\n                     n_steps,\n                     gamma,\n                     cat_idxs,\n                     cat_dims,\n                     cat_emb_dim,\n                     n_independent,\n                     n_shared,\n                     epsilon,\n                     virtual_batch_size,\n                     momentum,\n                     device_name,\n                     mask_type,\n                     ):\n        self.network = tab_network.TabNet(\n            input_dim,\n            output_dim,\n            n_d=n_d,\n            n_a=n_a,\n            n_steps=n_steps,\n            gamma=gamma,\n            cat_idxs=cat_idxs,\n            cat_dims=cat_dims,\n            cat_emb_dim=cat_emb_dim,\n            n_independent=n_independent,\n            n_shared=n_shared,\n            epsilon=epsilon,\n            virtual_batch_size=virtual_batch_size,\n            momentum=momentum,\n            device_name=device_name,\n            mask_type=mask_type).to(self.device)\n\n        self.reducing_matrix = create_explain_matrix(\n            self.network.input_dim,\n            self.network.cat_emb_dim,\n            self.network.cat_idxs,\n            self.network.post_embed_dim)\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, loss_fn=None,\n            weights=0, max_epochs=100, patience=10, batch_size=1024,\n            virtual_batch_size=128, num_workers=0, drop_last=False, pretrain=False, optimizer_params=None):\n        \"\"\"Train a neural network stored in self.network\n        Using train_dataloader for training data and\n        valid_dataloader for validation.\n        Parameters\n        ----------\n            X_train: np.ndarray\n                Train set\n            y_train : np.array\n                Train targets\n            X_train: np.ndarray\n                Train set\n            y_train : np.array\n                Train targets\n            weights : bool or dictionnary\n                0 for no balancing\n                1 for automated balancing\n                dict for custom weights per class\n            max_epochs : int\n                Maximum number of epochs during training\n            patience : int\n                Number of consecutive non improving epoch before early stopping\n            batch_size : int\n                Training batch size\n            virtual_batch_size : int\n                Batch size for Ghost Batch Normalization (virtual_batch_size < batch_size)\n            num_workers : int\n                Number of workers used in torch.utils.data.DataLoader\n            drop_last : bool\n                Whether to drop last batch during training\n        \"\"\"\n        # update model name\n\n        self.update_fit_params(X_train, y_train, X_valid, y_valid, loss_fn,\n                               weights, max_epochs, patience, batch_size,\n                               virtual_batch_size, num_workers, drop_last)\n\n        train_dataloader, valid_dataloader = self.construct_loaders(X_train,\n                                                                    y_train,\n                                                                    X_valid,\n                                                                    y_valid,\n                                                                    self.updated_weights,\n                                                                    self.batch_size,\n                                                                    self.num_workers,\n                                                                    self.drop_last)\n        if not pretrain:\n            self.init_network(\n                input_dim=self.input_dim,\n                output_dim=self.output_dim,\n                n_d=self.n_d,\n                n_a=self.n_a,\n                n_steps=self.n_steps,\n                gamma=self.gamma,\n                cat_idxs=self.cat_idxs,\n                cat_dims=self.cat_dims,\n                cat_emb_dim=self.cat_emb_dim,\n                n_independent=self.n_independent,\n                n_shared=self.n_shared,\n                epsilon=self.epsilon,\n                virtual_batch_size=self.virtual_batch_size,\n                momentum=self.momentum,\n                device_name=self.device_name,\n                mask_type=self.mask_type\n            )\n            self.optimizer = self.optimizer_fn(self.network.parameters(),\n                                               **self.optimizer_params)\n        else:\n            self.optimizer = self.optimizer_fn(self.network.parameters(),\n                                               **optimizer_params)\n\n        if self.scheduler_fn:\n            self.scheduler = self.scheduler_fn(self.optimizer, **self.scheduler_params)\n        else:\n            self.scheduler = None\n\n        self.losses_train = []\n        self.losses_valid = []\n        self.learning_rates = []\n        self.metrics_train = []\n        self.metrics_valid = []\n\n        if self.verbose > 0:\n            print(\"Will train until validation stopping metric\",\n                  f\"hasn't improved in {self.patience} rounds.\")\n            msg_epoch = f'| EPOCH |  train  |   valid  | total time (s)'\n            print('---------------------------------------')\n            print(msg_epoch)\n\n        total_time = 0\n        while (self.epoch < self.max_epochs and\n               self.patience_counter < self.patience):\n            starting_time = time.time()\n            # updates learning rate history\n            self.learning_rates.append(self.optimizer.param_groups[-1][\"lr\"])\n\n            fit_metrics = self.fit_epoch(train_dataloader, valid_dataloader)\n\n            # leaving it here, may be used for callbacks later\n            self.losses_train.append(fit_metrics['train']['loss_avg'])\n            self.losses_valid.append(fit_metrics['valid']['total_loss'])\n            self.metrics_train.append(fit_metrics['train']['stopping_loss'])\n            self.metrics_valid.append(fit_metrics['valid']['stopping_loss'])\n\n            stopping_loss = fit_metrics['valid']['stopping_loss']\n            if stopping_loss < self.best_cost:\n                self.best_cost = stopping_loss\n                self.patience_counter = 0\n                # Saving model\n                self.best_network = deepcopy(self.network)\n                has_improved = True\n            else:\n                self.patience_counter += 1\n                has_improved=False\n            self.epoch += 1\n            total_time += time.time() - starting_time\n            if self.verbose > 0:\n                if self.epoch % self.verbose == 0:\n                    separator = \"|\"\n                    msg_epoch = f\"| {self.epoch:<5} | \"\n                    msg_epoch += f\" {fit_metrics['train']['stopping_loss']:.5f}\"\n                    msg_epoch += f' {separator:<2} '\n                    msg_epoch += f\" {fit_metrics['valid']['stopping_loss']:.5f}\"\n                    msg_epoch += f' {separator:<2} '\n                    msg_epoch += f\" {np.round(total_time, 1):<10}\"\n                    msg_epoch += f\" {has_improved}\"\n                    print(msg_epoch)\n\n        if self.verbose > 0:\n            if self.patience_counter == self.patience:\n                print(f\"Early stopping occured at epoch {self.epoch}\")\n            print(f\"Training done in {total_time:.3f} seconds.\")\n            print('---------------------------------------')\n\n        self.history = {\"train\": {\"loss\": self.losses_train,\n                                  \"metric\": self.metrics_train,\n                                  \"lr\": self.learning_rates},\n                        \"valid\": {\"loss\": self.losses_valid,\n                                  \"metric\": self.metrics_valid}}\n        # load best models post training\n        self.load_best_model()\n\n        # compute feature importance once the best model is defined\n        self._compute_feature_importances(train_dataloader)\n\n    def save_model(self, path):\n        \"\"\"\n        Saving model with two distinct files.\n        \"\"\"\n        saved_params = {}\n        for key, val in self.get_params().items():\n            if isinstance(val, type):\n                # Don't save torch specific params\n                continue\n            else:\n                saved_params[key] = val\n\n        # Create folder\n        Path(path).mkdir(parents=True, exist_ok=True)\n\n        # Save models params\n        with open(Path(path).joinpath(\"model_params.json\"), \"w\", encoding=\"utf8\") as f:\n            json.dump(saved_params, f)\n\n        # Save state_dict\n        torch.save(self.network.state_dict(), Path(path).joinpath(\"network.pt\"))\n        shutil.make_archive(path, 'zip', path)\n        shutil.rmtree(path)\n        print(f\"Successfully saved model at {path}.zip\")\n        return f\"{path}.zip\"\n\n    def load_model(self, filepath):\n\n        try:\n            try:\n                with zipfile.ZipFile(filepath) as z:\n                    with z.open(\"model_params.json\") as f:\n                        loaded_params = json.load(f)\n                    with z.open(\"network.pt\") as f:\n                        try:\n                            saved_state_dict = torch.load(f)\n                        except io.UnsupportedOperation:\n                            # In Python <3.7, the returned file object is not seekable (which at least\n                            # some versions of PyTorch require) - so we'll try buffering it in to a\n                            # BytesIO instead:\n                            saved_state_dict = torch.load(io.BytesIO(f.read()))\n                            \n            except:\n                with open(os.path.join(filepath, \"model_params.json\")) as f:\n                        loaded_params = json.load(f)\n\n                saved_state_dict = torch.load(os.path.join(filepath, \"network.pt\"), map_location=\"cpu\")\n \n        except KeyError:\n            raise KeyError(\"Your zip file is missing at least one component\")\n\n        #print(loaded_params)\n        if torch.cuda.is_available():\n            device_name = 'cuda'\n        else:\n            device_name = 'cpu'\n        loaded_params[\"device_name\"] = device_name\n        self.__init__(**loaded_params)\n\n        self.init_network(\n            input_dim=self.input_dim,\n            output_dim=self.output_dim,\n            n_d=self.n_d,\n            n_a=self.n_a,\n            n_steps=self.n_steps,\n            gamma=self.gamma,\n            cat_idxs=self.cat_idxs,\n            cat_dims=self.cat_dims,\n            cat_emb_dim=self.cat_emb_dim,\n            n_independent=self.n_independent,\n            n_shared=self.n_shared,\n            epsilon=self.epsilon,\n            virtual_batch_size=1024,\n            momentum=self.momentum,\n            device_name=self.device_name,\n            mask_type=self.mask_type\n        )\n        self.network.load_state_dict(saved_state_dict)\n        self.network.eval()\n        return\n\n    def fit_epoch(self, train_dataloader, valid_dataloader):\n        \"\"\"\n        Evaluates and updates network for one epoch.\n        Parameters\n        ----------\n            train_dataloader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with train set\n            valid_dataloader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with valid set\n        \"\"\"\n        train_metrics = self.train_epoch(train_dataloader)\n        valid_metrics = self.predict_epoch(valid_dataloader)\n\n        fit_metrics = {'train': train_metrics,\n                       'valid': valid_metrics}\n\n        return fit_metrics\n\n    @abstractmethod\n    def train_epoch(self, train_loader):\n        \"\"\"\n        Trains one epoch of the network in self.network\n        Parameters\n        ----------\n            train_loader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with train set\n        \"\"\"\n        raise NotImplementedError('users must define train_epoch to use this base class')\n\n    @abstractmethod\n    def train_batch(self, data, targets):\n        \"\"\"\n        Trains one batch of data\n        Parameters\n        ----------\n            data: a :tensor: `torch.tensor`\n                Input data\n            target: a :tensor: `torch.tensor`\n                Target data\n        \"\"\"\n        raise NotImplementedError('users must define train_batch to use this base class')\n\n    @abstractmethod\n    def predict_epoch(self, loader):\n        \"\"\"\n        Validates one epoch of the network in self.network\n        Parameters\n        ----------\n            loader: a :class: `torch.utils.data.Dataloader`\n                    DataLoader with validation set\n        \"\"\"\n        raise NotImplementedError('users must define predict_epoch to use this base class')\n\n    @abstractmethod\n    def predict_batch(self, data, targets):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            batch_outs: dict\n        \"\"\"\n        raise NotImplementedError('users must define predict_batch to use this base class')\n\n    def load_best_model(self):\n        if self.best_network is not None:\n            self.network = self.best_network\n\n    @abstractmethod\n    def predict(self, X):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            predictions: np.array\n                Predictions of the regression problem or the last class\n        \"\"\"\n        raise NotImplementedError('users must define predict to use this base class')\n\n    def explain(self, X):\n        \"\"\"\n        Return local explanation\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            M_explain: matrix\n                Importance per sample, per columns.\n            masks: matrix\n                Sparse matrix showing attention masks used by network.\n        \"\"\"\n        self.network.eval()\n\n        dataloader = DataLoader(PredictDataset(X),\n                                batch_size=self.batch_size, shuffle=False)\n\n        for batch_nb, data in enumerate(dataloader):\n            data = data.to(self.device).float()\n\n            M_explain, masks = self.network.forward_masks(data)\n            for key, value in masks.items():\n                masks[key] = csc_matrix.dot(value.cpu().detach().numpy(),\n                                            self.reducing_matrix)\n\n            if batch_nb == 0:\n                res_explain = csc_matrix.dot(M_explain.cpu().detach().numpy(),\n                                             self.reducing_matrix)\n                res_masks = masks\n            else:\n                res_explain = np.vstack([res_explain,\n                                         csc_matrix.dot(M_explain.cpu().detach().numpy(),\n                                                        self.reducing_matrix)])\n                for key, value in masks.items():\n                    res_masks[key] = np.vstack([res_masks[key], value])\n        return res_explain, res_masks\n\n    def _compute_feature_importances(self, loader):\n        self.network.eval()\n        feature_importances_ = np.zeros((self.network.post_embed_dim))\n        for data, targets in loader:\n            data = data.to(self.device).float()\n            M_explain, masks = self.network.forward_masks(data)\n            feature_importances_ += M_explain.sum(dim=0).cpu().detach().numpy()\n\n        feature_importances_ = csc_matrix.dot(feature_importances_,\n                                              self.reducing_matrix)\n        self.feature_importances_ = feature_importances_ / np.sum(feature_importances_)\n        \n        \nclass TabNetRegressor(TabModel):\n\n    def construct_loaders(self, X_train, y_train, X_valid, y_valid, weights,\n                          batch_size, num_workers, drop_last):\n        \"\"\"\n        Returns\n        -------\n        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n            Training and validation dataloaders\n        -------\n        \"\"\"\n        if isinstance(weights, int):\n            if weights == 1:\n                raise ValueError(\"Please provide a list of weights for regression.\")\n        if isinstance(weights, dict):\n            raise ValueError(\"Please provide a list of weights for regression.\")\n\n        train_dataloader, valid_dataloader = create_dataloaders(X_train,\n                                                                y_train,\n                                                                X_valid,\n                                                                y_valid,\n                                                                weights,\n                                                                batch_size,\n                                                                num_workers,\n                                                                drop_last)\n        return train_dataloader, valid_dataloader\n\n    def update_fit_params(self, X_train, y_train, X_valid, y_valid, loss_fn,\n                          weights, max_epochs, patience,\n                          batch_size, virtual_batch_size, num_workers, drop_last):\n\n        if loss_fn is None:\n            self.loss_fn = torch.nn.functional.mse_loss\n        else:\n            self.loss_fn = loss_fn\n\n        assert X_train.shape[1] == X_valid.shape[1], \"Dimension mismatch X_train X_valid\"\n        self.input_dim = X_train.shape[1]\n\n        if len(y_train.shape) == 1:\n            raise ValueError(\"\"\"Please apply reshape(-1, 1) to your targets\n                                if doing single regression.\"\"\")\n        assert y_train.shape[1] == y_valid.shape[1], \"Dimension mismatch y_train y_valid\"\n        self.output_dim = y_train.shape[1]\n\n        self.updated_weights = weights\n\n        self.max_epochs = max_epochs\n        self.patience = patience\n        self.batch_size = batch_size\n        self.virtual_batch_size = virtual_batch_size\n        # Initialize counters and histories.\n        self.patience_counter = 0\n        self.epoch = 0\n        self.best_cost = np.inf\n        self.num_workers = num_workers\n        self.drop_last = drop_last\n\n    def train_epoch(self, train_loader):\n        \"\"\"\n        Trains one epoch of the network in self.network\n        Parameters\n        ----------\n            train_loader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with train set\n        \"\"\"\n\n        self.network.train()\n        y_preds = []\n        ys = []\n        total_loss = 0\n\n        for data, targets in train_loader:\n            batch_outs = self.train_batch(data, targets)\n            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n            total_loss += batch_outs[\"loss\"]\n\n        y_preds = np.vstack(y_preds)\n        ys = np.vstack(ys)\n\n        #stopping_loss = mean_squared_error(y_true=ys, y_pred=y_preds)\n        stopping_loss =log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  )\n        total_loss = total_loss / len(train_loader)\n        epoch_metrics = {'loss_avg': total_loss,\n                         'stopping_loss': total_loss,\n                         }\n\n        if self.scheduler is not None:\n            self.scheduler.step()\n        return epoch_metrics\n\n    def train_batch(self, data, targets):\n        \"\"\"\n        Trains one batch of data\n        Parameters\n        ----------\n            data: a :tensor: `torch.tensor`\n                Input data\n            target: a :tensor: `torch.tensor`\n                Target data\n        \"\"\"\n        self.network.train()\n        data = data.to(self.device).float()\n\n        targets = targets.to(self.device).float()\n        self.optimizer.zero_grad()\n\n        output, M_loss = self.network(data)\n\n        loss = self.loss_fn(output, targets)\n        \n        loss -= self.lambda_sparse*M_loss\n\n        loss.backward()\n        if self.clip_value:\n            clip_grad_norm_(self.network.parameters(), self.clip_value)\n        self.optimizer.step()\n\n        loss_value = loss.item()\n        batch_outs = {'loss': loss_value,\n                      'y_preds': output,\n                      'y': targets}\n        return batch_outs\n\n    def predict_epoch(self, loader):\n        \"\"\"\n        Validates one epoch of the network in self.network\n        Parameters\n        ----------\n            loader: a :class: `torch.utils.data.Dataloader`\n                    DataLoader with validation set\n        \"\"\"\n        y_preds = []\n        ys = []\n        self.network.eval()\n        total_loss = 0\n\n        for data, targets in loader:\n            batch_outs = self.predict_batch(data, targets)\n            total_loss += batch_outs[\"loss\"]\n            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n\n        y_preds = np.vstack(y_preds)\n        ys = np.vstack(ys)\n\n        stopping_loss =log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  ) #mean_squared_error(y_true=ys, y_pred=y_preds)\n\n        total_loss = total_loss / len(loader)\n        epoch_metrics = {'total_loss': total_loss,\n                         'stopping_loss': stopping_loss}\n\n        return epoch_metrics\n\n    def predict_batch(self, data, targets):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            batch_outs: dict\n        \"\"\"\n        self.network.eval()\n        data = data.to(self.device).float()\n        targets = targets.to(self.device).float()\n\n        output, M_loss = self.network(data)\n       \n        loss = self.loss_fn(output, targets)\n        #print(self.loss_fn, loss)\n        loss -= self.lambda_sparse*M_loss\n        #print(loss)\n        loss_value = loss.item()\n        batch_outs = {'loss': loss_value,\n                      'y_preds': output,\n                      'y': targets}\n        return batch_outs\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            predictions: np.array\n                Predictions of the regression problem\n        \"\"\"\n        self.network.eval()\n        dataloader = DataLoader(PredictDataset(X),\n                                batch_size=self.batch_size, shuffle=False)\n\n        results = []\n        for batch_nb, data in enumerate(dataloader):\n            data = data.to(self.device).float()\n\n            output, M_loss = self.network(data)\n            predictions = output.cpu().detach().numpy()\n            results.append(predictions)\n        res = np.vstack(results)\n        return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Config(object):\n    def __init__(self):\n        self.num_class = targets_tr.shape[1]\n        self.verbose=False\n        #\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.SPLITS = N_SPLITS\n        self.EPOCHS = 200\n        self.num_ensembling = N_STARTS\n        self.seed = 0\n        # Parameters model\n        self.cat_emb_dim=[1] * cat_tr.shape[1] #to choose\n        self.cats_idx = list(range(cat_tr.shape[1]))\n        self.cat_dims = [len(np.unique(cat_tr[:, i])) for i in range(cat_tr.shape[1])]\n        self.num_numericals = numerical_tr.shape[1]\n        # save\n        self.save_name = \"../input/multilabel-pbestpre-tabnet/tabnet_raw_step1\"\n        \n        self.strategy = \"KFOLD\" # \n        \ncfg = Config()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X_test = np.concatenate([cat_test, numerical_test], axis=1)\nif cfg.strategy == \"KFOLD\":\n    oof_preds_all = []\n    oof_targets_all = []\n    scores_all =  []\n    scores_auc_all= []\n    preds_test = []\n    res = np.zeros(targets_tr.shape)\n    for nums, seed in enumerate(range(cfg.num_ensembling)):\n        print(\"## SEED : \", seed)\n        mskf = MultilabelStratifiedKFold(n_splits=cfg.SPLITS, random_state=cfg.seed+seed, shuffle=True)\n        oof_preds = []\n        oof_targets = []\n        scores = []\n        scores_auc = []\n        p = []\n        for j, (train_idx, val_idx) in enumerate(mskf.split(np.zeros(len(cat_tr)), targets_tr)):\n            print(\"FOLDS : \", j)\n\n            ## model\n            X_train, y_train = torch.as_tensor(np.concatenate([cat_tr[train_idx], numerical_tr[train_idx] ], axis=1)), torch.as_tensor(targets_tr[train_idx])\n            X_val, y_val = torch.as_tensor(np.concatenate([cat_tr[val_idx], numerical_tr[val_idx] ], axis=1)), torch.as_tensor(targets_tr[val_idx])\n            model = TabNetRegressor(n_d=24, n_a=24, n_steps=1, gamma=1.3, lambda_sparse=0, cat_dims=cfg.cat_dims, cat_emb_dim=cfg.cat_emb_dim, cat_idxs=cfg.cats_idx, optimizer_fn=torch.optim.Adam,\n                                   optimizer_params=dict(lr=2e-2), mask_type='entmax', device_name=cfg.device, scheduler_params=dict(milestones=[ 50,100,150], gamma=0.9), scheduler_fn=torch.optim.lr_scheduler.MultiStepLR)\n            #'sparsemax'\n            \n            name = cfg.save_name + f\"_fold{j}_{seed}\"\n            model.load_model(name)\n            if CALCULATE_OOF:\n            # preds on val\n                preds = model.predict(X_val)\n                preds = torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n                score = log_loss_multi(y_val, preds)\n                res[val_idx] += preds / cfg.num_ensembling\n\n                ## save oof to compute the CV later\n                oof_preds.append(preds)\n                oof_targets.append(y_val)\n                scores.append(score)\n                scores_auc.append(auc_multi(y_val,preds))\n                print(f\"validation fold {j} : {score}\")\n                \n                \n            # preds on test\n            temp = model.predict(X_test)\n            p.append(torch.sigmoid(torch.as_tensor(temp)).detach().cpu().numpy())\n                \n        p = np.stack(p)\n        preds_test.append(p)\n        \n        if CALCULATE_OOF:\n            oof_preds_all.append(np.concatenate(oof_preds))\n            oof_targets_all.append(np.concatenate(oof_targets))\n            scores_all.append(np.array(scores))\n            scores_auc_all.append(np.array(scores_auc))\n            \n    preds_test = np.stack(preds_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if CALCULATE_OOF:\n\n    if cfg.strategy == \"KFOLD\":\n\n        for i in range(cfg.num_ensembling): \n            print(\"CV score fold : \", log_loss_multi(oof_targets_all[i], oof_preds_all[i]))\n            print(\"auc mean : \", sum(scores_auc_all[i])/len(scores_auc_all[i]))\n        \n    # Overall OOF CV Score\n    tr_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv').drop('sig_id', axis = 1)\n    res_all = np.zeros(tr_targets[cols].shape)\n    res_all[train_features['cp_type'] == 0] = res\n    overall_oof_score = log_loss_metric(tr_targets[cols].values, res_all)\n    print(f'TabNet Overall OOF CV Score:', overall_oof_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_tabnet = pd.read_csv('../input/lish-moa/sample_submission.csv')\nsubmission_tabnet[cols] = preds_test.mean(1).mean(0)\nsubmission_tabnet.loc[test['cp_type'] == 1, cols] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GrowNet"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\"batch_size\": 128,\n           \"lr\": 1e-3,\n           \"weight_decay\": 1e-5,\n           \"n_seeds\": 3,\n           \"n_folds\": 7,\n           \"feat_d\": len(top_feats),\n           \"early_stopping_steps\": 5,\n           \"hidden_size\": 192,\n           \"boost_rate\": 1.0,  # original: 1.0\n           \"num_nets\": 40,  # Number of weak NNs. original: 40\n           \"epochs_per_stage\": 1,  # Number of epochs to learn the Kth model. original: 1\n           \"correct_epoch\": 1,    #  Number of epochs to correct the whole week models original: 1\n           \"model_order\": \"second\"  # You could put \"first\" according to the original implemention, but error occurs. original: \"second\"\n          }\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :] , dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    \nfrom enum import Enum\nclass ForwardType(Enum):\n    SIMPLE = 0\n    STACKED = 1\n    CASCADE = 2\n    GRADIENT = 3\n\nclass DynamicNet(object):\n    def __init__(self, c0, lr):\n        self.models = []\n        self.c0 = c0\n        self.lr = lr\n        self.boost_rate  = nn.Parameter(torch.tensor(lr, requires_grad=True, device=device))\n\n    def add(self, model):\n        self.models.append(model)\n\n    def parameters(self):\n        params = []\n        for m in self.models:\n            params.extend(m.parameters())\n\n        params.append(self.boost_rate)\n        return params\n\n    def zero_grad(self):\n        for m in self.models:\n            m.zero_grad()\n\n    def to_cuda(self):\n        for m in self.models:\n#             m.cuda()\n            m.to(device)\n\n    def to_eval(self):\n        for m in self.models:\n            m.eval()\n\n    def to_train(self):\n        for m in self.models:\n            m.train(True)\n\n    def forward(self, x):\n        if len(self.models) == 0:\n            batch = x.shape[0]\n            c0 = np.repeat(self.c0.detach().cpu().numpy().reshape(1,-1), batch, axis=0)\n            return None, torch.Tensor(c0).cuda()\n        middle_feat_cum = None\n        prediction = None\n        with torch.no_grad():\n            for m in self.models:\n                if middle_feat_cum is None:\n                    middle_feat_cum, prediction = m(x, middle_feat_cum)\n                else:\n                    middle_feat_cum, pred = m(x, middle_feat_cum)\n                    prediction += pred\n        return middle_feat_cum, self.c0 + self.boost_rate * prediction\n\n    def forward_grad(self, x):\n        if len(self.models) == 0:\n            batch = x.shape[0]\n            c0 = np.repeat(self.c0.detach().cpu().numpy().reshape(1, -1), batch, axis=0)\n            return None, torch.Tensor(c0).cuda()\n        # at least one model\n        middle_feat_cum = None\n        prediction = None\n        for m in self.models:\n            if middle_feat_cum is None:\n                middle_feat_cum, prediction = m(x, middle_feat_cum)\n            else:\n                middle_feat_cum, pred = m(x, middle_feat_cum)\n                prediction += pred\n        return middle_feat_cum, self.c0 + self.boost_rate * prediction\n\n    @classmethod\n    def from_file(cls, path, builder):\n        d = torch.load(path, map_location = torch.device(device))\n        net = DynamicNet(d['c0'], d['lr'])\n        net.boost_rate = d['boost_rate']\n        for stage, m in enumerate(d['models']):\n            submod = builder(stage)\n            submod.load_state_dict(m)\n            net.add(submod)\n        return net\n\n    def to_file(self, path):\n        models = [m.state_dict() for m in self.models]\n        d = {'models': models, 'c0': self.c0, 'lr': self.lr, 'boost_rate': self.boost_rate}\n        torch.save(d, path)\n\nclass MLP_2HL(nn.Module):\n    def __init__(self, dim_in, dim_hidden1, dim_hidden2, sparse=False, bn=True):\n        super(MLP_2HL, self).__init__()\n        self.bn2 = nn.BatchNorm1d(dim_in)\n\n        self.layer1 = nn.Sequential(\n            nn.utils.weight_norm(nn.Linear(dim_in, dim_hidden1)),\n            nn.LeakyReLU(),\n            nn.BatchNorm1d(dim_hidden1),\n            nn.utils.weight_norm(nn.Linear(dim_hidden1, dim_hidden2)),\n        )\n        self.layer2 = nn.Sequential(\n            nn.LeakyReLU(),\n            nn.utils.weight_norm(nn.Linear(dim_hidden2, 206)),\n        )\n\n    def forward(self, x, lower_f):\n        if lower_f is not None:\n            x = torch.cat([x, lower_f], dim=1)\n            x = self.bn2(x)\n        middle_feat = self.layer1(x)\n        out = self.layer2(middle_feat)\n        return middle_feat, out\n\n    @classmethod\n    def get_model(cls, stage, params):\n        if stage == 0:\n            dim_in = params[\"feat_d\"]\n        else:\n            dim_in = params[\"feat_d\"] + params[\"hidden_size\"]\n        model = MLP_2HL(dim_in, params[\"hidden_size\"], params[\"hidden_size\"])\n        return model\n    \ndef get_optim(params, lr, weight_decay):\n    optimizer = optim.Adam(params, lr, weight_decay=weight_decay)\n    #optimizer = SGD(params, lr, weight_decay=weight_decay)\n    return optimizer\n\ndef logloss(net_ensemble, test_loader):\n    loss = 0\n    total = 0\n    loss_f = nn.BCEWithLogitsLoss() # Binary cross entopy loss with logits, reduction=mean by default\n    for data in test_loader:\n        x = data[\"x\"].cuda()\n        y = data[\"y\"].cuda()\n        # y = (y + 1) / 2\n        with torch.no_grad():\n            _, out = net_ensemble.forward(x)\n        # out = torch.as_tensor(out, dtype=torch.float32).cuda().view(-1, 1)\n        loss += loss_f(out, y)\n        total += 1\n\n    return loss / total","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"c0_ = np.log(np.mean(train_targets.values, axis=0))\ndef inference_fn(seed=0):\n    seed_everything(seed)\n    oof = np.zeros((train.shape[0], len(cols)))\n    overall_score = 0\n    predictions = np.zeros((test.shape[0], len(cols)))\n    skf = MultilabelStratifiedKFold(n_splits = N_SPLITS, random_state = seed, shuffle = True)\n    for foldno, (tr, te) in enumerate(skf.split(train_targets.values, train_targets.values)):\n        print(f'Seed {seed}: Fold {foldno}:', end = '\\r')\n        \n        train_idx = tr\n        val_idx = te\n        \n        x_train = train.values[train_idx][:, top_feats]\n        y_train = train_targets.values[train_idx]\n        \n#         x_train = np.concatenate([x_train, pseudo_train[:, top_feats]])\n#         y_train = np.concatenate([y_train, pseudo_targets])\n        \n        x_val = train.values[val_idx][:, top_feats]\n        y_val = train_targets.values[val_idx]\n        \n        train_ds = MoADataset(x_train, y_train)\n        val_ds = MoADataset(x_val, y_val)\n        train_loader = DataLoader(train_ds, batch_size=params[\"batch_size\"], shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=params[\"batch_size\"], shuffle=False)\n        \n        best_score = np.inf\n        val_score = best_score\n        best_stage = params[\"num_nets\"] - 1\n\n        c0 = torch.tensor(c0_, dtype=torch.float).to(device)\n        net_ensemble = DynamicNet(c0, params[\"boost_rate\"])\n        loss_f1 = nn.MSELoss(reduction='none')\n#         loss_f2 = nn.BCEWithLogitsLoss(reduction='none')\n        loss_f2 = SmoothBCEwLogits(smoothing=0.0008, reduction=\"none\")\n        loss_models = torch.zeros((params[\"num_nets\"], 3))\n\n        all_ensm_losses = []\n        all_ensm_losses_te = []\n        all_mdl_losses = []\n        dynamic_br = []\n\n        lr = params[\"lr\"]\n        L2 = params[\"weight_decay\"]        \n\n        net_ensemble = DynamicNet.from_file(f\"../input/multilabel-pbestpre-grownet/{foldno}FOLD_{seed}_.pth\", lambda stage: MLP_2HL.get_model(stage, params))\n        net_ensemble.to_cuda()\n        net_ensemble.to_eval()\n        \n        if CALCULATE_OOF:\n            preds = []\n            with torch.no_grad():\n                for data in val_loader:\n                    x = data[\"x\"].to(device)\n                    _, pred = net_ensemble.forward(x)\n                    preds.append(pred.sigmoid().detach().cpu().numpy())\n            oof[val_idx, :] = np.concatenate(preds)\n\n        x_test = test.values[:, top_feats]\n        test_ds = TestDataset(x_test)\n        test_loader = DataLoader(test_ds, batch_size=params[\"batch_size\"], shuffle=False)\n\n        preds = []\n        with torch.no_grad():\n            for data in test_loader:\n                x = data[\"x\"].to(device)\n                _, pred = net_ensemble.forward(x)\n                preds.append(pred.sigmoid().detach().cpu().numpy())\n        predictions += np.concatenate(preds) / params[\"n_folds\"]\n    \n    if CALCULATE_OOF:\n        overall_score = log_loss_metric(train_targets.values, oof)\n        print('OOF Score:', overall_score)\n        \n    predictions[test['cp_type'] == 1] = 0\n    sub = pd.read_csv('../input/lish-moa/sample_submission.csv')\n    sub.loc[:, cols] = predictions\n    \n    return overall_score, oof, sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_grownet = np.zeros((train.shape[0], len(cols)))\nsubmission_grownet = pd.read_csv('../input/lish-moa/sample_submission.csv')\nsubmission_grownet.loc[:, cols] = 0\nmean_score = 0\nfor seed in tqdm(range(N_STARTS)):\n    score_seed, oof_seed, sub_seed = inference_fn(seed)\n    submission_grownet.loc[:, cols] += sub_seed.loc[:, cols] / N_STARTS\n    if CALCULATE_OOF:\n        oof_grownet += oof_seed / N_STARTS\n        mean_score += score_seed / N_STARTS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if CALCULATE_OOF:\n    print('Mean OOF Score:', mean_score)\n    overall_score_grownet = log_loss_metric(train_targets.values, oof_grownet)\n    print('OOF Score:', overall_score_grownet)\n    # Overall OOF CV Score\n    tr_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv').drop('sig_id', axis = 1)\n    res_all_grownet = np.zeros(tr_targets[cols].shape)\n    res_all_grownet[train_features['cp_type'] == 0] = oof_grownet\n    overall_oof_score_grownet = log_loss_metric(tr_targets[cols].values, res_all_grownet)\n    print('Overall OOF CV Score:', overall_oof_score_grownet)","execution_count":null,"outputs":[]},{"metadata":{"id":"l9086lcvni30"},"cell_type":"markdown","source":"# RTN + MLP"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_mlp(num_columns, num_labels, hidden_units, dropout_rates, lr):\n    \n    inp = tf.keras.layers.Input(shape = (num_columns, ), name = 'inp')\n    x = tf.keras.layers.BatchNormalization(name = 'bn0')(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0], name = 'dp0')(x)\n    for i in range(len(hidden_units)): \n        x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(hidden_units[i], name = f'd{i}'), name = f'wn{i}')(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish, name = f'a{i}')(x)\n        x = tf.keras.layers.BatchNormalization(name = f'bn{i+1}')(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i+1], name = f'dp{i+1}')(x)    \n        \n    x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(num_labels, \n                                                             bias_initializer = tf.keras.initializers.Constant(6.3), \n                                                             name = f'output_d{num_labels}'), \n                                       name = f'output_wn{num_labels}')(x)\n    out = tf.keras.layers.Activation('sigmoid', name = f'output_a{num_labels}')(x)\n    \n    model = tf.keras.models.Model(inputs = inp, outputs = out)\n    model.compile(optimizer = tfa.optimizers.AdamW(weight_decay = 1e-5, learning_rate = lr),\n                  loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.0008), \n                  metrics = tf.keras.losses.BinaryCrossentropy(name = 'mean_loss'), \n                 )\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_rtn(num_columns, num_labels, rethink_iter, num_layers, hidden_units, dropout_rates, lr):\n    inp = tf.keras.layers.Input(shape = (num_columns, ), name = 'inp')\n    x = tf.keras.layers.BatchNormalization(name = 'bn0')(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0], name = 'dp0')(x)\n    x = tf.keras.layers.RepeatVector(rethink_iter, name = 'rv')(x)\n\n    x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(hidden_units[0], name = 'd0'), name = 'wn0')(x)\n    x = tf.keras.layers.Activation('swish', name = 'a0')(x)\n    \n    for i in range(num_layers):\n        if i != num_layers - 1:\n            x = tf.keras.layers.LSTM(hidden_units[1], return_sequences = True, dropout = dropout_rates[1], name = f'lstm{i}')(x)\n        else:\n            x = tf.keras.layers.LSTM(hidden_units[1], return_sequences = False, dropout = dropout_rates[1], name = f'lstm{i}')(x)\n\n    x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(num_labels, name = f'd{num_labels}'), name = f'wn{num_labels}')(x)\n    out = tf.keras.layers.Activation('sigmoid', name = f'out{num_labels}')(x)\n\n    model = tf.keras.models.Model(inputs = inp, outputs = out)\n    model.compile(optimizer = tfa.optimizers.AdamW(weight_decay = 1e-5, learning_rate = lr), \n                  loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.0008), \n                  metrics = tf.keras.losses.BinaryCrossentropy(name = 'mean_loss'),\n                 )\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inference_model(X_train, Y_train_2, Y_nonscored, features, model_name, save_path, num_seeds, num_splits, \n                    model_params, X_test = None, sample_sub_path = None, verbose = 0):\n    start_time_all = time()\n    oof = Y_train_2.copy()\n    oof.loc[:, Y_train_2.columns] = 0\n    overall_score = []\n    if X_test is not None:\n        sub = pd.read_csv(sample_sub_path)\n        sub.loc[:, Y_train_2.columns] = 0\n    else:\n        sub = None\n    if 'RTN' in model_name:\n        model = create_rtn(len(features), 206, **model_params)\n    else:\n        model = create_mlp(len(features), 206, **model_params)\n    for nums, seed in enumerate(range(num_seeds)):\n        start_time_seed = time()\n        tf.random.set_seed(seed)\n        np.random.seed(seed)\n        random.seed(seed) \n        mean_score = 0\n        skf = MultilabelStratifiedKFold(n_splits = num_splits, random_state = seed, shuffle = True)\n        for n, (tr, te) in enumerate(skf.split(Y_train_2, Y_train_2)):\n            print(f'Model:{model_name}, Seed:{seed}, Fold:{n}', end = '\\r')\n            start_time_fold = time()\n            if CALCULATE_OOF:\n                x_tr, x_val = X_train.values[tr][:, features], X_train.values[te][:, features]\n                y_tr, y_val = Y_train_2.values[tr], Y_train_2.values[te]\n\n            if X_test is not None:\n                x_tt = X_test.values[:, features]\n            \n            ckp_path = save_path + f'{model_name}_Seed_{seed}_Fold_{n}.hdf5' \n            model.load_weights(ckp_path)\n            \n            if X_test is not None:\n                test_predict = model.predict(x_tt, batch_size = 1024)\n                sub.loc[:, Y_train_2.columns] += test_predict / (num_splits * num_seeds)\n            \n            if CALCULATE_OOF:\n                val_predict = model.predict(x_val, batch_size = 1024)\n#                 fold_score = hist['val_mean_loss'].min()\n                fold_score = log_loss_metric(y_val, val_predict)\n                mean_score += fold_score / num_splits\n                oof.loc[te, Y_train_2.columns] += val_predict / num_seeds\n                print(f'[{str(datetime.timedelta(seconds = time() - start_time_fold))[0:7]}] {model_name} Seed {seed}, Fold {n}:', fold_score)\n            \n#             del model\n#             x = gc.collect()\n#             K.clear_session()\n            \n        if CALCULATE_OOF:\n            print(f'[{str(datetime.timedelta(seconds = time() - start_time_seed))[0:7]}] {model_name} Seed {seed} Mean Score:', mean_score)\n    \n    if X_test is not None:\n        sub.loc[X_test['cp_type'] == 1, Y_train_2.columns] = 0\n    \n    if CALCULATE_OOF:\n        oof.loc[X_train['cp_type'] == 1, Y_train_2.columns] = 0\n        overall_score = log_loss_metric(Y_train_2.values, oof[Y_train_2.columns].values)\n        print(f'[{str(datetime.timedelta(seconds = time() - start_time_all))[0:7]}] {model_name} OOF Score:', overall_score)\n        \n    return overall_score, oof, sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_names = ['RTN', 'MLP_2', 'MLP_3L_2', 'MLP_4L_0']\n\nmodel_params = [{'rethink_iter': 2,  \n                 'num_layers': 3, \n                 'hidden_units': [256, 384],  \n                 'dropout_rates': [0.35975066158109587, 0.5438632643464203],\n                 'lr': 1e-4,\n                }, \n                {'hidden_units': [128, 896, 1024, 1792],  \n                 'dropout_rates': [0.4131287697962003, 0.33921841003415876, 0.13058255266781393, 0.20075775903486198, 0.3354496944535896], \n                 'lr': 1e-4,\n                }, \n                {'hidden_units': [640, 1536, 256],  \n                 'dropout_rates': [0.24204059045909448, 0.5084554762418632, 0.5433377329387503, 0.5644985971088049], \n                 'lr': 1e-4,\n                }, \n                {'hidden_units': [896, 768, 1280, 128],  \n                 'dropout_rates': [0.29522834449539126, 0.47640800252146664, 0.48364063345774644, 0.10344277546242515, 0.49718804108139353],\n                 'lr': 1e-4,\n                },]","execution_count":null,"outputs":[]},{"metadata":{"id":"YpJRV0O5cxV8","executionInfo":{"status":"ok","timestamp":1602953542287,"user_tz":-60,"elapsed":4372657,"user":{"displayName":"Yirun Zhang","photoUrl":"","userId":"05891579764514658952"}},"outputId":"bd1f6ab2-ca29-4eaf-aff0-c678b1b7f494","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"from time import time\n\nVERBOSE = 0\nsave_path = '../input/multilabel-pbestpre-mlp/'\nsample_sub_path = '../input/lish-moa/sample_submission.csv'\n\noverall_oof_scores = []\nsubmission_mlp = []\nfor m in range(len(model_params)):\n    print(model_names[m], model_params[m])\n    model_name = model_names[m]\n    oof_score, res, ss = inference_model(train, train_targets, train_targets_nonscored, top_feats, model_name, save_path, \n                                         N_STARTS, N_SPLITS, model_params[m], test, sample_sub_path, VERBOSE)\n    submission_mlp.append(ss)\n    \n    if CALCULATE_OOF:\n        # Overall OOF CV Score\n        tr_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv').drop('sig_id', axis = 1)\n        res_all = np.zeros(tr_targets[cols].shape)\n        res_all[train_features['cp_type'] == 0] = res[cols].values\n        overall_oof_score = log_loss_metric(tr_targets[cols].values, res_all)\n        overall_oof_scores.append(overall_oof_score)\n        print(f'{model_name} Overall OOF CV Score:', overall_oof_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if CALCULATE_OOF:\n    for n, name in enumerate(model_names):\n        print(f'{name} OOF:\\t', overall_oof_scores[n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble"},{"metadata":{"trusted":true},"cell_type":"code","source":"@njit\ndef post_process(pred, low, high):\n    pred_copy = pred.copy()\n    num_idx = 0\n    for i in range(pred_copy.shape[0]):\n        flag = np.zeros(pred_copy.shape[1])\n        array = pred_copy[i].copy()\n        for j in range(pred_copy.shape[1]):\n            if (pred_copy[i, j] <= low) or (pred_copy[i, j] >= high):\n                flag[j] = 1\n            array[j] = round(array[j])\n        if flag.all() and pred_copy[i].any(): #array.any()\n            pred_copy[i] = array\n            num_idx += 1\n    return pred_copy, num_idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss = pd.read_csv('../input/lish-moa/sample_submission.csv')\nss[cols] = 0.1296464333222514 * submission_mlp[0][cols].values + \\\n           0.2250012012344317 * submission_mlp[1][cols].values + \\\n           0.06289126408177141 * submission_mlp[2][cols].values + \\\n           0.06845298029278386 * submission_mlp[3][cols].values + \\\n           0.39624897237262097 * submission_tabnet[cols].values + \\\n           0.1177591486961407 * submission_grownet[cols].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if POST_PROCESS:\n    low = 0.015\n    high = 0.987\n\n    ss[cols], num_idx = post_process(ss[cols].values, low, high)\n    print(num_idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del data, data2, train2, test2, data_transformed, train_transformed, test_transformed\nif CALCULATE_OOF:\n    del tr_targets, overall_oof_score, overall_oof_scores, oof_preds, oof_preds_all, oof_targets, oof_targets_all, oof_score\nrubbish = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pseudo Labelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"if FINETUNE:\n    pseudo_targets = ss.loc[test['cp_type'] == 0, cols].values\n    pseudo_train = test.loc[test['cp_type'] == 0, test.columns].values","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"if FINETUNE:\n    import time\n    if cfg.strategy == \"KFOLD\":\n        oof_preds_all = []\n        oof_targets_all = []\n        scores_all =  []\n        scores_auc_all= []\n        preds_test = []\n        res = np.zeros(targets_tr.shape)\n        for nums, seed in enumerate(range(cfg.num_ensembling)):\n            print(\"## SEED : \", seed)\n            mskf = MultilabelStratifiedKFold(n_splits=cfg.SPLITS, random_state=cfg.seed+seed, shuffle=True)\n            oof_preds = []\n            oof_targets = []\n            scores = []\n            scores_auc = []\n            p = []\n            for j, (train_idx, val_idx) in enumerate(mskf.split(np.zeros(len(cat_tr)), targets_tr)):\n                print(\"FOLDS : \", j)\n\n                ## model\n                x_tr, y_tr = np.concatenate([cat_tr[train_idx], numerical_tr[train_idx]], axis=1), targets_tr[train_idx]\n                x_tr, y_tr = np.concatenate([x_tr, X_test[test['cp_type'] == 0, :]]), np.concatenate([y_tr, pseudo_targets])\n\n                X_train, y_train = torch.as_tensor(x_tr), torch.as_tensor(y_tr)\n                X_val, y_val = torch.as_tensor(np.concatenate([cat_tr[val_idx], numerical_tr[val_idx]], axis=1)), torch.as_tensor(targets_tr[val_idx])\n\n                model = TabNetRegressor(n_d=24, n_a=24, n_steps=1, gamma=1.3, lambda_sparse=0, cat_dims=cfg.cat_dims, cat_emb_dim=cfg.cat_emb_dim, cat_idxs=cfg.cats_idx, optimizer_fn=torch.optim.Adam,\n                                        optimizer_params=dict(lr=2e-2, weight_decay=1e-5), mask_type='entmax', device_name=cfg.device, scheduler_params=dict(milestones=[100, 150], gamma=0.9), \n                                        scheduler_fn=torch.optim.lr_scheduler.MultiStepLR)\n                name = cfg.save_name + f\"_fold{j}_{seed}\"\n                model.load_model(name)\n                model.fit(X_train=X_train, y_train=y_train, X_valid=X_val, y_valid=y_val, max_epochs=200, patience=5, batch_size=1024, virtual_batch_size=128,\n                          num_workers=0, drop_last=False, loss_fn=sbcewlogits, pretrain=True, optimizer_params=dict(lr=1e-4, weight_decay=1e-5))\n                model.load_best_model()\n                save_name = f\"PL_tabnet_raw_step1_fold{j}_{seed}\"\n                model.save_model(save_name)\n                \n                # preds on test\n                temp = model.predict(X_test)\n                p.append(torch.sigmoid(torch.as_tensor(temp)).detach().cpu().numpy())\n                \n                if CALCULATE_OOF_PL:\n                    preds = model.predict(X_val)\n                    preds = torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n                    score = log_loss_multi(y_val, preds)\n                    res[val_idx] += preds / cfg.num_ensembling\n\n                    ## save oof to compute the CV later\n                    oof_preds.append(preds)\n                    oof_targets.append(y_val)\n                    scores.append(score)\n                    scores_auc.append(auc_multi(y_val,preds))\n                    print(f\"validation fold {j} : {score}\")\n\n            p = np.stack(p)\n            preds_test.append(p)\n            if CALCULATE_OOF_PL:\n                oof_preds_all.append(np.concatenate(oof_preds))\n                oof_targets_all.append(np.concatenate(oof_targets))\n                scores_all.append(np.array(scores))\n                scores_auc_all.append(np.array(scores_auc))\n\n        preds_test = np.stack(preds_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if FINETUNE:\n    if CALCULATE_OOF_PL:\n\n        if cfg.strategy == \"KFOLD\":\n\n            for i in range(cfg.num_ensembling): \n                print(\"CV score fold : \", log_loss_multi(oof_targets_all[i], oof_preds_all[i]))\n                print(\"auc mean : \", sum(scores_auc_all[i])/len(scores_auc_all[i]))\n\n        # Overall OOF CV Score\n        tr_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv').drop('sig_id', axis = 1)\n        res_all = np.zeros(tr_targets[cols].shape)\n        res_all[train_features['cp_type'] == 0] = res\n        overall_oof_score = log_loss_metric(tr_targets[cols].values, res_all)\n        print(f'TabNet Overall OOF CV Score:', overall_oof_score)\n        oof_tabnet2 = res_all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if FINETUNE:\n\n    submission_tabnet2 = pd.read_csv('../input/lish-moa/sample_submission.csv')\n    submission_tabnet2[cols] = preds_test.mean(1).mean(0)\n    submission_tabnet2.loc[test['cp_type'] == 1, cols] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom time import time\n\ndef train_model(X_train, Y_train_2, Y_nonscored, features, model_name, save_path, num_seeds, num_splits, \n                model_params, X_test = None, sample_sub_path = None, pseudo_labeling = True, verbose = 0):\n    start_time_all = time()\n    oof = Y_train_2.copy()\n    oof.loc[:, Y_train_2.columns] = 0\n    overall_score = []\n    if X_test is not None:\n        sub = pd.read_csv(sample_sub_path)\n        sub.loc[:, Y_train_2.columns] = 0\n    else:\n        sub = None\n    if 'RTN' in model_name:\n        model = create_rtn(len(features), 206, **model_params)\n    else:\n        model = create_mlp(len(features), 206, **model_params)\n    for nums, seed in enumerate(range(num_seeds)):\n        start_time_seed = time()\n        tf.random.set_seed(seed)\n        np.random.seed(seed)\n        random.seed(seed) \n        mean_score = 0\n        skf = MultilabelStratifiedKFold(n_splits = num_splits, random_state = seed, shuffle = True)\n        for n, (tr, te) in enumerate(skf.split(Y_train_2, Y_train_2)):\n            print(f'Model:{model_name}, Seed:{seed}, Fold:{n}', end = '\\r')\n            start_time_fold = time()\n            x_tr, x_val = X_train.values[tr][:, features], X_train.values[te][:, features]\n            y_tr, y_val = Y_train_2.values[tr], Y_train_2.values[te]\n            if pseudo_labeling:\n                x_tr = np.concatenate([x_tr, pseudo_train[:, features]])\n                y_tr = np.concatenate([y_tr, pseudo_targets])\n\n            if X_test is not None:\n                x_tt = X_test.values[:, features]\n            \n            ckp_path = save_path + f'{model_name}_Seed_{seed}_Fold_{n}.hdf5'\n\n            model.load_weights(ckp_path)\n            rlr = ReduceLROnPlateau(monitor = 'val_mean_loss', factor = 0.1, patience = 3, \n                                    verbose = verbose, min_delta = 1e-4, mode = 'min')\n            ckp = ModelCheckpoint(f'{model_name}_Seed_{seed}_Fold_{n}.hdf5', monitor = 'val_mean_loss', verbose = 0, \n                                  save_best_only = True, save_weights_only = True, mode = 'min')\n            es = EarlyStopping(monitor = 'val_mean_loss', min_delta = 1e-4, patience = 5, mode = 'min', \n                               baseline = None, restore_best_weights = True, verbose = verbose)\n            history = model.fit(x_tr, y_tr, validation_data = (x_val, y_val), epochs = 1000, \n                                batch_size = 128, callbacks = [rlr, ckp, es], verbose = verbose)\n            hist = pd.DataFrame(history.history)\n            model.load_weights(f'{model_name}_Seed_{seed}_Fold_{n}.hdf5')\n            \n            if X_test is not None:\n                test_predict = model.predict(x_tt, batch_size = 1024)\n                sub.loc[:, Y_train_2.columns] += test_predict / (num_splits * num_seeds)\n            \n            if CALCULATE_OOF_PL:\n                val_predict = model.predict(x_val, batch_size = 1024)\n                fold_score = hist['val_mean_loss'].min()\n#                 fold_score = log_loss_metric(y_val, val_predict)\n                mean_score += fold_score / num_splits\n                oof.loc[te, Y_train_2.columns] += val_predict / num_seeds\n                print(f'[{str(datetime.timedelta(seconds = time() - start_time_fold))[0:7]}] {model_name} Seed {seed}, Fold {n}:', fold_score)\n                  \n#             del model\n#             x = gc.collect()\n#             K.clear_session()\n            \n        if CALCULATE_OOF_PL:\n            print(f'[{str(datetime.timedelta(seconds = time() - start_time_seed))[0:7]}] {model_name} Seed {seed} Mean Score:', mean_score)\n    \n    if X_test is not None:\n        sub.loc[X_test['cp_type'] == 1, Y_train_2.columns] = 0\n    \n    if CALCULATE_OOF_PL:\n        oof.loc[X_train['cp_type'] == 1, Y_train_2.columns] = 0\n        overall_score = log_loss_metric(Y_train_2.values, oof[Y_train_2.columns].values)\n        print(f'[{str(datetime.timedelta(seconds = time() - start_time_all))[0:7]}] {model_name} OOF Score:', overall_score)\n        \n    return overall_score, oof, sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if FINETUNE:\n    Pseudo_Labeling = True\n    VERBOSE = 0\n    save_path = '../input/multilabel-pbestpre-mlp/'\n    sample_sub_path = '../input/lish-moa/sample_submission.csv'\n\n    overall_oof_scores = []\n    oof_mlp2 = []\n    submission_mlp2 = []\n    for m in range(len(model_params)):\n        print(model_names[m], model_params[m])\n        model_name = model_names[m]\n        oof_score, res, ss = train_model(train, train_targets, train_targets_nonscored, top_feats, model_name, save_path, \n                                         N_STARTS, N_SPLITS, model_params[m], test, sample_sub_path, Pseudo_Labeling, VERBOSE)\n        submission_mlp2.append(ss)\n\n        if CALCULATE_OOF_PL:\n            # Overall OOF CV Score\n            tr_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv').drop('sig_id', axis = 1)\n            res_all = np.zeros(tr_targets[cols].shape)\n            res_all[train_features['cp_type'] == 0] = res[cols].values\n            overall_oof_score = log_loss_metric(tr_targets[cols].values, res_all)\n            overall_oof_scores.append(overall_oof_score)\n            print(f'{model_name} Overall OOF CV Score:', overall_oof_score)\n            oof_mlp2.append(res_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if FINETUNE:\n    if CALCULATE_OOF_PL:\n        for n, name in enumerate(model_names):\n            print(f'{name} OOF:\\t', overall_oof_scores[n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Ensemble"},{"metadata":{"trusted":true},"cell_type":"code","source":"if FINETUNE:\n    ss = pd.read_csv('../input/lish-moa/sample_submission.csv')\n    ss[cols] = 0.1296464333222514 * submission_mlp2[0][cols].values + \\\n               0.2250012012344317 * submission_mlp2[1][cols].values + \\\n               0.06289126408177141 * submission_mlp2[2][cols].values + \\\n               0.06845298029278386 * submission_mlp2[3][cols].values + \\\n               0.39624897237262097 * submission_tabnet2[cols].values + \\\n               0.1177591486961407 * submission_grownet[cols].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if FINETUNE:\n    if POST_PROCESS:\n        low = 0.015\n        high = 0.987\n\n        ss[cols], num_idx = post_process(ss[cols].values, low, high)\n        print(num_idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.to_csv('submission.csv', index = False)\nss.head(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}