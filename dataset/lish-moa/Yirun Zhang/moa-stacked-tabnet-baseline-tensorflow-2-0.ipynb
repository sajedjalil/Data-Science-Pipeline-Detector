{"cells":[{"metadata":{},"cell_type":"markdown","source":"# MoA Stacked TabNet Baseline\n\nChange 'num_decision_steps' to 1 makes OOF score much more better..."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"kw1VW6DCvgSq","outputId":"030d81e0-579d-463d-b2ed-6c714151a063"},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport os\nimport gc\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\ntf.random.set_seed(42)\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nimport tensorflow_addons as tfa\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom scipy.optimize import minimize\nfrom tqdm.notebook import tqdm\nfrom time import time\n\nprint(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MIXED_PRECISION = False\nXLA_ACCELERATE = True\n\nif MIXED_PRECISION:\n    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n    mixed_precision.set_policy(policy)\n    print('Mixed precision enabled')\n\nif XLA_ACCELERATE:\n    tf.config.optimizer.set_jit(True)\n    print('Accelerated Linear Algebra enabled')","execution_count":null,"outputs":[]},{"metadata":{"id":"dSVuPpi2vgSv"},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"UvG3N1HHvgSv"},"cell_type":"code","source":"train_features = pd.read_csv('../input/lish-moa/train_features.csv')\ntrain_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\ntest_features = pd.read_csv('../input/lish-moa/test_features.csv')\n\nss = pd.read_csv('../input/lish-moa/sample_submission.csv')\n\ncols = [c for c in ss.columns.values if c != 'sig_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"JItYfC6jvgSy"},"cell_type":"code","source":"def preprocess(df):\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del df['sig_id']\n    return df\n\n# [Fast Numpy Log Loss] https://www.kaggle.com/gogo827jz/optimise-blending-weights-4-5x-faster-log-loss\ndef log_loss_metric(y_true, y_pred):\n    loss = 0\n    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    for i in range(y_pred.shape[1]):\n        loss += - np.mean(y_true[:, i] * np.log(y_pred_clip[:, i]) + (1 - y_true[:, i]) * np.log(1 - y_pred_clip[:, i]))\n    return loss / y_pred.shape[1]\n\ntrain = preprocess(train_features)\ntest = preprocess(test_features)\n\ndel train_targets['sig_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"id":"Cg0gF9u5vgS1","outputId":"2dcd7162-6e0e-427c-cfc0-9e36329cd8a2"},"cell_type":"code","source":"top_feats = [  0,   1,   2,   3,   5,   6,   8,   9,  10,  11,  12,  14,  15,\n        16,  18,  19,  20,  21,  23,  24,  25,  27,  28,  29,  30,  31,\n        32,  33,  34,  35,  36,  37,  39,  40,  41,  42,  44,  45,  46,\n        48,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,\n        63,  64,  65,  66,  68,  69,  70,  71,  72,  73,  74,  75,  76,\n        78,  79,  80,  81,  82,  83,  84,  86,  87,  88,  89,  90,  92,\n        93,  94,  95,  96,  97,  99, 100, 101, 103, 104, 105, 106, 107,\n       108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,\n       121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 132, 133, 134,\n       135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147,\n       149, 150, 151, 152, 153, 154, 155, 157, 159, 160, 161, 163, 164,\n       165, 166, 167, 168, 169, 170, 172, 173, 175, 176, 177, 178, 180,\n       181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 195,\n       197, 198, 199, 202, 203, 205, 206, 208, 209, 210, 211, 212, 213,\n       214, 215, 218, 219, 220, 221, 222, 224, 225, 227, 228, 229, 230,\n       231, 232, 233, 234, 236, 238, 239, 240, 241, 242, 243, 244, 245,\n       246, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260,\n       261, 263, 265, 266, 268, 270, 271, 272, 273, 275, 276, 277, 279,\n       282, 283, 286, 287, 288, 289, 290, 294, 295, 296, 297, 299, 300,\n       301, 302, 303, 304, 305, 306, 308, 309, 310, 311, 312, 313, 315,\n       316, 317, 320, 321, 322, 324, 325, 326, 327, 328, 329, 330, 331,\n       332, 333, 334, 335, 338, 339, 340, 341, 343, 344, 345, 346, 347,\n       349, 350, 351, 352, 353, 355, 356, 357, 358, 359, 360, 361, 362,\n       363, 364, 365, 366, 368, 369, 370, 371, 372, 374, 375, 376, 377,\n       378, 379, 380, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n       392, 393, 394, 395, 397, 398, 399, 400, 401, 403, 405, 406, 407,\n       408, 410, 411, 412, 413, 414, 415, 417, 418, 419, 420, 421, 422,\n       423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435,\n       436, 437, 438, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450,\n       452, 453, 454, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n       466, 468, 469, 471, 472, 473, 474, 475, 476, 477, 478, 479, 482,\n       483, 485, 486, 487, 488, 489, 491, 492, 494, 495, 496, 500, 501,\n       502, 503, 505, 506, 507, 509, 510, 511, 512, 513, 514, 516, 517,\n       518, 519, 521, 523, 525, 526, 527, 528, 529, 530, 531, 532, 533,\n       534, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547,\n       549, 550, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563,\n       564, 565, 566, 567, 569, 570, 571, 572, 573, 574, 575, 577, 580,\n       581, 582, 583, 586, 587, 590, 591, 592, 593, 595, 596, 597, 598,\n       599, 600, 601, 602, 603, 605, 607, 608, 609, 611, 612, 613, 614,\n       615, 616, 617, 619, 622, 623, 625, 627, 630, 631, 632, 633, 634,\n       635, 637, 638, 639, 642, 643, 644, 645, 646, 647, 649, 650, 651,\n       652, 654, 655, 658, 659, 660, 661, 662, 663, 664, 666, 667, 668,\n       669, 670, 672, 674, 675, 676, 677, 678, 680, 681, 682, 684, 685,\n       686, 687, 688, 689, 691, 692, 694, 695, 696, 697, 699, 700, 701,\n       702, 703, 704, 705, 707, 708, 709, 711, 712, 713, 714, 715, 716,\n       717, 723, 725, 727, 728, 729, 730, 731, 732, 734, 736, 737, 738,\n       739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751,\n       752, 753, 754, 755, 756, 758, 759, 760, 761, 762, 763, 764, 765,\n       766, 767, 769, 770, 771, 772, 774, 775, 780, 781, 782, 783, 784,\n       785, 787, 788, 790, 793, 795, 797, 799, 800, 801, 805, 808, 809,\n       811, 812, 813, 816, 819, 820, 821, 822, 823, 825, 826, 827, 829,\n       831, 832, 833, 834, 835, 837, 838, 839, 840, 841, 842, 844, 845,\n       846, 847, 848, 850, 851, 852, 854, 855, 856, 858, 860, 861, 862,\n       864, 867, 868, 870, 871, 873, 874]\nprint(len(top_feats))","execution_count":null,"outputs":[]},{"metadata":{"id":"3MC95RukvgS3"},"cell_type":"markdown","source":"# Model Functions\n\nModified from https://github.com/titu1994/tf-TabNet to support multi-label classification"},{"metadata":{"id":"GFWTAB8bXpDX","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def register_keras_custom_object(cls):\n    tf.keras.utils.get_custom_objects()[cls.__name__] = cls\n    return cls\n\n\ndef glu(x, n_units=None):\n    \"\"\"Generalized linear unit nonlinear activation.\"\"\"\n    if n_units is None:\n        n_units = tf.shape(x)[-1] // 2\n\n    return x[..., :n_units] * tf.nn.sigmoid(x[..., n_units:])\n\n\n\"\"\"\nCode replicated from https://github.com/tensorflow/addons/blob/master/tensorflow_addons/activations/sparsemax.py\n\"\"\"\n\n\n@register_keras_custom_object\n@tf.function\ndef sparsemax(logits, axis):\n    \"\"\"Sparsemax activation function [1].\n    For each batch `i` and class `j` we have\n      $$sparsemax[i, j] = max(logits[i, j] - tau(logits[i, :]), 0)$$\n    [1]: https://arxiv.org/abs/1602.02068\n    Args:\n        logits: Input tensor.\n        axis: Integer, axis along which the sparsemax operation is applied.\n    Returns:\n        Tensor, output of sparsemax transformation. Has the same type and\n        shape as `logits`.\n    Raises:\n        ValueError: In case `dim(logits) == 1`.\n    \"\"\"\n    logits = tf.convert_to_tensor(logits, name=\"logits\")\n\n    # We need its original shape for shape inference.\n    shape = logits.get_shape()\n    rank = shape.rank\n    is_last_axis = (axis == -1) or (axis == rank - 1)\n\n    if is_last_axis:\n        output = _compute_2d_sparsemax(logits)\n        output.set_shape(shape)\n        return output\n\n    # If dim is not the last dimension, we have to do a transpose so that we can\n    # still perform softmax on its last dimension.\n\n    # Swap logits' dimension of dim and its last dimension.\n    rank_op = tf.rank(logits)\n    axis_norm = axis % rank\n    logits = _swap_axis(logits, axis_norm, tf.math.subtract(rank_op, 1))\n\n    # Do the actual softmax on its last dimension.\n    output = _compute_2d_sparsemax(logits)\n    output = _swap_axis(output, axis_norm, tf.math.subtract(rank_op, 1))\n\n    # Make shape inference work since transpose may erase its static shape.\n    output.set_shape(shape)\n    return output\n\n\ndef _swap_axis(logits, dim_index, last_index, **kwargs):\n    return tf.transpose(\n        logits,\n        tf.concat(\n            [\n                tf.range(dim_index),\n                [last_index],\n                tf.range(dim_index + 1, last_index),\n                [dim_index],\n            ],\n            0,\n        ),\n        **kwargs,\n    )\n\n\ndef _compute_2d_sparsemax(logits):\n    \"\"\"Performs the sparsemax operation when axis=-1.\"\"\"\n    shape_op = tf.shape(logits)\n    obs = tf.math.reduce_prod(shape_op[:-1])\n    dims = shape_op[-1]\n\n    # In the paper, they call the logits z.\n    # The mean(logits) can be substracted from logits to make the algorithm\n    # more numerically stable. the instability in this algorithm comes mostly\n    # from the z_cumsum. Substacting the mean will cause z_cumsum to be close\n    # to zero. However, in practise the numerical instability issues are very\n    # minor and substacting the mean causes extra issues with inf and nan\n    # input.\n    # Reshape to [obs, dims] as it is almost free and means the remanining\n    # code doesn't need to worry about the rank.\n    z = tf.reshape(logits, [obs, dims])\n\n    # sort z\n    z_sorted, _ = tf.nn.top_k(z, k=dims)\n\n    # calculate k(z)\n    z_cumsum = tf.math.cumsum(z_sorted, axis=-1)\n    k = tf.range(1, tf.cast(dims, logits.dtype) + 1, dtype=logits.dtype)\n    z_check = 1 + k * z_sorted > z_cumsum\n    # because the z_check vector is always [1,1,...1,0,0,...0] finding the\n    # (index + 1) of the last `1` is the same as just summing the number of 1.\n    k_z = tf.math.reduce_sum(tf.cast(z_check, tf.int32), axis=-1)\n\n    # calculate tau(z)\n    # If there are inf values or all values are -inf, the k_z will be zero,\n    # this is mathematically invalid and will also cause the gather_nd to fail.\n    # Prevent this issue for now by setting k_z = 1 if k_z = 0, this is then\n    # fixed later (see p_safe) by returning p = nan. This results in the same\n    # behavior as softmax.\n    k_z_safe = tf.math.maximum(k_z, 1)\n    indices = tf.stack([tf.range(0, obs), tf.reshape(k_z_safe, [-1]) - 1], axis=1)\n    tau_sum = tf.gather_nd(z_cumsum, indices)\n    tau_z = (tau_sum - 1) / tf.cast(k_z, logits.dtype)\n\n    # calculate p\n    p = tf.math.maximum(tf.cast(0, logits.dtype), z - tf.expand_dims(tau_z, -1))\n    # If k_z = 0 or if z = nan, then the input is invalid\n    p_safe = tf.where(\n        tf.expand_dims(\n            tf.math.logical_or(tf.math.equal(k_z, 0), tf.math.is_nan(z_cumsum[:, -1])),\n            axis=-1,\n        ),\n        tf.fill([obs, dims], tf.cast(float(\"nan\"), logits.dtype)),\n        p,\n    )\n\n    # Reshape back to original size\n    p_safe = tf.reshape(p_safe, shape_op)\n    return p_safe\n\n\n\"\"\"\nCode replicated from https://github.com/tensorflow/addons/blob/master/tensorflow_addons/layers/normalizations.py\n\"\"\"\n\n\n@register_keras_custom_object\nclass GroupNormalization(tf.keras.layers.Layer):\n    \"\"\"Group normalization layer.\n    Group Normalization divides the channels into groups and computes\n    within each group the mean and variance for normalization.\n    Empirically, its accuracy is more stable than batch norm in a wide\n    range of small batch sizes, if learning rate is adjusted linearly\n    with batch sizes.\n    Relation to Layer Normalization:\n    If the number of groups is set to 1, then this operation becomes identical\n    to Layer Normalization.\n    Relation to Instance Normalization:\n    If the number of groups is set to the\n    input dimension (number of groups is equal\n    to number of channels), then this operation becomes\n    identical to Instance Normalization.\n    Arguments\n        groups: Integer, the number of groups for Group Normalization.\n            Can be in the range [1, N] where N is the input dimension.\n            The input dimension must be divisible by the number of groups.\n        axis: Integer, the axis that should be normalized.\n        epsilon: Small float added to variance to avoid dividing by zero.\n        center: If True, add offset of `beta` to normalized tensor.\n            If False, `beta` is ignored.\n        scale: If True, multiply by `gamma`.\n            If False, `gamma` is not used.\n        beta_initializer: Initializer for the beta weight.\n        gamma_initializer: Initializer for the gamma weight.\n        beta_regularizer: Optional regularizer for the beta weight.\n        gamma_regularizer: Optional regularizer for the gamma weight.\n        beta_constraint: Optional constraint for the beta weight.\n        gamma_constraint: Optional constraint for the gamma weight.\n    Input shape\n        Arbitrary. Use the keyword argument `input_shape`\n        (tuple of integers, does not include the samples axis)\n        when using this layer as the first layer in a model.\n    Output shape\n        Same shape as input.\n    References\n        - [Group Normalization](https://arxiv.org/abs/1803.08494)\n    \"\"\"\n\n    def __init__(\n            self,\n            groups: int = 2,\n            axis: int = -1,\n            epsilon: float = 1e-3,\n            center: bool = True,\n            scale: bool = True,\n            beta_initializer=\"zeros\",\n            gamma_initializer=\"ones\",\n            beta_regularizer=None,\n            gamma_regularizer=None,\n            beta_constraint=None,\n            gamma_constraint=None,\n            **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.supports_masking = True\n        self.groups = groups\n        self.axis = axis\n        self.epsilon = epsilon\n        self.center = center\n        self.scale = scale\n        self.beta_initializer = tf.keras.initializers.get(beta_initializer)\n        self.gamma_initializer = tf.keras.initializers.get(gamma_initializer)\n        self.beta_regularizer = tf.keras.regularizers.get(beta_regularizer)\n        self.gamma_regularizer = tf.keras.regularizers.get(gamma_regularizer)\n        self.beta_constraint = tf.keras.constraints.get(beta_constraint)\n        self.gamma_constraint = tf.keras.constraints.get(gamma_constraint)\n        self._check_axis()\n\n    def build(self, input_shape):\n\n        self._check_if_input_shape_is_none(input_shape)\n        self._set_number_of_groups_for_instance_norm(input_shape)\n        self._check_size_of_dimensions(input_shape)\n        self._create_input_spec(input_shape)\n\n        self._add_gamma_weight(input_shape)\n        self._add_beta_weight(input_shape)\n        self.built = True\n        super().build(input_shape)\n\n    def call(self, inputs, training=None):\n        # Training=none is just for compat with batchnorm signature call\n        input_shape = tf.keras.backend.int_shape(inputs)\n        tensor_input_shape = tf.shape(inputs)\n\n        reshaped_inputs, group_shape = self._reshape_into_groups(\n            inputs, input_shape, tensor_input_shape\n        )\n\n        normalized_inputs = self._apply_normalization(reshaped_inputs, input_shape)\n\n        outputs = tf.reshape(normalized_inputs, tensor_input_shape)\n\n        return outputs\n\n    def get_config(self):\n        config = {\n            \"groups\": self.groups,\n            \"axis\": self.axis,\n            \"epsilon\": self.epsilon,\n            \"center\": self.center,\n            \"scale\": self.scale,\n            \"beta_initializer\": tf.keras.initializers.serialize(self.beta_initializer),\n            \"gamma_initializer\": tf.keras.initializers.serialize(\n                self.gamma_initializer\n            ),\n            \"beta_regularizer\": tf.keras.regularizers.serialize(self.beta_regularizer),\n            \"gamma_regularizer\": tf.keras.regularizers.serialize(\n                self.gamma_regularizer\n            ),\n            \"beta_constraint\": tf.keras.constraints.serialize(self.beta_constraint),\n            \"gamma_constraint\": tf.keras.constraints.serialize(self.gamma_constraint),\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def _reshape_into_groups(self, inputs, input_shape, tensor_input_shape):\n\n        group_shape = [tensor_input_shape[i] for i in range(len(input_shape))]\n        group_shape[self.axis] = input_shape[self.axis] // self.groups\n        group_shape.insert(self.axis, self.groups)\n        group_shape = tf.stack(group_shape)\n        reshaped_inputs = tf.reshape(inputs, group_shape)\n        return reshaped_inputs, group_shape\n\n    def _apply_normalization(self, reshaped_inputs, input_shape):\n\n        group_shape = tf.keras.backend.int_shape(reshaped_inputs)\n        group_reduction_axes = list(range(1, len(group_shape)))\n        axis = -2 if self.axis == -1 else self.axis - 1\n        group_reduction_axes.pop(axis)\n\n        mean, variance = tf.nn.moments(\n            reshaped_inputs, group_reduction_axes, keepdims=True\n        )\n\n        gamma, beta = self._get_reshaped_weights(input_shape)\n        normalized_inputs = tf.nn.batch_normalization(\n            reshaped_inputs,\n            mean=mean,\n            variance=variance,\n            scale=gamma,\n            offset=beta,\n            variance_epsilon=self.epsilon,\n        )\n        return normalized_inputs\n\n    def _get_reshaped_weights(self, input_shape):\n        broadcast_shape = self._create_broadcast_shape(input_shape)\n        gamma = None\n        beta = None\n        if self.scale:\n            gamma = tf.reshape(self.gamma, broadcast_shape)\n\n        if self.center:\n            beta = tf.reshape(self.beta, broadcast_shape)\n        return gamma, beta\n\n    def _check_if_input_shape_is_none(self, input_shape):\n        dim = input_shape[self.axis]\n        if dim is None:\n            raise ValueError(\n                \"Axis \" + str(self.axis) + \" of \"\n                                           \"input tensor should have a defined dimension \"\n                                           \"but the layer received an input with shape \" + str(input_shape) + \".\"\n            )\n\n    def _set_number_of_groups_for_instance_norm(self, input_shape):\n        dim = input_shape[self.axis]\n\n        if self.groups == -1:\n            self.groups = dim\n\n    def _check_size_of_dimensions(self, input_shape):\n\n        dim = input_shape[self.axis]\n        if dim < self.groups:\n            raise ValueError(\n                \"Number of groups (\" + str(self.groups) + \") cannot be \"\n                                                          \"more than the number of channels (\" + str(dim) + \").\"\n            )\n\n        if dim % self.groups != 0:\n            raise ValueError(\n                \"Number of groups (\" + str(self.groups) + \") must be a \"\n                                                          \"multiple of the number of channels (\" + str(dim) + \").\"\n            )\n\n    def _check_axis(self):\n\n        if self.axis == 0:\n            raise ValueError(\n                \"You are trying to normalize your batch axis. Do you want to \"\n                \"use tf.layer.batch_normalization instead\"\n            )\n\n    def _create_input_spec(self, input_shape):\n\n        dim = input_shape[self.axis]\n        self.input_spec = tf.keras.layers.InputSpec(\n            ndim=len(input_shape), axes={self.axis: dim}\n        )\n\n    def _add_gamma_weight(self, input_shape):\n\n        dim = input_shape[self.axis]\n        shape = (dim,)\n\n        if self.scale:\n            self.gamma = self.add_weight(\n                shape=shape,\n                name=\"gamma\",\n                initializer=self.gamma_initializer,\n                regularizer=self.gamma_regularizer,\n                constraint=self.gamma_constraint,\n            )\n        else:\n            self.gamma = None\n\n    def _add_beta_weight(self, input_shape):\n\n        dim = input_shape[self.axis]\n        shape = (dim,)\n\n        if self.center:\n            self.beta = self.add_weight(\n                shape=shape,\n                name=\"beta\",\n                initializer=self.beta_initializer,\n                regularizer=self.beta_regularizer,\n                constraint=self.beta_constraint,\n            )\n        else:\n            self.beta = None\n\n    def _create_broadcast_shape(self, input_shape):\n        broadcast_shape = [1] * len(input_shape)\n        broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n        broadcast_shape.insert(self.axis, self.groups)\n        return broadcast_shape\n\nclass TransformBlock(tf.keras.Model):\n\n    def __init__(self, features,\n                 norm_type,\n                 momentum=0.9,\n                 virtual_batch_size=None,\n                 groups=2,\n                 block_name='',\n                 **kwargs):\n        super(TransformBlock, self).__init__(**kwargs)\n\n        self.features = features\n        self.norm_type = norm_type\n        self.momentum = momentum\n        self.groups = groups\n        self.virtual_batch_size = virtual_batch_size\n\n        self.transform = tf.keras.layers.Dense(self.features, use_bias=False, name=f'transformblock_dense_{block_name}')\n\n        if norm_type == 'batch':\n            self.bn = tf.keras.layers.BatchNormalization(axis=-1, momentum=momentum,\n                                                         virtual_batch_size=virtual_batch_size,\n                                                         name=f'transformblock_bn_{block_name}')\n\n        else:\n            self.bn = GroupNormalization(axis=-1, groups=self.groups, name=f'transformblock_gn_{block_name}')\n\n    def call(self, inputs, training=None):\n        x = self.transform(inputs)\n        x = self.bn(x, training=training)\n        return x\n\n\nclass TabNet(tf.keras.Model):\n\n    def __init__(self, feature_columns,\n                 feature_dim=64,\n                 output_dim=64,\n                 num_features=None,\n                 num_decision_steps=5,\n                 relaxation_factor=1.5,\n                 sparsity_coefficient=1e-5,\n                 norm_type='group',\n                 batch_momentum=0.98,\n                 virtual_batch_size=None,\n                 num_groups=2,\n                 epsilon=1e-5,\n                 **kwargs):\n        \"\"\"\n        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n        # Hyper Parameter Tuning (Excerpt from the paper)\n        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n        selection:\n            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n            overfitting and yield poor generalization.\n            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n            Nsteps value favors for a larger γ.\n            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n            much smaller than the batch size.\n            - Initially large learning rate is important, which should be gradually decayed until convergence.\n        Args:\n            feature_columns: The Tensorflow feature columns for the dataset.\n            feature_dim (N_a): Dimensionality of the hidden representation in feature\n                transformation block. Each layer first maps the representation to a\n                2*feature_dim-dimensional output and half of it is used to determine the\n                nonlinearity of the GLU activation where the other half is used as an\n                input to GLU, and eventually feature_dim-dimensional output is\n                transferred to the next layer.\n            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n                later mapped to the final classification or regression output.\n            num_features: The number of input features (i.e the number of columns for\n                tabular data assuming each feature is represented with 1 dimension).\n            num_decision_steps(N_steps): Number of sequential decision steps.\n            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n                feature at different decision steps. When it is 1, a feature is enforced\n                to be used only at one decision step and as it increases, more\n                flexibility is provided to use a feature at multiple decision steps.\n            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n                Sparsity may provide a favorable inductive bias for convergence to\n                higher accuracy for some datasets where most of the input features are redundant.\n            norm_type: Type of normalization to perform for the model. Can be either\n                'batch' or 'group'. 'group' is the default.\n            batch_momentum: Momentum in ghost batch normalization.\n            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n                overall batch size should be an integer multiple of virtual_batch_size.\n            num_groups: Number of groups used for group normalization.\n            epsilon: A small number for numerical stability of the entropy calculations.\n        \"\"\"\n        super(TabNet, self).__init__(**kwargs)\n\n        # Input checks\n        if feature_columns is not None:\n            if type(feature_columns) not in (list, tuple):\n                raise ValueError(\"`feature_columns` must be a list or a tuple.\")\n\n            if len(feature_columns) == 0:\n                raise ValueError(\"`feature_columns` must be contain at least 1 tf.feature_column !\")\n\n            if num_features is None:\n                num_features = len(feature_columns)\n            else:\n                num_features = int(num_features)\n\n        else:\n            if num_features is None:\n                raise ValueError(\"If `feature_columns` is None, then `num_features` cannot be None.\")\n\n        if num_decision_steps < 1:\n            raise ValueError(\"Num decision steps must be greater than 0.\")\n\n        if feature_dim < output_dim:\n            raise ValueError(\"To compute `features_for_coef`, feature_dim must be larger than output dim\")\n\n        feature_dim = int(feature_dim)\n        output_dim = int(output_dim)\n        num_decision_steps = int(num_decision_steps)\n        relaxation_factor = float(relaxation_factor)\n        sparsity_coefficient = float(sparsity_coefficient)\n        batch_momentum = float(batch_momentum)\n        num_groups = max(1, int(num_groups))\n        epsilon = float(epsilon)\n\n        if relaxation_factor < 0.:\n            raise ValueError(\"`relaxation_factor` cannot be negative !\")\n\n        if sparsity_coefficient < 0.:\n            raise ValueError(\"`sparsity_coefficient` cannot be negative !\")\n\n        if virtual_batch_size is not None:\n            virtual_batch_size = int(virtual_batch_size)\n\n        if norm_type not in ['batch', 'group']:\n            raise ValueError(\"`norm_type` must be either `batch` or `group`\")\n\n        self.feature_columns = feature_columns\n        self.num_features = num_features\n        self.feature_dim = feature_dim\n        self.output_dim = output_dim\n\n        self.num_decision_steps = num_decision_steps\n        self.relaxation_factor = relaxation_factor\n        self.sparsity_coefficient = sparsity_coefficient\n        self.norm_type = norm_type\n        self.batch_momentum = batch_momentum\n        self.virtual_batch_size = virtual_batch_size\n        self.num_groups = num_groups\n        self.epsilon = epsilon\n\n        # if num_decision_steps > 1:\n            # features_for_coeff = feature_dim - output_dim\n            # print(f\"[TabNet]: {features_for_coeff} features will be used for decision steps.\")\n\n        if self.feature_columns is not None:\n            self.input_features = tf.keras.layers.DenseFeatures(feature_columns, trainable=True)\n\n            if self.norm_type == 'batch':\n                self.input_bn = tf.keras.layers.BatchNormalization(axis=-1, momentum=batch_momentum, name='input_bn')\n            else:\n                self.input_bn = GroupNormalization(axis=-1, groups=self.num_groups, name='input_gn')\n\n        else:\n            self.input_features = None\n            self.input_bn = None\n\n        self.transform_f1 = TransformBlock(2 * self.feature_dim, self.norm_type,\n                                           self.batch_momentum, self.virtual_batch_size, self.num_groups,\n                                           block_name='f1')\n\n        self.transform_f2 = TransformBlock(2 * self.feature_dim, self.norm_type,\n                                           self.batch_momentum, self.virtual_batch_size, self.num_groups,\n                                           block_name='f2')\n\n        self.transform_f3_list = [\n            TransformBlock(2 * self.feature_dim, self.norm_type,\n                           self.batch_momentum, self.virtual_batch_size, self.num_groups, block_name=f'f3_{i}')\n            for i in range(self.num_decision_steps)\n        ]\n\n        self.transform_f4_list = [\n            TransformBlock(2 * self.feature_dim, self.norm_type,\n                           self.batch_momentum, self.virtual_batch_size, self.num_groups, block_name=f'f4_{i}')\n            for i in range(self.num_decision_steps)\n        ]\n\n        self.transform_coef_list = [\n            TransformBlock(self.num_features, self.norm_type,\n                           self.batch_momentum, self.virtual_batch_size, self.num_groups, block_name=f'coef_{i}')\n            for i in range(self.num_decision_steps - 1)\n        ]\n\n        self._step_feature_selection_masks = None\n        self._step_aggregate_feature_selection_mask = None\n\n    def call(self, inputs, training=None):\n        if self.input_features is not None:\n            features = self.input_features(inputs)\n            features = self.input_bn(features, training=training)\n\n        else:\n            features = inputs\n\n        batch_size = tf.shape(features)[0]\n        self._step_feature_selection_masks = []\n        self._step_aggregate_feature_selection_mask = None\n\n        # Initializes decision-step dependent variables.\n        output_aggregated = tf.zeros([batch_size, self.output_dim])\n        masked_features = features\n        mask_values = tf.zeros([batch_size, self.num_features])\n        aggregated_mask_values = tf.zeros([batch_size, self.num_features])\n        complementary_aggregated_mask_values = tf.ones(\n            [batch_size, self.num_features])\n\n        total_entropy = 0.0\n        entropy_loss = 0.\n\n        for ni in range(self.num_decision_steps):\n            # Feature transformer with two shared and two decision step dependent\n            # blocks is used below.=\n            transform_f1 = self.transform_f1(masked_features, training=training)\n            transform_f1 = glu(transform_f1, self.feature_dim)\n\n            transform_f2 = self.transform_f2(transform_f1, training=training)\n            transform_f2 = (glu(transform_f2, self.feature_dim) +\n                            transform_f1) * tf.math.sqrt(0.5)\n\n            transform_f3 = self.transform_f3_list[ni](transform_f2, training=training)\n            transform_f3 = (glu(transform_f3, self.feature_dim) +\n                            transform_f2) * tf.math.sqrt(0.5)\n\n            transform_f4 = self.transform_f4_list[ni](transform_f3, training=training)\n            transform_f4 = (glu(transform_f4, self.feature_dim) +\n                            transform_f3) * tf.math.sqrt(0.5)\n\n            if (ni > 0 or self.num_decision_steps == 1):\n                decision_out = tf.nn.relu(transform_f4[:, :self.output_dim])\n\n                # Decision aggregation.\n                output_aggregated += decision_out\n\n                # Aggregated masks are used for visualization of the\n                # feature importance attributes.\n                scale_agg = tf.reduce_sum(decision_out, axis=1, keepdims=True)\n\n                if self.num_decision_steps > 1:\n                    scale_agg = scale_agg / tf.cast(self.num_decision_steps - 1, tf.float32)\n\n                aggregated_mask_values += mask_values * scale_agg\n\n            features_for_coef = transform_f4[:, self.output_dim:]\n\n            if ni < (self.num_decision_steps - 1):\n                # Determines the feature masks via linear and nonlinear\n                # transformations, taking into account of aggregated feature use.\n                mask_values = self.transform_coef_list[ni](features_for_coef, training=training)\n                mask_values *= complementary_aggregated_mask_values\n                mask_values = sparsemax(mask_values, axis=-1)\n\n                # Relaxation factor controls the amount of reuse of features between\n                # different decision blocks and updated with the values of\n                # coefficients.\n                complementary_aggregated_mask_values *= (\n                        self.relaxation_factor - mask_values)\n\n                # Entropy is used to penalize the amount of sparsity in feature\n                # selection.\n                total_entropy += tf.reduce_mean(\n                    tf.reduce_sum(\n                        -mask_values * tf.math.log(mask_values + self.epsilon), axis=1)) / (\n                                     tf.cast(self.num_decision_steps - 1, tf.float32))\n\n                # Add entropy loss\n                entropy_loss = total_entropy\n\n                # Feature selection.\n                masked_features = tf.multiply(mask_values, features)\n\n                # Visualization of the feature selection mask at decision step ni\n                # tf.summary.image(\n                #     \"Mask for step\" + str(ni),\n                #     tf.expand_dims(tf.expand_dims(mask_values, 0), 3),\n                #     max_outputs=1)\n                mask_at_step_i = tf.expand_dims(tf.expand_dims(mask_values, 0), 3)\n                self._step_feature_selection_masks.append(mask_at_step_i)\n\n            else:\n                # This branch is needed for correct compilation by tf.autograph\n                entropy_loss = 0.\n\n        # Adds the loss automatically\n        self.add_loss(self.sparsity_coefficient * entropy_loss)\n\n        # Visualization of the aggregated feature importances\n        # tf.summary.image(\n        #     \"Aggregated mask\",\n        #     tf.expand_dims(tf.expand_dims(aggregated_mask_values, 0), 3),\n        #     max_outputs=1)\n\n        agg_mask = tf.expand_dims(tf.expand_dims(aggregated_mask_values, 0), 3)\n        self._step_aggregate_feature_selection_mask = agg_mask\n\n        return output_aggregated\n\n    @property\n    def feature_selection_masks(self):\n        return self._step_feature_selection_masks\n\n    @property\n    def aggregate_feature_selection_mask(self):\n        return self._step_aggregate_feature_selection_mask\n\n\nclass TabNetClassifier(tf.keras.Model):\n\n    def __init__(self, feature_columns,\n                 num_classes,\n                 num_features=None,\n                 feature_dim=64,\n                 output_dim=64,\n                 num_decision_steps=5,\n                 relaxation_factor=1.5,\n                 sparsity_coefficient=1e-5,\n                 norm_type='group',\n                 batch_momentum=0.98,\n                 virtual_batch_size=None,\n                 num_groups=1,\n                 epsilon=1e-5,\n                 multi_label=False, \n                 **kwargs):\n        \"\"\"\n        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n        # Hyper Parameter Tuning (Excerpt from the paper)\n        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n        selection:\n            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n            overfitting and yield poor generalization.\n            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n            Nsteps value favors for a larger γ.\n            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n            much smaller than the batch size.\n            - Initially large learning rate is important, which should be gradually decayed until convergence.\n        Args:\n            feature_columns: The Tensorflow feature columns for the dataset.\n            num_classes: Number of classes.\n            feature_dim (N_a): Dimensionality of the hidden representation in feature\n                transformation block. Each layer first maps the representation to a\n                2*feature_dim-dimensional output and half of it is used to determine the\n                nonlinearity of the GLU activation where the other half is used as an\n                input to GLU, and eventually feature_dim-dimensional output is\n                transferred to the next layer.\n            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n                later mapped to the final classification or regression output.\n            num_features: The number of input features (i.e the number of columns for\n                tabular data assuming each feature is represented with 1 dimension).\n            num_decision_steps(N_steps): Number of sequential decision steps.\n            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n                feature at different decision steps. When it is 1, a feature is enforced\n                to be used only at one decision step and as it increases, more\n                flexibility is provided to use a feature at multiple decision steps.\n            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n                Sparsity may provide a favorable inductive bias for convergence to\n                higher accuracy for some datasets where most of the input features are redundant.\n            norm_type: Type of normalization to perform for the model. Can be either\n                'group' or 'group'. 'group' is the default.\n            batch_momentum: Momentum in ghost batch normalization.\n            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n                overall batch size should be an integer multiple of virtual_batch_size.\n            num_groups: Number of groups used for group normalization.\n            epsilon: A small number for numerical stability of the entropy calculations.\n        \"\"\"\n        super(TabNetClassifier, self).__init__(**kwargs)\n\n        self.num_classes = num_classes\n\n        self.tabnet = TabNet(feature_columns=feature_columns,\n                    num_features=num_features,\n                    feature_dim=feature_dim,\n                    output_dim=output_dim,\n                    num_decision_steps=num_decision_steps,\n                    relaxation_factor=relaxation_factor,\n                    sparsity_coefficient=sparsity_coefficient,\n                    norm_type=norm_type,\n                    batch_momentum=batch_momentum,\n                    virtual_batch_size=virtual_batch_size,\n                    num_groups=num_groups,\n                    epsilon=epsilon,\n                    **kwargs)\n        \n        if multi_label:\n            \n            self.clf = tf.keras.layers.Dense(num_classes, activation='sigmoid', use_bias=False, name='classifier')\n            \n        else:\n            \n            self.clf = tf.keras.layers.Dense(num_classes, activation='softmax', use_bias=False, name='classifier')\n\n    def call(self, inputs, training=None):\n        self.activations = self.tabnet(inputs, training=training)\n        out = self.clf(self.activations)\n\n        return out\n\n    def summary(self, *super_args, **super_kwargs):\n        super().summary(*super_args, **super_kwargs)\n        self.tabnet.summary(*super_args, **super_kwargs)\n\n\nclass TabNetRegressor(tf.keras.Model):\n\n    def __init__(self, feature_columns,\n                 num_regressors,\n                 num_features=None,\n                 feature_dim=64,\n                 output_dim=64,\n                 num_decision_steps=5,\n                 relaxation_factor=1.5,\n                 sparsity_coefficient=1e-5,\n                 norm_type='group',\n                 batch_momentum=0.98,\n                 virtual_batch_size=None,\n                 num_groups=1,\n                 epsilon=1e-5,\n                 **kwargs):\n        \"\"\"\n        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n        # Hyper Parameter Tuning (Excerpt from the paper)\n        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n        selection:\n            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n            overfitting and yield poor generalization.\n            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n            Nsteps value favors for a larger γ.\n            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n            much smaller than the batch size.\n            - Initially large learning rate is important, which should be gradually decayed until convergence.\n        Args:\n            feature_columns: The Tensorflow feature columns for the dataset.\n            num_regressors: Number of regression variables.\n            feature_dim (N_a): Dimensionality of the hidden representation in feature\n                transformation block. Each layer first maps the representation to a\n                2*feature_dim-dimensional output and half of it is used to determine the\n                nonlinearity of the GLU activation where the other half is used as an\n                input to GLU, and eventually feature_dim-dimensional output is\n                transferred to the next layer.\n            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n                later mapped to the final classification or regression output.\n            num_features: The number of input features (i.e the number of columns for\n                tabular data assuming each feature is represented with 1 dimension).\n            num_decision_steps(N_steps): Number of sequential decision steps.\n            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n                feature at different decision steps. When it is 1, a feature is enforced\n                to be used only at one decision step and as it increases, more\n                flexibility is provided to use a feature at multiple decision steps.\n            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n                Sparsity may provide a favorable inductive bias for convergence to\n                higher accuracy for some datasets where most of the input features are redundant.\n            norm_type: Type of normalization to perform for the model. Can be either\n                'group' or 'group'. 'group' is the default.\n            batch_momentum: Momentum in ghost batch normalization.\n            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n                overall batch size should be an integer multiple of virtual_batch_size.\n            num_groups: Number of groups used for group normalization.\n            epsilon: A small number for numerical stability of the entropy calculations.\n        \"\"\"\n        super(TabNetRegressor, self).__init__(**kwargs)\n\n        self.num_regressors = num_regressors\n\n        self.tabnet = TabNet(feature_columns=feature_columns,\n                             num_features=num_features,\n                             feature_dim=feature_dim,\n                             output_dim=output_dim,\n                             num_decision_steps=num_decision_steps,\n                             relaxation_factor=relaxation_factor,\n                             sparsity_coefficient=sparsity_coefficient,\n                             norm_type=norm_type,\n                             batch_momentum=batch_momentum,\n                             virtual_batch_size=virtual_batch_size,\n                             num_groups=num_groups,\n                             epsilon=epsilon,\n                             **kwargs)\n\n        self.regressor = tf.keras.layers.Dense(num_regressors, use_bias=False, name='regressor')\n\n    def call(self, inputs, training=None):\n        self.activations = self.tabnet(inputs, training=training)\n        out = self.regressor(self.activations)\n        return out\n\n    def summary(self, *super_args, **super_kwargs):\n        super().summary(*super_args, **super_kwargs)\n        self.tabnet.summary(*super_args, **super_kwargs)\n\n\n# Aliases\nTabNetClassification = TabNetClassifier\nTabNetRegression = TabNetRegressor\n\nclass StackedTabNet(tf.keras.Model):\n\n    def __init__(self, feature_columns,\n                 num_layers=1,\n                 feature_dim=64,\n                 output_dim=64,\n                 num_features=None,\n                 num_decision_steps=5,\n                 relaxation_factor=1.5,\n                 sparsity_coefficient=1e-5,\n                 norm_type='group',\n                 batch_momentum=0.98,\n                 virtual_batch_size=None,\n                 num_groups=2,\n                 epsilon=1e-5,\n                 **kwargs):\n        \"\"\"\n        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n        Stacked variant of the TabNet model, which stacks multiple TabNets into a singular model.\n        # Hyper Parameter Tuning (Excerpt from the paper)\n        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n        selection:\n            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n            overfitting and yield poor generalization.\n            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n            Nsteps value favors for a larger γ.\n            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n            much smaller than the batch size.\n            - Initially large learning rate is important, which should be gradually decayed until convergence.\n        Args:\n            feature_columns: The Tensorflow feature columns for the dataset.\n            num_layers: Number of TabNets to stack together.\n            feature_dim (N_a): Dimensionality of the hidden representation in feature\n                transformation block. Each layer first maps the representation to a\n                2*feature_dim-dimensional output and half of it is used to determine the\n                nonlinearity of the GLU activation where the other half is used as an\n                input to GLU, and eventually feature_dim-dimensional output is\n                transferred to the next layer. Can be either a single int, or a list of\n                integers. If a list, must be of same length as the number of layers.\n            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n                later mapped to the final classification or regression output.\n                Can be either a single int, or a list of\n                integers. If a list, must be of same length as the number of layers.\n            num_features: The number of input features (i.e the number of columns for\n                tabular data assuming each feature is represented with 1 dimension).\n            num_decision_steps(N_steps): Number of sequential decision steps.\n            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n                feature at different decision steps. When it is 1, a feature is enforced\n                to be used only at one decision step and as it increases, more\n                flexibility is provided to use a feature at multiple decision steps.\n            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n                Sparsity may provide a favorable inductive bias for convergence to\n                higher accuracy for some datasets where most of the input features are redundant.\n            norm_type: Type of normalization to perform for the model. Can be either\n                'batch' or 'group'. 'group' is the default.\n            batch_momentum: Momentum in ghost batch normalization.\n            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n                overall batch size should be an integer multiple of virtual_batch_size.\n            num_groups: Number of groups used for group normalization.\n            epsilon: A small number for numerical stability of the entropy calculations.\n        \"\"\"\n        super(StackedTabNet, self).__init__(**kwargs)\n\n        if num_layers < 1:\n            raise ValueError(\"`num_layers` cannot be less than 1\")\n\n        if type(feature_dim) not in [list, tuple]:\n            feature_dim = [feature_dim] * num_layers\n\n        if type(output_dim) not in [list, tuple]:\n            output_dim = [output_dim] * num_layers\n\n        if len(feature_dim) != num_layers:\n            raise ValueError(\"`feature_dim` must be a list of length `num_layers`\")\n\n        if len(output_dim) != num_layers:\n            raise ValueError(\"`output_dim` must be a list of length `num_layers`\")\n\n        self.num_layers = num_layers\n\n        layers = []\n        layers.append(TabNet(feature_columns=feature_columns,\n                             num_features=num_features,\n                             feature_dim=feature_dim[0],\n                             output_dim=output_dim[0],\n                             num_decision_steps=num_decision_steps,\n                             relaxation_factor=relaxation_factor,\n                             sparsity_coefficient=sparsity_coefficient,\n                             norm_type=norm_type,\n                             batch_momentum=batch_momentum,\n                             virtual_batch_size=virtual_batch_size,\n                             num_groups=num_groups,\n                             epsilon=epsilon))\n\n        for layer_idx in range(1, num_layers):\n            layers.append(TabNet(feature_columns=None,\n                                 num_features=output_dim[layer_idx - 1],\n                                 feature_dim=feature_dim[layer_idx],\n                                 output_dim=output_dim[layer_idx],\n                                 num_decision_steps=num_decision_steps,\n                                 relaxation_factor=relaxation_factor,\n                                 sparsity_coefficient=sparsity_coefficient,\n                                 norm_type=norm_type,\n                                 batch_momentum=batch_momentum,\n                                 virtual_batch_size=virtual_batch_size,\n                                 num_groups=num_groups,\n                                 epsilon=epsilon))\n\n        self.tabnet_layers = layers\n\n    def call(self, inputs, training=None):\n        x = self.tabnet_layers[0](inputs, training=training)\n\n        for layer_idx in range(1, self.num_layers):\n            x = self.tabnet_layers[layer_idx](x, training=training)\n\n        return x\n\n    @property\n    def tabnets(self):\n        return self.tabnet_layers\n\n    @property\n    def feature_selection_masks(self):\n        return [tabnet.feature_selection_masks\n                for tabnet in self.tabnet_layers]\n\n    @property\n    def aggregate_feature_selection_mask(self):\n        return [tabnet.aggregate_feature_selection_mask\n                for tabnet in self.tabnet_layers]\n\n\nclass StackedTabNetClassifier(tf.keras.Model):\n\n    def __init__(self, feature_columns,\n                 num_classes,\n                 num_layers=1,\n                 feature_dim=64,\n                 output_dim=64,\n                 num_features=None,\n                 num_decision_steps=5,\n                 relaxation_factor=1.5,\n                 sparsity_coefficient=1e-5,\n                 norm_type='group',\n                 batch_momentum=0.98,\n                 virtual_batch_size=None,\n                 num_groups=2,\n                 epsilon=1e-5,\n                 multi_label = False, \n                 **kwargs):\n        \"\"\"\n        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n        Stacked variant of the TabNet model, which stacks multiple TabNets into a singular model.\n        # Hyper Parameter Tuning (Excerpt from the paper)\n        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n        selection:\n            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n            overfitting and yield poor generalization.\n            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n            Nsteps value favors for a larger γ.\n            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n            much smaller than the batch size.\n            - Initially large learning rate is important, which should be gradually decayed until convergence.\n        Args:\n            feature_columns: The Tensorflow feature columns for the dataset.\n            num_classes: Number of classes.\n            num_layers: Number of TabNets to stack together.\n            feature_dim (N_a): Dimensionality of the hidden representation in feature\n                transformation block. Each layer first maps the representation to a\n                2*feature_dim-dimensional output and half of it is used to determine the\n                nonlinearity of the GLU activation where the other half is used as an\n                input to GLU, and eventually feature_dim-dimensional output is\n                transferred to the next layer. Can be either a single int, or a list of\n                integers. If a list, must be of same length as the number of layers.\n            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n                later mapped to the final classification or regression output.\n                Can be either a single int, or a list of\n                integers. If a list, must be of same length as the number of layers.\n            num_features: The number of input features (i.e the number of columns for\n                tabular data assuming each feature is represented with 1 dimension).\n            num_decision_steps(N_steps): Number of sequential decision steps.\n            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n                feature at different decision steps. When it is 1, a feature is enforced\n                to be used only at one decision step and as it increases, more\n                flexibility is provided to use a feature at multiple decision steps.\n            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n                Sparsity may provide a favorable inductive bias for convergence to\n                higher accuracy for some datasets where most of the input features are redundant.\n            norm_type: Type of normalization to perform for the model. Can be either\n                'batch' or 'group'. 'group' is the default.\n            batch_momentum: Momentum in ghost batch normalization.\n            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n                overall batch size should be an integer multiple of virtual_batch_size.\n            num_groups: Number of groups used for group normalization.\n            epsilon: A small number for numerical stability of the entropy calculations.\n        \"\"\"\n        super(StackedTabNetClassifier, self).__init__(**kwargs)\n\n        self.num_classes = num_classes\n\n        self.stacked_tabnet = StackedTabNet(feature_columns=feature_columns,\n                                            num_layers=num_layers,\n                                            feature_dim=feature_dim,\n                                            output_dim=output_dim,\n                                            num_features=num_features,\n                                            num_decision_steps=num_decision_steps,\n                                            relaxation_factor=relaxation_factor,\n                                            sparsity_coefficient=sparsity_coefficient,\n                                            norm_type=norm_type,\n                                            batch_momentum=batch_momentum,\n                                            virtual_batch_size=virtual_batch_size,\n                                            num_groups=num_groups,\n                                            epsilon=epsilon)\n        if multi_label:\n            \n            self.clf = tf.keras.layers.Dense(num_classes, activation='sigmoid', use_bias=False)\n        \n        else:\n            \n            self.clf = tf.keras.layers.Dense(num_classes, activation='softmax', use_bias=False)\n\n    def call(self, inputs, training=None):\n        self.activations = self.stacked_tabnet(inputs, training=training)\n        out = self.clf(self.activations)\n\n        return out\n\n\nclass StackedTabNetRegressor(tf.keras.Model):\n\n    def __init__(self, feature_columns,\n                 num_regressors,\n                 num_layers=1,\n                 feature_dim=64,\n                 output_dim=64,\n                 num_features=None,\n                 num_decision_steps=5,\n                 relaxation_factor=1.5,\n                 sparsity_coefficient=1e-5,\n                 norm_type='group',\n                 batch_momentum=0.98,\n                 virtual_batch_size=None,\n                 num_groups=2,\n                 epsilon=1e-5,\n                 **kwargs):\n        \"\"\"\n        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n        Stacked variant of the TabNet model, which stacks multiple TabNets into a singular model.\n        # Hyper Parameter Tuning (Excerpt from the paper)\n        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n        selection:\n            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n            overfitting and yield poor generalization.\n            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n            Nsteps value favors for a larger γ.\n            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n            much smaller than the batch size.\n            - Initially large learning rate is important, which should be gradually decayed until convergence.\n        Args:\n            feature_columns: The Tensorflow feature columns for the dataset.\n            num_regressors: Number of regressors.\n            num_layers: Number of TabNets to stack together.\n            feature_dim (N_a): Dimensionality of the hidden representation in feature\n                transformation block. Each layer first maps the representation to a\n                2*feature_dim-dimensional output and half of it is used to determine the\n                nonlinearity of the GLU activation where the other half is used as an\n                input to GLU, and eventually feature_dim-dimensional output is\n                transferred to the next layer. Can be either a single int, or a list of\n                integers. If a list, must be of same length as the number of layers.\n            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n                later mapped to the final classification or regression output.\n                Can be either a single int, or a list of\n                integers. If a list, must be of same length as the number of layers.\n            num_features: The number of input features (i.e the number of columns for\n                tabular data assuming each feature is represented with 1 dimension).\n            num_decision_steps(N_steps): Number of sequential decision steps.\n            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n                feature at different decision steps. When it is 1, a feature is enforced\n                to be used only at one decision step and as it increases, more\n                flexibility is provided to use a feature at multiple decision steps.\n            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n                Sparsity may provide a favorable inductive bias for convergence to\n                higher accuracy for some datasets where most of the input features are redundant.\n            norm_type: Type of normalization to perform for the model. Can be either\n                'batch' or 'group'. 'group' is the default.\n            batch_momentum: Momentum in ghost batch normalization.\n            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n                overall batch size should be an integer multiple of virtual_batch_size.\n            num_groups: Number of groups used for group normalization.\n            epsilon: A small number for numerical stability of the entropy calculations.\n        \"\"\"\n        super(StackedTabNetRegressor, self).__init__(**kwargs)\n\n        self.num_regressors = num_regressors\n\n        self.stacked_tabnet = StackedTabNet(feature_columns=feature_columns,\n                                            num_layers=num_layers,\n                                            feature_dim=feature_dim,\n                                            output_dim=output_dim,\n                                            num_features=num_features,\n                                            num_decision_steps=num_decision_steps,\n                                            relaxation_factor=relaxation_factor,\n                                            sparsity_coefficient=sparsity_coefficient,\n                                            norm_type=norm_type,\n                                            batch_momentum=batch_momentum,\n                                            virtual_batch_size=virtual_batch_size,\n                                            num_groups=num_groups,\n                                            epsilon=epsilon)\n\n        self.regressor = tf.keras.layers.Dense(num_regressors, use_bias=False)\n\n    def call(self, inputs, training=None):\n        self.activations = self.tabnet(inputs, training=training)\n        out = self.regressor(self.activations)\n        return outl","execution_count":null,"outputs":[]},{"metadata":{"id":"0eDJ68r-vgTA"},"cell_type":"markdown","source":"# Stacked TabNet"},{"metadata":{"trusted":true,"id":"qiCub3F5vgTA","outputId":"a409642d-80cd-4fdb-d21b-586422655f38"},"cell_type":"code","source":"N_STARTS = 5\nN_SPILTS = 10\n\nres = train_targets.copy()\nss.loc[:, train_targets.columns] = 0\nres.loc[:, train_targets.columns] = 0\n\nfor seed in range(N_STARTS):\n    \n    for n, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits = N_SPILTS, random_state = seed, shuffle = True).split(train_targets, train_targets)):\n        \n        start_time = time()\n        x_tr, x_val = train.values[tr][:, top_feats], train.values[te][:, top_feats]\n        y_tr, y_val = train_targets.astype(float).values[tr], train_targets.astype(float).values[te]\n        x_tt = test_features.values[:, top_feats]\n        \n        model = StackedTabNetClassifier(feature_columns = None, num_classes = 206, num_layers = 2, \n                                        feature_dim = 128, output_dim = 64, num_features = len(top_feats),\n                                        num_decision_steps = 1, relaxation_factor = 1.5,\n                                        sparsity_coefficient = 1e-5, batch_momentum = 0.98,\n                                        virtual_batch_size = None, norm_type = 'group',\n                                        num_groups = -1, multi_label = True)\n\n        model.compile(optimizer = tfa.optimizers.Lookahead(tf.optimizers.Adam(1e-3), sync_period = 10), \n                      loss = 'binary_crossentropy')\n        \n        rlr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 3, verbose = 0, \n                                min_delta = 1e-4, mode = 'min')\n        ckp = ModelCheckpoint(f'TabNet_{seed}_{n}.hdf5', monitor = 'val_loss', verbose = 0, \n                              save_best_only = True, save_weights_only = True, mode = 'min')\n        es = EarlyStopping(monitor = 'val_loss', min_delta = 1e-4, patience = 10, mode = 'min', \n                           baseline = None, restore_best_weights = True, verbose = 0)\n        \n        model.fit(x_tr, y_tr, validation_data = (x_val, y_val), epochs = 100, batch_size = 128,\n                  callbacks = [rlr, ckp, es], verbose = 0)\n        \n        model.load_weights(f'TabNet_{seed}_{n}.hdf5')\n        ss.loc[:, train_targets.columns] += model.predict(x_tt, batch_size = x_tt.shape[0]) / (N_SPILTS * N_STARTS)\n        fold_pred = model.predict(x_val, batch_size = x_val.shape[0])\n        res.loc[te, train_targets.columns] += fold_pred / N_STARTS\n        fold_score = log_loss_metric(train_targets.loc[te].values, fold_pred)\n        print(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] TabNet: Seed {seed}, Fold {n}:', fold_score)\n        \n        K.clear_session()\n        del model\n        x = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"dgIrzdQZvgTC","outputId":"a2ab66fb-1783-45aa-b1ef-6a6f719dce6a"},"cell_type":"code","source":"print(f'TabNet OOF Metric: {log_loss_metric(train_targets.values, res.values)}')\nres.loc[train['cp_type'] == 1, train_targets.columns] = 0\nss.loc[test['cp_type'] == 1, train_targets.columns] = 0\nprint(f'TabNet OOF Metric with postprocessing: {log_loss_metric(train_targets.values, res.values)}')","execution_count":null,"outputs":[]},{"metadata":{"id":"4i2yuxNCvgTV"},"cell_type":"markdown","source":"# Submit"},{"metadata":{"trusted":true,"id":"fZG5AjqOvgTY"},"cell_type":"code","source":"ss.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}