{"cells":[{"metadata":{},"cell_type":"markdown","source":"Version list:\n- v1/v2/v3: Testing local code\n- v4: First working version, no clue why it doesn't work in commit\n- v5: Turn off pin memory, ran fully through (too much weight decay?)\n- v6: decay 0, learning rate 5e-4, drop connect 0.05\n- v7: Dropout increased to 0.5\n- v8: Patience increased to 3, epochs to 6, drop connect rate to 0.5\n- v9: epochs to 10\n- v10: Drop connect down to 0.4, patience up to 4, epochs up to 12, next to change swap noise and perplexity.\n- v11: Add postprocessing and perplexity to 12"},{"metadata":{"trusted":true},"cell_type":"code","source":"kernel_mode = True\n\n#!cp ../input/lish-moa-utils/utils.py .\nimport sys\nif kernel_mode:\n    sys.path.insert(0, \"../input/iterative-stratification\")\n    #sys.path.insert(0, './')\n    sys.path.insert(0, \"../input/gen-efficientnet-pytorch\")\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport time\nimport random\nimport torch\nimport math\nimport pytorch_lightning as pl\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom sklearn.metrics import log_loss\npd.options.display.max_columns = None\n#!pip install -q geffnet\nimport geffnet\n#!pip install -q iterative-stratification\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom sklearn.manifold import TSNE\nimport pickle\nimport seaborn as sns\nimport cv2\nsns.set(style=\"darkgrid\")\nfrom utils import DeepInsightTransformer, LogScaler\nimport gc\ngc.enable()\nexperiment_name='pleasework'\n\nmodel_info = {\n    \"model_path\": f\"../input/deepinsight-transformers-perplexity-5\"\n    #f\"../input/deepinsight-efficientnet-v4-b3/{experiment_name}\"\n    #if kernel_mode else\n    #f\"/workspace/Kaggle/MoA/completed/deepinsight_efficientnet_v4_b3/{experiment_name}\"\n}\n\nmodel_type='b3'\n\npretrained_model = f\"tf_efficientnet_{model_type}_ns\"\n\nmodel_output_folder = '.'\nrand_seed = 42\nperplexity = 24\npatience=3\nepochs=12\nweight_decay=0.00 #0.1 at first (too much)\n\ndrop_connect_rate = 0.2 #0.2 at first\nfc_size = 512\n\n# Swap Noise\nswap_prob = 0.15\nswap_portion = 0.1\n\n\nnum_workers = 4\ngpus = [0]\n\nif model_type == \"b0\":\n    batch_size = 48 # default 128\n    infer_batch_size = 256\n    image_size = 224  # B0\n    drop_rate = 0.2  # B0\n    resolution = 224\nelif model_type == \"b3\":\n    batch_size = 48 \n    infer_batch_size = 128 # 256 results in OOM\n    image_size = 300  # B3\n    drop_rate = 0.3  # B3\n    resolution = 300\nelif model_type == \"b5\":\n    batch_size = 8\n    infer_batch_size = 16\n    image_size = 456  # B5\n    drop_rate = 0.4  # B5\n    resolution = 456\nelif model_type == \"b7\":\n    batch_size = 2\n    infer_batch_size = 4\n    image_size = 800  # B7\n    image_size = 772  # B7\n    drop_rate = 0.5  # B7\n    resolution = 772","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv(\"../input/lish-moa/train_features.csv\")\ntrain_labels = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\ntrain_extra_labels = pd.read_csv(\"../input/lish-moa/train_targets_nonscored.csv\")\ntest_features = pd.read_csv(\"../input/lish-moa/test_features.csv\")\n\ndel train_labels['sig_id']\n\ncategory_features = [\"cp_type\", \"cp_dose\"]\nnumeric_features = [c for c in train_features.columns if c != \"sig_id\" and c not in category_features]\nall_features = category_features + numeric_features\ngene_experssion_features = [c for c in numeric_features if c.startswith(\"g-\")]\ncell_viability_features = [c for c in numeric_features if c.startswith(\"c-\")]\nlen(numeric_features), len(gene_experssion_features), len(cell_viability_features)\n\ntrain_classes = [c for c in train_labels.columns if c != \"sig_id\"]\ntrain_extra_classes = [c for c in train_extra_labels.columns if c != \"sig_id\"]\nlen(train_classes), len(train_extra_classes)\n\nfor df in [train_features, test_features]:\n    df['cp_type'] = df['cp_type'].map({'ctl_vehicle': 0, 'trt_cp': 1})\n    df['cp_dose'] = df['cp_dose'].map({'D1': 0, 'D2': 1})\n    df['cp_time'] = df['cp_time'].map({24: 0, 48: 0.5, 72: 1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_folds(num_starts, num_splits, control=True):\n    folds = []\n\n    # LOAD FILES\n    train_feats = pd.read_csv('../input/lish-moa/train_features.csv')\n    scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n    drug = pd.read_csv('../input/lish-moa/train_drug.csv')\n    if not control:\n        scored = scored.loc[train_feats['cp_type'] == 'trt_cp', :]\n        drug = drug.loc[train_feats['cp_type'] == 'trt_cp', :]\n    targets = scored.columns[1:]\n    scored = scored.merge(drug, on='sig_id', how='left') \n\n    # LOCATE DRUGS\n    vc = scored.drug_id.value_counts()\n    vc1 = vc.loc[vc <= 18].index\n    vc2 = vc.loc[vc > 18].index\n\n    for seed in range(num_starts):\n\n        # STRATIFY DRUGS 18X OR LESS\n        dct1 = {}; dct2 = {}\n        skf = MultilabelStratifiedKFold(n_splits = num_splits, shuffle = True, random_state = seed)\n        tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n        for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[targets])):\n            dd = {k:fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        # STRATIFY DRUGS MORE THAN 18X\n        skf = MultilabelStratifiedKFold(n_splits = num_splits, shuffle = True, random_state = seed)\n        tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop = True)\n        for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[targets])):\n            dd = {k:fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        # ASSIGN FOLDS\n        scored['fold'] = scored.drug_id.map(dct1)\n        scored.loc[scored.fold.isna(),'fold'] =\\\n            scored.loc[scored.fold.isna(),'sig_id'].map(dct2)\n        scored.fold = scored.fold.astype('int8')\n        folds.append(scored.fold.values)\n\n        del scored['fold']\n\n    return np.stack(folds) \n\ndef plot_embed_2D(X, title=None):\n    sns.set(style=\"darkgrid\")\n\n    # Create subplots\n    fig, ax = plt.subplots(1, 1, figsize=(10, 7), squeeze=False)\n    ax[0, 0].scatter(X[:, 0],\n                     X[:, 1],\n                     cmap=plt.cm.get_cmap(\"jet\", 10),\n                     marker=\"x\",\n                     alpha=1.0)\n    plt.gca().set_aspect('equal', adjustable='box')\n\n    if title is not None:\n        ax[0, 0].set_title(title, fontsize=20)\n\n    plt.rcParams.update({'font.size': 14})\n    plt.show()\n\ndef tsne_transform(data, perplexity=30, plot=True):\n    # Transpose to get (n_features, n_samples)\n    data = data.T\n\n    tsne = TSNE(n_components=2,\n                metric='cosine',\n                perplexity=perplexity,\n                n_iter=1000,\n                method='exact',\n                random_state=rand_seed,\n                n_jobs=-1)\n    # Transpose to get (n_features, n_samples)\n    transformed = tsne.fit_transform(data)\n\n    if plot:\n        plot_embed_2D(\n            transformed,\n            f\"All Feature Location Matrix of Training Set (Perplexity: {perplexity})\"\n        )\n    return transformed\n\ndef save_pickle(obj, model_output_folder, fold_i, name):\n    pickle.dump(obj, open(f\"{model_output_folder}/fold{fold_i}_{name}.pkl\", 'wb'),\n         pickle.HIGHEST_PROTOCOL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_weight_goog(m, n='', fix_group_fanout=True):\n    # weight init as per Tensorflow Official impl\n    # https://github.com/tensorflow/tpu/blob/master/models/official/mnasnet/mnasnet_model.py\n    if isinstance(m, torch.nn.Conv2d):\n        fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        if fix_group_fanout:\n            fan_out //= m.groups\n        m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, torch.nn.BatchNorm2d):\n        m.weight.data.fill_(1.0)\n        m.bias.data.zero_()\n    elif isinstance(m, torch.nn.Linear):\n        fan_out = m.weight.size(0)  # fan-out\n        fan_in = 0\n        if 'routing_fn' in n:\n            fan_in = m.weight.size(1)\n        init_range = 1.0 / math.sqrt(fan_in + fan_out)\n        m.weight.data.uniform_(-init_range, init_range)\n        m.bias.data.zero_()\n\ndef initialize_weight_default(m, n=''):\n    if isinstance(m, torch.nn.Conv2d):\n        torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n    elif isinstance(m, torch.nn.BatchNorm2d):\n        m.weight.data.fill_(1.0)\n        m.bias.data.zero_()\n    elif isinstance(m, torch.nn.Linear):\n        torch.nn.init.kaiming_uniform_(m.weight,\n                                 mode='fan_in',\n                                 nonlinearity='linear')\n\nclass MoAEfficientNet(pl.LightningModule):\n    def __init__(\n            self,\n            pretrained_model_name,\n            training_set=(None, None),  # tuple\n            valid_set=(None, None),  # tuple\n            test_set=None,\n            transformer=None,\n            num_classes=206,\n            in_chans=3,\n            drop_rate=0.,\n            drop_connect_rate=0.,\n            fc_size=512,\n            learning_rate=1e-3,\n            weight_init='goog'):\n        super(MoAEfficientNet, self).__init__()\n\n        self.train_data, self.train_labels = training_set\n        self.valid_data, self.valid_labels = valid_set\n        self.test_data = test_set\n        self.transformer = transformer\n\n        self.backbone = getattr(geffnet, pretrained_model)(\n            pretrained=True,\n            in_chans=in_chans,\n            drop_rate=drop_rate,\n            drop_connect_rate=drop_connect_rate,\n            weight_init=weight_init)\n\n        self.backbone.classifier = torch.nn.Sequential(\n            torch.nn.Linear(self.backbone.classifier.in_features, fc_size,\n                      bias=True), torch.nn.ELU(),\n            torch.nn.Linear(fc_size, num_classes, bias=True))\n\n        if self.training:\n            for m in self.backbone.classifier.modules():\n                initialize_weight_goog(m)\n\n        # Save passed hyperparameters\n        self.save_hyperparameters(\"pretrained_model_name\", \"num_classes\",\n                                  \"in_chans\", \"drop_rate\", \"drop_connect_rate\",\n                                  \"weight_init\", \"fc_size\", \"learning_rate\")\n\n    def forward(self, x):\n        return self.backbone(x)\n\n    def training_step(self, batch, batch_idx):\n        x = batch[\"x\"]\n        y = batch[\"y\"]\n        x = x.float()\n        y = y.type_as(x)\n        logits = self(x)\n\n        loss = F.binary_cross_entropy_with_logits(logits, y, reduction=\"mean\")\n\n        self.log('train_loss',\n                 loss,\n                 on_step=True,\n                 on_epoch=True,\n                 prog_bar=True,\n                 logger=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x = batch[\"x\"]\n        y = batch[\"y\"]\n        x = x.float()\n        y = y.type_as(x)\n        logits = self(x)\n\n        val_loss = F.binary_cross_entropy_with_logits(logits,\n                                                      y,\n                                                      reduction=\"mean\")\n\n        self.log('val_loss',\n                 val_loss,\n                 on_step=True,\n                 on_epoch=True,\n                 prog_bar=True,\n                 logger=True)\n\n        return val_loss\n\n    def test_step(self, batch, batch_idx):\n        x = batch[\"x\"]\n        y = batch[\"y\"]\n        x = x.float()\n        y = y.type_as(x)\n        logits = self(x)\n        return {\"pred_logits\": logits}\n\n    def test_epoch_end(self, output_results):\n        all_outputs = torch.cat([out[\"pred_logits\"] for out in output_results],\n                                dim=0)\n        print(\"Logits:\", all_outputs)\n        pred_probs = F.sigmoid(all_outputs).detach().cpu().numpy()\n        print(\"Predictions: \", pred_probs)\n        return {\"pred_probs\": pred_probs}\n\n    def setup(self, stage=None):\n        #         self.train_dataset = MoAImageDataset(self.train_data,\n        #                                              self.train_labels,\n        #                                              self.transformer)\n        self.train_dataset = MoAImageSwapDataset(self.train_data,\n                                                 self.train_labels,\n                                                 self.transformer,\n                                                 swap_prob=swap_prob,\n                                                 swap_portion=swap_portion)\n\n        self.val_dataset = MoAImageDataset(self.valid_data, self.valid_labels,\n                                           self.transformer)\n\n        self.test_dataset = TestDataset(self.test_data, None, self.transformer)\n\n    def train_dataloader(self):\n        train_dataloader = torch.utils.data.DataLoader(self.train_dataset,\n                                      batch_size=batch_size,\n                                      shuffle=True,\n                                      num_workers=num_workers,\n                                      pin_memory=False,\n                                      drop_last=False)\n        print(f\"Train iterations: {len(train_dataloader)}\")\n        return train_dataloader\n\n    def val_dataloader(self):\n        val_dataloader = torch.utils.data.DataLoader(self.val_dataset,\n                                    batch_size=infer_batch_size,\n                                    shuffle=False,\n                                    num_workers=num_workers,\n                                    pin_memory=False,\n                                    drop_last=False)\n        print(f\"Validate iterations: {len(val_dataloader)}\")\n        return val_dataloader\n\n    def test_dataloader(self):\n        test_dataloader = torch.utils.data.DataLoader(self.test_dataset,\n                                     batch_size=infer_batch_size,\n                                     shuffle=False,\n                                     num_workers=num_workers,\n                                     pin_memory=False,\n                                     drop_last=False)\n        print(f\"Test iterations: {len(test_dataloader)}\")\n        return test_dataloader\n\n    def configure_optimizers(self):\n        print(f\"Initial Learning Rate: {self.hparams.learning_rate:.6f}\")\n        optimizer = torch.optim.Adam(self.parameters(),\n                               lr=self.hparams.learning_rate,\n                               weight_decay=weight_decay)\n\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n                                                         T_max=T_max,\n                                                         eta_min=0,\n                                                         last_epoch=-1)\n\n        return [optimizer], [scheduler]\n\nclass MoAImageSwapDataset(torch.utils.data.Dataset):\n    def __init__(self,\n                 features,\n                 labels,\n                 transformer,\n                 swap_prob=0.15,\n                 swap_portion=0.1):\n        self.features = features\n        self.labels = labels\n        self.transformer = transformer\n        self.swap_prob = swap_prob\n        self.swap_portion = swap_portion\n\n    def __getitem__(self, index):\n        normalized = self.features[index, :]\n\n        # Swap row featurs randomly\n        normalized = self.add_swap_noise(index, normalized)\n\n        normalized = np.expand_dims(normalized, axis=0)\n\n        # Note: we are setting empty_value=1 to follow the setup in the paper\n        image = self.transformer.transform(normalized, empty_value=1)[0]\n\n        # Resize to target size\n        gene_cht = cv2.resize(image, (image_size, image_size),\n                              interpolation=cv2.INTER_CUBIC)\n\n        # Convert to 3 channels\n        image = np.repeat(gene_cht[np.newaxis, :, :], 3, axis=0)\n\n        return {\"x\": image.astype(float), \"y\": self.labels[index, :].astype(float)}\n\n    def add_swap_noise(self, index, X):\n        if np.random.rand() < self.swap_prob:\n            swap_index = np.random.randint(self.features.shape[0], size=1)[0]\n            # Select only gene expression and cell viability features\n            swap_features = np.random.choice(\n                np.array(range(3, self.features.shape[1])),\n                size=int(self.features.shape[1] * self.swap_portion),\n                replace=False)\n            X[swap_features] = self.features[swap_index, swap_features]\n\n        return X\n\n    def __len__(self):\n        return self.features.shape[0]\n\nclass MoAImageDataset(torch.utils.data.Dataset):\n    def __init__(self, features, labels, transformer):\n        self.features = features\n        self.labels = labels\n        self.transformer = transformer\n\n    def __getitem__(self, index):\n        normalized = self.features[index, :]\n        normalized = np.expand_dims(normalized, axis=0)\n\n        # Note: we are setting empty_value=1 to follow the setup in the paper\n        image = self.transformer.transform(normalized, empty_value=1)[0]\n\n        # Resize to target size\n        gene_cht = cv2.resize(image, (image_size, image_size),\n                              interpolation=cv2.INTER_CUBIC)\n\n        # Convert to 3 channels\n        image = np.repeat(gene_cht[np.newaxis, :, :], 3, axis=0)\n\n        return {\"x\": image.astype(float), \"y\": self.labels[index, :].astype(float)}\n\n    def __len__(self):\n        return len(self.features)\n\nclass TestDataset(torch.utils.data.Dataset):\n    def __init__(self, features, labels, transformer):\n        self.features = features\n        self.labels = labels\n        self.transformer = transformer\n\n    def __getitem__(self, index):\n        normalized = self.features[index, :]\n        normalized = np.expand_dims(normalized, axis=0)\n\n        # Note: we are setting empty_value=1 to follow the setup in the paper\n        image = self.transformer.transform(normalized, empty_value=1)[0]\n\n        # Resize to target size\n        gene_cht = cv2.resize(image, (image_size, image_size),\n                              interpolation=cv2.INTER_CUBIC)\n\n        # Convert to 3 channels\n        image = np.repeat(gene_cht[np.newaxis, :, :], 3, axis=0)\n\n        return {\"x\": image.astype(float), \"y\": -1}\n\n    def __len__(self):\n        return len(self.features)\n\ndef get_infer_model(model_path, test_set, transformer):\n    model = MoAEfficientNet.load_from_checkpoint(\n        model_path,\n        pretrained_model_name=pretrained_model,\n        training_set=(None, None),  # tuple\n        valid_set=(None, None),  # tuple\n        test_set=test_set,\n        transformer=transformer,\n        drop_rate=drop_rate,\n        drop_connect_rate=drop_connect_rate,\n        fc_size=fc_size,\n        weight_init='goog')\n\n    model.freeze()\n    model.eval()\n    return model\n\ndef get_train_model(training_set, valid_set, transformer, test_set=None):\n    model = MoAEfficientNet(\n        pretrained_model_name=pretrained_model,\n        training_set=training_set,  # tuple\n        valid_set=valid_set,  # tuple\n        test_set=test_set,\n        transformer=transformer,\n        drop_rate=drop_rate,\n        drop_connect_rate=drop_connect_rate,\n        fc_size=fc_size,\n        weight_init='goog', learning_rate=1e-3)\n    return model\n\ndef mean_logloss(y_pred, y_true):\n    logloss = (1 - y_true) * np.log(1 - y_pred +\n                                    1e-15) + y_true * np.log(y_pred + 1e-15)\n    return np.mean(-logloss)\n\ndef load_pickle(model_output_folder, fold_i, name):\n    return pickle.load(open(f\"{model_output_folder}/fold{fold_i}_{name}.pkl\", 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kfolds = 5\ncreatetransformer=True\n\nfold = create_folds(1, kfolds)\nres = train_labels.copy()\nres.loc[:, train_labels.columns] = 0\n\nfor i in range(kfolds):\n    \n    train_index, val_index = np.where(fold!=i)[1], np.where(fold==i)[1]\n\n    train_all_features = train_features.loc[train_index, all_features].copy().reset_index(drop=True).values\n    t_labels = train_labels.iloc[train_index].copy().reset_index(drop=True).values\n    valid_all_features = train_features.loc[val_index, all_features].copy().reset_index(drop=True).values\n    v_labels = train_labels.iloc[val_index].copy().reset_index(drop=True).values\n    test_all_features = train_features[all_features].copy().reset_index(drop=True).values\n    print(train_all_features.shape, t_labels.shape, valid_all_features.shape, v_labels.shape, test_all_features.shape)\n\n    T_max = math.floor(len(t_labels)/batch_size)\n\n    if createtransformer:\n        all_scaler = LogScaler()\n        train_all_features = all_scaler.fit_transform(train_all_features)\n        valid_all_features = all_scaler.transform(valid_all_features)\n        test_all_features = all_scaler.transform(test_all_features)\n        save_pickle(all_scaler, model_output_folder, i, \"log-scaler\")\n\n        transformer= DeepInsightTransformer(pixels=resolution,\n                                        perplexity=perplexity)\n        transformer=transformer.fit(train_all_features)\n        save_pickle(transformer, model_output_folder, i, \"deepinsight-transform\")\n    else: \n        scaler = load_pickle(model_info['model_path'], i, \"log-scaler\")\n        train_all_features = scaler.transform(train_all_features)\n        valid_all_features = scaler.transform(valid_all_features)\n        transformer = load_pickle(model_info['model_path'], i, \"deepinsight-transform\")\n    model = get_train_model(training_set=(train_all_features, t_labels), valid_set=(valid_all_features, v_labels), test_set=valid_all_features, transformer=transformer)\n    callbacks = [\n        pl.callbacks.EarlyStopping(monitor='val_loss_epoch',\n                    min_delta=1e-6,\n                    patience=patience,\n                    verbose=True,\n                    mode='min',\n                    strict=True),\n        pl.callbacks.LearningRateMonitor(logging_interval='step')\n    ]\n\n    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n        filepath=f\"{model_output_folder}/fold{i}\" +\n        \"/{epoch}-{train_loss_epoch:.6f}-{val_loss_epoch:.6f}\" +\n        f\"-image_size={image_size}-resolution={resolution}-perplexity={perplexity}-fc={fc_size}\",\n        save_top_k=1,\n        save_weights_only=False,\n        save_last=False,\n        verbose=True,\n        monitor='val_loss_epoch',\n        mode='min',\n        prefix='')\n\n    trainer = pl.Trainer(\n        gpus=gpus,\n        distributed_backend=\"dp\",  # multiple-gpus, 1 machine\n        max_epochs=epochs,\n        benchmark=False,\n        deterministic=True,\n        checkpoint_callback=checkpoint_callback,\n        callbacks=callbacks,\n        #accumulate_grad_batches=accumulate_grad_batches,\n        #gradient_clip_val=gradient_clip_val,\n        precision=16,\n        #logger=logger\n        )\n    trainer.fit(model)\n\n    output = trainer.test(model, verbose=False)[0]\n    res.iloc[val_index] += output[\"pred_probs\"]\n\nres.to_csv('res.csv', index=False)\n\nres.loc[train_features['cp_type'] == 0, train_labels.columns] = 0\n\nmetrics = []\nfor _target in train_labels.columns:\n    metrics.append(log_loss(train_labels.loc[:, _target], res.loc[:, _target]))\nprint(f'OOF Metric with postprocessing: {np.mean(metrics)}')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}