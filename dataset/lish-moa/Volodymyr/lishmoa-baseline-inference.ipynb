{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2020-10-27T19:06:31.810461Z","iopub.status.busy":"2020-10-27T19:06:31.809776Z","iopub.status.idle":"2020-10-27T19:06:33.238804Z","shell.execute_reply":"2020-10-27T19:06:33.237702Z"},"papermill":{"duration":1.469105,"end_time":"2020-10-27T19:06:33.238943","exception":false,"start_time":"2020-10-27T19:06:31.769838","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from collections import OrderedDict\nfrom copy import deepcopy\nfrom time import time\nimport sys\nsys.path.append('../input/iterativestratification')\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom torch.nn.modules.loss import _WeightedLoss\n\nfrom catalyst.contrib.nn.optimizers import RAdam, Lookahead, Adam\n\nfrom transformers import get_linear_schedule_with_warmup\n\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import KFold\n\nimport sys\nsys.path.insert(0, \"../input/tabnetdevelop/tabnet-develop/\")\n\n\nimport torch\nimport numpy as np\nfrom scipy.sparse import csc_matrix\nfrom abc import abstractmethod\nfrom pytorch_tabnet import tab_network\nfrom pytorch_tabnet.multiclass_utils import unique_labels\nfrom sklearn.metrics import roc_auc_score, mean_squared_error, accuracy_score\nfrom torch.nn.utils import clip_grad_norm_\nfrom pytorch_tabnet.utils import (PredictDataset,\n                                  create_dataloaders,\n                                  create_explain_matrix)\nfrom sklearn.base import BaseEstimator\nfrom torch.utils.data import DataLoader\nfrom copy import deepcopy\nimport io\nimport json\nfrom pathlib import Path\nimport shutil\nimport zipfile\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow_addons as tfa","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Notebook 1 inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"def notebook_1_inference():\n    train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n    train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n    train_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\n    test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n    sample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n    \n    EXP_NAME = 'reconst_pytorch_cv0145_firstpack_1024hidden_addfsnormed'\n    OMIT_VALIDATION = True\n    \n    GENES = [col for col in train_features.columns if col.startswith('g-')]\n    CELLS = [col for col in train_features.columns if col.startswith('c-')]\n    ADD_F = [\n        # First\n        'c_mean', 'g_mean', 'c_sum', 'g_sum', 'c_std', 'g_std',\n        'c_kurt', 'g_kurt', 'c_skew', 'g_skew', \n        # Second\n    #     'c_median', 'g_median',\n    #     'c_q_25', 'g_q_25', 'c_q_75', 'g_q_75',\n        # Third\n        #'c_min', 'g_min', 'c_max', 'g_max',\n    ]\n    len(ADD_F)\n    \n    train_features['c_mean'] = train_features[CELLS].values.mean(axis=1)\n    test_features['c_mean'] = test_features[CELLS].values.mean(axis=1)\n    train_features['g_mean'] = train_features[GENES].values.mean(axis=1)\n    test_features['g_mean'] = test_features[GENES].values.mean(axis=1)\n\n    train_features['c_sum'] = train_features[CELLS].sum(axis=1).values\n    test_features['c_sum'] = test_features[CELLS].sum(axis=1).values\n    train_features['g_sum'] = train_features[GENES].sum(axis=1).values\n    test_features['g_sum'] = test_features[GENES].sum(axis=1).values\n\n    train_features['c_std'] = train_features[CELLS].std(axis=1).values\n    test_features['c_std'] = test_features[CELLS].std(axis=1).values\n    train_features['g_std'] = train_features[GENES].std(axis=1).values\n    test_features['g_std'] = test_features[GENES].std(axis=1).values\n\n    train_features['c_kurt'] = train_features[CELLS].kurt(axis=1).values\n    test_features['c_kurt'] = test_features[CELLS].kurt(axis=1).values\n    train_features['g_kurt'] = train_features[GENES].kurt(axis=1).values\n    test_features['g_kurt'] = test_features[GENES].kurt(axis=1).values\n\n    train_features['c_skew'] = train_features[CELLS].skew(axis=1).values\n    test_features['c_skew'] = test_features[CELLS].skew(axis=1).values\n    train_features['g_skew'] = train_features[GENES].skew(axis=1).values\n    test_features['g_skew'] = test_features[GENES].skew(axis=1).values\n    '''\n    train_features['c_median'] = train_features[CELLS].median(axis=1).values\n    test_features['c_median'] = test_features[CELLS].median(axis=1).values\n    train_features['g_median'] = train_features[GENES].median(axis=1).values\n    test_features['g_median'] = test_features[GENES].median(axis=1).values\n\n    train_features['c_min'] = train_features[CELLS].min(axis=1).values\n    test_features['c_min'] = test_features[CELLS].min(axis=1).values\n    train_features['g_min'] = train_features[GENES].min(axis=1).values\n    test_features['g_min'] = test_features[GENES].min(axis=1).values\n\n    train_features['c_max'] = train_features[CELLS].max(axis=1).values\n    test_features['c_max'] = test_features[CELLS].max(axis=1).values\n    train_features['g_max'] = train_features[GENES].max(axis=1).values\n    test_features['g_max'] = test_features[GENES].max(axis=1).values\n\n    train_features['c_q_25'] = train_features[CELLS].quantile(0.25, axis=1).values\n    test_features['c_q_25'] = test_features[CELLS].quantile(0.25, axis=1).values\n    train_features['g_q_25'] = train_features[GENES].quantile(0.25, axis=1).values\n    test_features['g_q_25'] = test_features[GENES].quantile(0.25, axis=1).values\n\n    train_features['c_q_75'] = train_features[CELLS].quantile(0.75, axis=1).values\n    test_features['c_q_75'] = test_features[CELLS].quantile(0.75, axis=1).values\n    train_features['g_q_75'] = train_features[GENES].quantile(0.75, axis=1).values\n    test_features['g_q_75'] = test_features[GENES].quantile(0.75, axis=1).values\n    '''\n    \n    def seed_everything(seed=42):\n        random.seed(seed)\n        os.environ['PYTHONHASHSEED'] = str(seed)\n        np.random.seed(seed)\n    \n    seed_everything(seed=42)\n    \n    #RankGauss\n\n    for col in (GENES + CELLS + ADD_F):\n\n        transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n        vec_len = len(train_features[col].values)\n        vec_len_test = len(test_features[col].values)\n        raw_vec = train_features[col].values.reshape(vec_len, 1)\n        transformer.fit(raw_vec)\n\n        train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n        test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n    \n    # GENES\n    n_comp = 600  #<--Update\n\n    gen_pca = PCA(n_components=n_comp, random_state=42).fit(train_features[GENES])\n    train2 = gen_pca.transform(train_features[GENES]); test2 = gen_pca.transform(test_features[GENES])\n\n    train2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n    test2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n    # drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n    train_features = pd.concat((train_features, train2), axis=1)\n    test_features = pd.concat((test_features, test2), axis=1)\n    \n    #CELLS\n    n_comp = 50  #<--Update\n\n    cell_pca = PCA(n_components=n_comp, random_state=42).fit(train_features[CELLS])\n    train2 = cell_pca.transform(train_features[CELLS]); test2 = cell_pca.transform(test_features[CELLS])\n\n    train2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n    test2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n    # drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n    train_features = pd.concat((train_features, train2), axis=1)\n    test_features = pd.concat((test_features, test2), axis=1)\n        \n    from sklearn.feature_selection import VarianceThreshold\n\n\n    var_thresh = VarianceThreshold(0.8).fit(train_features.iloc[:,4:])  #<-- Update\n\n    train_features_transformed = var_thresh.transform(train_features.iloc[:,4:])#data_transformed[ : train_features.shape[0]]\n    test_features_transformed = var_thresh.transform(test_features.iloc[:,4:])#data_transformed[-test_features.shape[0] : ]\n\n\n    train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                                  columns=['sig_id','cp_type','cp_time','cp_dose'])\n\n    train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\n    test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                                 columns=['sig_id','cp_type','cp_time','cp_dose'])\n\n    test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\n    \n    train = train_features.merge(train_targets_scored, on='sig_id')\n    train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n    test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\n    target = train[train_targets_scored.columns.tolist()].reset_index(drop=True)\n    target_pseudo = train[train_targets_scored.columns.tolist()].reset_index(drop=True)\n\n    train = train.drop('cp_type', axis=1)\n    test = test.drop('cp_type', axis=1)\n    \n    target_cols = target.drop(columns='sig_id').columns.values.tolist()\n    \n    folds = train.copy()\n\n    mskf = MultilabelStratifiedKFold(n_splits=7)\n\n    for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n        folds.loc[v_idx, 'kfold'] = int(f)\n\n    folds['kfold'] = folds['kfold'].astype(int)\n    folds\n    \n    print(train.shape)\n    print(folds.shape)\n    print(test.shape)\n    print(target.shape)\n    print(sample_submission.shape)\n    \n    MIN_LOSS = 1e-5\n    GRAD_NORM = 10.0\n\n    def seed_everything(seed=42):\n        random.seed(seed)\n        os.environ['PYTHONHASHSEED'] = str(seed)\n        np.random.seed(seed)\n\n    seed_everything(seed=42)\n    \n    class MoADataset:\n        def __init__(self, features, targets):\n            self.features = features\n            self.targets = targets\n\n        def __len__(self):\n            return (self.features.shape[0])\n\n        def __getitem__(self, idx):\n            dct = {\n                'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n                'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n            }\n            return dct\n\n    class TestDataset:\n        def __init__(self, features):\n            self.features = features\n\n        def __len__(self):\n            return (self.features.shape[0])\n\n        def __getitem__(self, idx):\n            dct = {\n                'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n            }\n            return dct\n        \n    def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device, epoch, use_scheduling=True):\n        model.train()\n        final_loss = 0\n\n        for data in dataloader:\n            optimizer.zero_grad()\n            inputs, targets = data['x'].to(device), data['y'].to(device)\n    #         print(inputs.shape)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n            loss.backward()\n\n    #         for p in model.parameters():\n    #             torch.nn.utils.clip_grad_norm_(\n    #                 p, GRAD_NORM\n    #             )\n\n            optimizer.step()\n            if (optimizer.param_groups[0]['lr'] > MIN_LOSS or epoch < 3) and use_scheduling: \n                scheduler.step()\n\n            final_loss += loss.item()\n\n        final_loss /= len(dataloader)\n\n        return final_loss\n\n\n    def valid_fn(model, loss_fn, dataloader, device, scheduler=None):\n        model.eval()\n        final_loss = 0\n        valid_preds = []\n\n        for data in dataloader:\n            inputs, targets = data['x'].to(device), data['y'].to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n\n            final_loss += loss.item()\n            valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n        final_loss /= len(dataloader)\n\n        if scheduler is not None:\n            scheduler.step(final_loss) \n\n        valid_preds = np.concatenate(valid_preds)\n\n        return final_loss, valid_preds\n\n    def inference_fn(model, dataloader, device):\n        model.eval()\n        preds = []\n\n        for data in dataloader:\n            inputs = data['x'].to(device)\n\n            with torch.no_grad():\n                outputs = model(inputs)\n\n            preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n        preds = np.concatenate(preds)\n\n        return preds\n\n    class SmoothBCEwLogits(_WeightedLoss):\n        def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n            super().__init__(weight=weight, reduction=reduction)\n            self.smoothing = smoothing\n            self.weight = weight\n            self.reduction = reduction\n\n        @staticmethod\n        def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n            assert 0 <= smoothing < 1\n            with torch.no_grad():\n                targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n            return targets\n\n        def forward(self, inputs, targets):\n            targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n                self.smoothing)\n            loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n            if  self.reduction == 'sum':\n                loss = loss.sum()\n            elif  self.reduction == 'mean':\n                loss = loss.mean()\n\n            return loss\n        \n    class Model(nn.Module):\n        def __init__(self, num_features, num_targets, hidden_size):\n            super(Model, self).__init__()\n            self.batch_norm1 = nn.BatchNorm1d(num_features)\n            self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n\n            self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n            self.dropout2 = nn.Dropout(0.26)\n            self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n\n            self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n            self.dropout3 = nn.Dropout(0.26)\n            self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n\n        def recalibrate_layer(self, layer):\n\n            if(torch.isnan(layer.weight_v).sum() > 0):\n                print ('recalibrate layer.weight_v')\n                layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n                layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n\n            if(torch.isnan(layer.weight).sum() > 0):\n                print ('recalibrate layer.weight')\n                layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n                layer.weight += 1e-7\n\n        def forward(self, x):\n            x = self.batch_norm1(x)\n            #self.recalibrate_layer(self.dense1)\n            x = F.leaky_relu(self.dense1(x))\n\n            x = self.batch_norm2(x)\n            x = self.dropout2(x)\n            #self.recalibrate_layer(self.dense2)\n            x = F.leaky_relu(self.dense2(x))\n\n            x = self.batch_norm3(x)\n            x = self.dropout3(x)\n            #self.recalibrate_layer(self.dense3)\n            x = self.dense3(x)\n\n            return x\n\n    class ResModel(nn.Module):\n        def __init__(self, num_features, num_targets, hidden_size=None):\n            super(ResModel, self).__init__()\n\n            self.answer1 = nn.Sequential(\n                nn.BatchNorm1d(num_features),\n                nn.Dropout(0.2),\n                nn.Linear(num_features, 512), nn.ELU(),\n                nn.BatchNorm1d(512),\n                nn.Linear(512, 256), nn.ELU()\n            )\n\n            self.answer2 = nn.Sequential(\n                nn.BatchNorm1d(256 + num_features),\n                nn.Dropout(0.3),\n                nn.Linear(256 + num_features, 512), nn.ReLU(),\n                nn.BatchNorm1d(512),\n                nn.Linear(512, 512), nn.ELU(),\n                nn.BatchNorm1d(512),\n                nn.Linear(512, 256), nn.ReLU(),\n                nn.BatchNorm1d(256),\n                nn.Linear(256, 256), nn.ELU(),\n            )\n\n            self.answer3 = nn.Sequential(\n                nn.BatchNorm1d(256),\n                nn.Linear(256, 256), nn.SELU(),\n                nn.BatchNorm1d(256),\n                nn.Linear(256, 206), nn.SELU(),\n                nn.BatchNorm1d(206),\n                nn.Linear(206, num_targets)\n            )\n\n        def recalibrate_layer(self, layer):\n\n            if(torch.isnan(layer.weight_v).sum() > 0):\n                print ('recalibrate layer.weight_v')\n                layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n                layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n\n            if(torch.isnan(layer.weight).sum() > 0):\n                print ('recalibrate layer.weight')\n                layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n                layer.weight += 1e-7\n\n        def forward(self, x):\n            ans1 = self.answer1(x)\n\n            ans1_x = torch.cat((ans1, x), dim=-1)\n\n            ans_2 = self.answer2(ans1_x)\n\n            ans1_mean_ans_2 = (ans1 + ans_2) / 2\n\n            out = self.answer3(ans1_mean_ans_2)\n\n            return out\n \n    class LabelSmoothingLoss(nn.Module):\n        def __init__(self, classes, smoothing=0.0, dim=-1):\n            super(LabelSmoothingLoss, self).__init__()\n            self.confidence = 1.0 - smoothing\n            self.smoothing = smoothing\n            self.cls = classes\n            self.dim = dim\n\n        def forward(self, pred, target):\n            pred = pred.log_softmax(dim=self.dim)\n            with torch.no_grad():\n                # true_dist = pred.data.clone()\n                true_dist = torch.zeros_like(pred)\n                true_dist.fill_(self.smoothing / (self.cls - 1))\n                true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n            return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    \n        \n    def process_data(data):\n        data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n        return data\n    \n    feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\n    feature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\n    len(feature_cols)\n    \n    # HyperParameters\n\n    DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n    EPOCHS = 60\n    BATCH_SIZE = 128\n    LEARNING_RATE = 1e-3\n    WEIGHT_DECAY = 1e-5\n    NFOLDS = 7           \n    EARLY_STOPPING_STEPS = 10\n    EARLY_STOP = False\n    SWA_MODELS = 3\n    USE_PSEUDO = False\n\n    num_features=len(feature_cols)\n    num_targets=len(target_cols)\n    hidden_size=1024\n    \n    print(DEVICE)\n    \n    def run_training(fold, seed, oof):\n    \n        save_model_path = f\"../input/lish-moa-final-models/{EXP_NAME}/{EXP_NAME}/seed_{seed}\"    \n        seed_everything(seed)\n\n        test_ = process_data(test)\n        if not OMIT_VALIDATION:\n            train = process_data(folds)\n\n            trn_idx = train[train['kfold'] != fold].index\n            val_idx = train[train['kfold'] == fold].index\n\n            train_df = train[train['kfold'] != fold].reset_index(drop=True)\n            valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n\n            x_train, y_train  = train_df[feature_cols].values.astype(float), train_df[target_cols].values\n            x_valid, y_valid =  valid_df[feature_cols].values.astype(float), valid_df[target_cols].values\n\n            train_dataset = MoADataset(x_train, y_train)\n            valid_dataset = MoADataset(x_valid, y_valid)\n            trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n            validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n            loss_fn = nn.BCEWithLogitsLoss()\n\n        #--------------------- PREDICTION---------------------\n        x_test = test_[feature_cols].values.astype(float)\n        testdataset = TestDataset(x_test)\n        testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n\n        model = Model(\n            num_features=num_features,\n            num_targets=num_targets,\n            hidden_size=hidden_size,\n\n        )\n\n        model.load_state_dict(torch.load(f\"{save_model_path}/FOLD{fold}_.pth\", map_location='cpu'))\n        model.to(DEVICE)\n\n        predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n        predictions = inference_fn(model, testloader, DEVICE)\n\n        # Predict oof\n        if not OMIT_VALIDATION:\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n            oof[val_idx] = valid_preds\n\n            print(f\"loss : {valid_loss}\")\n\n        return oof, predictions\n    \n    def run_k_fold(NFOLDS, seed):\n        oof = np.zeros((len(train), len(target_cols)))\n        predictions = np.zeros((len(test), len(target_cols)))\n\n        for fold in range(NFOLDS):\n\n            oof, temp_pred = run_training(fold, seed, oof)\n\n            predictions += temp_pred \n            #oof += temp_oof \n\n        return oof, ( predictions / NFOLDS)\n    \n    # Averaging on multiple SEEDS\n\n    SEED = [0,1,2,3,4,5,6] #<-- Update\n    result_oof = np.zeros((len(train), len(target_cols)))\n    result_predictions = np.zeros((len(test), len(target_cols)))\n\n    for seed in SEED:\n\n        fold_oof, fold_predictions = run_k_fold(NFOLDS, seed)\n        result_oof += fold_oof \n        result_predictions += fold_predictions \n\n    train[target_cols] =  ( result_oof / len(SEED) )\n    test[target_cols] = ( result_predictions / len(SEED) )\n\n    if not OMIT_VALIDATION:\n        valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n\n\n        y_true = train_targets_scored[target_cols].values\n        y_pred = valid_results[target_cols].values\n\n        score = 0\n        for i in range(len(target_cols)):\n            score_ = log_loss(y_true[:, i], y_pred[:, i])\n            score += score_ / y_true.shape[1]\n\n        print(\"CV log_loss: \", score)\n    \n    sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n    \n    return sub","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Notebook 2 inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"def notebook_2_inference():\n    EXP_NAME = 'first_of_threemodels_baseline'\n    OMIT_VALIDATION = True\n    \n    data_dir = '../input/lish-moa/'\n    os.listdir(data_dir)\n    \n    train_features = pd.read_csv(data_dir + 'train_features.csv')\n    train_targets_scored = pd.read_csv(data_dir + 'train_targets_scored.csv')\n    train_targets_nonscored = pd.read_csv(data_dir + 'train_targets_nonscored.csv')\n    train_drug = pd.read_csv(data_dir + 'train_drug.csv')\n    test_features = pd.read_csv(data_dir + 'test_features.csv')\n    sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n\n    print('train_features: {}'.format(train_features.shape))\n    print('train_targets_scored: {}'.format(train_targets_scored.shape))\n    print('train_targets_nonscored: {}'.format(train_targets_nonscored.shape))\n    print('train_drug: {}'.format(train_drug.shape))\n    print('test_features: {}'.format(test_features.shape))\n    print('sample_submission: {}'.format(sample_submission.shape))\n    \n    train_features2=train_features.copy()\n    test_features2=test_features.copy()\n    \n    GENES = [col for col in train_features.columns if col.startswith('g-')]\n    CELLS = [col for col in train_features.columns if col.startswith('c-')]\n    \n    qt = QuantileTransformer(n_quantiles=100,random_state=42,output_distribution='normal')\n    qt.fit(pd.DataFrame(train_features[GENES+CELLS]))\n    train_features[GENES+CELLS] = qt.transform(train_features[GENES+CELLS])\n    test_features[GENES+CELLS] = qt.transform(test_features[GENES+CELLS])\n    \n    def seed_everything(seed=42):\n        random.seed(seed)\n        os.environ['PYTHONHASHSEED'] = str(seed)\n        np.random.seed(seed)\n\n    seed_everything(seed=42)\n    \n    # GENES\n    n_comp = 600  #<--Update\n\n    gen_pca = PCA(n_components=n_comp, random_state=42).fit(train_features[GENES])\n    train2 = gen_pca.transform(train_features[GENES]); test2 = gen_pca.transform(test_features[GENES])\n\n    train_gpca = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n    test_gpca = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n    # drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n    train_features = pd.concat((train_features, train_gpca), axis=1)\n    test_features = pd.concat((test_features, test_gpca), axis=1)\n    \n    #CELLS\n    n_comp = 50  #<--Update\n\n    cell_pca = PCA(n_components=n_comp, random_state=42).fit(train_features[CELLS])\n    train2 = cell_pca.transform(train_features[CELLS]); test2 = cell_pca.transform(test_features[CELLS])\n\n    train_cpca = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n    test_cpca = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n    # drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n    train_features = pd.concat((train_features, train_cpca), axis=1)\n    test_features = pd.concat((test_features, test_cpca), axis=1)\n    \n\n    var_thresh = VarianceThreshold(0.85).fit(train_features.iloc[:,4:])  #<-- Update\n    #data = train_features.append(test_features)\n    #data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\n    train_features_transformed = var_thresh.transform(train_features.iloc[:,4:])#data_transformed[ : train_features.shape[0]]\n    test_features_transformed = var_thresh.transform(test_features.iloc[:,4:])#data_transformed[-test_features.shape[0] : ]\n\n\n    train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                                  columns=['sig_id','cp_type','cp_time','cp_dose'])\n\n    train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\n    test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                                 columns=['sig_id','cp_type','cp_time','cp_dose'])\n\n    test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n    \n    def fe_cluster(train, test, n_clusters_g = 22, n_clusters_c = 4, SEED = 42):\n    \n        features_g = GENES\n        features_c = CELLS\n\n        def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n            train_ = train[features].copy()\n            test_ = test[features].copy()\n            kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(train_)\n            train[f'clusters_{kind}'] = kmeans.predict(train_.values)\n            test[f'clusters_{kind}'] = kmeans.predict(test_.values)\n            train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n            test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n            return train, test\n\n        train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n        train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n        return train, test\n\n    train_features2 ,test_features2=fe_cluster(train_features2,test_features2)\n    \n    train_pca=pd.concat((train_gpca,train_cpca),axis=1)\n    test_pca=pd.concat((test_gpca,test_cpca),axis=1)\n    \n    def fe_cluster_pca(train, test,n_clusters=5,SEED = 42):\n            kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(train)\n            train[f'clusters_pca'] = kmeans.predict(train.values)\n            test[f'clusters_pca'] = kmeans.predict(test.values)\n            train = pd.get_dummies(train, columns = [f'clusters_pca'])\n            test = pd.get_dummies(test, columns = [f'clusters_pca'])\n            return train, test\n    train_cluster_pca ,test_cluster_pca = fe_cluster_pca(train_pca,test_pca)\n    \n    train_cluster_pca = train_cluster_pca.iloc[:,650:]\n    test_cluster_pca = test_cluster_pca.iloc[:,650:]\n    \n    train_features_cluster=train_features2.iloc[:,876:]\n    test_features_cluster=test_features2.iloc[:,876:]\n    \n    gsquarecols=['g-574','g-211','g-216','g-0','g-255','g-577','g-153','g-389','g-60','g-370','g-248','g-167','g-203','g-177','g-301','g-332','g-517','g-6','g-744','g-224','g-162','g-3','g-736','g-486','g-283','g-22','g-359','g-361','g-440','g-335','g-106','g-307','g-745','g-146','g-416','g-298','g-666','g-91','g-17','g-549','g-145','g-157','g-768','g-568','g-396']\n    \n    def fe_stats(train, test):\n    \n        features_g = GENES\n        features_c = CELLS\n\n        for df in train, test:\n            df['g_sum'] = df[features_g].sum(axis = 1)\n            df['g_mean'] = df[features_g].mean(axis = 1)\n            df['g_std'] = df[features_g].std(axis = 1)\n            df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n            df['g_skew'] = df[features_g].skew(axis = 1)\n            df['c_sum'] = df[features_c].sum(axis = 1)\n            df['c_mean'] = df[features_c].mean(axis = 1)\n            df['c_std'] = df[features_c].std(axis = 1)\n            df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n            df['c_skew'] = df[features_c].skew(axis = 1)\n            df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n            df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n            df['gc_std'] = df[features_g + features_c].std(axis = 1)\n            df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n            df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n\n            df['c52_c42'] = df['c-52'] * df['c-42']\n            df['c13_c73'] = df['c-13'] * df['c-73']\n            df['c26_c13'] = df['c-23'] * df['c-13']\n            df['c33_c6'] = df['c-33'] * df['c-6']\n            df['c11_c55'] = df['c-11'] * df['c-55']\n            df['c38_c63'] = df['c-38'] * df['c-63']\n            df['c38_c94'] = df['c-38'] * df['c-94']\n            df['c13_c94'] = df['c-13'] * df['c-94']\n            df['c4_c52'] = df['c-4'] * df['c-52']\n            df['c4_c42'] = df['c-4'] * df['c-42']\n            df['c13_c38'] = df['c-13'] * df['c-38']\n            df['c55_c2'] = df['c-55'] * df['c-2']\n            df['c55_c4'] = df['c-55'] * df['c-4']\n            df['c4_c13'] = df['c-4'] * df['c-13']\n            df['c82_c42'] = df['c-82'] * df['c-42']\n            df['c66_c42'] = df['c-66'] * df['c-42']\n            df['c6_c38'] = df['c-6'] * df['c-38']\n            df['c2_c13'] = df['c-2'] * df['c-13']\n            df['c62_c42'] = df['c-62'] * df['c-42']\n            df['c90_c55'] = df['c-90'] * df['c-55']\n\n\n            for feature in features_c:\n                 df[f'{feature}_squared'] = df[feature] ** 2     \n\n            for feature in gsquarecols:\n                df[f'{feature}_squared'] = df[feature] ** 2        \n\n        return train, test\n\n    train_features2,test_features2=fe_stats(train_features2,test_features2)\n    \n    train_features_stats=train_features2.iloc[:,902:]\n    test_features_stats=test_features2.iloc[:,902:]\n\n    train_features = pd.concat((train_features, train_features_cluster,train_cluster_pca,train_features_stats), axis=1)\n    test_features = pd.concat((test_features, test_features_cluster,test_cluster_pca,test_features_stats), axis=1)\n    \n    train = train_features.merge(train_targets_scored, on='sig_id')\n    train = train.merge(train_targets_nonscored, on='sig_id')\n    train = train.merge(train_drug, on='sig_id')\n    train = train[train['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n    test = test_features[test_features['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n    \n    train = train.drop('cp_type', axis=1)\n    test = test.drop('cp_type', axis=1)\n    \n    target_cols = [x for x in train_targets_scored.columns if x != 'sig_id']\n    aux_target_cols = [x for x in train_targets_nonscored.columns if x != 'sig_id']\n    all_target_cols = target_cols + aux_target_cols\n\n    num_targets = len(target_cols)\n    num_aux_targets = len(aux_target_cols)\n    num_all_targets = len(all_target_cols)\n\n    print('num_targets: {}'.format(num_targets))\n    print('num_aux_targets: {}'.format(num_aux_targets))\n    print('num_all_targets: {}'.format(num_all_targets))\n    \n    def seed_everything(seed=42):\n        random.seed(seed)\n        os.environ['PYTHONHASHSEED'] = str(seed)\n        np.random.seed(seed)\n\n    seed_everything(seed=42)\n    \n    class MoADataset:\n        def __init__(self, features, targets):\n            self.features = features\n            self.targets = targets\n\n        def __len__(self):\n            return (self.features.shape[0])\n\n        def __getitem__(self, idx):\n            dct = {\n                'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n                'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)\n            }\n\n            return dct\n\n    class TestDataset:\n        def __init__(self, features):\n            self.features = features\n\n        def __len__(self):\n            return (self.features.shape[0])\n\n        def __getitem__(self, idx):\n            dct = {\n                'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n            }\n\n            return dct\n        \n\n    def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n        model.train()\n        final_loss = 0\n\n        for data in dataloader:\n            optimizer.zero_grad()\n            inputs, targets = data['x'].to(device), data['y'].to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            final_loss += loss.item()\n\n        final_loss /= len(dataloader)\n        return final_loss\n\n    def valid_fn(model, loss_fn, dataloader, device):\n        model.eval()\n        final_loss = 0\n        valid_preds = []\n\n        for data in dataloader:\n            inputs, targets = data['x'].to(device), data['y'].to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n\n            final_loss += loss.item()\n            valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n        final_loss /= len(dataloader)\n        valid_preds = np.concatenate(valid_preds)\n        return final_loss, valid_preds\n\n    def inference_fn(model, dataloader, device):\n        model.eval()\n        preds = []\n\n        for data in dataloader:\n            inputs = data['x'].to(device)\n\n            with torch.no_grad():\n                outputs = model(inputs)\n\n            preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n        preds = np.concatenate(preds)\n        return preds\n\n    class SmoothBCEwLogits(_WeightedLoss):\n        def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n            super().__init__(weight=weight, reduction=reduction)\n            self.smoothing = smoothing\n            self.weight = weight\n            self.reduction = reduction\n\n        @staticmethod\n        def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n            assert 0 <= smoothing < 1\n\n            with torch.no_grad():\n                targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n\n            return targets\n\n        def forward(self, inputs, targets):\n            targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n                self.smoothing)\n            loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n            if  self.reduction == 'sum':\n                loss = loss.sum()\n            elif  self.reduction == 'mean':\n                loss = loss.mean()\n\n            return loss\n\n    class Model(nn.Module):\n        def __init__(self, num_features, num_targets):\n            super(Model, self).__init__()\n            self.hidden_size = [1500, 1250, 1000, 750]\n            self.dropout_value = [0.5, 0.35, 0.3, 0.25]\n\n            self.batch_norm1 = nn.BatchNorm1d(num_features)\n            self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n\n            self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n            self.dropout2 = nn.Dropout(self.dropout_value[0])\n            self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n\n            self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n            self.dropout3 = nn.Dropout(self.dropout_value[1])\n            self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n\n            self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n            self.dropout4 = nn.Dropout(self.dropout_value[2])\n            self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n\n            self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n            self.dropout5 = nn.Dropout(self.dropout_value[3])\n            self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n\n        def forward(self, x):\n            x = self.batch_norm1(x)\n            x = F.leaky_relu(self.dense1(x))\n\n            x = self.batch_norm2(x)\n            x = self.dropout2(x)\n            x = F.leaky_relu(self.dense2(x))\n\n            x = self.batch_norm3(x)\n            x = self.dropout3(x)\n            x = F.leaky_relu(self.dense3(x))\n\n            x = self.batch_norm4(x)\n            x = self.dropout4(x)\n            x = F.leaky_relu(self.dense4(x))\n\n            x = self.batch_norm5(x)\n            x = self.dropout5(x)\n            x = self.dense5(x)\n            return x\n\n    class LabelSmoothingLoss(nn.Module):\n        def __init__(self, classes, smoothing=0.0, dim=-1):\n            super(LabelSmoothingLoss, self).__init__()\n            self.confidence = 1.0 - smoothing\n            self.smoothing = smoothing\n            self.cls = classes\n            self.dim = dim\n\n        def forward(self, pred, target):\n            pred = pred.log_softmax(dim=self.dim)\n\n            with torch.no_grad():\n                true_dist = torch.zeros_like(pred)\n                true_dist.fill_(self.smoothing / (self.cls - 1))\n                true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n\n            return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))   \n        \n    class FineTuneScheduler:\n        def __init__(self, epochs):\n            self.epochs = epochs\n            self.epochs_per_step = 0\n            self.frozen_layers = []\n\n        def copy_without_top(self, model, num_features, num_targets, num_targets_new):\n            self.frozen_layers = []\n\n            model_new = Model(num_features, num_targets)\n            model_new.load_state_dict(model.state_dict())\n\n            # Freeze all weights\n            for name, param in model_new.named_parameters():\n                layer_index = name.split('.')[0][-1]\n\n                if layer_index == 5:\n                    continue\n\n                param.requires_grad = False\n\n                # Save frozen layer names\n                if layer_index not in self.frozen_layers:\n                    self.frozen_layers.append(layer_index)\n\n            self.epochs_per_step = self.epochs // len(self.frozen_layers)\n\n            # Replace the top layers with another ones\n            model_new.batch_norm5 = nn.BatchNorm1d(model_new.hidden_size[3])\n            model_new.dropout5 = nn.Dropout(model_new.dropout_value[3])\n            model_new.dense5 = nn.utils.weight_norm(nn.Linear(model_new.hidden_size[-1], num_targets_new))\n            model_new.to(DEVICE)\n            return model_new\n\n        def step(self, epoch, model):\n            if len(self.frozen_layers) == 0:\n                return\n\n            if epoch % self.epochs_per_step == 0:\n                last_frozen_index = self.frozen_layers[-1]\n\n                # Unfreeze parameters of the last frozen layer\n                for name, param in model.named_parameters():\n                    layer_index = name.split('.')[0][-1]\n\n                    if layer_index == last_frozen_index:\n                        param.requires_grad = True\n\n                del self.frozen_layers[-1]  # Remove the last layer as unfrozen\n                \n    def process_data(data):\n        data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n        return data\n    \n    feature_cols = [c for c in process_data(train).columns if c not in all_target_cols]\n    feature_cols = [c for c in feature_cols if c not in ['kfold', 'sig_id', 'drug_id']]\n    num_features = len(feature_cols)\n    \n    # HyperParameters\n\n    DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n    EPOCHS = 24\n    BATCH_SIZE = 128\n\n    WEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 3e-6}\n    MAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 3e-3}\n    DIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2}\n    PCT_START = 0.1\n    \n    # Show model architecture\n    model = Model(num_features, num_all_targets)\n    model\n    \n    def make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH):\n        vc = train.drug_id.value_counts()\n        vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n        vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n\n        for seed_id in range(SEEDS):\n            kfold_col = 'kfold_{}'.format(seed_id)\n\n            # STRATIFY DRUGS 18X OR LESS\n            dct1 = {}\n            dct2 = {}\n\n            skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n            tmp = train.groupby('drug_id')[target_cols].mean().loc[vc1]\n\n            for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n                dd = {k: fold for k in tmp.index[idxV].values}\n                dct1.update(dd)\n\n            # STRATIFY DRUGS MORE THAN 18X\n            skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n            tmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\n\n            for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n                dd = {k: fold for k in tmp.sig_id[idxV].values}\n                dct2.update(dd)\n\n            # ASSIGN FOLDS\n            train[kfold_col] = train.drug_id.map(dct1)\n            train.loc[train[kfold_col].isna(), kfold_col] = train.loc[train[kfold_col].isna(), 'sig_id'].map(dct2)\n            train[kfold_col] = train[kfold_col].astype('int8')\n\n        return train\n\n    SEEDS = 7\n    NFOLDS = 7\n    DRUG_THRESH = 18\n\n    train = make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH)\n    train.head()\n    \n    def run_training(fold_id, seed_id):\n        seed_everything(seed_id)\n\n        save_model_path = f\"../input/lish-moa-final-models/{EXP_NAME}/{EXP_NAME}/seed_{seed_id}\"\n\n        if not OMIT_VALIDATION:\n            train_ = process_data(train)\n        test_ = process_data(test)\n\n        kfold_col = f'kfold_{seed_id}'\n        if not OMIT_VALIDATION:\n            val_idx = train_[train_[kfold_col] == fold_id].index\n\n        if not OMIT_VALIDATION:\n            valid_df = train_[train_[kfold_col] == fold_id].reset_index(drop=True)\n\n\n\n        # Load the fine-tuned model with the best loss\n        model = Model(num_features, num_targets)\n        model.load_state_dict(torch.load(f\"{save_model_path}/SCORED_ONLY_FOLD{fold_id}_.pth\"))\n        model.to(DEVICE)\n\n        #--------------------- PREDICTION---------------------\n        oof = np.zeros((len(train), num_targets))\n        # Predict oof\n        if not OMIT_VALIDATION:\n            x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values   \n            valid_dataset = MoADataset(x_valid, y_valid)\n            validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n            loss_fn = nn.BCEWithLogitsLoss()\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n            oof[val_idx] = valid_preds\n\n            print(f\"loss : {valid_loss}\")\n\n        x_test = test_[feature_cols].values\n        testdataset = TestDataset(x_test)\n        testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n\n        predictions = np.zeros((len(test_), num_targets))\n        predictions = inference_fn(model, testloader, DEVICE)\n        return oof, predictions\n    \n    def run_k_fold(NFOLDS, seed_id):\n        oof = np.zeros((len(train), len(target_cols)))\n        predictions = np.zeros((len(test), len(target_cols)))\n\n        for fold_id in range(NFOLDS):\n            oof_, pred_ = run_training(fold_id, seed_id)\n            predictions += pred_ / NFOLDS\n            oof += oof_\n\n        return oof, predictions\n    \n    # Averaging on multiple SEEDS\n\n    SEED = [0,1,2,3,4,5,6] #<-- Update\n    result_oof = np.zeros((len(train), len(target_cols)))\n    result_predictions = np.zeros((len(test), len(target_cols)))\n\n    for seed in SEED:\n\n        fold_oof, fold_predictions = run_k_fold(NFOLDS, seed)\n        result_oof += fold_oof \n        result_predictions += fold_predictions \n\n    train[target_cols] =  ( result_oof / len(SEED) )\n    test[target_cols] = ( result_predictions / len(SEED) )\n    \n    if not OMIT_VALIDATION:\n        valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n        y_true = train_targets_scored[target_cols].values\n        y_pred = valid_results[target_cols].values\n\n        score = 0\n\n        for i in range(len(target_cols)):\n            score += log_loss(y_true[:, i], y_pred[:, i])\n\n        score = score / y_pred.shape[1]\n\n        print(\"CV log_loss: \", score)\n        \n    sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n    \n    return sub","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Notebook 3 inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"def notebook_3_inference():\n    \n    train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n    train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n    train_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\n    test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n    submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n    GENES = [col for col in train_features.columns if col.startswith('g-')]\n    CELLS = [col for col in train_features.columns if col.startswith('c-')]\n    for col in (GENES + CELLS):\n\n        transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n        vec_len = len(train_features[col].values)\n        vec_len_test = len(test_features[col].values)\n        raw_vec = train_features[col].values.reshape(vec_len, 1)\n        transformer.fit(raw_vec)\n\n        train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n        test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n\n    def seed_everything(seed=42):\n        random.seed(seed)\n        os.environ['PYTHONHASHSEED'] = str(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n\n    seed_everything(seed=42)\n\n\n\n    from sklearn.decomposition import PCA\n    n_comp = 600  #<--Update\n\n    data = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n    data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\n    train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\n    train2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n    test2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n    # drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n    train_features = pd.concat((train_features, train2), axis=1)\n    test_features = pd.concat((test_features, test2), axis=1)\n\n\n\n    n_comp = 50  #<--Update\n\n    data = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n    data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\n    train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\n    train2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n    test2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n    # drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n    train_features = pd.concat((train_features, train2), axis=1)\n    test_features = pd.concat((test_features, test2), axis=1)\n\n    from sklearn.feature_selection import VarianceThreshold\n\n\n    var_thresh = VarianceThreshold(0.8)  #<-- Update\n    data = train_features.append(test_features)\n    data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\n    train_features_transformed = data_transformed[ : train_features.shape[0]]\n    test_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\n    train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                                  columns=['sig_id','cp_type','cp_time','cp_dose'])\n\n    train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\n    test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                                 columns=['sig_id','cp_type','cp_time','cp_dose'])\n\n    test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\n\n    \n\n    def get_ratio_labels(df):\n        columns = list(df.columns)\n        columns.pop(0)\n        ratios = []\n        toremove = []\n        for c in columns:\n            counts = df[c].value_counts()\n            if len(counts) != 1:\n                ratios.append(counts[0]/counts[1])\n            else:\n                toremove.append(c)\n        print(f\"remove {len(toremove)} columns\")\n\n        for t in toremove:\n            columns.remove(t)\n        return columns, np.array(ratios).astype(np.int32)\n\n    columns, ratios = get_ratio_labels(train_targets_scored)\n    columns_nonscored, ratios_nonscored = get_ratio_labels(train_targets_nonscored)\n\n\n    remove_vehicle=True\n    def transform_data(train, test, col, normalize=True, removed_vehicle=False):\n        \"\"\"\n            the first 3 columns represents categories, the others numericals features\n        \"\"\"\n        mapping = {\"cp_type\":{\"trt_cp\": 0, \"ctl_vehicle\":1},\n                   \"cp_time\":{48:0, 72:1, 24:2},\n                   \"cp_dose\":{\"D1\":0, \"D2\":1}}\n\n        if removed_vehicle:\n            categories_tr = np.stack([ train[c].apply(lambda x: mapping[c][x]).values for c in col[1:3]], axis=1)\n            categories_test = np.stack([ test[c].apply(lambda x: mapping[c][x]).values for c in col[1:3]], axis=1)\n        else:\n            categories_tr = np.stack([ train[c].apply(lambda x: mapping[c][x]).values for c in col[:3]], axis=1)\n            categories_test = np.stack([ test[c].apply(lambda x: mapping[c][x]).values for c in col[:3]], axis=1)\n\n        max_ = 10.\n        min_ = -10.\n\n        if removed_vehicle:\n            numerical_tr = train[col[3:]].values\n            numerical_test = test[col[3:]].values\n        else:\n            numerical_tr = train[col[3:]].values\n            numerical_test = test[col[3:]].values\n        if normalize:\n            numerical_tr = (numerical_tr-min_)/(max_ - min_)\n            numerical_test = (numerical_test-min_)/(max_ - min_)\n        return categories_tr, categories_test, numerical_tr, numerical_test\n    col_features = list(train_features.columns)[1:]\n    cat_tr, cat_test, numerical_tr, numerical_test = transform_data(train_features, test_features, col_features, normalize=False, removed_vehicle=remove_vehicle)\n    targets_tr = train_targets_scored[columns].values.astype(np.float32)\n    targets2_tr = train_targets_nonscored[columns_nonscored].values.astype(np.float32)\n    \n    def inference_fn(model, X ,verbose=True):\n        with torch.no_grad():\n            y_preds = model.predict( X )\n            y_preds = torch.sigmoid(torch.as_tensor(y_preds)).numpy()\n        return y_preds\n\n    \n\n    def log_loss_score(actual, predicted,  eps=1e-15):\n\n            \"\"\"\n            :param predicted:   The predicted probabilities as floats between 0-1\n            :param actual:      The binary labels. Either 0 or 1.\n            :param eps:         Log(0) is equal to infinity, so we need to offset our predicted values slightly by eps from 0 or 1\n            :return:            The logarithmic loss between between the predicted probability assigned to the possible outcomes for item i, and the actual outcome.\n            \"\"\"\n\n\n            p1 = actual * np.log(predicted+eps)\n            p0 = (1-actual) * np.log(1-predicted+eps)\n            loss = p0 + p1\n\n            return -loss.mean()\n\n    def log_loss_multi(y_true, y_pred):\n        M = y_true.shape[1]\n        results = np.zeros(M)\n        for i in range(M):\n            results[i] = log_loss_score(y_true[:,i], y_pred[:,i])\n        return results.mean()\n    \n    \n\n    def check_targets(targets):\n        ### check if targets are all binary in training set\n\n        for i in range(targets.shape[1]):\n            if len(np.unique(targets[:,i])) != 2:\n                return False\n        return True\n\n\n    def auc_multi(y_true, y_pred):\n        M = y_true.shape[1]\n        results = np.zeros(M)\n        for i in range(M):\n            try:\n                results[i] = roc_auc_score(y_true[:,i], y_pred[:,i])\n            except:\n                pass\n        return results.mean()\n\n\n\n    class TabModel(BaseEstimator):\n        def __init__(self, n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1,\n                     n_independent=2, n_shared=2, epsilon=1e-15,  momentum=0.02,\n                     lambda_sparse=1e-3, seed=0,\n                     clip_value=1, verbose=1,\n                     optimizer_fn=torch.optim.Adam,\n                     optimizer_params=dict(lr=2e-2),\n                     scheduler_params=None, scheduler_fn=None,\n                     mask_type=\"sparsemax\",\n                     input_dim=None, output_dim=None,\n                     device_name='auto'):\n            \"\"\" Class for TabNet model\n            Parameters\n            ----------\n                device_name: str\n                    'cuda' if running on GPU, 'cpu' if not, 'auto' to autodetect\n            \"\"\"\n\n            self.n_d = n_d\n            self.n_a = n_a\n            self.n_steps = n_steps\n            self.gamma = gamma\n            self.cat_idxs = cat_idxs\n            self.cat_dims = cat_dims\n            self.cat_emb_dim = cat_emb_dim\n            self.n_independent = n_independent\n            self.n_shared = n_shared\n            self.epsilon = epsilon\n            self.momentum = momentum\n            self.lambda_sparse = lambda_sparse\n            self.clip_value = clip_value\n            self.verbose = verbose\n            self.optimizer_fn = optimizer_fn\n            self.optimizer_params = optimizer_params\n            self.device_name = device_name\n            self.scheduler_params = scheduler_params\n            self.scheduler_fn = scheduler_fn\n            self.mask_type = mask_type\n            self.input_dim = input_dim\n            self.output_dim = output_dim\n\n            self.batch_size = 1024\n\n            self.seed = seed\n            torch.manual_seed(self.seed)\n            # Defining device\n            if device_name == 'auto':\n                if torch.cuda.is_available():\n                    device_name = 'cuda'\n                else:\n                    device_name = 'cpu'\n            self.device = torch.device(device_name)\n            print(f\"Device used : {self.device}\")\n\n        @abstractmethod\n        def construct_loaders(self, X_train, y_train, X_valid, y_valid,\n                              weights, batch_size, num_workers, drop_last):\n            \"\"\"\n            Returns\n            -------\n            train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n                Training and validation dataloaders\n            -------\n            \"\"\"\n            raise NotImplementedError('users must define construct_loaders to use this base class')\n\n        def init_network(\n                         self,\n                         input_dim,\n                         output_dim,\n                         n_d,\n                         n_a,\n                         n_steps,\n                         gamma,\n                         cat_idxs,\n                         cat_dims,\n                         cat_emb_dim,\n                         n_independent,\n                         n_shared,\n                         epsilon,\n                         virtual_batch_size,\n                         momentum,\n                         device_name,\n                         mask_type,\n                         ):\n            self.network = tab_network.TabNet(\n                input_dim,\n                output_dim,\n                n_d=n_d,\n                n_a=n_a,\n                n_steps=n_steps,\n                gamma=gamma,\n                cat_idxs=cat_idxs,\n                cat_dims=cat_dims,\n                cat_emb_dim=cat_emb_dim,\n                n_independent=n_independent,\n                n_shared=n_shared,\n                epsilon=epsilon,\n                virtual_batch_size=virtual_batch_size,\n                momentum=momentum,\n                device_name=device_name,\n                mask_type=mask_type).to(self.device)\n\n            self.reducing_matrix = create_explain_matrix(\n                self.network.input_dim,\n                self.network.cat_emb_dim,\n                self.network.cat_idxs,\n                self.network.post_embed_dim)\n\n        def fit(self, X_train, y_train, X_valid=None, y_valid=None, loss_fn=None,\n                weights=0, max_epochs=100, patience=10, batch_size=1024,\n                virtual_batch_size=128, num_workers=0, drop_last=False):\n            \"\"\"Train a neural network stored in self.network\n            Using train_dataloader for training data and\n            valid_dataloader for validation.\n            Parameters\n            ----------\n                X_train: np.ndarray\n                    Train set\n                y_train : np.array\n                    Train targets\n                X_train: np.ndarray\n                    Train set\n                y_train : np.array\n                    Train targets\n                weights : bool or dictionnary\n                    0 for no balancing\n                    1 for automated balancing\n                    dict for custom weights per class\n                max_epochs : int\n                    Maximum number of epochs during training\n                patience : int\n                    Number of consecutive non improving epoch before early stopping\n                batch_size : int\n                    Training batch size\n                virtual_batch_size : int\n                    Batch size for Ghost Batch Normalization (virtual_batch_size < batch_size)\n                num_workers : int\n                    Number of workers used in torch.utils.data.DataLoader\n                drop_last : bool\n                    Whether to drop last batch during training\n            \"\"\"\n            # update model name\n\n            self.update_fit_params(X_train, y_train, X_valid, y_valid, loss_fn,\n                                   weights, max_epochs, patience, batch_size,\n                                   virtual_batch_size, num_workers, drop_last)\n\n            train_dataloader, valid_dataloader = self.construct_loaders(X_train,\n                                                                        y_train,\n                                                                        X_valid,\n                                                                        y_valid,\n                                                                        self.updated_weights,\n                                                                        self.batch_size,\n                                                                        self.num_workers,\n                                                                        self.drop_last)\n\n            self.init_network(\n                input_dim=self.input_dim,\n                output_dim=self.output_dim,\n                n_d=self.n_d,\n                n_a=self.n_a,\n                n_steps=self.n_steps,\n                gamma=self.gamma,\n                cat_idxs=self.cat_idxs,\n                cat_dims=self.cat_dims,\n                cat_emb_dim=self.cat_emb_dim,\n                n_independent=self.n_independent,\n                n_shared=self.n_shared,\n                epsilon=self.epsilon,\n                virtual_batch_size=self.virtual_batch_size,\n                momentum=self.momentum,\n                device_name=self.device_name,\n                mask_type=self.mask_type\n            )\n\n            self.optimizer = self.optimizer_fn(self.network.parameters(),\n                                               **self.optimizer_params)\n\n            if self.scheduler_fn:\n                self.scheduler = self.scheduler_fn(self.optimizer, **self.scheduler_params)\n            else:\n                self.scheduler = None\n\n            self.losses_train = []\n            self.losses_valid = []\n            self.learning_rates = []\n            self.metrics_train = []\n            self.metrics_valid = []\n\n            if self.verbose > 0:\n                print(\"Will train until validation stopping metric\",\n                      f\"hasn't improved in {self.patience} rounds.\")\n                msg_epoch = f'| EPOCH |  train  |   valid  | total time (s)'\n                print('---------------------------------------')\n                print(msg_epoch)\n\n            total_time = 0\n            while (self.epoch < self.max_epochs and\n                   self.patience_counter < self.patience):\n                starting_time = time.time()\n                # updates learning rate history\n                self.learning_rates.append(self.optimizer.param_groups[-1][\"lr\"])\n\n                fit_metrics = self.fit_epoch(train_dataloader, valid_dataloader)\n\n                # leaving it here, may be used for callbacks later\n                self.losses_train.append(fit_metrics['train']['loss_avg'])\n                self.losses_valid.append(fit_metrics['valid']['total_loss'])\n                self.metrics_train.append(fit_metrics['train']['stopping_loss'])\n                self.metrics_valid.append(fit_metrics['valid']['stopping_loss'])\n\n                stopping_loss = fit_metrics['valid']['stopping_loss']\n                if stopping_loss < self.best_cost:\n                    self.best_cost = stopping_loss\n                    self.patience_counter = 0\n                    # Saving model\n                    self.best_network = deepcopy(self.network)\n                    has_improved = True\n                else:\n                    self.patience_counter += 1\n                    has_improved=False\n                self.epoch += 1\n                total_time += time.time() - starting_time\n                if self.verbose > 0:\n                    if self.epoch % self.verbose == 0:\n                        separator = \"|\"\n                        msg_epoch = f\"| {self.epoch:<5} | \"\n                        msg_epoch += f\" {fit_metrics['train']['stopping_loss']:.5f}\"\n                        msg_epoch += f' {separator:<2} '\n                        msg_epoch += f\" {fit_metrics['valid']['stopping_loss']:.5f}\"\n                        msg_epoch += f' {separator:<2} '\n                        msg_epoch += f\" {np.round(total_time, 1):<10}\"\n                        msg_epoch += f\" {has_improved}\"\n                        print(msg_epoch)\n\n            if self.verbose > 0:\n                if self.patience_counter == self.patience:\n                    print(f\"Early stopping occured at epoch {self.epoch}\")\n                print(f\"Training done in {total_time:.3f} seconds.\")\n                print('---------------------------------------')\n\n            self.history = {\"train\": {\"loss\": self.losses_train,\n                                      \"metric\": self.metrics_train,\n                                      \"lr\": self.learning_rates},\n                            \"valid\": {\"loss\": self.losses_valid,\n                                      \"metric\": self.metrics_valid}}\n            # load best models post training\n            self.load_best_model()\n\n            # compute feature importance once the best model is defined\n            self._compute_feature_importances(train_dataloader)\n\n        def save_model(self, path):\n            \"\"\"\n            Saving model with two distinct files.\n            \"\"\"\n            saved_params = {}\n            for key, val in self.get_params().items():\n                if isinstance(val, type):\n                    # Don't save torch specific params\n                    continue\n                else:\n                    saved_params[key] = val\n\n            # Create folder\n            Path(path).mkdir(parents=True, exist_ok=True)\n\n            # Save models params\n            with open(Path(path).joinpath(\"model_params.json\"), \"w\", encoding=\"utf8\") as f:\n                json.dump(saved_params, f)\n\n            # Save state_dict\n            torch.save(self.network.state_dict(), Path(path).joinpath(\"network.pt\"))\n            shutil.make_archive(path, 'zip', path)\n            shutil.rmtree(path)\n            print(f\"Successfully saved model at {path}.zip\")\n            return f\"{path}.zip\"\n\n        def load_model(self, filepath):\n\n            try:\n                try:\n                    with zipfile.ZipFile(filepath) as z:\n                        with z.open(\"model_params.json\") as f:\n                            loaded_params = json.load(f)\n                        with z.open(\"network.pt\") as f:\n                            try:\n                                saved_state_dict = torch.load(f)\n                            except io.UnsupportedOperation:\n                                # In Python <3.7, the returned file object is not seekable (which at least\n                                # some versions of PyTorch require) - so we'll try buffering it in to a\n                                # BytesIO instead:\n                                saved_state_dict = torch.load(io.BytesIO(f.read()))\n\n                except:\n                    with open(os.path.join(filepath, \"model_params.json\")) as f:\n                            loaded_params = json.load(f)\n\n                    saved_state_dict = torch.load(os.path.join(filepath, \"network.pt\"), map_location=\"cpu\")\n\n            except KeyError:\n                raise KeyError(\"Your zip file is missing at least one component\")\n\n            #print(loaded_params)\n            if torch.cuda.is_available():\n                device_name = 'cuda'\n            else:\n                device_name = 'cpu'\n            loaded_params[\"device_name\"] = device_name\n            self.__init__(**loaded_params)\n\n\n\n            self.init_network(\n                input_dim=self.input_dim,\n                output_dim=self.output_dim,\n                n_d=self.n_d,\n                n_a=self.n_a,\n                n_steps=self.n_steps,\n                gamma=self.gamma,\n                cat_idxs=self.cat_idxs,\n                cat_dims=self.cat_dims,\n                cat_emb_dim=self.cat_emb_dim,\n                n_independent=self.n_independent,\n                n_shared=self.n_shared,\n                epsilon=self.epsilon,\n                virtual_batch_size=1024,\n                momentum=self.momentum,\n                device_name=self.device_name,\n                mask_type=self.mask_type\n            )\n            self.network.load_state_dict(saved_state_dict)\n            self.network.eval()\n            return\n\n        def fit_epoch(self, train_dataloader, valid_dataloader):\n            \"\"\"\n            Evaluates and updates network for one epoch.\n            Parameters\n            ----------\n                train_dataloader: a :class: `torch.utils.data.Dataloader`\n                    DataLoader with train set\n                valid_dataloader: a :class: `torch.utils.data.Dataloader`\n                    DataLoader with valid set\n            \"\"\"\n            train_metrics = self.train_epoch(train_dataloader)\n            valid_metrics = self.predict_epoch(valid_dataloader)\n\n            fit_metrics = {'train': train_metrics,\n                           'valid': valid_metrics}\n\n            return fit_metrics\n\n        @abstractmethod\n        def train_epoch(self, train_loader):\n            \"\"\"\n            Trains one epoch of the network in self.network\n            Parameters\n            ----------\n                train_loader: a :class: `torch.utils.data.Dataloader`\n                    DataLoader with train set\n            \"\"\"\n            raise NotImplementedError('users must define train_epoch to use this base class')\n\n        @abstractmethod\n        def train_batch(self, data, targets):\n            \"\"\"\n            Trains one batch of data\n            Parameters\n            ----------\n                data: a :tensor: `torch.tensor`\n                    Input data\n                target: a :tensor: `torch.tensor`\n                    Target data\n            \"\"\"\n            raise NotImplementedError('users must define train_batch to use this base class')\n\n        @abstractmethod\n        def predict_epoch(self, loader):\n            \"\"\"\n            Validates one epoch of the network in self.network\n            Parameters\n            ----------\n                loader: a :class: `torch.utils.data.Dataloader`\n                        DataLoader with validation set\n            \"\"\"\n            raise NotImplementedError('users must define predict_epoch to use this base class')\n\n        @abstractmethod\n        def predict_batch(self, data, targets):\n            \"\"\"\n            Make predictions on a batch (valid)\n            Parameters\n            ----------\n                data: a :tensor: `torch.Tensor`\n                    Input data\n                target: a :tensor: `torch.Tensor`\n                    Target data\n            Returns\n            -------\n                batch_outs: dict\n            \"\"\"\n            raise NotImplementedError('users must define predict_batch to use this base class')\n\n        def load_best_model(self):\n            if self.best_network is not None:\n                self.network = self.best_network\n\n        @abstractmethod\n        def predict(self, X):\n            \"\"\"\n            Make predictions on a batch (valid)\n            Parameters\n            ----------\n                data: a :tensor: `torch.Tensor`\n                    Input data\n                target: a :tensor: `torch.Tensor`\n                    Target data\n            Returns\n            -------\n                predictions: np.array\n                    Predictions of the regression problem or the last class\n            \"\"\"\n            raise NotImplementedError('users must define predict to use this base class')\n\n        def explain(self, X):\n            \"\"\"\n            Return local explanation\n            Parameters\n            ----------\n                data: a :tensor: `torch.Tensor`\n                    Input data\n                target: a :tensor: `torch.Tensor`\n                    Target data\n            Returns\n            -------\n                M_explain: matrix\n                    Importance per sample, per columns.\n                masks: matrix\n                    Sparse matrix showing attention masks used by network.\n            \"\"\"\n            self.network.eval()\n\n            dataloader = DataLoader(PredictDataset(X),\n                                    batch_size=self.batch_size, shuffle=False)\n\n            for batch_nb, data in enumerate(dataloader):\n                data = data.to(self.device).float()\n\n                M_explain, masks = self.network.forward_masks(data)\n                for key, value in masks.items():\n                    masks[key] = csc_matrix.dot(value.cpu().detach().numpy(),\n                                                self.reducing_matrix)\n\n                if batch_nb == 0:\n                    res_explain = csc_matrix.dot(M_explain.cpu().detach().numpy(),\n                                                 self.reducing_matrix)\n                    res_masks = masks\n                else:\n                    res_explain = np.vstack([res_explain,\n                                             csc_matrix.dot(M_explain.cpu().detach().numpy(),\n                                                            self.reducing_matrix)])\n                    for key, value in masks.items():\n                        res_masks[key] = np.vstack([res_masks[key], value])\n            return res_explain, res_masks\n\n        def _compute_feature_importances(self, loader):\n            self.network.eval()\n            feature_importances_ = np.zeros((self.network.post_embed_dim))\n            for data, targets in loader:\n                data = data.to(self.device).float()\n                M_explain, masks = self.network.forward_masks(data)\n                feature_importances_ += M_explain.sum(dim=0).cpu().detach().numpy()\n\n            feature_importances_ = csc_matrix.dot(feature_importances_,\n                                                  self.reducing_matrix)\n            self.feature_importances_ = feature_importances_ / np.sum(feature_importances_)\n\n\n    class TabNetRegressor(TabModel):\n\n        def construct_loaders(self, X_train, y_train, X_valid, y_valid, weights,\n                              batch_size, num_workers, drop_last):\n            \"\"\"\n            Returns\n            -------\n            train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n                Training and validation dataloaders\n            -------\n            \"\"\"\n            if isinstance(weights, int):\n                if weights == 1:\n                    raise ValueError(\"Please provide a list of weights for regression.\")\n            if isinstance(weights, dict):\n                raise ValueError(\"Please provide a list of weights for regression.\")\n\n            train_dataloader, valid_dataloader = create_dataloaders(X_train,\n                                                                    y_train,\n                                                                    X_valid,\n                                                                    y_valid,\n                                                                    weights,\n                                                                    batch_size,\n                                                                    num_workers,\n                                                                    drop_last)\n            return train_dataloader, valid_dataloader\n\n        def update_fit_params(self, X_train, y_train, X_valid, y_valid, loss_fn,\n                              weights, max_epochs, patience,\n                              batch_size, virtual_batch_size, num_workers, drop_last):\n\n            if loss_fn is None:\n                self.loss_fn = torch.nn.functional.mse_loss\n            else:\n                self.loss_fn = loss_fn\n\n            assert X_train.shape[1] == X_valid.shape[1], \"Dimension mismatch X_train X_valid\"\n            self.input_dim = X_train.shape[1]\n\n            if len(y_train.shape) == 1:\n                raise ValueError(\"\"\"Please apply reshape(-1, 1) to your targets\n                                    if doing single regression.\"\"\")\n            assert y_train.shape[1] == y_valid.shape[1], \"Dimension mismatch y_train y_valid\"\n            self.output_dim = y_train.shape[1]\n\n            self.updated_weights = weights\n\n            self.max_epochs = max_epochs\n            self.patience = patience\n            self.batch_size = batch_size\n            self.virtual_batch_size = virtual_batch_size\n            # Initialize counters and histories.\n            self.patience_counter = 0\n            self.epoch = 0\n            self.best_cost = np.inf\n            self.num_workers = num_workers\n            self.drop_last = drop_last\n\n        def train_epoch(self, train_loader):\n            \"\"\"\n            Trains one epoch of the network in self.network\n            Parameters\n            ----------\n                train_loader: a :class: `torch.utils.data.Dataloader`\n                    DataLoader with train set\n            \"\"\"\n\n            self.network.train()\n            y_preds = []\n            ys = []\n            total_loss = 0\n\n            for data, targets in train_loader:\n                batch_outs = self.train_batch(data, targets)\n                y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n                ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n                total_loss += batch_outs[\"loss\"]\n\n            y_preds = np.vstack(y_preds)\n            ys = np.vstack(ys)\n\n            #stopping_loss = mean_squared_error(y_true=ys, y_pred=y_preds)\n            stopping_loss =log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  )\n            total_loss = total_loss / len(train_loader)\n            epoch_metrics = {'loss_avg': total_loss,\n                             'stopping_loss': total_loss,\n                             }\n\n            if self.scheduler is not None:\n                self.scheduler.step()\n            return epoch_metrics\n\n        def train_batch(self, data, targets):\n            \"\"\"\n            Trains one batch of data\n            Parameters\n            ----------\n                data: a :tensor: `torch.tensor`\n                    Input data\n                target: a :tensor: `torch.tensor`\n                    Target data\n            \"\"\"\n            self.network.train()\n            data = data.to(self.device).float()\n\n            targets = targets.to(self.device).float()\n            self.optimizer.zero_grad()\n\n            output, M_loss = self.network(data)\n\n            loss = self.loss_fn(output, targets)\n\n            loss -= self.lambda_sparse*M_loss\n\n            loss.backward()\n            if self.clip_value:\n                clip_grad_norm_(self.network.parameters(), self.clip_value)\n            self.optimizer.step()\n\n            loss_value = loss.item()\n            batch_outs = {'loss': loss_value,\n                          'y_preds': output,\n                          'y': targets}\n            return batch_outs\n\n        def predict_epoch(self, loader):\n            \"\"\"\n            Validates one epoch of the network in self.network\n            Parameters\n            ----------\n                loader: a :class: `torch.utils.data.Dataloader`\n                        DataLoader with validation set\n            \"\"\"\n            y_preds = []\n            ys = []\n            self.network.eval()\n            total_loss = 0\n\n            for data, targets in loader:\n                batch_outs = self.predict_batch(data, targets)\n                total_loss += batch_outs[\"loss\"]\n                y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n                ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n\n            y_preds = np.vstack(y_preds)\n            ys = np.vstack(ys)\n\n            stopping_loss =log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  ) #mean_squared_error(y_true=ys, y_pred=y_preds)\n\n            total_loss = total_loss / len(loader)\n            epoch_metrics = {'total_loss': total_loss,\n                             'stopping_loss': stopping_loss}\n\n            return epoch_metrics\n\n        def predict_batch(self, data, targets):\n            \"\"\"\n            Make predictions on a batch (valid)\n            Parameters\n            ----------\n                data: a :tensor: `torch.Tensor`\n                    Input data\n                target: a :tensor: `torch.Tensor`\n                    Target data\n            Returns\n            -------\n                batch_outs: dict\n            \"\"\"\n            self.network.eval()\n            data = data.to(self.device).float()\n            targets = targets.to(self.device).float()\n\n            output, M_loss = self.network(data)\n\n            loss = self.loss_fn(output, targets)\n            #print(self.loss_fn, loss)\n            loss -= self.lambda_sparse*M_loss\n            #print(loss)\n            loss_value = loss.item()\n            batch_outs = {'loss': loss_value,\n                          'y_preds': output,\n                          'y': targets}\n            return batch_outs\n\n        def predict(self, X):\n            \"\"\"\n            Make predictions on a batch (valid)\n            Parameters\n            ----------\n                data: a :tensor: `torch.Tensor`\n                    Input data\n                target: a :tensor: `torch.Tensor`\n                    Target data\n            Returns\n            -------\n                predictions: np.array\n                    Predictions of the regression problem\n            \"\"\"\n            self.network.eval()\n            dataloader = DataLoader(PredictDataset(X),\n                                    batch_size=self.batch_size, shuffle=False)\n\n            results = []\n            for batch_nb, data in enumerate(dataloader):\n                data = data.to(self.device).float()\n\n                output, M_loss = self.network(data)\n                predictions = output.cpu().detach().numpy()\n                results.append(predictions)\n            res = np.vstack(results)\n            return res\n\n\n    class Config(object):\n        def __init__(self):\n            self.num_class = targets_tr.shape[1]\n            self.verbose=False\n            #\n            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            self.SPLITS = 10\n            self.EPOCHS = 200\n            self.num_ensembling = 1\n            self.seed = 0\n            # Parameters model\n            self.cat_emb_dim=[1] * cat_tr.shape[1] #to choose\n            self.cats_idx = list(range(cat_tr.shape[1]))\n            self.cat_dims = [len(np.unique(cat_tr[:, i])) for i in range(cat_tr.shape[1])]\n            self.num_numericals= numerical_tr.shape[1]\n            # save\n            self.save_name = \"../input/tabnet-smoothing/tabnet_raw_step1\"\n\n            self.strategy = \"KFOLD\" # \n    cfg = Config()\n    \n    X_test = np.concatenate([cat_test, numerical_test ], axis=1)\n    \n    SEED  = [0,1,2,3,4,5,6]\n    if cfg.strategy == \"KFOLD\":\n        oof_preds_all = []\n        oof_targets_all = []\n        scores_all =  []\n        scores_auc_all= []\n        preds_test = []\n        for seed in SEED:\n            print(\"## SEED : \", seed)\n            mskf = MultilabelStratifiedKFold(n_splits=5, random_state=cfg.seed+seed, shuffle=True)\n            oof_preds = []\n            oof_targets = []\n            scores = []\n            scores_auc = []\n            p = []\n            for j, (train_idx, val_idx) in enumerate(mskf.split(np.zeros(len(cat_tr)), targets_tr)):\n                print(\"FOLDS : \", j)\n\n                ## model\n                X_train, y_train = torch.as_tensor(np.concatenate([cat_tr[train_idx], numerical_tr[train_idx] ], axis=1)), torch.as_tensor(targets_tr[train_idx])\n                X_val, y_val = torch.as_tensor(np.concatenate([cat_tr[val_idx], numerical_tr[val_idx] ], axis=1)), torch.as_tensor(targets_tr[val_idx])\n                model = TabNetRegressor(n_d=24, n_a=24, n_steps=1, gamma=1.3, lambda_sparse=0, cat_dims=cfg.cat_dims, cat_emb_dim=cfg.cat_emb_dim, cat_idxs=cfg.cats_idx, optimizer_fn=torch.optim.Adam,\n                                       optimizer_params=dict(lr=2e-2), mask_type='entmax', device_name=cfg.device, scheduler_params=dict(milestones=[ 50,100,150], gamma=0.9), scheduler_fn=torch.optim.lr_scheduler.MultiStepLR)\n                #'sparsemax'\n\n                name = cfg.save_name + f\"_fold{j}_{seed}\"\n                model.load_model(name)\n                # preds on val\n                preds = model.predict(X_val)\n                preds = torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n                score = log_loss_multi(y_val, preds)\n\n                # preds on test\n                temp = model.predict(X_test)\n                p.append(torch.sigmoid(torch.as_tensor(temp)).detach().cpu().numpy())\n                ## save oof to compute the CV later\n                oof_preds.append(preds)\n                oof_targets.append(y_val)\n                scores.append(score)\n                scores_auc.append(auc_multi(y_val,preds))\n                print(f\"validation fold {j} : {score}\")\n            p = np.stack(p)\n            preds_test.append(p)\n            oof_preds_all.append(np.concatenate(oof_preds))\n            oof_targets_all.append(np.concatenate(oof_targets))\n            scores_all.append(np.array(scores))\n            scores_auc_all.append(np.array(scores_auc))\n        preds_test = np.stack(preds_test)\n        \n        \n\n    if cfg.strategy == \"KFOLD\":\n\n        for i in range(cfg.num_ensembling): \n            print(\"CV score fold : \", log_loss_multi(oof_targets_all[i], oof_preds_all[i]))\n            print(\"auc mean : \", sum(scores_auc_all[i])/len(scores_auc_all[i]))\n\n\n    \n\n    submission[columns] = preds_test.mean(1).mean(0)\n    submission.loc[test_features['cp_type']=='ctl_vehicle', submission.columns[1:]] = 0\n\n    \n    return submission\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Notebook 4 inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"def notebook_4_inference():\n    FOLDS = 10\n    # Number of epochs to train each model\n    EPOCHS = 80\n    # Batch size\n    BATCH_SIZE = 124\n    # Learning rate\n    LR = 0.001\n    # Verbosity\n    VERBOSE = 0\n    # Seed for deterministic results\n    # Seed for deterministic results\n    SEEDS1 = [1, 2, 3, 4, 5, 6, 7]\n    SEEDS2 = [8, 9, 10, 11, 12, 13, 14]\n    SEEDS3 = [15, 16, 17, 18, 19, 20, 21]\n    SEEDS4 = [22, 23, 24, 25, 26, 27, 28]\n    SEEDS5 = [29, 30, 31, 32, 33, 34, 35]\n\n    # Function to seed everything\n    def seed_everything(seed):\n        random.seed(seed)\n        np.random.seed(seed)\n        os.environ['PYTHONHASHSEED'] = str(seed)\n        tf.random.set_seed(seed)\n\n    def mapping_and_filter(train, train_targets, test):\n        cp_type = {'trt_cp': 0, 'ctl_vehicle': 1}\n        cp_dose = {'D1': 0, 'D2': 1}\n        for df in [train, test]:\n            df['cp_type'] = df['cp_type'].map(cp_type)\n            df['cp_dose'] = df['cp_dose'].map(cp_dose)\n        train_targets = train_targets[train['cp_type'] == 0].reset_index(drop = True)\n        train = train[train['cp_type'] == 0].reset_index(drop = True)\n        train_targets.drop(['sig_id'], inplace = True, axis = 1)\n        return train, train_targets, test\n\n    # Function to scale our data\n    def scaling(train, test):\n        features = train.columns[2:]\n        scaler = RobustScaler()\n        scaler.fit(pd.concat([train[features], test[features]], axis = 0))\n        train[features] = scaler.transform(train[features])\n        test[features] = scaler.transform(test[features])\n        return train, test, features\n\n    # Function to extract pca features\n    def fe_pca(train, test, n_components_g = 70, n_components_c = 10, SEED = 123):\n\n        features_g = list(train.columns[4:776])\n        features_c = list(train.columns[776:876])\n\n        def create_pca(train, test, features, kind = 'g', n_components = n_components_g):\n            train_ = train[features].copy()\n            test_ = test[features].copy()\n            data = pd.concat([train_, test_], axis = 0)\n            pca = PCA(n_components = n_components,  random_state = SEED)\n            data = pca.fit_transform(data)\n            columns = [f'pca_{kind}{i + 1}' for i in range(n_components)]\n            data = pd.DataFrame(data, columns = columns)\n            train_ = data.iloc[:train.shape[0]]\n            test_ = data.iloc[train.shape[0]:].reset_index(drop = True)\n            train = pd.concat([train, train_], axis = 1)\n            test = pd.concat([test, test_], axis = 1)\n            return train, test\n\n        train, test = create_pca(train, test, features_g, kind = 'g', n_components = n_components_g)\n        train, test = create_pca(train, test, features_c, kind = 'c', n_components = n_components_c)\n        return train, test\n\n    # Function to extract common stats features\n    def fe_stats(train, test):\n\n        features_g = list(train.columns[4:776])\n        features_c = list(train.columns[776:876])\n\n        for df in [train, test]:\n            df['g_sum'] = df[features_g].sum(axis = 1)\n            df['g_mean'] = df[features_g].mean(axis = 1)\n            df['g_std'] = df[features_g].std(axis = 1)\n            df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n            df['g_skew'] = df[features_g].skew(axis = 1)\n            df['c_sum'] = df[features_c].sum(axis = 1)\n            df['c_mean'] = df[features_c].mean(axis = 1)\n            df['c_std'] = df[features_c].std(axis = 1)\n            df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n            df['c_skew'] = df[features_c].skew(axis = 1)\n            df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n            df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n            df['gc_std'] = df[features_g + features_c].std(axis = 1)\n            df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n            df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n\n        return train, test\n\n    def c_squared(train, test):\n\n        features_c = list(train.columns[776:876])\n        for df in [train, test]:\n            for feature in features_c:\n                df[f'{feature}_squared'] = df[feature] ** 2\n        return train, test\n\n    # Function to calculate the mean log loss of the targets including clipping\n    def mean_log_loss(y_true, y_pred):\n        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n        metrics = []\n        for target in range(206):\n            metrics.append(log_loss(y_true[:, target], y_pred[:, target]))\n        return np.mean(metrics)\n\n    def create_model_rs(shape1, shape2):    \n        input_1 = tf.keras.layers.Input(shape = (shape1))\n        input_2 = tf.keras.layers.Input(shape = (shape2))\n\n        head_1 = tf.keras.layers.BatchNormalization()(input_1)\n        head_1 = tf.keras.layers.Dropout(0.2)(head_1)\n        head_1 = tf.keras.layers.Dense(512, activation = \"elu\")(head_1)\n        head_1 = tf.keras.layers.BatchNormalization()(head_1)\n        input_3 = tf.keras.layers.Dense(256, activation = \"elu\")(head_1)\n\n        input_3_concat = tf.keras.layers.Concatenate()([input_2, input_3])\n\n        head_2 = tf.keras.layers.BatchNormalization()(input_3_concat)\n        head_2 = tf.keras.layers.Dropout(0.3)(head_2)\n        head_2 = tf.keras.layers.Dense(512, \"relu\")(head_2)\n        head_2 = tf.keras.layers.BatchNormalization()(head_2)\n        head_2 = tf.keras.layers.Dense(512, \"elu\")(head_2)\n        head_2 = tf.keras.layers.BatchNormalization()(head_2)\n        head_2 = tf.keras.layers.Dense(256, \"relu\")(head_2)\n        head_2 = tf.keras.layers.BatchNormalization()(head_2)\n        input_4 = tf.keras.layers.Dense(256, \"elu\")(head_2)\n\n        input_4_avg = tf.keras.layers.Average()([input_3, input_4]) \n\n        head_3 = tf.keras.layers.BatchNormalization()(input_4_avg)\n        head_3 = tf.keras.layers.Dense(256, kernel_initializer = 'lecun_normal', activation = 'selu')(head_3)\n        head_3 = tf.keras.layers.BatchNormalization()(head_3)\n        head_3 = tf.keras.layers.Dense(206, kernel_initializer = 'lecun_normal', activation = 'selu')(head_3)\n        head_3 = tf.keras.layers.BatchNormalization()(head_3)\n        output = tf.keras.layers.Dense(206, activation = \"sigmoid\")(head_3)\n\n        model = tf.keras.models.Model(inputs = [input_1, input_2], outputs = output)\n        opt = tf.optimizers.Adam(learning_rate = LR)\n        model.compile(optimizer = opt, \n                      loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.0015), \n                      metrics = tf.keras.metrics.BinaryCrossentropy())\n\n        return model\n\n\n    # Function to create our 5 layer dnn model\n    def create_model_5l(shape):\n        inp = tf.keras.layers.Input(shape = (shape))\n        x = tf.keras.layers.BatchNormalization()(inp)\n        x = tf.keras.layers.Dropout(0.4)(x)\n        x = tf.keras.layers.Dense(2560, activation = 'relu')(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(0.4)(x)\n        x = tf.keras.layers.Dense(2048, activation = 'relu')(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(0.4)(x)\n        x = tf.keras.layers.Dense(1524, activation = 'relu')(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(0.4)(x)\n        x = tf.keras.layers.Dense(1012, activation = 'relu')(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(0.4)(x)\n        x = tf.keras.layers.Dense(780, activation = 'relu')(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(0.2)(x)\n        out = tf.keras.layers.Dense(206, activation = 'sigmoid')(x)\n        model = tf.keras.models.Model(inputs = inp, outputs = out)\n        opt = tf.optimizers.Adam(learning_rate = LR)\n        opt = tfa.optimizers.SWA(opt)\n        model.compile(optimizer = opt, \n                      loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.0020),\n                      metrics = tf.keras.metrics.BinaryCrossentropy())\n        return model\n\n    # Function to create our 4 layer dnn model\n    def create_model_4l(shape):\n        inp = tf.keras.layers.Input(shape = (shape))\n        x = tf.keras.layers.BatchNormalization()(inp)\n        x = tf.keras.layers.Dropout(0.4)(x)\n        x = tf.keras.layers.Dense(2048, activation = 'relu')(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(0.4)(x)\n        x = tf.keras.layers.Dense(1524, activation = 'relu')(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(0.4)(x)\n        x = tf.keras.layers.Dense(1012, activation = 'relu')(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(0.4)(x)\n        x = tf.keras.layers.Dense(1012, activation = 'relu')(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(0.2)(x)\n        out = tf.keras.layers.Dense(206, activation = 'sigmoid')(x)\n        model = tf.keras.models.Model(inputs = inp, outputs = out)\n        opt = tf.optimizers.Adam(learning_rate = LR)\n        model.compile(optimizer = opt, \n                      loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.0020),\n                      metrics = tf.keras.metrics.BinaryCrossentropy())\n        return model\n\n    # Function to create our 3 layer dnn model\n    def create_model_3l(shape):\n        inp = tf.keras.layers.Input(shape = (shape))\n        x = tf.keras.layers.BatchNormalization()(inp)\n        x = tf.keras.layers.Dropout(0.4914099166744246)(x)\n        x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(1159, activation = 'relu'))(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(0.18817607797795838)(x)\n        x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(960, activation = 'relu'))(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(0.12542057776853896)(x)\n        x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(1811, activation = 'relu'))(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(0.20175242230280122)(x)\n        out = tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, activation = 'sigmoid'))(x)\n        model = tf.keras.models.Model(inputs = inp, outputs = out)\n        opt = tf.optimizers.Adam(learning_rate = LR)\n        opt = tfa.optimizers.Lookahead(opt, sync_period = 10)\n        model.compile(optimizer = opt, \n                      loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.0015),\n                      metrics = tf.keras.metrics.BinaryCrossentropy())\n        return model\n\n    # Function to create our 2 layer dnn model\n    def create_model_2l(shape):\n        inp = tf.keras.layers.Input(shape = (shape))\n        x = tf.keras.layers.BatchNormalization()(inp)\n        x = tf.keras.layers.Dropout(0.2688628097505064)(x)\n        x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(1292, activation = 'relu'))(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(0.4598218403250696)(x)\n        x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(983, activation = 'relu'))(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(0.4703144018483698)(x)\n        out = tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, activation = 'sigmoid'))(x)\n        model = tf.keras.models.Model(inputs = inp, outputs = out)\n        opt = tf.optimizers.Adam(learning_rate = LR)\n        opt = tfa.optimizers.Lookahead(opt, sync_period = 10)\n        model.compile(optimizer = opt, \n                      loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.0015),\n                      metrics = tf.keras.metrics.BinaryCrossentropy())\n        return model\n\n\n    # Function to train our dnn\n    def train_and_evaluate(train, test, train_targets, features, start_predictors, SEED = 123, MODEL = '3l'):\n        seed_everything(SEED)\n        oof_pred = np.zeros((train.shape[0], 206))\n        test_pred = np.zeros((test.shape[0], 206))   \n        for fold, (trn_ind, val_ind) in enumerate(MultilabelStratifiedKFold(n_splits = FOLDS, \n                                                                            random_state = SEED, \n                                                                            shuffle = True)\\\n                                                  .split(train_targets, train_targets)):\n            K.clear_session()\n            if MODEL == '5l':\n                model = create_model_5l(len(features))\n            elif MODEL == '4l':\n                model = create_model_4l(len(features))\n            elif MODEL == '3l':\n                model = create_model_3l(len(features))\n            elif MODEL == '2l':\n                model = create_model_2l(len(features))\n            elif MODEL == \"rs\":\n                model = create_model_rs(len(features), len(start_predictors))\n\n            early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_binary_crossentropy',\n                                                              mode = 'min',\n                                                              patience = 10,\n                                                              restore_best_weights = False,\n                                                              verbose = VERBOSE)\n\n            reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_binary_crossentropy',\n                                                             mode = 'min',\n                                                             factor = 0.3,\n                                                             patience = 3,\n                                                             verbose = VERBOSE)\n\n            checkpoint = tf.keras.callbacks.ModelCheckpoint(f'{MODEL}_{fold}_{SEED}.h5',\n                                                            monitor = 'val_binary_crossentropy',\n                                                            verbose = VERBOSE,\n                                                            save_best_only = True,\n                                                            save_weights_only = True)\n\n            x_train, x_val = train[features].values[trn_ind], train[features].values[val_ind]\n            y_train, y_val = train_targets.values[trn_ind], train_targets.values[val_ind]\n\n            if MODEL == \"rs\":\n                x_train_, x_val_ = train[start_predictors].values[trn_ind], train[start_predictors].values[val_ind]\n\n                model.fit([x_train, x_train_], y_train,\n                      validation_data = ([x_val, x_val_], y_val),\n                      epochs = EPOCHS, \n                      batch_size = BATCH_SIZE,\n                      callbacks = [early_stopping, reduce_lr,  checkpoint],\n                      verbose = VERBOSE)\n\n                model.load_weights(f'{MODEL}_{fold}_{SEED}.h5')\n\n                oof_pred[val_ind] = model.predict([x_val, x_val_])\n                test_pred += model.predict([test[features].values, test[start_predictors].values]) / FOLDS\n\n            else:\n                model.fit(x_train, y_train,\n                      validation_data = (x_val, y_val),\n                      epochs = EPOCHS, \n                      batch_size = BATCH_SIZE,\n                      callbacks = [early_stopping, reduce_lr,  checkpoint],\n                      verbose = VERBOSE)\n\n                model.load_weights(f'{MODEL}_{fold}_{SEED}.h5')\n\n                oof_pred[val_ind] = model.predict(x_val)\n                test_pred += model.predict(test[features].values) / FOLDS\n\n\n        oof_score = mean_log_loss(train_targets.values, oof_pred)\n        print(f'Our out of folds mean log loss score is {oof_score}')\n\n        return test_pred, oof_pred\n\n    # Function to train our dnn\n    def inference(train, test, train_targets, features, start_predictors, SEED = 123, MODEL = '3l', PATH = '../input/moa-3layer'):\n        seed_everything(SEED)\n        oof_pred = np.zeros((train.shape[0], 206))\n        test_pred = np.zeros((test.shape[0], 206))   \n        for fold, (trn_ind, val_ind) in enumerate(MultilabelStratifiedKFold(n_splits = FOLDS, \n                                                                            random_state = SEED, \n                                                                            shuffle = True)\\\n                                                  .split(train_targets, train_targets)):\n            K.clear_session()\n            if MODEL == '5l':\n                model = create_model_5l(len(features))\n            elif MODEL == '4l':\n                model = create_model_4l(len(features))\n            elif MODEL == '3l':\n                model = create_model_3l(len(features))\n            elif MODEL == '2l':\n                model = create_model_2l(len(features))\n            elif MODEL == \"rs\":\n                model = create_model_rs(len(features), len(start_predictors))\n\n\n            x_train, x_val = train[features].values[trn_ind], train[features].values[val_ind]\n            y_train, y_val = train_targets.values[trn_ind], train_targets.values[val_ind]\n\n            model.load_weights('../input/moa-5seed/Model_Weights'+'/'+MODEL+'_'+str(fold)+'_'+str(SEED)+'.h5')\n\n            if MODEL == \"rs\":\n                x_train_, x_val_ = train[start_predictors].values[trn_ind], train[start_predictors].values[val_ind]\n                oof_pred[val_ind] = model.predict([x_val, x_val_])\n                test_pred += model.predict([test[features].values, test[start_predictors].values]) / FOLDS\n            else:\n                oof_pred[val_ind] = model.predict(x_val)\n                test_pred += model.predict(test[features].values) / FOLDS\n\n        oof_score = mean_log_loss(train_targets.values, oof_pred)\n        print(f'Our out of folds mean log loss score is {oof_score}')\n\n        return test_pred, oof_pred\n\n\n\n    # Function to train our model with multiple seeds and average the predictions\n    def run_multiple_seeds(train, test, train_targets, features, start_predictors, SEEDS = [123], MODEL = '3l', PATH = '../input/moa-3layer'):\n\n        test_pred = []\n        oof_pred = []\n\n        for SEED in SEEDS:\n            print(f'Using model {MODEL} with seed {SEED} for inference')\n            print(f'Trained with {len(features)} features')\n            test_pred_, oof_pred_ = inference(train, test, train_targets, features, start_predictors, SEED = SEED, MODEL = MODEL, PATH = PATH)\n            test_pred.append(test_pred_)\n            oof_pred.append(oof_pred_)\n            print('-'*50)\n            print('\\n')\n\n        test_pred = np.average(test_pred, axis = 0)\n        oof_pred = np.average(oof_pred, axis = 0)\n\n        seed_log_loss = mean_log_loss(train_targets.values, oof_pred)\n        print(f'Our out of folds log loss for our seed blend model is {seed_log_loss}')\n\n        return test_pred, oof_pred\n\n    def submission(test_pred):\n        sample_submission.loc[:, train_targets.columns] = test_pred\n        sample_submission.loc[test['cp_type'] == 1, train_targets.columns] = 0\n        return sample_submission\n\n    # Got this predictors from public kernels for the resnet type model\n    start_predictors = [\"g-0\", \"g-7\", \"g-8\", \"g-10\", \"g-13\", \"g-17\", \"g-20\", \"g-22\", \"g-24\", \"g-26\", \"g-28\", \"g-29\", \"g-30\", \"g-31\", \"g-32\", \"g-34\", \"g-35\", \"g-36\", \"g-37\", \"g-38\",\n                        \"g-39\",\"g-41\", \"g-46\", \"g-48\", \"g-50\", \"g-51\", \"g-52\", \"g-55\", \"g-58\", \"g-59\", \"g-61\", \"g-62\", \"g-63\", \"g-65\", \"g-66\", \"g-67\", \"g-68\", \"g-70\", \"g-72\", \"g-74\", \n                        \"g-75\", \"g-79\", \"g-83\", \"g-84\", \"g-85\", \"g-86\", \"g-90\", \"g-91\", \"g-94\", \"g-95\", \"g-96\", \"g-97\", \"g-98\", \"g-100\", \"g-102\", \"g-105\", \"g-106\", \"g-112\", \"g-113\", \n                        \"g-114\", \"g-116\", \"g-121\", \"g-123\", \"g-126\", \"g-128\", \"g-131\", \"g-132\", \"g-134\", \"g-135\", \"g-138\", \"g-139\", \"g-140\", \"g-142\", \"g-144\", \"g-145\", \"g-146\", \n                        \"g-147\", \"g-148\", \"g-152\", \"g-155\", \"g-157\", \"g-158\", \"g-160\", \"g-163\", \"g-164\", \"g-165\", \"g-170\", \"g-173\", \"g-174\", \"g-175\", \"g-177\", \"g-178\", \"g-181\", \n                        \"g-183\", \"g-185\", \"g-186\", \"g-189\", \"g-192\", \"g-194\", \"g-195\", \"g-196\", \"g-197\", \"g-199\", \"g-201\", \"g-202\", \"g-206\", \"g-208\", \"g-210\", \"g-213\", \"g-214\", \n                        \"g-215\", \"g-220\", \"g-226\", \"g-228\", \"g-229\", \"g-235\", \"g-238\", \"g-241\", \"g-242\", \"g-243\", \"g-244\", \"g-245\", \"g-248\", \"g-250\", \"g-251\", \"g-254\", \"g-257\", \n                        \"g-259\", \"g-261\", \"g-266\", \"g-270\", \"g-271\", \"g-272\", \"g-275\", \"g-278\", \"g-282\", \"g-287\", \"g-288\", \"g-289\", \"g-291\", \"g-293\", \"g-294\", \"g-297\", \"g-298\",\n                        \"g-301\", \"g-303\", \"g-304\", \"g-306\", \"g-308\", \"g-309\", \"g-310\", \"g-311\", \"g-314\", \"g-315\", \"g-316\", \"g-317\", \"g-320\", \"g-321\", \"g-322\", \"g-327\", \"g-328\", \n                        \"g-329\", \"g-332\", \"g-334\", \"g-335\", \"g-336\", \"g-337\", \"g-339\", \"g-342\", \"g-344\", \"g-349\", \"g-350\", \"g-351\", \"g-353\", \"g-354\", \"g-355\", \"g-357\", \"g-359\", \n                        \"g-360\", \"g-364\", \"g-365\", \"g-366\", \"g-367\", \"g-368\", \"g-369\", \"g-374\", \"g-375\", \"g-377\", \"g-379\", \"g-385\", \"g-386\", \"g-390\", \"g-392\", \"g-393\", \"g-400\", \n                        \"g-402\", \"g-406\", \"g-407\", \"g-409\", \"g-410\", \"g-411\", \"g-414\", \"g-417\", \"g-418\", \"g-421\", \"g-423\", \"g-424\", \"g-427\", \"g-429\", \"g-431\", \"g-432\", \"g-433\", \n                        \"g-434\", \"g-437\", \"g-439\", \"g-440\", \"g-443\", \"g-449\", \"g-458\", \"g-459\", \"g-460\", \"g-461\", \"g-464\", \"g-467\", \"g-468\", \"g-470\", \"g-473\", \"g-477\", \"g-478\", \n                        \"g-479\", \"g-484\", \"g-485\", \"g-486\", \"g-488\", \"g-489\", \"g-491\", \"g-494\", \"g-496\", \"g-498\", \"g-500\", \"g-503\", \"g-504\", \"g-506\", \"g-508\", \"g-509\", \"g-512\", \n                        \"g-522\", \"g-529\", \"g-531\", \"g-534\", \"g-539\", \"g-541\", \"g-546\", \"g-551\", \"g-553\", \"g-554\", \"g-559\", \"g-561\", \"g-562\", \"g-565\", \"g-568\", \"g-569\", \"g-574\", \n                        \"g-577\", \"g-578\", \"g-586\", \"g-588\", \"g-590\", \"g-594\", \"g-595\", \"g-596\", \"g-597\", \"g-599\", \"g-600\", \"g-603\", \"g-607\", \"g-615\", \"g-618\", \"g-619\", \"g-620\", \n                        \"g-625\", \"g-628\", \"g-629\", \"g-632\", \"g-634\", \"g-635\", \"g-636\", \"g-638\", \"g-639\", \"g-641\", \"g-643\", \"g-644\", \"g-645\", \"g-646\", \"g-647\", \"g-648\", \"g-663\", \n                        \"g-664\", \"g-665\", \"g-668\", \"g-669\", \"g-670\", \"g-671\", \"g-672\", \"g-673\", \"g-674\", \"g-677\", \"g-678\", \"g-680\", \"g-683\", \"g-689\", \"g-691\", \"g-693\", \"g-695\", \n                        \"g-701\", \"g-702\", \"g-703\", \"g-704\", \"g-705\", \"g-706\", \"g-708\", \"g-711\", \"g-712\", \"g-720\", \"g-721\", \"g-723\", \"g-724\", \"g-726\", \"g-728\", \"g-731\", \"g-733\", \n                        \"g-738\", \"g-739\", \"g-742\", \"g-743\", \"g-744\", \"g-745\", \"g-749\", \"g-750\", \"g-752\", \"g-760\", \"g-761\", \"g-764\", \"g-766\", \"g-768\", \"g-770\", \"g-771\", \"c-0\", \n                        \"c-1\", \"c-2\", \"c-3\", \"c-4\", \"c-5\", \"c-6\", \"c-7\", \"c-8\", \"c-9\", \"c-10\", \"c-11\", \"c-12\", \"c-13\", \"c-14\", \"c-15\", \"c-16\", \"c-17\", \"c-18\", \"c-19\", \"c-20\", \n                        \"c-21\", \"c-22\", \"c-23\", \"c-24\", \"c-25\", \"c-26\", \"c-27\", \"c-28\", \"c-29\", \"c-30\", \"c-31\", \"c-32\", \"c-33\", \"c-34\", \"c-35\", \"c-36\", \"c-37\", \"c-38\", \"c-39\", \n                        \"c-40\", \"c-41\", \"c-42\", \"c-43\", \"c-44\", \"c-45\", \"c-46\", \"c-47\", \"c-48\", \"c-49\", \"c-50\", \"c-51\", \"c-52\", \"c-53\", \"c-54\", \"c-55\", \"c-56\", \"c-57\", \"c-58\", \n                        \"c-59\", \"c-60\", \"c-61\", \"c-62\", \"c-63\", \"c-64\", \"c-65\", \"c-66\", \"c-67\", \"c-68\", \"c-69\", \"c-70\", \"c-71\", \"c-72\", \"c-73\", \"c-74\", \"c-75\", \"c-76\", \"c-77\", \n                        \"c-78\", \"c-79\", \"c-80\", \"c-81\", \"c-82\", \"c-83\", \"c-84\", \"c-85\", \"c-86\", \"c-87\", \"c-88\", \"c-89\", \"c-90\", \"c-91\", \"c-92\", \"c-93\", \"c-94\", \"c-95\", \"c-96\", \n                        \"c-97\", \"c-98\", \"c-99\"]\n\n\n    train = pd.read_csv('../input/lish-moa/train_features.csv')\n    train_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n    test = pd.read_csv('../input/lish-moa/test_features.csv')\n    sample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n    train, train_targets, test = mapping_and_filter(train, train_targets, test)\n    train, test = fe_stats(train, test)\n    train, test = c_squared(train, test)\n    train, test = fe_pca(train, test, n_components_g = 70, n_components_c = 10, SEED = 123)\n    train, test, features = scaling(train, test)\n\n    # Inference time\n    test_pred_5l, oof_pred_5l = run_multiple_seeds(train, test, train_targets, features, start_predictors, SEEDS = SEEDS1, MODEL = '5l', PATH = '../input/moaaaaa/5l')\n    test_pred_4l, oof_pred_4l = run_multiple_seeds(train, test, train_targets, features, start_predictors, SEEDS = SEEDS2, MODEL = '4l', PATH = '../input/moaaaaa/4l')\n    test_pred_3l, oof_pred_3l = run_multiple_seeds(train, test, train_targets, features, start_predictors, SEEDS = SEEDS3, MODEL = '3l', PATH = '../input/moaaaaa/3l')\n    test_pred_2l, oof_pred_2l = run_multiple_seeds(train, test, train_targets, features, start_predictors, SEEDS = SEEDS4, MODEL = '2l', PATH = '../input/moaaaaa/2l')\n    test_pred_rs, oof_pred_rs = run_multiple_seeds(train, test, train_targets, features, start_predictors, SEEDS = SEEDS5, MODEL = 'rs', PATH = '../input/moaaaaa/rs')\n\n    # Blend 5l, 4l, 3l and l2 dnn model\n    oof_pred = np.average([oof_pred_5l, oof_pred_4l, oof_pred_3l, oof_pred_2l], axis = 0)\n    seed_log_loss = mean_log_loss(train_targets.values, oof_pred)\n    print(f'Our final out of folds log loss for our classic dnn blend is {seed_log_loss}')\n    test_pred = np.average([test_pred_5l, test_pred_4l, test_pred_3l, test_pred_2l], axis = 0)\n\n    # Blend the result of the previous model with the dnn resnet type model\n    oof_pred = np.average([oof_pred, oof_pred_rs], axis = 0)\n    seed_log_loss = mean_log_loss(train_targets.values, oof_pred)\n    print(f'Our final out of folds log loss for our classic dnn + dnn resnet type model is {seed_log_loss}')\n    test_pred = np.average([test_pred, test_pred_rs], axis = 0)\n\n    sample_submission = submission(test_pred)\n    \n    return sample_submission","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Notebook 5 inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"def notebook_5_inference():    \n    train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n    train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n    train_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n    train_drug = pd.read_csv('../input/lish-moa/train_drug.csv')\n    test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n    sample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n\n    EXP_NAME = 'pytorch-transfer-learning-with-k-folds-by-drug-ids'\n    OMIT_VALIDATION = True \n\n    os.makedirs(f'exp_models/{EXP_NAME}',exist_ok=True)\n\n    GENES = [col for col in train_features.columns if col.startswith('g-')]\n    CELLS = [col for col in train_features.columns if col.startswith('c-')]\n\n#     print('GENES: {}'.format(GENES[:10]))\n#     print('CELLS: {}'.format(CELLS[:10]))\n\n    for col in (GENES + CELLS):\n\n        transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n        vec_len = len(train_features[col].values)\n        vec_len_test = len(test_features[col].values)\n        raw_vec = train_features[col].values.reshape(vec_len, 1)\n        transformer.fit(raw_vec)\n\n        train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n        test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n\n    def seed_everything(seed=42):\n        random.seed(seed)\n        os.environ['PYTHONHASHSEED'] = str(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n\n\n    seed_everything(seed=42)\n\n    # GENES\n    n_comp = 600  #<--Update\n\n    gen_pca = PCA(n_components=n_comp, random_state=42).fit(train_features[GENES])\n    train2 = gen_pca.transform(train_features[GENES]); test2 = gen_pca.transform(test_features[GENES])\n\n    train2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n    test2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n    # drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n    train_features = pd.concat((train_features, train2), axis=1)\n    test_features = pd.concat((test_features, test2), axis=1)\n\n#     print('train_features: {}'.format(train_features.shape))\n#     print('test_features: {}'.format(test_features.shape))\n\n    #CELLS\n    n_comp = 50  #<--Update\n\n    cell_pca = PCA(n_components=n_comp, random_state=42).fit(train_features[CELLS])\n    train2 = cell_pca.transform(train_features[CELLS]); test2 = cell_pca.transform(test_features[CELLS])\n\n    train2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n    test2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n    # drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n    train_features = pd.concat((train_features, train2), axis=1)\n    test_features = pd.concat((test_features, test2), axis=1)\n\n    # print(train.shape)\n    # print(test.shape)\n    # print(sample_submission.shape)\n\n    var_thresh = VarianceThreshold(0.8).fit(train_features.iloc[:,4:])  #<-- Update\n    #data = train_features.append(test_features)\n    #data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\n    train_features_transformed = var_thresh.transform(train_features.iloc[:,4:])#data_transformed[ : train_features.shape[0]]\n    test_features_transformed = var_thresh.transform(test_features.iloc[:,4:])#data_transformed[-test_features.shape[0] : ]\n\n\n    train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                                  columns=['sig_id','cp_type','cp_time','cp_dose'])\n\n    train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\n    test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                                 columns=['sig_id','cp_type','cp_time','cp_dose'])\n\n    test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\n#     print('train_features: {}'.format(train_features.shape))\n#     print('test_features: {}'.format(test_features.shape))\n\n\n    train = train_features.merge(train_targets_scored, on='sig_id')\n    train = train.merge(train_targets_nonscored, on='sig_id')\n    train = train.merge(train_drug, on='sig_id')\n    train = train[train['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n    test = test_features[test_features['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n\n    train = train.drop('cp_type', axis=1)\n    test = test.drop('cp_type', axis=1)\n\n    target_cols = [x for x in train_targets_scored.columns if x != 'sig_id']\n    aux_target_cols = [x for x in train_targets_nonscored.columns if x != 'sig_id']\n    all_target_cols = target_cols + aux_target_cols\n\n    num_targets = len(target_cols)\n    num_aux_targets = len(aux_target_cols)\n    num_all_targets = len(all_target_cols)\n\n\n    class MoADataset:\n        def __init__(self, features, targets):\n            self.features = features\n            self.targets = targets\n\n        def __len__(self):\n            return (self.features.shape[0])\n\n        def __getitem__(self, idx):\n            dct = {\n                'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n                'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)\n            }\n\n            return dct\n\n    class TestDataset:\n        def __init__(self, features):\n            self.features = features\n\n        def __len__(self):\n            return (self.features.shape[0])\n\n        def __getitem__(self, idx):\n            dct = {\n                'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n            }\n\n            return dct\n\n    def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n        model.train()\n        final_loss = 0\n\n        for data in dataloader:\n            optimizer.zero_grad()\n            inputs, targets = data['x'].to(device), data['y'].to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            final_loss += loss.item()\n\n        final_loss /= len(dataloader)\n        return final_loss\n\n    def valid_fn(model, loss_fn, dataloader, device):\n        model.eval()\n        final_loss = 0\n        valid_preds = []\n\n        for data in dataloader:\n            inputs, targets = data['x'].to(device), data['y'].to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n\n            final_loss += loss.item()\n            valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n        final_loss /= len(dataloader)\n        valid_preds = np.concatenate(valid_preds)\n        return final_loss, valid_preds\n\n    def inference_fn(model, dataloader, device):\n        model.eval()\n        preds = []\n\n        for data in dataloader:\n            inputs = data['x'].to(device)\n\n            with torch.no_grad():\n                outputs = model(inputs)\n\n            preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n        preds = np.concatenate(preds)\n        return preds\n\n    class SmoothBCEwLogits(_WeightedLoss):\n        def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n            super().__init__(weight=weight, reduction=reduction)\n            self.smoothing = smoothing\n            self.weight = weight\n            self.reduction = reduction\n\n        @staticmethod\n        def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n            assert 0 <= smoothing < 1\n\n            with torch.no_grad():\n                targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n\n            return targets\n\n        def forward(self, inputs, targets):\n            targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n                self.smoothing)\n            loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n            if  self.reduction == 'sum':\n                loss = loss.sum()\n            elif  self.reduction == 'mean':\n                loss = loss.mean()\n\n            return loss\n\n    class Model(nn.Module):\n        def __init__(self, num_features, num_targets):\n            super(Model, self).__init__()\n            self.hidden_size = [1500, 1250, 1000, 750]\n            self.dropout_value = [0.5, 0.35, 0.3, 0.25]\n\n            self.batch_norm1 = nn.BatchNorm1d(num_features)\n            self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n\n            self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n            self.dropout2 = nn.Dropout(self.dropout_value[0])\n            self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n\n            self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n            self.dropout3 = nn.Dropout(self.dropout_value[1])\n            self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n\n            self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n            self.dropout4 = nn.Dropout(self.dropout_value[2])\n            self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n\n            self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n            self.dropout5 = nn.Dropout(self.dropout_value[3])\n            self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n\n        def forward(self, x):\n            x = self.batch_norm1(x)\n            x = F.leaky_relu(self.dense1(x))\n\n            x = self.batch_norm2(x)\n            x = self.dropout2(x)\n            x = F.leaky_relu(self.dense2(x))\n\n            x = self.batch_norm3(x)\n            x = self.dropout3(x)\n            x = F.leaky_relu(self.dense3(x))\n\n            x = self.batch_norm4(x)\n            x = self.dropout4(x)\n            x = F.leaky_relu(self.dense4(x))\n\n            x = self.batch_norm5(x)\n            x = self.dropout5(x)\n            x = self.dense5(x)\n            return x\n\n    class LabelSmoothingLoss(nn.Module):\n        def __init__(self, classes, smoothing=0.0, dim=-1):\n            super(LabelSmoothingLoss, self).__init__()\n            self.confidence = 1.0 - smoothing\n            self.smoothing = smoothing\n            self.cls = classes\n            self.dim = dim\n\n        def forward(self, pred, target):\n            pred = pred.log_softmax(dim=self.dim)\n\n            with torch.no_grad():\n                true_dist = torch.zeros_like(pred)\n                true_dist.fill_(self.smoothing / (self.cls - 1))\n                true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n\n            return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    \n\n    class FineTuneScheduler:\n        def __init__(self, epochs):\n            self.epochs = epochs\n            self.epochs_per_step = 0\n            self.frozen_layers = []\n\n        def copy_without_top(self, model, num_features, num_targets, num_targets_new):\n            self.frozen_layers = []\n\n            model_new = Model(num_features, num_targets)\n            model_new.load_state_dict(model.state_dict())\n\n            # Freeze all weights\n            for name, param in model_new.named_parameters():\n                layer_index = name.split('.')[0][-1]\n\n                if layer_index == 5:\n                    continue\n\n                param.requires_grad = False\n\n                # Save frozen layer names\n                if layer_index not in self.frozen_layers:\n                    self.frozen_layers.append(layer_index)\n\n            self.epochs_per_step = self.epochs // len(self.frozen_layers)\n\n            # Replace the top layers with another ones\n            model_new.batch_norm5 = nn.BatchNorm1d(model_new.hidden_size[3])\n            model_new.dropout5 = nn.Dropout(model_new.dropout_value[3])\n            model_new.dense5 = nn.utils.weight_norm(nn.Linear(model_new.hidden_size[-1], num_targets_new))\n            model_new.to(DEVICE)\n            return model_new\n\n        def step(self, epoch, model):\n            if len(self.frozen_layers) == 0:\n                return\n\n            if epoch % self.epochs_per_step == 0:\n                last_frozen_index = self.frozen_layers[-1]\n\n                # Unfreeze parameters of the last frozen layer\n                for name, param in model.named_parameters():\n                    layer_index = name.split('.')[0][-1]\n\n                    if layer_index == last_frozen_index:\n                        param.requires_grad = True\n\n                del self.frozen_layers[-1]  # Remove the last layer as unfrozen\n\n    def process_data(data):\n        data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n        return data\n\n    feature_cols = [c for c in process_data(train).columns if c not in all_target_cols]\n    feature_cols = [c for c in feature_cols if c not in ['kfold', 'sig_id', 'drug_id']]\n    num_features = len(feature_cols)\n\n    # HyperParameters\n\n    DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n    # DEVICE = \"cpu\"\n    EPOCHS = 24\n    BATCH_SIZE = 128\n\n    WEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 3e-6}\n    MAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 3e-3}\n    DIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2}\n    PCT_START = 0.1\n\n    folds = train.copy()\n\n    mskf = MultilabelStratifiedKFold(n_splits=7)\n\n    for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=train[target_cols])):\n        folds.loc[v_idx, 'kfold'] = int(f)\n\n    folds['kfold'] = folds['kfold'].astype(int)\n\n    def run_training(fold, seed, oof_df):\n\n        save_model_path = f\"../input/illialishmoaweights/{EXP_NAME}/seed_{seed}\"\n    #     os.makedirs(save_model_path, exist_ok=True)\n\n        seed_everything(seed)\n\n        test_ = process_data(test)        \n\n    #     #--------------------- PREDICTION---------------------\n        x_test = test_[feature_cols].values.astype(float)\n        testdataset = TestDataset(x_test)\n        testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n\n        model = Model(num_features, num_targets)\n        model.load_state_dict(torch.load(f\"{save_model_path}/SCORED_ONLY_FOLD{fold}_.pth\"))    \n        model.to(DEVICE)\n\n        predictions = np.zeros((len(test_), num_targets))\n        predictions = inference_fn(model, testloader, DEVICE)\n\n        # Predict oof\n        if not OMIT_VALIDATION:\n            train = process_data(folds)\n\n            val_idx = train[train['kfold'] == fold].index\n            valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n\n            x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n            valid_dataset = MoADataset(x_valid, y_valid)\n            validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n            loss_fn = nn.BCEWithLogitsLoss()\n\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n            oof_df[val_idx] = valid_preds\n\n            print(f\"loss : {valid_loss}\")\n\n        return oof_df, predictions\n\n    def run_k_fold(NFOLDS, seed):\n        oof = np.zeros((len(train), len(target_cols)))\n        predictions = np.zeros((len(test), len(target_cols)))\n\n        for fold in range(NFOLDS):\n\n            oof, temp_pred = run_training(fold, seed, oof)\n\n            predictions += temp_pred \n            #oof += temp_oof \n\n        return oof, ( predictions / NFOLDS)\n\n    # Averaging on multiple SEEDS\n\n    SEED = [0,1,2,3,4,5,6] #<-- Update\n    NFOLDS = 7\n\n    result_oof = np.zeros((len(train), len(target_cols)))\n    result_predictions = np.zeros((len(test), len(target_cols)))\n\n    for seed in SEED:\n\n        fold_oof, fold_predictions = run_k_fold(NFOLDS, seed)\n        result_oof += fold_oof \n        result_predictions += fold_predictions \n\n    train[target_cols] =  ( result_oof / len(SEED) )\n    test[target_cols] = ( result_predictions / len(SEED) )\n\n\n    if not OMIT_VALIDATION:\n        valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n\n\n        y_true = train_targets_scored[target_cols].values\n        y_pred = valid_results[target_cols].values\n\n        score = 0\n        for i in range(len(target_cols)):\n            score_ = log_loss(y_true[:, i], y_pred[:, i])\n            score += score_ / y_true.shape[1]\n\n        print(\"CV log_loss: \", score)\n\n\n    sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n    sub\n    \n    return sub","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Notebook 6 inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"def notebook_6_inference():\n\n\n    OMIT_VALIDATION = True\n\n    INFERENCE_FOLDER = '../input/alexlishmoamodels/results/'\n    import pickle\n\n    def load_pickle(file):\n        return pickle.load(open(INFERENCE_FOLDER + file, 'rb'))\n\n    import sys\n    sys.path.append('../input/iterativestratification')\n    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\n    import numpy as np\n    import random\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import os\n    import copy\n\n    from sklearn import preprocessing\n    from sklearn.metrics import log_loss ,roc_auc_score\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.decomposition import PCA\n    from pickle import load,dump\n\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n\n    import warnings\n    warnings.filterwarnings('ignore')\n\n    from sklearn.preprocessing import QuantileTransformer\n    \n    os.listdir('../input/lish-moa')\n\n    train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n    train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n    train_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n\n    test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n\n    df = pd.read_csv('../input/lish-moa/sample_submission.csv')\n\n    train_features2=train_features.copy()\n    test_features2=test_features.copy()\n\n    GENES = [col for col in train_features.columns if col.startswith('g-')]\n    CELLS = [col for col in train_features.columns if col.startswith('c-')]\n\n    for col in (GENES + CELLS):\n\n        transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n        vec_len = len(train_features[col].values)\n        vec_len_test = len(test_features[col].values)\n        raw_vec = train_features[col].values.reshape(vec_len, 1)\n        transformer.fit(raw_vec)\n\n        train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n        test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n\n\n    def seed_everything(seed_value):\n        random.seed(seed_value)\n        np.random.seed(seed_value)\n        torch.manual_seed(seed_value)\n\n        if torch.cuda.is_available(): \n            torch.cuda.manual_seed(seed_value)\n            torch.cuda.manual_seed_all(seed_value)\n            torch.backends.cudnn.deterministic = True\n            torch.backends.cudnn.benchmark = False\n\n    seed_everything(42)\n\n    n_comp = 600  #<--Update\n    # pca_g = PCA(n_components=n_comp, random_state=42)\n    # data = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n    # gpca= (pca_g.fit(data[GENES]))\n\n    gpca = load_pickle('gpca.pkl')\n    train2= (gpca.transform(train_features[GENES]))\n    test2 = (gpca.transform(test_features[GENES]))\n\n    train_gpca = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n    test_gpca = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n    # drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n    train_features = pd.concat((train_features, train_gpca), axis=1)\n    test_features = pd.concat((test_features, test_gpca), axis=1)\n\n    # dump(gpca, open('gpca.pkl', 'wb'))\n\n\n    #CELLS\n    n_comp = 50  #<--Update\n\n    # pca_c = PCA(n_components=n_comp, random_state=42)\n    # data = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n    # cpca= (pca_c.fit(data[CELLS]))\n    cpca = load_pickle('cpca.pkl')\n    train2= (cpca.transform(train_features[CELLS]))\n    test2 = (cpca.transform(test_features[CELLS]))\n\n    train_cpca = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n    test_cpca = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n    # drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n    train_features = pd.concat((train_features, train_cpca), axis=1)\n    test_features = pd.concat((test_features, test_cpca), axis=1)\n\n    # dump(cpca, open('cpca.pkl', 'wb'))\n\n    from sklearn.feature_selection import VarianceThreshold\n\n    c_n = [f for f in list(train_features.columns) if f not in ['sig_id', 'cp_type', 'cp_time', 'cp_dose']]\n    mask = (train_features[c_n].var() >= 0.85).values\n    tmp = train_features[c_n].loc[:, mask]\n    train_features = pd.concat([train_features[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)\n    tmp = test_features[c_n].loc[:, mask]\n    test_features = pd.concat([test_features[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)\n\n\n    from sklearn.cluster import KMeans\n    def fe_cluster_genes(train, test, n_clusters_g = 22, SEED = 42):\n\n        features_g = GENES\n        #features_c = CELLS\n\n        def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n            train_ = train[features].copy()\n            test_ = test[features].copy()\n    #         data = pd.concat([train_, test_], axis = 0)\n    #         kmeans_genes = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n    #         dump(kmeans_genes, open('kmeans_genes.pkl', 'wb'))\n\n            kmeans_genes = load_pickle('kmeans_genes.pkl')\n            train[f'clusters_{kind}'] = kmeans_genes.predict(train_.values)\n            test[f'clusters_{kind}'] = kmeans_genes.predict(test_.values)\n            train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n            test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n            return train, test\n\n        train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n       # train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n        return train, test\n\n    train_features2 ,test_features2=fe_cluster_genes(train_features2,test_features2)\n\n    def fe_cluster_cells(train, test, n_clusters_c = 4, SEED = 42):\n\n        #features_g = GENES\n        features_c = CELLS\n\n        def create_cluster(train, test, features, kind = 'c', n_clusters = n_clusters_c):\n            train_ = train[features].copy()\n            test_ = test[features].copy()\n    #         data = pd.concat([train_, test_], axis = 0)\n    #         kmeans_cells = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n    #         dump(kmeans_cells, open('kmeans_cells.pkl', 'wb'))\n            kmeans_cells = load_pickle('kmeans_cells.pkl')\n            train[f'clusters_{kind}'] = kmeans_cells.predict(train_.values)\n            test[f'clusters_{kind}'] = kmeans_cells.predict(test_.values)\n            train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n            test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n            return train, test\n\n       # train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n        train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n        return train, test\n\n    train_features2 ,test_features2=fe_cluster_cells(train_features2,test_features2)\n\n    train_pca=pd.concat((train_gpca,train_cpca),axis=1)\n    test_pca=pd.concat((test_gpca,test_cpca),axis=1)\n\n    def fe_cluster_pca(train, test,n_clusters=5,SEED = 42):\n    #         data=pd.concat([train,test],axis=0)\n    #         kmeans_pca = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n    #         dump(kmeans_pca, open('kmeans_pca.pkl', 'wb'))\n            kmeans_pca = load_pickle('kmeans_pca.pkl')\n            train[f'clusters_pca'] = kmeans_pca.predict(train.values)\n            test[f'clusters_pca'] = kmeans_pca.predict(test.values)\n            train = pd.get_dummies(train, columns = [f'clusters_pca'])\n            test = pd.get_dummies(test, columns = [f'clusters_pca'])\n            return train, test\n    train_cluster_pca ,test_cluster_pca = fe_cluster_pca(train_pca,test_pca)\n\n    train_cluster_pca = train_cluster_pca.iloc[:,650:]\n    test_cluster_pca = test_cluster_pca.iloc[:,650:]\n\n    train_features_cluster=train_features2.iloc[:,876:]\n    test_features_cluster=test_features2.iloc[:,876:]\n\n\n    gsquarecols=['g-574','g-211','g-216','g-0','g-255','g-577','g-153','g-389','g-60','g-370','g-248','g-167','g-203','g-177','g-301','g-332','g-517','g-6','g-744','g-224','g-162','g-3','g-736','g-486','g-283','g-22','g-359','g-361','g-440','g-335','g-106','g-307','g-745','g-146','g-416','g-298','g-666','g-91','g-17','g-549','g-145','g-157','g-768','g-568','g-396']\n\n    def fe_stats(train, test):\n\n        features_g = GENES\n        features_c = CELLS\n\n        for df in train, test:\n            df['g_sum'] = df[features_g].sum(axis = 1)\n            df['g_mean'] = df[features_g].mean(axis = 1)\n            df['g_std'] = df[features_g].std(axis = 1)\n            df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n            df['g_skew'] = df[features_g].skew(axis = 1)\n            df['c_sum'] = df[features_c].sum(axis = 1)\n            df['c_mean'] = df[features_c].mean(axis = 1)\n            df['c_std'] = df[features_c].std(axis = 1)\n            df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n            df['c_skew'] = df[features_c].skew(axis = 1)\n            df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n            df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n            df['gc_std'] = df[features_g + features_c].std(axis = 1)\n            df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n            df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n\n            df['c52_c42'] = df['c-52'] * df['c-42']\n            df['c13_c73'] = df['c-13'] * df['c-73']\n            df['c26_c13'] = df['c-23'] * df['c-13']\n            df['c33_c6'] = df['c-33'] * df['c-6']\n            df['c11_c55'] = df['c-11'] * df['c-55']\n            df['c38_c63'] = df['c-38'] * df['c-63']\n            df['c38_c94'] = df['c-38'] * df['c-94']\n            df['c13_c94'] = df['c-13'] * df['c-94']\n            df['c4_c52'] = df['c-4'] * df['c-52']\n            df['c4_c42'] = df['c-4'] * df['c-42']\n            df['c13_c38'] = df['c-13'] * df['c-38']\n            df['c55_c2'] = df['c-55'] * df['c-2']\n            df['c55_c4'] = df['c-55'] * df['c-4']\n            df['c4_c13'] = df['c-4'] * df['c-13']\n            df['c82_c42'] = df['c-82'] * df['c-42']\n            df['c66_c42'] = df['c-66'] * df['c-42']\n            df['c6_c38'] = df['c-6'] * df['c-38']\n            df['c2_c13'] = df['c-2'] * df['c-13']\n            df['c62_c42'] = df['c-62'] * df['c-42']\n            df['c90_c55'] = df['c-90'] * df['c-55']\n\n\n            for feature in features_c:\n                 df[f'{feature}_squared'] = df[feature] ** 2     \n\n            for feature in gsquarecols:\n                df[f'{feature}_squared'] = df[feature] ** 2        \n\n        return train, test\n\n    train_features2,test_features2=fe_stats(train_features2,test_features2)\n\n    train_features_stats=train_features2.iloc[:,902:]\n    test_features_stats=test_features2.iloc[:,902:]\n\n    train_features = pd.concat((train_features, train_features_cluster,train_cluster_pca,train_features_stats), axis=1)\n    test_features = pd.concat((test_features, test_features_cluster,test_cluster_pca,test_features_stats), axis=1)\n\n    train = train_features.merge(train_targets_nonscored, on='sig_id')\n    train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n    test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\n    target = train[train_targets_nonscored.columns]\n\n    train = train.drop('cp_type', axis=1)\n    test = test.drop('cp_type', axis=1)\n\n    target_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n\n    train = pd.get_dummies(train, columns=['cp_time','cp_dose'])\n    test_ = pd.get_dummies(test, columns=['cp_time','cp_dose'])\n\n    feature_cols = [c for c in train.columns if c not in target_cols]\n    feature_cols = [c for c in feature_cols if c not in ['sig_id']]\n\n    class MoADataset:\n        def __init__(self, features, targets):\n            self.features = features\n            self.targets = targets\n\n        def __len__(self):\n            return (self.features.shape[0])\n\n        def __getitem__(self, idx):\n            dct = {\n                'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n                'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n            }\n            return dct\n\n    class TestDataset:\n        def __init__(self, features):\n            self.features = features\n\n        def __len__(self):\n            return (self.features.shape[0])\n\n        def __getitem__(self, idx):\n            dct = {\n                'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n            }\n            return dct\n\n\n    def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n        model.train()\n        final_loss = 0\n\n        for data in dataloader:\n            optimizer.zero_grad()\n            inputs, targets = data['x'].to(device), data['y'].to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            final_loss += loss.item()\n\n        final_loss /= len(dataloader)\n\n        return final_loss\n\n\n    def valid_fn(model, loss_fn, dataloader, device):\n        model.eval()\n        final_loss = 0\n        valid_preds = []\n\n        for data in dataloader:\n            inputs, targets = data['x'].to(device), data['y'].to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n\n            final_loss += loss.item()\n            valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n        final_loss /= len(dataloader)\n        valid_preds = np.concatenate(valid_preds)\n\n        return final_loss, valid_preds\n\n    def inference_fn(model, dataloader, device):\n        model.eval()\n        preds = []\n\n        for data in dataloader:\n            inputs = data['x'].to(device)\n\n            with torch.no_grad():\n                outputs = model(inputs)\n\n            preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n        preds = np.concatenate(preds)\n\n        return preds\n\n    import torch\n    from torch.nn.modules.loss import _WeightedLoss\n    import torch.nn.functional as F\n\n    class SmoothBCEwLogits(_WeightedLoss):\n        def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n            super().__init__(weight=weight, reduction=reduction)\n            self.smoothing = smoothing\n            self.weight = weight\n            self.reduction = reduction\n\n        @staticmethod\n        def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n            assert 0 <= smoothing < 1\n            with torch.no_grad():\n                targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n            return targets\n\n        def forward(self, inputs, targets):\n            targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n                self.smoothing)\n            loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n            if  self.reduction == 'sum':\n                loss = loss.sum()\n            elif  self.reduction == 'mean':\n                loss = loss.mean()\n\n            return loss\n\n    class Model(nn.Module):\n        def __init__(self, num_features, num_targets, hidden_size):\n            super(Model, self).__init__()\n            self.batch_norm1 = nn.BatchNorm1d(num_features)\n            self.dropout1 = nn.Dropout(0.2)\n            self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n\n            self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n            self.dropout2 = nn.Dropout(0.2)\n            self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n\n            self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n            self.dropout3 = nn.Dropout(0.2)\n            self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n\n        def forward(self, x):\n            x = self.batch_norm1(x)\n            x = self.dropout1(x)\n            x = F.leaky_relu(self.dense1(x), 1e-3)\n\n            x = self.batch_norm2(x)\n            x = self.dropout2(x)\n            x = F.relu(self.dense2(x))\n\n            x = self.batch_norm3(x)\n            x = self.dropout3(x)\n            x = self.dense3(x)\n\n            return x\n\n    # HyperParameters\n\n    DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n    EPOCHS = 26\n    BATCH_SIZE = 256\n    LEARNING_RATE = 6e-4\n    WEIGHT_DECAY = 1e-5\n    NFOLDS = 7\n    EARLY_STOPPING_STEPS = 10\n    EARLY_STOP = True\n\n    num_features=len(feature_cols)\n    num_targets=len(target_cols)\n    hidden_size=2048\n\n    def run_training(fold, seed):\n\n        seed_everything(seed)\n\n        mskf = MultilabelStratifiedKFold(n_splits=7,random_state=seed)\n        for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n             train.loc[v_idx, 'kfold'] = int(f)\n        train['kfold'] = train['kfold'].astype(int)\n\n        trn_idx = train[train['kfold'] != fold].index\n        val_idx = train[train['kfold'] == fold].index\n\n        train_df = train[train['kfold'] != fold].reset_index(drop=True)\n        valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n\n        x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n        x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n\n        train_dataset = MoADataset(x_train, y_train)\n        valid_dataset = MoADataset(x_valid, y_valid)\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n        model = Model(\n            num_features=num_features,\n            num_targets=num_targets,\n            hidden_size=hidden_size,\n\n        )\n        model.load_state_dict(torch.load(INFERENCE_FOLDER + f\"SEED{seed}_FOLD{fold}_nonscored.pth\"))\n        model.to(DEVICE)\n\n        loss_fn = nn.BCEWithLogitsLoss()\n\n        oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n        \n        if not OMIT_VALIDATION:\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n            print(f\"SEED: {seed} ,FOLD: {fold}, valid_loss: {valid_loss}\")\n            oof[val_idx] = valid_preds\n\n\n        #--------------------- PREDICTION---------------------\n        x_test = test_[feature_cols].values\n        testdataset = TestDataset(x_test)\n        testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n\n\n\n        predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n        predictions = inference_fn(model, testloader, DEVICE)\n\n        return oof, predictions\n\n\n    def run_k_fold(NFOLDS, seed):\n        oof = np.zeros((len(train), len(target_cols)))\n        predictions = np.zeros((len(test), len(target_cols)))\n\n        for fold in range(NFOLDS):\n            oof_, pred_ = run_training(fold, seed)\n\n            predictions += pred_ / NFOLDS\n            oof += oof_\n\n        return oof, predictions\n\n\n    # Averaging on multiple SEEDS\n\n    SEED = [0,1,2,3,4,5,6]  #<-- Update\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n\n    for seed in SEED:\n\n        oof_, predictions_ = run_k_fold(NFOLDS, seed)\n        oof += oof_ / len(SEED)\n        predictions += predictions_ / len(SEED)\n\n    train[target_cols] = oof\n    test_[target_cols] = predictions\n\n\n    train = train.merge(train_targets_scored, on='sig_id')\n    target = train[train_targets_scored.columns]\n    target_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n\n    feature_cols = [c for c in train.columns if c not in target_cols]\n    feature_cols = [c for c in feature_cols if c not in ['sig_id','kfold']]\n\n    DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n    EPOCHS = 26\n    BATCH_SIZE = 256\n    LEARNING_RATE = 6e-4\n    WEIGHT_DECAY = 1e-5\n    NFOLDS = 7\n    EARLY_STOPPING_STEPS = 10\n    EARLY_STOP = True\n\n    num_features=len(feature_cols)\n    num_targets=len(target_cols)\n    hidden_size=2048\n\n    def run_training(fold, seed):\n\n        seed_everything(seed)\n\n        mskf = MultilabelStratifiedKFold(n_splits=7,random_state=seed)\n        for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n             train.loc[v_idx, 'kfold'] = int(f)\n        train['kfold'] = train['kfold'].astype(int)\n\n        trn_idx = train[train['kfold'] != fold].index\n        val_idx = train[train['kfold'] == fold].index\n\n        train_df = train[train['kfold'] != fold].reset_index(drop=True)\n        valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n\n        x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n        x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n\n        train_dataset = MoADataset(x_train, y_train)\n        valid_dataset = MoADataset(x_valid, y_valid)\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n        model = Model(\n            num_features=num_features,\n            num_targets=num_targets,\n            hidden_size=hidden_size,\n\n        )\n\n        model.load_state_dict(torch.load(INFERENCE_FOLDER + f\"SEED{seed}_FOLD{fold}_scored.pth\"))\n        model.to(DEVICE)\n\n        loss_fn = nn.BCEWithLogitsLoss()\n\n        oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    \n        if not OMIT_VALIDATION:\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n            print(f\"SEED: {seed} ,FOLD: {fold}, valid_loss: {valid_loss}\")\n            oof[val_idx] = valid_preds\n\n\n        #--------------------- PREDICTION---------------------\n        x_test = test_[feature_cols].values\n        testdataset = TestDataset(x_test)\n        testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n\n        predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n        predictions = inference_fn(model, testloader, DEVICE)\n\n        return oof, predictions\n\n    def run_k_fold(NFOLDS, seed):\n        oof = np.zeros((len(train), len(target_cols)))\n        predictions = np.zeros((len(test), len(target_cols)))\n\n        for fold in range(NFOLDS):\n            oof_, pred_ = run_training(fold, seed)\n\n            predictions += pred_ / NFOLDS\n            oof += oof_\n\n        return oof, predictions\n\n\n    # Averaging on multiple SEEDS\n\n    SEED = [0,1,2,3,4,5,6]  #<-- Update\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n\n    for seed in SEED:\n\n        oof_, predictions_ = run_k_fold(NFOLDS, seed)\n        oof += oof_ / len(SEED)\n        predictions += predictions_ / len(SEED)\n\n    train[target_cols] = oof\n    test[target_cols] = predictions\n\n    valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n    y_true = train_targets_scored[target_cols].values\n    y_pred = valid_results[target_cols].values\n\n    cv_score = 0\n    roc_score = 0\n\n    for i in range(len(target_cols)):\n        cv_score += log_loss(y_true[:, i], y_pred[:, i])\n        roc_score +=roc_auc_score(y_true[:, i], y_pred[:, i], average='micro')\n\n    print(\"CV log_loss: \", cv_score / y_pred.shape[1])\n    print(\"Overall AUC: \", roc_score / y_pred.shape[1])\n\n\n    oof_pretrain= valid_results[target_cols]\n    oof_pretrain.to_csv('off_pretrain_last.csv', index=False)\n\n    public_id = list(df['sig_id'].values)\n    test_id = list(test_features['sig_id'].values)\n    private_id = list(set(test_id)-set(public_id))\n    df_submit = pd.DataFrame(index = public_id+private_id, columns=target_cols)\n    df_submit.index.name = 'sig_id'\n    df_submit[:] = 0\n    df_submit.loc[test.sig_id,:] = test[target_cols].values\n    df_submit.loc[test_features[test_features.cp_type=='ctl_vehicle'].sig_id]= 0\n#     df_submit.to_csv('submission.csv',index=True)\n    return df_submit.reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Notebook 7 inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"def notebook_7_inference():\n    EXP_NAME = 'first_of_threemodels_net_tuning'\n    OMIT_VALIDATION = True\n    \n    data_dir = '../input/lish-moa/'\n    os.listdir(data_dir)\n    \n    train_features = pd.read_csv(data_dir + 'train_features.csv')\n    train_targets_scored = pd.read_csv(data_dir + 'train_targets_scored.csv')\n    train_targets_nonscored = pd.read_csv(data_dir + 'train_targets_nonscored.csv')\n    train_drug = pd.read_csv(data_dir + 'train_drug.csv')\n    test_features = pd.read_csv(data_dir + 'test_features.csv')\n    sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n\n    print('train_features: {}'.format(train_features.shape))\n    print('train_targets_scored: {}'.format(train_targets_scored.shape))\n    print('train_targets_nonscored: {}'.format(train_targets_nonscored.shape))\n    print('train_drug: {}'.format(train_drug.shape))\n    print('test_features: {}'.format(test_features.shape))\n    print('sample_submission: {}'.format(sample_submission.shape))\n    \n    train_features2=train_features.copy()\n    test_features2=test_features.copy()\n    \n    GENES = [col for col in train_features.columns if col.startswith('g-')]\n    CELLS = [col for col in train_features.columns if col.startswith('c-')]\n    \n    qt = QuantileTransformer(n_quantiles=100,random_state=42,output_distribution='normal')\n    qt.fit(pd.DataFrame(train_features[GENES+CELLS]))\n    train_features[GENES+CELLS] = qt.transform(train_features[GENES+CELLS])\n    test_features[GENES+CELLS] = qt.transform(test_features[GENES+CELLS])\n    \n    def seed_everything(seed=42):\n        random.seed(seed)\n        os.environ['PYTHONHASHSEED'] = str(seed)\n        np.random.seed(seed)\n\n    seed_everything(seed=42)\n    \n    # GENES\n    n_comp = 600  #<--Update\n\n    gen_pca = PCA(n_components=n_comp, random_state=42).fit(train_features[GENES])\n    train2 = gen_pca.transform(train_features[GENES]); test2 = gen_pca.transform(test_features[GENES])\n\n    train_gpca = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n    test_gpca = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n    # drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n    train_features = pd.concat((train_features, train_gpca), axis=1)\n    test_features = pd.concat((test_features, test_gpca), axis=1)\n    \n    #CELLS\n    n_comp = 50  #<--Update\n\n    cell_pca = PCA(n_components=n_comp, random_state=42).fit(train_features[CELLS])\n    train2 = cell_pca.transform(train_features[CELLS]); test2 = cell_pca.transform(test_features[CELLS])\n\n    train_cpca = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n    test_cpca = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n    # drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n    train_features = pd.concat((train_features, train_cpca), axis=1)\n    test_features = pd.concat((test_features, test_cpca), axis=1)\n    \n\n    var_thresh = VarianceThreshold(0.85).fit(train_features.iloc[:,4:])  #<-- Update\n    #data = train_features.append(test_features)\n    #data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\n    train_features_transformed = var_thresh.transform(train_features.iloc[:,4:])#data_transformed[ : train_features.shape[0]]\n    test_features_transformed = var_thresh.transform(test_features.iloc[:,4:])#data_transformed[-test_features.shape[0] : ]\n\n\n    train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                                  columns=['sig_id','cp_type','cp_time','cp_dose'])\n\n    train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\n    test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                                 columns=['sig_id','cp_type','cp_time','cp_dose'])\n\n    test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n    \n    def fe_cluster(train, test, n_clusters_g = 22, n_clusters_c = 4, SEED = 42):\n    \n        features_g = GENES\n        features_c = CELLS\n\n        def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n            train_ = train[features].copy()\n            test_ = test[features].copy()\n            kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(train_)\n            train[f'clusters_{kind}'] = kmeans.predict(train_.values)\n            test[f'clusters_{kind}'] = kmeans.predict(test_.values)\n            train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n            test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n            return train, test\n\n        train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n        train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n        return train, test\n\n    train_features2 ,test_features2=fe_cluster(train_features2,test_features2)\n    \n    train_pca=pd.concat((train_gpca,train_cpca),axis=1)\n    test_pca=pd.concat((test_gpca,test_cpca),axis=1)\n    \n    def fe_cluster_pca(train, test,n_clusters=5,SEED = 42):\n            kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(train)\n            train[f'clusters_pca'] = kmeans.predict(train.values)\n            test[f'clusters_pca'] = kmeans.predict(test.values)\n            train = pd.get_dummies(train, columns = [f'clusters_pca'])\n            test = pd.get_dummies(test, columns = [f'clusters_pca'])\n            return train, test\n    train_cluster_pca ,test_cluster_pca = fe_cluster_pca(train_pca,test_pca)\n    \n    train_cluster_pca = train_cluster_pca.iloc[:,650:]\n    test_cluster_pca = test_cluster_pca.iloc[:,650:]\n    \n    train_features_cluster=train_features2.iloc[:,876:]\n    test_features_cluster=test_features2.iloc[:,876:]\n    \n    gsquarecols=['g-574','g-211','g-216','g-0','g-255','g-577','g-153','g-389','g-60','g-370','g-248','g-167','g-203','g-177','g-301','g-332','g-517','g-6','g-744','g-224','g-162','g-3','g-736','g-486','g-283','g-22','g-359','g-361','g-440','g-335','g-106','g-307','g-745','g-146','g-416','g-298','g-666','g-91','g-17','g-549','g-145','g-157','g-768','g-568','g-396']\n    \n    def fe_stats(train, test):\n    \n        features_g = GENES\n        features_c = CELLS\n\n        for df in train, test:\n            df['g_sum'] = df[features_g].sum(axis = 1)\n            df['g_mean'] = df[features_g].mean(axis = 1)\n            df['g_std'] = df[features_g].std(axis = 1)\n            df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n            df['g_skew'] = df[features_g].skew(axis = 1)\n            df['c_sum'] = df[features_c].sum(axis = 1)\n            df['c_mean'] = df[features_c].mean(axis = 1)\n            df['c_std'] = df[features_c].std(axis = 1)\n            df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n            df['c_skew'] = df[features_c].skew(axis = 1)\n            df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n            df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n            df['gc_std'] = df[features_g + features_c].std(axis = 1)\n            df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n            df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n\n            df['c52_c42'] = df['c-52'] * df['c-42']\n            df['c13_c73'] = df['c-13'] * df['c-73']\n            df['c26_c13'] = df['c-23'] * df['c-13']\n            df['c33_c6'] = df['c-33'] * df['c-6']\n            df['c11_c55'] = df['c-11'] * df['c-55']\n            df['c38_c63'] = df['c-38'] * df['c-63']\n            df['c38_c94'] = df['c-38'] * df['c-94']\n            df['c13_c94'] = df['c-13'] * df['c-94']\n            df['c4_c52'] = df['c-4'] * df['c-52']\n            df['c4_c42'] = df['c-4'] * df['c-42']\n            df['c13_c38'] = df['c-13'] * df['c-38']\n            df['c55_c2'] = df['c-55'] * df['c-2']\n            df['c55_c4'] = df['c-55'] * df['c-4']\n            df['c4_c13'] = df['c-4'] * df['c-13']\n            df['c82_c42'] = df['c-82'] * df['c-42']\n            df['c66_c42'] = df['c-66'] * df['c-42']\n            df['c6_c38'] = df['c-6'] * df['c-38']\n            df['c2_c13'] = df['c-2'] * df['c-13']\n            df['c62_c42'] = df['c-62'] * df['c-42']\n            df['c90_c55'] = df['c-90'] * df['c-55']\n\n\n            for feature in features_c:\n                 df[f'{feature}_squared'] = df[feature] ** 2     \n\n            for feature in gsquarecols:\n                df[f'{feature}_squared'] = df[feature] ** 2        \n\n        return train, test\n\n    train_features2,test_features2=fe_stats(train_features2,test_features2)\n    \n    train_features_stats=train_features2.iloc[:,902:]\n    test_features_stats=test_features2.iloc[:,902:]\n\n    train_features = pd.concat((train_features, train_features_cluster,train_cluster_pca,train_features_stats), axis=1)\n    test_features = pd.concat((test_features, test_features_cluster,test_cluster_pca,test_features_stats), axis=1)\n    \n    train = train_features.merge(train_targets_scored, on='sig_id')\n    train = train.merge(train_targets_nonscored, on='sig_id')\n    train = train.merge(train_drug, on='sig_id')\n    train = train[train['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n    test = test_features[test_features['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n    \n    train = train.drop('cp_type', axis=1)\n    test = test.drop('cp_type', axis=1)\n    \n    target_cols = [x for x in train_targets_scored.columns if x != 'sig_id']\n    aux_target_cols = [x for x in train_targets_nonscored.columns if x != 'sig_id']\n    all_target_cols = target_cols + aux_target_cols\n\n    num_targets = len(target_cols)\n    num_aux_targets = len(aux_target_cols)\n    num_all_targets = len(all_target_cols)\n\n    print('num_targets: {}'.format(num_targets))\n    print('num_aux_targets: {}'.format(num_aux_targets))\n    print('num_all_targets: {}'.format(num_all_targets))\n    \n    def seed_everything(seed=42):\n        random.seed(seed)\n        os.environ['PYTHONHASHSEED'] = str(seed)\n        np.random.seed(seed)\n\n    seed_everything(seed=42)\n    \n    class MoADataset:\n        def __init__(self, features, targets):\n            self.features = features\n            self.targets = targets\n\n        def __len__(self):\n            return (self.features.shape[0])\n\n        def __getitem__(self, idx):\n            dct = {\n                'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n                'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)\n            }\n\n            return dct\n\n    class TestDataset:\n        def __init__(self, features):\n            self.features = features\n\n        def __len__(self):\n            return (self.features.shape[0])\n\n        def __getitem__(self, idx):\n            dct = {\n                'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n            }\n\n            return dct\n        \n\n    def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n        model.train()\n        final_loss = 0\n\n        for data in dataloader:\n            optimizer.zero_grad()\n            inputs, targets = data['x'].to(device), data['y'].to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            final_loss += loss.item()\n\n        final_loss /= len(dataloader)\n        return final_loss\n\n    def valid_fn(model, loss_fn, dataloader, device):\n        model.eval()\n        final_loss = 0\n        valid_preds = []\n\n        for data in dataloader:\n            inputs, targets = data['x'].to(device), data['y'].to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n\n            final_loss += loss.item()\n            valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n        final_loss /= len(dataloader)\n        valid_preds = np.concatenate(valid_preds)\n        return final_loss, valid_preds\n\n    def inference_fn(model, dataloader, device):\n        model.eval()\n        preds = []\n\n        for data in dataloader:\n            inputs = data['x'].to(device)\n\n            with torch.no_grad():\n                outputs = model(inputs)\n\n            preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n        preds = np.concatenate(preds)\n        return preds\n\n    class SmoothBCEwLogits(_WeightedLoss):\n        def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n            super().__init__(weight=weight, reduction=reduction)\n            self.smoothing = smoothing\n            self.weight = weight\n            self.reduction = reduction\n\n        @staticmethod\n        def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n            assert 0 <= smoothing < 1\n\n            with torch.no_grad():\n                targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n\n            return targets\n\n        def forward(self, inputs, targets):\n            targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n                self.smoothing)\n            loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n            if  self.reduction == 'sum':\n                loss = loss.sum()\n            elif  self.reduction == 'mean':\n                loss = loss.mean()\n\n            return loss\n\n    class Model(nn.Module):\n        def __init__(self, num_features, num_targets):\n            super(Model, self).__init__()\n            self.hidden_size = [1024, 1024, 1024, 512]\n            self.dropout_value = [0.4, 0.25, 0.25, 0.2]\n\n            self.batch_norm1 = nn.BatchNorm1d(num_features)\n            self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n\n            self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n            self.dropout2 = nn.Dropout(self.dropout_value[0])\n            self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n\n            self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n            self.dropout3 = nn.Dropout(self.dropout_value[1])\n            self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n\n            self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n            self.dropout4 = nn.Dropout(self.dropout_value[2])\n            self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n\n            self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n            self.dropout5 = nn.Dropout(self.dropout_value[3])\n            self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n\n        def forward(self, x):\n            x = self.batch_norm1(x)\n            x = F.leaky_relu(self.dense1(x))\n\n            x = self.batch_norm2(x)\n            x = self.dropout2(x)\n            x = F.leaky_relu(self.dense2(x))\n\n            x = self.batch_norm3(x)\n            x = self.dropout3(x)\n            x = F.leaky_relu(self.dense3(x))\n\n            x = self.batch_norm4(x)\n            x = self.dropout4(x)\n            x = F.leaky_relu(self.dense4(x))\n\n            x = self.batch_norm5(x)\n            x = self.dropout5(x)\n            x = self.dense5(x)\n            return x\n\n    class LabelSmoothingLoss(nn.Module):\n        def __init__(self, classes, smoothing=0.0, dim=-1):\n            super(LabelSmoothingLoss, self).__init__()\n            self.confidence = 1.0 - smoothing\n            self.smoothing = smoothing\n            self.cls = classes\n            self.dim = dim\n\n        def forward(self, pred, target):\n            pred = pred.log_softmax(dim=self.dim)\n\n            with torch.no_grad():\n                true_dist = torch.zeros_like(pred)\n                true_dist.fill_(self.smoothing / (self.cls - 1))\n                true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n\n            return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))   \n        \n    class FineTuneScheduler:\n        def __init__(self, epochs):\n            self.epochs = epochs\n            self.epochs_per_step = 0\n            self.frozen_layers = []\n\n        def copy_without_top(self, model, num_features, num_targets, num_targets_new):\n            self.frozen_layers = []\n\n            model_new = Model(num_features, num_targets)\n            model_new.load_state_dict(model.state_dict())\n\n            # Freeze all weights\n            for name, param in model_new.named_parameters():\n                layer_index = name.split('.')[0][-1]\n\n                if layer_index == 5:\n                    continue\n\n                param.requires_grad = False\n\n                # Save frozen layer names\n                if layer_index not in self.frozen_layers:\n                    self.frozen_layers.append(layer_index)\n\n            self.epochs_per_step = self.epochs // len(self.frozen_layers)\n\n            # Replace the top layers with another ones\n            model_new.batch_norm5 = nn.BatchNorm1d(model_new.hidden_size[3])\n            model_new.dropout5 = nn.Dropout(model_new.dropout_value[3])\n            model_new.dense5 = nn.utils.weight_norm(nn.Linear(model_new.hidden_size[-1], num_targets_new))\n            model_new.to(DEVICE)\n            return model_new\n\n        def step(self, epoch, model):\n            if len(self.frozen_layers) == 0:\n                return\n\n            if epoch % self.epochs_per_step == 0:\n                last_frozen_index = self.frozen_layers[-1]\n\n                # Unfreeze parameters of the last frozen layer\n                for name, param in model.named_parameters():\n                    layer_index = name.split('.')[0][-1]\n\n                    if layer_index == last_frozen_index:\n                        param.requires_grad = True\n\n                del self.frozen_layers[-1]  # Remove the last layer as unfrozen\n                \n    def process_data(data):\n        data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n        return data\n    \n    feature_cols = [c for c in process_data(train).columns if c not in all_target_cols]\n    feature_cols = [c for c in feature_cols if c not in ['kfold', 'sig_id', 'drug_id']]\n    num_features = len(feature_cols)\n    \n    # HyperParameters\n\n    DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n    EPOCHS = 24\n    BATCH_SIZE = 128\n\n    WEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 3e-6}\n    MAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 3e-3}\n    DIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2}\n    PCT_START = 0.1\n    \n    # Show model architecture\n    model = Model(num_features, num_all_targets)\n    model\n    \n    def make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH):\n        vc = train.drug_id.value_counts()\n        vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n        vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n\n        for seed_id in range(SEEDS):\n            kfold_col = 'kfold_{}'.format(seed_id)\n\n            # STRATIFY DRUGS 18X OR LESS\n            dct1 = {}\n            dct2 = {}\n\n            skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n            tmp = train.groupby('drug_id')[target_cols].mean().loc[vc1]\n\n            for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n                dd = {k: fold for k in tmp.index[idxV].values}\n                dct1.update(dd)\n\n            # STRATIFY DRUGS MORE THAN 18X\n            skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n            tmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\n\n            for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n                dd = {k: fold for k in tmp.sig_id[idxV].values}\n                dct2.update(dd)\n\n            # ASSIGN FOLDS\n            train[kfold_col] = train.drug_id.map(dct1)\n            train.loc[train[kfold_col].isna(), kfold_col] = train.loc[train[kfold_col].isna(), 'sig_id'].map(dct2)\n            train[kfold_col] = train[kfold_col].astype('int8')\n\n        return train\n\n    SEEDS = 7\n    NFOLDS = 7\n    DRUG_THRESH = 18\n\n    train = make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH)\n    train.head()\n    \n    def run_training(fold_id, seed_id):\n        seed_everything(seed_id)\n\n        save_model_path = f\"../input/lish-moa-final-models/{EXP_NAME}/{EXP_NAME}/seed_{seed_id}\"\n\n        if not OMIT_VALIDATION:\n            train_ = process_data(train)\n        test_ = process_data(test)\n\n        kfold_col = f'kfold_{seed_id}'\n        if not OMIT_VALIDATION:\n            val_idx = train_[train_[kfold_col] == fold_id].index\n\n        if not OMIT_VALIDATION:\n            valid_df = train_[train_[kfold_col] == fold_id].reset_index(drop=True)\n\n\n\n        # Load the fine-tuned model with the best loss\n        model = Model(num_features, num_targets)\n        model.load_state_dict(torch.load(f\"{save_model_path}/SCORED_ONLY_FOLD{fold_id}_.pth\"))\n        model.to(DEVICE)\n\n        #--------------------- PREDICTION---------------------\n        oof = np.zeros((len(train), num_targets))\n        # Predict oof\n        if not OMIT_VALIDATION:\n            x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values   \n            valid_dataset = MoADataset(x_valid, y_valid)\n            validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n            loss_fn = nn.BCEWithLogitsLoss()\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n            oof[val_idx] = valid_preds\n\n            print(f\"loss : {valid_loss}\")\n\n        x_test = test_[feature_cols].values\n        testdataset = TestDataset(x_test)\n        testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n\n        predictions = np.zeros((len(test_), num_targets))\n        predictions = inference_fn(model, testloader, DEVICE)\n        return oof, predictions\n    \n    def run_k_fold(NFOLDS, seed_id):\n        oof = np.zeros((len(train), len(target_cols)))\n        predictions = np.zeros((len(test), len(target_cols)))\n\n        for fold_id in range(NFOLDS):\n            oof_, pred_ = run_training(fold_id, seed_id)\n            predictions += pred_ / NFOLDS\n            oof += oof_\n\n        return oof, predictions\n    \n    # Averaging on multiple SEEDS\n\n    SEED = [0,1,2,3,4,5,6] #<-- Update\n    result_oof = np.zeros((len(train), len(target_cols)))\n    result_predictions = np.zeros((len(test), len(target_cols)))\n\n    for seed in SEED:\n\n        fold_oof, fold_predictions = run_k_fold(NFOLDS, seed)\n        result_oof += fold_oof \n        result_predictions += fold_predictions \n\n    train[target_cols] =  ( result_oof / len(SEED) )\n    test[target_cols] = ( result_predictions / len(SEED) )\n    \n    if not OMIT_VALIDATION:\n        valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n        y_true = train_targets_scored[target_cols].values\n        y_pred = valid_results[target_cols].values\n\n        score = 0\n\n        for i in range(len(target_cols)):\n            score += log_loss(y_true[:, i], y_pred[:, i])\n\n        score = score / y_pred.shape[1]\n\n        print(\"CV log_loss: \", score)\n        \n    sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n    \n    return sub","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Notebook 8 inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"def notebook_8_inference():\n    EXP_NAME = 'first_of_threemodels_longlr_tail'\n    OMIT_VALIDATION = True\n    \n    data_dir = '../input/lish-moa/'\n    os.listdir(data_dir)\n    \n    train_features = pd.read_csv(data_dir + 'train_features.csv')\n    train_targets_scored = pd.read_csv(data_dir + 'train_targets_scored.csv')\n    train_targets_nonscored = pd.read_csv(data_dir + 'train_targets_nonscored.csv')\n    train_drug = pd.read_csv(data_dir + 'train_drug.csv')\n    test_features = pd.read_csv(data_dir + 'test_features.csv')\n    sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n\n    print('train_features: {}'.format(train_features.shape))\n    print('train_targets_scored: {}'.format(train_targets_scored.shape))\n    print('train_targets_nonscored: {}'.format(train_targets_nonscored.shape))\n    print('train_drug: {}'.format(train_drug.shape))\n    print('test_features: {}'.format(test_features.shape))\n    print('sample_submission: {}'.format(sample_submission.shape))\n    \n    train_features2=train_features.copy()\n    test_features2=test_features.copy()\n    \n    GENES = [col for col in train_features.columns if col.startswith('g-')]\n    CELLS = [col for col in train_features.columns if col.startswith('c-')]\n    \n    qt = QuantileTransformer(n_quantiles=100,random_state=42,output_distribution='normal')\n    qt.fit(pd.DataFrame(train_features[GENES+CELLS]))\n    train_features[GENES+CELLS] = qt.transform(train_features[GENES+CELLS])\n    test_features[GENES+CELLS] = qt.transform(test_features[GENES+CELLS])\n    \n    def seed_everything(seed=42):\n        random.seed(seed)\n        os.environ['PYTHONHASHSEED'] = str(seed)\n        np.random.seed(seed)\n\n    seed_everything(seed=42)\n    \n    # GENES\n    n_comp = 600  #<--Update\n\n    gen_pca = PCA(n_components=n_comp, random_state=42).fit(train_features[GENES])\n    train2 = gen_pca.transform(train_features[GENES]); test2 = gen_pca.transform(test_features[GENES])\n\n    train_gpca = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n    test_gpca = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n    # drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n    train_features = pd.concat((train_features, train_gpca), axis=1)\n    test_features = pd.concat((test_features, test_gpca), axis=1)\n    \n    #CELLS\n    n_comp = 50  #<--Update\n\n    cell_pca = PCA(n_components=n_comp, random_state=42).fit(train_features[CELLS])\n    train2 = cell_pca.transform(train_features[CELLS]); test2 = cell_pca.transform(test_features[CELLS])\n\n    train_cpca = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n    test_cpca = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n    # drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n    train_features = pd.concat((train_features, train_cpca), axis=1)\n    test_features = pd.concat((test_features, test_cpca), axis=1)\n    \n\n    var_thresh = VarianceThreshold(0.85).fit(train_features.iloc[:,4:])  #<-- Update\n    #data = train_features.append(test_features)\n    #data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\n    train_features_transformed = var_thresh.transform(train_features.iloc[:,4:])#data_transformed[ : train_features.shape[0]]\n    test_features_transformed = var_thresh.transform(test_features.iloc[:,4:])#data_transformed[-test_features.shape[0] : ]\n\n\n    train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                                  columns=['sig_id','cp_type','cp_time','cp_dose'])\n\n    train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\n    test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                                 columns=['sig_id','cp_type','cp_time','cp_dose'])\n\n    test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n    \n    def fe_cluster(train, test, n_clusters_g = 22, n_clusters_c = 4, SEED = 42):\n    \n        features_g = GENES\n        features_c = CELLS\n\n        def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n            train_ = train[features].copy()\n            test_ = test[features].copy()\n            kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(train_)\n            train[f'clusters_{kind}'] = kmeans.predict(train_.values)\n            test[f'clusters_{kind}'] = kmeans.predict(test_.values)\n            train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n            test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n            return train, test\n\n        train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n        train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n        return train, test\n\n    train_features2 ,test_features2=fe_cluster(train_features2,test_features2)\n    \n    train_pca=pd.concat((train_gpca,train_cpca),axis=1)\n    test_pca=pd.concat((test_gpca,test_cpca),axis=1)\n    \n    def fe_cluster_pca(train, test,n_clusters=5,SEED = 42):\n            kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(train)\n            train[f'clusters_pca'] = kmeans.predict(train.values)\n            test[f'clusters_pca'] = kmeans.predict(test.values)\n            train = pd.get_dummies(train, columns = [f'clusters_pca'])\n            test = pd.get_dummies(test, columns = [f'clusters_pca'])\n            return train, test\n    train_cluster_pca ,test_cluster_pca = fe_cluster_pca(train_pca,test_pca)\n    \n    train_cluster_pca = train_cluster_pca.iloc[:,650:]\n    test_cluster_pca = test_cluster_pca.iloc[:,650:]\n    \n    train_features_cluster=train_features2.iloc[:,876:]\n    test_features_cluster=test_features2.iloc[:,876:]\n    \n    gsquarecols=['g-574','g-211','g-216','g-0','g-255','g-577','g-153','g-389','g-60','g-370','g-248','g-167','g-203','g-177','g-301','g-332','g-517','g-6','g-744','g-224','g-162','g-3','g-736','g-486','g-283','g-22','g-359','g-361','g-440','g-335','g-106','g-307','g-745','g-146','g-416','g-298','g-666','g-91','g-17','g-549','g-145','g-157','g-768','g-568','g-396']\n    \n    def fe_stats(train, test):\n    \n        features_g = GENES\n        features_c = CELLS\n\n        for df in train, test:\n            df['g_sum'] = df[features_g].sum(axis = 1)\n            df['g_mean'] = df[features_g].mean(axis = 1)\n            df['g_std'] = df[features_g].std(axis = 1)\n            df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n            df['g_skew'] = df[features_g].skew(axis = 1)\n            df['c_sum'] = df[features_c].sum(axis = 1)\n            df['c_mean'] = df[features_c].mean(axis = 1)\n            df['c_std'] = df[features_c].std(axis = 1)\n            df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n            df['c_skew'] = df[features_c].skew(axis = 1)\n            df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n            df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n            df['gc_std'] = df[features_g + features_c].std(axis = 1)\n            df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n            df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n\n            df['c52_c42'] = df['c-52'] * df['c-42']\n            df['c13_c73'] = df['c-13'] * df['c-73']\n            df['c26_c13'] = df['c-23'] * df['c-13']\n            df['c33_c6'] = df['c-33'] * df['c-6']\n            df['c11_c55'] = df['c-11'] * df['c-55']\n            df['c38_c63'] = df['c-38'] * df['c-63']\n            df['c38_c94'] = df['c-38'] * df['c-94']\n            df['c13_c94'] = df['c-13'] * df['c-94']\n            df['c4_c52'] = df['c-4'] * df['c-52']\n            df['c4_c42'] = df['c-4'] * df['c-42']\n            df['c13_c38'] = df['c-13'] * df['c-38']\n            df['c55_c2'] = df['c-55'] * df['c-2']\n            df['c55_c4'] = df['c-55'] * df['c-4']\n            df['c4_c13'] = df['c-4'] * df['c-13']\n            df['c82_c42'] = df['c-82'] * df['c-42']\n            df['c66_c42'] = df['c-66'] * df['c-42']\n            df['c6_c38'] = df['c-6'] * df['c-38']\n            df['c2_c13'] = df['c-2'] * df['c-13']\n            df['c62_c42'] = df['c-62'] * df['c-42']\n            df['c90_c55'] = df['c-90'] * df['c-55']\n\n\n            for feature in features_c:\n                 df[f'{feature}_squared'] = df[feature] ** 2     \n\n            for feature in gsquarecols:\n                df[f'{feature}_squared'] = df[feature] ** 2        \n\n        return train, test\n\n    train_features2,test_features2=fe_stats(train_features2,test_features2)\n    \n    train_features_stats=train_features2.iloc[:,902:]\n    test_features_stats=test_features2.iloc[:,902:]\n\n    train_features = pd.concat((train_features, train_features_cluster,train_cluster_pca,train_features_stats), axis=1)\n    test_features = pd.concat((test_features, test_features_cluster,test_cluster_pca,test_features_stats), axis=1)\n    \n    train = train_features.merge(train_targets_scored, on='sig_id')\n    train = train.merge(train_targets_nonscored, on='sig_id')\n    train = train.merge(train_drug, on='sig_id')\n    train = train[train['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n    test = test_features[test_features['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n    \n    train = train.drop('cp_type', axis=1)\n    test = test.drop('cp_type', axis=1)\n    \n    target_cols = [x for x in train_targets_scored.columns if x != 'sig_id']\n    aux_target_cols = [x for x in train_targets_nonscored.columns if x != 'sig_id']\n    all_target_cols = target_cols + aux_target_cols\n\n    num_targets = len(target_cols)\n    num_aux_targets = len(aux_target_cols)\n    num_all_targets = len(all_target_cols)\n\n    print('num_targets: {}'.format(num_targets))\n    print('num_aux_targets: {}'.format(num_aux_targets))\n    print('num_all_targets: {}'.format(num_all_targets))\n    \n    def seed_everything(seed=42):\n        random.seed(seed)\n        os.environ['PYTHONHASHSEED'] = str(seed)\n        np.random.seed(seed)\n\n    seed_everything(seed=42)\n    \n    class MoADataset:\n        def __init__(self, features, targets):\n            self.features = features\n            self.targets = targets\n\n        def __len__(self):\n            return (self.features.shape[0])\n\n        def __getitem__(self, idx):\n            dct = {\n                'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n                'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)\n            }\n\n            return dct\n\n    class TestDataset:\n        def __init__(self, features):\n            self.features = features\n\n        def __len__(self):\n            return (self.features.shape[0])\n\n        def __getitem__(self, idx):\n            dct = {\n                'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n            }\n\n            return dct\n        \n\n    def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n        model.train()\n        final_loss = 0\n\n        for data in dataloader:\n            optimizer.zero_grad()\n            inputs, targets = data['x'].to(device), data['y'].to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            final_loss += loss.item()\n\n        final_loss /= len(dataloader)\n        return final_loss\n\n    def valid_fn(model, loss_fn, dataloader, device):\n        model.eval()\n        final_loss = 0\n        valid_preds = []\n\n        for data in dataloader:\n            inputs, targets = data['x'].to(device), data['y'].to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n\n            final_loss += loss.item()\n            valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n        final_loss /= len(dataloader)\n        valid_preds = np.concatenate(valid_preds)\n        return final_loss, valid_preds\n\n    def inference_fn(model, dataloader, device):\n        model.eval()\n        preds = []\n\n        for data in dataloader:\n            inputs = data['x'].to(device)\n\n            with torch.no_grad():\n                outputs = model(inputs)\n\n            preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n        preds = np.concatenate(preds)\n        return preds\n\n    class SmoothBCEwLogits(_WeightedLoss):\n        def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n            super().__init__(weight=weight, reduction=reduction)\n            self.smoothing = smoothing\n            self.weight = weight\n            self.reduction = reduction\n\n        @staticmethod\n        def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n            assert 0 <= smoothing < 1\n\n            with torch.no_grad():\n                targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n\n            return targets\n\n        def forward(self, inputs, targets):\n            targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n                self.smoothing)\n            loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n            if  self.reduction == 'sum':\n                loss = loss.sum()\n            elif  self.reduction == 'mean':\n                loss = loss.mean()\n\n            return loss\n\n    class Model(nn.Module):\n        def __init__(self, num_features, num_targets):\n            super(Model, self).__init__()\n            self.hidden_size = [1500, 1250, 1000, 750]\n            self.dropout_value = [0.5, 0.35, 0.3, 0.25]\n\n            self.batch_norm1 = nn.BatchNorm1d(num_features)\n            self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n\n            self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n            self.dropout2 = nn.Dropout(self.dropout_value[0])\n            self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n\n            self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n            self.dropout3 = nn.Dropout(self.dropout_value[1])\n            self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n\n            self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n            self.dropout4 = nn.Dropout(self.dropout_value[2])\n            self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n\n            self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n            self.dropout5 = nn.Dropout(self.dropout_value[3])\n            self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n\n        def forward(self, x):\n            x = self.batch_norm1(x)\n            x = F.leaky_relu(self.dense1(x))\n\n            x = self.batch_norm2(x)\n            x = self.dropout2(x)\n            x = F.leaky_relu(self.dense2(x))\n\n            x = self.batch_norm3(x)\n            x = self.dropout3(x)\n            x = F.leaky_relu(self.dense3(x))\n\n            x = self.batch_norm4(x)\n            x = self.dropout4(x)\n            x = F.leaky_relu(self.dense4(x))\n\n            x = self.batch_norm5(x)\n            x = self.dropout5(x)\n            x = self.dense5(x)\n            return x\n\n    class LabelSmoothingLoss(nn.Module):\n        def __init__(self, classes, smoothing=0.0, dim=-1):\n            super(LabelSmoothingLoss, self).__init__()\n            self.confidence = 1.0 - smoothing\n            self.smoothing = smoothing\n            self.cls = classes\n            self.dim = dim\n\n        def forward(self, pred, target):\n            pred = pred.log_softmax(dim=self.dim)\n\n            with torch.no_grad():\n                true_dist = torch.zeros_like(pred)\n                true_dist.fill_(self.smoothing / (self.cls - 1))\n                true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n\n            return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))   \n        \n    class FineTuneScheduler:\n        def __init__(self, epochs):\n            self.epochs = epochs\n            self.epochs_per_step = 0\n            self.frozen_layers = []\n\n        def copy_without_top(self, model, num_features, num_targets, num_targets_new):\n            self.frozen_layers = []\n\n            model_new = Model(num_features, num_targets)\n            model_new.load_state_dict(model.state_dict())\n\n            # Freeze all weights\n            for name, param in model_new.named_parameters():\n                layer_index = name.split('.')[0][-1]\n\n                if layer_index == 5:\n                    continue\n\n                param.requires_grad = False\n\n                # Save frozen layer names\n                if layer_index not in self.frozen_layers:\n                    self.frozen_layers.append(layer_index)\n\n            self.epochs_per_step = self.epochs // len(self.frozen_layers)\n\n            # Replace the top layers with another ones\n            model_new.batch_norm5 = nn.BatchNorm1d(model_new.hidden_size[3])\n            model_new.dropout5 = nn.Dropout(model_new.dropout_value[3])\n            model_new.dense5 = nn.utils.weight_norm(nn.Linear(model_new.hidden_size[-1], num_targets_new))\n            model_new.to(DEVICE)\n            return model_new\n\n        def step(self, epoch, model):\n            if len(self.frozen_layers) == 0:\n                return\n\n            if epoch % self.epochs_per_step == 0:\n                last_frozen_index = self.frozen_layers[-1]\n\n                # Unfreeze parameters of the last frozen layer\n                for name, param in model.named_parameters():\n                    layer_index = name.split('.')[0][-1]\n\n                    if layer_index == last_frozen_index:\n                        param.requires_grad = True\n\n                del self.frozen_layers[-1]  # Remove the last layer as unfrozen\n                \n    def process_data(data):\n        data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n        return data\n    \n    feature_cols = [c for c in process_data(train).columns if c not in all_target_cols]\n    feature_cols = [c for c in feature_cols if c not in ['kfold', 'sig_id', 'drug_id']]\n    num_features = len(feature_cols)\n    \n    # HyperParameters\n\n    DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n    EPOCHS = 24\n    BATCH_SIZE = 128\n\n    WEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 3e-6}\n    MAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 3e-3}\n    DIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2}\n    PCT_START = 0.1\n    \n    # Show model architecture\n    model = Model(num_features, num_all_targets)\n    model\n    \n    def make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH):\n        vc = train.drug_id.value_counts()\n        vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n        vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n\n        for seed_id in range(SEEDS):\n            kfold_col = 'kfold_{}'.format(seed_id)\n\n            # STRATIFY DRUGS 18X OR LESS\n            dct1 = {}\n            dct2 = {}\n\n            skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n            tmp = train.groupby('drug_id')[target_cols].mean().loc[vc1]\n\n            for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n                dd = {k: fold for k in tmp.index[idxV].values}\n                dct1.update(dd)\n\n            # STRATIFY DRUGS MORE THAN 18X\n            skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n            tmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\n\n            for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n                dd = {k: fold for k in tmp.sig_id[idxV].values}\n                dct2.update(dd)\n\n            # ASSIGN FOLDS\n            train[kfold_col] = train.drug_id.map(dct1)\n            train.loc[train[kfold_col].isna(), kfold_col] = train.loc[train[kfold_col].isna(), 'sig_id'].map(dct2)\n            train[kfold_col] = train[kfold_col].astype('int8')\n\n        return train\n\n    SEEDS = 7\n    NFOLDS = 7\n    DRUG_THRESH = 18\n\n    train = make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH)\n    train.head()\n    \n    def run_training(fold_id, seed_id):\n        seed_everything(seed_id)\n\n        save_model_path = f\"../input/lish-moa-final-models/{EXP_NAME}/{EXP_NAME}/seed_{seed_id}\"\n\n        if not OMIT_VALIDATION:\n            train_ = process_data(train)\n        test_ = process_data(test)\n\n        kfold_col = f'kfold_{seed_id}'\n        if not OMIT_VALIDATION:\n            val_idx = train_[train_[kfold_col] == fold_id].index\n\n        if not OMIT_VALIDATION:\n            valid_df = train_[train_[kfold_col] == fold_id].reset_index(drop=True)\n\n\n\n        # Load the fine-tuned model with the best loss\n        model = Model(num_features, num_targets)\n        model.load_state_dict(torch.load(f\"{save_model_path}/SCORED_ONLY_FOLD{fold_id}_.pth\"))\n        model.to(DEVICE)\n\n        #--------------------- PREDICTION---------------------\n        oof = np.zeros((len(train), num_targets))\n        # Predict oof\n        if not OMIT_VALIDATION:\n            x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values   \n            valid_dataset = MoADataset(x_valid, y_valid)\n            validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n            loss_fn = nn.BCEWithLogitsLoss()\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n            oof[val_idx] = valid_preds\n\n            print(f\"loss : {valid_loss}\")\n\n        x_test = test_[feature_cols].values\n        testdataset = TestDataset(x_test)\n        testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n\n        predictions = np.zeros((len(test_), num_targets))\n        predictions = inference_fn(model, testloader, DEVICE)\n        return oof, predictions\n    \n    def run_k_fold(NFOLDS, seed_id):\n        oof = np.zeros((len(train), len(target_cols)))\n        predictions = np.zeros((len(test), len(target_cols)))\n\n        for fold_id in range(NFOLDS):\n            oof_, pred_ = run_training(fold_id, seed_id)\n            predictions += pred_ / NFOLDS\n            oof += oof_\n\n        return oof, predictions\n    \n    # Averaging on multiple SEEDS\n\n    SEED = [0,1,2,3,4,5,6] #<-- Update\n    result_oof = np.zeros((len(train), len(target_cols)))\n    result_predictions = np.zeros((len(test), len(target_cols)))\n\n    for seed in SEED:\n\n        fold_oof, fold_predictions = run_k_fold(NFOLDS, seed)\n        result_oof += fold_oof \n        result_predictions += fold_predictions \n\n    train[target_cols] =  ( result_oof / len(SEED) )\n    test[target_cols] = ( result_predictions / len(SEED) )\n    \n    if not OMIT_VALIDATION:\n        valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n        y_true = train_targets_scored[target_cols].values\n        y_pred = valid_results[target_cols].values\n\n        score = 0\n\n        for i in range(len(target_cols)):\n            score += log_loss(y_true[:, i], y_pred[:, i])\n\n        score = score / y_pred.shape[1]\n\n        print(\"CV log_loss: \", score)\n        \n    sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n    \n    return sub","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Run and Blend"},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time()\nsub_1 = notebook_1_inference()\ntaken_time = time() - start_time\nprint(f\"Notebook 1 took : {round(taken_time / 60, 3)} minutes\")\n\nstart_time = time()\nsub_2 = notebook_2_inference()\ntaken_time = time() - start_time\nprint(f\"Notebook 2 took : {round(taken_time / 60, 3)} minutes\")\n\nstart_time = time()\nsub_3 = notebook_3_inference()\ntaken_time = time() - start_time\nprint(f\"Notebook 3 took : {round(taken_time / 60, 3)} minutes\")\n\nstart_time = time()\nsub_4 = notebook_4_inference()\ntaken_time = time() - start_time\nprint(f\"Notebook 4 took : {round(taken_time / 60, 3)} minutes\")\n\nstart_time = time()\nsub_5 = notebook_5_inference()\ntaken_time = time() - start_time\nprint(f\"Notebook 5 took : {round(taken_time / 60, 3)} minutes\")\n\nstart_time = time()\nsub_6 = notebook_6_inference()\ntaken_time = time() - start_time\nprint(f\"Notebook 6 took : {round(taken_time / 60, 3)} minutes\")\n\nstart_time = time()\nsub_7 = notebook_7_inference()\ntaken_time = time() - start_time\nprint(f\"Notebook 7 took : {round(taken_time / 60, 3)} minutes\")\n\nstart_time = time()\nsub_8 = notebook_8_inference()\ntaken_time = time() - start_time\nprint(f\"Notebook 8 took : {round(taken_time / 60, 3)} minutes\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def order_sub(sub) : \n    return sub.sort_values('sig_id').reset_index(drop=True)\n\nsub_1 = order_sub(sub_1)\nsub_2 = order_sub(sub_2)\nsub_3 = order_sub(sub_3)\nsub_4 = order_sub(sub_4)\nsub_5 = order_sub(sub_5)\nsub_6 = order_sub(sub_6)\nsub_7 = order_sub(sub_7)\nsub_8 = order_sub(sub_8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BLEND=sub_1.copy()\nBLEND.iloc[:,1:] = (sub_1.iloc[:,1:] + sub_2.iloc[:,1:] + sub_3.iloc[:,1:] + sub_4.iloc[:,1:] + sub_5.iloc[:,1:] + sub_6.iloc[:,1:] + sub_7.iloc[:,1:] + sub_8.iloc[:,1:]) / 8\n\nBLEND","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BLEND.iloc[:,1:].max().max(), BLEND.iloc[:,1:].min().min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BLEND.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}