{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nimport statsmodels.api as sm\nimport pylab\nfrom scipy import stats\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import MinMaxScaler,PowerTransformer,StandardScaler\nfrom sklearn.decomposition import PCA\nimport tensorflow_addons as tfa\n# from sklearn.components import PCA\n# from keras.utils import to_categorical\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing the Dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_features=pd.read_csv(\"../input/lish-moa/train_features.csv\")\ntrain_targets=pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\ntest_features=pd.read_csv(\"../input/lish-moa/test_features.csv\")\nsubmission=pd.read_csv(\"../input/lish-moa/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exploring the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the Shape of Data\ntrain_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the Missing Values\ntrain_features.isnull().sum()/len(train_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing HeatMap\nplt.figure(figsize=(12,8))\nsns.heatmap(train_features.isnull(),cbar=False).set_title(\"Missing Values\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Feature Correlation\nplt.figure(figsize=(12,7))\nsns.heatmap(train_features[:10].corr())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking CP Type Distribution\ny=Counter(train_features.cp_type).most_common(train_features.cp_type.nunique())\ncp_type=[i[0] for i in y]\ncp_count=[i[1] for i in y]\nplt.figure(figsize=(12,7))\nsns.barplot(cp_count,cp_type).set_title(\"CP Type Distribution\")\nplt.xlabel(\"CP Count\")\nplt.ylabel(\"CP Type\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking CP Dose Distribution\ny=Counter(train_features.cp_dose).most_common(train_features.cp_dose.nunique())\ncp_dose=[i[0] for i in y]\ncp_count=[i[1] for i in y]\nplt.figure(figsize=(12,7))\nsns.barplot(cp_count,cp_dose).set_title(\"CP Dose Distribution\")\nplt.xlabel(\"Dose Count\")\nplt.ylabel(\"Dose Type\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.subplot(2,2,1)\nsns.distplot(train_features['g-0'])\nplt.subplot(2,2,2)\nsns.distplot(train_features['g-7'])\n\nplt.subplot(2,2,3)\nsns.distplot(train_features['c-0'])\nplt.subplot(2,2,4)\nsns.distplot(train_features['c-7'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Q-Q Plot\nplt.figure(figsize=(12,10))\nsm.qqplot(train_features['g-0'], line='45')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=Counter(train_features.cp_time).most_common(train_features.cp_time.nunique())\ncp_time=[i[0] for i in y]\ncp_count=[i[1] for i in y]\nplt.figure(figsize=(12,7))\nsns.barplot(cp_count,cp_time).set_title(\"CP Time Distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=Counter(train_targets[\"5-alpha_reductase_inhibitor\"]).most_common(train_targets[\"5-alpha_reductase_inhibitor\"].nunique())\nname=[i[0] for i in y]\ncount=[i[1] for i in y]\nplt.figure(figsize=(12,7))\nsns.barplot(name,count).set_title(\"5-alpha_reductase_inhibitor Distribution\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Featuring Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features=pd.get_dummies(train_features,columns=['cp_type'])\ntest_features=pd.get_dummies(test_features,columns=['cp_type'])\ncp_dose_enc={'D1':0,'D2':1}\ntrain_features['cp_dose']=train_features['cp_dose'].replace(cp_dose_enc)\n\ntest_features['cp_dose']=test_features['cp_dose'].replace(cp_dose_enc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping Columns\ntrain_features.drop(columns=['sig_id','cp_time','cp_type_ctl_vehicle'],inplace=True)\ntrain_targets.drop(columns=['sig_id'],inplace=True)\n\ntest_features.drop(columns=['sig_id','cp_type_ctl_vehicle','cp_time'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trt_cp=train_features[\"cp_type_trt_cp\"]\ntrain_features.drop(labels=['cp_type_trt_cp'], axis=1,inplace = True)\ntrain_features.insert(0, 'cp_type_trt_cp', trt_cp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trt_cp=test_features[\"cp_type_trt_cp\"]\ntest_features.drop(labels=['cp_type_trt_cp'], axis=1,inplace = True)\ntest_features.insert(0, 'cp_type_trt_cp', trt_cp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_features.iloc[:,0:2]=train_features.iloc[:,0:2].astype('category')\n# test_features.iloc[:,0:2]=test_features.iloc[:,0:2].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_features.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # # Scaling the Features\n# scaler=MinMaxScaler()\n# num_cols = list(train_features.select_dtypes(include=['float64']).columns)\n# train_features[num_cols] = scaler.fit_transform(train_features[num_cols])\n\n# test_features[num_cols]=scaler.transform(test_features[num_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Variance VS Components\n# pca = PCA().fit(train_features)\n# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n# plt.xlabel('number of components')\n# plt.ylabel('cumulative explained variance')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # PCA\n# pca=PCA(n_components=400)\n# train_components=pca.fit_transform(train_features)\n# test_components=pca.transform(test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train Test Split\n# x_train,x_val,y_train,y_val=train_test_split(train_components,train_targets,test_size=0.20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Transforming Skewed Data\n# pt=PowerTransformer()\n# pt.fit(x_train.iloc[:,2:])\n# x_train_pt=pd.DataFrame(pt.transform(x_train.iloc[:,2:]),columns=x_train.iloc[:,2:].columns).set_index(x_train.index)\n\n# x_val_pt=pd.DataFrame(pt.transform(x_val.iloc[:,2:]),columns=x_val.iloc[:,2:].columns).set_index(x_val.index)\n\n# test_features_pt=pd.DataFrame(pt.transform(test_features.iloc[:,2:]),columns=test_features.iloc[:,2:].columns).set_index(test_features.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# x_train.drop(columns=x_train.iloc[:,2:],inplace=True)\n# x_val.drop(columns=x_val.iloc[:,2:],inplace=True)\n\n# test_features.drop(columns=test_features.iloc[:,2:],inplace=True)\n\n# x_train=pd.concat([x_train,x_train_pt],axis=1)\n# x_val=pd.concat([x_val,x_val_pt],axis=1)\n\n# test_features=pd.concat([test_features,test_features_pt],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# x_train.skew(axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Building Model"},{"metadata":{},"cell_type":"markdown","source":"Model Constants"},{"metadata":{"trusted":true},"cell_type":"code","source":"LR=0.001\nBATCH_SIZE=16\nEPOCHS=30","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef build_model():\n    inp=tf.keras.layers.Input(shape=(train_features.shape[1],))\n\n    x=tfa.layers.WeightNormalization(tf.keras.layers.Dense(128,activation='relu'))(inp)\n    x=tf.keras.layers.BatchNormalization()(x)\n    x=tf.keras.layers.Dropout(0.5)(x)\n\n    x=tfa.layers.WeightNormalization(tf.keras.layers.Dense(64,activation='relu'))(inp)\n    x=tf.keras.layers.BatchNormalization()(x)\n    x=tf.keras.layers.Dropout(0.25)(x)\n\n    x=tfa.layers.WeightNormalization(tf.keras.layers.Dense(32,activation='relu'))(inp)\n    x=tf.keras.layers.BatchNormalization()(x)\n    x=tf.keras.layers.Dropout(0.25)(x)\n\n    out=tf.keras.layers.Dense(train_targets.shape[1],activation=\"sigmoid\")(x)\n\n    model=tf.keras.models.Model(inputs=inp,outputs=out)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"save_best=tf.keras.callbacks.ModelCheckpoint(filepath=\"best_model.h5\",monitor='val_loss',save_best_only=True)\nreduce_lr=tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",factor=0.6,min_lr_rate=0.000000001)\nearly_stopping=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Custom Metric\n# def log_loss(y_true,y_pred):\n# #     y_true = tf.cast(y_true,tf.float32)\n# #     y_pred = tf.cast(y_pred,tf.float32)\n#     loss = ((y_true*tf.math.log(y_pred))+((1.0-y_true)*tf.math.log(1.0-y_pred)))\n    \n#     return loss\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compiling the Model\nmodel=build_model()\nopt=tf.keras.optimizers.Adam(learning_rate=LR)\nmodel.compile(optimizer=opt,loss=\"binary_crossentropy\",metrics=[tf.metrics.AUC()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Summary\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features=train_features.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets=train_targets.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# K-fold Cross Validation model Training and Evaludation\n# Define per-fold score containers <-- these are new\nacc_per_fold = []\nloss_per_fold = []\nkfold=KFold(n_splits=10)\nfold_no = 1\n\nfor train_index, val_index in kfold.split(train_features, train_targets):\n\n    # Define the model architecture\n    model=build_model()\n    # Compiling the Model\n    \n    opt=tf.keras.optimizers.Adam(learning_rate=LR)\n    \n    model.compile(optimizer=opt,loss=\"binary_crossentropy\",metrics=[tf.keras.metrics.AUC()])\n\n    # Generate a print\n    print('------------------------------------------------------------------------')\n    print(f'Training for fold {fold_no} ...')\n\n  # Fit data to model\n    history = model.fit(train_features[train_index], train_targets[train_index],\n              batch_size=BATCH_SIZE,\n              epochs=EPOCHS,callbacks=[reduce_lr])\n\n\n    # Generate generalization metrics\n    scores = model.evaluate(train_features[val_index], train_targets[val_index], verbose=0)\n    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n    acc_per_fold.append(scores[1] * 100)\n    loss_per_fold.append(scores[0])\n\n    model.save(f'model_{fold_no}.h5')\n    \n    # Increase fold number\n    fold_no = fold_no + 1\n    \n    # == Provide average scores ==\n    print('------------------------------------------------------------------------')\n    print('Score per fold')\n    for i in range(0, len(acc_per_fold)):\n      print('------------------------------------------------------------------------')\n      print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n    print('------------------------------------------------------------------------')\n    print('Average scores for all folds:')\n    print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n    print(f'> Loss: {np.mean(loss_per_fold)}')\n    print('------------------------------------------------------------------------')\n\n\n# model.fit(x_train,y_train,batch_size=BATCH_SIZE,validation_data=(x_val,y_val),callbacks=[save_best,reduce_lr,early_stopping],epochs=EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the Model\nmodel=tf.keras.models.load_model(\"./model_10.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submitting the Predictions\nsig_id=submission['sig_id']\nsubmission.drop(columns=['sig_id'],inplace=True)\npredictions=pd.DataFrame(model.predict(test_features),columns=submission.columns)\npredictions.insert(0, 'sig_id', sig_id)\npredictions.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}