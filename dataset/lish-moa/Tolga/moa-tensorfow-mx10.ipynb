{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook demonstrates the importance of sampling. It makes use of the fast convergence trick I mentioned [here](https://www.kaggle.com/c/lish-moa/discussion/185161) and allows quick training of 30 models on 30 different sub-samples. This is in contrast to the use of small K in K-fold cross validation.\n\nCV (OOF) is available in V1, submission is made in V2.\n\n**If you find the notebook useful, please don't forget to upvote.**"},{"metadata":{},"cell_type":"markdown","source":"### Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/iter-strat","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom sklearn.preprocessing import OneHotEncoder, QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.models import load_model\n\nfrom tensorflow.keras.callbacks import (\n    ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nSEED = 1984\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"x_develop = pd.read_csv('../input/lish-moa/train_features.csv')\ny_develop = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\nx_test= pd.read_csv('../input/lish-moa/test_features.csv')\nsub = pd.read_csv('../input/lish-moa/sample_submission.csv')\n\nc_cols = x_develop.columns[x_develop.columns.str.startswith('c-')]\ng_cols = x_develop.columns[x_develop.columns.str.startswith('g-')]\ncont_cols = g_cols.to_list() + c_cols.to_list()\ntarget_cols = y_develop.columns[1:]  # All columns except sig_id\nN_TARGETS = len(target_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"#### Encode Categorical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_df(df):\n    if 'sig_id' in df.columns:\n        df.set_index('sig_id', inplace=True)\n    df['cp_type'] = (df['cp_type'] == 'trt_cp').astype(int)\n    df['cp_dose'] = (df['cp_dose'] == 'D2').astype(int)\n    \n    df = df.join(pd.get_dummies(df['cp_time'], drop_first=False, prefix='cp_time'))\n    df = df.drop('cp_time', axis=1)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_develop = preprocess_df(x_develop)\ny_develop = y_develop.set_index('sig_id')\nx_test = preprocess_df(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### PCA Decomposition"},{"metadata":{"trusted":true},"cell_type":"code","source":"# g-features\nn_comp = 28\npca_g = PCA(n_components=n_comp, random_state=SEED)\nx_develop_pca = pca_g.fit_transform(x_develop[g_cols])\nx_develop_pca = pd.DataFrame(x_develop_pca, columns=[f'pca_g-{i}' for i in range(n_comp)], index=x_develop.index)\nx_develop = pd.concat((x_develop, x_develop_pca), axis=1)\n\nx_test_pca = pca_g.transform(x_test[g_cols])\nx_test_pca = pd.DataFrame(x_test_pca, columns=[f'pca_g-{i}' for i in range(n_comp)], index=x_test.index)\nx_test = pd.concat((x_test, x_test_pca), axis=1)\n\ncont_cols += [f'pca_g-{i}' for i in range(n_comp)]\n\n# c-features\nn_comp = 5\npca_c = PCA(n_components=n_comp, random_state=SEED)\nx_develop_pca = pca_c.fit_transform(x_develop[c_cols])\nx_develop_pca = pd.DataFrame(x_develop_pca, columns=[f'pca_c-{i}' for i in range(n_comp)], index=x_develop.index)\nx_develop = pd.concat((x_develop, x_develop_pca), axis=1)\n\nx_test_pca = pca_c.transform(x_test[c_cols])\nx_test_pca = pd.DataFrame(x_test_pca, columns=[f'pca_c-{i}' for i in range(n_comp)], index=x_test.index)\nx_test = pd.concat((x_test, x_test_pca), axis=1)\n\ncont_cols += [f'pca_c-{i}' for i in range(n_comp)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Variance Threshold"},{"metadata":{"trusted":true},"cell_type":"code","source":"class VarianceThreshold:\n    def __init__(self, threshold):\n        self.threshold = threshold\n    def fit(self, df, cont_cols):\n        self.cont_cols = cont_cols\n        self.var = x_develop[cont_cols].var()\n        good_cols = self.var[self.var > self.threshold]\n        self.index = good_cols.index.to_list()\n        self.dropcols = [x for x in cont_cols if x not in self.var[self.var > self.threshold].index.to_list()]\n        self.validcols = [x for x in cont_cols if x in self.var[self.var > self.threshold].index.to_list()]\n    def transform(self, df):\n        return df.drop(self.dropcols, axis=1)\n    def fit_transform(self, df, cont_cols):\n        self.fit(df, cont_cols)\n        return self.transform(df), self.validcols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = 0.6 #x_develop[cont_cols].var().sort_values().quantile(0.01)\nprint('Variance Threshold:', threshold)\nVarThres = VarianceThreshold(threshold)\nx_develop, cont_cols = VarThres.fit_transform(x_develop, cont_cols)\nx_test = VarThres.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Transform Numerical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"qt = QuantileTransformer(output_distribution='normal')\nx_develop[cont_cols] = qt.fit_transform(x_develop[cont_cols])\nx_test[cont_cols] = qt.transform(x_test[cont_cols])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Group Data Into Folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_folds(df, fold_no, fold_type='mls_kfold', save=False):\n    \"\"\"\n    df: target dataframe\n    \"\"\"\n    if fold_type == 'kfold':\n        kf = KFold(n_splits=fold_no, shuffle=True, random_state=SEED)\n    elif fold_type == 'mls_kfold':\n        kf = MultilabelStratifiedKFold(n_splits=fold_no, random_state=SEED)\n        \n    df['Fold'] = -1\n    df.reset_index(inplace=True)\n    for fold, (t, v) in enumerate(kf.split(df, df)):\n        df.loc[v, 'Fold'] = fold\n    df.set_index('sig_id', inplace=True)\n    if save:\n        df.to_csv('y_develop.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_FOLDS = 30\nfold_type = 'mls_kfold'\ncreate_folds(y_develop, fold_no=N_FOLDS, fold_type=fold_type, save=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define Model Architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model():\n    def __init__(self, input_shape, output_bias=None):\n        self.input_shape = input_shape\n        self.output_bias = output_bias\n        \n    def create_model1(self):\n        if self.output_bias is not None:\n            self.output_bias = tf.keras.initializers.Constant(output_bias)\n\n        inputs = tf.keras.Input(shape=self.input_shape)\n        x = L.BatchNormalization()(inputs)\n        x = tfa.layers.WeightNormalization(L.Dense(800, activation='swish'))(x)\n        x = L.BatchNormalization()(x)\n        x = L.Dropout(0.4)(x)\n        x = tfa.layers.WeightNormalization(L.Dense(400, activation='swish'))(x)\n        x = L.BatchNormalization()(x)\n        x = L.Dropout(0.4)(x)\n        outputs = tfa.layers.WeightNormalization(L.Dense(N_TARGETS,\n                                                         activation='sigmoid',\n                                                         bias_initializer=self.output_bias\n                                                        )\n                                                 )(x)\n        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n        \n        metrics = [tf.keras.losses.BinaryCrossentropy(name='mean_loss')]\n        \n        OPTIMIZER = tfa.optimizers.Lookahead(\n            tfa.optimizers.AdamW(weight_decay=1e-5),\n            sync_period=5)\n        model.compile(optimizer=OPTIMIZER, loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0008), metrics=metrics)\n\n        return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_pred(x_df, models):\n    if isinstance(models[0], np.ndarray):\n        pred_test = np.repeat([models[0]], len(x_df), axis=0)\n    else:\n        for i in models:\n            if i == 0:\n                pred_test = models[i].predict(x_df)\n            else:\n                pred_test += models[i].predict(x_df)\n\n        pred_test = pred_test/len(models)\n    pred_test = pd.DataFrame(index=x_df.index, columns=target_cols, data=pred_test)\n    pred_test.loc[x_df['cp_type'] == 0] = 0\n    return pred_test\n\n\ndef oof_score(oof: dict):\n    return np.mean(list(oof.values())), np.std(list(oof.values()))\n\n\ndef combine_pred(pred):\n    for i in pred:\n        if i==0:\n            r = pred[i]\n        else:\n            r = np.append(r, pred[i], axis=0)\n    return r\n\n\ndef run_dump_baseline_cv(xtrain=x_develop, ytrain=y_develop, N_FOLDS=N_FOLDS, summary=True, debug=False):\n    models = {x: '' for x in range(N_FOLDS)}\n    results = {x: '' for x in range(N_FOLDS)}\n    oof_bp = {x: [] for x in range(N_FOLDS)}\n    oof_ap = {x: [] for x in range(N_FOLDS)}\n    pred_val_fold = {x: [] for x in range(N_FOLDS)}\n    \n    for foldno in np.sort(y_develop['Fold'].unique()):\n        x_train_fold = x_develop[y_develop['Fold'] != foldno]\n        y_train_fold = y_develop[y_develop['Fold'] != foldno].drop('Fold', axis=1)\n        x_val_fold = x_develop[y_develop['Fold'] == foldno]\n        y_val_fold = y_develop[y_develop['Fold'] == foldno].drop('Fold', axis=1)\n            \n        train_sample_size = len(y_train_fold)\n        val_sample_size = len(y_val_fold)\n        print(\" \")\n        print(f\"Fold-%d\" % (foldno))\n        print(\"Train sample size:\", train_sample_size, \", Validation sample size:\", val_sample_size)\n        \n        # Predict Validation probabilities\n        y_dev = y_develop.drop('Fold', axis=1)\n        models[foldno] = y_dev.mean(axis=0).to_numpy()\n        pred_val_fold[foldno] = np.repeat([models[foldno]], len(y_val_fold), axis=0)\n        \n        # Calculate OOF (Validation) Results\n        oof_bp[foldno] = tf.keras.losses.binary_crossentropy(y_val_fold, pred_val_fold[foldno]).numpy().mean()\n        print('Out-of-Fold Score: ', oof_bp[foldno])\n        \n        pred_val_fold[foldno][x_val_fold['cp_type'] == 0] == 0\n        oof_ap[foldno] = tf.keras.losses.binary_crossentropy(y_val_fold, pred_val_fold[foldno]).numpy().mean()\n        print('Out-of-Fold Score with post processing: ', oof_ap[foldno])\n        \n    pred_val_fold = combine_pred(pred_val_fold)\n        \n    print('\\n')\n    if summary:\n        print('Summary')\n        # Mean out of score before postprocessing\n        print('Mean OOF score: %f +/- %f' % (oof_score(oof_bp)))\n\n        # Mean out of score after postprocessing\n        print('Mean OOF score after postprocessing: %f +/- %f' % (oof_score(oof_ap)))\n            \n    return models, oof_ap, pred_val_fold\n\n\ndef run_cv(xtrain=x_develop, ytrain=y_develop, model=None, N_FOLDS=N_FOLDS, summary=True, debug=False):\n    histories = {x: '' for x in range(N_FOLDS)} \n    models = {x: '' for x in range(N_FOLDS)}\n    results = {x: '' for x in range(N_FOLDS)}\n    oof_bp = {x: [] for x in range(N_FOLDS)}\n    oof_ap = {x: [] for x in range(N_FOLDS)}\n    pred_val_fold = {x: [] for x in range(N_FOLDS)}\n    \n    for foldno in np.sort(y_develop['Fold'].unique()):\n        x_train_fold = x_develop[y_develop['Fold'] != foldno]\n        y_train_fold = y_develop[y_develop['Fold'] != foldno].drop('Fold', axis=1)\n        x_val_fold = x_develop[y_develop['Fold'] == foldno]\n        y_val_fold = y_develop[y_develop['Fold'] == foldno].drop('Fold', axis=1)\n            \n        train_sample_size = len(y_train_fold)\n        val_sample_size = len(y_val_fold)\n        print(\" \")\n        print(f\"Fold-%d\" % (foldno))\n        print(\"Train sample size:\", train_sample_size, \", Validation sample size:\", val_sample_size)\n\n        FEATURE_SIZE = x_train_fold.shape[-1]\n\n        # Train Data Pipeline\n        train_ds = tf.data.Dataset.from_tensor_slices((x_train_fold, y_train_fold))\n        # train_ds = train_ds.filter(lambda x, y: tf.reduce_any(y != np.zeros(206)))\n        train_ds = train_ds.shuffle(1024).batch(56)\n\n        # Validation Data Pipeline\n        val_ds = tf.data.Dataset.from_tensor_slices((x_val_fold, y_val_fold))\n        val_ds = val_ds.batch(val_sample_size)\n\n        # MODEL\n        models[foldno] = model.create_model1()\n        \n        # Train\n        cb_es = EarlyStopping(monitor='val_mean_loss', patience=5, restore_best_weights=True)\n        reduce_lr_loss = ReduceLROnPlateau(monitor='val_mean_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n        histories[foldno] = models[foldno].fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=[cb_es, reduce_lr_loss], verbose=1)\n        \n        # Predict Validation Probabilities\n        pred_val_fold[foldno] = models[foldno].predict(x_val_fold)\n        \n        # Calculate OOF (Validation) Results\n        oof_bp[foldno] = tf.keras.losses.binary_crossentropy(y_val_fold, pred_val_fold[foldno]).numpy().mean()\n        print('Out-of-Fold Score: ', oof_bp[foldno])\n        \n        pred_val_fold[foldno][x_val_fold['cp_type'] == 0] == 0\n        oof_ap[foldno] = tf.keras.losses.binary_crossentropy(y_val_fold, pred_val_fold[foldno]).numpy().mean()\n        print('Out-of-Fold Score with post processing: ', oof_ap[foldno])\n\n        # Save Model\n        if SAVE_MODEL:\n            models[foldno].save(f'weights-fold{foldno}.h5')\n            \n    pred_val_fold = combine_pred(pred_val_fold)\n    \n    print('\\n')\n    if summary:\n        print('Summary')\n        # Mean out of score before postprocessing\n        print('Mean OOF score: %f +/- %f' % (oof_score(oof_bp)))\n\n        # Mean out of score after postprocessing\n        print('Mean OOF score after postprocessing: %f +/- %f' % (oof_score(oof_ap)))\n    \n    return models, histories, oof_ap, pred_val_fold\n\n\ndef submit(res):\n    sub = res.reset_index()\n    sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RUN THE TRAINING\nEPOCHS = 45\nSAVE_MODEL = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if sub.shape[0] != 3982:\n    #models, oof_ap, pred_val_fold = run_dump_baseline_cv(x_develop, y_develop, N_FOLDS=N_FOLDS)\n    output_bias = -np.log(y_develop[y_develop.columns[:-1]].mean(axis=0).to_numpy())\n    models, histories, oof_ap, pred_val_fold = run_cv(x_develop, y_develop,\n                                                      model=Model(x_develop.shape[1], output_bias),\n                                                      N_FOLDS=N_FOLDS,\n                                                      debug=False)\n    pred_test = calc_pred(x_test, models) # This is for single model submission\n    submit(pred_test)\n    np.save('LBS.npy', pred_val_fold)\nelse:\n    sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame.from_dict(oof_ap, orient='index').hist()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}