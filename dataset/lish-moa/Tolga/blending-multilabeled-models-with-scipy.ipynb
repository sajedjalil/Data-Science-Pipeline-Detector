{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Blending Multilabeled Models with Scipy\n\nThis notebook demonstrates how to blend 3 different models with scipy to have an improved score.\n\nGiven the average cross entropy for N samples and M targets,\n\n\\begin{equation*}\nBCE(y, p) = -\\frac{1}{M} \\sum\\limits_{m=1}^{M} \\frac{1}{N} \\sum\\limits_{i=1}^{N} \\left( y_{i,m} ln(p_{i,m}) + (1-y_{i,m}) ln(1-p_{i,m})\\right),\n\\end{equation*}\n\nthe blending is based on the minimization of the following objective function:\n\n\\begin{equation*}\nOBJ = BCE(y, w_1 p_1 + w_2 p_2 + w_3 p_3).\n\\end{equation*}\n\nwhere, p1, p2, and p3 is the probability predictions (e.g. OOF predictions) from 3 different models; w1, w2, w3 are the respective blending weights; and y denotes the multilabel targets.\n\nThe optimization is subject to the following conditions:\n* The sum of the weights are required to be 1: w1 + w2 + w3 = 1\n* Each weight can only have values between 0 and 1:  0 < w$_i$ <1\n\n\nThe code below can be easily modified to blend more models.\n\n**If you find this notebook useful, please don't forget to upvote!**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom scipy.optimize import minimize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's generate some labels and 3 prediction sets.\nL = (np.random.rand(1800, 206) > 0.5).astype(int)\np1 = np.random.rand(1800, 206)\np2 = np.random.rand(1800, 206)\np3 = np.random.rand(1800, 206)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helpers\ndef individual_scores(label, preds: list):\n    for i, pred in enumerate(preds):\n        print('LogLoss for p%d: %.5f'  % (i, tf.keras.losses.binary_crossentropy(L, pred).numpy().mean()))\n\ndef show_weights(result):\n    print('\\n')\n    for i, w in enumerate(result.x):\n        print('Weight_%d: %f' % (i, w))\n\ndef sanity_check():\n    # All probabilities have to be between 0 and 1.\n    if ((blend_func(res.x) > 0) & (blend_func(res.x) < 1)).all():\n        print('\\nAll probabilities are between 0 and 1. \\n    Good to go!')\n    else:\n        print('\\nProbabilities are not between 0 and 1! \\nS    Something is wrong!')\n        \n# Optimization\ndef objective_func(x):\n    newp = blend_func(x)\n    return tf.keras.losses.binary_crossentropy(L, newp).numpy().mean()\n\ndef blend_func(x):\n    return p1*x[0] + p2*x[1] + p3*x[2]\n\n\nindividual_scores(L, [p1, p2, p3])\ninit_guess = [0.1,0.5,0.4]  # Initial guesses for the weights\nbounds = tuple((0,1) for x in init_guess)  # All weights will be between 0 and 1!\ncons = ({'type': 'eq', 'fun': lambda x:  1-x[0]-x[1]-x[2]})  # Constraints - the sum of weights will be 0!\n\nres = minimize(objective_func,\n               init_guess,\n               constraints=cons,\n               bounds=[(0, 1)] * 3,\n               method='SLSQP',\n               options={'disp': True,\n                        'maxiter': 100000}) # Minimize!\nprint('\\nBlend LogLoss: %.5f' % (res.fun))\nshow_weights(res)\nsanity_check()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"The final predictions can be calculated using the weights estimated above. A psuedo code to do this would be the following:\n\n`Submission_result = Model_1.predict(x_test) * w1 + Model_2.predict(x_test) * w2 + Model_3.predict(x_test) * w3`\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}