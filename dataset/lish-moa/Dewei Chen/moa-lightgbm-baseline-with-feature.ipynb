{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"fork from https://www.kaggle.com/ragnar123/moa-dnn-feature-engineering\nand https://www.kaggle.com/nroman/moa-lightgbm-206-models","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport multiprocessing\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport gc\nfrom time import time\nimport datetime\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import log_loss\n\nimport random\nimport os\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\n\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')\nsns.set()\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfiles = ['../input/lish-moa/test_features.csv', \n         '../input/lish-moa/train_targets_scored.csv',\n         '../input/lish-moa/train_features.csv',\n         '../input/lish-moa/train_targets_nonscored.csv',\n         '../input/lish-moa/sample_submission.csv']\n\ndef load_data(file):\n    return pd.read_csv(file)\n\nwith multiprocessing.Pool() as pool:\n    test, train_target, train, train_nonscored, sub = pool.map(load_data, files)\n    \n    \n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mapping_and_filter(train, train_targets, test):\n    cp_type = {'trt_cp': 0, 'ctl_vehicle': 1}\n    cp_dose = {'D1': 0, 'D2': 1}\n    for df in [train, test]:\n        df['cp_type'] = df['cp_type'].map(cp_type)\n        df['cp_dose'] = df['cp_dose'].map(cp_dose)\n    train_targets = train_targets[train['cp_type'] == 0].reset_index(drop = True)\n    train = train[train['cp_type'] == 0].reset_index(drop = True)\n    train_targets.drop(['sig_id'], inplace = True, axis = 1)\n    return train, train_targets, test\n\n# Function to scale our data\ndef scaling(train, test):\n    features = train.columns[2:]\n    scaler = RobustScaler()\n    scaler.fit(pd.concat([train[features], test[features]], axis = 0))\n    train[features] = scaler.transform(train[features])\n    test[features] = scaler.transform(test[features])\n    return train, test, features\n\n# Function to extract pca features\ndef fe_pca(train, test, n_components_g = 520, n_components_c = 46, SEED = 123):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    def create_pca(train, test, features, kind = 'g', n_components = n_components_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        pca = PCA(n_components = n_components,  random_state = SEED)\n        data = pca.fit_transform(data)\n        columns = [f'pca_{kind}{i + 1}' for i in range(n_components)]\n        data = pd.DataFrame(data, columns = columns)\n        train_ = data.iloc[:train.shape[0]]\n        test_ = data.iloc[train.shape[0]:].reset_index(drop = True)\n        train = pd.concat([train, train_], axis = 1)\n        test = pd.concat([test, test_], axis = 1)\n        return train, test\n    \n    train, test = create_pca(train, test, features_g, kind = 'g', n_components = n_components_g)\n    train, test = create_pca(train, test, features_c, kind = 'c', n_components = n_components_c)\n    return train, test\n\n# Function to extract common stats features\ndef fe_stats(train, test):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    for df in [train, test]:\n        df['g_sum'] = df[features_g].sum(axis = 1)\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n        df['g_skew'] = df[features_g].skew(axis = 1)\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n    return train, test\n\ndef c_squared(train, test):\n    \n    features_c = list(train.columns[776:876])\n    for df in [train, test]:\n        for feature in features_c:\n            df[f'{feature}_squared'] = df[feature] ** 2\n    return train, test\n\n# Function to calculate the mean log loss of the targets including clipping\ndef mean_log_loss(y_true, y_pred):\n    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    metrics = []\n    for target in range(206):\n        metrics.append(log_loss(y_true[:, target], y_pred[:, target]))\n    return np.mean(metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train = pd.read_csv('../input/lish-moa/train_features.csv')\n#train_target = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n#test = pd.read_csv('../input/lish-moa/test_features.csv')\n#sub = pd.read_csv('../input/lish-moa/sample_submission.csv')\ntrain, train_target, test = mapping_and_filter(train, train_target, test)\ntrain, test = fe_stats(train, test)\ntrain, test = c_squared(train, test)\ntrain, test = fe_pca(train, test, n_components_g = 520, n_components_c = 46, SEED = 123)\ntrain, test, features = scaling(train, test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = [col for col in train_target.columns if col != 'sig_id']\nprint('Number of different labels:', len(targets))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = [col for col in train.columns if col != 'sig_id']\nprint('Number of features:', len(features))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_target.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for feature in ['cp_type', 'cp_dose']:\n#    le = LabelEncoder()\n#    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n#    train[feature] = le.transform(list(train[feature].astype(str).values))\n#    test[feature] = le.transform(list(test[feature].astype(str).values))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train[features]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'num_leaves': 491,\n          'min_child_weight': 0.03,\n          'feature_fraction': 0.3,\n          'bagging_fraction': 0.4,\n          'min_data_in_leaf': 106,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.01,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'binary_logloss',\n          \"verbosity\": 0,\n          'reg_alpha': 0.4,\n          'reg_lambda': 0.6,\n          'random_state': 47\n         }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accumulative_loss = 0\nskf = StratifiedKFold(n_splits=3, random_state=47, shuffle=True)\n\n# 206 different models. One for each label\nfor model, target in enumerate(targets, 1):\n    y = train_target[target]\n    start_time = time()\n    preds = np.zeros(test.shape[0])\n    oof = np.zeros(X.shape[0])\n\n    for trn_idx, test_idx in skf.split(X, y):\n        trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n        val_data = lgb.Dataset(X.iloc[test_idx], label=y.iloc[test_idx])\n        clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=0, early_stopping_rounds=25)\n        oof[test_idx] = clf.predict(X.iloc[test_idx])\n        preds += clf.predict(test[features]) / skf.n_splits\n\n    sub[target] = preds\n    loss = log_loss(y, oof)\n    accumulative_loss += loss\n    print('[{}] Model: {} logloss: {:.3f}'.format(str(datetime.timedelta(seconds=time() - start_time))[:7], model, loss))\n\n    del preds, oof, start_time, y, loss\n    gc.collect();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Overall mean loss: {:.3f}'.format(accumulative_loss / 206))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def submission(test_pred):\n    sub.loc[:, train_target.columns] = test_pred\n    sub.loc[test['cp_type'] == 1, train_target.columns] = 0\n    sub.to_csv('submission.csv', index = False)\n    return sub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#submission(sub)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = submission(sub)\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}