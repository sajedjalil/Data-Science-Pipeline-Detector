{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Kaggle MoA - Hyperparameter Tuning Using Hyperopt"},{"metadata":{},"cell_type":"markdown","source":"Competition: https://www.kaggle.com/c/lish-moa\n\nThis is a sample script to showcase the use of hyperopt for hyperparameter tuning of a neural network.\n\nPlease go through the documentation of hyperopt here: http://hyperopt.github.io/hyperopt/"},{"metadata":{},"cell_type":"markdown","source":"Please have the following libraries in your path:\n\n\n1. iterativestrat - for multi-label stratified k-fold\n2. hyperopt - for hyperparameter tuning of NN\n3. tensorflow addons - for adding a weight normalization wrapper to the layers"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install hyperas\n!pip install iterative-stratification\n!pip install tensorflow-addons\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport os\nimport sys\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.metrics import log_loss, make_scorer\nfrom sklearn.decomposition import PCA\nfrom tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom tensorflow.keras.models import Sequential\nfrom functools import partial\nfrom hyperopt import fmin, tpe, hp, Trials, space_eval, STATUS_OK\nfrom hyperopt.pyll.base import scope\nimport hyperopt\nimport pickle\nfrom tensorflow_addons.layers import WeightNormalization\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nplt.style.use('dark_background')\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------------------------------------\n# book-keeping\n# --------------------------------------------------\n# Data location\n\ntrain_features_file = '/kaggle/input/lish-moa/train_features.csv'\ntrain_target_file = '/kaggle/input/lish-moa/train_targets_scored.csv'\ntest_features_file = '/kaggle/input/lish-moa/test_features.csv'\n\n# Some cfg - not use for hyperopt\nN_FOLDS = 5\nMAX_EVALS = 30\n\n# Some settings for the experiments\nrun_settings = dict()\nrun_settings[\"b_remove_control\"] = False\nrun_settings[\"b_hyperopt\"] = True\nrun_settings[\"b_run_submission\"] = True\n\n# Name of the hyperopt results log\nexperiment_name = \"experiment_name\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ====================================================\n# utils\n# ====================================================\n\n# ==========================\n# [data pre-processing]\n# ==========================\n\n\n# Load\ndef load_dataset():\n    df_train_feat = pd.read_csv(train_features_file)\n    df_train_target = pd.read_csv(train_target_file)\n    df_test_feat = pd.read_csv(test_features_file)\n\n    return df_train_feat, df_train_target, df_test_feat\n\n\n# Shuffle\ndef shuffle_data(data, targets):\n    # Reset index just to be sure\n    data.reset_index(drop=True, inplace=True)\n    targets.reset_index(drop=True, inplace=True)\n\n    # Grab col names to separate out later\n    cols_data = data.columns\n    cols_target = targets.columns\n\n    targets = targets.drop('sig_id', axis=1)\n\n    # Append targets tp df\n    df_agg = pd.concat([data, targets], axis=1)\n\n    # Shuffle\n    df_agg = df_agg.sample(frac=1).reset_index(drop=True)\n\n    # Separate\n    data = df_agg.loc[:, cols_data]\n    targets = df_agg.loc[:, cols_target]\n\n    return data, targets\n\n\ndef onehot(df, col):\n    # Encode\n    df_onehot = pd.get_dummies(df[col])\n\n    # Get col names\n    colnames = [\"{}_{}\".format(col, ii) for ii, tmp in enumerate(df_onehot.columns)]\n\n    # Change col names\n    df_onehot.columns = colnames\n\n    # Add to df\n    df = pd.concat([df, df_onehot], axis=1)\n\n    # Drop original columns\n    df = df.drop(col, axis=1)\n\n    return df\n\n\n# Do pre-processing\ndef prepare_data(df_train_feat, df_train_target, df_test_feat):\n    if run_settings[\"b_remove_control\"]:\n        mask = df_train_feat[\"cp_type\"] == \"trt_cp\"\n        df_train_feat = df_train_feat.loc[mask, :]\n        df_train_feat.reset_index(inplace=True, drop=True)\n        df_train_feat = df_train_feat.drop([\"cp_type\"], axis=1)\n\n        df_train_target = df_train_target.loc[mask, :]\n        df_train_target.reset_index(inplace=True, drop=True)\n\n        # Columns to one-hot encode\n        cols_onehot = ['cp_time', 'cp_dose']\n\n    else:\n        # Columns to one-hot encode\n        cols_onehot = ['cp_type', 'cp_time', 'cp_dose']\n\n    # One hot\n    for c in cols_onehot:\n        df_train_feat = onehot(df_train_feat, c)\n        df_test_feat = onehot(df_test_feat, c)\n\n    return df_train_feat, df_train_target, df_test_feat\n\n\n# Folds for CV\ndef get_folds(df_feat, df_target):\n    # Init\n    df_feat[\"k_fold\"] = -1\n\n    # Drop sig_id to use to stratified kfold\n    df_target = df_target.drop(\"sig_id\", axis=1)\n\n    # CV\n    cv_method = MultilabelStratifiedKFold(n_splits=5)\n\n    for fold_n, (train_idx_, val_idx_) in enumerate(cv_method.split(df_feat, df_target)):\n        df_feat.loc[val_idx_, \"k_fold\"] = fold_n\n\n    return df_feat\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ====================================================\n# [model]\n# ====================================================\nclass ModelLibrary:\n\n    def __init__(self):\n        pass\n    \n\n    @staticmethod\n    def model_1_hyper(n_feat, n_targets, params):\n\n        in_layer = Input(shape=n_feat)\n\n        z = Dense(params[\"n_hidden_1\"], activation=\"relu\")(in_layer)\n        z = BatchNormalization()(z)\n        z = Dropout(params[\"dropout\"])(z)\n\n        for i in range(params[\"num_block1\"]):\n            if params[\"weight_norm\"]:\n                z = WeightNormalization(Dense(params[\"n_hidden_1\"], activation=\"relu\"))(z)\n            else:\n                z = Dense(params[\"n_hidden_1\"], activation=\"relu\")(z)\n\n            z = BatchNormalization()(z)\n            z = Dropout(params[\"dropout\"])(z)\n\n        for i in range(params[\"num_block2\"]):\n            if params[\"weight_norm\"]:\n                z = WeightNormalization(Dense(params[\"n_hidden_2\"], activation=\"relu\"))(z)\n            else:\n                z = Dense(params[\"n_hidden_2\"], activation=\"relu\")(z)\n\n            z = BatchNormalization()(z)\n            z = Dropout(params[\"dropout\"])(z)\n\n        out_layer = Dense(n_targets, activation=\"sigmoid\")(z)\n\n        m = Model(inputs=[in_layer], outputs=[out_layer])\n\n        return m\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ====================================================\n# [training]\n# ====================================================\n\n# Train a single fold - with hyperopt\ndef train_fold_hyperopt(df_train_feat, df_train_target, fold_n, params):\n    # Grab training, val data for this fold\n    X_train = df_train_feat.loc[df_train_feat[\"k_fold\"] != fold_n, feat_names].values\n    y_train = df_train_target.loc[df_train_feat[\"k_fold\"] != fold_n, target_names].values\n\n    X_val = df_train_feat.loc[df_train_feat[\"k_fold\"] == fold_n, feat_names].values\n    y_val = df_train_target.loc[df_train_feat[\"k_fold\"] == fold_n, target_names].values\n\n    # train model for this fold\n    model_lib = ModelLibrary()\n    model = model_lib.model_1_hyper(n_feat=len(feat_names), n_targets=len(target_names), params=params)\n\n    model.compile(Adam(lr=params[\"learn_rate\"]), loss=BinaryCrossentropy())\n\n    # If using callbacks\n    if params[\"use_cb\"]:\n        cb_reducelr = ReduceLROnPlateau(\n            monitor='val_loss', factor=0.3, patience=5, verbose=0, mode='auto',\n            cooldown=0, min_lr=1e-5)\n\n        cb_es = EarlyStopping(\n            monitor='val_loss', min_delta=1e-5, patience=15, verbose=0, mode='auto',\n            baseline=None, restore_best_weights=True)\n\n        cb_list = [cb_reducelr, cb_es]\n    else:\n        cb_list = []\n\n    fit_log = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=params[\"batch_size\"],\n                        epochs=params[\"epochs\"], callbacks=cb_list)\n\n    return fit_log.history[\"val_loss\"][-1], model\n\n\n# Main optimization function - should just return the mean cv val loss - a single number!!\ndef optimize(params, df_train_feat, df_train_target, df_test_feat):\n    # Train\n    val_loss = []\n    test_pred = np.zeros((df_test_feat.shape[0], len(target_names)))\n    for fold_n in range(N_FOLDS):\n        fold_val_loss, model = train_fold_hyperopt(df_train_feat, df_train_target, fold_n, params)\n        val_loss.append(fold_val_loss)\n\n        # Predict on test set\n        test_pred += model.predict(df_test_feat[feat_names].values)\n\n    test_pred /= N_FOLDS\n\n    return {\"loss\": np.mean(val_loss), \"status\": STATUS_OK, \"test_pred\": test_pred}\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# ====================================================\n# main\n# ====================================================\n\n# ==========================\n# [data pre-processing]\n# ==========================\n# Load dataset\ndf_train_feat, df_train_target, df_test_feat = load_dataset()\ndf_train_feat.head()\n\n# Shuffle data\ndf_train_feat, df_train_target = shuffle_data(df_train_feat, df_train_target)\n\n# Print out some stuff\nprint(\"compound (cp_vehicle) vs control perturbation (ctrl_vehicle)\\n{}\".format(\n    df_train_feat['cp_type'].value_counts()))\nprint(\"\\n\\ncp_dose\\n{}\".format(df_train_feat['cp_dose'].value_counts()))\nprint(\"\\n\\ncp_time\\n{}\".format(df_train_feat['cp_time'].value_counts()))\n\n# Pre-process data (add dummies, etc)\ndf_train_feat, df_train_target, df_test_feat = prepare_data(df_train_feat, df_train_target, df_test_feat)\n\n\n# Get feature, target names\nfeat_names = list(df_train_feat.drop([\"sig_id\"], axis=1).columns)\ntarget_names = list(df_train_target.drop([\"sig_id\"], axis=1).columns)\n\n# ==========================\n# [training]\n# ==========================\n\n# --------------------------\n# [hyperopt]\n# --------------------------\nif run_settings[\"b_hyperopt\"]:\n\n    # Get folds\n    # Init\n    df_train_feat[\"k_fold\"] = -1\n\n    # Drop sig_id to use to stratified kfold\n    df_train_target = df_train_target.drop(\"sig_id\", axis=1)\n\n    cv_method = MultilabelStratifiedKFold(n_splits=N_FOLDS, random_state=20)\n\n    for fold_n, (train_idx_, val_idx_) in enumerate(cv_method.split(df_train_feat, df_train_target)):\n        df_train_feat.loc[val_idx_, \"k_fold\"] = fold_n\n\n    # Define param space to run experiments\n    param_search_space = {\n        \"n_hidden_1\": scope.int(hp.quniform(\"n_hidden_1\", 128, 2048, 64)),\n        \"n_hidden_2\": scope.int(hp.quniform(\"n_hidden_2\", 128, 2048, 64)),\n        \"num_block1\": hp.choice(\"num_block1\", [1, 2, 3, 4, 5]),\n        \"num_block2\": hp.choice(\"num_block2\", [1, 2, 3, 4, 5]),\n        \"learn_rate\": hp.loguniform(\"learn_rate\", -5, -2),\n        \"dropout\": hp.choice(\"dropout\", [0.1, 0.2, 0.3, 0.4, 0.5]),\n        \"batch_size\": scope.int(hp.quniform(\"batch_size\", 32, 256, 64)),\n        \"epochs\": hp.choice(\"epochs\", [1]),\n        \"use_cb\": hp.choice(\"use_cb\", [True, False]),\n        \"weight_norm\": hp.choice(\"weight_norm\", [True, False])\n    }\n    \n    # Create a partial function that takes leaves out params - hyperopt requires that\n    opt_func = partial(\n        optimize,\n        df_train_feat=df_train_feat,\n        df_train_target=df_train_target,\n        df_test_feat=df_test_feat\n    )\n\n    print(\"sample params:\\n\")\n    print(hyperopt.pyll.stochastic.sample(param_search_space))\n\n    # Init trials for logging\n    trials = Trials()\n\n    # Run optimization\n    gold_digger = fmin(\n        fn=opt_func,\n        space=param_search_space,\n        algo=tpe.suggest,\n        max_evals=MAX_EVALS,\n        trials=trials,\n        verbose=False\n    )\n\n    # Losses for trials\n    fnvals = [t['result']['loss'] for t in trials.trials]\n\n    # Best params\n    print(\"best params: {}\".format(space_eval(param_search_space, gold_digger)))\n    print(\"best loss: {}\".format(trials.best_trial['result']['loss']))\n\n    with open(experiment_name + \".pkl\", \"wb\") as output:\n        pickle.dump(trials, output, pickle.HIGHEST_PROTOCOL)\n        ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# ==========================\n# [test pred]\n# ==========================\nif run_settings[\"b_run_submission\"]:\n\n    if run_settings[\"b_hyperopt\"]:\n        # Get best pred from hyperopt\n        test_pred = trials.best_trial['result']['test_pred']\n\n    # If control is removed, you won't have dummy vars in test set\n    if run_settings[\"b_remove_control\"]:\n        test_pred[df_test_feat[\"cp_type\"] == \"ctl_vehicle\", :] = 0\n    else:\n        test_pred[df_test_feat['cp_type_0'] == 1, :] = 0\n\n    # Create submission\n    submission = pd.concat([df_test_feat[\"sig_id\"], pd.DataFrame(data=test_pred, columns=target_names)], axis=1)\n\n    submission.to_csv(\"submission.csv\", index=False)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}