{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Multi Input ResNet Model\n\n\n### Inspired by [this notebook](https://www.kaggle.com/demetrypascal/2heads-deep-resnets-pipeline-smoothing)\n### Transfer Learning as suggested [here](https://www.kaggle.com/c/lish-moa/discussion/191691#1058512)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\nfrom tensorflow.keras import layers,regularizers,Sequential,Model,backend,callbacks,optimizers,metrics,losses\nimport tensorflow as tf\nimport sys\nimport json\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import train data, drop sig_id, cp_type\n\ntrain_features = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\nnon_ctl_idx = train_features.loc[train_features['cp_type']!='ctl_vehicle'].index.to_list()\ntrain_features = train_features.drop(['sig_id','cp_type','cp_dose','cp_time'],axis=1)\ntrain_targets_scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ntrain_targets_scored = train_targets_scored.drop('sig_id',axis=1)\nlabels_train = train_targets_scored.values\n\n# Use non scored labels for transfer learning\n\ntrain_targets_nonscored = pd.read_csv('/kaggle/input/lish-moa/train_targets_nonscored.csv')\nnonscored_labels_train = train_targets_nonscored.drop('sig_id',axis=1).values\n\n# Drop Nonscored Labels with 0 MoAs\nnonscored_labels_train = nonscored_labels_train[:,nonscored_labels_train.sum(axis=0)!=0]\n\n# Drop training data with ctl vehicle\n\ntrain_features = train_features.iloc[non_ctl_idx]\nlabels_train = labels_train[non_ctl_idx]\nnonscored_labels_train = nonscored_labels_train[non_ctl_idx]\n\n# Import test data\n\ntest_features = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\ntest_features = test_features.drop(['sig_id','cp_dose','cp_time'],axis=1)\n\n# Import predictors from public kernel\n\njson_file_path = '../input/t-test-pca-rfe-logistic-regression/main_predictors.json'\n\nwith open(json_file_path, 'r') as j:\n    predictors = json.loads(j.read())\n    predictors = predictors['start_predictors']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create g-mean, c-mean, genes_pca (2 components), cells_pca (all components)\n\ncs = train_features.columns.str.contains('c-')\ngs = train_features.columns.str.contains('g-')\n\ndef preprocessor(train,test):\n    \n    # PCA\n    \n    n_gs = 2 # No of PCA comps to include\n    n_cs = 100 # No of PCA comps to include\n    \n    pca_cs = PCA(n_components = n_cs)\n    pca_gs = PCA(n_components = n_gs)\n\n    train_pca_gs = pca_gs.fit_transform(train[:,gs])\n    train_pca_cs = pca_cs.fit_transform(train[:,cs])\n    test_pca_gs = pca_gs.transform(test[:,gs])\n    test_pca_cs = pca_cs.transform(test[:,cs])\n    \n    # c-mean, g-mean\n    \n    train_c_mean = train[:,cs].mean(axis=1)\n    test_c_mean = test[:,cs].mean(axis=1)\n    train_g_mean = train[:,gs].mean(axis=1)\n    test_g_mean = test[:,gs].mean(axis=1)\n    \n    # Append Features\n    \n    train = np.concatenate((train,train_pca_gs,train_pca_cs,train_c_mean[:,np.newaxis]\n                            ,train_g_mean[:,np.newaxis]),axis=1)\n    test = np.concatenate((test,test_pca_gs,test_pca_cs,test_c_mean[:,np.newaxis],\n                           test_g_mean[:,np.newaxis]),axis=1)\n    \n    # Scaler for numerical values\n\n    # Scale train data\n    scaler = preprocessing.StandardScaler()\n\n    train = scaler.fit_transform(train)\n\n    # Scale Test data\n    test = scaler.transform(test)\n    \n    return train, test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_labels = labels_train.shape[1]\nn_labels_nonscored = nonscored_labels_train.shape[1]\nn_train = train_features.shape[0]\nn_test = test_features.shape[0]\n\n\n# Prediction Clipping Thresholds\n\np_min = 0.0005\np_max = 0.9995\n\n# OOF Evaluation Metric with clipping and no label smoothing\n\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(n_features, n_features_2, n_labels, n_labels_nonscored,label_smoothing = 0.0005):    \n    input_1 = layers.Input(shape = (n_features,), name = 'Input1')\n    input_2 = layers.Input(shape = (n_features_2,), name = 'Input2')\n\n    head_1 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dropout(0.6),\n        layers.Dense(512, activation=\"elu\"), \n        layers.BatchNormalization(),\n        layers.Dense(256, activation = \"elu\")\n        ],name='Head1') \n\n    input_3 = head_1(input_1)\n    input_3_concat = layers.Concatenate()([input_2, input_3])\n\n    head_2 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dropout(0.5),\n        layers.Dense(512, \"relu\"),\n        layers.BatchNormalization(),\n        layers.Dense(512, \"elu\"),\n        layers.BatchNormalization(),\n        layers.Dense(256, \"relu\"),\n        layers.BatchNormalization(),\n        layers.Dense(256, \"elu\")\n        ],name='Head2')\n\n    input_4 = head_2(input_3_concat)\n    input_4_concat = layers.Concatenate()([input_3, input_4])\n    \n    classifier_scored = Sequential([\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(256),\n        layers.Activation('relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.1),\n        layers.Dense(n_labels,activation='sigmoid')\n    ],name='Predicted_Scored_Labels')\n    \n    classifier_nonscored = Sequential([\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(512),\n        layers.Activation('relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.1),\n        layers.Dense(n_labels_nonscored,activation='sigmoid')\n    ],name='Predicted_Nonscored_Labels')\n    \n    output_2 = classifier_nonscored(input_4_concat)\n    input_4_avg = layers.Average()([input_3,input_4])\n    input_5 = layers.Concatenate()([input_4_avg, output_2])\n    output_1 = classifier_scored(input_5)\n\n\n    model = Model(inputs = [input_1, input_2], outputs = [output_1,output_2])\n    model.compile(optimizer=optimizers.Adam(learning_rate=3*1E-3), loss=losses.BinaryCrossentropy(label_smoothing=label_smoothing), metrics=logloss)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rebuild_model(model,n_features,n_features_2,label_smoothing = 0.0005):\n    \n    model.trainable = True\n    \n    input_1 = layers.Input(shape = (n_features,), name = 'Input1')\n    input_2 = layers.Input(shape = (n_features_2,), name = 'Input2')\n\n    head_1 = model.layers[1]\n    input_3 = head_1(input_1)\n    input_3_concat = layers.Concatenate()([input_2, input_3])\n\n    head_2 = model.layers[4]\n    input_4 = head_2(input_3_concat)\n    input_4_concat = layers.Concatenate()([input_3, input_4])\n    \n    classifier_scored = model.layers[9]\n    classifier_nonscored = model.layers[7]\n    \n    output_2 = classifier_nonscored(input_4_concat)\n    input_4_avg = layers.Average()([input_3,input_4])\n    input_5 = layers.Concatenate()([input_4_avg, output_2])\n    output_1 = classifier_scored(input_5)\n    \n    new_model = Model(inputs = [input_1,input_2], outputs = output_1)\n    new_model.compile(optimizer=optimizers.Adam(learning_rate=1E-4), loss=losses.BinaryCrossentropy(), metrics=logloss)\n    return new_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate Seeds\n\nn_seeds = 3\nnp.random.seed(1)\nseeds = np.random.randint(0,100,size=n_seeds)\n\n# Training Loop\n\nn_folds = 10\ny_pred = np.zeros((n_test,n_labels))\noof = tf.constant(0.0)\nhists = []\nfor seed in seeds:\n    fold = 0\n    kf = KFold(n_splits=n_folds,shuffle=True,random_state=seed)\n    for train, test in kf.split(train_features):\n        X_train, X_test = preprocessor(train_features.iloc[train].values,\n                                       train_features.iloc[test].values)\n        _,data_test = preprocessor(train_features.iloc[train].values,\n                                   test_features.drop('cp_type',axis=1).values)\n        X_train_2 = train_features.iloc[train][predictors].values\n        X_test_2 = train_features.iloc[test][predictors].values\n        data_test_2 = test_features[predictors].values\n        y_train = labels_train[train]\n        y_test = labels_train[test]        \n        y_train_2 = nonscored_labels_train[train]\n        y_test_2 = nonscored_labels_train[test]\n        n_features = X_train.shape[1]\n        n_features_2 = X_train_2.shape[1]\n\n        model = build_model(n_features, n_features_2, n_labels, n_labels_nonscored)\n        \n        reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_Predicted_Scored_Labels_logloss', factor=0.1, patience=3, mode='min', min_lr=1E-5)\n        early_stopping = callbacks.EarlyStopping(monitor='val_Predicted_Scored_Labels_logloss', min_delta=1E-5, patience=10, mode='min',\n                                                 restore_best_weights=True)\n        hist = model.fit([X_train,X_train_2],[y_train,y_train_2], batch_size=128, epochs=192,verbose=0,validation_data = ([X_test,X_test_2],[y_test,y_test_2]),\n                         callbacks=[reduce_lr, early_stopping])\n        hists.append(hist)\n        \n        # Rebuild model\n        \n        model = rebuild_model(model,n_features, n_features_2)\n        reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_logloss', factor=0.3, patience=3, mode='min', min_lr=1E-5)\n        early_stopping = callbacks.EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=10, mode='min',\n                                                 restore_best_weights=True)\n        \n        model.fit([X_train,X_train_2],y_train, batch_size=128, epochs=192,verbose=1,validation_data = ([X_test,X_test_2],y_test),\n                         callbacks=[reduce_lr, early_stopping])\n        # Save Model\n        model.save('TwoHeads_seed_'+str(seed)+'_fold_'+str(fold))\n\n        # OOF Score\n        y_val = model.predict([X_test,X_test_2])\n        oof += logloss(tf.constant(y_test,dtype=tf.float32),tf.constant(y_val,dtype=tf.float32))/(n_folds*n_seeds)\n\n        # Run prediction\n        y_pred += model.predict([data_test,data_test_2])/(n_folds*n_seeds)\n\n        fold += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Architecture\n\n#tf.keras.utils.plot_model(model,show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysis of Training\n\ntf.print('OOF score is ',oof)\n\nplt.figure(figsize=(12,8))\n\nhist_trains = []\nhist_lens = []\nfor i in range(n_folds*n_seeds):\n    hist_train = (hists[i]).history['Predicted_Scored_Labels_logloss']\n    hist_trains.append(hist_train)\n    hist_lens.append(len(hist_train))\nhist_train = []\nfor i in range(min(hist_lens)):\n    hist_train.append(np.mean([hist_trains[j][i] for j in range(n_folds*n_seeds)]))\n\nplt.plot(hist_train)\n\nhist_vals = []\nhist_lens = []\nfor i in range(n_folds*n_seeds):\n    hist_val = (hists[i]).history['val_Predicted_Scored_Labels_logloss']\n    hist_vals.append(hist_val)\n    hist_lens.append(len(hist_val))\nhist_val = []\nfor i in range(min(hist_lens)):\n    hist_val.append(np.mean([hist_vals[j][i] for j in range(n_folds*n_seeds)]))\n\nplt.plot(hist_val)\n\nplt.yscale('log')\nplt.yticks(ticks=[1,1E-1,1E-2])\nplt.xlabel('Epochs')\nplt.ylabel('Average Logloss')\nplt.legend(['Training','Validation'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate submission file, Clip Predictions\n\nsub = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\nsub.iloc[:,1:] = np.clip(y_pred,p_min,p_max)\n\n# Set ctl_vehicle to 0\nsub.iloc[test_features['cp_type'] == 'ctl_vehicle',1:] = 0\n\n# Save Submission\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}