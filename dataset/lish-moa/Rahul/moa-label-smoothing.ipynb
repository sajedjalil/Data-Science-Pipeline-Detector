{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Demonstration of Label smoothing\n\n### The competition metric punishes highly confident incorrect answers. In this notebook, I show that a simple modification where the labels are smoothed to a small extent, and predictions clipped to prevent prediction probabilities close to 0 and 1 boosts performance significantly\n\n### Inspired by this post https://www.kaggle.com/c/lish-moa/discussion/185593\n\n\n### For more ideas, check out my other notebooks:\n* [Pretrained Model Inference and Blending Starter](https://www.kaggle.com/rahulsd91/moa-starter-inference-blending-pretrained-models)\n* [Multi-input ResNet Architecture ](https://www.kaggle.com/rahulsd91/moa-multi-input-resnet-model)\n* [Autoencoder based approach](https://www.kaggle.com/rahulsd91/moa-autoencoder-features-only-lb-0-01884)\n\n### Best Version by LB Score (if not current) : V10\n\n#### Version 15: No scaling, drop cp_dose/cp_time\n\n#### Version 14: StandardScaler, remove WeightNormalization, no clipping for validation monitor metric\n\n#### Version 12: Add WeightNormalization layer to bias initialization\n\n#### Version 11: Test of bias initialization ([Idea from here](https://www.kaggle.com/tolgadincer/moa-tensorflow-fast-convergence) )\n\n#### Version 10: Increased NN units\n\n#### Version 8: Using keras's native label smoothing option\n\n#### Version 7: Save Models for Blending\n\n#### Version 6: Add Seed Averaging"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.feature_selection import VarianceThreshold\nfrom tensorflow.keras import layers,regularizers,Sequential,backend,callbacks,optimizers,metrics,losses\nimport tensorflow as tf\nimport sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import train data, drop sig_id, cp_type\n\ntrain_features = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\nnon_ctl_idx = train_features.loc[train_features['cp_type']!='ctl_vehicle'].index.to_list()\ntrain_features = train_features.drop(['sig_id','cp_type','cp_dose','cp_time'],axis=1)\ntrain_targets_scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ntrain_targets_scored = train_targets_scored.drop('sig_id',axis=1)\nlabels_train = train_targets_scored.values\n\n# Drop training data with ctl vehicle\n\ntrain_features = train_features.iloc[non_ctl_idx]\ndata_train = train_features.values\nlabels_train = labels_train[non_ctl_idx]\n\n# Import test data\n\ntest_features = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\ntest_features = test_features.drop(['sig_id','cp_dose','cp_time'],axis=1)\ndata_test = test_features.drop('cp_type',axis=1).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_labels = labels_train.shape[1]\nn_features = data_train.shape[1]\nn_train = data_train.shape[0]\nn_test = data_test.shape[0]\n\n\n# Prediction Clipping Thresholds\n\np_min = 0.001\np_max = 0.999\n\n# Evaluation Metric with clipping and no label smoothing\n\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))\n\n\n# Generate Seeds\n\nn_seeds = 1\n\n## I seed the np rng with a fixed seed and then use it to generate the random seeds for the MSKF. \n## Keep the same seed for different models that you plan to ensemble/blend so that their OOF performance is comparable.\n## Verify that the array \"seeds\" has unique integers. For eg. if np.random is seeded with 0, n_seeds = 6 results in 5 unique seeds.\nnp.random.seed(1)\nseeds = np.random.randint(0,100,size=n_seeds)\n\n# Training Loop\n\nn_folds = 5\ny_pred = np.zeros((n_test,n_labels))\noof = tf.constant(0.0)\nhists = []\nbias = tf.keras.initializers.Constant(np.log(labels_train.mean(axis=0)))\nfor seed in seeds:\n    fold = 0\n    mskf = MultilabelStratifiedKFold(n_splits=n_folds,shuffle=True,random_state=seed)\n    for train, test in mskf.split(data_train,labels_train):\n        X_train = data_train[train]\n        X_test = data_train[test]\n        y_train = labels_train[train]\n        y_test = labels_train[test]\n\n        # Define NN Model\n\n        model = Sequential()\n        model.add(layers.Dropout(0.4))\n        model.add(layers.Dense(1024))\n        model.add(layers.Activation('swish'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.1))\n        model.add(layers.Dense(1024))\n        model.add(layers.Activation('swish'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.1))\n        model.add(layers.Dense(n_labels, activation='sigmoid', bias_initializer=bias))\n        model.compile(optimizer=optimizers.Adam(learning_rate=5*1E-5), loss=losses.BinaryCrossentropy(label_smoothing=0.001),\n                      metrics=metrics.BinaryCrossentropy())\n        reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_binary_crossentropy', factor=0.5, patience=5, mode='min', min_lr=1E-5)\n        early_stopping = callbacks.EarlyStopping(monitor='val_binary_crossentropy', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True)\n\n        hist = model.fit(X_train,y_train, batch_size=128, epochs=192,verbose=0,validation_data = (X_test,y_test),callbacks=[reduce_lr, early_stopping])\n        hists.append(hist)\n        \n        # Save Model\n        model.save('LabelSmoothed_seed_'+str(seed)+'_fold_'+str(fold))\n\n        # OOF Score\n        y_val = model.predict(X_test)\n        oof += logloss(tf.constant(y_test,dtype=tf.float32),tf.constant(y_val,dtype=tf.float32))/(n_folds*n_seeds)\n\n        # Run prediction\n        y_pred += model.predict(data_test)/(n_folds*n_seeds)\n\n        fold += 1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysis of Training\n\ntf.print('OOF score is ',oof)\n\nplt.figure(figsize=(12,8))\n\nhist_trains = []\nhist_lens = []\nfor i in range(n_folds*n_seeds):\n    hist_train = (hists[i]).history['binary_crossentropy']\n    hist_trains.append(hist_train)\n    hist_lens.append(len(hist_train))\nhist_train = []\nfor i in range(min(hist_lens)):\n    hist_train.append(np.mean([hist_trains[j][i] for j in range(n_folds*n_seeds)]))\n\nplt.plot(hist_train)\n\nhist_vals = []\nhist_lens = []\nfor i in range(n_folds*n_seeds):\n    hist_val = (hists[i]).history['val_binary_crossentropy']\n    hist_vals.append(hist_val)\n    hist_lens.append(len(hist_val))\nhist_val = []\nfor i in range(min(hist_lens)):\n    hist_val.append(np.mean([hist_vals[j][i] for j in range(n_folds*n_seeds)]))\n\nplt.plot(hist_val)\n\nplt.yscale('log')\nplt.xlabel('Epochs')\nplt.ylabel('Average Logloss')\nplt.legend(['Training','Validation'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate submission file, Clip Predictions\n\nsub = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\nsub.iloc[:,1:] = np.clip(y_pred,p_min,p_max)\n\n# Set ctl_vehicle to 0\nsub.iloc[test_features['cp_type'] == 'ctl_vehicle',1:] = 0\n\n# Save Submission\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}