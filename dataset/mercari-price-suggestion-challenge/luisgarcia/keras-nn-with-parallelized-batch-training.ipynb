{"metadata":{"language_info":{"nbconvert_exporter":"python","version":"3.6.3","mimetype":"text/x-python","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","name":"python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat":4,"cells":[{"metadata":{"_cell_guid":"0f3ab6b4-5b89-447c-841b-82ea3139295d","_uuid":"4dd86826913b2f8779885ec7536aa9a483a01ce7"},"cell_type":"markdown","source":"I start with [Alex Papiu's](https://www.kaggle.com/apapiu/ridge-script) script where he preprocesses the data into a ```scipy.sparse``` matrix and train a neural network. Keras does not like ```scipy.sparse``` matrices and converting the entire training set to a matrix will lead to computer memory issues; so the model is trained in batches: 32 samples at a time, and these few samples can be converted to matrices and fed into the network. \n\nThis requieres a batch generator, which I pieced together from this [stack overflow question](https://stackoverflow.com/questions/41538692/using-sparse-matrices-with-keras-and-tensorflow) and I set up an iterator to make it threadsafe for parallelization. ~~Kaggle allows the use of 32 cores which speeds up the training~~. Seems like kaggle only allows four cores.  \n\nI have been tuning the network and it seems like a smaller network with longer epochs yields better results. Currently I have a two hidden layers with 25  and 10 nodes. This is quite small but, with the input layer considered, this network still yields approximately 1.5M parameters!\n\nGive it a try and let me know what you think. There are still plenty of things on can try:\n* Add a validation set for early stopping. \n* Tune `batch_size`, `samples_per_epoch`, and nodes in hidden layers.\n* Add dropout.\n* Add L1 and/or L2 regularization.\n   \n\n"},{"metadata":{"_cell_guid":"4a79269a-c302-48af-bd96-1a426a651c18","_uuid":"bddce000193062c3a129f2125d253e18926a2f71","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport scipy\nimport gc\nimport time\nimport threading\nimport multiprocessing\n\nfrom sklearn import metrics\nfrom sklearn.linear_model import Ridge, LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\n\nfrom keras import regularizers \nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.utils import np_utils\nfrom keras.callbacks import EarlyStopping\n\n\ndef hms_string(sec_elapsed):\n    h = int(sec_elapsed / (60 * 60))\n    m = int((sec_elapsed % (60 * 60)) / 60)\n    s = sec_elapsed % 60\n    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n\n\n# the following functions allow for a parallelized batch generator\nclass threadsafe_iter(object):\n    \"\"\"\n    Takes an iterator/generator and makes it thread-safe by\n    serializing call to the `next` method of given iterator/generator.\n    \"\"\"\n    def __init__(self, it):\n        self.it = it\n        self.lock = threading.Lock()\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        with self.lock:\n            return next(self.it)\n\ndef threadsafe_generator(f):\n    \"\"\"\n    A decorator that takes a generator function and makes it thread-safe.\n    \"\"\"\n    def g(*a, **kw):\n        return threadsafe_iter(f(*a, **kw))\n    return g\n\n@threadsafe_generator\ndef batch_generator(X_data, y_data, batch_size):\n    \n    #index = np.random.permutation(X_data.shape[0])    \n    #X_data = X_data[index]\n    #y_data = y_data[index]\n    \n    samples_per_epoch = X_data.shape[0]\n    number_of_batches = samples_per_epoch/batch_size\n    counter=0\n    index = np.arange(np.shape(y_data)[0])\n    #idx = 1\n    while 1:\n        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n        X_batch = X_data[index_batch,:].todense()\n        y_batch = y_data[index_batch]\n        counter += 1\n        yield np.array(X_batch),y_batch\n        #print(\"\")\n        #print(X_batch.shape)\n        #print(\"\")\n        #print('generator yielded a batch %d' % idx)\n        #idx += 1\n        if (counter > number_of_batches):\n            counter=0\n            \n            \n@threadsafe_generator\ndef batch_generator_x(X_data,batch_size):\n    samples_per_epoch = X_data.shape[0]\n    number_of_batches = samples_per_epoch/batch_size\n    counter=0\n    index = np.arange(np.shape(X_data)[0])\n    while 1:\n        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n        X_batch = X_data[index_batch,:].todense()\n        counter += 1\n        yield np.array(X_batch)\n        if (counter > number_of_batches):\n            counter=0\n\nnp.random.seed(1337)  # for reproducibility\nstart_time = time.time()\n\nNUM_BRANDS = 2500\nNAME_MIN_DF = 10\nMAX_FEAT_DESCP = 50000\n\nprint(\"Reading in Data\")\n\ndf_train = pd.read_csv('../input/train.tsv', sep='\\t')\ndf_test = pd.read_csv('../input/test.tsv', sep='\\t')\n\ndf = pd.concat([df_train, df_test], 0)\nnrow_train = df_train.shape[0]\ny_train = np.log1p(df_train[\"price\"])\n\ndel df_train\ngc.collect()\n\n#print(df.memory_usage(deep = True))\n\ndf[\"category_name\"] = df[\"category_name\"].fillna(\"Other\").astype(\"category\")\ndf[\"brand_name\"] = df[\"brand_name\"].fillna(\"unknown\")\n\npop_brands = df[\"brand_name\"].value_counts().index[:NUM_BRANDS]\ndf.loc[~df[\"brand_name\"].isin(pop_brands), \"brand_name\"] = \"Other\"\n\ndf[\"item_description\"] = df[\"item_description\"].fillna(\"None\")\ndf[\"item_condition_id\"] = df[\"item_condition_id\"].astype(\"category\")\ndf[\"brand_name\"] = df[\"brand_name\"].astype(\"category\")\n\n#print(df.memory_usage(deep = True))\n\nprint(\"Encodings...\")\ncount = CountVectorizer(min_df=NAME_MIN_DF)\nX_name = count.fit_transform(df[\"name\"])\n\nprint(\"Category Encoders...\")\nunique_categories = pd.Series(\"/\".join(df[\"category_name\"].unique().astype(\"str\")).split(\"/\")).unique()\ncount_category = CountVectorizer()\nX_category = count_category.fit_transform(df[\"category_name\"])\n\nprint(\"Descp encoders...\")\ncount_descp = TfidfVectorizer(max_features = MAX_FEAT_DESCP, \n                              ngram_range = (1,3),\n                              stop_words = \"english\")\nX_descp = count_descp.fit_transform(df[\"item_description\"])\n\nprint(\"Brand encoders...\")\nvect_brand = LabelBinarizer(sparse_output=True)\nX_brand = vect_brand.fit_transform(df[\"brand_name\"])\n\nprint(\"Dummy Encoders..\")\nX_dummies = scipy.sparse.csr_matrix(pd.get_dummies(df[[\n    \"item_condition_id\", \"shipping\"]], sparse = True).values)\n\nX = scipy.sparse.hstack((X_dummies, \n                         X_descp,\n                         X_brand,\n                         X_category,\n                         X_name)).tocsr()\n\n#print([X_dummies.shape, X_category.shape, X_name.shape, X_descp.shape, X_brand.shape])\n\nX_train = X[:nrow_train]\n\ntpoint1 = time.time()\nprint(\"Time for Preprocessing: {}\".format(hms_string(tpoint1-start_time)))\n\nprint(\"Train dimensions:{}\".format(X_train.shape))\n\n\n\n\nprint(\"Fitting Model\")\n\nn_workers = multiprocessing.cpu_count() #it's 32 on kaggle, 4 on my personal machine\nbatch_size = 32\n            \nmodel = Sequential()\nmodel.add(Dense(25,\n                input_dim=X_train.shape[1],\n                kernel_initializer='normal',\n                activation='relu',\n                kernel_regularizer=regularizers.l2(0.01),\n                activity_regularizer=regularizers.l1(0.01),\n               ))\nmodel.add(Dense(10, kernel_initializer='normal', activation='relu'))\n#model.add(Dropout(0.01))\nmodel.add(Dense(1, kernel_initializer='normal'))\n#monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\nmodel.fit_generator(generator=batch_generator(X_train, y_train, batch_size),\n                    workers=n_workers, \n                    steps_per_epoch=1024, #samples_per_epoch=1024,\n                    max_queue_size=128,\n                    epochs=37, \n                    verbose=1,\n                    use_multiprocessing=False\n                   )\n\ntpoint2 = time.time()\nprint(\"Time for Training: {}\".format(hms_string(tpoint2-tpoint1)))\n\n\nX_test = X[nrow_train:]\nprint(\"Test dimensions:{}\".format(X_test.shape))\n\ns = np.floor(X_test.shape[0]/batch_size)+1\nprint(s)\n\n# unfortunately, predicting is tougher to parallelize...\npred = model.predict_generator(generator=batch_generator_x(X_test, batch_size),\n                               steps=s,\n                               #workers=n_workers, \n                               #max_queue_size=32, \n                               use_multiprocessing=False,\n                               verbose=1\n                              )\n\ntpoint3 = time.time()\nprint(\"Predict dimensions:{}\".format(pred.shape))\nprint(\"Time for Predicting: {}\".format(hms_string(tpoint3-tpoint2)))\n\n\n\ndf_test[\"price\"] = np.expm1(pred)\ndf_test[[\"test_id\", \"price\"]].to_csv(\"submission_NN.csv\", index = False)\n\nelapsed_time = time.time() - start_time\nprint(\"Total Time: {}\".format(hms_string(elapsed_time)))\n\n\n\n\n"}],"nbformat_minor":1}