{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# We will use the official tokenization script created by the Google team\n!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install pandarallel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n# from pandarallel import pandarallel\n# pandarallel.initialize(nb_workers=4, progress_bar=True)\nimport numpy as np\nfrom tqdm import tqdm\nimport tokenization\nimport gc\n\nfrom sklearn.preprocessing import LabelEncoder\n\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom keras import backend as K\nfrom tensorflow.keras.layers import Dense, Input, concatenate, add, BatchNormalization, PReLU, Dropout\n# from tensorflow.keras.layers import MaxoutDense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nfrom transformers import TFBertModel\nfrom transformers.tokenization_bert import BertTokenizer\n\nfrom kaggle_datasets import KaggleDatasets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_table('../input/mercari/train.tsv')\n# test_df = pd.read_table('../input/test.tsv')\nprint(train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_missing_brand(line, all_brands):\n    brand = line[0]\n    name = line[1]\n    namesplit = name.split(\" \")\n    if brand == \"missing\":\n        for one_word_brand in namesplit:   # ä¸€ä¸ªè¯çš„å“ç‰Œ\n            if one_word_brand in all_brands:\n                return one_word_brand\n        two_word_brands = [namesplit[i] + \" \" + namesplit[i+1] for i in range(len(namesplit)-1)]\n        for two_word_brand in two_word_brands:   # 2ä¸ªè¯çš„å“ç‰Œ\n            if two_word_brand in all_brands:\n                return two_word_brand\n        three_word_brands = [namesplit[i] + \" \" + namesplit[i+1] + \" \" + namesplit[i+2] for i in range(len(namesplit)-2)]\n        for three_word_brand in three_word_brands:   # 3ä¸ªè¯çš„å“ç‰Œ\n            if three_word_brand in all_brands:\n                return three_word_brand\n        four_word_brands = [namesplit[i] + \" \" + namesplit[i+1] + \" \" + namesplit[i+2] + \" \" + namesplit[i+3] for i in range(len(namesplit)-3)]\n        for four_word_brand in four_word_brands:   # 4ä¸ªè¯çš„å“ç‰Œ\n            if four_word_brand in all_brands:\n                return four_word_brand\n    return brand  \n\ndef compose_full_text(line):\n    name = line[0]\n    category = line[1]\n    brand = line[2]\n    description = line[3]\n    composed_full_text = \"Item name: \" + str(name) + \", \" + \"item category: \" + str(category) + \", \" + \"item_brand: \" + str(brand) + \".\" + \"Item description: \" + str(description)\n    return composed_full_text\n\ndef bert_encode(texts, tokenizer, max_len):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\ndef split_cat(text):\n    try: return text.split(\"/\")\n    except: return (\"missing\", \"missing\", \"missing\")\n\ndef rmse(y_true, y_pred):\n    # Y and Y_red have already been in log scale.\n    # assert y_true.shape == y_pred.shape\n    return K.sqrt(K.mean(K.square(y_pred - y_true )))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, dataframe, batch_size, tokenizer, max_len):\n        self.dataframe = dataframe\n        self.batch_size = batch_size\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.ceil(len(self.dataframe) / self.batch_size))\n\n    def __getitem__(self, idx):\n        'Generate one batch of data'\n        train_full_text = bert_encode(self.dataframe[\"full_text\"][idx * self.batch_size: min((idx + 1) * self.batch_size, len(self.dataframe))].values, self.tokenizer, self.max_len)\n        train_subcat_0 = self.dataframe['subcat_0'][idx * self.batch_size: min((idx + 1) * self.batch_size, len(self.dataframe))].values\n        train_subcat_1 = self.dataframe['subcat_1'][idx * self.batch_size: min((idx + 1) * self.batch_size, len(self.dataframe))].values\n        train_subcat_2 = self.dataframe['subcat_2'][idx * self.batch_size: min((idx + 1) * self.batch_size, len(self.dataframe))].values\n        train_brand = train_df['brand'][idx * self.batch_size: min((idx + 1) * self.batch_size, len(self.dataframe))].values\n        train_condition = train_df['item_condition_id'][idx * self.batch_size: min((idx + 1) * self.batch_size, len(self.dataframe))].values\n        train_shipping = train_df['shipping'][idx * self.batch_size: min((idx + 1) * self.batch_size, len(self.dataframe))].values\n        \n        train_y = train_df['price'][idx * self.batch_size: min((idx + 1) * self.batch_size, len(self.dataframe))].values\n        return [train_full_text[0], train_full_text[1], train_full_text[2], train_subcat_0, train_subcat_1, train_subcat_2, train_brand, train_condition, train_shipping], train_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(df):\n    \n    # å»é™¤ price=0 çš„å€¼ 0ï¸âƒ£\n    df = df.drop(df[(df.price == 0)].index)\n    \n    # price æ”¾ç¼©åˆ° ln(price+1) ğŸ“‰\n    df[\"price\"] = np.log1p(df.price)\n    \n    # æ‰¾å›ä¸€äº›ç¼ºå¤±çš„ brand ğŸ³\n    all_brands = set(df['brand_name'].values)   # 1,2,3,4è¯çš„å“ç‰Œéƒ½å¾ˆå¤š\n    df[\"brand_name\"].fillna(value=\"missing\", inplace=True)\n    premissing = len(df.loc[df['brand_name'] == 'missing'])\n    tqdm.pandas(desc=\"æ‰¾å›ç¼ºå¤±brand_nameğŸ‘œ\")\n    df['brand_name'] = df[['brand_name', 'name']].progress_apply(lambda x: find_missing_brand(x, all_brands), axis = 1)\n    postmissing = len(df.loc[df['brand_name'] == 'missing'])\n    print(\"å¤„ç†å‰ç¼ºå¤±ï¼š{}ï¼Œå¤„ç†åç¼ºå¤±ï¼š{}ï¼Œæ‰¾åˆ°äº†ï¼š{}\".format(premissing, postmissing, premissing-postmissing))\n    \n    # å°†å•†å“ç±»åˆ«åˆ†å¼€ï¼Œå¹¶å¡«å……å…¶ç¼ºå¤±å€¼ â›\n    tqdm.pandas(desc=\"å°†å•†å“ç±»åˆ«åˆ†å¼€âœ‚\")\n    df['subcat_0'], df['subcat_1'], df['subcat_2'] = zip(*df['category_name'].progress_apply(lambda x: split_cat(x)))\n    # df.item_description.replace('No description yet',\"missing\", inplace=True)\n    \n    # ç»„åˆæˆæ–°æ–‡æœ¬ Item name: ..., item category: ..., item brand: ..., item description: .... ğŸ“•\n    tqdm.pandas(desc=\"ç»„åˆæ–°æ–‡æœ¬ğŸ§©\")\n    df[\"full_text\"] = df[['name', 'category_name', 'brand_name', 'item_description']].progress_apply(compose_full_text, axis = 1)\n    \n    # å¤„ç†ç±»åˆ«å˜é‡ subcat0, subcat1, subcat2 å’Œ brand_name ğŸ‘”\n    le = LabelEncoder()\n    \n    le.fit(df[\"subcat_0\"])\n    df[\"subcat_0\"] = le.transform(df[\"subcat_0\"])\n\n    le.fit(df[\"subcat_1\"])\n    df[\"subcat_1\"] = le.transform(df[\"subcat_1\"])\n\n    le.fit(df[\"subcat_2\"])\n    df[\"subcat_2\"] = le.transform(df[\"subcat_2\"])\n\n    le.fit(df.brand_name)\n    df['brand'] = le.transform(df.brand_name)\n    \n    del le\n    gc.collect()\n    \n    # åˆ é™¤ä¸€äº›åˆ—èŠ‚çº¦å†…å­˜ ğŸ˜­\n    df.drop(['category_name', 'brand_name', 'name', 'item_description'], axis=1, inplace=True) \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**å¤„ç†åçš„full_textæœ€é•¿1180ï¼Œå¹³å‡é•¿åº¦270**"},{"metadata":{},"cell_type":"markdown","source":"## å‡†å¤‡è®­ç»ƒæ•°æ®"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = preprocessing(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# full_text é•¿åº¦ â˜¹\nsns.distplot(a=train_df[\"full_text\"].apply(lambda x: len(x)), kde=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.percentile(train_df[\"full_text\"].apply(lambda x: len(x)), [25, 50, 75]), np.mean(train_df[\"full_text\"].apply(lambda x: len(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.to_csv(\"./train_preprocessed.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## æ¨¡å‹å»ºç«‹"},{"metadata":{"trusted":true},"cell_type":"code","source":"# module_url = \"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/1\"\n# bert_layer = hub.KerasLayer(module_url, trainable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/1\", trainable=True)\n# # æ–‡æœ¬åºåˆ— ğŸ“ƒ\n# full_text_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"full_text_ids\")\n# full_text_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"full_text_mask\")\n# full_text_segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"full_text_segment_ids\")\n\n# # åˆ†ç±»å˜é‡ ğŸ¨\n# subcat_0 = Input(shape=(1,), dtype=tf.float32, name='subcat_0')\n# subcat_1 = Input(shape=(1,), dtype=tf.float32, name='subcat_1')\n# subcat_2 = Input(shape=(1,), dtype=tf.float32, name='subcat_2')\n# brand = Input(shape=(1,), dtype=tf.float32, name='brand')\n# condition = Input(shape=(1,), dtype=tf.float32, name='condition')\n# shipping = Input(shape=(1,), dtype=tf.float32, name='shipping')\n\n# # BERT å¤„ç†æ–‡å­—åºåˆ— ğŸ˜º\n# _, sequence_output = bert_layer([full_text_ids, full_text_mask, full_text_segment_ids])\n# full_text_encoding = sequence_output[:, 0, :]\n\n# # concat åˆå¹¶ ğŸ§±\n# con = concatenate([full_text_encoding, subcat_0, subcat_1, subcat_2, brand, condition, shipping])\n# con = BatchNormalization()(con)\n\n# # å…¨è¿æ¥åˆ†æ”¯ 1ï¸âƒ£\n# x1 = Dense(30, activation='sigmoid')(con)\n\n# x2 = Dense(470)(con)\n# x2 = PReLU()(x2)\n# con = concatenate([x1,x2])\n# con = Dropout(0.02)(con)\n\n# # å…¨è¿æ¥åˆ†æ”¯ 2ï¸âƒ£\n# x1 = Dense(256, activation='sigmoid')(con)\n\n# x2 = Dense(11, activation='linear')(con)\n\n# x3 = Dense(11)(con)\n# x3 = PReLU()(x3)\n# con = concatenate([x1, x2, x3])\n# con = Dropout(0.02)(con)\n\n# # å…¨è¿æ¥åˆ†æ”¯ 3ï¸âƒ£\n# out1 = Dense(1, activation='linear')(con)\n# out2 = Dense(1, activation='relu')(con)\n# # out3 = MaxoutDense(1, 30)(con)\n# output = add([out1,out2])\n\n\n# model = Model(inputs=[full_text_ids, full_text_mask, full_text_segment_ids, subcat_0, subcat_1, subcat_2, brand, condition, shipping], outputs=output)\n# model.compile(Adam(lr=2e-6), loss=rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n# do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n# tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# batch_size = 8 * strategy.num_replicas_in_sync\n# epochs = 1\n\n# autotune = tf.data.experimental.AUTOTUNE\n# pretrained_weights = 'bert-base-uncased'\n# tokenizer = BertTokenizer.from_pretrained(pretrained_weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with strategy.scope():\n#     train_generator = DataGenerator(train_df[:1400000], bs, tokenizer, max_len=300)\n#     valid_generator = DataGenerator(train_df[1400000:], bs, tokenizer, max_len=300)\n# model.fit_generator(generator = train_generator,\n#           epochs=1,\n#           validation_data=valid_generator)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}