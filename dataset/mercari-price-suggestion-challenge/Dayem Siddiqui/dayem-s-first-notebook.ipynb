{"cells":[{"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\n# print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\ndf = pd.read_csv(\"..//input//train.tsv\", sep=\"\\t\")\ndf_test = pd.read_csv(\"..//input//test.tsv\", sep=\"\\t\")\ndf_sample = pd.read_csv(\"..//input//sample_submission.csv\", sep=\",\")\n# Any results you write to the current directory are saved as output.\n\ndf.head()\n","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"e8eb34d356556ca10c4befcdacc148b45f59d61d","_cell_guid":"d3831f1f-5ef1-4de4-871c-c4d63550a06c"}},{"source":"df['main_category'], df['sub_category'], df['nested_category'] = df['category_name'].str.split('/', 2).str\ndf_test['main_category'], df_test['sub_category'], df_test['nested_category'] = df_test['category_name'].str.split('/', 2).str\ndf.head()","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"25d5d79105057f4da453b06ab255a4b10f48a505","_cell_guid":"e62e2477-1ac7-4ec4-995f-81d6de4c4968"}},{"source":"# Filtered Columns","cell_type":"markdown","metadata":{"_uuid":"5b11d34583796ae4f44b77c5ef12514dc8359d60","_cell_guid":"e4bd55f3-a0a7-40de-bbd4-4a3aacfedfee"}},{"source":"filtered = df[['price', 'item_condition_id', 'shipping', 'main_category', 'sub_category', 'nested_category', 'brand_name', 'name']]\nfiltered_test = df_test[['test_id','item_condition_id', 'shipping', 'main_category', 'sub_category', 'nested_category', 'brand_name', 'name']]\nfiltered.head()","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"e5af00cc4d853e8f63d76c227aaaf6474f1be291","_cell_guid":"b012b718-0afb-467a-96d9-8ed1f43c16b4"}},{"source":"# Database Preprocessing\n\nWe will preprocess the data and covert it into lower case","cell_type":"markdown","metadata":{"_uuid":"03f2071e4cf9bd24ec923992d06f14131afcd1c0","_cell_guid":"a502db8e-3d1f-4fc3-b706-3d6c20e6a83d"}},{"source":"filtered = filtered.apply(lambda x: x.astype(str).str.lower())\nfiltered_test = filtered_test.apply(lambda x: x.astype(str).str.lower())\nfiltered.head()","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"988e669d61af6a94eadbe51f8b61e1bfbd8cbc7c","_cell_guid":"a22aab2a-81f4-4815-ae16-a7b5e40ae156"}},{"source":"# Sampling the dataset\n\nWe will sample the dataset to get a subset of data. As the current data is too large for analysis, sampling a smaller subset will make our data processing much faster. We are sampling around 30% of our data","cell_type":"markdown","metadata":{"_uuid":"d013261d32ce4cb9676d9ec78d340790d1e7fcea","_cell_guid":"9f98ab93-c6a4-497b-8f01-5f2222ebd783"}},{"source":"# sampled = filtered.sample(frac=0.3)\nsampled = filtered\nsampled.head()","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"f1500294a209a7f3ec8c0116b3400c4b3d4c565d","_cell_guid":"22e5ad04-3068-4515-a441-529b31994bba"}},{"source":"# One Hot Encoding of Categorical Variables\n\nNow we are going to transform our categorical variables into one hot encoding so we can use it for regression analysis","cell_type":"markdown","metadata":{"_uuid":"b71e9b49f130caab900c76aca353d4dca680a1e8","_cell_guid":"4042ef1c-5ee3-486f-bf11-8d235ff2bea3"}},{"source":"encoded = pd.get_dummies(sampled, columns=['brand_name', 'main_category', 'sub_category', 'nested_category'])\nencoded.head()","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"7770cc91bc37091a6cd967fbe0f357aa89fac57c","_cell_guid":"bc35b893-5dd6-4d23-8233-b3be468ecb09","collapsed":true}},{"source":"# Factorization\n\nAfter trying one hot encoding, I realized that there is an explosion of dimensions, so after doing some research I found out that factorization was another way to handle categorical variables. So lets factorize our data","cell_type":"markdown","metadata":{"_uuid":"64fcc8e755079fa12dc6faa12d349fff561093e2","_cell_guid":"b08372d0-8f91-46a5-9aa1-e5efae760263"}},{"source":"columns = ['main_category', 'brand_name', 'sub_category', 'nested_category', 'name']\nfactorized = sampled[columns].apply(lambda x: pd.factorize(x)[0])\nfactorized['price'] = sampled['price']\nfactorized['shipping'] = sampled['shipping']\nfactorized['item_condition_id'] = sampled['item_condition_id']\n\n\nfactorized_test = filtered_test[columns].apply(lambda x: pd.factorize(x)[0])\nfactorized_test['shipping'] = filtered_test['shipping']\nfactorized_test['item_condition_id'] = filtered_test['item_condition_id']\nfactorized_test['test_id'] = filtered_test['test_id']\n\nfactorized = factorized.apply(lambda x: x.astype('category'))\nfactorized_test = factorized_test.apply(lambda x: x.astype('category'))\nfactorized['price'] = factorized['price'].apply(pd.to_numeric)\nfactorized.head()\n ","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"83e50919e474d255a50bbf13a3f3856f5d899056","_cell_guid":"cfa93004-09df-4f39-81b5-889c7e17dc78","collapsed":true}},{"source":"# Splitting the data\n\nNow we are going to split the data into training and testing","cell_type":"markdown","metadata":{"_uuid":"4c5203a599227cc1351e4e48fe5ce290efb8909a","_cell_guid":"4cee3bfe-e903-405d-93c6-c2a744804b65"}},{"source":"#Spliting the sampled data into training and testing\nfactorized['is_train'] = np.random.uniform(0, 1, len(factorized)) <= .75\n# Create two new dataframes, one with the training rows, one with the test rows\ntrain, test = factorized[factorized['is_train']==True], factorized[factorized['is_train']==False]\n\n# Show the number of observations for the test and training dataframes\nprint('Number of observations in the training data:', len(train))\nprint('Number of observations in the test data:',len(test))","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"88567ea9c250752f53e04e5c917d6dba0abc99ad","_cell_guid":"c520ccdb-e1e7-4e24-9174-95c39497bd65","collapsed":true}},{"source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn import linear_model\n\n# regr = linear_model.Ridge (alpha = .9)\n# regr = linear_model.LinearRegression()\nregr = RandomForestRegressor(max_depth=30, random_state=0)\ncollist = train.columns.tolist()\ncollist.remove('price')\ncollist.remove('is_train')\n# collist\nregr.fit(train[collist], train['price'])\n","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"60b768a946d5f9ac2b651f78e72611a841677c98","_cell_guid":"b5d582f9-8dca-41c4-908f-9deb1cad8681","collapsed":true}},{"source":"import math\nfrom decimal import Decimal\ndef rmsle(y_pred, y):\n    assert len(y) == len(y_pred)\n    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n    return (sum(to_sum) * (1.0/len(y))) ** 0.5","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"29dd0f90fce0a8b3f45f3abbb3b23866a79df549","_cell_guid":"72c93671-9bb5-478d-8ca7-6cca8e6cbbe9","collapsed":true}},{"source":"# regr.score(test[collist], test['price'], sample_weight=None)\nfrom sklearn.metrics import mean_squared_log_error\n\n\n\n# mean_squared_log_error(test['price'], regr.predict(test[collist]))  \nprint(rmsle(list(regr.predict(test[collist])), list(test['price'])))\n# factorized_test['price'] = regr.predict(factorized_test[collist])\n# submission = factorized_test[['test_id', 'price']]\n# submission.to_csv('final.csv', index=False)\n# submission\n\n\n# test.head()","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"95ee1c4c9c5df182f142d4d76dfb6ba014f04f91","_cell_guid":"b456b333-4ec8-436e-8aa1-d997b9fa01b0","collapsed":true}},{"source":"import math\nfrom decimal import Decimal\nfrom keras import backend as K\n\ndef rmsle_k(y_pred, y):\n#     assert len(y) == len(y_pred)\n    to_sum = [(K.log(y_pred[i] + 1) - K.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n    return (K.sum(to_sum) * (1.0/len(y))) ** 0.5","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"collapsed":true}},{"source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=4, mode='auto')\n\n# train = (train - train.mean()) / (train.max() - train.min())\n# test = (test - test.mean()) / (test.max() - test.min())\n\nmodel = Sequential()\nmodel.add(Dense(units=64, activation='relu', input_dim=len(collist)))\nmodel.add(Dense(units=64, activation='relu'))\nmodel.add(Dense(units=64, activation='relu'))\nmodel.add(Dense(units=64, activation='relu'))\nmodel.add(Dense(units=64, activation='relu'))\nmodel.add(Dense(activation='relu', output_dim=1))\n\nmodel.compile(optimizer='rmsprop',loss='mean_squared_logarithmic_error')\n# model.fit(train[collist], train['price'],epochs=10, batch_size=128)\n# print(rmsle(list(model.predict(test[collist])), list(test['price'])))\n","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"collapsed":true}},{"source":"df['item_description']","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"collapsed":true}}],"nbformat":4,"nbformat_minor":1,"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","file_extension":".py","version":"3.6.3","nbconvert_exporter":"python"}}}