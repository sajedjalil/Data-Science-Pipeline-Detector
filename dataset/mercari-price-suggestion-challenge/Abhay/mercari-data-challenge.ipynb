{"nbformat_minor":1,"metadata":{"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.6.4","codemirror_mode":{"name":"ipython","version":3},"name":"python","pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"nbformat":4,"cells":[{"execution_count":null,"cell_type":"code","source":"# Based on Bojan: https://www.kaggle.com/tunguz/wordbatch-ftrl-fm-lgb-lbl-0-42506\n\nimport re\nimport gc\nimport time\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import csr_matrix,hstack\nfrom time import gmtime,strftime\nimport wordbatch\nfrom wordbatch.extractors import WordBag,WordHash\nfrom wordbatch.models import FTRL,FM_FTRL\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nfrom scipy.sparse import csr_matrix,hstack\nimport lightgbm as lgb\n\n\n# Function to handle the missing data.\ndef handle_missing_data(comb):\n    comb['first_category'].fillna(value='missing_category',inplace=True)\n    comb['second_category'].fillna(value='missing_category',inplace=True)\n    comb['third_category'].fillna(value='missing_category',inplace=True)\n    comb['item_description'].fillna(value='missing_description',inplace=True)\n    comb['brand_name'].fillna(value='missing_brand',inplace=True)\n    comb['name'].fillna(value='missing_name',inplace=True)\n    #return comb\n\n# Function to split the category into sub-categories\ndef custom_split(description):\n    try:\n        return description.split(\"/\")\n    except:\n        return [\"No Label\",\"No Label\",\"No Label\"]\n\n# Function to change the data type of the categorical variable.\ndef convert_to_categorical(comb):\n    comb['first_category'] = comb['first_category'].astype('category')\n    comb['second_category'] = comb['second_category'].astype('category')\n    comb['third_category'] = comb['third_category'].astype('category')\n    comb['item_condition_id'] = comb['item_condition_id'].astype('category')\n\n    \n# Function to filter the dataset to feed to the model.\ndef filtering_dataset(comb):\n    popular_brand = comb['brand_name'].value_counts().loc[lambda x: x.index !='missing_brand'].index[:4500]\n    comb.loc[~comb['brand_name'].isin(popular_brand),'brand_name'] = 'missing'\n    popular_first_category = comb['first_category'].value_counts().loc[lambda x: x.index!='missing_category'].index[0:1250]\n    comb.loc[~comb['first_category'].isin(popular_first_category),'first_category'] ='missing'\n    popular_second_category = comb['second_category'].value_counts().loc[lambda x: x.index!='missing_category'].index[0:1250]\n    comb.loc[~comb['second_category'].isin(popular_second_category),'third_category'] = 'missing'\n    popular_third_category = comb['third_category'].value_counts().loc[lambda x: x.index!='missing_category'].index[0:1250]\n    comb.loc[~comb['third_category'].isin(popular_third_category),'third_category'] = 'missing'\n\nstopwords = {x:1 for x in stopwords.words('english')}\nnon_alphanums = re.compile(u'[^A-Za-z0-9]+')\n# def normalize_text(text):\n#     return u\" \".join([x for x in [y for y in non_aplhanums.sub(' ',text).lower().strip().split(\" \")]\\\n#                      if len(x) > 1 and x not in stopwords])\n\n# Function to normalize the text\ndef normalize_text(text):\n    return u\" \".join(\n        [x for x in [y for y in non_alphanums.sub(' ', text).lower().strip().split(\" \")] \\\n         if len(x) > 1 and x not in stopwords])\n\n# Function to compute the Root Mean Squared Logarithmic Error.\ndef rmsle(y_act,y_pred):\n    assert len(y_act) == len(y_pred)\n    return np.sqrt(np.mean(np.power(np.log1p(y_act) - np.log1p(y_pred),2)))\n\n\n# Function for implementing the business logic.\ndef main_function():\n    start_time = time.time()\n    print(start_time)\n    print(strftime(\"%Y-%m-%d %H:%M:%S\",gmtime()))\n    mercari_data_train = pd.read_table('../input/train.tsv',engine='c')\n    mercari_data_test = pd.read_table('../input/test.tsv',engine='c')\n    print('[{}] Finished to load the data'.format(time.time() - start_time))\n    print('Shape of training Data',mercari_data_train.shape)\n    print('Shape of testing Data',mercari_data_test.shape)\n    nrow_test = mercari_data_train.shape[0]\n    dftt = mercari_data_train[(mercari_data_train.price < 1.0)]\n    mercari_data_train = mercari_data_train.drop(mercari_data_train[(mercari_data_train.price < 1.0)].index)\n    del dftt['price']\n    nrow_train = mercari_data_train.shape[0]\n    y = np.log1p(mercari_data_train[\"price\"])\n    print('Fields of training dataset',mercari_data_train.columns)\n    print('Fields of testing dataset',mercari_data_test.columns) \n    comb: pd.DataFrame = pd.concat([mercari_data_train,dftt,mercari_data_test])\n    print('Shape of training data:',comb.shape)\n    submission:pd.Dataframe = mercari_data_test[['test_id']]\n    #comb = comb[comb['category_name'].notnull()]\n    comb['first_category'],comb['second_category'],comb['third_category'] = zip(*comb['category_name'].apply(lambda x: custom_split(x)))\n    handle_missing_data(comb)\n    comb.drop(['category_name'],axis=1,inplace=True)\n    print('[{}] Split categories complete and original dropped'.format(time.time() - start_time))\n    filtering_dataset(comb)\n    convert_to_categorical(comb)\n    print(comb.dtypes)\n    comb = comb[comb['name'].notnull()]\n    print(comb['name'].head(n=500))\n    word_batch = wordbatch.WordBatch(normalize_text,extractor=(WordBag,{\"hash_ngrams\":2,\"hash_ngrams_weights\":[1.5,1.0],\"hash_size\":2**29,\"norm\":None,\"tf\":\"binary\",\"idf\":None}),procs=4)\n    word_batch.dictionary_freeze=True\n    #comb.head(n=5)\n    #comb.columns.values\n    X_name = word_batch.fit_transform(comb['name'])\n    X_name = X_name[:,np.where(X_name.getnnz(axis=0) > 1)[0]]\n    del(word_batch)\n    word_batch = CountVectorizer()\n    X_first_category = word_batch.fit_transform(comb['first_category'])\n    X_second_category = word_batch.fit_transform(comb['second_category'])\n    X_third_category = word_batch.fit_transform(comb['third_category'])\n    print('[{}] Count Vectorize categories completed.'.format(time.time() - start_time))\n    # Word Batch for item description\n    word_batch = wordbatch.WordBatch(normalize_text,extractor=(WordBag,{\"hash_ngrams\":2,\"hash_ngrams_weights\":[1.5,1.0],\"hash_size\":2**29,\"norm\":\"l2\",\"tf\":1.0,\"idf\":None,}),procs=8)\n    word_batch.dictionary_freeze=True\n    X_description = word_batch.fit_transform(comb['item_description'])\n    del(word_batch)\n    X_description = X_description[:,np.where(X_description.getnnz(axis=0)>1)[0]]\n    print('[{}] Vectorize item_description completed.'.format(time.time() - start_time))\n    lb = LabelBinarizer(sparse_output=True)\n    X_brand = lb.fit_transform(comb['brand_name'])\n    print('[{}] Label Binarize brand name completed.'.format(time.time() - start_time))\n    X_dummies = csr_matrix(pd.get_dummies(comb[['item_condition_id','shipping']],sparse=True).values)\n    print('[{}] Get dummies on item condition and shipping done.'.format(time.time() - start_time))\n    print(X_dummies.shape,X_description.shape,X_brand.shape,X_first_category.shape,X_second_category.shape,X_third_category.shape,X_name)\n    sparse_merge = hstack((X_dummies,X_description,X_brand,X_first_category,X_second_category,X_third_category,X_name)).tocsr()\n    print('[{}] Create sparse merge completed'.format(time.time() - start_time))\n    print(sparse_merge.shape)\n    sparse_merge = sparse_merge[:,np.where(sparse_merge.getnnz(axis=0) > 100)[0]]\n    X = sparse_merge[:nrow_train]\n    X_test = sparse_merge[nrow_test:]\n    #print(sparse_merge.head(n=5))\n    \n    gc.collect()\n    train_X,train_y = X,y\n    train_X,valid_X,train_y,valid_y = train_test_split(X,y,test_size=0.05,random_state=100)\n    model  = FTRL(alpha=0.01,beta=0.1,L1=0.00001,L2=1.0,D=sparse_merge.shape[1],iters=50,inv_link=\"identity\",threads=1)\n    model.fit(train_X,train_y)\n    print('[{}] Train FTRL completed'.format(time.time() - start_time))\n    preds = model.predict(X = valid_X)\n    print(\"FTRL Dev MSLE:\",rmsle(np.expm1(valid_y),np.expm1(preds)))\n    \n    predsF = model.predict(X_test)\n    print('[{}] Predict FTRL completed'.format(time.time() - start_time))\n    \n    model = FM_FTRL(alpha=0.01,beta=0.01,L1=0.00001,L2=0.1,D=sparse_merge.shape[1],alpha_fm=0.01,L2_fm=0.0,init_fm=0.01,D_fm=200,e_noise=0.0001,iters=20,inv_link='identity',threads=4)\n    model.fit(train_X,train_y)\n    print('[{}] Train ridge v2 completed'.format(time.time() - start_time))\n    preds = model.predict(X=valid_X)\n    print(\"FM_FTRL dev RMSLE:\",rmsle(np.expm1(valid_y),np.expm1(preds)))\n    \n    predsFM = model.predict(X_test)\n    print('[{}] Predict FM_FTRL completed'.format(time.time() - start_time))\n    \n    params = {'learning_rate':0.2,'application':'regression','max_depth':4,'num_leaves':15,'verbosity':-1,'metric':'RMSE','data_random_seed':1,'bagging_fraction':0.6,'bagging_freq':5,'feature_fraction':0.65,'nthread':4,'min_data_in_leaf':100,'max_bin':16}\n    \n    # Remove features with document frequency <=100\n    print(sparse_merge.shape)\n    sparse_merge = sparse_merge[:,np.where(sparse_merge.getnnz(axis=0) > 100)[0]]\n    X = sparse_merge[:nrow_train]\n    X_test = sparse_merge[nrow_test:]\n    print(sparse_merge.shape)\n    train_X,train_y = X,y\n    train_X,valid_X,train_y,valid_y = train_test_split(X,y,test_size=0.05,random_state=100)\n    d_train = lgb.Dataset(train_X,label=train_y)\n    watch_list = [d_train]\n    d_valid = lgb.Dataset(valid_X,label=valid_y)\n    watch_list = [d_train,d_valid]\n    model = lgb.train(params,train_set=d_train,num_boost_round=200,valid_sets=watch_list,early_stopping_rounds=50,verbose_eval=20)\n    preds = model.predict(valid_X)\n    print(\"LGB dev RMSLE:\",rmsle(np.expm1(valid_y),np.expm1(preds)))\n    predsL = model.predict(X_test)\n    print('[{}] Predict LGB completed.'.format(time.time() - start_time))\n    preds = (predsF*0.18 + predsL*0.27 + predsFM*0.55)\n    submission['price'] = np.expm1(preds)\n    submission.to_csv(\"wordbatch_ftrl_fm_lgb.csv\",index=False)\n    \nmain_function()\n\n#from subprocess import check_output\n#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","outputs":[],"metadata":{"_uuid":"1213de58c84354310e861196cc061b9388620e11","_cell_guid":"aef56894-4d7b-449d-87c7-3d550cd70614"}}]}