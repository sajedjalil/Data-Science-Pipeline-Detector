{"nbformat":4,"metadata":{"language_info":{"version":"3.6.3","name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","pygments_lexer":"ipython3"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"_cell_guid":"5d7fb6fb-b95d-4fb5-b3af-3d99b0d898e6","_uuid":"651d8deed11432573fdf44b6cd9b6ace8b7826a6"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport sklearn\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"metadata":{"_cell_guid":"b57578ce-0b0e-410e-8bd4-f020b767d50c","_uuid":"4b18b06806d867bcbcb4c73d207c610ba5e935b3"},"execution_count":null,"cell_type":"code","outputs":[],"source":"trainDF = pd.read_csv(\"../input/train.tsv\", sep = '\\t')\ntestDF = pd.read_csv(\"../input/test.tsv\", sep = '\\t')"},{"metadata":{"collapsed":true,"_cell_guid":"5cb4ae39-743e-459a-94fc-e6dcf1db2f0c","_uuid":"61d73de0d60ded31ec0b96279b19743fafbe1e28"},"execution_count":null,"cell_type":"code","outputs":[],"source":"#pd.get_dummies(df['brand_name'].sample(100), prefix_sep = 'brand_name',  dummy_na = False, sparse = True)\nclass oneHotEncoder:\n\n    def __init__(self, threshold):\n        self.threshold = threshold\n        \n    @staticmethod\n    def binary_variance(p):\n        return p * (1 - p)\n    \n    def dum_sign(self, df, col, threshold=0.01):\n        dummy_col = df[col].fillna('')\n        dummy_col = dummy_col.astype(str)\n        p = dummy_col.value_counts() / dummy_col.shape[0]\n        mask = dummy_col.isin(p[self.binary_variance(p) >= threshold].index)\n        dummy_col[~mask] = np.nan\n        res = pd.get_dummies(dummy_col, prefix=col, dummy_na=False)\n        return res\n    \n    def one_hot_encoding(self, X, threshold):\n        dfs = []\n        for col in X.columns:\n            if type(threshold) == float:\n                t = threshold\n            elif col in threshold:\n                t = threshold[col]\n            else:\n                t = 0.0\n            df = self.dum_sign(X, col, t)\n            dfs.append(df)\n        res = pd.concat(dfs, axis=1)\n        return res\n    \n    def fit_transform(self, df):\n        res = self.one_hot_encoding(df, self.threshold)\n        self.columns = res.columns\n        return res\n    \n    def transform(self, df):\n        res = self.one_hot_encoding(df, self.threshold)\n        return res.reindex(columns = self.columns, fill_value=0)\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom itertools import product\n\nclass MeanEncoder:\n    def __init__(self, categorical_features, n_splits=5, target_type='classification', prior_weight_func=None):\n        \"\"\"\n        :param categorical_features: list of str, the name of the categorical columns to encode\n\n        :param n_splits: the number of splits used in mean encoding\n\n        :param target_type: str, 'regression' or 'classification'\n\n        :param prior_weight_func:\n        a function that takes in the number of observations, and outputs prior weight\n        when a dict is passed, the default exponential decay function will be used:\n        k: the number of observations needed for the posterior to be weighted equally as the prior\n        f: larger f --> smaller slope\n        \"\"\"\n\n        self.categorical_features = categorical_features\n        self.n_splits = n_splits\n        self.learned_stats = {}\n\n        if target_type == 'classification':\n            self.target_type = target_type\n            self.target_values = []\n        else:\n            self.target_type = 'regression'\n            self.target_values = None\n\n        if isinstance(prior_weight_func, dict):\n            self.prior_weight_func = eval('lambda x: 1 / (1 + np.exp((x - k) / f))', dict(prior_weight_func, np=np))\n        elif callable(prior_weight_func):\n            self.prior_weight_func = prior_weight_func\n        else:\n            self.prior_weight_func = lambda x: 1 / (1 + np.exp((x - 2) / 1))\n\n    @staticmethod\n    def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func):\n        X_train = X_train[[variable]].copy()\n        X_test = X_test[[variable]].copy()\n\n        if target is not None:\n            nf_name = '{}_pred_{}'.format(variable, target)\n            X_train['pred_temp'] = (y_train == target).astype(int)  # classification\n        else:\n            nf_name = '{}_pred'.format(variable)\n            X_train['pred_temp'] = y_train  # regression\n        prior = X_train['pred_temp'].mean()\n\n        col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg({'mean': 'mean', 'beta': 'size'})\n        col_avg_y['beta'] = prior_weight_func(col_avg_y['beta'])\n        col_avg_y[nf_name] = col_avg_y['beta'] * prior + (1 - col_avg_y['beta']) * col_avg_y['mean']\n        col_avg_y.drop(['beta', 'mean'], axis=1, inplace=True)\n\n        nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values\n        nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values\n\n        return nf_train, nf_test, prior, col_avg_y\n\n    def fit_transform(self, X, y):\n        \"\"\"\n        :param X: pandas DataFrame, n_samples * n_features\n        :param y: pandas Series or numpy array, n_samples\n        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n        \"\"\"\n        X_new = X.copy()\n        if self.target_type == 'classification':\n            skf = StratifiedKFold(self.n_splits)\n        else:\n            skf = KFold(self.n_splits)\n\n        if self.target_type == 'classification':\n            self.target_values = sorted(set(y))\n            self.learned_stats = {'{}_pred_{}'.format(variable, target): [] for variable, target in\n                                  product(self.categorical_features, self.target_values)}\n            for variable, target in product(self.categorical_features, self.target_values):\n                nf_name = '{}_pred_{}'.format(variable, target)\n                X_new.loc[:, nf_name] = np.nan\n                for large_ind, small_ind in skf.split(y, y):\n                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target, self.prior_weight_func)\n                    X_new.iloc[small_ind, -1] = nf_small\n                    self.learned_stats[nf_name].append((prior, col_avg_y))\n        else:\n            self.learned_stats = {'{}_pred'.format(variable): [] for variable in self.categorical_features}\n            for variable in self.categorical_features:\n                nf_name = '{}_pred'.format(variable)\n                X_new.loc[:, nf_name] = np.nan\n                for large_ind, small_ind in skf.split(y, y):\n                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, None, self.prior_weight_func)\n                    X_new.iloc[small_ind, -1] = nf_small\n                    self.learned_stats[nf_name].append((prior, col_avg_y))\n        return X_new\n\n    def transform(self, X):\n        \"\"\"\n        :param X: pandas DataFrame, n_samples * n_features\n        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n        \"\"\"\n        X_new = X.copy()\n\n        if self.target_type == 'classification':\n            for variable, target in product(self.categorical_features, self.target_values):\n                nf_name = '{}_pred_{}'.format(variable, target)\n                X_new[nf_name] = 0\n                for prior, col_avg_y in self.learned_stats[nf_name]:\n                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n                        nf_name]\n                X_new[nf_name] /= self.n_splits\n        else:\n            for variable in self.categorical_features:\n                nf_name = '{}_pred'.format(variable)\n                X_new[nf_name] = 0\n                for prior, col_avg_y in self.learned_stats[nf_name]:\n                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n                        nf_name]\n                X_new[nf_name] /= self.n_splits\n\n        return X_new"},{"metadata":{"collapsed":true,"_cell_guid":"40577afb-e3a0-4914-9a26-30709b07c56a","_uuid":"c95ce3a491a62676479015ccab9c19d432f26605"},"execution_count":null,"cell_type":"code","outputs":[],"source":"#one hot\nX = df['name item_condition_id category_name brand_name shipping'.split()]\nY = df['price']\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.01, random_state=0)\n\noneHotEnc = oneHotEncoder(0.002)\nX_train_onehot = oneHotEnc.fit_transform(X_train)\nX_test_onehot = oneHotEnc.transform(X_test)"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"X = trainDF['name item_condition_id category_name brand_name shipping'.split()]\nY = trainDF['price']\nX_train, X_validation, y_train, y_validation = train_test_split(X, Y, test_size=0.05, random_state=0)\n\n\nmeanENC = MeanEncoder(X_validation.columns.tolist(), 5, 'regression', {\"k\":100, \"f\":100})\nX_train_new = meanENC.fit_transform(X_train, y_train)\nX_validataion_new = meanENC.transform(X_validation)\n\nnew_columns = []\nfor c in X_train_new.columns.tolist():\n    if(c.find(\"pred\") != -1):\n        new_columns.append(c)\nnew_columns\n\n\nX_train_new = X_train_new[new_columns]\nX_validataion_new = X_validataion_new[new_columns]\n\n\nminMaxScaler = sklearn.preprocessing.MinMaxScaler()\nX_train_scaler = minMaxScaler.fit_transform(X_train_new)\nX_validation_scaler = minMaxScaler.transform(X_validataion_new)\n"},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","outputs":[],"source":"# ridge regression\ndef ridgeRegression(x_train, y_train, x_test, y_test):\n    from sklearn import linear_model\n    from sklearn import metrics\n    reg = linear_model.Ridge(alpha = 1)\n    reg.fit(x_train, y_train)\n    pre_train = reg.predict(x_train)\n    pre_test = reg.predict(x_test)\n    print (\"Train-RMSE:\", np.sqrt(metrics.mean_squared_error(y_train, pre_train)))\n    print (\"Test-RMSE:\", np.sqrt(metrics.mean_squared_error(y_test, pre_test)))\n    print (\"Train-MAPE:\", metrics.mean_absolute_error(y_train, pre_train))\n    print (\"Test-MAPE:\", metrics.mean_absolute_error(y_test, pre_test))\n    return reg\n\nreg = ridgeRegression(X_train_scaler, y_train, X_validation_scaler, y_validation)\n"},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","outputs":[],"source":"testDF_meanENC = meanENC.transform(testDF['name item_condition_id category_name brand_name shipping'.split()])[new_columns]\ntestDF_meanENC_scaler = minMaxScaler.transform(testDF_meanENC)\npre_test = reg.predict(testDF_meanENC_scaler)\n"},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","outputs":[],"source":"testDF['price'] = pre_test\nm = testDF.pre_test.mean()\ntestDF['price'] = testDF.price.apply(lambda x: x if(x > 0) else m)\n\nsub = testDF['test_id price'.split()]\nsub.to_csv('sub_submission.csv', index = False)"},{"metadata":{"collapsed":true,"_cell_guid":"1965d23b-c7bc-403e-bd66-9edea4f18db7","_uuid":"f9b796927de62ea018f1abb2a2282a7aadaad628"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# ridge regression\nfrom sklearn import linear_model\nfrom sklearn import metrics\nreg = linear_model.Ridge(alpha = 1)\nreg.fit(X_train_onehot, y_train)\n\npre_train = reg.predict(X_train_onehot)\npre_test = reg.predict(X_test_onehot)\nprint (\"Train-RMSE:\", np.sqrt(metrics.mean_squared_error(y_train, pre_train)))\nprint (\"Test-RMSE:\", np.sqrt(metrics.mean_squared_error(y_test, pre_test)))\nprint (\"Train-MAPE:\", metrics.mean_absolute_error(y_train, pre_train))\nprint (\"Test-MAPE:\", metrics.mean_absolute_error(y_test, pre_test))\n\nX_test = oneHotEnc.transform(df_test['name item_condition_id category_name brand_name shipping'.split()])\npre_test = reg.predict(X_test)\ndf_test['price'] = pre_test"},{"metadata":{"collapsed":true,"_cell_guid":"0b64664e-9b47-4399-a4a9-a41378fab324","_uuid":"ae27f04a9829a4d79a41575d6718ed555822c89e"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df_test['price'] = df_test.price.apply(lambda x: x if(x > 0) else 26.74936370209377)"},{"metadata":{"collapsed":true,"_cell_guid":"0ce59a5b-1d52-48ed-81b5-12a626f7a3cb","_uuid":"813b367c2ad510c1671d06e66c5eeed9705aa2fa"},"execution_count":null,"cell_type":"code","outputs":[],"source":"sub = df_test['test_id price'.split()]\nsub.to_csv('sub_submission.csv', index = False)"},{"metadata":{"collapsed":true,"_cell_guid":"92c30a69-571b-4e5e-b62f-86c8fec2609f","_uuid":"fbfcc368e4ee5fad5399dd506fcf763ab6d60720"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df_test[df_test.price<0]"}],"nbformat_minor":1}