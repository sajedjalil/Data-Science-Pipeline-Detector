{"nbformat_minor":1,"cells":[{"cell_type":"markdown","metadata":{"_uuid":"ada14f875d2dc510a14e51e7e639a4c7f1cebe62","_cell_guid":"473a7e6a-ad12-4615-9096-ae9bf90ac2a2"},"source":"# Import"},{"cell_type":"code","metadata":{"_uuid":"226a0e08386e9cfcfca3db10ca7d6e78b40c4c43","_cell_guid":"616375a8-409f-4290-a4e3-2ce75e9778ae","collapsed":true},"outputs":[],"execution_count":null,"source":"import os\nimport gc\nimport time\nstart_time = time.time()\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nfrom nltk.corpus import stopwords\nstops = set(stopwords.words('english'))\nimport string\npuns = string.punctuation\n\nimport gensim"},{"cell_type":"markdown","metadata":{"_uuid":"ce75b9aae8dad082d7f1df36782b2fdb74208279","_cell_guid":"4b3bb412-ce27-472b-b1df-92924d57bfac"},"source":"# Loading Data"},{"cell_type":"code","metadata":{"_uuid":"2939e421506f02064089c3f1a799c03c54face34","_cell_guid":"a99d34bc-ef3b-40c9-8d08-d8fdc694fb86","scrolled":true,"collapsed":true},"outputs":[],"execution_count":null,"source":"df_train = pd.read_table('../input/mercari-price-suggestion-challenge/train.tsv')\ndf_test = pd.read_table('../input/mercari-price-suggestion-challenge/test.tsv')\n# for testing(concerning time)\n# df_train = pd.read_table('../input/mercari-price-suggestion-challenge/train.tsv').loc[:10000]\n# df_test = pd.read_table('../input/mercari-price-suggestion-challenge/test.tsv').loc[:10000]\ndf_train['target'] = np.log1p(df_train['price']) ## (np.log(array + 1))"},{"cell_type":"code","metadata":{"_uuid":"0a10b062122a85b57728657275aa4885e94afcd0","_cell_guid":"2e7d1077-5bf1-4537-b5bf-5c6c5321dc7a","scrolled":true,"collapsed":true},"outputs":[],"execution_count":null,"source":"print(df_train.shape)\nprint('5 folds scaling the test_df')\nprint(df_test.shape)\ntest_len = df_test.shape[0]\ndef simulate_test(df_test):\n    if df_test.shape[0] < 800000:\n        indices = np.random.choice(df_test.index.values, 2800000)\n        df_test_ = pd.concat([df_test, df_test.iloc[indices]], axis=0)\n        return df_test_.copy()\n    else:\n        return df_test\ndf_test = simulate_test(df_test)\n\nprint('new shape ', df_test.shape)\nprint('[{}] Finished scaling test set...'.format(time.time() - start_time))"},{"cell_type":"markdown","metadata":{"_uuid":"be2bdedbbda7ebca4f93e06c23c9ea95b55137f8","_cell_guid":"649f34c5-104b-41ed-82ae-a3cf27341e3c"},"source":"# Handling missing values"},{"cell_type":"code","metadata":{"_uuid":"35f1f35b5fa5445deaffa6c7abd3d50e4363b136","_cell_guid":"c273e333-0a13-4fb2-bb5a-198f9351717b","collapsed":true},"outputs":[],"execution_count":null,"source":"print(\"Handling missing values...\")\ndef handle_missing(dataset):\n    dataset.category_name.fillna(value=\"missing\", inplace=True)\n    dataset.brand_name.fillna(value=\"missing\", inplace=True)\n    dataset.item_description.fillna(value=\"missing\", inplace=True)\n    return (dataset)\n\ndf_train = handle_missing(df_train)\ndf_test = handle_missing(df_test)\nprint(df_train.shape)\nprint(df_test.shape)\n\nprint('[{}] Finished handling missing data...'.format(time.time() - start_time))"},{"cell_type":"markdown","metadata":{"_uuid":"39ef0394554141d2f511cf4965ddc64346a39efa","_cell_guid":"c5383da1-3fc4-4b8e-af7b-601eba6f9dfb"},"source":"# Preprocess - Word2Vec\nconsidering time, this part is abandomed"},{"cell_type":"code","metadata":{"_uuid":"ebf910850fdd4b9b08530187aa5c3f24a41390f5","_cell_guid":"231c806d-2948-4e34-8108-e82db80ab6a9","collapsed":true},"outputs":[],"execution_count":null,"source":"# def tokenize(Doc):\n#     if pd.notnull(Doc):\n#         tokens = nltk.wordpunct_tokenize(Doc)\n#         text = nltk.Text(tokens)\n#         words = [w.lower() for w in text if w not in stops and w not in puns]\n#         return words\n#     else:\n#         return None"},{"cell_type":"code","metadata":{"_uuid":"e04b6c4ccedb0a9f938f9724ef00899e61c14703","_cell_guid":"9d23cf80-d7c2-4f55-bab0-38fd01d9239f","collapsed":true},"outputs":[],"execution_count":null,"source":"# tokens = []\n# for sent in np.hstack([df_train['item_description'], df_test['item_description'], \n#            df_train['name'], df_test['name']]):\n#     if pd.notnull(sent):\n#         tokens.extend(tokenize(sent))\n\n# tokens = list(set(tokens))"},{"cell_type":"code","metadata":{"_uuid":"26f1e3602fcd79af55cf53547ff486bfd30f52bc","_cell_guid":"3dbcef36-6e4d-4457-b6b8-c8c7bc37d887","collapsed":true},"outputs":[],"execution_count":null,"source":"# word_vec_mapping = {}\n# path = \"../input/wrod2vec-twitter-50d/glove.twitter.27B.50d.txt\"\n\n# with open(path, 'r', encoding='utf8') as f:\n#     for line in f:\n#         tokens = line.split()\n#         token = tokens[0]\n#         vec = tokens[1:]\n#         if token in tokens:\n#             word_vec_mapping[token] = np.array(vec, dtype=np.float32)\n# vec_dimensions = len(word_vec_mapping.get('a'))"},{"cell_type":"code","metadata":{"_uuid":"e5428890b72a9f930176a965759f96783b9456da","_cell_guid":"7b82624a-a12b-4c2d-9e8a-37fe5bc1f5f0","collapsed":true},"outputs":[],"execution_count":null,"source":"# def doc2vec(doc, word2vec=word_vec_mapping):\n#     docvec=np.zeros(vec_dimensions, )\n#     vec_count = 1\n    \n#     if pd.notnull(doc):\n#         terms = tokenize(doc)\n#         for term in terms:\n#             termvec = word_vec_mapping.get(term, None)\n#             if termvec is not None:\n#                 docvec += np.array(termvec, dtype=np.float32)\n#                 vec_count += 1              \n#     return (docvec/vec_count)\n\n# def doc2vec_desc(doc, word2vec=word_vec_mapping):\n#     docvec=np.zeros(vec_dimensions, )\n#     vec_count = 1\n    \n#     if pd.notnull(doc):\n#         terms = tokenize(doc)\n#         for term in terms:\n#             termvec = word_vec_mapping.get(term, None)\n#             if termvec is not None:\n#                 docvec += np.array(termvec, dtype=np.float32)\n#                 vec_count += 1              \n#     return (docvec/vec_count)\n\n# def embedding_name_cat_word2vec(df):\n#     name_vecs = []\n#     desc_vecs = []\n#     for idx, row in df.iterrows():\n#         name_vecs.append(doc2vec(row['name']))\n#         desc_vecs.append(doc2vec(row['item_description']))\n#         if idx % 200000 == 0:\n#             print(idx)\n#     df['name_word2vec'] = name_vecs\n#     df['item_description_word2vec'] = desc_vecs\n#     return df\n\n# df_train = embedding_name_cat_word2vec(df_train)\n# df_test = embedding_name_cat_word2vec(df_test)"},{"cell_type":"markdown","metadata":{"_uuid":"04f7c69a2f9b1858be33b055b332b85edb9d5799","_cell_guid":"a64c437b-937f-4fce-8357-50de5a8c5bf3"},"source":"# Peprocess - category and brand"},{"cell_type":"code","metadata":{"_uuid":"d5d443c3a4c9b6422056ae3d68b6bc7b79d8a36e","_cell_guid":"7cbc8d9c-e05c-40b3-bab4-627de7112677","collapsed":true},"outputs":[],"execution_count":null,"source":"#PROCESS CATEGORICAL DATA\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, MultiLabelBinarizer\nprint(\"Handling categorical variables...\")\nle_cat = LabelEncoder()\nle_cat.fit(np.hstack([df_train.category_name, df_test.category_name]))\ndf_train['category'] = le_cat.transform(df_train.category_name)\ndf_test['category'] = le_cat.transform(df_test.category_name)\n\nle_brand = LabelEncoder()\nle_brand.fit(np.hstack([df_train.brand_name, df_test.brand_name]))\ndf_train['brand'] = le_brand.transform(df_train.brand_name)\ndf_test['brand'] = le_brand.transform(df_test.brand_name)\n# del le, df_train['brand_name'], df_test['brand_name']\n\nprint('[{}] Finished PROCESSING CATEGORICAL DATA...'.format(time.time() - start_time))\ndf_train.head(3)"},{"cell_type":"markdown","metadata":{"_uuid":"547505344f191f3c842c138a639e421086a4681b","_cell_guid":"dfdfb00f-be7e-48b7-8350-894dd1a5d37b"},"source":"# Peprocess - category_split"},{"cell_type":"code","metadata":{"_uuid":"ef4d81c4ad955106a20a734eda8f95fabd4077b1","_cell_guid":"c3f1cc61-df31-4489-8c96-25d217472da8","collapsed":true},"outputs":[],"execution_count":null,"source":"#Build Category LabelEncoder\ndef serialize_category(cats):\n    return cats.split('/')\ndf_train['category_split'] = df_train['category_name'].apply(serialize_category)\ndf_test['category_split'] = df_test['category_name'].apply(serialize_category)\nmlb = MultiLabelBinarizer()    \nmlb.fit(np.hstack([df_train['category_split'], df_test['category_split']]))\n\nidx_cat_mapping = {}\nfor idx, cat_name in enumerate(mlb.classes_):\n    idx_cat_mapping[cat_name] = idx\n\ndef getidxs(cats):\n    idxs = []\n    for cat in cats:\n        idx = idx_cat_mapping.get(cat)\n        idxs.append(idx)\n    return idxs\n\ndf_train['category_split'] = df_train['category_split'].apply(getidxs)\ndf_test['category_split'] = df_test['category_split'].apply(getidxs)"},{"cell_type":"markdown","metadata":{"_uuid":"547d8bed516bb8e285d35f6e87596ef4c38a4fb6","_cell_guid":"d8b24477-a82a-4de1-b9b6-fb7bb6856cc0"},"source":"# Prprocess - category_name_onehot, item_description_onehot, name_onehot"},{"cell_type":"code","metadata":{"_uuid":"37103e42b613d13342463701a0a67acbffd8640d","_cell_guid":"a3ef4034-9af8-49f3-85e8-3f65dd087ada","collapsed":true},"outputs":[],"execution_count":null,"source":"#PROCESS TEXT: RAW\nprint(\"Text to seq process...\")\nprint(\"   Fitting tokenizer...\")\nfrom keras.preprocessing.text import Tokenizer\nraw_text = np.hstack([df_train['category_name'].str.lower(), \n                      df_train['item_description'].str.lower(), \n                      df_train['name'].str.lower(),\n                      df_test['category_name'].str.lower(), \n                      df_test['item_description'].str.lower(), \n                      df_test['name'].str.lower()])\n\ntok_raw = Tokenizer()\ntok_raw.fit_on_texts(raw_text)\nprint(\"   Transforming text to seq...\")\ndf_train[\"category_name_onehot\"] = tok_raw.texts_to_sequences(df_train['category_name'].str.lower())\ndf_test[\"category_name_onehot\"] = tok_raw.texts_to_sequences(df_test['category_name'].str.lower())\ndf_train[\"item_description_onehot\"] = tok_raw.texts_to_sequences(df_train['item_description'].str.lower())\ndf_test[\"item_description_onehot\"] = tok_raw.texts_to_sequences(df_test['item_description'].str.lower())\ndf_train[\"name_onehot\"] = tok_raw.texts_to_sequences(df_train['name'].str.lower())\ndf_test[\"name_onehot\"] = tok_raw.texts_to_sequences(df_test['name'].str.lower())\ndf_train.head(3)\n\nprint('[{}] Finished PROCESSING TEXT DATA...'.format(time.time() - start_time))"},{"cell_type":"markdown","metadata":{"_uuid":"67973afcb65ad42cc742013e0b828267f0680cb1","_cell_guid":"2a794e74-6335-4d06-8f94-1428bd95850e"},"source":"# train_test_split"},{"cell_type":"code","metadata":{"_uuid":"65e6cb624dba6243706aa52f44fd4ace68df1558","_cell_guid":"3ac741fc-3611-421a-a10c-9b1eecc9e2f3","collapsed":true},"outputs":[],"execution_count":null,"source":"#EXTRACT DEVELOPTMENT TEST\nfrom sklearn.model_selection import train_test_split\ndtrain, dvalid = train_test_split(df_train, random_state=233, train_size=0.99)\nprint(dtrain.shape)\nprint(dvalid.shape)"},{"cell_type":"markdown","metadata":{"_uuid":"fef3dc6d9fb192b017979cfee20b664791bca325","_cell_guid":"cbd6d167-8022-4fe0-b49c-77a039a08efc"},"source":"# Preparing Keras Input"},{"cell_type":"code","metadata":{"_uuid":"b3e1d7c2c20beda598c8c71f5d7616cd63658405","_cell_guid":"375b4caa-ba0e-4563-83c4-2a40c116b52a","collapsed":true},"outputs":[],"execution_count":null,"source":"#EMBEDDINGS MAX VALUE\n#Base on the histograms, we select the next lengths\nMAX_NAME_SEQ = 20 #17\nMAX_ITEM_DESC_SEQ = 60 #269\nMAX_CATEGORY_NAME_SEQ = 20 #8\nMAX_CATEGORY_SPLIT_SEQ = np.max([df_train['category_split'].apply(len).max(), df_test['category_split'].apply(len).max()])\n\nMAX_TEXT = len(tok_raw.word_index) +1\nMAX_CATEGORY = len(le_cat.classes_)\nMAX_CATEGORY_SPLIT = len(mlb.classes_)\nMAX_BRAND = len(le_brand.classes_)\nMAX_CONDITION = np.max([df_train.item_condition_id.max(), \n                        df_test.item_condition_id.max()])+1\n\nprint('[{}] Finished EMBEDDINGS MAX VALUE...'.format(time.time() - start_time))"},{"cell_type":"code","metadata":{"_uuid":"87c10bd797c7e27290c028faa55069707e1e459e","_cell_guid":"62ce661f-2e42-4537-9006-ba134fda724e","collapsed":true},"outputs":[],"execution_count":null,"source":"#KERAS DATA DEFINITION\nfrom keras.preprocessing.sequence import pad_sequences\n\ndef get_keras_data(dataset):\n    X = {\n        'name_onehot': pad_sequences(dataset['name_onehot'], maxlen=MAX_NAME_SEQ),\n        'item_description_onehot': pad_sequences(dataset['item_description_onehot']\n                                    , maxlen=MAX_ITEM_DESC_SEQ),\n        'brand': np.array(dataset['brand']),\n        'category': np.array(dataset['category']),\n        'category_split': pad_sequences(dataset.category_split, maxlen=MAX_CATEGORY_SPLIT_SEQ),\n        'category_name_onehot': pad_sequences(dataset['category_name_onehot']\n                                        , maxlen=MAX_CATEGORY_NAME_SEQ),\n        'item_condition': np.array(dataset.item_condition_id),\n        'num_vars': np.array(dataset[[\"shipping\"]])\n    }\n    return X\n\nX_train = get_keras_data(dtrain)\nX_valid = get_keras_data(dvalid)\nX_test = get_keras_data(df_test)\n\nprint('[{}] Finished DATA PREPARARTION...'.format(time.time() - start_time))"},{"cell_type":"markdown","metadata":{"_uuid":"bf53de542ab6d73cb6a8ba7f03e294d6697d974c","_cell_guid":"b594649d-ac37-4ddb-bb81-ede195264b96"},"source":"# Building Keras Model"},{"cell_type":"code","metadata":{"_uuid":"c8399d04c61b37c9daed1095438e6db999fcbdb2","_cell_guid":"9f7d6bc5-f3c3-4fad-ac82-86017410a496","collapsed":true},"outputs":[],"execution_count":null,"source":"#KERAS MODEL DEFINITION\nfrom keras.layers import Input, Dropout, Dense, BatchNormalization, \\\n    Activation, concatenate, GRU, Embedding, Flatten\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping#, TensorBoard\nfrom keras import backend as K\nfrom keras import optimizers\nfrom keras import initializers\n\ndef rmsle(y, y_pred):\n    import math\n    assert len(y) == len(y_pred)\n    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 \\\n              for i, pred in enumerate(y_pred)]\n    return (sum(to_sum) * (1.0/len(y))) ** 0.5\n\ndr = 0.25\n\ndef get_model():\n    #params\n    dr_r = dr\n    \n    #Inputs\n    name_onehot = Input(shape=[X_train[\"name_onehot\"].shape[1]], name=\"name_onehot\")\n    item_description_onehot = Input(shape=[X_train[\"item_description_onehot\"].shape[1]], \n                                    name=\"item_description_onehot\")\n    brand = Input(shape=[1], name=\"brand\")\n    category = Input(shape=[1], name=\"category\")\n    category_split = Input(shape=[X_train[\"category_split\"].shape[1]], name=\"category_split\")\n    category_name_onehot = Input(shape=[X_train[\"category_name_onehot\"].shape[1]], \n                          name=\"category_name_onehot\")\n    item_condition = Input(shape=[1], name=\"item_condition\")\n    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n    \n    #Embeddings layers\n    emb_size = 60\n    \n    emb_name_onehot = Embedding(MAX_TEXT, emb_size//3)(name_onehot)\n    emb_item_description_onehot = Embedding(MAX_TEXT, emb_size)(item_description_onehot)\n    emb_category_name_onehot = Embedding(MAX_TEXT, emb_size//3)(category_name_onehot)\n    emb_category_split = Embedding(MAX_CATEGORY_SPLIT, emb_size//2)(category_split)\n    emb_brand = Embedding(MAX_BRAND, 10)(brand)\n    emb_category = Embedding(MAX_CATEGORY, 10)(category)\n    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n    \n    rnn_layer1 = GRU(16) (emb_item_description_onehot)\n    rnn_layer2 = GRU(8) (emb_category_name_onehot)\n    rnn_layer3 = GRU(8) (emb_category_split)\n    rnn_layer4 = GRU(8) (emb_name_onehot)\n    \n    #main layer\n    main_l = concatenate([\n        Flatten() (emb_brand)\n        , Flatten() (emb_category)\n        , Flatten() (emb_item_condition)\n        , rnn_layer1\n        , rnn_layer2\n        , rnn_layer3\n        , rnn_layer4\n        , num_vars\n    ])\n    main_l = Dropout(dr)(Dense(512,activation='relu') (main_l))\n    main_l = Dropout(dr)(Dense(64,activation='relu') (main_l))\n    main_l = Dropout(dr)(Dense(32,activation='relu') (main_l))\n    \n    #output\n    output = Dense(1,activation=\"linear\") (main_l)\n    \n    #model\n    model = Model([name_onehot, item_description_onehot, brand\n                   , category, category_name_onehot, category_split\n                   , item_condition, num_vars], output)\n    #optimizer = optimizers.RMSprop()\n    optimizer = optimizers.Adam()\n    model.compile(loss=\"mse\", \n                  optimizer=optimizer)\n    return model\n\ndef eval_model(model):\n    val_preds = model.predict(X_valid)\n    val_preds = np.expm1(val_preds)\n    \n    y_true = np.array(dvalid.price.values)\n    y_pred = val_preds[:, 0]\n    v_rmsle = rmsle(y_true, y_pred)\n    print(\" RMSLE error on dev test: \"+str(v_rmsle))\n    return v_rmsle\n#fin_lr=init_lr * (1/(1+decay))**(steps-1)\nexp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n\nprint('[{}] Finished DEFINEING MODEL...'.format(time.time() - start_time))"},{"cell_type":"markdown","metadata":{"_uuid":"9e2343c16ad8f19360d29e1bdf106062dcea71d8","_cell_guid":"d7a4a62e-1bf0-425b-8a41-c9b4f3c09f99"},"source":"# Train"},{"cell_type":"code","metadata":{"_uuid":"8f2e0230b773ecc5315ffe6e70a3144c18e5920b","_cell_guid":"eb6cdfd6-2257-4959-8f0a-aba624880ef9","collapsed":true},"outputs":[],"execution_count":null,"source":"gc.collect()\n#FITTING THE MODEL\nepochs = 2\nBATCH_SIZE = 512 * 3\nsteps = int(len(X_train['name_onehot'])/BATCH_SIZE) * epochs\nlr_init, lr_fin = 0.013, 0.009\nlr_decay = exp_decay(lr_init, lr_fin, steps)\nlog_subdir = '_'.join(['ep', str(epochs),\n                    'bs', str(BATCH_SIZE),\n                    'lrI', str(lr_init),\n                    'lrF', str(lr_fin),\n                    'dr', str(dr)])\n\nmodel = get_model()\nK.set_value(model.optimizer.lr, lr_init)\nK.set_value(model.optimizer.decay, lr_decay)\n\nhistory = model.fit(X_train, dtrain.target\n                    , epochs=epochs\n                    , batch_size=BATCH_SIZE\n                    , validation_split=0.01\n                    #, callbacks=[TensorBoard('./logs/'+log_subdir)]\n                    , verbose=10\n                    )\n\nprint('[{}] Finished FITTING MODEL...'.format(time.time() - start_time))\n#EVLUEATE THE MODEL ON DEV TEST\nv_rmsle = eval_model(model)\nprint('[{}] Finished predicting valid set...'.format(time.time() - start_time))"},{"cell_type":"markdown","metadata":{"_uuid":"ce1feb3103060791ae5bd70ea271dca5553c8b5d","_cell_guid":"2583cac6-fbbe-4531-8147-12faa6b42029"},"source":"# Pred"},{"cell_type":"code","metadata":{"_uuid":"a668cdd99d4a2da74638e641843bc1b0a8727324","_cell_guid":"6131d97e-2fa2-4eb2-b4db-94bd4f0880e7","collapsed":true},"outputs":[],"execution_count":null,"source":"#CREATE PREDICTIONS\npreds = model.predict(X_test, batch_size=BATCH_SIZE)\npreds = np.expm1(preds)\nprint('[{}] Finished predicting test set...'.format(time.time() - start_time))\nsubmission = df_test[[\"test_id\"]]\nsubmission[\"price\"] = preds\nsubmission.to_csv(\"./myNN\"+log_subdir+\"_{:.6}.csv\".format(v_rmsle), index=False)\nprint('[{}] Finished submission...'.format(time.time() - start_time))"},{"cell_type":"code","metadata":{"_uuid":"c9b8bf267bf0d0105ae7fb308433164cf56dd076","_cell_guid":"40483e4b-72ea-4919-8e5f-55ef54c9cbc6","collapsed":true},"outputs":[],"execution_count":null,"source":""}],"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"version":"3.6.4","file_extension":".py","name":"python","nbconvert_exporter":"python","mimetype":"text/x-python","pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4}