{"cells":[{"metadata":{"_uuid":"d9c4e75689c538b41aad45753aba45d601011830","_cell_guid":"049dad6e-bee5-4822-9e0e-ae11b0e7f26a"},"cell_type":"markdown","source":"# **Introduction**\n\nThis is an initial Explanatory Data Analysis for the [Mercari Price Suggestion Challenge](https://www.kaggle.com/c/mercari-price-suggestion-challenge#description) with matplotlib. [bokeh](https://bokeh.pydata.org/en/latest/) and [Plot.ly](https://plot.ly/feed/) - a visualization tool that creates beautiful interactive plots and dashboards.  The competition is hosted by Mercari, the biggest Japanese community-powered shopping app with the main objective to predict an accurate price that Mercari should suggest to its sellers, given the item's information. \n\n***Update***: The abundant amount of food from my family's Thanksgiving dinner has really energized me to continue working on this model. I decided to dive deeper into the NLP analysis and found an amazing tutorial by Ahmed BESBES. The framework below is  based on his [source code](https://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html).  It provides guidance on pre-processing documents and  machine learning techniques (K-means and LDA) to clustering topics.  So that this kernel will be divided into 2 parts: \n\n1. Explanatory Data Analysis \n2. Text Processing  \n    2.1. Tokenizing and  tf-idf algorithm  \n    2.2. K-means Clustering  \n    2.3. Latent Dirichlet Allocation (LDA)  / Topic Modelling\n "},{"metadata":{"_kg_hide-input":true,"_uuid":"af187b17315c15081fcdcdc99942db794844d90d","_cell_guid":"4f211658-d449-4052-87c4-0ab5d10545e8","collapsed":true,"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"import nltk\nimport string\nimport re\nimport numpy as np\nimport pandas as pd\nimport pickle\n#import lda\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\")\n\nfrom nltk.stem.porter import *\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction import stop_words\n\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n%matplotlib inline\n\nimport bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.plotting import figure, show, output_notebook\n#from bokeh.transform import factor_cmap\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport logging\nlogging.getLogger(\"lda\").setLevel(logging.WARNING)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"d82faf4720f0574d2be682880d2f662b96918ffa","_cell_guid":"5fa6bae8-c5fc-4c1a-8339-acfa14aac199","_kg_hide-output":true},"cell_type":"markdown","source":"# **Exploratory Data Analysis**\nOn the first look at the data, besides the unique identifier (item_id), there are 7 variables in this model. This notebook will sequentially go through each of them with a brief statistical summary. \n\n1. **Numerical/Continuous Features**\n    1. price: the item's final bidding price. This will be our reponse / independent variable that we need to predict in the test set\n    2. shipping cost     \n \n1. **Categorical Features**: \n    1. shipping cost: A binary indicator, 1 if shipping fee is paid by seller and 0 if it's paid by buyer\n    2. item_condition_id: The condition of the items provided by the seller\n    1. name: The item's name\n    2. brand_name: The item's producer brand name\n    2. category_name: The item's single or multiple categories that are separated by \"\\\" \n    3. item_description: A short description on the item that may include removed words, flagged by [rm]"},{"metadata":{"_kg_hide-input":true,"_uuid":"122a209567af15b4422a75b50bfb8bb4c7b5fdfe","_cell_guid":"fc7f8ff2-a9f2-488d-b9b8-0c47c1d64a9d","collapsed":true,"trusted":false},"cell_type":"code","source":"PATH = \"../input/\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca7edb8850b4b2d5dfbe044e08cc709886348c2e","_cell_guid":"6f1e2926-af11-4801-9df9-943162e72478","collapsed":true,"trusted":false},"cell_type":"code","source":"train = pd.read_csv(f'{PATH}train.tsv', sep='\\t')\ntest = pd.read_csv(f'{PATH}test.tsv', sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a6310c7717bb6993ea79e4057f6d5a75e4c3c2a","_cell_guid":"5fe57507-8f62-4e90-afb4-f78d5ee174a0","collapsed":true,"trusted":false},"cell_type":"code","source":"# size of training and dataset\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b247cf361d89fc2431adc5150d0c4533691f71b2","_cell_guid":"f58f905d-8279-42fa-8877-862c1f23aff8","collapsed":true,"trusted":false},"cell_type":"code","source":"# different data types in the dataset: categorical (strings) and numeric\ntrain.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04c5b96c1de907bf9dc259a0a8d2e4df4f02375a","_cell_guid":"8fd3b4d2-2bc9-47dc-8396-5980aad64a1f","collapsed":true,"trusted":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3dc222ed04ddb74c4cf4081192150dcd7de17d89","_cell_guid":"74b924c4-fecc-4a5b-b112-87625297d3b9"},"cell_type":"markdown","source":"## Target Variable: **Price**"},{"metadata":{"_uuid":"15c4c0eb2b7b138f52fba95cd11915ce4e15539f","_cell_guid":"cb846bc6-01be-4f61-8c2b-49a6af0f8f29"},"cell_type":"markdown","source":"The next standard check is with our response or target variables, which in this case is the `price` we are suggesting to the Mercari's marketplace sellers.  The median price of all the items in the training is about \\$267 but given the existence of some extreme values of over \\$100 and the maximum at \\$2,009, the distribution of the variables is heavily skewed to the left. So let's make log-transformation on the price (we added +1 to the value before the transformation to avoid zero and negative values)."},{"metadata":{"_uuid":"b3d3c18e99c7e49e81f3e51221ba26483f6ead81","_cell_guid":"621d00c1-6f22-42bc-8709-fd77399b6492","collapsed":true,"trusted":false},"cell_type":"code","source":"train.price.describe()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"314e556a16f6810ea01536c135d62e3e39d88431","_cell_guid":"ba824e0f-b23a-4e70-b621-fc1d979e57d6","collapsed":true,"trusted":false},"cell_type":"code","source":"plt.subplot(1, 2, 1)\n(train['price']).plot.hist(bins=50, figsize=(20,10), edgecolor='white',range=[0,250])\nplt.xlabel('price+', fontsize=17)\nplt.ylabel('frequency', fontsize=17)\nplt.tick_params(labelsize=15)\nplt.title('Price Distribution - Training Set', fontsize=17)\n\nplt.subplot(1, 2, 2)\nnp.log(train['price']+1).plot.hist(bins=50, figsize=(20,10), edgecolor='white')\nplt.xlabel('log(price+1)', fontsize=17)\nplt.ylabel('frequency', fontsize=17)\nplt.tick_params(labelsize=15)\nplt.title('Log(Price) Distribution - Training Set', fontsize=17)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ead68391b89acaa393f9f694c08addd4bd3a58a","_cell_guid":"91de0c69-7cb3-4503-aedf-9c396a1a8f19","collapsed":true},"cell_type":"markdown","source":"## **Shipping**\n\nThe shipping cost burden is decently splitted between sellers and buyers with more than half of the items' shipping fees are paid by the sellers (55%). In addition, the average price paid by users who have to pay for shipping fees is lower than those that don't require additional shipping cost. This matches with our perception that the sellers need a lower price to compensate for the additional shipping."},{"metadata":{"_kg_hide-input":true,"_uuid":"34df65c2eaa227091d1101a17ee43a99eae5e639","_cell_guid":"b3d38506-ebe3-4951-8fb0-9dd3de6c0851","collapsed":true,"trusted":false},"cell_type":"code","source":"train.shipping.value_counts()/len(train)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"938ea18ba993fd6f9296e3c1d1d541a728219622","_cell_guid":"956793f2-0e83-4de7-ac47-59a266e2a771","collapsed":true,"_kg_hide-output":false,"trusted":false},"cell_type":"code","source":"prc_shipBySeller = train.loc[train.shipping==1, 'price']\nprc_shipByBuyer = train.loc[train.shipping==0, 'price']","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"cc24211ac13b7d8bd88c1061e996826cfc3da597","_cell_guid":"b7c2c064-4561-4831-9cf7-02910a8cde00","collapsed":true,"trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,10))\nax.hist(np.log(prc_shipBySeller+1), color='#8CB4E1', alpha=1.0, bins=50,\n       label='Price when Seller pays Shipping')\nax.hist(np.log(prc_shipByBuyer+1), color='#007D00', alpha=0.7, bins=50,\n       label='Price when Buyer pays Shipping')\nax.set(title='Histogram Comparison', ylabel='% of Dataset in Bin')\nplt.xlabel('log(price+1)', fontsize=17)\nplt.ylabel('frequency', fontsize=17)\nplt.title('Price Distribution by Shipping Type', fontsize=17)\nplt.tick_params(labelsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c275d349c49e2dd0fc1ec10bd416900875f083a2","_cell_guid":"c19f85dd-6767-4329-88f2-75aae64623e5"},"cell_type":"markdown","source":"## **Item Category**\n\nThere are about **1,287** unique categories but among each of them, we will always see a main/general category firstly, followed by two more particular subcategories (e.g. Beauty/Makeup/Face or Lips). In adidition, there are about 6,327 items that do not have a category labels. Let's split the categories into three different columns. We will see later that this information is actually quite important from the seller's point of view and how we handle the missing information in the `brand_name` column will impact the model's prediction. "},{"metadata":{"_kg_hide-input":true,"_uuid":"944114da84aa5c946b0d102f908c2d303350d401","_cell_guid":"b73488c6-789e-4dda-a21c-cc661a715db7","collapsed":true,"trusted":false},"cell_type":"code","source":"print(\"There are %d unique values in the category column.\" % train['category_name'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_uuid":"07f9fdeb6dd4bd7e3a075a4cd5d779bbe2a64320","_cell_guid":"ac468824-7439-415e-b119-c24ff750e631","collapsed":true,"trusted":false},"cell_type":"code","source":"# TOP 5 RAW CATEGORIES\ntrain['category_name'].value_counts()[:5]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"e79d9bae48ea3daf13d3e83ea863d1eb51658fa8","_cell_guid":"a17d5bbc-f2b9-48f4-bf1e-8031000c992d","collapsed":true,"trusted":false},"cell_type":"code","source":"# missing categories\nprint(\"There are %d items that do not have a label.\" % train['category_name'].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_uuid":"f51ffaa9c0ffaa0cd04a5c2cfd7d76417971d530","_cell_guid":"ee8e9ba1-dd6b-493c-b97f-507eca5287fb","collapsed":true,"_kg_hide-output":false,"trusted":false},"cell_type":"code","source":"# reference: BuryBuryZymon at https://www.kaggle.com/maheshdadhich/i-will-sell-everything-for-free-0-55\ndef split_cat(text):\n    try: return text.split(\"/\")\n    except: return (\"No Label\", \"No Label\", \"No Label\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63f7d7f0f87b5599d3b970382432010a3968aee4","_cell_guid":"74e722fb-4bf1-4540-b5e4-f9f7bba64dd4","collapsed":true,"trusted":false},"cell_type":"code","source":"train['general_cat'], train['subcat_1'], train['subcat_2'] = \\\nzip(*train['category_name'].apply(lambda x: split_cat(x)))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"5e5ba730355ce75bf2dce3defb1c4fe1f80ff9fb","_cell_guid":"e35d96ec-fa00-4f49-b5f0-e8df56f466ff","collapsed":true,"_kg_hide-output":false,"trusted":false},"cell_type":"code","source":"# repeat the same step for the test set\ntest['general_cat'], test['subcat_1'], test['subcat_2'] = \\\nzip(*test['category_name'].apply(lambda x: split_cat(x)))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_uuid":"d191e5c7ed101922aed6c3f9734bf34508f01912","_cell_guid":"64559f3b-b70b-45a6-ba10-975993c4926f","collapsed":true,"_kg_hide-output":false,"trusted":false},"cell_type":"code","source":"print(\"There are %d unique first sub-categories.\" % train['subcat_1'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_uuid":"0d3250b78e86eddc7f621c1e7aabbcf7f762598d","_cell_guid":"0412ea5b-e8fc-4154-806c-043d34aae44c","collapsed":true,"_kg_hide-output":false,"trusted":false},"cell_type":"code","source":"print(\"There are %d unique second sub-categories.\" % train['subcat_2'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a03eb0d50c3103a7a3b2fa315254dfc9b5b6a894","_cell_guid":"d6e280f0-885e-43bb-bda7-8ed9fe0987ba"},"cell_type":"markdown","source":"Overall, we have  **7 main categories** (114 in the first sub-categories and 871 second sub-categories): women's and beauty items as the two most popular categories (more than 50% of the observations), followed by kids and electronics. "},{"metadata":{"_kg_hide-input":false,"_uuid":"427123e746f47e763bb351e9abe83041985a05ca","_cell_guid":"1977140a-f0ee-46af-8561-7fa5f3de43f2","collapsed":true,"_kg_hide-output":false,"trusted":false},"cell_type":"code","source":"x = train['general_cat'].value_counts().index.values.astype('str')\ny = train['general_cat'].value_counts().values\npct = [(\"%.2f\"%(v*100))+\"%\"for v in (y/len(train))]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_uuid":"95ff30e205b02024275d465d489648f0cc64cdb2","_cell_guid":"e40b2cfb-02e9-46fe-a8cf-75bcb3e7eec9","collapsed":true,"_kg_hide-output":false,"trusted":false},"cell_type":"code","source":"trace1 = go.Bar(x=x, y=y, text=pct)\nlayout = dict(title= 'Number of Items by Main Category',\n              yaxis = dict(title='Count'),\n              xaxis = dict(title='Category'))\nfig=dict(data=[trace1], layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_uuid":"87c01e755ab62d552d584822460ae80e58843966","_cell_guid":"ee50ae89-4496-46d0-96f4-6f9f9d5407cc","collapsed":true,"_kg_hide-output":false,"trusted":false},"cell_type":"code","source":"x = train['subcat_1'].value_counts().index.values.astype('str')[:15]\ny = train['subcat_1'].value_counts().values[:15]\npct = [(\"%.2f\"%(v*100))+\"%\"for v in (y/len(train))][:15]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"6fcb8ee80064d4c671c9e0f94d0025ad6ee4ff95","_cell_guid":"2cbdb24a-efc5-478f-b3d2-3acad54abbd1","collapsed":true,"trusted":false},"cell_type":"code","source":"trace1 = go.Bar(x=x, y=y, text=pct,\n                marker=dict(\n                color = y,colorscale='Portland',showscale=True,\n                reversescale = False\n                ))\nlayout = dict(title= 'Number of Items by Sub Category (Top 15)',\n              yaxis = dict(title='Count'),\n              xaxis = dict(title='SubCategory'))\nfig=dict(data=[trace1], layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e66bf56801446d3c32bef84f7ef5820b331776c7","_cell_guid":"59d504e8-a2cb-4c10-ae53-d5269000d937"},"cell_type":"markdown","source":"From the pricing (log of price) point of view, all the categories are pretty well distributed, with no category with an extraordinary pricing point "},{"metadata":{"_kg_hide-input":true,"_uuid":"788c3e10ed1ce263d2f2f2033e29cd01ed09454f","_cell_guid":"94892e05-8c33-4ff3-84aa-24e39b4075ee","collapsed":true,"_kg_hide-output":false,"trusted":false},"cell_type":"code","source":"general_cats = train['general_cat'].unique()\nx = [train.loc[train['general_cat']==cat, 'price'] for cat in general_cats]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"da641275501bfb5c5e18aa18ddb964a61fe6ad1c","_cell_guid":"3e171144-9090-4b21-949c-a4ac42879a8e","collapsed":true,"_kg_hide-output":false,"trusted":false},"cell_type":"code","source":"data = [go.Box(x=np.log(x[i]+1), name=general_cats[i]) for i in range(len(general_cats))]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"dba9498362d3b574d2d0a7bc2010f99369b99338","_cell_guid":"5b7dadb9-3b40-4883-bb07-942f83297364","collapsed":true,"trusted":false},"cell_type":"code","source":"layout = dict(title=\"Price Distribution by General Category\",\n              yaxis = dict(title='Frequency'),\n              xaxis = dict(title='Category'))\nfig = dict(data=data, layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d52355679d02f3f7d629c25384dc5751c6023de9","_cell_guid":"a2f9d3e4-9d8e-480d-8deb-465310fc8cdd"},"cell_type":"markdown","source":"## **Brand Name**"},{"metadata":{"_uuid":"5d75b2bf16078275fbd938a1483de3b526fa4c2c","_cell_guid":"352301ea-175f-435f-8b83-dad50cb390c0","collapsed":true,"trusted":false},"cell_type":"code","source":"print(\"There are %d unique brand names in the training dataset.\" % train['brand_name'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"39dfac5c98e102b2637137032d1a17c055d8b31e","_cell_guid":"795e315b-5c5b-481c-8302-7b4bfca0455d","collapsed":true,"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"x = train['brand_name'].value_counts().index.values.astype('str')[:10]\ny = train['brand_name'].value_counts().values[:10]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"3df62861a822c99f3c42800c8e64e39f4276f9ab","_cell_guid":"bd9a1b9b-c89d-4451-b1ca-52053cd6a611","collapsed":true,"scrolled":true,"trusted":false},"cell_type":"code","source":"# trace1 = go.Bar(x=x, y=y, \n#                 marker=dict(\n#                 color = y,colorscale='Portland',showscale=True,\n#                 reversescale = False\n#                 ))\n# layout = dict(title= 'Top 10 Brand by Number of Items',\n#               yaxis = dict(title='Brand Name'),\n#               xaxis = dict(title='Count'))\n# fig=dict(data=[trace1], layout=layout)\n# py.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2473fadc100c0ee057ebec8252dbd861751a947","_cell_guid":"19d21a67-0b3b-4ade-8dc0-15003f03a146"},"cell_type":"markdown","source":"## **Item Description**"},{"metadata":{"_uuid":"21c36e9a9a9f4ab2d66ac1a5f193051a8e894edb","_cell_guid":"d643eeaa-e3d7-4148-8418-2a9f9d1fa5cc"},"cell_type":"markdown","source":"It will be more challenging to parse through this particular item since it's unstructured data. Does it mean a more detailed and lengthy description will result in a higher bidding price? We will strip out all punctuations, remove some english stop words (i.e. redundant words such as \"a\", \"the\", etc.) and any other words with a length less than 3: "},{"metadata":{"_uuid":"40d28e05a35b4ef4fe11c436200cc5e40f5d561e","_cell_guid":"3d536351-69ac-41a5-ad40-3114a24bd326","collapsed":true,"trusted":false},"cell_type":"code","source":"def wordCount(text):\n    # convert to lower case and strip regex\n    try:\n         # convert to lower case and strip regex\n        text = text.lower()\n        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n        txt = regex.sub(\" \", text)\n        # tokenize\n        # words = nltk.word_tokenize(clean_txt)\n        # remove words in stop words\n        words = [w for w in txt.split(\" \") \\\n                 if not w in stop_words.ENGLISH_STOP_WORDS and len(w)>3]\n        return len(words)\n    except: \n        return 0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e72483da02ea3667ba3f14db0f0996eabc980861","_cell_guid":"b71db1b9-756c-4dc3-8445-94f39d697d8e","collapsed":true,"trusted":false},"cell_type":"code","source":"# add a column of word counts to both the training and test set\ntrain['desc_len'] = train['item_description'].apply(lambda x: wordCount(x))\ntest['desc_len'] = test['item_description'].apply(lambda x: wordCount(x))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79ebf5e7b0e03c993a3b4c53c2abf80b2c6d17b6","_cell_guid":"511aec70-6890-4504-8e43-742056a47733","collapsed":true,"trusted":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7e8accc896ef4a04e833fb2abca498e715e3785","_cell_guid":"d935df86-7977-434f-ad53-0c1c9aa44a20","collapsed":true,"trusted":false},"cell_type":"code","source":"df = train.groupby('desc_len')['price'].mean().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"6b30d788953627a01463ae3422f890f675c87d7b","_cell_guid":"e34e4bf9-073c-4ff4-a052-1f76f4283ea3","collapsed":true,"trusted":false},"cell_type":"code","source":"trace1 = go.Scatter(\n    x = df['desc_len'],\n    y = np.log(df['price']+1),\n    mode = 'lines+markers',\n    name = 'lines+markers'\n)\nlayout = dict(title= 'Average Log(Price) by Description Length',\n              yaxis = dict(title='Average Log(Price)'),\n              xaxis = dict(title='Description Length'))\nfig=dict(data=[trace1], layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52732f7292faf4e2fdd629f723d28a3bdc34ec93","_cell_guid":"cd93bed3-431f-4d42-bd98-ff287d29daa6","collapsed":true},"cell_type":"markdown","source":"We also need to check if there are any missing values in the item description (4 observations don't have a description) andl remove those observations from our training set."},{"metadata":{"_kg_hide-input":true,"_uuid":"27d17728a67f5ef8203f16164edcc5f939c5971d","_cell_guid":"82d38cb1-ad25-4479-bf35-7df79eef5a23","collapsed":true,"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"train.item_description.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"165c1267209c1f497224690cf9f4ca2829c571a1","_cell_guid":"eedf987b-9e21-4e7b-940a-ef0bd0e471c4","collapsed":true,"trusted":false},"cell_type":"code","source":"# remove missing values in item description\ntrain = train[pd.notnull(train['item_description'])]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"c12117ac987cbd79d6d7612c8e6e26e9c83e3e72","_cell_guid":"b90e6701-76be-476c-b720-380d631bfd03","collapsed":true,"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"# create a dictionary of words for each category\ncat_desc = dict()\nfor cat in general_cats: \n    text = \" \".join(train.loc[train['general_cat']==cat, 'item_description'].values)\n    cat_desc[cat] = tokenize(text)\n\n# flat list of all words combined\nflat_lst = [item for sublist in list(cat_desc.values()) for item in sublist]\nallWordsCount = Counter(flat_lst)\nall_top10 = allWordsCount.most_common(20)\nx = [w[0] for w in all_top10]\ny = [w[1] for w in all_top10]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"d2748e1e0f07eb31c68c907ddf49e96595a32e97","_cell_guid":"492f17f5-952e-4561-9c45-5722fce7afe3","collapsed":true,"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"trace1 = go.Bar(x=x, y=y, text=pct)\nlayout = dict(title= 'Word Frequency',\n              yaxis = dict(title='Count'),\n              xaxis = dict(title='Word'))\nfig=dict(data=[trace1], layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1d170c1d5d760b485de430f3ab3a3cf4cc1aa8a","_cell_guid":"a95bea2e-e175-43f1-81f2-d93ee6e16d2c"},"cell_type":"markdown","source":"If we look at the most common words by category, we could also see that, ***size***, ***free*** and ***shipping*** is very commonly used by the sellers, probably with the intention to attract customers, which is contradictory to what  we have shown previously that there is little correlation between the two variables `price` and `shipping` (or shipping fees do not account for a differentiation in prices). ***Brand names*** also played quite an important role - it's one of the most popular in all four categories.  "},{"metadata":{"_uuid":"c4c2be486e56d6d9ec662f913856fac5ea65570e","_cell_guid":"5a657436-9c24-4fbd-aada-eaa1330a60a2","collapsed":true},"cell_type":"markdown","source":"# **Text Processing - Item Description**\n*\nThe following section is based on the tutorial at https://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html*"},{"metadata":{"_uuid":"5634aee89798082ea4025145d560629e32c026aa","_cell_guid":"30158f97-7034-4d84-8d51-c7e8ca6ac56e"},"cell_type":"markdown","source":"## **Pre-processing:  tokenization**\n\nMost of the time, the first steps of an NLP project is to **\"tokenize\"** your documents, which main purpose is to normalize our texts. The three fundamental stages will usually include: \n* break the descriptions into sentences and then break the sentences into tokens\n* remove punctuation and stop words\n* lowercase the tokens\n* herein, I will also only consider words that have length equal to or greater than 3 characters"},{"metadata":{"_uuid":"e23a26ab8cccc9cfdf750ddcdfcb9cc1f28475c4","_cell_guid":"79b3df0f-535f-4b91-ab2e-caffbd73a315","collapsed":true,"trusted":false},"cell_type":"code","source":"stop = set(stopwords.words('english'))\ndef tokenize(text):\n    \"\"\"\n    sent_tokenize(): segment text into sentences\n    word_tokenize(): break sentences into words\n    \"\"\"\n    try: \n        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n        text = regex.sub(\" \", text) # remove punctuation\n        \n        tokens_ = [word_tokenize(s) for s in sent_tokenize(text)]\n        tokens = []\n        for token_by_sent in tokens_:\n            tokens += token_by_sent\n        tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n        filtered_tokens = [w for w in tokens if re.search('[a-zA-Z]', w)]\n        filtered_tokens = [w.lower() for w in filtered_tokens if len(w)>=3]\n        \n        return filtered_tokens\n            \n    except TypeError as e: print(text,e)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32ddfee0de0f5f64bdae8944e9eccfa0d31751b3","_cell_guid":"7250efc8-77ba-40c1-bbad-f716bc6eeb6d","collapsed":true,"trusted":false},"cell_type":"code","source":"# apply the tokenizer into the item descriptipn column\ntrain['tokens'] = train['item_description'].map(tokenize)\ntest['tokens'] = test['item_description'].map(tokenize)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"98cb8b403c7ea7c47663caf98e4259fb85c7dd72","_cell_guid":"b8ad8934-bbaf-4db7-8da4-fdb70fdf17cc","collapsed":true,"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"train.reset_index(drop=True, inplace=True)\ntest.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"548e888a1839b2c3c3d93e7578357b20af095b3d","_cell_guid":"f4fb8dcd-3f94-4fef-908a-42fc864be42c"},"cell_type":"markdown","source":"Let's look at the examples of if the tokenizer did a good job in cleaning up our descriptions"},{"metadata":{"_uuid":"57a867fd01f737b7a60e04a5e8ab6e59b3c4a150","_cell_guid":"74c356c2-e2b8-4dda-b6d5-ccad74d6302a","collapsed":true,"trusted":false},"cell_type":"code","source":"for description, tokens in zip(train['item_description'].head(),\n                              train['tokens'].head()):\n    print('description:', description)\n    print('tokens:', tokens)\n    print()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_uuid":"01942e57c0fcab0894c38e31cff43109a9bce23e","_cell_guid":"643b583b-11a3-45fe-94e4-1f6dcdafb196"},"cell_type":"markdown","source":"We could aso use the package `WordCloud` to easily visualize which words has the highest frequencies within each category:"},{"metadata":{"_uuid":"d17f807ae37a466f5e9410d97c37560670e08a74","_cell_guid":"349888f0-7ffe-44aa-8bf7-d03df78073c3","collapsed":true,"trusted":false},"cell_type":"code","source":"# build dictionary with key=category and values as all the descriptions related.\ncat_desc = dict()\nfor cat in general_cats: \n    text = \" \".join(train.loc[train['general_cat']==cat, 'item_description'].values)\n    cat_desc[cat] = tokenize(text)\n\n\n# find the most common words for the top 4 categories\nwomen100 = Counter(cat_desc['Women']).most_common(100)\nbeauty100 = Counter(cat_desc['Beauty']).most_common(100)\nkids100 = Counter(cat_desc['Kids']).most_common(100)\nelectronics100 = Counter(cat_desc['Electronics']).most_common(100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15b09d32c291e35db0de4c610a61ac3bf732f4a0","_cell_guid":"b2f2274a-d48c-43c6-8349-c1a597967f87","collapsed":true,"trusted":false},"cell_type":"code","source":"def generate_wordcloud(tup):\n    wordcloud = WordCloud(background_color='white',\n                          max_words=50, max_font_size=40,\n                          random_state=42\n                         ).generate(str(tup))\n    return wordcloud","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"974f2954233a235ca805ea393220ac5a1fbd1bfd","_cell_guid":"efd8b7a2-960b-47a7-a25a-a6b5b09362b8","collapsed":true,"trusted":false},"cell_type":"code","source":"fig,axes = plt.subplots(2, 2, figsize=(30, 15))\n\nax = axes[0, 0]\nax.imshow(generate_wordcloud(women100), interpolation=\"bilinear\")\nax.axis('off')\nax.set_title(\"Women Top 100\", fontsize=30)\n\nax = axes[0, 1]\nax.imshow(generate_wordcloud(beauty100))\nax.axis('off')\nax.set_title(\"Beauty Top 100\", fontsize=30)\n\nax = axes[1, 0]\nax.imshow(generate_wordcloud(kids100))\nax.axis('off')\nax.set_title(\"Kids Top 100\", fontsize=30)\n\nax = axes[1, 1]\nax.imshow(generate_wordcloud(electronics100))\nax.axis('off')\nax.set_title(\"Electronic Top 100\", fontsize=30)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"190223022141a75be619b3471e5469eae329cad1","_cell_guid":"e77c6bc3-3a04-4b0f-8a89-31a31878f639"},"cell_type":"markdown","source":"## **Pre-processing:  tf-idf**"},{"metadata":{"_uuid":"3b7d46cfb2279f955b639c68589fd839a46a3c4c","_cell_guid":"76a54be8-3ef9-4184-b5bd-ca535d9abf75"},"cell_type":"markdown","source":"tf-idf is the acronym for **Term Frequencyâ€“inverse Document Frequency**. It quantifies the importance of a particular word in relative to the vocabulary of a collection of documents or corpus. The metric depends on two factors: \n- **Term Frequency**: the occurences of a word in a given document (i.e. bag of words)\n- **Inverse Document Frequency**: the reciprocal number of times a word occurs in a corpus of documents\n\nThink about of it this way: If the word is used extensively in all documents, its existence within a specific document will not be able to provide us much specific information about the document itself. So the second term could be seen as a penalty term that penalizes common words such as \"a\", \"the\", \"and\", etc. tf-idf can therefore, be seen as a weighting scheme for words relevancy in a specific document."},{"metadata":{"_uuid":"3152ded0c36fc76a0e60f9b6aabb0824bb9e4f13","_cell_guid":"11e3a9f9-e94a-45b0-90df-f452727028cd","collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10,\n                             max_features=180000,\n                             tokenizer=tokenize,\n                             ngram_range=(1, 2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37f44a006ca9d2f4f71081f55278759d7a473808","_cell_guid":"7c556ca8-9969-4c5e-af24-f54411732c75","collapsed":true,"trusted":false},"cell_type":"code","source":"all_desc = np.append(train['item_description'].values, test['item_description'].values)\nvz = vectorizer.fit_transform(list(all_desc))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d581a19f324ec33c6bc13ae9c6197087f191f0c0","_cell_guid":"ce40e6f2-c56e-4412-815e-46852ae8e21c"},"cell_type":"markdown","source":"vz is a tfidf matrix where:\n* the number of rows is the total number of descriptions\n* the number of columns is the total number of unique tokens across the descriptions"},{"metadata":{"_uuid":"93a3d6f38fdc6f9eb756ff2a72c422d81191d395","_cell_guid":"bd07b7e0-33cd-4326-a02c-bc0b6660f1e6","collapsed":true,"trusted":false},"cell_type":"code","source":"#  create a dictionary mapping the tokens to their tfidf values\ntfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\ntfidf = pd.DataFrame(columns=['tfidf']).from_dict(\n                    dict(tfidf), orient='index')\ntfidf.columns = ['tfidf']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c27a5ff0f5a57fd4c50c8730aca758c05558426","_cell_guid":"891a9d4b-c684-48e1-9ac2-72dcf84bedd7"},"cell_type":"markdown","source":"Below is the 10 tokens with the lowest tfidf score, which is unsurprisingly, very generic words that we could not use to distinguish one description from another."},{"metadata":{"_uuid":"7b3dafb38d33b46ddb2509cca86b7fae2d4a678f","_cell_guid":"bac2d553-a3e8-48e6-97a8-228e75dd97ae","collapsed":true,"trusted":false},"cell_type":"code","source":"tfidf.sort_values(by=['tfidf'], ascending=True).head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea113f51e664f454b087db59757ad538f9632b9b","_cell_guid":"25b77b22-1e2c-4f37-851f-72b07d4e0c6e"},"cell_type":"markdown","source":"Below is the 10 tokens with the highest tfidf score, which includes words that are a lot specific that by looking at them, we could guess the categories that they belong to: "},{"metadata":{"_uuid":"68a97356bd6afaf78b441d5887f68aad53ac9170","_cell_guid":"e2c31570-8e83-4c88-bc50-dbb0727cc8a0","collapsed":true,"trusted":false},"cell_type":"code","source":"tfidf.sort_values(by=['tfidf'], ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2031b851f6edb5865308149ef18346f43b59a679","_cell_guid":"2540c3fa-61cc-41f8-9542-c0274c949271"},"cell_type":"markdown","source":"Given the high dimension of our tfidf matrix, we need to reduce their dimension using the Singular Value Decomposition (SVD) technique. And to visualize our vocabulary, we could next use t-SNE to reduce the dimension from 50 to 2. t-SNE is more suitable for dimensionality reduction to 2 or 3. \n\n### **t-Distributed Stochastic Neighbor Embedding (t-SNE)**\n\nt-SNE is a technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. The goal is to take a set of points in a high-dimensional space and find a representation of those points in a lower-dimensional space, typically the 2D plane. It is based on probability distributions with random walk on neighborhood graphs to find the structure within the data. But since t-SNE complexity is significantly high, usually we'd use other high-dimension reduction techniques before applying t-SNE.\n\nFirst, let's take a sample from the both training and testing item's description since t-SNE can take a very long time to execute. We can then reduce the dimension of each vector from to n_components (50) using SVD."},{"metadata":{"_uuid":"12c465edfe5158967bd400c7bfe0318a615ccdfa","_cell_guid":"0e99f022-1ef1-498f-82d4-55a41de11b66","collapsed":true,"trusted":false},"cell_type":"code","source":"trn = train.copy()\ntst = test.copy()\ntrn['is_train'] = 1\ntst['is_train'] = 0\n\nsample_sz = 15000\n\ncombined_df = pd.concat([trn, tst])\ncombined_sample = combined_df.sample(n=sample_sz)\nvz_sample = vectorizer.fit_transform(list(combined_sample['item_description']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f6e6d1e3e5d75062dc7e844b338bf3a33afecba","_cell_guid":"d1fdc518-d325-4fe4-b8a8-9f84a975de03","collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\nn_comp=30\nsvd = TruncatedSVD(n_components=n_comp, random_state=42)\nsvd_tfidf = svd.fit_transform(vz_sample)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92a3d8c0ba36fb567712d4d6a1470fc9ece2db73","_cell_guid":"4c4eded5-ed1f-482b-a83b-02e5aa38a2cd"},"cell_type":"markdown","source":"Now we can reduce the dimension from 50 to 2 using t-SNE!"},{"metadata":{"_uuid":"e2e9aa416aa12531e14911deafbaa5d302f36c67","_cell_guid":"894e4868-099f-47ae-aca2-b78f9ca73bcd","collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.manifold import TSNE\ntsne_model = TSNE(n_components=2, verbose=1, random_state=42, n_iter=500)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"612ee5dcdeafc698702cb094b370827d40dd03e1","_cell_guid":"4f19da97-4d22-4831-8469-ba8b3b099989","collapsed":true,"trusted":false},"cell_type":"code","source":"tsne_tfidf = tsne_model.fit_transform(svd_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fde4cd911968c645202d9e165ae62417ac376622","_cell_guid":"4f8b10c5-f01d-436a-8cc2-aa5048d71234"},"cell_type":"markdown","source":"It's now possible to visualize our data points. Note that the deviation as well as the size of the clusters imply little information  in t-SNE."},{"metadata":{"_kg_hide-input":false,"_uuid":"70e0a5e9402a42191a3ef5336d186b386a3885c2","_cell_guid":"87aff740-4718-4321-8468-573945e6c26e","collapsed":true,"trusted":false},"cell_type":"code","source":"output_notebook()\nplot_tfidf = bp.figure(plot_width=700, plot_height=600,\n                       title=\"tf-idf clustering of the item description\",\n    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n    x_axis_type=None, y_axis_type=None, min_border=1)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"b6195acc2f32fe21fbf863f4127d7993b4f4994d","_cell_guid":"74ea4283-9ab8-40d6-829a-5dc01e6b31f1","collapsed":true,"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"combined_sample.reset_index(inplace=True, drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea95c9e2c90336c2fdd2f89cf3ccffab3f9562d1","_cell_guid":"5976a500-8ec0-44f7-b7cd-cbc81a898989","collapsed":true,"trusted":false},"cell_type":"code","source":"tfidf_df = pd.DataFrame(tsne_tfidf, columns=['x', 'y'])\ntfidf_df['description'] = combined_sample['item_description']\ntfidf_df['tokens'] = combined_sample['tokens']\ntfidf_df['category'] = combined_sample['general_cat']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8011ce69a3ea6833ac871691ad1292ad80f62348","_cell_guid":"f0a19e78-1db9-4f69-a951-69fe9561f7bb","collapsed":true,"trusted":false},"cell_type":"code","source":"plot_tfidf.scatter(x='x', y='y', source=tfidf_df, alpha=0.7)\nhover = plot_tfidf.select(dict(type=HoverTool))\nhover.tooltips={\"description\": \"@description\", \"tokens\": \"@tokens\", \"category\":\"@category\"}\nshow(plot_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ddd57a9aad405e7ad03b3939fc58502697aa56e","_cell_guid":"9baa48f4-895c-46e8-b0c6-ab3a6f719cdc"},"cell_type":"markdown","source":"## **K-Means Clustering**\n\nK-means clustering obejctive is to minimize the average squared Euclidean distance of the document / description from their cluster centroids. "},{"metadata":{"_uuid":"efbd785dd4814c23ff2ab7ef2ba9dd06b21c154a","_cell_guid":"4145bb78-54ce-44b7-9014-e0d26e932f6a","collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.cluster import MiniBatchKMeans\n\nnum_clusters = 30 # need to be selected wisely\nkmeans_model = MiniBatchKMeans(n_clusters=num_clusters,\n                               init='k-means++',\n                               n_init=1,\n                               init_size=1000, batch_size=1000, verbose=0, max_iter=1000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf1cc88318c78e7b11e3c1ac7edbc79713661293","_cell_guid":"e1251d38-9813-4d12-bde9-6e3e8f16914b","collapsed":true,"trusted":false},"cell_type":"code","source":"kmeans = kmeans_model.fit(vz)\nkmeans_clusters = kmeans.predict(vz)\nkmeans_distances = kmeans.transform(vz)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_uuid":"aadb73b5ab0a3a691610357e128bf275a6b3312d","_cell_guid":"3b2a5a0a-670e-44b3-824e-bb32c3c8a849","collapsed":true,"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"sorted_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\nterms = vectorizer.get_feature_names()\n\nfor i in range(num_clusters):\n    print(\"Cluster %d:\" % i)\n    aux = ''\n    for j in sorted_centroids[i, :10]:\n        aux += terms[j] + ' | '\n    print(aux)\n    print() ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3763e33b915ed577f02d035028ee2c13035f2892","_cell_guid":"30af7fe9-c3f4-4e0e-88cd-16a03e0e2bf0"},"cell_type":"markdown","source":"In order to plot these clusters, first we will need to reduce the dimension of the distances to 2 using tsne: "},{"metadata":{"_kg_hide-output":true,"_uuid":"3d14784c614bb96c0ea626c1a98d8aef706206ed","_cell_guid":"909da1fa-d307-4a4a-a162-cc44820fd094","collapsed":true,"trusted":false},"cell_type":"code","source":"# repeat the same steps for the sample\nkmeans = kmeans_model.fit(vz_sample)\nkmeans_clusters = kmeans.predict(vz_sample)\nkmeans_distances = kmeans.transform(vz_sample)\n# reduce dimension to 2 using tsne\ntsne_kmeans = tsne_model.fit_transform(kmeans_distances)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"59438d7cdb5f767b6746e6924cc6cb1a6a46a62a","_cell_guid":"dd55dce0-2b65-4547-8e83-2f3f8c1aa841","collapsed":true,"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"colormap = np.array([\"#6d8dca\", \"#69de53\", \"#723bca\", \"#c3e14c\", \"#c84dc9\", \"#68af4e\", \"#6e6cd5\",\n\"#e3be38\", \"#4e2d7c\", \"#5fdfa8\", \"#d34690\", \"#3f6d31\", \"#d44427\", \"#7fcdd8\", \"#cb4053\", \"#5e9981\",\n\"#803a62\", \"#9b9e39\", \"#c88cca\", \"#e1c37b\", \"#34223b\", \"#bdd8a3\", \"#6e3326\", \"#cfbdce\", \"#d07d3c\",\n\"#52697d\", \"#194196\", \"#d27c88\", \"#36422b\", \"#b68f79\"])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_uuid":"0f8ff39015f3808158878bf69e2b4382f9a06afb","_cell_guid":"aefe9f45-c060-4ef1-8f4e-72c172d7828b","collapsed":true,"_kg_hide-output":false,"trusted":false},"cell_type":"code","source":"#combined_sample.reset_index(drop=True, inplace=True)\nkmeans_df = pd.DataFrame(tsne_kmeans, columns=['x', 'y'])\nkmeans_df['cluster'] = kmeans_clusters\nkmeans_df['description'] = combined_sample['item_description']\nkmeans_df['category'] = combined_sample['general_cat']\n#kmeans_df['cluster']=kmeans_df.cluster.astype(str).astype('category')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52318f096f8240d65e5d342587c086b1a5307b76","_cell_guid":"21da5d15-cae0-49e7-9067-b6f6766afbbf","collapsed":true,"trusted":false},"cell_type":"code","source":"plot_kmeans = bp.figure(plot_width=700, plot_height=600,\n                        title=\"KMeans clustering of the description\",\n    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n    x_axis_type=None, y_axis_type=None, min_border=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4fe434a55d528f21cd301d7c20c0bd8f3e4386f9","_cell_guid":"90e136bf-f1b1-41a0-96c1-0f58a4b1f4a5","collapsed":true,"trusted":false},"cell_type":"code","source":"source = ColumnDataSource(data=dict(x=kmeans_df['x'], y=kmeans_df['y'],\n                                    color=colormap[kmeans_clusters],\n                                    description=kmeans_df['description'],\n                                    category=kmeans_df['category'],\n                                    cluster=kmeans_df['cluster']))\n\nplot_kmeans.scatter(x='x', y='y', color='color', source=source)\nhover = plot_kmeans.select(dict(type=HoverTool))\nhover.tooltips={\"description\": \"@description\", \"category\": \"@category\", \"cluster\":\"@cluster\" }\nshow(plot_kmeans)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e75b6be2c799bade7aeae9027affd4f312334295","_cell_guid":"499de8b5-7be7-4e88-a7cd-ddc689be0498"},"cell_type":"markdown","source":"## **Latent Dirichlet Allocation**\n\nLatent Dirichlet Allocation (LDA) is an algorithms used to discover the topics that are present in a corpus.\n\n>  LDA starts from a fixed number of topics. Each topic is represented as a distribution over words, and each document is then represented as a distribution over topics. Although the tokens themselves are meaningless, the probability distributions over words provided by the topics provide a sense of the different ideas contained in the documents.\n> \n> Reference: https://medium.com/intuitionmachine/the-two-paths-from-natural-language-processing-to-artificial-intelligence-d5384ddbfc18\n\nIts input is a **bag of words**, i.e. each document represented as a row, with each columns containing the count of words in the corpus. We are going to use a powerful tool called pyLDAvis that gives us an interactive visualization for LDA. "},{"metadata":{"_uuid":"8db5a893242608e737efec9f50bda5d10a42a820","_cell_guid":"3f255cb6-11b5-43cd-986d-b16a532fbed6","collapsed":true,"trusted":false},"cell_type":"code","source":"cvectorizer = CountVectorizer(min_df=4,\n                              max_features=180000,\n                              tokenizer=tokenize,\n                              ngram_range=(1,2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58ebe6357a18e5928e199fb03ed5bd41ed23c0c0","_cell_guid":"26d7b384-a33a-4c99-a352-4ef32bb897ed","collapsed":true,"trusted":false},"cell_type":"code","source":"cvz = cvectorizer.fit_transform(combined_sample['item_description'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1382bea915784c1be5b636fddd78315fc825a38e","_cell_guid":"9acb64ea-9bb0-4019-84df-932a78cd806f","collapsed":true,"trusted":false},"cell_type":"code","source":"lda_model = LatentDirichletAllocation(n_components=20,\n                                      learning_method='online',\n                                      max_iter=20,\n                                      random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39184ac4788791ab09ca777b0f6ec1d589516b4e","_cell_guid":"68c7f202-c620-42ae-843f-472f67a0ef95","collapsed":true,"trusted":false},"cell_type":"code","source":"X_topics = lda_model.fit_transform(cvz)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"495379112353df675a8aad1c6df7a6762dc725a1","_cell_guid":"3f3e686d-797b-4c22-b970-68778bdc66e9","collapsed":true,"trusted":false},"cell_type":"code","source":"n_top_words = 10\ntopic_summaries = []\n\ntopic_word = lda_model.components_  # get the topic words\nvocab = cvectorizer.get_feature_names()\n\nfor i, topic_dist in enumerate(topic_word):\n    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n    topic_summaries.append(' '.join(topic_words))\n    print('Topic {}: {}'.format(i, ' | '.join(topic_words)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f0136acba090cf8cd2d9f3589087a6cb94bc5d3","_cell_guid":"635f7f3c-46c0-4f0b-8f05-1c092f81a87b","collapsed":true,"trusted":false},"cell_type":"code","source":"# reduce dimension to 2 using tsne\ntsne_lda = tsne_model.fit_transform(X_topics)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcae4082942a7131ce3b598e485902aabf64cd17","_cell_guid":"47df42d5-248f-4f92-aacf-5a8e86bbd1ab","collapsed":true,"trusted":false},"cell_type":"code","source":"unnormalized = np.matrix(X_topics)\ndoc_topic = unnormalized/unnormalized.sum(axis=1)\n\nlda_keys = []\nfor i, tweet in enumerate(combined_sample['item_description']):\n    lda_keys += [doc_topic[i].argmax()]\n\nlda_df = pd.DataFrame(tsne_lda, columns=['x','y'])\nlda_df['description'] = combined_sample['item_description']\nlda_df['category'] = combined_sample['general_cat']\nlda_df['topic'] = lda_keys\nlda_df['topic'] = lda_df['topic'].map(int)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"b3b3f9cacc7cac86db640d5b523f3a4b4373ca9b","_cell_guid":"6dfecf36-44d4-49b7-9909-93a230acddb6","collapsed":true,"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"plot_lda = bp.figure(plot_width=700,\n                     plot_height=600,\n                     title=\"LDA topic visualization\",\n    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n    x_axis_type=None, y_axis_type=None, min_border=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3457a62e7a68c3cc0ed3edc15a314b256775d705","_cell_guid":"3fd253f0-a446-47db-8faf-444adf37507e","collapsed":true,"trusted":false},"cell_type":"code","source":"source = ColumnDataSource(data=dict(x=lda_df['x'], y=lda_df['y'],\n                                    color=colormap[lda_keys],\n                                    description=lda_df['description'],\n                                    topic=lda_df['topic'],\n                                    category=lda_df['category']))\n\nplot_lda.scatter(source=source, x='x', y='y', color='color')\nhover = plot_kmeans.select(dict(type=HoverTool))\nhover = plot_lda.select(dict(type=HoverTool))\nhover.tooltips={\"description\":\"@description\",\n                \"topic\":\"@topic\", \"category\":\"@category\"}\nshow(plot_lda)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37f2c17318ce1008ebb36d1493fa8ae5ec1b1c5a","_cell_guid":"161db864-75a8-4580-842b-c720efacd585","collapsed":true,"trusted":false},"cell_type":"code","source":"def prepareLDAData():\n    data = {\n        'vocab': vocab,\n        'doc_topic_dists': doc_topic,\n        'doc_lengths': list(lda_df['len_docs']),\n        'term_frequency':cvectorizer.vocabulary_,\n        'topic_term_dists': lda_model.components_\n    } \n    return data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72972bccd94dcb10fcf9930878052187d570a7f2","_cell_guid":"3c2dd1c1-82fe-4d59-b7dd-4a20af223eaa"},"cell_type":"markdown","source":"*Note: It's a shame that by putting the HTML of the visualization using pyLDAvis, it will distort the layout of the kernel, I won't upload in here. But if you follow the below code, there should be an HTML file generated with very interesting interactive bubble chart that visualizes the space of your topic clusters and the term components within each topic.*\n\n![](https://farm5.staticflickr.com/4536/38709272151_7128c577ee_h.jpg)"},{"metadata":{"_uuid":"e32eb8f082d0960ebe719e33e35fbcb3a270dd30","_cell_guid":"6155fb2e-2677-44f0-add6-6993ff608df6","collapsed":true,"trusted":false},"cell_type":"code","source":"import pyLDAvis\n\nlda_df['len_docs'] = combined_sample['tokens'].map(len)\nldadata = prepareLDAData()\npyLDAvis.enable_notebook()\nprepared_data = pyLDAvis.prepare(**ldadata)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"df165b4bc7db4fa05adb98736b51343617c5f142","_cell_guid":"2d7ba29d-5f28-43c3-9920-4a787b08467e","collapsed":true,"_kg_hide-output":true},"cell_type":"markdown","source":"<a data-flickr-embed=\"true\"  href=\"https://www.flickr.com/photos/thykhuely/38709272151/in/dateposted-public/\" title=\"pyLDAvis\"><img src=\"https://farm5.staticflickr.com/4536/38709272151_7128c577ee_h.jpg\" width=\"1600\" height=\"976\" alt=\"pyLDAvis\"></a><script async src=\"//embedr.flickr.com/assets/client-code.js\" charset=\"utf-8\"></script>"},{"metadata":{"_uuid":"a94dd585876f23074115131542a2aae956d63024","_cell_guid":"0c8f9e12-d2fe-45ae-b060-0fbba4d821a8","collapsed":true,"trusted":false},"cell_type":"code","source":"import IPython.display\nfrom IPython.core.display import display, HTML, Javascript\n\n#h = IPython.display.display(HTML(html_string))\n#IPython.display.display_HTML(h)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}