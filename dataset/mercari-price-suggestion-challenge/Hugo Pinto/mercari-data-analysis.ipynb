{"nbformat_minor":1,"cells":[{"cell_type":"markdown","source":"# Introduction\nThis Python notebook seeks to explore Mercari's data set, in order to extract \"what to do\" and \"what not to do\".\n\nThe main idea is to contribute to the community with quick, but deep, first view of the data.\n\nIt contains some visualizations and data manipulation, that can be further used to build your own Machine Learning Models.\n\nFeel free to contribute to this Kernel with your thoughts.\n\nHappy Kaggling!","metadata":{"_cell_guid":"f0972c68-ee82-41ed-b822-ea13024dbfa6","_uuid":"c033e9a8d8646d519bf145a535b42e2c5a07ec67"}},{"outputs":[],"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport string\n\n%matplotlib inline","execution_count":null,"metadata":{"_kg_hide-input":true,"_cell_guid":"50a7909e-3951-4271-b5bb-32580b566a49","collapsed":true,"_uuid":"86988f620dad9c960cc88a4143fb24697f924557"}},{"cell_type":"markdown","source":"## Readindg data sets","metadata":{"_cell_guid":"0ed313e9-6e11-4967-b95d-7d00d6ef930d","_uuid":"19de4d98a29ffbf3f8acc4b27ba26d63e02c4597"}},{"outputs":[],"cell_type":"code","source":"df_train = pd.read_csv('../input/train.tsv', sep='\\t')\ndf_test = pd.read_csv('../input/test.tsv', sep='\\t')","execution_count":null,"metadata":{"_cell_guid":"e894e44f-9fd6-4f5f-9103-000e0a091c87","collapsed":true,"_uuid":"970707ac3884fbe3381a3fd6c40b0bce438bf54e"}},{"cell_type":"markdown","source":"### Training set first look","metadata":{"_cell_guid":"5e84b2c7-1fe6-44c8-bff4-b58356f94db5","_uuid":"9ee1f6ecf93a7d0b96d8b060f053c94d56413019"}},{"outputs":[],"cell_type":"code","source":"df_train.head()","execution_count":null,"metadata":{"_kg_hide-input":true,"_cell_guid":"aed80e4b-3336-49a3-8d52-d577cd083646","collapsed":true,"_kg_hide-output":false,"_uuid":"7af69bc9494dca19a43f9019c183025626367d80"}},{"outputs":[],"cell_type":"code","source":"print('Train shape:{}\\nTest shape:{}'.format(df_train.shape, df_test.shape))","execution_count":null,"metadata":{"_kg_hide-input":true,"_cell_guid":"80e2c825-e61f-47bd-9776-340c0122115f","collapsed":true,"_uuid":"396d4436fd460c66d77b0128d962654a769ddab3"}},{"cell_type":"markdown","source":"## Target distribution","metadata":{"_cell_guid":"ca546ba7-13ff-4ae9-bd3e-9ee4de91dacf","_uuid":"c02364b2f15a7f2af4a6b25ea02334d6b8e672a1"}},{"outputs":[],"cell_type":"code","source":"plt.figure(figsize=(20, 15))\nplt.hist(df_train['price'], bins=50, range=[0,250], label='price')\nplt.title('Train \"price\" distribution', fontsize=15)\nplt.xlabel('Price', fontsize=15)\nplt.ylabel('Samples', fontsize=15)\nplt.legend(fontsize=15)\nplt.show()","execution_count":null,"metadata":{"_kg_hide-input":true,"_cell_guid":"3a7c0866-63a4-4914-9990-0c0060f4903c","collapsed":true,"_uuid":"4bb3fce31edc16c2f9659e74f68ce41086b5cfc2"}},{"cell_type":"markdown","source":"### Price distribution","metadata":{"_cell_guid":"5be619e3-4e27-436d-b284-c828545d5e22","_uuid":"02863048841f8b0a499e958bb6e4ed52e5a0a701"}},{"outputs":[],"cell_type":"code","source":"df_train['price'].describe()","execution_count":null,"metadata":{"_kg_hide-input":true,"_cell_guid":"d83a7ba8-d868-439a-b57f-de5bdc65b758","collapsed":true,"_uuid":"6416c40832ce867552e5e215ffacae5e7aff6de4"}},{"cell_type":"markdown","source":"Looks like the target distribution is more concentrated between **0~100**, but there are still values until **2000**.","metadata":{"_cell_guid":"e53f8ff7-3f06-4636-b9a7-0d02e124f647","_uuid":"c5d095f0476a82caf76e58e61ee2d7ca640eb897"}},{"outputs":[],"cell_type":"code","source":"plt.figure(figsize=(20, 15))\nbins=50\nplt.hist(df_train[df_train['shipping']==1]['price'], bins, normed=True, range=[0,250],\n         alpha=0.6, label='price when shipping==1')\nplt.hist(df_train[df_train['shipping']==0]['price'], bins, normed=True, range=[0,250],\n         alpha=0.6, label='price when shipping==0')\nplt.title('Train price over shipping type distribution', fontsize=15)\nplt.xlabel('Price', fontsize=15)\nplt.ylabel('Normalized Samples', fontsize=15)\nplt.legend(fontsize=15)\nplt.show()","execution_count":null,"metadata":{"_kg_hide-input":true,"_cell_guid":"a8aeb043-7a27-43ba-b2fe-ce74b3207137","collapsed":true,"_uuid":"a846436061de4ca2089f3a2f67f33d99dd8cdf3d"}},{"cell_type":"markdown","source":"The comparison of target class when shipping is 1 or 0 do not seems to be REALLY separated. But this does not means that this feature is useless, just that it can be further explored","metadata":{"_cell_guid":"cacad2e7-1ffc-4684-a250-86f0ead752d8","_uuid":"76ac0935d17f3ba185fa8e97e017ba1f8fa3ca81"}},{"outputs":[],"cell_type":"code","source":"df = df_train[df_train['price']<100]\n\nmy_plot = []\nfor i in df_train['item_condition_id'].unique():\n    my_plot.append(df[df['item_condition_id']==i]['price'])\n\nfig, axes = plt.subplots(figsize=(20, 15))\nbp = axes.boxplot(my_plot,vert=True,patch_artist=True,labels=range(1,6)) \n\ncolors = ['cyan', 'lightblue', 'lightgreen', 'tan', 'pink']\nfor patch, color in zip(bp['boxes'], colors):\n    patch.set_facecolor(color)\n\naxes.yaxis.grid(True)\n\nplt.title('BoxPlot price X item_condition_id', fontsize=15)\nplt.xlabel('item_condition_id', fontsize=15)\nplt.ylabel('price', fontsize=15)\nplt.show()\n\ndel df","execution_count":null,"metadata":{"_kg_hide-input":true,"_cell_guid":"81586160-c18b-4549-9f9a-ea5111afac36","collapsed":true,"_uuid":"9b7da62374036df4108df81e130bdc8fe81b2d34"}},{"cell_type":"markdown","source":"The training data was reduced to just have sample with **target < 100**.\nitem_condition_id feature does not seems to vary to much in our data. Their medians do not change as the item_condition_id changes. Maybe it is just an ID and should be discarded or maybe it can help the learning algorithms or Feature Egeneering in some way.\n\nJust to be fair, ID n°5 looks like a little bit different from others. It has a higher 3rd quartile and median.","metadata":{"_cell_guid":"9b0d9d38-0ee3-4424-9acf-8fd116351a57","_uuid":"9f6d5e707a61268dded576b38e3320e32eca05b7"}},{"cell_type":"markdown","source":"# Text exploration\nLets take a look into our textual features. I bet we can have a lot of insights from it.\n### Most commom letters in Items Descriptions","metadata":{"_cell_guid":"abd2ef8d-86c1-4359-bac4-02a115f465b1","_uuid":"48aab3abc61209ebcb8fd657ad70a4bd045708cc"}},{"outputs":[],"cell_type":"code","source":"cloud = WordCloud(width=1440, height=1080).generate(\" \".join(df_train['item_description']\n.astype(str)))\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud)\nplt.axis('off')","execution_count":null,"metadata":{"_kg_hide-input":true,"_cell_guid":"9db9f0d1-b158-435d-9b2f-e6b6d5ac45f0","collapsed":true,"_uuid":"1c2adf8b668d09f11bb152a8a5de30697ecfdfae"}},{"cell_type":"markdown","source":"### Verifying if products actually have description\nThe words \"description yet\" took my attention as they are commom according to the Word Cloud. Lets investigate.","metadata":{"_cell_guid":"3cdeac9c-d964-421c-a3a7-f7c152aae564","_uuid":"67f9ea03eb77b6c677321aff62bee44a7adc2e61"}},{"outputs":[],"cell_type":"code","source":"df_train['has_description'] = 1\ndf_train.loc[df_train['item_description']=='No description yet', 'has_description'] = 0","execution_count":null,"metadata":{"_kg_hide-input":true,"_cell_guid":"f1c15ef1-eb31-4488-9113-b3a614ca85f1","collapsed":true,"_uuid":"58862d79e617188a4e8bcd79c9a1258b99c1b735"}},{"outputs":[],"cell_type":"code","source":"plt.figure(figsize=(20, 15))\nbins=50\nplt.hist(df_train[df_train['has_description']==1]['price'], bins, range=[0,250],\n         alpha=0.6, label='price when has_description==1')\nplt.hist(df_train[df_train['has_description']==0]['price'], bins, range=[0,250],\n         alpha=0.6, label='price when has_description==0')\nplt.title('Train price X has_description type distribution', fontsize=15)\nplt.xlabel('Price', fontsize=15)\nplt.ylabel('Samples', fontsize=15)\nplt.legend(fontsize=15)\nplt.show()","execution_count":null,"metadata":{"_kg_hide-input":true,"_cell_guid":"e6c67983-f017-46f8-8bd2-b012f0f55c19","collapsed":true,"_uuid":"bd5786227dbeb5848a03c05a57b3843657ee9a5b"}},{"cell_type":"markdown","source":"Looking to this histograms, the distribution of prices when an item does not have a description yet is very similar to when they already have a description, considering that the amount of simples in each histogram is very different, of course. To be more clear, there are normed histograms in the next chart.\n#### Same as above, but normalized","metadata":{"_cell_guid":"d7363e4c-5478-4533-9738-c0ca184ca374","_uuid":"905d354256b9f7de0d5950d1a16d4507b8d07017"}},{"outputs":[],"cell_type":"code","source":"plt.figure(figsize=(20, 15))\nbins=50\nplt.hist(df_train[df_train['has_description']==1]['price'], bins, normed=True,range=[0,250],\n         alpha=0.6, label='price when has_description==1')\nplt.hist(df_train[df_train['has_description']==0]['price'], bins, normed=True,range=[0,250],\n         alpha=0.6, label='price when has_description==0')\nplt.title('Train price X has_description type distribution', fontsize=15)\nplt.xlabel('Price', fontsize=15)\nplt.ylabel('Samples', fontsize=15)\nplt.legend(fontsize=15)\nplt.show()","execution_count":null,"metadata":{"_kg_hide-input":true,"_cell_guid":"eccf5492-ef76-4fba-bbb2-8bb0fd8352d4","collapsed":true,"_uuid":"4cc350062a08ee3624d2385e21036bbf9698933f"}},{"cell_type":"markdown","source":"Explore products without description does not look to be the best initial approach in this competition.","metadata":{"_cell_guid":"7123aa50-cef8-4ba9-bb48-dbb715c441f0","_uuid":"a15c08fc5eb010d82da7128f964e1c89852da4d9"}},{"cell_type":"markdown","source":"# TF-IDF\nMaybe Term Frequency – Inverse Document Frequency (TF-IDF) can be a good approach to deal with items descriptions.\n\nI'll give it a chance here, even as the descriptions are pretty short, as it had given me good results in other contexts.","metadata":{"_cell_guid":"38846f75-35dd-4cc1-a75f-ec5c63772cdc","_uuid":"1a43248dbc677ce6746ee5740b6a062cec88ac57"}},{"outputs":[],"cell_type":"code","source":"def compute_tfidf(description):\n    description = str(description)\n    description.translate(string.punctuation)\n\n    tfidf_sum=0\n    words_count=0\n    for w in description.lower().split():\n        words_count += 1\n        if w in tfidf_dict:\n            tfidf_sum += tfidf_dict[w]\n    \n    if words_count > 0:\n        return tfidf_sum/words_count\n    else:\n        return 0\n\ntfidf = TfidfVectorizer(\n    min_df=5, strip_accents='unicode', lowercase =True,\n    analyzer='word', token_pattern=r'\\w+', ngram_range=(1, 3), use_idf=True, \n    smooth_idf=True, sublinear_tf=True, stop_words='english')","execution_count":null,"metadata":{"_kg_hide-input":true,"_cell_guid":"f23daa0c-8433-4874-a8bc-b04c5f1d2c58","collapsed":true,"_uuid":"2ca9b03a12ddaf6a38dfcaf791643f75b29748b2"}},{"outputs":[],"cell_type":"code","source":"tfidf.fit_transform(df_train['item_description'].apply(str))\ntfidf_dict = dict(zip(tfidf.get_feature_names(), tfidf.idf_))\ndf_train['tfidf'] = df_train['item_description'].apply(compute_tfidf)","execution_count":null,"metadata":{"_kg_hide-input":true,"_cell_guid":"e12d7d6e-beed-4d49-8941-95284587ab3f","collapsed":true,"_uuid":"d8c00efad81e096ede2fda0e60aaa63eb4825034"}},{"outputs":[],"cell_type":"code","source":"plt.figure(figsize=(20, 15))\nplt.scatter(df_train['tfidf'], df_train['price'])\nplt.title('Train price X item_description TF-IDF', fontsize=15)\nplt.xlabel('Price', fontsize=15)\nplt.ylabel('TF-IDF', fontsize=15)\nplt.legend(fontsize=15)\nplt.show()","execution_count":null,"metadata":{"_kg_hide-input":true,"_cell_guid":"6133201a-20b0-474d-b5d5-8cd644b9bbf3","collapsed":true,"_uuid":"964472c29bae96028baad7287458c4078878e4d3"}},{"cell_type":"markdown","source":"Well, looking to this scatter plot is possible to notice that as higher the TF-IDF, lower the 'price' gets. I'll definitely try this feature in my models.","metadata":{"_cell_guid":"30d1dcc2-964c-4cc1-a328-4527d5e8ec82","_uuid":"2c9885bb81b2f9e0fe10ff1e516783f2db9a9551"}},{"cell_type":"markdown","source":"## Item Description Lengths\n### Characters count","metadata":{"_cell_guid":"c7d73737-7308-4afd-b9b1-e8c3b748c025","_uuid":"ab1da441cd0a7f6078ca511fb4ef068ca64b0945"}},{"outputs":[],"cell_type":"code","source":"train_ds = pd.Series(df_train['item_description'].tolist()).astype(str)\ntest_ds = pd.Series(df_test['item_description'].tolist()).astype(str)\n\nbins=100\nplt.figure(figsize=(20, 15))\nplt.hist(train_ds.apply(len), bins, range=[0,600], label='train')\nplt.hist(test_ds.apply(len), bins, alpha=0.6,range=[0,600], label='test')\nplt.title('Histogram of character count', fontsize=15)\nplt.xlabel('Characters Number', fontsize=15)\nplt.ylabel('Frequency', fontsize=15)\nplt.legend(fontsize=15)\nplt.show()","execution_count":null,"metadata":{"_kg_hide-input":true,"_cell_guid":"c76ada2a-b7eb-45d4-a118-c41d807caa1d","collapsed":true,"_uuid":"312c0f7ed7da7ba4cdb7a479dc0b9a0bd4862e77"}},{"cell_type":"markdown","source":"The histograms seems to have a pretty similar distribution. There is a gap near **20 characters**, maybe because of \"no description yet\" descriptions.","metadata":{"_cell_guid":"16d3ad8f-54ba-40c5-b6cb-6a941e32aec1","_uuid":"08f15c4f5205ebb25ecd2d798892d4730c0215a6"}},{"cell_type":"markdown","source":"### Word count","metadata":{"_cell_guid":"7b53019e-0804-48cf-89e9-de92ab5d5aff","_uuid":"a4cc86b0649abc00984712426e37035653906d16"}},{"outputs":[],"cell_type":"code","source":"bins=100\nplt.figure(figsize=(20, 15))\nplt.hist(train_ds.apply(lambda x: len(x.split())), bins, range=[0,100], label='train')\nplt.hist(test_ds.apply(lambda x: len(x.split())), bins, alpha=0.6,range=[0,100], label='test')\nplt.title('Histogram of word count', fontsize=15)\nplt.xlabel('Number of words', fontsize=15)\nplt.ylabel('Frequency', fontsize=15)\nplt.legend(fontsize=15)\nplt.show()","execution_count":null,"metadata":{"_kg_hide-input":true,"_cell_guid":"9b10a5f8-32c6-4266-a708-85d0136daedc","collapsed":true,"_uuid":"f7a21b65f01242612c096cd7ac78d38a67f8cd13"}},{"cell_type":"markdown","source":"As well as character count, word count histograms are very similar for train and test. Further exploration will be needed here!","metadata":{"_cell_guid":"9b0e3628-d679-477e-83ac-7c5f180b2e28","_uuid":"c10530923b5eaca387b9da7df686ab4841d16479"}},{"cell_type":"markdown","source":"# Category Name\nIn our data set, each item have fits in a specific category, or a group of categories (mainly 3 for item).\n\nBasically, the categories are arranged as from top to bottom of comprehensiveness. This means that the first category is less specific and the next are more specifics.\n\nLets investigate the 'main' categories first.","metadata":{"_cell_guid":"7370e0c8-4539-40a6-8afe-825f92a6cb12","_uuid":"7c975522e81a222d600f9adedb69783f0ae64c5a"}},{"outputs":[],"cell_type":"code","source":"def transform_category_name(category_name):\n    try:\n        main, sub1, sub2= category_name.split('/')\n        return main, sub1, sub2\n    except:\n        return np.nan, np.nan, np.nan\n\ndf_train['category_main'], df_train['category_sub1'], df_train['category_sub2'] = zip(*df_train['category_name'].apply(transform_category_name))","execution_count":null,"metadata":{"_kg_hide-input":true,"_cell_guid":"ddb7b9ba-47a6-42ce-8e06-75b3a87f650f","collapsed":true,"_uuid":"5792f0d3254db94c93ed8c2bc3988c494e20f1d4"}},{"outputs":[],"cell_type":"code","source":"main_categories = [c for c in df_train['category_main'].unique() if type(c)==str]\ncategories_sum=0\nfor c in main_categories:\n    categories_sum+=100*len(df_train[df_train['category_main']==c])/len(df_train)\n    print('{:25}{:3f}% of training data'.format(c, 100*len(df_train[df_train['category_main']==c])/len(df_train)))\nprint('nan\\t\\t\\t {:3f}% of training data'.format(100-categories_sum))","execution_count":null,"metadata":{"_kg_hide-input":true,"_cell_guid":"afa76219-562f-473c-944e-d54baa388d8f","collapsed":true,"_uuid":"15e929aae2a7870b9fa63365ff8c81f38f8a47be"}},{"outputs":[],"cell_type":"code","source":"df = df_train[df_train['price']<80]\n\nmy_plot = []\nfor i in main_categories:\n    my_plot.append(df[df['category_main']==i]['price'])\n    \nfig, axes = plt.subplots(figsize=(20, 15))\nbp = axes.boxplot(my_plot,vert=True,patch_artist=True,labels=main_categories) \n\ncolors = ['cyan', 'lightblue', 'lightgreen', 'tan', 'pink']*2\nfor patch, color in zip(bp['boxes'], colors):\n    patch.set_facecolor(color)\n\naxes.yaxis.grid(True)\n\nplt.title('BoxPlot price X Main product category', fontsize=15)\nplt.xlabel('Main Category', fontsize=15)\nplt.ylabel('Price', fontsize=15)\nplt.show()","execution_count":null,"metadata":{"_kg_hide-input":true,"_cell_guid":"86d7be65-47b3-4f34-807d-7f2cd0ecbce8","collapsed":true,"scrolled":false,"_uuid":"333038c2cd33b4b31682fcad135571c3cd3b6091"}},{"cell_type":"markdown","source":"Well, these boxplots are telling us more than the last ones. At least we have some variation when looking to their medians. But I feel like these results can be improved.\n\n\"Men\" products are showing themselves more expensives than other in a general way.","metadata":{"_cell_guid":"e9b88982-1760-4cf1-85e0-f37d2ea811f2","_uuid":"4d80e9e74cc7d704791df1d01ebf5504d187239e"}},{"cell_type":"markdown","source":"# To be continued...","metadata":{"_cell_guid":"7c8019c8-16bc-4668-9bbe-bf5707f910dd","_uuid":"235f34f13bb8cb0d21dd532a9400c5e92b786de1"}},{"outputs":[],"cell_type":"code","source":"","execution_count":null,"metadata":{"_cell_guid":"d1804555-f426-4d18-b6bf-0eeef55432a1","collapsed":true,"_uuid":"04308f36c4877a55864d29dca286e1ea1153b7fe"}}],"nbformat":4,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"nbconvert_exporter":"python","name":"python","pygments_lexer":"ipython3","file_extension":".py","version":"3.6.3","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python"}}}