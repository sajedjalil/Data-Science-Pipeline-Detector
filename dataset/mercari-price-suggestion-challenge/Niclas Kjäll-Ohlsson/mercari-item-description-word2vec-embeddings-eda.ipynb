{"cells":[{"cell_type":"markdown","source":"It is interesting that a word2vec model based on Mercari item descriptions can be trained from scratch in approximately 3 minutes using gensim package (https://radimrehurek.com/gensim/). In this workbook a word2vec model is trained on the item descriptions from the training set. Further, the resulting embeddings are projected to 2-dimensional vector space using a singular value decomposition. Then, a sample of similar words are visualised, according to their 2-d coordinates, in a scatter plot. Finally, a K-means clustering is performed over the embeddings and the resulting centroid embeddings are visualised by way of a sample of nearest word embeddings with 2-d projections, again in a scatter plot.","metadata":{"_uuid":"59860e1ecb5291af3d641136c7c07b8b41622a7b","_cell_guid":"c851c372-4f18-4d8a-af76-51847c14d962"}},{"metadata":{"_uuid":"e11ed77faab1f23f2aed06d815863c075bc956f0","_cell_guid":"cdbda8d0-c7d3-4b4e-bbc9-c7c0cab0929e","collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re, itertools, random\nimport matplotlib.pyplot as plt\nimport time\n\nfrom gensim.models import word2vec\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.cluster import KMeans\n\nfrom scipy.spatial.distance import cdist","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78eff2bd8dcbd9b5a9343a29bdce92f69a8d920b","_cell_guid":"6b1e474f-77f5-4d3d-8e47-407b5338e4e5"},"cell_type":"code","source":"def read_data():\n    global input_train, input_test\n    # Read full training and test sets\n    input_train = pd.read_csv(\"../input/train.tsv\", sep = \"\\t\")\n    input_test = pd.read_csv(\"../input/test.tsv\", sep = \"\\t\")\n    print(\"Read input\")\n\n# Read the raw data\nread_data()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"List sentences","metadata":{"_uuid":"ec213c574ae2f2268cf33f83b7631b404dbe7df3","_cell_guid":"be5f852c-1f59-4070-a3d8-afe37a2800a7"}},{"metadata":{"_uuid":"d6c5148f5ed0a4d1be95ef0453f82cbed16a5628","_cell_guid":"6875efac-fa48-43e4-acc1-56c01c57470d","collapsed":true},"cell_type":"code","source":"sentences = [s for s in [s.strip().lower().split(\" \") for i in input_train[\"item_description\"].values for s in re.split(\"\\.\", str(i))] if len(s) > 2]","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train word2vec model with embedding vector size 20 and word to word contingency window of 5.","metadata":{"_uuid":"6cdbbee2314413eb9ed0f6d38bf239be18224d8d","_cell_guid":"110df346-3709-4312-9e0d-0f4c61bcbfdc"}},{"metadata":{"_uuid":"16db235496b89b7a92badd3f7daaf6f005b58c96","_cell_guid":"496892ce-2bd6-46f8-88f4-954167b7df84"},"cell_type":"code","source":"embedding_size = 20\nstarttime = time.time()\nmodel = word2vec.Word2Vec(sentences, size=embedding_size, window=5, min_count=5, workers=4)\nendtime = time.time()\nprint(\"Trained word2vec model in \" + str(int(np.floor((endtime - starttime)/60))) + \"m \" + str(int((endtime - starttime)%60)) + \"s.\")","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lookup some words and their nearest contextual siblings in embedding vector space. Seems to make a lot of sense :-)","metadata":{"_uuid":"a2d9e451b543d6b474003526b0b73190c0409ea3","_cell_guid":"0b94aeb5-d08d-4316-b946-c01fa26c923e"}},{"metadata":{"_uuid":"685076c5a7286c52fa922b55bc49c7bcae999376","_cell_guid":"aa5baf6f-f268-4540-bf2d-b7b04b5ceb1d"},"cell_type":"code","source":"lookup_words = [\"shorts\", \"xbox\", \"shoes\"]\nwords_to_visualize = [] # Save for visualization below\n\nfor w in lookup_words:\n    print(w)\n    for s in model.wv.most_similar([w]):\n        print(s)\n        words_to_visualize.append(s[0])","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize the embeddings by projecting embeddings in 2-dimensional space using singular value decomposition.","metadata":{"_uuid":"e0be6468e7a0fe0165d0fddadcdd97f74f10d5ba","_cell_guid":"5b2194f4-7dd9-43dd-8515-5f53f3fbde57"}},{"metadata":{"_uuid":"9dab330b2845f5c9968704dde7cb2b2664c6d735","_cell_guid":"ae59761a-667a-404a-a032-ec12b8f269a6"},"cell_type":"code","source":"# First get the embeddings into a matrix\nembeddings = np.zeros((len(model.wv.index2word), embedding_size))\nfor i in range(0, len(model.wv.index2word)):\n    w = model.wv.index2word[i]\n    embeddings[i] = model.wv[w]\n\n# Look at a few samples\nfor i in range(1000, 1003):\n    print(model.wv.index2word[i] + \":\\n\" + str(embeddings[i]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5da4c2d1a5938925048427a48b8cd8fb5c224458","_cell_guid":"1c26293d-22b8-4f1a-b3b8-71aeaf6b68b6","collapsed":true},"cell_type":"code","source":"svd = TruncatedSVD(n_components=2, algorithm='randomized', n_iter=500, random_state=101)\nembeddings_2d_projection = svd.fit_transform(embeddings)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb5632eff5c7efe4fca2aa63f8b6d47fdbdcec47","_cell_guid":"bb5b21d7-f8e8-440d-85fa-88fc2f26ac4f"},"cell_type":"code","source":"sample = np.in1d(model.wv.index2word, words_to_visualize)\nx = embeddings_2d_projection[sample,0]\ny = embeddings_2d_projection[sample,1]\nplt.figure(figsize=(7,7))\nplt.scatter(x, y)\nfor i, txt in enumerate([model.wv.index2word[i] for i in np.where(sample)[0]]):\n    plt.annotate(txt, (x[i], y[i]))","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the sample of words we see that some of the variance is lost when projecting down to 2-d, but it's still cool to see that contextually similar words cluster together also here, for example joggers, pants, sweatpants, capri, etc. We could try other dimension reduction techniques to maybe get even better fidelity between embeddings and 2-d projection, for example t-SNE.\n\n\nLet's try some clustering.","metadata":{"_uuid":"84df594fbf402ddc6ba7859d89abe244d50cb49d","_cell_guid":"0bc8d722-74d4-4cdd-9b04-4d22a9da7d86"}},{"metadata":{"_uuid":"372982e3c61dc6bf706a72f4d0c95f4029e1d989","_cell_guid":"6abef9f0-1c51-4ff3-bf75-3327c5170e0c","collapsed":true},"cell_type":"code","source":"# Train a K-means cluster model with 6 clusters\nn_clusters = 6\nembedding_cluster_model = KMeans(n_clusters=n_clusters, random_state=0).fit(embeddings)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For each cluster's centroid embedding collect the 20 nearest word embeddings","metadata":{"_uuid":"d70b3971332a8aa34f4c931bf15cf841ab44bd79","_cell_guid":"06a6711a-45c2-4d10-a8b7-d7af94a1a631"}},{"metadata":{"_uuid":"379db488ad1a64315a5a62c846fd4bff5f6ee5b2","_cell_guid":"11da95b5-71aa-4ff3-be6a-0ef1b0aa03cf","collapsed":true},"cell_type":"code","source":"centroid_embedding_nearest_words = []\nfor centroid_embedding in embedding_cluster_model.cluster_centers_:\n    centroid_embedding_nearest_words.append(\n        np.argsort([i[0] for i in cdist(embeddings, np.array([centroid_embedding]), \"euclidean\")])[0:20]\n    )","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot the 20 nearest word embeddings using a distinct color per cluster","metadata":{"_uuid":"9616b8ad12dc325339fcf65bb3b1223290f9e530","_cell_guid":"ab33a3fd-6c3a-4d49-9451-25223dc8071b"}},{"metadata":{"_uuid":"0816a536cee66ca9e009938896f0784924ffa9a5","_cell_guid":"cf657ad6-0b9e-4c21-8a89-7d6332677ee2"},"cell_type":"code","source":"plt.figure(figsize=(10,10))\ncolors = itertools.cycle([\"b\",\"g\",\"r\",\"c\",\"m\",\"y\",\"k\",\"w\"])\nc = 0\nfor word_indices in centroid_embedding_nearest_words:\n    clr = next(colors)\n    plt.scatter(\n        embeddings_2d_projection[word_indices,0],\n        embeddings_2d_projection[word_indices,1],\n        color=clr,\n        label=\"Cluster \" + str(c)\n    )\n    for ix in word_indices:\n        x, y = embeddings_2d_projection[ix,:]\n        plt.annotate(model.wv.index2word[ix], (x, y))\n    c+=1\nplt.legend(loc='lower left')\nplt.show()","execution_count":null,"outputs":[]}],"nbformat":4,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","version":"3.6.4","codemirror_mode":{"name":"ipython","version":3},"name":"python","file_extension":".py","nbconvert_exporter":"python","mimetype":"text/x-python"}},"nbformat_minor":1}