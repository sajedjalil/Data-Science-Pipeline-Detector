{"nbformat_minor":1,"cells":[{"cell_type":"markdown","metadata":{"_uuid":"8713596eb698e398be13ba08d5e4dca74cbbd17c"},"source":"I'm new to kaggle, but a major part of this challenge seems to be efficiently processing data to leave time for supervised learning algorithms.  Below is my attempt to make this efficient.  I would appreciate any comments/improvements/bug calls!"},{"cell_type":"code","metadata":{"_uuid":"104eda7a5f9c19286fb7a17453bbbd22ea476cb1","_cell_guid":"6936436a-cbd4-4fde-8fb2-b4f96af54f18","collapsed":true},"execution_count":null,"source":"import time\nimport numpy as np\nimport pandas as pd\nimport gc","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"4132552bbff7f67d064306b58fe40d896fbb6dc7","_cell_guid":"ce3e2939-f65c-4065-83ef-35e423593ace","collapsed":true},"execution_count":null,"source":"def split_cat(text):\n    try:\n        return text.split(\"/\")\n    except:\n        return (\"No Label\", \"No Label\", \"No Label\")","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"b80a17442ee595b6c17ac02980bd8832d4602bd2","_cell_guid":"14a21f6a-6ffb-4c4b-ba96-f0407fef3223"},"execution_count":null,"source":"%%time\n\nstart_time = time.time()\n    \n\ntrain = pd.read_table('../input/train.tsv', engine='c', \n                      dtype={'item_condition_id': 'category',\n                             'shipping': 'category',\n                            }, \n                     converters={'category_name': split_cat})\ntest = pd.read_table('../input/test.tsv', engine='c', \n                      dtype={'item_condition_id': 'category',\n                             'shipping': 'category',\n                            },\n                    converters={'category_name': split_cat})\nprint('[{}] Finished load data'.format(time.time() - start_time))\n\nnrow_test = train.shape[0]\ndftt = train[(train.price < 1.0)]\ntrain = train.drop(train[(train.price < 1.0)].index)\ndel dftt['price']\nnrow_train = train.shape[0]\ny = np.log1p(train['price'])\nmerge = pd.concat([train, dftt, test])\nsubmission: pd.DataFrame = test[['test_id']]\n\ndel train, test\ngc.collect()\n\nmerge['gencat_name'] = merge['category_name'].str.get(0).replace('', 'missing').astype('category')\nmerge['subcat1_name'] = merge['category_name'].str.get(1).fillna('missing').astype('category')\nmerge['subcat2_name'] = merge['category_name'].str.get(2).fillna('missing').astype('category')\nmerge.drop('category_name', axis=1, inplace=True)\nprint('[{}] Split categories completed.'.format(time.time() - start_time))\n\nmerge['item_condition_id'] = merge['item_condition_id'].cat.add_categories(['missing']).fillna('missing')\nmerge['shipping'] = merge['shipping'].cat.add_categories(['missing']).fillna('missing')\nmerge['item_description'].fillna('missing', inplace=True)\nmerge['brand_name'] = merge['brand_name'].fillna('missing').astype('category')\nprint('[{}] Handle missing completed.'.format(time.time() - start_time))","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"f439a33691a5e105d128cfd070e8ca1458b90011"},"source":"Below I subclass sklearn's CountVectorizer and make it multiprocessing, adding in some efficiencies to the original code.  Thanks to Keras and sklearn code for some ideas. Help wanted on making the pickling for multiprocessing more efficient.\n\nThis class can be chained with sklearn's TfidfTransformer as seen in a few code blocks."},{"cell_type":"code","metadata":{"_uuid":"1564f464320c0a179a36d6ce04eb276df73a81c9","_cell_guid":"20afb268-4fa5-4363-9ea7-000802c10d5a","collapsed":true},"execution_count":null,"source":"from scipy import sparse as sp\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom multiprocessing import Pool\nfrom collections import Counter\nfrom contextlib import closing\nimport array\nimport dill\ndill.settings['byref'] = True\nfrom operator import itemgetter\nfrom numbers import Integral\nfrom six import string_types\nfrom bisect import bisect_left, bisect_right\n\ndef apply_packed_function_for_map(x,):\n    \"\"\"\n    https://stackoverflow.com/questions/8804830/python-multiprocessing-pickling-error\n    Unpack dumped function as target function and call it with arguments.\n\n    :param (dumped_function, item, args, kwargs):\n        a tuple of dumped function and its arguments\n    :return:\n        result of target function\n    \"\"\"\n    dumped_function, item = x\n    target_function = dill.loads(dumped_function)\n    res = target_function(item)\n    return res\n\n\ndef pack_function_for_map(target_function, items):\n    \"\"\"\n    https://stackoverflow.com/questions/8804830/python-multiprocessing-pickling-error\n    Pack function and arguments to object that can be sent from one\n    multiprocessing.Process to another. The main problem is:\n        «multiprocessing.Pool.map*» or «apply*»\n        cannot use class methods or closures.\n    It solves this problem with «dill».\n    It works with target function as argument, dumps it («with dill»)\n    and returns dumped function with arguments of target function.\n    For more performance we dump only target function itself\n    and don't dump its arguments.\n    How to use (pseudo-code):\n\n        ~>>> import multiprocessing\n        ~>>> images = [...]\n        ~>>> pool = multiprocessing.Pool(100500)\n        ~>>> features = pool.map(\n        ~...     *pack_function_for_map(\n        ~...         super(Extractor, self).extract_features,\n        ~...         images,\n        ~...         type='png'\n        ~...         **options,\n        ~...     )\n        ~... )\n        ~>>>\n\n    :param target_function:\n        function, that you want to execute like  target_function(item, *args, **kwargs).\n    :param items:\n        list of items for map\n    :param args:\n        positional arguments for target_function(item, *args, **kwargs)\n    :param kwargs:\n        named arguments for target_function(item, *args, **kwargs)\n    :return: tuple(function_wrapper, dumped_items)\n        It returs a tuple with\n            * function wrapper, that unpack and call target function;\n            * list of packed target function and its' arguments.\n    \"\"\"\n    dumped_function = dill.dumps(target_function)\n    dumped_items = list(zip([dumped_function] * len(items), items))\n#     print('done pickling')\n    return apply_packed_function_for_map, dumped_items\n\nclass MPCountVectorizer(CountVectorizer):\n    \"\"\"Subclass CountVectorizer and make multiprocessing \"\"\"\n    def __init__(self, n_jobs, chunk_size, save_vocab, input='content', encoding='utf-8',\n                 decode_error='strict', strip_accents=None,\n                 lowercase=True, preprocessor=None, tokenizer=None,\n                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n                 ngram_range=(1, 1), analyzer='word',\n                 max_df=1.0, min_df=1, max_features=None,\n                 vocabulary=None, binary=False, dtype=np.int64):\n\n        self.n_jobs = n_jobs\n        self.chunk_size = chunk_size\n        self.save_vocab = save_vocab\n        \n        super().__init__(input=input, encoding=encoding,\n                 decode_error=decode_error, strip_accents=strip_accents,\n                 lowercase=lowercase, preprocessor=preprocessor, tokenizer=tokenizer,\n                 stop_words=stop_words, token_pattern=token_pattern,\n                 ngram_range=ngram_range, analyzer=analyzer,\n                 max_df=max_df, min_df=min_df, max_features=max_features,\n                 vocabulary=vocabulary, binary=binary, dtype=dtype)\n        \n    def _task_multiprocess(self, task, args, vocabulary, analyzer):\n        def init(x, y):\n            global vocabulary_, analyzer_\n            vocabulary_ = x\n            analyzer_ = y\n            \n        with closing(Pool(self.n_jobs, \n                          initializer=init, \n                          initargs=(vocabulary, analyzer),\n                          maxtasksperchild=2)) as pool:\n            results = pool.map_async(*pack_function_for_map(task, args))\n            results.wait(timeout=600)\n            if results.ready():  \n                results = results.get()\n        return results\n    \n    def _chunker(self, l, n):\n            \"\"\"Yield successive n-sized chunks from l.\"\"\"\n            for i in range(0, len(l), n):\n                yield l.iloc[i:i + n]\n    \n    def _feat_vector_task(self, raw_documents):\n        \"\"\" Subprocess to create sparse feature matrix\n        \"\"\"\n        \n        j_indices = []\n        values = array.array(\"i\")\n        indptr = array.array(\"i\")\n        indptr.append(0)\n        \n        for doc in raw_documents:\n            feature_counter = {}\n            for feature in analyzer_(doc):\n                try:\n                    feature_idx = vocabulary_[feature]\n                    if feature not in feature_counter:\n                        feature_counter[feature_idx] = 1\n                    else:\n                        feature_counter[feature_idx] += 1\n                except KeyError:\n                    continue\n            j_indices += feature_counter.keys()\n            values.extend(feature_counter.values())\n            indptr.append(len(j_indices))\n        \n        return sp.csr_matrix((values, j_indices, indptr),\n                             shape=(len(indptr) - 1, len(vocabulary_)),\n                             dtype=self.dtype)\n\n    def _word_count_task(self, raw_documents):\n\n        word_counts = {}\n        for doc in raw_documents:\n            for tok in analyzer_(doc):\n                if tok in word_counts:\n                    word_counts[tok] += 1\n                else:\n                    word_counts[tok] = 1\n\n        return word_counts\n    \n    def _count_vocab(self, raw_documents, fixed_vocab):\n        \"\"\"Create sparse feature matrix, and vocabulary where fixed_vocab=False\n        \"\"\"\n\n#         print('create chunks')\n        chunks = list(self._chunker(raw_documents, self.chunk_size))\n        analyzer = self.build_analyzer()\n\n        if fixed_vocab:\n            vocabulary = self.vocabulary_\n\n        else:\n            max_df = self.max_df\n            min_df = self.min_df\n\n#             print('build vocabulary')\n            self.vocabulary_ = None\n            \n            vocabulary = {}\n            partial_counts = self._task_multiprocess(\n                self._word_count_task,\n                chunks,\n                None,\n                analyzer\n            )\n#             print('merge vocabulary')\n            word_counts = Counter(partial_counts[0])\n            for count in partial_counts[1:]:\n                word_counts.update(count)\n            word_counts = dict(word_counts)\n        \n#             print('filter vocabulary')\n            n_doc = len(raw_documents)\n            max_features = self.max_features\n            max_doc_count = (max_df\n                                 if isinstance(max_df, Integral)\n                                 else max_df * n_doc)\n            min_doc_count = (min_df\n                             if isinstance(min_df, Integral)\n                             else min_df * n_doc)\n\n            if max_doc_count < min_doc_count:\n                raise ValueError(\"max_df corresponds to < documents than min_df\")\n\n#             removed_terms = []\n            word_counts = sorted(word_counts.items(), key=itemgetter(1))\n            keys, vals = list(zip(*word_counts))\n            left_index = bisect_left(vals, min_doc_count)\n            right_index = bisect_right(vals, max_doc_count)\n            \n#             removed_terms += keys[:left_index]\n#             removed_terms += keys[right_index:]\n            word_counts = word_counts[left_index:right_index]\n            if max_features:\n#                 removed_terms += keys[:-max_features]\n                word_counts = word_counts[-max_features:]\n            vocabulary = dict(zip([kv[0] for kv in word_counts], \n                                   range(len(word_counts))))\n                     \n#             self.stop_words_ = removed_terms\n\n            if not vocabulary:\n                raise ValueError(\"empty vocabulary; perhaps the documents only\"\n                                 \" contain stop words or min_df, max_df too stringent\")\n        \n#         self.vocabulary_ = word_counts\n        \n#         print('create counts')\n        partial_X = self._task_multiprocess(\n            self._feat_vector_task,\n            chunks,\n            vocabulary,\n            analyzer\n        )\n                        \n#         print('merge counts')\n        X = sp.vstack(partial_X)\n        X.sort_indices()\n        \n#         print('done')\n        \n        return word_counts, X\n    \n    def fit_transform(self, raw_documents, y=None):\n        \"\"\"Learn the vocabulary dictionary and return term-document matrix.\n        This is equivalent to fit followed by transform, but more efficiently\n        implemented.\n        Parameters\n        ----------\n        raw_documents : iterable\n            An iterable which yields either str, unicode or file objects.\n        Returns\n        -------\n        X : array, [n_samples, n_features]\n            Document-term matrix.\n        \"\"\"\n        # We intentionally don't call the transform method to make\n        # fit_transform overridable without unwanted side effects in\n        # TfidfVectorizer.\n        if isinstance(raw_documents, string_types):\n            raise ValueError(\n                \"Iterable over raw text documents expected, \"\n                \"string object received.\")\n\n        self._validate_vocabulary()\n        max_df = self.max_df\n        min_df = self.min_df\n        max_features = self.max_features\n\n        vocabulary, X = self._count_vocab(raw_documents,\n                                          self.fixed_vocabulary_)\n\n        if self.binary:\n            X.data.fill(1)\n            \n        if not self.fixed_vocabulary_ and self.save_vocab:\n            self.vocabulary_ = vocabulary\n\n        return X\n","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"5ce7bcb31d0efefcfe029007e26a9634ac9e897d"},"source":"Below is a selector class to easily feed pandas data to sklearn.  Categorical data can be simply accessed using df.cat.codes, rather than using sklearn's LabelEncoder as seen in some other public kernels."},{"cell_type":"code","metadata":{"_uuid":"207585819cdb4577193a14ae810cdab87e29eaa1","_cell_guid":"5d54666b-0074-444c-9a8e-df3f094fa825","collapsed":true},"execution_count":null,"source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass ItemSelector(BaseEstimator, TransformerMixin):\n\n    def __init__(self, field, dtype=None):\n        self.field = field\n        self.dtype = dtype\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, dataframe):\n        if self.dtype == 'category':\n            return dataframe[self.field].cat.codes[:, None]\n        elif self.dtype == 'numeric':\n            return dataframe[self.field][:, None]\n        else:\n            return dataframe[self.field]","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"343c76325d6d68c71e687b8443a9fa8b1f4bcf9d","_cell_guid":"c951bc68-0498-450f-add6-2d8cf7368c0e"},"execution_count":null,"source":"%%time\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\n\nvectorizer_cat = FeatureUnion([\n    ('item_condition_id', Pipeline([\n        ('selector', ItemSelector(field='item_condition_id', dtype='category')),\n        ('ohe', OneHotEncoder())\n    ])),\n    ('shipping', Pipeline([\n        ('selector', ItemSelector(field='shipping', dtype='category')),\n        ('ohe', OneHotEncoder())\n    ])),\n    ('gencat_name', Pipeline([\n        ('selector', ItemSelector(field='gencat_name', dtype='category')),\n        ('ohe', OneHotEncoder())\n    ])),\n    ('subcat1_name', Pipeline([\n        ('selector', ItemSelector(field='subcat1_name', dtype='category')),\n        ('ohe', OneHotEncoder())\n    ])),\n    ('subcat2_name', Pipeline([\n        ('selector', ItemSelector(field='subcat2_name', dtype='category')),\n        ('ohe', OneHotEncoder())\n    ])),\n    ('brand_name', Pipeline([\n        ('selector', ItemSelector(field='brand_name', dtype='category')),\n        ('ohe', OneHotEncoder())\n    ])),\n    ('name', Pipeline([\n        ('selector', ItemSelector(field='name')),\n        ('cv', MPCountVectorizer(\n            n_jobs=4,\n            chunk_size=50000,\n            save_vocab=False,\n            ngram_range=(1, 2),\n            stop_words='english',\n            min_df=10\n        )),\n    ])),\n    ('item_description', Pipeline([\n        ('selector', ItemSelector(field='item_description')),\n        ('cv', MPCountVectorizer(\n            n_jobs=4,\n            chunk_size=50000,\n            save_vocab=False,\n            ngram_range=(1, 3),\n            stop_words='english',\n            max_features=1500000\n        )),\n        ('tfidf', TfidfTransformer()\n        )\n    ]))\n], n_jobs=1)\n\nX_train = vectorizer_cat.fit_transform(merge)\n\nprint('[{}] Data vectorization completed'.format(time.time() - start_time))","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"9323f3b32f8186389fbe4a64ff179f5c0c1aa07f"},"execution_count":null,"source":"print('Total examples: {}, total features: {}'.format(X_train.shape[0], X_train.shape[1]))","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"2614e73f6c0ba17c12563c5a773fed2d81baa3b4","_cell_guid":"6f4260c8-48b9-400f-8fe5-5a1641d7426c","collapsed":true},"source":"About 4 minutes on this run from load to vectorization for ~1.6m features, not bad compared to what I was getting before.\n\nLets test out the features with a simple model:"},{"cell_type":"code","metadata":{"_uuid":"ed3f14929562343099d44c8c6e0ac2b1743ca0a0"},"execution_count":null,"source":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\ntrain_X, valid_X, train_y, valid_y = train_test_split(X_train[:nrow_train], y, test_size = 0.1, random_state = 144)\n\nmodel = Ridge(alpha=.5, copy_X=True, fit_intercept=True, max_iter=100,\n      normalize=False, random_state=101, solver='auto', tol=0.01)\nmodel.fit(train_X, train_y)\nprint('[{}] Train ridge completed'.format(time.time() - start_time))\n\n# valid_X = lsa.transform(valid_X)\npredsR = model.predict(valid_X)\nprint('[{}] Predict ridge completed'.format(time.time() - start_time))\n\ndef rmsle(y, y0):\n    assert len(y) == len(y0)\n    return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(y0), 2)))\nprint('valid rmsle is', rmsle(np.expm1(predsR), np.expm1(valid_y)))","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"716ce41e8b178db750b347ea71fba121bf805a67"},"source":"About 6.5 minutes total on this (inconsistent) kernel, down from >15 minutes for me before.\n\nAnd yeah better features are out there ;-)!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","nbconvert_exporter":"python","version":"3.6.4","name":"python","pygments_lexer":"ipython3"}},"nbformat":4}