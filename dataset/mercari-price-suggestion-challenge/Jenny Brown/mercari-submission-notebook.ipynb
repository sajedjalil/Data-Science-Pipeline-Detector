{"cells":[{"metadata":{"scrolled":true,"_uuid":"a74c573c40b41ce11abfeee5225df16c46ef07b5","_cell_guid":"9d8f6bc5-fff3-4e73-afb9-8e8b94234f8a"},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","cell_type":"code","outputs":[],"execution_count":1},{"metadata":{"collapsed":true,"_uuid":"667acb994cfd4290a9ee33707ff8de150583dba2","_cell_guid":"99404083-e498-4c2e-8681-90cc3c513fc4"},"source":"import lzma\nimport os\nfrom scipy import stats, integrate\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport csv\n\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\npd.options.display.float_format = '{:.0f}'.format","cell_type":"code","outputs":[],"execution_count":2},{"metadata":{"collapsed":true,"_uuid":"08e2ffb1066cb6cbdacbc3636308fd12059b6172","_cell_guid":"be82be7c-9b82-430a-bb85-45aa900c0bb8"},"source":"df_train = pd.read_csv('../input/train.tsv', delimiter='\\t')\ndf_test = pd.read_csv('../input/test.tsv', delimiter='\\t')\n\n# These don't help us train. Nor do they make any sense.\ndf_train.drop(df_train[df_train['price'] == 0].index, inplace=True)","cell_type":"code","outputs":[],"execution_count":3},{"metadata":{"collapsed":true,"_uuid":"13915adb1a53893cb21d90cd1450549ce1a8b9ba","_cell_guid":"d6ca7dc3-1efa-4cf2-b69c-6877d75e41d2"},"source":"# We might need to filter out price=0 lines as weird outliers.\n# Replace the brand name NaN with 'Unknown' or some real string.\n# We want to split category names on the slash, into separate columns, and provide min/max/median for each of those.\n# We want to add columns for length of item description and length of item name.\n# Eventually pick up words like 'guarantee' or 'authentic*' or 'lot' or 'brand new' or 'tags' or something,\n# but first train a model on purely the numeric information.  We'll come back to these.\n# We want to augment brand name columns and category columns for min/max/median.\n","cell_type":"code","outputs":[],"execution_count":4},{"metadata":{"collapsed":true,"_uuid":"69ed7a4656e72ab0247c28fb6dbbf678ad97b82d","_cell_guid":"dbff8796-ef61-4356-8cb8-d7dd1ab18e22"},"source":"def clean_df(df):\n    # Clean up missing attributes\n    df['brand_name']=df['brand_name'].fillna('Unknown')\n    df['category_name']=df['category_name'].fillna('Unknown/Unknown/Unknown')\n    df['item_description']=df['item_description'].fillna('Unknown')\n    df['item_condition_id']=df['item_condition_id'].fillna(3)\n\n    # Handle category split, and count lengths of name/desc\n    df[['category1', 'category2', 'category3']] = df['category_name'].str.split('/', 2, expand=True)\n    df['category12'] = df['category1'].astype(str)+'_'+df['category2'].astype(str)\n    # category123 just = the original category name column.\n\n    df['name_length'] = df['name'].str.len()\n    df['item_description_length'] = df['item_description'].str.len()\n\n    # Throw in a couple word indicators for the fun of it\n    df['word_brand_new'] = df['item_description'].str.lower().str.contains('brand new')\n    df['word_brand_new'] = df['word_brand_new'].astype(int)\n    df['word_tag'] = df['item_description'].str.lower().str.contains('tag').astype(int)\n    df['word_tag'] = df['word_tag'].astype(int)\n    # TODO: Use unicodedata.normalize(\"NFKD\", text.casefold()) eventually instead of .lower() ?\n\n    return df\n    \ndf_train = clean_df(df_train)\ndf_test = clean_df(df_test)\n\n","cell_type":"code","outputs":[],"execution_count":5},{"metadata":{"collapsed":true,"_uuid":"f7680dac74f9c3c21718cdc4e91983d9c3942754","_cell_guid":"fa5144dc-de3e-4811-bb13-6c2e5171c6cc"},"source":"# From a grouped name/price aggregation, extracts the kv pairs to a dict for fast lookups.\ndef create_pricedict(grouped, operation_name, orig_colname):\n    pricedict = {}\n    for index, row in grouped.iterrows():\n        pricedict[row[orig_colname]] = row['price']\n    return {orig_colname + \"_\" + operation_name: pricedict}\n\n\n# This training knowledge creates reusable lookups, let's hold on to it for reuse later as pricedicts!\npricedicts = {}    \nfor col in ['brand_name', 'category1', 'category12', 'category_name']:\n    pricedicts.update(create_pricedict(df_train.groupby(col, as_index=False).min(), 'min', col))\n    pricedicts.update(create_pricedict(df_train.groupby(col, as_index=False).median(), 'median', col))\n    pricedicts.update(create_pricedict(df_train.groupby(col, as_index=False).mean(), 'mean', col))\n    pricedicts.update(create_pricedict(df_train.groupby(col, as_index=False).max(), 'max', col))","cell_type":"code","outputs":[],"execution_count":6},{"metadata":{"collapsed":true,"_uuid":"ac3a9c0bf5f2fa5c06c18f64f1e242c70aab6a29","_cell_guid":"3f83303c-8d57-4a7d-a7a4-3b7e1a60d992"},"source":"# Now we add the features to the chosen df; this is the real work done on each df.\ndef price_augment_df(df, pricedicts):\n    for col in ['brand_name', 'category1', 'category12', 'category_name']:\n        for oper in ['_min', '_median', '_mean', '_max']:\n            df[col + oper] = df[col].map(pricedicts[col + oper])\n            df[col + oper] = df[col + oper].fillna(pricedicts[col + oper].get('Unknown', 5))\n\nprice_augment_df(df_train, pricedicts)\nprice_augment_df(df_test, pricedicts)\n","cell_type":"code","outputs":[],"execution_count":7},{"metadata":{"collapsed":true,"_uuid":"487ab86ebf4fb2cb4129f26a17aeaec87eba9191","_cell_guid":"c866e137-c873-4c4e-ba9f-3c8449154fed"},"source":"xs = ['item_condition_id', 'shipping', \n      'brand_name_min', 'brand_name_max', 'brand_name_median', 'brand_name_mean', \n      'category_name_min', 'category_name_max', 'category_name_median', 'category_name_mean', 'word_brand_new', 'word_tag']\n\ndf_train_xs = df_train[xs]\ndf_train_y = df_train[['price']]\n\ndf_test_xs  = df_test[xs]\n","cell_type":"code","outputs":[],"execution_count":8},{"metadata":{"_uuid":"a97f7269dc5d675bd15faf9ec194a1b4164dafa6","_cell_guid":"d5cf6f3d-96d1-45c4-b844-714b158b66eb"},"source":"# Create linear regression object\nregr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(df_train_xs, df_train_y)\n\n# Make predictions using the testing set\ndf_test['price'] = regr.predict(df_test_xs)\n\n# Negative prices throw an error, so fix them for now.\ndf_test.loc[df_test.price < 0, 'price'] = 0\n\n# The coefficients\nprint('Coefficients: \\n', regr.coef_)\n\n# The mean squared error\n#print(\"Mean squared error: %.2f\" % mean_squared_error(df_test_y, y_pred))\n# Explained variance score: 1 is perfect prediction\n# print('Variance score: %.2f' % r2_score(df_test_y, y_pred))\n","cell_type":"code","outputs":[],"execution_count":9},{"metadata":{"collapsed":true,"_uuid":"77a9ae3e61e6c8e3efff364f32908e03d41c5c94","_cell_guid":"e08cb627-e195-40a3-8834-c9fc9bfa06be"},"source":"submissiondf = df_test[['test_id', 'price']]\nsubmissiondf.to_csv('sample_submission.csv', index=False)","cell_type":"code","outputs":[],"execution_count":10},{"metadata":{"collapsed":true,"_uuid":"ac7304fda6b9c43d2caa1d9a2020102bf7ae3130","_cell_guid":"07e8d977-7ca6-4b71-b80e-cb77b028a798"},"source":"","cell_type":"code","outputs":[],"execution_count":null}],"nbformat_minor":1,"metadata":{"language_info":{"mimetype":"text/x-python","version":"3.6.3","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","name":"python","pygments_lexer":"ipython3","nbconvert_exporter":"python"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4}