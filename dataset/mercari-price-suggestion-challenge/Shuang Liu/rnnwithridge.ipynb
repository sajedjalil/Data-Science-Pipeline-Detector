{"cells":[{"metadata":{"_cell_guid":"5b986c3f-64a6-4d45-948a-ec114abf9a26","_uuid":"5b4cd66934364f7ed43dba5a2fe792edd2f232fc","collapsed":true},"cell_type":"markdown","source":"# Description\n\nThis is an associated model using RNN ,Ridge, and a RidgeCV to solve Mercari Price Suggestion Challenge competition. It currently gets a RMSLE of ~0.419 in the development set and around ~0.427 in the competition, which is better than the currently public Kernals for the competition. I figure I don't have time to keep working on this for the next few weeks so I will simply share this model, along with some notes and thoughts, so that it can help others with their own models or function as a good jumping off point. After this kernal, I will probably bow out for the rest of the comp. If you find this kernal useful, please give me some of them sweet sweet kernel likes.\n\nI left some old code attempts in comments as well as a few repeated lines. This is just more matterial to play with if they change the code around.\n\n(Referenced Kernal links at the bottom.)","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b3ac57a4-fbde-48c0-ba83-4e6281defc1d","_uuid":"dd786f0add7f6e630dacbfce2c9e83ca4d911a09"},"cell_type":"markdown","source":"## Import packages\n\nImport all needed packages for constructing models and solving the competition","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"adb99d41-c151-4dc7-80d9-405ec3d45a18","_uuid":"ef91ec21ad7dbb55a77b760091e35ea8bdbb9a91","trusted":true},"cell_type":"code","source":"from datetime import datetime \nstart_real = datetime.now()\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Dropout, Dense, concatenate, GRU, Embedding, Flatten, Activation\n# from keras.layers import Bidirectional\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras import backend as K\nfrom nltk.corpus import stopwords\nimport math\n# set seed\nnp.random.seed(123)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"da0c86f0-897b-42e6-8bf4-792d7a8bdc3d","_uuid":"2fe904526a603d8f0ff4a9b1c8f6cef40f0702ea"},"cell_type":"markdown","source":"## Define RMSL Error Function\nThis is for checking the predictions at the end. Note that the Y and Y_pred will already be in log scale by the time this is used, so no need to log them in the function.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b84221d8-2ced-43db-90c8-94ef5ee9e707","_uuid":"3b4290b96e65b16b80a7f171b44f546f3b2ea493","collapsed":true,"trusted":true},"cell_type":"code","source":"def rmsle(Y, Y_pred):\n    assert Y.shape == Y_pred.shape\n    return np.sqrt(np.mean(np.square(Y_pred - Y )))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"883b408d-686f-4c54-93e6-5632a21f2220","_uuid":"22e7509badd537bbc17d9a1d823915889b4a6933"},"cell_type":"markdown","source":"## Load train and test data","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"949a8eb5-a530-46ae-8874-859a785967f8","_uuid":"ed0effa8967da098941de9ea8521de8bb4544da3","trusted":true},"cell_type":"code","source":"%%time\n\ntrain_df = pd.read_table('../input/train.tsv')\ntest_df = pd.read_table('../input/test.tsv')\nprint(train_df.shape, test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3d6235a8-2b92-43c7-a1f4-f1dbe105c88b","_uuid":"091d0eb8a2caee61aa7650fa85b9228972270d23"},"cell_type":"markdown","source":"## Preprossing the data for RNN and Ridge models\nThis is the preprocessing that can be done for both models at the same time","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"a1484165-9071-4672-bc07-6c9634f45116","_uuid":"6eee032eb59c13071f1367c92019ffc9d77ab9d0"},"cell_type":"markdown","source":"Remove low prices, anything below 3. Mercari does not allow postings below 3 so below that is an error. Removing them helps the models.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c8de06b8-d733-4f85-bd79-839543a2f97f","_uuid":"fb54e437c2a6d9d30a5eb1e25af16c48e40a52a9","collapsed":true,"trusted":true},"cell_type":"code","source":"# remove low prices\ntrain_df = train_df.drop(train_df[(train_df.price < 3.0)].index)\ntrain_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2ea0ee1e-b83b-47b5-8786-8249fe2d912a","_uuid":"86e64f0ea46c21dbdbd840d5fb00ccfb6a4fc00f"},"cell_type":"markdown","source":"Mercari also does not allow postings over 2000. Could get rid of those, but only 3 and they are only a few dollars higher so that is probably just shipping fees.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"9d63d456-4088-4e1b-8538-589f6a2ae60a","_uuid":"193ff2acb220af238389dfa406b1e191562de358","collapsed":true,"trusted":true},"cell_type":"code","source":"# train_df = train_df.drop(train_df[(train_df.price > 2000)].index)\n# train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fb61873e-5714-438b-a603-8f7df50125f4","_uuid":"79778ede68c68b34c91eb13e37abf5a6bb18cb57"},"cell_type":"markdown","source":"The following block removes stopwords. The models are robust against stopwords but it might help to remove them when looking for such small improvements with such a strict time limit. However, this did not seem to help enough to merit the 1-2 minutes it takes for this block to run.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"8915d778-7232-49d4-8765-fd0247404fab","_uuid":"50e63ca309e741f61a2b00f5bc00dd2a8f5bbca8","collapsed":true,"trusted":true},"cell_type":"code","source":"# %%time\n\n# stop = stopwords.words('english')\n# train_df.item_description.fillna(value='No description yet', inplace=True)\n# train_df['item_description'] = train_df['item_description'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n# train_df.name.fillna(value=\"missing\", inplace=True)\n# train_df['name'] = train_df['name'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n\n# test_df.item_description.fillna(value='No description yet', inplace=True)\n# test_df['item_description'] = test_df['item_description'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n# test_df.name.fillna(value=\"missing\", inplace=True)\n# test_df['name'] = test_df['name'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n\n# train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"058b2587-2e28-4a6c-9e20-f347876be9c6","_uuid":"4278639276800659944935d1011fa81d74cba9e7"},"cell_type":"markdown","source":"The length of the description, that is the raw number of words used, does have some correlation with price. The RNN might find this out on it's own, but since a max depth is used to save computations, it does not always know. Description length clearly helps the model, name length maybe not so much. Does not hurt the models so leaving name length in.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"01bae3d0-9cb9-4282-9cd8-87228e1488b4","_uuid":"0845d793f16ca4b36b6c86822a9028b2ac696251","collapsed":true,"trusted":true},"cell_type":"code","source":"%%time\n# get name and description lengths\ndef wordCount(text):\n    try:\n        if text == 'No description yet':\n            return 0\n        else:\n            text = text.lower()\n            words = [w for w in text.split(\" \")]\n            return len(words)\n    except: \n        return 0\ntrain_df['desc_len'] = train_df['item_description'].apply(lambda x: wordCount(x))\ntest_df['desc_len'] = test_df['item_description'].apply(lambda x: wordCount(x))\ntrain_df['name_len'] = train_df['name'].apply(lambda x: wordCount(x))\ntest_df['name_len'] = test_df['name'].apply(lambda x: wordCount(x))\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6ea3d95d-561e-4ff2-9563-5232648b97a4","_uuid":"29819edf2648b39975f7e431e0cdef4e4cebd2e6"},"cell_type":"markdown","source":"Here we split the category_name into 3 parts. Our models can get more information this way. I tried making a small 3 part RNN layer for this instead which does worse than this method *but* is occasionally faster.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"8933d90c-32b2-4fcb-abf5-58edb50d9870","_uuid":"0c77f5bc7457ee7646f9b50918a14361408adc4f","collapsed":true,"trusted":true},"cell_type":"code","source":"%%time\n# split category name into 3 parts\ndef split_cat(text):\n    try: return text.split(\"/\")\n    except: return (\"No Label\", \"No Label\", \"No Label\")\ntrain_df['subcat_0'], train_df['subcat_1'], train_df['subcat_2'] = \\\nzip(*train_df['category_name'].apply(lambda x: split_cat(x)))\ntest_df['subcat_0'], test_df['subcat_1'], test_df['subcat_2'] = \\\nzip(*test_df['category_name'].apply(lambda x: split_cat(x)))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"94316c8c-1c6b-4065-92b4-830c0da359eb","_uuid":"d2ec2dbe5a464db771e0d835e1f937363d0356f8"},"cell_type":"markdown","source":"The brand name data is sparse, missing over 600,000 values. This gets *some* of those values back by checking their names. However, It does not seem to help the models either way at this point. An *exact* name match against all_brand names will find about 3000 of these. We can be pretty confident in these. At the other extreme, we can search for *any* matches throughout all words in name. This finds over 200,000 but a lot of these are incorrect. Can land somewhere in the middle by either keeping cases or trimming out some of the 5000 brand names.\n\nFor example, PINK is a brand by victoria secret. If we remove case, then almost all *pink* items are labeled as PINK brand. The other issue is that some of the \"brand names\" are not brands but really categories like \"Boots\" or \"Keys\". \n\nCurrently, checking every word in name of a case-sensitive match does best. This gets around 137,000 finds while avoiding the problems with brands like PINK.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"1fd00304-f8b1-48fd-acd2-4601a12f3c7b","_uuid":"1fd724dc18f501c1c057e5a28fcf5e5d086f9dad","collapsed":true,"trusted":true},"cell_type":"code","source":"# %%time\n# attempt to find missing brand names\n# train_df['name'] = train_df.name.str.lower()\n# train_df['brand_name'] = train_df.brand_name.str.lower()\n# test_df['name'] = test_df.name.str.lower()\n# test_df['brand_name'] = test_df.brand_name.str.lower()\nfull_set = pd.concat([train_df,test_df])\nall_brands = set(full_set['brand_name'].values)\ntrain_df.brand_name.fillna(value=\"missing\", inplace=True)\ntest_df.brand_name.fillna(value=\"missing\", inplace=True)\n\n# get to finding!\npremissing = len(train_df.loc[train_df['brand_name'] == 'missing'])\ndef brandfinder(line):\n    brand = line[0]\n    name = line[1]\n    namesplit = name.split(' ')\n    if brand == 'missing':\n        for x in namesplit:\n            if x in all_brands:\n                return name\n    if name in all_brands:\n        return name\n    return brand\ntrain_df['brand_name'] = train_df[['brand_name','name']].apply(brandfinder, axis = 1)\ntest_df['brand_name'] = test_df[['brand_name','name']].apply(brandfinder, axis = 1)\nfound = premissing-len(train_df.loc[train_df['brand_name'] == 'missing'])\nprint(found)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ba28847d-aa45-4153-9ae1-fd0aa0f48ac4","_uuid":"2262c6d7cbe571316d4f64bcac6647fe0b98654c"},"cell_type":"markdown","source":"Standard split the train test for validation and log the price","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"ed6bc20a-1674-454e-9a88-c7a7fa05dcae","_uuid":"cd3fe6e0c98aa258ac9747b58bfb865451df1016","collapsed":true,"trusted":true},"cell_type":"code","source":"# Scale target variable to log.\ntrain_df[\"target\"] = np.log1p(train_df.price)\n\n# Split training examples into train/dev examples.\ntrain_df, dev_df = train_test_split(train_df, random_state=123, train_size=0.99)\n\n# Calculate number of train/dev/test examples.\nn_trains = train_df.shape[0]\nn_devs = dev_df.shape[0]\nn_tests = test_df.shape[0]\nprint(\"Training on\", n_trains, \"examples\")\nprint(\"Validating on\", n_devs, \"examples\")\nprint(\"Testing on\", n_tests, \"examples\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"34572b8c-63cc-4524-8acc-1cd7d5c515cc","_uuid":"7d562df976dc974844db6995ceca669b9a145e31"},"cell_type":"markdown","source":"# RNN Model\n\nThis section will use RNN Model to solve the competition with following steps:\n\n1. Preprocessing data\n1. Define RNN model\n1. Fitting RNN model on training examples\n1. Evaluating RNN model on dev examples\n1. Make prediction for test data using RNN model","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"92abc87d-6fb1-4000-b0ae-c21f364694d3","_uuid":"46d9a48d339dd0255ba8750ca6c2a24b4c8fa786","collapsed":true,"trusted":true},"cell_type":"code","source":"# Concatenate train - dev - test data for easy to handle\nfull_df = pd.concat([train_df, dev_df, test_df])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fb5e57de-c1e3-42cb-aa4a-9d27aa8d4db2","_uuid":"779b5a4dc58578c7839d2a7c8b10070fa8438671"},"cell_type":"markdown","source":"## Fill missing data\nNote that replacing 'No description yet' with \"missing\" helps the model a bit by treating it the same as the NA values","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"266ae953-b73f-43f2-ad52-1adf146478fc","_uuid":"5976cac976353d474a8e81663a1be05bda1a4eef","collapsed":true,"trusted":true},"cell_type":"code","source":"%%time\n\n# Filling missing values\ndef fill_missing_values(df):\n    df.category_name.fillna(value=\"missing\", inplace=True)\n    df.brand_name.fillna(value=\"missing\", inplace=True)\n    df.item_description.fillna(value=\"missing\", inplace=True)\n    df.item_description.replace('No description yet',\"missing\", inplace=True)\n    return df\n\nprint(\"Filling missing data...\")\nfull_df = fill_missing_values(full_df)\nprint(full_df.category_name[1])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8203fba3-991d-4d53-b4ee-ef731782ddd7","_uuid":"bbf287ccc320ff94ccac70d44d8cdbc70b03d154"},"cell_type":"markdown","source":"## Process categorical data","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c092125b-dfeb-4621-bc47-d3ef28880b5d","_uuid":"e18af15196678765a108ace4020f4ad89f6a5e55","collapsed":true,"trusted":true},"cell_type":"code","source":"%%time\n\nprint(\"Processing categorical data...\")\nle = LabelEncoder()\n# full_df.category = full_df.category_name\nle.fit(full_df.category_name)\nfull_df['category'] = le.transform(full_df.category_name)\n\nle.fit(full_df.brand_name)\nfull_df.brand_name = le.transform(full_df.brand_name)\n\nle.fit(full_df.subcat_0)\nfull_df.subcat_0 = le.transform(full_df.subcat_0)\n\nle.fit(full_df.subcat_1)\nfull_df.subcat_1 = le.transform(full_df.subcat_1)\n\nle.fit(full_df.subcat_2)\nfull_df.subcat_2 = le.transform(full_df.subcat_2)\n\ndel le","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"84c7cc19-0784-4653-906c-4ec87f6fd6c2","_uuid":"48418acfaa734e761cadf5976ab4befff08ac8bb"},"cell_type":"markdown","source":"## Process text data\nFrom here til the end of the RNN model are some commented out code lines when I used a short RNN layer to process category_name. Using the 3 subcats makes a better model but can sometimes be *slightly* slower.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"e376d24d-87b6-4de6-9748-3c2d7c3b2834","_uuid":"9e11c6b6c0b413335df0be0f1f17ee5a6d48844b","collapsed":true,"trusted":true},"cell_type":"code","source":"# %%time\n# # Break category_name into parts\n# def catgsub(col):\n#     col = col.str.replace(' ','')\n#     col = col.str.replace('/',' ')\n#     col = col.str.replace('&','')\n#     return col\n# full_df['category_name'] = catgsub(full_df['category_name'])\n# print(full_df.category_name[1])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4967bb28-577c-4d15-b956-6f66787dd7bf","_uuid":"dddf4d1490a92501c893c29a02e6f9b67546d984","collapsed":true,"trusted":true},"cell_type":"code","source":"%%time\n\nprint(\"Transforming text data to sequences...\")\nraw_text = np.hstack([full_df.item_description.str.lower(), full_df.name.str.lower(), full_df.category_name.str.lower()])\n\nprint(\"   Fitting tokenizer...\")\ntok_raw = Tokenizer()\ntok_raw.fit_on_texts(raw_text)\n\nprint(\"   Transforming text to sequences...\")\nfull_df['seq_item_description'] = tok_raw.texts_to_sequences(full_df.item_description.str.lower())\nfull_df['seq_name'] = tok_raw.texts_to_sequences(full_df.name.str.lower())\n# full_df['seq_category'] = tok_raw.texts_to_sequences(full_df.category_name.str.lower())\n\ndel tok_raw","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2b7ddb31-7092-4010-b164-ac74ed62300b","_uuid":"bc37c8c0b0a764bd77b898ec86e18ab7ba7e9f0b","collapsed":true,"trusted":true},"cell_type":"code","source":"full_df['seq_name'][:5]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fa5d2aba-2538-45a9-beb6-2698c4279ba0","_uuid":"25c6f4f075e8bad52111bd95c81cd615c6cecdad"},"cell_type":"markdown","source":"# Define constants to use when define RNN model\nNote the comments next to the first few lines indicate the longest entry in that column. Just for reference.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"bdad0de1-2288-4a0e-ab61-e115284ac70f","_uuid":"db44552705681d6b48938a4a102ce002a92e2850","collapsed":true,"trusted":true},"cell_type":"code","source":"MAX_NAME_SEQ = 10 #17\nMAX_ITEM_DESC_SEQ = 75 #269\nMAX_CATEGORY_SEQ = 8 #8\nMAX_TEXT = np.max([\n    np.max(full_df.seq_name.max()),\n    np.max(full_df.seq_item_description.max()),\n#     np.max(full_df.seq_category.max()),\n]) + 100\nMAX_CATEGORY = np.max(full_df.category.max()) + 1\nMAX_BRAND = np.max(full_df.brand_name.max()) + 1\nMAX_CONDITION = np.max(full_df.item_condition_id.max()) + 1\nMAX_DESC_LEN = np.max(full_df.desc_len.max()) + 1\nMAX_NAME_LEN = np.max(full_df.name_len.max()) + 1\nMAX_SUBCAT_0 = np.max(full_df.subcat_0.max()) + 1\nMAX_SUBCAT_1 = np.max(full_df.subcat_1.max()) + 1\nMAX_SUBCAT_2 = np.max(full_df.subcat_2.max()) + 1","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"eacd3e58-9c02-4cee-b526-7f7a7e9f2dcf","_uuid":"c199135fa649b0fe03e4817ca7313cfb5fbd57db"},"cell_type":"markdown","source":"## Get data for RNN model","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"52c34eb2-9039-4c63-81b2-4662d69c1137","_uuid":"e930d4cb31f6cc233e5144be04650b89afc5bc4f","collapsed":true,"trusted":true},"cell_type":"code","source":"%%time\n\ndef get_rnn_data(dataset):\n    X = {\n        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ),\n        'item_desc': pad_sequences(dataset.seq_item_description, maxlen=MAX_ITEM_DESC_SEQ),\n        'brand_name': np.array(dataset.brand_name),\n        'category': np.array(dataset.category),\n#         'category_name': pad_sequences(dataset.seq_category, maxlen=MAX_CATEGORY_SEQ),\n        'item_condition': np.array(dataset.item_condition_id),\n        'num_vars': np.array(dataset[[\"shipping\"]]),\n        'desc_len': np.array(dataset[[\"desc_len\"]]),\n        'name_len': np.array(dataset[[\"name_len\"]]),\n        'subcat_0': np.array(dataset.subcat_0),\n        'subcat_1': np.array(dataset.subcat_1),\n        'subcat_2': np.array(dataset.subcat_2),\n    }\n    return X\n\ntrain = full_df[:n_trains]\ndev = full_df[n_trains:n_trains+n_devs]\ntest = full_df[n_trains+n_devs:]\n\nX_train = get_rnn_data(train)\nY_train = train.target.values.reshape(-1, 1)\n\nX_dev = get_rnn_data(dev)\nY_dev = dev.target.values.reshape(-1, 1)\n\nX_test = get_rnn_data(test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4d668a8c-7bcd-4c00-95cc-8cc0d3641b55","_uuid":"375f4e53dea8e4c716c36472bb9de4f85a0a8d16"},"cell_type":"markdown","source":"Here are some unused RMSE and RMSLE functions. They can be used as a loss function in the model but the built in 'mse' works just as well. Worth having around just in case. There needs to be a small non-zero number added to the means or a zero value might sneak in and cause it to return NaN. As is, they work fine as a loss function just not special.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"77b0f028-d271-469c-ad24-1fdfbab09027","_uuid":"10f87db20c1a42d534aff0c6f03a2f97b2f3c3c6","collapsed":true,"trusted":true},"cell_type":"code","source":"def root_mean_squared_logarithmic_error(y_true, y_pred):\n    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n    return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1)+0.0000001)\ndef root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)+0.0000001)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7c6a54f1-81ce-49c7-b2e2-e38eced7a958","_uuid":"a6b6da051c3ab6d278e1986de9244e7f06628cee"},"cell_type":"markdown","source":"## Define RNN model\nNow to build the model. Old category stuff is commented out but left in case of revist. (other adjustment notes in comments)","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"5e681e7b-ce3c-41cd-b182-b2a9fbe3f214","_uuid":"314ef9524f32da1a031389025dd4848d56b0752f","collapsed":true,"trusted":true},"cell_type":"code","source":"# set seed again in case testing models adjustments by looping next 2 blocks\nnp.random.seed(123)\n\ndef new_rnn_model(lr=0.001, decay=0.0):\n    # Inputs\n    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n    brand_name = Input(shape=[1], name=\"brand_name\")\n#     category = Input(shape=[1], name=\"category\")\n#     category_name = Input(shape=[X_train[\"category_name\"].shape[1]], name=\"category_name\")\n    item_condition = Input(shape=[1], name=\"item_condition\")\n    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n    desc_len = Input(shape=[1], name=\"desc_len\")\n    name_len = Input(shape=[1], name=\"name_len\")\n    subcat_0 = Input(shape=[1], name=\"subcat_0\")\n    subcat_1 = Input(shape=[1], name=\"subcat_1\")\n    subcat_2 = Input(shape=[1], name=\"subcat_2\")\n\n    # Embeddings layers (adjust outputs to help model)\n    emb_name = Embedding(MAX_TEXT, 20)(name)\n    emb_item_desc = Embedding(MAX_TEXT, 60)(item_desc)\n    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n#     emb_category_name = Embedding(MAX_TEXT, 20)(category_name)\n#     emb_category = Embedding(MAX_CATEGORY, 10)(category)\n    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n    emb_desc_len = Embedding(MAX_DESC_LEN, 5)(desc_len)\n    emb_name_len = Embedding(MAX_NAME_LEN, 5)(name_len)\n    emb_subcat_0 = Embedding(MAX_SUBCAT_0, 10)(subcat_0)\n    emb_subcat_1 = Embedding(MAX_SUBCAT_1, 10)(subcat_1)\n    emb_subcat_2 = Embedding(MAX_SUBCAT_2, 10)(subcat_2)\n    \n\n    # rnn layers (GRUs are faster than LSTMs and speed is important here)\n    rnn_layer1 = GRU(16) (emb_item_desc)\n    rnn_layer2 = GRU(8) (emb_name)\n#     rnn_layer3 = GRU(8) (emb_category_name)\n\n    # main layers\n    main_l = concatenate([\n        Flatten() (emb_brand_name)\n#         , Flatten() (emb_category)\n        , Flatten() (emb_item_condition)\n        , Flatten() (emb_desc_len)\n        , Flatten() (emb_name_len)\n        , Flatten() (emb_subcat_0)\n        , Flatten() (emb_subcat_1)\n        , Flatten() (emb_subcat_2)\n        , rnn_layer1\n        , rnn_layer2\n#         , rnn_layer3\n        , num_vars\n    ])\n    # (incressing the nodes or adding layers does not effect the time quite as much as the rnn layers)\n    main_l = Dropout(0.1)(Dense(512,kernel_initializer='normal',activation='relu') (main_l))\n    main_l = Dropout(0.1)(Dense(256,kernel_initializer='normal',activation='relu') (main_l))\n    main_l = Dropout(0.1)(Dense(128,kernel_initializer='normal',activation='relu') (main_l))\n    main_l = Dropout(0.1)(Dense(64,kernel_initializer='normal',activation='relu') (main_l))\n\n    # the output layer.\n    output = Dense(1, activation=\"linear\") (main_l)\n    \n    model = Model([name, item_desc, brand_name , item_condition, \n                   num_vars, desc_len, name_len, subcat_0, subcat_1, subcat_2], output)\n\n    optimizer = Adam(lr=lr, decay=decay)\n    # (mean squared error loss function works as well as custom functions)  \n    model.compile(loss = 'mse', optimizer = optimizer)\n\n    return model\n\nmodel = new_rnn_model()\nmodel.summary()\ndel model","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e26a8a7b-f946-49ca-baf3-bd05fbd62ea8","_uuid":"b5b91c2ed4c45d859624c512a15e53d829e7c8df"},"cell_type":"markdown","source":"## Fit RNN model to train data\nThis is where most of the time is spent. It takes around 35-40 minutes to run the RNN model. 2 epochs with smaller batches tends to do better than more epochs with larger batches. Trimming time off here will be important if adding more models.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"0fe2e204-3df1-4d42-b85c-2963bc1f168f","_uuid":"6c6b35398cab7ab1e88ccc532d9abdf798fb67c4","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"%%time\n\n# Set hyper parameters for the model.\nBATCH_SIZE = 512 * 3\nepochs = 2\n\n# Calculate learning rate decay.\nexp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\nsteps = int(len(X_train['name']) / BATCH_SIZE) * epochs\nlr_init, lr_fin = 0.005, 0.001\nlr_decay = exp_decay(lr_init, lr_fin, steps)\n\n# Create model and fit it with training dataset.\nrnn_model = new_rnn_model(lr=lr_init, decay=lr_decay)\nrnn_model.fit(\n        X_train, Y_train, epochs=epochs, batch_size=BATCH_SIZE,\n        validation_data=(X_dev, Y_dev), verbose=1,\n)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5c71f814-7f4c-42a3-8030-9e869df6cb5d","_uuid":"e172a260b930d63a679cb21f084342755d60a8b4"},"cell_type":"markdown","source":"## Evaluate RNN model on dev data","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b48221eb-ac00-4bd3-bc4e-6465b40a87fc","_uuid":"0e671e64429d2eca87bf0cacabb307d67da3e6c7","collapsed":true,"trusted":true},"cell_type":"code","source":"%%time\n\nprint(\"Evaluating the model on validation data...\")\nY_dev_preds_rnn = rnn_model.predict(X_dev, batch_size=BATCH_SIZE)\nprint(\" RMSLE error:\", rmsle(Y_dev, Y_dev_preds_rnn))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"106ddfdc-ac33-432d-8b6a-db656680480d","_uuid":"9ccfd39ff8da963f36fa45c1e0511f3f5683033a"},"cell_type":"markdown","source":"## Make prediction for test data","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"f0356339-126f-491c-9d22-c9c128546168","_uuid":"35c4ed5d85952ec34f0059535e753cd537faab53","collapsed":true,"trusted":true},"cell_type":"code","source":"rnn_preds = rnn_model.predict(X_test, batch_size=BATCH_SIZE, verbose=1)\nrnn_preds = np.expm1(rnn_preds)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3ce824f3-dcbd-453b-92ba-e490e4cd9d84","_uuid":"74cbbf48ca09e2f487ca6f6a7e170c2f58c32291"},"cell_type":"markdown","source":"# Ridge Models\n\nNow onto the Ridge models. Less to play with in the Ridge models but it is faster than the RNN. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"34c19c19-e90e-4a3e-b4a7-eb55ca00848c","_uuid":"d234a8ac2c9aa67c7e173fcab684f4856ed82752","collapsed":true,"trusted":true},"cell_type":"code","source":"# Concatenate train - dev - test data for easy to handle\nfull_df = pd.concat([train_df, dev_df, test_df])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"950a555e-fc91-45db-b969-fbe141af9aea","_uuid":"02fe1644a1da61d0e4e6d88f13e03fb60f09d43a"},"cell_type":"markdown","source":"## Handle missing data and convert data type to string\nAll inputs must be strings in a ridge model. The other note here is that filling NAs for item_description use 'No description yet' so it is read the same as the 'No description yet' entries.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"8835e22b-d064-4bc6-acb4-567c44d4428f","_uuid":"7cc69ef79fecd5dc042599f3ec04d0207668d287","collapsed":true,"trusted":true},"cell_type":"code","source":"%%time\n\nprint(\"Handling missing values...\")\nfull_df['category_name'] = full_df['category_name'].fillna('missing').astype(str)\nfull_df['subcat_0'] = full_df['subcat_0'].astype(str)\nfull_df['subcat_1'] = full_df['subcat_1'].astype(str)\nfull_df['subcat_2'] = full_df['subcat_2'].astype(str)\nfull_df['brand_name'] = full_df['brand_name'].fillna('missing').astype(str)\nfull_df['shipping'] = full_df['shipping'].astype(str)\nfull_df['item_condition_id'] = full_df['item_condition_id'].astype(str)\nfull_df['desc_len'] = full_df['desc_len'].astype(str)\nfull_df['name_len'] = full_df['name_len'].astype(str)\nfull_df['item_description'] = full_df['item_description'].fillna('No description yet').astype(str)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"185c6a6d-5d2a-4fd7-a7f2-747c680fab8e","_uuid":"51dabf318564e7521af76ac161d2086d8d0ff599"},"cell_type":"markdown","source":"## Vectorizing all the data\nTakes around 8-10 minutes depending on the inputs used.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"04a35027-7b04-4e7c-a432-d0178fcbecac","_uuid":"575135f0c3eede733bfdf704853204905580207e","collapsed":true,"trusted":true},"cell_type":"code","source":"%%time\n\nprint(\"Vectorizing data...\")\ndefault_preprocessor = CountVectorizer().build_preprocessor()\ndef build_preprocessor(field):\n    field_idx = list(full_df.columns).index(field)\n    return lambda x: default_preprocessor(x[field_idx])\n\nvectorizer = FeatureUnion([\n    ('name', CountVectorizer(\n        ngram_range=(1, 2),\n        max_features=50000,\n        preprocessor=build_preprocessor('name'))),\n#     ('category_name', CountVectorizer(\n#         token_pattern='.+',\n#         preprocessor=build_preprocessor('category_name'))),\n    ('subcat_0', CountVectorizer(\n        token_pattern='.+',\n        preprocessor=build_preprocessor('subcat_0'))),\n    ('subcat_1', CountVectorizer(\n        token_pattern='.+',\n        preprocessor=build_preprocessor('subcat_1'))),\n    ('subcat_2', CountVectorizer(\n        token_pattern='.+',\n        preprocessor=build_preprocessor('subcat_2'))),\n    ('brand_name', CountVectorizer(\n        token_pattern='.+',\n        preprocessor=build_preprocessor('brand_name'))),\n    ('shipping', CountVectorizer(\n        token_pattern='\\d+',\n        preprocessor=build_preprocessor('shipping'))),\n    ('item_condition_id', CountVectorizer(\n        token_pattern='\\d+',\n        preprocessor=build_preprocessor('item_condition_id'))),\n    ('desc_len', CountVectorizer(\n        token_pattern='\\d+',\n        preprocessor=build_preprocessor('desc_len'))),\n    ('name_len', CountVectorizer(\n        token_pattern='\\d+',\n        preprocessor=build_preprocessor('name_len'))),\n    ('item_description', TfidfVectorizer(\n        ngram_range=(1, 3),\n        max_features=100000,\n        preprocessor=build_preprocessor('item_description'))),\n])\n\nX = vectorizer.fit_transform(full_df.values)\n\nX_train = X[:n_trains]\nY_train = train_df.target.values.reshape(-1, 1)\n\nX_dev = X[n_trains:n_trains+n_devs]\nY_dev = dev_df.target.values.reshape(-1, 1)\n\nX_test = X[n_trains+n_devs:]\nprint(X.shape, X_train.shape, X_dev.shape, X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0f34eca4-762c-480e-a269-8ed327b9b21f","_uuid":"403ef92d6982787c5f5a46bb5a0710d475046259"},"cell_type":"markdown","source":"## Fitting Ridge model on training data\nA Ridge model with cross validation does *slighly* better than one without, but even with the minimum of 2 CV, it still takes 4-5 minutes. Any more and it becomes impractical for our narrow time limit. A regular ridge model will only take ~30 seconds. So, for the purposes of having another model to predict on, might as well make both.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"ef1a3b85-f68e-4ddd-94e4-4c7765dd0cf5","_uuid":"af45b7a71173895c06eb876b1c9c7e1d0c55b4c1","collapsed":true,"trusted":true},"cell_type":"code","source":"%%time\n\nprint(\"Fitting Ridge model on training examples...\")\nridge_model = Ridge(\n    solver='auto', fit_intercept=True, alpha=1.0,\n    max_iter=100, normalize=False, tol=0.05, random_state = 1,\n)\nridge_modelCV = RidgeCV(\n    fit_intercept=True, alphas=[5.0],\n    normalize=False, cv = 2, scoring='neg_mean_squared_error',\n)\nridge_model.fit(X_train, Y_train)\nridge_modelCV.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1cf25071-13be-43e9-be20-6b1200332fd4","_uuid":"75ca7fe30319238f8a6c5697d5ac5782de9caf93"},"cell_type":"markdown","source":"## Evaluating Ridge model on dev data","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"d56220b1-8fb0-444d-b1f2-80c6d866146c","_uuid":"7722eb4220fe38f6c5676f1f68cf417f741ed014","collapsed":true,"trusted":true},"cell_type":"code","source":"Y_dev_preds_ridge = ridge_model.predict(X_dev)\nY_dev_preds_ridge = Y_dev_preds_ridge.reshape(-1, 1)\nprint(\"RMSL error on dev set:\", rmsle(Y_dev, Y_dev_preds_ridge))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a7da3039-e644-4bb1-8654-c2d82ebde9b1","_uuid":"49e5ec7ac594be05d5f2065b61c02b7b53216a06","collapsed":true,"trusted":true},"cell_type":"code","source":"Y_dev_preds_ridgeCV = ridge_modelCV.predict(X_dev)\nY_dev_preds_ridgeCV = Y_dev_preds_ridgeCV.reshape(-1, 1)\nprint(\"CV RMSL error on dev set:\", rmsle(Y_dev, Y_dev_preds_ridgeCV))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d0cdcc08-ceb7-4121-bfc4-e32e1866f66a","_uuid":"685838b5e39b6cfcce7f87dd1f79fc90f85730fe"},"cell_type":"markdown","source":"## Make prediction for test data","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"84bc9193-99da-494d-a669-93041e111aae","_uuid":"750620b691ad0e208c3c4b9122c3c689fa3a8ba5","collapsed":true,"trusted":true},"cell_type":"code","source":"%%time\n\nridge_preds = ridge_model.predict(X_test)\nridge_preds = np.expm1(ridge_preds)\nridgeCV_preds = ridge_modelCV.predict(X_test)\nridgeCV_preds = np.expm1(ridgeCV_preds)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a23fffab-ceb6-4703-b8fe-c290312a780f","_uuid":"5a6e8228e853d9a71b70fff4cfe3e8797709e6ac"},"cell_type":"markdown","source":"# Evaluating for associated model on dev data\nThis combines the 3 predicts into one. Rather than take a simple average, aggregate predicts will use ratios to vary the weights of the 3 models. It also use a simple loop to run through all the possible ratios to find the best ratio on the dev set. It is not the most computationally efficient loop but it only takes 2 seconds to run so no big deal.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"62efeb5c-ac19-4f33-8896-37f4175fd788","_uuid":"bdcd77a11f45f898f2684aaf4571039ca149f8d6","collapsed":true,"trusted":true},"cell_type":"code","source":"%%time\ndef aggregate_predicts3(Y1, Y2, Y3, ratio1, ratio2):\n    assert Y1.shape == Y2.shape\n    return Y1 * ratio1 + Y2 * ratio2 + Y3 * (1.0 - ratio1-ratio2)\n\n# Y_dev_preds = aggregate_predicts3(Y_dev_preds_rnn, Y_dev_preds_ridgeCV, Y_dev_preds_ridge, 0.4, 0.3)\n# print(\"RMSL error for RNN + Ridge + RidgeCV on dev set:\", rmsle(Y_dev, Y_dev_preds))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a6bf603b-92eb-46eb-83a8-dec6192aa4a8","_uuid":"3981845e611ed9b55a3b8b7d1c0d085bfb389f81","collapsed":true,"trusted":true},"cell_type":"code","source":"%%time\n#ratio optimum finder for 3 models\nbest1 = 0\nbest2 = 0\nlowest = 0.99\nfor i in range(100):\n    for j in range(100):\n        r = i*0.01\n        r2 = j*0.01\n        if r+r2 < 1.0:\n            Y_dev_preds = aggregate_predicts3(Y_dev_preds_rnn, Y_dev_preds_ridgeCV, Y_dev_preds_ridge, r, r2)\n            fpred = rmsle(Y_dev, Y_dev_preds)\n            if fpred < lowest:\n                best1 = r\n                best2 = r2\n                lowest = fpred\n#             print(str(r)+\"-RMSL error for RNN + Ridge + RidgeCV on dev set:\", fpred)\nY_dev_preds = aggregate_predicts3(Y_dev_preds_rnn, Y_dev_preds_ridgeCV, Y_dev_preds_ridge, best1, best2)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"59483a3c-e3f4-4f5a-9cb6-efd3ee7c41f6","_uuid":"1b9f475392bec617de0bff05f42f5beb04c83436","collapsed":true,"trusted":true},"cell_type":"code","source":"print(best1)\nprint(best2)\nprint(\"(Best) RMSL error for RNN + Ridge + RidgeCV on dev set:\", rmsle(Y_dev, Y_dev_preds))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6d138dff-d71f-4a32-900e-8fcf8c90ee61","_uuid":"21f16dbb064c51cad4bc890e7647ada0ed589515"},"cell_type":"markdown","source":"Here is the commented out version for when I was working with 2 models instead of 3. Left here just in case.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"5e1be1ed-2853-4157-b3bf-1ac7c20a7fdb","_uuid":"cb48fc2ac6e913b6200d77d3f3a2fba3f86de30d","collapsed":true,"trusted":true},"cell_type":"code","source":"# %%time\n# def aggregate_predicts2(Y1, Y2,ratio):\n#     assert Y1.shape == Y2.shape\n#     return Y1 * ratio + Y2 * (1.0 - ratio)\n\n# #ratio optimum finder\n# best = 0\n# lowest = 0.99\n# for i in range(100):\n#     r = i*0.01\n#     Y_dev_preds = aggregate_predicts2(Y_dev_preds_rnn, Y_dev_preds_ridge, r)\n#     fpred = rmsle(Y_dev, Y_dev_preds)\n#     if fpred < lowest:\n#         best = r\n#         lowest = fpred\n#     print(str(r)+\"-RMSL error for RNN + Ridge on dev set:\", fpred)\n# Y_dev_preds = aggregate_predicts2(Y_dev_preds_rnn, Y_dev_preds_ridge, best)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a81edb6f-fb2d-48d3-9afc-3c3f954aa558","_uuid":"081b38c1e45242943159d8e44fca976b14f169a2"},"cell_type":"markdown","source":"# Creating Submission\nI create 4 submissions with every run of the code. Might as well if the notebook takes one hour to run. They are all just variations of the aggregate prediction ratios (best, average, and 2 variations). I don't normally submit them all but sometimes a variation will do just a smidge better than the best prediction. When having extra submissions in a day, it could not hurt.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"864a2906-4ff6-4f3b-92a3-7a007f92f503","_uuid":"e60ac502c9d977de54bbfdc91701229b5ad3b14e","collapsed":true,"trusted":true},"cell_type":"code","source":"# best predicted submission\npreds = aggregate_predicts3(rnn_preds, ridgeCV_preds, ridge_preds, best1, best2)\nsubmission = pd.DataFrame({\n        \"test_id\": test_df.test_id,\n        \"price\": preds.reshape(-1),\n})\nsubmission.to_csv(\"./rnn_ridge_submission_best.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"556753d2-1ab3-4220-954c-bcd011d3ea92","_uuid":"45876461bbbe07797e2443936c16eaf3bae4cc79","collapsed":true,"trusted":true},"cell_type":"code","source":"# mean submission\npreds = aggregate_predicts3(rnn_preds, ridgeCV_preds, ridge_preds, 0.334, 0.333)\nsubmission = pd.DataFrame({\n        \"test_id\": test_df.test_id,\n        \"price\": preds.reshape(-1),\n})\nsubmission.to_csv(\"./rnn_ridge_submission_mean.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c069cae6-d2a6-4e0a-8a41-8e3bf3a4538d","_uuid":"af642e0a130077e9a68964b2465261f511f79260","collapsed":true,"trusted":true},"cell_type":"code","source":"# variation 1 submission\npreds = aggregate_predicts3(rnn_preds, ridgeCV_preds, ridge_preds, best1-0.1, best2+0.1)\nsubmission = pd.DataFrame({\n        \"test_id\": test_df.test_id,\n        \"price\": preds.reshape(-1),\n})\nsubmission.to_csv(\"./rnn_ridge_submission_var_1.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b7b19d6c-0f9c-4bf2-a467-318041fb35ea","_uuid":"8d68db9f6130468c68ff84f8ee446b12069c752c","collapsed":true,"trusted":true},"cell_type":"code","source":"# variation 2 submission\npreds = aggregate_predicts3(rnn_preds, ridgeCV_preds, ridge_preds, best1+0.1, best2-0.1)\nsubmission = pd.DataFrame({\n        \"test_id\": test_df.test_id,\n        \"price\": preds.reshape(-1),\n})\nsubmission.to_csv(\"./rnn_ridge_submission_var_2.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"22b99cae-796b-491f-a930-1d2dc07b7eba","_uuid":"c76a1e20f5c8e424fd4ead42f496abf31fab8b99"},"cell_type":"markdown","source":"# Time keeper\nTime keeping is really important in this challenge, as any Kernel that runs more than one hour when published will be rejected and a waste of time. Make sure that the following printout is less than that before publishing, hopefully with some wiggle room.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"e134328d-4b81-4f89-9796-10732b404ed3","_uuid":"6e1f16e0bfb77b3ec8b9ddd74c6f651d9b73201c","collapsed":true,"trusted":true},"cell_type":"code","source":"stop_real = datetime.now()\nexecution_time_real = stop_real-start_real \nprint(execution_time_real)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"31eb19bd-abff-4a51-a0fc-a7119767bdca","_uuid":"6c2cbc5f33ae9df64c1a76916126465e5ae26bcf"},"cell_type":"markdown","source":"# Ideas that might help the models\n\n### RNN model\nNNs always have lots of paramiters to play with, it is a time for predictive power trade. (learning rate/decay, batch size, embedding output dimension, adding or removing layers, ect.) The tricky part is balancing those while keeping the total run time under 1 hour. Less epochs with smaller batch size tend to do better than more epochs with larger batches.  If a comperable RNN could be run in under 15-20 minutes then 2 RNN models might work better for fitting. Either way, if you want to fit in more and/or different models into the aggregate, probably need to trim some fat here. Note that the RNN, since it cannot run that long, is more sensive to any random seed changes.\n\n### Ridge models\nThe more inputs the better with Ridge, but it does cost a bit of time. benifits from weak inputs, like name length, are very marginal but still there. Trimming them out might be better to save time. Aside from changing the inputs, I can only think that changing CV will improve the model notably more but at the cost of much more time. Cutting out the CV model could save a few minutes that could be better spent elsewhere. \n### General\nUsing more and/or different models tends to do better, but the models need to be both good and different from each other. Hard to do with limited time frame.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"2ae17cee-26d3-4a98-ab0a-8b6ee47ffadc","_uuid":"f30b94f118190d2da1e5aefaa6e52ff9aabee8bf"},"cell_type":"markdown","source":"# References\n\nThis was originally based off of this Kernal: https://www.kaggle.com/nvhbk16k53/associated-model-rnn-ridge\n\nWith ideas gained from the visualizations here: https://www.kaggle.com/thykhuely/mercari-interactive-eda-topic-modelling\n\nYou can find description of the competition here https://www.kaggle.com/c/mercari-price-suggestion-challenge","outputs":[],"execution_count":null}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}