{"cells":[{"cell_type":"markdown","source":"I'm new to Kaggle competion. I have reviewed many public kernels before I aggregate many parts into this final working version. I'm mostly inspired by the follow kernels.\n\nhttps://www.kaggle.com/lopuhin/eli5-for-mercari\n\nhttps://www.kaggle.com/apapiu/ridge-script\n\nhttps://www.kaggle.com/thykhuely/mercari-interactive-eda-topic-modelling\n\nhttps://www.kaggle.com/tunguz/more-effective-ridge-lgbm-script-lb-0-44823","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"import eli5\nimport nltk\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import mean_squared_log_error\nfrom nltk.stem.porter import PorterStemmer\nimport gc\nimport re","metadata":{"collapsed":true}},{"cell_type":"markdown","source":"Load and preprocess data.","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"train = pd.read_table('../input/train.tsv')\ntest = pd.read_table('../input/test.tsv')\nSTOP_WORDS = frozenset([\n    \"a\", \"about\", \"after\", \"afterwards\", \"again\",\n    \"all\", \"almost\", \"along\", \"already\", \"also\", \"although\",\n    \"am\", \"among\", \"amongst\", \"amoungst\",  \"an\", \"and\", \"another\",\n    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n    \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n    \"becomes\", \"becoming\", \"been\", \"before\",\"behind\", \"being\",\n    \"beside\", \"between\", \"both\",\n    \"but\", \"by\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n    \"could\", \"couldnt\", \"de\", \"do\",\n    \"each\", \"eg\", \"eight\", \"either\", \"else\",\n    \"elsewhere\", \"etc\", \"even\", \"ever\", \"every\",\n    \"everything\", \"everywhere\",  \"few\",\n    \"find\", \"for\",\n    \"from\", \"go\",\n    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n    \"how\", \"however\", \"i\", \"ie\", \"if\", \"in\"\n    \"into\", \"is\", \"it\", \"its\", \"itself\",\n    \"latterly\", \"ltd\", \"many\", \"may\", \"me\",\n    \"meanwhile\", \"might\", \"mill\", \"mine\",\"moreover\",\n    \"my\", \"myself\",\"neither\",\n    \"never\", \"nevertheless\", \"no\", \"nobody\", \"none\", \"noone\",\n    \"nor\", \"not\", \"now\", \"of\",  \"on\",\n    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n    \"ours\", \"ourselves\", \"out\", \"per\", \"perhaps\",\n     \"re\",\n    \"seeming\", \"seems\", \"she\",\n    \"since\", \"so\", \"some\", \"somehow\", \"someone\",\n    \"something\", \"sometime\", \"sometimes\", \"somewhere\",\n    \"that\", \"the\", \"their\", \"them\",\n    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\",\n    \"this\", \"those\", \"though\",\n    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"toward\", \"towards\",\n    \"twelve\",  \"un\", \"until\", \"up\", \"upon\", \"us\",\n     \"via\", \"was\", \"we\",  \"were\", \"what\", \"when\",\n    \"whence\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n    \"who\", \"whoever\", \"whom\", \"whose\", \"why\", \"with\",\n    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n    \"yourselves\"])\n\nNAME_MIN_DF = 10\nMAX_FEATURES_ITEM_NAME = 50000\nMAX_FEATURES_ITEM_DESCRIPTION = 100000\n\ntransformerWeights={\n        'name': 1.0,\n        'general_cat': 1.0,\n        'subcat_1': 1.0,\n        'subcat_2': 1.0,\n        'brand_name': 1.2,\n        'shipping': 1.0,\n        'item_condition_id': 1.0,\n        'len_description': 1.0,\n        'item_description': 0.8\n    }\n\n\ntrain.drop(train[train.price < 1.0].index, inplace=True)\ntrain = train.reset_index(drop=True)\nnrow_train = train.shape[0]\ntrain_test : pd.DataFrame = pd.concat([train, test])\n\ny_train = np.log1p(train['price'])\n\ndel train\ngc.collect()","metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"train_test['category_name'] = train_test['category_name'].fillna('Other').astype(str)\ntrain_test['brand_name'] = train_test['brand_name'].fillna('missing').astype(str)\ntrain_test['shipping'] = train_test['shipping'].astype(str)\ntrain_test['item_condition_id'] = train_test['item_condition_id'].astype(str)\ntrain_test['item_description'] = train_test['item_description'].fillna('[ndy]')\n\ndef replace_text(df, variable, text_to_replace, replacement):\n    df.loc[df[variable] == text_to_replace, variable] = replacement\n    \n    \nreplace_text(train_test, 'item_description', 'No description yet', '[ndy]')\n\ndef wordCount(text):\n    # convert to lower case and strip regex\n    try:\n         # convert to lower case and strip regex\n        text = text.lower()\n        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n        txt = regex.sub(\" \", text)\n        # tokenize\n        # words = nltk.word_tokenize(clean_txt)\n        # remove words in stop words\n        words = [w for w in txt.split(\" \") \\\n                 if not w in stop_words.ENGLISH_STOP_WORDS and len(w)>3]\n        \n        if len(words) < 30:\n            return \"1\"\n        elif len(words) < 90:\n            return \"2\"\n        elif len(words) < 120:\n            return \"3\"\n        else: \n            return \"4\"\n    except: \n        return \"1\"\n    \ntrain_test['len_description'] = train_test['item_description'].apply(lambda x: wordCount(x))\n\ntrain_test['general_cat'], train_test['subcat_1'], train_test['subcat_2'] = train_test['category_name'].str.split(\"/\", 2).str\ntrain_test.drop('category_name', axis=1, inplace=True)\n\ntrain_test['general_cat'] = train_test['general_cat'].fillna('Other').astype(str)\ntrain_test['subcat_1'] = train_test['subcat_1'].fillna('Other').astype(str)\ntrain_test['subcat_2'] = train_test['subcat_2'].fillna('Other').astype(str)","metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"train_test.head()","metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"y_train.head()","metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"%%time\n\ndefault_preprocessor = CountVectorizer().build_preprocessor()\n\ndef build_preprocessor(field):\n    field_idx = list(train_test.columns).index(field)\n    return lambda x: default_preprocessor(x[field_idx])\n\ndef rex_tokenizer(text):\n    token_pattern = re.compile(r\"(?u)\\b\\w[\\w-]*\\w\\b\")\n    tokens = token_pattern.findall(text)\n    item_list = {\"1tb\" : \"1 tb\", \"2tb\" : \"2 tb\", \"4tb\" : \"4 tb\", \"4g\" : \"4 gb\",\"4gb\" : \"4 gb\",\"8g\" : \"8 gb\",\"8gb\" : \"8 gb\",\"16g\" : \"16 gb\", \"16gb\" : \"16 gb\", \"32gb\" : \"32 gb\", \"32g\" : \"32 gb\",\"64gb\" : \"64 gb\", \"64g\" : \"64 gb\", \"64gb\" : \"64 gb\", \"80gb\" : \"80 gb\", \"120gb\" : \"128 gb\", \"128gb\" : \"128 gb\", \"128g\" : \"128 gb\", \n                 \"160gb\" : \"160 gb\", \"250gb\" : \"256 gb\", \"256gb\" : \"256 gb\", \n                \"500g\" : \"512 gb\", \"500gb\" : \"512 gb\", \"512g\" : \"512 gb\",\"512gb\" : \"512 gb\", \"10k\" : \"10 k\", \"10kt\" : \"10 k\", \"12k\" : \"12 k\",\"14k\" : \"14 k\", \"14kt\" : \"14 k\" , \"18k\" : \"18 k\" ,\"18kt\" : \"18 k\" ,  \"1oz\" : \"1 oz\", \"4oz\" : \"4 oz\", \"5oz\" : \"5 oz\", \"8oz\" : \"8 oz\",\"36oz\" : \"36 oz\", \"64oz\" : \"64 oz\"}\n    postTokens = []\n    for item in tokens:\n        if item in item_list:\n            item = item_list[item]\n        postTokens.append(item)\n    return postTokens\n    \nvectorizer = FeatureUnion([\n    ('name', CountVectorizer(\n        ngram_range=(1, 2),\n        min_df=NAME_MIN_DF,\n        tokenizer=rex_tokenizer,\n        stop_words = 'english',\n        preprocessor=build_preprocessor('name'))),\n    ('general_cat', CountVectorizer(\n        token_pattern='.+',\n        preprocessor=build_preprocessor('general_cat'))),\n    ('subcat_1', CountVectorizer(\n        token_pattern='.+',\n        preprocessor=build_preprocessor('subcat_1'))),\n    ('subcat_2', CountVectorizer(\n        token_pattern='.+',\n        preprocessor=build_preprocessor('subcat_2'))),\n    ('brand_name', CountVectorizer(\n        token_pattern='.+',\n        preprocessor=build_preprocessor('brand_name'))),\n    ('shipping', CountVectorizer(\n        token_pattern='\\d+',\n        preprocessor=build_preprocessor('shipping'))),\n    ('item_condition_id', CountVectorizer(\n        token_pattern='\\d+',\n        preprocessor=build_preprocessor('item_condition_id'))),\n    ('item_description', TfidfVectorizer(\n        ngram_range=(1, 3),\n        max_features=MAX_FEATURES_ITEM_DESCRIPTION,\n        stop_words = 'english',\n        analyzer = 'word',\n        tokenizer=rex_tokenizer,\n        preprocessor=build_preprocessor('item_description'))),\n    ('len_description', CountVectorizer(\n        token_pattern='\\d+',\n        preprocessor=build_preprocessor('len_description'))),\n], transformer_weights=transformerWeights)\n\nX_train_test = vectorizer.fit_transform(train_test.values)","metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"%%time\n\ndef get_rmsle(y_true, y_pred):\n    return np.sqrt(mean_squared_log_error(np.expm1(y_true), np.expm1(y_pred)))\n\ncv = KFold(n_splits=10, shuffle=True, random_state=42)\nfor train_ids, valid_ids in cv.split(X_train_test[:nrow_train]):\n    model = Ridge(\n            solver='auto',\n            fit_intercept=True,\n            alpha=0.5,\n            max_iter=100,\n            normalize=False,\n            copy_X=True,\n            random_state=101,\n            tol=0.025)\n    model.fit(X_train_test[train_ids], y_train[train_ids])\n    y_pred_valid = model.predict(X_train_test[valid_ids])\n    rmsle = get_rmsle(y_pred_valid, y_train[valid_ids])\n    print(f'valid rmsle: {rmsle:.5f}')","metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"eli5.show_weights(model, vec=vectorizer, top=100, feature_filter=lambda x: x != '<BIAS>')","metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"eli5.show_prediction(model, doc=train_test.values[1], vec=vectorizer)","metadata":{"collapsed":true}},{"cell_type":"markdown","source":"Make final prediction.","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"preds = model.predict(X_train_test[nrow_train:])\ntest[\"price\"] = np.expm1(preds)\ntest[[\"test_id\", \"price\"]].to_csv(\"submission_ridge.csv\", index = False)","metadata":{"collapsed":true}},{"cell_type":"markdown","source":"","metadata":{}}],"nbformat":4,"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","version":"3.6.3","name":"python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3}}},"nbformat_minor":1}