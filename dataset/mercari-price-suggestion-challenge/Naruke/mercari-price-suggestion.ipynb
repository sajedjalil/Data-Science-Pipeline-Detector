{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport os\nfrom tqdm import tqdm\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet') \nlemmatizer = WordNetLemmatizer() \nfrom spacy.lang.en.stop_words import STOP_WORDS\n# plt.xkcd()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# read the train and test tsv files (seperator is tab that's why tsv)\ntrain = pd.read_csv(\"../input/mercari-dataset/train.tsv\",sep = '\\t') \nprint(\"Number of rows {} and columns {} \".format(train.shape[0],train.shape[1]))\ntrain.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/mercari-dataset/test.tsv\",sep = \"\\t\")\nprint(\"Number of rows {} and columns {} \".format(test.shape[0],test.shape[1]))\ntest.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. So from above train and test information we can conclude that there are null values in brand name and category name for both files.\n\n2. We will counter this problem by filling in the missing or NaN values.But, first let's explore more features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's first explore our target variable \n# let's see target variable (price) distribution\nplt.xkcd()\nfig,(ax1,ax2) = plt.subplots(nrows = 1,ncols=2,figsize = (18,8))\nax1.hist(train.price,bins = 30,range = [0,200])\nax2.hist(np.log(train.price + 1),bins = 30,range = [0,8])\nax1.set_title(\"Actual Price Distribution\")\nax1.axvline(x = np.percentile(train.price,q = 80),color = 'red',label = \"80 percent of population lies on left\")\nax1.set_xlabel(\"Price\")\nax2.set_title(\"Log(Price) Distribution\")\nax2.set_xlabel(\"Price\")\n#plt.legend()\nplt.suptitle(\"Price Feature\",fontsize = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. As you can see from above plot that price feature is having right skewed distribution or you might say it's having power law distribution.\n\n2. It's just an intuition after seeing the plot that variable x (price) can be log-normal distribution i.e log of variable can be normally distributed.\n\n3. After plotting **log(price)** we got normal distribution.\n\n4. Red Line above is giving out the info that 80% of population lies on the left of that line.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# from above analysis it will be better if we add log(Price) as different features, so that forthcoming analysis will be comfortable.\ntrain['log(price)'] = np.log(train['price']+1)\n# added new features \n# this is called feature engineering where from the given features you come up with intuitive and useful features.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shipping feature in data seems to be reasonable to explore.\n# let's see distribution of shipping data with log(price)\n\nfig, ax = plt.subplots(figsize=(14,8))\nax.hist(train[train.shipping==1]['log(price)'],bins=30,range=[0,8],label=\"Buyer_charged\",color='b',alpha=0.5)\nax.hist(train[train.shipping==0]['log(price)'],bins=30,range=[0,8],label=\"seller_charged\",color='r',alpha=0.5)\nplt.title('Price distribution', fontsize=15)\nax.set_xlabel('log(Price+1)',fontsize=15)\nax.set_ylabel('No of items',fontsize=15)\nplt.legend(loc='upper right')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. From above we can't say seller pay the shipping charged if the amount is high because the distribution of both buyer and seller charged is highly overlapping. Hence, we can't come to the conclusion straightaway.\n\n2. The shipping cost burden is decently splitted between sellers and buyers with more than half of the items' shipping fees are paid by the sellers (55%). In addition, the average price paid by users who have to pay for shipping fees is lower than those that don't require additional shipping cost. This matches with our perception that the sellers need a lower price to compensate for the additional shipping.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's fill in missing data for further analysis with brand name and category name\ndef fill_missing_data(data):\n    data.category_name.fillna(value = \"others\", inplace = True)\n    data.brand_name.fillna(value = \"not known\", inplace = True)\n    data.item_description.fillna(value = \"no description\", inplace = True)\n    return(data)\ntrain = fill_missing_data(train)\n#test  = fill_missing_data(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's also split category_name into 3 parts main_category, sub_category_1 and sub_category_2\n\n\n# Let's split the categories into three different columns. We will see later that this information is actually quite important from the seller's point of view and how we handle \n# the missing information in the brand_name column will impact the model's prediction.\n\ndef split_cat(text):\n    try:\n        text1, text2, text3 = text.split('/')\n        return text1, text2, text3\n    except: \n        return (\"No Label\", \"No Label\", \"No Label\")\ntrain['main_cat'], train['subcat_1'], train['subcat_2'] = zip(*train['category_name'].apply(lambda x: split_cat(x)))\n#test['main_cat'], test['subcat_1'], test['subcat_2'] = zip(*test['category_name'].apply(lambda x: split_cat(x)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['summary'] = train['name'] + train['item_description']\n#test['summary'] = test['name'] + test['item_description']\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,8))\nplt.suptitle(\"Distribution of Price per Main Category\")\nsns.boxplot(y = 'log(price)', x = 'main_cat',data = train)\nplt.xticks(rotation = 45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n1. Each main category has so much overlapping in prize distribution that no pair of categories price can be distinguished. \n\n2. Price of each main category is almost completely overlapping.\n\n3. Median of log(Pirce) of Men main category is higher then all present in main category (if we don't count No Label imputaion value. )\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly\nx = train['subcat_1'].value_counts().index.values.astype('str')[:15]\ny = train['subcat_1'].value_counts().values[:15]\npct = [(\"%.2f\"%(v*100))+\"%\"for v in (y/len(train))][:15]\n\ntrace1 = go.Bar(x=x, y=y, text=pct)\nlayout = dict(title= 'Number of Items by Sub Category 1',\n              yaxis = dict(title='Count'),\n              xaxis = dict(title='Category'))\nfig=go.Figure(data = trace1,layout=layout)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly\nx = train['subcat_2'].value_counts().index.values.astype('str')[:15]\ny = train['subcat_2'].value_counts().values[:15]\npct = [(\"%.2f\"%(v*100))+\"%\"for v in (y/len(train))][:15]\n\ntrace1 = go.Bar(x=x, y=y, text=pct)\nlayout = dict(title= 'Number of Items by Sub Category 2',\n              yaxis = dict(title='Count'),\n              xaxis = dict(title='Category'))\nfig=go.Figure(data = trace1,layout=layout)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n# here STOP_WORDS is spacy stopwords list\ndef wordCount(text):\n    # convert to lower case and strip regex\n    try:\n         # convert to lower case and strip regex\n        text = text.lower()\n        #regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n        #txt = regex.sub(\" \", text)\n        txt = re.sub('[^a-zA-Z]+',' ',text)\n        words = [lemmatizer.lemmatize(w) for w in txt.split(\" \") \\\n                 if not w in STOP_WORDS and len(w)>3]\n        return(len(words))\n    except: \n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add a column of word counts to both the training and test set\ntrain['desc_len'] = train['summary'].apply(lambda x: wordCount(x))\nplt.figure(figsize = (16,8))\nsns.violinplot(x = 'main_cat',y = 'desc_len',data = train)\nplt.xticks(rotation = 45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Description Length distribution for each main category is having almost same shape which is right skewed. \n\n2. And even each category is perfectly aligned to each other, which means just taking desc_len as feature we can't not predict price precisely. They are so much overlapping.\n\n3. You might be thinking why I am making such absurd conclusion even though we are comparing desc_len with main_category. Well, you are right and the reason is given below....\n\n4. If A is correlated with B and B is correlated with C it does not always True that A and C are also correlated.\n\n5. For more info go to https://stats.stackexchange.com/questions/5747/if-a-and-b-are-correlated-with-c-why-are-a-and-b-not-necessarily-correlated\n\n6. But we will explore that is description has any effect on price. Does having long description means it'll have more price on tag?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. It will be more challenging to parse through this particular item since it's unstructured data. Does it mean a more detailed and lengthy description will result in a higher bidding price?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's see for top 10 description length the prize distribution.\n\ntemp = pd.DataFrame()\nfor i in tqdm(train['desc_len'].value_counts().index.values[:10]):\n    temp = pd.concat([temp,train[train['desc_len']==i]],axis=0)\n\nplt.figure(figsize = (16,8))\nsns.boxplot(x = 'desc_len',y = 'log(price)',data = temp)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Boxplot of top 10 desc_len have almost the same mean,25th percentile and 75th percentile value.\n\n2. Description Length feature alone can't distinguish the price.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Loading Twitter Glove Vectors","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nembeddings_index = {}\nf = open('../input/glovetwitter27b100dtxt/glove.twitter.27B.100d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text Preprocessing, Stemming and Removing Stopwords","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Most of the time, the first steps of an NLP project is to \"tokenize\" your documents, which main purpose is to normalize our texts. The three fundamental stages will usually include:\n\n1. break the descriptions into sentences and then break the sentences into tokens\n2. remove punctuation and stop words\n3. lowercase the tokens","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n\ndef text_preprocess(df):\n    pre_list = []\n    for sentance in tqdm(df['summary'].values):\n        sent = decontracted(sentance)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = re.sub('[^A-Za-z]+', ' ', sent)\n        # https://gist.github.com/sebleier/554280\n        sent = ' '.join(lemmatizer.lemmatize(e) for e in sent.split() if e.lower() not in stopwords)\n        pre_list.append(sent.lower().strip())\n    return(pre_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_preprocess = text_preprocess(train)\n#test_preprocess = text_preprocess(test)\ntrain['summary'] = train_preprocess\n#test['summary'] = test_preprocess","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Average Word2Vec","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = embeddings_index.keys()\n# this function will add the vectors of each word and returns the avg vector of given sentance\ndef build_avg_vec(sentence, num_features, doc_id, m_name):\n    # sentace: its title of the apparel\n    # num_features: the lenght of word2vec vector, its values = 300\n    # m_name: model information it will take two values\n        # if  m_name == 'avg', we will append the model[i], w2v representation of word i\n        # if m_name == 'weighted', we will multiply each w2v[word] with the idf(word)\n\n    featureVec = np.zeros((num_features,), dtype=\"float32\")\n    # we will intialize a vector of size 300 with all zeros\n    # we add each word2vec(wordi) to this fetureVec\n    nwords = 0\n    \n    for word in sentence.split():\n        nwords += 1\n        if word in vocab:\n            if m_name == 'weighted' and word in  idf_title_vectorizer.vocabulary_:\n                featureVec = np.add(featureVec, idf_title_features[doc_id, idf_title_vectorizer.vocabulary_[word]] * embeddings_index[word])\n            elif m_name == 'avg':\n                featureVec = np.add(featureVec, embeddings_index[word])\n    if(nwords>0):\n        featureVec = np.divide(featureVec, nwords)\n    # returns the avg vector of given sentance, its of shape (1, 300)\n    return featureVec\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_avgw2v(train_preprocess,test_preprocess):\n    doc_id = 0\n    avg_w2v = []\n    # for every description we build a avg vector representation\n    for i in tqdm(train_preprocess):\n        avg_w2v.append(build_avg_vec(i, 100, doc_id,'avg'))\n        doc_id += 1\n    # w2v_desc = np.array(# number of doc in courpus * 100), each row corresponds to a doc \n    avg_w2v_train = np.array(avg_w2v)\n\n    avg_w2v = []\n    # for every title we build a avg vector representation\n    for i in tqdm(test_preprocess):\n        avg_w2v.append(build_avg_vec(i, 100, doc_id,'avg'))\n        doc_id += 1\n    # w2v_title = np.array(# number of doc in courpus * 100), each row corresponds to a doc \n    avg_w2v_test = np.array(avg_w2v)\n    return(avg_w2v_train,avg_w2v_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_w2v_train,_ = calc_avgw2v(train['summary'].values,[])\n\n# splitting the data\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(avg_w2v_train,train['log(price)'].values, test_size = 0.2,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's Clear Doubt\n\n1. If you know about data leakage problem you brain would have definitely striked that \"what hell I am doing\". How can I split the train data into X_train and X_test after doing featurization.\n\n2. You are right, but you are wrong!!! **Confused Again...**\n\n3. Well you are right in general case but, here for featurization I am using avg_w2v which doesn't need whole training data i.e I am not exposing my whole training data for featurization.\n\n4. If I would have been using tfidf_w2v or idf_w2v then that could be the data leakage. Because tfidf,idf is calculated using all training data.\n\n5. Hope I have clear your doubt...","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### First Model - DNN with FineTuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential,load_model\nfrom tensorflow.keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation,BatchNormalization,LeakyReLU\nfrom tensorflow.keras.layers import Embedding\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras.callbacks import (\n    ReduceLROnPlateau,\n    EarlyStopping,\n    ModelCheckpoint,\n    TensorBoard\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(300,activation=LeakyReLU(),kernel_initializer='glorot_normal',input_shape = (100,)))\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\nmodel.add(Dense(300,activation=LeakyReLU(),kernel_initializer='glorot_normal'))\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\nmodel.add(Dense(200,activation=LeakyReLU(),kernel_initializer='glorot_normal'))\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\nmodel.add(Dense(200,activation=LeakyReLU(),kernel_initializer='glorot_normal'))\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\nmodel.add(Dense(100,activation=LeakyReLU(),kernel_initializer='glorot_normal'))\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\nmodel.add(Dense(50,activation=LeakyReLU(),kernel_initializer='glorot_normal'))\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\nmodel.add(Dense(1,activation='linear'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam,Nadam\n\n# tf.keras.losses.MeanSquaredLogarithmicError(\n#     reduction=\"auto\", name=\"mean_squared_logarithmic_error\"\n# )\n# mae = tf.keras.losses.MeanAbsoluteError(name = \"mean_absolute_error\")\n\nreduce_lr = ReduceLROnPlateau(monitor= \"mse\", factor=0.2,\n                              patience=2, min_lr=0.00001)\n\ntnsr_brd = TensorBoard(log_dir='logs')\n\n\nckpt = ModelCheckpoint('cnn_model.h5',\n                            verbose=1, save_weights_only=True,save_best_only = True)\n\ncallbacks = [reduce_lr, tnsr_brd, ckpt ]\n\nadam = Adam(learning_rate = 2e-3)\nnadam = Nadam(learning_rate = 1e-4)\nmodel.compile(loss=[\"mse\"], optimizer=nadam)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train,y_train,\n                    batch_size=256,\n                    epochs=20,\n                    verbose=1,\n                    validation_data = (X_test,y_test),callbacks = callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nprint(\"root mean squared error is : \",np.sqrt(mean_squared_error(y_test,model.predict(X_test))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 2 - LightGBM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(train.drop(['price','log(price)'],axis=1),train[['log(price)','price']],test_size = 0.05,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Encoding --","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Brand Name Feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer_brnd = CountVectorizer()\nvectorizer_brnd.fit(X_train['brand_name'].values) # fit has to happen only on train data\n\n# we use the fitted CountVectorizer to convert the text to vector\ntrain_brandname = vectorizer_brnd.transform(X_train['brand_name'].values)\ntest_brandname = vectorizer_brnd.transform(X_test['brand_name'].values)\n\n\nprint(\"After vectorizations\")\nprint(train_brandname.shape)\nprint(test_brandname.shape)\n\nprint(len(vectorizer_brnd.get_feature_names()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Name Feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer_name = CountVectorizer()\nvectorizer_name.fit(X_train['name'].values) # fit has to happen only on train data\n\n# we use the fitted CountVectorizer to convert the text to vector\ntrain_name = vectorizer_name.transform(X_train['name'].values)\ntest_name = vectorizer_name.transform(X_test['name'].values)\n\n\nprint(\"After vectorizations\")\nprint(train_brandname.shape)\nprint(test_brandname.shape)\n\nprint(len(vectorizer_name.get_feature_names()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Main Category Feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer_main = CountVectorizer()\nvectorizer_main.fit(X_train['main_cat'].values) # fit has to happen only on train data\n\n# we use the fitted CountVectorizer to convert the text to vector\ntrain_cat = vectorizer_main.transform(X_train['main_cat'].values)\ntest_cat= vectorizer_main.transform(X_test['main_cat'].values)\n\n\nprint(\"After vectorizations\")\nprint(train_cat.shape)\nprint(test_cat.shape)\n\nprint(len(vectorizer_main.get_feature_names()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Subcategory 1 Feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer_sub_1 = CountVectorizer()\nvectorizer_sub_1.fit(X_train['subcat_1'].values) # fit has to happen only on train data\n\n# we use the fitted CountVectorizer to convert the text to vector\ntrain_subcat_1 = vectorizer_sub_1.transform(X_train['subcat_1'].values)\ntest_subcat_1 = vectorizer_sub_1.transform(X_test['subcat_1'].values)\n\n\nprint(\"After vectorizations\")\nprint(train_subcat_1.shape)\nprint(test_subcat_1.shape)\n\nprint(len(vectorizer_sub_1.get_feature_names()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Subcategory 2 Feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer_sub_2 = CountVectorizer()\nvectorizer_sub_2.fit(X_train['subcat_2'].values) # fit has to happen only on train data\n\n# we use the fitted CountVectorizer to convert the text to vector\ntrain_subcat_2 = vectorizer_sub_2.transform(X_train['subcat_2'].values)\ntest_subcat_2 = vectorizer_sub_2.transform(X_test['subcat_2'].values)\n\n\nprint(\"After vectorizations\")\nprint(train_subcat_2.shape)\nprint(test_subcat_2.shape)\n\nprint(len(vectorizer_sub_2.get_feature_names()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Description Length Feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## number of words in description\n\ndef tokens(text):\n    text = re.sub(\"[^A-Za-z]+\",\" \",text)\n    return(len(text.split(\" \")))\n\nX_train['description_len'] = X_train['item_description'].apply(tokens)\nX_test['description_len'] = X_test['item_description'].apply(tokens)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standard Scaling of Descripion Length","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import Normalizer,StandardScaler,normalize\nnormalizer = StandardScaler()\nnormalizer.fit(X_train['description_len'].values.reshape(-1,1))\n\n#X_train_price_norm = normalizer.transform(X_train['price'].values.reshape(-1,1))\ntrain_des_norm = normalizer.transform(X_train['description_len'].values.reshape(-1,1))\n\ntest_des_norm = normalizer.transform(X_test['description_len'].values.reshape(-1,1))\n\nprint(\"After normalizations\")\nprint(train_des_norm.shape, y_train.shape)\n\nprint(test_des_norm.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sentiment Intensity of Each Item description","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nimport nltk\nnltk.download('vader_lexicon')\n\nsid = SentimentIntensityAnalyzer()\ndef sentiment_analyzer(df,preprocess_text):\n    temp = [] \n    for sentence in tqdm(preprocess_text):\n        for_sentiment = sentence\n        ss = sid.polarity_scores(for_sentiment)\n        temp.append(ss)\n    negative=[]\n    neutral=[]\n    positive=[]\n    compounding=[]\n    for i in temp:\n        for polarity,score in i.items():\n            if(polarity=='neg'):\n                negative.append(score)\n            if(polarity=='neu'):\n                neutral.append(score)\n            if(polarity=='pos'):\n                positive.append(score)\n            if(polarity=='compound'):\n                compounding.append(score)\n    df['negative']=negative\n    df['neutral']=neutral\n    df['positive']=positive\n    df['compound']=compounding\n    return(df)\n\nX_train = sentiment_analyzer(X_train,X_train['item_description'].values)\nX_test = sentiment_analyzer(X_test,X_test['item_description'].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standard Scaling of Sentimental Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def encoding(feature,tr,ts):\n    vectorizer = CountVectorizer()\n    vectorizer.fit(tr[feature].values) # fit has to happen only on train data\n\n    # we use the fitted CountVectorizer to convert the text to vector\n    train_temp = vectorizer.transform(tr[feature].values)\n    test_temp= vectorizer.transform(ts[feature].values)\n\n\n    print(\"After vectorizations\")\n    print(train_temp.shape)\n    print(test_temp.shape)\n\n    print(len(vectorizer.get_feature_names()))\n    \n    \ndef norm(feature,tr,ts):\n    normalizer = StandardScaler()\n    normalizer.fit(tr[feature].values.reshape(-1,1))\n\n    #X_train_price_norm = normalizer.transform(X_train['price'].values.reshape(-1,1))\n    train_temp = normalizer.transform(tr[feature].values.reshape(-1,1))\n\n    test_temp = normalizer.transform(ts[feature].values.reshape(-1,1))\n\n    print(\"After normalizations\")\n    print(train_temp.shape, y_train.shape)\n\n    print(test_temp.shape, y_test.shape)\n    return(train_temp,test_temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_neu,test_neu = norm('neutral',X_train,X_test)\ntrain_pos,test_pos = norm('positive',X_train,X_test)\ntrain_neg,test_neg = norm('negative',X_train,X_test)\ntrain_comp,test_comp = norm('compound',X_train,X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_w2v_train,avg_w2v_test = calc_avgw2v(X_train['summary'].values,X_test['summary'].values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import csr_matrix\ntrain_dummies = csr_matrix(pd.get_dummies(X_train[['item_condition_id', 'shipping']],\n                                          sparse=True).values)\n\ntest_dummies = csr_matrix(pd.get_dummies(X_test[['item_condition_id', 'shipping']],\n                                          sparse=True).values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=10,max_features=5000)\nvectorizer.fit(X_train['summary'].values)\nX_train_itemdes = vectorizer.transform(X_train['summary'].values)\nX_test_itemdes = vectorizer.transform(X_test['summary'].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Concatenating Each Feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import hstack\ntrain_data = hstack((X_train_itemdes,avg_w2v_train,train_brandname, train_name,train_dummies,train_cat,train_subcat_1,train_subcat_2,train_des_norm,train_neu,train_neg,train_pos,train_comp))\ntest_data = hstack((X_test_itemdes,avg_w2v_test,test_brandname,test_name,test_dummies ,test_cat,test_subcat_1,test_subcat_2,test_des_norm,test_neu,test_neg,test_pos,test_comp))\nprint(\"Final Data matrix\")\nprint(train_data.shape, y_train['log(price)'].shape)\n\nprint(test_data.shape, y_test['log(price)'].shape)\nprint(\"=\"*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom lightgbm import LGBMRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_params ={'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_samples': 50, 'objective': 'regression','boosting_type': 'gbdt','learning_rate': 0.5,\n              'max_depth': 8,'n_estimators': 500,'num_leaves': 80,\n      }\nmodel = LGBMRegressor(**lgbm_params)\nmodel.fit(train_data, y_train['log(price)'].values,\n         verbose=True)\n\npreds2 = model.predict(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sqrt(mean_squared_error(y_test['log(price)'].values, preds2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub = pd.read_csv(\"../input/mercari-dataset-asd/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(dataframe):\n    \n    dataframe  = fill_missing_data(dataframe)\n    dataframe['main_cat'], dataframe['subcat_1'], dataframe['subcat_2'] = zip(*dataframe['category_name'].apply(lambda x: split_cat(x)))\n    dataframe['summary'] = dataframe['name'] + dataframe['item_description']\n    dataframe['summary'] = text_preprocess(dataframe)\n    test_brand = vectorizer_brnd.transform(dataframe['brand_name'].values)\n    test_name = vectorizer_name.transform(dataframe['name'].values)\n    test_cat= vectorizer_main.transform(dataframe['main_cat'].values)\n    test_subcat_1 = vectorizer_sub_1.transform(dataframe['subcat_1'].values)\n    test_subcat_2 = vectorizer_sub_2.transform(dataframe['subcat_2'].values)\n    dataframe['description_len'] = dataframe['item_description'].apply(tokens)\n    test_dummies = csr_matrix(pd.get_dummies(dataframe[['item_condition_id', 'shipping']],\n                                          sparse=True).values)\n    test_des_norm = normalizer.transform(dataframe['description_len'].values.reshape(-1,1))\n    dataframe = sentiment_analyzer(dataframe,dataframe['item_description'].values)\n    train_neu,test_neu = norm('neutral',X_train,dataframe)\n    train_pos,test_pos = norm('positive',X_train,dataframe)\n    train_neg,test_neg = norm('negative',X_train,dataframe)\n    train_comp,test_comp = norm('compound',X_train,dataframe)\n    avg_w2v = []\n    doc_id=0\n    for i in tqdm(dataframe['summary'].values):\n        avg_w2v.append(build_avg_vec(i, 100, doc_id,'avg'))\n        doc_id += 1\n    avg_w2v = np.array(avg_w2v)\n    X_test_itemdes = vectorizer.transform(dataframe['summary'].values)\n    test_data = hstack((X_test_itemdes,avg_w2v,test_brand,test_name,test_dummies,test_cat,test_subcat_1,test_subcat_2,test_des_norm,test_neu,test_neg,test_pos,test_comp))\n    return(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_data = predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub['price'] = np.expm1(model.predict(test_data))\nsample_sub.to_csv(\"submission_lgbm.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\n1. I have done some feature engineering like description length, Sentiment Analyzer etc.\n2. Price Feature seems to be having log-normal distributon. After taking log(Price) we can see it's forming nearly normal distribution.\n3. First Model I have tried using only summary feature which is a combination of two prior faeture i.e item_description and name. I have trained Dense NN with little bit of fine tuning and got RMSLE as 0.60.\n4. Second Model I have tried using Bag of Words for each feature and concatenate it with average word2vec feature. I have got RMSLE as 0.47.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}