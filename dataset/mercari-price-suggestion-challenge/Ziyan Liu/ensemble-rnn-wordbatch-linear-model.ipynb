{"nbformat_minor":1,"cells":[{"source":"import time\nstart_time = time.time()\n\nimport gc\nimport numpy as np\nimport pandas as pd\nfrom subprocess import check_output\n\ndevelop = True\ntrain_set_ratio = .99\nsplit_seed = 123\n\ntrain_path = '../input/mercari-price-suggestion-challenge/train.tsv'\ntest_path = '../input/mercari-price-suggestion-challenge/test.tsv'\n\ndef rmsle(y, y_pred):\n    assert y.shape == y_pred.shape\n    return np.sqrt(np.square(np.log(y_pred + 1) - np.log(y + 1)).mean())","outputs":[],"cell_type":"code","metadata":{"_uuid":"3f9b83b5977802c262f272613858c2ccb442df19","collapsed":true,"_cell_guid":"95407d6e-a33b-4733-9936-58bdaa5f7cf6"},"execution_count":null},{"source":"# # # # # # # # # # # # # # # # # # # # # # RNN # # # # # # # # # # # # # # # # # # # # # # # #\n# import modules\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.cross_validation import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# # load data\ntrain_df = pd.read_csv(train_path, sep='\\t')\ntest_df = pd.read_csv(test_path, sep='\\t')\nprint('size of train set:', train_df.shape)\nprint('size of test set:', test_df.shape)\n\n# missing values imputation\ndef impute_missing(data_df):\n    data_df.category_name.fillna(value=\"missing\", inplace=True)\n    data_df.brand_name.fillna(value=\"missing\", inplace=True)\n    data_df.item_description.fillna(value=\"missing\", inplace=True)\n\nimpute_missing(train_df)\nimpute_missing(test_df)\n\n# process categorical data\nle = LabelEncoder()\nle.fit(np.hstack([train_df.category_name, test_df.category_name]))\ntrain_df.category_name = le.transform(train_df.category_name)\ntest_df.category_name = le.transform(test_df.category_name)\n\nle.fit(np.hstack([train_df.brand_name, test_df.brand_name]))\ntrain_df.brand_name = le.transform(train_df.brand_name)\ntest_df.brand_name = le.transform(test_df.brand_name)\ndel le\n\n# process text variable\nprint(\"[{}] text to seq process...\".format(time.time() - start_time))\nraw_text = np.hstack([\n    train_df.item_description.str.lower(),\n    train_df.name.str.lower()\n])\nprint(\"[{}] fitting tokenizer...\".format(time.time() - start_time))\ntok_raw = Tokenizer()\ntok_raw.fit_on_texts(raw_text)\n\ntrain_df[\"seq_item_description\"] = tok_raw.texts_to_sequences(train_df.item_description.str.lower())\ntest_df[\"seq_item_description\"] = tok_raw.texts_to_sequences(test_df.item_description.str.lower())\ntrain_df[\"seq_name\"] = tok_raw.texts_to_sequences(train_df.name.str.lower())\ntest_df[\"seq_name\"] = tok_raw.texts_to_sequences(test_df.name.str.lower())\nprint(\"[{}] transforming text to seq...\".format(time.time() - start_time))\n\n# length of sequences\nmax_name_seq = np.max([np.max(train_df.seq_name.apply(lambda x: len(x))),\n                       np.max(test_df.seq_name.apply(lambda x: len(x)))])\nmax_seq_item_description = np.max([np.max(train_df.seq_item_description.apply(lambda x: len(x))),\n                                   np.max(test_df.seq_item_description.apply(lambda x: len(x)))])\nprint('[{}] finish calculating MAX_seq'.format(time.time() - start_time))\n\n# EMBEDDINGS MAX VALUE\nMAX_NAME_SEQ = 10\nMAX_ITEM_DESC_SEQ = 75\nMAX_TEXT = np.max([np.max(train_df.seq_name.max()),\n                   np.max(test_df.seq_name.max()),\n                   np.max(train_df.seq_item_description.max()),\n                   np.max(test_df.seq_item_description.max())]) + 2\nMAX_CATEGORY = np.max([train_df.category_name.max(), test_df.category_name.max()]) + 1\nMAX_BRAND = np.max([train_df.brand_name.max(), test_df.brand_name.max()]) + 1\nMAX_CONDITION = np.max([train_df.item_condition_id.max(), test_df.item_condition_id.max()]) + 1\n\n# scale target variable\ntrain_df[\"target\"] = np.log(train_df.price + 1)\ntarget_scaler = MinMaxScaler(feature_range=(-1, 1))\ntrain_df[\"target\"] = target_scaler.fit_transform(train_df.target.reshape(-1, 1))\n\n\n# keras data definition\ndef get_keras_data(dataset):\n    X = {\n        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ)\n        # ,'category_name' : pad_sequences(dataset.seq_category_name, maxlen=MAX_CATEGORY_SEQ)\n        , 'item_desc': pad_sequences(dataset.seq_item_description, maxlen=MAX_ITEM_DESC_SEQ)\n        , 'brand_name': np.array(dataset.brand_name)\n        , 'category_name': np.array(dataset.category_name)\n        , 'item_condition': np.array(dataset.item_condition_id)\n        , 'num_vars': np.array(dataset[[\"shipping\"]])\n    }\n    return X\n\n\n# extract development test\nif develop:\n    dtrain, dvalid = train_test_split(train_df, train_size=train_set_ratio, random_state=split_seed)\n    X_train = get_keras_data(dtrain)\n    X_valid = get_keras_data(dvalid)\n    X_test = get_keras_data(test_df)\nelse:\n    dtrain = train_df\n    dvalid = train_df   # this might be the part that make it slow\n    X_train = get_keras_data(dtrain)\n    X_valid = get_keras_data(dvalid)\n    X_test = get_keras_data(test_df)\nids = test_df[[\"test_id\"]]\ndel train_df, test_df; gc.collect()\nprint('[{}] finish forming keras data input for rnn model'.format(time.time() - start_time))\n\n\n# keras model definition\nfrom keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, \\\n    BatchNormalization\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras import backend as K\nfrom keras import regularizers\n\ndr_r = .1\n\ndef get_callbacks(filepath, patience=2):\n    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n    msave = ModelCheckpoint(filepath, save_best_only=True)\n    return [es, msave]\n\n\ndef rmsle_cust(y_true, y_pred):\n    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n    return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))\n\n\ndef rmsle2(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_true - y_pred), axis=-1))\n\n\ndef get_model():\n    # Inputs\n    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n    brand_name = Input(shape=[1], name=\"brand_name\")\n    category_name = Input(shape=[1], name=\"category_name\")\n    item_condition = Input(shape=[1], name=\"item_condition\")\n    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n\n    # Embeddings layers\n    emb_name = Embedding(MAX_TEXT, 50)(name)\n    emb_item_desc = Embedding(MAX_TEXT, 50)(item_desc)\n    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n    emb_category_name = Embedding(MAX_CATEGORY, 10)(category_name)\n    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n\n    # rnn layer\n    rnn_layer1 = GRU(16)(emb_item_desc)\n    rnn_layer2 = GRU(8)(emb_name)\n\n    # main layer\n    main_l = concatenate([\n        Flatten()(emb_brand_name)\n        , Flatten()(emb_category_name)\n        , Flatten()(emb_item_condition)\n        , rnn_layer1\n        , rnn_layer2\n        , num_vars\n    ])\n    main_l = Dropout(dr_r)(Dense(128, activation='relu')(main_l))\n\n    # output\n    output = Dense(1, activation=\"linear\")(main_l)\n\n    # model\n    model = Model([name, item_desc, brand_name, category_name, item_condition, num_vars], output)\n    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", rmsle_cust])\n\n    return model\n\nmodel = get_model()\n#model.summary()\n\n# fitting the model\nBATCH_SIZE = 2000\nepochs = 2\nsteps = int(len(X_train['name']) / BATCH_SIZE) * epochs\nlr_init, lr_fin = 0.009, 0.006\nexp_decay = lambda init, fin, steps: (init / fin) ** (1/(steps - 1)) - 1\nlr_decay = exp_decay(lr_init, lr_fin, steps)\nlog_subdir = '_'.join(['ep', str(epochs),\n                       'bs', str(BATCH_SIZE),\n                       'lrI', str(lr_init),\n                       'lrF', str(lr_fin),\n                       'dr', str(dr_r)\n                       ])\n\nmodel = get_model()\nK.set_value(model.optimizer.lr, lr_init)\nK.set_value(model.optimizer.decay, lr_decay)\n\nmodel.fit(X_train, dtrain.target\n          , epochs=epochs\n          , batch_size=BATCH_SIZE\n          , validation_data=(X_valid, dvalid.target)\n          , verbose=1)\nprint('[{}] finish fitting rnn model'.format(time.time() - start_time))\n\n# create predictions for validation set\nif develop:\n    val_preds_rnn = model.predict(X_valid)\n    val_preds_rnn = target_scaler.inverse_transform(val_preds_rnn)\n    val_preds_rnn = np.expm1(val_preds_rnn)\n    y_true = np.array(dvalid.price.values).reshape(dvalid.shape[0], 1)  # price, not the target column\n    v_rmsle = rmsle(y_true, val_preds_rnn)\n    print('[{}] RMSLE of validation set with RNN model is: {}'.format(\n        time.time() - start_time, v_rmsle))\n\n# create predictions for test set\npreds_rnn = model.predict(X_test, batch_size=BATCH_SIZE)\npreds_rnn = target_scaler.inverse_transform(preds_rnn)\npreds_rnn = np.expm1(preds_rnn)\nprint('[{}] finish prediction with rnn'.format(time.time() - start_time))\n\n# clear\ndel dtrain, dvalid, X_train, X_valid, X_test; gc.collect()","outputs":[],"cell_type":"code","metadata":{"_uuid":"fc4b0ddcb07d2c7238d2e259981ac20095ce9160","collapsed":true,"_cell_guid":"b37e8a4c-cb49-4882-9674-4226086c7f39"},"execution_count":null},{"source":"# # # # # # # # # # # # # # # # # WordBatch Linear Models # # # # # # # # # # # # # # # # # # # #\nfrom scipy.sparse import csr_matrix, hstack\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nimport sys\n\nsys.path.insert(0, '../input/wordbatch/wordbatch/')\nimport wordbatch\nfrom wordbatch.extractors import WordBag, WordHash\nfrom wordbatch.models import FTRL, FM_FTRL\nfrom nltk.corpus import stopwords\nimport re\n\nNUM_BRANDS = 4500\nNUM_CATEGORIES = 1250\n\ndef rmsle(y, y0):\n    assert len(y) == len(y0)\n    return np.sqrt(np.mean(np.power(np.log1p(y) - np.log1p(y0), 2)))\n\n\ndef split_cat(text):\n    try:\n        return text.split(\"/\")\n    except:\n        return (\"No Label\", \"No Label\", \"No Label\")\n\n\ndef handle_missing_inplace(dataset):\n    dataset['general_cat'].fillna(value='missing', inplace=True)\n    dataset['subcat_1'].fillna(value='missing', inplace=True)\n    dataset['subcat_2'].fillna(value='missing', inplace=True)\n    dataset['brand_name'].fillna(value='missing', inplace=True)\n    dataset['item_description'].fillna(value='missing', inplace=True)\n\n\ndef cutting(dataset):\n    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n    pop_category1 = dataset['general_cat'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n    pop_category2 = dataset['subcat_1'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n    pop_category3 = dataset['subcat_2'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n    dataset.loc[~dataset['general_cat'].isin(pop_category1), 'general_cat'] = 'missing'\n    dataset.loc[~dataset['subcat_1'].isin(pop_category2), 'subcat_1'] = 'missing'\n    dataset.loc[~dataset['subcat_2'].isin(pop_category3), 'subcat_2'] = 'missing'\n\n\ndef to_categorical(dataset):\n    dataset['general_cat'] = dataset['general_cat'].astype('category')\n    dataset['subcat_1'] = dataset['subcat_1'].astype('category')\n    dataset['subcat_2'] = dataset['subcat_2'].astype('category')\n    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')\n\n\n# Define helpers for text normalization\nstopwords = {x: 1 for x in stopwords.words('english')}\nnon_alphanums = re.compile(u'[^A-Za-z0-9]+')\n\n\ndef normalize_text(text):\n    return u\" \".join(\n        [x for x in [y for y in non_alphanums.sub(' ', text).lower().strip().split(\" \")] \\\n         if len(x) > 1 and x not in stopwords])\n\n\ndef construct_features(start_time):\n    train = pd.read_table(train_path, engine='c')\n    test = pd.read_table(test_path, engine='c')\n    print('[{}] Finished to load data'.format(time.time() - start_time))\n    print('Train shape: ', train.shape)\n    print('Test shape: ', test.shape)\n    nrow_test = train.shape[0]  # -dftt.shape[0]\n    dftt = train[(train.price < 1.0)]\n    train = train.drop(train[(train.price < 1.0)].index)\n    del dftt['price']\n    nrow_train = train.shape[0]\n    # print(nrow_train, nrow_test)\n    y = np.log1p(train[\"price\"])\n    merge: pd.DataFrame = pd.concat([train, dftt, test])\n    #merge: pd.DataFrame = pd.concat([train, test])\n    #submission: pd.DataFrame = test[['test_id']]\n\n    del train\n    del test\n    gc.collect()\n\n    merge['general_cat'], merge['subcat_1'], merge['subcat_2'] = \\\n        zip(*merge['category_name'].apply(lambda x: split_cat(x)))\n    merge.drop('category_name', axis=1, inplace=True)\n    print('[{}] Split categories completed.'.format(time.time() - start_time))\n\n    handle_missing_inplace(merge)\n    print('[{}] Handle missing completed.'.format(time.time() - start_time))\n\n    cutting(merge)\n    print('[{}] Cut completed.'.format(time.time() - start_time))\n\n    to_categorical(merge)\n    print('[{}] Convert categorical completed'.format(time.time() - start_time))\n\n    wb = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.5, 1.0],\n                                                                  \"hash_size\": 2 ** 29, \"norm\": None, \"tf\": 'binary',\n                                                                  \"idf\": None,\n                                                                  }), procs=8)\n    wb.dictionary_freeze = True\n    X_name = wb.fit_transform(merge['name'])\n    del (wb)\n    X_name = X_name[:, np.array(np.clip(X_name.getnnz(axis=0) - 1, 0, 1), dtype=bool)]\n    print('[{}] Vectorize `name` completed.'.format(time.time() - start_time))\n\n    wb = CountVectorizer()\n    X_category1 = wb.fit_transform(merge['general_cat'])\n    X_category2 = wb.fit_transform(merge['subcat_1'])\n    X_category3 = wb.fit_transform(merge['subcat_2'])\n    print('[{}] Count vectorize `categories` completed.'.format(time.time() - start_time))\n\n    wb = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.0, 1.0],\n                                                                  \"hash_size\": 2 ** 28, \"norm\": \"l2\", \"tf\": 1.0,\n                                                                  \"idf\": None}), procs=8)\n    wb.dictionary_freeze = True\n    X_description = wb.fit_transform(merge['item_description'])\n    del (wb)\n    X_description = X_description[:, np.array(np.clip(X_description.getnnz(axis=0) - 1, 0, 1), dtype=bool)]\n    print('[{}] Vectorize `item_description` completed.'.format(time.time() - start_time))\n\n    lb = LabelBinarizer(sparse_output=True)\n    X_brand = lb.fit_transform(merge['brand_name'])\n    print('[{}] Label binarize `brand_name` completed.'.format(time.time() - start_time))\n\n    X_dummies = csr_matrix(pd.get_dummies(merge[['item_condition_id', 'shipping']],\n                                          sparse=True).values)\n    print('[{}] Get dummies on `item_condition_id` and `shipping` completed.'.format(time.time() - start_time))\n    print(X_dummies.shape, X_description.shape, X_brand.shape, X_category1.shape, X_category2.shape, X_category3.shape,\n          X_name.shape)\n    sparse_merge = hstack((X_dummies, X_description, X_brand, X_category1, X_category2, X_category3, X_name)).tocsr()\n\n    print('[{}] Create sparse merge completed'.format(time.time() - start_time))\n    return sparse_merge, y, nrow_train, nrow_test\n\n\ndef main(start_time):\n    from time import gmtime, strftime\n    print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n\n    sparse_merge, y, nrow_train, nrow_test = construct_features(start_time)\n\n    # Remove features with document frequency <=1\n    print(sparse_merge.shape)\n    mask = np.array(np.clip(sparse_merge.getnnz(axis=0) - 1, 0, 1), dtype=bool)\n    sparse_merge = sparse_merge[:, mask]\n    X = sparse_merge[:nrow_train]\n    X_test = sparse_merge[nrow_test:]\n    print('X size:', X.shape)\n    print('X_test size:', X_test.shape)\n    print(sparse_merge.shape)\n\n    gc.collect()\n    train_X, train_y = X, y\n    if develop:\n        train_X, valid_X, train_y, valid_y = train_test_split(X, y, train_size=train_set_ratio, random_state=split_seed)\n\n    # # FTRL\n    # model = FTRL(alpha=0.01, beta=0.1, L1=0.00001, L2=1.0, D=sparse_merge.shape[1], iters=50, inv_link=\"identity\",\n    #              threads=1)\n    # model.fit(train_X, train_y)\n    # print('[{}] Train FTRL completed'.format(time.time() - start_time))\n    # if develop:\n    #     val_predsF = model.predict(X=valid_X)\n    #     print(\"FTRL dev RMSLE:\", rmsle(np.expm1(valid_y), np.expm1(val_predsF)))\n    # predsF = model.predict(X_test)\n    # print('[{}] Predict FTRL completed'.format(time.time() - start_time))\n\n    # FM_FTRL\n    model = FM_FTRL(alpha=0.01, beta=0.01, L1=0.00001, L2=0.1, \n                    D=sparse_merge.shape[1], alpha_fm=0.01, L2_fm=0.0,\n                    init_fm=0.01, D_fm=200, e_noise=0.0001, iters=17, \n                    inv_link=\"identity\", threads=4)\n    model.fit(train_X, train_y)\n    print('[{}] Train ridge v2 completed'.format(time.time() - start_time))\n    if develop:\n        val_predsFM = np.expm1(model.predict(X=valid_X))\n        print(\"FM_FTRL dev RMSLE:\", rmsle(np.expm1(valid_y), val_predsFM))\n    predsFM = np.expm1(model.predict(X_test))\n    print('[{}] Predict FM_FTRL completed'.format(time.time() - start_time))\n\n    # delete\n    del train_X, train_y, sparse_merge\n    if develop:\n        del valid_X, X, y\n    gc.collect()\n\n    if develop:\n        results = val_predsFM, predsFM, valid_y\n    else:\n        results = predsFM\n\n    return results\n\nresults_linear_models = main(start_time)","outputs":[],"cell_type":"code","metadata":{"_uuid":"42debf1c11faac44028df67a41552c0bc255adbd","collapsed":true,"_cell_guid":"429a315b-3862-43cf-913b-58d313cd4d2f"},"execution_count":null},{"source":"# combination\nif develop:\n    val_predsFM, predsFM, valid_y = results_linear_models\n    # validation sizeof two models still not the same yet\n    #val_preds_ensemble = val_preds_rnn.reshape(val_preds_rnn.shape[0],) * .8 + val_predsFM * .2\n    #print(\"ensemble model validation RMSLE: \", rmsle(np.expm1(valid_y).as_matrix(), val_preds_ensemble))\n    preds_ensemble = preds_rnn.reshape(preds_rnn.shape[0],) * .6 + predsFM * .4\nelse:\n    predsFM = results_linear_models\n    preds_ensemble = preds_rnn.reshape(preds_rnn.shape[0],) * .6 + predsFM * .4","outputs":[],"cell_type":"code","metadata":{"_uuid":"e7be123cce68857d21c86f95f2e7465409705207","collapsed":true,"_cell_guid":"d404d8c7-461c-4d66-99a9-dbbf47e303f6"},"execution_count":null},{"source":"# submit\nsubmission = ids\nsubmission[\"price\"] = preds_ensemble\nsubmission.to_csv(\"ensemble_rnn_wordbatch_ridge_submission_v3.csv\", index=False)\nprint('[{}] finish everything'.format(time.time()-start_time))","outputs":[],"cell_type":"code","metadata":{"_uuid":"c99ad0084399e65d8ea962598a8c7a0202ad08d3","collapsed":true,"_cell_guid":"0a114c2e-2a03-4fb1-9f0f-f49031b8cb86"},"execution_count":null},{"source":"","outputs":[],"cell_type":"code","metadata":{"_uuid":"9f554f42e2ce9ca88f4967b12d5bf95576539db9","collapsed":true,"_cell_guid":"eea74ad8-d97a-43cb-ab40-37c3e5c90f1a"},"execution_count":null}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","name":"python","version":"3.6.4","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"}}},"nbformat":4}