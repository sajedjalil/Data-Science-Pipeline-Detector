{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#libs\nfrom copy import deepcopy\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tqdm.notebook import tqdm\nimport types\nimport ast\nimport gc\ndef imports():\n    for name, val in globals().items():\n        # module imports\n        if isinstance(val, types.ModuleType):\n            yield name, val\n        # functions / callables\n        if hasattr(val, '__call__'):\n            yield name, val\nnp.seterr(divide='ignore', invalid='ignore')\nnoglobal = lambda fn: types.FunctionType(fn.__code__, dict(imports()))\nimport collections\nfrom scipy.sparse import lil_matrix\nimport scipy.sparse\n%load_ext Cython\nfrom itertools import chain\nfrom IPython.display import display, HTML\nimport lightgbm as lgb\nfrom pprint import pprint\nfrom sklearn.metrics import roc_auc_score\nimport warnings\nwarnings.filterwarnings('ignore', 'Mean of empty slice')\n\n#for GPU\nimport torch\nfrom torch import nn\nimport torch.utils.data as torchdata\nimport torch.nn.functional as F\nimport math\nfrom collections import OrderedDict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#utils\nclass RiiidEnv:\n    def __init__(self, sn, iterate_wo_predict = False):\n        self.sn = sn\n        self.sub = sn.loc[sn['content_type_id'] == 0][['row_id']].copy()\n        self.sub['answered_correctly'] = 0.5\n        self.can_yield = True\n        self.iterate_wo_predict = iterate_wo_predict\n        self.num_groups = self.sn.index.max() + 1\n        \n    def iter_test(self):\n        for i in range(self.num_groups):\n            self.i = i\n            assert(self.can_yield)\n            if not self.iterate_wo_predict:\n                self.can_yield = False\n\n            if i in self.sub.index:\n                yield self.sn.loc[[i]], self.sub.loc[[i]]\n            elif i not in self.sub.index:\n                yield self.sn.loc[[i]], None\n                    \n    def predict(self, my_sub):\n        assert(my_sub['row_id'].dtype == 'int64')\n        assert(my_sub['answered_correctly'].dtype == 'float64')\n        assert(my_sub.index.name == 'group_num')\n        assert(my_sub.index.dtype == 'int64')\n        \n        if self.i in self.sub.index:\n            assert(np.all(my_sub['answered_correctly'] >= 0))\n            assert(np.all(my_sub['answered_correctly'] <= 1))\n            assert(np.all(my_sub.index == self.i))\n            assert(np.all(my_sub['row_id'] == self.sub.loc[[self.i]]['row_id']))\n            self.sub.loc[[self.i]] = my_sub\n            self.can_yield = True\n            \n        elif self.i not in self.sub.index:\n            assert(my_sub.shape[0] == 0)\n            self.can_yield = True\n\n@noglobal\ndef save_pickle(obj, path):\n    with open(path, mode='wb') as f:\n        pickle.dump(obj, f)\n\n@noglobal\ndef load_pickle(path):\n    with open(path, mode='rb') as f:\n        obj = pickle.load(f)\n    return obj\n\n@noglobal\ndef encode(train, column_name):\n    encoded = pd.merge(train[[column_name]], pd.DataFrame(train[column_name].unique(), columns=[column_name])\\\n                       .reset_index().dropna(), how='left', on=column_name)[['index']].rename(\n        columns={'index': column_name})\n    return encoded\n\n@noglobal\ndef update_user_map(tes, user_map_ref, n_users_ref):\n    #new_users = tes[tes['timestamp'] == 0]['user_id'].unique()\n    users = tes['user_id'].unique()\n    keys = user_map_ref.keys()\n    new_users = users[np.array([user not in keys for user in users])]\n    n_new_users = new_users.shape[0]\n    if n_new_users > 0:\n        user_map_ref[new_users] = np.arange(n_users_ref, n_users_ref + n_new_users)\n    return user_map_ref, n_users_ref + n_new_users\n    \n@noglobal\ndef write_to_ref_map(path, ref_name, f_names):\n    ref_map = load_pickle(path)\n    ref_map[ref_name] = f_names\n    save_pickle(ref_map, path)\n    \nclass VectorizedDict():\n    def __init__(self):\n        self.tr_dict = dict()\n        self.set_value = np.vectorize(self.tr_dict.__setitem__)\n        self.get_value = np.vectorize(self.tr_dict.__getitem__)\n        \n    def keys(self):\n        return self.tr_dict.keys()\n        \n    def __setitem__(self, indices, values):\n        self.set_value(indices, values)\n    \n    def __getitem__(self, indices):\n        if indices.shape[0] == 0:\n            return np.array([], dtype=np.int32)\n        return self.get_value(indices)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%cython \nimport numpy as np\ncimport numpy as np\n\ncpdef np.ndarray[int] cget_memory_indices(np.ndarray task):\n    \n    cdef Py_ssize_t n = task.shape[1]\n    cdef np.ndarray[int, ndim = 2] res = np.zeros_like(task, dtype = np.int32)\n    cdef np.ndarray[int] tmp_counter = np.full(task.shape[0], -1, dtype = np.int32)\n    cdef np.ndarray[int] u_counter = np.full(task.shape[0], task.shape[1] - 1, dtype = np.int32)\n    \n    for i in range(n):\n        res[:, i] = u_counter\n        tmp_counter += 1\n        if i != n - 1:\n            mask = (task[:, i] != task[:, i + 1])\n            u_counter[mask] = tmp_counter[mask]\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#func_gpu\n@noglobal\ndef nn_online_get_content_id_history(r_ref, tes, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int16)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = spc_tes['content_id'].values\n    return pd.DataFrame(res, columns = ['nn_content_id_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_content_id_history(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    content_ids = spc_prev_tes_cp['content_id'].values\n    for user, content_id in zip(enc_users, content_ids):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = content_id\n    return r_ref\n\n@noglobal\ndef nn_online_get_normed_log_timestamp_history(r_ref, tes, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    spc_tes['log_timestamp'] = np.log1p(spc_tes['timestamp'].values.astype(np.float32))\n    std = 3.3530\n    mean = 20.863\n    spc_tes['normed_log_timestamp'] = (spc_tes['log_timestamp'].values - mean)/std\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = spc_tes['normed_log_timestamp'].values\n    return pd.DataFrame(res, columns = ['nn_normed_log_timestamp_history_' + str(i) + '_length_' + str(n_sample) \\\n                                        for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_normed_log_timestamp_history(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    timestamp = spc_prev_tes_cp['timestamp'].values.astype(np.float32)\n    std = 3.3530\n    mean = 20.863\n    normed_log_timestamps = (np.log1p(timestamp) - mean)/std\n    for user, normed_log_timestamp in zip(enc_users, normed_log_timestamps):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = normed_log_timestamp\n    return r_ref\n\n@noglobal\ndef nn_online_get_correctness_history(r_ref, tes, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int8)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = 2\n    return pd.DataFrame(res, columns = ['nn_correctness_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_correctness_history(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    targets = spc_prev_tes_cp['answered_correctly'].values\n    for user, target in zip(enc_users, targets):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = target\n    return r_ref\n\n@noglobal\ndef nn_online_get_question_had_explanation_history(r_ref, tes, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int8)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = 2\n    return pd.DataFrame(res, columns = ['nn_question_had_explanation_history_' + str(i) + '_length_' + str(n_sample) \\\n                                        for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_early_update_reference_get_question_had_explanation_history(r_ref, tes, user_map_ref):\n    spc_tes_cp = tes[(tes['content_type_id'] == 0) \\\n                    & (~tes['prior_question_had_explanation'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    explanations = spc_tes_cp['prior_question_had_explanation'].values.astype('int')\n    for explanation, idx in zip(explanations, row_idx):\n        r_ref[idx, r_ref[idx] == 2] = explanation\n    return r_ref\n\n@noglobal\ndef nn_update_reference_get_question_had_explanation_history(r_ref, prev_tes, tes, user_map_ref):\n    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 0].copy()\n    row_idx = user_map_ref[spc_prev_tes['user_id'].values]\n    for user in row_idx:\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = 2\n    return r_ref\n\n@noglobal\ndef nn_online_get_normed_elapsed_history(r_ref, tes, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = -999\n    return pd.DataFrame(res, columns = ['nn_normed_elapsed_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_early_update_reference_get_normed_elapsed_history(r_ref, tes, user_map_ref):\n    spc_tes_cp = tes[(tes['content_type_id'] == 0) \\\n                    & (~tes['prior_question_elapsed_time'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    elapsed = spc_tes_cp['prior_question_elapsed_time'].values\n    clipped = np.clip(elapsed, 0, 1000 * 300).astype(np.float32)\n    mean = 25953\n    std = 20418\n    normed_elapsed = (clipped - mean)/std\n    for el, idx in zip(normed_elapsed, row_idx):\n        r_ref[idx, r_ref[idx] == -999] = el\n    return r_ref\n\n@noglobal\ndef nn_update_reference_get_normed_elapsed_history(r_ref, prev_tes, tes, user_map_ref):\n    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 0].copy()\n    row_idx = user_map_ref[spc_prev_tes['user_id'].values]\n    for user in row_idx:\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = -999\n    return r_ref\n\n@noglobal\ndef online_get_modified_timedelta(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    uniq, enc = np.unique(spc_tes['user_id'].values, return_inverse = True)\n    spc_tes['count'] = np.bincount(enc)[enc]\n    res = (spc_tes['timestamp'].values - r_ref[user_map_ref[spc_tes['user_id'].values]])/spc_tes['count'].values\n    return pd.DataFrame(res, columns = ['modified_timedelta']).astype(np.float32)\n\n@noglobal\ndef update_reference_get_timestamp_diff(r_ref, prev_tes, tes, user_map_ref):\n    r_ref[user_map_ref[prev_tes['user_id'].values]] = prev_tes['timestamp'].values\n    return r_ref\n\n@noglobal\ndef nn_online_get_normed_modified_timedelta_history(r_ref, tes, f_tes_delta, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n\n    clipped = np.clip(f_tes_delta['modified_timedelta'].values, 0, 1000 * 800)\n    mean = 126568\n    std = 218000\n    clipped = (clipped - mean)/std\n    clipped[np.isnan(clipped)] = 0\n    spc_tes['normed_modified_timedelta'] = clipped.astype(np.float32)\n\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = spc_tes['normed_modified_timedelta'].values\n    return pd.DataFrame(res, columns = ['nn_normed_modified_timedelta_history_' + str(i) + '_length_' + str(n_sample) \\\n                                        for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_normed_modified_timedelta_history(r_ref, prev_tes, tes, f_tes_delta, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    \n    clipped = np.clip(f_tes_delta['modified_timedelta'].values, 0, 1000 * 800)\n    mean = 126568\n    std = 218000\n    clipped = (clipped - mean)/std\n    clipped[np.isnan(clipped)] = 0\n    spc_prev_tes_cp['normed_modified_timedelta'] = clipped.astype(np.float32)\n\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    normed_modified_timedeltas = spc_prev_tes_cp['normed_modified_timedelta'].values\n    for user, normed_modified_timedelta in zip(enc_users, normed_modified_timedeltas):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = normed_modified_timedelta\n    return r_ref\n\n@noglobal\ndef nn_online_get_user_answer_history(r_ref, tes, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int8)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = 4\n    return pd.DataFrame(res, columns = ['nn_user_answer_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_user_answer_history(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    targets = spc_prev_tes_cp['user_answer'].values\n    for user, target in zip(enc_users, targets):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = target\n    return r_ref\n\n@noglobal\ndef online_get_task_container_id_diff(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = spc_tes['task_container_id'].values - r_ref[user_map_ref[spc_tes['user_id'].values]]\n    return pd.DataFrame(res, columns = ['task_container_id_diff']).astype(np.float32)\n\n@noglobal\ndef update_reference_get_task_container_id_diff(r_ref, prev_tes, tes, user_map_ref):\n    r_ref[user_map_ref[prev_tes['user_id'].values]] = prev_tes['task_container_id'].values\n    return r_ref\n\n@noglobal\ndef nn_online_get_task_container_id_diff_history(r_ref, tes, f_tes_delta, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    value = f_tes_delta['task_container_id_diff'].values\n    value[np.isnan(value)] = 0\n    spc_tes['task_container_id_diff'] = value\n    \n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = spc_tes['task_container_id_diff'].values\n    return pd.DataFrame(res, columns = ['nn_task_container_id_diff_history_' + str(i) + '_length_' + str(n_sample) \\\n                                        for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_task_container_id_diff_history(r_ref, prev_tes, tes, f_tes_delta, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    \n    value = f_tes_delta['task_container_id_diff'].values\n    value[np.isnan(value)] = 0\n    spc_prev_tes_cp['task_container_id_diff'] = value\n\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    task_container_id_diffs = spc_prev_tes_cp['task_container_id_diff'].values\n    for user, task_container_id_diff in zip(enc_users, task_container_id_diffs):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = task_container_id_diff\n    return r_ref\n\n@noglobal\ndef online_get_content_type_id_diff(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = spc_tes['content_type_id'].values - r_ref[user_map_ref[spc_tes['user_id'].values]]\n    return pd.DataFrame(res, columns = ['content_type_id_diff']).astype(np.float32)\n\n@noglobal\ndef update_reference_get_content_type_id_diff(r_ref, prev_tes, tes, user_map_ref):\n    r_ref[user_map_ref[prev_tes['user_id'].values]] = prev_tes['content_type_id'].values\n    return r_ref\n\n@noglobal\ndef nn_online_get_content_type_id_diff_history(r_ref, tes, f_tes_delta, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    value = f_tes_delta['content_type_id_diff'].values\n    value[np.isnan(value)] = 0\n    spc_tes['content_type_id_diff'] = value\n    \n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = spc_tes['content_type_id_diff'].values\n    return pd.DataFrame(res, columns = ['nn_content_type_id_diff_history_' + str(i) + '_length_' + str(n_sample) \\\n                                        for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_content_type_id_diff_history(r_ref, prev_tes, tes, f_tes_delta, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    \n    value = f_tes_delta['content_type_id_diff'].values\n    value[np.isnan(value)] = 0\n    spc_prev_tes_cp['content_type_id_diff'] = value\n\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    content_type_id_diffs = spc_prev_tes_cp['content_type_id_diff'].values\n    for user, content_type_id_diff in zip(enc_users, content_type_id_diffs):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = content_type_id_diff\n    return r_ref\n\n@noglobal\ndef nn_online_get_task_container_id_history(r_ref, tes, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int16)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = spc_tes['task_container_id'].values\n    return pd.DataFrame(res, columns = ['nn_task_container_id_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_task_container_id_history(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    task_container_ids = spc_prev_tes_cp['task_container_id'].values\n    for user, task_container_id in zip(enc_users, task_container_ids):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = task_container_id\n    return r_ref\n\nclass Embedder(nn.Module):\n    def __init__(self, n_proj, n_dims):\n        super(Embedder, self).__init__()\n        self.n_proj = n_proj\n        self.n_dims = n_dims\n        self.embed = nn.Embedding(n_proj, n_dims)\n\n    def forward(self, indices):\n        z = self.embed(indices)\n        return z\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\nclass MyEncoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n        super(MyEncoderLayer, self).__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        # Implementation of Feedforward model\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        if activation == 'relu':\n            self.activation = F.relu\n        elif activation == 'gelu':\n            self.activation = F.gelu\n\n    def forward(self, q, k, v, src_mask, src_key_padding_mask):\n        src2 = self.self_attn(q, k, v, attn_mask=src_mask,\n                              key_padding_mask=src_key_padding_mask)[0]\n        src = q + self.dropout1(src2)\n        src = self.norm1(src)\n        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n        src = src + self.dropout2(src2)\n        src = self.norm2(src)\n        return src\n\nclass MyEncoderExp162(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp162, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 4 * args.n_conv, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n        \n        #conv\n        self.conv1 = nn.Conv1d(11 + 2, 8, 7, padding = 3)\n        self.conv2 = nn.Conv1d(8, 8, 5, padding = 2)\n        self.conv3 = nn.Conv1d(8, 4, 3, padding = 1)\n\n    def forward(self, batch, args):\n        #input\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n            \n        # relative positional encoding\n        position = batch['position']\n            \n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #key and value\n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n        \n        #query process\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n        \n        #key and value process\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n        \n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        \n        #conv process\n        conv_inputs_int = batch['conv_inputs_int']\n        conv_inputs_float = batch['conv_inputs_float']\n        conv_inputs_const = batch['conv_inputs_const']\n\n        conv_explanation = conv_inputs_int[:, :, :, 0]\n        conv_correctness = conv_inputs_int[:, :, :, 1]\n        conv_normed_elapsed = conv_inputs_float[:, :, :, 0]\n        conv_normed_log_timestamp = conv_inputs_float[:, :, :, 1]\n        conv_normed_timedelta = conv_inputs_float[:, :, :, 2]\n        conv_task_container_id_diff = conv_inputs_float[:, :, :, 3]\n        conv_content_type_id_diff = conv_inputs_float[:, :, :, 4]\n    \n        const_normed_log_timestamp = conv_inputs_const[:, :, :, 0]\n        const_normed_timedelta = conv_inputs_const[:, :, :, 1]\n        ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n        ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n        for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n        conv_normed_elapsed.unsqueeze(3), conv_normed_log_timestamp.unsqueeze(3),\n        conv_normed_timedelta.unsqueeze(3), conv_task_container_id_diff.unsqueeze(3),\n        conv_content_type_id_diff.unsqueeze(3), const_normed_log_timestamp.unsqueeze(3), \n        const_normed_timedelta.unsqueeze(3)], dim = 3).transpose(0, 1)\n        for_conv = for_conv.contiguous().transpose(2, 3).view(N * n_length * 2, -1, args.n_conv)\n        out_conv = self.conv3(F.relu(self.conv2(F.relu(self.conv1(for_conv))))).view(n_length * 2, N, -1)\n        \n        #concat \n        out = torch.cat([out, out_conv, inputs_add], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 4 * args.n_conv)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n            \n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n            \n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n            \n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query[:, :n_sample].clone()\n            query_clone[token_idx, -1] = query_clone[token_idx, -1] * 0\n            memory_cat = torch.cat([query_clone, ohe_explanation[:, :-1], ohe_correctness[:, :-1], \n                                    normed_elapsed.unsqueeze(2)[:, :-1], ohe_user_answer[:, :-1]], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n            \n            #conv process\n            conv_explanation = explanation[:, -args.n_conv - 1 : -1]\n            conv_correctness = correctness[:, -args.n_conv - 1 : -1]\n            conv_normed_elapsed = normed_elapsed[:, -args.n_conv - 1 : -1]\n            conv_normed_log_timestamp = normed_log_timestamp[:, -args.n_conv - 1 : -1]\n            conv_normed_timedelta = normed_timedelta[:, -args.n_conv - 1 : -1]\n            conv_task_container_id_diff = task_container_id_diff[:, -args.n_conv - 1 : -1]\n            conv_content_type_id_diff = content_type_id_diff[:, -args.n_conv - 1 : -1]\n            \n            \n            const_normed_log_timestamp = normed_log_timestamp[:, -1:].repeat(1, args.n_conv)\n            const_normed_timedelta = normed_timedelta[:, -1:].repeat(1, args.n_conv)\n\n            ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n            ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n            for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n            conv_normed_elapsed.unsqueeze(2), conv_normed_log_timestamp.unsqueeze(2),\n            conv_normed_timedelta.unsqueeze(2), conv_task_container_id_diff.unsqueeze(2),\n            conv_content_type_id_diff.unsqueeze(2), const_normed_log_timestamp.unsqueeze(2), \n            const_normed_timedelta.unsqueeze(2)], dim = 2).transpose(1, 2)\n\n            out_conv = self.conv3(F.relu(self.conv2(F.relu(self.conv1(for_conv))))).view(n_batch, - 1)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, out_conv, features], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n    \nclass MyEncoderExp166(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp166, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 4 * args.n_conv, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n        \n        #conv\n        self.conv1 = nn.Conv1d(11 + 2, 8, 7, padding = 3)\n        self.conv2 = nn.Conv1d(8, 8, 5, padding = 2)\n        self.conv3 = nn.Conv1d(8, 4, 3, padding = 1)\n\n    def forward(self, batch, args):\n        #input\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n            \n        # relative positional encoding\n        position = batch['position']\n            \n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #key and value\n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n        \n        #query process\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n        \n        #key and value process\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n        \n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        \n        #conv process\n        conv_inputs_int = batch['conv_inputs_int']\n        conv_inputs_float = batch['conv_inputs_float']\n        conv_inputs_const = batch['conv_inputs_const']\n\n        conv_explanation = conv_inputs_int[:, :, :, 0]\n        conv_correctness = conv_inputs_int[:, :, :, 1]\n        conv_normed_elapsed = conv_inputs_float[:, :, :, 0]\n        conv_normed_log_timestamp = conv_inputs_float[:, :, :, 1]\n        conv_normed_timedelta = conv_inputs_float[:, :, :, 2]\n        conv_task_container_id_diff = conv_inputs_float[:, :, :, 3]\n        conv_content_type_id_diff = conv_inputs_float[:, :, :, 4]\n    \n        const_normed_log_timestamp = conv_inputs_const[:, :, :, 0]\n        const_normed_timedelta = conv_inputs_const[:, :, :, 1]\n        ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n        ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n        for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n        conv_normed_elapsed.unsqueeze(3), conv_normed_log_timestamp.unsqueeze(3),\n        conv_normed_timedelta.unsqueeze(3), conv_task_container_id_diff.unsqueeze(3),\n        conv_content_type_id_diff.unsqueeze(3), const_normed_log_timestamp.unsqueeze(3), \n        const_normed_timedelta.unsqueeze(3)], dim = 3).transpose(0, 1)\n        for_conv = for_conv.contiguous().transpose(2, 3).view(N * n_length * 2, -1, args.n_conv)\n        out_conv = self.conv3(F.relu(self.conv2(F.relu(self.conv1(for_conv))))).view(n_length * 2, N, -1)\n        \n        #concat \n        out = torch.cat([out, out_conv, inputs_add], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 4 * args.n_conv)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n            \n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n            \n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n            \n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query[:, :n_sample].clone()\n            query_clone[token_idx, -1] = query_clone[token_idx, -1] * 0\n            memory_cat = torch.cat([query_clone, ohe_explanation[:, :-1], ohe_correctness[:, :-1], \n                                    normed_elapsed.unsqueeze(2)[:, :-1], ohe_user_answer[:, :-1]], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n            \n            #conv process\n            conv_explanation = explanation[:, -args.n_conv - 1 : -1]\n            conv_correctness = correctness[:, -args.n_conv - 1 : -1]\n            conv_normed_elapsed = normed_elapsed[:, -args.n_conv - 1 : -1]\n            conv_normed_log_timestamp = normed_log_timestamp[:, -args.n_conv - 1 : -1]\n            conv_normed_timedelta = normed_timedelta[:, -args.n_conv - 1 : -1]\n            conv_task_container_id_diff = task_container_id_diff[:, -args.n_conv - 1 : -1]\n            conv_content_type_id_diff = content_type_id_diff[:, -args.n_conv - 1 : -1]\n            \n            \n            const_normed_log_timestamp = normed_log_timestamp[:, -1:].repeat(1, args.n_conv)\n            const_normed_timedelta = normed_timedelta[:, -1:].repeat(1, args.n_conv)\n\n            ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n            ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n            for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n            conv_normed_elapsed.unsqueeze(2), conv_normed_log_timestamp.unsqueeze(2),\n            conv_normed_timedelta.unsqueeze(2), conv_task_container_id_diff.unsqueeze(2),\n            conv_content_type_id_diff.unsqueeze(2), const_normed_log_timestamp.unsqueeze(2), \n            const_normed_timedelta.unsqueeze(2)], dim = 2).transpose(1, 2)\n\n            out_conv = self.conv3(F.relu(self.conv2(F.relu(self.conv1(for_conv))))).view(n_batch, - 1)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, out_conv, features], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n    \nclass MyEncoderExp184(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp184, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 16 * 8, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n        \n        #conv\n#         self.conv1 = nn.Conv1d(11 + 2, 8, 7, padding = 3)\n#         self.conv2 = nn.Conv1d(8, 8, 5, padding = 2)\n#         self.conv3 = nn.Conv1d(8, 4, 3, padding = 1)\n        \n        #rnn\n        self.rnn = nn.GRU(13, 16, 8, batch_first = True)\n\n    def forward(self, batch, args):\n        #input\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n            \n        # relative positional encoding\n        position = batch['position']\n            \n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #key and value\n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n        \n        #query process\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n        \n        #key and value process\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n        \n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n        \n        #conv process\n        conv_inputs_int = batch['conv_inputs_int']\n        conv_inputs_float = batch['conv_inputs_float']\n        conv_inputs_const = batch['conv_inputs_const']\n\n        conv_explanation = conv_inputs_int[:, :, :, 0]\n        conv_correctness = conv_inputs_int[:, :, :, 1]\n        conv_normed_elapsed = conv_inputs_float[:, :, :, 0]\n        conv_normed_log_timestamp = conv_inputs_float[:, :, :, 1]\n        conv_normed_timedelta = conv_inputs_float[:, :, :, 2]\n        conv_task_container_id_diff = conv_inputs_float[:, :, :, 3]\n        conv_content_type_id_diff = conv_inputs_float[:, :, :, 4]\n    \n        const_normed_log_timestamp = conv_inputs_const[:, :, :, 0]\n        const_normed_timedelta = conv_inputs_const[:, :, :, 1]\n        ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n        ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n        for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n        conv_normed_elapsed.unsqueeze(3), conv_normed_log_timestamp.unsqueeze(3),\n        conv_normed_timedelta.unsqueeze(3), conv_task_container_id_diff.unsqueeze(3),\n        conv_content_type_id_diff.unsqueeze(3), const_normed_log_timestamp.unsqueeze(3), \n        const_normed_timedelta.unsqueeze(3)], dim = 3).transpose(0, 1)\n        \n        #rnn\n        for_rnn = for_conv.contiguous().view(N * n_length * 2, args.n_conv, -1)\n        _, out_rnn = self.rnn(for_rnn)\n        out_rnn = out_rnn.transpose(0, 1).contiguous().view(n_length *2, N, -1)\n\n        #cat\n        out = torch.cat([out, out_rnn, inputs_add], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 16 * 8)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n            \n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n            \n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n            \n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query[:, :n_sample].clone()\n            query_clone[token_idx, -1] = query_clone[token_idx, -1] * 0\n            memory_cat = torch.cat([query_clone, ohe_explanation[:, :-1], ohe_correctness[:, :-1], \n                                    normed_elapsed.unsqueeze(2)[:, :-1], ohe_user_answer[:, :-1]], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n            \n            #conv process\n            conv_explanation = explanation[:, -args.n_conv - 1 : -1]\n            conv_correctness = correctness[:, -args.n_conv - 1 : -1]\n            conv_normed_elapsed = normed_elapsed[:, -args.n_conv - 1 : -1]\n            conv_normed_log_timestamp = normed_log_timestamp[:, -args.n_conv - 1 : -1]\n            conv_normed_timedelta = normed_timedelta[:, -args.n_conv - 1 : -1]\n            conv_task_container_id_diff = task_container_id_diff[:, -args.n_conv - 1 : -1]\n            conv_content_type_id_diff = content_type_id_diff[:, -args.n_conv - 1 : -1]\n            \n            \n            const_normed_log_timestamp = normed_log_timestamp[:, -1:].repeat(1, args.n_conv)\n            const_normed_timedelta = normed_timedelta[:, -1:].repeat(1, args.n_conv)\n\n            ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n            ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n            for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n            conv_normed_elapsed.unsqueeze(2), conv_normed_log_timestamp.unsqueeze(2),\n            conv_normed_timedelta.unsqueeze(2), conv_task_container_id_diff.unsqueeze(2),\n            conv_content_type_id_diff.unsqueeze(2), const_normed_log_timestamp.unsqueeze(2), \n            const_normed_timedelta.unsqueeze(2)], dim = 2)\n            \n            #rnn\n            for_rnn = for_conv\n            _, out_rnn = self.rnn(for_rnn)\n            out_rnn = out_rnn.transpose(0, 1).contiguous().view(n_batch, -1)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, out_rnn, features], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n    \nclass MyEncoderExp218(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp218, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #for rnn\n        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\n        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.norm3 = nn.LayerNorm(args.emb_dim)\n\n        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm4 = nn.LayerNorm(args.emb_dim)\n        self.rnn = nn.GRU(512, 256, 4, batch_first = True, dropout = args.dropout)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n\n    def forward(self, batch, args):\n        #inputs\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n\n        # relative positional encoding\n        position = batch['position']\n\n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #initial query and memory \n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #for rnn process\n        memory[padding_mask] = memory[padding_mask] * 0\n\n        tmp, _ = self.rnn(memory)\n        out_rnn = tmp.clone()\n        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\n\n        memory_idx = batch['memory_idx']\n        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\n        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\n        memory_rnn = out_rnn[memory_idx_]\n\n        #new query and memory\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n\n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n\n        #cat\n        out = torch.cat([out, inputs_add], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n\n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n\n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n\n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n            memory[:, :-1][padding_mask] = 0\n            memory[:, -1] = 0\n            out_rnn, _ = self.rnn(memory)\n            out_rnn[:, -1] = 0\n\n            task_container_id = inputs['task_container_id'][i: i + pred_bs]\n            memory_idx = cget_memory_indices(task_container_id)\n            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\n            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\n            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\n\n            #query process 2 \n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n            #key and value process 2\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n            memory = memory[:, :-1]\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, features], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n    \nclass MyEncoderExp224(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp224, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #for rnn\n        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\n        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.norm3 = nn.LayerNorm(args.emb_dim)\n\n        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm4 = nn.LayerNorm(args.emb_dim)\n        self.rnn = nn.LSTM(512, 256, 4, batch_first = True, dropout = args.dropout)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 256 + 16 * 8, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n        \n        self.fixlen_rnn = nn.GRU(13, 16, 8, batch_first = True)\n\n    def forward(self, batch, args):\n        #inputs\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n\n        # relative positional encoding\n        position = batch['position']\n\n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #initial query and memory \n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #for rnn process\n        memory[padding_mask] = memory[padding_mask] * 0\n\n        tmp, _ = self.rnn(memory)\n        out_rnn = tmp.clone()\n        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\n\n        memory_idx = batch['memory_idx']\n        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\n        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\n        memory_rnn = out_rnn[memory_idx_]\n\n        #new query and memory\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n\n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n        \n        #conv process\n        conv_inputs_int = batch['conv_inputs_int']\n        conv_inputs_float = batch['conv_inputs_float']\n        conv_inputs_const = batch['conv_inputs_const']\n\n        conv_explanation = conv_inputs_int[:, :, :, 0]\n        conv_correctness = conv_inputs_int[:, :, :, 1]\n        conv_normed_elapsed = conv_inputs_float[:, :, :, 0]\n        conv_normed_log_timestamp = conv_inputs_float[:, :, :, 1]\n        conv_normed_timedelta = conv_inputs_float[:, :, :, 2]\n        conv_task_container_id_diff = conv_inputs_float[:, :, :, 3]\n        conv_content_type_id_diff = conv_inputs_float[:, :, :, 4]\n    \n        const_normed_log_timestamp = conv_inputs_const[:, :, :, 0]\n        const_normed_timedelta = conv_inputs_const[:, :, :, 1]\n        ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n        ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n        for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n        conv_normed_elapsed.unsqueeze(3), conv_normed_log_timestamp.unsqueeze(3),\n        conv_normed_timedelta.unsqueeze(3), conv_task_container_id_diff.unsqueeze(3),\n        conv_content_type_id_diff.unsqueeze(3), const_normed_log_timestamp.unsqueeze(3), \n        const_normed_timedelta.unsqueeze(3)], dim = 3).transpose(0, 1)\n        \n        #rnn\n        for_rnn = for_conv.contiguous().view(N * n_length * 2, args.n_conv, -1)\n        _, out_fixlen_rnn = self.fixlen_rnn(for_rnn)\n        out_fixlen_rnn = out_fixlen_rnn.transpose(0, 1).contiguous().view(n_length *2, N, -1)\n\n        #cat\n        out = torch.cat([out, out_fixlen_rnn, inputs_add, memory_rnn.transpose(0, 1)], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 256 + 16 * 8)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n\n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n\n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n\n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n            memory[:, :-1][padding_mask] = 0\n            memory[:, -1] = 0\n            out_rnn, _ = self.rnn(memory)\n            out_rnn[:, -1] = 0\n\n            task_container_id = inputs['task_container_id'][i: i + pred_bs]\n            memory_idx = cget_memory_indices(task_container_id)\n            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\n            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\n            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\n\n            #query process 2 \n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n            #key and value process 2\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n            memory = memory[:, :-1]\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n            \n            #conv process\n            conv_explanation = explanation[:, -args.n_conv - 1 : -1]\n            conv_correctness = correctness[:, -args.n_conv - 1 : -1]\n            conv_normed_elapsed = normed_elapsed[:, -args.n_conv - 1 : -1]\n            conv_normed_log_timestamp = normed_log_timestamp[:, -args.n_conv - 1 : -1]\n            conv_normed_timedelta = normed_timedelta[:, -args.n_conv - 1 : -1]\n            conv_task_container_id_diff = task_container_id_diff[:, -args.n_conv - 1 : -1]\n            conv_content_type_id_diff = content_type_id_diff[:, -args.n_conv - 1 : -1]\n            \n            \n            const_normed_log_timestamp = normed_log_timestamp[:, -1:].repeat(1, args.n_conv)\n            const_normed_timedelta = normed_timedelta[:, -1:].repeat(1, args.n_conv)\n\n            ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n            ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n            for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n            conv_normed_elapsed.unsqueeze(2), conv_normed_log_timestamp.unsqueeze(2),\n            conv_normed_timedelta.unsqueeze(2), conv_task_container_id_diff.unsqueeze(2),\n            conv_content_type_id_diff.unsqueeze(2), const_normed_log_timestamp.unsqueeze(2), \n            const_normed_timedelta.unsqueeze(2)], dim = 2)\n            \n            #rnn\n            for_rnn = for_conv\n            _, out_fixlen_rnn = self.fixlen_rnn(for_rnn)\n            out_fixlen_rnn = out_fixlen_rnn.transpose(0, 1).contiguous().view(n_batch, -1)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, out_fixlen_rnn, features, memory_rnn[:, -1]], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n    \nclass MyEncoderExp219(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp219, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #for rnn\n        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\n        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.norm3 = nn.LayerNorm(args.emb_dim)\n\n        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm4 = nn.LayerNorm(args.emb_dim)\n        self.rnn = nn.LSTM(512, 256, 4, batch_first = True)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n\n    def forward(self, batch, args):\n        #inputs\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n\n        # relative positional encoding\n        position = batch['position']\n\n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #initial query and memory \n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #for rnn process\n        memory[padding_mask] = memory[padding_mask] * 0\n\n        tmp, _ = self.rnn(memory)\n        out_rnn = tmp.clone()\n        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\n\n        memory_idx = batch['memory_idx']\n        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\n        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\n        memory_rnn = out_rnn[memory_idx_]\n\n        #new query and memory\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n\n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n\n        #cat\n        out = torch.cat([out, inputs_add], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n\n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n\n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n\n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n            memory[:, :-1][padding_mask] = 0\n            memory[:, -1] = 0\n            out_rnn, _ = self.rnn(memory)\n            out_rnn[:, -1] = 0\n\n            task_container_id = inputs['task_container_id'][i: i + pred_bs]\n            memory_idx = cget_memory_indices(task_container_id)\n            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\n            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\n            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\n\n            #query process 2 \n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n            #key and value process 2\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n            memory = memory[:, :-1]\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, features], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n    \nclass MyEncoderExp221(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp221, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #for rnn\n        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\n        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.norm3 = nn.LayerNorm(args.emb_dim)\n\n        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm4 = nn.LayerNorm(args.emb_dim)\n        self.rnn = nn.GRU(512, 256, 4, batch_first = True)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 256, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n\n    def forward(self, batch, args):\n        #inputs\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n\n        # relative positional encoding\n        position = batch['position']\n\n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #initial query and memory \n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #for rnn process\n        memory[padding_mask] = memory[padding_mask] * 0\n\n        tmp, _ = self.rnn(memory)\n        out_rnn = tmp.clone()\n        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\n\n        memory_idx = batch['memory_idx']\n        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\n        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\n        memory_rnn = out_rnn[memory_idx_]\n\n        #new query and memory\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n\n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n\n        #cat\n        out = torch.cat([out, inputs_add, memory_rnn.transpose(0, 1)], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 256)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n\n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n\n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n\n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n            memory[:, :-1][padding_mask] = 0\n            memory[:, -1] = 0\n            out_rnn, _ = self.rnn(memory)\n            out_rnn[:, -1] = 0\n\n            task_container_id = inputs['task_container_id'][i: i + pred_bs]\n            memory_idx = cget_memory_indices(task_container_id)\n            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\n            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\n            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\n\n            #query process 2 \n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n            #key and value process 2\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n            memory = memory[:, :-1]\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, features, memory_rnn[:, -1]], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n    \nclass MyEncoderExp222(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp222, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #for rnn\n        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\n        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.norm3 = nn.LayerNorm(args.emb_dim)\n\n        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm4 = nn.LayerNorm(args.emb_dim)\n        self.rnn = nn.LSTM(512, 256, 4, batch_first = True, dropout = args.dropout)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 256, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n\n    def forward(self, batch, args):\n        #inputs\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n\n        # relative positional encoding\n        position = batch['position']\n\n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #initial query and memory \n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #for rnn process\n        memory[padding_mask] = memory[padding_mask] * 0\n\n        tmp, _ = self.rnn(memory)\n        out_rnn = tmp.clone()\n        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\n\n        memory_idx = batch['memory_idx']\n        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\n        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\n        memory_rnn = out_rnn[memory_idx_]\n\n        #new query and memory\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n\n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n\n        #cat\n        out = torch.cat([out, inputs_add, memory_rnn.transpose(0, 1)], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 256)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n\n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n\n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n\n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n            memory[:, :-1][padding_mask] = 0\n            memory[:, -1] = 0\n            out_rnn, _ = self.rnn(memory)\n            out_rnn[:, -1] = 0\n\n            task_container_id = inputs['task_container_id'][i: i + pred_bs]\n            memory_idx = cget_memory_indices(task_container_id)\n            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\n            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\n            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\n\n            #query process 2 \n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n            #key and value process 2\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n            memory = memory[:, :-1]\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, features, memory_rnn[:, -1]], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n\n#func_cpu\n@noglobal\ndef online_get_user_id_content_id_task_container_id(tes):\n    f_tes = tes[tes['content_type_id'] == 0][['user_id', \n                                           'content_id', 'task_container_id']].astype('float32').reset_index(drop = True)\n    return f_tes\n\n@noglobal\ndef online_get_timestamp_and_prior_question_elapsed_time_and_prior_question_had_explanation(tes):\n    f_tes = tes[tes['content_type_id'] == 0][['timestamp', \n                                              'prior_question_elapsed_time', 'prior_question_had_explanation']].astype('float32').reset_index(drop = True)\n    return f_tes\n\n@noglobal\ndef online_get_part(tes, que):\n    spc_tes_cp = tes[tes['content_type_id'] == 0].copy().reset_index(drop = True)\n    f_tes = pd.DataFrame(que['part'].values[spc_tes_cp['content_id'].values], columns = ['part']).astype(np.float32)\n    return f_tes\n\n@noglobal\ndef online_get_correct_answer(tes, que):\n    spc_tes_cp = tes[tes['content_type_id'] == 0].copy().reset_index(drop = True)\n    f_tes = pd.DataFrame(que['correct_answer'].values[spc_tes_cp['content_id'].values], columns = ['correct_answer']).astype(np.float32)\n    return f_tes\n\n@noglobal\ndef online_get_modified_timedelta(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    uniq, enc = np.unique(spc_tes['user_id'].values, return_inverse = True)\n    spc_tes['count'] = np.bincount(enc)[enc]\n    res = (spc_tes['timestamp'].values - r_ref[user_map_ref[spc_tes['user_id'].values]])/spc_tes['count'].values\n    return pd.DataFrame(res, columns = ['modified_timedelta']).astype(np.float32)\n\n@noglobal\ndef online_get_tags(tes, que_proc_2):\n    spc_tes_cp = tes[tes['content_type_id'] == 0].copy().reset_index(drop = True)\n    return pd.DataFrame(que_proc_2.values[spc_tes_cp['content_id'].values], columns = ['tags_' + str(i) for i in range(6)])\n\n@noglobal\ndef online_get_hell_rolling_mean_for_tags(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[:, :-1][user_map_ref[spc_tes['user_id'].values]]\n    f_tes = pd.DataFrame(sum_count[:, :, 0]/sum_count[:, :, 1], columns = ['hell_rolling_mean_for_tag_' + str(i) for i in range(188)])\n    return f_tes\n\n@noglobal\ndef online_get_rolling_mean_sum_count(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    mean = sum_count[:, 0]/sum_count[:, 1]\n    res = np.zeros((spc_tes.shape[0], 3), dtype = np.float32)\n    res[:, 0] = mean\n    res[:, 1:] = sum_count\n    res = np.zeros((spc_tes.shape[0], 3), dtype = np.float32)\n    res[:, 0] = mean\n    res[:, 1:] = sum_count\n    return pd.DataFrame(res, columns = ['target_full_mean', 'target_full_sum', 'target_count'])\n\n@noglobal\ndef update_reference_get_rolling_mean_sum_count(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    cts = spc_prev_tes_cp['user_id'].value_counts()\n    pos_cts = spc_prev_tes_cp['user_id'][spc_prev_tes_cp['answered_correctly'] == 1].value_counts()\n    r_ref[user_map_ref[pos_cts.index], 0] += pos_cts.values\n    r_ref[user_map_ref[cts.index], 1] += cts.values\n    return r_ref\n\n@noglobal\ndef online_get_rolling_mean_sum_count_for_6_tags_and_whole_tag(r_ref, tes, que_proc, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    col_idx = que_proc.values[spc_tes['content_id'].values]\n    users = user_map_ref[spc_tes['user_id'].values]\n    row_idx = np.repeat(users[:, None], 6, axis = 1)\n\n    res = np.zeros((spc_tes.shape[0], 7, 3), dtype = np.float32)\n    sliced = r_ref[row_idx, col_idx]\n    res[:, :6, 1:] = sliced\n    res[:, 6, 1:] = np.nansum(sliced, axis = 1)\n    res[:, :, 0] = res[:, :, 1]/res[:, :, 2]\n    res = res.reshape(-1, 3 * 7)\n    f_names = sum([['tags_order_mean_' + str(i), 'tags_order_sum_' + str(i), 'tags_order_count_' + str(i)] for i in range(6)], [])\n    f_names += ['whole_tags_order_mean', 'whole_tags_order_sum', 'whole_tags_order_count']\n    return pd.DataFrame(res, columns = f_names)\n\n@noglobal\ndef update_reference_get_rolling_mean_sum_count_for_6_tags_and_whole_tag(r_ref, prev_tes, tes, que_onehot, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n\n    for_sum = que_onehot.values[spc_prev_tes_cp['content_id'].values[spc_prev_tes_cp['answered_correctly'] == 1]]\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values[spc_prev_tes_cp['answered_correctly'] == 1], return_inverse = True)\n    sum_cts = np.array([np.bincount(enc, weights = for_sum[:, i]) for i in range(188)]).T\n    r_ref[user_map_ref[uniq], :-1, 0] += sum_cts\n\n    for_count = que_onehot.values[spc_prev_tes_cp['content_id']]\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'], return_inverse = True)\n    cts = np.array([np.bincount(enc, weights = for_count[:, i]) for i in range(188)]).T\n    r_ref[user_map_ref[uniq], :-1, 1] += cts\n    return r_ref\n\n@noglobal\ndef online_get_rolling_mean_sum_count_for_part(r_ref, tes, que, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    spc_tes['part'] = que['part'].values[spc_tes['content_id'].values]\n    res = np.zeros((spc_tes.shape[0], 8, 3), dtype = np.float32)\n    res[:, :7, 1:] = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res[:, 7, 1:] = r_ref[user_map_ref[spc_tes['user_id'].values], spc_tes['part'] - 1]\n    res[:, :, 0] = res[:, :, 1]/res[:, :, 2]\n    res = res.reshape(-1, 3 * 8)\n    f_names = sum([['part_' + str(i) + '_mean', 'part_' + str(i) + '_sum', 'part_' + str(i) + '_count'] for i in range(1, 8)], []) + \\\n    ['part_cut_mean', 'part_cut_sum', 'part_cut_count']\n    return pd.DataFrame(res, columns = f_names)\n\n@noglobal\ndef update_reference_get_rolling_mean_sum_count_for_part(r_ref, prev_tes, tes, que_part_onehot, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    \n    for_sum = que_part_onehot.values[spc_prev_tes_cp['content_id'].values[spc_prev_tes_cp['answered_correctly'] == 1]]\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values[spc_prev_tes_cp['answered_correctly'] == 1], return_inverse = True)\n    sum_cts = np.array([np.bincount(enc, weights = for_sum[:, i]) for i in range(7)]).T\n    r_ref[user_map_ref[uniq], :, 0] += sum_cts\n    \n    for_count = que_part_onehot.values[spc_prev_tes_cp['content_id']]\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'], return_inverse = True)\n    cts = np.array([np.bincount(enc, weights = for_count[:, i]) for i in range(7)]).T\n    r_ref[user_map_ref[uniq], :, 1] += cts\n    return r_ref\n\n@noglobal\ndef online_get_lec_rolling_count(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    return pd.DataFrame(r_ref[user_map_ref[spc_tes['user_id'].values]], columns = ['lec_rolling_count'])\n\n@noglobal\ndef update_reference_get_lec_rolling_count(r_ref, prev_tes, tes, user_map_ref):\n    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 1]\n    r_ref[user_map_ref[spc_prev_tes['user_id'].values]] += 1\n    return r_ref\n\n@noglobal\ndef online_get_lec_part_rolling_count(r_ref, tes, user_map_ref, que):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = np.zeros((spc_tes.shape[0], 8), dtype = np.float32)\n    part_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res[:, :7] = part_count\n    res[:, 7] = part_count[np.arange(spc_tes.shape[0]), que['part'].values[spc_tes['content_id'].values] - 1]\n    return pd.DataFrame(res, columns = ['lec_part_' + str(i + 1) for i in range(7)] + ['lec_part_cut'])\n\n@noglobal\ndef update_reference_get_lec_part_rolling_count(r_ref, prev_tes, tes, user_map_ref, lec, lec_map):\n    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 1]\n    row_idx = user_map_ref[spc_prev_tes['user_id']]\n    col_idx = lec['part'].values[lec_map[spc_prev_tes['content_id'].values]] - 1\n    r_ref[row_idx, col_idx] += 1\n    return r_ref\n\n@noglobal\ndef online_get_lec_type_of_rolling_count(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    return pd.DataFrame(res, columns = ['lec_type_of_' + str(i) for i in range(4)]).astype(np.float32)\n\n@noglobal\ndef update_reference_get_lec_type_of_rolling_count(r_ref, prev_tes, tes, user_map_ref, lec_proc, lec_map):\n    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 1]\n    row_idx = user_map_ref[spc_prev_tes['user_id']]\n    col_idx = lec_proc['type_of'].values[lec_map[spc_prev_tes['content_id'].values]]\n    r_ref[row_idx, col_idx] += 1\n    return r_ref\n\n@noglobal\ndef online_get_lec_tags_rolling_count(r_ref, tes, user_map_ref, que_proc):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = np.zeros((spc_tes.shape[0], 7), dtype = np.float32)\n    users = user_map_ref[spc_tes['user_id'].values]\n    row_idx = np.repeat(users[:, None], 6, axis = 1)\n    col_idx = que_proc.values[spc_tes['content_id'].values]\n    \n    sliced = r_ref[row_idx, col_idx]\n    res[:, :6] = sliced\n    res[:, 6] = np.nansum(sliced, axis = 1)\n    return pd.DataFrame(res, columns = sum([['lec_tags_order_count_' + str(i)] for i in range(6)], []) + ['lec_whole_tags_order_count'])\n\n@noglobal\ndef update_reference_get_lec_tags_rolling_count(r_ref, prev_tes, tes, user_map_ref, lec, lec_map):\n    prev_tes_cp = prev_tes.copy()\n    #prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 1].copy().reset_index(drop = True)\n    col_idx = lec['tag'].values[lec_map[spc_prev_tes_cp['content_id'].values]]\n    row_idx = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    r_ref[row_idx, col_idx] += 1\n    return r_ref\n\n@noglobal \ndef online_get_rolling_mean_sum_count_for_content_id(r_ref_sum, r_ref_count, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    row_idx = user_map_ref[spc_tes['user_id'].values]\n    col_idx = spc_tes['content_id'].values\n    f_sum = r_ref_sum[row_idx, col_idx].toarray()[0]\n    f_count = r_ref_count[row_idx, col_idx].toarray()[0]\n    f_mean = f_sum/f_count\n    f_tr = pd.DataFrame()\n    f_tr['content_id_cut_mean'] = f_mean.astype(np.float32)\n    f_tr['content_id_cut_sum'] = f_sum.astype(np.float32)\n    f_tr['content_id_cut_count'] = f_count.astype(np.float32)\n    return f_tr\n\n@noglobal\ndef update_reference_get_rolling_mean_sum_count_for_content_id(r_ref_sum, r_ref_count, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    contents = spc_prev_tes_cp['content_id'].values\n    targets = spc_prev_tes_cp['answered_correctly'].values\n    for user, content, target in zip(users, contents, targets):\n        r_ref_sum[user, content] = r_ref_sum[user, content] + target\n        r_ref_count[user, content] = r_ref_count[user, content] + 1\n    return r_ref_sum, r_ref_count\n\n@noglobal\ndef online_get_timestamp_diff(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = spc_tes['timestamp'].values - r_ref[user_map_ref[spc_tes['user_id'].values]]\n    return pd.DataFrame(res, columns = ['timestamp_diff']).astype(np.float32)\n\n@noglobal\ndef update_reference_get_timestamp_diff(r_ref, prev_tes, tes, user_map_ref):\n    r_ref[user_map_ref[prev_tes['user_id'].values]] = prev_tes['timestamp'].values\n    return r_ref\n\n@noglobal\ndef online_get_whole_oof_target_encoding_content_id(r_ref, tes):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[spc_tes['content_id'].values]\n    res = (sum_count[:, 0]/sum_count[:, 1]).astype(np.float32)\n    return pd.DataFrame(res, columns = ['whole_oof_target_encoding_content_id'])\n\n@noglobal\ndef update_reference_get_whole_oof_target_encoding_content_id(r_ref, prev_tes, tes):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    cts = spc_prev_tes_cp['content_id'].value_counts()\n    pos_cts = spc_prev_tes_cp['content_id'][spc_prev_tes_cp['answered_correctly'] == 1].value_counts()\n    r_ref[pos_cts.index.values, 0] += pos_cts.values\n    r_ref[cts.index.values, 1] += cts.values\n    return r_ref\n\n@noglobal\ndef online_get_whole_oof_target_encoding_tags_order_and_whole_tags(r_ref, tes, que_proc):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[que_proc.values[spc_tes['content_id'].values]]\n    mean = sum_count[:, :, 0]/sum_count[:, :, 1]\n    nansum = np.nansum(sum_count, axis = 1)\n    whole_mean = nansum[:, 0]/nansum[:, 1]\n    res = np.zeros((spc_tes.shape[0], 7), dtype = np.float32)\n    res[:, :6] = mean\n    res[:, 6] = whole_mean\n    f_names = ['whole_oof_target_encoding_tags_order_' + str(i) for i in range(6)] + ['whole_oof_target_encoding_whole_tags']\n    return pd.DataFrame(res, columns = f_names)\n\n@noglobal\ndef update_reference_get_whole_oof_target_encoding_tags_order_and_whole_tags(r_ref, prev_tes, tes, que_onehot):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    for_sum = que_onehot.values[spc_prev_tes_cp['content_id'].values[spc_prev_tes_cp['answered_correctly'] == 1]]\n    sum_cts = for_sum.sum(axis = 0)\n    r_ref[:-1, 0] += sum_cts\n\n    for_count = que_onehot.values[spc_prev_tes_cp['content_id'].values]\n    cts = for_count.sum(axis = 0)\n    r_ref[:-1, 1] += cts\n    return r_ref\n\n@noglobal\ndef online_get_correct_answer(tes, que):\n    spc_tes_cp = tes[tes['content_type_id'] == 0].copy().reset_index(drop = True)\n    f_tes = pd.DataFrame(que['correct_answer'].values[spc_tes_cp['content_id'].values], columns = ['correct_answer']).astype(np.float32)\n    return f_tes\n\n@noglobal\ndef online_get_modified_timedelta(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    uniq, enc = np.unique(spc_tes['user_id'].values, return_inverse = True)\n    spc_tes['count'] = np.bincount(enc)[enc]\n    res = (spc_tes['timestamp'].values - r_ref[user_map_ref[spc_tes['user_id'].values]])/spc_tes['count'].values\n    return pd.DataFrame(res, columns = ['modified_timedelta']).astype(np.float32)\n\n@noglobal\ndef update_reference_get_norm_rolling_count_and_cut_for_user_answer(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n\n    for_count = np.zeros((spc_prev_tes_cp.shape[0], 4))\n    for_count[np.arange(for_count.shape[0]), spc_prev_tes_cp['user_answer'].values] = 1\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\n    cts = np.array([np.bincount(enc, weights = for_count[:, i]) for i in range(4)]).T\n    r_ref[user_map_ref[uniq]] += cts\n    return r_ref\n\n@noglobal\ndef online_get_norm_rolling_count_and_cut_for_user_answer(r_ref, tes, que, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = np.zeros((spc_tes.shape[0], 5), dtype = np.float32)\n    cts = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    norm_cts = cts/cts.sum(axis = 1)[:, None]\n    res[:, :4] = norm_cts\n    correct_answer = que['correct_answer'].values[spc_tes['content_id'].values]\n    res[:, 4] = norm_cts[np.arange(norm_cts.shape[0]), correct_answer]\n    f_names = ['norm_rolling_count_user_answer_' + str(i) for i in range(4)] + ['cut_norm_rolling_count_user_answer']\n    return pd.DataFrame(res, columns = f_names)\n\n@noglobal\ndef online_get_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res = sum_count[:, 0]/sum_count[:, 1]\n    return pd.DataFrame(res, columns = ['rolling_mean_for_prior_question_elapsed_time']).astype(np.float32)\n\n@noglobal\ndef early_update_reference_get_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n    tes_cp = tes.copy()\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n                        & (~tes_cp['prior_question_elapsed_time'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_elapsed_time'].values * r_ref[row_idx, 2]\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n    uniq, enc = np.unique(tes_cp[tes_cp['content_type_id'] == 0]['user_id'].values, return_inverse = True)\n    r_ref[user_map_ref[uniq], 2] = np.bincount(enc)\n    return r_ref\n\n@noglobal\ndef online_get_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res = sum_count[:, 0]/sum_count[:, 1]\n    return pd.DataFrame(res, columns = ['rolling_mean_for_prior_question_had_explanation']).astype(np.float32)\n\n@noglobal\ndef early_update_reference_get_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n    tes_cp = tes.copy()\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n                        & (~tes_cp['prior_question_had_explanation'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_had_explanation'].values.astype('int') * r_ref[row_idx, 2]\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n    uniq, enc = np.unique(tes_cp[tes_cp['content_type_id'] == 0]['user_id'].values, return_inverse = True)\n    r_ref[user_map_ref[uniq], 2] = np.bincount(enc)\n    return r_ref\n\n@noglobal\ndef online_get_rolling_sum_for_prior_question_isnull(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    return pd.DataFrame(res, columns = ['rolling_sum_for_prior_question_isnull']).astype(np.float32)\n\n@noglobal\ndef update_reference_get_rolling_sum_for_prior_question_isnull(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes[(prev_tes['content_type_id'] == 0) & (prev_tes['prior_question_elapsed_time'].isnull())].copy()\n    r_ref[user_map_ref[prev_tes_cp['user_id'].values]] += prev_tes_cp['prior_question_elapsed_time'].isnull().astype('int').values\n    return r_ref\n\n@noglobal\ndef online_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res = sum_count[:, 0]/sum_count[:, 1]\n    return pd.DataFrame(res, columns = ['positive_rolling_mean_for_prior_question_elapsed_time']).astype(np.float32)\n\n@noglobal\ndef early_update_reference_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n    tes_cp = tes.copy()\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n                        & (~tes_cp['prior_question_elapsed_time'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_elapsed_time'].values * r_ref[row_idx, 2]\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n    return r_ref\n\n@noglobal\ndef update_reference_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\n    sum_cts = np.bincount(enc, weights = spc_prev_tes_cp['answered_correctly'].values)\n    r_ref[user_map_ref[uniq], 2] = sum_cts\n    return r_ref\n\n@noglobal\ndef online_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res = sum_count[:, 0]/sum_count[:, 1]\n    return pd.DataFrame(res, columns = ['negative_rolling_mean_for_prior_question_elapsed_time']).astype(np.float32)\n\n@noglobal\ndef early_update_reference_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n    tes_cp = tes.copy()\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n                        & (~tes_cp['prior_question_elapsed_time'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_elapsed_time'].values * r_ref[row_idx, 2]\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n    return r_ref\n\n@noglobal\ndef update_reference_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\n    sum_cts = np.bincount(enc, weights = 1 - spc_prev_tes_cp['answered_correctly'].values)\n    r_ref[user_map_ref[uniq], 2] = sum_cts\n    return r_ref\n\n@noglobal\ndef online_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res = sum_count[:, 0]/sum_count[:, 1]\n    return pd.DataFrame(res, columns = ['positive_rolling_mean_for_prior_question_had_explanation']).astype(np.float32)\n\n@noglobal\ndef early_update_reference_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n    tes_cp = tes.copy()\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n                        & (~tes_cp['prior_question_had_explanation'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_had_explanation'].values * r_ref[row_idx, 2]\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n    return r_ref\n\n@noglobal\ndef update_reference_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\n    sum_cts = np.bincount(enc, weights = spc_prev_tes_cp['answered_correctly'].values)\n    r_ref[user_map_ref[uniq], 2] = sum_cts\n    return r_ref\n\n@noglobal\ndef online_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res = sum_count[:, 0]/sum_count[:, 1]\n    return pd.DataFrame(res, columns = ['negative_rolling_mean_for_prior_question_had_explanation']).astype(np.float32)\n\n@noglobal\ndef early_update_reference_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n    tes_cp = tes.copy()\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n                        & (~tes_cp['prior_question_had_explanation'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_had_explanation'].values * r_ref[row_idx, 2]\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n    return r_ref\n\n@noglobal\ndef update_reference_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\n    sum_cts = np.bincount(enc, weights = 1 - spc_prev_tes_cp['answered_correctly'].values)\n    r_ref[user_map_ref[uniq], 2] = sum_cts\n    return r_ref\n\n@noglobal\ndef online_get_diff_modified_timedelta_and_prior_elapsed_time_rolling_mean(f_tes_base, f_tes_pn, f_tes_p, f_tes_n):\n    f_names = ['rolling_mean_for_prior_question_elapsed_time_diff_modified_timedelta', \n                                   'positive_rolling_mean_for_prior_question_elapsed_time_diff_modified_timedelta', \n                                   'negative_rolling_mean_for_prior_question_elapsed_time_diff_modified_timedelta']\n    f_tes = pd.DataFrame()\n    f_tes[f_names[0]] = f_tes_base.values[:, 0] - f_tes_pn.values[:, 0]\n    f_tes[f_names[1]] = f_tes_base.values[:, 0] - f_tes_p.values[:, 0]\n    f_tes[f_names[2]] = f_tes_base.values[:, 0] - f_tes_n.values[:, 0]\n    return f_tes\n\n@noglobal\ndef online_get_n_samples_rolling_mean(r_ref, tes, user_map_ref):\n    n_samples = np.array([1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 100, 200])\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    users = spc_tes['user_id'].values\n    history = r_ref[user_map_ref[users]].astype(np.float32)\n    history[history == -1] = np.nan\n    res = np.zeros((spc_tes.shape[0], len(n_samples)), dtype = np.float32)\n    for i, n_sample in enumerate(n_samples):\n        res[:, i] = np.nanmean(history[:, -n_sample:], axis = 1)\n    return pd.DataFrame(res, columns = [str(i) + '_samples_rolling_mean' for i in n_samples])\n\n@noglobal\ndef update_reference_get_n_samples_rolling_mean(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    targets = spc_prev_tes_cp['answered_correctly'].values\n    for user, target in zip(enc_users, targets):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = target\n    return r_ref\n\n@noglobal \ndef online_get_rolling_mean_sum_count_for_content_id_darkness(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    row_idx = user_map_ref[spc_tes['user_id'].values]\n    col_idx = spc_tes['content_id'].values\n    f_base = r_ref[row_idx, col_idx].toarray()[0]\n    f_sum = f_base % 128\n    f_count = f_base // 128\n    f_mean = f_sum/f_count\n    f_tr = pd.DataFrame()\n    f_tr['content_id_cut_mean'] = f_mean.astype(np.float32)\n    f_tr['content_id_cut_sum'] = f_sum.astype(np.float32)\n    f_tr['content_id_cut_count'] = f_count.astype(np.float32)\n    return f_tr\n\n@noglobal\ndef update_reference_get_rolling_mean_sum_count_for_content_id_darkness(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    contents = spc_prev_tes_cp['content_id'].values\n    targets = spc_prev_tes_cp['answered_correctly'].values\n    for user, content, target in zip(users, contents, targets):\n        r_ref[user, content] = r_ref[user, content] + target + 128\n    return r_ref","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#initialization\n#comment out\n#prefix = '..'\nprefix = '/kaggle/input/mamastan-gpu-v29'\n#sn = pd.read_pickle('../others/ex_tes.pkl'); env = RiiidEnv(sn, iterate_wo_predict = False)\nimport riiideducation; env = riiideducation.make_env()\niter_test = env.iter_test()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#params162\nargs = OrderedDict()\nargs.n_length = 200\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.num_features = 90\nargs.n_conv = 30\nargs.pred_bs = 256\nargs.device = torch.device('cuda')\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp162 = deepcopy(args)\ndel args;\n\n#params166\nargs = OrderedDict()\nargs.n_length = 400\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.num_features = 90\nargs.n_conv = 30\nargs.pred_bs = 256\nargs.device = torch.device('cuda')\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp166 = deepcopy(args)\ndel args;\n\n#params184\nargs = OrderedDict()\nargs.n_length = 400\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.num_features = 90\nargs.n_conv = 30\nargs.device = torch.device('cuda')\nargs.pred_bs = 256\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\n\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\n\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\n\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\n\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp184 = deepcopy(args)\ndel args;\n\n#params218\nargs = OrderedDict()\nargs.n_length = 400\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.num_features = 90\nargs.n_conv = 30\nargs.device = torch.device('cuda')\nargs.pred_bs = 256\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\n\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\n\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\n\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\n\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp218 = deepcopy(args)\ndel args;\n\n#params224\nargs = OrderedDict()\nargs.n_length = 400\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.num_features = 90\nargs.n_conv = 30\nargs.device = torch.device('cuda')\nargs.pred_bs = 256\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\n\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp224 = deepcopy(args)\ndel args;\n\n#params219\nargs = OrderedDict()\nargs.n_length = 400\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.lr = 0.2 * 1e-3\nargs.num_features = 90\nargs.device = torch.device('cuda')\nargs.pred_bs = 256\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\n\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp219 = deepcopy(args)\ndel args;\n\n#params221\nargs = OrderedDict()\nargs.n_length = 400\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.num_features = 90\nargs.device = torch.device('cuda')\nargs.pred_bs = 256\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\n\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp221 = deepcopy(args)\ndel args;\n\n#params222\nargs = OrderedDict()\nargs.n_length = 400\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.num_features = 90\nargs.device = torch.device('cuda')\nargs.pred_bs = 256\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\n\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp222 = deepcopy(args)\ndel args;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load data\nmy_encoder_exp162 = MyEncoderExp162(args_exp162)\nmy_encoder_exp162.load_state_dict(torch.load(prefix + '/models/my_encoder_exp162_best.pth'))\nmy_encoder_exp162.to(args_exp162.device)\nmy_encoder_exp162.eval();\n\nmy_encoder_exp166 = MyEncoderExp166(args_exp166)\nmy_encoder_exp166.load_state_dict(torch.load(prefix + '/models/my_encoder_exp166_best.pth'))\nmy_encoder_exp166.to(args_exp166.device)\nmy_encoder_exp166.eval();\n\nmy_encoder_exp184 = MyEncoderExp184(args_exp184)\nmy_encoder_exp184.load_state_dict(torch.load(prefix + '/models/my_encoder_exp184_best.pth'))\nmy_encoder_exp184.to(args_exp184.device)\nmy_encoder_exp184.eval();\n\nmy_encoder_exp218 = MyEncoderExp218(args_exp218)\nmy_encoder_exp218.load_state_dict(torch.load(prefix + '/models/my_encoder_exp218_best.pth'))\nmy_encoder_exp218.to(args_exp218.device)\nmy_encoder_exp218.eval();\n\nmy_encoder_exp224 = MyEncoderExp224(args_exp224)\nmy_encoder_exp224.load_state_dict(torch.load(prefix + '/models/my_encoder_exp224_best.pth'))\nmy_encoder_exp224.to(args_exp224.device)\nmy_encoder_exp224.eval();\n\nmy_encoder_exp222 = MyEncoderExp222(args_exp222)\nmy_encoder_exp222.load_state_dict(torch.load(prefix + '/models/my_encoder_exp222_best.pth'))\nmy_encoder_exp222.to(args_exp222.device)\nmy_encoder_exp222.eval();\n\nque =  pd.read_csv(prefix + '/data/questions.csv').astype({'question_id': 'int16', 'bundle_id': 'int16', 'correct_answer': 'int8', 'part': 'int8'})\nque_proc = pd.read_pickle(prefix + '/others/que_proc.pkl')\nque_proc_2 = pd.read_pickle(prefix + '/others/que_proc_2.pkl')\nque_onehot = pd.read_pickle(prefix + '/others/que_onehot.pkl')\nque_part_onehot = pd.read_pickle(prefix + '/others/que_part_onehot.pkl')\nlec = pd.read_csv(prefix + '/data/lectures.csv').astype({'lecture_id': 'int16', 'tag': 'int16', 'part': 'int8'})\nlec_map = np.load(prefix + '/others/lec_map.npy')\nlec_proc = pd.read_pickle(prefix + '/others/lec_proc.pkl')\nque_proc_int = np.load(prefix + '/others/que_proc_int.npy')\n\nn_sample = 400\nuser_map_ref = load_pickle(prefix + '/others/user_map_train.npy')\nn_users_ref = np.load(prefix + '/others/n_users_train.npy')\n#nn\nr_ref_g1 = np.load(prefix + '/references/nn_content_id_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g2 = np.load(prefix + '/references/nn_normed_log_timestamp_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g3 = np.load(prefix + '/references/nn_correctness_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g4 = np.load(prefix + '/references/nn_question_had_explanation_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g5 = np.load(prefix + '/references/nn_normed_elapsed_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g6 = np.load(prefix + '/references/nn_normed_modified_timedelta_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g7 = np.load(prefix + '/references/nn_user_answer_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g8 = np.load(prefix + '/references/nn_task_container_id_diff_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g9 = np.load(prefix + '/references/nn_content_type_id_diff_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g10 = np.load(prefix + '/references/nn_task_container_id_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_delta = np.load(prefix + '/references/timestamp_diff_train.npy')\nr_ref_delta_2 = np.load(prefix + '/references/task_container_id_diff_train.npy')\nr_ref_delta_3 = np.load(prefix + '/references/content_type_id_diff_train.npy')\n#cpu\nr_ref_1 = np.load(prefix + '/references/rolling_mean_sum_count_train.npy')\nr_ref_2 = np.load(prefix + '/references/rolling_mean_sum_count_for_6_tags_and_whole_tag_train.npy')\nr_ref_3 = np.load(prefix + '/references/rolling_mean_sum_count_for_part_train.npy')\nr_ref_4 = np.load(prefix + '/references/lec_rolling_count_train.npy')\nr_ref_5 = np.load(prefix + '/references/lec_part_rolling_count_train.npy')\nr_ref_6 = np.load(prefix + '/references/lec_type_of_rolling_count_train.npy')\nr_ref_7 = np.load(prefix + '/references/lec_tags_rolling_count_train.npy')\nr_ref_8 = load_pickle(prefix + '/references/rolling_mean_sum_count_for_content_id_darkness_train.npy')\nr_ref_12 = np.load(prefix + '/references/norm_rolling_count_and_cut_for_user_answer_train.npy')\nr_ref_13 = np.load(prefix + '/references/rolling_mean_for_prior_question_elapsed_time_train.npy')\nr_ref_14 = np.load(prefix + '/references/rolling_mean_for_prior_question_had_explanation_train.npy')\nr_ref_15 = np.load(prefix + '/references/rolling_sum_for_prior_question_isnull_train.npy')\nr_ref_16 = np.load(prefix + '/references/positive_rolling_mean_for_prior_question_elapsed_time_train.npy')\nr_ref_17 = np.load(prefix + '/references/negative_rolling_mean_for_prior_question_elapsed_time_train.npy')\nr_ref_18 = np.load(prefix + '/references/positive_rolling_mean_for_prior_question_had_explanation_train.npy')\nr_ref_19 = np.load(prefix + '/references/negative_rolling_mean_for_prior_question_had_explanation_train.npy')\nr_ref_20 = np.load(prefix + '/references/n_samples_rolling_mean_train.npy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run\ni = 0\nfor (tes, _) in tqdm(iter_test):\n    user_map_ref, n_users_ref = update_user_map(tes, user_map_ref, n_users_ref)\n    if i != 0:\n        # update\n        r_ref_1 = update_reference_get_rolling_mean_sum_count(r_ref_1, prev_tes, tes, user_map_ref)\n        r_ref_2 = update_reference_get_rolling_mean_sum_count_for_6_tags_and_whole_tag(r_ref_2, prev_tes, tes, que_onehot, user_map_ref)\n        r_ref_3 = update_reference_get_rolling_mean_sum_count_for_part(r_ref_3, prev_tes, tes, que_part_onehot, user_map_ref)\n        r_ref_4 = update_reference_get_lec_rolling_count(r_ref_4, prev_tes, tes, user_map_ref)\n        r_ref_5 = update_reference_get_lec_part_rolling_count(r_ref_5, prev_tes, tes, user_map_ref, lec, lec_map)\n        r_ref_6 = update_reference_get_lec_type_of_rolling_count(r_ref_6, prev_tes, tes, user_map_ref, lec_proc, lec_map)\n        r_ref_7 = update_reference_get_lec_tags_rolling_count(r_ref_7, prev_tes, tes, user_map_ref, lec, lec_map)\n        r_ref_8 = update_reference_get_rolling_mean_sum_count_for_content_id_darkness(r_ref_8, prev_tes, tes, user_map_ref)\n        \n        r_ref_12 = update_reference_get_norm_rolling_count_and_cut_for_user_answer(r_ref_12, prev_tes, tes, user_map_ref)\n        r_ref_15 = update_reference_get_rolling_sum_for_prior_question_isnull(r_ref_15, prev_tes, tes, user_map_ref)\n        r_ref_16 = update_reference_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref_16, prev_tes, tes, user_map_ref)\n        r_ref_17 = update_reference_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref_17, prev_tes, tes, user_map_ref)\n        r_ref_18 = update_reference_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref_18, prev_tes, tes, user_map_ref)\n        r_ref_19 = update_reference_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref_19, prev_tes, tes, user_map_ref)\n        r_ref_20 = update_reference_get_n_samples_rolling_mean(r_ref_20, prev_tes, tes, user_map_ref)\n\n        r_ref_delta = update_reference_get_timestamp_diff(r_ref_delta, prev_tes, tes, user_map_ref)\n        r_ref_delta_2 = update_reference_get_task_container_id_diff(r_ref_delta_2, prev_tes, tes, user_map_ref)\n        r_ref_delta_3 = update_reference_get_content_type_id_diff(r_ref_delta_3, prev_tes, tes, user_map_ref)\n        r_ref_g1 = nn_update_reference_get_content_id_history(r_ref_g1, prev_tes, tes, user_map_ref)\n        r_ref_g2 = nn_update_reference_get_normed_log_timestamp_history(r_ref_g2, prev_tes, tes, user_map_ref)\n        r_ref_g3 = nn_update_reference_get_correctness_history(r_ref_g3, prev_tes, tes, user_map_ref)\n        r_ref_g4 = nn_update_reference_get_question_had_explanation_history(r_ref_g4, prev_tes, tes, user_map_ref)\n        r_ref_g5 = nn_update_reference_get_normed_elapsed_history(r_ref_g5, prev_tes, tes, user_map_ref)\n        r_ref_g6 = nn_update_reference_get_normed_modified_timedelta_history(r_ref_g6, prev_tes, tes, f_tes_delta, user_map_ref)\n        r_ref_g7 = nn_update_reference_get_user_answer_history(r_ref_g7, prev_tes, tes, user_map_ref)\n        r_ref_g8 = nn_update_reference_get_task_container_id_diff_history(r_ref_g8, prev_tes, tes, f_tes_delta_2, user_map_ref)\n        r_ref_g9 = nn_update_reference_get_content_type_id_diff_history(r_ref_g9, prev_tes, tes, f_tes_delta_3, user_map_ref)\n        r_ref_g10 = nn_update_reference_get_task_container_id_history(r_ref_g10, prev_tes, tes, user_map_ref)\n        \n    # early update\n    r_ref_g4 = nn_early_update_reference_get_question_had_explanation_history(r_ref_g4, tes, user_map_ref)\n    r_ref_g5 = nn_early_update_reference_get_normed_elapsed_history(r_ref_g5, tes, user_map_ref)\n    r_ref_13 = early_update_reference_get_rolling_mean_for_prior_question_elapsed_time(r_ref_13, tes, user_map_ref)\n    r_ref_14 = early_update_reference_get_rolling_mean_for_prior_question_had_explanation(r_ref_14, tes, user_map_ref)\n    r_ref_16 = early_update_reference_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref_16, tes, user_map_ref)\n    r_ref_17 = early_update_reference_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref_17, tes, user_map_ref)\n    r_ref_18 = early_update_reference_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref_18, tes, user_map_ref)\n    r_ref_19 = early_update_reference_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref_19, tes, user_map_ref)\n    \n    # online function\n    f_tes_1 = online_get_rolling_mean_sum_count(r_ref_1, tes, user_map_ref)\n    f_tes_2 = online_get_rolling_mean_sum_count_for_6_tags_and_whole_tag(r_ref_2, tes, que_proc, user_map_ref)\n    f_tes_3 = online_get_rolling_mean_sum_count_for_part(r_ref_3, tes, que, user_map_ref)\n    f_tes_4 = online_get_lec_rolling_count(r_ref_4, tes, user_map_ref)\n    f_tes_5 = online_get_lec_part_rolling_count(r_ref_5, tes, user_map_ref, que)\n    f_tes_6 = online_get_lec_type_of_rolling_count(r_ref_6, tes, user_map_ref)\n    f_tes_7 = online_get_lec_tags_rolling_count(r_ref_7, tes, user_map_ref, que_proc)\n    f_tes_8 = online_get_rolling_mean_sum_count_for_content_id_darkness(r_ref_8, tes, user_map_ref)\n    \n    f_tes_12 = online_get_norm_rolling_count_and_cut_for_user_answer(r_ref_12, tes, que, user_map_ref)\n    f_tes_13 = online_get_rolling_mean_for_prior_question_elapsed_time(r_ref_13, tes, user_map_ref)\n    f_tes_14 = online_get_rolling_mean_for_prior_question_had_explanation(r_ref_14, tes, user_map_ref)\n    f_tes_15 = online_get_rolling_sum_for_prior_question_isnull(r_ref_15, tes, user_map_ref)\n    f_tes_16 = online_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref_16, tes, user_map_ref)\n    f_tes_17 = online_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref_17, tes, user_map_ref)\n    f_tes_18 = online_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref_18, tes, user_map_ref)\n    f_tes_19 = online_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref_19, tes, user_map_ref)\n    f_tes_20 = online_get_n_samples_rolling_mean(r_ref_20, tes, user_map_ref)\n    \n    f_tes_delta = online_get_modified_timedelta(r_ref_delta, tes, user_map_ref)\n    f_tes_delta_2 = online_get_task_container_id_diff(r_ref_delta_2, tes, user_map_ref)\n    f_tes_delta_3 = online_get_content_type_id_diff(r_ref_delta_3, tes, user_map_ref)\n    \n    f_tes_g1 = nn_online_get_content_id_history(r_ref_g1, tes, user_map_ref)\n    f_tes_g2 = nn_online_get_normed_log_timestamp_history(r_ref_g2, tes, user_map_ref)\n    f_tes_g3 = nn_online_get_correctness_history(r_ref_g3, tes, user_map_ref)\n    f_tes_g4 = nn_online_get_question_had_explanation_history(r_ref_g4, tes, user_map_ref)\n    f_tes_g5 = nn_online_get_normed_elapsed_history(r_ref_g5, tes, user_map_ref)\n    f_tes_g6 = nn_online_get_normed_modified_timedelta_history(r_ref_g6, tes, f_tes_delta, user_map_ref)\n    f_tes_g7 = nn_online_get_user_answer_history(r_ref_g7, tes, user_map_ref)\n    f_tes_g8 = nn_online_get_task_container_id_diff_history(r_ref_g8, tes, f_tes_delta_2, user_map_ref)\n    f_tes_g9 = nn_online_get_content_type_id_diff_history(r_ref_g9, tes, f_tes_delta_3, user_map_ref)\n    f_tes_g10 = nn_online_get_task_container_id_history(r_ref_g10, tes, user_map_ref)\n    \n\n    # make a prediction \n    concated = pd.concat([f_tes_1, f_tes_2, f_tes_3, f_tes_4, f_tes_5, f_tes_6, f_tes_7, \n                         f_tes_8, f_tes_12, f_tes_13, f_tes_14, f_tes_15, f_tes_16, f_tes_17, f_tes_18, \n                         f_tes_19, f_tes_20], axis = 1)\n    X_tes = concated[args_exp166.f_names].values.astype(np.float32)\n    \n    inputs = {'content_id' : f_tes_g1.values, 'normed_timedelta' : f_tes_g6.values, 'normed_log_timestamp' : f_tes_g2.values,\n         'explanation': f_tes_g4.values, 'correctness': f_tes_g3.values, 'normed_elapsed': f_tes_g5.values, 'user_answer' : f_tes_g7.values,\n             'task_container_id_diff': f_tes_g8.values, 'content_type_id_diff': f_tes_g9.values, 'task_container_id': f_tes_g10.values, \n              'features': X_tes}\n    inputs_200 = {key: inputs[key][:, -201:] for key in inputs.keys()}\n    \n    \n    score_exp162 = my_encoder_exp162.predict_on_batch(inputs_200, args_exp162, args_exp162.pred_bs, que, que_proc_int) \\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n    score_exp166 = my_encoder_exp166.predict_on_batch(inputs, args_exp166, args_exp166.pred_bs, que, que_proc_int) \\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n    score_exp184 = my_encoder_exp184.predict_on_batch(inputs, args_exp184, args_exp184.pred_bs, que, que_proc_int) \\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n    score_exp218 = my_encoder_exp218.predict_on_batch(inputs, args_exp218, args_exp218.pred_bs, que, que_proc_int) \\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n    score_exp224 = my_encoder_exp224.predict_on_batch(inputs, args_exp224, args_exp224.pred_bs, que, que_proc_int) \\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n    score_exp222 = my_encoder_exp222.predict_on_batch(inputs, args_exp222, args_exp222.pred_bs, que, que_proc_int) \\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n    \n    preds = [score_exp162, score_exp166, score_exp184, score_exp218, score_exp222, score_exp224]\n    weights = np.array([0.42, 0.25, 0.49, 0.85, 0.84, 0.82])\n    weights = weights/weights.sum()\n    score = 0\n    for pred, weight in zip(preds, weights):\n        score += pred * weight\n    \n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    spc_tes['answered_correctly'] = score.astype(np.float64)\n    env.predict(spc_tes[['row_id', 'answered_correctly']])\n    \n    # save previous test\n    prev_tes = tes.copy()\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}