{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Why do I care about tags?\n\nI like to approach feature engineering for an ML problem by asking: if **I** had to make the prediction I'm asking my ML system to make, what information would I want to have?\n\nFor predicting whether a given person will answer a given question correctly, I would want to know about two broad areas: \n\n- **Concepts**: What skills or concepts are being tested by this question, and how well can this person apply each concept?\n- **Misconceptions**: Does the person have any specific misconceptions that are relevant to the question? Are specific things they understand *incorrectly* about the problem at hand?\n\nGetting at misconceptions would be non-trivial with the data provided. Normally, I would look for meaningful patterns in incorrect answers to questions. But in this case, all we know about the incorrect answers is what letter the student chose. We don't have any data about *what* makes that particular answer incorrect. One potential way to go would be to cluster incorrect answers that frequently co-occur, but I'm not sure yet whether I want to go there.\n\nOn the other hand, the dataset does seem to provide a direct way of examining the concepts or skills that each question tests: each question has one or more tags associated with it, and each lecture is tagged with exactly one of those tags.\n\n## But what *are* these tags?\n\nThe dataset description says:\n> The meaning of the tags will not be provided, but these codes are sufficient for clustering the questions together.\n\nI don't find this super informative or reassuring (doesn't sufficiency sort of depend on what kind of clustering I want to do?), so I dug a bit deeper.\n\nThe [ArXiv paper](https://arxiv.org/abs/1912.03072) describess the question/tag/lecture part of the dataset as:\n> 13,169 problems and 1,021 lectures tagged with 293 types of skills\n\n*(although in a later table it notes that the \"level 1\" dataset - which seems to be what this challenge dataset is mostly based on - only has 188 tags. This matches the number of unique values in the tag field for lectures, so that checks out!)*\n\nThe [GitHub page for the EdNet dataset](https://github.com/riiid/ednet) says that the tags are \"expert-annotated\", and also notes that:\n> Once the number of incorrect answers to questions with particular tags exceeds certain threshold, Santa suggests lectures and questions with corresponding tags. ... It also offers lectures and questions if the average correctness rate of questions with particular tags decreased by more than a certain threshold.\n\nFinally, [a page on the associated AAAI workshop website](https://sites.google.com/view/tipce-2021/shared-task) says:\n> Since the table includes educational tags of each learning item, methods like BKT can effectively make use of pedagogical properties to estimate a student's knowledge state. \n\nSo, it does seem like the tags are meant to represent skills or aspects of the knowledge state of the student. And the set of tags seems to have been designed by experts, presumably ones whose expertise is in the subject matter being taught (namely, the TOEIC test).\n\n\n## My plan\n\nIn the longer run, I'd like to analyze how useful these tags might be as a representation of atomic skills in a student-knowledge model; and even possibly use the student behavior data to refine the set of tags currently present in the dataset. If and when I've convinced myself that I have a set of tags which represent learnable skills, I can use these tags to model individual users' knowledge state, and use the knowledge state to predict how they will answer a given question.\n\nSpecifically, I'm looking at [these](https://scholar.google.com/scholar?cluster=3456251199517922701) [two](https://scholar.google.com/scholar?cluster=8158178723780769138) papers, which describe data-driven methods for evaluating, improving and using expert-designed cogintive models.\n\nBut first, I want to explore a couple of interesting patterns I noticed in the tag-related data, which should help me have a better understanding of what these tags actually are."},{"metadata":{},"cell_type":"markdown","source":"# The Data\n\nFirst, I want to get the data from the questions and lectures tables into a useful format."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom scipy import signal # just in case","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"questions_df = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/questions.csv')\n# Transform tags into lists of ints:\nquestions_df['tags'] = questions_df['tags'].apply(lambda ts: [int(x) for x in str(ts).split() if x != 'nan'])\nquestions_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lectures_df = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/lectures.csv')\nlectures_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nI also want to make a convenient table for tag data. I thought about doing all this the proper pandas way, but then I remembered that I don't care about the pandas way because it does not care about me and my data-processing desires."},{"metadata":{"trusted":true},"cell_type":"code","source":"tag_to_questions = {}\nfor i, row in questions_df.iterrows():\n    for t in row['tags']:\n        if t not in tag_to_questions:\n            tag_to_questions[t] = set()\n        tag_to_questions[t].add(row['question_id'])\ntags_df = pd.DataFrame([{'tag':t,'questions':qs}for t,qs in tag_to_questions.items()])\ntags_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Two types of skills\n\nThe histogram below shows a very strongly bimodal distribution of tags.\n\nThe per-tag metric in the histogram is **the fraction of questions with this tag that also have other tags**. So for each tag, either all questions with that tag have multiple tags, or (nearly) none do."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"questions_df['multitag'] = questions_df['tags'].apply(lambda ts: len(ts)>1)\n\ndef calc_fract_multitagged(tag_row):\n    tag_qs = questions_df[questions_df['question_id'].isin(tag_row['questions'])]\n    return tag_qs[tag_qs['multitag']==True].size/tag_qs.size\ntags_df['fraction_multitagged'] = tags_df.apply(calc_fract_multitagged, axis=1)\nplt.hist(tags_df['fraction_multitagged'])\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seem to be two types of tags: ones that only ever appear by themselves in a question, and ones that only ever appear together with other tags.\n\nIf we interpret tags as skills, this means that there are two kinds of skills present in this training app:\n- a set of skills are only ever tested/trained by themselves\n- a separate set of skills that are only ever tested in conjunction with other skills. \n\nAnd nothing in between.\n\n\nFrom a pedagogical perspective, this was surprising to me at first. I expected to see, for each skill, some exercises that let the user practice that skill separately, and some that challenge the user to use it in conjunction with other skills.\n\nThen I realized that the questions in this app are probably just designed to mimic actual questions on the TOEIC; not necessarily provide explicit teaching of the individual skills. Indeed, it's probably unreasonable to expect a test-prep app to take on actually teacing the English language; it's probably more focused on test-taking skills and things you can improve by just raw grindy practice, like understanding speakers with various accents. Still, I imagine having interactive exercises that target individual skills couldn't have hurt.[](http://)"},{"metadata":{},"cell_type":"markdown","source":"# Skills are (mostly) specific to TOEIC parts\n\nIf each part of the TOEIC test was actually tagged with a separate set of skills, that could explain the two different categories of tags we see above: Some parts of the test might only ever contain questions that test one skill, while others only contain questions that test multiple skills at once.\n\nThe diagrams below shows this isn't *quite* the case - there is *some* overlap in the skills being tested by each part - but there are still meaningful patterns in how skills are split between parts"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def calc_tag_parts(tag_row):\n    tag_qs = questions_df[questions_df['question_id'].isin(tag_row['questions'])]\n    return tuple(tag_qs['part'].unique())\ntags_df['parts'] = tags_df.apply(calc_tag_parts, axis=1)\n\npart_sizes = tags_df.groupby('parts').size()\nplt.bar(range(len(part_sizes)), part_sizes)\nplt.xticks(range(len(part_sizes)),part_sizes.index, rotation=80)\nplt.title('number of tags which span a given combination of test parts')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that:\n\n- each test part does have at least some tags that are specific to that part\n- there is very little intersection between parts 1-4 (the Listening section of the test) and parts 5-7 (the Reading section). In fact, the intersection is just one tag, which spans all parts *except* part 5.\n- parts 5 & 6 have a lot of tags in common. That makes ense, since these parts of the test tend to ask very similar questions (fill in the blank with the most appropriate response).\n- There are a few tags that apply to all listening parts (1-4). These probably have to do with general aural comprehension of English.\n- There are also a few tags that are common to parts 3 and 4. These parts ask you to listen to one (in part 4) or several (in part 3) people talking and then answer questions about what was said. So these tags probably have to do with more specifically inferring meaning from spoken conversation.\n\n---\n\nGoing back to the \"tags for single-tagged questions\" vs. \"tags for multi-tagged questions\" dichotomy, below I've split out the \"fraction-multitagged\" histogram by test part. As I suspected, the test parts play a big role: \n- parts 1-4 and 7 only ever test skills in conjunction with other skills\n- part 5 only ever tests skills in isolation from other skills\n- part 6, which is very similar to part 5, also mostly tests skills in isolation, but occasionally has questions with several skill tags. I bet this is caused by that one tag which spans all parts except 5."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, axarr = plt.subplots(4, 2, figsize=(15, 15))\nflat_axes_list = [item for sublist in axarr for item in sublist]\n\nfig.tight_layout()\n\nfor part in range(1,8):\n    # I want to specifically limit the logic to questions and tags in this part\n    def calc_multitagged_in_part(tag_row):\n        part_qs = questions_df[questions_df['part']==part]\n        tag_qs = part_qs[part_qs['question_id'].isin(tag_row['questions'])]\n        return tag_qs[tag_qs['multitag']==True].size/tag_qs.size\n    part_multitagged = tags_df[tags_df['parts'].apply(lambda ps: part in ps)].apply(calc_multitagged_in_part, axis=1)\n    ax = flat_axes_list[part-1]\n    ax.set_title(f'part {part}')\n    ax.hist(part_multitagged, bins=[x/10 for x in range(11)])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Relationships between tags\n\nLet's examine how tags are related to each other through questions. Namely, which tags occur together in the same question, and how often to pairs of tags co-occur?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def tag_relationship_graph(tags, show_disconnected=True, print_values=False, return_graph=False):\n    G = nx.Graph()\n    for i in tags.index:\n        first_tag = tags.loc[i]['tag']\n        first_qs = tags.loc[i]['questions']\n        if show_disconnected:\n            G.add_node(first_tag)\n        for _, second_row in tags.loc[i:].iterrows():\n            second_tag = second_row['tag']\n            second_qs = second_row['questions']\n            if first_tag != second_tag:\n                qs_in_common = len(first_qs.intersection(second_qs))\n                if qs_in_common > 0:\n                    G.add_edge(first_tag, second_tag, weight=qs_in_common)\n                if print_values:\n                    print(f'{first_tag} <-> {second_tag}: {qs_in_common}')\n            \n    pos=nx.spring_layout(G)\n    nx.draw(G,  pos=pos, node_color='bisque', with_labels=True)\n    nx.draw_networkx_edge_labels(G, pos=pos,edge_labels=nx.get_edge_attributes(G,'weight'))\n    plt.show()\n    if return_graph:\n        return G\n\n    \ndef tags_used_in_part(part):\n    return tags_df[tags_df['parts'].apply(lambda ps: part in ps)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tag_relationship_graph(tags_used_in_part(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph of all tags used in part 5 offers no surprises, but it's a nice sanity check: None of the tags are related to each other in any way, since they are all \"singleton\" tags that only ever occur by themselves in questions.\n\nLet's take a look at the similar, but slightly more interesing, part 6:"},{"metadata":{"trusted":true},"cell_type":"code","source":"tag_relationship_graph(tags_used_in_part(6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The interesting part of the graph is a bit too clumped together in the center, because of all the disconnected tags. Let's look at just those tags that are connected to any other tag:"},{"metadata":{"trusted":true},"cell_type":"code","source":"tag_relationship_graph(tags_used_in_part(6), show_disconnected=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only tag 162 is connected to any other tags! Without it, part 6 would look exactly like part 5.\n\nTag 162 is the one tag that's present in all parts except 5, so my suspicion that it is the one tag that made the part 6 histogram look weird is confirmed:"},{"metadata":{"trusted":true},"cell_type":"code","source":"tags_df[tags_df['tag']==162]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The more-connected parts of the test all look pretty similar to each other in terms of tag relationships - basically, everything's connected:"},{"metadata":{"trusted":true},"cell_type":"code","source":"tag_relationship_graph(tags_used_in_part(4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It might be interesting to dig into whether the outlier tags are interesting in some way, and/or what's going on in the middle. But later.\n\n---\n\nOne other interesing group of tags are those tags which apply to both parts 3 and 4 (but not others):\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"tag_relationship_graph(tags_df[tags_df['parts']==(3,4)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even though we know that these tags are always used in conjunction with other tags, apparently they are never used in conjunction with *each other*. \n\nI bet that they represent some kind of one-hot encoding of a variable with 7 possible values. In fact, each question in parts 3 and 4 are tagged with exactly one of these tags:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"all_qs = set()\nfor q_set in tags_df[tags_df['parts']==(3,4)]['questions']:\n    all_qs.update(q_set)\nprint(f'{len(questions_df[questions_df[\"part\"].isin((3,4))])} total questions in parts 3 and 4')\nprint(f'{len(all_qs)} questions tagged with one of these 7 disjoint tags')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tags and lectures\n\nEach lecture is about exactly one tag, though there can be many lectures about the same tag, since there are 188 tags and over 1000 lectures.\n\nTags with more lectures probably represent harder concepts, and possibly even compound or umbrella concepts that can't be logically and concisely explained. \n\nFor example, may questions on the TOEIC test your understanding of colloquialisms and phrases common in spoken English. It might make sense to tag this kind of question with a \"colloquialism\" tag. But it's not really a concept that is teachable through a few short lectures: the \"skill\" involved is just remembering and getting used to a bunch of different unrelated phrases."},{"metadata":{"trusted":true},"cell_type":"code","source":"tags_df.set_index('tag', inplace=True, drop=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags_df['num_lectures'] = lectures_df.groupby('tag').count()['lecture_id']\ntags_df['num_lectures'] = tags_df['num_lectures'].fillna(0).astype(int)\ntags_df['lectures'] = lectures_df.groupby('tag')['lecture_id'].unique()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags_df['lectures'] = tags_df['lectures'].fillna('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(tags_df['num_lectures'], bins=range(9))\nplt.title('Histogram of the number of lectures a particular tag has')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interestingly, there are quite a few tags with no lectures at all. These could be tags that represent somehting other than a \"learnable\" concept, or they could be concepts like my proposed \"colloquialism\" tag, that are in principle learnable, but not through lectures. A third possibility is that these tags represent very easy concepts which don't even need a lecture.\n\nIt could, of course, be a mix of the three. One way to tell might be to look at the learning curves for each tag: how quickly do people get better at answering questions associated with this tag?\n\n---\n\nOur mystery 7-value one-hot encoding set of tags has a pretty interesting distribution of lectures:"},{"metadata":{"trusted":true},"cell_type":"code","source":"tags_df[tags_df['parts']==(3,4)].sort_values('num_lectures')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seems to be almost a linear growth in number of lectures from tag to tag. This might mean that these 7 values are actually some kind of \"difficulty level\" encoding, not a categorical variable.\n\nTo test that, I would also need to look at how hard the questions in each tag tend to be."},{"metadata":{},"cell_type":"markdown","source":"# Future work\n\n## Cognitive Modeling\n\nAs I mentioned at the beginning of this notebook, my goal is actually to use tags (or a modified set of tags) to model the cognitive state of the student.\n\nI plan on making separate notebooks about each one, so I won't go into details, except to link again the two papers I'm looking at:\n- [Learning Factor Analysis](https://scholar.google.com/scholar?cluster=3456251199517922701) \n- [Q-Matrix](https://scholar.google.com/scholar?cluster=8158178723780769138) \n\n## Combining questions/tags/lectures with student data\n\nAside from analyzing the learnability and/or difficulty of individual tags, there are several other things (of varying importance) I might like to explore that will require pulling in the actual student data:\n\n- Are there specific questions and/or specific wrong answers to questions which seem to trigger lectures?\n    - If so, are there some lectures that seem \"interchangeable\", in that they are equally likely to trigger after a certain question? or are lectures that cover the same tag nevertheless are triggered in different contexts, and are probably distinct in their content?\n- Diagnostic questions: According to the dataset description, each new user is asked a series of diagnostic questions to determine their current level. \n    - Which questions or tags get asked the most in the diagnostic part? \n    - Is there a separate set of diagnostic questions, or are all questions used for both diagnosis and training? \n    - Is there a deterministic decision tree of what diagnostic question gets asked next, given the answers so far?\n- Do different users answer roughly the same sequences of questions? what are common subsequences of questions?\n- Do lectures help increase performance in some tags more than others?\n- Are there tags that seem to represent specific types of misconceptions? Specifically, are there tags that correlate with clusters of co-occurring wrong answers?\n- Are people more likely to view lectures for the listening part of the test? (Because they are already in a position to consume sound and video from their phone/computer - not, for example, in a quiet public place without headphones)\n- Is there a lot of variation in lecture elapsed time? (might be a good way to infer lecture length, and/or whether people get bored with lectures and stop them)\n"},{"metadata":{},"cell_type":"markdown","source":"# Output\n\nI added some useful information, so I'm going to save the new dataframes (and tag relationship graph) for reuse.\n\nThis output is also [available as a dataset](https://www.kaggle.com/yanamal/riiid-questions-tags-lectures-expanded-metadata)."},{"metadata":{"trusted":true},"cell_type":"code","source":"tag_graph = tag_relationship_graph(tags_df, return_graph=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from networkx.readwrite import json_graph\nimport json, numpy\n\ndef default_serialize_int64(o):\n    if isinstance(o, numpy.integer): return int(o)\n    raise TypeError\n\ntag_json = json_graph.node_link_data(tag_graph)\nwith open('tag_relationships.json', 'w') as tag_file:\n    json.dump(tag_json, tag_file, default=default_serialize_int64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# put tags column back into space_separated string\nquestions_df['tags'] = questions_df['tags'].apply(lambda t_lst: ' '.join(str(t) for t in t_lst))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"questions_df.to_csv('questions_and_tags.csv', index=False)\nquestions_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# put columns back into space_separated string\ntags_df['questions'] = tags_df['questions'].apply(lambda t_lst: ' '.join(str(t) for t in t_lst))\ntags_df['parts'] = tags_df['parts'].apply(lambda t_lst: ' '.join(str(t) for t in t_lst))\ntags_df['lectures'] = tags_df['lectures'].apply(lambda t_lst: ' '.join(str(t) for t in t_lst))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags_df.to_csv('tags.csv', index=False)\ntags_df.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}