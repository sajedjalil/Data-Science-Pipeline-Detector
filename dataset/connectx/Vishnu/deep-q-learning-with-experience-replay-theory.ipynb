{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ConnectX: Deep Q Learning + Theory\nIn this notebook I'll create a deep Q-Learning agent trained using an experience replay and and explain the math behind the algorithm. This notebook was created referring several other notebooks and resources. Do refer those to get an in depth idea about RL. I will link them below:\n\n- [Intro to Game AI and Reinforcement Learning](https://www.kaggle.com/alexisbcook/deep-reinforcement-learning)\n- [Reinforcement course by David Silver](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZBiG_XpjnPrSNw-1XQaM_gB)\n- [DQN tutorial from PyTorch docs](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n- [Introduction to RL by Sutton and Barto](http://incompleteideas.net/book/RLbook2020.pdf)\n\nQLearning is an off-policy learning method that is similar to TD but the difference being it uses a replay memory and estimates the target from a fixed target policy. This helps in stabilizing the neural network used.","metadata":{"id":"7PfnT_zvk9T5"}},{"cell_type":"code","source":"from kaggle_environments import evaluate, make, utils\nfrom collections import namedtuple\nimport random\nimport numpy as np\nimport torch\nfrom kaggle_environments import make, evaluate\nfrom gym import spaces\nimport torch.nn as nn\nimport torch.nn.functional as f\nimport torch.optim as optim\nimport tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"8DRGvc-9W2au","outputId":"78c95360-b9f4-4783-d360-3224b43eff3a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Why replay?\nIn sequential training methods like SARSA or SARSA($\\lambda$), we train the function approximator by bootstrapping with the next state. And the next state is highly correlated with the current one. This correlation causes the network to blow up and be unstable, hindering the performance of the network. Introduction of replay aims to decorrelate and optimize the network. Instead of training against a sequential step, a experience replay will randonmy select de-correlated steps and optimizes the network. This stabilizes the training of the network. \n\nThe replay will store the state, action, reward and the next_state ($s_t, a_t, r_t, s_{t+1}$).\n\nThe memory implemented will have fixed capacity, as more experience comes in, older experiences will be removed from the memory","metadata":{"id":"okWMchVcnA-I"}},{"cell_type":"code","source":"transition = namedtuple('transitions', ('state', 'action', 'reward', 'next_state'))\n\nclass ReplayMemory:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.memory = []\n        \n    def push(self, state, action, reward, next_state):\n        if len(self.memory) > self.capacity:\n            self.memory.pop()\n        self.memory.append(transition(state, action, reward, next_state))\n        \n    def sample_batch(self, batch_size):\n        if len(self) > batch_size:\n            return random.sample(self.memory, batch_size)\n        return []\n    \n    def __len__(self):\n        return len(self.memory)","metadata":{"id":"tvCnlUKtW2bi","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Environment\n\nWe'll make a custom environment for the game and modify the reward system. This code is taken from [here](https://www.kaggle.com/alexisbcook/deep-reinforcement-learning) and modified.\n\n## States\nStates are converted into pytorch tensors (for convenience) of size of the connectx board (6x7). 0 is an empty space, 1 is player one and 2 is player two\n\n## Rewards\nThe reward system yields:\n- 1 if the game is won\n- -1 is the game is lost\n- 1/42 is the game is continuing\n- -10 is the agent makes an invalid move (trying to fill a already filled column) and game is ended","metadata":{"id":"4Rh7AGV2r1U5"}},{"cell_type":"code","source":"class ConnectFourGym:\n    def __init__(self, agent2=\"random\"):\n        ks_env = make(\"connectx\", debug=True)\n        self.env = ks_env.train([None, agent2])\n        self.rows = ks_env.configuration.rows\n        self.columns = ks_env.configuration.columns\n        self.action_space = spaces.Discrete(self.columns)\n        self.observation_space = spaces.Box(\n            low=0, high=2, \n            shape=(self.rows, self.columns, 1), \n            dtype=np.int\n        )\n        # Tuple corresponding to the min and max possible rewards\n        self.reward_range = (-10, 1)\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n        \n    def random_action(self):\n        return torch.tensor([self.action_space.sample()])\n\n    def reset(self):\n        self.obs = self.env.reset()\n        return torch.tensor(self.obs['board'], \n                            dtype=torch.float).reshape(1, self.rows, self.columns)\n\n    def change_reward(self, old_reward, done):\n        if old_reward == 1: # The agent won the game\n            return torch.tensor([1.])\n        elif done: # The opponent won the game\n            return torch.tensor([-1.])\n        else: # Reward 1/42\n            return torch.tensor([1/(self.rows*self.columns)])\n\n    def step(self, action):\n        # Check if agent's move is valid\n        action = action[0].item()\n        is_valid = (self.obs['board'][int(action)] == 0)\n        if is_valid: # Play the move\n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            reward = self.change_reward(old_reward, done)\n        else: # End the game and penalize agent\n            reward, done, _ = torch.tensor([-10.]), True, {}\n        return torch.tensor(self.obs['board'], dtype=torch.float).reshape(1, self.rows, self.columns), reward, done, _","metadata":{"id":"lBJ0WSkMW2bk","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function Approximator\nThe policy network will be a simple network with two convolutional layers followed by a fully connected layer and an output layer. ReLU is used as the activation function. The approximator outputs the state action value of each action (the column to be played).","metadata":{"id":"hNQEtV7ps4YP"}},{"cell_type":"code","source":"class DQNCNNPolicy(nn.Module):\n    def __init__(self, op):\n        super(DQNCNNPolicy, self).__init__()\n        self.inp = nn.Conv2d(1, 16, (2, 2))\n        self.conv_1 = nn.Conv2d(16, 32, (2, 2))\n        self.linear_1 = nn.Linear(32 * 4 * 5, 64)\n        self.linear_2 = nn.Linear(64, op)\n        \n    def forward(self, x):\n        x = f.relu(self.inp(x))\n        x = f.relu(self.conv_1(x))\n        x = f.relu(x.flatten(1))\n        x = f.relu(self.linear_1(x))\n        return f.relu(self.linear_2(x))","metadata":{"id":"R1uv1JWnW2bo","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Action selection\nAction selection will be based on $\\epsilon-greedy$ action selection. greedy action if probability > $\\epsilon$ else a random action.","metadata":{"id":"29B-CoaSwAyO"}},{"cell_type":"code","source":"def get_greedy_action(policy, state):\n    assert state.shape == (1, 6, 7)\n    state = state.unsqueeze(0)\n    return policy(state)\n\ndef get_epsilon_greedy_action(policy, state, env, eps):\n    prob = np.random.uniform()\n    \n    if prob > eps:\n        with torch.no_grad():\n            return get_greedy_action(policy, state).argmax().unsqueeze(0)\n    return env.random_action()","metadata":{"id":"DoEQUcL0W2bq","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_win_percentages(agent1, agent2, n_rounds=100):\n    # Use default Connect Four setup\n    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n    # Agent 1 goes first (roughly) half the time          \n    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n    # Agent 2 goes first (roughly) half the time      \n    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))","metadata":{"id":"g2sBHqn4W2bt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Learning\nIn tradtitional TD methods, the state action value of the current state is estimated from the next state action value. Given by the Bellman equation: $r + \\gamma Q(s', a')$. Taking in consideration function approximation: $r + \\gamma  Q(s', a', w)$. QLearning is a special case of TD(0) where the target state action value is taken as the maximum from a different previous target policy (called the off-policy). The target policy is kept fixed and updated at specific intervals. This is again done to stabilize the network. The network is updated using the temporal difference of the estimate and the current state action value given by $\\delta = r + \\gamma Q(s', a', w') - Q(s, a, w)$ using MSE loss.","metadata":{"id":"SCZyLxAFxkuN"}},{"cell_type":"code","source":"BATCH_SIZE = 32\nGAMMA = 0.9\n\ndef optimize_model(policy, target, criterion, optimizer, memory):\n    sample = memory.sample_batch(BATCH_SIZE)\n    \n    non_final_states_mask = [True if item.next_state is not None else False for item in sample]\n\n    states = torch.stack([item.state for item in sample])\n    actions = torch.stack([item.action for item in sample])\n    rewards = torch.stack([item.reward for item in sample])\n\n    non_final_states = torch.stack([item.next_state for item in sample if item.next_state is not None])\n    next_states = torch.zeros(states.shape)\n    next_states[non_final_states_mask] = non_final_states\n    \n    state_action = policy(states).gather(1, actions)\n    next_state_action = target(next_states).max(1)[0].unsqueeze(0).T.detach()\n    \n    expected = reward + GAMMA * next_state_action\n    optimizer.zero_grad()\n    loss = criterion(state_action, expected)\n    loss.backward()\n    optimizer.step()\n    return loss.item()","metadata":{"id":"Q_B2TS1VW2bv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ALPHA = 0.01\nn_episodes = 10000\nEPSILON = 0.2\nPOLICY_UPDATE = 15\n\npolicy = DQNCNNPolicy(7)\ntarget_policy = DQNCNNPolicy(7)\ntarget_policy.load_state_dict(policy.state_dict())\ntarget_policy.eval()\n\nenv = ConnectFourGym('random')\nreplay_memory = ReplayMemory(15000)\nloss_criterion = nn.MSELoss()\noptimzier_func = optim.SGD(policy.parameters(), lr=ALPHA)\n\n\nwith tqdm.tqdm(range(n_episodes), unit='episode') as tq_ep:\n    for ep in tq_ep:\n        tq_ep.set_description(f\"Episode: {ep + 1}\")\n        done = False\n        state = env.reset()\n        ep_loss, ep_reward = 0, 0\n        while not done:\n            action = get_epsilon_greedy_action(policy, state, env, EPSILON)\n            next_state, reward, done, _ = env.step(action)\n            ep_reward += reward[0].item()\n            if done:\n                next_state = None\n            replay_memory.push(state, action, reward, next_state)\n\n            if len(replay_memory) > BATCH_SIZE:\n                ep_loss += optimize_model(policy, target_policy, loss_criterion, \n                                          optimzier_func, replay_memory)\n            state = next_state\n        tq_ep.set_postfix(ep_reward=ep_reward)\n        \n        if ep % POLICY_UPDATE == 0:\n            target_policy.load_state_dict(policy.state_dict())","metadata":{"id":"HPHDGwjNW2b0","outputId":"fa65b8a2-c516-4d1b-9fc1-82cb9e802d84","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def agent1(obs, config):\n    with torch.no_grad():\n        state = torch.tensor(obs['board'], dtype=torch.float).reshape(1, 1, 6, 7)\n        col = policy(state).argmax().item()\n    is_valid = (obs['board'][int(col)] == 0)\n    if is_valid:\n        return int(col)\n    else:\n        return random.choice(\n            [col for col in range(config.columns) if obs.board[int(col)] == 0]\n        )","metadata":{"id":"1WdcNvUJW2cB","outputId":"5ce90637-ec5f-4a3f-f934-699f139cee29","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing\nLet's try running the trained agent against random and negamax agent. But first, let's try running random vs random to set a baseline.","metadata":{}},{"cell_type":"code","source":"get_win_percentages('random', 'random')\nget_win_percentages(agent1, 'random')\nget_win_percentages(agent1, 'negamax')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Random vs random has an almost 1:1 win lose rate.\n- Trained agent vs random has a better win rate for the trained agent. The trained agent fairs better than the random agent.\n- Trained agent vs negamax is absolutely horrible with negamax agent beating the trained agent in 99% of the games. ","metadata":{}},{"cell_type":"markdown","source":"# Possible improvements\n- Use better exploration strategies.\n- Come up with a better reward system if there are any.\n- Tinker with the hyperparameters for better performance","metadata":{}}]}