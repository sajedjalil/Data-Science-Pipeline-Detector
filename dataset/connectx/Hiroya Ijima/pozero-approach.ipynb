{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PoZero Approach\nThis approach is based on https://arxiv.org/abs/2007.12509","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"}},{"cell_type":"code","source":"from kaggle_environments import evaluate, make, utils\nfrom kaggle_environments.envs.connectx.connectx import play,is_win\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport time\nimport random\nimport collections\nfrom joblib import Parallel, delayed\nimport joblib\nfrom numba import jit","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Alpha_Net(torch.nn.Module):\n    def __init__(self):\n        super(Alpha_Net,self).__init__()\n        self.block1 = torch.nn.Sequential(\n        torch.nn.Conv2d(2,20,kernel_size=2,padding=0,stride=1),\n        torch.nn.BatchNorm2d(20),\n        torch.nn.LeakyReLU(),\n        torch.nn.Conv2d(20,40,kernel_size=3,padding=0,stride=1),\n        torch.nn.BatchNorm2d(40),\n        torch.nn.LeakyReLU(),\n        torch.nn.Conv2d(40,120,kernel_size=3,padding=0,stride=1),\n        torch.nn.BatchNorm2d(120),\n        torch.nn.LeakyReLU(),\n        )\n        self.block2 = torch.nn.Sequential(\n        torch.nn.Linear(240, 128),\n        torch.nn.LeakyReLU(),\n        torch.nn.Linear(128, 64),\n        torch.nn.LeakyReLU(),\n        torch.nn.Linear(64, 7),\n        )\n        self.t_out = torch.nn.LogSoftmax(dim=1)\n        self.e_out = torch.nn.Softmax(dim=1)\n        self.block3 = torch.nn.Sequential(\n        torch.nn.Linear(240, 64),\n        torch.nn.LeakyReLU(),\n        torch.nn.Linear(64, 32),\n        torch.nn.LeakyReLU(),\n        torch.nn.Linear(32, 1),\n        torch.nn.Tanh()\n        )\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = x.view(-1,240)\n        x1 = self.block2(x)\n        if self.training:\n            x1 = self.t_out(x1)\n        else:\n            x1 = self.e_out(x1)\n        x2 = self.block3(x)\n        x = torch.cat([x1,x2],dim=1)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jit\ndef opt_tp(policy,Q,ramda):\n    alpha_min = np.max(Q+ramda*policy)\n    alpha_max = np.max(Q)+ramda\n    tp0 = np.sum(ramda*policy/(alpha_min-Q))-1\n    tp1 = np.sum(ramda*policy/(alpha_max-Q))-1\n    if tp0 >= 0 and tp1 >= 0:\n        alpha0 = alpha_max\n    elif tp0 < 0 and tp1 < 0:\n        alpha0 = alpha_min\n    else:\n        for i in range(100):\n            alpha0 = (alpha_max + alpha_min) / 2\n            tpr = np.sum(ramda*policy/(alpha0-Q))-1\n            if np.abs(tpr) < 1e-4:\n                break\n            if tpr >= 0:\n                alpha_min = alpha0\n            else:\n                alpha_max = alpha0\n    target_policy = ramda*policy/(alpha0-Q) / np.sum(ramda*policy/(alpha0-Q))\n    return target_policy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def playout(board0,config,num=7):\n    score = 0.0\n    turn0 = sum([1 if p != 0 else 0 for p in board0])%2 + 1\n    for n in range(num):\n        board = board0[:]\n        for t in range(42):\n            turn = sum([1 if p != 0 else 0 for p in board])%2 + 1\n            action = random.choice([c for c in range(7) if board[c] == 0])\n            play(board,action,turn,config)\n            if sum([1 if p != 0 else 0 for p in board]) >= 42:\n                break\n            if is_win(board,action,turn,config,True):\n                if turn0 == turn:\n                    score += 1\n                else:\n                    score -= 1\n                break\n    return score / num","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def alpha_MCTS(net,board,config,start_time=None,info=None,expand=False,root=False,C=1.25,expand_threshold=1,time_lim=7.5,gamma=0.98,steps=350):#parameter C_p should be tuned\n    if info == None:\n        b1 = np.array([[[1 if p == 1 else 0 for p in board]]]).reshape(1,1,6,7)\n        b2 = np.array([[[1 if p == 2 else 0 for p in board]]]).reshape(1,1,6,7)\n        board2 = np.concatenate([b1,b2],axis=1)\n        board2 = torch.from_numpy(board2).float()\n        pred = net(board2).detach().numpy()[0]\n        policy = pred[:7]\n        value = pred[7]\n        count = [0,0,0,0,0,0,0]\n        info_each = [0,0,0,0,0,0,0]\n        Q = np.zeros(7)\n        selectable = np.ones_like(Q)\n        for pin in range(7):\n            if board[pin] != 0:\n                Q[pin] = -1\n                selectable[pin] = 0\n    else:\n        policy = info['policy']\n        info_each = info['info_each']\n        count = info['count']\n        Q = info['Q']\n        value = info['value']\n        selectable = info['selectable']\n        playout_value = info['playout_value']\n    turn_count = sum([1 if p != 0 else 0 for p in board]) + 1\n    turn = sum([1 if p != 0 else 0 for p in board])%2 + 1\n    if root:\n        start_time = time.time()\n        for t in range(steps):\n            if sum(count) < 1:\n                target_policy = policy\n            else:\n                ramda = C*np.sqrt(sum(count))/(7+sum(count))\n                target_policy = opt_tp(policy,Q,ramda)\n            child_index = random.choices([0,1,2,3,4,5,6],weights=list(target_policy),k=1)[0]\n            if board[child_index] != 0:\n                Q[child_index] = -1\n            else:\n                next_board = board[:]\n                if is_win(next_board,child_index,turn,config,False):\n                    Q[child_index] = 1\n                else:\n                    play(next_board,child_index,turn,config)\n                    fillness = sum([1 if p != 0 else 0 for p in next_board])\n                    if fillness == 42:\n                        Q[child_index] = 0\n                    else:\n                        if count[child_index] >= expand_threshold:\n                            scc, ie = alpha_MCTS(net,next_board,config,start_time,expand=True,info=info_each[child_index])\n                            Q[child_index] = -scc\n                            info_each[child_index] = ie\n                        else:\n                            if count[child_index] == 0:\n                                scc, pol, Q_e, selectable_e, pee = alpha_MCTS(net,next_board,config,start_time,info=None)\n                                vava = scc*(1-turn_count/42) + pee*(turn_count/42)\n                                Q[child_index] = -vava\n                                info_each[child_index] = {'policy':pol,'info_each':[0,0,0,0,0,0,0],'count':[0,0,0,0,0,0,0],'Q':Q_e,'value':scc,'selectable':selectable_e,'playout_value':pee}\n            count[child_index] += 1\n            if time.time()-start_time >= time_lim:\n                break\n        ramda = C*np.sqrt(sum(count))/(7+sum(count))\n        target_policy = opt_tp(policy,Q,ramda)\n        return target_policy\n    else:\n        if expand:\n            if sum(count) < 1:\n                target_policy = policy\n            else:\n                ramda = C*np.sqrt(sum(count))/(7+sum(count))\n                target_policy = opt_tp(policy,Q,ramda)\n            child_index = random.choices([0,1,2,3,4,5,6],weights=list(target_policy),k=1)[0]\n            if board[child_index] != 0:\n                Q[child_index] = -1\n            else:\n                next_board = board[:]\n                if is_win(next_board,child_index,turn,config,False):\n                    Q[child_index] = 1\n                else:\n                    play(next_board,child_index,turn,config)\n                    fillness = sum([1 if p != 0 else 0 for p in next_board])\n                    if fillness == 42:\n                        Q[child_index] = 0\n                    else:\n                        if count[child_index] >= expand_threshold:\n                            scc, ie = alpha_MCTS(net,next_board,config,start_time,expand=True,info=info_each[child_index])\n                            Q[child_index] = -gamma*scc\n                            info_each[child_index] = ie\n                        else:\n                            if count[child_index] == 0:\n                                scc, pol, Q_e, selectable_e, pee = alpha_MCTS(net,next_board,config,start_time,info=None)\n                                vava = scc*(1-turn_count/42) + pee*(turn_count/42)\n                                Q[child_index] = -gamma*vava\n                                info_each[child_index] = {'policy':pol,'info_each':[0,0,0,0,0,0,0],'count':[0,0,0,0,0,0,0],'Q':Q_e,'value':scc,'selectable':selectable_e,'playout_value':pee}\n            count[child_index] += 1\n            if sum(count) < 1:\n                return value, {'policy':policy,'info_each':info_each,'count':count,'Q':Q,'value':value,'selectable':selectable,'playout_value':playout_value}\n            ramda = C*np.sqrt(sum(count))/(7+sum(count))\n            target_policy = opt_tp(policy,Q,ramda)\n            re_Q = np.array(Q) * selectable * target_policy / np.sum(selectable * target_policy)\n            r = np.power(2,sum(count)-1)/(1+np.power(2,sum(count)-1))\n            vava = value*(1-turn_count/42) + playout_value*(turn_count/42)\n            return np.sum(re_Q)*r + vava*(1-r), {'policy':policy,'info_each':info_each,'count':count,'Q':Q,'value':value,'selectable':selectable,'playout_value':playout_value}\n        else:\n            playout_value = playout(board,config)\n            return value, policy, Q, selectable, playout_value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def self_play(net,search=70,gamma=0.98,random_turn=7):\n    env = make('connectx',debug=True)\n    log = []\n    log2 = []\n    turn = 0\n    while True:\n        turn += 1\n        p_log = {}\n        b1 = np.array([[[1 if p == 1 else 0 for p in env.state[0]['observation']['board']]]]).reshape(1,1,6,7)\n        b2 = np.array([[[1 if p == 2 else 0 for p in env.state[0]['observation']['board']]]]).reshape(1,1,6,7)\n        board = np.concatenate([b1,b2],axis=1)\n        liner_board = env.state[0]['observation']['board']\n        p_log['board'] = board[0]\n        pol = alpha_MCTS(net,liner_board,env.configuration,root=True,steps=search)\n        p_log['policy'] = pol\n        log.append(p_log)\n        action = random.choices([0,1,2,3,4,5,6],weights=list(pol),k=1)[0]\n        if liner_board[action] != 0 or turn < random_turn:\n            action = random.choice([c for c in range(7) if liner_board[c] == 0])\n        env.step([action,action])\n        if env.state[0]['status'] == 'DONE':\n            if env.state[0]['reward'] == 1:\n                first_reward = 1\n            elif env.state[0]['reward'] == 0:\n                first_reward = -1\n            else:\n                first_reward = 0\n            break\n    #Augmentation by reflection\n    for i in range(len(log)):\n        turn_reward = first_reward * (gamma ** (turn-1-i)) * ((-1)**i)\n        log[i]['reward'] = turn_reward\n        b = log[i]['board']\n        board2 = np.array([np.fliplr(b[0]),np.fliplr(b[1])]).copy()\n        if (b == board2).all():\n            continue\n        policy2 = log[i]['policy'][::-1].copy()\n        log2.append({'board':board2,'policy':policy2,'reward':turn_reward})\n    log = log + log2\n    return log","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class original_loss(torch.nn.Module):\n    def __init__(self):\n        super(original_loss,self).__init__()\n    def forward(self,pred,target):\n        CEL = (-target[:,:7] * pred[:,:7]).sum(dim=1).sum()\n        MSE = torch.pow((target[:,7]-pred[:,7]),2).sum()\n        loss = CEL+MSE\n        return CEL,MSE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fight(agent,net,fs,mode=\"negamax\"):\n    env = make('connectx',debug=True)\n    if fs == 1:\n        trainer = env.train([None,mode])\n    else:\n        trainer = env.train([mode,None])\n    observation = trainer.reset()\n    done = False\n    for step in range(42):\n        action = agent(observation, env.configuration,net)\n        observation, reward, done, info = trainer.step(action)\n        if done:\n            break\n    return reward","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AlphaN():\n    def __init__(self):\n        self.net = Alpha_Net()\n        self.buff = collections.deque([],maxlen=100000)\n        self.optim = torch.optim.RMSprop(self.net.parameters(),lr=0.001,weight_decay=1e-4)\n        self.criterion = original_loss()\n        self.loss_log = []\n        self.CEL_log = []\n        self.MSE_log = []\n        self.score_log = []\n        self.env = make(\"connectx\", debug=True)\n    def evaluate(self,num,mode='random'):\n        def forward(observation, configuration,net):\n            policy = alpha_MCTS(net,observation['board'],configuration,root=True,steps=70)\n            action = random.choices([0,1,2,3,4,5,6],weights=list(policy),k=1)[0]\n            if observation['board'][action] != 0:\n                action = random.choice([c for c in range(configuration.columns) if observation['board'][c] == 0])\n            return action\n        result1 = Parallel(n_jobs=-1,verbose=0)([delayed(fight)(forward,self.net,fs=1,mode=mode) for n in range(num)])\n        result2 = Parallel(n_jobs=-1,verbose=0)([delayed(fight)(forward,self.net,fs=2,mode=mode) for n in range(num)])\n        reward1 = sum(result1)\n        reward2 = sum(result2)\n        return reward1 / num, reward2 / num\n    def train(self,num,play_num=40,batch_num=32,batch_size=64,train_loop=50,time_lim=30000):\n        start_time = time.time()\n        for t in range(num):\n            #self play\n            self.net.eval()\n            with torch.no_grad():\n                if t%5 == 0:\n                    score1, score2 = self.evaluate(24,mode='negamax')\n                    self.score_log.append((score1+score2)/2)\n                    print('step: '+str(t)+'  score1: '+str(score1)+'  score2: '+str(score2))\n                if t == 0:\n                    bu = Parallel(n_jobs=-1,verbose=0)([delayed(self_play)(net,search=49) for net in [self.net]*1000])\n                else:\n                    bu = Parallel(n_jobs=-1,verbose=0)([delayed(self_play)(net) for net in [self.net]*play_num])\n                for ebb in bu:\n                    self.buff += ebb\n            #Updating Network\n            self.net.train()\n            for loop in range(train_loop):\n                run_loss = 0\n                run_CEL = 0\n                run_MSE = 0\n                for batchs in range(batch_num):\n                    batch = random.choices(self.buff,k=batch_size)\n                    self.optim.zero_grad()\n                    board = np.array([b['board'] for b in batch])\n                    board = torch.from_numpy(board).float()\n                    policy = np.array([b['policy'] for b in batch])\n                    value = np.array([[b['reward']] for b in batch])\n                    target = np.concatenate([policy,value],axis=1)\n                    target = torch.from_numpy(target).float()\n                    pred = self.net(board)\n                    with torch.autograd.detect_anomaly():\n                        CEL,MSE = self.criterion(pred,target)\n                        loss = CEL + MSE\n                        loss.backward()\n                    self.optim.step()\n                    run_loss += loss.item() / batch_num / batch_size\n                    run_CEL += CEL.item() / batch_num / batch_size\n                    run_MSE += MSE.item() / batch_num / batch_size\n                self.loss_log.append(run_loss)\n                self.CEL_log.append(run_CEL)\n                self.MSE_log.append(run_MSE)\n            print('step: '+str(t)+'  loss: '+str(run_loss))\n            if time.time() - start_time > time_lim:\n                print('time over')\n                break\n        plt.plot(self.score_log)\n        plt.title('Score vsNegaMax')\n        plt.xlabel('step')\n        plt.ylabel('score')\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"agent1 = AlphaN()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"agent1.train(1000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(agent1.loss_log)\nplt.title('total loss')\nplt.xlabel('loop')\nplt.ylabel('loss')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(agent1.MSE_log)\nplt.title('value loss')\nplt.xlabel('loop')\nplt.ylabel('loss')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(agent1.CEL_log)\nplt.title('policy loss')\nplt.xlabel('loop')\nplt.ylabel('loss')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.set_printoptions(threshold=np.inf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out = \"\"\"\nfrom kaggle_environments.envs.connectx.connectx import play,is_win\nimport time\nimport random\nimport torch\nimport numpy as np\nfrom numba import jit\nclass Alpha_Net(torch.nn.Module):\n    def __init__(self):\n        super(Alpha_Net,self).__init__()\n        self.block1 = torch.nn.Sequential(\n        torch.nn.Conv2d(2,20,kernel_size=2,padding=0,stride=1),\n        torch.nn.BatchNorm2d(20),\n        torch.nn.LeakyReLU(),\n        torch.nn.Conv2d(20,40,kernel_size=3,padding=0,stride=1),\n        torch.nn.BatchNorm2d(40),\n        torch.nn.LeakyReLU(),\n        torch.nn.Conv2d(40,120,kernel_size=3,padding=0,stride=1),\n        torch.nn.BatchNorm2d(120),\n        torch.nn.LeakyReLU(),\n        )\n        self.block2 = torch.nn.Sequential(\n        torch.nn.Linear(240, 128),\n        torch.nn.LeakyReLU(),\n        torch.nn.Linear(128, 64),\n        torch.nn.LeakyReLU(),\n        torch.nn.Linear(64, 7),\n        )\n        self.t_out = torch.nn.LogSoftmax(dim=1)\n        self.e_out = torch.nn.Softmax(dim=1)\n        self.block3 = torch.nn.Sequential(\n        torch.nn.Linear(240, 64),\n        torch.nn.LeakyReLU(),\n        torch.nn.Linear(64, 32),\n        torch.nn.LeakyReLU(),\n        torch.nn.Linear(32, 1),\n        torch.nn.Tanh()\n        )\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = x.view(-1,240)\n        x1 = self.block2(x)\n        if self.training:\n            x1 = self.t_out(x1)\n        else:\n            x1 = self.e_out(x1)\n        x2 = self.block3(x)\n        x = torch.cat([x1,x2],dim=1)\n        return x\n@jit\ndef opt_tp(policy,Q,ramda):\n    alpha_min = np.max(Q+ramda*policy)\n    alpha_max = np.max(Q)+ramda\n    tp0 = np.sum(ramda*policy/(alpha_min-Q))-1\n    tp1 = np.sum(ramda*policy/(alpha_max-Q))-1\n    if tp0 >= 0 and tp1 >= 0:\n        alpha0 = alpha_max\n    elif tp0 < 0 and tp1 < 0:\n        alpha0 = alpha_min\n    else:\n        for i in range(100):\n            alpha0 = (alpha_max + alpha_min) / 2\n            tpr = np.sum(ramda*policy/(alpha0-Q))-1\n            if np.abs(tpr) < 1e-4:\n                break\n            if tpr >= 0:\n                alpha_min = alpha0\n            else:\n                alpha_max = alpha0\n    target_policy = ramda*policy/(alpha0-Q) / np.sum(ramda*policy/(alpha0-Q))\n    return target_policy\ndef playout(board0,config,num=21):\n    score = 0.0\n    turn0 = sum([1 if p != 0 else 0 for p in board0])%2 + 1\n    for n in range(num):\n        board = board0[:]\n        for t in range(42):\n            turn = sum([1 if p != 0 else 0 for p in board])%2 + 1\n            action = random.choice([c for c in range(7) if board[c] == 0])\n            play(board,action,turn,config)\n            if sum([1 if p != 0 else 0 for p in board]) >= 42:\n                break\n            if is_win(board,action,turn,config,True):\n                if turn0 == turn:\n                    score += 1\n                else:\n                    score -= 1\n                break\n    return score / num\ndef alpha_MCTS(net,board,config,start_time=None,info=None,expand=False,root=False,C=1.25,expand_threshold=1,time_lim=2.0,gamma=0.98,steps=1000000):\n    if info == None:\n        b1 = np.array([[[1 if p == 1 else 0 for p in board]]]).reshape(1,1,6,7)\n        b2 = np.array([[[1 if p == 2 else 0 for p in board]]]).reshape(1,1,6,7)\n        board2 = np.concatenate([b1,b2],axis=1)\n        board2 = torch.from_numpy(board2).float()\n        pred = net(board2).detach().numpy()[0]\n        policy = pred[:7]\n        value = pred[7]\n        count = [0,0,0,0,0,0,0]\n        info_each = [0,0,0,0,0,0,0]\n        Q = np.zeros(7)\n        selectable = np.ones_like(Q)\n        for pin in range(7):\n            if board[pin] != 0:\n                Q[pin] = -1\n                selectable[pin] = 0\n    else:\n        policy = info['policy']\n        info_each = info['info_each']\n        count = info['count']\n        Q = info['Q']\n        value = info['value']\n        selectable = info['selectable']\n        playout_value = info['playout_value']\n    turn_count = sum([1 if p != 0 else 0 for p in board]) + 1\n    turn = sum([1 if p != 0 else 0 for p in board])%2 + 1\n    if root:\n        start_time = time.time()\n        for t in range(steps):\n            if sum(count) < 1:\n                target_policy = policy\n            else:\n                ramda = C*np.sqrt(sum(count))/(7+sum(count))\n                target_policy = opt_tp(policy,Q,ramda)\n            child_index = random.choices([0,1,2,3,4,5,6],weights=list(target_policy),k=1)[0]\n            if board[child_index] != 0:\n                Q[child_index] = -1\n            else:\n                next_board = board[:]\n                if is_win(next_board,child_index,turn,config,False):\n                    Q[child_index] = 1\n                else:\n                    play(next_board,child_index,turn,config)\n                    fillness = sum([1 if p != 0 else 0 for p in next_board])\n                    if fillness == 42:\n                        Q[child_index] = 0\n                    else:\n                        if count[child_index] >= expand_threshold:\n                            scc, ie = alpha_MCTS(net,next_board,config,start_time,expand=True,info=info_each[child_index])\n                            Q[child_index] = -scc\n                            info_each[child_index] = ie\n                        else:\n                            if count[child_index] == 0:\n                                scc, pol, Q_e, selectable_e, pee = alpha_MCTS(net,next_board,config,start_time,info=None)\n                                vava = scc*(1-turn_count/42) + pee*(turn_count/42)\n                                Q[child_index] = -vava\n                                info_each[child_index] = {'policy':pol,'info_each':[0,0,0,0,0,0,0],'count':[0,0,0,0,0,0,0],'Q':Q_e,'value':scc,'selectable':selectable_e,'playout_value':pee}\n            count[child_index] += 1\n            if time.time()-start_time >= time_lim:\n                break\n        ramda = C*np.sqrt(sum(count))/(7+sum(count))\n        target_policy = opt_tp(policy,Q,ramda)\n        return target_policy\n    else:\n        if expand:\n            if sum(count) < 1:\n                target_policy = policy\n            else:\n                ramda = C*np.sqrt(sum(count))/(7+sum(count))\n                target_policy = opt_tp(policy,Q,ramda)\n            child_index = random.choices([0,1,2,3,4,5,6],weights=list(target_policy),k=1)[0]\n            if board[child_index] != 0:\n                Q[child_index] = -1\n            else:\n                next_board = board[:]\n                if is_win(next_board,child_index,turn,config,False):\n                    Q[child_index] = 1\n                else:\n                    play(next_board,child_index,turn,config)\n                    fillness = sum([1 if p != 0 else 0 for p in next_board])\n                    if fillness == 42:\n                        Q[child_index] = 0\n                    else:\n                        if count[child_index] >= expand_threshold:\n                            scc, ie = alpha_MCTS(net,next_board,config,start_time,expand=True,info=info_each[child_index])\n                            Q[child_index] = -gamma*scc\n                            info_each[child_index] = ie\n                        else:\n                            if count[child_index] == 0:\n                                scc, pol, Q_e, selectable_e, pee = alpha_MCTS(net,next_board,config,start_time,info=None)\n                                vava = scc*(1-turn_count/42) + pee*(turn_count/42)\n                                Q[child_index] = -gamma*vava\n                                info_each[child_index] = {'policy':pol,'info_each':[0,0,0,0,0,0,0],'count':[0,0,0,0,0,0,0],'Q':Q_e,'value':scc,'selectable':selectable_e,'playout_value':pee}\n            count[child_index] += 1\n            if sum(count) < 1:\n                return value, {'policy':policy,'info_each':info_each,'count':count,'Q':Q,'value':value,'selectable':selectable,'playout_value':playout_value}\n            ramda = C*np.sqrt(sum(count))/(7+sum(count))\n            target_policy = opt_tp(policy,Q,ramda)\n            re_Q = np.array(Q) * selectable * target_policy / np.sum(selectable * target_policy)\n            r = np.power(2,sum(count)-1)/(1+np.power(2,sum(count)-1))\n            vava = value*(1-turn_count/42) + playout_value*(turn_count/42)\n            return np.sum(re_Q)*r + vava*(1-r), {'policy':policy,'info_each':info_each,'count':count,'Q':Q,'value':value,'selectable':selectable,'playout_value':playout_value}\n        else:\n            playout_value = playout(board,config)\n            return value, policy, Q, selectable, playout_value\nnet = Alpha_Net()\n\"\"\"\nfor key in agent1.net.state_dict().keys():\n    if 'num' in key:\n        out += \"net.state_dict()['\"+key+\"'] = torch.tensor(\"+str(agent1.net.state_dict()[key].item())+\")\\n\"\n    else:\n        out += \"net.state_dict()['\"+key+\"'][:] = torch.tensor(\"+str(list(agent1.net.state_dict()[key].to(torch.device(\"cpu\")).numpy())).replace('array(', '').replace(')', '').replace(' ', '').replace('\\n', '').replace(',dtype=float32','')+\")\\n\"\nout += \"\"\"\ndef agent(observation,configuration):\n    start_time = time.time()\n    net.eval()\n    with torch.no_grad():\n        pol = alpha_MCTS(net,observation['board'],configuration,start_time=start_time,root=True)\n    action = int(np.argmax(np.array(pol)))\n    if observation['board'][action] != 0:\n        action = random.choice([c for c in range(configuration.columns) if observation['board'][c] == 0])\n    return action\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('submission.py', 'w') as f:\n    f.write(out)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i_time = time.time()\nfrom submission import agent\nf_time = time.time() - i_time\nprint(str(f_time)+'s')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = make(\"connectx\", debug=True)\ntrainer = env.train([\"negamax\",None])\n\nobservation = trainer.reset()\n\nwhile not env.done:\n    i_time = time.time()\n    my_action = agent(observation, env.configuration)\n    f_time = time.time() - i_time\n    print(str(f_time)+'s')\n    print(\"My Action\", my_action)\n    observation, reward, done, info = trainer.step(my_action)\nenv.render()\nprint(reward)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}