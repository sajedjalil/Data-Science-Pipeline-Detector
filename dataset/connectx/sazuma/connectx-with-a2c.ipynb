{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install 'kaggle-environments>=0.1.6' > /dev/null\n!pip install git+https://github.com/openai/baselines > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport base64\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom tqdm import tqdm_notebook as tqdm\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch import optim\n\nimport gym\nfrom gym import spaces\nfrom gym.spaces.box import Box\n\nfrom baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n\nfrom kaggle_environments import evaluate, make\nfrom kaggle_environments.envs.connectx.connectx import is_win","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Set Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_PROCESSES = 16\nNUM_ADVANCED_STEP = 5\nGAMMA = 0.99\n\nTOTAL_MOVES = 3e5\nNUM_UPDATES = int(TOTAL_MOVES / NUM_ADVANCED_STEP / NUM_PROCESSES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A2C loss\nvalue_loss_coef = 0.5\nentropy_coef = 0.01\nmax_grad_norm = 0.5\n\n# RMSprop\nlr = 7e-4\neps = 1e-5\nalpha = 0.99","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Classes and Functions"},{"metadata":{},"cell_type":"markdown","source":"## ConnectX Environment"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConnectX(gym.Env):\n    def __init__(self, switch_prob=0.5, opponent='random'):\n        self.env = make('connectx')\n        self.pair = [None, opponent]\n        self.trainer = self.env.train(self.pair)\n        self.switch_prob = switch_prob\n        \n        self.rows = self.env.configuration.rows\n        self.columns = self.env.configuration.columns\n        self.action_space = spaces.Discrete(self.columns)\n        self.observation_space = spaces.Box(low=0, high=1,\n                                            shape=(3, self.rows, self.columns), dtype=np.uint8)\n    def switch_trainer(self):\n        self.pair = self.pair[::-1]\n        self.trainer = self.env.train(self.pair)\n    \n    def observation(self, observation):\n        obs = observation.board\n        if observation.mark == 2:\n            obs = [3 - x if x != 0 else 0 for x in obs]\n        \n        obs = np.array(obs).reshape(self.rows, self.columns)\n        obs = np.eye(3)[obs].transpose(2, 0, 1)\n        return obs\n\n    def step(self, action):\n        obs, reward, done, info = self.trainer.step(int(action))\n        \n        if reward == 1: # Won\n            reward = 1\n        elif reward == 0: # Lost\n            reward = -1\n        else:\n            reward = 0\n            \n        return self.observation(obs), reward, done, info\n    \n    def reset(self):\n        if np.random.random() < self.switch_prob:\n            self.switch_trainer()\n        obs = self.trainer.reset()\n        return self.observation(obs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Opponent"},{"metadata":{"trusted":true},"cell_type":"code","source":"def weighted_random(obs, config):\n    from kaggle_environments.envs.connectx.connectx import is_win\n    from random import choices\n    from scipy.stats import norm\n    \n    columns = [c for c in range(config.columns) if obs.board[c] == 0]\n    for mark in [obs.mark, 3 - obs.mark]:\n        for column in columns:\n            if is_win(obs.board, column, mark, config, False):\n                return column\n\n    return choices(columns, weights=norm.pdf(columns, 3, 1))[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_env():\n    def _thunk():\n        env = ConnectX(opponent=weighted_random)\n        return env\n\n    return _thunk","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Memory"},{"metadata":{"trusted":true},"cell_type":"code","source":"class RolloutStorage(object):\n    def __init__(self, num_steps, num_processes, obs_shape):\n        self.observations = torch.zeros(\n            num_steps + 1, num_processes, *obs_shape).cuda()\n        self.masks = torch.ones(num_steps + 1, num_processes, 1).cuda()\n        self.rewards = torch.zeros(num_steps, num_processes, 1).cuda()\n        self.actions = torch.zeros(\n            num_steps, num_processes, 1).long().cuda()\n\n        self.returns = torch.zeros(num_steps + 1, num_processes, 1).cuda()\n        self.index = 0\n\n    def insert(self, current_obs, action, reward, mask):\n        self.observations[self.index + 1].copy_(current_obs)\n        self.masks[self.index + 1].copy_(mask)\n        self.rewards[self.index].copy_(reward)\n        self.actions[self.index].copy_(action)\n\n        self.index = (self.index + 1) % NUM_ADVANCED_STEP\n\n    def after_update(self):\n        self.observations[0].copy_(self.observations[-1])\n        self.masks[0].copy_(self.masks[-1])\n\n    def compute_returns(self, next_value):\n        self.returns[-1] = next_value\n        for ad_step in reversed(range(self.rewards.size(0))):\n            self.returns[ad_step] = self.returns[ad_step + 1] * \\\n                GAMMA * self.masks[ad_step + 1] + self.rewards[ad_step]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def init(module, gain):\n    nn.init.orthogonal_(module.weight.data, gain=gain)\n    nn.init.constant_(module.bias.data, 0)\n    return module\n\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\n\nclass Net(nn.Module):\n    def __init__(self, n_out):\n        super(Net, self).__init__()\n\n        def init_(module): return init(\n            module, gain=nn.init.calculate_gain('relu'))\n\n        self.conv = nn.Sequential(\n            init_(nn.Conv2d(3, 24, kernel_size=3, padding=1)),\n            nn.ReLU(),\n            init_(nn.Conv2d(24, 48, kernel_size=3)),\n            nn.ReLU(),\n            init_(nn.Conv2d(48, 48, kernel_size=3)),\n            nn.ReLU(),\n            Flatten(),\n            init_(nn.Linear(48 * 2 * 3, 80)),\n            nn.ReLU()\n        )\n\n        def init_(module): return init(module, gain=1.0)\n        # Critic\n        self.critic = init_(nn.Linear(80, 1))\n\n        def init_(module): return init(module, gain=0.01)\n        # Actor\n        self.actor = init_(nn.Linear(80, n_out))\n\n        self.train()\n\n    def forward(self, x):\n        conv_output = self.conv(x)\n        critic_output = self.critic(conv_output)\n        actor_output = self.actor(conv_output)\n\n        return critic_output, actor_output\n\n    def act(self, x):\n        value, actor_output = self(x)  \n\n        for i in range(x.size(0)):\n            for j in range(x.size(-1)):\n                if x[i][0][0][j] != 1:\n                    actor_output[i][j] = -1e7\n        \n        probs = F.softmax(actor_output, dim=1)\n        action = probs.multinomial(num_samples=1)\n\n        return action\n\n    def get_value(self, x):\n        value, actor_output = self(x)\n        return value\n\n    def evaluate_actions(self, x, actions):\n        value, actor_output = self(x)\n        \n        log_probs = F.log_softmax(actor_output, dim=1)\n        action_log_probs = log_probs.gather(1, actions)\n\n        probs = F.softmax(actor_output, dim=1)\n        dist_entropy = -(log_probs * probs).sum(-1).mean()\n\n        return value, action_log_probs, dist_entropy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Brain"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Brain(object):\n    def __init__(self, actor_critic):\n        self.actor_critic = actor_critic\n        self.optimizer = optim.RMSprop(\n            actor_critic.parameters(), lr=lr, eps=eps, alpha=alpha)\n\n    def update(self, rollouts):\n        obs_shape = rollouts.observations.size()[2:]\n        num_steps = NUM_ADVANCED_STEP\n        num_processes = NUM_PROCESSES\n\n        values, action_log_probs, dist_entropy = self.actor_critic.evaluate_actions(\n            rollouts.observations[:-1].view(-1, *obs_shape),\n            rollouts.actions.view(-1, 1))\n\n        values = values.view(num_steps, num_processes, 1)\n        action_log_probs = action_log_probs.view(num_steps, num_processes, 1)\n\n        advantages = rollouts.returns[:-1] - values\n        value_loss = advantages.pow(2).mean()\n\n        action_gain = (advantages.detach() * action_log_probs).mean()\n\n        total_loss = (value_loss * value_loss_coef -\n                      action_gain - dist_entropy * entropy_coef)\n\n        self.optimizer.zero_grad()\n        total_loss.backward()\n        nn.utils.clip_grad_norm_(self.actor_critic.parameters(), max_grad_norm)\n        self.optimizer.step()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Trainer(object):\n    def __init__(self, config):\n        self.config = config\n    \n    def checkmate(self, board, mark):    \n        columns = [c for c in range(self.config.columns) if board[c] == 0]\n        for mark in [mark, 3 - mark]:\n            for column in columns:\n                if is_win(board, column, mark, self.config, False):\n                    return column\n\n    def train(self):\n        seed_num = 1\n        torch.manual_seed(seed_num)\n        torch.cuda.manual_seed(seed_num)\n\n        torch.set_num_threads(seed_num)\n        envs = [make_env() for i in range(NUM_PROCESSES)]\n        envs = SubprocVecEnv(envs)\n\n        n_out = envs.action_space.n\n        actor_critic = Net(n_out).cuda()\n        global_brain = Brain(actor_critic)\n\n        obs_shape = envs.observation_space.shape\n        rollouts = RolloutStorage(\n            NUM_ADVANCED_STEP, NUM_PROCESSES, obs_shape)\n        episode_rewards = torch.zeros([NUM_PROCESSES, 1])\n        final_rewards = torch.zeros([NUM_PROCESSES, 1])\n\n        obs = envs.reset()\n        obs = torch.from_numpy(obs).float()\n        rollouts.observations[0].copy_(obs)\n        \n        complete_count = 0\n\n        for i in tqdm(range(NUM_UPDATES)):\n            for step in range(NUM_ADVANCED_STEP):\n                with torch.no_grad():\n                    action = actor_critic.act(rollouts.observations[step])\n\n                cpu_actions = action.squeeze(1).cpu().numpy()\n\n                for j in range(NUM_PROCESSES):\t\n                    board = rollouts.observations[step][j].cpu().numpy().argmax(axis=0).reshape(-1)\t\n                    forced_action = self.checkmate(board, 1)\n                    if forced_action is not None:\n                        cpu_actions[j] = forced_action\n\n                obs, reward, done, info = envs.step(cpu_actions)\n\n                reward = np.expand_dims(np.stack(reward), 1)\n                reward = torch.from_numpy(reward).float()\n                episode_rewards += reward\n\n                masks = torch.FloatTensor(\n                    [[0.0] if done_ else [1.0] for done_ in done])\n\n                final_rewards *= masks\n                final_rewards += (1 - masks) * episode_rewards\n\n                episode_rewards *= masks\n\n                masks = masks.cuda()\n\n                obs = torch.from_numpy(obs).float()\n                rollouts.insert(obs, action.data, reward, masks)\n\n            with torch.no_grad():\n                next_value = actor_critic.get_value(\n                    rollouts.observations[-1]).detach()\n\n            rollouts.compute_returns(next_value)\n\n            global_brain.update(rollouts)\n            rollouts.after_update()\n\n            if i % 125 == 0:\n                print(\"finished moves {}, mean/median reward {:.2f}/{:.2f}, min/max reward {:.2f}/{:.2f}\".\n                      format(i*NUM_PROCESSES*NUM_ADVANCED_STEP,\n                             final_rewards.mean(),\n                             final_rewards.median(),\n                             final_rewards.min(),\n                             final_rewards.max()))\n            \n            complete_count = complete_count + 1 if final_rewards.mean() >= 0.75 else 0\t            \t\n            if complete_count == 10:\t\n                print(\"finished training\")\t\n                break\n\n        torch.save(global_brain.actor_critic.state_dict(), 'weight.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config = make(\"connectx\").configuration\ntrainer = Trainer(config)\ntrainer.train()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create an Agent and Write Submission File"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile submission.py\n\nimport numpy as np\nimport io\nimport base64\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom kaggle_environments.envs.connectx.connectx import is_win\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\nclass Net(nn.Module):\n    def __init__(self, n_out):\n        super(Net, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(3, 24, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(24, 48, kernel_size=3),\n            nn.ReLU(),\n            nn.Conv2d(48, 48, kernel_size=3),\n            nn.ReLU(),\n            Flatten(),\n            nn.Linear(48 * 2 * 3, 80),\n            nn.ReLU()\n        )\n        self.critic = nn.Linear(80, 1)\n        self.actor = nn.Linear(80, n_out)\n\n    def forward(self, x):\n        conv_output = self.conv(x)\n        critic_output = self.critic(conv_output)\n        actor_output = self.actor(conv_output)\n\n        return critic_output, actor_output\n\n    def act(self, x):\n        value, actor_output = self(x)  \n\n        for i in range(x.size(0)):\n            for j in range(x.size(-1)):\n                if x[i][0][0][j] != 1:\n                    actor_output[i][j] = -1e7\n        \n        probs = F.softmax(actor_output, dim=1)\n        action = probs.multinomial(num_samples=1)\n\n        return action","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('weight.pth', 'rb') as f:\n    raw_bytes = f.read()\n    encoded_weights = base64.encodebytes(raw_bytes)\n\ntemplate = f\"\"\"\nactor_critic = Net({config.columns})\ndecoded = base64.b64decode({encoded_weights})\nbuffer = io.BytesIO(decoded)\nactor_critic.load_state_dict(torch.load(buffer, map_location='cpu'))\n\"\"\"\n\nwith open('submission.py', 'a') as f:\n    f.write(template)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile -a submission.py\n\ndef my_agent(obs, config):    \n    board = obs.board\n    columns = [c for c in range(config.columns) if board[c] == 0]\n    for mark in [obs.mark, 3 - obs.mark]:\n        for column in columns:\n            if is_win(board, column, mark, config, False):\n                return column\n    \n    if obs.mark == 2:\n        board = [3 - x if x != 0 else 0 for x in board]\n    board = np.array(board).reshape(config.rows, config.columns)\n    board = np.eye(3)[board].transpose(2, 0, 1)\n    board = torch.from_numpy(board).view([1, 3, config.rows, config.columns]).float()\n    \n    with torch.no_grad():\n        action = actor_critic.act(board)\n    action = action.item()\n\n    return action","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%run submission.py","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test the Agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = make(\"connectx\", debug=True)\nenv.reset()\nenv.run([my_agent, weighted_random])\nenv.render(mode=\"ipython\", width=500, height=450)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluate the Agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\n\n# Run multiple episodes to estimate its performance.\nprint(\"My Agent vs Weighted Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, weighted_random], num_episodes=100)))\nprint(\"Weighted Random Agent vs My Agent:\", mean_reward(evaluate(\"connectx\", [weighted_random, my_agent], num_episodes=100)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))\nprint(\"Negamax Agent vs My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", my_agent], num_episodes=10)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}