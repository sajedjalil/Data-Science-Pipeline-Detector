{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ConnectX - Implementing Functions in Pytorch\n\nAccording the the [Universal Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) a sufficently large neural network, with a sufficently large sample of input/outputs should be capable of approximating any mathematical function.\n\nMany of the usecases presented as Kaggle Competitions involve using neural networks to model functions that would be impossible to code classically by hand.\n\nThis notebook will cover the opposite usecase, that of trying to create a neural network implemention of a python function that we already have the sourcecode to, using reinforcement learning to (hopefully) achieve 100% accuracy, then to compare the performance of these two implementions."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Bitshifting Implemention\n\nThe example function we will be attempting to reverse-engineer is `is_gameover()` which was descibed in detail in my \n[Vectorized Bitshifting Tutorial](https://www.kaggle.com/jamesmcguigan/connectx-vectorized-bitshifting-tutorial/).\n\nThe function is a combination of two edgecases for detecting the endgame state of a Connect4 game\n1. Win/Loss  - One of the players has achieved 4-in-a-row\n2. Stalemate - All the squares on the board have been played, and there are no more valid actions left \n\nThis is the bitshifting code required to implement this function[](http://)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Source: https://github.com/JamesMcGuigan/ai-games/blob/master/games/connectx/core/ConnectXBBNN.py\n\nimport itertools\nimport random\nimport time\nimport numpy as np\nfrom numba import njit, int8, int64\nfrom collections import defaultdict, namedtuple\nfrom typing import Union, Tuple, List\n\n\n\nConfiguration = namedtuple('configuration', ['rows', 'columns', 'inarow', 'steps', 'timeout'])\nconfiguration = Configuration(**{'columns': 7, 'rows': 6, 'inarow': 4, 'steps': 1000, 'timeout': 8})\n\n\ndef is_gameover(bitboard: np.ndarray) -> bool:\n    if has_no_more_moves(bitboard):  return True\n    if get_winner(bitboard) != 0:    return True\n    return False\n\n\ndef has_no_more_moves(bitboard: np.ndarray) -> bool:\n    \"\"\"If all the squares on the top row have been played, then there are no more moves\"\"\"\n    return bitboard[0] & mask_legal_moves == mask_legal_moves \n\n\ndef get_winner(bitboard: np.ndarray) -> int:\n    \"\"\" Endgame get_winner: 0 for no get_winner, 1 = player 1, 2 = player 2\"\"\"\n    p2_wins = (bitboard[0] &  bitboard[1]) & gameovers[:] == gameovers[:]\n    if np.any(p2_wins): return 2\n    p1_wins = (bitboard[0] & ~bitboard[1]) & gameovers[:] == gameovers[:]\n    if np.any(p1_wins): return 1\n    return 0\n\n\ndef get_gameovers() -> np.ndarray:\n    \"\"\"Creates a list of all winning board positions, over 4 directions: horizontal, vertical and 2 diagonals\"\"\"\n    rows    = configuration.rows\n    columns = configuration.columns\n    inarow  = configuration.inarow\n\n    gameovers = []\n\n    mask_horizontal  = 0\n    mask_vertical    = 0\n    mask_diagonal_dl = 0\n    mask_diagonal_ul = 0\n    for n in range(inarow):  # use prange() with numba(parallel=True)\n        mask_horizontal  |= 1 << n\n        mask_vertical    |= 1 << n * columns\n        mask_diagonal_dl |= 1 << n * columns + n\n        mask_diagonal_ul |= 1 << n * columns + (inarow - 1 - n)\n\n    row_inner = rows    - inarow\n    col_inner = columns - inarow\n    for row in range(rows):         # use prange() with numba(parallel=True)\n        for col in range(columns):  # use prange() with numba(parallel=True)\n            offset = col + row * columns\n            if col <= col_inner:\n                gameovers.append( mask_horizontal << offset )\n            if row <= row_inner:\n                gameovers.append( mask_vertical << offset )\n            if col <= col_inner and row <= row_inner:\n                gameovers.append( mask_diagonal_dl << offset )\n                gameovers.append( mask_diagonal_ul << offset )\n    return gameovers\ngameovers = get_gameovers()\n\n\n\n### Utility Functions\n\ndef empty_bitboard() -> np.ndarray:\n    bitboard = np.array([0, 0], dtype=np.int64)\n    return bitboard\n\n\ndef is_bitboard(bitboard) -> bool:\n    return isinstance(bitboard, np.ndarray) and bitboard.dtype == np.int64 and bitboard.shape == (2,)\n\n\n@njit\ndef list_to_bitboard(listboard: Union[np.ndarray,List[int]]) -> np.ndarray:\n    # bitboard[0] = played, is a square filled             | 0 = empty, 1 = filled\n    # bitboard[1] = player, who's token is this, if filled | 0 = empty, 1 = filled\n    bitboard_played = 0  # 42 bit number for if board square has been played\n    bitboard_player = 0  # 42 bit number for player 0=p1 1=p2\n    if isinstance(listboard, np.ndarray): listboard = listboard.flatten()\n    for n in range(len(listboard)):  # prange\n        if listboard[n] != 0:\n            bitboard_played |= (1 << n)        # is a square filled (0 = empty | 1 = filled)\n            if listboard[n] == 2:\n                bitboard_player |= (1 << n)    # mark as player 2 square, else assume p1=0 as default\n    bitboard = np.array([bitboard_played, bitboard_player], dtype=np.int64)\n    return bitboard\n\n\n@njit(int8[:,:](int64[:]))\ndef bitboard_to_numpy2d(bitboard: np.ndarray) -> np.ndarray:\n    global configuration\n    rows    = configuration.rows\n    columns = configuration.columns\n    size    = rows * columns\n    output  = np.zeros((size,), dtype=np.int8)\n    for i in range(size):  # prange\n        is_played = (bitboard[0] >> i) & 1\n        if is_played:\n            player = (bitboard[1] >> i) & 1\n            output[i] = 1 if player == 0 else 2\n    return output.reshape((rows, columns))\n\n\n\n# Use string reverse to create mirror bit lookup table: mirror_bits[ 0100000 ] == 0000010\nmirror_bits = np.array([\n    int( \"\".join(reversed(f'{n:07b}')), 2 )\n    for n in range(2**configuration.columns)\n], dtype=np.int64)\n\n@njit\ndef mirror_bitstring( bitstring: int ) -> int:\n    \"\"\" Return the mirror view of the board for hashing:  0100000 -> 0000010 \"\"\"\n    global configuration\n\n    if bitstring == 0:\n        return 0  # short-circuit for empty board\n\n    bitsize     = configuration.columns * configuration.rows        # total number of bits to process\n    unit_size   = configuration.columns                             # size of each row in bits\n    unit_mask   = (1 << unit_size) - 1                              # == 0b1111111 | 0x7f\n    offsets     = np.arange(0, bitsize, unit_size, dtype=np.int64)  # == [ 0, 7, 14, 21, 28, 35 ]\n\n    # This can technically be done as a one liner:\n    output = np.sum( mirror_bits[ (bitstring & (unit_mask << offsets)) >> offsets ] << offsets )\n\n    return int(output)\n\n\n@njit\ndef mirror_bitboard( bitboard: np.ndarray ) -> np.ndarray:\n    return np.array([\n        mirror_bitstring(bitboard[0]),\n        mirror_bitstring(bitboard[1]),\n    ], dtype=bitboard.dtype)\n\n\n@njit\ndef get_bitcount_mask(size: int = configuration.columns * configuration.rows) -> np.ndarray:\n    # return np.array([1 << index for index in range(0, size)], dtype=np.int64)\n    return 1 << np.arange(0, size, dtype=np.int64)\nbitcount_mask = get_bitcount_mask()\n\n\n@njit\ndef get_move_number(bitboard: np.ndarray) -> int:\n    global configuration\n    if bitboard[0] == 0: return 0\n    size          = configuration.columns * configuration.rows\n    mask_bitcount = get_bitcount_mask(size)\n    move_number   = np.count_nonzero(bitboard[0] & mask_bitcount)\n    return move_number\n\n\n@njit\ndef current_player_id( bitboard: np.ndarray ) -> int:\n    \"\"\" Returns next player to move: 1 = p1, 2 = p2 \"\"\"\n    move_number = get_move_number(bitboard)\n    next_player = 1 if move_number % 2 == 0 else 2  # player 1 has the first move on an empty board\n    return next_player\n\n\n@njit\ndef next_player_id(player_id: int) -> int:\n    assert player_id in [1,2]\n    return 1 if player_id == 2 else 2\n\n\n@njit\ndef get_next_index(bitboard: np.ndarray, action: int) -> int:\n    global configuration\n    assert is_legal_move(bitboard, action)\n\n    # Start at the ground, and return first row that contains a 0\n    for row in range(configuration.rows-1, -1, -1):\n        index = action + (row * configuration.columns)\n        value = (bitboard[0] >> index) & 1\n        if value == 0:\n            return index\n    return action  # this should never happen - implies not is_legal_move(action)\n\n\n@njit\ndef result_action(bitboard: np.ndarray, action: int, player_id: int) -> np.ndarray:\n    assert is_legal_move(bitboard, action)\n    index    = get_next_index(bitboard, action)\n    mark     = 0 if player_id == 1 else 1\n    output = np.array([\n        bitboard[0] | 1    << index,\n        bitboard[1] | mark << index\n    ], dtype=bitboard.dtype)\n    return output\n\n\nmask_board       = (1 << configuration.columns * configuration.rows) - 1\nmask_legal_moves = (1 << configuration.columns) - 1        \n_is_legal_move_mask  = ((1 << configuration.columns) - 1)\n_is_legal_move_cache = np.array([ \n    [ int( (bits >> action) & 1 == 0 ) for action in range(configuration.columns) ]\n    for bits in range(2**configuration.columns)\n], dtype=np.int8)\n@njit\ndef is_legal_move(bitboard: np.ndarray, action: int) -> int:\n    bits = bitboard[0] & _is_legal_move_mask   # faster than: int( (bitboard[0] >> action) & 1 == 0 )\n    return _is_legal_move_cache[bits, action]  # NOTE: [bits,action] is faster than [bits][action]\n\n\n@njit\ndef get_random_move(bitboard: np.ndarray) -> int:\n    \"\"\" This is slightly quicker than random.choice(get_all_moves())\"\"\"\n    while True:\n        action = np.random.randint(0, configuration.columns)\n        if is_legal_move(bitboard, action):\n            return action","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Network Implemention\n\nGiven that we have a source code implemention of the function, we can generate supply of training examples. Lets see if we can train a neural network to learn this function.\n\nFirst we define a base class to handle the mechanics of loading and saving model files, as well as an interface for casting between bitboard and pytorch data formats.\n\nThis improves code cleanliness, as the neural network model configuration will be defined in a subclass, uncluttered by infrastructure logic."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy save files from previous notebook\n!cp -v ../input/*/*.pth ./","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\ndevice = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n__file__ = './undefined.py'\n\n\nclass BitboardNN(nn.Module):\n    \"\"\"Base class for bitboard based NNs, handles casting inputs and save/load functionality\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n    def cast(self, x):\n        if is_bitboard(x):\n            x = bitboard_to_numpy2d(x)\n        x = torch.from_numpy(x).to(torch.int64)  # int64 required for functional.one_hot()\n        x = F.one_hot(x, num_classes=self.one_hot_size)\n        x = x.to(torch.float32)                  # float32 required for self.fc1(x)\n        x = x.to(device)  \n        return x  # x.shape = (42,3)\n\n    def cast_to_labels(self, expected):\n        labels = torch.tensor([ expected ], dtype=torch.float).to(device)  # nn.MSELoss() == float | nn.CrossEntropyLoss() == long\n        return labels\n\n    def cast_from_outputs(self, outputs: torch.Tensor) -> bool:\n        \"\"\" convert (1,1) tensor back to bool \"\"\"\n        actual = bool( round( outputs.data.cpu().numpy().flatten()[0] ) )\n        return actual\n\n    @property\n    def filename(self):\n        return os.path.join( os.path.dirname(__file__), f'{self.__class__.__name__}.pth')\n\n    # DOCS: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n    def save(self):\n        torch.save(self.state_dict(), self.filename)\n\n    def load(self):\n        if os.path.exists(self.filename):\n            # Ignore errors caused by model size mismatch\n            try:\n                self.load_state_dict(torch.load(self.filename))\n                self.eval()\n            except: pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dense Layer Model\n\nThe first approach to try is a dense only model, with 3 hidden layers of 128 nodes, which outputs a single scalar."},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n# self.model_size = 128 | game:  100000 | move:  2130207 | loss: 0.137 | accuracy: 0.810 / 0.953 | time: 519s\n# self.model_size = 128 | game:  200000 | move:  2132342 | loss: 0.134 | accuracy: 0.834 / 0.953\n# self.model_size = 128 | game: 1000000 | move: 17053998 | loss: 0.099 | accuracy: 0.890 / 0.953 | time: 4274.1s\nclass IsGameoverSquareNN(BitboardNN):\n    def __init__(self):\n        super().__init__()\n        self.device       = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n        self.one_hot_size = 3\n        self.input_size   = configuration.rows * configuration.columns\n        self.output_size  = 1\n        self.model_size   = 128\n\n        self.fc1    = nn.Linear(self.input_size * self.one_hot_size, self.model_size)\n        self.fc2    = nn.Linear(self.model_size, self.model_size)\n        self.fc3    = nn.Linear(self.model_size, self.model_size)\n        self.output = nn.Linear(self.model_size, self.output_size)\n\n    def cast(self, x):\n        x = super().cast(x)\n        x = x.view(-1, self.input_size * self.one_hot_size)\n        return x\n\n    def forward(self, x):\n        x = self.cast(x)                   # x.shape = (1,126)\n        x = F.leaky_relu(self.fc1(x))      # x.shape = (1,256)\n        x = F.leaky_relu(self.fc2(x))      # x.shape = (1,256)\n        x = F.leaky_relu(self.fc3(x))      # x.shape = (1,256)\n        x = torch.sigmoid(self.output(x))  # x.shape = (1,1)\n        x = x.view(1)                      # x.shape = (1,)  | return 1d array of outputs, to match isGameoverCNN\n        return x\n\n\nisGameoverSquareNN = IsGameoverSquareNN()\nisGameoverSquareNN.to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CNN Model\n\nA second approach is to try a CNN architecture. Connect4 functions lend themselves naturally to a 4x4 sized CNN kernel"},{"metadata":{"trusted":true},"cell_type":"code","source":"# DOCS: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass IsGameoverCNN(BitboardNN):\n    def __init__(self):\n        super().__init__()\n        self.one_hot_size     = 3\n        self.input_size       = configuration.rows * configuration.columns * self.one_hot_size\n        self.output_size      = 1\n        self.cnn_channels     = (10 + 16)  # 4 vertical, 4 horizontal, 2 diagonal lines = 10 + 16 squares\n        self.cnn_kernel_size  = configuration.inarow\n        self.cnn_output_size  = (configuration.rows-self.cnn_kernel_size+1) * (configuration.columns-self.cnn_kernel_size+1) * self.cnn_channels\n        self.dense_layer_size = self.cnn_output_size // 2\n\n        self.conv1  = nn.Conv2d(self.one_hot_size, self.cnn_channels, self.cnn_kernel_size)\n        self.fc1    = nn.Linear(self.cnn_output_size,    self.cnn_output_size//2)\n        self.fc2    = nn.Linear(self.cnn_output_size//2, self.cnn_output_size//4)\n        self.fc3    = nn.Linear(self.cnn_output_size//4, self.cnn_output_size//8)\n        self.output = nn.Linear(self.cnn_output_size//8, self.output_size)\n\n\n    # DOCS: https://towardsdatascience.com/understanding-input-and-output-shapes-in-convolution-network-keras-f143923d56ca\n    # pytorch requires:    contiguous_format = (batch_size, channels, height, width)\n    # tensorflow requires: channels_last     = (batch_size, height, width, channels)\n    def cast(self, x):\n        x = super(IsGameoverCNN, self).cast(x)                                        # x.shape = (height, width, channels)\n        x = x.view(-1, configuration.rows, configuration.columns, self.one_hot_size)  # x.shape = (batch_size, height, width, channels)\n        x = x.permute(0, 3, 1, 2)                                                     # x.shape = (batch_size, channels, height, width)\n        return x\n\n\n    def forward(self, x):\n        x = self.cast(x)                    # x.shape = (1,3,6,7)\n        x = F.leaky_relu(self.conv1(x))     # x.shape = (1,26,3,4)\n        x = x.permute(0,2,3,1).reshape(-1)  # x.shape = (312,) + convert to columns_last (batch_size, height, width, channels)\n        x = F.leaky_relu(self.fc1(x))       # x.shape = (156,)\n        x = F.leaky_relu(self.fc2(x))       # x.shape = (78,)\n        x = F.leaky_relu(self.fc3(x))       # x.shape = (39,)\n        x = torch.sigmoid(self.output(x))   # x.shape = (1,)\n        return x\n\n\nisGameoverCNN = IsGameoverCNN()\nisGameoverCNN.to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Loop\n\nInput data (bitboard positions) can be generated by continually simulating a Connect4 games between two random agents. \n\nHowever one of the issues faced by reinforcement learning is the imbalanced nature of generated datasets. \nWe spend most of the time playing moves, and it is only at the very end do we get a single is_gameover() event. \nThen there is the issue of `has_no_more_moves()` == Draw edgecase, which happens only rarely when two random agents fight.\n\nThe raw statistical distribution of the generated dataset is:\n- not_gameover():      0.95305 (95% of the generated dataset)\n- is_gameover():       0.04680 (average game lasts 21.3 moves)\n- has_no_more_moves(): 0.00016 (1 in 6250 moves or 1 in 292.5 games)\n\nMy initial implemention generated a new datapoint for each iteration of the training loop. \nThis approach ran into the imbalanced dataset problem. Most of the inputs predicted a 0, and the network was unable to achieve greater than 95% accuracy, \nwhich would be the baseline accuracy of a model that always predicted `0 = not_gameover`.\n\nI borrowed a couple of ideas from the [RL Course by David Silver](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ) lectures.\n- Instead of generating the examples inside the training loop, it is possible to generate an batched dataset, then treat it as a supervised learning problem\n- RL training examples need not be use-once-and-throw-away, but can be reused to train the model for several iterations. This also reduces the \ncomputational burden of generating new datasets.\n\nBy creating a batched dataset, it was possible to normalize the distribution of generated data via undersampling. \nDataset size is 1000 bitboards, with 500 examples of not_gameover, and 250 each of the `is_gameover()` and `has_no_more_moves()` edgecases.\nGames of random_agent vs random_agent would be repeatedly run until there where enough examples of each of the edgecases to create a dataset.\n\n1.5 million bitboards must be generated to observe 250 `has_no_more_moves()` events. On my RazerBlade laptop with multithreading, it requires 80s (50μs/sample) of compute on my localhost laptop. Inside a Kaggle notebook, the same dataset generation is 9.5x slower (473μs/sample) which can be improved slightly with @njit to only 7.8x slower (391μs/sample). This would still data generation time of 10 minutes per epoch.\n\n\nFor the purposes of simplity, the neural network is trained and backpropergated on single values, rather than mini-batches. \nI am unsure how this effects training, accuracy and computational performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"#!/usr/bin/env python3\nimport itertools\nimport random\nimport time\nimport itertools\nfrom collections import defaultdict\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom joblib import delayed\nfrom joblib import Parallel\n\nDatasetItem = namedtuple('DatasetItem', ['expected', 'bitboard'])\ndef generate_dataset(dataset_size: int, bitboard_fn, verbose=False) -> Tuple[int, np.ndarray]:\n    \"\"\" Creates a statistically balanced dataset of all the edgecases \"\"\"\n    time_start   = time.perf_counter() \n    dataset_size = int(dataset_size/20)  # 20 = (5 + 5 + 10)\n    data = {\n        \"not_gameover\":      [],\n        \"is_gameover\":       [],\n        \"has_no_more_moves\": []\n    }\n    while min(map(len, data.values())) < dataset_size:\n        def generate(games=100):\n            data      = defaultdict(list)\n            bitboard  = empty_bitboard()\n            player_id = current_player_id(bitboard)\n            for n in range(games):\n                while not is_gameover(bitboard):\n                    action    = get_random_move(bitboard)\n                    bitboard  = result_action(bitboard, action, player_id)\n                    mirror    = mirror_bitboard(bitboard)  # mirror here for random sampling or mirrors and bitboards\n                    player_id = next_player_id(player_id)\n                    if   has_no_more_moves(bitboard): [ data['has_no_more_moves'].append(x) for x in [ bitboard, mirror ] ]\n                    elif is_gameover(bitboard):       [ data['is_gameover'      ].append(x) for x in [ bitboard, mirror ] ] \n                    else:                             [ data['not_gameover'     ].append(x) for x in [ bitboard, mirror ] ] \n            return data\n        \n        batched_datas = [ generate() ]  # for debugging\n        batched_datas = Parallel(n_jobs=os.cpu_count())([ delayed(generate)(100) for round in range(100) ])\n        for (key, value), batched_data in itertools.product(data.items(), batched_datas):\n            data[key] += batched_data[key]\n\n    # has_no_more_moves are very rare, so duplicate datapoints, mirror and then resample from the rest\n    bitboards = [\n        *random.sample(data['not_gameover']      * 10,  dataset_size * 10),\n        *random.sample(data['is_gameover']       *  5,  dataset_size *  5),\n        *random.sample(data['has_no_more_moves'] *  5,  dataset_size *  5),  \n    ]\n    np.random.shuffle(bitboards)\n    output = [ \n        DatasetItem(bitboard_fn(bitboard), bitboard) \n        for bitboard in bitboards\n    ]    \n    if verbose:\n        time_taken = time.perf_counter() - time_start\n        data_count = sum(map(len, data.values()))\n        statistics = {\n            \"time_taken\": f'{time_taken:.0f}s',\n            \"time_count\": f'{time_taken/data_count*1000000:.0f}μs',\n            \"count\": data_count,\n            **{ key: round( len(value)/data_count, 5) for key, value in data.items() }\n        }\n        print('dataset statistics: ', statistics)\n        # {'time_taken':  '84s', 'time_count':  '50μs', 'count': 1693800, 'not_gameover': 0.95312, 'is_gameover': 0.04673, 'has_no_more_moves': 0.00015}  # RazerBlade laptop\n        # {'time_taken':  '77s', 'time_count': '391μs', 'count':  197574, 'not_gameover': 0.95293, 'is_gameover': 0.04694, 'has_no_more_moves': 0.00013}  # Kaggle Notebook with    @njit - 10% dataset\n        # {'time_taken':  '67s', 'time_count': '473μs', 'count':  142134, 'not_gameover': 0.95286, 'is_gameover': 0.04696, 'has_no_more_moves': 0.00018}  # Kaggle Notebook without @njit - 10% dataset\n    return output\n\n\ndef train(model, criterion, optimizer, bitboard_fn=is_gameover, dataset_size=1000, timeout=4*60*60):\n    print(f'Training: {model.__class__.__name__}')\n    time_start = time.perf_counter()\n    epoch = 0\n    try:\n        model.load()\n        hist_accuracy = [0]\n\n        # dataset generation is expensive, so loop over each dataset until fully learnt\n        while np.min(hist_accuracy[-10:]) < 1.0:  # need multiple epochs of 100% accuracy to pass\n            if time.perf_counter() - time_start > timeout: break\n            epoch         += 1                    \n            epoch_start    = time.perf_counter()\n            bitboard_count = 0\n            \n            dataset_epoch = 0\n            dataset = generate_dataset(dataset_size, bitboard_fn, verbose=True)        \n            while hist_accuracy[-1] < 1.0:      # loop until 100% accuracy on dataset\n                if time.perf_counter() - time_start > timeout: break\n                if dataset_epoch > 100: break  # if we plataeu after many iterations, then generate a new dataset\n                    \n                dataset_epoch   += 1\n                running_accuracy = 0\n                running_loss     = 0.0\n                running_count    = 0\n\n                random.shuffle(dataset)\n                for (expected, bitboard) in dataset:\n                    assert isinstance(expected, bool)\n                    assert isinstance(bitboard, np.ndarray)\n                    \n                    bitboard_count += 1\n                    labels = model.cast_to_labels(expected)\n\n                    # zero the parameter gradients\n                    optimizer.zero_grad()\n\n                    outputs = model(bitboard)\n                    loss    = criterion(outputs, labels)\n                    loss.backward()\n                    optimizer.step()\n\n                    # Update running losses and accuracy\n                    actual            = model.cast_from_outputs(outputs)  # convert (1,1) tensor back to bool\n                    running_accuracy += int( actual == expected )\n                    running_loss     += loss.item()\n                    running_count    += 1\n\n                epoch_time       = time.perf_counter() - epoch_start                    \n                last_loss        = running_loss     / running_count\n                last_accuracy    = running_accuracy / running_count\n                hist_accuracy.append(last_accuracy)\n\n                # Print statistics after each epoch\n                if bitboard_count % 10000 == 0:\n                    print(f'epoch: {epoch:4d} | bitboards: {bitboard_count:5d} | loss: {last_loss:.5f} | accuracy: {last_accuracy:.5f} | time: {epoch_time :.0f}s')\n            print(f'epoch: {epoch:4d} | bitboards: {bitboard_count:5d} | loss: {last_loss:.5f} | accuracy: {last_accuracy:.5f} | time: {epoch_time :.0f}s')\n            model.save()\n\n                    \n    except KeyboardInterrupt:\n        pass\n    except Exception as exception:\n        print(exception)\n        raise exception\n    finally:\n        time_taken = time.perf_counter() - time_start\n        print(f'Finished Training: {model.__class__.__name__} - {epoch} epochs in {time_taken:.1f}s')\n        model.save()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the Models\n\n[Adadelta](https://ruder.io/optimizing-gradient-descent/index.html#adadelta) is chosen at the optimizer because it automatically adjusts the learning rate without \nthe additional complexity of an externally defined scheduler.\n\nOn Kaggle after 4 hours of training, regenerating each dataset after 100 epochs I was able to get:\n> Dense: | epoch:   13 | bitboards: 100000 | loss: 0.01836 | accuracy: 0.98032 | time: 1099s\n> CNN.   | epoch:   12 | bitboards: 100000 | loss: 0.01455 | accuracy: 0.98483 | time: 1148s\n\nSecond run created mirror bitboards, duplicated has_no_more_moves datapoints, and modified the training loop to keep training on each epoch until 100% accuracy on a given dataset. Accuracy plataeued at: \n- 0.98960 for isGameoverSquareNN (Dense Model)\n- 0.99470 for isGameoverCNN.\n\nThird run regenerates the dataset after 100 iterations to prevent a plateau, and also tests the effect of weight_decay. This article seems to suggest that weight_decay can be added later in the process to improve results, with 0.01 or 0.001 being optimal values (for their usecase). \n- https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/\n\nHowever for this usecase, weight_decay=0.001 reduces accuracy to:\n- 0.81190 for isGameoverSquareNN (Dense Model)\n- 0.83840 for isGameoverCNN\n\nFourth experiment is to switch from relu to leaky_relu. The shape of Relu is that the lines it draws can only see distance in one direction (as the other side is 0). Leaky Relu can see distance in both directions away from the line, so lets see if this improves the accuracy of the model. Also removing weight_decay here."},{"metadata":{"trusted":true},"cell_type":"code","source":"model     = isGameoverSquareNN\ncriterion = nn.MSELoss()  # NOTE: nn.CrossEntropyLoss() is for multi-output classification\noptimizer = optim.Adadelta(model.parameters())\ntrain(model, criterion, optimizer, bitboard_fn=is_gameover)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model     = isGameoverCNN\ncriterion = nn.MSELoss()  # NOTE: nn.CrossEntropyLoss() is for multi-output classification\noptimizer = optim.Adadelta(model.parameters())\ntrain(model, criterion, optimizer, bitboard_fn=is_gameover)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Further Reading\n\nThis notebook is part of series exploring the Neural Network implementions, which I have extended to the Game of Life Foward Problem\n- [Pytorch Game of Life - First Attempt](https://www.kaggle.com/jamesmcguigan/pytorch-game-of-life-first-attempt)\n- [Pytorch Game of Life - Hardcoding Network Weights](https://www.kaggle.com/jamesmcguigan/pytorch-game-of-life-hardcoding-network-weights)\n- [Its Easy for Neural Networks To Learn Game of Life](https://www.kaggle.com/jamesmcguigan/its-easy-for-neural-networks-to-learn-game-of-life)\n- [OuroborosLife - Function Reversal GAN](https://www.kaggle.com/jamesmcguigan/ouroboroslife-function-reversal-gan)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}