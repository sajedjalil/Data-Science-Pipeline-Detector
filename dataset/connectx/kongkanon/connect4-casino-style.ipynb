{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"text-align:center;\">Monte Carlo Methods for Connect4!</h1>\n<img src=\"https://i.ibb.co/sKnhdbw/casino.jpg\"  style=\"width:768px;height:512px;\">\n\nThis algorithm is extremely simple, yet pretty strong. It beats negamax everytime.\n\nThe core idea is to use Monte Carlo experiments to pick the best move. It works as follows:\n1. Check if there is a move yielding an instant win. If yes, win.\n2. Check if the opponent has a move yielding him an instant win. If yes, block it.\n3. Construct a pseudogame with the current board state of the game.\n4. Make a move in column 0 in the pseudogame.\n5. You have two random agents play each other from that board state, until they finish the pseudogame.\n6. Repeat step 3 to 5, but now for the other columns.\n7. Repeat 3 to 6 as many times as possible during the time limit.\n8. Pick whatever move got the most wins in the random games."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# ConnectX environment was defined in v0.1.6\n!pip install 'kaggle-environments>=0.1.6'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import evaluate, make, utils","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_agent(observation, configuration):\n    import time\n    start = time.time()\n    import numpy as np\n    prob = np.zeros(configuration.columns)\n    from random import choice\n    #The amount of time you have to make a move. More time -> Better performance.\n    time_limit = configuration.timeout-0.7\n    \n    def play(board, column, mark, config):\n        columns = config.columns\n        rows = config.rows\n        row = max([r for r in range(rows) if board[column + (r * columns)] == 0])\n        board[column + (row * columns)] = mark\n\n\n    def is_win(board, column, mark, config, has_played=True):\n        columns = config.columns\n        rows = config.rows\n        inarow = config.inarow - 1\n        row = (\n            min([r for r in range(rows) if board[column + (r * columns)] == mark])\n            if has_played\n            else max([r for r in range(rows) if board[column + (r * columns)] == 0])\n        )\n\n        def count(offset_row, offset_column):\n            for i in range(1, inarow + 1):\n                r = row + offset_row * i\n                c = column + offset_column * i\n                if (\n                        r < 0\n                        or r >= rows\n                        or c < 0\n                        or c >= columns\n                        or board[c + (r * columns)] != mark\n                ):\n                    return i - 1\n            return inarow\n\n        return (\n            count(1, 0) >= inarow  # vertical.\n            or (count(0, 1) + count(0, -1)) >= inarow  # horizontal.\n            or (count(-1, -1) + count(1, 1)) >= inarow  # top left diagonal.\n            or (count(-1, 1) + count(1, -1)) >= inarow  # top right diagonal.\n        )\n\n    def check_instant_win(_board, _mark, _configuration):\n        for column_choice in [c for c in range(_configuration.columns) if _board[c] == 0]:\n            won = is_win(_board, column_choice, _mark, _configuration, has_played=False)\n            if won:\n                return column_choice\n        return -1.0\n    \n    def check_instant_loss(_board, _mark, _configuration):\n        for column_choice in [c for c in range(_configuration.columns) if _board[c] == 0]:\n            opponent_mark = 1\n            if _mark == 1:\n                opponent_mark = 2\n            lost = is_win(_board, column_choice, opponent_mark, _configuration, has_played=False)\n            if lost:\n                return column_choice\n        return -1.0\n\n    def rand_agent(_board, _mark, _configuration):\n        from random import choice\n        options = [c for c in range(_configuration.columns) if _board[c] == 0]\n        if not options:\n            return -1\n        return choice(options)\n\n    win = check_instant_win(observation.board,observation.mark,configuration)\n    if win != -1.0:\n        return int(win)\n    loss = check_instant_loss(observation.board,observation.mark,configuration)\n    if loss != -1.0:\n        return int(loss)\n\n    end = time.time()\n    while end-start < time_limit:\n        for column_choice in [c for c in range(configuration.columns) if observation.board[c] == 0]:\n            dummy_board = observation.board.copy()\n            move = observation.mark\n            play(dummy_board, column_choice, move, configuration)\n            if move == 1:\n                move = 2\n            else:\n                move = 1\n            ongoing = True\n            while ongoing:\n                my_action = rand_agent(dummy_board,move, configuration)\n                if my_action == -1:\n                    winner = -1\n                    ongoing = False\n                    continue\n                play(dummy_board,my_action,move,configuration)\n                if is_win(dummy_board,my_action,move,configuration,has_played=True):\n                    winner = move\n                    ongoing = False\n                    continue\n                if move == 1:\n                    move = 2\n                else:\n                    move = 1\n            if observation.mark == winner:\n                prob[column_choice] = prob[column_choice] + 1.5\n            elif winner == -1:\n                prob[column_choice] = prob[column_choice] + 1.0\n            else:\n                prob[column_choice] = prob[column_choice] + 0.0\n        end = time.time()\n    best_move = int(np.argmax(prob))\n    if observation.board[best_move] == 0:\n        return best_move\n    else:\n        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\n\n# Run multiple episodes to estimate its performance.\nprint(\"Reward against negamax with negamax going first:\", 1-mean_reward(evaluate(\"connectx\", [\"negamax\", my_agent], num_episodes=10)))\nprint(\"Reward against negamax with MC method going first:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport inspect\n\nwith open('submission.py', 'w') as file:\n    file.write(inspect.getsource(my_agent))\n\n\nout = sys.stdout\nsubmission = utils.read_file('/kaggle/working/submission.py')\nagent = utils.get_last_callable(submission)\nsys.stdout = out\n\nenv = make('connectx', debug=True)\nenv.run([agent, agent])\nprint('Success!' if env.state[0].status == env.state[1].status == 'DONE' else 'Failed...')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}