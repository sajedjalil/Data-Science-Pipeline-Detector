{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Deep NeuroEvolution\n\nDeep NeuroEvolution is an interesting alternative for Training Deep Neural Networks for Reinforcement Learning, where the idea is replace the gradient based methods such a back-propagation by Genetic Algorithms. This script is based on the paper https://arxiv.org/pdf/1712.06567.pdf of the Uber AI Labs."},{"metadata":{},"cell_type":"markdown","source":"## Installing Libraries."},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install kaggle-environments","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import evaluate, make, utils\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nimport time\nimport datetime\nimport math\nimport copy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Deep NeuroEvolution Model implementation."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class ConnectX(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.fc = nn.Sequential(\n                        nn.Linear(42,256, bias=True),\n                        nn.Linear(256,7, bias=True),\n                        nn.Softmax(dim=1)\n                        )\n\n        def forward(self, inputs):\n            x = self.fc(inputs)\n            return x\n        \ndef init_weights(m):\n    if ((type(m) == nn.Linear) | (type(m) == nn.Conv2d)):\n        torch.nn.init.xavier_uniform(m.weight)\n        m.bias.data.fill_(0.00)            \n            \ndef return_random_agents(num_agents):\n    \n    agents = []\n    for _ in range(num_agents):\n        \n        agent = ConnectX()\n        \n        for param in agent.parameters():\n            param.requires_grad = False\n            \n        init_weights(agent)\n        agents.append(agent)\n    \n    return agents\n\ndef run_agents(agents,show_game=True):\n    \n    reward_agents = []\n    \n    env = make(\"connectx\", debug=True)\n\n    trainer = env.train([None, \"random\"])\n\n    for agent in agents:\n        agent.eval()\n\n        observation = trainer.reset()\n\n        r=0\n        \n        tini=datetime.datetime.now()\n\n        while True:\n\n            if(show_game):\n                trainer.render()\n\n            inp = torch.tensor(observation.board).type('torch.FloatTensor').view(1,-1)\n            output_probabilities = agent(inp).detach().numpy()[0]\n\n            # k = np.where(np.array(observation.board) == 0)[0]\n            candidates = np.array([0,1,2,3,4,5,6])\n\n            action = np.random.choice(candidates, 1, p=output_probabilities).item()\n            try: \n                observation, reward, done, info = trainer.step(action)\n\n                if(done):\n                    if reward == 1: # Won\n                        reward = 20\n                    elif reward == 0: # Lost\n                        reward = -20\n                    else: # Draw\n                        reward = 10    \n                    break\n                else:\n                    reward = 0.5\n                    r=r+reward\n            except:\n                pass        \n\n        reward_agents.append(r)    \n        \n    return reward_agents\n\ndef return_average_score(agent, runs,show_game=True):\n    score = 0.\n    for i in range(runs):\n        score_temp=run_agents([agent],show_game)[0]\n        score += score_temp\n        #print(\"Score for run\",i,\"has been\",score_temp)\n    return score/runs\n\ndef run_agents_n_times(agents, runs,show_game=True):\n    avg_score = []\n    for agent in agents:\n        avg_score.append(return_average_score(agent,runs,show_game))\n    return avg_score\n\ndef mutate(agent):\n\n    child_agent = copy.deepcopy(agent)\n    \n    mutation_power = 0.02 #hyper-parameter, set from https://arxiv.org/pdf/1712.06567.pdf\n    \n    for param in child_agent.parameters():\n        \n        if(len(param.shape)==4): #weights of Conv2D\n\n            for i0 in range(param.shape[0]):\n                for i1 in range(param.shape[1]):\n                    for i2 in range(param.shape[2]):\n                        for i3 in range(param.shape[3]):\n                            \n                            param[i0][i1][i2][i3]+= mutation_power * np.random.randn()\n                                \n                                    \n\n        elif(len(param.shape)==2): #weights of linear layer\n            for i0 in range(param.shape[0]):\n                for i1 in range(param.shape[1]):\n                    \n                    param[i0][i1]+= mutation_power * np.random.randn()\n                        \n\n        elif(len(param.shape)==1): #biases of linear layer or conv layer\n            for i0 in range(param.shape[0]):\n                \n                param[i0]+=mutation_power * np.random.randn()\n        \n\n\n    return child_agent\n\n\ndef cruce(mother_agent,father_agent):\n    \n    child_agent = copy.deepcopy(mother_agent)\n    \n    dim_father=[]\n    for j in range(len(list(father_agent.parameters()))):\n        dim_father.append(list(father_agent.parameters())[j].shape)\n\n\n    \n    for param in child_agent.parameters():\n        for j in range(len(dim_father)):\n            if(dim_father[j]==param.shape):\n                idx=j\n        \n        \n        if(len(param.shape)==4): #weights of Conv2D\n\n            for i0 in range(param.shape[0]):\n                for i1 in range(param.shape[1]):\n                    for i2 in range(param.shape[2]):\n                        for i3 in range(param.shape[3]):\n                            if np.random.uniform(0,1) <= 0.5:\n                                param[i0][i1][i2][i3]= list(father_agent.parameters())[idx][i0][i1][i2][i3]\n                                \n                                    \n\n        elif(len(param.shape)==2): #weights of linear layer\n            for i0 in range(param.shape[0]):\n                for i1 in range(param.shape[1]):\n                    if np.random.uniform(0,1) <= 0.5:\n                        param[i0][i1]= list(father_agent.parameters())[idx][i0][i1]\n\n                        \n\n        elif(len(param.shape)==1): #biases of linear layer or conv layer\n            for i0 in range(param.shape[0]):\n                if np.random.uniform(0,1) <= 0.5:\n                    param[i0]= list(father_agent.parameters())[idx][i0]\n        \n\n\n    return child_agent    \n\ndef return_children(agents, sorted_parent_indexes, elite_index):\n    \n    children_agents = []\n    \n    print(datetime.datetime.now(),\"Start: Crossing and Muting agents...\")\n    \n    max_idx=1\n    while max_idx<=len(sorted_parent_indexes):\n        \n        for i in range(max_idx):\n            children=cruce(mother_agent=agents[sorted_parent_indexes[i]],father_agent=agents[max_idx])\n            children_agents.append(mutate(children))\n            if(len(children_agents)>=(num_agents-1)):\n                break\n        \n        max_idx=max_idx+1\n           \n    \n    print(datetime.datetime.now(),\"End: Crossing and Muting agents...\")\n    \n    mutated_agents=[]\n   \n    #now add one elite\n    elite_child = add_elite(agents, sorted_parent_indexes, elite_index)\n    children_agents.append(elite_child)\n    elite_index=len(children_agents) -1 #it is the last one\n    \n    return children_agents, elite_index\n\n\n\ndef add_elite(agents, sorted_parent_indexes, elite_index=None, only_consider_top_n=10):\n    \n    candidate_elite_index = sorted_parent_indexes[:only_consider_top_n]\n    \n    if(elite_index is not None):\n        candidate_elite_index = np.append(candidate_elite_index,[elite_index])\n        \n    top_score = None\n    top_elite_index = None\n    \n    print(datetime.datetime.now(),\"Start: Playing candidates to elite...\")\n    for i in candidate_elite_index:\n\n        if(i%10==0):\n            show= False # True\n        else:\n            show=False\n        \n        score = return_average_score(agents[i],runs=3,show_game=show)\n        print(\"Score for elite i \", i, \" is on average\", score)\n        \n        if(top_score is None):\n            top_score = score\n            top_elite_index = i\n        elif(score > top_score):\n            top_score = score\n            top_elite_index = i\n    \n    print(datetime.datetime.now(),\"End: Playing candidates to elite...\")    \n    print(\"Elite selected with index \",top_elite_index, \" and average score\", top_score)\n    \n    child_agent = copy.deepcopy(agents[top_elite_index])\n    return child_agent\n\n\n\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    return np.exp(x) / np.sum(np.exp(x), axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Disable gradients since we won't use it."},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.set_grad_enabled(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Parameter's initialization."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_agents = 500\nagents = return_random_agents(num_agents)\ntop_limit = int((-1 + np.sqrt(1 + 4*2*num_agents))/2)+1 \ngenerations = 3 #1000\nelite_index = None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Agent's training"},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"for generation in range(generations):\n\n    # return rewards of agents\n    rewards = run_agents_n_times(agents, 1,show_game=False) #return average of 3 runs\n    \n    # sort by rewards\n    sorted_parent_indexes = np.argsort(rewards)[::-1][:top_limit]\n    \n    top_rewards = []\n    for best_parent in sorted_parent_indexes:\n        top_rewards.append(rewards[best_parent])\n    \n    print(\"Generation \", generation, \" | Mean rewards all players: \", np.mean(rewards), \" | Mean of top 5: \",np.mean(top_rewards[:5]))\n    print(\"Top \",top_limit,\" scores\", sorted_parent_indexes)\n    print(\"Rewards for top: \",top_rewards)\n    \n    # setup an empty list for containing children agents\n    children_agents, elite_index = return_children(agents, sorted_parent_indexes, elite_index)\n    print(\"elite index:\",elite_index)\n    # kill all agents, and replace them with their children\n    agents = children_agents","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating submission.\n\nThis part is thanks to @phunghieu on his Notebook https://www.kaggle.com/phunghieu/connectx-with-deep-q-learning."},{"metadata":{"trusted":true},"cell_type":"code","source":"layers = []\n\nfor param in agents[elite_index].parameters():\n    if(len(param.shape)==2):\n        weights = param.numpy().transpose().tolist()\n    if(len(param.shape)==1):\n        bias = param.numpy().tolist()\n\n        layers.extend([\n            weights,\n            bias\n        ])     \n\n# Convert all layers into usable form before integrating to final agent\nprecision = 7\n\nlayers = list(map(\n    lambda x: str(list(np.round(x, precision))) \\\n        .replace('array(', '').replace(')', '') \\\n        .replace(' ', '') \\\n        .replace('\\n', ''),\n    layers\n))\n\nlayers = np.reshape(layers, (-1, 2))\n\n# Create the agent\nmy_agent = '''def my_agent(observation, configuration):\n    import numpy as np\n\n'''\n\n# Write hidden layers\nfor i, (w, b) in enumerate(layers[:-1]):\n    my_agent += '    hl{}_w = np.array({}, dtype=np.float32)\\n'.format(i+1, w)\n    my_agent += '    hl{}_b = np.array({}, dtype=np.float32)\\n'.format(i+1, b)\n    \n# Write output layer\nmy_agent += '    ol_w = np.array({}, dtype=np.float32)\\n'.format(layers[-1][0])\nmy_agent += '    ol_b = np.array({}, dtype=np.float32)\\n'.format(layers[-1][1])\n\nmy_agent += '''\n    state = observation.board[:]\n    out = np.array(state, dtype=np.float32)\n\n'''\n\n# Calculate hidden layers\nfor i in range(len(layers[:-1])):\n    my_agent += '    out = np.matmul(out, hl{0}_w) + hl{0}_b\\n'.format(i+1)\n    \n# Calculate output layer\nmy_agent += '    out = np.matmul(out, ol_w) + ol_b\\n'\nmy_agent += '    out = np.exp(out) / np.sum(np.exp(out), axis=0)\\n'\n\nmy_agent += '''\n    for i in range(configuration.columns):\n        if observation.board[i] != 0:\n            out[i] = -1e7\n\n    return int(np.argmax(out))\n    '''\n\n\nwith open('submission_dn.py', 'w') as f:\n    f.write(my_agent)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing the agent."},{"metadata":{"trusted":true},"cell_type":"code","source":"from submission_dn import my_agent\n\ndef mean_reward(rewards):\n    return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\n\nprint(\"My Agent vs. Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs. Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))\nprint(\"Random Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"random\", my_agent], num_episodes=10)))\nprint(\"Negamax Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", my_agent], num_episodes=10)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}