{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Dueling double Deep Q-Network  \n  \nThis implementation I took from [marload repository](https://github.com/marload/DeepRL-TensorFlow2/blob/master/DuelingDoubleDQN/DuelingDoubleDQN_Discrete.py) and fit to ConnectX game."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install kaggle-environments","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A little change ConnectX environment"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from kaggle_environments import evaluate, make, utils\nfrom gym import spaces\nclass ConnectX:\n    DRAW = 0.5\n    WIN = 1\n    LOSE = -1\n    ERROR = -10 \n    \n    def __init__(self, pair=[None, \"random\"], config = {\"rows\": 6, \"columns\": 7, \"inarow\": 4}):\n        self.ks_env = make(\"connectx\", config, debug=True)\n        self.pair = pair\n        self.env = self.ks_env.train(pair)\n        self.config = config\n        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n        self.action_space = spaces.Discrete(config[\"columns\"])\n        self.observation_space = spaces.Box(low=0, high=2, \n                                            shape=(config[\"rows\"],config[\"columns\"],1), dtype=np.int)\n\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n        \n    def reset(self):\n        self.obs = self.env.reset()\n        self.obs = self.get_board(self.obs, self.config)\n        return self.obs\n    \n    def switch_pair(self):\n        self.pair = self.pair[::-1]\n        self.env = self.ks_env.train(self.pair)\n        \n    def change_pair(self, pair):\n        self.pair = pair\n        self.env = self.ks_env.train(self.pair)\n        \n    def change_reward(self, reward, done):\n        \n        if done:\n            if reward is None: #Error \n                reward = ConnectX.ERROR\n            elif reward == 1:\n                reward = ConnectX.WIN\n            elif reward == -1:\n                reward = ConnectX.LOSE\n            elif reward == 0:\n                reward = ConnectX.DRAW\n        else:\n            reward = -1/(self.config['rows'] * self.config['columns'])\n            \n        return reward\n    \n    # get board independent of player number\n    def get_board(self, observation, configuration):\n        rows = configuration['rows']\n        columns = configuration['columns']\n\n        board = np.array(observation['board']).reshape((rows,columns,1))\n        new_board = np.zeros_like(board)\n\n        mark = observation[\"mark\"]\n        new_board[board == mark] = 1\n        new_board[(board != mark) & (board != 0)] = 2\n        return new_board / 2 #normalization\n    \n    def step(self, action):\n        if not np.any(self.obs[:, action] == 0):\n            reward, done, _ = ConnectX.ERROR, True, {}\n        else:\n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            reward = self.change_reward(old_reward, done)\n            self.obs = self.get_board(self.obs, self.config)\n        \n        return self.obs, reward, done, _","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Add, Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization, LeakyReLU\nfrom tensorflow.keras.optimizers import Adam, SGD\n\nimport numpy as np\nfrom collections import deque\nimport random\n\ntf.keras.backend.set_floatx('float64')\n\nBATCH_SIZE = 64\nDISCOUNT = 0.99\n\nclass ReplayBuffer:\n    def __init__(self, capacity=10000):\n        self.buffer = deque(maxlen=capacity)\n    \n    def put(self, state, action, reward, next_state, done):\n        self.buffer.append([state, action, reward, next_state, done])\n    \n    def sample(self):\n        sample = random.sample(self.buffer, BATCH_SIZE)\n        states, actions, rewards, next_states, done = map(np.asarray, zip(*sample))\n        return states, actions, rewards, next_states, done\n    \n    def size(self):\n        return len(self.buffer)\n\nclass ActionStateModel:\n    \n    def __init__(self, state_dim, aciton_dim, epsilon = 1.0, eps_decay = 0.999, eps_min = 0.01, lr = 0.007):\n        self.state_dim  = state_dim\n        self.action_dim = aciton_dim\n        \n        self.epsilon = epsilon\n        self.eps_decay = eps_decay\n        self.eps_min = eps_min\n        self.lr = lr\n\n        self.model = self.create_model()\n        \n    # you can create your model, and experiment with it\n    def create_model(self):\n        state_input = Input(self.state_dim)\n        initializer = tf.initializers.VarianceScaling(scale=2.0)\n#         layers = [    \n#             Conv2D(64, 3, activation='relu', kernel_initializer=initializer, kernel_regularizer='l2'),\n#             BatchNormalization(),\n#             Conv2D(64, 2, activation='relu', kernel_regularizer='l2', kernel_initializer=initializer),\n#             BatchNormalization(),\n#             Flatten(),\n#             Dense(50, activation='relu', kernel_regularizer='l2', kernel_initializer=initializer),\n#             BatchNormalization(),\n#             Dropout(0.1),\n#             Dense(20, activation='relu', kernel_regularizer='l2', kernel_initializer=initializer),\n#         ]\n        \n        layers = [ \n            Conv2D(20, (5, 5), padding='same'),\n            LeakyReLU(alpha=0.3),\n            Conv2D(20, (4, 4), padding='same'),\n            LeakyReLU(alpha=0.3),\n            Conv2D(20, (4, 4), padding='same'),\n            LeakyReLU(alpha=0.3),\n            Conv2D(30, (4, 4), padding='same'),\n            LeakyReLU(alpha=0.3),\n            Conv2D(30, (4, 4), padding='same'),\n            LeakyReLU(alpha=0.3),\n            Conv2D(30, (4, 4), padding='same'),\n            LeakyReLU(alpha=0.3),\n            Flatten(),\n            Dense(50, activation='relu', kernel_regularizer='l2', kernel_initializer=initializer),\n            BatchNormalization(),\n            Dropout(0.1),\n            Dense(20, activation='relu', kernel_regularizer='l2', kernel_initializer=initializer),\n        ]        \n        \n        backbone = tf.keras.Sequential(layers)(state_input)\n        value_output = Dense(1)(backbone)\n        advantage_output = Dense(self.action_dim)(backbone)\n        output = Add()([value_output, advantage_output])\n        model = tf.keras.Model(state_input, output)\n        model.compile(loss='mse', optimizer=Adam(lr=self.lr))    \n        return model\n    \n    def predict(self, state):\n        return self.model.predict(state)\n    \n    def get_action(self, state):\n        state = np.reshape(state, [1, *self.state_dim])\n        self.epsilon *= self.eps_decay\n        self.epsilon = max(self.epsilon, self.eps_min)\n        q_value = self.predict(state)[0]\n        if np.random.random() < self.epsilon:\n            return random.randint(0, self.action_dim-1)\n        return np.argmax(q_value)\n\n    def train(self, states, targets):\n        self.model.fit(states, targets, epochs=1, verbose=0)\n\n        \nclass Agent:\n    def __init__(self, env):\n        self.env = env\n        self.state_dim = self.env.observation_space.shape\n        self.action_dim = self.env.action_space.n\n\n        self.model = ActionStateModel(self.state_dim, self.action_dim)\n        self.target_model = ActionStateModel(self.state_dim, self.action_dim)\n        self.target_update()\n\n        self.buffer = ReplayBuffer()\n\n    def target_update(self):\n        weights = self.model.model.get_weights()\n        self.target_model.model.set_weights(weights)\n    \n    def replay(self):\n        for _ in range(10):\n            states, actions, rewards, next_states, done = self.buffer.sample()\n            targets = self.target_model.predict(states)\n            next_q_values = self.target_model.predict(next_states)[range(BATCH_SIZE),np.argmax(self.model.predict(next_states), axis=1)]\n            targets[range(BATCH_SIZE), actions] = rewards + (1-done) * next_q_values * DISCOUNT \n            self.model.train(states, targets)\n\n    def _print_statistics(self, rewards):\n        rewards = np.array(rewards)\n        print(\"Wins:\", (rewards == ConnectX.WIN).sum())\n        print(\"Loses:\", (rewards == ConnectX.LOSE).sum())\n        print(\"Errors:\", (rewards == ConnectX.ERROR).sum())\n        \n    def train(self, episodes=1000, every = 100, switch = False):\n        rewards = []\n        total_rewards = []\n        for ep in range(episodes):\n            done, total_reward = False, 0\n            state = self.env.reset()\n            while not done:\n                action = self.model.get_action(state)\n                next_state, reward, done, _ = self.env.step(action)\n                \n                self.buffer.put(state, action, reward, next_state, done)\n                #put symmetrical state\n                self.buffer.put(state[:, ::-1, :], agent.action_dim - 1 - action, reward, next_state[:, ::-1, :], done)\n                total_reward += reward\n                state = next_state\n            \n            if self.buffer.size() >= BATCH_SIZE:\n                self.replay()\n            self.target_update()\n            \n            if (ep + 1) % 10 == 0:\n                print('EP {} EpisodeReward={}'.format(ep + 1, total_reward))\n            total_rewards.append(total_reward)\n            rewards.append(reward)\n            \n            if (ep + 1) % every == 0:\n                if switch:\n                    self.env.switch_pair()\n                self._print_statistics(rewards)\n                rewards = []\n        \n        return total_rewards\n\ndef plot(x, h = 100):\n    plt.plot(np.convolve(x, np.ones(h), 'valid')/h)\n    plt.xlabel('Episode')\n    plt.ylabel('Rewards')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here I experiment with only Connect 3.\nenv = ConnectX(pair = [\"random\", None ], config = {\"rows\": 4, \"columns\": 5, \"inarow\": 3})\nagent = Agent(env)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"total_rewards = agent.train(episodes = 3000, every = 100, switch = True)\nplot(total_rewards, 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot last 500 games\nplot(total_rewards[-500:], 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agent.model.epsilon = 1.0\nenv.change_pair([None, \"negamax\"])\ntotal_rewards = agent.train(episodes= 3000, every = 100, switch = True)\nplot(total_rewards, 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot last 500 games\nplot(total_rewards[-500:], 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Conclusion  \nAs you can see, dueling double DQN doesn't solve Connect 3 game. And I don't have idea why. I also tried usual DQN and PPO algorithm with different model architectures, but get the same result.  \n\nLeave comment and write, where I can have a mistake. \n\nMy other notebooks:  \n1) [Usual DQN](https://www.kaggle.com/masurte/deep-q-learning)   \n2) [Usual DQN, my own implementation](https://www.kaggle.com/masurte/deep-q-learning-implementation)  \n3) [PPO](https://www.kaggle.com/masurte/ppo-algorithm)  \n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}