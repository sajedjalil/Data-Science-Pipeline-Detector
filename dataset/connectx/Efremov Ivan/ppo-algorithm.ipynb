{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Proximal Policy Optimization algorithm \n  \nThis implementation I took from [marload repository](https://github.com/marload/DeepRL-TensorFlow2/blob/master/PPO/PPO_Discrete.py) and fit to ConnectX game."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install kaggle-environments","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A little change ConnectX environment"},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import evaluate, make, utils\nfrom gym import spaces\nclass ConnectX:\n    DRAW = 0.5\n    WIN = 1.0\n    LOSE = -1.0\n    ERROR = -10.0 \n    \n    def __init__(self, pair=[None, \"random\"], config = {\"rows\": 6, \"columns\": 7, \"inarow\": 4}):\n        self.ks_env = make(\"connectx\", config, debug=True)\n        self.pair = pair\n        self.env = self.ks_env.train(pair)\n        self.config = config\n        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n        self.action_space = spaces.Discrete(config[\"columns\"])\n        self.observation_space = spaces.Box(low=0, high=2, \n                                            shape=(config[\"rows\"],config[\"columns\"],1), dtype=np.int)\n\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n        \n    def reset(self):\n        self.obs = self.env.reset()\n        self.obs = self.get_board(self.obs, self.config)\n        return self.obs\n    \n    def switch_pair(self):\n        self.pair = self.pair[::-1]\n        self.env = self.ks_env.train(self.pair)\n        \n    def change_pair(self, pair):\n        self.pair = pair\n        self.env = self.ks_env.train(self.pair)\n        \n    def change_reward(self, reward, done):\n        \n        if done:\n            if reward is None: #Error \n                reward = ConnectX.ERROR\n            elif reward == 1:\n                reward = ConnectX.WIN\n            elif reward == -1:\n                reward = ConnectX.LOSE\n            elif reward == 0:\n                reward = ConnectX.DRAW\n        else:\n            reward = -1/(self.config['rows'] * self.config['columns'])\n            \n        return reward\n    \n    def get_board(self, observation, configuration):\n        rows = configuration['rows']\n        columns = configuration['columns']\n\n        board = np.array(observation['board']).reshape((rows,columns,1))\n        new_board = np.zeros_like(board)\n\n        mark = observation[\"mark\"]\n        new_board[board == mark] = 1\n        new_board[(board != mark) & (board != 0)] = 2\n        return new_board / 2 #normalization\n    \n    def step(self, action):\n        if not np.any(self.obs[:, action] == 0):\n            reward, done, _ = ConnectX.ERROR, True, {}\n        else:\n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            reward = self.change_reward(old_reward, done)\n            self.obs = self.get_board(self.obs, self.config)\n        \n        return self.obs, reward, done, _","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization\n\nimport numpy as np\n\ntf.keras.backend.set_floatx('float64')\n\nclass Actor:\n    def __init__(self, state_dim, action_dim, clip_ratio = 0.1, lr = 0.005):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.model = self.create_model()\n        self.opt = tf.keras.optimizers.Adam(lr)\n        \n        self.clip_ratio = clip_ratio\n\n    def create_model(self):\n        initializer = tf.initializers.VarianceScaling(scale=2.0)\n        return tf.keras.Sequential([\n            Input(self.state_dim),\n            Flatten(),\n            Dense(100, activation='relu', kernel_regularizer='l2', kernel_initializer=initializer),\n            BatchNormalization(),\n            Dropout(0.1),\n            Dense(20, activation='relu', kernel_regularizer='l2', kernel_initializer=initializer),\n            BatchNormalization(),\n            Dropout(0.1),\n            Dense(self.action_dim, activation='softmax')\n        ])\n\n    def compute_loss(self, old_policy, new_policy, actions, gaes):\n        gaes = tf.stop_gradient(gaes)\n        old_log_p = tf.math.log(\n            tf.reduce_sum(old_policy * actions))\n        old_log_p = tf.stop_gradient(old_log_p)\n        log_p = tf.math.log(tf.reduce_sum(\n            new_policy * actions))\n        ratio = tf.math.exp(log_p - old_log_p)\n        clipped_ratio = tf.clip_by_value(\n            ratio, 1 - self.clip_ratio, 1 + self.clip_ratio)\n        surrogate = -tf.minimum(ratio * gaes, clipped_ratio * gaes)\n        return tf.reduce_mean(surrogate)\n\n    def train(self, old_policy, states, actions, gaes):\n        actions = tf.one_hot(actions, self.action_dim)\n        actions = tf.reshape(actions, [-1, self.action_dim])\n        actions = tf.cast(actions, tf.float64)\n\n        with tf.GradientTape() as tape:\n            logits = self.model(states, training=True)\n            loss = self.compute_loss(old_policy, logits, actions, gaes)\n        grads = tape.gradient(loss, self.model.trainable_variables)\n        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n        return loss\n\n\nclass Critic:\n    def __init__(self, state_dim, lr = 0.005):\n        self.state_dim = state_dim\n        self.model = self.create_model()\n        self.opt = tf.keras.optimizers.Adam(lr)\n\n    def create_model(self):\n        initializer = tf.initializers.VarianceScaling(scale=2.0)\n        return tf.keras.Sequential([\n            Input(self.state_dim),\n            Flatten(),\n            Dense(100, activation='relu', kernel_regularizer='l2', kernel_initializer=initializer),\n            BatchNormalization(),\n            Dropout(0.1),\n            Dense(20, activation='relu', kernel_regularizer='l2', kernel_initializer=initializer),\n            BatchNormalization(),\n            Dropout(0.1),\n            Dense(1, activation='linear')\n        ])\n\n    def compute_loss(self, v_pred, td_targets):\n        mse = tf.keras.losses.MeanSquaredError()\n        return mse(td_targets, v_pred)\n\n    def train(self, states, td_targets):\n        with tf.GradientTape() as tape:\n            v_pred = self.model(states, training=True)\n            assert v_pred.shape == td_targets.shape\n            loss = self.compute_loss(v_pred, tf.stop_gradient(td_targets))\n        grads = tape.gradient(loss, self.model.trainable_variables)\n        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n        return loss\n\n\nclass Agent:\n    def __init__(self, env, discount = 0.99, lmbda = 0.95):\n        self.env = env\n        self.state_dim = self.env.observation_space.shape\n        self.action_dim = self.env.action_space.n\n\n        self.actor = Actor(self.state_dim, self.action_dim)\n        self.critic = Critic(self.state_dim)\n        \n        self.discount = discount\n        self.lmbda = lmbda\n\n    def gae_target(self, rewards, v_values, next_v_value, done):\n        n_step_targets = np.zeros_like(rewards)\n        gae = np.zeros_like(rewards)\n        gae_cumulative = 0\n        forward_val = 0\n\n        if not done:\n            forward_val = next_v_value\n\n        for k in reversed(range(0, len(rewards))):\n            delta = rewards[k] + self.discount * forward_val - v_values[k]\n            gae_cumulative = self.discount * self.lmbda * gae_cumulative + delta\n            gae[k] = gae_cumulative\n            forward_val = v_values[k]\n            n_step_targets[k] = gae[k] + v_values[k]\n        return gae, n_step_targets\n\n    def list_to_batch(self, list_):\n        batch = list_[0]\n        for elem in list_[1:]:\n            batch = np.append(batch, elem, axis=0)\n        return batch\n\n    def _print_statistics(self, rewards):\n        rewards = np.array(rewards)\n        print(\"Wins:\", (rewards == ConnectX.WIN).sum())\n        print(\"Loses:\", (rewards == ConnectX.LOSE).sum())\n        print(\"Errors:\", (rewards == ConnectX.ERROR).sum())\n        \n    def train(self, episodes=1000, every = 100, switch = False, update_interval = 2, epochs = 3):\n        results = []\n        total_rewards = []\n        \n        for ep in range(episodes):\n            state_batch = []\n            action_batch = []\n            reward_batch = []\n            old_policy_batch = []\n\n            episode_reward, done = 0, False\n            state = self.env.reset()\n\n            while not done:\n                probs = self.actor.model.predict(\n                    np.reshape(state, [1, *self.state_dim]))\n                action = np.random.choice(self.action_dim, p=probs[0])\n\n                next_state, reward, done, _ = self.env.step(action)\n\n                state = np.reshape(state, [1, *self.state_dim])\n                action = np.reshape(action, [1, 1])\n                next_state = np.reshape(next_state, [1, *self.state_dim])\n                reward = np.reshape(reward, [1, 1])\n\n                state_batch.append(state)\n                action_batch.append(action)\n                reward_batch.append(reward)\n                old_policy_batch.append(probs)\n\n                if len(state_batch) >= update_interval or done:\n                    states = self.list_to_batch(state_batch)\n                    actions = self.list_to_batch(action_batch)\n                    rewards = self.list_to_batch(reward_batch)\n                    old_policys = self.list_to_batch(old_policy_batch)\n\n                    v_values = self.critic.model.predict(states)\n                    next_v_value = self.critic.model.predict(next_state)\n\n                    gaes, td_targets = self.gae_target(\n                        rewards, v_values, next_v_value, done)\n\n                    for epoch in range(epochs):\n                        actor_loss = self.actor.train(\n                            old_policys, states, actions, gaes)\n                        critic_loss = self.critic.train(states, td_targets)\n\n                    state_batch = []\n                    action_batch = []\n                    reward_batch = []\n                    old_policy_batch = []\n\n                episode_reward += reward[0][0]\n                state = next_state[0]\n            \n            if (ep + 1) % 5 == 0:\n                print('EP {} EpisodeReward={}'.format(ep + 1, episode_reward))\n            \n            total_rewards.append(episode_reward)\n            results.append(reward[0][0])\n            \n            if (ep + 1) % every == 0:\n                if switch:\n                    self.env.switch_pair()\n                self._print_statistics(results)\n                results = []\n                \n        return total_rewards\n\ndef plot(x, h = 100):\n    plt.plot(np.convolve(x, np.ones(h), 'valid')/h)\n    plt.xlabel('Episode')\n    plt.ylabel('Rewards')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here I experiment with only Connect 3.\nenv = ConnectX(pair = [\"random\", None ], config = {\"rows\": 4, \"columns\": 5, \"inarow\": 3})\nagent = Agent(env)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_rewards = agent.train(episodes = 7000, every = 100, switch = True, update_interval = 2)\nplot(total_rewards, 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot last 500 games\nplot(total_rewards[-500:], 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.change_pair([None, \"negamax\"])\ntotal_rewards = agent.train(episodes= 7000, every = 100, switch = True, update_interval = 2)\nplot(total_rewards, 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot last 500 games\nplot(total_rewards[-500:], 30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion  \nAs you can see, dueling double DQN doesn't solve Connect 3 game. And I don't have idea why. I also tried dueling double DQN and PPO algorithm with different model architectures, but get the same result.  \n\nLeave comment and write, where I can have a mistake. \n\nMy other notebooks:  \n1) [Usual DQN](https://www.kaggle.com/masurte/deep-q-learning)   \n2) [Dueling double DQN](https://www.kaggle.com/masurte/dueling-double-dqn)    \n3) [Usual DQN, my own implementation](https://www.kaggle.com/masurte/deep-q-learning-implementation)   "},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}