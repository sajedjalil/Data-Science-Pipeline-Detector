{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is still work in progress , still trying to resolve the error , any help would be appreciated. \n\nCurrent Issue: Reward coming as none in the training loop.\n\n**Resolved the reward issues** - taking help from - https://www.kaggle.com/phunghieu/connectx-with-deep-q-learning\n\nAdded -\n        if done:\n            if reward == 1: # Won\n                reward = 20\n            elif reward == 0: # Lost\n                reward = -20\n            else: # Draw\n                reward = 10\n        else:\n            reward = -0.05"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install 'kaggle-environments>=0.1.6' > /dev/null 2>&1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport gym\nimport random\nimport matplotlib.pyplot as plt\nfrom random import choice\nfrom tqdm.notebook import tqdm\nfrom kaggle_environments import evaluate, make\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConnectX(gym.Env):\n    def __init__(self, switch_prob=0.5):\n        self.env = make('connectx', debug=True)\n        self.pair = [None, 'random']\n        self.trainer = self.env.train(self.pair)\n        self.switch_prob = switch_prob\n        \n        # Define required gym fields (examples):\n        config = self.env.configuration\n        self.action_space = gym.spaces.Discrete(config.columns)\n        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n\n    def switch_trainer(self):\n        self.pair = self.pair[::-1]\n        self.trainer = self.env.train(self.pair)\n\n    def step(self, action):\n        return self.trainer.step(action)\n    \n    def reset(self):\n        if random.uniform(0, 1) < self.switch_prob:\n            self.switch_trainer()\n        return self.trainer.reset()\n    \n    def render(self, **kwargs):\n        return self.env.render(**kwargs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = ConnectX()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch import optim\nimport torch.nn as nn\n# if gpu is to be used\nuse_cuda = torch.cuda.is_available()\n\ndevice = torch.device(\"cuda:0\" if use_cuda else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_value = 23\nenv.seed(seed_value)\ntorch.manual_seed(seed_value)\nrandom.seed(seed_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###### PARAMS ######\nlearning_rate = 0.01\nnum_episodes = 2000\ngamma = 0.85\n\negreedy = 0.9\negreedy_final = 0.02\negreedy_decay = 500\n\n####################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.observation_space.n , env.action_space.n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"number_of_inputs = env.observation_space.n + 1\nnumber_of_outputs = env.action_space.n\nhidden_layer = 64\nnumber_of_inputs , number_of_outputs , hidden_layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_epsilon(steps_done):\n    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n              math.exp(-1. * steps_done / egreedy_decay )\n    return epsilon","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NeauralNetwork(nn.Module):\n    def __init__(self):\n        super(NeauralNetwork , self).__init__()\n        self.l1 = nn.Linear(number_of_inputs,hidden_layer)\n        self.l2 = nn.Linear(hidden_layer,number_of_outputs)\n        self.activation = nn.ReLU()\n        \n    def forward(self , x):\n        output = self.activation(self.l1(x))\n        output = self.l2(output)\n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class QAgent(object):\n    def __init__(self):\n        self.nn = NeauralNetwork()\n        self.loss_func = nn.MSELoss()\n        self.optimizer = optim.Adam(params=self.nn.parameters() , lr=learning_rate)\n        \n    def select_action(self,state,epsilon):\n        random_for_egreedy = torch.rand(1)[0]\n        \n        if random_for_egreedy > epsilon:      \n            \n            with torch.no_grad():\n                \n                state = torch.Tensor(state).to(device)\n                action_from_nn = self.nn(state)\n                action = torch.max(action_from_nn,0)[1]\n                action = action.item()        \n        else:\n            action = env.action_space.sample()\n        \n        return action\n    \n    def optimize(self, state, action, new_state, reward, done):\n        state = torch.Tensor(state).to(device)\n        new_state = torch.Tensor(new_state).to(device)\n        #print(reward)\n        #print(action)\n        reward = torch.Tensor([reward]).to(device)\n        \n        if done:\n            target_value = reward\n        else:\n            new_state_values = self.nn(new_state).detach()\n            max_new_state_values = torch.max(new_state_values)\n            target_value = reward + gamma * max_new_state_values\n        #print(action)\n        #print(self.nn(state))\n        predicted_value = self.nn(state)[action]\n        \n        loss = self.loss_func(predicted_value , target_value)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nqnet_agent = QAgent()\n\nsteps_total = []\n\nframes_total = 0 \n\nfor i_episode in range(num_episodes):\n    print('No of episode:',i_episode)\n    state1 = env.reset()\n    state = state1.board[:]\n    state.append(state1.mark)\n    state = np.array(state, dtype=np.float32)\n    step = 0\n    #for step in range(100):\n    while True:\n        \n        step += 1\n        frames_total += 1\n        \n        epsilon = calculate_epsilon(frames_total)\n        \n        #action = env.action_space.sample()\n        action = qnet_agent.select_action(state, epsilon)\n        \n        new_state1, reward, done, info = env.step(action)\n        \n        if done:\n            if reward == 1: # Won\n                reward = 20\n            elif reward == 0: # Lost\n                reward = -20\n            else: # Draw\n                reward = 10\n        else:\n            reward = -0.05\n        \n        new_state = new_state1.board[:]\n        new_state.append(new_state1.mark)\n        new_state1 = np.array(new_state, dtype=np.float32)\n\n        qnet_agent.optimize(state, action, new_state, reward, done )\n        \n        state = new_state\n        \n        if done:\n            steps_total.append(step)\n            print(\"Episode finished after %i steps\" % step )\n            break\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Average reward: %.2f\" % (sum(steps_total)/num_episodes))\nprint(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n\nplt.figure(figsize=(12,5))\nplt.title(\"Rewards\")\nplt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color='green', width=5)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}