{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ConnectX with Minimax-DQN\nIn this notebook I show a simple extension of [DQN](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) to 2 player alternating-turn games known as Minimax-DQN. [Minimax-DQN](https://arxiv.org/abs/1901.00137) is based on [Minimax Q learning](https://www2.cs.duke.edu/courses/spring07/cps296.3/littman94markov.pdf) a paper from 1994. Minimax-DQN is the modern neural network equivalent to the tabular Q learning version.\n\n## DQN\nVanilla DQN isn't meant for the Multi-Agent Reinforcement Learning (MARL) setting, but we can use DQN here to learn against a fixed opponent and consider it as part of the environment. For example in [this notebook](https://www.kaggle.com/phunghieu/connectx-with-deep-q-learning/notebook), off of which my notebook was forked, DQN is used to learn the game by playing against a random opponent. Note that a random opponent is considered \"fixed\" in the sense that its policy does not change over time.\n\n## IQL\nTo learn in self play, one *can* simply pit 2 DQN agents against each other in the environment but this violates the fundamental assumption of Q learning that the environment is fixed. This is referred to as Independent Q learning (IQL) in this [paper](https://ora.ox.ac.uk/objects/uuid:2b650b3b-2fce-4875-b4df-70f4a4d64c8a/download_file?file_format=pdf&safe_filename=main.pdf&type_of_work=Conference+item) which discussed a way of making this technique work better in practice.\n\n## Minimax DQN\nIt is however possible to do a more principled extension of DQN. In DQN we have the bellman equation \n\n$Q(s,p)=r+\\gamma max_a Q(s',a)$\n\nwith $s$ the current state, $s'$ the next state, $p$ the action that lead from $s$ to $s'$, $r$ the immediate reward of going from state $s$ to $s'$ due to action $p$ and $\\gamma$ the discount factor of DQN.\n\nFor a 1v1 alternating-turn game we can formalise our problem as a zero-sum game, where 1 is a certain win, -1 is a certain loss and the bellman equation becomes\n\n$Q(s,p,player\\_0)=r-\\gamma max_a Q(s',a,player\\_1)$\n\nNote that this equation is very much akin to what is done in [Negamax](https://en.wikipedia.org/wiki/Negamax#Negamax_base_algorithm).\n\n## \"Nash-DQN\"\nFor your curiosity, Minimax-DQN can be generalised to 1v1 simultaneous-turn games by having the network output a matrix of Q values of size \\[N_actions,N_Actions\\] representing the value of each possible pairs of actions for both players. This can then be viewed as the payoff matrix of the game which can be solved with algorithms [such as this one](http://code.activestate.com/recipes/496825-game-theory-payoff-matrix-solver/). This again allows to transform the game back into a single agent form. More details in [this article](https://github.com/pb4git/Nash-DQN-CSB-Article) where me and a friend used this technique on another game.\n\n## Further improvements\n* The DQN network learned in this manner could be wrapped in a Negamax search, which would, in my experience, yield massive elo gains. The deep Q network can be used as the evaluation function required in a Negamax agent.\n* DQN can be improved with [Double DQN (DDQN)](https://arxiv.org/abs/1509.06461), [Prioritised Experience Replay](https://arxiv.org/abs/1511.05952), [Duelling DQN](https://arxiv.org/abs/1511.06581), [Noisy DQN](https://arxiv.org/abs/1706.10295) and the other improvements detailed in [the Rainbow paper](https://arxiv.org/abs/1710.02298).\n\nThis agent currently has a leaderboard score of ~1050 playing greedily on the Q values. The current \\#1 player [is also using this technique](https://www.kaggle.com/c/connectx/discussion/129145)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport gym\nimport os \nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom kaggle_environments import evaluate, make","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"Train = True\ngamma = 0.99\ncopy_step = 250\nhidden_units = [100, 200, 200, 100]\nmax_experiences = 1000000\nmin_experiences = 100000\nSteps_Till_Backprop = 64\nbatch_size = 512\nlr = 1e-3\nepsilon = 0.99\ndecay = 0.9999\nmin_epsilon = 0.1\nepisodes = 20000 #Set this to longer\nprecision = 5\nDiscard_Q_Value = -1e7\nMetric_Titles = ['Max_Q','Avg_Q','Min_Q']\nN_Downsampling_Episodes = 200","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OpenAI gym environment definition"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConnectX(gym.Env):\n    def __init__(self):\n        self.env = make('connectx', debug=False)\n\n        # Define required gym fields (examples):\n        config = self.env.configuration\n        self.action_space = gym.spaces.Discrete(config.columns)\n        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n\n    def step(self, actions):\n        return self.env.step(actions)\n\n    def reset(self):\n        return self.env.reset()\n    \n    def render(self, **kwargs):\n        return self.env.render(**kwargs)\n\n    def get_state(self):\n        return self.env.state\n\n    def game_over(self):\n        return self.env.done\n\n    def current_player(self):\n        active = -1\n        if self.env.state[0].status == \"ACTIVE\":\n            active=0\n        if self.env.state[1].status == \"ACTIVE\":\n            active=1\n        return active\n\n    def get_configuration(self):\n        return self.env.configuration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DeepModel(tf.keras.Model):\n    def __init__(self, num_states, hidden_units, num_actions):\n        super(DeepModel, self).__init__()\n        self.input_layer = tf.keras.layers.InputLayer(input_shape=(num_states,))\n        self.hidden_layers = []\n        for i in hidden_units:\n            self.hidden_layers.append(tf.keras.layers.Dense(i, activation='relu', kernel_initializer='he_normal'))\n        self.output_layer = tf.keras.layers.Dense(num_actions, activation='tanh', kernel_initializer='RandomNormal')\n\n    @tf.function\n    def call(self, inputs):\n        z = self.input_layer(inputs)\n        for layer in self.hidden_layers:\n            z = layer(z)\n        output = self.output_layer(z)\n        return output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note the tanh output layer which restricts output to \\[-1,1\\]"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DQN:\n    def __init__(self, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr):\n        self.num_actions = num_actions\n        self.batch_size = batch_size\n        self.optimizer = tf.keras.optimizers.Nadam(lr)\n        self.gamma = gamma\n        self.model = DeepModel(num_states, hidden_units, num_actions)\n        self.experience = {'inputs': [], 'a': [], 'r': [], 'inputs2': [], 'done': []} # The buffer\n        self.max_experiences = max_experiences\n        self.min_experiences = min_experiences\n\n    def predict(self, inputs):\n        return self.model(np.atleast_2d(inputs.astype('float32')))\n\n    #@tf.function\n    def train(self, TargetNet):\n        # Only start the training process when we have enough experiences in the buffer\n        if len(self.experience['inputs']) < self.min_experiences:\n            return 0\n\n        # Randomly select n experience in the buffer, n is batch-size\n        ids = np.random.randint(low=0, high=len(self.experience['inputs']), size=self.batch_size)\n        states = np.asarray([self.experience['inputs'][i] for i in ids])\n        actions = np.asarray([self.experience['a'][i] for i in ids])\n        rewards = np.asarray([self.experience['r'][i] for i in ids])\n\n        # Prepare labels for training process\n        states_next = np.asarray([self.experience['inputs2'][i] for i in ids])\n        dones = np.asarray([self.experience['done'][i] for i in ids])\n        \n        # Find the value of the next states by computing the max over valid actions in these next states\n        Move_Validity = states_next[:,:self.num_actions]==0\n        Next_Q_Values = TargetNet.predict(states_next)\n        Next_Q_Values = np.where(Move_Validity,Next_Q_Values,Discard_Q_Value)\n        value_next = -np.max(Next_Q_Values,axis=1)\n        \n        actual_values = np.where(dones, rewards, rewards+self.gamma*value_next)\n\n        with tf.GradientTape() as tape:\n            selected_action_values = tf.math.reduce_sum(self.predict(states) * tf.one_hot(actions, self.num_actions), axis=1)\n            loss = tf.math.reduce_sum(tf.square(actual_values - selected_action_values))\n        variables = self.model.trainable_variables\n        gradients = tape.gradient(loss, variables)\n        self.optimizer.apply_gradients(zip(gradients, variables))\n\n    # Get an action by using epsilon-greedy\n    def get_action(self, state, epsilon):\n        prediction = self.predict(np.atleast_2d(self.preprocess(state)))[0].numpy()\n        if np.random.random() < epsilon:\n            return int(np.random.choice([c for c in range(self.num_actions) if state.board[c] == 0])), prediction\n        else:\n            for i in range(self.num_actions):\n                if state.board[i] != 0:\n                    prediction[i] = Discard_Q_Value\n            return int(np.argmax(prediction)) , prediction\n\n    def add_experience(self, exp):\n        if len(self.experience['inputs']) >= self.max_experiences:\n            for key in self.experience.keys():\n                self.experience[key].pop(0)\n        for key, value in exp.items():\n            self.experience[key].append(value)\n\n    def copy_weights(self, TrainNet):\n        variables1 = self.model.trainable_variables\n        variables2 = TrainNet.model.trainable_variables\n        for v1, v2 in zip(variables1, variables2):\n            v1.assign(v2.numpy())\n\n    def save_weights(self, path):\n        self.model.save_weights(path)\n\n    def load_weights(self, path):\n        ref_model = tf.keras.Sequential()\n\n        ref_model.add(self.model.input_layer)\n        for layer in self.model.hidden_layers:\n            ref_model.add(layer)\n        ref_model.add(self.model.output_layer)\n\n        ref_model.load_weights(path)\n    \n    # Each state is represented as 1s for the current player's pieces, -1s for the opponent's pieces and 0s\n    def preprocess(self, state):\n        return np.array([1 if val==state.mark else 0 if val==0 else -1 for val in state.board])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note here that in the DQN::preprocess() method we represent the state for the current player. The current player (given by state.mark==1/2) will see his pieces as 1s and the opponent's pieces as -1s. Once he plays, the next player will see the state \"from his point of view\" where all *his* pieces are 1s. That way the Neural network does not make a distinction between playing player 0 or 1."},{"metadata":{},"cell_type":"markdown","source":"Define a function that will play 1 ConnectX game and learn from it"},{"metadata":{"trusted":true},"cell_type":"code","source":"def play_game(env, TrainNet, TargetNet, epsilon, copy_step, Global_Step_Counter):\n    turns = 0\n    env.reset()\n    Metric_Buffer={key:[] for key in Metric_Titles}\n    while not env.game_over():\n        active = env.current_player()\n\n        # Using epsilon-greedy to get an action\n        observations = env.get_state()[active].observation\n        action, Q_Values = TrainNet.get_action(observations, epsilon)\n        Q_Values = [val for val in Q_Values if val!=Discard_Q_Value]\n        Metric_Buffer['Avg_Q'].append(np.mean(Q_Values))\n        Metric_Buffer['Max_Q'].append(np.max(Q_Values))\n        Metric_Buffer['Min_Q'].append(np.min(Q_Values))\n\n        # Caching the information of current state\n        prev_observations = observations\n\n        # Take action\n        env.step([action if i==active else None for i in [0,1]])\n\n        reward=env.get_state()[active].reward\n\n        #Convert environment's [0,0.5,1] reward scheme to [-1,1]\n        if env.game_over():\n            if reward == 1: # Won\n                reward = 1\n            elif reward == 0: # Lost\n                reward = -1\n            else: # Draw\n                reward = 0\n        else:\n            reward = 0\n\n        next_active = 1 if active==0 else 0\n        # Adding experience into buffer\n        observations = env.get_state()[next_active].observation\n        exp = {'inputs': TrainNet.preprocess(prev_observations), 'a': action, 'r': reward, 'inputs2': TrainNet.preprocess(observations), 'done': env.game_over()}\n        TrainNet.add_experience(exp)\n\n        turns += 1\n        total_turns = Global_Step_Counter+turns\n        # Train the training model by using experiences in buffer and the target model\n        if total_turns%Steps_Till_Backprop==0:\n            TrainNet.train(TargetNet)\n        if total_turns%copy_step==0:\n            # Update the weights of the target model when reaching enough \"copy step\"\n            TargetNet.copy_weights(TrainNet)\n    results={key:[] for key in Metric_Titles}\n    for metric_name in Metric_Titles:\n        results[metric_name]=np.mean(Metric_Buffer[metric_name])\n    return results, turns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Start the training loop. Note here that we track some metrics (min/max/mean Q values). Although it may be hard to tell from these metrics what constitutes a good agent, tracking them can help you ascertain convergence."},{"metadata":{"trusted":true},"cell_type":"code","source":"env = ConnectX()\n\nnum_states = env.observation_space.n\nnum_actions = env.action_space.n\n\nMetrics = {key:[] for key in Metric_Titles} #Here we will store metrics for plotting after training\nMetrics_Buffer = {key:[] for key in Metric_Titles} #Downsampling buffer\n\n# Initialize models\nTrainNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\nTargetNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n\nif Train:\n    Global_Step_Counter=0\n    pbar = tqdm(range(episodes))\n    pbar2 = tqdm()\n    for n in pbar:\n        epsilon = max(min_epsilon, epsilon * decay)\n        results, steps = play_game(env, TrainNet, TargetNet, epsilon, copy_step, Global_Step_Counter)\n        Global_Step_Counter += steps\n        for metric_name in Metric_Titles:\n            Metrics_Buffer[metric_name].append(results[metric_name])\n\n        if Global_Step_Counter%N_Downsampling_Episodes==0:\n            for metric_name in Metric_Titles: #Downsample our metrics from the buffer\n                Metrics[metric_name].append(np.mean(Metrics_Buffer[metric_name]))\n                Metrics_Buffer[metric_name].clear()\n            pbar.set_postfix({\n                'Steps': Global_Step_Counter,\n                'Updates': Global_Step_Counter*batch_size/Steps_Till_Backprop\n            })\n\n            pbar2.set_postfix({\n                'max_Q': Metrics['Max_Q'][-1],\n                'avg_Q': Metrics['Avg_Q'][-1],\n                'min_Q': Metrics['Min_Q'][-1],\n                'epsilon': epsilon,\n                'turns': steps\n            })\n\n    def Plot(data,title):\n        plt.figure()\n        plt.plot(data)\n        plt.xlabel('Episode')\n        plt.ylabel(title)\n        plt.savefig(title+'.png')\n        plt.close()\n\n    for metric_name in Metric_Titles:\n        Plot(Metrics[metric_name],metric_name)\n\n    TrainNet.save_weights('./weights.h5')\nelse:\n    TrainNet.load_weights('./weights.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Write our agent into a submission.py file"},{"metadata":{"trusted":true},"cell_type":"code","source":"fc_layers = []\n\n# Get all hidden layers' weights\nfor i in range(len(hidden_units)):\n    fc_layers.extend([\n        TrainNet.model.hidden_layers[i].weights[0].numpy().tolist(), # weights\n        TrainNet.model.hidden_layers[i].weights[1].numpy().tolist() # bias\n    ])\n\n# Get output layer's weights\nfc_layers.extend([\n    TrainNet.model.output_layer.weights[0].numpy().tolist(), # weights\n    TrainNet.model.output_layer.weights[1].numpy().tolist() # bias\n])\n\n# Convert all layers into usable form before integrating to final agent\nfc_layers = list(map(\n    lambda x: str(list(np.round(x, precision))) \\\n        .replace('array(', '').replace(')', '') \\\n        .replace(' ', '') \\\n        .replace('\\n', ''),\n    fc_layers\n))\nfc_layers = np.reshape(fc_layers, (-1, 2))\n\n# Create the agent\nmy_agent = '''def my_agent(observation, configuration):\n    import numpy as np\n\n'''\n\n# Write hidden layers\nfor i, (w, b) in enumerate(fc_layers[:-1]):\n    my_agent += '    hl{}_w = np.array({}, dtype=np.float32)\\n'.format(i+1, w)\n    my_agent += '    hl{}_b = np.array({}, dtype=np.float32)\\n'.format(i+1, b)\n# Write output layer\nmy_agent += '    ol_w = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][0])\nmy_agent += '    ol_b = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][1])\n\nmy_agent += '''\n    board = observation.board[:]\n    out = np.array([1 if val==observation.mark else 0 if val==0 else -1 for val in board],np.float32)\n'''\n\n# Calculate hidden layers\nfor i in range(len(fc_layers[:-1])):\n    my_agent += '    out = np.matmul(out, hl{0}_w) + hl{0}_b\\n'.format(i+1)\n    my_agent += '    out = np.maximum(0,out)\\n' # Relu function\n# Calculate output layer\nmy_agent += '    out = np.matmul(out, ol_w) + ol_b\\n'\nmy_agent += '    out = np.tanh(out)\\n'\n\nmy_agent += '''\n    for i in range(configuration.columns):\n        if observation.board[i] != 0:\n            out[i] = -1e7\n\n    return int(np.argmax(out))\n    '''\n\nwith open('submission.py', 'w') as f:\n    f.write(my_agent)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test our agent against the builtin Negamax opponent"},{"metadata":{"trusted":true},"cell_type":"code","source":"from submission import my_agent\n\ndef epsilon_greedify(agent,epsilon=0.05): #Greedify our agent so we don't play the same games over and over in strength evaluation\n    def greedified_agent(observation,configuration):\n        import random\n        if random.random()<epsilon:\n            return random.choice([i for i in range(num_actions) if observation.board[i]==0])\n        else:\n            return agent(observation,configuration)\n    return greedified_agent\n\ndef mean_reward(rewards):\n    return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\n\nmy_agent = epsilon_greedify(my_agent)\nprint(\"My Agent vs. Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=20)))\nprint(\"Negamax Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", my_agent], num_episodes=20)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}