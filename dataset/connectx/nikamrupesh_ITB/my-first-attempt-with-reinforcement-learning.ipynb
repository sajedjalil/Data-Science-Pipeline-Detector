{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Reinforcement Learning\n![intro](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRQbPftOs4MgPHo3sI07NFflfjB0JwEJRTY3XX8yARgohl640fI)\n## Definition\nReinforcement Learning is the science of making optimal decisions using experience. It is categorized with supervised learning and unsupervised learning, rather than categorizing it with Machine Learning and Deep Learning. Reinforcement learning is the methodology that deals with the interaction between agent and environment  through actions and has got nothing to do with labeled and unlabeled data,  although there is another category called **Semi-supervised learning**, which is indeed a hybrid of supervised and unsupervised learning.<br><br>\nThe word \"reinforce\" means strengthen or support (an object or substance), especially with additional material.\n<br>\nBut what we are strengthening here, and what is our support which strengthen?<br>\nWe are trying to strengthen the learning ability of an **agent** to understand the environment. But that also happens in machine learning and deep learning, where the model is trained and the model learns a pattern from the trained data while minimizing the loss and improving the accuracy. The factor that strengthens the learning ability in reinforcement learning is **Reward**. A high positive reward is awarded to the agent for making a correct decision, and the agent should be penalized for making a wrong decision. The agent should get a slight negative reward for not making a correct decision after every time-step. \"Slight\" negative because we would prefer our agent to take more time in taking a decision rather than making the wrong decision."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Now, lets talk about some of the most important terms like agent, policy, states.\n<br>\nIn reinforcement learning an **Agent** is a self-learning model that learns some type of interaction between it and the environment. The agent wants to achieve some kind of **goal** within mentioned environment while it interacts with it. This interaction is divided into time steps. In each time step, **action** is performed by agent. This action changes the **state** of the environment and based on the success of it agent gets a certain **reward**. This way the agent learns what actions should be performed an which shouldn’t in a defined environment state. "},{"metadata":{},"cell_type":"markdown","source":"At each time step, the agent takes an action on the environment based on its policy $\\pi(a_t|s_t)$, where $s_t$ is the current observation from the environment, and receives a reward $r_{t+1}$ and the next observation $s_{t+1}$ from the environment. The goal is to improve the policy so as to maximize the sum of rewards (return). \n> a policy is an agent's strategy."},{"metadata":{},"cell_type":"markdown","source":"## Markov Decision Processes (MDPs)\nThis framework gives us a formal description of the problem in which an agent seeks for the best policy, defined as the function π. Policy maps states of the environment to the best action that the agent can take in certain environment state. Inside of the MDPs, we recognize several concepts, like a set of states – S, a set of actions – A, expected reward that agent will get going from one state to another – Ra(s, s’), etc. \n<br>\nMarkov Decision Process is tuple of four elements (S, A, Pa, Ra):\n* S – Represents the set of states. At each time step t, the agent gets the environment’s state – St, where St ∈ S.\n* A – Represents the set of actions that the agent can foretake. At each time step t, based on the received state St, the agent makes the decision to perform an action – At, where At ∈ A(St). A(St) represents a set of possible actions in the state St.\n* Pa – Represents the probability that action in some state s will result in the time step t, will result in the state s’ in the time step t+1.\n![](https://i0.wp.com/rubikscode.net/wp-content/uploads/2019/06/image-11.png)\n* Ra – Or more precisely Ra(s, s’), represents expected reward received after going from state s to the state s’, as a result of action a.\n\nwhere γ is the discount factor which determines how much importance we want to give to future rewards. In an essence, this value represents quality of an action."},{"metadata":{},"cell_type":"markdown","source":"## About this kernel\nIn this kernel I have discussed and implemented two reinforment learning techniques, both involves the use of neural networks. One of them is **Advantage actor-critic (A2C) agent** and another is **Deep Q-Learning**. "},{"metadata":{},"cell_type":"markdown","source":"## Some basic knowledge about the Competition.\nIn this game, your objective is to get a certain number of your checkers in a row horizontally, vertically, or diagonally on the game board before your opponent. When it's your turn, you “drop” one of your checkers into one of the columns at the top of the board. Then, let your opponent take their turn. This means each move may be trying to either win for you, or trying to stop your opponent from winning. The default number is four-in-a-row.\n![](https://storage.googleapis.com/kaggle-media/competitions/ConnectX/Walter's%20image.png)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport inspect\nimport random\nimport gym\nimport tensorflow as tf\nimport tensorflow.keras.layers as kl\nimport tensorflow.keras.losses as kls\nimport tensorflow.keras.optimizers as ko\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import backend as K\nfrom collections import deque\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#ConnectX environment was defined in v0.1.6\n!pip install 'kaggle-environments>=0.1.6'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import evaluate, make, utils\nenv = make(\"connectx\", debug=True)\nenv.render()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.agents","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.configuration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.specification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_agent(observation, configuration):\n    from random import choice\n    return choice([c for c in range(configuration.columns) if observation.board[c] == 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Play as first position against random agent.\ntrainer = env.train([None, \"random\"])\nobservation = trainer.reset()\n\nprint(\"Observation contains:\\t\", observation)\nprint(\"Configuration contains:\\t\", env.configuration)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_action = my_agent(observation, env.configuration)\nprint(\"My Action\", my_action)\nobservation, reward, done, info = trainer.step(my_action)\nenv.render(mode=\"ipython\", width=100, height=90, header=False, controls=False)\nprint(\"Observation after:\\t\", observation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train your agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer = env.train([None, \"random\"])\nobservation = trainer.reset()\nwhile not env.done:\n    my_action = my_agent(observation, env.configuration)\n    print(\"My Action\", my_action)\n    observation, reward, done, info = trainer.step(my_action)\n    print(reward)\nenv.render(mode=\"ipython\", width=100, height=90, header=False, controls=False)\nenv.render()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards)/sum(r[0]+r[1] for r in rewards)\n# Run multiple episodes to estimate its performance.\nprint(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=100)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=100)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Negamax algorithm\nprint(inspect.getsource(env.agents['negamax']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# random agent algorithm\nprint(inspect.getsource(env.agents['random']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Note\nThe code shown above this cell in this notebook above shows the configuration, specifications and agents provided by Kaggle themselves for this competitions."},{"metadata":{},"cell_type":"markdown","source":"## Declaring the environment "},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConnectX(gym.Env):\n    def __init__(self):\n        self.env = make(\"connectx\", debug=True)\n        self.pair = [None,\"negamax\"]\n        self.config = self.env.configuration\n        self.trainer = self.env.train(self.pair)\n        \n        # Define required gym fields (examples):\n        config = self.env.configuration\n        self.action_space = gym.spaces.Discrete(config.columns)\n        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n        \n    def step(self,action):\n        return self.trainer.step(action)\n    def reset(self):\n        return self.trainer.reset()\n    def render(self, **kwargs):\n        return self.env.render(**kwargs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Advantage actor-critic (A2C) agent\nTher are two different reenforcement learning method.<br>\n* **Value based method** - Q-learning, Deep Q-learning falls under this category. Here we learn a value that will map sate action pair to a value. These techniques reduce the error between predicted and actual state(-action) values. This works well when you have a finite set of actions.\n* **Policy based method** - Policy gardient directly optimize the policy by adjusting the parameters. This is useful when the action space is continuous or stochastic.\n<br>\nActor-critic method is a hybrid of both the above mentioned methods. We’ll using two neural networks:\n* a Critic that measures how good the action taken is (value-based)\n* an Actor that controls how our agent behaves (policy-based)\n<br>\n\npolicy gradients optimize agent’s policy, and the value-based/temporal-difference method is used as a bootstrap for the expected value estimates.\n<br>\nIn policy gradient method we are ina situation of Monte Carlo, waiting until the end of episode to calculate the reward. We may conclude that if we have a high reward (R(t)), all actions that we took were good, even if some were really bad.<br>\n> **What is Monte Carlo Simulation?**<br>\n> Monte Carlo simulation is a computerized mathematical technique that allows people to account for risk in quantitative analysis and decision making. Monte Carlo simulation performs risk analysis by building models of possible results by substituting a range of values—a probability distribution—for any factor that has inherent uncertainty. It then calculates results over and over, each time using a different set of random values from the probability functions. Depending upon the number of uncertainties and the ranges specified for them, a Monte Carlo simulation could involve thousands or tens of thousands of recalculations before it is complete. Monte Carlo simulation produces distributions of possible outcome values.\n\nIn Actor-Critic, instead of waiting until the end of the episode as we do in Monte Carlo REINFORCE, we make an update at each step. Because we do an update at each time step, we can’t use the total rewards R(t). Instead, we need to train a Critic model that approximates the value function.  This value function replaces the reward function in policy gradient that calculates the rewards only at the end of the episode."},{"metadata":{"trusted":true},"cell_type":"code","source":"class ProbabilityDistribution(tf.keras.Model):\n    def call(self, logits,  **kwargs):\n        return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(tf.keras.Model):\n    def __init__(self, env, num_actions):\n        super(Model, self).__init__('mlp_policy')\n        self.env = env\n        self.num_actions = num_actions\n        self.hidden1 = kl.Dense(128, activation='relu')\n        self.hidden2 = kl.Dense(128, activation='relu')\n        self.value = kl.Dense(1, name='value')\n        # Logits are unnormalized log probabilities.\n        self.logits = kl.Dense(num_actions,  name='policy_logits')\n        self.dist = ProbabilityDistribution()\n        self.action_ = None\n        self.value_ = None\n        self.space = None\n        self.empty = []\n        \n    def call(self, inputs, **kwargs):\n        # Inputs is a numpy array, convert to a tensor.\n        x = tf.convert_to_tensor(inputs)\n        # Separate hidden layers from the same input tensor.\n        hidden_logs = self.hidden1(x)\n        hidden_vals = self.hidden2(x)\n        return self.logits(hidden_logs), self.value(hidden_vals)\n    \n    def action_value(self, obs):\n        # Executes `call()` under the hood.\n        logits, values = self.predict_on_batch(obs)\n        action = self.dist.predict_on_batch(logits)\n        # Another way to sample actions:\n        #   action = tf.random.categorical(logits, 1)\n        # Will become clearer later why we don't use it.\n        \n        # The recursion shown below works absolutely fine but, \n        # while commit I am facing recursion error, so I have commented this out\n        # This recursion prevents invalid column problem.\n        \n        #self.action_,  self.value_ = np.squeeze(action, axis = -1), np.squeeze(values, axis=-1)\n        #self.space = [c for c in range(self.env.config.columns) if (obs[0][c] == 0)]\n        #if self.action_ not in self.space and self.space!=self.empty:\n        #    self.action_value(obs)\n        return np.squeeze(action, axis = -1), np.squeeze(values, axis=-1)\n    \n    def preprocess(self, state):\n        result = state.board[:]\n        result.append(state.mark)\n\n        return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = ConnectX()\nmodel = Model(env, num_actions=env.action_space.n)\nobs = env.reset()\nobs = np.array(model.preprocess(obs))\n# No feed_dict or tf.Session() needed at all!\naction, value = model.action_value(obs[None, :])\nprint(\"Action: \" +str(action)+\", Value: \" + str(value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"K.clear_session()\nclass Agent_Advanced:\n    def __init__(self, model, lr=7e-3, gamma=0.8, value_c=0.5, entropy_c=1e-4):\n        # Coefficients are used for the loss terms.\n        self.value_c = value_c\n        self.entropy_c = entropy_c\n        # `gamma` is the discount factor\n        self.gamma = gamma\n        self.model = model\n        self.model.compile(\n                          optimizer=tf.keras.optimizers.RMSprop(learning_rate=lr),\n                          # Define separate losses for policy logits and value estimate.\n                          loss=[self._logits_loss, self._value_loss]\n        )\n    def train(self, env, batch_sz=64, updates=500):\n        \n        # Training loop: collect samples, send to optimizer, repeat updates times.\n        ep_rewards = [0.0]\n        next_obs = env.reset()\n        next_obs = np.array(model.preprocess(next_obs))\n        # Storage helpers for a single batch of data.\n        actions = np.empty((batch_sz,), dtype=np.int32)\n        rewards, dones, values = np.zeros((3, batch_sz,))\n        \n        observations = np.empty((batch_sz,len(next_obs.copy())) + env.observation_space.shape)\n        for update in range(updates):\n            for step in range(batch_sz):\n                observations[step] = next_obs.copy()\n                #print(observations[step])\n                actions[step], values[step] = self.model.action_value(next_obs[None, :])\n                next_obs, rewards[step], dones[step], _ = env.step(int(actions[step]))\n                \n                #values[step] = np.where(dones[step], rewards[step], rewards[step]+self.gamma*values[step])\n                \n                if rewards[step] >= 0.5: # Won\n                    rewards[step] = 20\n                elif rewards[step] == 0.0: # Lost\n                    rewards[step] = -20\n                else: # Draw\n                    rewards[step] = 0.05\n                ep_rewards[-1] += rewards[step]    \n                \n                next_obs = np.array(model.preprocess(next_obs))    \n                #print(rewards[step])\n                if dones[step]:\n                    ep_rewards.append(0.0)\n                    next_obs = env.reset()\n                    next_obs = np.array(model.preprocess(next_obs))\n                    print(\"Episode: %03d, Reward: %03d\" % (len(ep_rewards) - 1, ep_rewards[-2]))\n                    \n                    \n                \n            _, next_value = self.model.action_value(next_obs[None, :])\n            #next_value= np.where(dones, rewards, rewards+self.gamma*values)\n            returns, advs = self._returns_advantages(rewards, dones, values, next_value)\n            # To input actions and advantages through same API.\n            acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n            # Performs a full training step on the collected batch\n            losses = self.model.fit(observations, [acts_and_advs, returns])\n            \n            print(\"[%d/%d] Losses: %s\" % (update + 1, updates, losses.history['loss']))\n\n        return ep_rewards\n    \n    def _returns_advantages(self, rewards, done, values, next_value):\n        # `next_value` is the bootstrap value estimate of the future state (critic).\n        returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n        \n        # Returns are calculated as discounted sum of future rewards.\n        for t in reversed(range(rewards.shape[0])):\n            returns[t] = rewards[t] + self.gamma * returns[t+1] * (1 - done[t])\n        returns = returns[:-1]\n        # Advantages are equal to returns - baseline (value estimates in our case).\n        advantages = returns - values\n        \n        return returns, advantages\n        \n    def _value_loss(self, return_, value):\n        # Value loss is typically MSE between value estimates and returns.\n        return self.value_c * kls.mean_squared_error(return_, value)\n    \n    def _logits_loss(self, actions_and_advantages, logits):\n        actions, advantages = tf.split(actions_and_advantages, 2, axis=-1)\n        # Sparse categorical CE loss obj that supports sample_weight arg on `call()`.\n        # `from_logits` argument ensures transformation into normalized probabilities.\n        weighted_sparse_ce = kls.SparseCategoricalCrossentropy(from_logits=True)\n        # Policy loss is defined by policy gradients, weighted by advantages.\n        # Note: we only calculate the loss on the actions we've actually taken.\n        policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n        \n        # Entropy loss can be calculated as cross-entropy over itself.\n        probs = tf.nn.softmax(logits)\n        entropy_loss = kls.categorical_crossentropy(probs, probs)\n        # We want to minimize policy and maximize entropy losses.\n        # Here signs are flipped because the optimizer minimizes.\n        return policy_loss - self.entropy_c * entropy_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = ConnectX()\nmodel = Model(env, num_actions=env.action_space.n)\nmodel.run_eagerly = True\nprint(\"Eager Execution:  \", tf.executing_eagerly())\nprint(\"Eager Keras Model:\", model.run_eagerly)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"agent = Agent_Advanced(model)\nrewards_history = agent.train(env)\nprint(\"Finished training, testing....\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[20,10])\nplt.plot(rewards_history)\nplt.xlabel('Episode')\nplt.ylabel('Avg rewards ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Q-Learning\nIn the cells above I talked about the term Q-learning and Deep Q-Learning, but are these algorithms? let us learn about them and try to implement them....<br>\nQ-learning lets the agent use the environment's rewards to learn, over time, the best action to take in a given state. A **Q-value** for a particular state-action combination is representative of the \"quality\" of an action taken from that state. Better Q-values imply better chances of getting greater rewards.<br>\nQ-values are initialized to an arbitrary value, and as the agent exposes itself to the environment and receives different rewards by executing different actions, the Q-values are updated using the equation:\n> Q(state,action)←(1−α)Q(state,action)+α(reward+γmaxaQ(next state,all actions))\n\n* α (alpha) is the learning rate (0<α≤1) - Just like in supervised learning settings, α is the extent to which our Q-values are being updated in every iteration.\n* γ (gamma) is the discount factor (0≤γ≤1) - determines how much importance we want to give to future rewards. A high value for the discount factor (close to 1) captures the long-term effective award, whereas, a discount factor of 0 makes our agent consider only immediate reward, hence making it greedy.\n<br>\nWe are updating, the Q-value of the agent's current state and action by first taking a weight (1−α) of the old Q-value, then adding the learned value. The learned value is a combination of the reward for taking the current action in the current state, and the discounted maximum reward from the next state we will be in once we take the current action.\n<br>\nmaxaQ(next state,all actions), means that Q-value of the current step is based on the Q-value of the future step. This means that we initialize Q-Values for St and St+1 to some random values at first. In the first training iteration we update Q-Value in the state St based on reward and on those random value of Q-Value in the state St+1. Since reward is still guiding our system this will eventually converge to the best result. we are learning the proper action to take in the current state by looking at the reward for the current state/action combo, and the max rewards for the next state.\n<br>\nAll these Q-Values are stored inside of the **Q-Table**, which is just the matrix with the rows for states and the columns for actions.\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/Q-Learning_Matrix_Initialized_and_After_Training.png/440px-Q-Learning_Matrix_Initialized_and_After_Training.png)"},{"metadata":{},"cell_type":"markdown","source":"## about implementation of q-learning\nI have not implemented Q-learning, but here is a kenel I would definitely suggest <br>\n[ConnectX with Q-Learning](https://www.kaggle.com/phunghieu/connectx-with-q-learning) by [Hieu Phung](https://www.kaggle.com/phunghieu). If u are interested then please go through this kernel."},{"metadata":{},"cell_type":"markdown","source":"## Deep Q-Learning\nDeep Q-Learning harness the power of deep learning with so-called Deep Q-Networks. These are standard feed forward neural networks which are utilized for calculating Q-Value. In this case, the agent has to store previous experiences in a local memory and use max output of neural networks to get new Q-Value. <br>\nThe important thing to notice here is that Deep Q-Networks don’t use standard supervised learning, simply because we don’t have labeled expected output. We depend on the policy or value functions in reinforcement learning, so the target is continuously changing with each iteration. Because of this reason the agent doesn’t use just one neural network, but two of them. So, how does this all fit together? The first network, called Q-Network is calculating Q-Value in the state St, while the other network, called Target Network is calculating Q-Value in the state St+1. \n<br>\n![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/04/Screenshot-2019-04-17-at-12.48.05-PM-768x638.png)"},{"metadata":{},"cell_type":"markdown","source":"In simple words Deep Q-learning is a combination of deep learning and Q-learning where the q-table is replaced by a deep neural network.\n![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/04/Screenshot-2019-04-16-at-5.46.01-PM.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"K.clear_session()\nclass Agent_deepQ:\n    def __init__(self, enviroment, optimizer):\n        \n        # Initialize atributes\n        self.environment = environment\n        self._state_size = enviroment.observation_space.n\n        self._action_size = enviroment.action_space.n\n        self._optimizer = optimizer\n        self.space = None\n        self.empty = []\n        self.action_ = None\n\n        self.expirience_replay = deque(maxlen=2000)\n\n        # Initialize discount and exploration rate\n        self.gamma = 0.7\n        self.epsilon = 0.1\n\n        # Build networks\n        self.q_network = self._build_compile_model()\n        self.target_network = self._build_compile_model()\n        self.alighn_target_model()\n        \n    def _build_compile_model(self):\n        model = tf.keras.Sequential()\n        model.add(kl.Embedding(self._state_size, 100, input_length=1))\n        model.add(kl.Reshape((100,)))\n        model.add(kl.Dense(256, activation='relu'))\n        model.add(kl.Dense(128, activation='relu'))\n        model.add(kl.Dense(self._action_size, activation='linear'))\n        \n        model.compile(loss='mse', optimizer=self._optimizer)\n        return model\n    def alighn_target_model(self):\n        self.target_network.set_weights(self.q_network.get_weights())\n        \n    def store(self, state, action, reward, next_state, terminated):\n        self.expirience_replay.append((state, action, reward, next_state, terminated))\n    \n    def act(self, state):\n        if np.random.rand() <= self.epsilon:\n            self.action_ = int(np.random.choice([c for c in range(environment.config.columns) if state[c] == 0]))\n        \n        else:\n            q_values = self.q_network.predict(state)        \n            self.action_ = int(np.argmax(q_values[0]))\n        self.space = [c for c in range(self.environment.config.columns) if (state[c] == 0)]\n        if self.action_ not in self.space and self.space!=self.empty:\n            self.act(state)\n        return self.action_\n    \n    def preprocess(self, state):\n        result = state.board[:]\n        result.append(state.mark)\n        return result\n    \n    def train(self, batch_size):\n        minibatch = random.sample(self.expirience_replay, batch_size)\n        for state, action, reward, next_state, terminated in minibatch:\n            target = self.q_network.predict(state)\n            if terminated:\n                target[0][action] = reward\n            else:\n                next_state = self.preprocess(next_state)\n                t = self.target_network.predict(next_state)\n                target[0][action] = reward + self.gamma*np.amax(t)\n                self.q_network.fit(np.array(state), np.array(target), epochs=1, verbose=0)\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"environment = ConnectX()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\nagent = Agent_deepQ(environment, optimizer)\n\nbatch_size = 32\nnum_of_episodes = 800\ntimesteps_per_episode = 500\nagent.q_network.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"reward_ = 0\ntotal_reward = []\nfor e in range(num_of_episodes):\n    # Reset the enviroment\n    state = environment.reset()    \n    terminated=False\n    for time_step in range(timesteps_per_episode):\n        state = agent.preprocess(state)\n        # Run Action\n        action = agent.act(state)\n        # Take action \n        next_state, reward, terminated, info = environment.step(action)\n        agent.store(state, action, reward, next_state, terminated)\n        state = next_state\n        reward_+=reward\n        if terminated:\n            agent.alighn_target_model()\n            total_reward.append(reward_)\n            reward_ = 0\n            break\n            \n        if len(agent.expirience_replay) > batch_size:\n            agent.train(batch_size)\n    if (e + 1) % 10 == 0:\n        print(\"**********************************\")\n        print(\"Episode: {}\".format(e + 1))\n        environment.render()\n        print(\"**********************************\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[20,10])\nplt.plot(total_reward)\nplt.xlabel('Episode')\nplt.ylabel('Avg rewards')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## My personal confession\nThis my first time, dealing with reinforcement learning. I dont even know how much this kernel would be helpful to others. I am very much thankful to kaggle for organizing this competition and creating an opportunity to learn about this beautiful piece of A.I. \n<br>\nI went through various websites and Notebooks related to this competition, learned from all of them and tried to give a compilation of all of them. It was fun to learn re-enforcement learning, and would definitely suggest other people to go through this topic even though you might say that your topics of interest are only machine learning and deep learning. I might be wrong about the concept or wrong from the implementation point of view. So do feel free to correct me in the comment section below.\n<br>\nIn the cell below I have mentioned the resources that have helped me through this journey, so please go through them for a more detailed explanation.\n<br><br>\n**Thank you**"},{"metadata":{},"cell_type":"markdown","source":"## Future Scopes\nI tried to solve this probem with tensorflow-agent but I failed due to some errors while training. I would suggest you all to use Tensorflow-agent to solve this problem.\n\nlink to tensorflow-agent - https://github.com/tensorflow/agents"},{"metadata":{},"cell_type":"markdown","source":"## Credits\n* https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n* http://inoryy.com/post/tensorflow2-deep-reinforcement-learning/\n* https://www.freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d/\n* https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368\n* Also very much thankful to AnalyticsVidhya and Geek forGeeks for their wonderful explaination.\n* The Notebooks related to this competition is also very inspirational."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}