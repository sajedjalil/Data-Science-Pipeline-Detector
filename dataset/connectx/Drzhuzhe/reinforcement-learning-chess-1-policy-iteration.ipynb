{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Reinforcement Learning Chess\nHi there! If you're interested in learning about reinforcement learning, you are in the right place. As we all know the best way to learn about a topic is to build something and make a kernel about it. My plan is to make a series of notebooks where I work may way towards a full-fledged chess AI named RLC (Reinforcement Learning Chess). \n\nTackling chess is a big challenge, mainly because of its huge state-space. Therefore I start with simpler forms of chess and solve these problems with elementary RL-techniques. Gradually I will expand this untill we end up in a chess AI that can play actual games of chess somewhat intelligibly. The forms of chess I want to cover in my notebooks are:  \n\n#### 1. Move Chess \n- Goal: Learn to find the shortest path between 2 squares on a chess board  \n- Motivation: Move Chess has a small statespace, which allows us to tackle this with simple RL algorithms.\n- Concepts: Dynamic Programming, Policy Evaluation, Policy Improvement, Policy Iteration, Value Iteration, Synchronous & Asynchronous back-ups, Monte Carlo (MC) Prediction, MC Control, Temporal Difference (TD) Learning, TD control, TD-lambda, SARSA(-max)\n\n#### 2. Capture Chess\n- Goal: Capture as many pieces from the opponent within n fullmoves\n- Motivation: Piece captures happen more frequently than win-lose-draw events. This give the algorithm more information to learn from.\n- Concepts: Q-learning, value function approximation, experience replay, fixed-q-targets, policy gradients, REINFORCE, actor-critic\n\n\n#### 3. Real Chess (a.k.a. chess)\n- Goal: Play chess competitively against a human beginner\n- Motivation: A RL chess AI\n- Concepts: Monte Carlo Tree Search\n\n#### Other notebooks\n[**Notebook 2: Model free learning**](https://www.kaggle.com/arjanso/reinforcement-learning-chess-2-model-free-methods)  \n[**Notebook 3: Q-networks**](https://www.kaggle.com/arjanso/reinforcement-learning-chess-3-q-networks)  \n[**Notebook 4: Policy Gradients**](https://www.kaggle.com/arjanso/reinforcement-learning-chess-4-policy-gradients)  \n[**Notebook 5: Monte Carlo Tree Search**](https://www.kaggle.com/arjanso/reinforcement-learning-chess-5-tree-search)\n\n\nIn my notebooks, I will describe and reference the Reinforcement Learning theory but I will not fully explain it. For that there are resources available that do a match better job at explaining RL than I could. For that my advice would be to check out David Silver's (Deepmind) lectures that are available on Youtube and the book Introduction to Reinforcement Learning by Sutton and Barto referenced below.","metadata":{}},{"cell_type":"markdown","source":"# Notebook I: Solving Move Chess","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport inspect","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-28T07:14:50.564289Z","iopub.execute_input":"2021-10-28T07:14:50.564985Z","iopub.status.idle":"2021-10-28T07:14:50.572166Z","shell.execute_reply.started":"2021-10-28T07:14:50.564902Z","shell.execute_reply":"2021-10-28T07:14:50.57132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install python-chess  # Python-Chess is the Python Chess Package that handles the chess environment\n!pip install --upgrade git+https://github.com/arjangroen/RLC.git  # RLC is the Reinforcement Learning package","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-28T07:14:50.573748Z","iopub.execute_input":"2021-10-28T07:14:50.574233Z","iopub.status.idle":"2021-10-28T07:15:04.756505Z","shell.execute_reply.started":"2021-10-28T07:14:50.574189Z","shell.execute_reply":"2021-10-28T07:15:04.755577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"execution":{"iopub.status.busy":"2021-10-28T07:15:12.87868Z","iopub.execute_input":"2021-10-28T07:15:12.878996Z","iopub.status.idle":"2021-10-28T07:15:12.908612Z","shell.execute_reply.started":"2021-10-28T07:15:12.878958Z","shell.execute_reply":"2021-10-28T07:15:12.907661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from RLC.move_chess.environment import Board\nfrom RLC.move_chess.agent import Piece\nfrom RLC.move_chess.learn import Reinforce","metadata":{"execution":{"iopub.status.busy":"2021-10-28T07:15:14.427249Z","iopub.execute_input":"2021-10-28T07:15:14.42774Z","iopub.status.idle":"2021-10-28T07:15:14.447586Z","shell.execute_reply.started":"2021-10-28T07:15:14.427521Z","shell.execute_reply":"2021-10-28T07:15:14.446885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The environment","metadata":{}},{"cell_type":"markdown","source":"- The state space is a 8 by 8 grid\n- The starting state S is the top-left square (0,0)\n- The terminal state F is square (5,7). \n- Every move from state to state gives a reward of minus 1\n- Naturally the best policy for this evironment is to move from S to F in the lowest amount of moves possible.","metadata":{}},{"cell_type":"code","source":"env = Board()\nenv.render()\nenv.visual_board","metadata":{"execution":{"iopub.status.busy":"2021-10-28T07:59:31.402819Z","iopub.execute_input":"2021-10-28T07:59:31.403105Z","iopub.status.idle":"2021-10-28T07:59:31.425426Z","shell.execute_reply.started":"2021-10-28T07:59:31.403068Z","shell.execute_reply":"2021-10-28T07:59:31.423896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The agent\n- The agent is a chess Piece (king, queen, rook, knight or bishop)\n- The agent has a behavior policy determining what the agent does in what state","metadata":{"trusted":true}},{"cell_type":"code","source":"p = Piece(piece='king')","metadata":{"execution":{"iopub.status.busy":"2021-10-28T07:59:33.861232Z","iopub.execute_input":"2021-10-28T07:59:33.861527Z","iopub.status.idle":"2021-10-28T07:59:33.88165Z","shell.execute_reply.started":"2021-10-28T07:59:33.86148Z","shell.execute_reply":"2021-10-28T07:59:33.880786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reinforce\n- The reinforce object contains the algorithms for solving move chess\n- The agent and the environment are attributes of the Reinforce object","metadata":{}},{"cell_type":"code","source":"r = Reinforce(p,env)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T07:59:35.710156Z","iopub.execute_input":"2021-10-28T07:59:35.710443Z","iopub.status.idle":"2021-10-28T07:59:35.730095Z","shell.execute_reply.started":"2021-10-28T07:59:35.710398Z","shell.execute_reply":"2021-10-28T07:59:35.729267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.1 State Evaluation","metadata":{"trusted":true}},{"cell_type":"markdown","source":"**Theory**\n\nIf we want our agent to optimize its rewards, we want its policy to guide behavior towards the states with the highest value. This value can be estimated using bootstrapping:\n* A state (s) is as valuable (V) as the successor state (s') plus the reward (R) for going from s to s'. \n* Since there can be mulitple actions (a) and multiple successor states they are summed and weighted by their probability (pi). \n* In a non-deterministic environment, a given action could result in multiple successor states. We don't have to take this into account for this problem because move chess is a deterministic game.\n* Successor state values are discounted with discount factor (gamma) that varies between 0 and 1.  \n* This gives us the following formula:  \n![](http://incompleteideas.net/book/ebook/numeqnarraytmp7-2-2.png)  \n\nNote that:\n* The successor state value is also en estimate. \n* Evaluating a state is bootstrapping because you are making an estimate based on another estimate\n* In the code you'll see a synchronous parameter that will be explained later in the policy evaluation section\n\n","metadata":{"trusted":true}},{"cell_type":"markdown","source":"**Python Implementation**","metadata":{}},{"cell_type":"code","source":"# print(inspect.getsource(r.evaluate_state))\n\ndef evaluate_state(self, state, gamma=0.9, synchronous=True):\n        \"\"\"\n        Calculates the value of a state based on the successor states and the immediate rewards.\n        Args:\n            state: tuple of 2 integers 0-7 representing the state\n            gamma: float, discount factor\n            synchronous: Boolean\n\n        Returns: The expected value of the state under the current policy.\n\n        \"\"\"\n        greedy_action_value = np.max(self.agent.policy[state[0], state[1], :])\n        #print(greedy_action_value)\n        greedy_indices = [i for i, a in enumerate(self.agent.policy[state[0], state[1], :]) if\n                          a == greedy_action_value]  # List of all greedy actions\n        prob = 1 / len(greedy_indices)  # probability of an action occuring\n        state_value = 0\n        for i in greedy_indices:\n            self.env.state = state  # reset state to the one being evaluated\n            reward, episode_end = self.env.step(self.agent.action_space[i])\n            #print(reward)\n            if synchronous:\n                successor_state_value = self.agent.value_function_prev[self.env.state]\n            else:\n                successor_state_value = self.agent.value_function[self.env.state]\n            state_value += (prob * (\n                    reward + gamma * successor_state_value))  # sum up rewards and discounted successor state value\n        return state_value\n    \nr.evaluate_state = evaluate_state","metadata":{"execution":{"iopub.status.busy":"2021-10-28T07:45:10.127711Z","iopub.execute_input":"2021-10-28T07:45:10.128266Z","iopub.status.idle":"2021-10-28T07:45:10.151182Z","shell.execute_reply.started":"2021-10-28T07:45:10.128215Z","shell.execute_reply":"2021-10-28T07:45:10.15014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Demonstration**\n* The initial value function assigns value 0 to each state\n* The initial policy gives an equal probability to each action\n* We evaluate state (0,0)","metadata":{}},{"cell_type":"code","source":"r.agent.value_function.astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T07:45:12.378355Z","iopub.execute_input":"2021-10-28T07:45:12.378689Z","iopub.status.idle":"2021-10-28T07:45:12.401321Z","shell.execute_reply.started":"2021-10-28T07:45:12.378632Z","shell.execute_reply":"2021-10-28T07:45:12.400686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state = (0,0)\nr.agent.value_function[0,0] = r.evaluate_state(r,state,gamma=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T07:45:15.083076Z","iopub.execute_input":"2021-10-28T07:45:15.083637Z","iopub.status.idle":"2021-10-28T07:45:15.101857Z","shell.execute_reply.started":"2021-10-28T07:45:15.083576Z","shell.execute_reply":"2021-10-28T07:45:15.100865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r.agent.value_function.astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T07:45:17.298391Z","iopub.execute_input":"2021-10-28T07:45:17.298706Z","iopub.status.idle":"2021-10-28T07:45:17.325008Z","shell.execute_reply.started":"2021-10-28T07:45:17.298649Z","shell.execute_reply":"2021-10-28T07:45:17.323969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.2 Policy Evaluation\n* Policy evaluation is the act of doe state evaluation for each state in the statespace\n* As you can see in my implementatin I simply iterate over all state and update the value function\n* This is the algorithm provided by Sutton and Barto:  \n![](http://incompleteideas.net/book/ebook/pseudotmp0.png)\n","metadata":{}},{"cell_type":"code","source":"# print(inspect.getsource(r.evaluate_policy))\n\ndef evaluate_policy(self, gamma=0.9, synchronous=True):\n        self.agent.value_function_prev = self.agent.value_function.copy()  # For synchronous updates\n        for row in range(self.agent.value_function.shape[0]):\n            for col in range(self.agent.value_function.shape[1]):\n                self.agent.value_function[row, col] = self.evaluate_state(self, (row, col), gamma=gamma,\n                                                                          synchronous=synchronous)\n                \nr.evaluate_policy = evaluate_policy","metadata":{"execution":{"iopub.status.busy":"2021-10-28T07:45:19.862425Z","iopub.execute_input":"2021-10-28T07:45:19.862913Z","iopub.status.idle":"2021-10-28T07:45:19.883339Z","shell.execute_reply.started":"2021-10-28T07:45:19.862704Z","shell.execute_reply":"2021-10-28T07:45:19.882465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r.evaluate_policy(r,gamma=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T07:45:22.93821Z","iopub.execute_input":"2021-10-28T07:45:22.938712Z","iopub.status.idle":"2021-10-28T07:45:22.972501Z","shell.execute_reply.started":"2021-10-28T07:45:22.938493Z","shell.execute_reply":"2021-10-28T07:45:22.971759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We end up with the following value of -1 for all states except the terminal state. ","metadata":{}},{"cell_type":"code","source":"r.agent.value_function.astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T07:45:25.389253Z","iopub.execute_input":"2021-10-28T07:45:25.389806Z","iopub.status.idle":"2021-10-28T07:45:25.414987Z","shell.execute_reply.started":"2021-10-28T07:45:25.389758Z","shell.execute_reply":"2021-10-28T07:45:25.413696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can iterate this until the value function is stable:","metadata":{}},{"cell_type":"markdown","source":"**Demonstration**","metadata":{}},{"cell_type":"code","source":"eps=0.1\nk_max = 1000\nvalue_delta_max = 0\ngamma = 1\nsynchronous=True\nvalue_delta_max = 0\nfor k in range(k_max):\n    r.evaluate_policy(r, gamma=gamma,synchronous=synchronous)\n    value_delta = np.max(np.abs(r.agent.value_function_prev - r.agent.value_function))\n    value_delta_max = value_delta\n    if value_delta_max < eps:\n        print('converged at iter',k)\n        break","metadata":{"execution":{"iopub.status.busy":"2021-10-28T07:45:28.038562Z","iopub.execute_input":"2021-10-28T07:45:28.039082Z","iopub.status.idle":"2021-10-28T07:45:31.380522Z","shell.execute_reply.started":"2021-10-28T07:45:28.039016Z","shell.execute_reply":"2021-10-28T07:45:31.379673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r.visualize_policy()","metadata":{"execution":{"iopub.status.busy":"2021-10-28T07:45:38.870494Z","iopub.execute_input":"2021-10-28T07:45:38.870824Z","iopub.status.idle":"2021-10-28T07:45:38.890951Z","shell.execute_reply.started":"2021-10-28T07:45:38.870776Z","shell.execute_reply":"2021-10-28T07:45:38.889833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This value function below shows the expected discounted future reward from state (0,0) = -185","metadata":{}},{"cell_type":"code","source":"r.agent.value_function.astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T07:45:45.423Z","iopub.execute_input":"2021-10-28T07:45:45.423272Z","iopub.status.idle":"2021-10-28T07:45:45.443784Z","shell.execute_reply.started":"2021-10-28T07:45:45.423237Z","shell.execute_reply":"2021-10-28T07:45:45.443072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Policy Improvement","metadata":{}},{"cell_type":"markdown","source":"Now that we know what the values of the states are, we want to improve our Policy so that we the behavior is guided towards the state with the highest value. Policy Improvement is simply the act of making the policy greedy with respect to the value function.\n* In my implementation, we do this by setting the value of the action that leads to the most valuable state to 1 (while the rest remains 0)","metadata":{}},{"cell_type":"code","source":"#print(inspect.getsource(r.improve_policy))\ndef improve_policy(self):\n    \"\"\"\n    Finds the greedy policy w.r.t. the current value function\n    \"\"\"\n\n    self.agent.policy_prev = self.agent.policy.copy()\n    for row in range(self.agent.action_function.shape[0]):\n        for col in range(self.agent.action_function.shape[1]):\n            for action in range(self.agent.action_function.shape[2]):\n                self.env.state = (row, col)  # reset state to the one being evaluated\n                reward, episode_end = self.env.step(self.agent.action_space[action])\n                successor_state_value = 0 if episode_end else self.agent.value_function[self.env.state]\n                self.agent.policy[row, col, action] = reward + successor_state_value\n\n            max_policy_value = np.max(self.agent.policy[row, col, :])\n            max_indices = [i for i, a in enumerate(self.agent.policy[row, col, :]) if a == max_policy_value]\n            for idx in max_indices:\n                self.agent.policy[row, col, idx] = 1\n\nfuncType = type(r.improve_policy)\nr.improve_policy = funcType(improve_policy, r)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T07:46:07.755578Z","iopub.execute_input":"2021-10-28T07:46:07.756156Z","iopub.status.idle":"2021-10-28T07:46:07.780159Z","shell.execute_reply.started":"2021-10-28T07:46:07.75609Z","shell.execute_reply":"2021-10-28T07:46:07.779497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r.evaluate_policy(r,gamma=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T07:46:10.834694Z","iopub.execute_input":"2021-10-28T07:46:10.83525Z","iopub.status.idle":"2021-10-28T07:46:10.868944Z","shell.execute_reply.started":"2021-10-28T07:46:10.8352Z","shell.execute_reply":"2021-10-28T07:46:10.86808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r.agent.value_function.astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T07:46:13.127369Z","iopub.execute_input":"2021-10-28T07:46:13.127716Z","iopub.status.idle":"2021-10-28T07:46:13.149322Z","shell.execute_reply.started":"2021-10-28T07:46:13.127655Z","shell.execute_reply":"2021-10-28T07:46:13.148224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r.improve_policy()\nr.visualize_policy()","metadata":{"execution":{"iopub.status.busy":"2021-10-28T07:46:30.028066Z","iopub.execute_input":"2021-10-28T07:46:30.028607Z","iopub.status.idle":"2021-10-28T07:46:30.070692Z","shell.execute_reply.started":"2021-10-28T07:46:30.028542Z","shell.execute_reply":"2021-10-28T07:46:30.069618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r.evaluate_policy(r,gamma=1)\nr.agent.value_function.astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T07:46:33.027552Z","iopub.execute_input":"2021-10-28T07:46:33.027901Z","iopub.status.idle":"2021-10-28T07:46:33.052Z","shell.execute_reply.started":"2021-10-28T07:46:33.027847Z","shell.execute_reply":"2021-10-28T07:46:33.051039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Please note that my visual can print only 1 arrow per square, but there may be multiple optimal actions.","metadata":{}},{"cell_type":"markdown","source":"# 1.3 Policy Iteration  \n**Theory**  \nWe can now find the optimal policy by doing policy evaluation and policy improvement untill the policy is stable:\n![](http://www.incompleteideas.net/book/first/ebook/pseudotmp1.png)","metadata":{}},{"cell_type":"markdown","source":"**Python implementation**","metadata":{}},{"cell_type":"code","source":"# print(inspect.getsource(r.policy_iteration))\ndef policy_iteration(self, eps=0.1, gamma=0.9, iteration=1, k=32, synchronous=True):\n        \"\"\"\n        Finds the optimal policy\n        Args:\n            eps: float, exploration rate\n            gamma: float, discount factor\n            iteration: the iteration number\n            k: (int) maximum amount of policy evaluation iterations\n            synchronous: (Boolean) whether to use synchronous are asynchronous back-ups \n\n        Returns:\n\n        \"\"\"\n        policy_stable = True\n        print(\"\\n\\n______iteration:\", iteration, \"______\")\n        print(\"\\n policy:\")\n        self.visualize_policy()\n\n        print(\"\")\n        value_delta_max = 0\n        for _ in range(k):\n            self.evaluate_policy(self, gamma=gamma, synchronous=synchronous)\n            value_delta = np.max(np.abs(self.agent.value_function_prev - self.agent.value_function))\n            value_delta_max = value_delta\n            if value_delta_max < eps:\n                break\n        print(\"Value function for this policy:\")\n        print(self.agent.value_function.round().astype(int))\n        action_function_prev = self.agent.action_function.copy()\n        print(\"\\n Improving policy:\")\n        self.improve_policy()\n        policy_stable = self.agent.compare_policies() < 1\n        print(\"policy diff:\", policy_stable)\n\n        if not policy_stable and iteration < 1000:\n            iteration += 1\n            self.policy_iteration(iteration=iteration)\n        elif policy_stable:\n            print(\"Optimal policy found in\", iteration, \"steps of policy evaluation\")\n        else:\n            print(\"failed to converge.\")\n            \nfuncType = type(r.policy_iteration)\nr.policy_iteration = funcType(policy_iteration, r)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T07:58:43.865948Z","iopub.execute_input":"2021-10-28T07:58:43.866392Z","iopub.status.idle":"2021-10-28T07:58:43.890185Z","shell.execute_reply.started":"2021-10-28T07:58:43.86635Z","shell.execute_reply":"2021-10-28T07:58:43.88914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Demonstration**","metadata":{"trusted":true}},{"cell_type":"code","source":"r.policy_iteration()","metadata":{"execution":{"iopub.status.busy":"2021-10-28T07:59:44.891222Z","iopub.execute_input":"2021-10-28T07:59:44.891519Z","iopub.status.idle":"2021-10-28T07:59:45.463141Z","shell.execute_reply.started":"2021-10-28T07:59:44.891472Z","shell.execute_reply":"2021-10-28T07:59:45.462064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.4 Asynchronous Policy Iteration\n","metadata":{"trusted":true}},{"cell_type":"markdown","source":"**Theory**  \nWith policy evaluation, we bootstrap: we make an estimate based on another estimate. So which estimate do we take? We have to options:\n1. We bootstrap from the previous policy evaluation. This means each state value estimate update is based on the same iteration of policy evaluation. This is called synchronous policy iteration\n2. We bootstrap from the freshest estimate. This means a estimate update can be based on the previous or the current value funtion, or a combination of the two. This is called asynchrronous policy iteration\n\nThe **Implementation** is the same as policy iteration, only we pass the argument sychronous=False","metadata":{}},{"cell_type":"markdown","source":"**Demonstration**","metadata":{}},{"cell_type":"code","source":"agent = Piece(piece='king')\nr = Reinforce(agent,env)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T08:06:32.962334Z","iopub.execute_input":"2021-10-28T08:06:32.962649Z","iopub.status.idle":"2021-10-28T08:06:32.983473Z","shell.execute_reply.started":"2021-10-28T08:06:32.962573Z","shell.execute_reply":"2021-10-28T08:06:32.982712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r.policy_iteration(gamma=1,synchronous=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T08:06:34.741508Z","iopub.execute_input":"2021-10-28T08:06:34.742034Z","iopub.status.idle":"2021-10-28T08:06:35.521332Z","shell.execute_reply.started":"2021-10-28T08:06:34.7418Z","shell.execute_reply":"2021-10-28T08:06:35.520443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r.visualize_policy()","metadata":{"execution":{"iopub.status.busy":"2021-10-28T08:07:31.247179Z","iopub.execute_input":"2021-10-28T08:07:31.247489Z","iopub.status.idle":"2021-10-28T08:07:31.269413Z","shell.execute_reply.started":"2021-10-28T08:07:31.247435Z","shell.execute_reply":"2021-10-28T08:07:31.268309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r.agent.value_function.astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T08:06:43.934429Z","iopub.execute_input":"2021-10-28T08:06:43.934919Z","iopub.status.idle":"2021-10-28T08:06:43.958388Z","shell.execute_reply.started":"2021-10-28T08:06:43.93483Z","shell.execute_reply":"2021-10-28T08:06:43.957432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.5 Value Iteration","metadata":{}},{"cell_type":"markdown","source":"** Theory **  \nValue iteration is nothing more than a simple parameter modification to policy iteration. Remember that policy iteration consists of policy evaluation and policy improvement. The policy evaluation step does not necessarily have to be repeated until convergence before we improve our policy. Recall that the policy iteration above took over 400 iterations to converge. If we use ony 1 iteration instead we call it value iteration.","metadata":{}},{"cell_type":"markdown","source":"**Demonstration**","metadata":{}},{"cell_type":"code","source":"agent = Piece(piece='rook')  # Let's pick a rook for a change.\nr = Reinforce(agent,env)\nr.policy_iteration(k=1,gamma=1)  # The only difference here is that we set k_max to 1.","metadata":{"execution":{"iopub.status.busy":"2021-10-28T08:05:05.230158Z","iopub.execute_input":"2021-10-28T08:05:05.230473Z","iopub.status.idle":"2021-10-28T08:05:06.116965Z","shell.execute_reply.started":"2021-10-28T08:05:05.230423Z","shell.execute_reply":"2021-10-28T08:05:06.116232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# That's all!\nIn the next notebook I'll cover model-free methods such as Monte Carlo and Temporal Difference based methods. These methods help us when we don't know the transition probalities of a Markov Decision Process. \n\nI expect to have my second RLC notebook up and running around mid-june!\nHope you enjoyed!","metadata":{"trusted":true}},{"cell_type":"markdown","source":"# References","metadata":{}},{"cell_type":"markdown","source":"1. Reinforcement Learning: An Introduction  \n   Richard S. Sutton and Andrew G. Barto  \n   1st Edition  \n   MIT Press, march 1998\n2. RL Course by David Silver: Lecture playlist  \n   https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}