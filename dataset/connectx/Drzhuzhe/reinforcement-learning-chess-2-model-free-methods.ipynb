{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Reinforcement Learning Chess \nReinforcement Learning Chess is a series of notebooks where I implement Reinforcement Learning algorithms to develop a chess AI. I start of with simpler versions (environments) that can be tackled with simple methods and gradually expand on those concepts untill I have a full-flegded chess AI. \n\n[**Notebook 1: Policy Iteration**](https://www.kaggle.com/arjanso/reinforcement-learning-chess-1-policy-iteration)  \n[**Notebook 3: Q-networks**](https://www.kaggle.com/arjanso/reinforcement-learning-chess-3-q-networks)  \n[**Notebook 4: Policy Gradients**](https://www.kaggle.com/arjanso/reinforcement-learning-chess-4-policy-gradients)  \n[**Notebook 5: Monte Carlo Tree Search**](https://www.kaggle.com/arjanso/reinforcement-learning-chess-5-tree-search)  ","metadata":{}},{"cell_type":"markdown","source":"# Notebook II: Model-free control\nIn this notebook I use the same move-chess environment as in notebook 1. In this notebook I mentioned that policy evaluation calculates the state value by backing up the successor state values and the transition probabilities to those states. The problem is that these probabilities are usually unknown in real-world problems. Luckily there are control techniques that can work in these unknown environments. These techniques don't leverage any prior knowledge about the environment's dynamics, they are model-free.","metadata":{}},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"execution":{"iopub.status.busy":"2021-10-29T07:38:49.037999Z","iopub.execute_input":"2021-10-29T07:38:49.038523Z","iopub.status.idle":"2021-10-29T07:38:49.071699Z","shell.execute_reply.started":"2021-10-29T07:38:49.038474Z","shell.execute_reply":"2021-10-29T07:38:49.070515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport inspect","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-29T07:55:20.762533Z","iopub.execute_input":"2021-10-29T07:55:20.763094Z","iopub.status.idle":"2021-10-29T07:55:20.767566Z","shell.execute_reply.started":"2021-10-29T07:55:20.763028Z","shell.execute_reply":"2021-10-29T07:55:20.766665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade git+https://github.com/arjangroen/RLC.git  # RLC is the Reinforcement Learning package","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2021-10-29T07:38:49.101097Z","iopub.execute_input":"2021-10-29T07:38:49.101611Z","iopub.status.idle":"2021-10-29T07:39:02.051395Z","shell.execute_reply.started":"2021-10-29T07:38:49.101543Z","shell.execute_reply":"2021-10-29T07:39:02.050455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from RLC.move_chess.environment import Board\nfrom RLC.move_chess.agent import Piece\nfrom RLC.move_chess.learn import Reinforce","metadata":{"execution":{"iopub.status.busy":"2021-10-29T07:41:35.877483Z","iopub.execute_input":"2021-10-29T07:41:35.878136Z","iopub.status.idle":"2021-10-29T07:41:35.883403Z","shell.execute_reply.started":"2021-10-29T07:41:35.87807Z","shell.execute_reply":"2021-10-29T07:41:35.882365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The environment\n- The state space is a 8 by 8 grid\n- The starting state S is the top-left square (0,0)\n- The terminal state F is square (5,7). \n- Every move from state to state gives a reward of minus 1\n- Naturally the best policy for this evironment is to move from S to F in the lowest amount of moves possible.","metadata":{}},{"cell_type":"code","source":"env = Board()\nenv.render()\nenv.visual_board","metadata":{"execution":{"iopub.status.busy":"2021-10-29T07:41:37.513681Z","iopub.execute_input":"2021-10-29T07:41:37.514282Z","iopub.status.idle":"2021-10-29T07:41:37.52186Z","shell.execute_reply.started":"2021-10-29T07:41:37.51423Z","shell.execute_reply":"2021-10-29T07:41:37.52072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The agent\n- The agent is a chess Piece (king, queen, rook, knight or bishop)\n- The agent has a behavior policy determining what the agent does in what state","metadata":{}},{"cell_type":"code","source":"p = Piece(piece='king')","metadata":{"execution":{"iopub.status.busy":"2021-10-29T07:41:40.228433Z","iopub.execute_input":"2021-10-29T07:41:40.22904Z","iopub.status.idle":"2021-10-29T07:41:40.233757Z","shell.execute_reply.started":"2021-10-29T07:41:40.228991Z","shell.execute_reply":"2021-10-29T07:41:40.232236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reinforce\n- The reinforce object contains the algorithms for solving move chess\n- The agent and the environment are attributes of the Reinforce object","metadata":{}},{"cell_type":"code","source":"r = Reinforce(p,env)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T07:41:41.565579Z","iopub.execute_input":"2021-10-29T07:41:41.566355Z","iopub.status.idle":"2021-10-29T07:41:41.571377Z","shell.execute_reply.started":"2021-10-29T07:41:41.566294Z","shell.execute_reply":"2021-10-29T07:41:41.570291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.1 Monte Carlo Control","metadata":{}},{"cell_type":"markdown","source":"**Theory**  \nThe basic intuition is:\n* We do not know the environment, so we sample an episode from beginning to end by running our current policy\n* We try to estimate the action-values rather than the state values. This is because we are working model-free so just knowning state values won't help us select the best actions. \n* The value of a state-action value is defined as the future returns from the first visit of that state-action\n* Based on this we can improve our policy and repeat the process untill the algorithm converges\n\n![](http://incompleteideas.net/book/first/ebook/pseudotmp5.png)","metadata":{}},{"cell_type":"markdown","source":"**Implementation**","metadata":{}},{"cell_type":"code","source":"#print(inspect.getsource(r.agent.apply_policy))\ndef apply_policy(self, state, epsilon):\n        \"\"\"\n        Apply the policy of the agent\n        Args:\n            state: tuple of length 2\n            epsilon: exploration probability, 0 for greedy behavior, 1 for pure exploration\n\n        Returns:\n            the selected action for the state under the current policy\n\n        \"\"\"\n        greedy_action_value = np.max(self.policy[state[0], state[1], :])\n        greedy_indices = [i for i, a in enumerate(self.policy[state[0], state[1], :]) if\n                          a == greedy_action_value]\n        action_index = np.random.choice(greedy_indices)\n        if np.random.uniform(0, 1) < epsilon:\n            action_index = np.random.choice(range(len(self.action_space)))\n        return action_index\n\nfunc_type = type(r.agent.apply_policy) # method not function\nr.apply_policy = func_type(r.agent.apply_policy, r)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T07:39:02.176838Z","iopub.execute_input":"2021-10-29T07:39:02.177426Z","iopub.status.idle":"2021-10-29T07:39:02.206742Z","shell.execute_reply.started":"2021-10-29T07:39:02.17736Z","shell.execute_reply":"2021-10-29T07:39:02.205969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(inspect.getsource(r.play_episode))\n\ndef play_episode(self, state, max_steps=1e3, epsilon=0.1):\n    \"\"\"\n    Play an episode of move chess\n    :param state: tuple describing the starting state on 8x8 matrix\n    :param max_steps: integer, maximum amount of steps before terminating the episode\n    :param epsilon: exploration parameter\n    :return: tuple of lists describing states, actions and rewards in a episode\n    \"\"\"\n    self.env.state = state\n    states = []\n    actions = []\n    rewards = []\n    episode_end = False\n\n    # Play out an episode\n    count_steps = 0\n    while not episode_end:\n        count_steps += 1\n        states.append(state)\n        action_index = self.agent.apply_policy(state, epsilon)  # get the index of the next action\n        action = self.agent.action_space[action_index]\n        actions.append(action_index)\n        reward, episode_end = self.env.step(action)\n        state = self.env.state\n        rewards.append(reward)\n\n        #  avoid infinite loops\n        if count_steps > max_steps:\n            episode_end = True\n\n    return states, actions, rewards\n\nfunc_type = type(r.play_episode) # method not function\nr.play_episode = func_type(play_episode, r)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T07:39:02.20831Z","iopub.execute_input":"2021-10-29T07:39:02.208591Z","iopub.status.idle":"2021-10-29T07:39:02.237145Z","shell.execute_reply.started":"2021-10-29T07:39:02.208537Z","shell.execute_reply":"2021-10-29T07:39:02.236335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(inspect.getsource(r.monte_carlo_learning))\n\ndef monte_carlo_learning(self, epsilon=0.1):\n        \"\"\"\n        Learn move chess through monte carlo control\n        :param epsilon: exploration rate\n        :return:\n        \"\"\"\n        state = (0, 0)\n        self.env.state = state\n\n        # Play out an episode\n        states, actions, rewards = self.play_episode(state, epsilon=epsilon)\n        \n        #print(states, actions, rewards)\n        first_visits = []\n        for idx, state in enumerate(states):\n            action_index = actions[idx]\n            if (state, action_index) in first_visits:\n                continue\n            r = np.sum(rewards[idx:])\n            if (state, action_index) in self.agent.Returns.keys():\n                self.agent.Returns[(state, action_index)].append(r)\n            else:\n                self.agent.Returns[(state, action_index)] = [r]\n            self.agent.action_function[state[0], state[1], action_index] = \\\n                np.mean(self.agent.Returns[(state, action_index)])\n            first_visits.append((state, action_index))\n        \n        #print(len(self.agent.Returns.keys()))\n        #print(self.agent.action_function.shape)\n        # Update the policy. In Monte Carlo Control, this is greedy behavior with respect to the action function\n        self.agent.policy = self.agent.action_function.copy()\n\n        \nfunc_type = type(r.monte_carlo_learning) # method not function\nr.monte_carlo_learning = func_type(monte_carlo_learning, r)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T07:39:02.238499Z","iopub.execute_input":"2021-10-29T07:39:02.238901Z","iopub.status.idle":"2021-10-29T07:39:02.270031Z","shell.execute_reply.started":"2021-10-29T07:39:02.238845Z","shell.execute_reply":"2021-10-29T07:39:02.269087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Demo**  \nWe do 100 iterations of monte carlo learning while maintaining a high exploration rate of 0.5:","metadata":{}},{"cell_type":"code","source":"for k in range(100):\n    eps = 0.5\n    r.monte_carlo_learning(epsilon=eps)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T07:39:02.27136Z","iopub.execute_input":"2021-10-29T07:39:02.271751Z","iopub.status.idle":"2021-10-29T07:39:02.982741Z","shell.execute_reply.started":"2021-10-29T07:39:02.271589Z","shell.execute_reply":"2021-10-29T07:39:02.981485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k in range(1):\n    eps = 0.1\n    r.monte_carlo_learning(epsilon=eps)\n    print(\"------------\")\n    r.visualize_policy()","metadata":{"execution":{"iopub.status.busy":"2021-10-29T07:39:02.984504Z","iopub.execute_input":"2021-10-29T07:39:02.984855Z","iopub.status.idle":"2021-10-29T07:39:03.023362Z","shell.execute_reply.started":"2021-10-29T07:39:02.984788Z","shell.execute_reply":"2021-10-29T07:39:03.022215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best action value for each state:","metadata":{}},{"cell_type":"code","source":"r.agent.action_function.max(axis=2).astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T07:39:03.024876Z","iopub.execute_input":"2021-10-29T07:39:03.025255Z","iopub.status.idle":"2021-10-29T07:39:03.054219Z","shell.execute_reply.started":"2021-10-29T07:39:03.025178Z","shell.execute_reply":"2021-10-29T07:39:03.053238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.2 Temporal Difference Learning ","metadata":{}},{"cell_type":"markdown","source":"**Theory**\n* Like Policy Iteration, we can back up state-action values from the successor state action without waiting for the episode to end. \n* We update our state-action value in the direction of the successor state action value.\n* The algorithm is called SARSA: State-Action-Reward-State-Action.\n* Epsilon is gradually lowered (the GLIE property)","metadata":{}},{"cell_type":"markdown","source":"**Implementation**","metadata":{}},{"cell_type":"code","source":"p = Piece(piece='king')\nenv = Board()\nr = Reinforce(p,env)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T07:45:34.989641Z","iopub.execute_input":"2021-10-29T07:45:34.990033Z","iopub.status.idle":"2021-10-29T07:45:34.996688Z","shell.execute_reply.started":"2021-10-29T07:45:34.989974Z","shell.execute_reply":"2021-10-29T07:45:34.994686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(inspect.getsource(r.sarsa_td))\n\ndef sarsa_td(self, n_episodes=1000, alpha=0.01, gamma=0.9):\n        \"\"\"\n        Run the sarsa control algorithm (TD0), finding the optimal policy and action function\n        :param n_episodes: int, amount of episodes to train\n        :param alpha: learning rate\n        :param gamma: discount factor of future rewards\n        :return: finds the optimal policy for move chess\n        \"\"\"\n        print(\"-----\")\n        for k in range(n_episodes):\n            state = (0, 0)\n            self.env.state = state\n            episode_end = False\n            # go as eps ï¼Œ there will be less random\n            epsilon = max(1 / (1 + k), 0.05)\n            while not episode_end:\n                state = self.env.state\n                action_index = self.agent.apply_policy(state, epsilon)\n                action = self.agent.action_space[action_index]\n                reward, episode_end = self.env.step(action)\n                successor_state = self.env.state\n                successor_action_index = self.agent.apply_policy(successor_state, epsilon)\n\n                action_value = self.agent.action_function[state[0], state[1], action_index]\n\n                successor_action_value = self.agent.action_function[successor_state[0],\n                                                                    successor_state[1], successor_action_index]\n                q_update = alpha * (reward + gamma * successor_action_value - action_value)\n\n                self.agent.action_function[state[0], state[1], action_index] += q_update\n                \n                #print(q_update)\n                    \n                self.agent.policy = self.agent.action_function.copy()\n\n                \nfunc_type = type(r.sarsa_td) # method not function\nr.sarsa_td = func_type(sarsa_td, r)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T07:48:34.925543Z","iopub.execute_input":"2021-10-29T07:48:34.925857Z","iopub.status.idle":"2021-10-29T07:48:34.942147Z","shell.execute_reply.started":"2021-10-29T07:48:34.925819Z","shell.execute_reply":"2021-10-29T07:48:34.940671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Demonstration**","metadata":{}},{"cell_type":"code","source":"r.sarsa_td(n_episodes=1,alpha=0.2,gamma=0.9)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T07:48:37.656908Z","iopub.execute_input":"2021-10-29T07:48:37.657348Z","iopub.status.idle":"2021-10-29T07:48:37.708493Z","shell.execute_reply.started":"2021-10-29T07:48:37.657283Z","shell.execute_reply":"2021-10-29T07:48:37.707138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r.visualize_policy()","metadata":{"execution":{"iopub.status.busy":"2021-10-29T07:52:33.55711Z","iopub.execute_input":"2021-10-29T07:52:33.557498Z","iopub.status.idle":"2021-10-29T07:52:33.56919Z","shell.execute_reply.started":"2021-10-29T07:52:33.557449Z","shell.execute_reply":"2021-10-29T07:52:33.565149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.3 TD-lambda\n**Theory**  \nIn Monte Carlo we do a full-depth backup while in Temporal Difference Learning we de a 1-step backup. You could also choose a depth in-between: backup by n steps. But what value to choose for n?\n* TD lambda uses all n-steps and discounts them with factor lambda\n* This is called lambda-returns\n* TD-lambda uses an eligibility-trace to keep track of the previously encountered states\n* This way action-values can be updated in retrospect","metadata":{}},{"cell_type":"markdown","source":"**Implementation**","metadata":{}},{"cell_type":"code","source":"p = Piece(piece='king')\nenv = Board()\nr = Reinforce(p,env)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  print(inspect.getsource(r.sarsa_lambda))\ndef sarsa_lambda(self, n_episodes=1000, alpha=0.05, gamma=0.9, lamb=0.8):\n        \"\"\"\n        Run the sarsa control algorithm (TD lambda), finding the optimal policy and action function\n        :param n_episodes: int, amount of episodes to train\n        :param alpha: learning rate\n        :param gamma: discount factor of future rewards\n        :param lamb: lambda parameter describing the decay over n-step returns\n        :return: finds the optimal move chess policy\n        \"\"\"\n        for k in range(n_episodes):\n            # only backup policy with in same eps\n            self.agent.E = np.zeros(shape=self.agent.action_function.shape)\n            state = (0, 0)\n            self.env.state = state\n            episode_end = False\n            epsilon = max(1 / (1 + k), 0.2)\n            action_index = self.agent.apply_policy(state, epsilon)\n            action = self.agent.action_space[action_index]\n            #print(action_index, action)\n            while not episode_end:\n                reward, episode_end = self.env.step(action)\n                successor_state = self.env.state\n                successor_action_index = self.agent.apply_policy(successor_state, epsilon)                \n                action_value = self.agent.action_function[state[0], state[1], action_index]\n                if not episode_end:\n                    successor_action_value = self.agent.action_function[successor_state[0],\n                                                                        successor_state[1], successor_action_index]\n                else:\n                    successor_action_value = 0\n                delta = reward + gamma * successor_action_value - action_value                  \n                self.agent.E[state[0], state[1], action_index] += 1\n                self.agent.action_function = self.agent.action_function + alpha * delta * self.agent.E\n                self.agent.E = gamma * lamb * self.agent.E\n                state = successor_state\n                action = self.agent.action_space[successor_action_index]\n                action_index = successor_action_index\n                #print(action)\n                self.agent.policy = self.agent.action_function.copy()\n                \nfunc_type = type(r.sarsa_lambda) # method not function\nr.sarsa_lambda = func_type(sarsa_lambda, r)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T08:01:15.590471Z","iopub.execute_input":"2021-10-29T08:01:15.590965Z","iopub.status.idle":"2021-10-29T08:01:15.606208Z","shell.execute_reply.started":"2021-10-29T08:01:15.590904Z","shell.execute_reply":"2021-10-29T08:01:15.605259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Demonstration**","metadata":{}},{"cell_type":"code","source":"r.sarsa_lambda(n_episodes=1,alpha=0.2,gamma=0.9)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T08:01:20.240297Z","iopub.execute_input":"2021-10-29T08:01:20.240778Z","iopub.status.idle":"2021-10-29T08:01:20.250265Z","shell.execute_reply.started":"2021-10-29T08:01:20.240713Z","shell.execute_reply":"2021-10-29T08:01:20.24949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r.agent.E.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-29T07:56:48.0751Z","iopub.execute_input":"2021-10-29T07:56:48.075711Z","iopub.status.idle":"2021-10-29T07:56:48.082189Z","shell.execute_reply.started":"2021-10-29T07:56:48.075632Z","shell.execute_reply":"2021-10-29T07:56:48.081286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r.visualize_policy()","metadata":{"execution":{"iopub.status.busy":"2021-10-29T08:01:24.086722Z","iopub.execute_input":"2021-10-29T08:01:24.087289Z","iopub.status.idle":"2021-10-29T08:01:24.097385Z","shell.execute_reply.started":"2021-10-29T08:01:24.087037Z","shell.execute_reply":"2021-10-29T08:01:24.096173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.4 Q-learning","metadata":{}},{"cell_type":"markdown","source":"**Theory**\n* In SARSA/TD0, we back-up our action values with the succesor action value\n* In SARSA-max/Q learning, we back-up using the maximum action value. ","metadata":{}},{"cell_type":"markdown","source":"**Implementation**","metadata":{}},{"cell_type":"code","source":"p = Piece(piece='king')\nenv = Board()\nr = Reinforce(p,env)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T08:04:37.102636Z","iopub.execute_input":"2021-10-29T08:04:37.102996Z","iopub.status.idle":"2021-10-29T08:04:37.110052Z","shell.execute_reply.started":"2021-10-29T08:04:37.102943Z","shell.execute_reply":"2021-10-29T08:04:37.108508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(inspect.getsource(r.q_learning))\n\ndef q_learning(self, n_episodes=1000, alpha=0.05, gamma=0.9):\n        \"\"\"\n        Run Q-learning (also known as sarsa-max, finding the optimal policy and value function\n        :param n_episodes: int, amount of episodes to train\n        :param alpha: learning rate\n        :param gamma: discount factor of future rewards\n        :return: finds the optimal move chess policy\n        \"\"\"\n        for k in range(n_episodes):\n            state = (0, 0)\n            self.env.state = state\n            episode_end = False\n            epsilon = max(1 / (k + 1), 0.1)\n            while not episode_end:\n                action_index = self.agent.apply_policy(state, epsilon)\n                action = self.agent.action_space[action_index]\n                reward, episode_end = self.env.step(action)\n                successor_state = self.env.state\n                # no randomness\n                successor_action_index = self.agent.apply_policy(successor_state, -1)\n\n                action_value = self.agent.action_function[state[0], state[1], action_index]\n                \n                #   why end ep end \n                if not episode_end:\n                    successor_action_value = self.agent.action_function[successor_state[0],\n                                                                        successor_state[1], successor_action_index]\n                else:\n                    successor_action_value = 0\n                    \n                # as same a TD-sarsa\n                av_new = self.agent.action_function[state[0], state[1], action_index] + alpha * (reward +\n                                                                                                 gamma *\n                                                                                                 successor_action_value\n                                                                                                 - action_value)\n                self.agent.action_function[state[0], state[1], action_index] = av_new\n                self.agent.policy = self.agent.action_function.copy()\n                state = successor_state\n                \n                \nfunc_type = type(r.q_learning) # method not function\nr.q_learning = func_type(q_learning, r)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T08:04:39.282181Z","iopub.execute_input":"2021-10-29T08:04:39.282794Z","iopub.status.idle":"2021-10-29T08:04:39.296934Z","shell.execute_reply.started":"2021-10-29T08:04:39.282729Z","shell.execute_reply":"2021-10-29T08:04:39.296169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Demonstration**","metadata":{}},{"cell_type":"code","source":"r.q_learning(n_episodes=1000,alpha=0.2,gamma=0.9)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T08:04:44.037953Z","iopub.execute_input":"2021-10-29T08:04:44.038306Z","iopub.status.idle":"2021-10-29T08:04:44.713491Z","shell.execute_reply.started":"2021-10-29T08:04:44.038247Z","shell.execute_reply":"2021-10-29T08:04:44.71266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r.visualize_policy()","metadata":{"execution":{"iopub.status.busy":"2021-10-29T08:14:57.019711Z","iopub.execute_input":"2021-10-29T08:14:57.020095Z","iopub.status.idle":"2021-10-29T08:14:57.027736Z","shell.execute_reply.started":"2021-10-29T08:14:57.020035Z","shell.execute_reply":"2021-10-29T08:14:57.026071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r.agent.action_function.max(axis=2).round().astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T08:15:18.962897Z","iopub.execute_input":"2021-10-29T08:15:18.963514Z","iopub.status.idle":"2021-10-29T08:15:18.974349Z","shell.execute_reply.started":"2021-10-29T08:15:18.963464Z","shell.execute_reply":"2021-10-29T08:15:18.972884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n1. Reinforcement Learning: An Introduction  \n   Richard S. Sutton and Andrew G. Barto  \n   1st Edition  \n   MIT Press, march 1998\n2. RL Course by David Silver: Lecture playlist  \n   https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ","metadata":{"trusted":true}}]}