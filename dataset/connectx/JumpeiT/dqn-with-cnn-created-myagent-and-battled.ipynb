{"cells":[{"metadata":{},"cell_type":"markdown","source":"## This kernel was extended from https://www.kaggle.com/phunghieu/connectx-with-deep-q-learning.\nThanks for your tutorial of DQN using pytorch and connectX!\n\nI modified it and introduced CNN as DQN model.\n\n- 4/6\n    - introduced CNN and change rewards function.\n    \n- 4/8 \n    - <font color=\"red\">I struggled with how to make \"my_agent\" method using CNN module. It is an ongoing task</font>\n- 6/14 \n    - completed the implementation with 1 CNN layer!!!\n    - There might be some problems about the ability to win...\n\n- 6/16\n    - I will modify this kernel as I can change multiple layers of CNN.\n    - Also I have to set reward functions when the last piece of the sequence is located in the middle of the sequence.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# About Reinforcement Learning and Deep Q-Learning\n> \"Reinforcement learning is an area of machine learning that is focused on training agents to take certain actions at certain states from within an environment to maximize rewards. DQN (Deep Q-Net) is a reinforcement learning algorithm where a deep learning model is built to find the actions an agent can take at each state.\" \n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<!--\n- Using pretrained models, I plan to make the two models battle and transfer learning-->\n<!--\n- why do we need TargetNet ?\n    - might be Critic module.\n- Can we implement RL adversarially ?\n     could be easily\n- DQN can be modified to CNN(20' 4/7)\n-->\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"ToC\"></a>\n# Table of Contents\n1. [Install libraries](#install_libraries)\n1. [Import libraries](#import_libraries)\n1. [Define useful classes](#define_useful_classes)\n1. [Define helper-functions](#define_helper_functions)\n1. [Create ConnectX environment](#create_connectx_environment)\n1. [Configure hyper-parameters](#configure_hyper_parameters)\n1. [Train the agent](#train_the_agent)\n1. [Save weights](#save_weights)\n1. [Create an agent](#create_an_agent)\n1. [Evaluate the agent](#evaluate_the_agent)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"install_libraries\"></a>\n# Install libraries\n[Back to Table of Contents](#ToC)","execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# !pip install 'kaggle-environments>=0.1.6' > /dev/null 2>&1\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"import_libraries\"></a>\n# Import libraries\n[Back to Table of Contents](#ToC)","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom kaggle_environments import evaluate, make","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"define_useful_classes\"></a>\n# Define useful classes\nNOTE: All classes here were copied from my previous [*kernel*](https://www.kaggle.com/phunghieu/connectx-with-deep-q-learning) and switched from using TF2.0 to PyTorch. If you prefer TF2.0, let check [ConnectX with Deep Q-Learning](https://www.kaggle.com/phunghieu/connectx-with-deep-q-learning) kernel.\n\n---\n[Back to Table of Contents](#ToC)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConnectX(gym.Env):\n    def __init__(self, switch_prob=0.5, pair=[None, 'random']):\n        self.env = make('connectx', debug=False)\n        self.pair = [None, 'random']\n        self.trainer = self.env.train(self.pair)\n        self.switch_prob = switch_prob\n\n        # Define required gym fields (examples):\n        config = self.env.configuration\n        self.action_space = gym.spaces.Discrete(config.columns)\n        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n\n    def switch_trainer(self):\n        self.pair = self.pair[::-1]\n        self.trainer = self.env.train(self.pair)\n\n    def step(self, action):\n        return self.trainer.step(action)\n    \n    def reset(self):\n        if np.random.random() < self.switch_prob:\n            self.switch_trainer()\n        return self.trainer.reset()\n    \n    def render(self, **kwargs):\n        return self.env.render(**kwargs)\n    \n    \nclass CNN_model(nn.Module):\n\n    def __init__(self, h, w, outputs):\n\n        super(CNN_model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=2, stride=1)\n#         self.bn1 = nn.BatchNorm2d(16)\n#         self.conv2 = nn.Conv2d(16, 32, kernel_size=2, stride=1)\n#         self.bn2 = nn.BatchNorm2d(32)\n#         self.conv3 = nn.Conv2d(32, 32, kernel_size=2, stride=1)\n#         self.bn3 = nn.BatchNorm2d(32)\n\n        # Number of Linear input connections depends on output of conv2d layers\n        # and therefore the input image size, so compute it.\n        def conv2d_size_out(size, kernel_size = 2, stride = 1):\n            return (size - (kernel_size - 1) - 1) // stride  + 1\n        convw = conv2d_size_out(w)\n        convh = conv2d_size_out(h)\n        linear_input_size = convw * convh * 16\n        self.head = nn.Linear(linear_input_size, outputs)\n\n    # Called with either one element to determine next action, or a batch\n    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n    def forward(self, x):\n        # import pdb; pdb.set_trace()\n        x = F.relu(self.conv1(x))\n#         x = F.relu(self.bn2(self.conv2(x)))\n#         x = F.relu(self.bn3(self.conv3(x)))\n        return self.head(x.view(x.size(0), -1))\n\n\nclass DQN:\n    def __init__(self, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr):\n        self.num_actions = num_actions\n        self.batch_size = batch_size\n        self.gamma = gamma\n        self.model = CNN_model(6,7, num_actions)\n        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n        self.criterion = nn.MSELoss()\n        self.experience = {'s': [], 'a': [], 'r': [], 's2': [], 'done': []} # The buffer\n        self.max_experiences = max_experiences\n        self.min_experiences = min_experiences\n\n    def predict(self, inputs):\n        return self.model(torch.from_numpy(inputs).float().view(-1, 1, 6, 7))\n\n    def train(self, TargetNet):\n        if len(self.experience['s']) < self.min_experiences:\n            # Only start the training process when we have enough experiences in the buffer\n            return 0\n\n        # Randomly select n experience in the buffer, n is batch-size\n        ids = np.random.randint(low=0, high=len(self.experience['s']), size=self.batch_size)\n        states = np.asarray([self.preprocess(self.experience['s'][i]) for i in ids])\n        actions = np.asarray([self.experience['a'][i] for i in ids])\n        rewards = np.asarray([self.experience['r'][i] for i in ids])\n\n        # Prepare labels for training process\n        states_next = np.asarray([self.preprocess(self.experience['s2'][i]) for i in ids])\n        dones = np.asarray([self.experience['done'][i] for i in ids])\n        value_next = np.max(TargetNet.predict(states_next).detach().numpy(), axis=1)\n        actual_values = np.where(dones, rewards, rewards+self.gamma*value_next)\n\n        actions = np.expand_dims(actions, axis=1)\n        actions_one_hot = torch.FloatTensor(self.batch_size, self.num_actions).zero_()\n        actions_one_hot = actions_one_hot.scatter_(1, torch.LongTensor(actions), 1)\n        selected_action_values = torch.sum(self.predict(states) * actions_one_hot, dim=1)\n        actual_values = torch.FloatTensor(actual_values)\n\n        self.optimizer.zero_grad()\n        loss = self.criterion(selected_action_values, actual_values)\n        loss.backward()\n        self.optimizer.step()\n\n    # Get an action by using epsilon-greedy\n    def get_action(self, state, epsilon):\n        if np.random.random() < epsilon:\n            return int(np.random.choice([c for c in range(self.num_actions) if state.board[c] == 0]))\n        else:\n            prediction = self.predict(np.atleast_2d(self.preprocess(state)))[0].detach().numpy()\n            for i in range(self.num_actions):\n                # もうすでに埋まっているcellは対象外\n                if state.board[i] != 0:\n                    prediction[i] = -1e7\n            return int(np.argmax(prediction))\n\n    # Method used to manage the buffer\n    def add_experience(self, exp):\n        if len(self.experience['s']) >= self.max_experiences:\n            for key in self.experience.keys():\n                self.experience[key].pop(0)\n        for key, value in exp.items():\n            self.experience[key].append(value)\n\n    def copy_weights(self, TrainNet):\n        self.model.load_state_dict(TrainNet.state_dict())\n\n    def save_weights(self, path):\n        torch.save(self.model.state_dict(), path)\n\n    def load_weights(self, path):\n        self.model.load_state_dict(torch.load(path))\n    \n    # Each state will consist of the board and the mark\n    # in the observations\n    def preprocess(self, state):\n        result = state.board[:]\n        \n        # I omitted state.mark from input. I'm guessing this feat is redundant\n#         result.append(state.mark)\n\n        return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef set_direction(d, direction):\n    if direction == \"row\":\n        dx, dy = d, 0\n    elif direction == \"col\":\n        dx, dy = 0, d\n    elif direction == \"diag\":\n        dx, dy = d, d\n    elif direction == \"anti-diag\":\n        dx, dy = d, -1*d\n    return dx, dy \n\ndef count_seq(new_stone_loc, state,mark):\n    \"\"\"change state for each direction\"\"\"\n    ans = 0\n    i, j = new_stone_loc\n    for direction in [\"row\", \"col\", \"diag\", \"anti-diag\"]:\n        count_sequences = 0\n        for dir_ in [1, -1]:\n            for d in range(4):\n                try:\n                    dx, dy = set_direction(dir_*d,direction)\n                    if dx == 0 and dy == 0:\n                        continue\n                    elif state[i + dx, j + dy] == mark:\n                        count_sequences += 1\n                    else:\n                        break\n                except IndexError:\n                    break\n        ans = max(count_sequences, ans)\n    return ans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.zeros((6,7))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"define_helper_functions\"></a>\n# Define helper-functions\n[Back to Table of Contents](#ToC)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def reward_coordination(obs, prev_obs):\n    # prev_observationとobservationを比較して\n    # 自分のstoneが連結しているかいなかでrewardを変更する。\n    # 連結確認メソッド\n    # import pdb; pdb.set_trace()\n\n    obs_mat = np.array(obs.board).reshape(-1,7)\n    prev_obs_mat = np.array(prev_obs.board).reshape(-1,7)\n    new_stone_loc = np.where(obs_mat - prev_obs_mat == obs.mark)\n    out = count_seq(new_stone_loc, obs_mat, obs.mark)\n\n    return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n    rewards = 0\n    iter_ = 0\n    done = False\n    observations = env.reset()\n    while not done:\n        # Using epsilon-greedy to get an action\n        action = TrainNet.get_action(observations, epsilon)\n\n        # Caching the information of current state\n        prev_observations = observations\n\n        # Take action\n        observations, reward, done, _ = env.step(action)\n\n        # Apply new rules\n        if done:\n            if reward == 1: # Won\n                reward = 20\n            elif reward == 0: # Lost\n                reward = -20\n            else: # Draw\n                reward = 10\n        else:\n#             reward = -0.05 # Try to prevent the agent from taking a long move\n\n            # Try to promote the agent to \"struggle\" when playing against negamax agent\n            # as Magolor's (@magolor) idea\n            reward = 0.5\n#             reward = reward_coordination(observations, prev_observations) * 0.5\n\n        rewards += reward\n\n        # Adding experience into buffer\n        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n        TrainNet.add_experience(exp)\n\n        # Train the training model by using experiences in buffer and the target model\n        TrainNet.train(TargetNet)\n        iter_ += 1\n        if iter_ % copy_step == 0:\n            # Update the weights of the target model when reaching enough \"copy step\"\n            TargetNet.copy_weights(TrainNet)\n    return rewards","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"create_connectx_environment\"></a>\n# Create ConnectX environment\n[Back to Table of Contents](#ToC)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def rule_based(obs, conf):\n    def get_results(x, y, mark, multiplier):\n        \"\"\" get list of points, lowest cells and \"in air\" cells of a board[x][y] cell considering mark \"\"\"\n        # set board[x][y] as mark\n        board[x][y] = mark\n        results = []\n        # if some points in axis already found - axis blocked\n        blocked = [False, False, False, False]\n        # i is amount of marks required to add points\n        for i in range(conf.inarow, 2, -1):\n            # points\n            p = 0\n            # lowest cell\n            lc = 0\n            # \"in air\" points\n            ap = 0\n            # axis S -> N, only if one mark required for victory\n            if i == conf.inarow and blocked[0] is False:\n                (p, lc, ap, blocked[0]) = process_results(p, lc, ap,\n                              check_axis(mark, i, x, lambda z : z, y + inarow_m1, lambda z : z - 1))\n            # axis SW -> NE\n            if blocked[1] is False:\n                (p, lc, ap, blocked[1]) = process_results(p, lc, ap,\n                    check_axis(mark, i, x - inarow_m1, lambda z : z + 1, y + inarow_m1, lambda z : z - 1))\n            # axis E -> W\n            if blocked[2] is False:\n                (p, lc, ap, blocked[2]) = process_results(p, lc, ap,\n                    check_axis(mark, i, x + inarow_m1, lambda z : z - 1, y, lambda z : z))\n            # axis SE -> NW\n            if blocked[3] is False:\n                (p, lc, ap, blocked[3]) = process_results(p, lc, ap, \n                    check_axis(mark, i, x + inarow_m1, lambda z : z - 1, y + inarow_m1, lambda z : z - 1))\n            results.append((p * multiplier, lc, ap))\n        # restore board[x][y] original value\n        board[x][y] = 0\n        return results\n    \n    def check_axis(mark, inarow, x, x_fun, y, y_fun):\n        \"\"\" check axis (NE -> SW etc.) for lowest cell and amounts of points and \"in air\" cells \"\"\"\n        (x, y, axis_max_range) = get_x_y_and_axis_max_range(x, x_fun, y, y_fun)\n        zeros_allowed = conf.inarow - inarow\n        #lowest_cell = y\n        # lowest_cell calculation turned off\n        lowest_cell = 0\n        for i in range(axis_max_range):\n            x_temp = x\n            y_temp = y\n            zeros_remained = zeros_allowed\n            marks = 0\n            # amount of empty cells that are \"in air\" (don't have board bottom or mark under them)\n            in_air = 0\n            for j in range(conf.inarow):\n                if board[x_temp][y_temp] != mark and board[x_temp][y_temp] != 0:\n                    break\n                elif board[x_temp][y_temp] == mark:\n                    marks += 1\n                # board[x_temp][y_temp] is 0\n                else:\n                    zeros_remained -= 1\n                    if (y_temp + 1) < conf.rows and board[x_temp][y_temp + 1] == 0:\n                        in_air -= 1\n#                 if y_temp > lowest_cell:\n#                     lowest_cell = y_temp\n                if marks == inarow and zeros_remained == 0:\n                    return (sp, lowest_cell, in_air, True)\n                x_temp = x_fun(x_temp)\n                y_temp = y_fun(y_temp)\n                if y_temp < 0 or y_temp >= conf.rows or x_temp < 0 or x_temp >= conf.columns:\n                    return (0, 0, 0, False)\n            x = x_fun(x)\n            y = y_fun(y)\n        return (0, 0, 0, False)\n        \n    def get_x_y_and_axis_max_range(x, x_fun, y, y_fun):\n        \"\"\" set x and y inside board boundaries and get max range of axis \"\"\"\n        axis_max_range = conf.inarow\n        while y < 0 or y >= conf.rows or x < 0 or x >= conf.columns:\n            x = x_fun(x)\n            y = y_fun(y)\n            axis_max_range -= 1\n        return (x, y, axis_max_range)\n    \n    def process_results(p, lc, ap, axis_check_results):\n        \"\"\" process results of check_axis function, return lowest cell and sums of points and \"in air\" cells \"\"\"\n        (points, lowest_cell, in_air, blocked) = axis_check_results\n        if points > 0:\n            if lc < lowest_cell:\n                lc = lowest_cell\n            ap += in_air\n            p += points\n        return (p, lc, ap, blocked)\n    \n    def get_best_cell(best_cell, current_cell):\n        \"\"\" get best cell by comparing factors of cells \"\"\"\n        for i in range(len(current_cell[\"factors\"])):\n            # index 0 = points, 1 = lowest cell, 2 = \"in air\" cells\n            for j in range(3):\n                # if value of best cell factor is smaller than value of\n                # the same factor in the current cell\n                # best cell = current cell and break the loop,\n                # don't compare lower priority factors\n                if best_cell[\"factors\"][i][j] < current_cell[\"factors\"][i][j]:\n                    return current_cell\n                # if value of best cell factor is bigger than value of\n                # the same factor in the current cell\n                # break loop and don't compare lower priority factors\n                if best_cell[\"factors\"][i][j] > current_cell[\"factors\"][i][j]:\n                    return best_cell\n        return best_cell\n    \n    def get_factors(results):\n        \"\"\" get list of factors represented by results and ordered by priority from highest to lowest \"\"\"\n        factors = []\n        for i in range(conf.inarow - 2):\n            if i == 1:\n                # my checker in this cell means my victory two times\n                factors.append(results[0][0][i] if results[0][0][i][0] > st else (0, 0, 0))\n                # opponent's checker in this cell means my defeat two times\n                factors.append(results[0][1][i] if results[0][1][i][0] > st else (0, 0, 0))\n                # if there are results of a cell one row above current\n                if len(results) > 1:\n                    # opponent's checker in cell one row above current means my defeat two times\n                    factors.append(results[1][1][i] if -results[1][1][i][0] > st else (0, 0, 0))\n                    # my checker in cell one row above current means my victory two times\n                    factors.append(results[1][0][i] if -results[1][0][i][0] > st else (0, 0, 0))\n                else:\n                    for j in range(2):\n                        factors.append((0, 0, 0))\n            else:\n                for j in range(2):\n                    factors.append((0, 0, 0))\n                for j in range(2):\n                    factors.append((0, 0, 0))\n            # consider only if there is no \"in air\" cells\n            if results[0][1][i][2] == 0:\n                # placing opponent's checker in this cell means opponent's victory\n                factors.append(results[0][1][i])\n            else:\n                factors.append((0, 0, 0))\n            # placing my checker in this cell means my victory\n            factors.append(results[0][0][i])\n            # central column priority\n            factors.append((1 if i == 1 and shift == 0 else 0, 0, 0))\n            # if there are results of a cell one row above current\n            if len(results) > 1:\n                # opponent's checker in cell one row above current means my defeat\n                factors.append(results[1][1][i])\n                # my checker in cell one row above current means my victory\n                factors.append(results[1][0][i])\n            else:\n                for j in range(2):\n                    factors.append((0, 0, 0))\n        # if there are results of a cell two rows above current\n        if len(results) > 2:\n            for i in range(conf.inarow - 2):\n                # my checker in cell two rows above current means my victory\n                factors.append(results[2][0][i])\n                # opponent's checker in cell two rows above current means my defeat\n                factors.append(results[2][1][i])\n        else:\n            for i in range(conf.inarow - 2):\n                for j in range(2):\n                    factors.append((0, 0, 0))\n        return factors\n\n\n    # define my mark and opponent's mark\n    my_mark = obs.mark\n    opp_mark = 2 if my_mark == 1 else 1\n    \n    # define board as two dimensional array\n    board = []\n    for column in range(conf.columns):\n        board.append([])\n        for row in range(conf.rows):\n            board[column].append(obs.board[conf.columns * row + column])\n    \n    best_cell = None\n    board_center = conf.columns // 2\n    inarow_m1 = conf.inarow - 1\n    \n    # standard amount of points\n    sp = 1\n    # \"seven\" pattern threshold points\n    st = 1\n    \n    # start searching for best_cell from board center\n    x = board_center\n    \n    # shift to right or left from board center\n    shift = 0\n    \n    # searching for best_cell\n    while x >= 0 and x < conf.columns:\n        # find first empty cell starting from bottom of the column\n        y = conf.rows - 1\n        while y >= 0 and board[x][y] != 0:\n            y -= 1\n        # if column is not full\n        if y >= 0:\n            # results of current cell and cells above it\n            results = []\n            results.append((get_results(x, y, my_mark, 1), get_results(x, y, opp_mark, 1)))\n            # if possible, get results of a cell one row above current\n            if (y - 1) >= 0:\n                results.append((get_results(x, y - 1, my_mark, -1), get_results(x, y - 1, opp_mark, -1)))\n            # if possible, get results of a cell two rows above current\n            if (y - 2) >= 0:\n                results.append((get_results(x, y - 2, my_mark, 1), get_results(x, y - 2, opp_mark, 1)))\n            \n            # list of factors represented by results\n            # ordered by priority from highest to lowest\n            factors = get_factors(results)\n\n            # if best_cell is not yet found\n            if best_cell is None:\n                best_cell = {\n                    \"column\": x,\n                    \"factors\": factors\n                }\n            # compare values of factors in best cell and current cell\n            else:\n                current_cell = {\n                    \"column\": x,\n                    \"factors\": factors\n                }\n                best_cell = get_best_cell(best_cell, current_cell)\n                        \n        # shift x to right or left from board center\n        if shift >= 0: shift += 1\n        shift *= -1\n        x = board_center + shift\n\n    # return index of the best cell column\n    return best_cell[\"column\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"env = ConnectX(pair=[rule_based,\"negamax\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"configure_hyper_parameters\"></a>\n# Configure hyper-parameters\n[Back to Table of Contents](#ToC)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"gamma = 0.99\ncopy_step = 25\nhidden_units = [128, 128, 128, 128, 128]\nmax_experiences = 50000\nmin_experiences = 100\nbatch_size = 32\nlr = 1e-2\nepsilon = 0.95\ndecay = 0.999\nmin_epsilon = 0.05\nepisodes = 60000\n\nprecision = 7","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"train_the_agent\"></a>\n# Train the agent\n[Back to Table of Contents](#ToC)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"num_states = env.observation_space.n + 1\nnum_actions = env.action_space.n\n\nall_total_rewards = np.empty(episodes)\nall_avg_rewards = np.empty(episodes) # Last 100 steps\nall_epsilons = np.empty(episodes)\n\n# Initialize models\nTrainNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\nTargetNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import copy\n\n\npbar = tqdm(range(episodes))\nfor n in pbar:\n    epsilon = max(min_epsilon, epsilon * decay)\n    total_reward = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n    all_total_rewards[n] = total_reward\n    avg_reward = all_total_rewards[max(0, n - 100):(n + 1)].mean()\n    all_avg_rewards[n] = avg_reward\n    all_epsilons[n] = epsilon\n\n    pbar.set_postfix({\n        'episode reward': total_reward,\n        'avg (100 last) reward': avg_reward,\n        'epsilon': epsilon\n    })\n\n    if n % 10000 == 0:\n        epsilon = 0.999\n        TrainNet_adversarial = copy.deepcopy(TrainNet)\n        env = ConnectX(switch_prob=0.5, pair=[\"negamax\", TrainNet_adversarial])\n        range_st = n//5000\n        range_ed = range_st + 5000\n        plt.plot(all_avg_rewards[range_st:range_ed])\n        plt.xlabel('Episode')\n        plt.ylabel('Avg rewards (100)')\n        plt.show()\n    if n % 10000 == 0:\n        epsilon = 0.999\n        TrainNet_adversarial = copy.deepcopy(TrainNet)\n        env = ConnectX(switch_prob=0.5, pair=[\"negamax\", rule_based])\n        range_st = n//5000\n        range_ed = range_st + 5000\n        plt.plot(all_avg_rewards[range_st:range_ed])\n        plt.xlabel('Episode')\n        plt.ylabel('Avg rewards (100)')\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#　上記繰り返す。","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_avg_rewards","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(all_avg_rewards)\nplt.xlabel('Episode')\nplt.ylabel('Avg rewards (100)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"TrainNet","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(all_epsilons)\nplt.xlabel('Episode')\nplt.ylabel('Epsilon')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"save_weights\"></a>\n# Save weights\n[Back to Table of Contents](#ToC)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"TrainNet.save_weights('./weights.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n    \"\"\"\n    Parameters\n    ----------\n    input_data : (データ数, チャンネル, 高さ, 幅)の4次元配列からなる入力データ\n    filter_h : フィルターの高さ\n    filter_w : フィルターの幅\n    stride : ストライド\n    pad : パディング\n    Returns\n    -------\n    col : 2次元配列\n    \"\"\"\n    N, C, H, W = input_data.shape\n    out_h = (H + 2*pad - filter_h)//stride + 1\n    out_w = (W + 2*pad - filter_w)//stride + 1\n\n    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n\n    for y in range(filter_h):\n        y_max = y + stride*out_h\n        for x in range(filter_w):\n            x_max = x + stride*out_w\n            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n\n    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n    return col\ndef relu(tensor):\n    return np.where(tensor>0, tensor,0 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# TrainNet.model.conv1.weight.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_h = 6; in_w = 7\n\nstride = 1;pad = 0; filter_h = 2;filter_w = 2\nboard = np.arange(42).reshape((1, 1, in_h, in_w))\nout_C = 16\nN, C, H, W = board.shape\nout_h = (H + 2*pad - filter_h)//stride + 1\nout_w = (W + 2*pad - filter_w)//stride + 1\noutput_cnn = np.zeros((N, out_C, out_h, out_w), dtype=np.float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# vectorized_board = im2col(board.reshape((1,1, in_h, in_w)),\n#           filter_h=filter_h, filter_w=filter_w, stride=stride, pad=pad) \n\n# conv_weight = TrainNet.model.conv1.weight[i].flatten().detach().numpy()\n# conv_bias = TrainNet.model.conv1.bias.flatten().detach().numpy()\n\n# for i in range(out_C):\n#     filter_ = conv_weight\n#     # 'filter_{} = np.array({}, dtype=np.float32)\\n'.format(i, filter_)\n#     \"out_elems_n = reshaped_board@filter_0.T\"\n#     # print(vectorized_board.shape, \":\", filter_0.detach().numpy().shape)\n#     vectorized_out = vectorized_board@filter_.reshape((-1, 1))\n#     vectorized_out = vectorized_out.reshape((out_h, out_w))\n#     output_cnn[:, i, :, :] = vectorized_out\n# output_cnn = output_cnn + conv_bias.reshape(1, out_C, 1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# tensor_board = torch.FloatTensor(board.reshape((1, 1, 6, -1)))\n\n# out = TrainNet.model.head.weight.detach().numpy()@output_cnn.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# out.argmax()\n# out # myagent関数の最終的なリターン","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# TrainNet.model.conv1(tensor_board).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# 再現\n# out = TrainNet.model.head.weight.detach().numpy()@output_cnn.reshape((-1, 1)) #+ out_bias.reshape(-1, 1)\n# out","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # Write hidden layers\n# for i, (w, b) in enumerate(fc_layers[:-1]):\n#     my_agent += '    hl{}_w = np.array({}, dtype=np.float32)\\n'.format(i+1, w)\n#     my_agent += '    hl{}_b = np.array({}, dtype=np.float32)\\n'.format(i+1, b)\n# # Write output layer\n# my_agent += '    ol_w = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][0])\n# my_agent += '    ol_b = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# print(my_agent)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"create_an_agent\"></a>\n# Create an agent\n[Back to Table of Contents](#ToC)\n\n- Let me remind the model as follows\n\n```py\nclass CNN_model(nn.Module):\n\n    def __init__(self, h, w, outputs):\n\n        super(CNN_model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=2, stride=1)\n\n        def conv2d_size_out(size, kernel_size = 2, stride = 1):\n            return (size - (kernel_size - 1) - 1) // stride  + 1\n        convw = conv2d_size_out(w)\n        convh = conv2d_size_out(h)\n        linear_input_size = convw * convh * 16\n        self.head = nn.Linear(linear_input_size, outputs)\n\n    # Called with either one element to determine next action, or a batch\n    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return self.head(x.view(x.size(0), -1))\n```","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"evaluate_the_agent\"></a>\n# Evaluate the agent\n[Back to Table of Contents](#ToC)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create the agent\nmy_agent = '''def my_agent(observation, configuration):\n    import numpy as np\n\n'''\nmy_agent += f'''    in_h = {in_h}; in_w = {in_w}; stride = {stride};pad = {pad}; filter_h = {filter_h};filter_w = {filter_w}\n    board = np.array(observation.board[:])\n    board = board.reshape((1, 1, in_h, in_w))\n    out_C = {out_C}\n    N, C, H, W = board.shape\n    out_h = (H + 2*pad - filter_h)//stride + 1\n    out_w = (W + 2*pad - filter_w)//stride + 1\n    output_cnn = np.zeros((N, out_C, out_h, out_w), dtype=np.float)\\n'''\n\nimport inspect \ninner_method = \"\"\n# print('    ' + inspect.getsource(im2col))\nmethod_list = [im2col, relu]\nfor method in method_list:\n    for line in inspect.getsource(method).split('\\n'):\n        inner_method += \"    \" + line + \"\\n\"\nmy_agent += inner_method\n\n#f\"np.array({TrainNet.model.conv1.weight[i].flatten().detach().tolist()}, dtype=np.float32)\\n\"\n\nmy_agent += '''    vectorized_board = im2col(board.reshape((1,1, in_h, in_w)),\n          filter_h=filter_h, filter_w=filter_w, stride=stride, pad=pad)\\n'''\n\n\n\n# Ongoing development 6/13\n#ここを numpy.arrayとして変数conv_weightに渡してあげる\n#　複数CNN層対応できるようにする...TBD\nmy_agent += f\"    conv_weight = np.array({TrainNet.model.conv1.weight.detach().tolist()}, dtype=np.float32)\\n\"\nmy_agent += f\"    conv_bias = np.array({TrainNet.model.conv1.bias.flatten().detach().tolist()}, dtype=np.float32)\\n\"\n\n\nmy_agent += \"\"\"    for i in range(out_C):\n        filter_ = conv_weight[i].flatten()\n        vectorized_out = vectorized_board@filter_.reshape((-1, 1))\n        vectorized_out = vectorized_out.reshape((out_h, out_w))\n        output_cnn[:, i, :, :] = vectorized_out\n\n    output_cnn = output_cnn + conv_bias.reshape((1, out_C, 1, 1))\n    output_cnn = relu(output_cnn)\n\"\"\"\n\n\n\nmy_agent += f\"    out_weight = np.array({TrainNet.model.head.weight.detach().tolist()}, dtype=np.float32)\\n\"\nmy_agent += f\"    out_bias = np.array({TrainNet.model.head.bias.detach().tolist()}, dtype=np.float32)\\n\"\n\nmy_agent += \"\"\"    out = out_weight@output_cnn.reshape((-1, 1)) + out_bias.reshape((-1, 1))\\n\"\"\"\n# 各列一番上のセルが空いていなかった場合、そこは選ばないようにする。\nmy_agent += '''\n    for i in range(configuration.columns):\n        if observation.board[i] != 0:\n            out[i] = -1e7\n    return int(np.argmax(out))\n    '''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"with open('submission.py', 'w') as f:\n    f.write(my_agent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from submission import my_agent","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\n\n# Run multiple episodes to estimate agent's performance.\nprint(\"My Agent vs. Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs. Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))\nprint(\"Random Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"random\", my_agent], num_episodes=10)))\nprint(\"Negamax Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", my_agent], num_episodes=10)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# evaluate(\"connectx\", [\"negamax\", \"random\"], num_episodes=10)\n# print(inspect.getsource(evaluate))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# env.run([\"random\", \"negamax\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}