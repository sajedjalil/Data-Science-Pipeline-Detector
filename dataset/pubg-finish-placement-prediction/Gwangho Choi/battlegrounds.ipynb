{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 배틀그라운드 게임 빅데이터와 AI를 활용한 유저 순위 예측 알고리즘 개발\n\n\n## 개요\n\n- PUBG 배틀그라운드의 유저 행동 데이터로 게임 경기에서의 유저의 순위를 예측합니다.\n\n\n## 주최/주관\n\n- 주최 : Kaggle\n\n\n## 데이터 분석\n#### Table of contents\n* [Loading Data](#1) \n* [Initial Exploration](#2) \n* [Data Cleansing](#3) \n* [Feature engineering](#4) \n* [Outlier Detection for PUBG Cheaters](#5) \n* [Data Preparation (Data Cleansing + Feature Engineering)](#6)\n* [Data Partitioning](#7)\n* [Modeling - light Gradient Boosting Machine (LGBM)](#8)\n* [Feature Importance](#9)\n* [Feature selection](#10)\n* [Modeling - Multi Layer Perceptrons (MLP)](#11)\n* [Performance evaluation of MLP](#12)\n* [Submit Test Results](#13)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standard libraries\nimport os\nimport numpy as np\nimport random\nimport pandas as pd\nimport time\nimport gc # memory\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pdpbox import pdp\nfrom plotnine import *\n\n# Pre-processing\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n# Correlation\nfrom scipy.cluster import hierarchy as hc # dendrogram\n\n# Model\nfrom sklearn.ensemble import RandomForestRegressor\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.losses import mse, binary_crossentropy\nfrom keras import optimizers, regularizers\nfrom keras.layers import Input, Dense, Lambda\nfrom keras.models import Sequential, Model, load_model \nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n# Evaluate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn import metrics\nfrom sklearn.tree import export_graphviz\n\n# For notebook plotting\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.float_format', '{:.2f}'.format)\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 300)\n# pd.reset_option('display.float_format')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\nimport tensorflow as tf\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10, verbose=0):\n    ''' Wrapper function to create a LearningRateScheduler with step decay schedule. '''\n    def schedule(epoch):\n        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n    \n    return tf.keras.callbacks.LearningRateScheduler(schedule, verbose)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                #if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                #    df[col] = df[col].astype(np.float16)\n                #el\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        #else:\n            #df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB --> {:.2f} MB (Decreased by {:.1f}%)'.format(\n        start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Loading Data <a id=\"1\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def reload():\n    gc.collect()\n    df = pd.read_csv('/kaggle/input/pubg-finish-placement-prediction/train_V2.csv')\n    invalid_match_ids = df[df['winPlacePerc'].isna()]['matchId'].values\n    df = df[-df['matchId'].isin(invalid_match_ids)]\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/pubg-finish-placement-prediction/train_V2.csv')\ntrain = reduce_mem_usage(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Initial Exploration <a id=\"2\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary Statistics of the training data.\nround(train.describe(include=np.number).drop('count').T,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in ['Id','groupId','matchId']:\n    print(f'unique [{c}] count:', train[c].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,7))\nsns.barplot(x=train['matchType'].unique(),\n            y=train['matchType'].value_counts())\ntrain['matchType'].value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Continuous & Discrete variable \n\n# group id: team id // solo의 경우 각자 group id 할당\n# matchid: 각 게임의 고유값\ndisplay(list(train.columns[train.dtypes != 'object']))\ndisplay(list(train.columns[train.dtypes == 'object']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Data Cleansing <a id=\"3\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Handle Missing value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.dropna(axis='rows')\ndisplay(train.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handle Duplicate value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# display(train[train.duplicated()].count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handle Outlier value\nThe players could be cheaters, maniacs or just anomalies. Removing these outliers will likely improve results.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def delete_cheaters(df):\n    ### Anomalies in roadKills ### \n    # Drop roadKill 'cheaters'\n    df.drop(df[df['roadKills'] >= 10].index, inplace=True)\n\n    ### Anomalies in aim 1 (More than 50 kills) ### \n    df.drop(df[df['kills'] >= 50].index, inplace=True)\n\n    ### Anomalies in aim 2 (100% headshot rate) ### \n    #df['headshot_rate'] = df['headshotKills'] / df['kills']\n    #df['headshot_rate'] = df['headshot_rate'].fillna(0)   \n\n    ### Anomalies in aim 3 (Longest kill) ### \n    df.drop(df[df['longestKill'] >= 1000].index, inplace=True)\n\n    ### Anomalies in movement ### \n    # walkDistance anomalies \n    df.drop(df[df['walkDistance'] >= 13000].index, inplace=True)\n\n    # rideDistance anomalies \n    df.drop(df[df['rideDistance'] >= 25000].index, inplace=True)\n\n    # swimDistance\n    df.drop(df[df['swimDistance'] >= 1500].index, inplace=True)\n\n    ### Anomalies in supplies 1 (weaponsAcquired) ### \n    df.drop(df[df['weaponsAcquired'] >= 50].index, inplace=True)\n\n    ### Anomalies in supplies 2 (heals) ###\n    # Remove outliers\n    df.drop(df[df['heals'] >= 40].index, inplace=True)\n    \n    ## ETC ## \n    # drop savage killer (kill streak > 10)\n    df = df.drop(df[df['killStreaks'] >= 10].index)\n    \n    df.drop(df[(df['walkDistance']<=10.0) & (df['damageDealt'] >= 1000)].index, inplace=True)\n    df.drop(df[(df['walkDistance']<=10.0) & (df['kills'] >= 5)].index, inplace=True)\n    df.drop(df[(df['walkDistance']<=10.0) & (df['heals'] >= 5)].index, inplace=True)\n    df.drop(df[(df['walkDistance']<=10.0) & (df['headshotKills'] >= 5)].index, inplace=True)\n    df.drop(df[(df['walkDistance']<=10.0) & (df['headshotKills'] >= 5)].index, inplace=True)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Feature engineering <a id=\"4\"></a> ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def top_k_corr(data, k):\n    #f, ax = plt.subplot(figsize=(7,7))\n    plt.figure(figsize=(7,7))\n    cols = data.corr().nlargest(5, 'winPlacePerc').index\n    sns.heatmap(data[cols].corr(), annot=True, fmt='.2f',cbar=True,\n                yticklabels=cols.values, xticklabels=cols.values,\n                linecolor='white', linewidths=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(15,15))\ncol = train.columns[train.dtypes != 'object']\ndf_corr = train[col].corr()\nsns.heatmap(df_corr, annot=True, fmt='.2f', cbar=True, \n           xticklabels=train[col].columns.values, yticklabels=train[col].columns.values,\n           linecolor='white', linewidths=0.1)\ntop_k_corr(train, 5)           ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_engineering(df):\n    # New feature\n    df['heals_and_boosts'] = df['heals']+df['boosts']\n    df['total_distance'] = df['walkDistance']+df['rideDistance']+df['swimDistance']\n    df['kills_over_walkDistance'] = df['kills'] / df['walkDistance']\n    df['killPlace_over_maxPlace'] = df['killPlace'] / df['maxPlace']\n    df['players_joined'] = df.groupby('matchId')['matchId'].transform('count')\n    df['players_in_team'] = df.groupby('groupId')['matchId'].transform('count')\n    \n    # Drop feature (low correlation with winPlacePerc or similar feature)\n    df.drop(['boosts','heals'], axis=1, inplace=True)\n    df.drop(['rideDistance','swimDistance','matchDuration'], axis=1, inplace=True)\n    df.drop(['rankPoints','killPoints','winPoints'], axis=1, inplace=True)\n    df.drop(['headshotKills','roadKills','vehicleDestroys','teamKills'], axis=1, inplace=True)\n    \n    # Rank as percent\n    match = df.groupby('matchId')\n    df['killsPerc'] = match['kills'].rank(pct=True).values\n    df['killPlacePerc'] = match['killPlace'].rank(pct=True).values\n    df['walkDistancePerc'] = match['walkDistance'].rank(pct=True).values\n    df['walkPerc_killsPerc'] = df['walkDistancePerc'] / df['killsPerc']\n\n    check_cols = ['kills_over_walkDistance', 'killPlace_over_maxPlace', 'walkPerc_killsPerc']\n    df[df == np.Inf] = np.NaN\n    df[df == np.NINF] = np.NaN # - Inf\n    for c in check_cols: df[c].fillna(0, inplace=True)\n    \n    numcols = df.select_dtypes(include='number').columns\n    use_cols = numcols[numcols != 'winPlacePerc']\n    \n    rank_cols = use_cols.drop(['numGroups', 'maxPlace', 'players_joined', 'players_in_team'])\n    \n    stat_cols = use_cols.drop(['numGroups', 'maxPlace', 'players_joined', 'players_in_team',\n                               'assists', 'longestKill', 'weaponsAcquired','heals_and_boosts', \n                               'total_distance', 'kills_over_walkDistance'])\n    \n    group = df.groupby(['matchId', 'groupId'])\n    \n    # Median_by_team\n    df = df.merge(group[stat_cols].median(), suffixes=['', '_team_median'], how='left', \n                  on=['matchId', 'groupId'])\n    df = reduce_mem_usage(df)\n    \n    # Max_by_team\n    df = df.merge(group[stat_cols].max(), suffixes=['', '_team_max'], how='left', \n                  on=['matchId', 'groupId'])\n    df = reduce_mem_usage(df)\n    \n    # Min_by_team \n    df = df.merge(group[stat_cols].min(), suffixes=['', '_team_min'], how='left', \n                  on=['matchId', 'groupId'])\n    df = reduce_mem_usage(df)\n    \n    # Mean_by_team\n    df = df.merge(group[stat_cols].mean(), suffixes=['', '_team_mean'], how='left', \n                  on=['matchId', 'groupId'])\n    df = reduce_mem_usage(df)\n    \n    # Sum_by_team\n    df = df.merge(group[stat_cols].sum(), suffixes=['', '_team_sum'], how='left', \n                  on=['matchId', 'groupId'])\n    df = reduce_mem_usage(df)\n    \n    # Rank_by_team \n    df = df.merge(group[rank_cols].mean().groupby('matchId')[rank_cols].rank(pct=True), \n                  suffixes=['', '_team_mean_rank'], how='left', on=['matchId', 'groupId'])\n    df = reduce_mem_usage(df)\n    \n    # One hot encode matchType\n    mapper = lambda x: 'solo' if 'solo' in x else 'duo' if ('duo' in x) or ('crash' in x) else 'squad'\n    df['matchType'] = df['matchType'].apply(mapper)\n    df = pd.get_dummies(df, columns=['matchType'])\n    \n    df.drop(['Id', 'matchId','groupId'], axis=1, inplace=True)\n    \n    # drop constant column\n    constant_column = [col for col in df.columns if df[col].nunique() == 1]\n    print('drop constant columns:', constant_column)\n    df.drop(constant_column, axis=1, inplace=True)\n    \n    assert df.isna().sum().sum() == 0\n\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Outlier Detection for PUBG Cheaters <a id=\"5\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\ndef isolation_forest(df, contamination):\n    cols = list(df.columns[df.dtypes != 'object'])\n    train_df = df[cols]   \n    \n    # max_samples: 각 estimators를 학습시키는데 사용하는 샘플 수 \n    # contamination: 데이터셋의 오염 정도 \n    # max_features: outlier 측정에 사용할 변수의 수 \n    outlier_detect = IsolationForest(n_estimators=500, \n                                     contamination=contamination, \n                                     max_features=train_df.shape[1])\n    outlier_detect.fit(train_df) \n    outliers_predicted = outlier_detect.predict(train_df)\n    \n    df['outlier'] = outliers_predicted\n    df = df[df['outlier'] == 1]\n    \n    return df ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def delete_outlier(y_pred, y_true, remain=0.99):\n    \n    mse_array = np.square(np.subtract(y_pred, y_true)).mean(axis=1)\n    mse_series = pd.Series(mse_array)\n    \n    check_value = mse_series.quantile(remain)\n    check_outlier = np.where(mse_array <= check_value, 1, -1)\n    \n    return check_outlier, mse_series\n\ndef sampling(args):\n    \"\"\"\n    Reparameterization trick by sampling from an isotropic unit Gaussian.\n\n    # Arguments\n        args (tensor): mean and log of variance of Q(z|X)\n\n    # Returns\n        z (tensor): sampled latent vector\n    \"\"\"\n\n    z_mean, z_log_var = args\n    batch = tf.keras.backend.shape(z_mean)[0]\n    dim = tf.keras.backend.int_shape(z_mean)[1]\n    # by default, random_normal has mean = 0 and std = 1.0\n    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n    return z_mean + tf.keras.backend.exp(0.5 * z_log_var) * epsilon\n\ndef VAE_model(inputs, origin_dim):\n    # network parameters\n    latent_dim = 2\n    origin_half_dim = origin_dim//2\n\n    # VAE model = encoder + decoder\n    # build encoder model\n \n    x = tf.keras.layers.Dense(origin_half_dim, activation='relu')(inputs)\n    z_mean = tf.keras.layers.Dense(latent_dim, name='z_mean')(x)\n    z_log_var = tf.keras.layers.Dense(latent_dim, name='z_log_var')(x)\n\n    # use reparameterization trick to push the sampling out as input\n    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n    z = tf.keras.layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n    # instantiate encoder model\n    encoder = tf.keras.Model(inputs, [z_mean, z_log_var, z], name='encoder')\n\n    # build decoder model\n    latent_inputs = tf.keras.Input(shape=(latent_dim,), name='z_sampling')\n    x = tf.keras.layers.Dense(origin_half_dim, activation='relu')(latent_inputs)\n    outputs = tf.keras.layers.Dense(origin_dim, activation='sigmoid')(x)\n\n    # instantiate decoder model\n    decoder = tf.keras.Model(latent_inputs, outputs, name='decoder')\n\n    # instantiate VAE model\n    outputs = decoder(encoder(inputs)[2])\n    vae = tf.keras.Model(inputs, outputs, name='vae_mlp')\n\n    # VAE loss = mse_loss + kl_loss\n    reconstruction_loss = tf.keras.losses.MSE(inputs, outputs)\n\n    reconstruction_loss *= origin_dim\n    kl_loss = 1 + z_log_var - tf.keras.backend.square(z_mean) - tf.keras.backend.exp(z_log_var)\n    kl_loss = tf.keras.backend.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = tf.keras.backend.mean(reconstruction_loss + kl_loss)\n    vae.add_loss(vae_loss)\n    \n    encoder.summary()\n    decoder.summary()\n\n    return vae","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def VAE(df, remain_ratio):\n    np.random.seed(0)\n    tf.random.set_seed(0)\n    \n    lr_sched = step_decay_schedule(initial_lr=0.001, decay_factor=0.97, step_size=1, verbose=0)\n    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n    \n    cols = list(df.columns[df.dtypes != 'object'])\n    train_df = df[cols]   \n    input_shape = (train_df.shape[1], ) \n    origin_dim = train_df.shape[1]\n    inputs = tf.keras.Input(shape = input_shape)\n    \n    # training parameters\n    epochs = 300\n    batch_size = 20480  # 8  * strategy.num_replicas_in_sync * 16 * 20\n    validation_split = 0.2\n    steps = len(train_df) * (1-validation_split) // batch_size\n    optimizer = tf.keras.optimizers.Adam()\n    callbacks_list = [lr_sched, early_stopping]\n\n    # Scaling\n    scaler = StandardScaler()                                  \n    x_train = scaler.fit_transform(train_df.astype(float))\n\n    # VAE\n    with strategy.scope():\n        model = VAE_model(inputs, origin_dim)\n        model.compile(optimizer=optimizer)\n\n        model.fit( \n                x_train.astype(np.float32),\n                epochs=epochs,\n                batch_size=batch_size,\n                validation_split=validation_split,\n                steps_per_epoch=steps,\n                callbacks=callbacks_list,\n                verbose=0)\n        \n        print('\\nVAE: Predict cheaters')\n        y_pred = model.predict(x_train.astype(np.float32), verbose=1, batch_size=batch_size, \n                      workers=strategy.num_replicas_in_sync, use_multiprocessing=True)\n        \n        print('VAE: Check cheaters')\n        # outlier: 1 (normal), -1 (cheater)\n        df['outlier'], mse_serise = delete_outlier(y_pred, y_true=x_train, remain=remain_ratio) \n        df = df[df['outlier'] == 1]\n        df.drop(columns='outlier', inplace=True)\n        \n        return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def std_n_sigma(df, n, filter_):\n    cols = list(df.columns[df.dtypes != 'object'])\n    filter_ = list(df[filter_]) \n    df = df[cols]   \n\n    scaler = StandardScaler()\n    std_array = scaler.fit_transform(df.astype(float))\n    std_df = pd.DataFrame(std_array, columns=df.columns, index=filter_)\n\n    # delete outlier  \n    remove_idx = set([])\n    for col in std_df.columns:\n        remove_idx.update(std_df[(std_df[col] < -n) | (n < std_df[col])].index)\n                        \n    print('PUBG cheaters: {}%'.format(round(len(remove_idx) / len(df) * 100,2)))\n    return remove_idx  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Data Preparation (Data Cleansing + Feature Engineering) <a id=\"6\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_preparation(df, train_flag=True):\n    origin_size = df.shape[0]\n    \n    if train_flag:\n        ## Delete cheaters ## \n        print(\"Delete PUBG Cheaters \\n\")\n        \n        # Experience-based\n        # df = delete_cheaters(df)\n\n        # Standadization\n        # remove_id = std_n_sigma(df, 3.5, 'Id')\n        # df = df[-df.Id.isin(remove_id)]\n\n        # Isolation forest\n        #contamination = 0.001\n        #df = isolation_forest(df, contamination)\n        \n        # VAE\n        df = VAE(df, remain_ratio=0.998)\n        \n        print('PUBG cheaters: {0}, {1}% \\n'.format(origin_size -len(df), \n                                                round(100 - 100 * len(df) / origin_size,4))) \n    else:\n        pass\n    \n    print(\"Feature engineering\")\n    df = feature_engineering(df)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ = data_preparation(train)\ntrain_.head()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Data Partitioning <a id=\"7\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Debug mode ## \n# sample = 1000000\n# df_sample = train.sample(sample, random_state=0)\n# df_sample = reduce_mem_usage(df_sample)\n\ntrain_data = train_.drop(columns = ['winPlacePerc'])\ntrain_labels = train_['winPlacePerc']\n\ntrain_x, val_x, train_y, val_y = train_test_split(train_data, train_labels, test_size=0.1, random_state=0)\nprint(train_x.shape, train_y.shape, val_y.shape, val_x.shape)\n\ndel train_, train_data, train_labels\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Modeling - light Gradient Boosting Machine (LGBM) <a id=\"8\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMRegressor\nparams = {\n    'n_estimators': 300,\n    'learning_rate': 0.3, \n    'num_leaves': 20,\n    'objective': 'regression_l2', \n    'metric': 'mae',\n    'verbose': -1,\n}\n\nmodel = LGBMRegressor(**params)\nmodel.fit(\n    train_x, train_y,\n    eval_set=[(val_x, val_y)],\n    eval_metric='mae',\n    verbose=20,\n)\n\nfeature_importance = pd.DataFrame(sorted(zip(model.feature_importances_, train_x.columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_importance.sort_values(by=\"Value\", ascending=False)[:20])\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9. Feature Importance <a id=\"9\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy\n\n# Keep only significant features\nto_keep = feature_importance.sort_values(by='Value', ascending=False)[:50].Feature\n\n## Create a Dendrogram to view highly correlated features\ncorr = np.round(scipy.stats.spearmanr(train_x[to_keep]).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(14,20))\ndendrogram = hc.dendrogram(z, labels=train_x[to_keep].columns, orientation='left', leaf_font_size=16)\nplt.plot()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 10. Feature selection <a id=\"10\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"use_features = 80\nim_features = feature_importance.sort_values(by='Value', ascending=False)[:use_features].Feature \n\n## Scaling ##\nscaler = StandardScaler()\nX_train = scaler.fit_transform(train_x[im_features].astype(np.float32))\nY_train = train_y.values\nX_val = scaler.fit_transform(val_x[im_features].astype(np.float32))\nY_val = val_y.values\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 11. Modeling - Multi Layer Perceptrons (MLP) <a id=\"11\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import optimizers, regularizers\nfrom keras.models import Sequential, Model, load_model \nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.layers import Input, Dense, Lambda\nfrom keras.losses import mse, binary_crossentropy\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.losses import mse, binary_crossentropy\nfrom sklearn import preprocessing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model():\n    hidden_layer = tf.keras.layers.Dense(2048, kernel_initializer='he_normal', activation='relu')(inputs)\n    hidden_layer = tf.keras.layers.BatchNormalization()(hidden_layer)\n    hidden_layer = tf.keras.layers.Dropout(rate=0.1, seed=1234)(hidden_layer)\n    \n    hidden_layer = tf.keras.layers.Dense(1024, kernel_initializer='he_normal', activation='relu')(hidden_layer)\n    hidden_layer = tf.keras.layers.BatchNormalization()(hidden_layer)\n    hidden_layer = tf.keras.layers.Dropout(rate=0.1, seed=1234)(hidden_layer)\n    \n    hidden_layer = tf.keras.layers.Dense(512, kernel_initializer='he_normal', activation='relu')(hidden_layer)\n    hidden_layer = tf.keras.layers.BatchNormalization()(hidden_layer)\n    \n    hidden_layer = tf.keras.layers.Dense(256, kernel_initializer='he_normal', activation='relu')(hidden_layer)\n    hidden_layer = tf.keras.layers.BatchNormalization()(hidden_layer)\n    \n    hidden_layer = tf.keras.layers.Dense(128, kernel_initializer='he_normal', activation='relu')(hidden_layer)\n    hidden_layer = tf.keras.layers.BatchNormalization()(hidden_layer)\n    \n    hidden_layer = tf.keras.layers.Dense(64, kernel_initializer='he_normal', activation='relu')(hidden_layer)\n    hidden_layer = tf.keras.layers.BatchNormalization()(hidden_layer)\n    \n    prediction = tf.keras.layers.Dense(1, kernel_initializer='normal', activation='sigmoid')(hidden_layer) \n    \n    return prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(0)\ntf.random.set_seed(0)\n\n# Train parameters\nepochs = 500\nbatch_size = 20480 # 8  * strategy.num_replicas_in_sync * 16 * 20\nsteps = len(X_train) // batch_size\noptimizer = tf.keras.optimizers.Adam()\nlr_sched = step_decay_schedule(initial_lr=0.001, decay_factor=0.97, step_size=1, verbose=0)\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_mae', mode='min', patience=10, verbose=1)\n\ncallbacks_list = [lr_sched, early_stopping]\n\n# Deep learning model\nwith strategy.scope():\n    inputs = tf.keras.Input(shape=(X_train.shape[1],))\n    predictions = model()\n    model = tf.keras.Model(inputs=inputs, outputs=predictions)\n    model.compile(optimizer=optimizer, loss='mae', metrics=['mae']) # loss='mae' or 'mse'\n\n    history = model.fit(\n            X_train.astype(np.float32), Y_train.astype(np.float32),\n            shuffle=True,\n            epochs=epochs,\n            batch_size=batch_size,\n            validation_data = (X_val.astype(np.float32), Y_val.astype(np.float32)),\n            steps_per_epoch=steps,\n            callbacks=callbacks_list,\n            verbose=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 12. Performance evaluation of MLP <a id=\"12\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training & validation loss values\nplt.figure(figsize=(7,5))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation mae values\nplt.figure(figsize=(7,5))\nplt.plot(history.history['mae'])\nplt.plot(history.history['val_mae'])\nplt.title('Mean Abosulte Error')\nplt.ylabel('Mean absolute error')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_train, Y_train, X_val, Y_val\ngc.collect()    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 13. Submit Test Results <a id=\"13\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Scaling ##\nscaler = StandardScaler()\nX_train = scaler.fit_transform(train_x[im_features].astype(np.float32))\nY_train = train_y.values\nX_val = scaler.fit_transform(val_x[im_features].astype(np.float32))\nY_val = val_y.values\n\ntest = pd.read_csv('/kaggle/input/pubg-finish-placement-prediction/test_V2.csv')\ntest = reduce_mem_usage(test)\n\npre_test = data_preparation(test, train_flag=False)\nX_test = scaler.fit_transform(pre_test[im_features].astype(np.float32))\n\n# Predict using DNN\npred = model.predict(X_test.astype(np.float32)) # .ravel()\nprint('Prediction values range: {0} ~ {1}'.format(pred.min(), pred.max()))\n\ntest['winPlacePerc'] = pred\nsubmission = test[['Id', 'winPlacePerc']]\nsubmission.to_csv('submission.csv', index=False)\n\n# Last check of submission\nprint('Head of submission: ')\ndisplay(submission.head())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}