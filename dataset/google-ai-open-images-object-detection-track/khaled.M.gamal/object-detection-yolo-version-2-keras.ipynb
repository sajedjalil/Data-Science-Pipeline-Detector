{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Object Detection using YOLO version_2 and keras with tensorflow 1.13.1 as backend  \n**\n\n* YOLO website https://pjreddie.com/darknet/yolo/\n* YOLO paper :https://arxiv.org/abs/1612.08242\n* Inspiration from professor Andrew NG deep learning course on coursera \n* link to video of the course :https://www.youtube.com/watch?v=5e5pjeojznk&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=26\n* I used pretrained model of yolo in h5 format and loaded the model using keras i used YAD2K repository to   convert YOLO Darknet model cfg and weights to keras model \n* link to YAD2k repository :https://github.com/allanzelener/YAD2K\n* link to the keras model after conversion that i use in this notebook : https://www.kaggle.com/khaledmgamal/yolo-version-2-h5\n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First downgrade to downgrade to tensorflow 1.13.1 because the default is 2.0.0 and my implementation is not compatible with 2.0.0"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tensorflow==1.13.1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"check tensorflow version "},{"metadata":{"trusted":true},"cell_type":"code","source":"!python3 -c 'import tensorflow as tf; print(tf.__version__)'\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The functions that help in drawing bounding boxes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import argparse\nimport os\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport scipy.io\nimport scipy.misc\nimport numpy as np\nimport pandas as pd\nimport PIL\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.layers import Input, Lambda, Conv2D\nfrom keras.models import load_model, Model\nimport colorsys\nimport imghdr\nimport os\nimport random\nfrom keras import backend as K\n\nimport numpy as np\nfrom PIL import Image, ImageDraw, ImageFont\n\ndef read_classes(classes_path):\n    with open(classes_path) as f:\n        class_names = f.readlines()\n    class_names = [c.strip() for c in class_names]\n    return class_names\n\ndef read_anchors(anchors_path):\n    with open(anchors_path) as f:\n        anchors = f.readline()\n        anchors = [float(x) for x in anchors.split(',')]\n        anchors = np.array(anchors).reshape(-1, 2)\n    return anchors\n\ndef generate_colors(class_names):\n    hsv_tuples = [(x / len(class_names), 1., 1.) for x in range(len(class_names))]\n    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n    random.seed(10101)  # Fixed seed for consistent colors across runs.\n    random.shuffle(colors)  # Shuffle colors to decorrelate adjacent classes.\n    random.seed(None)  # Reset seed to default.\n    return colors\n\ndef scale_boxes(boxes, image_shape):\n    \"\"\" Scales the predicted boxes in order to be drawable on the image\"\"\"\n    height = image_shape[0]\n    width = image_shape[1]\n    image_dims = K.stack([height, width, height, width])\n    image_dims = K.reshape(image_dims, [1, 4])\n    boxes = boxes * image_dims\n    return boxes\n\ndef preprocess_image(img_path, model_image_size):\n    image_type = imghdr.what(img_path)\n    image = Image.open(img_path)\n    resized_image = image.resize(tuple(reversed(model_image_size)), Image.BICUBIC)\n    image_data = np.array(resized_image, dtype='float32')\n    image_data /= 255.\n    image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\n    return image, image_data\n\ndef draw_boxes(image, out_scores, out_boxes, out_classes, class_names, colors):\n    #font_path = '/home/khaled/Jupiter_notebook/Car detection for Autonomous Driving/arial.ttf'\n    #assert os.path.isfile(font_path)\n    font = ImageFont.truetype(font='/kaggle/input/fontdata/FiraMono-Medium.otf',size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))\n    thickness = (image.size[0] + image.size[1]) // 300\n\n    for i, c in reversed(list(enumerate(out_classes))):\n        predicted_class = class_names[c]\n        box = out_boxes[i]\n        score = out_scores[i]\n\n        label = '{} {:.2f}'.format(predicted_class, score)\n\n        draw = ImageDraw.Draw(image)\n        label_size = draw.textsize(label, font)\n\n        top, left, bottom, right = box\n        top = max(0, np.floor(top + 0.5).astype('int32'))\n        left = max(0, np.floor(left + 0.5).astype('int32'))\n        bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))\n        right = min(image.size[0], np.floor(right + 0.5).astype('int32'))\n        print(label, (left, top), (right, bottom))\n\n        if top - label_size[1] >= 0:\n            text_origin = np.array([left, top - label_size[1]])\n        else:\n            text_origin = np.array([left, top + 1])\n\n        # My kingdom for a good redistributable image drawing library.\n        for i in range(thickness):\n            draw.rectangle([left + i, top + i, right - i, bottom - i], outline=colors[c])\n        draw.rectangle([tuple(text_origin), tuple(text_origin + label_size)], fill=colors[c])\n        draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n        del draw","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The functions responsible of processing the the output of the Yolo model to create bounding boxes , classes and scores  "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\ndef yolo_head(feats, anchors, num_classes):\n    \"\"\"Convert final layer features to bounding box parameters.\n\n    Parameters\n    ----------\n    feats : tensor\n        Final convolutional layer features.\n    anchors : array-like\n        Anchor box widths and heights.\n    num_classes : int\n        Number of target classes.\n\n    Returns\n    -------\n    box_xy : tensor\n        x, y box predictions adjusted by spatial location in conv layer.\n    box_wh : tensor\n        w, h box predictions adjusted by anchors and conv spatial resolution.\n    box_conf : tensor\n        Probability estimate for whether each box contains any object.\n    box_class_pred : tensor\n        Probability distribution estimate for each box over class labels.\n    \"\"\"\n    num_anchors = len(anchors)\n    # Reshape to batch, height, width, num_anchors, box_params.\n    anchors_tensor = K.reshape(K.variable(anchors), [1, 1, 1, num_anchors, 2])\n    # Static implementation for fixed models.\n    # TODO: Remove or add option for static implementation.\n    # _, conv_height, conv_width, _ = K.int_shape(feats)\n    # conv_dims = K.variable([conv_width, conv_height])\n\n    # Dynamic implementation of conv dims for fully convolutional model.\n    conv_dims = K.shape(feats)[1:3]  # assuming channels last\n    # In YOLO the height index is the inner most iteration.\n    conv_height_index = K.arange(0, stop=conv_dims[0])\n    conv_width_index = K.arange(0, stop=conv_dims[1])\n    conv_height_index = K.tile(conv_height_index, [conv_dims[1]])\n\n    # TODO: Repeat_elements and tf.split doesn't support dynamic splits.\n    # conv_width_index = K.repeat_elements(conv_width_index, conv_dims[1], axis=0)\n    conv_width_index = K.tile(K.expand_dims(conv_width_index, 0), [conv_dims[0], 1])\n    conv_width_index = K.flatten(K.transpose(conv_width_index))\n    conv_index = K.transpose(K.stack([conv_height_index, conv_width_index]))\n    conv_index = K.reshape(conv_index, [1, conv_dims[0], conv_dims[1], 1, 2])\n    conv_index = K.cast(conv_index, K.dtype(feats))\n    \n    feats = K.reshape(feats, [-1, conv_dims[0], conv_dims[1], num_anchors, num_classes + 5])\n    conv_dims = K.cast(K.reshape(conv_dims, [1, 1, 1, 1, 2]), K.dtype(feats))\n\n    # Static generation of conv_index:\n    # conv_index = np.array([_ for _ in np.ndindex(conv_width, conv_height)])\n    # conv_index = conv_index[:, [1, 0]]  # swap columns for YOLO ordering.\n    # conv_index = K.variable(\n    #     conv_index.reshape(1, conv_height, conv_width, 1, 2))\n    # feats = Reshape(\n    #     (conv_dims[0], conv_dims[1], num_anchors, num_classes + 5))(feats)\n\n    box_confidence = K.sigmoid(feats[..., 4:5])\n    box_xy = K.sigmoid(feats[..., :2])\n    box_wh = K.exp(feats[..., 2:4])\n    box_class_probs = K.softmax(feats[..., 5:])\n\n    # Adjust preditions to each spatial grid point and anchor size.\n    # Note: YOLO iterates over height index before width index.\n    box_xy = (box_xy + conv_index) / conv_dims\n    box_wh = box_wh * anchors_tensor / conv_dims\n\n    return box_confidence, box_xy, box_wh, box_class_probs\n\n\ndef yolo_boxes_to_corners(box_xy, box_wh):\n    \"\"\"Convert YOLO box predictions to bounding box corners.\"\"\"\n    box_mins = box_xy - (box_wh / 2.)\n    box_maxes = box_xy + (box_wh / 2.)\n\n    return K.concatenate([\n        box_mins[..., 1:2],  # y_min\n        box_mins[..., 0:1],  # x_min\n        box_maxes[..., 1:2],  # y_max\n        box_maxes[..., 0:1]  # x_max\n    ])\n\n\ndef yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold=.6):\n    \"\"\"Filter YOLO boxes based on object and class confidence.\"\"\"\n\n    box_scores = box_confidence * box_class_probs\n    box_classes = K.argmax(box_scores, axis=-1)\n    box_class_scores = K.max(box_scores, axis=-1)\n    prediction_mask = box_class_scores >= threshold\n\n    # TODO: Expose tf.boolean_mask to Keras backend?\n    boxes = tf.boolean_mask(boxes, prediction_mask)\n    scores = tf.boolean_mask(box_class_scores, prediction_mask)\n    classes = tf.boolean_mask(box_classes, prediction_mask)\n\n    return boxes, scores, classes\n\n\ndef yolo_eval(yolo_outputs,\n              image_shape,\n              max_boxes=10,\n              score_threshold=.6,\n              iou_threshold=.5):\n    \"\"\"Evaluate YOLO model on given input batch and return filtered boxes.\"\"\"\n    box_confidence, box_xy, box_wh, box_class_probs = yolo_outputs\n    boxes = yolo_boxes_to_corners(box_xy, box_wh)\n    boxes, scores, classes = yolo_filter_boxes(\n        box_confidence, boxes, box_class_probs, threshold=score_threshold)\n    \n    # Scale boxes back to original image shape.\n    height = image_shape[0]\n    width = image_shape[1]\n    image_dims = K.stack([height, width, height, width])\n    image_dims = K.reshape(image_dims, [1, 4])\n    boxes = boxes * image_dims\n\n    # TODO: Something must be done about this ugly hack!\n    max_boxes_tensor = K.variable(max_boxes, dtype='int32')\n    K.get_session().run(tf.variables_initializer([max_boxes_tensor]))\n    nms_index = tf.image.non_max_suppression(\n        boxes, scores, max_boxes_tensor, iou_threshold=iou_threshold)\n    boxes = K.gather(boxes, nms_index)\n    scores = K.gather(scores, nms_index)\n    classes = K.gather(classes, nms_index)\n    \n    return boxes, scores, classes\n\n\n\ndef yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold = .6):\n    \"\"\"Filters YOLO boxes by thresholding on object and class confidence.\n    \n    Arguments:\n    box_confidence -- tensor of shape (19, 19, 5, 1)\n    boxes -- tensor of shape (19, 19, 5, 4)\n    box_class_probs -- tensor of shape (19, 19, 5, 80)\n    threshold -- real value, if [ highest class probability score < threshold], then get rid of the corresponding box\n    \n    Returns:\n    scores -- tensor of shape (None,), containing the class probability score for selected boxes\n    boxes -- tensor of shape (None, 4), containing (b_x, b_y, b_h, b_w) coordinates of selected boxes\n    classes -- tensor of shape (None,), containing the index of the class detected by the selected boxes\n    \n    Note: \"None\" is here because you don't know the exact number of selected boxes, as it depends on the threshold. \n    For example, the actual output size of scores would be (10,) if there are 10 boxes.\n    \"\"\"\n    \n    # Step 1: Compute box scores\n    ### START CODE HERE ### (≈ 1 line)\n    box_scores = np.multiply(box_confidence, box_class_probs)\n    ### END CODE HERE ###\n    \n    # Step 2: Find the box_classes thanks to the max box_scores, keep track of the corresponding score\n    ### START CODE HERE ### (≈ 2 lines)\n    box_classes = K.argmax(box_scores, axis=-1)\n    box_class_scores = K.max(box_scores, axis=-1)\n    ### END CODE HERE ###\n    \n    # Step 3: Create a filtering mask based on \"box_class_scores\" by using \"threshold\". The mask should have the\n    # same dimension as box_class_scores, and be True for the boxes you want to keep (with probability >= threshold)\n    ### START CODE HERE ### (≈ 1 line)\n    filtering_mask = K.greater_equal(box_class_scores, threshold)\n    ### END CODE HERE ###\n    \n    # Step 4: Apply the mask to scores, boxes and classes\n    ### START CODE HERE ### (≈ 3 lines)\n    scores = tf.boolean_mask(box_class_scores, filtering_mask)\n    boxes = tf.boolean_mask(boxes, filtering_mask)\n    classes = tf.boolean_mask(box_classes, filtering_mask)\n    ### END CODE HERE ###\n    \n    return scores, boxes, classes\n\n\n\ndef yolo_non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5):\n    \"\"\"\n    Applies Non-max suppression (NMS) to set of boxes\n    \n    Arguments:\n    scores -- tensor of shape (None,), output of yolo_filter_boxes()\n    boxes -- tensor of shape (None, 4), output of yolo_filter_boxes() that have been scaled to the image size (see later)\n    classes -- tensor of shape (None,), output of yolo_filter_boxes()\n    max_boxes -- integer, maximum number of predicted boxes you'd like\n    iou_threshold -- real value, \"intersection over union\" threshold used for NMS filtering\n    \n    Returns:\n    scores -- tensor of shape (, None), predicted score for each box\n    boxes -- tensor of shape (4, None), predicted box coordinates\n    classes -- tensor of shape (, None), predicted class for each box\n    \n    Note: The \"None\" dimension of the output tensors has obviously to be less than max_boxes. Note also that this\n    function will transpose the shapes of scores, boxes, classes. This is made for convenience.\n    \"\"\"\n    \n    max_boxes_tensor = K.variable(max_boxes, dtype='int32')     # tensor to be used in tf.image.non_max_suppression()\n    K.get_session().run(tf.variables_initializer([max_boxes_tensor])) # initialize variable max_boxes_tensor\n    \n    # Use tf.image.non_max_suppression() to get the list of indices corresponding to boxes you keep\n    ### START CODE HERE ### (≈ 1 line)\n    nms_indices = tf.image.non_max_suppression(boxes, scores, max_boxes_tensor, iou_threshold=iou_threshold)\n\n    ### END CODE HERE ###\n    \n    # Use K.gather() to select only nms_indices from scores, boxes and classes\n    ### START CODE HERE ### (≈ 3 lines)\n    scores = K.gather(scores, nms_indices)\n    boxes = K.gather(boxes, nms_indices)\n    classes = K.gather(classes, nms_indices)\n    ### END CODE HERE ###\n    \n    return scores, boxes, classes\n\n\n\ndef yolo_eval(yolo_outputs, image_shape = (720., 1280.), max_boxes=10, score_threshold=.6, iou_threshold=.5):\n    \"\"\"\n    Converts the output of YOLO encoding (a lot of boxes) to your predicted boxes along with their scores, box coordinates and classes.\n    \n    Arguments:\n    yolo_outputs -- output of the encoding model (for image_shape of (608, 608, 3)), contains 4 tensors:\n                    box_confidence: tensor of shape (None, 19, 19, 5, 1)\n                    box_xy: tensor of shape (None, 19, 19, 5, 2)\n                    box_wh: tensor of shape (None, 19, 19, 5, 2)\n                    box_class_probs: tensor of shape (None, 19, 19, 5, 80)\n    image_shape -- tensor of shape (2,) containing the input shape, in this notebook we use (608., 608.) (has to be float32 dtype)\n    max_boxes -- integer, maximum number of predicted boxes you'd like\n    score_threshold -- real value, if [ highest class probability score < threshold], then get rid of the corresponding box\n    iou_threshold -- real value, \"intersection over union\" threshold used for NMS filtering\n    \n    Returns:\n    scores -- tensor of shape (None, ), predicted score for each box\n    boxes -- tensor of shape (None, 4), predicted box coordinates\n    classes -- tensor of shape (None,), predicted class for each box\n    \"\"\"\n    \n    ### START CODE HERE ### \n    \n    # Retrieve outputs of the YOLO model (≈1 line)\n    box_confidence, box_xy, box_wh, box_class_probs = yolo_outputs\n\n    # Convert boxes to be ready for filtering functions \n    boxes = yolo_boxes_to_corners(box_xy, box_wh)\n\n    # Use one of the functions you've implemented to perform Score-filtering with a threshold of score_threshold (≈1 line)\n    scores, boxes, classes = yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold = score_threshold)\n    \n    # Scale boxes back to original image shape.\n    boxes = scale_boxes(boxes, image_shape)\n\n    # Use one of the functions you've implemented to perform Non-max suppression with a threshold of iou_threshold (≈1 line)\n    scores, boxes, classes = yolo_non_max_suppression(scores, boxes, classes, max_boxes = max_boxes, iou_threshold = iou_threshold)\n    \n    ### END CODE HERE ###\n    \n    return scores, boxes, classes\n\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The predicition function it takes the input image and tensorflow session **"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(sess, image_file):\n    \"\"\"\n    Runs the graph stored in \"sess\" to predict boxes for \"image_file\". Prints and plots the preditions.\n    \n    Arguments:\n    sess -- your tensorflow/Keras session containing the YOLO graph\n    image_file -- name of an image stored in the \"images\" folder.\n    \n    Returns:\n    out_scores -- tensor of shape (None, ), scores of the predicted boxes\n    out_boxes -- tensor of shape (None, 4), coordinates of the predicted boxes\n    out_classes -- tensor of shape (None, ), class index of the predicted boxes\n    \n    Note: \"None\" actually represents the number of predicted boxes, it varies between 0 and max_boxes. \n    \"\"\"\n\n    # Preprocess your image\n    image, image_data = preprocess_image(image_file, model_image_size = (608, 608))\n\n    # Run the session with the correct tensors and choose the correct placeholders in the feed_dict.\n    # You'll need to use feed_dict={yolo_model.input: ... , K.learning_phase(): 0})\n    ### START CODE HERE ### (≈ 1 line)\n    out_scores, out_boxes, out_classes = sess.run([scores, boxes, classes], feed_dict={yolo_model.input: image_data, K.learning_phase(): 0})\n    ### END CODE HERE ###\n\n    # Print predictions info\n    print('Found {} boxes for {}'.format(len(out_boxes), image_file))\n    # Generate colors for drawing bounding boxes.\n    colors = generate_colors(class_names)\n    # Draw bounding boxes on the image file\n    draw_boxes(image, out_scores, out_boxes, out_classes, class_names, colors)\n    # Save the predicted bounding box on the image\n    image.save('/kaggle/input/1.jpg', quality=90)\n    # Display the results in the notebook\n    output_image = scipy.misc.imread('/kaggle/input/1.jpg')\n    imshow(output_image)\n    \n    return out_scores, out_boxes, out_classes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n**Now all the component of model is ready let's build the model **\n* first read class names\n* secnd read anchors boxes\n* third load pretrained model that is the result of using YAD2K repository to convert YOLO Darknet model cfg and weights to keras model .h5\n* then send yolo_model.output to yolo_head method to make some transformations on YOLO_model output \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sess = K.get_session()\n\nclass_names = read_classes(\"/kaggle/input/cars-yolo/coco_classes.txt\")\nanchors = read_anchors(\"/kaggle/input/model-data/yolo_anchors.txt\")\nyolo_model = load_model(\"/kaggle/input/yolo-version-2-h5/yolo.h5\")\nyolo_outputs = yolo_head(yolo_model.output, anchors, len(class_names))\nyolo_model.summary()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **The Predictions of the model**\n* you have to give the model the input image shape to estimate bounding boxes correcttly using yolo_eval method\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nimage_=\"/kaggle/input/google-ai-open-images-object-detection-track/test/challenge2018_test/ffd7f76659857611.jpg\"\nim = cv2.imread(image_)\nimage_shape = (im.shape[0]*1.0,im.shape[1]*1.0)\n\n\nscores, boxes, classes = yolo_eval(yolo_outputs, image_shape)\n\nout_scores, out_boxes, out_classes = predict(sess,image_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_=\"/kaggle/input/google-ai-open-images-object-detection-track/test/challenge2018_test/e252f53d4dfb5afd.jpg\"\nim = cv2.imread(image_)\nimage_shape = (im.shape[0]*1.0,im.shape[1]*1.0)\n\n\nscores, boxes, classes = yolo_eval(yolo_outputs, image_shape)\n\nout_scores, out_boxes, out_classes = predict(sess,image_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_=\"/kaggle/input/google-ai-open-images-object-detection-track/test/challenge2018_test/c69f8f40bdf7ad53.jpg\"\nim = cv2.imread(image_)\nimage_shape = (im.shape[0]*1.0,im.shape[1]*1.0)\n\n\nscores, boxes, classes = yolo_eval(yolo_outputs, image_shape)\n\nout_scores, out_boxes, out_classes = predict(sess,image_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_=\"/kaggle/input/google-ai-open-images-object-detection-track/test/challenge2018_test/1858cfe5c26f18f8.jpg\"\nim = cv2.imread(image_)\nimage_shape = (im.shape[0]*1.0,im.shape[1]*1.0)\n\n\nscores, boxes, classes = yolo_eval(yolo_outputs, image_shape)\n\nout_scores, out_boxes, out_classes = predict(sess,image_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_=\"/kaggle/input/google-ai-open-images-object-detection-track/test/challenge2018_test/6db9b5d07da41430.jpg\"\nim = cv2.imread(image_)\nimage_shape = (im.shape[0]*1.0,im.shape[1]*1.0)\n\n\nscores, boxes, classes = yolo_eval(yolo_outputs, image_shape)\n\nout_scores, out_boxes, out_classes = predict(sess,image_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_=\"/kaggle/input/google-ai-open-images-object-detection-track/test/challenge2018_test/e96b35e036492067.jpg\"\nim = cv2.imread(image_)\nimage_shape = (im.shape[0]*1.0,im.shape[1]*1.0)\n\n\nscores, boxes, classes = yolo_eval(yolo_outputs, image_shape)\n\nout_scores, out_boxes, out_classes = predict(sess,image_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_=\"/kaggle/input/google-ai-open-images-object-detection-track/test/challenge2018_test/760febac191eb035.jpg\"\nim = cv2.imread(image_)\nimage_shape = (im.shape[0]*1.0,im.shape[1]*1.0)\n\n\nscores, boxes, classes = yolo_eval(yolo_outputs, image_shape)\n\nout_scores, out_boxes, out_classes = predict(sess,image_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}