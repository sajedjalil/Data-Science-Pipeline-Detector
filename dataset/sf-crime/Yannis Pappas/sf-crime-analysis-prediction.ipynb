{"cells":[{"metadata":{"_uuid":"49e7c956cca13689497a131988269949e76edcfe"},"cell_type":"markdown","source":"# Introduction\nSan Francisco was infamous for housing some of the world's most notorious criminals on the inescapable island of Alcatraz. Today, the city is known more for its tech scene than its criminal past. \nFrom Sunset to SOMA, and Marina to Excelsior, this project analyzes 12 years of crime reports from across all of San Francisco's neighborhoods to create a model that predicts the category of crime that occurred, given time and location.  \n# Definition  \n## Project Overview  \nCrime is a social phenomenon as old as societies themselves, and although there will never be a free from crime society - just because it would need everyone in that society to think and act in the same way - societies always look for a way to minimize it and prevent it.\nIn the modern United States history, crime rates increased after World War II, peaking from the 1970s to the early 1990s. Violent crime nearly quadrupled between 1960 and its peak in 1991. Property crime more than doubled over the same period. Since the 1990s, however, crime in the United States has declined steadily.\nUntil recently crime prevention was studied based on strict behavioral and social methods, but the recent developments in Data Analysis have allowed a more quantitative approach in the subject. We will explore a dataset of nearly 12 years of crime reports from across all of San Francisco's neighborhoods, and we will create a model that predicts the category of crime that occurred, given the time and location.  \n## Problem Statement\nTo examine the specific problem, we will apply a full Data Science life cycle composed of the following steps:\n1. Data Wrangling to audit the quality of the data and perform all the necessary actions to clean the dataset.\n2. Data Exploration for understanding the variables and create intuition on the data.\n3. Feature Engineering to create additional variables from the existing.\n4. Data Normalization and Data Transformation for preparing the dataset for the learning algorithms (if needed).\n5. Training / Testing data creation to evaluate the performance of our models and fine-tune their hyperparameters.\n6. Model selection and evaluation. This will be the final goal; creating a model that predicts the probability of each type of crime based on the location and the date.  \n\n## Metrics\nThe most appropriate evaluation metric for such problems is the **multi-class logarithmic loss**. Logarithmic loss measures the performance of a classification model where the prediction output is a probability value between 0 and 1. For each incident, we will predict a set of predicted probabilities (one for every class), and we will calculate the average deviation from the real values. \nTo get a little more intuition on the metric, for a specific incident:\n* We get 0 loss from the categories of crimes that did not happen (since yij =0 yijlog(pij)) no matter our predicted probability.\n* We get log(pij) loss from the category that happened, where pij is our predicted probability for the specific category.  \n\nWe have to note here that since all the probabilities for a specific incident sum to 1, each probability we predict for a category that did not happen creates an “indirect” loss since it decreases our predicted probability for the category of crime that happened.\nIn other words, the metric evaluates the certainty of our model for each category of crime/incident.  \n\n# Analysis  \n## Data Exploration  \nThe dataset is in a tabular form and includes chronological, geographical and text data and contains incidents derived from the SFPD Crime Incident Reporting system. "},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"abd80502f1cfc3bed6402f86ef9374f97c845c7a"},"cell_type":"code","source":"import pandas as pd\nfrom shapely.geometry import  Point\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nfrom matplotlib import cm\nimport urllib.request\nimport shutil\nimport zipfile\nimport os\nimport re\nimport contextily as ctx\nimport geoplot as gplt\nimport lightgbm as lgb\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom lightgbm import LGBMClassifier\nfrom matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\nimport shap","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"f2d723c29db2f8e8565887f2a784704b1f60b572"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv', parse_dates=['Dates'])\ntest = pd.read_csv('../input/test.csv', parse_dates=['Dates'], index_col='Id')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"66c966319fa299f29c9260cf880d8ef97296e99f"},"cell_type":"code","source":"print('First date: ', str(train.Dates.describe()['first']))\nprint('Last date: ', str(train.Dates.describe()['last']))\nprint('Test data shape ', train.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f4ed3b23ae84cc7a8a1def77eb7224ebcff8a21"},"cell_type":"markdown","source":"The data ranges from 1/1/2003 to 5/13/2015 creating a training dataset with nine features and 878,049 samples"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"b89bfc9c8de63768af8cefc488992ef6597e2c7e"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"650616254a8a01d196a5fe8196c9cf71ab634f60"},"cell_type":"markdown","source":"More specifically it includes the following variables.\n* Dates - timestamp of the crime incident\n* Category - category of the crime incident. (This is our target variable.)\n* Descript - detailed description of the crime incident\n* DayOfWeek - the day of the week\n* PdDistrict - the name of the Police Department District\n* Resolution - The resolution of the crime incident\n* Address - the approximate street address of the crime incident \n* X - Longitude\n* Y - Latitude\n"},{"metadata":{"trusted":true,"_uuid":"990b17a49cbb4990e31689aaf97545504f91a9b9"},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b5d238165631f96b0dd7caea1104c8f21a76bf4"},"cell_type":"markdown","source":"The dataset contains a lot of 'object' variables (aka strings) that we will need to encode."},{"metadata":{"trusted":true,"_uuid":"c1ee420a26c17a509d20ed82bc65d634d047b8aa"},"cell_type":"code","source":"train.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a68a9e7bc55893d29574fca90e4121c35266aad6"},"cell_type":"markdown","source":"It also contains 2323 duplicates that we should remove.  \nWe will also evaluate the position of the data points using the coordinates."},{"metadata":{"trusted":true,"_uuid":"a965b1444083b292a72f517cc2f98158f5f56234"},"cell_type":"code","source":"def create_gdf(df):\n    gdf = df.copy()\n    gdf['Coordinates'] = list(zip(gdf.X, gdf.Y))\n    gdf.Coordinates = gdf.Coordinates.apply(Point)\n    gdf = gpd.GeoDataFrame(\n        gdf, geometry='Coordinates', crs={'init': 'epsg:4326'})\n    return gdf\n\ntrain_gdf = create_gdf(train)\n\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nax = world.plot(color='white', edgecolor='black')\ntrain_gdf.plot(ax=ax, color='red')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fed40706190c708d2200157aff985a802633c20a"},"cell_type":"markdown","source":"Some points are misplaced. Let's see how many they are."},{"metadata":{"trusted":true,"_uuid":"90e3a7c2bd2be831452cde95b145d00e920021be"},"cell_type":"code","source":"print(train_gdf.loc[train_gdf.Y > 50].count()[0])\ntrain_gdf.loc[train_gdf.Y > 50].sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87e4b75f0275365ae8992f9d1c166d010cd39198"},"cell_type":"markdown","source":"We will replace the outlying coordinates with the average coordinates of the district they belong."},{"metadata":{"trusted":true,"_uuid":"2bc2ff9ba64243f70c8ba72a13e2744a7658ccdd"},"cell_type":"code","source":"train.drop_duplicates(inplace=True)\ntrain.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\ntest.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\n\nimp = SimpleImputer(strategy='mean')\n\nfor district in train['PdDistrict'].unique():\n    train.loc[train['PdDistrict'] == district, ['X', 'Y']] = imp.fit_transform(\n        train.loc[train['PdDistrict'] == district, ['X', 'Y']])\n    test.loc[test['PdDistrict'] == district, ['X', 'Y']] = imp.transform(\n        test.loc[test['PdDistrict'] == district, ['X', 'Y']])\n\ntrain_gdf = create_gdf(train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6fa13315fbd22a8417de5e99e265465b4faa90d"},"cell_type":"markdown","source":"After cleaning the dataset from outliers and duplicates, we examine the variables.  \n### Dates & Day of the week  \nThese variables are distributed uniformly between 1/1/2003 to 5/13/2015 (and Monday to Sunday) and split between the training and the testing dataset as mentioned before. We did not notice any anomalies on these variables.  \nThe median frequency of incidents is 389 per day with a standard deviation of 48.51."},{"metadata":{"trusted":true,"_uuid":"1c9ec328a2beeb811020bd018356d832a8dd79ed"},"cell_type":"code","source":"col = sns.color_palette()\n\ntrain['Date'] = train.Dates.dt.date\ntrain['Hour'] = train.Dates.dt.hour\n\nplt.figure(figsize=(10, 6))\ndata = train.groupby('Date').count().iloc[:, 0]\nsns.kdeplot(data=data, shade=True)\nplt.axvline(x=data.median(), ymax=0.95, linestyle='--', color=col[1])\nplt.annotate(\n    'Median: ' + str(data.median()),\n    xy=(data.median(), 0.004),\n    xytext=(200, 0.005),\n    arrowprops=dict(arrowstyle='->', color=col[1], shrinkB=10))\nplt.title(\n    'Distribution of number of incidents per day', fontdict={'fontsize': 16})\nplt.xlabel('Incidents')\nplt.ylabel('Density')\nplt.legend().remove()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"815d0407f51fcd919cf9a6f03a8d0c0bda4062bc"},"cell_type":"markdown","source":"Also, there is no significant deviation of incidents frequency throughout the week. Thus we do not expect this variable to play a significant role in the prediction."},{"metadata":{"trusted":true,"_uuid":"4e5c858139ed1bef557d9895852dc544698a4cd7"},"cell_type":"code","source":"data = train.groupby('DayOfWeek').count().iloc[:, 0]\ndata = data.reindex([\n    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday',\n    'Sunday'\n])\n\nplt.figure(figsize=(10, 5))\nwith sns.axes_style(\"whitegrid\"):\n    ax = sns.barplot(\n        data.index, (data.values / data.values.sum()) * 100,\n        orient='v',\n        palette=cm.ScalarMappable(cmap='Reds').to_rgba(data.values))\n\nplt.title('Incidents per Weekday', fontdict={'fontsize': 16})\nplt.xlabel('Weekday')\nplt.ylabel('Incidents (%)')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46436f6ab08d64bd6bb4b463a615b83287a29a85"},"cell_type":"markdown","source":"### Category  \nThere are 39 discrete categories that the police department file the incidents with the most common being Larceny/Theft (19.91%), Non/Criminal (10.50%), and Assault(8.77%)."},{"metadata":{"trusted":true,"_uuid":"316ad684669edfdc3064728f26c2122a09b97083"},"cell_type":"code","source":"data = train.groupby('Category').count().iloc[:, 0].sort_values(\n    ascending=False)\ndata = data.reindex(np.append(np.delete(data.index, 1), 'OTHER OFFENSES'))\n\nplt.figure(figsize=(10, 10))\nwith sns.axes_style(\"whitegrid\"):\n    ax = sns.barplot(\n        (data.values / data.values.sum()) * 100,\n        data.index,\n        orient='h',\n        palette=\"Reds_r\")\n\nplt.title('Incidents per Crime Category', fontdict={'fontsize': 16})\nplt.xlabel('Incidents (%)')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48262500e83132e28d2ef5853a29f622b312ab76"},"cell_type":"markdown","source":"### Police District  \nThere are significant differences between the different districts of the City with the Southern district having the most incidents (17.87%) followed by Mission (13.67%) and Northern (12.00%)."},{"metadata":{"trusted":true,"_uuid":"5fc41a1caeb561517721483016f5ac7aaf4e0e13"},"cell_type":"code","source":"# Downloading the shapefile of the area \nurl = 'https://data.sfgov.org/api/geospatial/wkhw-cjsf?method=export&format=Shapefile'\nwith urllib.request.urlopen(url) as response, open('pd_data.zip', 'wb') as out_file:\n    shutil.copyfileobj(response, out_file)\n# Unzipping it\nwith zipfile.ZipFile('pd_data.zip', 'r') as zip_ref:\n    zip_ref.extractall('pd_data')\n# Loading to a geopandas dataframe\nfor filename in os.listdir('./pd_data/'):\n    if re.match(\".+\\.shp\", filename):\n        pd_districts = gpd.read_file('./pd_data/'+filename)\n        break\n# Defining the coordinate system to longitude/latitude\npd_districts.crs={'init': 'epsg:4326'}\n\n# Merging our train dataset with the geo-dataframe\npd_districts = pd_districts.merge(\n    train.groupby('PdDistrict').count().iloc[:, [0]].rename(\n        columns={'Dates': 'Incidents'}),\n    how='inner',\n    left_on='district',\n    right_index=True,\n    suffixes=('_x', '_y'))\n\n# Transforming the coordinate system to Spherical Mercator for\n# compatibility with the tiling background\npd_districts = pd_districts.to_crs({'init': 'epsg:3857'})\n\n# Calculating the incidents per day for every district\ntrain_days = train.groupby('Date').count().shape[0]\npd_districts['inc_per_day'] = pd_districts.Incidents/train_days\n\n# Ploting the data\nfig, ax = plt.subplots(figsize=(10, 10))\npd_districts.plot(\n    column='inc_per_day',\n    cmap='Reds',\n    alpha=0.6,\n    edgecolor='r',\n    linestyle='-',\n    linewidth=1,\n    legend=True,\n    ax=ax)\n\ndef add_basemap(ax, zoom, url='http://tile.stamen.com/terrain/tileZ/tileX/tileY.png'):\n    \"\"\"Function that add the tile background to the map\"\"\"\n    xmin, xmax, ymin, ymax = ax.axis()\n    basemap, extent = ctx.bounds2img(xmin, ymin, xmax, ymax, zoom=zoom, url=url)\n    ax.imshow(basemap, extent=extent, interpolation='bilinear')\n    # restore original x/y limits\n    ax.axis((xmin, xmax, ymin, ymax))\n\n# Adding the background\nadd_basemap(ax, zoom=11, url=ctx.sources.ST_TONER_LITE)\n\n# Adding the name of the districts\nfor index in pd_districts.index:\n    plt.annotate(\n        pd_districts.loc[index].district,\n        (pd_districts.loc[index].geometry.centroid.x,\n         pd_districts.loc[index].geometry.centroid.y),\n        color='#353535',\n        fontsize='large',\n        fontweight='heavy',\n        horizontalalignment='center'\n    )\n\nax.set_axis_off()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7326225c0886d42f59e09ee921374e4ce33c4ccf"},"cell_type":"markdown","source":"### Address  \nAddress, as a text field, requires advanced techniques to use it for the prediction. Instead in this project, we will use it to extract if the incident has happened on the road or in a building block.  \n### X - Longitude Y - Latitude  \nWe have tested that the coordinates belong inside the boundaries of the city. Although longitude does not contain any outliers, latitude includes some 90o values which correspond to the North Pole. \n## Exploratory Visualization\nBased on the Project’s statement, we need to predict the probability of each type of crime based on time and location. That being said, we present two diagrams to visualize the importance of these variables.\nThe first one presents the geographic density of 9 random crime categories. We can see that although the epicenter of most of the crimes resides on the northeast of the city, each crime has a different density on the rest of the city. This fact is a reliable indication that the location ( coordinates / Police District) will be a significant factor for the analysis and the forecasting. \n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"crimes = train['Category'].unique().tolist()\ncrimes.remove('TREA')\n\npd_districts = pd_districts.to_crs({'init':'epsg:4326'})\nsf_land = pd_districts.unary_union\nsf_land = gpd.GeoDataFrame(gpd.GeoSeries(sf_land), crs={'init':'epsg:4326'})\nsf_land = sf_land.rename(columns={0:'geometry'}).set_geometry('geometry')\n\nfig, ax = plt.subplots(3, 3, sharex=True, sharey=True, figsize=(12,12))\nfor i , crime in enumerate(np.random.choice(crimes, size=9, replace=False)):\n    data = train_gdf.loc[train_gdf['Category'] == crime]\n    ax = fig.add_subplot(3, 3, i+1)\n    gplt.kdeplot(data,\n                 shade=True,\n                 shade_lowest=False,\n                 clip = sf_land.geometry,\n                 cmap='Reds',\n                 ax=ax)\n    gplt.polyplot(sf_land, ax=ax)\n    ax.set_title(crime) \nplt.suptitle('Geographic Density of Different Crimes')\nfig.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f45fedf76e84b277be00f615b69d84f27c11d384"},"cell_type":"markdown","source":"The second diagram presents the average number of incidents per hour for five of the crimes' categories. It is evident that different crimes have different frequency during different times of the day. \nSome examples are that prostitution picks during the evening and all through the night, Gambling incidents start late at night until the morning and Burglary picks early in the morning until the afternoon.\nAs before these are sharp pieces of evidence that the time parameters will have a significant role also.\n"},{"metadata":{"trusted":true,"_uuid":"41cfac02b5b3b373d14c47f3062a0c8d1afdf65f"},"cell_type":"code","source":"data = train.groupby(['Hour', 'Date', 'Category'],\n                     as_index=False).count().iloc[:, :4]\ndata.rename(columns={'Dates': 'Incidents'}, inplace=True)\ndata = data.groupby(['Hour', 'Category'], as_index=False).mean()\ndata = data.loc[data['Category'].isin(\n    ['ROBBERY', 'GAMBLING', 'BURGLARY', 'ARSON', 'PROSTITUTION'])]\n\nsns.set_style(\"whitegrid\")\nfig, ax = plt.subplots(figsize=(14, 4))\nax = sns.lineplot(x='Hour', y='Incidents', data=data, hue='Category')\nax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=6)\nplt.suptitle('Average number of incidents per hour')\nfig.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5fcc3692620e81993999f8de5e074bffb696caa"},"cell_type":"markdown","source":"## Algorithms and Techniques  \nThe specific problem is a typical multiclass classification problem, and there are several categories of algorithms for solving it. Initially, we evaluated several appropriate algorithms from Linear Models (Stochastic Gradient Descent), Nearest Neighbors (K nearest neighbors), Ensemble methods (Random Forests & AdaBoost) and Boosting Algorithms (XGBoost & LIghtGBM) using basic feature engineering and the default parameters to evaluate if any of them has a significant head start:  \n\n| **Algorithm**                   | **Parameters**                                        | **Logloss**  |\n|:---------------------------:|:-------------------------------------------------:|:--------:|\n| Stochastic Gradient Descent | Default Scikit-Learn Parameters (with 'log' loss) |  2.86631 |\n| K-Nearest Neighbors         | Default Scikit-Learn Parameters                   | 23.29263 |\n| Random Forest               | Default Scikit-Learn Parameters                   |  2.92716 |\n| AdaBoost                    | Default Scikit-Learn Parameters                   |  3.58856 |\n| XGBoost                     | Default Scikit-Learn Parameters                   |  2.91656 |\n| LIghtGBM                    | Default Scikit-Learn Parameters                   |  2.98336 |  "},{"metadata":{"_uuid":"1ca652ca38fde26b54965b4afb1e00ac7374e9de"},"cell_type":"markdown","source":"SGD scored the best initial result, but after a lengthy hyperparameter tuning, it was not able to pass a 2.54503 threshold.  \n\nFinally, from the algorithms that scored under 3.0, we decided to work with LightGBM due to its efficiency and versatility in the hyperparameters tuning.\nLightGBM is a decision tree boosting algorithm uses histogram-based algorithms which bucket continuous feature (attribute) values into discrete bins. This technique speeds up training and reduces memory usage. In layman terms the algorithm works like this:  \n\n1. Fit a decision tree to the data\n2. Evaluate the model\n3. Increase the weight to the incorrect samples.\n4. Choose the leaf with max delta loss to grow.\n5. Grow the tree.\n6. Go to step 2 ![lightgbm](https://lightgbm.readthedocs.io/en/latest/_images/leaf-wise.png) \n\n## Benchmark  \nThere are two types of benchmarks we need to set. The first will be a naive prediction. This prediction will be a baseline score to compare with our model’s score to evaluate if we have any significant progress.  \n\nIn a Multiclass Classification, the best way to calculate the baseline is by assuming that the probability of each category equals its average frequency in the train set. The frequency can be calculated easily by dividing the sum of incidents of each category by the number of rows of the training set.\n"},{"metadata":{"trusted":true,"_uuid":"793c149fc7a20583d1962cd4ee5eb5df433298cb"},"cell_type":"code","source":"naive_vals = train.groupby('Category').count().iloc[:,0]/train.shape[0]\nn_rows = test.shape[0]\n\nsubmission = pd.DataFrame(\n    np.repeat(np.array(naive_vals), n_rows).reshape(39, n_rows).transpose(),\n    columns=naive_vals.index)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb19d047d5bea62fb7056c83ffdec686aa823b60"},"cell_type":"markdown","source":"The baseline calculated this way is 2.68015. (Details in [SF-Crime Analysis & Prediction (Naive Prediction)](https://www.kaggle.com/yannisp/sf-crime-analysis-prediction-naive-prediction) Notebook. We can notice that this baseline is already lower than the initial score of our classifiers.  \n\nAnother critical benchmark is usually the ‘Human Performance’, as a proxy for the Bayes error rate. The specific problem does not belong to a field that humans excel (like computer vision or NLP), so as a proxy for the  [Bayes error rate](https://en.wikipedia.org/wiki/Bayes_error_rate), we will use the score of the best kernel so far which is [initial benchmark need tuning](https://www.kaggle.com/sergeylebedev/initial-benchmark-need-tuning) by the user [Sergey Lebedev](https://www.kaggle.com/sergeylebedev) with score 2.29318. \n\nThe small distance between the baseline score and the Bayes error rate indicate that this is a hard problem with a low margin of improvement.  \n# Methodology\n## Data Preprocessing\n### Data Wrangling\nFollowing the methodology described in the Problem Statement, we identified 2323 duplicate values and 67 wrong latitudes. The duplicates removed and the outliers imputed.  \n### Feature Engineering\nThen, we created additional features. More specifically:\n* From the ‘Dates’ field, we extracted the Day, the Month, the Year, the Hour, the Minute, the Weekday, and the number of days since the first day in the data.\n* From the ‘Address’ field we extracted if the incident has taken place in a crossroad or on a building block.\n\n\n"},{"metadata":{"trusted":true,"_uuid":"e9642aac981d00da7e6c7f98580e598400e97598"},"cell_type":"code","source":"def feature_engineering(data):\n    data['Date'] = pd.to_datetime(data['Dates'].dt.date)\n    data['n_days'] = (\n        data['Date'] - data['Date'].min()).apply(lambda x: x.days)\n    data['Day'] = data['Dates'].dt.day\n    data['DayOfWeek'] = data['Dates'].dt.weekday\n    data['Month'] = data['Dates'].dt.month\n    data['Year'] = data['Dates'].dt.year\n    data['Hour'] = data['Dates'].dt.hour\n    data['Minute'] = data['Dates'].dt.minute\n    data['Block'] = data['Address'].str.contains('block', case=False)\n    \n    data.drop(columns=['Dates','Date','Address'], inplace=True)\n        \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97d7a3a2016f80d4635ce928194e391076e43307"},"cell_type":"code","source":"train = feature_engineering(train)\ntrain.drop(columns=['Descript','Resolution'], inplace=True)\ntest = feature_engineering(test)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b9968f90b11c1dac5864fdd521b7135b5a7ea6f"},"cell_type":"markdown","source":"### Feature Scaling\nDeciding to continue with a tree-based algorithm there was no need for scaling on the final dataset.\n### Feature Selection\nAfter the feature engineering described above, we ended up with 11 features. To identify if any of them increased the complexity of the model without adding significant gain to the model, we used the method of Permutation Importance.  \n\nThe idea is that the importance of a feature can be measured by looking at how much the loss decreases when a feature is not available. To do that we can remove each feature from the dataset, re-train the estimator and check the impact. Doing this would require re-training an estimator for each feature, which can be computationally intensive. Instead, we can replace it with noise by shuffle values for a feature.  \n\nThe implementation of the above technique showed that there is no need for any feature removal since all of them have a positive impact in the dataset.\n"},{"metadata":{"trusted":true,"_uuid":"5458f8193aeb168bddce69be0b1dca3d57326f5d"},"cell_type":"code","source":"le1 = LabelEncoder()\ntrain['PdDistrict'] = le1.fit_transform(train['PdDistrict'])\ntest['PdDistrict'] = le1.transform(test['PdDistrict'])\nle2 = LabelEncoder()\ny = le2.fit_transform(train.pop('Category'))\n\ntrain_X, val_X, train_y, val_y = train_test_split(train, y)\n\nmodel =LGBMClassifier(objective='multiclass', num_class=39).fit(train_X, train_y)\n\nperm = PermutationImportance(model).fit(val_X, val_y)\neli5.show_weights(perm, feature_names=val_X.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88d5f60166b3de09c089697e3fd02306a0990749"},"cell_type":"markdown","source":"## Building the Initial Model\nTo build the model we used the LightGBM’s Python API.\nFirst we created the dataset by combining the features, the target and declaring the PdDistrict as a categorical variable using ‘lightgbm.Dataset()`.  \n\nThen we used Cross-Validation with early stopping (10 rounds) and parameters:  \n* Objective = ‘'multiclass',  \n* 'Metric = ‘multi_logloss',  \n* 'Num_class = 39  \n\nThe above setup achieved 2.46799  cross-validation score after 23 epochs and 2.49136 on the testing set.  \n[SF-Crime Analysis & Prediction (Base Model)](https://www.kaggle.com/yannisp/sf-crime-analysis-prediction-base-model/notebook?scriptVersionId=9334889) \n![base_model](https://i.imgur.com/AcJHphZ.png)\n## Refinement  \nInstead of the most popular methods of Exhaustive Grid Search and Randomized Parameter Optimization, we selected another more efficient way to tune the hyperparameters of the algorithm; **Bayesian optimization**.  \n\nThe problem with the two techniques mentioned above is that they do not use previous results to pick the next input values. Instead, Bayesian optimization, also called Sequential Model-Based Optimization (SMBO), implements this idea by building a probability model of the objective function that maps input values to a probability of a loss: p (loss | input values). The probability model, (also called the surrogate or response surface), is easier to optimize than the actual objective function. Bayesian methods select the next values to evaluate by applying a criterion (usually Expected Improvement) to the surrogate. The concept is to limit the evaluations of the objective function by spending more time choosing the next values to try.  \n\nTo conclude to the final model, we used five folds Cross-Validation for 100 epochs and early stopping with Bayesian Optimization. Also, we created a custom callback function so we can write proper logs that can be read by Tensorboard. This way we were able to monitor the validation process in real time.  \n\nThe above was up to some point an iterative process. We run the optimization process until we noticed in Tensorboard that the models converge. Then we stopped and evaluated the results and move to the next iteration. (example of the process [here](https://www.kaggle.com/yannisp/sf-crime-analysis-prediction-optimiz-ex)) \n\nFirst we optimized a few basic hyperparameters including:\n* `Boosting` selection between gbdt and dart\n* `Max_delta_step` uniformly in the range  [0, 2]\n* `Min_data_in_leaf` uniformly in the range [10,30]\n* `Num_leaves` uniformly in the range [20,40]  \n\nAfter the model converged, a second round of tuning followed:\n* `Boosting`: gbdt\n* `Max_delta_step` uniformly in the range  [0.5, 2.5]\n* `Min_data_in_leaf` uniformly in the range [10, 25]\n* `Num_leaves` uniformly in the range [20, 45]\n* `Max_bin` uniformly in the range [200, 500],\n* `Learning_rate` uniformly in the range [0.1, 2]\n\nFinally we concluded to the following hyperparameters:  \n* `Boosting`: gbdt\n* `Max_delta_step`: 0.9\n* `Min_data_in_leaf`:  21\n* `Num_leaves`: 41\n* `Max_bin`: 465,\n* `Learning_rate`: 0.4  \n\nIn the following figure, we present the performance of the best model from each step of optimization.  \n![](https://i.imgur.com/grRIpi5.png?1)\n"},{"metadata":{"_uuid":"4a94ee3d432d6bc02d8ab35f9007c3b682b9c5d9"},"cell_type":"markdown","source":"## Building the final model"},{"metadata":{"trusted":true,"_uuid":"1b675ca8c60ee991f7121c31d067a76713b315e5"},"cell_type":"code","source":"# Loading the data\ntrain = pd.read_csv('../input/train.csv', parse_dates=['Dates'])\ntest = pd.read_csv('../input/test.csv', parse_dates=['Dates'], index_col='Id')\n\n# Data cleaning\ntrain.drop_duplicates(inplace=True)\ntrain.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\ntest.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\n\nimp = SimpleImputer(strategy='mean')\n\nfor district in train['PdDistrict'].unique():\n    train.loc[train['PdDistrict'] == district, ['X', 'Y']] = imp.fit_transform(\n        train.loc[train['PdDistrict'] == district, ['X', 'Y']])\n    test.loc[test['PdDistrict'] == district, ['X', 'Y']] = imp.transform(\n        test.loc[test['PdDistrict'] == district, ['X', 'Y']])\ntrain_data = lgb.Dataset(\n    train, label=y, categorical_feature=['PdDistrict'], free_raw_data=False)\n\n# Feature Engineering\ntrain = feature_engineering(train)\ntrain.drop(columns=['Descript','Resolution'], inplace=True)\ntest = feature_engineering(test)\n\n# Encoding the Categorical Variables\nle1 = LabelEncoder()\ntrain['PdDistrict'] = le1.fit_transform(train['PdDistrict'])\ntest['PdDistrict'] = le1.transform(test['PdDistrict'])\n\nle2 = LabelEncoder()\nX = train.drop(columns=['Category'])\ny= le2.fit_transform(train['Category'])\n\n# Creating the model\ntrain_data = lgb.Dataset(\n    X, label=y, categorical_feature=['PdDistrict'])\n\nparams = {'boosting':'gbdt',\n          'objective':'multiclass',\n          'num_class':39,\n          'max_delta_step':0.9,\n          'min_data_in_leaf': 21,\n          'learning_rate': 0.4,\n          'max_bin': 465,\n          'num_leaves': 41\n         }\n\nbst = lgb.train(params, train_data, 100)\n\npredictions = bst.predict(test)\n\n# Submitting the results\nsubmission = pd.DataFrame(\n    predictions,\n    columns=le2.inverse_transform(np.linspace(0, 38, 39, dtype='int16')),\n    index=test.index)\nsubmission.to_csv(\n    'LGBM_final.csv', index_label='Id')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed146225d56f334349b49ea9397d61cd4633316a"},"cell_type":"markdown","source":"# Model Evaluation and Validation\nThe final model scored 2.25697 on the training set which is 16% lower from the naive prediction (2.68015)  and 2% better than the benchmark. Taking into account the low margin between the naive and the benchmark we knew that we would probably have a small improvement.  \n\nThat being said, we could say that the results are satisfactory.  \n\nBased on the Permutation Importance analysis we performed before, the model should be susceptible to changes in Minute and the coordinates and less sensitive to changes in Day, Year or Day of the week.  \n\nIndeed, by removing the ‘Minute’ feature from the dataset, we had an increase of loss to 2.53743 and by removing the ‘DayOfWeek’ feature the loss increased to 2.25900.  \n\nThe Permutation importance is a great tool to understand **how much** a specific feature affect our prediction but it does not tell us anything about **the direction** it affects it. We can solve this issue and understand even deeper our model by using Partial Dependencies. In other words, if for an incident we change only the value of one feature how will this affect the probability of each crime category?  \n\nAs an example, we can evaluate how the Hour affects the probabilities of three different crimes. We can see that the hour does not affect the probability for BRIBERY (class 3). In contrast, during the night the probability for BURGLARY (class 4) increases (up to 2%), and during the day the probability for DISORDERLY CONDUCT (class 5) decreases.  \n\nWe can conclude that the model is aligned with our intuition.\n\n"},{"metadata":{"trusted":true,"_uuid":"44af3dc239d812c8e0802deb40c55c162d532afe"},"cell_type":"code","source":"model = LGBMClassifier(**params).fit(X, y, categorical_feature=['PdDistrict'])\n\npdp_Pd = pdp.pdp_isolate(\n    model=model,\n    dataset=X,\n    model_features=X.columns.tolist(),\n    feature='Hour',\n    n_jobs=-1)\n\npdp.pdp_plot(\n    pdp_Pd,\n    'Hour',\n    ncols=3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9983e427f0c4e3102567329bf29befbbd3a72428"},"cell_type":"markdown","source":"# Conclusion\n## Free-Form Visualization\nAn interesting visualization would be to depict how each feature affects a specific prediction. Insights like this are possible with the use of the SHAP library. \n\nSHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods and representing the only possible consistent and locally accurate additive feature attribution method based on expectations (see the [SHAP NIPS paper](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions) for details).  \n\nAs an example let’s select a row from the testing dataset:"},{"metadata":{"trusted":true,"_uuid":"22c2a8c4bca9bf236ed26ae1220bd5103bd4a748"},"cell_type":"code","source":"data_for_prediction = test.loc[[846262]]\ndata_for_prediction","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"893470e9059d287fc12fcae73fbb33a74627640d"},"cell_type":"markdown","source":"This incident has taken place in 03:30 in the night in a block. As we saw in the Partial Dependencies graphs before, BURGLARY has a higher probability and burglaries happen by definition in blocks. Let’s see if our model aligns with our intuition."},{"metadata":{"trusted":true,"_uuid":"be0c20f2881770ab3ba244fd9d556a84145922a1"},"cell_type":"code","source":"shap.initjs()\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(model)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)\n\nshap.force_plot(explainer.expected_value[4], shap_values[4], data_for_prediction, link='logit')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8aa4eba243aeffd49f90ae7275e3abc53094b70e"},"cell_type":"markdown","source":"We can see that there is a 10% probability for BURGLARY and that this is mostly increased  because it takes place to a block (not on a crossroad) and from the time (hour and minute). Both of these are aligned again with our intuition, making us more confident about the validity of our model."},{"metadata":{"_uuid":"b8d7d231cfb8eb22370a63719ccfa10fcc1dd26b"},"cell_type":"markdown","source":"## Reflection\nAs described in the previous sections a full cycle data processing have been followed and lead us to a satisfactory prediction model.  \n\nThe most challenging part was that due to the nature of the features, there was a little room for feature engineering. For this reason, we had to be creative and use advanced techniques during the hyperparameter optimization to make a difference.  \nThis is a hard problem to solve with a heavily unbalanced dataset and the unpredictability (up to some point) of the “human factor”.  \n## Improvements\nWe are sure there is space for improvement. Two additional techniques we would like to implement if there was the necessary time would be:  \n\n* Create **ordinal representations** for the features that present a kind of cyclicity (Month, Weekday, Hour, Minute). The reasoning behind this is that if we take the Hour as an example, the default representation implies that 23 and 00 (midnight) are 23 “units” away although in reality, they are 1 “unit” apart. A way to solve it is to imagine the hour in a real clock and take their projections on the axes passing from the center of the clock. This way the distance between 23 and 00 is the same as between 00 and 01. We can achieve this with functions like Hx = sin(2\\*π\\*H/23) & Hy = cos(2\\*π\\*H/23) for the hour and accordingly for the rest.    \n\n* Use **embeddings** or any other text processing technique, for the addresses. By extracting if the incident has happened to a block or a crossroad we have extracted the minimum gain from this feature, and maybe there are some patterns to exploit and give us even better score."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}