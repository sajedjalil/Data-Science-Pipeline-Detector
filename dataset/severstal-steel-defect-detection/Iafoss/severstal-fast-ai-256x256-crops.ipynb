{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Overview\nThe primary goal of this competition is identification and segmentation of surface defects in flat sheet steel. In contrast to previous segmentation competitions, such as [Airbus Ship Detection Challenge](https://www.kaggle.com/c/airbus-ship-detection/discussion) and [TGS Salt Identification Challenge](https://www.kaggle.com/c/tgs-salt-identification-challenge), as well as the ongoing [SIIM-ACR Pneumothorax Segmentation](https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation) challenge, here there are several classes of defects must be predicted. The kernel below provides a Unet starter code written with use of fast.ai library and incorporation of Hypercolumns method. Additional details of this method could be found in [this kernel](https://www.kaggle.com/iafoss/hypercolumns-pneumothorax-fastai-0-831-lb). \n\nIn contrast to Pneumothorax challenge, where segmentation may require information from entire image, here surface defects can be identified locally. Therefore, splitting of images into crops and training only on images with nonzero masks may be an effective strategy. In particular, it allows to train the model with using large enough batches and keeping only parts of images containing useful information. Hard negative examples identified based on the model prediction on crops with empty masks are essential for training, and the training set is composed of positive examples, images with texture, and 12000 most difficult negative crops. Despite training is done on crops, the predictions are generated based on full size test images ([check this kernel](https://www.kaggle.com/iafoss/severstal-fast-ai-256x256-crops-sub)). Though, the model could be fine-tuned on full 256x1600 images to further boost the performance. Such an approach was successfully used in [6th place solution](https://www.kaggle.com/c/airbus-ship-detection/discussion/71782#latest-558831) in Ship Detection challenge. Therefore, the train/val split is done based on full images (avoid sharing crops between train and val). The dataset for this kernel with 256x256 crops containing defects is prepared [here](https://www.kaggle.com/iafoss/256x256-images-with-defects). That kernel also provides function for RLE from/to masks conversion, generates masks, and computes the pixel statistics. Because of the 1-hour time limit in the submission kernel (20 min for prediction on the public portion of the test set), I wrote a [separate kernel](https://www.kaggle.com/iafoss/severstal-fast-ai-256x256-crops-sub) for generation of the submission based on models trained here.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport fastai\nfrom fastai.vision import *\nfrom fastai.callbacks import SaveModelCallback\nimport gc\nimport os\nfrom sklearn.model_selection import KFold\nfrom PIL import Image\nimport zipfile\nimport io\nimport cv2\nimport warnings\nfrom radam import RAdam\nwarnings.filterwarnings(\"ignore\")\n\nfastai.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sz = 256\nbs = 16\nnfolds = 4\nfold = 0\nSEED = 2019\nTRAIN = '../input/severstal-256x256-images-with-defects/images/'\nMASKS = '../input/severstal-256x256-images-with-defects/masks/'\nTRAIN_N = '../input/severstal-256x256-images-with-defects/images_n/'\nHARD_NEGATIVE = '../input/hard-negative-severstal-crops/pred.csv'\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    #tf.set_random_seed(seed)\nseed_everything(SEED)\ntorch.backends.cudnn.benchmark = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#the code below modifies fast.ai functions to incorporate Hcolumns into fast.ai Dynamic Unet\n\nfrom fastai.vision.learner import create_head, cnn_config, num_features_model, create_head\nfrom fastai.callbacks.hooks import model_sizes, hook_outputs, dummy_eval, Hook, _hook_inner\nfrom fastai.vision.models.unet import _get_sfs_idxs, UnetBlock\n\nclass Hcolumns(nn.Module):\n    def __init__(self, hooks:Collection[Hook], nc:Collection[int]=None):\n        super(Hcolumns,self).__init__()\n        self.hooks = hooks\n        self.n = len(self.hooks)\n        self.factorization = None \n        if nc is not None:\n            self.factorization = nn.ModuleList()\n            for i in range(self.n):\n                self.factorization.append(nn.Sequential(\n                    conv2d(nc[i],nc[-1],3,padding=1,bias=True),\n                    conv2d(nc[-1],nc[-1],3,padding=1,bias=True)))\n                #self.factorization.append(conv2d(nc[i],nc[-1],3,padding=1,bias=True))\n        \n    def forward(self, x:Tensor):\n        n = len(self.hooks)\n        out = [F.interpolate(self.hooks[i].stored if self.factorization is None\n            else self.factorization[i](self.hooks[i].stored), scale_factor=2**(self.n-i),\n            mode='bilinear',align_corners=False) for i in range(self.n)] + [x]\n        return torch.cat(out, dim=1)\n\nclass DynamicUnet_Hcolumns(SequentialEx):\n    \"Create a U-Net from a given architecture.\"\n    def __init__(self, encoder:nn.Module, n_classes:int, blur:bool=False, blur_final=True, \n                 self_attention:bool=False,\n                 y_range:Optional[Tuple[float,float]]=None,\n                 last_cross:bool=True, bottle:bool=False, **kwargs):\n        imsize = (256,256)\n        sfs_szs = model_sizes(encoder, size=imsize)\n        sfs_idxs = list(reversed(_get_sfs_idxs(sfs_szs)))\n        self.sfs = hook_outputs([encoder[i] for i in sfs_idxs])\n        x = dummy_eval(encoder, imsize).detach()\n\n        ni = sfs_szs[-1][1]\n        middle_conv = nn.Sequential(conv_layer(ni, ni*2, **kwargs),\n                                    conv_layer(ni*2, ni, **kwargs)).eval()\n        x = middle_conv(x)\n        layers = [encoder, batchnorm_2d(ni), nn.ReLU(), middle_conv]\n\n        self.hc_hooks = [Hook(layers[-1], _hook_inner, detach=False)]\n        hc_c = [x.shape[1]]\n        \n        for i,idx in enumerate(sfs_idxs):\n            not_final = i!=len(sfs_idxs)-1\n            up_in_c, x_in_c = int(x.shape[1]), int(sfs_szs[idx][1])\n            do_blur = blur and (not_final or blur_final)\n            sa = self_attention and (i==len(sfs_idxs)-3)\n            unet_block = UnetBlock(up_in_c, x_in_c, self.sfs[i], final_div=not_final, \n                blur=blur, self_attention=sa, **kwargs).eval()\n            layers.append(unet_block)\n            x = unet_block(x)\n            self.hc_hooks.append(Hook(layers[-1], _hook_inner, detach=False))\n            hc_c.append(x.shape[1])\n\n        ni = x.shape[1]\n        if imsize != sfs_szs[0][-2:]: layers.append(PixelShuffle_ICNR(ni, **kwargs))\n        if last_cross:\n            layers.append(MergeLayer(dense=True))\n            ni += in_channels(encoder)\n            layers.append(res_block(ni, bottle=bottle, **kwargs))\n        hc_c.append(ni)\n        layers.append(Hcolumns(self.hc_hooks, hc_c))\n        layers += [conv_layer(ni*len(hc_c), n_classes, ks=1, use_activ=False, **kwargs)]\n        if y_range is not None: layers.append(SigmoidRange(*y_range))\n        super().__init__(*layers)\n\n    def __del__(self):\n        if hasattr(self, \"sfs\"): self.sfs.remove()\n            \ndef unet_learner(data:DataBunch, arch:Callable, pretrained:bool=True, blur_final:bool=True,\n        norm_type:Optional[NormType]=NormType, split_on:Optional[SplitFuncOrIdxList]=None, \n        blur:bool=False, self_attention:bool=False, y_range:Optional[Tuple[float,float]]=None, \n        last_cross:bool=True, bottle:bool=False, cut:Union[int,Callable]=None, \n        hypercolumns=True, **learn_kwargs:Any)->Learner:\n    \"Build Unet learner from `data` and `arch`.\"\n    meta = cnn_config(arch)\n    body = create_body(arch, pretrained, cut)\n    M = DynamicUnet_Hcolumns if hypercolumns else DynamicUnet\n    model = to_device(M(body, n_classes=data.c, blur=blur, blur_final=blur_final,\n        self_attention=self_attention, y_range=y_range, norm_type=norm_type, \n        last_cross=last_cross, bottle=bottle), data.device)\n    learn = Learner(data, model, **learn_kwargs)\n    learn.split(ifnone(split_on, meta['split']))\n    if pretrained: learn.freeze()\n    apply_init(model[2], nn.init.kaiming_normal_)\n    return learn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The function below computes multiclass dice averaged for each image-class pair. This metric is used in the competition, though one should not be confused with the val score in this kernel and the model performance on the public LB. In particular, images used in training are represented by only a part of all crops (positive examples + images with texture + 12000 hard negative examples)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def dice(input:Tensor, targs:Tensor, iou:bool=False, eps:float=1e-8)->Rank0Tensor:\n    n,c = targs.shape[0], input.shape[1]\n    input = input.argmax(dim=1).view(n,-1)\n    targs = targs.view(n,-1)\n    intersect,union = [],[]\n    for i in range(1,c):\n        intersect.append(((input==i) & (targs==i)).sum(-1).float())\n        union.append(((input==i).sum(-1) + (targs==i).sum(-1)).float())\n    intersect = torch.stack(intersect)\n    union = torch.stack(union)\n    if not iou: return ((2.0*intersect + eps) / (union+eps)).mean()\n    else: return ((intersect + eps) / (union - intersect + eps)).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SegmentationLabelList(SegmentationLabelList):\n    def open(self, fn): return open_mask(fn, div=True)\n    \nclass SegmentationItemList(SegmentationItemList):\n    _label_cls = SegmentationLabelList\n\n# Setting transformations on masks to False on test set\ndef transform(self, tfms:Optional[Tuple[TfmList,TfmList]]=(None,None), **kwargs):\n    if not tfms: tfms=(None,None)\n    assert is_listy(tfms) and len(tfms) == 2\n    self.train.transform(tfms[0], **kwargs)\n    self.valid.transform(tfms[1], **kwargs)\n    kwargs['tfm_y'] = False # Test data has no labels\n    if self.test: self.test.transform(tfms[1], **kwargs)\n    return self\nfastai.data_block.ItemLists.transform = transform\n\ndef open_mask(fn:PathOrStr, div:bool=True, convert_mode:str='L', cls:type=ImageSegment,\n        after_open:Callable=None)->ImageSegment:\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", UserWarning)\n        #generate empty mask if file doesn't exist\n        x = PIL.Image.open(fn).convert(convert_mode) \\\n          if Path(fn).exists() \\\n          else PIL.Image.fromarray(np.zeros((sz,sz)).astype(np.uint8))\n    if after_open: x = after_open(x)\n    x = pil2tensor(x,np.float32)\n    return cls(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(HARD_NEGATIVE)\ndf['index'] = df.index\ndf.plot(x='index', y='pixels', kind = 'line');\nplt.yscale('log')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats = ([0.396,0.396,0.396], [0.179,0.179,0.179])\n#check https://www.kaggle.com/iafoss/256x256-images-with-defects for stats\n\n#the code below eliminates sharing patches of the same image across folds\nimg_p = set([p.stem[:-2] for p in Path(TRAIN).ls()])\n#select 12000 of the most difficult negative exaples\nneg = list(pd.read_csv(HARD_NEGATIVE).head(12000).fname)\nneg = [Path(TRAIN_N)/f for f in neg]\nimg_n = set([p.stem for p in neg])\nimg_set = img_p | img_n\nimg_p_list = sorted(img_p)\nimg_n_list = sorted(img_n)\nimg_list = img_p_list + img_n_list\nkf = KFold(n_splits=nfolds, shuffle=True, random_state=SEED)\n\ndef get_data(fold):\n    #split with making sure that crops of the same original image \n    #are not shared between folds, so additional training and validation \n    #could be done on full images later\n    valid_idx = list(kf.split(list(range(len(img_list)))))[fold][1]\n    valid = set([img_list[i] for i in valid_idx])\n    valid_idx = []\n    for i,p in enumerate(Path(TRAIN).ls() + neg):\n        if p.stem[:-2] in valid: valid_idx.append(i)\n            \n    # Create databunch\n    sl = SegmentationItemList.from_folder(TRAIN)\n    sl.items = np.array((list(sl.items) + neg))\n    data = (sl.split_by_idx(valid_idx)\n        .label_from_func(lambda x : str(x).replace('/images', '/masks'), classes=[0,1,2,3,4])\n        .transform(get_transforms(xtra_tfms=dihedral()), size=sz, tfm_y=True)\n        .databunch(path=Path('.'), bs=bs)\n        .normalize(stats))\n    return data\n\n# Display some images with masks\nget_data(0).show_batch()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"@dataclass\nclass CSVLogger(LearnerCallback):\n    def __init__(self, learn, filename= 'history'):\n        self.learn = learn\n        self.path = self.learn.path/f'{filename}.csv'\n        self.file = None\n\n    @property\n    def header(self):\n        return self.learn.recorder.names\n\n    def read_logged_file(self):\n        return pd.read_csv(self.path)\n\n    def on_train_begin(self, metrics_names: StrList, **kwargs: Any) -> None:\n        self.path.parent.mkdir(parents=True, exist_ok=True)\n        e = self.path.exists()\n        self.file = self.path.open('a')\n        if not e: self.file.write(','.join(self.header) + '\\n')\n\n    def on_epoch_end(self, epoch: int, smooth_loss: Tensor, last_metrics: MetricsList, **kwargs: Any) -> bool:\n        self.write_stats([epoch, smooth_loss] + last_metrics)\n\n    def on_train_end(self, **kwargs: Any) -> None:\n        self.file.flush()\n        self.file.close()\n\n    def write_stats(self, stats: TensorOrNumList) -> None:\n        stats = [str(stat) if isinstance(stat, int) else f'{stat:.6f}'\n                 for name, stat in zip(self.header, stats)]\n        str_stats = ','.join(stats)\n        self.file.write(str_stats + '\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The function below generates predictions with 3 fold TTA (horizontal flip, vertical flip, and both). The default fast.ai implementation is too memory hungry since predictions for all images and TTA folds are kept in memory. So it becomes hardly possible to generate a prediction for a reasonable number of high resolution images."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction with flip TTA\ndef model_pred(learn:fastai.basic_train.Learner,F_save,\n        ds_type:fastai.basic_data.DatasetType=DatasetType.Valid, \n        tta:bool=True): #if use train dl, disable shuffling\n    learn.model.eval();\n    dl = learn.data.dl(ds_type)\n    #sampler = dl.batch_sampler.sampler\n    #dl.batch_sampler.sampler = torch.utils.data.sampler.SequentialSampler(sampler.data_source)\n    name_list = [Path(n).stem for n in dl.dataset.items]\n    num_batchs = len(dl)\n    t = progress_bar(iter(dl), leave=False, total=num_batchs)\n    count = 0\n    with torch.no_grad():\n        for x,y in t:\n            x = x.cuda()\n            py = torch.softmax(learn.model(x),dim=1).permute(0,2,3,1).detach()\n            if tta:\n                flips = [[-1],[-2],[-2,-1]]\n                for f in flips:\n                    py += torch.softmax(torch.flip(learn.model(torch.flip(x,f)),f),dim=1)\\\n                      .permute(0,2,3,1).detach()\n                py /= (1+len(flips))\n            py = py.cpu().numpy()\n            batch_size = len(py)\n            for i in range(batch_size):\n                taget = y[i].detach().cpu().numpy() if y is not None else None\n                F_save(py[i],taget,name_list[count])\n                count += 1\n    #dl.batch_sampler.sampler = sampler\n    \ndef save_img(data,name,out):\n    data = data[:,:,1:]\n    img = cv2.imencode('.png',(data*255).astype(np.uint8))[1]\n    out.writestr(name, img)\n    \n#dice for threshold selection\ndef dice_np(pred, targs, noise_th = 0, eps=1e-7):\n    targs = targs[0,:,:]\n    c = pred.shape[-1]\n    pred = np.argmax(pred, axis=-1)\n    dices = []\n    for i in range(1,c):\n        if (pred==i).sum() > noise_th:\n            intersect = ((pred==i) & (targs==i)).sum().astype(np.float)\n            union = ((pred==i).sum() + (targs==i).sum()).astype(np.float)\n            dices.append((2.0*intersect + eps) / (union + eps))\n        else: dices.append( 1.0 if (targs==i).sum() == 0 else 0.0)\n    return np.array(dices).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{},"cell_type":"markdown","source":"Check log{fold}.csv files in the output to see details of training for each fold."},{"metadata":{"trusted":true},"cell_type":"code","source":"dices = []\nnoise_ths = np.arange(0, 1501, 125)\n\nwith zipfile.ZipFile('val_masks_tta.zip', 'w') as archive_out:\n    #the function to save val masks and dices\n    def to_mask(yp, y, id):\n        name = id + '.png'\n        save_img(yp,name,archive_out)\n        dices_th = []\n        for noise_th in noise_ths:\n            dices_th.append(dice_np(yp,y,noise_th))\n        dices.append(dices_th)\n        \n    \n    data = get_data(fold)\n    learn = unet_learner(data, models.resnet34, metrics=[dice], opt_func=RAdam)\n    learn.clip_grad(1.0);\n    logger = CSVLogger(learn,f'log{fold}')\n\n    #fit the decoder part of the model keeping the encode frozen\n    lr = 1e-3\n    learn.fit_one_cycle(4, lr, callbacks = [logger])\n    \n    #fit entire model with saving on the best epoch\n    learn.unfreeze()\n    learn.fit_one_cycle(15, slice(lr/50, lr/2), callbacks = [logger])\n        \n    #save model\n    learn.save('fold'+str(fold));\n    np.save('items_fold'+str(fold), data.valid_ds.items)\n     \n    #run TTA prediction on val, save masks and dices\n    model_pred(learn,to_mask)\n    \n    gc.collect()\n    torch.cuda.empty_cache()\ndices = np.array(dices).mean(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this competition FP predictions are heavily penalized: if a model predicts even a single pixel for an image without particular kind of defect, the score of the predicted mask goes to zero. Meanwhile, if model predicts zero pixels for such mask, the score is 1. It is a series issue given the fraction of empty masks (~0.85 according to empty mask benchmark submission). To eliminate FP, we can impose a threshold on the minimum number of predicted pixels in a mask. If the prediction has less pixels, they are considered as noise and removed. The plot below depicts the optimal value of noise_th. However, you should keep in mind that the optimal value of noise_th is larger if the prediction is generated not on crops but on full images. In particular, according to my experiments, the optimal threshold for full size images is approximetly 4 times larger."},{"metadata":{"trusted":true},"cell_type":"code","source":"best_dice = dices.max()\nbest_thr = noise_ths[dices.argmax()]\nplt.figure(figsize=(8,4))\nplt.plot(noise_ths, dices)\nplt.vlines(x=best_thr, ymin=dices.min(), ymax=dices.max())\nplt.text(best_thr+50, best_dice-0.01, f'DICE = {best_dice:.3f}', fontsize=14);\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}