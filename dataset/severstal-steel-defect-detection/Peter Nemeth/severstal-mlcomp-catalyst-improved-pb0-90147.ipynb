{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir efficientnet\n!cp -r ../input/segmentation-masters/efficientnet-master/efficientnet-master/efficientnet/* ./efficientnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir classification_models\n!cp -r ../input/segmentation-masters/classification_models-master/classification_models-master/classification_models/* ./classification_models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir segmentation_models\n!cp -r ../input/segmentation-masters/segmentation_models-master/segmentation_models-master/segmentation_models/* ./segmentation_models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ./efficientnet","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the competition does not allow commit with the kernel that uses internet connection, we use offline installation"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"! python ../input/mlcomp/mlcomp/mlcomp/setup.py","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import required libraries"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport gc\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport cv2\nimport albumentations as A\nfrom tqdm import tqdm_notebook\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.jit import load\n\nfrom mlcomp.contrib.transform.albumentations import ChannelTranspose\nfrom mlcomp.contrib.dataset.classify import ImageDataset\nfrom mlcomp.contrib.transform.rle import rle2mask, mask2rle\nfrom mlcomp.contrib.transform.tta import TtaWrap","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Catalyst allows to trace models. That is an extremely useful features in Pytorch since 1.0 version: \n\nhttps://pytorch.org/docs/stable/jit.html\n\nNow we can load models without re-defining them"},{"metadata":{"trusted":true},"cell_type":"code","source":"unet_se_resnext50_32x4d = load('/kaggle/input/severstalmodels/unet_se_resnext50_32x4d.pth').cuda()\nunet_mobilenet2 = load('/kaggle/input/severstalmodels/unet_mobilenet2.pth').cuda()\nunet_resnet34 = load('/kaggle/input/severstalmodels/unet_resnet34.pth').cuda()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Models' mean aggregator"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"class Model:\n    def __init__(self, models):\n        self.models = models\n    \n    def __call__(self, x):\n        res = []\n        x = x.cuda()\n        with torch.no_grad():\n            for m in self.models:\n                res.append(m(x))\n        res = torch.stack(res)\n        return torch.mean(res, dim=0)\n\nmodel = Model([unet_se_resnext50_32x4d, unet_mobilenet2, unet_resnet34])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create TTA transforms, datasets, loaders"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def create_transforms(additional):\n    res = list(additional)\n    # add necessary transformations\n    res.extend([\n        A.Normalize(\n            mean=(0.485, 0.456, 0.406), std=(0.230, 0.225, 0.223)\n        ),\n        ChannelTranspose()\n    ])\n    res = A.Compose(res)\n    return res\n\nimg_folder = '/kaggle/input/severstal-steel-defect-detection/test_images'\nbatch_size = 2\nnum_workers = 0\n\n# Different transforms for TTA wrapper\ntransforms = [\n    [],\n    [A.HorizontalFlip(p=1)]\n]\n\ntransforms = [create_transforms(t) for t in transforms]\ndatasets = [TtaWrap(ImageDataset(img_folder=img_folder, transforms=t), tfms=t) for t in transforms]\nloaders = [DataLoader(d, num_workers=num_workers, batch_size=batch_size, shuffle=False) for d in datasets]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loaders' mean aggregator"},{"metadata":{"trusted":true},"cell_type":"code","source":"thresholds = [0.5, 0.7, 0.5, 0.5]\nmin_area = [600, 600, 1000, 2000]\n\nres = []\n# Iterate over all TTA loaders\ntotal = len(datasets[0])//batch_size\nfor loaders_batch in tqdm_notebook(zip(*loaders), total=total):\n    preds = []\n    image_file = []\n    for i, batch in enumerate(loaders_batch):\n        features = batch['features'].cuda()\n        p = torch.sigmoid(model(features))\n        # inverse operations for TTA\n        p = datasets[i].inverse(p)\n        preds.append(p)\n        image_file = batch['image_file']\n    \n    # TTA mean\n    preds = torch.stack(preds)\n    preds = torch.mean(preds, dim=0)\n    preds = preds.detach().cpu().numpy()\n    \n    # Batch post processing\n    for p, file in zip(preds, image_file):\n        file = os.path.basename(file)\n        # Image postprocessing\n        for i in range(4):\n            p_channel = p[i]\n            imageid_classid = file+'_'+str(i+1)\n            p_channel = (p_channel>thresholds[i]).astype(np.uint8)\n            if p_channel.sum() < min_area[i]:\n                p_channel = np.zeros(p_channel.shape, dtype=p_channel.dtype)\n\n            res.append({\n                'ImageId_ClassId': imageid_classid,\n                'EncodedPixels': mask2rle(p_channel)\n            })\n            \ndf = pd.DataFrame(res)\ndf = df.fillna('')   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport random\nimport math as math\nimport os\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom PIL import Image\nimport os\nimport tensorflow as tf\nimport keras.backend as K\nfrom keras.utils import Sequence\nfrom keras.applications import Xception\nfrom keras.layers import UpSampling2D, Conv2D, Activation, LeakyReLU, BatchNormalization\nfrom keras import Model\nfrom keras.losses import binary_crossentropy\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom keras.optimizers import Adam\n\nimport imgaug as ia\nimport imgaug.augmenters as iaa\nimport segmentation_models as sm\n\nimport efficientnet.keras as efn \nfrom keras.optimizers import Adam\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base = efn.EfficientNetB3(weights=None, include_top=False, input_shape=(256, 1600, 3), pooling='avg')\nbase.trainable=True\ndropout_dense_layer = 0.2 # for B0\n\nclassifier_model = Sequential()\nclassifier_model.add(base)\nclassifier_model.add(Dropout(dropout_dense_layer))\nclassifier_model.add(Dense(4, activation='sigmoid'))\n\nclassifier_model.compile(\n    loss='binary_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)\n\nclassifier_model.load_weights('../input/single-models/B3_opt_multi_new3.h5')\n\nIMAGE_RGB_MEAN = [0.485, 0.456, 0.406]\nIMAGE_RGB_STD  = [0.229, 0.224, 0.225]\n\ndef normalize(images):\n    images = images / 255.0\n    images[:,:,:,0] = (images[:,:,:,0]-IMAGE_RGB_MEAN[0])/IMAGE_RGB_STD[0]\n    images[:,:,:,1] = (images[:,:,:,1]-IMAGE_RGB_MEAN[1])/IMAGE_RGB_STD[1]\n    images[:,:,:,2] = (images[:,:,:,2]-IMAGE_RGB_MEAN[2])/IMAGE_RGB_STD[2]\n    return images\n    \ndef denormalize(images):\n    images[:,:,:,0] = images[:,:,:,0]*IMAGE_RGB_STD[0]+IMAGE_RGB_MEAN[0]\n    images[:,:,:,1] = images[:,:,:,1]*IMAGE_RGB_STD[1]+IMAGE_RGB_MEAN[1]\n    images[:,:,:,2] = images[:,:,:,2]*IMAGE_RGB_STD[2]+IMAGE_RGB_MEAN[2]\n    images = np.array(images * 255).astype('uint8')\n    \nTEST_PATH = '../input/severstal-steel-defect-detection/test_images/'\ndef prepareData(source, path, cleanup=True):\n    source['defect'] = False\n    if cleanup: source.EncodedPixels = ''\n    \n    source['ClassId'] = source['ImageId_ClassId'].str[-1:]\n    source['ImageId'] = source['ImageId_ClassId'].str[:-2]\n    source = source[['ImageId_ClassId', 'ImageId','ClassId','defect','EncodedPixels']]\n    source.ClassId = source.ClassId.astype('int')\n    source['path'] = path + source['ImageId']\n    return source","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('../input/severstal-steel-defect-detection/sample_submission.csv')\ntest_df = prepareData(test_df, TEST_PATH)\n\n# Predictions\n\nids = test_df['ImageId'].unique()\ntest_df.EncodedPixels = ''\n\n\nclass_weights = [0.5, 0.5, 0.5, 0.5] \n\nfor picIdx in tqdm_notebook(range(len(ids))):\n    \n    batch = np.zeros((4, 256, 1600, 3))\n    filename = ids[picIdx]  \n    img = cv2.imread(TEST_PATH+filename)    \n\n    batch[0, :, :, :] = img\n    batch[1, :, :, :] = img[:,::-1,:]\n    batch[2, :, :, :] = img[::-1,:,:]\n    batch[3, :, :, :] = img[::-1,::-1,:]\n    \n    batch = normalize(batch)\n    classTTA = classifier_model.predict(batch)\n    hasDefect = np.mean(classTTA, axis=0)>class_weights\n\n    for i, classId in enumerate(hasDefect):\n        if classId == False:\n\n            name = filename+\"_\"+str(i+1)\n            line = test_df[test_df.ImageId_ClassId == name].index[0] \n\n            test_df.loc[line, 'EncodedPixels'] = -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mark = test_df[test_df.EncodedPixels == -1]\nmark.ClassId.value_counts()\n\nsub = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for filename in mark.ImageId_ClassId:\n    sub.loc[sub.ImageId_ClassId == filename, 'EncodedPixels'] = -1\n\nsub = sub.replace(-1, '')\nsub.to_csv('submission.csv', index=False)\nsub.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Histogram of predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['Image'] = sub['ImageId_ClassId'].map(lambda x: x.split('_')[0])\nsub['Class'] = sub['ImageId_ClassId'].map(lambda x: x.split('_')[1])\nsub['empty'] = sub['EncodedPixels'].map(lambda x: not x)\nsub[sub['empty'] == False]['Class'].value_counts()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"2d558d43f12745d8a826eb84f7dcd0b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"60b23d613e584fa0abb75460b84b1d80":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e3c6cad600e04ebfb71a6c202a8eb513","IPY_MODEL_d6bd450456d24af1b8f635c10fee4445"],"layout":"IPY_MODEL_a7f23b4d7ebb4499b486f861d1c52b77"}},"6feb39965873459cb61d32fbbc2c2225":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7f23b4d7ebb4499b486f861d1c52b77":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c389718f59fc45eba86cebc46dc72f99":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6bd450456d24af1b8f635c10fee4445":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c389718f59fc45eba86cebc46dc72f99","placeholder":"â€‹","style":"IPY_MODEL_dcff928e32c743529e899d038dee4e66","value":" 901/? [05:36&lt;00:00,  2.67it/s]"}},"dcff928e32c743529e899d038dee4e66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3c6cad600e04ebfb71a6c202a8eb513":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6feb39965873459cb61d32fbbc2c2225","max":900,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2d558d43f12745d8a826eb84f7dcd0b2","value":900}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":1}