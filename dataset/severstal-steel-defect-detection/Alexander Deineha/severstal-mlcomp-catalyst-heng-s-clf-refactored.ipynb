{"cells":[{"metadata":{},"cell_type":"markdown","source":"![MLComp](https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/pics/MLcomp.png)\n![Catalyst](https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/pics/catalyst_logo.png)"},{"metadata":{},"cell_type":"markdown","source":"This kernel demonstrates:\n\n1. Results of training models with [the training kernel](https://www.kaggle.com/lightforever/severstal-mlcomp-catalyst-train-0-90672-offline) and achieves 0.90672 score on public LB\n\n2. Useful code in MLComp library: TtaWrapp, ImageDataset, ChannelTranspose, rle utilities\n\n3. Output statistics and basic visualization"},{"metadata":{},"cell_type":"markdown","source":"Approach descripton:\n\n1. Segmentation via 3 Unet networks. The predictions are being averaged. \n\n2. Thresholding and removeing small areas. This method gives 0.90672 on public LB.\n\n**Improving**:\n\n1. As many participations have seen, that is the key to remove false positives from your predictions.\n\n2. To cope with that, a classification network may be used. \n\n3. Heng CherKeng posted a classifier here: https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/106462#latest-634450 resent34_cls_01, **if you remove false positives with it you should get 0.9117 on LB**"},{"metadata":{},"cell_type":"markdown","source":"About the libraries:\n\n1. [MLComp](https://github.com/catalyst-team/mlcomp) is a distributed DAG  (Directed acyclic graph)  framework for machine learning with UI. It helps to train, manipulate, and visualize. All models in this kernel were trained offline via MLComp + Catalyst libraries. \n\nYou can control an execution process via Web-site\n\nDags\n![Dags](https://github.com/catalyst-team/mlcomp/blob/master/docs/imgs/dags.png?raw=true)\n\nComputers\n![Computers](https://github.com/catalyst-team/mlcomp/blob/master/docs/imgs/computers.png?raw=true)\n\nReports\n![Reports](https://github.com/catalyst-team/mlcomp/blob/master/docs/imgs/reports.png?raw=true)\n\nCode\n![Code](https://github.com/catalyst-team/mlcomp/blob/master/docs/imgs/code.png?raw=true)\n\nPlease follow [the web site](https://github.com/catalyst-team/mlcomp) to get the details.\n\nhttps://github.com/catalyst-team/mlcomp\n\n2. Catalys: High-level utils for PyTorch DL & RL research. It was developed with a focus on reproducibility, fast experimentation and code/ideas reusing. Being able to research/develop something new, rather then write another regular train loop. Break the cycle - use the Catalyst!\n\nhttps://github.com/catalyst-team/catalyst\n\nDocs and examples\n- Detailed [classification tutorial](https://github.com/catalyst-team/catalyst/blob/master/examples/notebooks/classification-tutorial.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/catalyst-team/catalyst/blob/master/examples/notebooks/classification-tutorial.ipynb)\n- Comprehensive [classification pipeline](https://github.com/catalyst-team/classification).\n\nAPI documentation and an overview of the library can be found here\n[![Docs](https://img.shields.io/badge/dynamic/json.svg?label=docs&url=https%3A%2F%2Fpypi.org%2Fpypi%2Fcatalyst%2Fjson&query=%24.info.version&colorB=brightgreen&prefix=v)](https://catalyst-team.github.io/catalyst/index.html)"},{"metadata":{},"cell_type":"markdown","source":"### Install MLComp library(offline version):"},{"metadata":{},"cell_type":"markdown","source":"As the competition does not allow commit with the kernel that uses internet connection, we use offline installation"},{"metadata":{},"cell_type":"markdown","source":"### Get heng's classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\ntime_start = time.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/pretrainedmodels/pretrainedmodels-0.7.4/pretrainedmodels-0.7.4/ > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! python ../input/mlcomp/mlcomp/mlcomp/setup.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ! pip install ../input/pytorch-toolbelt/pytorch-toolbelt-develop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"package_path = '../input/senetunetmodelcode' # add unet script dataset\nimport sys\nsys.path.append(package_path)\n\n# Get necessary Imports\nimport pdb\nimport os\nimport cv2\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data import DataLoader, Dataset\nfrom albumentations import (Normalize, Compose)\nfrom albumentations.pytorch import ToTensor\nimport torch.utils.data as data\nimport torchvision.models as models\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom senet_unet_model_code import Unet\n\n##########################################\n\nimport matplotlib.pyplot as plt\n\nimport albumentations as A\nfrom tqdm import tqdm_notebook\nfrom torch.jit import load\n\nfrom mlcomp.contrib.transform.albumentations import ChannelTranspose\nfrom mlcomp.contrib.dataset.classify import ImageDataset\nfrom mlcomp.contrib.transform.rle import rle2mask, mask2rle\nfrom mlcomp.contrib.transform.tta import TtaWrap\n\n###########################################\n\n# from pathlib import Path\n\n# from pytorch_toolbelt.inference import tta\n# from pytorch_toolbelt.utils.rle import rle_encode\n# from pytorch_toolbelt.utils.rle import rle_decode\n# from pytorch_toolbelt.utils.rle import rle_to_string\n# from pytorch_toolbelt.inference.functional import pad_image_tensor, unpad_image_tensor\n# from pytorch_toolbelt.modules import decoders as D\n# from pytorch_toolbelt.modules import encoders as E\n# from pytorch_toolbelt.modules.fpn import *\n\n# from models import PretrainedUNet\n\n###########################################\nimport fastai\nfrom fastai.vision import *\nfrom PIL import Image\nimport zipfile\nimport io\n\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def load_hengs_clf_model():\n    # Codes from Heng's baseline\n    # This code is for classifcation model\n\n    BatchNorm2d = nn.BatchNorm2d\n\n    ###############################################################################\n    CONVERSION=[\n     'block0.0.weight',\t(64, 3, 7, 7),\t 'conv1.weight',\t(64, 3, 7, 7),\n     'block0.1.weight',\t(64,),\t 'bn1.weight',\t(64,),\n     'block0.1.bias',\t(64,),\t 'bn1.bias',\t(64,),\n     'block0.1.running_mean',\t(64,),\t 'bn1.running_mean',\t(64,),\n     'block0.1.running_var',\t(64,),\t 'bn1.running_var',\t(64,),\n     'block1.1.conv_bn1.conv.weight',\t(64, 64, 3, 3),\t 'layer1.0.conv1.weight',\t(64, 64, 3, 3),\n     'block1.1.conv_bn1.bn.weight',\t(64,),\t 'layer1.0.bn1.weight',\t(64,),\n     'block1.1.conv_bn1.bn.bias',\t(64,),\t 'layer1.0.bn1.bias',\t(64,),\n     'block1.1.conv_bn1.bn.running_mean',\t(64,),\t 'layer1.0.bn1.running_mean',\t(64,),\n     'block1.1.conv_bn1.bn.running_var',\t(64,),\t 'layer1.0.bn1.running_var',\t(64,),\n     'block1.1.conv_bn2.conv.weight',\t(64, 64, 3, 3),\t 'layer1.0.conv2.weight',\t(64, 64, 3, 3),\n     'block1.1.conv_bn2.bn.weight',\t(64,),\t 'layer1.0.bn2.weight',\t(64,),\n     'block1.1.conv_bn2.bn.bias',\t(64,),\t 'layer1.0.bn2.bias',\t(64,),\n     'block1.1.conv_bn2.bn.running_mean',\t(64,),\t 'layer1.0.bn2.running_mean',\t(64,),\n     'block1.1.conv_bn2.bn.running_var',\t(64,),\t 'layer1.0.bn2.running_var',\t(64,),\n     'block1.2.conv_bn1.conv.weight',\t(64, 64, 3, 3),\t 'layer1.1.conv1.weight',\t(64, 64, 3, 3),\n     'block1.2.conv_bn1.bn.weight',\t(64,),\t 'layer1.1.bn1.weight',\t(64,),\n     'block1.2.conv_bn1.bn.bias',\t(64,),\t 'layer1.1.bn1.bias',\t(64,),\n     'block1.2.conv_bn1.bn.running_mean',\t(64,),\t 'layer1.1.bn1.running_mean',\t(64,),\n     'block1.2.conv_bn1.bn.running_var',\t(64,),\t 'layer1.1.bn1.running_var',\t(64,),\n     'block1.2.conv_bn2.conv.weight',\t(64, 64, 3, 3),\t 'layer1.1.conv2.weight',\t(64, 64, 3, 3),\n     'block1.2.conv_bn2.bn.weight',\t(64,),\t 'layer1.1.bn2.weight',\t(64,),\n     'block1.2.conv_bn2.bn.bias',\t(64,),\t 'layer1.1.bn2.bias',\t(64,),\n     'block1.2.conv_bn2.bn.running_mean',\t(64,),\t 'layer1.1.bn2.running_mean',\t(64,),\n     'block1.2.conv_bn2.bn.running_var',\t(64,),\t 'layer1.1.bn2.running_var',\t(64,),\n     'block1.3.conv_bn1.conv.weight',\t(64, 64, 3, 3),\t 'layer1.2.conv1.weight',\t(64, 64, 3, 3),\n     'block1.3.conv_bn1.bn.weight',\t(64,),\t 'layer1.2.bn1.weight',\t(64,),\n     'block1.3.conv_bn1.bn.bias',\t(64,),\t 'layer1.2.bn1.bias',\t(64,),\n     'block1.3.conv_bn1.bn.running_mean',\t(64,),\t 'layer1.2.bn1.running_mean',\t(64,),\n     'block1.3.conv_bn1.bn.running_var',\t(64,),\t 'layer1.2.bn1.running_var',\t(64,),\n     'block1.3.conv_bn2.conv.weight',\t(64, 64, 3, 3),\t 'layer1.2.conv2.weight',\t(64, 64, 3, 3),\n     'block1.3.conv_bn2.bn.weight',\t(64,),\t 'layer1.2.bn2.weight',\t(64,),\n     'block1.3.conv_bn2.bn.bias',\t(64,),\t 'layer1.2.bn2.bias',\t(64,),\n     'block1.3.conv_bn2.bn.running_mean',\t(64,),\t 'layer1.2.bn2.running_mean',\t(64,),\n     'block1.3.conv_bn2.bn.running_var',\t(64,),\t 'layer1.2.bn2.running_var',\t(64,),\n     'block2.0.conv_bn1.conv.weight',\t(128, 64, 3, 3),\t 'layer2.0.conv1.weight',\t(128, 64, 3, 3),\n     'block2.0.conv_bn1.bn.weight',\t(128,),\t 'layer2.0.bn1.weight',\t(128,),\n     'block2.0.conv_bn1.bn.bias',\t(128,),\t 'layer2.0.bn1.bias',\t(128,),\n     'block2.0.conv_bn1.bn.running_mean',\t(128,),\t 'layer2.0.bn1.running_mean',\t(128,),\n     'block2.0.conv_bn1.bn.running_var',\t(128,),\t 'layer2.0.bn1.running_var',\t(128,),\n     'block2.0.conv_bn2.conv.weight',\t(128, 128, 3, 3),\t 'layer2.0.conv2.weight',\t(128, 128, 3, 3),\n     'block2.0.conv_bn2.bn.weight',\t(128,),\t 'layer2.0.bn2.weight',\t(128,),\n     'block2.0.conv_bn2.bn.bias',\t(128,),\t 'layer2.0.bn2.bias',\t(128,),\n     'block2.0.conv_bn2.bn.running_mean',\t(128,),\t 'layer2.0.bn2.running_mean',\t(128,),\n     'block2.0.conv_bn2.bn.running_var',\t(128,),\t 'layer2.0.bn2.running_var',\t(128,),\n     'block2.0.shortcut.conv.weight',\t(128, 64, 1, 1),\t 'layer2.0.downsample.0.weight',\t(128, 64, 1, 1),\n     'block2.0.shortcut.bn.weight',\t(128,),\t 'layer2.0.downsample.1.weight',\t(128,),\n     'block2.0.shortcut.bn.bias',\t(128,),\t 'layer2.0.downsample.1.bias',\t(128,),\n     'block2.0.shortcut.bn.running_mean',\t(128,),\t 'layer2.0.downsample.1.running_mean',\t(128,),\n     'block2.0.shortcut.bn.running_var',\t(128,),\t 'layer2.0.downsample.1.running_var',\t(128,),\n     'block2.1.conv_bn1.conv.weight',\t(128, 128, 3, 3),\t 'layer2.1.conv1.weight',\t(128, 128, 3, 3),\n     'block2.1.conv_bn1.bn.weight',\t(128,),\t 'layer2.1.bn1.weight',\t(128,),\n     'block2.1.conv_bn1.bn.bias',\t(128,),\t 'layer2.1.bn1.bias',\t(128,),\n     'block2.1.conv_bn1.bn.running_mean',\t(128,),\t 'layer2.1.bn1.running_mean',\t(128,),\n     'block2.1.conv_bn1.bn.running_var',\t(128,),\t 'layer2.1.bn1.running_var',\t(128,),\n     'block2.1.conv_bn2.conv.weight',\t(128, 128, 3, 3),\t 'layer2.1.conv2.weight',\t(128, 128, 3, 3),\n     'block2.1.conv_bn2.bn.weight',\t(128,),\t 'layer2.1.bn2.weight',\t(128,),\n     'block2.1.conv_bn2.bn.bias',\t(128,),\t 'layer2.1.bn2.bias',\t(128,),\n     'block2.1.conv_bn2.bn.running_mean',\t(128,),\t 'layer2.1.bn2.running_mean',\t(128,),\n     'block2.1.conv_bn2.bn.running_var',\t(128,),\t 'layer2.1.bn2.running_var',\t(128,),\n     'block2.2.conv_bn1.conv.weight',\t(128, 128, 3, 3),\t 'layer2.2.conv1.weight',\t(128, 128, 3, 3),\n     'block2.2.conv_bn1.bn.weight',\t(128,),\t 'layer2.2.bn1.weight',\t(128,),\n     'block2.2.conv_bn1.bn.bias',\t(128,),\t 'layer2.2.bn1.bias',\t(128,),\n     'block2.2.conv_bn1.bn.running_mean',\t(128,),\t 'layer2.2.bn1.running_mean',\t(128,),\n     'block2.2.conv_bn1.bn.running_var',\t(128,),\t 'layer2.2.bn1.running_var',\t(128,),\n     'block2.2.conv_bn2.conv.weight',\t(128, 128, 3, 3),\t 'layer2.2.conv2.weight',\t(128, 128, 3, 3),\n     'block2.2.conv_bn2.bn.weight',\t(128,),\t 'layer2.2.bn2.weight',\t(128,),\n     'block2.2.conv_bn2.bn.bias',\t(128,),\t 'layer2.2.bn2.bias',\t(128,),\n     'block2.2.conv_bn2.bn.running_mean',\t(128,),\t 'layer2.2.bn2.running_mean',\t(128,),\n     'block2.2.conv_bn2.bn.running_var',\t(128,),\t 'layer2.2.bn2.running_var',\t(128,),\n     'block2.3.conv_bn1.conv.weight',\t(128, 128, 3, 3),\t 'layer2.3.conv1.weight',\t(128, 128, 3, 3),\n     'block2.3.conv_bn1.bn.weight',\t(128,),\t 'layer2.3.bn1.weight',\t(128,),\n     'block2.3.conv_bn1.bn.bias',\t(128,),\t 'layer2.3.bn1.bias',\t(128,),\n     'block2.3.conv_bn1.bn.running_mean',\t(128,),\t 'layer2.3.bn1.running_mean',\t(128,),\n     'block2.3.conv_bn1.bn.running_var',\t(128,),\t 'layer2.3.bn1.running_var',\t(128,),\n     'block2.3.conv_bn2.conv.weight',\t(128, 128, 3, 3),\t 'layer2.3.conv2.weight',\t(128, 128, 3, 3),\n     'block2.3.conv_bn2.bn.weight',\t(128,),\t 'layer2.3.bn2.weight',\t(128,),\n     'block2.3.conv_bn2.bn.bias',\t(128,),\t 'layer2.3.bn2.bias',\t(128,),\n     'block2.3.conv_bn2.bn.running_mean',\t(128,),\t 'layer2.3.bn2.running_mean',\t(128,),\n     'block2.3.conv_bn2.bn.running_var',\t(128,),\t 'layer2.3.bn2.running_var',\t(128,),\n     'block3.0.conv_bn1.conv.weight',\t(256, 128, 3, 3),\t 'layer3.0.conv1.weight',\t(256, 128, 3, 3),\n     'block3.0.conv_bn1.bn.weight',\t(256,),\t 'layer3.0.bn1.weight',\t(256,),\n     'block3.0.conv_bn1.bn.bias',\t(256,),\t 'layer3.0.bn1.bias',\t(256,),\n     'block3.0.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.0.bn1.running_mean',\t(256,),\n     'block3.0.conv_bn1.bn.running_var',\t(256,),\t 'layer3.0.bn1.running_var',\t(256,),\n     'block3.0.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.0.conv2.weight',\t(256, 256, 3, 3),\n     'block3.0.conv_bn2.bn.weight',\t(256,),\t 'layer3.0.bn2.weight',\t(256,),\n     'block3.0.conv_bn2.bn.bias',\t(256,),\t 'layer3.0.bn2.bias',\t(256,),\n     'block3.0.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.0.bn2.running_mean',\t(256,),\n     'block3.0.conv_bn2.bn.running_var',\t(256,),\t 'layer3.0.bn2.running_var',\t(256,),\n     'block3.0.shortcut.conv.weight',\t(256, 128, 1, 1),\t 'layer3.0.downsample.0.weight',\t(256, 128, 1, 1),\n     'block3.0.shortcut.bn.weight',\t(256,),\t 'layer3.0.downsample.1.weight',\t(256,),\n     'block3.0.shortcut.bn.bias',\t(256,),\t 'layer3.0.downsample.1.bias',\t(256,),\n     'block3.0.shortcut.bn.running_mean',\t(256,),\t 'layer3.0.downsample.1.running_mean',\t(256,),\n     'block3.0.shortcut.bn.running_var',\t(256,),\t 'layer3.0.downsample.1.running_var',\t(256,),\n     'block3.1.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.1.conv1.weight',\t(256, 256, 3, 3),\n     'block3.1.conv_bn1.bn.weight',\t(256,),\t 'layer3.1.bn1.weight',\t(256,),\n     'block3.1.conv_bn1.bn.bias',\t(256,),\t 'layer3.1.bn1.bias',\t(256,),\n     'block3.1.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.1.bn1.running_mean',\t(256,),\n     'block3.1.conv_bn1.bn.running_var',\t(256,),\t 'layer3.1.bn1.running_var',\t(256,),\n     'block3.1.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.1.conv2.weight',\t(256, 256, 3, 3),\n     'block3.1.conv_bn2.bn.weight',\t(256,),\t 'layer3.1.bn2.weight',\t(256,),\n     'block3.1.conv_bn2.bn.bias',\t(256,),\t 'layer3.1.bn2.bias',\t(256,),\n     'block3.1.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.1.bn2.running_mean',\t(256,),\n     'block3.1.conv_bn2.bn.running_var',\t(256,),\t 'layer3.1.bn2.running_var',\t(256,),\n     'block3.2.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.2.conv1.weight',\t(256, 256, 3, 3),\n     'block3.2.conv_bn1.bn.weight',\t(256,),\t 'layer3.2.bn1.weight',\t(256,),\n     'block3.2.conv_bn1.bn.bias',\t(256,),\t 'layer3.2.bn1.bias',\t(256,),\n     'block3.2.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.2.bn1.running_mean',\t(256,),\n     'block3.2.conv_bn1.bn.running_var',\t(256,),\t 'layer3.2.bn1.running_var',\t(256,),\n     'block3.2.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.2.conv2.weight',\t(256, 256, 3, 3),\n     'block3.2.conv_bn2.bn.weight',\t(256,),\t 'layer3.2.bn2.weight',\t(256,),\n     'block3.2.conv_bn2.bn.bias',\t(256,),\t 'layer3.2.bn2.bias',\t(256,),\n     'block3.2.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.2.bn2.running_mean',\t(256,),\n     'block3.2.conv_bn2.bn.running_var',\t(256,),\t 'layer3.2.bn2.running_var',\t(256,),\n     'block3.3.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.3.conv1.weight',\t(256, 256, 3, 3),\n     'block3.3.conv_bn1.bn.weight',\t(256,),\t 'layer3.3.bn1.weight',\t(256,),\n     'block3.3.conv_bn1.bn.bias',\t(256,),\t 'layer3.3.bn1.bias',\t(256,),\n     'block3.3.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.3.bn1.running_mean',\t(256,),\n     'block3.3.conv_bn1.bn.running_var',\t(256,),\t 'layer3.3.bn1.running_var',\t(256,),\n     'block3.3.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.3.conv2.weight',\t(256, 256, 3, 3),\n     'block3.3.conv_bn2.bn.weight',\t(256,),\t 'layer3.3.bn2.weight',\t(256,),\n     'block3.3.conv_bn2.bn.bias',\t(256,),\t 'layer3.3.bn2.bias',\t(256,),\n     'block3.3.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.3.bn2.running_mean',\t(256,),\n     'block3.3.conv_bn2.bn.running_var',\t(256,),\t 'layer3.3.bn2.running_var',\t(256,),\n     'block3.4.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.4.conv1.weight',\t(256, 256, 3, 3),\n     'block3.4.conv_bn1.bn.weight',\t(256,),\t 'layer3.4.bn1.weight',\t(256,),\n     'block3.4.conv_bn1.bn.bias',\t(256,),\t 'layer3.4.bn1.bias',\t(256,),\n     'block3.4.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.4.bn1.running_mean',\t(256,),\n     'block3.4.conv_bn1.bn.running_var',\t(256,),\t 'layer3.4.bn1.running_var',\t(256,),\n     'block3.4.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.4.conv2.weight',\t(256, 256, 3, 3),\n     'block3.4.conv_bn2.bn.weight',\t(256,),\t 'layer3.4.bn2.weight',\t(256,),\n     'block3.4.conv_bn2.bn.bias',\t(256,),\t 'layer3.4.bn2.bias',\t(256,),\n     'block3.4.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.4.bn2.running_mean',\t(256,),\n     'block3.4.conv_bn2.bn.running_var',\t(256,),\t 'layer3.4.bn2.running_var',\t(256,),\n     'block3.5.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.5.conv1.weight',\t(256, 256, 3, 3),\n     'block3.5.conv_bn1.bn.weight',\t(256,),\t 'layer3.5.bn1.weight',\t(256,),\n     'block3.5.conv_bn1.bn.bias',\t(256,),\t 'layer3.5.bn1.bias',\t(256,),\n     'block3.5.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.5.bn1.running_mean',\t(256,),\n     'block3.5.conv_bn1.bn.running_var',\t(256,),\t 'layer3.5.bn1.running_var',\t(256,),\n     'block3.5.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.5.conv2.weight',\t(256, 256, 3, 3),\n     'block3.5.conv_bn2.bn.weight',\t(256,),\t 'layer3.5.bn2.weight',\t(256,),\n     'block3.5.conv_bn2.bn.bias',\t(256,),\t 'layer3.5.bn2.bias',\t(256,),\n     'block3.5.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.5.bn2.running_mean',\t(256,),\n     'block3.5.conv_bn2.bn.running_var',\t(256,),\t 'layer3.5.bn2.running_var',\t(256,),\n     'block4.0.conv_bn1.conv.weight',\t(512, 256, 3, 3),\t 'layer4.0.conv1.weight',\t(512, 256, 3, 3),\n     'block4.0.conv_bn1.bn.weight',\t(512,),\t 'layer4.0.bn1.weight',\t(512,),\n     'block4.0.conv_bn1.bn.bias',\t(512,),\t 'layer4.0.bn1.bias',\t(512,),\n     'block4.0.conv_bn1.bn.running_mean',\t(512,),\t 'layer4.0.bn1.running_mean',\t(512,),\n     'block4.0.conv_bn1.bn.running_var',\t(512,),\t 'layer4.0.bn1.running_var',\t(512,),\n     'block4.0.conv_bn2.conv.weight',\t(512, 512, 3, 3),\t 'layer4.0.conv2.weight',\t(512, 512, 3, 3),\n     'block4.0.conv_bn2.bn.weight',\t(512,),\t 'layer4.0.bn2.weight',\t(512,),\n     'block4.0.conv_bn2.bn.bias',\t(512,),\t 'layer4.0.bn2.bias',\t(512,),\n     'block4.0.conv_bn2.bn.running_mean',\t(512,),\t 'layer4.0.bn2.running_mean',\t(512,),\n     'block4.0.conv_bn2.bn.running_var',\t(512,),\t 'layer4.0.bn2.running_var',\t(512,),\n     'block4.0.shortcut.conv.weight',\t(512, 256, 1, 1),\t 'layer4.0.downsample.0.weight',\t(512, 256, 1, 1),\n     'block4.0.shortcut.bn.weight',\t(512,),\t 'layer4.0.downsample.1.weight',\t(512,),\n     'block4.0.shortcut.bn.bias',\t(512,),\t 'layer4.0.downsample.1.bias',\t(512,),\n     'block4.0.shortcut.bn.running_mean',\t(512,),\t 'layer4.0.downsample.1.running_mean',\t(512,),\n     'block4.0.shortcut.bn.running_var',\t(512,),\t 'layer4.0.downsample.1.running_var',\t(512,),\n     'block4.1.conv_bn1.conv.weight',\t(512, 512, 3, 3),\t 'layer4.1.conv1.weight',\t(512, 512, 3, 3),\n     'block4.1.conv_bn1.bn.weight',\t(512,),\t 'layer4.1.bn1.weight',\t(512,),\n     'block4.1.conv_bn1.bn.bias',\t(512,),\t 'layer4.1.bn1.bias',\t(512,),\n     'block4.1.conv_bn1.bn.running_mean',\t(512,),\t 'layer4.1.bn1.running_mean',\t(512,),\n     'block4.1.conv_bn1.bn.running_var',\t(512,),\t 'layer4.1.bn1.running_var',\t(512,),\n     'block4.1.conv_bn2.conv.weight',\t(512, 512, 3, 3),\t 'layer4.1.conv2.weight',\t(512, 512, 3, 3),\n     'block4.1.conv_bn2.bn.weight',\t(512,),\t 'layer4.1.bn2.weight',\t(512,),\n     'block4.1.conv_bn2.bn.bias',\t(512,),\t 'layer4.1.bn2.bias',\t(512,),\n     'block4.1.conv_bn2.bn.running_mean',\t(512,),\t 'layer4.1.bn2.running_mean',\t(512,),\n     'block4.1.conv_bn2.bn.running_var',\t(512,),\t 'layer4.1.bn2.running_var',\t(512,),\n     'block4.2.conv_bn1.conv.weight',\t(512, 512, 3, 3),\t 'layer4.2.conv1.weight',\t(512, 512, 3, 3),\n     'block4.2.conv_bn1.bn.weight',\t(512,),\t 'layer4.2.bn1.weight',\t(512,),\n     'block4.2.conv_bn1.bn.bias',\t(512,),\t 'layer4.2.bn1.bias',\t(512,),\n     'block4.2.conv_bn1.bn.running_mean',\t(512,),\t 'layer4.2.bn1.running_mean',\t(512,),\n     'block4.2.conv_bn1.bn.running_var',\t(512,),\t 'layer4.2.bn1.running_var',\t(512,),\n     'block4.2.conv_bn2.conv.weight',\t(512, 512, 3, 3),\t 'layer4.2.conv2.weight',\t(512, 512, 3, 3),\n     'block4.2.conv_bn2.bn.weight',\t(512,),\t 'layer4.2.bn2.weight',\t(512,),\n     'block4.2.conv_bn2.bn.bias',\t(512,),\t 'layer4.2.bn2.bias',\t(512,),\n     'block4.2.conv_bn2.bn.running_mean',\t(512,),\t 'layer4.2.bn2.running_mean',\t(512,),\n     'block4.2.conv_bn2.bn.running_var',\t(512,),\t 'layer4.2.bn2.running_var',\t(512,),\n     'logit.weight',\t(1000, 512),\t 'fc.weight',\t(1000, 512),\n     'logit.bias',\t(1000,),\t 'fc.bias',\t(1000,),\n\n    ]\n\n    ###############################################################################\n    class ConvBn2d(nn.Module):\n\n        def __init__(self, in_channel, out_channel, kernel_size=3, padding=1, stride=1):\n            super(ConvBn2d, self).__init__()\n            self.conv = nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, padding=padding, stride=stride, bias=False)\n            self.bn   = nn.BatchNorm2d(out_channel, eps=1e-5)\n\n        def forward(self,x):\n            x = self.conv(x)\n            x = self.bn(x)\n            return x\n\n\n\n\n    #############  resnext50 pyramid feature net #######################################\n    # https://github.com/Hsuxu/ResNeXt/blob/master/models.py\n    # https://github.com/D-X-Y/ResNeXt-DenseNet/blob/master/models/resnext.py\n    # https://github.com/miraclewkf/ResNeXt-PyTorch/blob/master/resnext.py\n\n\n    # bottleneck type C\n    class BasicBlock(nn.Module):\n        def __init__(self, in_channel, channel, out_channel, stride=1, is_shortcut=False):\n            super(BasicBlock, self).__init__()\n            self.is_shortcut = is_shortcut\n\n            self.conv_bn1 = ConvBn2d(in_channel,    channel, kernel_size=3, padding=1, stride=stride)\n            self.conv_bn2 = ConvBn2d(   channel,out_channel, kernel_size=3, padding=1, stride=1)\n\n            if is_shortcut:\n                self.shortcut = ConvBn2d(in_channel, out_channel, kernel_size=1, padding=0, stride=stride)\n\n\n        def forward(self, x):\n            z = F.relu(self.conv_bn1(x),inplace=True)\n            z = self.conv_bn2(z)\n\n            if self.is_shortcut:\n                x = self.shortcut(x)\n\n            z += x\n            z = F.relu(z,inplace=True)\n            return z\n\n\n    #resnet18\n    class ResNet18(nn.Module):\n\n        def __init__(self, num_class=1000 ):\n            super(ResNet18, self).__init__()\n\n\n            self.block0  = nn.Sequential(\n                nn.Conv2d(3, 64, kernel_size=7, padding=3, stride=2, bias=False),\n                BatchNorm2d(64),\n                nn.ReLU(inplace=True),\n            )\n\n            self.block1  = nn.Sequential(\n                 nn.MaxPool2d(kernel_size=3, padding=1, stride=2),\n                 BasicBlock( 64, 64, 64, stride=1, is_shortcut=False,),\n              * [BasicBlock( 64, 64, 64, stride=1, is_shortcut=False,) for i in range(1,2)],\n            )\n            self.block2  = nn.Sequential(\n                 BasicBlock( 64,128,128, stride=2, is_shortcut=True, ),\n              * [BasicBlock(128,128,128, stride=1, is_shortcut=False,) for i in range(1,2)],\n            )\n            self.block3  = nn.Sequential(\n                 BasicBlock(128,256,256, stride=2, is_shortcut=True, ),\n              * [BasicBlock(256,256,256, stride=1, is_shortcut=False,) for i in range(1,2)],\n            )\n            self.block4 = nn.Sequential(\n                 BasicBlock(256,512,512, stride=2, is_shortcut=True, ),\n              * [BasicBlock(512,512,512, stride=1, is_shortcut=False,) for i in range(1,2)],\n            )\n            self.logit = nn.Linear(512,num_class)\n\n\n\n        def forward(self, x):\n            batch_size = len(x)\n\n            x = self.block0(x)\n            x = F.max_pool2d(x,kernel_size=3, padding=1, stride=2, ceil_mode=False)\n\n            x = self.block1(x)\n            x = self.block2(x)\n            x = self.block3(x)\n            x = self.block4(x)\n            x = F.adaptive_avg_pool2d(x,1).reshape(batch_size,-1)\n            logit = self.logit(x)\n            return logit\n\n\n    ####################################################################################################\n    def upsize(x,scale_factor=2):\n        #x = F.interpolate(x, size=e.shape[2:], mode='nearest')\n        x = F.interpolate(x, scale_factor=scale_factor, mode='nearest')\n        return x\n\n    class Swish(nn.Module):\n        def forward(self, x):\n            return x * torch.sigmoid(x)\n\n    class Decode(nn.Module):\n        def __init__(self, in_channel, out_channel):\n            super(Decode, self).__init__()\n\n            self.top = nn.Sequential(\n                nn.Conv2d(in_channel, out_channel//2, kernel_size=3, stride=1, padding=1, bias=False),\n                BatchNorm2d( out_channel//2),\n                nn.ReLU(inplace=True),\n                #nn.Dropout(0.1),\n\n                nn.Conv2d(out_channel//2, out_channel//2, kernel_size=3, stride=1, padding=1, bias=False),\n                BatchNorm2d(out_channel//2),\n                nn.ReLU(inplace=True),\n                #nn.Dropout(0.1),\n\n                nn.Conv2d(out_channel//2, out_channel, kernel_size=1, stride=1, padding=0, bias=False),\n                BatchNorm2d(out_channel),\n                nn.ReLU(inplace=True), #Swish(), #\n            )\n\n        def forward(self, x):\n            x = self.top(torch.cat(x, 1))\n            return x\n\n\n\n    class Net(nn.Module):\n\n        def load_pretrain(self, skip, is_print=True):\n            conversion=copy.copy(CONVERSION)\n            for i in range(0,len(conversion)-8,4):\n                conversion[i] = 'block.' + conversion[i][5:]\n            load_pretrain(self, skip, pretrain_file=PRETRAIN_FILE, conversion=conversion, is_print=is_print)\n\n        def __init__(self, num_class=5, drop_connect_rate=0.2):\n            super(Net, self).__init__()\n\n            e = ResNet18()\n            self.block = nn.ModuleList([\n               e.block0,\n               e.block1,\n               e.block2,\n               e.block3,\n               e.block4\n            ])\n            e = None  #dropped\n\n            self.decode1 =  Decode(512,     128)\n            self.decode2 =  Decode(128+256, 128)\n            self.decode3 =  Decode(128+128, 128)\n            self.decode4 =  Decode(128+ 64, 128)\n            self.decode5 =  Decode(128+ 64, 128)\n            self.logit = nn.Conv2d(128,num_class, kernel_size=1)\n\n        def forward(self, x):\n            batch_size,C,H,W = x.shape\n\n            #----------------------------------\n            backbone = []\n            for i in range( len(self.block)):\n                x = self.block[i](x)\n                #print(i, x.shape)\n\n                if i in [0,1,2,3,4]:\n                    backbone.append(x)\n\n            #----------------------------------\n            x = self.decode1([backbone[-1], ])                   #; print('d1',d1.size())\n            x = self.decode2([backbone[-2], upsize(x)])          #; print('d2',d2.size())\n            x = self.decode3([backbone[-3], upsize(x)])          #; print('d3',d3.size())\n            x = self.decode4([backbone[-4], upsize(x)])          #; print('d4',d4.size())\n            x = self.decode5([backbone[-5], upsize(x)])          #; print('d5',d5.size())\n\n            logit = self.logit(x)\n            logit = F.interpolate(logit, size=(H,W), mode='bilinear', align_corners=False)\n            return logit\n\n    class ResNet34(nn.Module):\n\n        def __init__(self, num_class=1000 ):\n            super(ResNet34, self).__init__()\n\n\n            self.block0  = nn.Sequential(\n                nn.Conv2d(3, 64, kernel_size=7, padding=3, stride=2, bias=False),\n                BatchNorm2d(64),\n                nn.ReLU(inplace=True),\n            )\n            self.block1  = nn.Sequential(\n                 nn.MaxPool2d(kernel_size=3, padding=1, stride=2),\n                 BasicBlock( 64, 64, 64, stride=1, is_shortcut=False,),\n              * [BasicBlock( 64, 64, 64, stride=1, is_shortcut=False,) for i in range(1,3)],\n            )\n            self.block2  = nn.Sequential(\n                 BasicBlock( 64,128,128, stride=2, is_shortcut=True, ),\n              * [BasicBlock(128,128,128, stride=1, is_shortcut=False,) for i in range(1,4)],\n            )\n            self.block3  = nn.Sequential(\n                 BasicBlock(128,256,256, stride=2, is_shortcut=True, ),\n              * [BasicBlock(256,256,256, stride=1, is_shortcut=False,) for i in range(1,6)],\n            )\n            self.block4 = nn.Sequential(\n                 BasicBlock(256,512,512, stride=2, is_shortcut=True, ),\n              * [BasicBlock(512,512,512, stride=1, is_shortcut=False,) for i in range(1,3)],\n            )\n            self.logit = nn.Linear(512,num_class)\n\n\n\n        def forward(self, x):\n            batch_size = len(x)\n\n            x = self.block0(x)\n            x = self.block1(x)\n            x = self.block2(x)\n            x = self.block3(x)\n            x = self.block4(x)\n            x = F.adaptive_avg_pool2d(x,1).reshape(batch_size,-1)\n            logit = self.logit(x)\n            return logit\n\n\n    class Resnet34_classification(nn.Module):\n        def __init__(self,num_class=4):\n            super(Resnet34_classification, self).__init__()\n            e = ResNet34()\n            self.block = nn.ModuleList([\n                e.block0,\n                e.block1,\n                e.block2,\n                e.block3,\n                e.block4,\n            ])\n            e = None  #dropped\n            self.feature = nn.Conv2d(512,32, kernel_size=1) #dummy conv for dim reduction\n            self.logit = nn.Conv2d(32,num_class, kernel_size=1)\n\n        def forward(self, x):\n            batch_size,C,H,W = x.shape\n\n            for i in range( len(self.block)):\n                x = self.block[i](x)\n                #print(i, x.shape)\n\n            x = F.dropout(x,0.5,training=self.training)\n            x = F.adaptive_avg_pool2d(x, 1)\n            x = self.feature(x)\n            logit = self.logit(x)\n            return logit\n\n    model_classification = Resnet34_classification()\n    model_classification.load_state_dict(torch.load(\"../input/hengs-models-20190910/00007500_model.pth\",\n                                                    map_location=lambda storage, loc: storage), strict=True)\n\n    return model_classification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_classification = load_hengs_clf_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_hengs_clf_model_preds(model_classification):\n    # Dataset setup\n    class TestDataset(Dataset):\n        '''Dataset for test prediction'''\n        def __init__(self, root, df, mean, std):\n            self.root = root\n            df['ImageId'] = df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\n            self.fnames = df['ImageId'].unique().tolist()\n            self.num_samples = len(self.fnames)\n            self.transform = Compose(\n                [\n                    Normalize(mean=mean, std=std, p=1),\n                    ToTensor(),\n                ]\n            )\n\n        def __getitem__(self, idx):\n            fname = self.fnames[idx]\n            path = os.path.join(self.root, fname)\n            image = cv2.imread(path)\n            images = self.transform(image=image)[\"image\"]\n            return fname, images\n\n        def __len__(self):\n            return self.num_samples\n\n    sample_submission_path = '../input/severstal-steel-defect-detection/sample_submission.csv'\n    test_data_folder = \"../input/severstal-steel-defect-detection/test_images\"\n\n    # hyperparameters\n    batch_size = 1\n\n    # mean and std\n    mean = (0.485, 0.456, 0.406)\n    std = (0.229, 0.224, 0.225)\n\n    df = pd.read_csv(sample_submission_path)\n\n    # dataloader\n    testset = DataLoader(\n        TestDataset(test_data_folder, df, mean, std),\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=0,\n        pin_memory=True\n    )\n\n\n    # useful functions for setting up inference\n\n    def sharpen(p,t=0.5):\n            if t!=0:\n                return p**t\n            else:\n                return p\n\n    def get_classification_preds(net,test_loader):\n        test_probability_label = []\n        test_id   = []\n\n        net = net.cuda()\n        for t, (fnames, images) in enumerate(tqdm(test_loader)):\n            batch_size,C,H,W = images.shape\n            images = images.cuda()\n\n            with torch.no_grad():\n                net.eval()\n\n                num_augment = 0\n                if 1: #  null\n                    logit =  net(images)\n                    probability = torch.sigmoid(logit)\n\n                    probability_label = sharpen(probability,0)\n                    num_augment+=1\n\n                if 'flip_lr' in augment:\n                    logit = net(torch.flip(images,dims=[3]))\n                    probability  = torch.sigmoid(logit)\n\n                    probability_label += sharpen(probability)\n                    num_augment+=1\n\n                if 'flip_ud' in augment:\n                    logit = net(torch.flip(images,dims=[2]))\n                    probability = torch.sigmoid(logit)\n\n                    probability_label += sharpen(probability)\n                    num_augment+=1\n\n                probability_label = probability_label/num_augment\n\n            probability_label = probability_label.data.cpu().numpy()\n\n            test_probability_label.append(probability_label)\n            test_id.extend([i for i in fnames])\n\n\n        test_probability_label = np.concatenate(test_probability_label)\n        return test_probability_label, test_id\n\n    # threshold for classification\n    threshold_label = [0.50,0.50,0.50,0.50,]\n\n    augment = ['null'] #['null', 'flip_lr','flip_ud','5crop'] # ['null', 'flip_lr','flip_ud'] # # #\n\n    # Get prediction for classification model\n\n    probability_label, image_id = get_classification_preds(model_classification, testset)\n    predict_label = probability_label>np.array(threshold_label).reshape(1,4,1,1)\n\n    image_id_class_id = []\n    encoded_pixel = []\n    for b in range(len(image_id)):\n        for c in range(4):\n            image_id_class_id.append(image_id[b]+'_%d'%(c+1))\n            if predict_label[b,c]==0:\n                rle=''\n            else:\n                rle ='1 1'\n            encoded_pixel.append(rle)\n\n    df_classification = pd.DataFrame(zip(image_id_class_id, encoded_pixel), columns=['ImageId_ClassId', 'EncodedPixels'])\n\n    return df_classification\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_classification = get_hengs_clf_model_preds(model_classification)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del model_classification\ntorch.cuda.empty_cache()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_classification.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def load_denis_gontcharov_segmentation_model():\n    # https://www.kaggle.com/gontcharovd/unet-pytorch-inference-kernel-extended-0-89648\n    #Model from https://www.kaggle.com/gontcharovd/unet-pytorch-inference-kernel-extended-0-89648/comments\n    # ckpt_path = \"../input/resnetmodels/resnet18_20_epochs.pth\"\n    # ckpt_path = \"../input/senetmodels/senet50_20_epochs.pth\"\n    ckpt_path = \"../input/senetmodels/senext50_30_epochs_high_threshold.pth\"\n    device = torch.device(\"cuda\")\n    # change the encoder name in the Unet() call.\n    model_segmentation = Unet('se_resnext50_32x4d', encoder_weights=None, classes=4, activation=None)\n    model_segmentation.to(device)\n    model_segmentation.eval()\n    state = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\n    model_segmentation.load_state_dict(state[\"state_dict\"])\n    model_segmentation = model_segmentation.cuda()\n    \n    return model_segmentation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"denis_gontcharov_model = load_denis_gontcharov_segmentation_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# def load_ilya_model():\n#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n\n#     models_folder = Path(\"/kaggle/input/segmentation-model\")\n# #     empty_model_folder = Path(\"/kaggle/input/empty-model\")\n\n#     class PretrainedModel(torch.nn.Module):\n#         def __init__(self, output_features, pretrained=True):\n#             super().__init__()\n#             model = torchvision.models.resnet34(pretrained=pretrained)\n#             num_ftrs = model.fc.in_features\n#             model.fc = torch.nn.Linear(num_ftrs, output_features)\n#             self.model = model\n\n#         def forward(self, x):\n#             return self.model(x)\n# #     empty_model = PretrainedModel(2, False)\n    \n# #     pretrained_model_name = \"best_model.pt\"\n# #     empty_model.load_state_dict(torch.load(\n# #         empty_model_folder / pretrained_model_name,\n# #         map_location=torch.device(\"cpu\")\n# #     ))\n\n# #     empty_model.to(device)\n# #     empty_model.eval();\n    \n#     segmentation_model = PretrainedUNet(\n#     in_channels=3,\n#     out_channels=4, \n#     batch_norm=True, \n#     upscale_mode=\"bilinear\",\n#     pretrained=False\n#     )\n    \n#     pretrained_model_name = \"severstal-unet-v51.pt\"\n\n#     if pretrained_model_name is not None:\n#         segmentation_model.load_state_dict(torch.load(\n#             models_folder / pretrained_model_name,\n#             map_location=torch.device(\"cpu\")\n#         ))\n\n#     segmentation_model.to(device)\n#     segmentation_model.eval();\n    \n#     return segmentation_model.cuda()\n\n# ilya_model = load_ilya_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nfolds = 1#4\nbs = 2\nn_cls = 4\nnoise_th = 2000 #predicted masks must be larger than noise_th\nTEST = '../input/severstal-steel-defect-detection/test_images/'\nBASE = '../input/severstal-fast-ai-256x256-crops/'\n\ntorch.backends.cudnn.benchmark = True\n\n\n# def get_fast_ai_learn():\n\nfrom fastai.vision.learner import create_head, cnn_config, num_features_model, create_head\nfrom fastai.callbacks.hooks import model_sizes, hook_outputs, dummy_eval, Hook, _hook_inner\nfrom fastai.vision.models.unet import _get_sfs_idxs, UnetBlock\n\nclass Hcolumns(nn.Module):\n    def __init__(self, hooks:Collection[Hook], nc:Collection[int]=None):\n        super(Hcolumns,self).__init__()\n        self.hooks = hooks\n        self.n = len(self.hooks)\n        self.factorization = None \n        if nc is not None:\n            self.factorization = nn.ModuleList()\n            for i in range(self.n):\n                self.factorization.append(nn.Sequential(\n                    conv2d(nc[i],nc[-1],3,padding=1,bias=True),\n                    conv2d(nc[-1],nc[-1],3,padding=1,bias=True)))\n                #self.factorization.append(conv2d(nc[i],nc[-1],3,padding=1,bias=True))\n        \n    def forward(self, x:Tensor):\n        n = len(self.hooks)\n        out = [F.interpolate(self.hooks[i].stored if self.factorization is None\n            else self.factorization[i](self.hooks[i].stored), scale_factor=2**(self.n-i),\n            mode='bilinear',align_corners=False) for i in range(self.n)] + [x]\n        return torch.cat(out, dim=1)\n\nclass DynamicUnet_Hcolumns(SequentialEx):\n    \"Create a U-Net from a given architecture.\"\n    def __init__(self, encoder:nn.Module, n_classes:int, blur:bool=False, blur_final=True, \n                 self_attention:bool=False,\n                 y_range:Optional[Tuple[float,float]]=None,\n                 last_cross:bool=True, bottle:bool=False, **kwargs):\n        imsize = (256,256)\n        sfs_szs = model_sizes(encoder, size=imsize)\n        sfs_idxs = list(reversed(_get_sfs_idxs(sfs_szs)))\n        self.sfs = hook_outputs([encoder[i] for i in sfs_idxs])\n        x = dummy_eval(encoder, imsize).detach()\n\n        ni = sfs_szs[-1][1]\n        middle_conv = nn.Sequential(conv_layer(ni, ni*2, **kwargs),\n                                    conv_layer(ni*2, ni, **kwargs)).eval()\n        x = middle_conv(x)\n        layers = [encoder, batchnorm_2d(ni), nn.ReLU(), middle_conv]\n\n        self.hc_hooks = [Hook(layers[-1], _hook_inner, detach=False)]\n        hc_c = [x.shape[1]]\n        \n        for i,idx in enumerate(sfs_idxs):\n            not_final = i!=len(sfs_idxs)-1\n            up_in_c, x_in_c = int(x.shape[1]), int(sfs_szs[idx][1])\n            do_blur = blur and (not_final or blur_final)\n            sa = self_attention and (i==len(sfs_idxs)-3)\n            unet_block = UnetBlock(up_in_c, x_in_c, self.sfs[i], final_div=not_final, \n                blur=blur, self_attention=sa, **kwargs).eval()\n            layers.append(unet_block)\n            x = unet_block(x)\n            self.hc_hooks.append(Hook(layers[-1], _hook_inner, detach=False))\n            hc_c.append(x.shape[1])\n\n        ni = x.shape[1]\n        if imsize != sfs_szs[0][-2:]: layers.append(PixelShuffle_ICNR(ni, **kwargs))\n        if last_cross:\n            layers.append(MergeLayer(dense=True))\n            ni += in_channels(encoder)\n            layers.append(res_block(ni, bottle=bottle, **kwargs))\n        hc_c.append(ni)\n        layers.append(Hcolumns(self.hc_hooks, hc_c))\n        layers += [conv_layer(ni*len(hc_c), n_classes, ks=1, use_activ=False, **kwargs)]\n        if y_range is not None: layers.append(SigmoidRange(*y_range))\n        super().__init__(*layers)\n\n    def __del__(self):\n        if hasattr(self, \"sfs\"): self.sfs.remove()\n            \ndef unet_learner(data:DataBunch, arch:Callable, pretrained:bool=True, blur_final:bool=True,\n        norm_type:Optional[NormType]=NormType, split_on:Optional[SplitFuncOrIdxList]=None, \n        blur:bool=False, self_attention:bool=False, y_range:Optional[Tuple[float,float]]=None, \n        last_cross:bool=True, bottle:bool=False, cut:Union[int,Callable]=None, \n        hypercolumns=True, **learn_kwargs:Any)->Learner:\n    \"Build Unet learner from `data` and `arch`.\"\n    meta = cnn_config(arch)\n    body = create_body(arch, pretrained, cut)\n    M = DynamicUnet_Hcolumns if hypercolumns else DynamicUnet\n    model = to_device(M(body, n_classes=data.c, blur=blur, blur_final=blur_final,\n        self_attention=self_attention, y_range=y_range, norm_type=norm_type, \n        last_cross=last_cross, bottle=bottle), data.device)\n    learn = Learner(data, model, **learn_kwargs)\n    learn.split(ifnone(split_on, meta['split']))\n    if pretrained: learn.freeze()\n    apply_init(model[2], nn.init.kaiming_normal_)\n    return learn\nclass SegmentationLabelList(SegmentationLabelList):\n    def open(self, fn): return open_mask(fn, div=True)\n    \nclass SegmentationItemList(SegmentationItemList):\n    _label_cls = SegmentationLabelList\n\n# Setting transformations on masks to False on test set\ndef transform(self, tfms:Optional[Tuple[TfmList,TfmList]]=(None,None), **kwargs):\n    if not tfms: tfms=(None,None)\n    assert is_listy(tfms) and len(tfms) == 2\n    self.train.transform(tfms[0], **kwargs)\n    self.valid.transform(tfms[1], **kwargs)\n    kwargs['tfm_y'] = False # Test data has no labels\n    if self.test: self.test.transform(tfms[1], **kwargs)\n    return self\nfastai.data_block.ItemLists.transform = transform\n\ndef open_mask(fn:PathOrStr, div:bool=True, convert_mode:str='L', cls:type=ImageSegment,\n        after_open:Callable=None)->ImageSegment:\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", UserWarning)\n        x = PIL.Image.open(fn).convert(convert_mode)\n    if after_open: x = after_open(x)\n    x = pil2tensor(x,np.float32)\n    return cls(x)\n\n\ndef get_fast_ai_learn():\n    stats = ([0.396,0.396,0.396], [0.179,0.179,0.179])\n    #check https://www.kaggle.com/iafoss/256x256-images-with-defects for stats\n\n    data = (SegmentationItemList.from_folder(TEST)\n            .split_by_idx([0])\n            .label_from_func(lambda x : str(x), classes=[0,1,2,3,4])\n            .add_test(Path(TEST).ls(), label=None)\n            .databunch(path=Path('.'), bs=bs)\n            .normalize(stats))\n\n\n    learn = unet_learner(data, models.resnet34, pretrained=False)\n    learn.model.load_state_dict(torch.load(Path(BASE)/f'models/fold0.pth')['model'])\n    learn.model.eval()\n    return learn\n\nlearn = get_fast_ai_learn()\n\nn_cls = 4\ntta = True\nnoise_th = 2000 #predicted masks must be larger than noise_th\n\ndef post_proc(yp):\n    yp = np.argmax(yp, axis=-1)\n    for i in range(n_cls):\n        idxs = yp == i+1\n        if idxs.sum() < noise_th: \n            yp[idxs] = 0\n    return yp\n\ndef argmax_mask_to_binary_masks(mask):\n    return [(mask == i).astype(np.int8)for i in range(1,n_cls+1)]\n\n# def pred_batch_fast_ai(x):\n#     x = x.cuda()\n#     py = torch.softmax(learn.model(x),dim=1).permute(0,2,3,1).detach()\n#     if tta:\n#         flips = [[-1],[-2],[-2,-1]]\n#         for f in flips:\n#             py += torch.softmax(torch.flip(learn.model(torch.flip(x,f)),f),dim=1).permute(0,2,3,1).detach()\n#         py /= len(flips) + 1\n#     py = py.cpu().numpy() \n#     argmax_masks = [post_proc(yp) for yp in py]\n#     binary_masks = np.array([argmax_mask_to_binary_masks(argmax_mask) for argmax_mask in argmax_masks])\n#     return binary_masks\n\ndef pred_batch_fast_ai(x):\n    x = x.cuda()\n    py = torch.softmax(learn.model(x),dim=1).detach()\n    if tta:\n        flips = [[-1],[-2],[-2,-1]]\n        for f in flips:\n            py += torch.softmax(torch.flip(learn.model(torch.flip(x,f)),f),dim=1).detach()\n        py /= len(flips) + 1\n        \n#     print(py[:,1:].cpu().numpy().shape)\n    return py[:,1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unet_se_resnext50_32x4d = \\\n    load('/kaggle/input/severstalmodels/unet_se_resnext50_32x4d.pth').cuda()\nunet_mobilenet2 = load('/kaggle/input/severstalmodels/unet_mobilenet2.pth').cuda()\nunet_resnet34 = load('/kaggle/input/severstalmodels/unet_resnet34.pth').cuda()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Models' mean aggregator"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"class Model:\n    def __init__(self, models, weights=None):\n        self.models = models\n        self.weights = weights or np.ones(len(models))\n    \n    def __call__(self, x):\n        res = []\n        x = x.cuda()\n        with torch.no_grad():\n            for model, weight in zip(self.models, self.weights):\n                res.append(torch.sigmoid(model(x))*weight)\n        res = torch.stack(res)\n        return torch.sum(res, dim=0) /sum(self.weights)\n\nmodel = Model([unet_se_resnext50_32x4d, unet_mobilenet2, unet_resnet34,\n               denis_gontcharov_model])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create TTA transforms, datasets, loaders"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def create_transforms(additional):\n    res = list(additional)\n    # add necessary transformations\n    res.extend([\n        ChannelTranspose()\n    ])\n    res = A.Compose(res)\n    return res\n\nimg_folder = '/kaggle/input/severstal-steel-defect-detection/test_images'\nbatch_size = 2\nnum_workers = 0\n# [0.388,0.390,0.394], [0.178,0.181,0.175]\n# Different transforms for TTA wrapper\ntransforms = [\n    [A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))],\n    [A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),A.HorizontalFlip(p=1)],\n    [A.Normalize(mean=(0.388,0.390,0.394), std=(0.178,0.181,0.175))],#dataset for fast ai\n]\n\ntransforms = [create_transforms(t) for t in transforms]\ndatasets = [TtaWrap(ImageDataset(img_folder=img_folder, transforms=t), tfms=t) for t in transforms]\nloaders = [DataLoader(d, num_workers=num_workers, batch_size=batch_size, shuffle=False) for d in datasets]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loaders' mean aggregator"},{"metadata":{"trusted":true},"cell_type":"code","source":"thresholds = [0.5, 0.5, 0.5, 0.5]\nmin_area = [600, 600, 1000, 2000]\n\nres = []\n# Iterate over all TTA loaders\ntotal = len(datasets[0])//batch_size\nfor loaders_batch in tqdm_notebook(zip(*loaders), total=total):\n\n    ############## Get preds with tta for fasi ai model ################# \n    fast_ai_batch = loaders_batch[-1]\n    fast_ai_features = fast_ai_batch['features'].cuda()\n    fast_ai_pred = pred_batch_fast_ai(fast_ai_features).cpu().numpy()\n    \n    ############## Get preds with tta for ensemble ################# \n    preds = []\n    \n    loaders_batch = loaders_batch[:-1]\n    for i, batch in enumerate(loaders_batch):\n        features = batch['features'].cuda()\n        p = model(features)\n        # inverse operations for TTA\n        p = datasets[i].inverse(p)\n        preds.append(p)\n    # TTA mean\n    preds = torch.stack(preds)\n    preds = torch.mean(preds, dim=0)\n    preds = preds.detach().cpu().numpy()\n    \n    ############## Combine preds with weights ################# \n    preds = np.average([preds, fast_ai_pred], weights=(4.0, 1.0), axis=0)\n\n    \n    # Batch post processing\n    for p, file in zip(preds, loaders_batch[0]['image_file']):\n        file = os.path.basename(file)\n        # Image postprocessing\n        for i in range(4):\n            p_channel = p[i]\n            imageid_classid = file+'_'+str(i+1)\n            p_channel = (p_channel>thresholds[i]).astype(np.uint8)\n            if p_channel.sum() < min_area[i]:\n                p_channel = np.zeros(p_channel.shape, dtype=p_channel.dtype)\n\n            res.append({\n                'ImageId_ClassId': imageid_classid,\n                'EncodedPixels': mask2rle(p_channel)\n            })        \t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(res)\ndf = df.fillna('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df['EncodedPixels'] == '').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df_classification['EncodedPixels'] == '').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"((df_classification['EncodedPixels'] == '')&(df['EncodedPixels'] == '')).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Delete false positives"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df_classification['EncodedPixels'] == '', 'EncodedPixels'] = ''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# a = pd.read_csv('submission.csv')\n# a[~a['EncodedPixels'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Histogram of predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"time_end = time.time()\nprint((time_end - time_start))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df['Image'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[0])\n# df['Class'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[1])\n# df['empty'] = df['EncodedPixels'].map(lambda x: not x)\n# df[df['empty'] == False]['Class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %matplotlib inline\n\n# df = pd.read_csv('submission.csv')[:40]\n# df['Image'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[0])\n# df['Class'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[1])\n\n# for row in df.itertuples():\n#     img_path = os.path.join(img_folder, row.Image)\n#     img = cv2.imread(img_path)\n#     mask = rle2mask(row.EncodedPixels, (1600, 256)) \\\n#         if isinstance(row.EncodedPixels, str) else np.zeros((256, 1600))\n#     if mask.sum() == 0:\n#         continue\n    \n#     fig, axes = plt.subplots(1, 2, figsize=(20, 60))\n#     axes[0].imshow(img/255)\n#     axes[1].imshow(mask*60)\n#     axes[0].set_title(row.Image)\n#     axes[1].set_title(row.Class)\n#     plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"2d558d43f12745d8a826eb84f7dcd0b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"60b23d613e584fa0abb75460b84b1d80":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e3c6cad600e04ebfb71a6c202a8eb513","IPY_MODEL_d6bd450456d24af1b8f635c10fee4445"],"layout":"IPY_MODEL_a7f23b4d7ebb4499b486f861d1c52b77"}},"6feb39965873459cb61d32fbbc2c2225":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7f23b4d7ebb4499b486f861d1c52b77":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c389718f59fc45eba86cebc46dc72f99":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6bd450456d24af1b8f635c10fee4445":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c389718f59fc45eba86cebc46dc72f99","placeholder":"","style":"IPY_MODEL_dcff928e32c743529e899d038dee4e66","value":" 901/? [05:36&lt;00:00,  2.67it/s]"}},"dcff928e32c743529e899d038dee4e66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3c6cad600e04ebfb71a6c202a8eb513":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6feb39965873459cb61d32fbbc2c2225","max":900,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2d558d43f12745d8a826eb84f7dcd0b2","value":900}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":1}