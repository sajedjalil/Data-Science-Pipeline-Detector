{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# library imports\nimport numpy as np \nimport pandas as pd\nimport random as rn\nimport cv2 as cv \nimport os\nimport sys\nfrom pathlib import Path\n\n# neural network wizardry\nimport tensorflow as tf\n\n# visuals\nfrom matplotlib import pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# for reproducibility\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntf.set_random_seed(RANDOM_SEED)\nrn.seed(RANDOM_SEED)\n\n# paths\nimg_train_folder = Path('/kaggle/input/train_images/')\nimg_test_folder = Path('/kaggle/input/test_images/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Foreword\nThe main goal of this notebook is to make the ideas presented easy to understand. I try my best to be as verbose as possible, but if you have remaining questions, I'll happily answer them in the comments.\n\nCarefully illustrating ideas takes a lot more time than just writing up code, so upvotes are much appreciated.\n\n# Kernel Structure\n* [Competition Information](#1)\n<br><span style=\"font-size:10px\">We discuss the format required from us by the competition, as well as the loss function of choice and the type of encoding used.</span>\n* [Exploratory Data Analysis & Baseline](#2)\n<br><span style=\"font-size:10px\">There is a class imbalance that will have to be dealt with. We also show accuracy is the wrong metric due to the majority class (no defects) being the majority class.</span>\n* [Input/Output Data Shapes](#3)\n<br><span style=\"font-size:10px\">Understanding the dimensions of the in- and output that the neural network will be expecting.</span>\n* [A First Look: Visualising the Masks](#4)\n<br><span style=\"font-size:10px\">We define some utility functions and show some instances of defects.</span>\n* [Formulating the Problem: Semantic Segmentation](#5)\n<br><span style=\"font-size:10px\">We define what task is expected of us.</span>\n* [Setting up our ML model: Mask R-CNN](#6)\n<br><span style=\"font-size:10px\">Cloning and configuring Matterport's implementation of Mask RCNN.</span>\n* [Training](#7)\n<br><span style=\"font-size:10px\">Currently configured to run a single epoch. This should allow you to get started!</span>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n# Competition Information\n\n## Prediction Output Format\nFrom the competition's [data](https://www.kaggle.com/c/severstal-steel-defect-detection/data) page:\n> Each image may have no defects, a defect of a single class, or defects of multiple classes. For each image you must segment defects of each class ```(ClassId = [1, 2, 3, 4])```.\n\nThe submission format requires us to make the classifications for each respective class on a separate row:\n![format](https://i.imgur.com/x3rWaJP.png)\n\n## Loss Function\n\n### Dice Coefficient\nFrom the [evaluation](https://www.kaggle.com/c/severstal-steel-defect-detection/overview/evaluation) page:\n\n> This competition is evaluated on the mean Dice coefficient. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:\n>\n>$$Dice(X,Y) = \\frac{2∗|X∩Y|}{|X|+|Y|}$$\n>\n>\n>where X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 1 when both X and Y are empty. The leaderboard score is the mean of the Dice coefficients for each ```<ImageId, ClassId>``` pair in the test set.\n\nOr if you prefer a visual illustration:\n![dice_viz](https://i.imgur.com/zl2W0xQ.png)\n\n\nTo get a better understanding, let's demonstrate with a quick toy example as we write the function:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# imagine a 3*3 image with a diagional line across\nX = np.eye(3,3, dtype=np.uint8)\nY = np.eye(3,3, dtype=np.uint8)\n\n# we change one pixel\nX[1,1] = 0","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(X)\nprint('')\nprint(Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dice_coefficient(X, y):\n    \n    # convert the pixel/mask matrix to a one-dimensional series\n    predicted = X.flatten()\n    truth = y.flatten()\n    \n    # our masks will consist of ones and zeros\n    # summing the result of their product gives us the cross section\n    overlap = np.sum(predicted * truth)\n    total_surface_area = np.sum(predicted + truth)\n    \n    # passing our calculated values to the formula\n    return 2 * overlap / total_surface_area","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(f'The dice coefficient for 1 wrongly labeled pixel in a 3*3 image is: {dice_coefficient(X, Y)}')\nprint('(2 * 2 overlapping \"1\" pixels / 5 total \"1\" surface area)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### *Mean* Dice Coefficient\nThe dataset's original format (one row per *imageId:classId* pair) points at the fact that we will have to run this dice coefficient function over every layer in our mask and take the average. If we train multiple images at a time, we will have to take the average across a batch. More about this in the **Data Shapes** chapter."},{"metadata":{},"cell_type":"markdown","source":"## Run-Length Encoding\n> In order to reduce the submission file size, our metric uses run-length encoding on the pixel values. Instead of submitting an exhaustive list of indices for your segmentation, you will submit **pairs of values** that contain a **start position and a run length**. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).\n>\n>The competition format requires a **space delimited list of pairs**. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are **sorted, positive, and the decoded pixel values are not duplicated**. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.\n\nSo, if we were to encode something like our example above, we would have to write it as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# a more elaborate version of kaggle.com/paulorzp/rle-functions-run-lenght-encode-decode\n# note that we will transpose the incoming array outside of the function, \n# as I find this a clearer illustration\n\ndef mask_to_rle(mask):\n    \"\"\"\n    params:  mask - numpy array\n    returns: run-length encoding string (pairs of start & length of encoding)\n    \"\"\"\n    \n    # turn a n-dimensional array into a 1-dimensional series of pixels\n    # for example:\n    #     [[1. 1. 0.]\n    #      [0. 0. 0.]   --> [1. 1. 0. 0. 0. 0. 1. 0. 0.]\n    #      [1. 0. 0.]]\n    flat = mask.flatten()\n    \n    # we find consecutive sequences by overlaying the mask\n    # on a version of itself that is displaced by 1 pixel\n    # for that, we add some padding before slicing\n    padded = np.concatenate([[0], flat, [0]])\n    \n    # this returns the indeces where the sliced arrays differ\n    runs = np.where(padded[1:] != padded[:-1])[0] \n    # indexes start at 0, pixel numbers start at 1\n    runs += 1\n\n    # every uneven element represents the start of a new sequence\n    # every even element is where the run comes to a stop\n    # subtract the former from the latter to get the length of the run\n    runs[1::2] -= runs[0::2]\n \n    # convert the array to a string\n    return ' '.join(str(x) for x in runs)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"rle_example = mask_to_rle(X)\nprint(f'The run-length encoding for our example would be: \"{rle_example}\"')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rle_to_mask(lre, shape=(1600,256)):\n    '''\n    params:  rle   - run-length encoding string (pairs of start & length of encoding)\n             shape - (width,height) of numpy array to return \n    \n    returns: numpy array with dimensions of shape parameter\n    '''    \n    # the incoming string is space-delimited\n    runs = np.asarray([int(run) for run in lre.split(' ')])\n    \n    # we do the same operation with the even and uneven elements, but this time with addition\n    runs[1::2] += runs[0::2]\n    # pixel numbers start at 1, indexes start at 0\n    runs -= 1\n    \n    # extract the starting and ending indeces at even and uneven intervals, respectively\n    run_starts, run_ends = runs[0::2], runs[1::2]\n    \n    # build the mask\n    h, w = shape\n    mask = np.zeros(h*w, dtype=np.uint8)\n    for start, end in zip(run_starts, run_ends):\n        mask[start:end] = 1\n    \n    # transform the numpy array from flat to the original image shape\n    return mask.reshape(shape)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(f'The mask reconstructed from the run-length encoding (\"{rle_example}\") \\\nfor our example would be:\\n{rle_to_mask(rle_example, shape=(3,3))}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n# Exploratory Data Analysis\n\n## Class Imbalances\nA huge imbalance quickly becomes apparent when looking at the training set description:\n\n![class_imbalance](https://i.imgur.com/B4Dsxur.png)\n\nOnly 7095 pictures will be of any use to us when training... (you wouldn't train a pedestrian detector on a dataset of empty streets, either).\n\n## Baseline\nIf we'd train on the entire dataset, the risk is substantial that our model will simply learn the **majority class** (no defect, ever).\nIndeed, if we simply upload the sample submission, we have a score of:\n\n![baseline_code](https://i.imgur.com/zAxYg4g.png)\n![baseline_score](https://i.imgur.com/AXaygTV.png)\n\nThis is the benchmark to beat. \n\n## Class Imbalances (continued)\nLet's see how often each class appears, as well as how the class distribution is inside images."},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading in the training set\ndata = pd.read_csv('/kaggle/input/train.csv')\ndata['ClassId'] = data['ClassId'].astype(np.uint8)\n\ndata.info()\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# keep only the images with labels\nsquashed = data.dropna(subset=['EncodedPixels'], axis='rows', inplace=True)\n\n# squash multiple rows per image into a list\nsquashed = (\n    data[['ImageId', 'EncodedPixels', 'ClassId']]\n        .groupby('ImageId', as_index=False) \n        .agg(list) \n)\n\n# count the amount of class labels per image\nsquashed['DistinctDefectTypes'] = squashed['ClassId'].apply(lambda x: len(x))\n\n# display first ten to show new structure\nsquashed.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(f\"\"\"The training set now consists of {len(squashed):,} distinct images,\nfor a total of {squashed[\"DistinctDefectTypes\"].sum():,} labeled mask instances.\"\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\"\"\" use a consistent color palette per label throughout the notebook \"\"\"\nimport colorlover as cl\n\n# see: https://plot.ly/ipython-notebooks/color-scales/\ncolors = cl.scales['4']['qual']['Set3']\nlabels = np.array(range(1,5))\n\n# combining into a dictionary\npalette = dict(zip(labels, np.array(cl.to_numeric(colors))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# we want counts & frequency of the labels\nclasses = (\n    data.groupby(by='ClassId', as_index=False)\n        .agg({'ImageId':'count'})\n        .rename(columns={'ImageId':'Count'})\n)\n\nclasses['Frequency'] = round(classes['Count'] / classes['Count'].sum() * 100, 2) \nclasses['Frequency'] = classes['Frequency'].astype(str) + '%'\n\n# plotly for interactive graphs\nfig = go.Figure(\n    \n    data=go.Bar(\n        orientation='h',\n        x=classes.Count,\n        y=classes.ClassId,\n        hovertext=classes.Frequency,\n        text=classes.Count,\n        textposition='auto',\n        marker_color=colors),\n    \n    layout=go.Layout(\n        title='Defect Type: Count & Frequency',\n        showlegend=False,\n        xaxis=go.layout.XAxis(showticklabels=False),\n        yaxis=go.layout.YAxis(autorange='reversed'),\n        width=750, height=400\n    )\n)\n\n# display\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"An overwhelming amount of the observations is for class 3. Hopefully we can balance this out at least a little with some data augmentation later.\n\nLet's see what the distributions are if we consider all possible combinations, inccluding multi-class instances."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# we want counts of the possible combinations of labels\npermutations = pd.DataFrame(data=squashed.ClassId.astype(str).value_counts())\n\n# and their frequency\npermutations['Frequency'] = round(permutations['ClassId'] / permutations['ClassId'].sum() * 100, 2)\npermutations['Frequency'] = permutations['Frequency'].astype(str) + '%'\n\n# plotly for interactive graphs\nfig = go.Figure(\n    \n    data=go.Bar(\n        orientation='h',\n        x=permutations.ClassId,\n        y=permutations.index,\n        hovertext=permutations.Frequency,\n        text=permutations.ClassId,\n        textposition='auto'),\n    \n    layout=go.Layout(\n        title='Count of Distinct Defect Combinations in Images',\n        showlegend=False,\n        xaxis=go.layout.XAxis(showticklabels=False),\n        yaxis=go.layout.YAxis(autorange='reversed'),\n        width=750, height=500\n    )\n)\n\n# display\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like combinations of two labels in a single image are reasonably frequent, too. In fact, 3 & 4 appear together more often than 2 does on its own!\n\n<a id=\"3\"></a> <br>\n# Data Shapes\n\n## Images\nThe input shape will be an image we convert to a three-dimensional array with shape ```(256, 1600, 3)```, for height, width, and the three colour channels (RGB), respectively.\n\n## Labels\nNaturally, the masks will share the same width and height, but the third dimension will be as large as there are labels ```(256, 1600, 4)```, with each class occupying a different layer. Somewhat like this:\n![label shape](https://i.imgur.com/PePSemo.png)\n\n## Batch Size\nTo leverage the parellel computation a GPU offers, we will feed the images and their labels to the algorithm in batches. Consequently, our array dimensions will be expanded to ```(batch size, 256, 1600, 3)``` and ```(batch size, 256, 1600, 4)```, respectively.\n\nIf you remember our dice coefficient, we will have to calculate it for evey layer in every mask, and take the average over the entire batch."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n# Visualising the Masks\nLet's take a look at some examples of each class, and of some of the images containing multiple classes."},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_mask(encodings, labels):\n    \"\"\" takes a pair of lists of encodings and labels, \n        and turns them into a 3d numpy array of shape (256, 1600, 4) \n    \"\"\"\n    \n    # initialise an empty numpy array \n    mask = np.zeros((256,1600,4), dtype=np.uint8)\n   \n    # building the masks\n    for rle, label in zip(encodings, labels):\n        \n        # classes are [1, 2, 3, 4], corresponding indeces are [0, 1, 2, 3]\n        index = label - 1\n        \n        # fit the mask into the correct layer\n        # note we need to transpose the matrix to account for \n        # numpy and openCV handling width and height in reverse order \n        mask[:,:,index] = rle_to_mask(rle).T\n    \n    return mask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mask_to_contours(image, mask_layer, color):\n    \"\"\" converts a mask to contours using OpenCV and draws it on the image\n    \"\"\"\n\n    # https://docs.opencv.org/4.1.0/d4/d73/tutorial_py_contours_begin.html\n    contours, hierarchy = cv.findContours(mask_layer, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n    image = cv.drawContours(image, contours, -1, color, 2)\n        \n    return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualise_mask(file_name, mask):\n    \"\"\" open an image and draws clear masks, so we don't lose sight of the \n        interesting features hiding underneath \n    \"\"\"\n    \n    # reading in the image\n    image = cv.imread(f'{img_train_folder}/{file_name}')\n\n    # going through the 4 layers in the last dimension \n    # of our mask with shape (256, 1600, 4)\n    for index in range(mask.shape[-1]):\n        \n        # indeces are [0, 1, 2, 3], corresponding classes are [1, 2, 3, 4]\n        label = index + 1\n        \n        # add the contours, layer per layer \n        image = mask_to_contours(image, mask[:,:,index], color=palette[label])   \n        \n    return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# the images we want to see\nconditions = [\n    squashed['ClassId'].astype(str)=='[1]',\n    squashed['ClassId'].astype(str)=='[2]',\n    squashed['ClassId'].astype(str)=='[3]',\n    squashed['ClassId'].astype(str)=='[4]',\n    squashed['DistinctDefectTypes']==2,\n    squashed['DistinctDefectTypes']==3\n]\n\n# max 2 due to limited population of [squashed['Distinct Defect Types']==3]\n# remove that condition if you wish to increase the sample size, \n# or add replace=True to the .sample() method\nsample_size = 2\n\n# looping over the different combinations of labels \nfor condition in conditions:\n    \n    # isolate from dataset and draw a sample\n    sample = squashed[condition].sample(sample_size) \n    \n    # make a subplot+\n    fig, axes = plt.subplots(sample_size, 1, figsize=(16, sample_size*3))\n    fig.tight_layout()\n    \n    # looping over sample\n    for i, (index, row) in enumerate(sample.iterrows()):\n        \n        # current ax\n        ax = axes[i,]\n        \n        # build the mask\n        mask = build_mask(encodings=row.EncodedPixels, labels=row.ClassId)\n\n        # fetch the image and draw the contours\n        image = visualise_mask(file_name=row.ImageId, mask=mask)\n        \n        # display\n        ax.set_title(f'{row.ImageId}: {row.ClassId}')\n        ax.axis('off')\n        ax.imshow(image);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n# Semantic Segmentation\nBelow are some of the common tasks in the field of Machine Vision:\n![semantic_segmentation](https://miro.medium.com/max/1838/1*Tb3CvTONAA4IVL-HciJscw.jpeg)\n\nWe are dealing with the problem of semantic segmentation: predicting a pixel-by-pixel mask of distinct classes.\nCheck out Priya Dwivedi's [excellent blogpost](https://towardsdatascience.com/semantic-segmentation-popular-architectures-dff0a75f39d0) on the topic if you want to read more.\n\n<a id=\"6\"></a> <br>\n# Mask R-CNN\nMask R-CNN falls under the category of meta-algorithms, rather than purely a neural network architecture. In fact, it builds on the faster R-CNN architecture, so you even have a choice of what neural net 'backbone' you want it to use.\n\n![maskrcnn-framework](https://miro.medium.com/max/1285/1*IWWOPIYLqqF9i_gXPmBk3g.png)\n\n\nThe most important aspects of this algorithm are:\n* **FPN (feature pyramid network)** - A fully convolutional neural architecture designed to extract features.\n* **RPN (region proposal network)** - A lightweight neural network that scans over the FPN features to suggest ROI (regions of interest)\n* **ROIAlign** - a novel way to pass the object to the classifier an mask generator. Contrary to the ROIpool mechanism that was the standard, this one uses [bilinear interpolation](https://www.quora.com/How-does-ROTAlign-work-in-Mask-RCNN) to improve performance significantly.\n* **Classifier & Bounding Box Regressor**. \n* **Mask Generator** - A convolutional network that takes the regions selected by the ROI classifier and generates soft masks for them. \n\nRead more on [Matterport's official blog](https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46) <br>\nYou can also check out the original paper's authors presenting the Mask R-CNN [on YouTube](https://youtu.be/g7z4mkfRjI4)\n\n## Importing\nFor instructions on how to import models into a Kaggle kernel, check out the following Medium article: [Setting Up Mask-RCNN on Kaggle](https://medium.com/@umdfirecoml/setting-up-mask-rcnn-on-kaggle-34b656140b5e)\n\n2019/10/10: <br> \nThis method currently produces an error message when committing. <br> \n```Output path '/Mask_RCNN/.git/logs/refs/remotes/origin/HEAD' contains too many nested subdirectories (max 6)```. <br> \nI resort to Simon Walker's method to get around this.\n\n2019/10/17: <br>\nA Keras update means the model now produces an error [documented in issue #1754](https://github.com/matterport/Mask_RCNN/issues/1754). <br>\nI therefore cloned the repo and applied a small fix. <br>\n```changing: self.keras_model.metrics_tensors.append(loss)\nto: self.keras_model.add_metric(loss, name)````\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"WORKING_DIR = '/kaggle/working'\nLOGS_DIR = os.path.join(WORKING_DIR, \"logs\")\nMASK_RCNN_DIR = os.path.join(WORKING_DIR, 'Mask_RCNN-master')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !git clone https://www.github.com/matterport/Mask_RCNN.git \n# results in Commit Error (too many nested subdirectories)\n\n\"\"\" Credit to Simon Walker, whose method helped me to \n    circumvent the commit error. Check out his kernel at \n    https://www.kaggle.com/srwalker101/mask-rcnn-model\n\"\"\"\n!pip install git+https://github.com/rteuwens/Mask_RCNN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mrcnn.utils import Dataset\nfrom mrcnn.config import Config\nfrom mrcnn.model import MaskRCNN","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Configuring"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SeverstalConfig(Config):\n\n    # Give the configuration a recognizable name\n    NAME = \"severstal\"\n\n    # We use a GPU with 12GB memory, which can fit two images.\n    # Adjust down if you use a smaller GPU.\n    IMAGES_PER_GPU = 2\n\n    # Number of classes (including background)\n    NUM_CLASSES = 1 + 4  # background + steel defects\n\n    # Number of training steps per epoch\n    STEPS_PER_EPOCH = 100\n\n    # Skip detections with < 90% confidence\n    DETECTION_MIN_CONFIDENCE = 0.9\n    \n    # Discard inferior model weights\n    SAVE_BEST_ONLY = True\n    \n# instantiating \nseverstal_config = SeverstalConfig()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# super class can be found here:\n# https://github.com/matterport/Mask_RCNN/blob/v2.1/utils.py\n\nclass SeverstalDataset(Dataset):\n    \n    def __init__(self, dataframe):\n        \n        # https://rhettinger.wordpress.com/2011/05/26/super-considered-super/\n        super().__init__(self)\n        \n        # needs to be in the format of our squashed df, \n        # i.e. image id and list of rle plus their respective label on a single row\n        self.dataframe = dataframe\n        \n    def load_dataset(self, subset='train'):\n        \"\"\" takes:\n                - pandas df containing \n                    1) file names of our images \n                       (which we will append to the directory to find our images)\n                    2) a list of rle for each image \n                       (which will be fed to our build_mask() \n                       function we also used in the eda section)         \n            does:\n                adds images to the dataset with the utils.Dataset's add_image() metho\n        \"\"\"\n        \n        # input hygiene\n        assert subset in ['train', 'test'], f'\"{subset}\" is not a valid value.'\n        img_folder = img_train_folder if subset=='train' else img_test_folder\n        \n        # add our four classes\n        for i in range(1,5):\n            self.add_class(source='', class_id=i, class_name=f'defect_{i}')\n        \n        # add the image to our utils.Dataset class\n        for index, row in self.dataframe.iterrows():\n            file_name = row.ImageId\n            file_path = f'{img_folder}/{file_name}'\n            \n            assert os.path.isfile(file_path), 'File doesn\\'t exist.'\n            self.add_image(source='', \n                           image_id=file_name, \n                           path=file_path)\n    \n    def load_mask(self, image_id):\n        \"\"\"As found in: \n            https://github.com/matterport/Mask_RCNN/blob/master/samples/coco/coco.py\n        \n        Load instance masks for the given image\n        \n        This function converts the different mask format to one format\n        in the form of a bitmap [height, width, instances]\n        \n        Returns:\n            - masks    : A bool array of shape [height, width, instance count] with\n                         one mask per instance\n            - class_ids: a 1D array of class IDs of the instance masks\n        \"\"\"\n        \n        # find the image in the dataframe\n        row = self.dataframe.iloc[image_id]\n        \n        # extract function arguments\n        rle = row['EncodedPixels']\n        labels = row['ClassId']\n        \n        # create our numpy array mask\n        mask = build_mask(encodings=rle, labels=labels)\n        \n        # we're actually doing semantic segmentation, so our second return value is a bit awkward\n        # we have one layer per class, rather than per instance... so it will always just be \n        # 1, 2, 3, 4. See the section on Data Shapes for the Labels.\n        return mask.astype(np.bool), np.array([1, 2, 3, 4], dtype=np.int32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train/Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# stratified split to maintain the same class balance in both sets\ntrain, validate = train_test_split(squashed, test_size=0.2, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have a validation set that has the same class distribution as the training set."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(train['ClassId'].astype(str).value_counts(normalize=True))\nprint('')\nprint(validate['ClassId'].astype(str).value_counts(normalize=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# instantiating training set\ndataset_train = SeverstalDataset(dataframe=train)\ndataset_train.load_dataset()\ndataset_train.prepare()\n\n# instantiating validation set\ndataset_validate = SeverstalDataset(dataframe=validate)\ndataset_validate.load_dataset()\ndataset_validate.prepare()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pre-Trained Weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"!curl -LO https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Instantiating the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# configuration\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\n\n# session stuff\nsession = tf.Session(config=config)\nsession.run(tf.global_variables_initializer())\nsession.run(tf.local_variables_initializer())\n\n# initialiazing model\nmodel = MaskRCNN(mode='training', config=severstal_config, model_dir='modeldir')\n\n# we will retrain starting with the coco weights\nmodel.load_weights('mask_rcnn_coco.h5', \n                   by_name=True, \n                   exclude=['mrcnn_bbox_fc',\n                            'mrcnn_class_logits', \n                            'mrcnn_mask',\n                            'mrcnn_bbox'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n\n# ignore UserWarnongs\nimport warnings\nwarnings.filterwarnings('ignore', category=UserWarning)\n\n# training at last\nmodel.train(dataset_train,\n            dataset_validate,\n            epochs=1,\n            layers='heads',\n            learning_rate=severstal_config.LEARNING_RATE)\n\nhistory = model.keras_model.history.history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# To-Do\n* Split kernels into training & inference\n* \"Squashed\" DataFrame for inference on the competition set\n* Improving the model\n\nHave fun!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}