{"cells":[{"metadata":{},"cell_type":"markdown","source":"# This is basic U-Net submission notebook\n\nThis is based on https://www.kaggle.com/go1dfish/u-net-baseline-by-pytorch-in-fgvc6-resize\n\nThis competition doesnot allow internet access in submission. I have trained a U-net model and using its output to create a submission file.\n\n1. How to train U-net model? https://www.kaggle.com/nikhilikhar/u-net-baseline-by-pytorch-steel\n1. How to create necessary labels for training? https://www.kaggle.com/nikhilikhar/steel-create-labels\n1. How to convert RLE to Mask image to RLE required for training? https://www.kaggle.com/nikhilikhar/rle-to-mask-to-rle\n\n## Log\n* LB: 0.82135\n * https://www.kaggle.com/nikhilikhar/u-net-steel-submission?scriptVersionId=19147163 \n* LB: 0.85674\n * https://www.kaggle.com/nikhilikhar/pytorch-u-net-steel-1-submission?scriptVersionId=19286194\n*"},{"metadata":{},"cell_type":"markdown","source":"# Previous Model training\nModel training is in this kernel as rule specifies kernel timeout of 1 hr. But, it allows pretrainied model.\n"},{"metadata":{},"cell_type":"markdown","source":"# Import modules"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook as tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torchvision.transforms as transforms\nfrom torchvision import models\nimport torch.nn.functional as F\nfrom torch.autograd import Function, Variable\nfrom pathlib import Path\nfrom itertools import groupby\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nstart = time.time()\n\ninput_dir = \"../input/severstal-steel-defect-detection/\"\ntrain_img_dir = \"../input/severstal-steel-defect-detection/train_images/\"\ntest_img_dir = \"../input/severstal-steel-defect-detection/test_images/\"\n\ncategory_num = 4 + 1\n\nratio = 1\nepoch_num = 1\nbatch_size = 2\ndevice = \"cuda:0\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(input_dir + \"train.csv\")\ntrain_df[['ImageId', 'ClassId']] = train_df['ImageId_ClassId'].str.split('_', expand=True)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"# Define utils\nFor simplicity, It focus only category"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_mask_img(segment_df):\n    seg_width = 1600\n    seg_height = 256\n    seg_img = np.full(seg_width*seg_height, category_num-1, dtype=np.int32)\n    for encoded_pixels, class_id in zip(segment_df[\"EncodedPixels\"].values, segment_df[\"ClassId\"].values):\n        if pd.isna(encoded_pixels): continue\n        pixel_list = list(map(int, encoded_pixels.split(\" \")))\n        for i in range(0, len(pixel_list), 2):\n            start_index = pixel_list[i] -1 \n            index_len = pixel_list[i+1] \n            seg_img[start_index:start_index+index_len] = int(class_id) \n    seg_img = seg_img.reshape((seg_height, seg_width), order='F')\n   \n    return seg_img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_generator(df, batch_size):\n    img_ind_num = df.groupby(\"ImageId\")[\"ClassId\"].count()\n    index = df.index.values[0]\n    trn_images = []\n    seg_images = []\n    for i, (img_name, ind_num) in enumerate(img_ind_num.items()):\n        img = cv2.imread(train_img_dir + img_name)\n        segment_df = (df.loc[index:index+ind_num-1, :]).reset_index(drop=True)\n        index += ind_num\n        if segment_df[\"ImageId\"].nunique() != 1:\n            raise Exception(\"Index Range Error\")\n        seg_img = make_mask_img(segment_df)\n        \n        # HWC -> CHW\n        img = img.transpose((2, 0, 1))\n        #seg_img = seg_img.transpose((2, 0, 1))\n        \n        trn_images.append(img)\n        seg_images.append(seg_img)\n        if((i+1) % batch_size == 0):\n            yield np.array(trn_images, dtype=np.float32) / 255, np.array(seg_images, dtype=np.int32)\n            trn_images = []\n            seg_images = []\n    if(len(trn_images) != 0):\n        yield np.array(trn_images, dtype=np.float32) / 255, np.array(seg_images, dtype=np.int32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_generator(img_names):\n    for img_name in img_names:\n        img = cv2.imread(test_img_dir + img_name)\n        # HWC -> CHW\n        img = img.transpose((2, 0, 1))\n        yield img_name, np.asarray([img], dtype=np.float32) / 255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode(input_string):\n    return [(len(list(g)), k) for k,g in groupby(input_string)]\n\ndef run_length(label_vec):\n    encode_list = encode(label_vec)\n    index = 1\n    class_dict = {}\n    for i in encode_list:\n        if i[1] != category_num-1:\n            if i[1] not in class_dict.keys():\n                class_dict[i[1]] = []\n            class_dict[i[1]] = class_dict[i[1]] + [index, i[0]]\n        index += i[0]\n    return class_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Network"},{"metadata":{},"cell_type":"markdown","source":"# Unet"},{"metadata":{"trusted":true},"cell_type":"code","source":"class double_conv(nn.Module):\n    '''(conv => BN => ReLU) * 2'''\n    def __init__(self, in_ch, out_ch):\n        super(double_conv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass inconv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(inconv, self).__init__()\n        self.conv = double_conv(in_ch, out_ch)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass down(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(down, self).__init__()\n        self.mpconv = nn.Sequential(\n            nn.MaxPool2d(2),\n            double_conv(in_ch, out_ch)\n        )\n\n    def forward(self, x):\n        x = self.mpconv(x)\n        return x\n\n\nclass up(nn.Module):\n    def __init__(self, in_ch, out_ch, bilinear=True):\n        super(up, self).__init__()\n\n        #  would be a nice idea if the upsampling could be learned too,\n        #  but my machine do not have enough memory to handle all those weights\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        else:\n            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n\n        self.conv = double_conv(in_ch, out_ch)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        diffX = x1.size()[2] - x2.size()[2]\n        diffY = x1.size()[3] - x2.size()[3]\n        x2 = F.pad(x2, (diffX // 2, int(diffX / 2),\n                        diffY // 2, int(diffY / 2)))\n        x = torch.cat([x2, x1], dim=1)\n        x = self.conv(x)\n        return x\n\n\nclass outconv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(outconv, self).__init__()\n        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n    \nclass UNet(nn.Module):\n    def __init__(self, n_channels, n_classes):\n        super(UNet, self).__init__()\n        self.inc = inconv(n_channels, 64)\n        self.down1 = down(64, 128)\n        self.down2 = down(128, 256)\n        self.down3 = down(256, 512)\n        self.down4 = down(512, 512)\n        self.up1 = up(1024, 256)\n        self.up2 = up(512, 128)\n        self.up3 = up(256, 64)\n        self.up4 = up(128, 64)\n        self.outc = outconv(64, n_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        x = self.outc(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ResNet + Unet"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://github.com/usuyama/pytorch-unet\n\ndef convrelu(in_channels, out_channels, kernel, padding):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n        nn.ReLU(inplace=True),\n    )\n\n\nclass ResNetUNet(nn.Module):\n    def __init__(self, n_class):\n        super().__init__()\n\n        self.base_model = models.resnet18(pretrained=True)\n        self.base_layers = list(self.base_model.children())\n\n        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n        self.layer2_1x1 = convrelu(128, 128, 1, 0)\n        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n        self.layer3_1x1 = convrelu(256, 256, 1, 0)\n        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n\n        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n\n        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n\n        self.conv_last = nn.Conv2d(64, n_class, 1)\n\n    def forward(self, input):\n        x_original = self.conv_original_size0(input)\n        x_original = self.conv_original_size1(x_original)\n\n        layer0 = self.layer0(input)\n        layer1 = self.layer1(layer0)\n        layer2 = self.layer2(layer1)\n        layer3 = self.layer3(layer2)\n        layer4 = self.layer4(layer3)\n\n        layer4 = self.layer4_1x1(layer4)\n        x = self.upsample(layer4)\n        layer3 = self.layer3_1x1(layer3)\n        x = torch.cat([x, layer3], dim=1)\n        x = self.conv_up3(x)\n\n        x = self.upsample(x)\n        layer2 = self.layer2_1x1(layer2)\n        x = torch.cat([x, layer2], dim=1)\n        x = self.conv_up2(x)\n\n        x = self.upsample(x)\n        layer1 = self.layer1_1x1(layer1)\n        x = torch.cat([x, layer1], dim=1)\n        x = self.conv_up1(x)\n\n        x = self.upsample(x)\n        layer0 = self.layer0_1x1(layer0)\n        x = torch.cat([x, layer0], dim=1)\n        x = self.conv_up0(x)\n\n        x = self.upsample(x)\n        x = torch.cat([x, x_original], dim=1)\n        x = self.conv_original_size2(x)\n\n        out = self.conv_last(x)\n\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir -p /tmp/.cache/torch/checkpoints/\n!cp ../input/resnet18/resnet18.pth /tmp/.cache/torch/checkpoints/resnet18-5c106cde.pth","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# net = UNet(n_channels=3, n_classes=category_num).to(device)\nnet = ResNetUNet(n_class=category_num).to(device)\n\noptimizer = optim.SGD(\n    net.parameters(),\n    lr=0.1,\n    momentum=0.9,\n    weight_decay=0.0005\n)\n\ncriterion = nn.CrossEntropyLoss()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ignore output `IncompatibleKeys(missing_keys=[], unexpected_keys=[])`"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncheckpoint = torch.load(Path('../input/u-net-baseline-by-pytorch-steel/model-exported'))\nnet.load_state_dict(checkpoint)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total length of train df {}\".format(len(train_df)))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# val_sta = 40000\n# val_end = 50000\n# train_loss = []\n# valid_loss = []\n# for epoch in range(epoch_num):\n#     epoch_trn_loss = 0\n#     train_len = 0\n#     net.train()\n#     for iteration, (X_trn, Y_trn) in enumerate(tqdm(train_generator(train_df.iloc[:val_sta, :], batch_size))):\n#         X = torch.tensor(X_trn, dtype=torch.float32).to(device)\n#         Y = torch.tensor(Y_trn, dtype=torch.long).to(device)\n#         train_len += len(X)\n        \n#         #Y_flat = Y.view(-1)\n#         mask_pred = net(X)\n#         #mask_prob = torch.softmax(mask_pred, dim=1)\n#         #mask_prob_flat = mask_prob.view(-1)\n#         loss = criterion(mask_pred, Y)\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n#         epoch_trn_loss += loss.item()\n        \n#         if iteration % 100 == 0:\n#             print(\"train loss in {:0>2}epoch  /{:>5}iter:    {:<10.8}\".format(epoch+1, iteration, epoch_trn_loss/(iteration+1)))\n        \n#     train_loss.append(epoch_trn_loss/(iteration+1))\n#     print(\"train {}epoch loss({}iteration):    {:10.8}\".format(epoch+1, iteration, train_loss[-1]))\n    \n#     epoch_val_loss = 0\n#     val_len = 0\n#     net.eval()\n#     for iteration, (X_val, Y_val) in enumerate(tqdm(train_generator(train_df.iloc[val_sta:val_end, :], batch_size))):\n#         X = torch.tensor(X_val, dtype=torch.float32).to(device)\n#         Y = torch.tensor(Y_val, dtype=torch.long).to(device)\n#         val_len += len(X)\n        \n#         #Y_flat = Y.view(-1)\n        \n#         mask_pred = net(X)\n#         #mask_prob = torch.softmax(mask_pred, dim=1)\n#         #mask_prob_flat = mask_prob.view(-1)\n#         loss = criterion(mask_pred, Y)\n#         epoch_val_loss += loss.item()\n        \n#         if iteration % 100 == 0:\n#             print(\"valid loss in {:0>2}epoch  /{:>5}iter:    {:<10.8}\".format(epoch+1, iteration, epoch_val_loss/(iteration+1)))\n        \n#     valid_loss.append(epoch_val_loss/(iteration+1))\n#     print(\"valid {}epoch loss({}iteration):    {:10.8}\".format(epoch+1, iteration, valid_loss[-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.plot(list(range(epoch_num)), train_loss, color='green')\n# plt.plot(list(range(epoch_num)), valid_loss, color='blue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Export File"},{"metadata":{"trusted":true},"cell_type":"code","source":"# torch.save(net.state_dict(), './model-exported')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df = pd.read_csv(input_dir + \"sample_submission.csv\")\nsample_df[['ImageId', 'ClassId']] = sample_df['ImageId_ClassId'].str.split('_', expand=True)\nsample_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# import torch\n# import gc\n# for obj in gc.get_objects():\n#     try:\n#         if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n#             print(type(obj), obj.size())\n#     except:\n#         pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(sample_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_list = []\nnet.eval()\ntest_images = sample_df[\"ImageId\"].unique()\nfor img_name, img in tqdm(test_generator(test_images), total=len(test_images)):\n    X = torch.tensor(img, dtype=torch.float32).to(device)\n    mask_pred = net(X)\n    mask_pred = mask_pred.cpu().detach().numpy()\n    mask_prob = np.argmax(mask_pred, axis=1)\n    mask_prob = mask_prob.T.ravel(order='F')\n    class_dict = run_length(mask_prob)\n    if len(class_dict) == 0:\n        for i in range(4):\n            sub_list.append([img_name+ \"_\" + str(i+1), ''])\n    else:\n        for key, val in class_dict.items():\n            sub_list.append([img_name + \"_\" + str(key+1), \" \".join(map(str, val))])\n        for i in range(4):\n            if i not in class_dict.keys():\n                sub_list.append([img_name+ \"_\" + str(i+1), ''])\n                \nprint(\"Total len {0}\".format(len(sub_list)))\nprint(sub_list[:5])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Debug"},{"metadata":{"trusted":true},"cell_type":"code","source":"# img_name = '5e581254c.jpg'\n# img = cv2.imread(train_img_dir + img_name)\n# # HWC -> CHW\n# img = img.transpose((2, 0, 1))\n# img = np.asarray([img], dtype=np.float32) / 255\n# X = torch.tensor(img, dtype=torch.float32).to(device)\n# mask_pred = net(X)\n# mask_pred = mask_pred.cpu().detach().numpy()\n# mask_prob = np.argmax(mask_pred, axis=1)\n# mask_prob = mask_prob.ravel()\n# mask_prob\n# mask_prob.resize(256, 1600)\n# plt.imshow(mask_prob)\n\n# d = run_length(mask_prob.ravel())\n# nmask = {}\n# nmask['EncodedPixels'] = []\n# nmask['ClassId'] = []\n# for k,v in d.items():\n#     nmask['ClassId'].append(str(k))\n#     nmask['EncodedPixels'].append(' '.join(map(str,v)))\n# for i in range(4):\n#     if str(i) not in nmask['ClassId']:\n#         nmask['ClassId'].append(str(i))\n#         nmask['EncodedPixels'].append(np.nan)\n# nmask = pd.DataFrame.from_dict(nmask)\n# nmask","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make Submission File"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.DataFrame(sub_list, columns=['ImageId_ClassId', 'EncodedPixels'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"end = time.time()\nhours, rem = divmod(end-start, 3600)\nminutes, seconds = divmod(rem, 60)\nprint(\"Execution Time  {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Thanks for your comment"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"1384aabdaee94a08b5413f41284bdadc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1df37816160c44969f2d16c6c48f2603","placeholder":"â€‹","style":"IPY_MODEL_9cef963a89504e7595a953066a231d62","value":"100% 1801/1801 [03:22&lt;00:00,  9.05it/s]"}},"1df37816160c44969f2d16c6c48f2603":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e3500d06a75456f8df23470cfd5df46":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_804f0af1a5d2422cb913381ebf0205fe","IPY_MODEL_1384aabdaee94a08b5413f41284bdadc"],"layout":"IPY_MODEL_f4833a5a051d46e1aea21a44b07e82ba"}},"804f0af1a5d2422cb913381ebf0205fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c20de7ad353243119a2e1c5edd9fdbc8","max":1801,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ae1b669d0d0a4bf3a61fdd2d778c0efd","value":1801}},"9cef963a89504e7595a953066a231d62":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae1b669d0d0a4bf3a61fdd2d778c0efd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c20de7ad353243119a2e1c5edd9fdbc8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4833a5a051d46e1aea21a44b07e82ba":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":1}