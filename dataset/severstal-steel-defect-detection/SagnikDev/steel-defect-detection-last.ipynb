{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Import Packages"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"#Importing modules\nimport os\nimport sys\nimport cv2\nimport torch\nimport random\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport torch.nn as nn\nfrom IPython import display\nimport albumentations as albu\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\nimport torchvision.models as models\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms.functional as TF\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom sklearn.model_selection import StratifiedShuffleSplit","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Progress Bar"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def update_progress(progress,line):\n    '''\n    update_progress() : Displays or updates a console progress bar\n    Accepts a float between 0 and 1. Any int will be converted to a float.\n    A value under 0 represents a 'halt'.\n    A value at 1 or bigger represents 100%\n    '''\n    barLength = 50 # Modify this to change the length of the progress bar\n    status = line\n    if isinstance(progress, int):\n        progress = float(progress)\n    if not isinstance(progress, float):\n        progress = 0\n        status = \"error: progress var must be float\"\n    if progress < 0:\n        progress = 0\n        status = \"Halt...\"\n    if progress >= 1:\n        progress = 1\n        status = \"Done...\"+\" \"*50\n    block = int(round(barLength*progress))\n    text = \"\\rPercent: [{:s}] {:.2f}% {:s}\".format( \"â–ˆ\"*block + \"-\"*(barLength-block), progress*100, status)\n    sys.stdout.write(text)\n    sys.stdout.flush()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mask Visualize Func"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Image Visualization\n# helper function for data visualization\ndef visualize(**images):\n    \"\"\"PLot images in one row.\"\"\"\n    n = len(images)\n    plt.figure(figsize=(16, 5))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(np.array(image))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Class List"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Total Number of Classes\nclassList = ['1','2','3','4']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Image Preprocessor"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Processing of Image\nclass ImageProcessor:\n    def HorizontalFlip(self,p = 0.5):\n        return albu.HorizontalFlip(p = p)\n    \n    def ShiftScaleRotate(self,scale_limit = 0.5, rotate_limit = 0, shift_limit = 0.1, p = 1, border_mode = 0):\n        return albu.ShiftScaleRotate(\n            scale_limit = scale_limit,\n            rotate_limit = rotate_limit,\n            shift_limit = rotate_limit,\n            p = p, border_mode = border_mode\n        )\n\n    def PadIfNeeded(self,min_height = 256, min_width = 256, always_apply = True, border_mode = 0):\n        return albu.PadIfNeeded(\n            min_height = min_height,\n            min_width = min_width,\n            always_apply = always_apply,\n            border_mode = border_mode\n        )\n    \n    def RandomCrop(self,height = 256, width = 256, always_apply = True):\n        return albu.RandomCrop(height = height, width = width, always_apply = always_apply)\n    \n    def IAAAdditiveGaussianNoise(self,p = 0.2):\n        return albu.IAAAdditiveGaussianNoise(p = p)\n    \n    def IAAPerspective(self,p = 1):\n        return albu.IAAPerspective(p = p)\n    \n    def CLAHE(self,p = 1):\n        return albu.CLAHE(p = p)\n    \n    def RandomBrightness(self,p = 1):\n        return albu.RandomBrightness(p = p)\n    \n    def RandomGamma(self,p = 1):\n        return albu.RandomGamma(p = p)\n    \n    def IAASharpen(self,p = 1):\n        return albu.IAASharpen(p = p)\n    \n    def Blur(self,blur_limit = 3, p = 1):\n        return albu.Blur(blur_limit = blur_limit, p = p)\n    \n    def MotionBlur(self,blur_limit = 3, p = 1):\n        return albu.MotionBlur(blur_limit = blur_limit, p = p)\n    \n    def RandomContrast(self, p = 1):\n        return albu.RandomContrast(p = p)\n    \n    def HueSaturationValue(self,p = 1):\n        return albu.HueSaturationValue(p = p)\n    \n    def OneOf(self,operations = [], p = 0.9):\n        return albu.OneOf(operations, p = p)\n    \n    def Compose(self,trans = []):\n        return albu.Compose(trans)\n    \n    def Lambda(self,**kwargs):\n        return albu.Lambda(**kwargs)\n    \n    def augment(self,**kwargs):\n        kwargs = dict(map(lambda item:(item[0],np.array(item[1])),kwargs.items()))\n        transform = self.Compose([\n            self.HorizontalFlip(p=0.5),\n\n            self.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n\n            self.PadIfNeeded(min_height=256, min_width=256, always_apply=True, border_mode=0),\n            self.RandomCrop(height=256, width=256, always_apply=True),\n\n            self.IAAAdditiveGaussianNoise(p=0.2),\n            self.IAAPerspective(p=0.5),\n\n            self.OneOf(\n                [\n                    self.CLAHE(p=1),\n                    self.RandomBrightness(p=1),\n                    self.RandomGamma(p=1),\n                ],\n                p=0.9,\n            ),\n\n            self.OneOf(\n                [\n                    self.IAASharpen(p=1),\n                    self.Blur(blur_limit=3, p=1),\n                    self.MotionBlur(blur_limit=3, p=1),\n                ],\n                p=0.9,\n            ),\n\n            self.OneOf(\n                [\n                    self.RandomContrast(p=1),\n                    self.HueSaturationValue(p=1),\n                ],\n                p=0.9,\n            )\n        ])\n        sample = transform(**kwargs)\n        return dict(map(lambda item:(item[0],Image.fromarray(sample[item[0]])),kwargs.items())).values()\n    #Run Length Encoding\n    def rle2mask(self,mask_rle, shape = (1600, 256)):\n        img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n        if mask_rle:\n            s = mask_rle.split()\n            starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n            starts -= 1\n            ends = starts + lengths\n            for lo, hi in zip(starts, ends):\n                img[lo:hi] = 1\n        return img.reshape(shape).T\n    \n    def mask2rle(self,img):\n        '''\n        img: numpy array, 1 - mask, 0 - background\n        Returns run length as string formated\n        '''\n        pixels= np.array(img).T.flatten()\n        pixels = np.concatenate([[0], pixels, [0]])\n        runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n        runs[1::2] -= runs[::2]\n        return ' '.join(str(x) for x in runs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Class for getting Datas from training set and processing Image\nclass TrainDataset:\n    class_map = torch.Tensor([\n        [1,0],\n        [0,1]\n    ])\n    def __init__(self,path,class_id):\n        self.path = path\n        self.input = pd.read_csv('%s/train.csv'%self.path)\n        \n        self.naInput = self.input[self.input.isnull().any(axis=1)].reset_index(drop=True)\n        \n        self.input.dropna(inplace=True)\n        self.input = self.input[self.input.ClassId == int(class_id)].reset_index(drop=True)\n        \n        self.ip = ImageProcessor()\n\n    def __len__(self):\n        return len(self.input)\n\n    def __getitem__(self,idx):\n        image = Image.open('%s/train_images/%s'%(self.path,self.input.iloc[idx,0]))\n        mask = Image.fromarray(self.ip.rle2mask(self.input.iloc[idx,2])*255)\n        p = random.random()\n        while True:\n            img,msk = self.ip.augment(image = image, mask = mask)\n            img = TF.to_tensor(img)\n            msk = TF.to_tensor(msk)\n            if (p > 0.5 and msk.sum()):\n                break\n            if p < 0.5 and not msk.sum():\n                break\n            if p < 0.5 and msk.sum():\n                n = random.randint(0,len(self.naInput))\n                image = image = Image.open('%s/train_images/%s'%(self.path,self.naInput.iloc[idx,0]))\n        image, mask = img, msk\n        image = TF.normalize(image,mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]) \n        return image,self.class_map[mask[0].round().long()].permute(2,0,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distributing to different classes\ntrain_dataset = {}\nfor classId in classList:\n    train_dataset[classId] = TrainDataset('../input/severstal-steel-defect-detection/',classId)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize Mask"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Visualize random image of different classes\ncls = str(np.random.choice([1,4]))\nprint('Class: %s'%cls)\nn = np.random.choice(len(train_dataset[cls]))\nimage,mask = train_dataset[cls][n]\nvisualize(image = TF.to_pil_image(image),mask = TF.to_pil_image(mask).convert('L'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Hard Attention\nclass SpatialAttention2d(nn.Module):\n    def __init__(self, channel):\n        super(SpatialAttention2d, self).__init__()\n        self.squeeze = nn.Conv2d(channel, 1, kernel_size=1, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        z = self.squeeze(x)\n        z = self.sigmoid(z)\n        return x * z","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Global Attention Unit-Soft Attention\nclass GAB(nn.Module):\n    def __init__(self, input_dim, reduction=4):\n        super(GAB, self).__init__()\n        self.global_avgpool = nn.AdaptiveAvgPool2d(1)\n        self.conv1 = nn.Conv2d(input_dim, input_dim // reduction, kernel_size=1, stride=1)\n        self.conv2 = nn.Conv2d(input_dim // reduction, input_dim, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        z = self.global_avgpool(x)\n        z = self.relu(self.conv1(z))\n        z = self.sigmoid(self.conv2(z))\n        return x * z","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Concurrent Spital and Channel Squeze and Excitation\nclass SCse(nn.Module):\n    def __init__(self, dim):\n        super(SCse, self).__init__()\n        self.satt = SpatialAttention2d(dim)\n        self.catt = GAB(dim)\n\n    def forward(self, x):\n        return self.satt(x) + self.catt(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Residual Network- Adding More Layer\nclass ResNet34(nn.Module):\n    def __init__(self, pretrained=True):\n        \"\"\"Declare all needed layers.\"\"\"\n        super(ResNet34, self).__init__()\n        resnet = models.resnet34(pretrained=pretrained)\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False)\n        self.bn1 =  resnet.bn1\n        self.relu = resnet.relu\n        self.maxpool = resnet.maxpool\n        self.layer1 = nn.Sequential(resnet.layer1,SCse(64))\n        self.layer2 = nn.Sequential(resnet.layer2,SCse(128))\n        self.layer3 = nn.Sequential(resnet.layer3,SCse(256))\n        self.layer4 = nn.Sequential(resnet.layer4,SCse(512))\n        del resnet\n\n    def forward(self, x):\n        feature_map = []\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        for i in range(1,5):\n            x = getattr(self,'layer%s'%i)(x)\n            feature_map.append(x)\n\n        out = nn.AvgPool2d(x.shape[2:])(x).view(x.shape[0], -1)\n\n        return feature_map, out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Pyramid Attention--Extract dense feature of Picture\nclass FPA(nn.Module):\n    def __init__(self, channels=512):\n        \"\"\"\n        Feature Pyramid Attention\n        :type channels: int\n        \"\"\"\n        super(FPA, self).__init__()\n        channels_mid = int(channels/4)\n\n        self.channels_cond = channels\n\n        # Master branch\n        self.conv_master = nn.Conv2d(self.channels_cond, channels, kernel_size=1, bias=False)\n        self.bn_master = nn.BatchNorm2d(channels)\n\n        # Global pooling branch\n        self.conv_gpb = nn.Conv2d(self.channels_cond, channels, kernel_size=1, bias=False)\n        self.bn_gpb = nn.BatchNorm2d(channels)\n\n        # C333 because of the shape of last feature maps is (16, 16).\n        self.conv7x7_1 = nn.Conv2d(self.channels_cond, channels_mid, kernel_size=(7, 7), stride=2, padding=3, bias=False)\n        self.bn1_1 = nn.BatchNorm2d(channels_mid)\n        self.conv5x5_1 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(5, 5), stride=2, padding=2, bias=False)\n        self.bn2_1 = nn.BatchNorm2d(channels_mid)\n        self.conv3x3_1 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(3, 3), stride=2, padding=1, bias=False)\n        self.bn3_1 = nn.BatchNorm2d(channels_mid)\n\n        self.conv7x7_2 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(7, 7), stride=1, padding=3, bias=False)\n        self.bn1_2 = nn.BatchNorm2d(channels_mid)\n        self.conv5x5_2 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(5, 5), stride=1, padding=2, bias=False)\n        self.bn2_2 = nn.BatchNorm2d(channels_mid)\n        self.conv3x3_2 = nn.Conv2d(channels_mid, channels_mid, kernel_size=(3, 3), stride=1, padding=1, bias=False)\n        self.bn3_2 = nn.BatchNorm2d(channels_mid)\n\n        # Convolution Upsample\n        self.conv_upsample_3 = nn.ConvTranspose2d(channels_mid, channels_mid, kernel_size=4, stride=2, padding=1, bias=False)\n        self.bn_upsample_3 = nn.BatchNorm2d(channels_mid)\n\n        self.conv_upsample_2 = nn.ConvTranspose2d(channels_mid, channels_mid, kernel_size=4, stride=2, padding=1, bias=False)\n        self.bn_upsample_2 = nn.BatchNorm2d(channels_mid)\n\n        self.conv_upsample_1 = nn.ConvTranspose2d(channels_mid, channels, kernel_size=4, stride=2, padding=1, bias=False)\n        self.bn_upsample_1 = nn.BatchNorm2d(channels)\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: Shape: [b, 2048, h, w]\n        :return: out: Feature maps. Shape: [b, 2048, h, w]\n        \"\"\"\n        # Master branch\n        x_master = self.conv_master(x)\n        x_master = self.bn_master(x_master)\n\n        # Global pooling branch\n        x_gpb = nn.AvgPool2d(x.shape[2:])(x).view(x.shape[0], self.channels_cond, 1, 1)\n        x_gpb = self.conv_gpb(x_gpb)\n        x_gpb = self.bn_gpb(x_gpb)\n\n        # Branch 1\n        x1_1 = self.conv7x7_1(x)\n        x1_1 = self.bn1_1(x1_1)\n        x1_1 = self.relu(x1_1)\n        x1_2 = self.conv7x7_2(x1_1)\n        x1_2 = self.bn1_2(x1_2)\n\n        # Branch 2\n        x2_1 = self.conv5x5_1(x1_1)\n        x2_1 = self.bn2_1(x2_1)\n        x2_1 = self.relu(x2_1)\n        x2_2 = self.conv5x5_2(x2_1)\n        x2_2 = self.bn2_2(x2_2)\n\n        # Branch 3\n        x3_1 = self.conv3x3_1(x2_1)\n        x3_1 = self.bn3_1(x3_1)\n        x3_1 = self.relu(x3_1)\n        x3_2 = self.conv3x3_2(x3_1)\n        x3_2 = self.bn3_2(x3_2)\n        # Merge branch 1 and 2\n        x3_upsample = self.relu(self.bn_upsample_3(self.conv_upsample_3(x3_2)))\n        x2_merge = self.relu(x2_2 + x3_upsample)\n        x2_upsample = self.relu(self.bn_upsample_2(self.conv_upsample_2(x2_merge)))\n        x1_merge = self.relu(x1_2 + x2_upsample)\n\n        x_master = x_master * self.relu(self.bn_upsample_1(self.conv_upsample_1(x1_merge)))\n\n        out = self.relu(x_master + x_gpb)\n\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Global Attention Unit\nclass GAU(nn.Module):\n    def __init__(self, channels_high, channels_low, upsample=True):\n        super(GAU, self).__init__()\n        # Global Attention Upsample\n        self.upsample = upsample\n        self.conv3x3 = nn.Conv2d(channels_low, channels_low, kernel_size=3, padding=1, bias=False)\n        self.bn_low = nn.BatchNorm2d(channels_low)\n\n        self.conv1x1 = nn.Conv2d(channels_high, channels_low, kernel_size=1, padding=0, bias=False)\n        self.bn_high = nn.BatchNorm2d(channels_low)\n\n        if upsample:\n            self.conv_upsample = nn.ConvTranspose2d(channels_high, channels_low, kernel_size=4, stride=2, padding=1, bias=False)\n            self.bn_upsample = nn.BatchNorm2d(channels_low)\n        else:\n            self.conv_reduction = nn.Conv2d(channels_high, channels_low, kernel_size=1, padding=0, bias=False)\n            self.bn_reduction = nn.BatchNorm2d(channels_low)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, fms_high, fms_low, fm_mask=None):\n        \"\"\"\n        Use the high level features with abundant catagory information to weight the low level features with pixel\n        localization information. In the meantime, we further use mask feature maps with catagory-specific information\n        to localize the mask position.\n        :param fms_high: Features of high level. Tensor.\n        :param fms_low: Features of low level.  Tensor.\n        :param fm_mask:\n        :return: fms_att_upsample\n        \"\"\"\n        b, c, h, w = fms_high.shape\n\n        fms_high_gp = nn.AvgPool2d(fms_high.shape[2:])(fms_high).view(len(fms_high), c, 1, 1)\n        fms_high_gp = self.conv1x1(fms_high_gp)\n        fms_high_gp = self.bn_high(fms_high_gp)\n        fms_high_gp = self.relu(fms_high_gp)\n\n        fms_low_mask = self.conv3x3(fms_low)\n        fms_low_mask = self.bn_low(fms_low_mask)\n\n        fms_att = fms_low_mask * fms_high_gp\n        if self.upsample:\n            out = self.relu(\n                self.bn_upsample(self.conv_upsample(fms_high)) + fms_att)\n        else:\n            out = self.relu(\n                self.bn_reduction(self.conv_reduction(fms_high)) + fms_att)\n\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PANPlusPlus(nn.Module):\n    def __init__(self, channels_blocks = [64,128,256,512]):\n        super(PANPlusPlus, self).__init__()\n        \n        self.fpa_1 = FPA(channels=channels_blocks[1])\n        self.fpa_2 = FPA(channels=channels_blocks[2])\n        self.fpa_3 = FPA(channels=channels_blocks[3])\n\n        self.gau_01 = GAU(channels_blocks[1],channels_blocks[0])\n        self.gau_11 = GAU(channels_blocks[2],channels_blocks[1])\n        self.gau_21 = GAU(channels_blocks[3],channels_blocks[2])\n\n        self.gau_02 = GAU(channels_blocks[1],2*channels_blocks[0])\n        self.gau_12 = GAU(channels_blocks[2],2*channels_blocks[1])\n\n        self.gau_03 = GAU(2*channels_blocks[1],4*channels_blocks[0])\n\n    def forward(self, fms=[]):\n        \"\"\"\n        :param fms: Feature maps of forward propagation in the network. shape:[b, c, h, w]\n        \"\"\"\n        fm_low_00 = fms[0] #[B,64,H,W]\n        fm_low_10 = fms[1] #[B,128,H,W]\n        fm_high_10 = self.fpa_1(fm_low_10) #[B,128,H,W]\n        fm_high_01 = self.gau_01(fm_high_10, fm_low_00) #[B,64,H,W]\n\n        fm_low_20 = fms[2] #[B,256,H,W]\n        fm_high_20 = self.fpa_2(fm_low_20) #[B,256,H,W]\n        fm_high_11 = self.gau_11(fm_high_20, fm_low_10) #[B,128,H,W]\n        fm_high_02 = self.gau_02(fm_high_11, torch.cat([fm_low_00,fm_high_01],dim=1)) #[B,64+64,H,W]\n \n        fm_low_30 = fms[3] #[B,512,H,W]\n        fm_high_30 = self.fpa_3(fm_low_30) #[B,512,H,W]\n        fm_high_21 = self.gau_21(fm_high_30,fm_low_20) #[B,256,H,W]\n        fm_high_12 = self.gau_12(fm_high_21, torch.cat([fm_low_10,fm_high_11],dim=1)) #[B,128+128,H,W]\n        fm_high_03 = self.gau_03(fm_high_12, torch.cat([fm_low_00,fm_high_01,fm_high_02],dim=1))\n        return fm_high_03","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class RESPANPLUS(nn.Module):\n    def __init__(self,num_class,mask_criterion,defect_criterion):\n        super(RESPANPLUS, self).__init__()\n\n        self.mask_criterion = mask_criterion\n        self.defect_criterion = defect_criterion\n\n        self.enc = ResNet34(False)\n        self.dec = PANPlusPlus()\n        self.defect_classifier = nn.Sequential(\n            nn.Linear(512,256),\n            nn.ReLU(inplace=True),\n            nn.Linear(256,num_class),\n            nn.Softmax(dim=1))\n\n        self.mask_classifier = nn.Sequential(\n            nn.ConvTranspose2d(256, 64, kernel_size=2, stride=2, padding=0, bias=False), #kernel_size=4, stride=2, padding=1\n            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(32, num_class, kernel_size=1, bias=False),\n            nn.Softmax(dim=1))\n    \n    def forward(self,x, y = None):\n        fms,cls_feats = self.enc(x)\n        logits = self.defect_classifier(cls_feats)\n        out = torch.zeros(*logits.shape,*x.shape[2:], device = x.device)\n        if y is not None:\n            c_y = y.max(1)[1].sum([1,2]) > 0\n            if c_y.sum() > 1:\n                out[c_y == 1] = self.mask_classifier(self.dec([fm[c_y == 1] for fm in fms]))\n                mask_loss = self.mask_criterion(out[:,1,:,:],y[:,1,:,:])\n            else:\n                mask_loss = 0\n            class_loss = self.defect_criterion(logits,c_y.long())\n            loss = class_loss + mask_loss\n            return out,loss\n        else:\n            c_y = logits.max(1)[1]\n            if c_y.sum() > 0:\n                out[c_y == 1] = self.mask_classifier(self.dec([fm[c_y == 1] for fm in fms]))\n            return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Focal BCE Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cross entropy Loss\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.logits = logits\n        self.reduce = reduce\n\n    def forward(self, inputs, targets):\n        if self.logits:\n            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n        else:\n            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n\n        if self.reduce:\n            return torch.mean(F_loss)\n        else:\n            return F_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Checkpoint"},{"metadata":{"trusted":true},"cell_type":"code","source":"if os.path.exists('../input/steel-defect-detection-last/model.pth'):\n    checkpoint = torch.load(\n        '../input/steel-defect-detection-last/model.pth',\n        map_location = lambda storage, loc: storage.cuda()\n    )\nelse:\n    checkpoint = {}\ntorch.save(checkpoint,'model.pth')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model and loss function"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = checkpoint.get('model',{})\noptimizer = checkpoint.get('optimizer',{})\nexp_lr_scheduler = {}\n\nmask_criterion = FocalLoss()\ndefect_criterion = nn.CrossEntropyLoss()\n\nif torch.cuda.is_available():\n    mask_criterion = mask_criterion.cuda()\n    defect_criterion = defect_criterion.cuda()\n\nfor key in classList:\n    model[key] = model.get(key,RESPANPLUS(2,mask_criterion,defect_criterion))\n    if torch.cuda.is_available():\n        model[key] = model[key].cuda()\n    optimizer[key] = optimizer.get(key,torch.optim.Adam(\n        [\n            {'params': model[key].dec.parameters(), 'lr': 1e-4}, \n    \n            # decrease lr for encoder in order not to permute \n            # pre-trained weights with large gradients on training start\n            {'params': model[key].enc.parameters(), 'lr': 1e-5}\n        ]\n    ))\n    exp_lr_scheduler[key] = torch.optim.lr_scheduler.StepLR(optimizer[key], step_size=7, gamma=0.1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train and validation split with loader"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 10\ntrain_loader = {}\nval_loader = {}\ntrain_indices = checkpoint.get('train_indices',{})\nval_indices = checkpoint.get('val_indices',{})\nfor key in classList:\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.33, random_state=42)\n    splitter = sss.split(train_dataset[key].input, train_dataset[key].input.EncodedPixels.apply(str.split).apply(len)>0)\n    if not (key in train_indices and key in val_indices):\n        train_indices[key], val_indices[key] = next(splitter)\n    # Creating PT data samplers and loaders\n    train_sampler = SubsetRandomSampler(train_indices[key])\n    val_sampler = SubsetRandomSampler(val_indices[key])\n    train_loader[key] = DataLoader(train_dataset[key],batch_size=batch_size,sampler=train_sampler)\n    val_loader[key] = DataLoader(train_dataset[key], batch_size=batch_size,sampler=val_sampler)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"#If two identical then 1 or 0\ndef dice_coeff(prob, target):\n    batch_size = target.size(0)\n    prob = prob.view(batch_size,-1)\n    target = target.view(batch_size,-1)\n    smooth = 1e-6\n    intersection = (prob * target)\n    score = (2. * intersection.sum(1) + smooth) / (prob.sum(1) + target.sum(1) + smooth)\n    return score.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Validation Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def validate():\n    total_loss = 0\n    total_accuracy = 0\n    for i,key in enumerate(classList,start=1):\n        print('\\nvalidating of type %s'%key)\n        label_loss = 0\n        label_accuracy = 0\n        completed = 0\n        model[key].eval()\n        for batch_idx,(data,target) in enumerate(val_loader[key],start=1):\n            if torch.cuda.is_available():\n                data = data.cuda()\n                target = target.cuda()\n            output,loss = model[key](data,target)\n            accuracy = dice_coeff(output.max(1)[1].float(), target.max(1)[1].float()).item()\n            label_loss += loss.item()\n            label_accuracy += accuracy\n            # print statistics\n            completed += target.size(0)/len(val_indices[key])\n            update_progress(completed,\"Acuuracy: {:.2f} Loss: {:.4f}\".format(accuracy*100,loss.item()))\n            torch.cuda.empty_cache()\n        print('\\nAccuracy:',round(label_accuracy*100/batch_idx,2),'Loss:',round(label_loss/batch_idx,4))\n        total_loss += label_loss/batch_idx\n        total_accuracy += label_accuracy/batch_idx\n    print(\"\\nValidation Accuracy:\",round(total_accuracy*100/i,2),'Validation Loss:',round(total_loss/i,4))\n    return total_accuracy/i, total_loss/i","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(epoch):\n    total_loss = 0\n    total_accuracy = 0\n    print(\"Epoch:\",epoch)\n    for i,key in enumerate(classList,start=1):\n        print('\\nTraining of type %s'%key)\n        label_loss = 0\n        label_accuracy = 0\n        completed = 0\n        model[key].train()\n        for batch_idx,(data,target) in enumerate(train_loader[key],start=1):\n            if torch.cuda.is_available():\n                data = data.cuda()\n                target = target.cuda()\n            output,loss = model[key](data,target)\n            optimizer[key].zero_grad()\n            loss.backward()\n            optimizer[key].step()\n            accuracy = dice_coeff(output.max(1)[1].float(), target.max(1)[1].float()).item()\n            label_loss += loss.item()\n            label_accuracy += accuracy\n            # print statistics\n            completed += target.size(0)/len(train_indices[key])\n            update_progress(completed,\"Acuuracy: {:.2f} Loss: {:.4f}\".format(accuracy*100,loss.item()))\n            torch.cuda.empty_cache()\n        print('\\nAccuracy:',round(label_accuracy*100/batch_idx,2),'Loss:',round(label_loss/batch_idx,4))\n        total_loss += label_loss/batch_idx\n        total_accuracy += label_accuracy/batch_idx\n        exp_lr_scheduler[key].step()\n    print(\"\\nTrain Accuracy:\",round(total_accuracy*100/i,2),'Train Loss:',round(total_loss/i,4))\n    val_accuracy, val_loss = validate()\n    return total_accuracy/i,val_accuracy,total_loss/i,val_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train and Validate"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = checkpoint.get('train_data',[])\nval_data = checkpoint.get('val_data',[])\nepoch_data = checkpoint.get('epoch_data',[])\nfor i in range(len(epoch_data)+1,len(epoch_data)+1+1):\n#     train_accuracy,val_accuracy,train_loss,val_loss = train(i)\n#     train_data.append([train_accuracy,train_loss])\n#     val_data.append([val_accuracy,val_loss])\n#     epoch_data.append(i)\n#     # Save model\n#     #if np.array(val_data)[:,1].min() == val_loss:\n#     torch.save({'model':model,'optimizer':optimizer,'train_indices':train_indices,'val_indices':val_indices,'train_data':train_data,'val_data':val_data,'epoch_data':epoch_data},'model.pth')\n    # Visualize\n    fig, ax = plt.subplots(1, 2, figsize=(10*2,7*1))\n    ax[0].plot(epoch_data, np.array(train_data)[:,0], label=\"Train Accuracy {:.2f}\".format(train_data[-1][0]*100))\n    ax[0].plot(epoch_data, np.array(val_data)[:,0], label=\"Validation Accuracy {:.2f}\".format(val_data[-1][0]*100))\n    ax[1].plot(epoch_data, np.array(train_data)[:,1], label=\"Train Loss {:.4f}\".format(train_data[-1][1]))\n    ax[1].plot(epoch_data,np.array(val_data)[:,1], label=\"Validation Loss {:.4f}\".format(val_data[-1][1]))                     \n    display.clear_output(wait=False)\n    ax[0].legend()\n    ax[1].legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize on train dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(typ,ind):\n    typ = str(typ)\n    x,y = train_dataset[typ][ind]\n    x = torch.Tensor(x).unsqueeze(0)\n    y = torch.Tensor(y).unsqueeze(0)\n    if torch.cuda.is_available():\n        x = x.cuda()\n        y = y.cuda()\n    model[typ].eval()\n    pred = model[typ](x)\n    print('Dice Score:',dice_coeff(pred.max(1)[1].float(), y.max(1)[1].float()).item())\n    x = transforms.ToPILImage()(x.squeeze(0).cpu())\n    y = transforms.ToPILImage()(y.squeeze(0).cpu())\n    p = transforms.ToPILImage()(pred.squeeze(0).cpu())\n    mask = transforms.ToPILImage()((pred.max(1)[1]*255).type(torch.uint8).cpu())\n    return x,y,p,mask","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction Visualize"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Visualize\ncls = str(np.random.choice([1,4]))\nprint('Class: %s'%cls)\nn = np.random.choice(len(train_dataset[cls]))\ni,gt,pp,pm = predict(cls,n)\n\n\nvisualize(\n    image = i,\n    ground_truth = gt.convert('L'),\n    predicted_mask = pm.convert('L'),\n    predicted_proba = pp.convert('L'),\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}