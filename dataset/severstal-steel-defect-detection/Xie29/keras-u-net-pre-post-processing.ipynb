{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Info\n\nThis kernel is using regular U-Net with slightly depper layers as training model. <br />\nThe total training time will be quite long, usually I would take 2-3 commit to finish the training. <br />\nAnd I also did some pre/post-processing on the data and prediction to improve the score. <br />\nSince the submission time limit is 60 mins, so I will save the model and submit the result in other commit version. <br />\n<br />\n**preprocessing** <br />\n1. gamma correction\n2. contrast enhnacement\n<br />\n\n**postprocessing** <br />\n1. mask reduction (Which is the same as Rishabh Agrahari's share)\n<br />\n<br />\n\n**Future work** <br />\n1. Try more complex model\n2. Use pre-train model for decoding part\n3. Deal with imbalance class\n4. Tune the callbacks\n\nI am quite new to this field, if I had done something, please let me know. <br />\nAlso I am looking for a team thread. Reach me if you are interested. Thanks!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\n\nimport keras\nimport keras.backend as K\nimport keras.layers as klayers\nfrom keras.preprocessing.image import load_img, ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nimport gc\nimport random\nmain_dir = '../input/severstal-steel-defect-detection/'\nos.listdir(main_dir)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyper-parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrain_weights_path = None\nbatch_size = 32\nepochs = 15\nreshape_rgb = (256, 512, 3)\nreshape_mask = (256, 512)\nmask_threshold = 3500\nmask_bound = 0.8\nlr = 3e-3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preparation/EDA/Visualization"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_fns = os.listdir(main_dir + 'train_images')\ntest_fns = os.listdir(main_dir + 'test_images')\ntrain_seg = pd.read_csv(main_dir + 'train.csv')\n\nprint(len(train_fns))\nprint(len(test_fns))\nprint(train_seg.shape)\n\ntrain_seg.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_seg['ImageId'] = train_seg['ImageId_ClassId'].map(lambda x : x.split('_')[0])\ntrain_seg['ClassId'] = train_seg['ImageId_ClassId'].map(lambda x : x.split('_')[1])\ntrain_seg = train_seg.drop(['ImageId_ClassId'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_seg['has_label'] = train_seg['EncodedPixels'].map(lambda x : 1 if isinstance(x,str) else 0)\ntrain_seg.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image_with_label = train_seg.groupby(['ImageId'])['has_label'].sum().value_counts()\nprint(Image_with_label)\nplt.figure(figsize = (6,4))\nplt.bar(Image_with_label.index, Image_with_label.values)\nplt.xlabel('label number')\nplt.ylabel('count')\nplt.title('Count of label number in single image')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_with_label = train_seg.groupby(['ClassId'])['has_label'].sum().reset_index()\nplt.figure(figsize = (6,4))\nplt.bar(class_with_label.ClassId.values, class_with_label.has_label.values)\nplt.xlabel('class id')\nplt.ylabel('count')\nplt.title('Count of each class id who has labeled')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some utilities function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rle_encoding(mask):\n    \n    pixels = mask.T.flatten()\n    pixels = np.concatenate([[0], pixels,[0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    if len(runs) % 2:\n        runs = np.append(runs,len(pixels))\n    runs[1::2] -= runs[0::2]\n    \n    return ' '.join(str(x) for x in runs)\n\ndef rle_decoding(rle, mask_shape = (256,1600)):\n    strs = rle.split(' ')\n    starts = np.asarray(strs[0::2], dtype = int) - 1\n    lengths = np.asarray(strs[1::2], dtype = int)\n    ends = starts + lengths\n    \n    mask = np.zeros(mask_shape[0] * mask_shape[1], dtype = np.uint8)\n    for s,e in zip(starts, ends):\n        mask[s:e] = 1\n    return mask.reshape(mask_shape, order = 'F')\n\ndef merge_masks(image_id, df, mask_shape = (256,1600), reshape = None):\n    \n    rles = df[df['ImageId'] == image_id].EncodedPixels.iloc[:]\n    depth = rles.shape[0]\n    if reshape:\n        masks = np.zeros((*reshape, depth), dtype = np.uint8)\n    else:\n        masks = np.zeros((mask_shape[0], mask_shape[1],depth), dtype = np.uint8)\n    \n    for idx in range(depth):\n        if isinstance(rles.iloc[idx], str):\n            if reshape:\n                cur_mask = rle_decoding(rles.iloc[idx], mask_shape)\n                cur_mask = cv2.resize(cur_mask, (reshape[1], reshape[0]))\n                masks[:,:,idx] += cur_mask\n            else:         \n                masks[:,:,idx] += rle_decoding(rles.iloc[idx], mask_shape)\n    return masks   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check rle_encoding and rle_decoding\n\nrle_1 = train_seg['EncodedPixels'].iloc[0]\nmask_1 = rle_decoding(rle_1)\nrle_2 = rle_encoding(mask_1)\nmask_2 = rle_decoding(rle_2)\n\nplt.figure(figsize = (16,8))\nplt.imshow(mask_1)\nplt.show()\nplt.figure(figsize = (16,8))\nplt.imshow(mask_2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_img_masks(img, masks, image_id = \"\", title = \"\"):\n    for idx in range(masks.shape[-1]):\n        plt.figure(figsize = (24,6))\n        plt.imshow(img)\n        plt.imshow(masks[:,:,idx], alpha = 0.35, cmap = 'gray')\n        plt.title(image_id + '_class_' + str(idx+1) + title)\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_dir = main_dir + 'train_images/' + train_seg['ImageId'].iloc[0]\nimage_id = train_seg['ImageId'].iloc[0]\nimg = plt.imread(image_dir)\nmasks = merge_masks(image_id, train_seg)\n\ndisplay_img_masks(img,masks, image_id)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Generator\n\nHere I using two data pre-processing techique and trying to improve the result.\n\n**A. gamma correction** <br />\nGamma correction is a very common operation in traditional image processing. Since human visual system is non-linear response to visible light, so the ISP on phone or digital camera will perform gamma correction to produce better contrast for human visual system. Anyway, it is a image enhancement techique. <br />\n\n**B. Contrast Limited Adaptive Histogram Equalization(CLAHE)** <br />\nThis is also a contrast enhancement techique, it is very similar to regular histogram equalization. But it is doing the effect on small blocks, which can avoid over-brightness in regular histogram equalization.\n\nreference: https://docs.opencv.org/3.3.1/d5/daf/tutorial_py_histogram_equalization.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"gamma = 1.2\ninverse_gamma = 1.0 / gamma\nlook_up_table = np.array([((i/255.0) ** inverse_gamma) * 255.0 for i in np.arange(0,256,1)]).astype(\"uint8\")\nclahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n\ndef contrast_enhancement(img):\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n    img[:,:,0] = clahe.apply(img[:,:,0])\n    img = cv2.cvtColor(img, cv2.COLOR_YUV2RGB)\n    return img\n\ndef gamma_correction(img):\n    return cv2.LUT(img.astype('uint8'), look_up_table)\n\ndef load_target_image(path, grayscale = False, color_mode = 'rgb', target_size = reshape_rgb,\n                     interpolation = 'nearest'):\n    \n    return load_img(path = path, grayscale = grayscale, color_mode = color_mode,\n                   target_size = target_size, interpolation = interpolation)\n\ndef input_gen(filenames, segs, data_gen, batch_size = 4, reshape = (256,1600)):\n    \n    load_dir = main_dir + 'train_images/'\n    \n    batch_rgb = []\n    batch_mask = []\n    \n    while True:\n        fns = random.sample(filenames, batch_size)\n        seed = np.random.choice(range(999))\n        for fn in fns:\n            cur_img = np.asarray(load_target_image(path = load_dir + fn))\n            cur_img = gamma_correction(cur_img)\n            cur_img = contrast_enhancement(cur_img)\n            masks = merge_masks(fn, segs, reshape = reshape)\n            batch_rgb.append(cur_img)\n            batch_mask.append(masks)\n        \n        batch_rgb, batch_mask = np.stack(batch_rgb), np.stack(batch_mask)\n        x = data_gen.flow(batch_rgb, batch_size = batch_size,seed = seed, shuffle = False)\n        y = data_gen.flow(batch_mask, batch_size = batch_size,seed = seed, shuffle = False)\n        \n        yield next(x)/255.0, next(y)\n        batch_rgb = []\n        batch_mask = []\n        gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x, valid_x = train_test_split(train_fns, test_size = 0.2, random_state = 2019)\nprint(len(train_x))\nprint(len(valid_x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_gen = ImageDataGenerator(rotation_range = 15,\n                                    height_shift_range = 0.1,\n                                    width_shift_range = 0.1,\n                                    vertical_flip = True,\n                                    horizontal_flip = True,\n                                    data_format = \"channels_last\",\n                                    fill_mode = 'reflect'\n                                    )\n\nvalid_data_gen = ImageDataGenerator()\n\ntrain_aug_gen = input_gen(train_x, train_seg, train_data_gen, batch_size = batch_size,reshape = reshape_mask)\nvalid_aug_gen = input_gen(valid_x, train_seg, valid_data_gen, batch_size = batch_size,reshape = reshape_mask)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build the U-Net\n\nI was using a slightly deeper U-Net architecture to be my model. Since I had tried the regular U-Net(Same depth but less filter numebr), the training and validation loss seems to be decreased quite nice until 0.4~0.5 and it wont improve anymore. And the old model seems to have no over-fitting problem, so I assume I can keep increase the complexity of model until the score decrease or the over-fitting problem becomes my concern."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Unet:\n    \n    def __init__(self, input_shape = (256,1600,3), output_units = 4):\n        \n        self.input_shape = input_shape\n        self.output_units = output_units\n    \n    def _cn_bn_relu(self, filters = 64, kernel_size = (3,3), bn_flag = False):\n        \n        def f(input_x):\n            \n            x = input_x\n            x = klayers.Conv2D(filters = filters, kernel_size = kernel_size, strides = (1,1), padding = \"same\", kernel_initializer = \"he_normal\")(x)\n            if bn_flag:\n                x = klayers.BatchNormalization()(x)\n            x = klayers.Activation(\"relu\")(x)\n            \n            return x\n        return f\n    \n    def _UpSamplingBlock(self, filters = 64, kernel_size = (3,3), upsize = (2,2), bn_flag = True, up_flag = False):\n        \n        def f(up_c, con_c):\n            \n            if up_flag:\n                x = klayers.UpSampling2D(size = upsize, interpolation = 'bilinear')(up_c)\n            else:\n                x = klayers.Conv2DTranspose(filters = filters, kernel_size = (2,2), strides = upsize, padding = \"same\", kernel_initializer = \"he_normal\")(up_c)\n            \n            x = klayers.concatenate([x,con_c])\n            x = self._cn_bn_relu(filters = filters, kernel_size = kernel_size, bn_flag = bn_flag)(x)\n            x = self._cn_bn_relu(filters = filters, kernel_size = kernel_size, bn_flag = bn_flag)(x)\n            \n            return x\n        return f\n    \n    def _DownSamplingBlock(self, filters = 64, kernel_size = (3,3), downsize = (2,2), bn_flag = True, is_bottom = False):\n        \n        def f(input_x):\n            \n            x = self._cn_bn_relu(filters = filters, kernel_size = kernel_size, bn_flag = bn_flag)(input_x)\n            c = self._cn_bn_relu(filters = filters, kernel_size = kernel_size, bn_flag = bn_flag)(x)\n            if is_bottom:\n                return c\n            else:\n                p = klayers.MaxPooling2D(pool_size = downsize)(c)\n                return c,p\n        return f\n    \n    def build_unet(self):\n        \n        #encoder region\n        input_x = klayers.Input(shape = self.input_shape)\n        \n        c1,p1 = self._DownSamplingBlock(filters = 32)(input_x)\n        c2,p2 = self._DownSamplingBlock(filters = 32)(p1)\n        c3,p3 = self._DownSamplingBlock(filters = 64)(p2)\n        c4,p4 = self._DownSamplingBlock(filters = 64)(p3)\n        c5,p5 = self._DownSamplingBlock(filters = 128)(p4)\n        c6,p6 = self._DownSamplingBlock(filters = 256)(p5)\n        \n        c7 = self._DownSamplingBlock(filters = 512, is_bottom = True)(p6)\n        \n        #decoder region\n        u8 = self._UpSamplingBlock(filters = 256)(c7,c6)\n        u9 = self._UpSamplingBlock(filters = 128)(u8,c5)\n        u10 = self._UpSamplingBlock(filters = 64)(u9,c4)\n        u11 = self._UpSamplingBlock(filters = 64)(u10,c3)\n        u12 = self._UpSamplingBlock(filters = 32)(u11,c2)\n        u13 = self._UpSamplingBlock(filters = 32)(u12,c1)\n\n        output_x = klayers.Conv2D(filters = self.output_units, kernel_size = (1,1), padding = \"same\", activation = \"sigmoid\", kernel_initializer = \"he_normal\")(u13)\n        model = keras.models.Model(inputs = [input_x], outputs = [output_x])\n        return model\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unet_builder = Unet(input_shape = reshape_rgb)\nunet = unet_builder.build_unet()\nunet.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if pretrain_weights_path != None:\n    unet.load_weights(pretrain_weights_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loss fucntion\n<br />\nI was using dice loss + weighted bce to be my loss function. Since dummy submission can reach 0.85+ score, so I think the weighted bce can decrease the false postivie. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def Dice_Coef(y_true, y_pred, smooth = 1):\n    \n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    \n    intersection = K.sum(y_true_f * y_pred_f)\n    \n    return (2*intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef Dice_Loss(y_true, y_pred):\n    return 1.0 - Dice_Coef(y_true, y_pred)\n\ndef bce_dice_loss(y_true, y_pred):\n    return keras.losses.binary_crossentropy(y_true, y_pred) + Dice_Loss(y_true, y_pred)\n\ndef wbce_dice_loss(y_true, y_pred):\n    return weighted_bce()(y_true, y_pred) + Dice_Loss(y_true, y_pred)\n\ndef weighted_bce(weight = 0.6):\n    \n    def convert_2_logits(y_pred):\n        y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1 - K.epsilon())\n        return tf.log(y_pred / (1-y_pred))\n    \n    def weighted_binary_crossentropy(y_true, y_pred):\n        y_pred = convert_2_logits(y_pred)\n        loss = tf.nn.weighted_cross_entropy_with_logits(logits = y_pred, targets = y_true, pos_weight = weight)\n        return loss\n    \n    return weighted_binary_crossentropy\n\n#optimizer = keras.optimizers.SGD(lr = lr, momentum = 0.95, nesterov = True)\noptimizer = keras.optimizers.Adam(lr = lr, decay = 1e-6)\nunet.compile(loss = wbce_dice_loss, optimizer = optimizer, metrics = [Dice_Coef])\n\nreduce_lr = keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', patience = 7, mode = 'min', factor = 0.5, verbose = 1)\ncp = keras.callbacks.ModelCheckpoint('unet_out.hdf5', monitor = 'val_loss', verbose = 1, save_best_only = True, mode = 'min')\nes = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min')\ntraining_callbacks = [reduce_lr, cp, es]\n\nsteps_per_epoch = len(train_x) // batch_size\nvalidation_steps = len(valid_x) // batch_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nhistory = unet.fit_generator(train_aug_gen, steps_per_epoch = steps_per_epoch, epochs = epochs,\n                              validation_data = valid_aug_gen, validation_steps = validation_steps, verbose = 1, callbacks = training_callbacks)\n\n\nunet.load_weights('unet_out.hdf5')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef plot_training_result(history):\n    \n    plt.figure(figsize = (8,6))\n    plt.plot(history.history['loss'], '-', label = 'train_loss', color = 'g')\n    plt.plot(history.history['val_loss'], '--', label = 'valid_loss', color ='r')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.title('Loss on unet')\n    plt.legend()\n    plt.show()\n    \n    plt.figure(figsize = (8,6))\n    plt.plot(history.history['Dice_Coef'], '-', label = 'train_Dice_coef', color = 'g')\n    plt.plot(history.history['val_Dice_Coef'], '--', label = 'valid_Dice_coef', color ='r')\n    plt.xlabel('epoch')\n    plt.ylabel('Dice_Coef')\n    plt.title('Dice_Coef on unet')\n    plt.legend()\n    plt.show()\n\nplot_training_result(history)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef predict_masks(img):\n    \n    masks = unet.predict(np.expand_dims(img, axis = 0))\n    masks = np.squeeze(masks, axis = 0)\n    \n    return masks\n\nvalid_aug_gen = input_gen(valid_x, train_seg, valid_data_gen, batch_size = 8, reshape = reshape_mask)\n\nvalid_data, valid_label = next(valid_aug_gen)\nfor x,y in zip(valid_data, valid_label):\n    display_img_masks(x,y,title = \"_ground truth\")\n    prediction = predict_masks(x)\n    display_img_masks(x,prediction, title = '_prediction')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nsub = pd.DataFrame(columns = ['ImageId_ClassId', 'EncodedPixels'])\n\ndef masks_reduce(masks):\n    \n    for idx in range(masks.shape[-1]):\n        label_num, labeled_mask = cv2.connectedComponents(masks[:,:,idx].astype(np.uint8))\n        reduced_mask = np.zeros(masks.shape[:2],np.float32)\n        \n        for label in range(1, label_num):\n            single_label_mask = (labeled_mask == label)\n            if single_label_mask.sum() > mask_threshold:\n                reduced_mask[single_label_mask] = 1\n        \n        masks[:,:,idx] = reduced_mask\n        \n    return masks\n\ndef masks_reduce2(masks):\n    for idx in range(masks.shape[-1]):\n        if np.sum(masks[:,:,idx]) < mask_threshold:\n            masks[:,:,idx] = np.zeros(masks.shape[:2], dtype = np.uint8)\n    return masks\n\ndef prediction_encoding(fn, img_dir, submission, target_shape = (256,1600)):\n    img = np.asarray(load_target_image(path = os.path.join(img_dir,fn)))\n    img = gamma_correction(img)\n    img = contrast_enhancement(img)/255.0\n    masks = unet.predict(np.expand_dims(img, axis = 0))\n    masks = np.squeeze( np.round(masks), axis = 0)\n    masks = np.array(masks > mask_bound, dtype = np.uint8)    \n    masks = cv2.resize(masks, (target_shape[1], target_shape[0]))\n    masks = masks_reduce(masks)\n    \n    ImageId_ClassId = np.asarray([ fn+'_'+str(id) for id in range(1,5) ])\n    for idx in range(masks.shape[-1]):\n        submission = submission.append(pd.DataFrame([[ImageId_ClassId[idx], rle_encoding(masks[:,:,idx])]], columns = [\"ImageId_ClassId\", \"EncodedPixels\"]))\n        \n    return submission\n\nload_dir = main_dir + 'test_images/'\n\nfor fn in tqdm_notebook(test_fns):\n    sub = prediction_encoding(fn, load_dir, sub)\n    gc.collect()\n    \nsub.head(10)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nsub_sample = pd.read_csv(main_dir + 'sample_submission.csv')\nsub_sample = sub_sample.drop(['EncodedPixels'], axis = 1)\n\nsubmission = sub_sample.merge(sub, on = ['ImageId_ClassId'])\nsubmission.head(10)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}