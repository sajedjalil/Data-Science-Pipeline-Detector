{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Info\n\nThis kernel uses regular U-Net with slightly deeper layers composing with SEBlocks as training model. <br />\nAlso I use swish to replace relu to be my activation function. <br />\nAnd I changed the loss function from weighted bce + DiceLoss to ComboLoss( weighted bce + FocalLoss + Tversky_Loss) with certain weights <br />\nWith these three changes in my model training, the LB score can increase about 0.02( 0.86 -> 0.88 ) <br />\n\n\nAnd as I mentioned in my last share kernel, I also did some pre/post-processing on the data and prediction to improve the score. <br />\nThe total training time would be a little bit long (about 7-8 hours to convergenceï¼‰since I didnt use pretrain weights. <br />\nThe training processes are at version 8, 9 and 10 in this kernel. <br />\nI think the depth can be increased further after adding shortcut in the model, which might help the convergence either. <br />\n\n**model**<br />\n1.UNet + Squeeze and Excitation network <br />\n2.Swish activation <br />\n<br />\n\n**loss function**<br />\nCombeLoss (weighted bce + FocalLoss + Tversky_Loss) with 0.4 : 0.2 : 0.4 weighting <br />\n<br />\n\n**preprocessing** <br />\n1. gamma correction\n2. contrast enhnacement\n<br />\n\n**postprocessing** <br />\n1. mask reduction (Which is the same as Rishabh Agrahari's share)\n<br />\n<br />\n\n**Future work** <br />\n1. Try more complex model or add short cut for making deeper network.\n2. Use pre-train model for decoding part.\n3. Deal with imbalance class.\n\nAgain, I am quite new to this field, if I had done something wrong, please let me know. <br />\n\nSqueeze and Excitiation Networks: https://arxiv.org/abs/1709.01507 <br />\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\n\nimport keras\nimport keras.backend as K\nimport keras.layers as klayers\nfrom keras.preprocessing.image import load_img\nimport albumentations as albu\nfrom albumentations import (HorizontalFlip, VerticalFlip, ShiftScaleRotate, CLAHE , GridDistortion)\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nimport gc\nimport random\nmain_dir = '../input/severstal-steel-defect-detection/'\nos.listdir(main_dir)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyper-parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrain_weights_path = '../input/steel-pretrain/unet_out.hdf5'\nbatch_size = 32\nepochs = 15\nreshape_rgb = (256, 512, 3)\nreshape_mask = (256, 512)\nmask_threshold = 3500\nmask_bound = 0.8\nlr = 1e-4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preparation/EDA/Visualization"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_fns = os.listdir(main_dir + 'train_images')\ntest_fns = os.listdir(main_dir + 'test_images')\ntrain_seg = pd.read_csv(main_dir + 'train.csv')\n\nprint(len(train_fns))\nprint(len(test_fns))\nprint(train_seg.shape)\n\ntrain_seg.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_seg['ImageId'] = train_seg['ImageId_ClassId'].map(lambda x : x.split('_')[0])\ntrain_seg['ClassId'] = train_seg['ImageId_ClassId'].map(lambda x : x.split('_')[1])\ntrain_seg = train_seg.drop(['ImageId_ClassId'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_seg['has_label'] = train_seg['EncodedPixels'].map(lambda x : 1 if isinstance(x,str) else 0)\ntrain_seg.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image_with_label = train_seg.groupby(['ImageId'])['has_label'].sum().value_counts()\nprint(Image_with_label)\nplt.figure(figsize = (6,4))\nplt.bar(Image_with_label.index, Image_with_label.values)\nplt.xlabel('label number')\nplt.ylabel('count')\nplt.title('Count of label number in single image')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_with_label = train_seg.groupby(['ClassId'])['has_label'].sum().reset_index()\nplt.figure(figsize = (6,4))\nplt.bar(class_with_label.ClassId.values, class_with_label.has_label.values)\nplt.xlabel('class id')\nplt.ylabel('count')\nplt.title('Count of each class id who has labeled')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some utilities function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rle_encoding(mask):\n    \n    pixels = mask.T.flatten()\n    pixels = np.concatenate([[0], pixels,[0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    if len(runs) % 2:\n        runs = np.append(runs,len(pixels))\n    runs[1::2] -= runs[0::2]\n    \n    return ' '.join(str(x) for x in runs)\n\ndef rle_decoding(rle, mask_shape = (256,1600)):\n    strs = rle.split(' ')\n    starts = np.asarray(strs[0::2], dtype = int) - 1\n    lengths = np.asarray(strs[1::2], dtype = int)\n    ends = starts + lengths\n    \n    mask = np.zeros(mask_shape[0] * mask_shape[1], dtype = np.uint8)\n    for s,e in zip(starts, ends):\n        mask[s:e] = 1\n    return mask.reshape(mask_shape, order = 'F')\n\ndef merge_masks(image_id, df, mask_shape = (256,1600), reshape = None):\n    \n    rles = df[df['ImageId'] == image_id].EncodedPixels.iloc[:]\n    depth = rles.shape[0]\n    if reshape:\n        masks = np.zeros((*reshape, depth), dtype = np.uint8)\n    else:\n        masks = np.zeros((mask_shape[0], mask_shape[1],depth), dtype = np.uint8)\n    \n    for idx in range(depth):\n        if isinstance(rles.iloc[idx], str):\n            if reshape:\n                cur_mask = rle_decoding(rles.iloc[idx], mask_shape)\n                cur_mask = cv2.resize(cur_mask, (reshape[1], reshape[0]))\n                masks[:,:,idx] += cur_mask\n            else:         \n                masks[:,:,idx] += rle_decoding(rles.iloc[idx], mask_shape)\n    return masks   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check rle_encoding and rle_decoding\n\nrle_1 = train_seg['EncodedPixels'].iloc[0]\nmask_1 = rle_decoding(rle_1)\nrle_2 = rle_encoding(mask_1)\nmask_2 = rle_decoding(rle_2)\n\nplt.figure(figsize = (16,8))\nplt.imshow(mask_1)\nplt.show()\nplt.figure(figsize = (16,8))\nplt.imshow(mask_2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_img_masks(img, masks, image_id = \"\", title = \"\"):\n    for idx in range(masks.shape[-1]):\n        plt.figure(figsize = (24,6))\n        plt.imshow(img)\n        plt.imshow(masks[:,:,idx], alpha = 0.35, cmap = 'gray')\n        plt.title(image_id + '_class_' + str(idx+1) + title)\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_dir = main_dir + 'train_images/' + train_seg['ImageId'].iloc[0]\nimage_id = train_seg['ImageId'].iloc[0]\nimg = plt.imread(image_dir)\nmasks = merge_masks(image_id, train_seg)\n\ndisplay_img_masks(img,masks, image_id)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Generator\n\nHere I using two data pre-processing techiques and trying to improve the result.\n\n**A. gamma correction** <br />\nGamma correction is a very common operation in traditional image processing. Since human visual system is non-linear response to visible light, so the ISP on phone or digital camera will perform gamma correction to produce better contrast for human visual system. Anyway, it is a image enhancement techique. <br />\n\n**B. Contrast Limited Adaptive Histogram Equalization(CLAHE)** <br />\nThis is also a contrast enhancement techique, it is very similar to regular histogram equalization. But it is doing the effect on small blocks, which can avoid over-brightness in regular histogram equalization.\n\nreference: https://docs.opencv.org/3.3.1/d5/daf/tutorial_py_histogram_equalization.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"gamma = 1.2\ninverse_gamma = 1.0 / gamma\nlook_up_table = np.array([((i/255.0) ** inverse_gamma) * 255.0 for i in np.arange(0,256,1)]).astype(\"uint8\")\nclahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n\ndef contrast_enhancement(img):\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n    img[:,:,0] = clahe.apply(img[:,:,0])\n    img = cv2.cvtColor(img, cv2.COLOR_YUV2RGB)\n    return img\n\ndef gamma_correction(img):\n    return cv2.LUT(img.astype('uint8'), look_up_table)\n\ndef load_target_image(path, grayscale = False, color_mode = 'rgb', target_size = reshape_rgb,\n                     interpolation = 'nearest'):\n    \n    return load_img(path = path, grayscale = grayscale, color_mode = color_mode,\n                   target_size = target_size, interpolation = interpolation)\n\ndef input_gen(filenames, segs, aug, batch_size = 4, reshape = (256,1600)):\n    \n    load_dir = main_dir + 'train_images/'\n    \n    batch_rgb = []\n    batch_mask = []\n    \n    while True:\n        fns = random.sample(filenames, batch_size)\n        seed = np.random.choice(range(999))\n        for fn in fns:\n            cur_img = np.asarray(load_target_image(path = load_dir + fn))\n            cur_img = gamma_correction(cur_img)\n            cur_img = contrast_enhancement(cur_img)\n            masks = merge_masks(fn, segs, reshape = reshape)            \n            processed = aug(image = cur_img, mask = masks)\n\n            batch_rgb.append(processed['image']/255.0)\n            batch_mask.append(processed['mask'])\n        \n        batch_rgb, batch_mask = np.stack(batch_rgb), np.stack(batch_mask)\n        \n        yield batch_rgb, batch_mask\n        batch_rgb = []\n        batch_mask = []\n        gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x, valid_x = train_test_split(train_fns, test_size = 0.2, random_state = 2019)\nprint(len(train_x))\nprint(len(valid_x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug_for_train = albu.Compose([HorizontalFlip(p=0.5),\n                             ShiftScaleRotate(scale_limit=0.2, shift_limit=0.1, rotate_limit=15, p=0.5),\n                             GridDistortion(p=0.5)])\naug_for_valid = albu.Compose([])\n\n\ntrain_aug_gen = input_gen(train_x, train_seg, aug_for_train, batch_size = batch_size,reshape = reshape_mask)\nvalid_aug_gen = input_gen(valid_x, train_seg, aug_for_valid, batch_size = batch_size,reshape = reshape_mask)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build the SEUNet\n\nSqueeze-and-Excitiation Networks can be divide in two parts, squeeze and excitation parts. <br />\nIn very simple explanation, it aggregates feature maps across their spatial dimension and makes them as channel descriptor. <br />\nAnd it can learn the importance of dependencies of each channels during training. <br />\nThen multiply the weights to its corresponding channels of input tensor to enhance/decrease the importance. <br />"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Unet:\n    \n    def __init__(self, input_shape = (256,1600,3), output_units = 4):\n        \n        self.input_shape = input_shape\n        self.output_units = output_units\n        \n    def _swish(self,x):\n        return K.sigmoid(x)*x\n    \n    def _SEBlock(self, se_ratio=16, activation = \"relu\"):\n        \n        def f(input_x):\n\n            input_channels = input_x._keras_shape[-1]\n            reduced_channels = max(input_channels // se_ratio, 8)\n            \n            x = klayers.GlobalAveragePooling2D()(input_x)\n            x = klayers.Dense(units = reduced_channels, kernel_initializer = \"he_normal\")(x)\n            x = klayers.Activation(activation)(x)\n            x = klayers.Dense(units = input_channels, activation = 'sigmoid', kernel_initializer = \"he_normal\")(x)\n            \n            return klayers.multiply([input_x, x])\n        \n        return f    \n    \n    def _cn_bn_act(self, filters = 64, kernel_size = (3,3), bn_flag = False, activation = \"relu\"):\n        \n        def f(input_x):\n            \n            x = input_x\n            x = klayers.Conv2D(filters = filters, kernel_size = kernel_size, strides = (1,1), padding = \"same\", kernel_initializer = \"he_normal\")(x)\n            x = klayers.BatchNormalization()(x) if bn_flag == True else x\n            x = klayers.Activation(activation)(x)\n            \n            return x\n        \n        return f\n    \n    def _UpSamplingBlock(self, filters = 64, kernel_size = (3,3), upsize = (2,2), bn_flag = True, up_flag = False, se_flag = True):\n        \n        def f(up_c, con_c):\n            \n            if up_flag:\n                x = klayers.UpSampling2D(size = upsize, interpolation = 'bilinear')(up_c)\n            else:\n                x = klayers.Conv2DTranspose(filters = filters, kernel_size = (2,2), strides = upsize, padding = \"same\", kernel_initializer = \"he_normal\")(up_c)\n            \n            x = klayers.concatenate([x,con_c])\n            x = self._cn_bn_act(filters = filters, kernel_size = kernel_size, bn_flag = bn_flag, activation = self._swish)(x)\n            x = self._SEBlock(activation = self._swish)(x) if se_flag == True else x\n            x = self._cn_bn_act(filters = filters, kernel_size = kernel_size, bn_flag = bn_flag, activation = self._swish)(x)\n            \n            return x\n        return f\n    \n    def _DownSamplingBlock(self, filters = 64, kernel_size = (3,3), downsize = (2,2), bn_flag = True, is_bottom = False, se_flag = True):\n        \n        def f(input_x):\n            \n            x = self._cn_bn_act(filters = filters, kernel_size = kernel_size, bn_flag = bn_flag, activation = self._swish)(input_x)\n            x = self._SEBlock(activation = self._swish)(x) if se_flag == True else x\n            c = self._cn_bn_act(filters = filters, kernel_size = kernel_size, bn_flag = bn_flag, activation = self._swish)(x)\n            return c if (is_bottom == True) else (c,klayers.MaxPooling2D(pool_size = downsize)(c))\n            \n        return f\n    \n    def build_unet(self):\n        \n        \n        input_x = klayers.Input(shape = self.input_shape)\n        #encoder region\n        c1,p1 = self._DownSamplingBlock(filters = 16)(input_x)\n        c2,p2 = self._DownSamplingBlock(filters = 32)(p1)\n        c3,p3 = self._DownSamplingBlock(filters = 32)(p2)\n        c4,p4 = self._DownSamplingBlock(filters = 64)(p3)\n        c5,p5 = self._DownSamplingBlock(filters = 128)(p4)\n        \n        c6 = self._DownSamplingBlock(filters = 256, is_bottom = True)(p5)\n        \n        #decoder region\n        u7 = self._UpSamplingBlock(filters = 128)(c6,c5)\n        u8 = self._UpSamplingBlock(filters = 64)(u7,c4)\n        u9 = self._UpSamplingBlock(filters = 32)(u8,c3)\n        u10 = self._UpSamplingBlock(filters = 32)(u9,c2)\n        u11 = self._UpSamplingBlock(filters = 16)(u10,c1)\n        output_x = klayers.Conv2D(filters = self.output_units, kernel_size = (1,1), padding = \"same\", activation = \"sigmoid\", kernel_initializer = \"he_normal\")(u11)\n        model = keras.models.Model(inputs = [input_x], outputs = [output_x])\n        return model\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unet_builder = Unet(input_shape = reshape_rgb)\nunet = unet_builder.build_unet()\nunet.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if pretrain_weights_path != None:\n    unet.load_weights(pretrain_weights_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loss fucntion\n<br />\nThe main idea of using Combo loss is very simple: <br /><br />\nTversky Loss and weighted bce -> reduce the false positive<br />\nFocal Loss -> Learn hard samples in training set.<br />"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Tversky_Loss(y_true, y_pred, smooth = 1, alpha = 0.3, beta = 0.7, flatten = False):\n    \n    if flatten:\n        y_true = K.flatten(y_true)\n        y_pred = K.flatten(y_pred)\n    \n    TP = K.sum(y_true * y_pred)\n    FP = K.sum((1-y_true) * y_pred)\n    FN = K.sum(y_true * (1-y_pred))\n    \n    tversky_coef = (TP + smooth) / (TP + alpha * FP + beta * FN + smooth)\n    \n    return 1 - tversky_coef\n\ndef Focal_Loss(y_true, y_pred, alpha = 0.8, gamma = 2.0, flatten = False):\n    \n    if flatten:\n        y_true = K.flatten(y_true)\n        y_pred = K.flatten(y_pred)    \n    \n    bce = keras.losses.binary_crossentropy(y_true, y_pred)\n    bce_exp = K.exp(-bce)\n    \n    loss = K.mean(alpha * K.pow((1-bce_exp), gamma) * bce)\n    return loss\n\ndef weighted_bce(weight = 0.6):\n    \n    def convert_2_logits(y_pred):\n        y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1 - K.epsilon())\n        return tf.log(y_pred / (1-y_pred))\n    \n    def weighted_binary_crossentropy(y_true, y_pred):\n        y_pred = convert_2_logits(y_pred)\n        loss = tf.nn.weighted_cross_entropy_with_logits(logits = y_pred, targets = y_true, pos_weight = weight)\n        return loss\n    \n    return weighted_binary_crossentropy\n\ndef Combo_Loss(y_true, y_pred, a = 0.4, b = 0.2, c= 0.4):\n    \n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    \n    return a*weighted_bce()(y_true, y_pred) + b*Focal_Loss(y_true_f, y_pred_f) + c*Tversky_Loss(y_true_f, y_pred_f)\n\ndef Dice_coef(y_true, y_pred, smooth = 1):\n\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n \n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef Dice_loss(y_true, y_pred):   \n    return  1.0 - Dice_coef(y_true, y_pred) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = keras.optimizers.Adam(lr = lr, decay = 1e-6)\nunet.compile(loss = Combo_Loss, optimizer = optimizer, metrics = [Dice_coef])\n\nreduce_lr = keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', patience = 2, mode = 'min', factor = 0.5, verbose = 1)\ncp = keras.callbacks.ModelCheckpoint('unet_out.hdf5', monitor = 'val_loss', verbose = 1, save_best_only = True, mode = 'min')\nes = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min')\ntraining_callbacks = [reduce_lr, cp, es]\n\nsteps_per_epoch = len(train_x) // batch_size\nvalidation_steps = len(valid_x) // batch_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nhistory = unet.fit_generator(train_aug_gen, steps_per_epoch = steps_per_epoch, epochs = epochs,\n                              validation_data = valid_aug_gen, validation_steps = validation_steps, verbose = 1, callbacks = training_callbacks)\n\n\nunet.load_weights('unet_out.hdf5')\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ndef plot_training_result(history):\n    \n    plt.figure(figsize = (8,6))\n    plt.plot(history.history['loss'], '-', label = 'train_loss', color = 'g')\n    plt.plot(history.history['val_loss'], '--', label = 'valid_loss', color ='r')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.title('Loss on unet')\n    plt.legend()\n    plt.show()\n    \n    plt.figure(figsize = (8,6))\n    plt.plot(history.history['Dice_coef'], '-', label = 'train_Dice_coef', color = 'g')\n    plt.plot(history.history['val_Dice_coef'], '--', label = 'valid_Dice_coef', color ='r')\n    plt.xlabel('epoch')\n    plt.ylabel('Dice_Coef')\n    plt.title('Dice_Coef on unet')\n    plt.legend()\n    plt.show()\n\nplot_training_result(history)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef predict_masks(img):\n    \n    masks = unet.predict(np.expand_dims(img, axis = 0))\n    masks = masks > mask_bound\n    masks = np.squeeze(masks, axis = 0)\n    \n    return masks\n\nvalid_aug_gen = input_gen(valid_x, train_seg, aug_for_valid, batch_size = 8, reshape = reshape_mask)\n\nvalid_data, valid_label = next(valid_aug_gen)\nfor x,y in zip(valid_data, valid_label):\n    display_img_masks(x,y,title = \"_ground truth\")\n    prediction = predict_masks(x)\n    display_img_masks(x,prediction, title = '_prediction')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nvalid_count = 1600\nvalid_predictions = np.zeros((valid_count,256,1600))\nvalid_truths = []\nvalid_aug_gen = input_gen(valid_x, train_seg, aug_for_valid, batch_size = 1, reshape = reshape_mask)\n\nfor idx in tqdm_notebook(range(int(valid_count/4))):\n    img, true_masks = next(valid_aug_gen)  \n    true_masks = true_masks[0]\n    pred_masks = np.squeeze( unet.predict(img), axis = 0)\n    pred_masks = cv2.resize(pred_masks, (1600, 256), interpolation = cv2.INTER_LINEAR)\n    \n    for i in range(pred_masks.shape[-1]):\n        valid_predictions[idx*4+i, :, :] = pred_masks[:,:,i]\n    \n    for tm_id in range(true_masks.shape[-1]):\n        valid_truths.append(cv2.resize(true_masks[:,:,tm_id], dsize = (1600,256), interpolation = cv2.INTER_LINEAR))\n    \n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef Dice(y_true, y_pred, smooth = 1):\n\n    y_true_f = y_true.flatten()\n    y_pred_f = y_pred.flatten()\n \n    intersection = np.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n\ndef single_mask_reduce(mask, minSize):\n    label_num, label_mask = cv2.connectedComponents(mask.astype(np.uint8))\n    reduced_mask = np.zeros(mask.shape, np.float32)\n    for label in range(1, label_num):\n        single_label_mask = (label_mask == label)\n        if single_label_mask.sum() > minSize:\n            reduced_mask[single_label_mask] = 1\n            \n    return reduced_mask\n\nclass_params = {}\n\nfor class_id in range(4):\n    print(class_id)\n    attemps = []\n    for th in range(0,100,5):\n        th = th / 100\n        d = 0\n        for ms in [3500]:\n            for i in range(class_id, len(valid_predictions), 4):\n                valid_pred = valid_predictions[i]\n                valid_pred = np.array(valid_pred > th, np.uint8)\n                valid_pred = single_mask_reduce(valid_pred, ms)\n                if valid_pred.sum() == 0 and np.sum(valid_truths[i]) == 0:\n                    d += 1\n                else:\n                    d += Dice(valid_pred, valid_truths[i])\n            attemps.append((th, ms, d/(len(valid_predictions)//4) ))\n            \n    attempts_df = pd.DataFrame(attemps, columns = ['threshold', 'size', 'dice'])\n    attempts_df = attempts_df.sort_values('dice', ascending = False)\n    print(attempts_df.head())\n    best_threshold = attempts_df['threshold'].values[0]\n    best_size = attempts_df['size'].values[0]\n    \n    class_params[class_id] = (best_threshold, best_size)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsub = pd.DataFrame(columns = ['ImageId_ClassId', 'EncodedPixels'])\n\ndef mask_reduce(masks, class_params):\n    \n    for idx in range(masks.shape[-1]):\n        masks[:,:,idx] = np.array(masks[:,:,idx] > class_params[idx][0])\n        \n        label_num, labeled_mask = cv2.connectedComponents(masks[:,:,idx].astype(np.uint8))\n        reduced_mask = np.zeros(masks.shape[:2],np.float32)\n        \n        for label in range(1, label_num):\n            single_label_mask = (labeled_mask == label)\n            if single_label_mask.sum() >  class_params[idx][1]:\n                reduced_mask[single_label_mask] = 1\n        \n        masks[:,:,idx] = reduced_mask\n        \n    return masks\n\n\ndef prediction_encoding(fn, img_dir, submission, target_shape = (256,1600)):\n    img = np.asarray(load_target_image(path = os.path.join(img_dir,fn)))\n    img = gamma_correction(img)\n    img = contrast_enhancement(img)/255.0\n    masks = unet.predict(np.expand_dims(img, axis = 0))\n    masks = np.squeeze( np.round(masks), axis = 0)\n    masks = cv2.resize(masks, (target_shape[1], target_shape[0]))\n    masks = mask_reduce(masks, class_params)\n    \n    ImageId_ClassId = np.asarray([ fn+'_'+str(id) for id in range(1,5) ])\n    for idx in range(masks.shape[-1]):\n        submission = submission.append(pd.DataFrame([[ImageId_ClassId[idx], rle_encoding(masks[:,:,idx])]], columns = [\"ImageId_ClassId\", \"EncodedPixels\"]))\n        \n    return submission\n\nload_dir = main_dir + 'test_images/'\n\nfor fn in tqdm_notebook(test_fns):\n    sub = prediction_encoding(fn, load_dir, sub)\n    gc.collect()\n    \nsub.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsub_sample = pd.read_csv(main_dir + 'sample_submission.csv')\nsub_sample = sub_sample.drop(['EncodedPixels'], axis = 1)\n\nsubmission = sub_sample.merge(sub, on = ['ImageId_ClassId'])\nsubmission.to_csv('submission.csv', index = False)\nsubmission.head(10)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}