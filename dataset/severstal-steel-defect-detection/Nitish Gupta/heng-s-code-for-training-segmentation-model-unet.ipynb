{"cells":[{"metadata":{},"cell_type":"markdown","source":"Heng's Starter code for training the segmentation model on Kaggle.\n\nI have not run the kernel with GPU enabled because I do not have much of Kaggle GPU left as of now. So the kernel as expected is giving CUDA error. This is just a simple kernel for training model on Kaggle easily. Made some minor changes to his code and seems like it will run fine here on kaggle. I have not tested the training time. It can exceed the 9 hour limit.\n\nThe kernel is based on Heng's starter kit version 20190910 you can find it [here](https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/106462#latest-645576) . I have imported 2 utility scripts one for the utility functions with code for plotting and another one is for model. You can fork and edit the utility scripts and add the model classes as you feel like. The model architecture can be changed from this kernel below by changing the Net() class.\n\nIf you face any problems or errors then feel free to comment them. At last thank you very much Heng and other leaderboard rankers for helping newbies like me."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nimport random \nfrom timeit import default_timer as timer\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler, Sampler\nimport torch.utils.data as data\nimport torchvision.models as models\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch\n\nfrom fork_of_heng_s_utility_functions import *\nfrom heng_s_models_all import *\n\nPI = np.pi\nIMAGE_RGB_MEAN = [0.485, 0.456, 0.406]\nIMAGE_RGB_STD  = [0.229, 0.224, 0.225]\nDEFECT_COLOR = [(0,0,0),(0,0,255),(0,255,0),(255,0,0),(0,255,255)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SPLIT_DIR = '../input/hengs-split'\nDATA_DIR = '../input/severstal-steel-defect-detection'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FourBalanceClassSampler(Sampler):\n\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n        label = (self.dataset.df['Label'].values)\n        label = label.reshape(-1,4)\n        label = np.hstack([label.sum(1,keepdims=True)==0,label]).T\n\n        self.neg_index  = np.where(label[0])[0]\n        self.pos1_index = np.where(label[1])[0]\n        self.pos2_index = np.where(label[2])[0]\n        self.pos3_index = np.where(label[3])[0]\n        self.pos4_index = np.where(label[4])[0]\n\n        #assume we know neg is majority class\n        num_neg = len(self.neg_index)\n        self.length = 4*num_neg\n\n\n    def __iter__(self):\n        neg = self.neg_index.copy()\n        random.shuffle(neg)\n        num_neg = len(self.neg_index)\n\n        pos1 = np.random.choice(self.pos1_index, num_neg, replace=True)\n        pos2 = np.random.choice(self.pos2_index, num_neg, replace=True)\n        pos3 = np.random.choice(self.pos3_index, num_neg, replace=True)\n        pos4 = np.random.choice(self.pos4_index, num_neg, replace=True)\n\n        l = np.stack([neg,pos1,pos2,pos3,pos4]).T\n        l = l.reshape(-1)\n        return iter(l)\n\n    def __len__(self):\n        return self.length","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# UNet\ndef upsize(x,scale_factor=2):\n    #x = F.interpolate(x, size=e.shape[2:], mode='nearest')\n    x = F.interpolate(x, scale_factor=scale_factor, mode='nearest')\n    return x\n\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\nclass Decode(nn.Module):\n    def __init__(self, in_channel, out_channel):\n        super(Decode, self).__init__()\n\n        self.top = nn.Sequential(\n            nn.Conv2d(in_channel, out_channel//2, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d( out_channel//2),\n            nn.ReLU(inplace=True),\n            #nn.Dropout(0.1),\n\n            nn.Conv2d(out_channel//2, out_channel//2, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(out_channel//2),\n            nn.ReLU(inplace=True),\n            #nn.Dropout(0.1),\n\n            nn.Conv2d(out_channel//2, out_channel, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(out_channel),\n            nn.ReLU(inplace=True), #Swish(), #\n        )\n\n    def forward(self, x):\n        x = self.top(torch.cat(x, 1))\n        return x\n\nclass Net(nn.Module):\n\n    def load_pretrain(self, skip, is_print=True):\n        conversion=copy.copy(CONVERSION)\n        for i in range(0,len(conversion)-8,4):\n            conversion[i] = 'block.' + conversion[i][5:]\n        load_pretrain(self, skip, pretrain_file=PRETRAIN_FILE, conversion=conversion, is_print=is_print)\n\n    def __init__(self, num_class=5, drop_connect_rate=0.2):\n        super(Net, self).__init__()\n\n        e = ResNet18()\n        self.block = nn.ModuleList([\n           e.block0,\n           e.block1,\n           e.block2,\n           e.block3,\n           e.block4\n        ])\n        e = None  #dropped\n\n        self.decode1 =  Decode(512,     128)\n        self.decode2 =  Decode(128+256, 128)\n        self.decode3 =  Decode(128+128, 128)\n        self.decode4 =  Decode(128+ 64, 128)\n        self.decode5 =  Decode(128+ 64, 128)\n        self.logit = nn.Conv2d(128,num_class, kernel_size=1)\n\n    def forward(self, x):\n        batch_size,C,H,W = x.shape\n\n        #----------------------------------\n        backbone = []\n        for i in range( len(self.block)):\n            x = self.block[i](x)\n            #print(i, x.shape)\n\n            if i in [0,1,2,3,4]:\n                backbone.append(x)\n\n        #----------------------------------\n        x = self.decode1([backbone[-1], ])                   #; print('d1',d1.size())\n        x = self.decode2([backbone[-2], upsize(x)])          #; print('d2',d2.size())\n        x = self.decode3([backbone[-3], upsize(x)])          #; print('d3',d3.size())\n        x = self.decode4([backbone[-4], upsize(x)])          #; print('d4',d4.size())\n        x = self.decode5([backbone[-5], upsize(x)])          #; print('d5',d5.size())\n\n        logit = self.logit(x)\n        logit = F.interpolate(logit, size=(H,W), mode='bilinear', align_corners=False)\n        return logit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Class which is used by the infor object in __get_item__\nclass Struct(object):\n    def __init__(self, is_copy=False, **kwargs):\n        self.add(is_copy, **kwargs)\n\n    def add(self, is_copy=False, **kwargs):\n        #self.__dict__.update(kwargs)\n\n        if is_copy == False:\n            for key, value in kwargs.items():\n                setattr(self, key, value)\n        else:\n            for key, value in kwargs.items():\n                try:\n                    setattr(self, key, copy.deepcopy(value))\n                    #setattr(self, key, value.copy())\n                except Exception:\n                    setattr(self, key, value)\n\n    def __str__(self):\n        text =''\n        for k,v in self.__dict__.items():\n            text += '\\t%s : %s\\n'%(k, str(v))\n        return text\n\n# Creating masks\ndef run_length_decode(rle, height=256, width=1600, fill_value=1):\n    mask = np.zeros((height,width), np.float32)\n    if rle != '':\n        mask=mask.reshape(-1)\n        r = [int(r) for r in rle.split(' ')]\n        r = np.array(r).reshape(-1, 2)\n        for start,length in r:\n            start = start-1  #???? 0 or 1 index ???\n            mask[start:(start + length)] = fill_value\n        mask=mask.reshape(width, height).T\n    return mask\n\n# Collations\ndef null_collate(batch):\n#     pdb.set_trace()\n    batch_size = len(batch)\n    input = []\n    truth_mask  = []\n    truth_label = []\n    infor = []\n    for b in range(batch_size):\n        input.append(batch[b][0])\n        #truth_mask.append(batch[b][1])\n        infor.append(batch[b][2])\n\n        mask  = batch[b][1]\n        label = (mask.reshape(4,-1).sum(1)>0).astype(np.int32)\n\n        num_class,H,W = mask.shape\n        mask = mask.transpose(1,2,0)*[1,2,3,4]\n        mask = mask.reshape(-1,4)\n        mask = mask.max(-1).reshape(1,H,W)\n\n        truth_mask.append(mask)\n        truth_label.append(label)\n\n    \n    input = np.stack(input)\n    input = image_to_input(input, IMAGE_RGB_MEAN,IMAGE_RGB_STD)\n    input = torch.from_numpy(input).float()\n\n    truth_mask = np.stack(truth_mask)\n    truth_mask = torch.from_numpy(truth_mask).long()\n\n    truth_label = np.array(truth_label)\n    truth_label = torch.from_numpy(truth_label).float()\n\n    return input, truth_mask, truth_label, infor\n\n# Metric\ndef metric_dice(logit, truth, threshold=0.1, sum_threshold=1):\n\n    with torch.no_grad():\n        probability = torch.softmax(logit,1)\n        probability = one_hot_encode_predict(probability)\n        truth = one_hot_encode_truth(truth)\n\n        batch_size,num_class, H,W = truth.shape\n        probability = probability.view(batch_size,num_class,-1)\n        truth = truth.view(batch_size,num_class,-1)\n        p = (probability>threshold).float()\n        t = (truth>0.5).float()\n\n        t_sum = t.sum(-1)\n        p_sum = p.sum(-1)\n\n        d_neg = (p_sum < sum_threshold).float()\n        d_pos = 2*(p*t).sum(-1)/((p+t).sum(-1)+1e-12)\n\n        neg_index = (t_sum==0).float()\n        pos_index = 1-neg_index\n\n        num_neg = neg_index.sum()\n        num_pos = pos_index.sum(0)\n        dn = (neg_index*d_neg).sum()/(num_neg+1e-12)\n        dp = (pos_index*d_pos).sum(0)/(num_pos+1e-12)\n\n        #----\n\n        dn = dn.item()\n        dp = list(dp.data.cpu().numpy())\n        num_neg = num_neg.item()\n        num_pos = list(num_pos.data.cpu().numpy())\n\n    return dn,dp, num_neg,num_pos\n\ndef metric_hit(logit, truth, threshold=0.5):\n    batch_size,num_class, H,W = logit.shape\n\n    with torch.no_grad():\n        logit = logit.view(batch_size,num_class,-1)\n        truth = truth.view(batch_size,-1)\n\n        probability = torch.softmax(logit,1)\n        p = torch.max(probability, 1)[1]\n        t = truth\n        correct = (p==t)\n\n        index0 = t==0\n        index1 = t==1\n        index2 = t==2\n        index3 = t==3\n        index4 = t==4\n\n        num_neg  = index0.sum().item()\n        num_pos1 = index1.sum().item()\n        num_pos2 = index2.sum().item()\n        num_pos3 = index3.sum().item()\n        num_pos4 = index4.sum().item()\n\n        neg  = correct[index0].sum().item()/(num_neg +1e-12)\n        pos1 = correct[index1].sum().item()/(num_pos1+1e-12)\n        pos2 = correct[index2].sum().item()/(num_pos2+1e-12)\n        pos3 = correct[index3].sum().item()/(num_pos3+1e-12)\n        pos4 = correct[index4].sum().item()/(num_pos4+1e-12)\n\n        num_pos = [num_pos1,num_pos2,num_pos3,num_pos4,]\n        tn = neg\n        tp = [pos1,pos2,pos3,pos4,]\n\n    return tn,tp, num_neg,num_pos\n\n# Loss\ndef criterion(logit, truth, weight=None):\n    logit = logit.permute(0, 2, 3, 1).contiguous().view(-1, 5)\n    truth = truth.permute(0, 2, 3, 1).contiguous().view(-1)\n\n    if weight is not None: weight = torch.FloatTensor([1]+weight).cuda()\n    loss = F.cross_entropy(logit, truth, weight=weight, reduction='none')\n\n    loss = loss.mean()\n    return loss\n\n#One-Hot for segmentation\ndef one_hot_encode_truth(truth, num_class=4):\n    one_hot = truth.repeat(1,num_class,1,1)\n    arange  = torch.arange(1,num_class+1).view(1,num_class,1,1).to(truth.device)\n    one_hot = (one_hot == arange).float()\n    return one_hot\n\ndef one_hot_encode_predict(predict, num_class=4):\n    value, index = torch.max(predict, 1, keepdim=True)\n    value  = value.repeat(1,num_class,1,1)\n    index  = index.repeat(1,num_class,1,1)\n    arange = torch.arange(1,num_class+1).view(1,num_class,1,1).to(predict.device)\n    one_hot = (index == arange).float()\n    value = value*one_hot\n    return value\n\n# Learning Rate Adjustments\ndef adjust_learning_rate(optimizer, lr):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\ndef get_learning_rate(optimizer):\n    lr=[]\n    for param_group in optimizer.param_groups:\n       lr +=[ param_group['lr'] ]\n\n    assert(len(lr)==1) #we support only one param_group\n    lr = lr[0]\n    return lr\n\n# Learning Rate Schedule\nclass NullScheduler():\n    def __init__(self, lr=0.01 ):\n        super(NullScheduler, self).__init__()\n        self.lr    = lr\n        self.cycle = 0\n\n    def __call__(self, time):\n        return self.lr\n\n    def __str__(self):\n        string = 'NullScheduler\\n' \\\n                + 'lr=%0.5f '%(self.lr)\n        return string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import pdb; \n\n# def abc():\n\n#     next(iter(train_loader))\n# abc()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"schduler = NullScheduler(lr=0.001)\nbatch_size = 4 #8\niter_accum = 8\nloss_weight = None#[5,5,2,5] #\ntrain_sampler = FourBalanceClassSampler #RandomSampler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SteelDataset(Dataset):\n    def __init__(self, split, csv, mode, augment=None):\n#         import pdb; pdb.set_trace()\n        self.split   = split\n        self.csv     = csv\n        self.mode    = mode\n        self.augment = augment\n\n        self.uid = list(np.concatenate([np.load(SPLIT_DIR + '/%s'%f , allow_pickle=True) for f in split]))\n        df = pd.concat([pd.read_csv(DATA_DIR + '/%s'%f) for f in csv])\n        df.fillna('', inplace=True)\n        df['Class'] = df['ImageId_ClassId'].str[-1].astype(np.int32)\n        df['Label'] = (df['EncodedPixels']!='').astype(np.int32)\n        df = df_loc_by_list(df, 'ImageId_ClassId', [ u.split('/')[-1] + '_%d'%c  for u in self.uid for c in [1,2,3,4] ])\n        self.df = df\n\n    def __str__(self):\n        num1 = (self.df['Class']==1).sum()\n        num2 = (self.df['Class']==2).sum()\n        num3 = (self.df['Class']==3).sum()\n        num4 = (self.df['Class']==4).sum()\n        pos1 = ((self.df['Class']==1) & (self.df['Label']==1)).sum()\n        pos2 = ((self.df['Class']==2) & (self.df['Label']==1)).sum()\n        pos3 = ((self.df['Class']==3) & (self.df['Label']==1)).sum()\n        pos4 = ((self.df['Class']==4) & (self.df['Label']==1)).sum()\n\n        length = len(self)\n        num = len(self)*4\n        pos = (self.df['Label']==1).sum()\n        neg = num-pos\n\n        #---\n\n        string  = ''\n        string += '\\tmode    = %s\\n'%self.mode\n        string += '\\tsplit   = %s\\n'%self.split\n        string += '\\tcsv     = %s\\n'%str(self.csv)\n        string += '\\t\\tlen   = %5d\\n'%len(self)\n        if self.mode == 'train':\n            string += '\\t\\tnum   = %5d\\n'%num\n            string += '\\t\\tneg   = %5d  %0.3f\\n'%(neg,neg/num)\n            string += '\\t\\tpos   = %5d  %0.3f\\n'%(pos,pos/num)\n            string += '\\t\\tpos1  = %5d  %0.3f  %0.3f\\n'%(pos1,pos1/length,pos1/pos)\n            string += '\\t\\tpos2  = %5d  %0.3f  %0.3f\\n'%(pos2,pos2/length,pos2/pos)\n            string += '\\t\\tpos3  = %5d  %0.3f  %0.3f\\n'%(pos3,pos3/length,pos3/pos)\n            string += '\\t\\tpos4  = %5d  %0.3f  %0.3f\\n'%(pos4,pos4/length,pos4/pos)\n        return string\n\n\n    def __len__(self):\n        return len(self.uid)\n\n\n    def __getitem__(self, index):\n        # print(index)\n        folder, image_id = self.uid[index].split('/')\n\n        rle = [\n            self.df.loc[self.df['ImageId_ClassId']==image_id + '_1','EncodedPixels'].values[0],\n            self.df.loc[self.df['ImageId_ClassId']==image_id + '_2','EncodedPixels'].values[0],\n            self.df.loc[self.df['ImageId_ClassId']==image_id + '_3','EncodedPixels'].values[0],\n            self.df.loc[self.df['ImageId_ClassId']==image_id + '_4','EncodedPixels'].values[0],\n        ]\n        image = cv2.imread(DATA_DIR + '/%s/%s'%(folder,image_id), cv2.IMREAD_COLOR)\n        mask  = np.array([run_length_decode(r, height=256, width=1600, fill_value=1) for r in rle])\n\n        infor = Struct(\n            index    = index,\n            folder   = folder,\n            image_id = image_id,\n        )\n\n        if self.augment is None:\n            return image, mask, infor\n        else:\n            return self.augment(image, mask, infor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def do_valid(net, valid_loader, displays=None):\n    valid_num  = np.zeros(11, np.float32)\n    valid_loss = np.zeros(11, np.float32)\n    \n    for t, (input, truth_mask, truth_label, infor) in enumerate(valid_loader):\n\n        #if b==5: break\n        net.eval()\n        input = input.cuda()\n        truth_mask  = truth_mask.cuda()\n        truth_label = truth_label.cuda()\n\n        with torch.no_grad():\n            logit = net(input) #data_parallel(net, input)\n            loss  = criterion(logit, truth_mask)\n            tn,tp, num_neg,num_pos = metric_hit(logit, truth_mask)\n            dn,dp, num_neg,num_pos = metric_dice(logit, truth_mask, threshold=0.5, sum_threshold=100)\n            \n            #zz=0\n        #---\n        batch_size = len(infor)\n        l = np.array([ loss.item(), tn,*tp, dn,*dp ])\n        n = np.array([ batch_size, num_neg,*num_pos, num_neg,*num_pos ])\n        valid_loss += l*n\n        valid_num  += n\n\n        #debug-----------------------------\n        if displays is not None:\n            probability = torch.sigmoid(logit)\n            image = input_to_image(input, IMAGE_RGB_MEAN,IMAGE_RGB_STD)\n\n            probability = one_hot_encode_predict(probability)\n            truth_mask  = one_hot_encode_truth(truth_mask)\n            \n            probability_mask = probability.data.cpu().numpy()\n            truth_label = truth_label.data.cpu().numpy()\n            truth_mask  = truth_mask.data.cpu().numpy()\n\n            for b in range(0, batch_size, 4):\n                image_id = infor[b].image_id[:-4]\n                result = draw_predict_result_label(image[b], truth_mask[b], truth_label[b], probability_mask[b], stack='vertical')\n                draw_shadow_text(result,'%05d    %s.jpg'%(valid_num[0]-batch_size+b, image_id),(5,24),1,[255,255,255],2)\n                image_show('result',result,resize=1)\n#                 cv2.imwrite(out_dir +'/valid/%s.png'%(image_id), result)\n#                 cv2.waitKey(1)\n                pass\n        #debug-----------------------------\n\n        #print(valid_loss)\n        print('\\r %8d /%8d'%(valid_num[0], len(valid_loader.dataset)),end='',flush=True)\n\n        pass  #-- end of one data loader --\n    assert(valid_num[0] == len(valid_loader.dataset))\n    valid_loss = valid_loss/valid_num\n\n    return valid_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_train():\n    batch_size = 4\n\n    initial_checkpoint = \\\n    '/root/share/project/kaggle/2019/steel/result1/resnet34-cls-full-foldb0-0/checkpoint/00007500_model.pth'\n    \n    train_dataset = SteelDataset(\n        mode    = 'train',\n        csv     = ['train.csv',],\n        split   = ['train_b1_11568.npy',],\n        augment = train_augment,\n    )\n    train_loader  = DataLoader(\n        train_dataset,\n        #sampler     = BalanceClassSampler(train_dataset, 3*len(train_dataset)),\n        #sampler    = SequentialSampler(train_dataset),\n        sampler    = train_sampler(train_dataset),\n        batch_size  = batch_size,\n        drop_last   = True,\n        num_workers = 2,\n        pin_memory  = True,\n        collate_fn  = null_collate\n    )\n\n    valid_dataset = SteelDataset(\n        mode    = 'train',\n        csv     = ['train.csv',],\n        split   = ['valid_b1_1000.npy',],\n        augment = valid_augment,\n    )\n    valid_loader = DataLoader(\n        valid_dataset,\n        sampler    = SequentialSampler(valid_dataset),\n        #sampler     = RandomSampler(valid_dataset),\n        batch_size  = 4,\n        drop_last   = False,\n        num_workers = 2,\n        pin_memory  = True,\n        collate_fn  = null_collate\n    )\n    \n    assert(len(train_dataset)>=batch_size)\n    \n    net = Net().cuda()\n    \n    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=schduler(0), momentum=0.9, weight_decay=0.0001)\n\n    num_iters   = 3000*1000\n    iter_smooth = 50\n    iter_log    = 500\n    iter_valid  = 1500\n    iter_save   = [0, num_iters-1]\\\n                   + list(range(0, num_iters, 1500))#1*1000\n\n    start_iter = 0\n    start_epoch= 0\n    rate       = 0\n    if initial_checkpoint is not None:\n        initial_optimizer = initial_checkpoint.replace('_model.pth','_optimizer.pth')\n        if os.path.exists(initial_optimizer):\n            checkpoint  = torch.load(initial_optimizer)\n            start_iter  = checkpoint['iter' ]\n            start_epoch = checkpoint['epoch']\n            #optimizer.load_state_dict(checkpoint['optimizer'])\n        pass\n    \n    train_loss = np.zeros(20,np.float32)\n    valid_loss = np.zeros(20,np.float32)\n    batch_loss = np.zeros(20,np.float32)\n    iter = 0\n    i    = 0\n    \n    start = timer()\n    \n    while  iter<num_iters:\n        sum_train_loss = np.zeros(20,np.float32)\n        sum = np.zeros(20,np.float32)\n\n        optimizer.zero_grad()\n#         import pdb; pdb.set_trace()\n        for t, (input, truth_mask, truth_label, infor) in enumerate(train_loader):\n            batch_size = len(infor)\n            iter  = i + start_iter\n            epoch = (iter-start_iter)*batch_size/len(train_dataset) + start_epoch\n            \n            # Weather to display images or not! While in validation loss\n            displays = None\n            #if 0:\n            if (iter % iter_valid==0):\n                valid_loss = do_valid(net, valid_loader, displays) # omitted outdir variable\n                #pass\n\n            if (iter % iter_log==0):\n                print('\\r',end='',flush=True)\n                asterisk = '*' if iter in iter_save else ' '\n                print('%0.5f  %5.1f%s %5.1f |  %5.3f   %4.2f [%4.2f,%4.2f,%4.2f,%4.2f]   %4.2f [%4.2f,%4.2f,%4.2f,%4.2f]  |  %5.3f   %4.2f [%4.2f,%4.2f,%4.2f,%4.2f]  | %s' % (\\\n                         rate, iter/1000, asterisk, epoch,\n                         *valid_loss[:11],\n                         *train_loss[:6],\n                         time_to_str((timer() - start),'min'))\n                )\n                print('\\n')\n                \n            #if 0:\n            if iter in iter_save:\n                torch.save(net.state_dict(),'../working/%08d_model.pth'%(iter))\n                torch.save({\n                    #'optimizer': optimizer.state_dict(),\n                    'iter'     : iter,\n                    'epoch'    : epoch,\n                }, '../working/%08d_optimizer.pth'%(iter))\n                pass\n\n            # learning rate schduler -------------\n            lr = schduler(iter)\n            if lr<0 : break\n            adjust_learning_rate(optimizer, lr)\n            rate = get_learning_rate(optimizer)\n            \n            net.train()\n            input = input.cuda()\n            truth_label = truth_label.cuda()\n            truth_mask  = truth_mask.cuda()\n\n            logit =  net(input) #data_parallel(net,input)  \n            loss = criterion(logit, truth_mask, loss_weight)\n            tn,tp, num_neg,num_pos = metric_hit(logit, truth_mask)\n            \n            (loss/iter_accum).backward()\n            if (iter % iter_accum)==0:\n                optimizer.step()\n                optimizer.zero_grad()\n\n            # print statistics  ------------\n            l = np.array([ loss.item(), tn,*tp ])\n            n = np.array([ batch_size, num_neg,*num_pos ])\n\n            batch_loss[:6] = l\n            sum_train_loss[:6] += l*n\n            sum[:6] += n\n            if iter%iter_smooth == 0:\n                train_loss = sum_train_loss/(sum+1e-12)\n                sum_train_loss[...] = 0\n                sum[...]            = 0\n\n\n            print('\\r',end='',flush=True)\n            asterisk = ' '\n            print('%0.5f  %5.1f%s %5.1f |  %5.3f   %4.2f [%4.2f,%4.2f,%4.2f,%4.2f]   %4.2f [%4.2f,%4.2f,%4.2f,%4.2f]  |  %5.3f   %4.2f [%4.2f,%4.2f,%4.2f,%4.2f]  | %s' % (\\\n                     rate, iter/1000, asterisk, epoch,\n                     *valid_loss[:11],\n                     *train_loss[:6],\n                     time_to_str((timer() - start),'min'))\n            )\n            print('\\n')\n            i=i+1\n            \n            # debug-----------------------------\n            if 1:\n                for di in range(3):\n                    if (iter+di)%1000==0:\n\n                        probability = torch.softmax(logit,1)\n                        image = input_to_image(input, IMAGE_RGB_MEAN,IMAGE_RGB_STD)\n                        \n                        probability = one_hot_encode_predict(probability)\n                        truth_mask  = one_hot_encode_truth(truth_mask)\n                        \n                        probability_mask = probability.data.cpu().numpy()\n                        truth_label = truth_label.data.cpu().numpy()\n                        truth_mask  = truth_mask.data.cpu().numpy()\n\n\n                        for b in range(batch_size):\n                    \n                            result = draw_predict_result_label(image[b], truth_mask[b], truth_label[b], probability_mask[b], stack='vertical')\n\n                            image_show('result',result,resize=1)\n#                             cv2.imwrite('../working/%05d.png'%(di*100+b), result)\n#                             cv2.waitKey(1)\n                            pass\n        pass  #-- end of one data loader --\n    pass #-- end of all iterations --","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('                      |-------------------------------- VALID-----------------------------|---------- TRAIN/BATCH ------------------------------\\n')\nprint('rate     iter   epoch |  loss    hit_neg,pos1,2,3,4           dice_neg,pos1,2,3,4         |  loss    hit_neg,pos1,2,3,4          | time         \\n')\nprint('------------------------------------------------------------------------------------------------------------------------------------------------\\n')\n          #0.00000    0.0*   0.0 |  0.690   0.50 [0.00,1.00,0.00,1.00]   0.44 [0.00,0.02,0.00,0.15]  |  0.000   0.00 [0.00,0.00,0.00,0.00]  |  0 hr 00 min\nrun_train()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}