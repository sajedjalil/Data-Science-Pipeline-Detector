{"cells":[{"metadata":{},"cell_type":"markdown","source":"The problem in this competition is to get a prediction on defects (masks) of 256x1600 top snapshot of steel surface images.\nDefects are of 4 class and as per my understading can be classfied roughly as cracking of steel\n1.  Multiple chips on the surface \n2.  Single Vertical crack\n3.  Multiple vertical cracks\n4.  Muliptle large surface patches\n\nMask is provided as a RLE (Encoded String)\n\nFormat is like below\n\n1234 3 1239 16\n\nThis means flatten image mask exists at index [1234:1234+3] [1239:1239+16]etc.\n\nNeed to decode the RLE to get the mask on the image\n\nThe notebook here trains array of 4 models, all custom unet \n(based on different loss function and atrous convs of different sized)\nEach of the model is shown a certain typeof defect and not shown all other\nFinal prediction is a combination of the prediction of each of the above model\n- > Note the submission is not complete yet.\n"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# imports\nimport os \nimport numpy as np\nimport pandas as pd \nfrom mpl_toolkits.mplot3d import axes3d\nfrom tqdm import tqdm_notebook \nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.ticker import LinearLocator, FormatStrFormatter\nimport matplotlib.pyplot as plt\nimport cv2\nimport time\nimport seaborn as sns\nimport datetime\n\ndef timin():\n    ts = time.time()\n    st = str(datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))\n    print (st.replace(\" \",\"-\"))\n\ntimin()\n# These are each binary classifiers for each mask\nlabels = [\"1\",\"2\",\"3\",\"4\"]\nlabelsInt = [1,2,3,4]\ntrainImgPath = \"/kaggle/input/severstal-steel-defect-detection/train_images/\"\ntrainCsv = \"/kaggle/input/severstal-steel-defect-detection/train.csv\"\ndfFull = pd.read_csv(trainCsv)\ndf1 = dfFull[~dfFull['EncodedPixels'].isnull()]# get only image with labeled data for defects\ndf1['ImageId'] = df1['ImageId_ClassId'].apply(lambda s:s.split(\"_\")[0])\ndf1['Labels'] =  df1['ImageId_ClassId'].apply(lambda s:int(s.split(\"_\")[1]))\ndf1.drop(columns=\"ImageId_ClassId\", inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nfrom skimage.io import imread\nfrom scipy.ndimage.filters import convolve\nimport numpy as np \n# Conv kernels\nembossLower = np.array([  [0, 0, 0],   [0, 0, 1],  [0, 1, 1]])\nembossUpper = np.array([ [1, 1, 0],   [1, 0, 0],  [0, 0, 0]])\nemboss = np.array([ [-2, -1, 0],   [-1, 1, 1],  [0, 1, 2]])\nedge = np.array([    [-1,-1,-1],  [-1,4, -1], [-1, -1, -1]])\nhorizontal  = np.array([[-1,-2,-1],  [0, 0, 0],  [1, 2, 1]])\nvertical = np.array([[-1,0,1],    [-2, 0, 2], [-1, 0, 1]])\nsharp = np.array([[-1/9,-1/9,-1/9],   [-1/9, 1, -1/9],    [-1/9,-1/9, -1/9]])\nedgex = np.array([[1,1,1],   [1, -7, 1],  [1,1, 1]])\n\nclass PreProcessor:\n\n    kernels = None\n    iterations = None\n\n    \n    def __init__(self,kernel, iteration):\n        self.kernels= kernel\n        self.iterations = iteration\n        \n    \n    def conv(self,image):\n        if (image.shape[2]==3):\n            print(\"e\")\n            return self.convc(image,3)\n        else:\n            return self.convc(image,1)\n            \n    \n    def convc(self, image, channels):\n        for k,itr in zip(self.kernels, self.iterations):\n            for i in range(0,itr):\n                for c in range(0,channels):\n                    image[:,:,c]  = convolve(image[:,:,c], k)\n        return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2 \nimport numpy as np \nimport os\nimport random\nk = os.listdir(trainImgPath)\nprint( k[0])\nfrom skimage.io import imread\ndef drawContours(image):\n    PreProcessor([emboss],[1,1]).conv(image)*1.02\n    edged = cv2.Canny(image, 120, 240) \n    return image, edged\nplt.figure(figsize=(35,10))\np, y = drawContours(cv2.imread( trainImgPath+k[random.randint(0,90)]))\nplt.imshow(p, cmap = 'Greys', interpolation = 'bicubic')\nplt.figure(figsize=(35,10))\nplt.imshow(y, cmap = 'Greys', interpolation = 'bicubic')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\nimport tensorflow as tf\nimport random\nimport threading\nimport os\n\ndef getMaskFromString(listEncodedString, listLabels):\n    mask = np.zeros((256, 1600, 4), dtype=np.int8)\n    for encodedString,labels in zip (listEncodedString, listLabels):\n        if len(str(encodedString))==0:\n            mask[:,:,labels-1] =  np.zeros((256, 1600), dtype=np.int16)\n        else:\n            encodedString = str(encodedString).split(\" \")\n            flatmask = np.zeros(1600*256, dtype=np.int8)\n            for i in range(0,len(encodedString)//2):\n                start = int(encodedString[2*i])\n                end = start + int(encodedString[2*i+1])\n                flatmask[start:end-1] =  1\n            mask[:,:,labels-1] = np.transpose(flatmask.reshape(1600,256))\n    return mask\n#PreProcessor([emboss,sharp],[1,1]).conv(cv2.resize(cv2.imread(trainImgPath+img),(800,128)))*1.05\n\nclass SafePandasDG:\n    \n    \"\"\"This is a utility data generator that utilizes pandas, opencv\"\"\"\n    \n    datapath =  \"/kaggle/input/severstal-steel-defect-detection/train_images/\"\n    testpath =  \"/kaggle/input/severstal-steel-defect-detection/test_images/\"\n    df = None\n    dfo = None\n    split = 1.7\n    label = None\n    index = 0\n    batch = 0\n    valid = False\n    maskColumn =  \"EncodedPixels\"   \n    dataColumn = 'ImageId'\n    labelColumn = 'Labels'\n    aggDict = {labelColumn:list, maskColumn:list}\n    getMask =  lambda self, x: getMaskFromString(x[self.maskColumn], x[self.labelColumn])\n    getImage = lambda self, x: cv2.resize(cv2.imread(self.datapath+x),(800,128))\n    test = False\n    pollute = False\n    batchNum = 0\n    alllabels = [1,2,3,4]\n\n    def __init__(self, df, label=None, batch=24, valid=False, test=False):\n        \n        \"\"\"Constructor agrumnents are dataframe, label for which data, if validation or test data \"\"\"\n        \n        self.dfo = df\n        self.label = label \n        self.batch = batch\n        self.valid = valid\n        self.test = test\n        self.lock = threading.Lock()\n\n    \n    def __iter__(self):\n        return self\n\n    \n    def __next__(self):\n        with self.lock:\n            if self.test == False:\n                return self.getRandomBatch().__next__()\n            else:\n                return self.getRandomTestBatch().__next__()\n    \n    \n    def getSliceMask(self, dfAgg):\n        if self.pollute == True:\n            return np.zeros((dfAgg.shape[0],256, 1600, 4), dtype=np.int8)\n        dfAgg[self.maskColumn] = dfAgg.apply(self.getMask,axis=1)\n        mask = dfAgg[self.maskColumn].tolist()\n        mask = np.array(mask).reshape(dfAgg.shape[0],256,1600,4)\n        return mask \n    \n    \n    \n    def setRandomIndex(self):\n        if self.valid:\n            self.index = int( random.randint(self.df.shape[0] // self.split, self.df.shape[0] - self.batch))\n        else:\n            self.index = int( random.randint(0, self.df.shape[0] // self.split))\n        \n    \n    def getDataByIndex(self):\n        dfSlice = self.df.iloc[self.index: self.index + self.batch].copy()\n        dfAgg = dfSlice.groupby([self.dataColumn]).agg(self.aggDict).reset_index()\n        dfAgg = dfAgg.head(self.batch)\n        data = dfAgg[self.dataColumn].apply(self.getImage)\n        data = np.array(data.tolist(), dtype=np.int16)\n        mask = self.getSliceMask(dfAgg)\n        if self.label is  None:\n            return data, mask\n        else:\n            return data, mask[:,:,:,self.label-1].reshape(dfAgg.shape[0],256,1600,1)\n    \n    \n    def getRandomBatch(self):\n        while True:\n            self.batchNum = self.batchNum + 1\n            if self.batchNum%3==0:\n                self.pollute = True\n            else:\n                self.pollute = False\n            if self.label is not None and self.pollute == False:\n                self.df = self.dfo[ self.dfo[ self.labelColumn ] == self.label ]\n            if self.label is not None and self.pollute == True:\n                plabel  = [it for it in self.alllabels if it != self.label] \n                plabel  = plabel[random.randint(0,2)]\n                self.df = self.dfo[ self.dfo[ self.labelColumn]  == plabel ]\n            self.setRandomIndex()\n            data, labels = self.getDataByIndex()\n            yield  data, labels\n            \n    \n    def getRandomTestBatch(self):\n        testImages = os.listdir(self.testpath)\n        self.index = int( random.randint(0,len(testImages)))\n        while True:\n            data = []\n            for ind in range(self.index, self.index + self.batch):\n                data.append(cv2.resize(cv2.imread(self.testpath+testImages[ind]),(800,128)))\n            yield np.array(data).reshape(self.batch, 128, 800, 3), None\n            \n  \nfor label in range(1,5):\n    x, y = next(SafePandasDG(df1, label, 1, False, False))   \n    plt.figure(figsize=(35,10))\n    plt.imshow(x[0], cmap = 'Greys')\n    plt.figure(figsize=(35,10))\n    plt.imshow(y[0,:,:,0]*255, cmap = 'Greys', interpolation = 'bicubic')\n\n    \nx, y = next(SafePandasDG(df1, None, 1, False, True))   \nplt.figure(figsize=(35,10))\nplt.imshow(x[0], cmap = 'Greys')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.losses import binary_crossentropy\nimport tensorflow as tf\nfrom keras import backend as K\n\ndef dice_coef(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2*intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef bce_dice_loss(y_true, y_pred):\n    return 2 * binary_crossentropy(y_true, y_pred)\n\ndef diff(y_true, y_pred):\n    y_true_f = np.array(y_true).flatten()\n    y_pred_f = np.array(y_pred).flatten()\n    return np.abs(np.sum(  np.subtract(y_true_f,y_pred_f)))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Activation, Dropout, Flatten, Dense, GlobalAveragePooling2D, Conv2D, Conv2DTranspose, LeakyReLU, UpSampling2D, concatenate, DepthwiseConv2D\nfrom keras import optimizers\nfrom keras.layers.normalization import BatchNormalization \nfrom keras.regularizers import l2\nfrom keras.layers import Lambda, Reshape, Add, AveragePooling2D, MaxPooling2D, Concatenate, SeparableConv2D, ELU\nfrom keras.initializers import RandomUniform\n\ndef conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True, dilation_rate=(1,1)):\n    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n    # first layer\n    x = BatchNormalization()(input_tensor)\n    x = Conv2DTranspose(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n              kernel_initializer ='he_normal', padding = 'same', dilation_rate=dilation_rate, kernel_regularizer=l2(0.00003))(x)\n    if batchnorm:\n        x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    # second layer\n    x = BatchNormalization()(input_tensor)\n    x = Conv2D(filters = n_filters, kernel_size = ( kernel_size, kernel_size),\\\n              kernel_initializer =  'he_normal', dilation_rate=dilation_rate, kernel_regularizer=l2(0.00003), padding = 'same')(x)\n    if batchnorm:\n        x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    return x\n\n\ndef get_unet(input_img, n_filters = 32, dropout = 0.1, batchnorm = True, dilation_rate=(1,1)):\n    # Contracting Path\n    c1 = conv2d_block(input_img, n_filters * 1, kernel_size =3, batchnorm = batchnorm, dilation_rate=dilation_rate)\n    p1 = AveragePooling2D((2, 2))(c1)\n    p1 = Dropout(dropout)(p1)\n    \n    c2 = conv2d_block(p1, n_filters * 2, kernel_size = 2, batchnorm = batchnorm)\n    p2 = AveragePooling2D((2, 2))(c2)\n    p2 = Dropout(dropout)(p2)\n    \n    c3 = conv2d_block(p2, n_filters * 4, kernel_size = 1, batchnorm = batchnorm)\n    p3 = MaxPooling2D((2, 2))(c3)\n    p3 = Dropout(dropout)(p3)\n    \n\n    c5 = conv2d_block(p3, n_filters = n_filters * 8, kernel_size = 1, batchnorm = batchnorm)\n\n    # Expansive Path\n   \n    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c5)\n    u7 = concatenate([u7, c3])\n    u7 = Dropout(dropout)(u7)\n    c7 = conv2d_block(u7, n_filters * 4, kernel_size = 1, batchnorm = batchnorm)\n    \n    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n    u8 = concatenate([u8, c2])\n    u8 = Dropout(dropout)(u8)\n    c8 = conv2d_block(u8, n_filters * 2, kernel_size = 2, batchnorm = batchnorm)\n    \n    u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n    u9 = concatenate([u9, c1])\n    u9 = Dropout(dropout)(u9)\n    c9 = conv2d_block(u9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n    c10 = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n    outputs = UpSampling2D()(c10)\n\n    model = Model(inputs=[input_img], outputs=[outputs])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model\nfrom keras.layers import Dense, Activation, Conv2D, Input\nfrom keras.layers.normalization import BatchNormalization as BN\nfrom keras.optimizers import Adam\nfrom keras import layers\nimport tensorflow as tf\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nimport os\nimport datetime\nfrom tensorflow.keras.metrics import TruePositives, TrueNegatives\nimport time\nimport gc\ngc.collect()\nfrom tensorflow import set_random_seed\nset_random_seed(2)\nepoches = 50\ndilation_rate=(1,1)\nlearning_rate = 0.0007\nsep = 36\nvs = 24\n\nadams = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.000005, amsgrad=False)\n\nif (True):\n    timin()\n    model1 = load_model('../input/oldmodel1/model1111.h5',custom_objects={'diff': diff,'dice_coef':dice_coef} )\n    timin()\n    model2 = load_model('../input/oldmodel1/model2111.h5',custom_objects={'diff': diff,'dice_coef':dice_coef} )\n    timin()\n    model3 = load_model('../input/oldmodel1/model3111.h5',custom_objects={'diff': diff,'dice_coef':dice_coef} )\n    timin()\n    model4 = load_model('../input/oldmodel1/model4111.h5',custom_objects={'diff': diff,'dice_coef':dice_coef} )    \n    timin()\n    \n\n\nif(False):\n    model1 = get_unet(Input(shape=(128, 800, 3),dtype='float32'), dilation_rate=(2,2))\n    model1.compile(optimizer=adams, \n                   loss='binary_crossentropy', \n                   metrics=[diff,'binary_accuracy',dice_coef])\n\n\n    model2 = get_unet(Input(shape=(128, 800, 3)),dilation_rate=(2,2))\n    model2.compile(optimizer=adams, \n                   loss='binary_crossentropy', \n                   metrics=[diff,'binary_accuracy',dice_coef])\n    \n    model3 = get_unet(Input(shape=(128, 800, 3)),  dilation_rate=(4,4))\n    model3.compile(optimizer=adams, \n                   loss='binary_crossentropy', \n                   metrics=[diff,'binary_accuracy',dice_coef])\n\n    model4 = get_unet(Input(shape=(128, 800, 3)), dilation_rate=(5,5))\n    model4.compile(optimizer=adams, \n                   loss='binary_crossentropy', \n                   metrics=[diff,'binary_accuracy',dice_coef])\n\nmodels = [model1,model2,model3,model4]\n    \ngetBatch =   [ SafePandasDG(df1, i, 24, False) for i in range(1,5) ]\nvalidBatch = [ SafePandasDG(df1, i, 24, True) for i in range(1,5) ] \n\nif(False):\n    timin()\n    history = [models[i-1].fit_generator(getBatch[i-1],\n                                        steps_per_epoch=sep, \n                                        epochs=epoches*1, \n                                        verbose=1, callbacks=None, \n                                        validation_data=validBatch[i-1],\n                                        validation_steps=vs,  \n                                        workers=1) for i in range (1,5)]\n    timin()\n\n\nif (False):\n    model1.save(\"model1111.h5\")\n    model2.save(\"model2111.h5\")\n    model3.save(\"model3111.h5\")\n    model4.save(\"model4111.h5\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nimport cv2\nimport numpy as np\nimport urllib.request\nimport os\nsubmission =  pd.read_csv(\"/kaggle/input/severstal-steel-defect-detection/sample_submission.csv\")\ntestImgPath = \"/kaggle/input/severstal-steel-defect-detection/test_images/\"\nk = os.listdir(testImgPath)\nsubmission[\"EncodedPixels\"] = \"\"\n\ni =0 \n\ndef mask2rle(img):\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef getResults(x, image,models):\n    inputImg = np.array(cv2.resize(cv2.imread(testImgPath+image),(800,128))).reshape(128, 800, 3).reshape(1, 128, 800, 3)\n    prediction = models[x-1].predict(inputImg,batch_size=None, verbose=0, steps=None)\n    if x==1:\n       return  np.where(cv2.blur(cv2.blur(prediction[0,:,:,0],(5,5)),(5,5))>0.995,1,0)\n    elif x==2:\n       return  np.where(cv2.blur(cv2.blur(prediction[0,:,:,0],(5,5)),(5,5))>0.995,1,0)\n    elif x==3:\n       return  np.where(cv2.blur(cv2.blur(prediction[0,:,:,0],(5,5)),(5,5))>0.97,1,0)\n    elif x==4:\n       return  np.where(cv2.blur(cv2.blur(prediction[0,:,:,0],(5,5)),(5,5))>0.97,1,0)\n\n\ndef getResult(image):\n    models = [model1,model2,model3,model4]\n    return [getResults( x, image, models) for x in labelsInt]\n    \nif(True):\n    models = [model1,model2,model3,model4]\n    for image in k:\n        results =  getResult(image)\n        i = i +1\n        for l,r in zip(labels, results):\n            if (len(r))>0:\n                submission.loc[submission['ImageId_ClassId']==image+\"_\"+l,[\"EncodedPixels\"]] =  mask2rle(r).strip()\n        if (i%50==0):\n            print(\"done\",i,timin())\n    submission.to_csv(\"submission.csv\",index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if (False):\n    models = [model1,model2,model3,model4]\n    x, y = next(SafePandasDG(df1, None, 10, False, True))   \n    print(x[0].shape)\n    for ii in range(0, x.shape[0] ):\n        plt.figure(figsize=(35,10))\n        plt.imshow(x[ii], cmap = 'Greys', interpolation = 'bicubic')\n        for i in range(0,4):\n            predict = models[i].predict(x,batch_size=10, verbose=0, steps=None)\n            plt.figure(figsize=(35,10))\n            plt.imshow(np.where(cv2.blur(cv2.blur(predict[ii,:,:,0],(5,5)),(5,5))>0.26,1,0), cmap = 'Greys', interpolation = 'bicubic')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clss = [1,2,3,4]\nmodelss = [model1,model2,model3,model4]\nfor m in modelss:\n    for c in clss:\n        print(\"Evaluate for model\",c, m)\n        print(m.evaluate_generator(getBatch[c-1], steps=16,   verbose=0))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}