{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\nThis competition is about identifying steel defects by the means of *semantic segmentation*. \n\nIn this notebook we are going to perform an Exploratory Data Analysis (EDA) and train a ResUnet model in TensorFlow.\n\nLearning goals:\n\n- Understand run length encoding\n- Learn how to create a data loader for the segmentation dataset\n- Learn how to build a Unet in Tensorflow"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nfrom os.path import join\n\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport cv2\n\nfrom tqdm import tqdm\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Main Configuration"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_DIR = join('..', 'input')\n\nTRAIN_IMG_DIR = join(DATA_DIR, 'train_images')\nTEST_IMG_DIR = join(DATA_DIR, 'test_images')\nSAMPLE_SUB = join(DATA_DIR, 'sample_submission.csv')\nTRAIN_DATA = join(DATA_DIR, 'train.csv')\nmodel_save_path = join('.', 'ResUNetSteel_z.h5')\npretrained_model_path = join('..', 'input', 'severstal-pretrained-model', 'ResUNetSteel_z.h5')\n\n\ntrain_df = pd.read_csv(TRAIN_DATA)\ntrain_df['ImageId_ClassId'] = train_df.apply(lambda x: '{}_{}'.format(x.ImageId, x.ClassId), axis=1)\nsub_df = pd.read_csv(SAMPLE_SUB)\nsave_model = True\n\n\n# Kernel Configurations\nmake_submission = False # used to turn off lengthy model analysis so a submission version doesn't run into memory error\nload_pretrained_model = True # load a pre-trained model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## Run-Length Encoding\n> In order to reduce the submission file size, our metric uses run-length encoding on the pixel values. Instead of submitting an exhaustive list of indices for your segmentation, you will submit **pairs of values** that contain a **start position and a run length**. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).\n>\n>The competition format requires a **space delimited list of pairs**. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are **sorted, positive, and the decoded pixel values are not duplicated**. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.\n\nSo, if we were to encode something like our example above, we would have to write it as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# a more elaborate version of kaggle.com/paulorzp/rle-functions-run-lenght-encode-decode\n# note that we will transpose the incoming array outside of the function, \n# as I find this a clearer illustration\n\ndef mask_to_rle(mask):\n    \"\"\"\n    params:  mask - numpy array\n    returns: run-length encoding string (pairs of start & length of encoding)\n    \"\"\"\n    \n    # turn a n-dimensional array into a 1-dimensional series of pixels\n    # for example:\n    #     [[1. 1. 0.]\n    #      [0. 0. 0.]   --> [1. 1. 0. 0. 0. 0. 1. 0. 0.]\n    #      [1. 0. 0.]]\n    flat = mask.flatten()\n    \n    # we find consecutive sequences by overlaying the mask\n    # on a version of itself that is displaced by 1 pixel\n    # for that, we add some padding before slicing\n    padded = np.concatenate([[0], flat, [0]])\n    \n    # this returns the indices where the sliced arrays differ\n    runs = np.where(padded[1:] != padded[:-1])[0] \n    # indexes start at 0, pixel numbers start at 1\n    runs += 1\n\n    # every uneven element represents the start of a new sequence\n    # every even element is where the run comes to a stop\n    # subtract the former from the latter to get the length of the run\n    runs[1::2] -= runs[0::2]\n \n    # convert the array to a string\n    return ' '.join(str(x) for x in runs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rle_to_mask(lre, shape=(1600, 256)):\n    '''\n    params:  rle   - run-length encoding string (pairs of start & length of encoding)\n             shape - (width,height) of numpy array to return \n    \n    returns: numpy array with dimensions of shape parameter\n    '''    \n    # the incoming string is space-delimited\n    runs = np.asarray([int(run) for run in lre.split(' ')])\n    \n    # we do the same operation with the even and uneven elements, but this time with addition\n    runs[1::2] += runs[0::2]\n    # pixel numbers start at 1, indexes start at 0\n    runs -= 1\n    \n    # extract the starting and ending indeces at even and uneven intervals, respectively\n    run_starts, run_ends = runs[0::2], runs[1::2]\n    \n    # build the mask\n    h, w = shape\n    mask = np.zeros(h*w, dtype=np.uint8)\n    for start, end in zip(run_starts, run_ends):\n        mask[start:end] = 1\n    \n    # transform the numpy array from flat to the original image shape\n    return mask.reshape(shape).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n\n## Prediction Output Format\nThe following explanation is taken from here: https://www.kaggle.com/robinteuwens/mask-rcnn-detailed-starter-code\n\nFrom the competition's [data](https://www.kaggle.com/c/severstal-steel-defect-detection/data) page:\n> Each image may have no defects, a defect of a single class, or defects of multiple classes. For each image you must segment defects of each class ```(ClassId = [1, 2, 3, 4])```.\n\nThe submission format requires us to make the classifications for each respective class on a separate row, adding the *_class* to the imageId:\n![format](https://i.imgur.com/uEeoOQg.png)\n\n## Loss Function\n\n### Dice Coefficient\nFrom the [evaluation](https://www.kaggle.com/c/severstal-steel-defect-detection/overview/evaluation) page:\n\n> This competition is evaluated on the mean Dice coefficient. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:\n>\n>$$Dice(X,Y) = \\frac{2∗|X∩Y|}{|X|+|Y|}$$\n>\n>\n>where X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 1 when both X and Y are empty. The leaderboard score is the mean of the Dice coefficients for each ```<ImageId, ClassId>``` pair in the test set.\n\nVisual illustration of the Dice Coefficient:\n![dice_viz](https://i.imgur.com/zl2W0xQ.png)\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import layers\n\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# network configuration parameters\n# original image is 1600x256, so we will resize it\nimg_w = 512 # resized weidth\nimg_h = 128 # resized height\nbatch_size = 8\nepochs = 20\n# batch size for training unet\nval_size = .20 # split of training set between train and validation set\n# we will repeat the images with lower samples to make the training process more fair\nrepeat = False\n# only valid if repeat is True\nclass_1_repeat = 1 # repeat class 1 examples x times\nclass_2_repeat = 1\nclass_3_repeat = 1\nclass_4_repeat = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, list_ids, labels, image_dir, batch_size=32,\n                 img_h=256, img_w=512, shuffle=True):\n        \n        self.list_ids = list_ids\n        self.labels = labels\n        self.image_dir = image_dir\n        self.batch_size = batch_size\n        self.img_h = img_h\n        self.img_w = img_w\n        self.shuffle = shuffle\n        self.on_epoch_end()\n    \n    def __len__(self):\n        'denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_ids)) / self.batch_size)\n    \n    def __getitem__(self, index):\n        'generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        # get list of IDs\n        list_ids_temp = [self.list_ids[k] for k in indexes]\n        # generate data\n        X, y = self.__data_generation(list_ids_temp)\n        # return data \n        return X, y\n    \n    def on_epoch_end(self):\n        'update ended after each epoch'\n        self.indexes = np.arange(len(self.list_ids))\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n            \n    def __data_generation(self, list_ids_temp):\n        'generate data containing batch_size samples'\n        X = np.empty((self.batch_size, self.img_h, self.img_w, 1))\n        y = np.empty((self.batch_size, self.img_h, self.img_w, 4))\n        \n        for idx, id in enumerate(list_ids_temp):\n            file_path =  os.path.join(self.image_dir, id)\n            image = cv2.imread(file_path, 0)\n            image_resized = cv2.resize(image, (self.img_w, self.img_h))\n            image_resized = np.array(image_resized, dtype=np.float64)\n            # standardization of the image\n#             image_resized -= image_resized.mean()\n#             image_resized /= image_resized.std()\n            image_resized = image_resized.astype('float') / 255.\n            mask = np.empty((img_h, img_w, 4))\n            \n            for idm, image_class in enumerate(['1','2','3','4']):\n                rle = self.labels.get(id + '_' + image_class)\n                # if there is no mask create empty mask\n                if rle is None:\n                    class_mask = np.zeros((1600, 256))\n                else:\n                    class_mask = rle_to_mask(rle, (1600, 256))\n             \n                class_mask_resized = cv2.resize(class_mask, (self.img_w, self.img_h))\n                mask[..., idm] = class_mask_resized\n            \n            X[idx,] = np.expand_dims(image_resized, axis=2)\n            y[idx,] = mask\n        \n        # normalize Y\n        y = (y > 0).astype(int)\n            \n        return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a dict of all the masks\nmasks = {}\nfor index, row in train_df.iterrows():\n    masks[row['ImageId_ClassId']] = row['EncodedPixels']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# repeat low represented samples more frequently to balance our dataset\nif repeat:\n    class_1_img_id = train_df[(train_df['EncodedPixels']!=-1) & (train_df['ClassId']=='1')]['ImageId'].values\n    class_1_img_id = np.repeat(class_1_img_id, class_1_repeat)\n    class_2_img_id = train_df[(train_df['EncodedPixels']!=-1) & (train_df['ClassId']=='2')]['ImageId'].values\n    class_2_img_id = np.repeat(class_2_img_id, class_2_repeat)\n    class_3_img_id = train_df[(train_df['EncodedPixels']!=-1) & (train_df['ClassId']=='3')]['ImageId'].values\n    class_3_img_id = np.repeat(class_3_img_id, class_3_repeat)\n    class_4_img_id = train_df[(train_df['EncodedPixels']!=-1) & (train_df['ClassId']=='4')]['ImageId'].values\n    class_4_img_id = np.repeat(class_4_img_id, class_4_repeat)\n    train_image_ids = np.concatenate([class_1_img_id, class_2_img_id, class_3_img_id, class_4_img_id])\nelse:\n    # split the training data into train and validation set (stratified)\n    train_image_ids = train_df['ImageId'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val = train_test_split(train_image_ids, test_size=val_size, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'img_h': img_h,\n          'img_w': img_w,\n          'image_dir': TRAIN_IMG_DIR,\n          'batch_size': batch_size,\n          'shuffle': True}\n\n# Get Generators\ntraining_generator = DataGenerator(X_train, masks, **params)\nvalidation_generator = DataGenerator(X_val, masks, **params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check out the shapes\nx, y = training_generator.__getitem__(1)\nprint(x.shape, y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize steel image with four classes of faults in seperate columns\ndef viz_steel_img_mask(img, masks):\n    img = cv2.cvtColor(img.astype('float32'), cv2.COLOR_BGR2RGB)\n    fig, ax = plt.subplots(nrows=1, ncols=4, sharey=True, figsize=(20,10))\n    cmaps = [\"Oranges\", \"Blues\", \"Purples\", \"Reds\"]\n    for idx, mask in enumerate(masks):\n        ax[idx].imshow(img)\n        ax[idx].imshow(mask, alpha=0.3, cmap=cmaps[idx])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets visualize some images with their faults to make sure our data generator is working like it should\nx, y = training_generator[np.random.randint(len(training_generator))]\nfor ix in range(0,batch_size):\n    if y[ix].sum() > 0:\n        img = x[ix]\n        masks_temp = [y[ix][...,i] for i in range(0,4)]\n        viz_steel_img_mask(img, masks_temp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building Model"},{"metadata":{},"cell_type":"markdown","source":"For this competition we are going to use ResUnet [[paper]](https://arxiv.org/pdf/1904.00592.pdf). \n\n<img src='https://www.researchgate.net/profile/Diab_Abueidda2/publication/344100424/figure/fig2/AS:932383985528836@1599309073617/Illustration-of-the-a-architecture-of-the-ResUNet-b-building-block-used-in-U-Net-and.jpg'>\n<center> Illustration of the a) architecture of the ResUNet, b) building block used in U-Net, and c) building block used in ResUNet. </center>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source:\n# https://github.com/fchollet/deep-learning-models/blob/master/resnet50.py\n\n\npretrained_url = (\n    \"https://github.com/fchollet/deep-learning-models/\"\n    \"releases/download/v0.2/\"\n    \"resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n)\n\n\ndef one_side_pad(x):\n    x = ZeroPadding2D((1, 1))(x)\n    x = Lambda(lambda x: x[:, :-1, :-1, :])(x)\n    return x\n\n\ndef identity_block(input_tensor, kernel_size, filters, stage, block):\n    \"\"\"The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: defualt 3, the kernel size of middle conv layer at\n                     main path\n        filters: list of integers, the filterss of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    \"\"\"\n    filters1, filters2, filters3 = filters\n\n    conv_name_base = \"res\" + str(stage) + block + \"_branch\"\n    bn_name_base = \"bn\" + str(stage) + block + \"_branch\"\n\n    x = Conv2D(filters1, (1, 1), name=conv_name_base + \"2a\")(input_tensor)\n    x = BatchNormalization(name=bn_name_base + \"2a\")(x)\n    x = Activation(\"relu\")(x)\n\n    x = Conv2D(filters2, kernel_size, padding=\"same\", name=conv_name_base + \"2b\")(x)\n    x = BatchNormalization(name=bn_name_base + \"2b\")(x)\n    x = Activation(\"relu\")(x)\n\n    x = Conv2D(filters3, (1, 1), name=conv_name_base + \"2c\")(x)\n    x = BatchNormalization(name=bn_name_base + \"2c\")(x)\n\n    x = layers.add([x, input_tensor])\n    x = Activation(\"relu\")(x)\n    return x\n\n\ndef conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n    \"\"\"conv_block is the block that has a conv layer at shortcut\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: defualt 3, the kernel size of middle conv layer at\n                     main path\n        filters: list of integers, the filterss of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    Note that from stage 3, the first conv layer at main path is with\n    strides=(2,2) and the shortcut should have strides=(2,2) as well\n    \"\"\"\n    filters1, filters2, filters3 = filters\n\n    conv_name_base = \"res\" + str(stage) + block + \"_branch\"\n    bn_name_base = \"bn\" + str(stage) + block + \"_branch\"\n\n    x = Conv2D(filters1, (1, 1), strides=strides, name=conv_name_base + \"2a\")(\n        input_tensor\n    )\n    x = BatchNormalization(name=bn_name_base + \"2a\")(x)\n    x = Activation(\"relu\")(x)\n\n    x = Conv2D(filters2, kernel_size, padding=\"same\", name=conv_name_base + \"2b\")(x)\n    x = BatchNormalization(name=bn_name_base + \"2b\")(x)\n    x = Activation(\"relu\")(x)\n\n    x = Conv2D(filters3, (1, 1), name=conv_name_base + \"2c\")(x)\n    x = BatchNormalization(name=bn_name_base + \"2c\")(x)\n\n    shortcut = Conv2D(filters3, (1, 1), strides=strides, name=conv_name_base + \"1\")(\n        input_tensor\n    )\n    shortcut = BatchNormalization(name=bn_name_base + \"1\")(shortcut)\n\n    x = layers.add([x, shortcut])\n    x = Activation(\"relu\")(x)\n    return x\n\n\ndef resnet50_encoder(input_shape, weights=\"imagenet\"):\n    assert input_shape[0] % 32 == 0\n    assert input_shape[1] % 32 == 0\n\n    img_input = Input(shape=input_shape)\n\n    x = ZeroPadding2D((3, 3))(img_input)\n    x = Conv2D(64, (7, 7), strides=(1, 1), name=\"conv1\")(x)\n    f1 = x\n\n    x = BatchNormalization(name=\"bn_conv1\")(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n    x = conv_block(x, 3, [64, 64, 256], stage=2, block=\"a\", strides=(1, 1))\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block=\"b\")\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block=\"c\")\n    f2 = one_side_pad(x)\n\n    x = conv_block(x, 3, [128, 128, 512], stage=3, block=\"a\")\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block=\"b\")\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block=\"c\")\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block=\"d\")\n    f3 = x\n\n    x = conv_block(x, 3, [256, 256, 1024], stage=4, block=\"a\")\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\"b\")\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\"c\")\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\"d\")\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\"e\")\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\"f\")\n    f4 = x\n\n    x = conv_block(x, 3, [512, 512, 2048], stage=5, block=\"a\")\n    x = identity_block(x, 3, [512, 512, 2048], stage=5, block=\"b\")\n    x = identity_block(x, 3, [512, 512, 2048], stage=5, block=\"c\")\n    f5 = x\n\n    x = AveragePooling2D((7, 7), name=\"avg_pool\")(x)\n    # f6 = x\n\n    if weights == \"imagenet\":\n        weights_path = keras.utils.get_file(\n            pretrained_url.split(\"/\")[-1], pretrained_url\n        )\n        Model(img_input, x).load_weights(weights_path)\n\n    return img_input, [f1, f2, f3, f4, f5]\n\n\ndef unet(input_shape, n_classes, encoder, l1_skip_conn=True, weights='imagenet', output_act_f='softmax'):\n\n    img_input, levels = encoder(input_shape=input_shape, weights=weights)\n    [f1, f2, f3, f4, f5] = levels\n    \n\n#     x = f5\n#     x = ZeroPadding2D((1, 1))(x)\n#     x = Conv2D(512, (3, 3), padding=\"valid\", activation=\"relu\")(x)\n#     x = BatchNormalization()(x)\n#     x = UpSampling2D((2, 2))(x)\n#     x = Concatenate(axis=-1)([x, f4])\n    \n    x = f4\n    x = ZeroPadding2D((1, 1))(x)\n    x = Conv2D(512, (3, 3), padding=\"valid\", activation=\"relu\")(x)\n    x = BatchNormalization()(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Concatenate(axis=-1)([x, f3])\n    \n    x = ZeroPadding2D((1, 1))(x)\n    x = Conv2D(256, (3, 3), padding=\"valid\", activation=\"relu\")(x)\n    x = BatchNormalization()(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Concatenate(axis=-1)([x, f2])\n    \n    x = ZeroPadding2D((1, 1))(x)\n    x = Conv2D(128, (3, 3), padding=\"valid\", activation=\"relu\")(x)\n    x = BatchNormalization()(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Concatenate(axis=-1)([x, f1])\n\n    x = ZeroPadding2D((1, 1))(x)\n    x = Conv2D(64, (3, 3), padding=\"valid\", activation=\"relu\")(x)\n    x = BatchNormalization()(x)\n\n    x = Conv2D(n_classes, (3, 3), padding=\"same\", activation=output_act_f)(x)\n\n    model = Model(inputs=img_input, outputs=x)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loss and Performance Metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dice similarity coefficient loss, brought to you by: https://github.com/nabsabraham/focal-tversky-unet\ndef dsc(y_true, y_pred):\n    smooth = 1.\n    y_true_f = Flatten()(y_true)\n    y_pred_f = Flatten()(y_pred)\n    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n    return score\n\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dsc(y_true, y_pred)\n    return loss\n\ndef bce_dice_loss(y_true, y_pred):\n    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n    return loss\n\n# Focal Tversky loss, brought to you by:  https://github.com/nabsabraham/focal-tversky-unet\ndef tversky(y_true, y_pred, smooth=1e-6):\n    y_true_pos = tf.keras.layers.Flatten()(y_true)\n    y_pred_pos = tf.keras.layers.Flatten()(y_pred)\n    true_pos = tf.reduce_sum(y_true_pos * y_pred_pos)\n    false_neg = tf.reduce_sum(y_true_pos * (1-y_pred_pos))\n    false_pos = tf.reduce_sum((1-y_true_pos)*y_pred_pos)\n    alpha = 0.7\n    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n\ndef tversky_loss(y_true, y_pred):\n    return 1 - tversky(y_true, y_pred)\n\ndef focal_tversky_loss(y_true,y_pred):\n    pt_1 = tversky(y_true, y_pred)\n    gamma = 0.75\n    return tf.keras.backend.pow((1-pt_1), gamma)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = unet(input_shape=(img_h, img_w, 1), encoder=resnet50_encoder, n_classes=4, weights=None, output_act_f='sigmoid')\n\n\nfrom tensorflow.keras.metrics import binary_accuracy\nfrom tensorflow.keras.optimizers import SGD, Adam, Nadam\n\nmodel.compile(optimizer = 'adam', loss=dice_loss, metrics=['accuracy', dsc, tversky])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if load_pretrained_model:\n    try:\n        model.load_weights(pretrained_model_path)\n        print('pre-trained model loaded!')\n    except OSError:\n        print('You need to run the model and load the trained model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\n\nbest_w = ModelCheckpoint('best.h5',\n                                monitor='val_dsc',\n                                verbose=0,\n                                save_best_only=True,\n                                save_weights_only=True,\n                                mode='auto',\n                                period=1)\n\nlast_w = ModelCheckpoint('last.h5',\n                                monitor='val_dsc',\n                                verbose=0,\n                                save_best_only=False,\n                                mode='auto',\n                                period=1)\n\ncallbacks = [best_w, last_w]\n\nhistory = model.fit_generator(\n    generator=training_generator,\n    validation_data=validation_generator,\n    callbacks=callbacks,\n    epochs=epochs,\n    verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if save_model: \n    model.save(model_save_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n\ndef visualize_training_process(history):\n    \"\"\" \n    Visualize loss and accuracy from training history\n    \n    :param history: A Keras History object\n    \"\"\"\n    history_df = pd.DataFrame(history.history)\n    epochs = np.arange(1, len(history_df) + 1)\n    fig = make_subplots(3, 1)\n    fig.append_trace(go.Scatter(x=epochs, y=history_df['tversky'], mode='lines+markers', name='Tversky'), row=1, col=1)\n    fig.append_trace(go.Scatter(x=epochs, y=history_df['val_tversky'], mode='lines+markers', name='Val Tversky'), row=1, col=1)\n    \n    fig.append_trace(go.Scatter(x=epochs, y=history_df['dsc'], mode='lines+markers', name='Dice'), row=2, col=1)\n    fig.append_trace(go.Scatter(x=epochs, y=history_df['val_dsc'], mode='lines+markers', name='Val Dice'), row=2, col=1)\n    \n    \n    \n    fig.append_trace(go.Scatter(x=epochs, y=history_df['loss'], mode='lines+markers', name='Loss Train'), row=3, col=1)\n    fig.append_trace(go.Scatter(x=epochs, y=history_df['val_loss'], mode='lines+markers', name='Loss Val'), row=3, col=1)\n    \n    fig.update_layout( xaxis_title=\"Epochs\", template=\"plotly_white\", height=1000)\n    \n    return fig\nvisualize_training_process(history)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a function to plot image with mask and image with predicted mask next to each other\ndef viz_single_fault(img, mask, pred, image_class):\n    \n    fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(15,5))\n    \n    cmaps = [\"Reds\", \"Blues\", \"Greens\", \"Purples\"]\n    \n    ax[0].imshow(img)\n    ax[0].imshow(mask, alpha=0.3, cmap=cmaps[image_class-1])\n    ax[0].set_title('Mask - Defect Class %s' % image_class)\n    \n    ax[1].imshow(img)\n    ax[1].imshow(pred, alpha=0.3, cmap=cmaps[image_class-1])\n    ax[1].set_title('Predicted Mask - Defect Class %s' % image_class)\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.jeremyjordan.me/evaluating-image-segmentation-models/\ndef calculate_iou(target, prediction):\n    intersection = np.logical_and(target, prediction)\n    union = np.logical_or(target, prediction)\n    if np.sum(union) == 0:\n        iou_score = 0\n    else:\n        iou_score = np.sum(intersection) / np.sum(union)\n    return iou_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not make_submission:\n    # lets loop over the predictions and print 5 of each image cases with defects\n    count = 0\n    # a list to keep count of the number of plots made per image class\n    class_viz_count = [0, 0, 0, 0]\n    # to keep the total iou score per image class\n    class_iou_score = [0, 0, 0, 0]\n    # to keep sum of mask pixels per image class\n    class_mask_sum = [0, 0, 0, 0]\n    # to keep sum of predicted mask pixels per image class\n    class_pred_sum = [0, 0, 0, 0]\n\n    # loop over to all the batches in one epoch \n    for i in range(0, 4):\n        # get a batch of image, true mask, and predicted mask\n        x, y = validation_generator.__getitem__(i)\n        predictions = model.predict(x)\n\n        # loop through x to get all the images in the batch\n        for idx, val in enumerate(x):\n            # we are only interested if there is a fault. if we are dropping images with no faults before this will become redundant\n            if y[idx].sum() > 0: \n                # get an image and convert to make it matplotlib.pyplot friendly\n                img = x[idx]\n                img = cv2.cvtColor(img.astype('float32'), cv2.COLOR_BGR2RGB)\n                # loop over the four ourput layers to create a list of all the masks for this image\n                masks_temp = [y[idx][...,i] for i in range(0,4)]\n                # loop over the four output layers to create a list of all the predictions for this image\n                preds_temp = [predictions[idx][...,i] for i in range(0,4)]\n                # turn to binary (prediction) mask \n                preds_temp = [p > .5 for p in preds_temp]\n\n                for i, (mask, pred) in enumerate(zip(masks_temp, preds_temp)):\n                    image_class = i + 1\n                    class_iou_score[i] += calculate_iou(mask, pred)\n                    class_mask_sum[i] += mask.sum()\n                    class_pred_sum[i] += pred.sum()\n                    if mask.sum() > 0 and class_viz_count[i] < 5:\n                        viz_single_fault(img, mask, pred, image_class)\n                        class_viz_count[i] += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nclass TestDataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, image_dir, batch_size=32, img_h=256, img_w=512, shuffle=False):\n        \n        self.list_ids = [f.split('/')[-1] for f in glob.glob(join(image_dir, \"*.jpg\"), recursive=True)]\n        self.image_dir = image_dir\n        self.batch_size = batch_size\n        self.img_h = img_h\n        self.img_w = img_w\n        self.shuffle = shuffle\n        self.on_epoch_end()\n    \n    def __len__(self):\n        'denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_ids)) / self.batch_size)\n    \n    def __getitem__(self, index):\n        'generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        # get list of IDs\n        list_ids_temp = [self.list_ids[k] for k in indexes]\n        # generate data\n        X, list_ids_temp = self.__data_generation(list_ids_temp)\n        # return data \n        return X, list_ids_temp\n    \n    def on_epoch_end(self):\n        'update ended after each epoch'\n        self.indexes = np.arange(len(self.list_ids))\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n            \n    def __data_generation(self, list_ids_temp):\n        'generate data containing batch_size samples'\n        X = np.empty((self.batch_size, self.img_h, self.img_w, 1))\n        \n        for idx, image_id in enumerate(list_ids_temp):\n            file_path =  os.path.join(self.image_dir, image_id)\n            image = cv2.imread(file_path, 0)\n            image_resized = cv2.resize(image, (self.img_w, self.img_h))\n            image_resized = np.array(image_resized, dtype=np.float64)\n            # standardization of the image\n#             image_resized -= image_resized.mean()\n#             image_resized /= image_resized.std()\n            image_resized = image_resized.astype('float') / 255.\n            X[idx, ...] = image_resized[..., None]\n            \n        return X, list_ids_temp ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this is an awesome little function to remove small spots in our predictions\nfrom skimage import morphology\n\ndef remove_small_regions(img, size):\n    \"\"\"Morphologically removes small (less than size) connected regions of 0s or 1s.\"\"\"\n    img = morphology.remove_small_objects(img, size)\n    img = morphology.remove_small_holes(img, size)\n    return img\n\n# a function to apply all the processing steps necessery to each of the individual masks\ndef process_pred_mask(pred_mask):\n    \n    pred_mask = cv2.resize(pred_mask.astype('float32'),(256, 1600))\n    pred_mask = (pred_mask > .5).astype(int)\n    pred_mask = remove_small_regions(pred_mask, 0.02 * np.prod(512))\n    pred_mask = mask_to_rle(pred_mask)\n    return pred_mask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\n# get all files using glob\ntest_files = [f for f in glob.glob(join(TEST_IMG_DIR, \"*.jpg\"), recursive=True)]\n\ntest_generator = TestDataGenerator(image_dir=TEST_IMG_DIR, img_h=img_h, img_w=img_w, batch_size=16)\n\nsubmission_entries = []\n\nif make_submission:\n    for test_batch, image_ids in test_generator:\n        pred_batch = model.predict(test_batch)\n\n        for i, img_id in enumerate(image_ids):\n            for j in range(0, 4):\n                lre_mask = process_pred_mask(pred_batch[i, ..., j])\n                submission_entries.append((img_id, lre_mask, j + 1))\n                \nsubmission = pd.DataFrame(submission_entries, columns=sub_df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}