{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Severstal: Mask R-CNN tutorial for beginners\nThis tutorial covers a brief overview of Mask R-CNN and how to train a model from scratch using the Matterport Mask R-CNN implementation."},{"metadata":{},"cell_type":"markdown","source":"## Severstal: Steel Defect Detection\nSteel is one of the most important building materials of modern times. Steel buildings are resistant to natural and man-made wear which has made the material ubiquitous around the world. To help make production of steel more efficient, this competition will help identify defects.\n<br><br>\nSeverstal is leading the charge in efficient steel mining and production. They believe the future of metallurgy requires development across the economic, ecological, and social aspects of the industry—and they take corporate responsibility seriously. The company recently created the country’s largest industrial data lake, with petabytes of data that were previously discarded. Severstal is now looking to machine learning to improve automation, increase efficiency, and maintain high quality in their production.\n<br><br>\nThe production process of flat sheet steel is especially delicate. From heating and rolling, to drying and cutting, several machines touch flat steel by the time it’s ready to ship. Today, Severstal uses images from high frequency cameras to power a defect detection algorithm."},{"metadata":{},"cell_type":"markdown","source":"## Mask R-CNN\nMask R-CNN (regional convolutional neural network) is a two stage framework: the first stage scans the image and generates proposals(areas likely to contain an object). And the second stage classifies the proposals and generates bounding boxes and masks.\n<br><br>\nIt was introduced in 2017 via the [Mask R-CNN paper](https://arxiv.org/abs/1703.06870) to extend its predecessor, Faster R-CNN, by the same authors. Faster R-CNN is a popular framework for object detection, and Mask R-CNN extends it with instance segmentation, among other things.\n<br><br>\nInstance segmentation is the task of identifying object outlines at the pixel level. Compared to similar computer vision tasks, it’s one of the hardest possible vision tasks.\n<br><br>\n![](https://miro.medium.com/max/1928/1*IWWOPIYLqqF9i_gXPmBk3g.png)"},{"metadata":{},"cell_type":"markdown","source":"## Matterport implementation\nMatterport open-sourced their [implementation of Mask R-CNN](https://github.com/matterport/Mask_RCNN) and since then it’s been used in a lot of projects. This is an implementation of Mask R-CNN on Python 3, Keras, and TensorFlow. The model generates bounding boxes and segmentation masks for each instance of an object in the image. It's based on Feature Pyramid Network (FPN) and a ResNet101 backbone.\n<br><br>\nSince the competition requires internet connection to be turned off, the required packages are added as dataset to this notebook. Except for some minor import changes the packages are unchanged."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Basics\nfrom glob import glob # finds pathnames\nimport os # Miscellaneous operating system interfaces\nimport sys\nimport random\nimport timeit\nimport imp\nimport gc\n\n# Processing\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom scipy.ndimage import label as scipy_label\nfrom scipy.ndimage import generate_binary_structure\n\n# Image manipulation\nimport skimage\n\n# Import Mask RCNN\nsys.path.append('../input/mask-rcnn')  # To find local version of the library\nfrom config import Config\n# Imp import to ensure loading the correct utils package\nfp, pathname, description = imp.find_module('utils',['../input/mask-rcnn'])\nutils = imp.load_module('utils',fp,pathname,description)\nimport model as modellib\nimport visualize\nfrom model import log\n\n# Plotting\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_ax(rows=1, cols=1, size=8):\n    \"\"\"Return a Matplotlib Axes array to be used in\n    all visualizations in the notebook. Provide a\n    central point to control graph sizes.\n    \n    Change the default size attribute to control the size\n    of rendered images\n    \"\"\"\n    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n    return ax","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Paths\nROOT_DIR = '../input/severstal-steel-defect-detection/'\nTrain_Dir = ROOT_DIR + 'train_images/'\nTest_Dir = ROOT_DIR + 'test_images/'\n\n# Directory to save logs and trained model\nMODEL_DIR = 'logs'\n\n# Local path to trained weights file\n# Trained_Weights = '../input/????.h5'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Configuration\nThe Matterport implementation provides a Config class that you inherit from and then override values. \n<br><br>\nIMAGE_MAX_DIM is set to 256 for demonstration purposes and can be increased to 1024 / 1600 for Kaggle kernel with GPU disabled / enabled.\n<br><br>\nSTEPS_PER_EPOCH and VALIDATION_STEPS are set to low values for demonstration purposes and should be increased by at least a factor of 10."},{"metadata":{"trusted":true},"cell_type":"code","source":"class SteelConfig(Config):\n    \"\"\"Configuration for training on the steel dataset.\n    Derives from the base Config class and overrides values specific\n    to the steel dataset.\n    \"\"\"\n    # Give the configuration a recognizable name\n    NAME = \"steel\"\n\n    # Train on 1 GPU and 1 image per GPU.\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n\n    # Number of classes (including background)\n    NUM_CLASSES = 1 + 4  # background + 4 defect classes\n\n    # Use small images (128x128) for faster training. Set the limits of the small side\n    # the large side, and that determines the image shape.\n    IMAGE_MIN_DIM = 256\n    IMAGE_MAX_DIM = 256\n\n    # Use smaller anchors because our objects tend to be small \n    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n\n    # Reduce training ROIs per image because the images have few objects.\n    # Aim to allow ROI sampling to pick 33% positive ROIs.\n    TRAIN_ROIS_PER_IMAGE = 32\n\n    # Minimum probability value to accept a detected instance\n    # ROIs below this threshold are skipped\n    DETECTION_MIN_CONFIDENCE = 0.7\n    \n    # Number of epochs\n    EPOCHS = 3\n    \n    # Steps per epoch\n    STEPS_PER_EPOCH = 100\n\n    # validation steps per epoch\n    VALIDATION_STEPS = 5\n    \n    # Non-maximum suppression threshold for detection, default 0.3\n    DETECTION_NMS_THRESHOLD = 0.0\n    \n    # Non-max suppression threshold to filter RPN proposals. default 0.7\n    # You can increase this during training to generate more propsals.\n    RPN_NMS_THRESHOLD = 0.0\n    \nconfig = SteelConfig()\nconfig.display()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the Severstal Dataset\nThere isn’t a universally accepted format to store segmentation masks. To handle all cases, the Matterport implementation provides a Dataset class that you inherit from and then override a few functions to read the Severstal dataset.\n<br><br>\nThe Severstal encoded pixels metric uses run-length encoding (RLE) on the pixel values. RLE consists of pairs of values that contain a pixel start position and a pixel run length.\n<br><br>\n**load_steel** reads the train.csv file, extracts annotations and classes and iteratively calls the internal **add_class** funtion to build the dataset. Images without annotations are not included in the dataset.\n<br><br>\n**load_mask** converts RLE to a single bitmap mask, labels individual objects and creates a bitmap mask for every object in the image. A list of classes is created containing the classID for every object bitmap mask.\n<br><br>\n**image_reference** returns a string that identifies the image for debugging purposes. In this tutorial it simply returns the path of the image file."},{"metadata":{"trusted":true},"cell_type":"code","source":"class SteelDataset(utils.Dataset):\n\n    def load_steel(self, dataset_dir, files):\n        \"\"\"Load a subset of the Steel dataset.\n        \n        Input:\n        dataset_dir: Root directory of the dataset.\n        files: filenames of images to load\n        \n        Creates:\n        image objects:\n            source: source label\n            image_id: id, used filename\n            path: path + filename\n            rle: rle mask encoded pixels, required for mask conversion\n            classes: classes for the rle masks, required for mask conversion        \n        \"\"\"\n        # Add classes.\n        self.add_class(\"steel\", 1, \"defect1\")\n        self.add_class(\"steel\", 2, \"defect2\")\n        self.add_class(\"steel\", 3, \"defect3\")\n        self.add_class(\"steel\", 4, \"defect4\")\n        \n        # Load annotations CSV\n        annotations_train = pd.read_csv(dataset_dir + 'train.csv')\n\n        # Remove images without Encoding\n        annotations_train_Encoded = annotations_train[annotations_train['EncodedPixels'].notna()].copy()        \n        \n        # Split ImageId_ClassId\n        ImageId_ClassId_split = annotations_train_Encoded[\"ImageId_ClassId\"].str.split(\"_\", n = 1, expand = True)\n        annotations_train_Encoded['ImageId'] = ImageId_ClassId_split.loc[:,0]\n        annotations_train_Encoded['ClassId'] = ImageId_ClassId_split.loc[:,1]     \n\n        for file in files:\n            EncodedPixels = [annotations_train_Encoded['EncodedPixels'][annotations_train_Encoded['ImageId'] == file]]\n            ClassID = (annotations_train_Encoded['ClassId'][annotations_train_Encoded['ImageId'] == file])\n\n            self.add_image(\n                source = \"steel\",\n                image_id = file,  # use filename as a unique image id\n                path = Train_Dir + '/' + file,\n                rle = EncodedPixels,\n                classes = ClassID)\n\n    def load_mask(self, image_id):\n        \"\"\"Generate instance masks for an image.\n        Input:\n        image_id: id of the image\n        \n        Returns:\n        masks: A bool array of shape [height, width, instance count] with one mask per instance.\n        class_ids: a 1D int array of class IDs of the instance masks.\n        \"\"\"\n        # If not a steel dataset image, delegate to parent class.\n        image_info = self.image_info[image_id]\n        if image_info[\"source\"] != \"steel\":\n            return super(self.__class__, self).load_mask(image_id)\n        \n        # Convert rle to single mask\n        ClassIDIndex = 0\n        ClassID = np.empty(0, dtype = int)\n        maskarray = np.empty((256, 1600, 0), dtype = int)\n        for rlelist in image_info['rle']:\n            for row in rlelist:\n                mask= np.zeros(1600 * 256 ,dtype=np.uint8)\n                array = np.asarray([int(x) for x in row.split()])\n                starts = array[0::2]-1\n                lengths = array[1::2]    \n                for index, start in enumerate(starts):\n                    mask[int(start):int(start+lengths[index])] = 1\n                mask = mask.reshape((256,1600), order='F')\n                # Label mask elements\n                structure = generate_binary_structure(2,2)\n                labeled_array, labels = scipy_label(mask, structure)\n                # Convert labeled_array elements to bitmap mask array\n                for label in range(labels):\n                    labelmask = np.copy(labeled_array)    \n                    labelmask[labelmask != label + 1] = 0\n                    if label == 0:\n                        labelmask = np.expand_dims(labelmask, axis = 2)\n                        maskarray = np.concatenate((maskarray, labelmask), axis = 2)\n                    else:\n                        labelmask[labelmask == label + 1] = 1\n                        labelmask = np.expand_dims(labelmask, axis = 2)\n                        maskarray = np.concatenate((maskarray, labelmask), axis = 2)\n                    # Update ClassID list\n                    ClassID = np.append(ClassID,int(image_info['classes'].iloc[ClassIDIndex]))\n                ClassIDIndex = ClassIDIndex + 1\n\n        # Return mask, and array of class IDs of each instance.\n        return maskarray.astype(np.bool), ClassID\n\n    def image_reference(self, image_id):\n        \"\"\"Return the path of the image.\"\"\"\n        info = self.image_info[image_id]\n        if info[\"source\"] == \"steel\":\n            return info[\"path\"]\n        else:\n            super(self.__class__, self).image_reference(image_id)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Verify the Dataset\nTo verify that **SteelDataset** code is implemented correctly the loading and pre-processing can be visualized and inspected with the **visualize** function. Below script loads the dataset, visualizes masks and bounding boxes, and visualizes the anchors to verify that anchor sizes are a good fit for the object sizes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load example images and masks.\nfiles = ['ef24da2ba.jpg', 'db4867ee8.jpg', 'a28a7b7be.jpg', 'c44784905.jpg']\ndataset = SteelDataset()\ndataset.load_steel(ROOT_DIR, files)\ndataset.prepare()\n\nimage_ids = [0,1,2,3] \nfor image_id in image_ids:\n    image = dataset.load_image(image_id)\n    mask, class_ids = dataset.load_mask(image_id)\n    # Compute Bounding box\n    bbox = utils.extract_bboxes(mask)\n    # Display image and additional stats\n    print(\"image_id \", image_id, dataset.image_reference(image_id))\n    log(\"image\", image)\n    log(\"mask\", mask)\n    log(\"class_ids\", class_ids)\n    log(\"bbox\", bbox)\n    # Display image and instances\n    visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names)\n    visualize.display_top_masks(image, mask, class_ids, dataset.class_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training\nMask R-CNN is a fairly large model. Especially that the Matterport implementation uses ResNet101 and FPN. Therefore using a GPU is recommended."},{"metadata":{"trusted":true},"cell_type":"code","source":"# select files for test and validation dataset\n# Load annotations CSV\nannotations_train = pd.read_csv(ROOT_DIR + 'train.csv')\n\n# Remove images without Encoding\nannotations_train_Encoded = annotations_train[annotations_train['EncodedPixels'].notna()].copy()        \n\n# Split ImageId_ClassId\nImageId_ClassId_split = annotations_train_Encoded[\"ImageId_ClassId\"].str.split(\"_\", n = 1, expand = True)\nannotations_train_Encoded['ImageId'] = ImageId_ClassId_split.loc[:,0]\nannotations_train_Encoded['ClassId'] = ImageId_ClassId_split.loc[:,1]\n\n# Split dataframe\nmsk = np.random.rand(len(annotations_train_Encoded)) < 0.85\ntrain_msk = annotations_train_Encoded[msk]\nval_msk = annotations_train_Encoded[~msk]\ntrain = train_msk['ImageId'].unique().copy()\nval = val_msk['ImageId'].unique().copy()\nprint('Test images: ' + str(len(train)))\nprint('Val images: ' + str(len(val)))\n# Cleanup\ndel annotations_train, ImageId_ClassId_split, annotations_train_Encoded, msk, train_msk, val_msk \ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training preperations\n# Training dataset\ndataset_train = SteelDataset()\ndataset_train.load_steel(ROOT_DIR, train)\ndataset_train.prepare()\n\n# Validation dataset\ndataset_val = SteelDataset()\ndataset_val.load_steel(ROOT_DIR, val)\ndataset_val.prepare()\n\n# Build training model\nmodel = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=MODEL_DIR)\n# Required due to change in new Tensorflow / Keras version\nmodel.keras_model.metrics_tensors = []\n# Load weights to continue training\n# model.load_weights(Trained_Weights, by_name=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train model\ntimestart = timeit.default_timer()\nprint(\"Training\")\nmodel.train(dataset_train, \n            dataset_val,\n            learning_rate=config.LEARNING_RATE,\n            epochs=config.EPOCHS,\n            layers='all')\ntimestop = timeit.default_timer()\nruntime = np.round((timestop - timestart) / 60, 2)\nprint ('Total run time: ' + str(runtime) + ' minutes')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot loss\nhistory = model.keras_model.history.history\nepochs = range(1,config.EPOCHS + 1)\nplt.figure(figsize=(10, 5))\nplt.plot(epochs, history['loss'], label=\"train loss\")\nplt.plot(epochs, history['val_loss'], label=\"valid loss\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}