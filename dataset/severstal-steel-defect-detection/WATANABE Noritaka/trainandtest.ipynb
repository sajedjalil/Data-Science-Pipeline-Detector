{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is based on \"Keras UNET with EDA\".\n# Meta Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport time\n\n!ls -l ../input/trainandtest\n\nmode = 'train' # internet on\n#mode = 'test' # internet off\n#small = True # small data set\nsmall = False\n\npath = '../input/severstal-steel-defect-detection/'\npath_self = '../input/trainandtest/'\nos.environ['path_self'] = path_self\n\ntrain_rate = 0.9\nprob_threshold = 0.5\n\n#!cat ../input/trainandtest/submission.csv\n#!cat ../input/severstal-steel-defect-detection/train.csv\n#import numpy as np, pandas as pd\n#submission = pd.read_csv(\"../input/trainandtest/submission.csv\")\n#type(submission)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n# Data and check\nImage files in directory and names in csv are checked."},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"import numpy as np, pandas as pd\nimport matplotlib.pyplot as plt, time\nfrom PIL import Image \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntrain = pd.read_csv(path + 'train.csv')\n\n# RESTRUCTURE TRAIN DATAFRAME\n#train['ImageId'] = train['ImageId_ClassId'].map(lambda x: x.split('.')[0]+'.jpg')\ntrain['ImageId'] = train['ImageId_ClassId'].map(lambda x: x.split('_')[0]) # 追加\ntrain2 = pd.DataFrame({'ImageId':train['ImageId'][::4]})\ntrain2['e1'] = train['EncodedPixels'][::4].values\ntrain2['e2'] = train['EncodedPixels'][1::4].values\ntrain2['e3'] = train['EncodedPixels'][2::4].values\ntrain2['e4'] = train['EncodedPixels'][3::4].values\ntrain2.reset_index(inplace=True,drop=True)\ntrain2.fillna('',inplace=True); \ntrain2['count'] = np.sum(train2.iloc[:,1:]!='',axis=1).values\n\nfiles = os.listdir(path+\"train_images/\")\nprint(\"train_images == train.csv:\",set(files) == set(train2['ImageId']))\n\nif small == True: train2 = train2[:256]\n\nim = Image.open(path+\"train_images/\"+train2['ImageId'].iloc[0])\nprint(im.format,im.size,im.mode)\n\nidx = int(train_rate*len(train2)); print(len(train2),idx)\ntrain_set = train2.iloc[:idx]\nval_set = train2.iloc[idx:]\n\n# separate no defect, e1, e2, e3, e4 data\ntrain2_no = train2[train2['count']==0]\ntrain2_no.reset_index(inplace=True,drop=True)\ntrain2_e1 = train2[train2['e1']!='']\ntrain2_e1.reset_index(inplace=True,drop=True)\ntrain2_e2 = train2[train2['e2']!='']\ntrain2_e2.reset_index(inplace=True,drop=True)\ntrain2_e3 = train2[train2['e3']!='']\ntrain2_e3.reset_index(inplace=True,drop=True)\ntrain2_e4 = train2[train2['e4']!='']\ntrain2_e4.reset_index(inplace=True,drop=True)\n\nidx = int(train_rate*len(train2_no))\ntrain_no_set = train2_no.iloc[:idx]\nval_no_set = train2_no.iloc[idx:]\nidx = int(train_rate*len(train2_e1))\ntrain_e1_set = train2_e1.iloc[:idx]\nval_e1_set = train2_e1.iloc[idx:]\nidx = int(train_rate*len(train2_e2))\ntrain_e2_set = train2_e2.iloc[:idx]\nval_e2_set = train2_e2.iloc[idx:]\nidx = int(train_rate*len(train2_e3))\ntrain_e3_set = train2_e3.iloc[:idx]\nval_e3_set = train2_e3.iloc[idx:]\nidx = int(train_rate*len(train2_e4))\ntrain_e4_set = train2_e4.iloc[:idx]\nval_e4_set = train2_e4.iloc[idx:]\n\nprint('train_no_set {}\\n'.format(len(train_no_set)), train_no_set.head())\nprint('val_no_set {}\\n'.format(len(val_no_set)), val_no_set.head())\nprint('train_e1_set {}\\n'.format(len(train_e1_set)), train_e1_set.head())\nprint('val_e1_set {}\\n'.format(len(val_e1_set)), val_e1_set.head())\nprint('train_e2_set {}\\n'.format(len(train_e2_set)), train_e2_set.head())\nprint('val_e2_set {}\\n'.format(len(val_e2_set)), val_e2_set.head())\nprint('train_e3_set {}\\n'.format(len(val_e3_set)), train_e3_set.head())\nprint('val_e3_set {}\\n'.format(len(val_e3_set)), val_e3_set.head())\nprint('train_e4_set {}\\n'.format(len(train_e4_set)), train_e4_set.head())\nprint('val_e4_set {}\\n'.format(len(val_e4_set)), val_e4_set.head())\n\n# TEST DATAFRAME\ntest = pd.read_csv(path + 'sample_submission.csv')\ntest['ImageId'] = test['ImageId_ClassId'].map(lambda x: x.split('_')[0])\ntest2 = pd.DataFrame({'ImageId':test['ImageId'][::4]})\ntest2.reset_index(inplace=True,drop=True)\n\nfiles = os.listdir(path+\"test_images/\")\nprint(\"test_images == sample_submission.csv:\",set(files) == set(test2['ImageId']))\n\nif small == True: test2 = test2[:64]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utility Functions\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def rle2mask(rle):\n    height= 256\n    width = 1600\n\n    # CONVERT RLE TO MASK \n    if (pd.isnull(rle))|(rle==''): \n        return np.zeros((height,width) ,dtype=np.uint8)   \n\n    mask= np.zeros( width*height ,dtype=np.uint8)\n\n    array = np.asarray([int(x) for x in rle.split()])\n    starts = array[0::2]-1 # 0 origin every 2\n    lengths = array[1::2]  # 1 origin every 2    \n    for index, start in enumerate(starts):\n        mask[int(start):int(start+lengths[index])] = 1\n    \n    return mask.reshape( (height,width), order='F' ) # column order\n'''\ndef mask2rle(mask):\n    if np.sum(mask) == 0: return ''\n    ar = mask.flatten(order='F')\n    EncodedPixel = ''\n    l = 0\n    for i in range(len(ar)):\n        if ar[i] == 0:\n            if l > 0:\n                if EncodedPixel != '': EncodedPixel += ' '\n                EncodedPixel += str(st+1)+' '+str(l)\n                l = 0\n        else: # == 1\n            if l == 0: st = i\n            l += 1\n    return EncodedPixel\n'''\ndef mask2rle(mask):\n    if np.sum(mask) == 0: return ''\n    pixels = mask.flatten(order='F')\n    pixels = np.concatenate(([0],pixels,[0]),axis=None) #　add 0 at both ends\n    runs = np.where(pixels[1:] != pixels[:-1])[0]+1 # get change index as array([...],) \n    runs[1::2] -= runs[::2] # get length\n    return ' '.join(str(x) for x in runs) # blank is inserted between str(x)\n\ndef mask2contour(mask, width=3):\n    # CONVERT MASK TO ITS CONTOUR\n    w = mask.shape[1]\n    h = mask.shape[0]\n    mask2 = np.concatenate([mask[:,width:],np.zeros((h,width))],axis=1)\n    mask2 = np.logical_xor(mask,mask2)\n    mask3 = np.concatenate([mask[width:,:],np.zeros((width,w))],axis=0)\n    mask3 = np.logical_xor(mask,mask3)\n    return np.logical_or(mask2,mask3) \n\ndef mask2pad(mask, pad=2):\n    # ENLARGE MASK TO INCLUDE MORE SPACE AROUND DEFECT\n    w = mask.shape[1]\n    h = mask.shape[0]\n    \n    # MASK UP\n    for k in range(1,pad,2):\n        temp = np.concatenate([mask[k:,:],np.zeros((k,w))],axis=0)\n        mask = np.logical_or(mask,temp)\n    # MASK DOWN\n    for k in range(1,pad,2):\n        temp = np.concatenate([np.zeros((k,w)),mask[:-k,:]],axis=0)\n        mask = np.logical_or(mask,temp)\n    # MASK LEFT\n    for k in range(1,pad,2):\n        temp = np.concatenate([mask[:,k:],np.zeros((h,k))],axis=1)\n        mask = np.logical_or(mask,temp)\n    # MASK RIGHT\n    for k in range(1,pad,2):\n        temp = np.concatenate([np.zeros((h,k)),mask[:,:-k]],axis=1)\n        mask = np.logical_or(mask,temp)\n    \n    return mask \n\nrle = train2.iloc[0]['e1']\nprint(rle)\nmask = rle2mask(rle)\nprint(mask.shape)\n\n#mask = np.array([[1,1,1],\n#               [1,0,0]])\nrle1 = mask2rle(mask)\nprint(rle == rle1)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Analyze\nDice coefficient is written as $\\frac{|A\\cap B|}{(|A|\\cup|B|)/2}$.Without defect it is written and defined as $\\frac{|\\phi\\cap \\phi|}{(|\\phi|\\cup|\\phi|)/2} = 1$.When we predict some defect of the no defect material, Dice coefficient is $\\frac{|A\\cap \\phi|}{(|A|\\cup\\phi)/2}=0$. That means  a little mistake prediction of no-defect material makes maximum reduce Dice coefficient.We have to take away this risk.  \nTraining minimum defect pixels are e1 163, e2 316, e3 115 e4 491, it is better to make prediction no defects under these minimum.   \nSomeone recomended that at the first step we classify no-defects and defects and at the second step we do segmentation. "},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ndf('e1','e2','e3','e4') added ('pixels_e1','pixels_e2',...)\n\"\"\"\ndef defects_rate(df):\n    min_pixels = []\n    total = len(df)\n    print('Total:', total)\n    for j in range(4):\n        pixels_e = 'pixels_e'+str(j+1)\n        defects_e = 'e'+str(j+1)\n        df[pixels_e] = [np.sum(rle2mask(x)) for x in df[defects_e]]\n        defects = sum(df[pixels_e] != 0)\n        if defects > 0:\n            defects_min = min(df[df[pixels_e] > 0][pixels_e])\n        else:\n            defects_min = 0\n        min_pixels.append(defects_min)\n        defects_max = max(df[pixels_e])\n        print(defects_e,':',defects, '({:.3%})'.format(defects/total),\n              'max:',defects_max,'min:',defects_min)\n    return min_pixels\nif mode == 'train' or mode == 'test':\n    min_pixels = defects_rate(train2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Generator"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import keras\n\nclass DataGenerator(keras.utils.Sequence):\n    def __init__(self, df, batch_size = 32, subset=\"train\", shuffle=False, \n                 preprocess=None, rate=0.0, flipH=False, flipV=False, divide=1):\n        super().__init__()\n        self.df = df\n        self.shuffle = shuffle\n        self.subset = subset\n        self.batch_size = batch_size\n        self.preprocess = preprocess\n        self.IDs = {}\n        self.rate = rate\n        self.flipH = flipH\n        self.flipV = flipV\n        self.length = int(len(self.df)*divide)\n        self.batches = int(np.ceil(self.length/ self.batch_size)) # round up\n#        self.res = len(self.df) // self.batch_size\n        self.divide = int(divide)\n        self.H = 256\n        self.W = int(1600/divide)\n        \n        if self.subset == \"train\" or self.subset == \"classify\":\n            self.data_path = path + 'train_images/'\n        elif self.subset == \"test\":\n            self.data_path = path + 'test_images/'\n        self.on_epoch_end()\n\n    def __len__(self):\n        return self.batches\n#        return int(np.floor(len(self.df) / self.batch_size))\n    \n    def on_epoch_end(self):\n#        self.indexes = np.arange(len(self.df))\n        self.indexes = np.arange(self.length)\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n    \n    def __getitem__(self, index):\n        st = self.batch_size*index\n        en = self.batch_size*(index+1)\n        if en > self.length: en = self.length\n        X = np.empty((en-st,self.H,self.W,3),dtype=np.float32)\n        if self.subset == 'train':\n            y = np.empty((en-st,self.H,self.W,4),dtype=np.uint8)\n        elif self.subset == 'classify':\n            c = np.zeros((en-st),dtype=np.uint8)\n\n        indexes = self.indexes[st:en]\n#        for i,f in enumerate(self.df['ImageId'].iloc[indexes]):\n        for i, ind in enumerate(indexes):\n            f_ind = ind // self.divide\n            f_div = ind % self.divide\n#            print(f_ind,f_div)\n            f = self.df['ImageId'].iloc[f_ind]\n            if self.divide ==1:\n#                self.IDs[index*self.batch_size+i]=f\n                self.IDs[i]=f\n            else:\n#                self.IDs[index*self.batch_size+i]=f+'_'+str(f_div)\n                self.IDs[i]=f+'_'+str(f_div)\n            img = Image.open(self.data_path + f)\n#            print(img.size)\n#            assert img.size == (1600,256) # (W,H)\n            x = img.resize((1600,256))\n            x = np.array(x,dtype=np.float32) # (W,H)->(H,W)\n#            print(x.shape)\n            x = x[:,f_div*self.W:(f_div+1)*self.W]\n#            print(x.shape)\n            if self.rate > 0.0:\n                if np.random.rand() <= self.rate and self.flipV == True: flipV = True\n                else: flipV = False\n                    \n                if np.random.rand() <= self.rate and self.flipH == True: flipH = True\n                else: flipH = False\n            else:\n                flipV = self.flipV\n                flipH = self.flipH\n                    \n            if flipV == True: x = x[:,::-1]\n            if flipH == True: x = x[::-1,:]\n            X[i,] = x\n#            print(\"H:\",flipH, \"V:\",flipV)\n            if self.subset == 'train': \n                for j in range(4):\n#                    m = rle2mask(self.df['e'+str(j+1)].iloc[indexes[i]])\n                    m = rle2mask(self.df['e'+str(j+1)].iloc[f_ind])\n                    m = m[:,f_div*self.W:(f_div+1)*self.W]\n                    if flipV == True: m = m[:,::-1]\n                    if flipH == True: m = m[::-1,:]\n                    y[i,:,:,j] = m\n            elif self.subset == 'classify':\n                for j in range(4):\n                    m = rle2mask(self.df['e'+str(j+1)].iloc[f_ind])\n                    m = m[:,f_div*self.W:(f_div+1)*self.W]\n                    if np.sum(m) != 0:\n                        c[i]= 1\n                        break\n              \n        if self.preprocess!=None: X = self.preprocess(X)\n        if self.subset == 'train': return X,y\n        elif self.subset == 'classify':return X,c\n        else: return X\n\n#        if self.subset == 'train': return X.reshape((-1,256,400,3)),y.reshape((-1,256,400,4))\n#        else: return X.reshape((-1,256,400,3)) #? flip every part ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#\n#  Generator check\n#\ndef plot_imageMask_predMask(X, y=None, preds=None, IDs=None):\n    print('KEY: yellow=defect1, green=defect2, blue=defect3, magenta=defect4')\n    \n    def make_image_masked(img, mask, extra):\n        for j in range(4):\n            msk = mask[:,:,j]\n            msk = mask2pad(msk,pad=3)\n            msk = mask2contour(msk,width=3)\n            if np.sum(msk)!=0: extra += ' '+str(j+1)+'('+str(np.sum(msk))+')'\n            if j==0: # yellow\n                img[msk==1,0] = 235 \n                img[msk==1,1] = 235\n            elif j==1: img[msk==1,1] = 210 # green\n            elif j==2: img[msk==1,2] = 255 # blue\n            elif j==3: # magenta\n                img[msk==1,0] = 255\n                img[msk==1,2] = 255\n        return img, extra        \n\n    batch_size = X.shape[0]\n    # DISPLAY IMAGES WITH DEFECTS\n#    plt.figure(figsize=(14,50)) #20,18\n    if batch_size != 1:\n        if preds is None:\n            fig, ax = plt.subplots(batch_size, 1, figsize=(14,3*batch_size))#(14,50))\n        else:\n            if preds.dtype != 'uint8':\n                fig, ax = plt.subplots(batch_size, 5, figsize=(18,1*batch_size))#(20,36))\n            else:\n                fig, ax = plt.subplots(batch_size, 2, figsize=(14,2*batch_size))#(20,36))\n    else:\n        plt.figure(figsize=(14,50)) #20,18\n        \n    for k in range(batch_size):\n#        plt.subplot(batch_size,1,k+1)\n        img = X[k,]\n        img = Image.fromarray(img.astype('uint8'))\n        img = np.array(img)\n        extra = ''\n        if y is not None:\n            if np.sum(y[k]) != 0: extra = '  has defect'\n            img_y = np.copy(img)\n            img_y, extra_y = make_image_masked(img_y,y[k],extra)\n        else:\n            img_y = np.copy(img)\n            extra_y = ''\n                    \n        if batch_size != 1:\n            if preds is None:\n                if IDs is not None:ax[k].set_title(IDs[k] + extra_y)\n                ax[k].axis('off')\n                ax[k].imshow(img_y)\n            else:\n                if IDs is not None:ax[k,0].set_title(IDs[k] + extra_y)\n                ax[k,0].axis('off')\n                ax[k,0].imshow(img_y)\n        else:\n            if IDs is not None:plt.title(IDs[k]+extra_y)\n            plt.axis('off') \n            plt.imshow(img_y)           \n            \n        # prediction\n        if preds is not None:\n            if preds.dtype != 'uint8':\n                for j in range(4):\n                    ax[k,j+1].set_title(\" max pixel({:.3f})\"\n                                    .format(np.max(preds[k,:,:,j])))\n                    ax[k,j+1].axis('off')\n                    ax[k,j+1].imshow(preds[k,:,:,j])\n            else:\n                img_p = img.copy()\n                img_p, extra_p = make_image_masked(img_p,preds[k],'')\n                if IDs is not None:ax[k,1].set_title(IDs[k] + extra_p)\n                ax[k,1].axis('off')\n                ax[k,1].imshow(img_p)\n            \n    plt.show()\n\nprint(\"Classify Data\")\nclassify_check = DataGenerator(train2[:2],subset='classify')\n(X,c) = classify_check[0]\nprint(c)\nprint(\"Classify Data-2\")\nclassify_check = DataGenerator(train2[:2],subset='classify',divide=2)\n(X,c) = classify_check[0]\nprint(c)\n\nprint(\"Test Data\")\ntest_check = DataGenerator(test2[:2],subset='test')\nX = test_check[0]\nprint(X.shape[0])\nprint(test_check.indexes)\nprint(test_check.IDs)\nplot_imageMask_predMask(X[:2], IDs=test_check.IDs)\nprint(\"Test Data-2\")\ntest_check = DataGenerator(test2[:2],subset='test',divide=2)\nX = test_check[0]\nprint(X.shape[0])\nprint(test_check.indexes)\nprint(test_check.IDs)\nplot_imageMask_predMask(X[:4], IDs=test_check.IDs)\n\nprint(\"Train Data\")\ndata_check = DataGenerator(train2[:2])\n(X,y) = data_check[0]\nprint(X.shape[0])\nprint(data_check.indexes)\nprint(data_check.IDs)\n#print(train2['ImageId'].iloc[data_check.indexes])\n\nfor i, (X, y) in enumerate(data_check):\n    print(i, X.shape[0])\nprint(data_check.IDs)\n    \nplot_imageMask_predMask(X[:2], y, IDs=data_check.IDs)\n\nprint(\"Train Data-2\")\ndata_check = DataGenerator(train2[:2],divide=2)\n(X,y) = data_check[0]\nprint(data_check.IDs)\n    \nplot_imageMask_predMask(X[:4], y, IDs=data_check.IDs)\n\n\nprint(\"flipH\")\ndata_check = DataGenerator(train2[:2], rate=0.5, flipH=True)\n(X,y) = data_check[0]\nplot_imageMask_predMask(X[:2], y, IDs=data_check.IDs)\n\nprint(\"flipH-2\")\ndata_check = DataGenerator(train2[:2], divide=2, rate=0.5, flipH=True)\n(X,y) = data_check[0]\nplot_imageMask_predMask(X[:4], y, IDs=data_check.IDs)\n\n\nprint(\"flipV\")\ndata_check = DataGenerator(train2[:2], rate=0.5, flipV=True)\n(X,y) = data_check[0]\nplot_imageMask_predMask(X[:2], y, IDs=data_check.IDs)\nprint(\"flipV-2\")\ndata_check = DataGenerator(train2[:2], divide=2, rate=0.5, flipV=True)\n(X,y) = data_check[0]\nplot_imageMask_predMask(X[:4], y, IDs=data_check.IDs)\n\n\nprint(\"flipH,flipV\")\ndata_check = DataGenerator(train2[:2],rate=0.5, flipH=True, flipV=True)\n(X,y) = data_check[0]\nplot_imageMask_predMask(X[:2], y, IDs=data_check.IDs)\nprint(\"flipH,flipV-2\")\ndata_check = DataGenerator(train2[:2], divide=2, rate=0.5, flipH=True, flipV=True)\n(X,y) = data_check[0]\nplot_imageMask_predMask(X[:4], y, IDs=data_check.IDs)\n\n\nprint(\"y,preds\")\nplot_imageMask_predMask(X[:8], y, y, IDs=data_check.IDs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dice coefficients"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"from keras import backend as K\n\nfrom keras import losses\n\n# COMPETITION METRIC\n# Variable\ndef dice_coef(y_true, y_pred, smooth=1):\n#    print(\"dice_coef:\",y_true.shape,y_pred.shape,y_true.dtype,y_pred.dtype)\n#    raise Exception\n#    assert y_true.shape[0] == y_pred.shape[0]\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2.*intersection+smooth)/(K.sum(y_true_f)+K.sum(y_pred_f)+smooth)\n\n# ndarray\ndef dice_coef_nd(y_true, y_pred):\n    y_true_f = y_true.flatten()\n    y_pred_f = y_pred.flatten()\n    intersection = np.sum(y_true_f * y_pred_f)\n    return 2.*intersection, (np.sum(y_true_f)+np.sum(y_pred_f))\n\ndef dice_loss(y_true,y_pred):\n    return K.constant(1.0) - dice_coef(y_true,y_pred)\n\ndef bce_dice_loss(y_true, y_pred):\n    loss = losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{},"cell_type":"markdown","source":"## Classify"},{"metadata":{"trusted":true},"cell_type":"code","source":"#from keras.applications.vgg16 import VGG16,preprocess_input\nfrom keras.applications.vgg19 import VGG19,preprocess_input\n#from keras.applications.densenet import DenseNet121,preprocess_input\n#from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\nfrom keras.models import Model\nfrom keras.layers import Dense, GlobalAveragePooling2D, Dropout\nfrom keras import backend as K\n\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nimport dill\n\ndef make_classify_model():\n    # create the base pre-trained model\n#    base_model = VGG16(weights='imagenet', include_top=False)\n    base_model = VGG19(weights='imagenet', include_top=False)\n#    base_model = DenseNet121(weights='imagenet', include_top=False)\n#    base_model = InceptionResNetV2(weights='imagenet', include_top=False)\n    # add a global spatial average pooling layer\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    # let's add a fully-connected layer\n    x = Dropout(0.3)(x)\n    x = Dense(128, activation='relu')(x)\n    # and a logistic layer -- let's say we have 2-values\n    x = Dropout(0.3)(x)\n    predictions = Dense(1, activation='sigmoid')(x)\n\n    # this is the model we will train\n    classify_model = Model(inputs=base_model.input, outputs=predictions)\n\n    # first: train only the top layers (which were randomly initialized)\n    # i.e. freeze all convolutional InceptionV3 layers\n    for layer in base_model.layers:\n        layer.trainable = False\n    # compile the model (should be done *after* setting layers to non-trainable)\n\n    classify_model.summary()\n    \n    return classify_model, preprocess_input\n\ndef train_classify_model(model,train_set,val_set,preprocess=None,epochs=30):\n    train_batches = DataGenerator(train_set,shuffle=True,preprocess=preprocess,\n                                  rate=0.5, flipH=True, flipV=True, subset='classify')\n    valid_batches = DataGenerator(val_set,preprocess=preprocess,subset='classify')\n    \n    f = open(\"classify_preprocess.dill\",\"wb\")\n    dill.dump(preprocess,f)\n    f.close\n\n    history = model.fit_generator(train_batches,\n                              validation_data = valid_batches, \n                              epochs = epochs, \n                              verbose=1,\n                              callbacks=[EarlyStopping(patience=4,min_delta=1.e-6),\n                                        ReduceLROnPlateau(monitor='val_acc', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001),\n                                         ModelCheckpoint(\"classify_model.h5\", \n                                                         monitor='val_acc', \n                                                         verbose=1, \n                                                         save_best_only=True, \n                                                         save_weights_only=False, \n                                                         mode='auto', period=1)])\n    plt.figure(figsize=(15,5))\n    plt.plot(range(history.epoch[-1]+1),history.history['val_acc'],label='val_acc')\n    plt.plot(range(history.epoch[-1]+1),history.history['acc'],label='acc')\n    plt.title('Training Accuracy'); plt.xlabel('Epoch'); plt.ylabel('Accuracy');plt.legend(); \n    plt.show()\n\nif mode == 'train':\n    (classify_model, classify_preprocess) = make_classify_model()\n    classify_model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['acc'])\n    classify_history = train_classify_model(classify_model,train_set,val_set,\n                                        preprocess=classify_preprocess)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Segmentation"},{"metadata":{},"cell_type":"markdown","source":"### Download UNET\n"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"if mode == 'train':\n    #!pip install segmentation-models\n    #!pip install git+https://github.com/qubvel/segmentation_models\n    !pip download --no-binary :all: segmentation-models==0.2.1\n    !pip download --no-deps --no-binary :all: image-classifiers==0.2.0\n    !pip install segmentation_models-0.2.1.tar.gz\n    !pip install image_classifiers-0.2.0.tar.gz\nelse:\n    !pip install $path_self/segmentation_models-0.2.1.tar.gz\n    !pip install $path_self/image_classifiers-0.2.0.tar.gz\n\n!ls -l ../input\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Segmentation model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from segmentation_models import Unet, PSPNet\nfrom segmentation_models.backbones import get_preprocessing\n#from segmentation_models.losses import  DiceLoss\nfrom segmentation_models.losses import bce_jaccard_loss#, bce_dice_loss\nfrom segmentation_models.metrics import iou_score\n\nimport dill\n\nbackbone = 'vgg19' #'resnet50'#'vgg16' #'resnet34' \nMODEL = Unet\n\n# LOAD UNET WITH PRETRAINING FROM IMAGENET\ndef make_model(MODEL, backbone):\n#preprocess = get_preprocessing('resnet34') # for resnet, img = (img-110.0)/1.0\n    preprocess = get_preprocessing(backbone)\n    model = MODEL(backbone, freeze_encoder=True,encoder_weights='imagenet',  \n             input_shape=(256, 1600, 3), \n                  classes=4, activation='sigmoid')\n#    model.compile(optimizer='adam', loss=bce_dice_loss, metrics=[dice_coef])\n\n    #trainable = False\n    #for i, layer in enumerate(model.layers):\n    #    if 'stage4' in layer.name: trainable = True\n    #    layer.trainable = trainable\n    #    print(layer.name, layer.trainable)\n    model.summary()\n    \n\n    \n    return model, preprocess\n\nif mode == 'train':\n    model, preprocess = make_model(MODEL, backbone)\n    print(preprocess)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"#from keras.utils import plot_model\n#plot_model(model, to_file='./model.png')\n#!ls -l\n\n\n#from IPython.display import SVG\n#from keras.utils.vis_utils import model_to_dot\n\n#SVG(model_to_dot(model).create(prog='dot', format='svg')) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TRAIN AND VALIDATE MODEL\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nimport dill\n\ndef train(model,preprocess,train_set,val_set,epochs=30):\n    train_batches = DataGenerator(train_set,shuffle=True,preprocess=preprocess,\n                                  rate=0.5, flipH=True, flipV=True,batch_size=10)\n    valid_batches = DataGenerator(val_set,preprocess=preprocess,batch_size=10)\n    \n    f = open(\"preprocess.dill\",\"wb\")\n    dill.dump(preprocess,f)\n    f.close\n\n\n    history = model.fit_generator(train_batches,\n                              validation_data = valid_batches, \n                              epochs = epochs, \n                              verbose=1,\n                              callbacks=[EarlyStopping(patience=5,min_delta=1.e-6),\n                                        ReduceLROnPlateau(monitor='val_dice_coef', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001),\n                                        ModelCheckpoint(\"UNET.h5\", \n                                                        monitor='val_dice_coef', \n                                                        verbose=1, \n                                                        save_best_only=True, \n                                                        save_weights_only=False, \n                                                        mode='max', period=1)])\n    plt.figure(figsize=(15,5))\n    plt.plot(range(history.epoch[-1]+1),history.history['val_dice_coef'],label='val_dice_coef')\n    plt.plot(range(history.epoch[-1]+1),history.history['dice_coef'],label='trn_dice_coef')\n    plt.title('Training Accuracy'); plt.xlabel('Epoch'); plt.ylabel('Dice_coef');plt.legend(); \n    plt.show()\nif mode == 'train':\n    \n#    print(\"e2\")\n#    train(model,train_e2_set,val_e2_set, epochs=5)\n    \n#    print(\"e2+e4\")\n#    model.compile(optimizer='adam', loss=bce_dice_loss, metrics=[dice_coef])\n#    train(model,\n#          pd.concat([train_e2_set,train_e4_set]),\n#          pd.concat([val_e2_set,val_e4_set]),\n#          epochs=5)\n\n#    print(\"e2+e4+e1\")\n#    model.compile(optimizer='adam', loss=bce_dice_loss, metrics=[dice_coef])\n#    train(model,\n#          pd.concat([train_e2_set,train_e4_set,train_e1_set]),\n#          pd.concat([val_e2_set,val_e4_set,val_e1_set]),\n#          epochs=5)\n\n#    print(\"e2+e4+e1+e3\")\n#    model.compile(optimizer='adam', loss=bce_dice_loss, metrics=[dice_coef])\n#    train(model,\n#          pd.concat([train_e2_set,train_e4_set,train_e1_set,train_e3_set]),\n#          pd.concat([val_e2_set,val_e4_set,val_e1_set,val_e3_set]),\n#          epochs=5)\n    \n#    print(\"Total\")\n#    model.compile(optimizer='adam', loss=bce_dice_loss, metrics=[dice_coef])    \n#    train(model,train_set,val_set)\n\n    print(\"e2+e4+e1+e3\")\n    model.compile(optimizer='adam', loss=bce_dice_loss, metrics=[dice_coef])\n    train(model,preprocess,\n          pd.concat([train_e2_set,train_e4_set,train_e1_set,train_e3_set]),\n          pd.concat([val_e2_set,val_e4_set,val_e1_set,val_e3_set]) )\n\n#    print(K.eval(model.optimizer.lr))\n#print(model.optimizer.get_config())\n#pd.concat([val_e2_set, val_e4_set])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load object"},{"metadata":{"trusted":true},"cell_type":"code","source":"# LOAD MODEL\nfrom keras.models import load_model\nimport dill\n\ndef load_objects(load_path): \n\n    model = load_model(load_path+'UNET.h5',\n                custom_objects={'dice_coef':dice_coef,\n                                'bce_dice_loss':bce_dice_loss})\n    f = open(load_path+\"preprocess.dill\",\"rb\")\n    preprocess = dill.load(f)\n    f.close\n    print(preprocess)\n        \n    classify_model = load_model(load_path +'classify_model.h5')\n    f = open(load_path+\"classify_preprocess.dill\",\"rb\")\n    classify_preprocess = dill.load(f)\n    f.close\n    print(classify_preprocess)\n\n    return classify_model, classify_preprocess, model, preprocess\n\n# LOAD model, preprocess\nif mode == 'train':\n    classify_model, classify_preprocess, model, preprocess = load_objects('./')\nelse:\n    classify_model, classify_preprocess, model, preprocess = load_objects(path_self)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluate"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef prob2mask(prob,threshold):\n    mask = np.zeros(prob.shape,dtype='uint8')\n    mask[prob>=threshold] = 1\n    return mask\n\ndef filter_defects(val_set,classify_preprocess, classify_model):\n    ind =[]\n    cls = []\n    B = DataGenerator(val_set,batch_size=32,\n                      preprocess=classify_preprocess,subset='classify')\n    for X, c in B:\n        f = classify_model.predict(X,batch_size=X.shape[0])\n        f = f.reshape(-1)\n        ind.extend(f>0.5)\n        cls.extend(c)\n    return np.array(ind), np.array(cls)\n\n#defects, cls = filter_defects(val_set,classify_preprocess, classify_model)\n#for i in range(10):\n#    print( i, \" \", defects[i], \" \", cls[i])\n#pred_real = defects.astype(np.int) - cls.astype(np.int)\n#print(\"Total:\", len(pred_real),\n#      \"pred>real(defect<-no)\",np.sum(pred_real>0),\n#      \"pred == real\",np.sum(pred_real==0),\n#      \"pred<real(no<-defect)\",np.sum(pred_real<0))\n\ndef evaluate_batch(val_set, preprocess, M, threshold, divide=1):\n    \n    B = DataGenerator(val_set,batch_size=64,preprocess=preprocess,divide=divide)\n\n    total = 0\n    defects = np.zeros(4,dtype='uint')\n    pixel_min = np.zeros(4,dtype='uint') \n    pixel_max = np.zeros(4,dtype='uint')\n    fact = 0.0\n    denom = 0.0\n    for X, y in B:\n        if defects is None: defects = np.zeros((X.shape[1],X.shape[2]))\n        total += X.shape[0]\n        p = M.predict(X,batch_size=X.shape[0])\n#        print(p.shape,p[0,10,10,:])\n#        plot_imageMask_predMask(X,y,p,IDs=B.IDs)\n        \n#        XH = X[:,:,::-1]\n#        pH = M.predict(XH,batch_size=XH.shape[0])\n#        plot_imageMask_predMask(XH,y[:,:,::-1],pH,IDs=B.IDs)\n        \n#        XV = X[:,::-1,:]\n#        pV = M.predict(XV,batch_size=XV.shape[0])\n#        plot_imageMask_predMask(XV,y[:,::-1,:],pH,IDs=B.IDs)\n        \n        XHV = X[:,::-1,::-1]\n        pHV = M.predict(XHV,batch_size=XHV.shape[0])\n#        plot_imageMask_predMask(XHV,y[:,::-1,::-1],pHV,IDs=B.IDs)\n#        break\n        \n#        pH = pH[:,:,::-1]\n#        pV = pV[:,::-1,:]\n        pHV= pHV[:,::-1,::-1]\n        \n#        preds = (p + pH + pV + pHV)/4.0\n        preds = (p+pHV)/2.0\n\n        mask = prob2mask(preds,threshold)\n\n        for i in range(X.shape[0]):\n            for j in range(4):\n                pixel = np.sum(mask[i,:,:,j],dtype='uint')\n                if pixel < min_pixels[j]:\n                    mask[i,:,:,j] = 0\n                    pixel = 0 \n                if pixel > 0:\n                    if pixel_min[j] == 0: pixel_min[j] = pixel\n                    elif pixel < pixel_min[j]: pixel_min[j] = pixel\n                \n                if pixel > pixel_max[j]: pixel_max[j] = pixel\n                if pixel > 0:defects[j] += 1\n#                if pixel > 0:print(i,j,defects[j],pixel_min[j],pixel_max[j])\n        \n        (f, d) = dice_coef_nd(y,mask)\n        fact += f\n        denom += d\n    return fact, denom,total,defects,pixel_min,pixel_max\n\n\n# PREDICT FROM VALIDATION SET (USE ALL)\nif mode == 'train':\n    defects = sum(val_set['count']!=0)\n    print(\"real defect rate:{:.3}={}/{}\".format(defects/len(val_set), defects, len(val_set)))\n\n    filter_defects, cls = filter_defects(val_set,classify_preprocess, classify_model)\n    pred_real = filter_defects.astype(np.int) - cls.astype(np.int)\n    filter_total = np.sum(filter_defects==False)\n    filter_mistake_defect = np.sum(pred_real>0)\n    filter_mistake_no = np.sum(pred_real<0)\n    defect_set = val_set[filter_defects]\n    # defect_set.index.values\n    fact, denom,total,defects,pixel_min,pixel_max = evaluate_batch(defect_set, #val_set,\n                                                                    preprocess,\n                                                                    model,\n                                                                    prob_threshold)\n    dice_coef_ = (fact+1.0)/(denom+1.0)   \n    print(\"filtered dice_coef:{:.3} total:{}\".format(dice_coef_, total))\n    for j in range(4):\n        print(\"e{}:{}({:.3%}) max:{} min:{}\".format(\n                j+1, defects[j], defects[j]/total, pixel_max[j], pixel_min[j]))\n\n# Weighted average is some indication\n    weighted_dice = (dice_coef_*(len(val_set)-filter_total)+filter_total)/len(val_set)\n    print(\"total dice_coef:{:.3} total:{}\".format(weighted_dice, len(val_set)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\ndef predict_batch(test2, preprocess, model, threshold, mini_pixels,\n                     classify_preprocess, classify_model ):\n    \n    B = DataGenerator(test2,batch_size=32,preprocess=None,subset='test')\n    \n    ImageId_ClassIds = []\n    EncodedPixels = []\n\n    for X in B:\n        #\n        # classify no-defect or defect\n        #\n        Y = classify_preprocess(X)\n        d = classify_model.predict(Y,batch_size=Y.shape[0])\n        \n        YH = Y[:,:,::-1]\n        dH = classify_model.predict(YH,batch_size=YH.shape[0])\n                \n        YV = Y[:,::-1,:]\n        dV = classify_model.predict(YV,batch_size=YV.shape[0])\n \n        YHV = Y[:,::-1,::-1]\n        dHV = classify_model.predict(YHV,batch_size=YHV.shape[0])\n\n        defects = (d+dH+dV+dHV)/4.0\n        \n        defects = defects.reshape(-1)\n        ind = defects > threshold\n\n        #\n        # classify and segment defect\n        #\n        Y = X[ind]\n        Y = preprocess(Y)\n        p = model.predict(Y,batch_size=Y.shape[0])\n        \n        YH = Y[:,:,::-1]\n        pH = model.predict(YH,batch_size=YH.shape[0])\n        \n        YV = Y[:,::-1,:]\n        pV = model.predict(YV,batch_size=YV.shape[0])\n        \n        YHV = Y[:,::-1,::-1]\n        pHV = model.predict(YHV,batch_size=YHV.shape[0])\n        \n        pH = pH[:,:,::-1]\n        pV = pV[:,::-1,:]\n        pHV= pHV[:,::-1,::-1]\n        \n        preds = (p + pH + pV + pHV)/4.0\n\n        mask = prob2mask(preds,threshold)\n        ii = 0\n        for i, defect in enumerate(ind):\n            ImageId = B.IDs[i]\n            for j in range(4):\n                Id = ImageId + '_'+str(j+1)\n                if defect == False:\n                    EncodedPixel = ''\n                else:\n                    if np.sum(mask[ii,:,:,j])<min_pixels[j]:\n                        EncodedPixel = ''\n                    else:\n                        EncodedPixel = mask2rle(mask[ii,:,:,j])\n                ImageId_ClassIds.append(Id)\n                EncodedPixels.append(EncodedPixel)\n            if defect == True: ii += 1\n                \n    return ImageId_ClassIds, EncodedPixels\n\nif mode == 'test' or mode == 'train':\n    start = time.time()\n    Ids, Encodeds = predict_batch(test2,\n                                  preprocess, model, 0.5, min_pixels,\n                                  classify_preprocess, classify_model)\n    end = time.time()\n    print(\"Time:\", end-start)\n    \n    \n    submission = pd.DataFrame({'ImageId_ClassId':Ids,\n                     'EncodedPixels':Encodeds})\n\n    submission.to_csv('submission.csv',index=False)\n#    submission[submission['EncodedPixels']!='']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Save objects for next step"},{"metadata":{"trusted":true},"cell_type":"code","source":"# SAVE MODEL(for the sake of test again)\nmodel.save('UNET.h5', include_optimizer=False)\nf = open(\"preprocess.dill\",\"wb\")\ndill.dump(preprocess,f)\nf.close\nclassify_model.save('classify_model.h5', include_optimizer=False)\nf = open(\"classify_preprocess.dill\",\"wb\")\ndill.dump(classify_preprocess,f)\nf.close\n# UNET\nif mode != 'train':\n    !cp $path_self/segmentation_models-0.2.1.tar.gz .\n    !cp $path_self/trainandtest/image_classifiers-0.2.0.tar.gz .\n\n!ls -l .","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}