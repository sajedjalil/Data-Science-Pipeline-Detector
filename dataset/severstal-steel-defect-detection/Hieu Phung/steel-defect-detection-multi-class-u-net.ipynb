{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import packages, modules, ..."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nimport os\nimport cv2\nimport h5py\nfrom datetime import datetime\nfrom tqdm import tqdm\n\nimport tensorflow as tf\nimport keras\nimport keras.backend as K\nimport keras.callbacks\nfrom keras.models import Model\nfrom keras.models import load_model\nfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Dropout, concatenate","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Utilities"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def mask2rle(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n \ndef rle2mask(mask_rle, shape=(1600,256)):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (width,height) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CSVGenerator(keras.utils.Sequence):\n    def __init__(\n        self,\n        dataset_dir,\n        batch_size=1,\n        group_method='random',  # one of 'none', 'random'\n        shuffle_groups=True\n    ):\n        \"\"\" Initialize Generator object.\n\n        Args\n            batch_size             : The size of the batches to generate.\n            group_method           : Determines how images are grouped together (defaults to 'ratio', one of ('none', 'random')).\n            shuffle_groups         : If True, shuffles the groups each epoch.\n        \"\"\"\n        self.batch_size = int(batch_size)\n        self.group_method = group_method\n        self.shuffle_groups = shuffle_groups\n\n        self.dataset_dir = dataset_dir\n        self.df = pd.read_csv(os.path.join(self.dataset_dir, 'train.csv'))\n        self.image_files = os.listdir(os.path.join(self.dataset_dir, 'train_images'))\n        self.image_ids = range(len(self.image_files))\n\n        # self.load_classes()\n\n        # Define groups\n        self.group_images()\n\n        # Shuffle when initializing\n        if self.shuffle_groups:\n            self.on_epoch_end()\n\n    def on_epoch_end(self):\n        if self.shuffle_groups:\n            random.shuffle(self.groups)\n\n    def size(self):\n        \"\"\" Size of the COCO dataset.\n        \"\"\"\n        return len(self.image_ids)\n\n    def load_image(self, image_index):\n        \"\"\" Load an image at the image_index.\n        \"\"\"\n\n        path = os.path.join(self.dataset_dir, 'train_images', self.image_files[image_index])\n\n#         with rasterio.open(path) as src:\n#             image = src.read().transpose(1, 2, 0)\n\n#         image = cv2.imread(path, 1)\n#         image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n        image = cv2.imread(path, 0)\n        image = image.reshape(256, 1600, 1)\n\n        return image\n\n    def load_mask(self, image_index):\n        \"\"\" Load mask for an image_index.\n        \"\"\"\n\n        # Search for annotations\n        labels = self.df[self.df['ImageId_ClassId'].str.contains(self.image_files[image_index])]['EncodedPixels']\n\n        # 5 classes: 0-background, 1-1st class, 2-2nd class, 3-3th class, 4-4th class\n        masks = np.zeros((256, 1600, 5), dtype=np.uint8)\n\n#         for idx, label in enumerate(labels.values):\n#             if label is not np.nan:\n#                 label = label.split(\" \")\n#                 positions = map(int, label[0::2])\n#                 length = map(int, label[1::2])\n#                 mask = np.zeros(256 * 1600, dtype=np.uint8)\n#                 for pos, le in zip(positions, length):\n#                     mask[pos:(pos + le)] = 1\n#                 masks[:, :, idx] = mask.reshape(256, 1600, order='F')\n        background = np.zeros((256, 1600), dtype=np.uint8)\n        for idx, rle in enumerate(labels.values):\n            if rle is not np.nan:\n                masks[:, :, idx+1] = rle2mask(rle)\n                background += masks[:, :, idx+1]\n                \n        np.clip(background, 0, 1, background)\n        background = np.ones((256, 1600), dtype=np.uint8) - background\n        masks[:, :, 0] = background\n\n        return masks\n\n    def load_image_group(self, group):\n        \"\"\" Load images for all images in a group.\n        \"\"\"\n        return np.array([self.load_image(image_index) for image_index in group])\n\n    def load_mask_group(self, group):\n        \"\"\" Load annotations for all images in group.\n        \"\"\"\n        mask_group = np.array([self.load_mask(image_index) for image_index in group])\n\n        return mask_group\n\n    def preprocess_group_entry(self, image):\n        \"\"\" Preprocess image.\n        \"\"\"\n        # preprocess the image\n        image = image / 255.\n\n        # convert to the wanted keras floatx\n        # image = keras.backend.cast_to_floatx(image)\n\n        return image\n\n    def preprocess_group(self, image_group):\n        \"\"\" Preprocess each image in its group.\n        \"\"\"\n        for index in range(len(image_group)):\n            # preprocess a single group entry\n            image_group[index] = self.preprocess_group_entry(image_group[index])\n\n        return image_group\n\n    def group_images(self):\n        \"\"\" Order the images according to self.order and makes groups of self.batch_size.\n        \"\"\"\n        # determine the order of the images\n        order = list(range(self.size()))\n        if self.group_method == 'random':\n            random.shuffle(order)\n\n        # divide into groups, one group = one batch\n        self.groups = [[order[x % len(order)] for x in range(i, i + self.batch_size)] for i in range(0, len(order), self.batch_size)]\n\n    def compute_input_output(self, group):\n        \"\"\" Compute inputs and target outputs for the network.\n        \"\"\"\n        # load images and annotations\n        image_group = self.load_image_group(group)\n        mask_group = self.load_mask_group(group)\n\n        assert(len(image_group) == len(mask_group))\n\n        # randomly transform data\n        # image_group, annotations_group = self.random_transform_group(image_group, annotations_group)\n\n        # perform preprocessing steps\n        image_group = self.preprocess_group(image_group)\n\n\n        return image_group, mask_group\n\n    def __len__(self):\n        \"\"\"\n        Number of batches for generator.\n        \"\"\"\n\n        return len(self.groups)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Keras sequence method for generating batches.\n        \"\"\"\n        group = self.groups[index]\n        inputs, targets = self.compute_input_output(group)\n\n        return inputs, targets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define losses"},{"metadata":{"trusted":true},"cell_type":"code","source":"def binary_focal_loss(gamma=2., alpha=.25):\n    \"\"\"\n    Binary form of focal loss.\n\n      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n\n      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n\n    References:\n        https://arxiv.org/pdf/1708.02002.pdf\n    Usage:\n     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n\n    \"\"\"\n    def binary_focal_loss_fixed(y_true, y_pred):\n        \"\"\"\n        :param y_true: A tensor of the same shape as `y_pred`\n        :param y_pred:  A tensor resulting from a sigmoid\n        :return: Output tensor.\n        \"\"\"\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n\n        epsilon = K.epsilon()\n        # clip to prevent NaN's and Inf's\n        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n\n        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n\n    return binary_focal_loss_fixed\n\n\ndef categorical_focal_loss(gamma=2., alpha=.25):\n    \"\"\"\n    Softmax version of focal loss.\n\n           m\n      FL = âˆ‘  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n          c=1\n\n      where m = number of classes, c = class and o = observation\n\n    Parameters:\n      alpha -- the same as weighing factor in balanced cross entropy\n      gamma -- focusing parameter for modulating factor (1-p)\n\n    Default value:\n      gamma -- 2.0 as mentioned in the paper\n      alpha -- 0.25 as mentioned in the paper\n\n    References:\n        Official paper: https://arxiv.org/pdf/1708.02002.pdf\n        https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy\n\n    Usage:\n     model.compile(loss=[categorical_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n    \"\"\"\n    def categorical_focal_loss_fixed(y_true, y_pred):\n        \"\"\"\n        :param y_true: A tensor of the same shape as `y_pred`\n        :param y_pred: A tensor resulting from a softmax\n        :return: Output tensor.\n        \"\"\"\n\n        # Scale predictions so that the class probas of each sample sum to 1\n        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n\n        # Clip the prediction value to prevent NaN's and Inf's\n        epsilon = K.epsilon()\n        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n\n        # Calculate Cross Entropy\n        cross_entropy = -y_true * K.log(y_pred)\n\n        # Calculate Focal Loss\n        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n\n        # Sum the losses in mini_batch\n        return K.sum(loss, axis=1)\n\n    return categorical_focal_loss_fixed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard_coef(y_true, y_pred):\n    smooth = 1e-12\n#     intersection = K.sum(y_true * y_pred, axis=[0, -1, -2])\n#     sum_ = K.sum(y_true + y_pred, axis=[0, -1, -2])\n    intersection = K.sum(y_true * y_pred, axis=[1, 2, 3])\n    sum_ = K.sum(y_true + y_pred, axis=[1, 2, 3])\n    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n    return K.mean(jac)\n\n\ndef jaccard_coef_int(y_true, y_pred):\n    smooth = 1e-12\n    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n#     intersection = K.sum(y_true * y_pred_pos, axis=[0, -1, -2])\n#     sum_ = K.sum(y_true + y_pred_pos, axis=[0, -1, -2])\n    intersection = K.sum(y_true * y_pred_pos, axis=[1, 2, 3])\n    sum_ = K.sum(y_true + y_pred_pos, axis=[1, 2, 3])\n    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n    return K.mean(jac)\n\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\n\ndef dice_coef(y_true, y_pred, smooth=1):\n    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n    return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define U-Net model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def unet(input_size=(256, 256, 6)):\n    kwargs = {\n        'activation': 'relu',\n        'padding': 'same',\n        'kernel_initializer': 'he_normal'\n    }\n\n    inputs = Input(input_size)\n    conv1 = Conv2D(64, 3, **kwargs)(inputs)\n    conv1 = Conv2D(64, 3, **kwargs)(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n    conv2 = Conv2D(128, 3, **kwargs)(pool1)\n    conv2 = Conv2D(128, 3, **kwargs)(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    conv3 = Conv2D(256, 3, **kwargs)(pool2)\n    conv3 = Conv2D(256, 3, **kwargs)(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    conv4 = Conv2D(512, 3, **kwargs)(pool3)\n    conv4 = Conv2D(512, 3, **kwargs)(conv4)\n    drop4 = Dropout(0.5)(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n\n    conv5 = Conv2D(1024, 3, **kwargs)(pool4)\n    conv5 = Conv2D(1024, 3, **kwargs)(conv5)\n    drop5 = Dropout(0.5)(conv5)\n\n    up6 = Conv2D(512, 2, **kwargs)(UpSampling2D(size=(2,2))(drop5))\n    merge6 = concatenate([drop4,up6], axis=3)\n    conv6 = Conv2D(512, 3, **kwargs)(merge6)\n    conv6 = Conv2D(512, 3, **kwargs)(conv6)\n\n    up7 = Conv2D(256, 2, **kwargs)(UpSampling2D(size=(2,2))(conv6))\n    merge7 = concatenate([conv3,up7], axis=3)\n    conv7 = Conv2D(256, 3, **kwargs)(merge7)\n    conv7 = Conv2D(256, 3, **kwargs)(conv7)\n\n    up8 = Conv2D(128, 2, **kwargs)(UpSampling2D(size=(2,2))(conv7))\n    merge8 = concatenate([conv2,up8], axis=3)\n    conv8 = Conv2D(128, 3, **kwargs)(merge8)\n    conv8 = Conv2D(128, 3, **kwargs)(conv8)\n\n    up9 = Conv2D(64, 2, **kwargs)(UpSampling2D(size=(2,2))(conv8))\n    merge9 = concatenate([conv1,up9], axis=3)\n    conv9 = Conv2D(64, 3, **kwargs)(merge9)\n    conv9 = Conv2D(64, 3, **kwargs)(conv9)\n    # conv9 = Conv2D(2, 3, **kwargs)(conv9)\n    conv10 = Conv2D(5, 1, activation='softmax')(conv9)\n\n    model = Model(inputs=inputs, outputs=conv10)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define callbacks"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LRTensorBoard(keras.callbacks.TensorBoard): # The callback that logs learning rate in TensorBoard\n    def __init__(self,\n        log_dir,\n        histogram_freq,\n        batch_size,\n        write_graph,\n        write_grads,\n        write_images,\n        embeddings_freq,\n        embeddings_layer_names,\n        embeddings_metadata\n    ):\n        super().__init__(\n            log_dir=log_dir,\n            histogram_freq=histogram_freq,\n            batch_size=batch_size,\n            write_graph=write_graph,\n            write_grads=write_grads,\n            write_images=write_images,\n            embeddings_freq=embeddings_freq,\n            embeddings_layer_names=embeddings_layer_names,\n            embeddings_metadata=embeddings_metadata\n        )\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs.update({'lr': K.eval(self.model.optimizer.lr)})\n        super().on_epoch_end(epoch, logs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training configuration"},{"metadata":{"trusted":true},"cell_type":"code","source":"snap_path = '../input/steel-defect-detection-multi-class-u-net/model.h5'\ndataset_dir = '../input/severstal-steel-defect-detection/'\nbatch_size = 4\nsteps_per_epoch = np.ceil(12568 / batch_size).astype(np.uint64)\n# validation_steps = args.validation_steps\nepochs = 5\nlr = 1e-4\ngamma = 2.0\nalpha = [0.1, 0.9, 0.9, 0.9, 0.9]\ninput_shape = (256, 1600, 1)\nnum_trainable_layers = 0 # All layers are trainable\noptimizer = 'Adam'\nthreads = 2\nlog_dir = './'\nsnap_dir = './'\nhistogram_freq = 0\nwrite_graph = False\nwrite_grads = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Start training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_callbacks(batch_size, log_dir, snap_dir, input_shape, histogram_freq, write_graph, write_grads):\n    tensorboard_callback = LRTensorBoard(\n        log_dir=log_dir,\n        histogram_freq=histogram_freq,\n        batch_size=batch_size,\n        write_graph=write_graph,\n        write_grads=write_grads,\n        write_images=False,\n        embeddings_freq=0,\n        embeddings_layer_names=None,\n        embeddings_metadata=None\n    )\n\n    reduce_lr_on_plateau_callback = keras.callbacks.ReduceLROnPlateau(\n        monitor='loss',\n        factor=0.1,\n        patience=2,\n        verbose=1,\n        mode='auto',\n        min_delta=0.0001,\n        cooldown=0,\n        min_lr=0\n    )\n\n    checkpointer = keras.callbacks.ModelCheckpoint(\n        os.path.join(snap_dir, 'model.h5'),\n        monitor='loss',\n        save_best_only=True,\n        verbose=1\n    )\n\n    return [tensorboard_callback, reduce_lr_on_plateau_callback, checkpointer]\n\n\n\n\n# Create snapshot directory if not exist\nif not os.path.exists(snap_dir):\n    os.makedirs(snap_dir)\n\n\n# Get generators\ntrain_generator = CSVGenerator(\n    dataset_dir,\n    batch_size=batch_size\n)\n\n\nif not snap_path:\n    # Train U-Net model from scratch\n    model = unet(input_shape)\nelse:\n    print('\\nLoading snapshot...')\n    model = load_model(\n        snap_path,\n        custom_objects={\n            'dice_coef': dice_coef,\n            'f1': f1,\n            'jaccard_coef': jaccard_coef,\n            'jaccard_coef_int': jaccard_coef_int,\n            'categorical_focal_loss_fixed': categorical_focal_loss(gamma=gamma, alpha=alpha)\n        }\n    )\n\n\n# Freeze all layers except TOP n layers, where n = num_trainable_layers\nfor layer in model.layers:\n    layer.trainable = True\nfor layer in model.layers[:-num_trainable_layers]:\n    layer.trainable = False\n\nprint('\\nThere are total {} layers'.format(len(model.layers)))\nfor i, layer in enumerate(model.layers[::-1]):\n    print('#{}: {} - '.format(i+1, layer.__class__.__name__), end='')\n    if layer.trainable:\n        print('trainable')\n    else:\n        print('untrainable')\n\n\n# Select optimizer\nif optimizer == 'adam':\n    optimizer = keras.optimizers.Adam\nelif optimizer == 'sgd':\n    optimizer = keras.optimizers.SGD\n\n\n# Compile the model\nmodel.compile(\n    optimizer=keras.optimizers.Adam(lr=lr),\n    loss=categorical_focal_loss(gamma=gamma, alpha=alpha),\n    metrics=[dice_coef, f1, jaccard_coef, jaccard_coef_int],\n)\n\n\n# Create callbacks\ncallbacks = create_callbacks(\n    batch_size,\n    log_dir,\n    snap_dir,\n    input_shape,\n    histogram_freq,\n    write_graph,\n    write_grads\n)\n\n\n# Train the model\nmodel.fit_generator(\n    generator=train_generator,\n    steps_per_epoch=steps_per_epoch,\n    epochs=epochs,\n    # validation_data=val_generator,\n    # validation_steps=validation_steps,\n    callbacks=callbacks,\n    use_multiprocessing=True,\n    workers=threads\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting configuration"},{"metadata":{"trusted":true},"cell_type":"code","source":"im_dir = r'../input/severstal-steel-defect-detection/test_images/'\nmodel_path = r'./model.h5'\nthreshold = 0.5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Start prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load model\nmodel = load_model(model_path, custom_objects={\n    'dice_coef': dice_coef,\n    'f1': f1,\n    'jaccard_coef': jaccard_coef,\n    'jaccard_coef_int': jaccard_coef_int,\n    'categorical_focal_loss_fixed': categorical_focal_loss()\n})\n\n# Get all image files\nall_items = os.listdir(im_dir)\nimage_files = [file for file in all_items if os.path.isfile(os.path.join(im_dir, file))]\n\nimageid_classid = []\nencodedpixels = []\nfor image_file in tqdm(image_files):\n    im_name = image_file.split('.')[0]\n    im_path = os.path.join(im_dir, image_file)\n\n    # Get image data\n#     image = cv2.imread(im_path, 1)\n#     image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n    image = cv2.imread(im_path, 0)\n    image = image.reshape(256, 1600, 1)\n    image = image / 255.\n    \n    # Make prediction\n    masks = model.predict(np.expand_dims(image, axis=0))[0]\n    masks = (masks >= threshold).transpose(2, 0, 1).astype(np.uint8)\n    \n    \n    # Save results\n    for i, mask in enumerate(masks[1:]):\n        if np.count_nonzero(mask) > 0:\n            rle = mask2rle(mask)\n        else:\n            rle = np.nan\n\n        imageid_classid.append('{}_{}'.format(image_file, i+1))\n        encodedpixels.append(rle)\n\n\ndf = pd.DataFrame({\n    'ImageId_ClassId': imageid_classid,\n    'EncodedPixels': encodedpixels\n})\n\ndf.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}