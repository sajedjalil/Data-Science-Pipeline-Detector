{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id=\"toc\"></a>\n# Table of Contents\n1. [Install MLComp library (offline version)](#install_mlcomp_library)\n1. [Import required libraries](#import_required_libraries)\n1. [Load models](#load_models)\n  1. [Load segmentation models](#load_segmentation_models)\n  1. [Load classification models](#load_classification_models)\n1. [Define models' mean aggregator](#define_models_mean_aggregator)\n1. [Create TTA transforms, datasets, loaders](#create_tta_etc)\n1. [Save predictions](#save_predictions)\n1. [Draw histogram of predictions](#draw_histogram_of_predictions)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"install_mlcomp_library\"></a>\n# Install MLComp library (offline version)\n[Back to Table of Contents](#toc)"},{"metadata":{},"cell_type":"markdown","source":"As the competition does not allow commit with the kernel that uses internet connection, we use offline installation"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!python ../input/mlcomp/mlcomp/mlcomp/setup.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir -p /tmp/pip/cache/\n!cp ../input/segmentation-models-zip-003/efficientnet_pytorch-0.4.0.xyz /tmp/pip/cache/efficientnet_pytorch-0.4.0.tar.gz\n!cp ../input/segmentation-models-zip-003/pretrainedmodels-0.7.4.xyz /tmp/pip/cache/pretrainedmodels-0.7.4.tar.gz\n!cp ../input/segmentation-models-zip-003/segmentation_models_pytorch-0.0.3.xyz /tmp/pip/cache/segmentation_models_pytorch-0.0.3.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --no-index --find-links /tmp/pip/cache/ efficientnet-pytorch\n!pip install --no-index --find-links /tmp/pip/cache/ segmentation-models-pytorch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir -p /tmp/.cache/torch/checkpoints/\n!cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b0-355c32eb.pth /tmp/.cache/torch/checkpoints/\n!cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b4-6ed6700e.pth /tmp/.cache/torch/checkpoints/\n!cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b5-b6417697.pth /tmp/.cache/torch/checkpoints/\n!cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b7-dcc49843.pth /tmp/.cache/torch/checkpoints/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/pretrainedmodels/pretrainedmodels-0.7.4/pretrainedmodels-0.7.4/ > /dev/null\npackage_path = '../input/senetunetmodelcode'\n\nimport sys\nsys.path.append(package_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"import_required_libraries\"></a>\n# Import required libraries\n[Back to Table of Contents](#toc)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport albumentations as A\nimport pandas as pd\nfrom tqdm import tqdm_notebook\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.jit import load\nimport torch.utils.data as data\n\nfrom mlcomp.contrib.transform.albumentations import ChannelTranspose\nfrom mlcomp.contrib.dataset.classify import ImageDataset\nfrom mlcomp.contrib.transform.rle import rle2mask, mask2rle\nfrom mlcomp.contrib.transform.tta import TtaWrap\n\nimport segmentation_models_pytorch as smp\nfrom senet_unet_model_code import Unet\nfrom efficientnet_pytorch import EfficientNet","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"load_models\"></a>\n# Load models\n[Back to Table of Contents](#toc)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"load_segmentation_models\"></a>\n## Load segmentation models\n[Back to Table of Contents](#toc)"},{"metadata":{"trusted":true},"cell_type":"code","source":"unet_efficientnet = smp.Unet(\"efficientnet-b5\", encoder_weights=\"imagenet\", classes=4, activation=\"sigmoid\").cuda()\nckpt_path = \"../input/unet-efficientnet-baseline/unet_efficientnet_model.pth\"\ndevice = torch.device(\"cuda\")\nunet_efficientnet.to(device)\nunet_efficientnet.eval()\nstate = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\nunet_efficientnet.load_state_dict(state[\"state_dict\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ckpt_path = \"../input/senetmodels/senext50_30_epochs_high_threshold.pth\"\ndevice = torch.device(\"cuda\")\nsenext50_30_epochs_high_threshold = Unet('se_resnext50_32x4d', encoder_weights=None, classes=4, activation=None).cuda()\nsenext50_30_epochs_high_threshold.to(device)\nsenext50_30_epochs_high_threshold.eval()\nstate = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\nsenext50_30_epochs_high_threshold.load_state_dict(state[\"state_dict\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unet_se_resnext50_32x4d = \\\n    load('/kaggle/input/severstalmodels/unet_se_resnext50_32x4d.pth').cuda()\nunet_mobilenet2 = load('/kaggle/input/severstalmodels/unet_mobilenet2.pth').cuda()\nunet_resnet34 = load('/kaggle/input/severstalmodels/unet_resnet34.pth').cuda()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"load_classification_models\"></a>\n## Load classification models\n[Back to Table of Contents](#toc)"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_resnet34 = load('/kaggle/input/severstalmodels/resnet34_classify.pth').cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ckpt_path = \"/kaggle/input/pytorch-multi-label-classification/efficientnet_b0_model.pth\"\ndevice = torch.device(\"cuda\")\nclf_efficientnet_b0 = EfficientNet.from_pretrained('efficientnet-b0', num_classes=4).cuda()\nclf_efficientnet_b0.to(device)\nclf_efficientnet_b0.eval()\nstate = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\nclf_efficientnet_b0.load_state_dict(state[\"state_dict\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ckpt_path = \"/kaggle/input/clf-efficientnet-b4/efficientnet_b4_model.pth\"\ndevice = torch.device(\"cuda\")\nclf_efficientnet_b4 = EfficientNet.from_pretrained('efficientnet-b4', num_classes = 4).cuda()\nclf_efficientnet_b4.to(device)\nclf_efficientnet_b4.eval()\nstate = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\nclf_efficientnet_b4.load_state_dict(state[\"state_dict\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ckpt_path = \"/kaggle/input/pytorch-multi-label-classification-effnet-b5/efficientnet_b5_model.pth\"\ndevice = torch.device(\"cuda\")\nclf_efficientnet_b5 = EfficientNet.from_pretrained('efficientnet-b5', num_classes=4).cuda()\nclf_efficientnet_b5.to(device)\nclf_efficientnet_b5.eval()\nstate = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\nclf_efficientnet_b5.load_state_dict(state[\"state_dict\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ckpt_path = \"/kaggle/input/pytorch-multi-label-classification-effnet-b7/efficientnet_b7_model.pth\"\ndevice = torch.device(\"cuda\")\nclf_efficientnet_b7 = EfficientNet.from_pretrained('efficientnet-b7', num_classes=4).cuda()\nclf_efficientnet_b7.to(device)\nclf_efficientnet_b7.eval()\nstate = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\nclf_efficientnet_b7.load_state_dict(state[\"state_dict\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"define_models_mean_aggregator\"></a>\n# Define models' mean aggregator\n[Back to Table of Contents](#toc)"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"class Model:\n    def __init__(self, models):\n        self.models = models\n    \n    def __call__(self, x):\n        res = []\n        x = x.cuda()\n        with torch.no_grad():\n            for m in self.models:\n                res.append(m(x))\n        res = torch.stack(res)\n        return torch.mean(res, dim=0)\n\nmodel = Model([\n    unet_se_resnext50_32x4d,\n    unet_mobilenet2,\n    unet_resnet34,\n    unet_efficientnet,\n    senext50_30_epochs_high_threshold\n])\nmodel_clf = Model([\n    clf_efficientnet_b0,\n    clf_efficientnet_b4,\n    clf_efficientnet_b5,\n    clf_efficientnet_b7,\n    clf_resnet34\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"create_tta_etc\"></a>\n# Create TTA transforms, datasets, loaders\n[Back to Table of Contents](#toc)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def create_transforms(additional):\n    res = list(additional)\n    # add necessary transformations\n    res.extend([\n        A.Normalize(\n            mean=(0.485, 0.456, 0.406), std=(0.230, 0.225, 0.223)\n        ),\n        ChannelTranspose()\n    ])\n    res = A.Compose(res)\n    return res\n\nimg_folder = '/kaggle/input/severstal-steel-defect-detection/test_images'\nbatch_size = 2\nnum_workers = 0\n\n# Different transforms for TTA wrapper\ntransforms = [\n    [],\n    [A.HorizontalFlip(p=1)]\n]\n\ntransforms = [create_transforms(t) for t in transforms]\ndatasets = [TtaWrap(ImageDataset(img_folder=img_folder, transforms=t), tfms=t) for t in transforms]\nloaders = [DataLoader(d, num_workers=num_workers, batch_size=batch_size, shuffle=False) for d in datasets]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def close(mask):\n    \"\"\"\n    Parameters:\n        mask: Input mask.\n\n    Returns:\n        closing: Output mask.\n    \"\"\"\n\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(5,5))\n    closing = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n    \n    return closing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loaders' mean aggregator"},{"metadata":{"trusted":true},"cell_type":"code","source":"thresholds = [0.5, 0.75, 0.5, 0.75]\nclf_thresholds = [0.5, 0.5, 0.75, 0.85]\nmin_area = [600, 1000, 1200, 2000]\n\nres = []\n# Iterate over all TTA loaders\ntotal = len(datasets[0])//batch_size\nfor loaders_batch in tqdm_notebook(zip(*loaders), total=total):\n    preds = []\n    image_file = []\n    features_no_tta = None\n    for i, batch in enumerate(loaders_batch):\n        features = batch['features'].cuda()\n        p = torch.sigmoid(model(features))\n        # inverse operations for TTA\n        p = datasets[i].inverse(p)\n        preds.append(p)\n        image_file = batch['image_file']\n        if i == 0:\n            features_no_tta = features\n    \n    # TTA mean\n    preds = torch.stack(preds)\n    preds = torch.mean(preds, dim=0)\n    preds = preds.detach().cpu().numpy()\n    \n    clf_preds = torch.sigmoid(model_clf(features_no_tta)).detach().cpu().numpy()\n    \n    # Batch post processing\n    for p, clf_pred, file in zip(preds, clf_preds, image_file):\n        file = os.path.basename(file)\n        # Image postprocessing\n        for i in range(4):\n            p_channel = p[i]\n            imageid_classid = file+'_'+str(i+1)\n            p_channel = (p_channel>thresholds[i]).astype(np.uint8)\n            if p_channel.sum() < min_area[i]:\n                p_channel = np.zeros(p_channel.shape, dtype=p_channel.dtype)\n                \n            # Remove false positives with classifier\n            if clf_pred[i] <= clf_thresholds[i]:\n                p_channel = np.zeros(p_channel.shape, dtype=p_channel.dtype)\n            else:\n                p_channel = close(p_channel)   \n                \n            res.append({\n                'ImageId_ClassId': imageid_classid,\n                'EncodedPixels': mask2rle(p_channel)\n            })","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"save_predictions\"></a>\n# Save predictions\n[Back to Table of Contents](#toc)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(res)\ndf = df.fillna('')\ndf.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"draw_histogram_of_predictions\"></a>\n# Draw histogram of predictions\n[Back to Table of Contents](#toc)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Image'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[0])\ndf['Class'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[1])\ndf['empty'] = df['EncodedPixels'].map(lambda x: not x)\ndf[df['empty'] == False]['Class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n\ndf = pd.read_csv('submission.csv')[:40]\ndf['Image'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[0])\ndf['Class'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[1])\n\nfor row in df.itertuples():\n    img_path = os.path.join(img_folder, row.Image)\n    img = cv2.imread(img_path)\n    mask = rle2mask(row.EncodedPixels, (1600, 256)) \\\n        if isinstance(row.EncodedPixels, str) else np.zeros((256, 1600))\n    if mask.sum() == 0:\n        continue\n    \n    fig, axes = plt.subplots(1, 2, figsize=(20, 60))\n    axes[0].imshow(img/255)\n    axes[1].imshow(mask*60)\n    axes[0].set_title(row.Image)\n    axes[1].set_title(row.Class)\n    plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"2d558d43f12745d8a826eb84f7dcd0b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"60b23d613e584fa0abb75460b84b1d80":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e3c6cad600e04ebfb71a6c202a8eb513","IPY_MODEL_d6bd450456d24af1b8f635c10fee4445"],"layout":"IPY_MODEL_a7f23b4d7ebb4499b486f861d1c52b77"}},"6feb39965873459cb61d32fbbc2c2225":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7f23b4d7ebb4499b486f861d1c52b77":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c389718f59fc45eba86cebc46dc72f99":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6bd450456d24af1b8f635c10fee4445":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c389718f59fc45eba86cebc46dc72f99","placeholder":"â€‹","style":"IPY_MODEL_dcff928e32c743529e899d038dee4e66","value":" 901/? [05:36&lt;00:00,  2.67it/s]"}},"dcff928e32c743529e899d038dee4e66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3c6cad600e04ebfb71a6c202a8eb513":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6feb39965873459cb61d32fbbc2c2225","max":900,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2d558d43f12745d8a826eb84f7dcd0b2","value":900}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":1}