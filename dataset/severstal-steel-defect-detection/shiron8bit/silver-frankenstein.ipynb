{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Install MLComp library(offline version):"},{"metadata":{},"cell_type":"markdown","source":"As the competition does not allow commit with the kernel that uses internet connection, we use offline installation"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"! python ../input/mlcomp/mlcomp/mlcomp/setup.py","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import required libraries"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport cv2\nimport albumentations as A\nfrom tqdm import tqdm_notebook\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.jit import load\nfrom albumentations import (Normalize, Compose)\nfrom albumentations.pytorch import ToTensor\nimport torch.utils.data as data\nfrom torch.nn import functional as F\n\nfrom mlcomp.contrib.transform.albumentations import ChannelTranspose\nfrom mlcomp.contrib.dataset.classify import ImageDataset\nfrom mlcomp.contrib.transform.rle import rle2mask, mask2rle\nfrom mlcomp.contrib.transform.tta import TtaWrap\n\nfrom tqdm import tqdm\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Codes from Heng's baseline\n# This code is for classifcation model\n\nBatchNorm2d = nn.BatchNorm2d\n\nIMAGE_RGB_MEAN = [0.485, 0.456, 0.406]\nIMAGE_RGB_STD  = [0.229, 0.224, 0.225]\n\n\n###############################################################################\nCONVERSION=[\n 'block0.0.weight',\t(64, 3, 7, 7),\t 'conv1.weight',\t(64, 3, 7, 7),\n 'block0.1.weight',\t(64,),\t 'bn1.weight',\t(64,),\n 'block0.1.bias',\t(64,),\t 'bn1.bias',\t(64,),\n 'block0.1.running_mean',\t(64,),\t 'bn1.running_mean',\t(64,),\n 'block0.1.running_var',\t(64,),\t 'bn1.running_var',\t(64,),\n 'block1.1.conv_bn1.conv.weight',\t(64, 64, 3, 3),\t 'layer1.0.conv1.weight',\t(64, 64, 3, 3),\n 'block1.1.conv_bn1.bn.weight',\t(64,),\t 'layer1.0.bn1.weight',\t(64,),\n 'block1.1.conv_bn1.bn.bias',\t(64,),\t 'layer1.0.bn1.bias',\t(64,),\n 'block1.1.conv_bn1.bn.running_mean',\t(64,),\t 'layer1.0.bn1.running_mean',\t(64,),\n 'block1.1.conv_bn1.bn.running_var',\t(64,),\t 'layer1.0.bn1.running_var',\t(64,),\n 'block1.1.conv_bn2.conv.weight',\t(64, 64, 3, 3),\t 'layer1.0.conv2.weight',\t(64, 64, 3, 3),\n 'block1.1.conv_bn2.bn.weight',\t(64,),\t 'layer1.0.bn2.weight',\t(64,),\n 'block1.1.conv_bn2.bn.bias',\t(64,),\t 'layer1.0.bn2.bias',\t(64,),\n 'block1.1.conv_bn2.bn.running_mean',\t(64,),\t 'layer1.0.bn2.running_mean',\t(64,),\n 'block1.1.conv_bn2.bn.running_var',\t(64,),\t 'layer1.0.bn2.running_var',\t(64,),\n 'block1.2.conv_bn1.conv.weight',\t(64, 64, 3, 3),\t 'layer1.1.conv1.weight',\t(64, 64, 3, 3),\n 'block1.2.conv_bn1.bn.weight',\t(64,),\t 'layer1.1.bn1.weight',\t(64,),\n 'block1.2.conv_bn1.bn.bias',\t(64,),\t 'layer1.1.bn1.bias',\t(64,),\n 'block1.2.conv_bn1.bn.running_mean',\t(64,),\t 'layer1.1.bn1.running_mean',\t(64,),\n 'block1.2.conv_bn1.bn.running_var',\t(64,),\t 'layer1.1.bn1.running_var',\t(64,),\n 'block1.2.conv_bn2.conv.weight',\t(64, 64, 3, 3),\t 'layer1.1.conv2.weight',\t(64, 64, 3, 3),\n 'block1.2.conv_bn2.bn.weight',\t(64,),\t 'layer1.1.bn2.weight',\t(64,),\n 'block1.2.conv_bn2.bn.bias',\t(64,),\t 'layer1.1.bn2.bias',\t(64,),\n 'block1.2.conv_bn2.bn.running_mean',\t(64,),\t 'layer1.1.bn2.running_mean',\t(64,),\n 'block1.2.conv_bn2.bn.running_var',\t(64,),\t 'layer1.1.bn2.running_var',\t(64,),\n 'block1.3.conv_bn1.conv.weight',\t(64, 64, 3, 3),\t 'layer1.2.conv1.weight',\t(64, 64, 3, 3),\n 'block1.3.conv_bn1.bn.weight',\t(64,),\t 'layer1.2.bn1.weight',\t(64,),\n 'block1.3.conv_bn1.bn.bias',\t(64,),\t 'layer1.2.bn1.bias',\t(64,),\n 'block1.3.conv_bn1.bn.running_mean',\t(64,),\t 'layer1.2.bn1.running_mean',\t(64,),\n 'block1.3.conv_bn1.bn.running_var',\t(64,),\t 'layer1.2.bn1.running_var',\t(64,),\n 'block1.3.conv_bn2.conv.weight',\t(64, 64, 3, 3),\t 'layer1.2.conv2.weight',\t(64, 64, 3, 3),\n 'block1.3.conv_bn2.bn.weight',\t(64,),\t 'layer1.2.bn2.weight',\t(64,),\n 'block1.3.conv_bn2.bn.bias',\t(64,),\t 'layer1.2.bn2.bias',\t(64,),\n 'block1.3.conv_bn2.bn.running_mean',\t(64,),\t 'layer1.2.bn2.running_mean',\t(64,),\n 'block1.3.conv_bn2.bn.running_var',\t(64,),\t 'layer1.2.bn2.running_var',\t(64,),\n 'block2.0.conv_bn1.conv.weight',\t(128, 64, 3, 3),\t 'layer2.0.conv1.weight',\t(128, 64, 3, 3),\n 'block2.0.conv_bn1.bn.weight',\t(128,),\t 'layer2.0.bn1.weight',\t(128,),\n 'block2.0.conv_bn1.bn.bias',\t(128,),\t 'layer2.0.bn1.bias',\t(128,),\n 'block2.0.conv_bn1.bn.running_mean',\t(128,),\t 'layer2.0.bn1.running_mean',\t(128,),\n 'block2.0.conv_bn1.bn.running_var',\t(128,),\t 'layer2.0.bn1.running_var',\t(128,),\n 'block2.0.conv_bn2.conv.weight',\t(128, 128, 3, 3),\t 'layer2.0.conv2.weight',\t(128, 128, 3, 3),\n 'block2.0.conv_bn2.bn.weight',\t(128,),\t 'layer2.0.bn2.weight',\t(128,),\n 'block2.0.conv_bn2.bn.bias',\t(128,),\t 'layer2.0.bn2.bias',\t(128,),\n 'block2.0.conv_bn2.bn.running_mean',\t(128,),\t 'layer2.0.bn2.running_mean',\t(128,),\n 'block2.0.conv_bn2.bn.running_var',\t(128,),\t 'layer2.0.bn2.running_var',\t(128,),\n 'block2.0.shortcut.conv.weight',\t(128, 64, 1, 1),\t 'layer2.0.downsample.0.weight',\t(128, 64, 1, 1),\n 'block2.0.shortcut.bn.weight',\t(128,),\t 'layer2.0.downsample.1.weight',\t(128,),\n 'block2.0.shortcut.bn.bias',\t(128,),\t 'layer2.0.downsample.1.bias',\t(128,),\n 'block2.0.shortcut.bn.running_mean',\t(128,),\t 'layer2.0.downsample.1.running_mean',\t(128,),\n 'block2.0.shortcut.bn.running_var',\t(128,),\t 'layer2.0.downsample.1.running_var',\t(128,),\n 'block2.1.conv_bn1.conv.weight',\t(128, 128, 3, 3),\t 'layer2.1.conv1.weight',\t(128, 128, 3, 3),\n 'block2.1.conv_bn1.bn.weight',\t(128,),\t 'layer2.1.bn1.weight',\t(128,),\n 'block2.1.conv_bn1.bn.bias',\t(128,),\t 'layer2.1.bn1.bias',\t(128,),\n 'block2.1.conv_bn1.bn.running_mean',\t(128,),\t 'layer2.1.bn1.running_mean',\t(128,),\n 'block2.1.conv_bn1.bn.running_var',\t(128,),\t 'layer2.1.bn1.running_var',\t(128,),\n 'block2.1.conv_bn2.conv.weight',\t(128, 128, 3, 3),\t 'layer2.1.conv2.weight',\t(128, 128, 3, 3),\n 'block2.1.conv_bn2.bn.weight',\t(128,),\t 'layer2.1.bn2.weight',\t(128,),\n 'block2.1.conv_bn2.bn.bias',\t(128,),\t 'layer2.1.bn2.bias',\t(128,),\n 'block2.1.conv_bn2.bn.running_mean',\t(128,),\t 'layer2.1.bn2.running_mean',\t(128,),\n 'block2.1.conv_bn2.bn.running_var',\t(128,),\t 'layer2.1.bn2.running_var',\t(128,),\n 'block2.2.conv_bn1.conv.weight',\t(128, 128, 3, 3),\t 'layer2.2.conv1.weight',\t(128, 128, 3, 3),\n 'block2.2.conv_bn1.bn.weight',\t(128,),\t 'layer2.2.bn1.weight',\t(128,),\n 'block2.2.conv_bn1.bn.bias',\t(128,),\t 'layer2.2.bn1.bias',\t(128,),\n 'block2.2.conv_bn1.bn.running_mean',\t(128,),\t 'layer2.2.bn1.running_mean',\t(128,),\n 'block2.2.conv_bn1.bn.running_var',\t(128,),\t 'layer2.2.bn1.running_var',\t(128,),\n 'block2.2.conv_bn2.conv.weight',\t(128, 128, 3, 3),\t 'layer2.2.conv2.weight',\t(128, 128, 3, 3),\n 'block2.2.conv_bn2.bn.weight',\t(128,),\t 'layer2.2.bn2.weight',\t(128,),\n 'block2.2.conv_bn2.bn.bias',\t(128,),\t 'layer2.2.bn2.bias',\t(128,),\n 'block2.2.conv_bn2.bn.running_mean',\t(128,),\t 'layer2.2.bn2.running_mean',\t(128,),\n 'block2.2.conv_bn2.bn.running_var',\t(128,),\t 'layer2.2.bn2.running_var',\t(128,),\n 'block2.3.conv_bn1.conv.weight',\t(128, 128, 3, 3),\t 'layer2.3.conv1.weight',\t(128, 128, 3, 3),\n 'block2.3.conv_bn1.bn.weight',\t(128,),\t 'layer2.3.bn1.weight',\t(128,),\n 'block2.3.conv_bn1.bn.bias',\t(128,),\t 'layer2.3.bn1.bias',\t(128,),\n 'block2.3.conv_bn1.bn.running_mean',\t(128,),\t 'layer2.3.bn1.running_mean',\t(128,),\n 'block2.3.conv_bn1.bn.running_var',\t(128,),\t 'layer2.3.bn1.running_var',\t(128,),\n 'block2.3.conv_bn2.conv.weight',\t(128, 128, 3, 3),\t 'layer2.3.conv2.weight',\t(128, 128, 3, 3),\n 'block2.3.conv_bn2.bn.weight',\t(128,),\t 'layer2.3.bn2.weight',\t(128,),\n 'block2.3.conv_bn2.bn.bias',\t(128,),\t 'layer2.3.bn2.bias',\t(128,),\n 'block2.3.conv_bn2.bn.running_mean',\t(128,),\t 'layer2.3.bn2.running_mean',\t(128,),\n 'block2.3.conv_bn2.bn.running_var',\t(128,),\t 'layer2.3.bn2.running_var',\t(128,),\n 'block3.0.conv_bn1.conv.weight',\t(256, 128, 3, 3),\t 'layer3.0.conv1.weight',\t(256, 128, 3, 3),\n 'block3.0.conv_bn1.bn.weight',\t(256,),\t 'layer3.0.bn1.weight',\t(256,),\n 'block3.0.conv_bn1.bn.bias',\t(256,),\t 'layer3.0.bn1.bias',\t(256,),\n 'block3.0.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.0.bn1.running_mean',\t(256,),\n 'block3.0.conv_bn1.bn.running_var',\t(256,),\t 'layer3.0.bn1.running_var',\t(256,),\n 'block3.0.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.0.conv2.weight',\t(256, 256, 3, 3),\n 'block3.0.conv_bn2.bn.weight',\t(256,),\t 'layer3.0.bn2.weight',\t(256,),\n 'block3.0.conv_bn2.bn.bias',\t(256,),\t 'layer3.0.bn2.bias',\t(256,),\n 'block3.0.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.0.bn2.running_mean',\t(256,),\n 'block3.0.conv_bn2.bn.running_var',\t(256,),\t 'layer3.0.bn2.running_var',\t(256,),\n 'block3.0.shortcut.conv.weight',\t(256, 128, 1, 1),\t 'layer3.0.downsample.0.weight',\t(256, 128, 1, 1),\n 'block3.0.shortcut.bn.weight',\t(256,),\t 'layer3.0.downsample.1.weight',\t(256,),\n 'block3.0.shortcut.bn.bias',\t(256,),\t 'layer3.0.downsample.1.bias',\t(256,),\n 'block3.0.shortcut.bn.running_mean',\t(256,),\t 'layer3.0.downsample.1.running_mean',\t(256,),\n 'block3.0.shortcut.bn.running_var',\t(256,),\t 'layer3.0.downsample.1.running_var',\t(256,),\n 'block3.1.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.1.conv1.weight',\t(256, 256, 3, 3),\n 'block3.1.conv_bn1.bn.weight',\t(256,),\t 'layer3.1.bn1.weight',\t(256,),\n 'block3.1.conv_bn1.bn.bias',\t(256,),\t 'layer3.1.bn1.bias',\t(256,),\n 'block3.1.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.1.bn1.running_mean',\t(256,),\n 'block3.1.conv_bn1.bn.running_var',\t(256,),\t 'layer3.1.bn1.running_var',\t(256,),\n 'block3.1.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.1.conv2.weight',\t(256, 256, 3, 3),\n 'block3.1.conv_bn2.bn.weight',\t(256,),\t 'layer3.1.bn2.weight',\t(256,),\n 'block3.1.conv_bn2.bn.bias',\t(256,),\t 'layer3.1.bn2.bias',\t(256,),\n 'block3.1.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.1.bn2.running_mean',\t(256,),\n 'block3.1.conv_bn2.bn.running_var',\t(256,),\t 'layer3.1.bn2.running_var',\t(256,),\n 'block3.2.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.2.conv1.weight',\t(256, 256, 3, 3),\n 'block3.2.conv_bn1.bn.weight',\t(256,),\t 'layer3.2.bn1.weight',\t(256,),\n 'block3.2.conv_bn1.bn.bias',\t(256,),\t 'layer3.2.bn1.bias',\t(256,),\n 'block3.2.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.2.bn1.running_mean',\t(256,),\n 'block3.2.conv_bn1.bn.running_var',\t(256,),\t 'layer3.2.bn1.running_var',\t(256,),\n 'block3.2.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.2.conv2.weight',\t(256, 256, 3, 3),\n 'block3.2.conv_bn2.bn.weight',\t(256,),\t 'layer3.2.bn2.weight',\t(256,),\n 'block3.2.conv_bn2.bn.bias',\t(256,),\t 'layer3.2.bn2.bias',\t(256,),\n 'block3.2.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.2.bn2.running_mean',\t(256,),\n 'block3.2.conv_bn2.bn.running_var',\t(256,),\t 'layer3.2.bn2.running_var',\t(256,),\n 'block3.3.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.3.conv1.weight',\t(256, 256, 3, 3),\n 'block3.3.conv_bn1.bn.weight',\t(256,),\t 'layer3.3.bn1.weight',\t(256,),\n 'block3.3.conv_bn1.bn.bias',\t(256,),\t 'layer3.3.bn1.bias',\t(256,),\n 'block3.3.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.3.bn1.running_mean',\t(256,),\n 'block3.3.conv_bn1.bn.running_var',\t(256,),\t 'layer3.3.bn1.running_var',\t(256,),\n 'block3.3.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.3.conv2.weight',\t(256, 256, 3, 3),\n 'block3.3.conv_bn2.bn.weight',\t(256,),\t 'layer3.3.bn2.weight',\t(256,),\n 'block3.3.conv_bn2.bn.bias',\t(256,),\t 'layer3.3.bn2.bias',\t(256,),\n 'block3.3.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.3.bn2.running_mean',\t(256,),\n 'block3.3.conv_bn2.bn.running_var',\t(256,),\t 'layer3.3.bn2.running_var',\t(256,),\n 'block3.4.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.4.conv1.weight',\t(256, 256, 3, 3),\n 'block3.4.conv_bn1.bn.weight',\t(256,),\t 'layer3.4.bn1.weight',\t(256,),\n 'block3.4.conv_bn1.bn.bias',\t(256,),\t 'layer3.4.bn1.bias',\t(256,),\n 'block3.4.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.4.bn1.running_mean',\t(256,),\n 'block3.4.conv_bn1.bn.running_var',\t(256,),\t 'layer3.4.bn1.running_var',\t(256,),\n 'block3.4.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.4.conv2.weight',\t(256, 256, 3, 3),\n 'block3.4.conv_bn2.bn.weight',\t(256,),\t 'layer3.4.bn2.weight',\t(256,),\n 'block3.4.conv_bn2.bn.bias',\t(256,),\t 'layer3.4.bn2.bias',\t(256,),\n 'block3.4.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.4.bn2.running_mean',\t(256,),\n 'block3.4.conv_bn2.bn.running_var',\t(256,),\t 'layer3.4.bn2.running_var',\t(256,),\n 'block3.5.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.5.conv1.weight',\t(256, 256, 3, 3),\n 'block3.5.conv_bn1.bn.weight',\t(256,),\t 'layer3.5.bn1.weight',\t(256,),\n 'block3.5.conv_bn1.bn.bias',\t(256,),\t 'layer3.5.bn1.bias',\t(256,),\n 'block3.5.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.5.bn1.running_mean',\t(256,),\n 'block3.5.conv_bn1.bn.running_var',\t(256,),\t 'layer3.5.bn1.running_var',\t(256,),\n 'block3.5.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.5.conv2.weight',\t(256, 256, 3, 3),\n 'block3.5.conv_bn2.bn.weight',\t(256,),\t 'layer3.5.bn2.weight',\t(256,),\n 'block3.5.conv_bn2.bn.bias',\t(256,),\t 'layer3.5.bn2.bias',\t(256,),\n 'block3.5.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.5.bn2.running_mean',\t(256,),\n 'block3.5.conv_bn2.bn.running_var',\t(256,),\t 'layer3.5.bn2.running_var',\t(256,),\n 'block4.0.conv_bn1.conv.weight',\t(512, 256, 3, 3),\t 'layer4.0.conv1.weight',\t(512, 256, 3, 3),\n 'block4.0.conv_bn1.bn.weight',\t(512,),\t 'layer4.0.bn1.weight',\t(512,),\n 'block4.0.conv_bn1.bn.bias',\t(512,),\t 'layer4.0.bn1.bias',\t(512,),\n 'block4.0.conv_bn1.bn.running_mean',\t(512,),\t 'layer4.0.bn1.running_mean',\t(512,),\n 'block4.0.conv_bn1.bn.running_var',\t(512,),\t 'layer4.0.bn1.running_var',\t(512,),\n 'block4.0.conv_bn2.conv.weight',\t(512, 512, 3, 3),\t 'layer4.0.conv2.weight',\t(512, 512, 3, 3),\n 'block4.0.conv_bn2.bn.weight',\t(512,),\t 'layer4.0.bn2.weight',\t(512,),\n 'block4.0.conv_bn2.bn.bias',\t(512,),\t 'layer4.0.bn2.bias',\t(512,),\n 'block4.0.conv_bn2.bn.running_mean',\t(512,),\t 'layer4.0.bn2.running_mean',\t(512,),\n 'block4.0.conv_bn2.bn.running_var',\t(512,),\t 'layer4.0.bn2.running_var',\t(512,),\n 'block4.0.shortcut.conv.weight',\t(512, 256, 1, 1),\t 'layer4.0.downsample.0.weight',\t(512, 256, 1, 1),\n 'block4.0.shortcut.bn.weight',\t(512,),\t 'layer4.0.downsample.1.weight',\t(512,),\n 'block4.0.shortcut.bn.bias',\t(512,),\t 'layer4.0.downsample.1.bias',\t(512,),\n 'block4.0.shortcut.bn.running_mean',\t(512,),\t 'layer4.0.downsample.1.running_mean',\t(512,),\n 'block4.0.shortcut.bn.running_var',\t(512,),\t 'layer4.0.downsample.1.running_var',\t(512,),\n 'block4.1.conv_bn1.conv.weight',\t(512, 512, 3, 3),\t 'layer4.1.conv1.weight',\t(512, 512, 3, 3),\n 'block4.1.conv_bn1.bn.weight',\t(512,),\t 'layer4.1.bn1.weight',\t(512,),\n 'block4.1.conv_bn1.bn.bias',\t(512,),\t 'layer4.1.bn1.bias',\t(512,),\n 'block4.1.conv_bn1.bn.running_mean',\t(512,),\t 'layer4.1.bn1.running_mean',\t(512,),\n 'block4.1.conv_bn1.bn.running_var',\t(512,),\t 'layer4.1.bn1.running_var',\t(512,),\n 'block4.1.conv_bn2.conv.weight',\t(512, 512, 3, 3),\t 'layer4.1.conv2.weight',\t(512, 512, 3, 3),\n 'block4.1.conv_bn2.bn.weight',\t(512,),\t 'layer4.1.bn2.weight',\t(512,),\n 'block4.1.conv_bn2.bn.bias',\t(512,),\t 'layer4.1.bn2.bias',\t(512,),\n 'block4.1.conv_bn2.bn.running_mean',\t(512,),\t 'layer4.1.bn2.running_mean',\t(512,),\n 'block4.1.conv_bn2.bn.running_var',\t(512,),\t 'layer4.1.bn2.running_var',\t(512,),\n 'block4.2.conv_bn1.conv.weight',\t(512, 512, 3, 3),\t 'layer4.2.conv1.weight',\t(512, 512, 3, 3),\n 'block4.2.conv_bn1.bn.weight',\t(512,),\t 'layer4.2.bn1.weight',\t(512,),\n 'block4.2.conv_bn1.bn.bias',\t(512,),\t 'layer4.2.bn1.bias',\t(512,),\n 'block4.2.conv_bn1.bn.running_mean',\t(512,),\t 'layer4.2.bn1.running_mean',\t(512,),\n 'block4.2.conv_bn1.bn.running_var',\t(512,),\t 'layer4.2.bn1.running_var',\t(512,),\n 'block4.2.conv_bn2.conv.weight',\t(512, 512, 3, 3),\t 'layer4.2.conv2.weight',\t(512, 512, 3, 3),\n 'block4.2.conv_bn2.bn.weight',\t(512,),\t 'layer4.2.bn2.weight',\t(512,),\n 'block4.2.conv_bn2.bn.bias',\t(512,),\t 'layer4.2.bn2.bias',\t(512,),\n 'block4.2.conv_bn2.bn.running_mean',\t(512,),\t 'layer4.2.bn2.running_mean',\t(512,),\n 'block4.2.conv_bn2.bn.running_var',\t(512,),\t 'layer4.2.bn2.running_var',\t(512,),\n 'logit.weight',\t(1000, 512),\t 'fc.weight',\t(1000, 512),\n 'logit.bias',\t(1000,),\t 'fc.bias',\t(1000,),\n\n]\n\n###############################################################################\nclass ConvBn2d(nn.Module):\n\n    def __init__(self, in_channel, out_channel, kernel_size=3, padding=1, stride=1):\n        super(ConvBn2d, self).__init__()\n        self.conv = nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, padding=padding, stride=stride, bias=False)\n        self.bn   = nn.BatchNorm2d(out_channel, eps=1e-5)\n\n    def forward(self,x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\n\n\n#############  resnext50 pyramid feature net #######################################\n# https://github.com/Hsuxu/ResNeXt/blob/master/models.py\n# https://github.com/D-X-Y/ResNeXt-DenseNet/blob/master/models/resnext.py\n# https://github.com/miraclewkf/ResNeXt-PyTorch/blob/master/resnext.py\n\n\n# bottleneck type C\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channel, channel, out_channel, stride=1, is_shortcut=False):\n        super(BasicBlock, self).__init__()\n        self.is_shortcut = is_shortcut\n\n        self.conv_bn1 = ConvBn2d(in_channel,    channel, kernel_size=3, padding=1, stride=stride)\n        self.conv_bn2 = ConvBn2d(   channel,out_channel, kernel_size=3, padding=1, stride=1)\n\n        if is_shortcut:\n            self.shortcut = ConvBn2d(in_channel, out_channel, kernel_size=1, padding=0, stride=stride)\n\n\n    def forward(self, x):\n        z = F.relu(self.conv_bn1(x),inplace=True)\n        z = self.conv_bn2(z)\n\n        if self.is_shortcut:\n            x = self.shortcut(x)\n\n        z += x\n        z = F.relu(z,inplace=True)\n        return z\n\n\n\n\nclass ResNet34(nn.Module):\n\n    def __init__(self, num_class=1000 ):\n        super(ResNet34, self).__init__()\n\n\n        self.block0  = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, padding=3, stride=2, bias=False),\n            BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n        )\n        self.block1  = nn.Sequential(\n             nn.MaxPool2d(kernel_size=3, padding=1, stride=2),\n             BasicBlock( 64, 64, 64, stride=1, is_shortcut=False,),\n          * [BasicBlock( 64, 64, 64, stride=1, is_shortcut=False,) for i in range(1,3)],\n        )\n        self.block2  = nn.Sequential(\n             BasicBlock( 64,128,128, stride=2, is_shortcut=True, ),\n          * [BasicBlock(128,128,128, stride=1, is_shortcut=False,) for i in range(1,4)],\n        )\n        self.block3  = nn.Sequential(\n             BasicBlock(128,256,256, stride=2, is_shortcut=True, ),\n          * [BasicBlock(256,256,256, stride=1, is_shortcut=False,) for i in range(1,6)],\n        )\n        self.block4 = nn.Sequential(\n             BasicBlock(256,512,512, stride=2, is_shortcut=True, ),\n          * [BasicBlock(512,512,512, stride=1, is_shortcut=False,) for i in range(1,3)],\n        )\n        self.logit = nn.Linear(512,num_class)\n\n\n\n    def forward(self, x):\n        batch_size = len(x)\n\n        x = self.block0(x)\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = F.adaptive_avg_pool2d(x,1).reshape(batch_size,-1)\n        logit = self.logit(x)\n        return logit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Resnet34_classification(nn.Module):\n    def __init__(self,num_class=4):\n        super(Resnet34_classification, self).__init__()\n        e = ResNet34()\n        self.block = nn.ModuleList([\n            e.block0,\n            e.block1,\n            e.block2,\n            e.block3,\n            e.block4,\n        ])\n        e = None  #dropped\n        self.feature = nn.Conv2d(512,32, kernel_size=1) #dummy conv for dim reduction\n        self.logit = nn.Conv2d(32,num_class, kernel_size=1)\n\n    def forward(self, x):\n        batch_size,C,H,W = x.shape\n\n        for i in range( len(self.block)):\n            x = self.block[i](x)\n            #print(i, x.shape)\n\n        x = F.dropout(x,0.5,training=self.training)\n        x = F.adaptive_avg_pool2d(x, 1)\n        x = self.feature(x)\n        logit = self.logit(x)\n        return logit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_classification = Resnet34_classification()\nmodel_classification.load_state_dict(torch.load('../input/severstal-cls-model/00007500_model.pth', map_location=lambda storage, loc: storage), strict=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mask2rle(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestDataset(Dataset):\n    '''Dataset for test prediction'''\n    def __init__(self, root, df, mean, std):\n        self.root = root\n        df['ImageId'] = df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\n        self.fnames = df['ImageId'].unique().tolist()\n        self.num_samples = len(self.fnames)\n        self.transform = Compose(\n            [\n                Normalize(mean=mean, std=std, p=1),\n                ToTensor(),\n            ]\n        )\n\n    def __getitem__(self, idx):\n        fname = self.fnames[idx]\n        path = os.path.join(self.root, fname)\n        image = cv2.imread(path)\n        images = self.transform(image=image)[\"image\"]\n        return fname, images\n\n    def __len__(self):\n        return self.num_samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_path = '../input/severstal-steel-defect-detection/sample_submission.csv'\ntest_data_folder = \"../input/severstal-steel-defect-detection/test_images\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def post_process(probability, threshold, min_size):\n    '''Post processing of each predicted mask, components with lesser number of pixels\n    than `min_size` are ignored'''\n    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n    predictions = np.zeros((256, 1600), np.float32)\n    num = 0\n    for c in range(1, num_component):\n        p = (component == c)\n        if p.sum() > min_size:\n            predictions[p] = 1\n            num += 1\n    return predictions, num","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 1\nmean = (0.485, 0.456, 0.406)\nstd = (0.229, 0.224, 0.225)\ndf = pd.read_csv(sample_submission_path)\ntestset = DataLoader(\n    TestDataset(test_data_folder, df, mean, std),\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=0,\n    pin_memory=True\n)\n\ndef sharpen(p,t=0.5):\n        if t!=0:\n            return p**t\n        else:\n            return p\n\ndef get_classification_preds(net,test_loader):\n    test_probability_label = []\n    test_id   = []\n    \n    net = net.cuda()\n    for t, (fnames, images) in enumerate(tqdm(test_loader)):\n        batch_size,C,H,W = images.shape\n        images = images.cuda()\n\n        with torch.no_grad():\n            net.eval()\n\n            num_augment = 0\n            if 1: #  null\n                logit =  net(images)\n                probability = torch.sigmoid(logit)\n\n                probability_label = sharpen(probability,0)\n                num_augment+=1\n\n            if 'flip_lr' in augment:\n                logit = net(torch.flip(images,dims=[3]))\n                probability  = torch.sigmoid(logit)\n\n                probability_label += sharpen(probability)\n                num_augment+=1\n\n            if 'flip_ud' in augment:\n                logit = net(torch.flip(images,dims=[2]))\n                probability = torch.sigmoid(logit)\n\n                probability_label += sharpen(probability)\n                num_augment+=1\n\n            probability_label = probability_label/num_augment\n\n        probability_label = probability_label.data.cpu().numpy()\n        \n        test_probability_label.append(probability_label)\n        test_id.extend([i for i in fnames])\n\n    \n    test_probability_label = np.concatenate(test_probability_label)\n    return test_probability_label, test_id\n# threshold for classification\nthreshold_label = [0.50,0.50,0.50,0.50]\n\naugment = ['null'] #['null', 'flip_lr','flip_ud'] #['null, 'flip_lr','flip_ud','5crop']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probability_label, image_id = get_classification_preds(model_classification, testset)\npredict_label = probability_label>np.array(threshold_label).reshape(1,4,1,1)\n\nimage_id_class_id = []\nencoded_pixel = []\nfor b in range(len(image_id)):\n    for c in range(4):\n        image_id_class_id.append(image_id[b]+'_%d'%(c+1))\n        if predict_label[b,c]==0:\n            rle=''\n        else:\n            rle ='1 1'\n        encoded_pixel.append(rle)\n\ndf_classification = pd.DataFrame(zip(image_id_class_id, encoded_pixel), columns=['ImageId_ClassId', 'EncodedPixels'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del model_classification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unet_se_resnext50_32x4d = \\\n    load('/kaggle/input/severstalmodels/unet_se_resnext50_32x4d.pth').cuda()\nunet_mobilenet2 = load('/kaggle/input/severstalmodels/unet_mobilenet2.pth').cuda()\nunet_resnet34 = load('/kaggle/input/severstalmodels/unet_resnet34.pth').cuda()\n#unet_vgg16 = load('/kaggle/input/severstal-mlcomp/vgg16_mlcomp.pth').cuda()\nunet_resnext50_fold3 = load('/kaggle/input/severstal-mlcomp/resnext50_mlcomp.pth').cuda()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Models' mean aggregator"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"class Model:\n    def __init__(self, models):\n        self.models = models\n    \n    def __call__(self, x):\n        res = []\n        x = x.cuda()\n        with torch.no_grad():\n            for m in self.models:\n                res.append(m(x))\n        res = torch.stack(res)\n        return torch.mean(res, dim=0)\n\nmodel = Model([unet_se_resnext50_32x4d, unet_mobilenet2, unet_resnet34, unet_resnext50_fold3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create TTA transforms, datasets, loaders"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def create_transforms(additional):\n    res = list(additional)\n    # add necessary transformations\n    res.extend([\n        A.Normalize(\n            mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n        ),\n        ChannelTranspose()\n    ])\n    res = A.Compose(res)\n    return res\n\nimg_folder = '/kaggle/input/severstal-steel-defect-detection/test_images'\nbatch_size = 2\nnum_workers = 0\n\n# Different transforms for TTA wrapper\ntransforms = [\n    [],\n    [A.HorizontalFlip(p=1)]  # addvflip?\n]\n\ntransforms = [create_transforms(t) for t in transforms]\ndatasets = [TtaWrap(ImageDataset(img_folder=img_folder, transforms=t), tfms=t) for t in transforms]\nloaders = [DataLoader(d, num_workers=num_workers, batch_size=batch_size, shuffle=False) for d in datasets]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loaders' mean aggregator"},{"metadata":{"trusted":true},"cell_type":"code","source":"thresholds = [0.5, 0.5, 0.5, 0.5]\nmin_area = [600, 600, 1000, 2000]\n#min_area = [1400,1000,1400,2500]\n\nres = []\n# Iterate over all TTA loaders\ntotal = len(datasets[0])//batch_size\nfor loaders_batch in tqdm_notebook(zip(*loaders), total=total):\n    preds = []\n    image_file = []\n    for i, batch in enumerate(loaders_batch):\n        features = batch['features'].cuda()\n        p = torch.sigmoid(model(features))\n        # inverse operations for TTA\n        p = datasets[i].inverse(p)\n        preds.append(p)\n        image_file = batch['image_file']\n    \n    # TTA mean\n    preds = torch.stack(preds)\n    preds = torch.mean(preds, dim=0)\n    preds = preds.detach().cpu().numpy()\n    \n    # Batch post processing\n    for p, file in zip(preds, image_file):\n        file = os.path.basename(file)\n        # Image postprocessing\n        for i in range(4):\n            p_channel = p[i]\n            imageid_classid = file+'_'+str(i+1)\n            p_channel = (p_channel>thresholds[i]).astype(np.uint8)\n            if p_channel.sum() < min_area[i]:\n                p_channel = np.zeros(p_channel.shape, dtype=p_channel.dtype)\n\n            res.append({\n                'ImageId_ClassId': imageid_classid,\n                'EncodedPixels': mask2rle(p_channel)\n            })\n        \ndf = pd.DataFrame(res)\n# df.to_csv('submission.csv', index=False)\t","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"#df = pd.DataFrame(res)\ndf = df.fillna('')\n#df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_segmentation = pd.DataFrame(df, columns=['ImageId_ClassId', 'EncodedPixels'])\n\ndf_mask = df_segmentation.copy()\ndf_label = df_classification.copy()\n\nassert(np.all(df_mask['ImageId_ClassId'].values == df_label['ImageId_ClassId'].values))\nprint((df_mask.loc[df_label['EncodedPixels']=='','EncodedPixels'] != '').sum() ) #202\ndf_mask.loc[df_label['EncodedPixels']=='','EncodedPixels']=''\n\ndf_mask.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Histogram of predictions"},{"metadata":{"trusted":false},"cell_type":"code","source":"# df['Image'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[0])\n# df['Class'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[1])\n# df['empty'] = df['EncodedPixels'].map(lambda x: not x)\n# df[df['empty'] == False]['Class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualization"},{"metadata":{"trusted":false},"cell_type":"code","source":"# %matplotlib inline\n\n# df = pd.read_csv('submission.csv')[:40]\n# df['Image'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[0])\n# df['Class'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[1])\n\n# for row in df.itertuples():\n#     img_path = os.path.join(img_folder, row.Image)\n#     img = cv2.imread(img_path)\n#     mask = rle2mask(row.EncodedPixels, (1600, 256)) \\\n#         if isinstance(row.EncodedPixels, str) else np.zeros((256, 1600))\n#     if mask.sum() == 0:\n#         continue\n    \n#     fig, axes = plt.subplots(1, 2, figsize=(20, 60))\n#     axes[0].imshow(img/255)\n#     axes[1].imshow(mask*60)\n#     axes[0].set_title(row.Image)\n#     axes[1].set_title(row.Class)\n#     plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"2d558d43f12745d8a826eb84f7dcd0b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"60b23d613e584fa0abb75460b84b1d80":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e3c6cad600e04ebfb71a6c202a8eb513","IPY_MODEL_d6bd450456d24af1b8f635c10fee4445"],"layout":"IPY_MODEL_a7f23b4d7ebb4499b486f861d1c52b77"}},"6feb39965873459cb61d32fbbc2c2225":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7f23b4d7ebb4499b486f861d1c52b77":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c389718f59fc45eba86cebc46dc72f99":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6bd450456d24af1b8f635c10fee4445":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c389718f59fc45eba86cebc46dc72f99","placeholder":"â€‹","style":"IPY_MODEL_dcff928e32c743529e899d038dee4e66","value":" 901/? [05:36&lt;00:00,  2.67it/s]"}},"dcff928e32c743529e899d038dee4e66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3c6cad600e04ebfb71a6c202a8eb513":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6feb39965873459cb61d32fbbc2c2225","max":900,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2d558d43f12745d8a826eb84f7dcd0b2","value":900}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":1}