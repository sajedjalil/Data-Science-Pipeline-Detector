{"cells":[{"metadata":{"_uuid":"e6d94b76-ee72-40bb-8bed-532a698120e0","_cell_guid":"3785123a-c0fa-4c59-8d83-739b4b6c0771","trusted":true},"cell_type":"code","source":"from fastai.vision import *\nimport cv2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import mish_torch\nfrom mish_torch import MishFunction,Mish\nimport bundled_gen_efficientnet\nimport gen_efficientnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BS = 8\nDROP_RATE = 0.0\nSTATS = (tensor([0.3436]), tensor([0.1961]))\n\nINPUTS = Path('/kaggle/input')\nSTATE  = INPUTS/'severstal-eur-mish'\nDATA   = INPUTS/'severstal-steel-defect-detection'\n\nCLASSES=['1','2','3','4']\n\n# PEr-class thresholds from analysis of valid\nTHRESHOLDS = [0.29,0.552,0.447,0.330]\nMIN_PXS = 2000 # Minimum contiguous pixels to not ignore\nTTA_WEIGHTS = tensor([0.6, 0.2, 0.07, 0.07, 0.05]) # Weight full and non-TTA blocks higher","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Model and Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# efficientnet expects inplace argument for activation function, ignore\ndef mish_fn(x,inplace=False): return MishFunction.apply(x)\n\nclass EfficientUnetEncoder(nn.Module):\n    def __init__(self, eff_net, first_skip=False):\n        super().__init__()\n        self.first_skip = first_skip\n        self.mod_names = ['conv_stem', 'bn1', 'act_fn', 'blocks']\n        for n in self.mod_names: setattr(self, n, getattr(eff_net, n))\n        self.blk_strides  = [\n            max((l.stride[0] if hasattr(l, 'stride')\n                 else 0\n                 for l in flatten_model(blk)))\n            for blk in self.blocks]\n        self.blk_channels = [\n            [ l.out_channels\n              for l in flatten_model(blk)\n              if isinstance(l, nn.Conv2d)\n            ][-1]\n            for blk in self.blocks]\n\n    def forward(self, x, hook=False):\n        x = self.conv_stem(x)\n        x = self.bn1(x)\n        x = self.act_fn(x)\n        if hook:\n            hooks = []\n            for i,(blk,stride) in enumerate(zip(self.blocks,self.blk_strides)):\n                if stride > 1 and (i > 0 or self.first_skip):\n                    hooks.append(x)\n                x = blk(x)\n        else: x = self.blocks(x)\n        return x if not hook else (x,hooks)\n    \n    def load_state(self, state_dict):\n        sd = {n: v for n,v in state_dict.items() if n.split('.')[0] in self.mod_names}\n        return self.load_state_dict(sd)\n\nclass EfficientUnetClassifier(nn.Module):\n    def __init__(self, enc, eff_net, drop_rate=0.):\n        super().__init__()\n        self.enc,self.drop_rate = enc,drop_rate\n        self.efficient_head,self.act_fn = eff_net.efficient_head, enc.act_fn\n        self.mod_names = ['conv_head','classifier']\n        if not self.efficient_head: self.mod_names.insert(1, 'bn2')\n        for n in self.mod_names: setattr(self, n, getattr(eff_net, n))\n        \n    def forward(self, x):\n        x = self.enc(x)\n        if self.efficient_head:\n            x = F.adaptive_avg_pool2d(x, 1)\n            x = self.conv_head(x)\n            # no BN\n            x = self.act_fn(x, inplace=True)\n        else:\n            x = self.conv_head(x)\n            x = self.bn2(x)\n            x = self.act_fn(x, inplace=True)\n            x = F.adaptive_avg_pool2d(x, 1)\n        x = x.flatten(1)\n        if self.drop_rate > 0.:\n            x = F.dropout(x, p=self.drop_rate, training=self.training)\n        return self.classifier(x)\n        return x\n    \n    def load_state(self, state_dict):\n        res_enc = self.enc.load_state(state_dict)\n        sd = {n: v for n,v in state_dict.items() if n.split('.')[0] in self.mod_names}\n        res = self.load_state_dict(sd)\n        res.missing_keys.extend(res_enc.missing_keys)\n        res.unexpected_keys.extend(res_enc.unexpected_keys)\n        return res\n\nclass ResBlock(nn.Module):\n    def __init__(self, ni:int, nf:int, ks:int=3, stride:int=1, padding:int=None,\n                 identity:bool=True, pre_activ:bool=True, bias:bool=True, act_fn:Callable[[None],nn.Module]=None,\n                 leaky:float=None,init:Callable=nn.init.kaiming_normal_, norm_type:NormType=NormType.Batch):\n        super().__init__()\n        \n        act = ifnone(act_fn, partial(relu, inplace=True, leaky=leaky))\n        if type(act) == type and issubclass(act, nn.Module): act = act()\n        if padding is None: padding = (ks-1)//2\n        if pre_activ: self.act1,self.bn1 = act,batchnorm_2d(ni, norm_type=norm_type)\n        else:         self.act1,self.bn1 = noop,noop\n        self.conv1 = conv2d(ni, nf, ks, stride=stride, padding=padding, bias=(bias and not pre_activ), init=init)\n        #norm2 = NormType.BatchZero if norm_type == NormType.Batch else norm_type\n        self.bn2   = batchnorm_2d(nf, norm_type=norm_type)\n        self.act2  = act\n        self.conv2 = conv2d(nf, nf, ks, padding=padding, bias=False, init=init)\n        if identity:\n            l = [batchnorm_2d(ni, norm_type=norm_type)]\n            if ni != nf: l.append(conv2d(ni, nf, ks=1, bias=False, init=init))\n            if stride != 1: l.append(nn.AvgPool2d(2, ceil_mode=True))\n            self.identity = nn.Sequential(*l)\n        else: self.identity = None\n        \n    def forward(self, x):\n        res = self.act1(self.bn1(x))\n        res = self.conv1(res)\n        res = self.act2(self.bn2(res))\n        res = self.conv2(res)\n        if self.identity is not None: res += self.identity(x)\n        return res\n    \nfrom gen_efficientnet.gen_efficientnet import GenEfficientNet, decode_arch_def, round_channels, _resolve_bn_args\ndef gen_efficientnet(c_out=1000, c_in=1, act_fn=mish_fn, **kwargs):\n    # Modified arch to not have an immeadiate stride 2 conv and to reduce channels\n    channel_multiplier=1.0\n    depth_multiplier=1.0\n\n    arch_def = [\n        ['ds_r1_k3_s2_e2_c16_se0.50'],\n        ['ir_r2_k3_s1_e4_c24_se0.50'],\n        ['ir_r2_k5_s2_e4_c40_se0.50'],\n        ['ir_r3_k3_s1_e3_c80_se0.50'],\n        ['ir_r3_k5_s2_e2_c112_se0.50'],\n        ['ir_r3_k5_s1_e3_c160_se0.50'],\n        ['ir_r1_k3_s2_e2_c192_se0.50'],\n    ]\n    model = GenEfficientNet(\n        decode_arch_def(arch_def, depth_multiplier),\n        num_classes=c_out,\n        in_chans=c_in,\n        stem_size=32,\n        channel_multiplier=channel_multiplier,\n        channel_divisor=8,\n        channel_min=None,\n        num_features=round_channels(1280, channel_multiplier, 8, None),\n        bn_args=_resolve_bn_args(kwargs),\n        act_fn=act_fn,\n        **kwargs\n    )\n    model.conv_stem.stride = (1,1)\n    return model\n\n    \nclass EfficientUResnet(nn.Module):\n    def __init__(self, c_out, c_in=1, drop_rate=0.0, resize_mode='nearest',\n                 act_fn=mish_fn, res_act_fn=None, leaky=False, final_act=None,  identity=True,\n                 with_cfn=False, cfn_c_out=None, cfn_drop_rate=None, **kwargs):\n        super().__init__()\n        if type(act_fn) is type and issubclass(act_fn, nn.Module):\n            # EfficientNet expects inplace parameter on activation\n            act = act_fn()\n            def _act_fn(self, x, inplace=False): return act(x)\n            act_fn = _act_fn\n        resize_mode = {'mode': resize_mode}\n        if resize_mode == 'bilinear': resize_mode['align_corners'] = False\n        eff_net = gen_efficientnet(ifnone(cfn_c_out, c_out), c_in, act_fn=act_fn, **kwargs)\n        #gen_efficientnet.efficientnet_b0(in_chans=c_in, act_fn=act_fn, drop_connect_rate=drop_rate)\n        self.enc = EfficientUnetEncoder(eff_net)\n        if with_cfn:\n            self.classifier = EfficientUnetClassifier(self.enc, eff_net, drop_rate=ifnone(cfn_drop_rate, drop_rate))\n        # Bridge and decoder\n        self.resize = nn.Upsample(scale_factor=2, **resize_mode)\n        res_args = dict(identity=identity, act_fn=res_act_fn, leaky=leaky)\n        skip_nfs = [c for s,c in zip(self.enc.blk_strides[-1:1:-1], self.enc.blk_channels[-2::-1])\n                    if s == 2]\n        self.bridge = ResBlock(self.enc.blk_channels[-1], skip_nfs[0], ks=3, stride=1, **res_args)\n        dec_nfs = skip_nfs[1:] + [self.enc.blk_channels[0]]\n        #print(f\"skip_nfs: {skip_nfs}; dec_nfs: {dec_nfs}\")\n        self.dec = nn.ModuleList([\n            # Input also includes concatenated skip connection of size nf\n            ResBlock(ni*2, nf, **res_args)\n            for ni,nf in zip(skip_nfs,dec_nfs) ])\n        self.final_conv = conv2d(dec_nfs[-1], c_out, ks=3, padding=1)\n        self.final_act = final_act\n\n        \n    def forward(self, inp):\n        x,skips = self.enc(inp, hook=True)\n        x = self.bridge(x)\n        for dec_blk,skip in zip(self.dec,reversed(skips)):\n            x = self.resize(x)\n            x = torch.cat((x, skip), dim=1) # Concatenate channels\n            x = dec_blk(x)\n        x = self.resize(x)\n        x = self.final_conv(x)\n        if self.final_act: x = self.final_act(x)\n        return x\n    \n    def load_classifier_state(self, state_dict):\n        if 'model' in state_dict: state_dict = state_dict['model']\n        return self.enc.load_state(state_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/paulorzp/rle-functions-run-lenght-encode-decode\ndef mask2rle(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load model\nmdl = EfficientUResnet(c_out=4, act_fn=mish_fn, res_act_fn=Mish, drop_rate=DROP_RATE, with_cfn=False)\nstate = torch.load(STATE/'seg.pth', map_location='cpu')\nif 'model' in state: state = state['model'] # Handle both fastai saves and torch saves\nres = mdl.load_state_dict(state)\nmdl = mdl.cuda().eval()\nres","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get test data\ndf_samp_subm = pd.read_csv(DATA/'sample_submission.csv')\ndf_samp_subm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images = ImageId=df_samp_subm.ImageId_ClassId.str.slice(0, -2).unique()\ndf = pd.DataFrame.from_dict({'ImageId':images})\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = (SegmentationItemList([], convert_mode='L', ignore_empty=True).split_none().label_empty()\n         .databunch(bs=BS, num_workers=2)\n         .normalize(STATS))\ndata.add_test(SegmentationItemList.from_df(df, path=DATA, folder='test_images', cols='ImageId', convert_mode='L'))\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Collect Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"thresh = torch.tensor(THRESHOLDS, dtype=torch.float32, device='cuda:0')[None,:,None,None]\ntta_wgts = TTA_WEIGHTS.view(5, *([1]*4)).cuda()\nbatch_images = np.pad(images, (0,np.abs(len(images) % -BS)), 'constant', constant_values='').reshape(-1, BS)\n\ndef get_predictions(xb:Tensor):\n    \"\"\"Get predictions with TTA, and block processing\"\"\"\n    pred_raws = []\n    # Original prediction\n    with torch.no_grad(): pred_raws.append(mdl(xb).sigmoid())\n    # Block predictions with TTA\n    blks = (xb .view(*xb.shape[:-1], 4, -1)\n               .permute(0, 3, 1, 2, 4))\n    blks = blks.reshape(-1, *blks.shape[2:])\n    for flip in range(4):\n        # Apply flip to input\n        if flip == 1: blks = blks.flip(2) # hflip\n        if flip == 2: blks = blks.flip(3) # hflip + vflip\n        if flip == 3: blks = blks.flip(2) # vflip (reversed hflip)\n\n        # Predict - (BSxBlk)xCxHxW\n        with torch.no_grad(): blk_raw = mdl(blks)\n        # Revert flip on prediction\n        if flip == 1: blk_raw = blk_raw.flip(2) # hflip\n        if flip == 2: blk_raw = blk_raw.flip((2,3)) # hflip + vflip\n        if flip == 3: blk_raw = blk_raw.flip(3) # vflip\n        # Reorder blocks - to BxCxHxBlkxW\n        blk_raw = (blk_raw.view(-1, 4, *blk_raw.shape[1:])\n                          .permute(0, 2, 3, 1, 4))\n        # to Now BxCxHxW\n        blk_raw =  blk_raw.reshape(*blk_raw.shape[:-2], -1)\n        pred_raws.append(blk_raw.sigmoid())\n    # Multiply by weights and sum, then theshold\n    pred_raw = (torch.stack(pred_raws) * tta_wgts).sum(0)\n    return (pred_raw > thresh).to(dtype=torch.uint8, device='cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#it = iter(zip(data.test_dl,batch_images))\n# (xb,_),imgs = next(it)\n# xb.shape, xb.device, xb.dtype, imgs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = []\nfor (xb,_),imgs in zip(progress_bar(data.test_dl),batch_images):\n    preds = get_predictions(xb)\n    for img,img_pred in zip(imgs,preds):\n        for c,pred in zip(CLASSES,img_pred):\n            n_comp, comps = cv2.connectedComponents(pred.to(dtype=torch.uint8, device='cpu').numpy())\n            res = np.zeros((256, 1600), np.uint8)\n            for comp in range(1, n_comp):\n                p = (comps == comp)\n                if p.sum() > MIN_PXS:\n                    res[p] = 1\n            rle = mask2rle(res)\n            predictions.append([f\"{img}_{c}\", rle])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_subm = pd.DataFrame(predictions, columns=['ImageId_ClassId', 'EncodedPixels'])\ndf_subm.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_subm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df_subm.EncodedPixels!=\"\").sum()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}