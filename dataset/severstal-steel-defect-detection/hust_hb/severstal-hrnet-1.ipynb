{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nimport time\n\ns_time = time.time()\n\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"post-process"},{"metadata":{"trusted":true},"cell_type":"code","source":"def post_process(mask, min_size):\n    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n\n    predict = np.zeros((256, 1600), np.float32)\n    for c in range(1, num_component):\n        p = (component == c)\n        if p.sum() > min_size:\n            predict[p] = 1\n\n    return predict\n\ndef probability_mask_to_probability_label(probability):\n    probability = F.adaptive_max_pool2d(probability,1).squeeze(-1).squeeze(-1)\n    return probability\n\ndef run_length_encode(mask):\n    m = mask.T.flatten()\n    if m.sum()==0:\n        rle=''\n    else:\n        m   = np.concatenate([[0], m, [0]])\n        run = np.where(m[1:] != m[:-1])[0] + 1\n        run[1::2] -= run[::2]\n        rle = ' '.join(str(r) for r in run)\n    return rle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DataSet**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport os.path\nimport imageio\nimport glob\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\nfrom PIL import Image\nimport random\nimport torchvision.transforms as transforms\nimport os\n\nclass SteelDataset(Dataset):\n\n    def __init__(self, defect_root,\n                 resize_long=None, img_normal=True, hor_flip=False,\n                 crop_size=None, to_torch=True, random_hue=False):\n\n        self.img_name_list = glob.glob(defect_root + '/*.jpg')\n        self.defect_root = defect_root\n        # self.annotaions = pd.read_csv(csv_file, converters={u'ImageId': str})\n\n        self.resize_long = resize_long\n        self.crop_size = crop_size\n        self.img_normal = img_normal\n        self.hor_flip = hor_flip\n        self.to_torch = to_torch\n        self.random_hue = random_hue\n\n    def __len__(self):\n        return len(self.img_name_list)\n\n    def __getitem__(self, idx):\n        name = self.img_name_list[idx]\n\n        img = Image.open(name)\n\n        if self.resize_long:\n            target_long = random.randint(self.resize_long[0], self.resize_long[1])\n            img = transforms.Resize((256, target_long))(img)  # w, h\n\n        if self.hor_flip:\n            img = transforms.RandomHorizontalFlip()(img)\n\n        if self.crop_size:\n            img = transforms.RandomCrop((self.crop_size[0], self.crop_size[1]), padding=0, pad_if_needed=True)(img)\n\n        if self.random_hue:\n            # target_hue = random.random()\n            # img = transforms.ColorJitter(hue=target_hue)(img)\n            img = transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.25)(img)\n            # img = transforms.ColorJitter(hue = 0.5)(img) \n\n        if self.to_torch:\n            img = transforms.ToTensor()(img)\n\n        if self.img_normal:\n            img = transforms.Normalize(mean=(0.485, 0.456, 0.406),\n                                       std=(0.229, 0.224, 0.225))(img)\n\n        return {'name': name, 'img': img}\n\n    \nclass SteelDatasetMSF(Dataset):\n\n    def __init__(self, defect_root,\n                 resize_long=None, img_normal=True, hor_flip=False,\n                 crop_size=None, to_torch=True, random_hue=False, scales=[1.]):\n\n        self.img_name_list = glob.glob(defect_root + '/*.jpg')\n        self.defect_root = defect_root\n        self.scales = scales\n        # self.annotaions = pd.read_csv(csv_file, converters={u'ImageId': str})\n\n        self.resize_long = resize_long\n        self.crop_size = crop_size\n        self.img_normal = img_normal\n        self.hor_flip = hor_flip\n        self.to_torch = to_torch\n        self.random_hue = random_hue\n\n    def __len__(self):\n        return len(self.img_name_list)\n\n    def __getitem__(self, idx):\n        name = self.img_name_list[idx]\n\n        img = Image.open(name)\n        ori_w, ori_h = img.size\n        \n        ms_img_list = []\n        for s in self.scales:\n            if s == 1:\n                s_img = img\n            else:\n                w = int(s*ori_w)\n                s_img = transforms.Resize((ori_h, w))(img)  # w, h\n            \n            if self.random_hue:\n                # target_hue = random.random()\n                # img = transforms.ColorJitter(hue=target_hue)(img)\n                s_img = transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.25)(s_img)\n                # img = transforms.ColorJitter(hue = 0.5)(img) \n            if self.to_torch:\n                s_img = transforms.ToTensor()(s_img)\n\n            if self.img_normal:\n                s_img = transforms.Normalize(mean=(0.485, 0.456, 0.406),\n                                             std=(0.229, 0.224, 0.225))(s_img)\n            ms_img_list.append(s_img)\n\n            if len(self.scales) == 1:\n                ms_img_list = ms_img_list[0]\n\n        \n\n        return {'name': name, 'img': ms_img_list}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"HRNet Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport yaml\n\nBatchNorm2d = nn.BatchNorm2d\nBN_MOMENTUM = 0.01\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=False)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = out + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n                               bias=False)\n        self.bn3 = BatchNorm2d(planes * self.expansion,\n                               momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=False)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = out + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass HighResolutionModule(nn.Module):\n    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n                 num_channels, fuse_method, multi_scale_output=True):\n        super(HighResolutionModule, self).__init__()\n        self._check_branches(\n            num_branches, blocks, num_blocks, num_inchannels, num_channels)\n\n        self.num_inchannels = num_inchannels\n        self.fuse_method = fuse_method\n        self.num_branches = num_branches\n\n        self.multi_scale_output = multi_scale_output\n\n        self.branches = self._make_branches(\n            num_branches, blocks, num_blocks, num_channels)\n        self.fuse_layers = self._make_fuse_layers()\n        self.relu = nn.ReLU(inplace=False)\n\n    def _check_branches(self, num_branches, blocks, num_blocks,\n                        num_inchannels, num_channels):\n        if num_branches != len(num_blocks):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(\n                num_branches, len(num_blocks))\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_channels):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(\n                num_branches, len(num_channels))\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_inchannels):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(\n                num_branches, len(num_inchannels))\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n                         stride=1):\n        downsample = None\n        if stride != 1 or \\\n                self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.num_inchannels[branch_index],\n                          num_channels[branch_index] * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm2d(num_channels[branch_index] * block.expansion,\n                            momentum=BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(self.num_inchannels[branch_index],\n                            num_channels[branch_index], stride, downsample))\n        self.num_inchannels[branch_index] = \\\n            num_channels[branch_index] * block.expansion\n        for i in range(1, num_blocks[branch_index]):\n            layers.append(block(self.num_inchannels[branch_index],\n                                num_channels[branch_index]))\n\n        return nn.Sequential(*layers)\n\n    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n        branches = []\n\n        for i in range(num_branches):\n            branches.append(\n                self._make_one_branch(i, block, num_blocks, num_channels))\n\n        return nn.ModuleList(branches)\n\n    def _make_fuse_layers(self):\n        if self.num_branches == 1:\n            return None\n\n        num_branches = self.num_branches\n        num_inchannels = self.num_inchannels\n        fuse_layers = []\n        for i in range(num_branches if self.multi_scale_output else 1):\n            fuse_layer = []\n            for j in range(num_branches):\n                if j > i:\n                    fuse_layer.append(nn.Sequential(\n                        nn.Conv2d(num_inchannels[j],\n                                  num_inchannels[i],\n                                  1,\n                                  1,\n                                  0,\n                                  bias=False),\n                        BatchNorm2d(num_inchannels[i], momentum=BN_MOMENTUM)))\n                elif j == i:\n                    fuse_layer.append(None)\n                else:\n                    conv3x3s = []\n                    for k in range(i - j):\n                        if k == i - j - 1:\n                            num_outchannels_conv3x3 = num_inchannels[i]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_inchannels[j],\n                                          num_outchannels_conv3x3,\n                                          3, 2, 1, bias=False),\n                                BatchNorm2d(num_outchannels_conv3x3,\n                                            momentum=BN_MOMENTUM)))\n                        else:\n                            num_outchannels_conv3x3 = num_inchannels[j]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_inchannels[j],\n                                          num_outchannels_conv3x3,\n                                          3, 2, 1, bias=False),\n                                BatchNorm2d(num_outchannels_conv3x3,\n                                            momentum=BN_MOMENTUM),\n                                nn.ReLU(inplace=False)))\n                    fuse_layer.append(nn.Sequential(*conv3x3s))\n            fuse_layers.append(nn.ModuleList(fuse_layer))\n\n        return nn.ModuleList(fuse_layers)\n\n    def get_num_inchannels(self):\n        return self.num_inchannels\n\n    def forward(self, x):\n        if self.num_branches == 1:\n            return [self.branches[0](x[0])]\n\n        for i in range(self.num_branches):\n            x[i] = self.branches[i](x[i])\n\n        x_fuse = []\n        for i in range(len(self.fuse_layers)):\n            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n            for j in range(1, self.num_branches):\n                if i == j:\n                    y = y + x[j]\n                elif j > i:\n                    width_output = x[i].shape[-1]\n                    height_output = x[i].shape[-2]\n                    y = y + F.interpolate(\n                        self.fuse_layers[i][j](x[j]),\n                        size=[height_output, width_output],\n                        mode='bilinear')\n                else:\n                    y = y + self.fuse_layers[i][j](x[j])\n            x_fuse.append(self.relu(y))\n\n        return x_fuse\n\n\nblocks_dict = {\n    'BASIC': BasicBlock,\n    'BOTTLENECK': Bottleneck\n}\n\n\nclass HighResolutionNet(nn.Module):\n\n    def __init__(self, config, **kwargs):\n        extra = config['MODEL']['EXTRA']\n        super(HighResolutionNet, self).__init__()\n\n        # stem net\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn1 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn2 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=False)\n\n        self.stage1_cfg = extra['STAGE1']\n        num_channels = self.stage1_cfg['NUM_CHANNELS'][0]\n        block = blocks_dict[self.stage1_cfg['BLOCK']]\n        num_blocks = self.stage1_cfg['NUM_BLOCKS'][0]\n        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks)\n        stage1_out_channel = block.expansion * num_channels\n\n        self.stage2_cfg = extra['STAGE2']\n        num_channels = self.stage2_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage2_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition1 = self._make_transition_layer(\n            [stage1_out_channel], num_channels)\n        self.stage2, pre_stage_channels = self._make_stage(\n            self.stage2_cfg, num_channels)\n\n        self.stage3_cfg = extra['STAGE3']\n        num_channels = self.stage3_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage3_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition2 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage3, pre_stage_channels = self._make_stage(\n            self.stage3_cfg, num_channels)\n\n        self.stage4_cfg = extra['STAGE4']\n        num_channels = self.stage4_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage4_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition3 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage4, pre_stage_channels = self._make_stage(\n            self.stage4_cfg, num_channels, multi_scale_output=True)\n\n        last_inp_channels = np.int(np.sum(pre_stage_channels))\n\n        self.last_layer = nn.Sequential(\n            nn.Conv2d(\n                in_channels=last_inp_channels,\n                out_channels=last_inp_channels,\n                kernel_size=1,\n                stride=1,\n                padding=0),\n            BatchNorm2d(last_inp_channels, momentum=BN_MOMENTUM),\n            nn.ReLU(inplace=False),\n            nn.Conv2d(\n                in_channels=last_inp_channels,\n                out_channels=config['MODEL']['NUM_CLASSES'],\n                kernel_size=extra['FINAL_CONV_KERNEL'],\n                stride=1,\n                padding=1 if extra['FINAL_CONV_KERNEL'] == 3 else 0)\n        )\n\n        self.backbone = nn.ModuleList([self.conv1, self.bn1, self.conv2, self.bn2, self.layer1, self.transition1,\n                                       self.stage2, self.transition2, self.stage3, self.transition3, self.stage4])\n        self.fuse = nn.ModuleList([self.last_layer])\n\n    def _make_transition_layer(\n            self, num_channels_pre_layer, num_channels_cur_layer):\n        num_branches_cur = len(num_channels_cur_layer)\n        num_branches_pre = len(num_channels_pre_layer)\n\n        transition_layers = []\n        for i in range(num_branches_cur):\n            if i < num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(nn.Sequential(\n                        nn.Conv2d(num_channels_pre_layer[i],\n                                  num_channels_cur_layer[i],\n                                  3,\n                                  1,\n                                  1,\n                                  bias=False),\n                        BatchNorm2d(\n                            num_channels_cur_layer[i], momentum=BN_MOMENTUM),\n                        nn.ReLU(inplace=False)))\n                else:\n                    transition_layers.append(None)\n            else:\n                conv3x3s = []\n                for j in range(i + 1 - num_branches_pre):\n                    inchannels = num_channels_pre_layer[-1]\n                    outchannels = num_channels_cur_layer[i] \\\n                        if j == i - num_branches_pre else inchannels\n                    conv3x3s.append(nn.Sequential(\n                        nn.Conv2d(\n                            inchannels, outchannels, 3, 2, 1, bias=False),\n                        BatchNorm2d(outchannels, momentum=BN_MOMENTUM),\n                        nn.ReLU(inplace=False)))\n                transition_layers.append(nn.Sequential(*conv3x3s))\n\n        return nn.ModuleList(transition_layers)\n\n    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(inplanes, planes, stride, downsample))\n        inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_stage(self, layer_config, num_inchannels,\n                    multi_scale_output=True):\n        num_modules = layer_config['NUM_MODULES']\n        num_branches = layer_config['NUM_BRANCHES']\n        num_blocks = layer_config['NUM_BLOCKS']\n        num_channels = layer_config['NUM_CHANNELS']\n        block = blocks_dict[layer_config['BLOCK']]\n        fuse_method = layer_config['FUSE_METHOD']\n\n        modules = []\n        for i in range(num_modules):\n            # multi_scale_output is only used last module\n            if not multi_scale_output and i == num_modules - 1:\n                reset_multi_scale_output = False\n            else:\n                reset_multi_scale_output = True\n            modules.append(\n                HighResolutionModule(num_branches,\n                                     block,\n                                     num_blocks,\n                                     num_inchannels,\n                                     num_channels,\n                                     fuse_method,\n                                     reset_multi_scale_output)\n            )\n            num_inchannels = modules[-1].get_num_inchannels()\n\n        return nn.Sequential(*modules), num_inchannels\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n\n        x_list = []\n        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n            if self.transition1[i] is not None:\n                x_list.append(self.transition1[i](x))\n            else:\n                x_list.append(x)\n        y_list = self.stage2(x_list)\n\n        x_list = []\n        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n            if self.transition2[i] is not None:\n                if i < self.stage2_cfg['NUM_BRANCHES']:\n                    x_list.append(self.transition2[i](y_list[i]))\n                else:\n                    x_list.append(self.transition2[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage3(x_list)\n\n        x_list = []\n        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n            if self.transition3[i] is not None:\n                if i < self.stage3_cfg['NUM_BRANCHES']:\n                    x_list.append(self.transition3[i](y_list[i]))\n                else:\n                    x_list.append(self.transition3[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        x = self.stage4(x_list)\n\n        # Upsampling\n        x0_h, x0_w = x[0].size(2), x[0].size(3)\n        x1 = F.upsample(x[1], size=(x0_h, x0_w), mode='bilinear')\n        x2 = F.upsample(x[2], size=(x0_h, x0_w), mode='bilinear')\n        x3 = F.upsample(x[3], size=(x0_h, x0_w), mode='bilinear')\n\n        x = torch.cat([x[0], x1, x2, x3], 1)\n\n        x = self.last_layer(x)\n        return x\n\n    def trainable_parameters(self):\n        return list(self.backbone.parameters()), list(self.fuse.parameters())\n\n    def init_weights(self, pretrained='', ):\n        # logger.info('=> init weights from normal distribution')\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.normal_(m.weight, std=0.001)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        if os.path.isfile(pretrained):\n            pretrained_dict = torch.load(pretrained)\n            model_dict = self.state_dict()\n            pretrained_dict = {k: v for k, v in pretrained_dict.items()\n                               if k in model_dict.keys()}\n\n            # for k, _ in pretrained_dict.items():\n            #     print('load', k, end='!')\n            print('length of pretrained_dict:', len(pretrained_dict.items()))\n\n            model_dict.update(pretrained_dict)\n            self.load_state_dict(model_dict)\n\n\nclass CAM(HighResolutionNet):\n    def __init__(self, cfg, **kwargs):\n        super(CAM, self).__init__(cfg, **kwargs)\n\n    def forward(self, x):\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n\n        x_list = []\n        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n            if self.transition1[i] is not None:\n                x_list.append(self.transition1[i](x))\n            else:\n                x_list.append(x)\n        y_list = self.stage2(x_list)\n\n        x_list = []\n        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n            if self.transition2[i] is not None:\n                if i < self.stage2_cfg['NUM_BRANCHES']:\n                    x_list.append(self.transition2[i](y_list[i]))\n                else:\n                    x_list.append(self.transition2[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage3(x_list)\n\n        x_list = []\n        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n            if self.transition3[i] is not None:\n                if i < self.stage3_cfg['NUM_BRANCHES']:\n                    x_list.append(self.transition3[i](y_list[i]))\n                else:\n                    x_list.append(self.transition3[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        x = self.stage4(x_list)\n\n        # Upsampling\n        x0_h, x0_w = x[0].size(2), x[0].size(3)\n        x1 = F.upsample(x[1], size=(x0_h, x0_w), mode='bilinear')\n        x2 = F.upsample(x[2], size=(x0_h, x0_w), mode='bilinear')\n        x3 = F.upsample(x[3], size=(x0_h, x0_w), mode='bilinear')\n\n        x = torch.cat([x[0], x1, x2, x3], 1)\n\n        x = self.last_layer(x)\n        x = F.relu(x)\n\n        x = x[0] + x[1].flip(-1)\n\n        return x\n\n\ndef get_hr_seg_model(cfg, **kwargs):\n    cfg['MODEL']['NUM_CLASSES'] = 5\n    model = HighResolutionNet(cfg, **kwargs)\n    # model.init_weights(cfg['MODEL']['PRETRAINED'])\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"HRNet_OCR_model"},{"metadata":{"trusted":true},"cell_type":"code","source":"ALIGN_CORNERS = True\nBN_MOMENTUM = 0.1\nrelu_inplace = True\n\nBatchNorm2d_class = BatchNorm2d = torch.nn.BatchNorm2d\n\nclass ModuleHelper:\n\n    @staticmethod\n    def BNReLU(num_features, bn_type=None, **kwargs):\n        return nn.Sequential(\n            BatchNorm2d(num_features, **kwargs),\n            nn.ReLU()\n        )\n\n    @staticmethod\n    def BatchNorm2d(*args, **kwargs):\n        return BatchNorm2d\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass SpatialGather_Module(nn.Module):\n    \"\"\"\n        Aggregate the context features according to the initial \n        predicted probability distribution.\n        Employ the soft-weighted method to aggregate the context.\n    \"\"\"\n    def __init__(self, cls_num=0, scale=1):\n        super(SpatialGather_Module, self).__init__()\n        self.cls_num = cls_num\n        self.scale = scale\n\n    def forward(self, feats, probs):\n        batch_size, c, h, w = probs.size(0), probs.size(1), probs.size(2), probs.size(3)\n        probs = probs.view(batch_size, c, -1)\n        feats = feats.view(batch_size, feats.size(1), -1)\n        feats = feats.permute(0, 2, 1) # batch x hw x c \n        probs = F.softmax(self.scale * probs, dim=2)# batch x k x hw\n        ocr_context = torch.matmul(probs, feats)\\\n        .permute(0, 2, 1).unsqueeze(3)# batch x k x c\n        return ocr_context\n\n\nclass _ObjectAttentionBlock(nn.Module):\n    '''\n    The basic implementation for object context block\n    Input:\n        N X C X H X W\n    Parameters:\n        in_channels       : the dimension of the input feature map\n        key_channels      : the dimension after the key/query transform\n        scale             : choose the scale to downsample the input feature maps (save memory cost)\n        bn_type           : specify the bn type\n    Return:\n        N X C X H X W\n    '''\n    def __init__(self, \n                 in_channels, \n                 key_channels, \n                 scale=1, \n                 bn_type=None):\n        super(_ObjectAttentionBlock, self).__init__()\n        self.scale = scale\n        self.in_channels = in_channels\n        self.key_channels = key_channels\n        self.pool = nn.MaxPool2d(kernel_size=(scale, scale))\n        self.f_pixel = nn.Sequential(\n            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels,\n                kernel_size=1, stride=1, padding=0, bias=False),\n            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),\n            nn.Conv2d(in_channels=self.key_channels, out_channels=self.key_channels,\n                kernel_size=1, stride=1, padding=0, bias=False),\n            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),\n        )\n        self.f_object = nn.Sequential(\n            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels,\n                kernel_size=1, stride=1, padding=0, bias=False),\n            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),\n            nn.Conv2d(in_channels=self.key_channels, out_channels=self.key_channels,\n                kernel_size=1, stride=1, padding=0, bias=False),\n            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),\n        )\n        self.f_down = nn.Sequential(\n            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels,\n                kernel_size=1, stride=1, padding=0, bias=False),\n            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),\n        )\n        self.f_up = nn.Sequential(\n            nn.Conv2d(in_channels=self.key_channels, out_channels=self.in_channels,\n                kernel_size=1, stride=1, padding=0, bias=False),\n            ModuleHelper.BNReLU(self.in_channels, bn_type=bn_type),\n        )\n\n    def forward(self, x, proxy):\n        batch_size, h, w = x.size(0), x.size(2), x.size(3)\n        if self.scale > 1:\n            x = self.pool(x)\n\n        query = self.f_pixel(x).view(batch_size, self.key_channels, -1)\n        query = query.permute(0, 2, 1)\n        key = self.f_object(proxy).view(batch_size, self.key_channels, -1)\n        value = self.f_down(proxy).view(batch_size, self.key_channels, -1)\n        value = value.permute(0, 2, 1)\n\n        sim_map = torch.matmul(query, key)\n        sim_map = (self.key_channels**-.5) * sim_map\n        sim_map = F.softmax(sim_map, dim=-1)   \n\n        # add bg context ...\n        context = torch.matmul(sim_map, value)\n        context = context.permute(0, 2, 1).contiguous()\n        context = context.view(batch_size, self.key_channels, *x.size()[2:])\n        context = self.f_up(context)\n        if self.scale > 1:\n            context = F.interpolate(input=context, size=(h, w), mode='bilinear', align_corners=ALIGN_CORNERS)\n\n        return context\n\n\nclass ObjectAttentionBlock2D(_ObjectAttentionBlock):\n    def __init__(self, \n                 in_channels, \n                 key_channels, \n                 scale=1, \n                 bn_type=None):\n        super(ObjectAttentionBlock2D, self).__init__(in_channels,\n                                                     key_channels,\n                                                     scale, \n                                                     bn_type=bn_type)\n\n\nclass SpatialOCR_Module(nn.Module):\n    \"\"\"\n    Implementation of the OCR module:\n    We aggregate the global object representation to update the representation for each pixel.\n    \"\"\"\n    def __init__(self, \n                 in_channels, \n                 key_channels, \n                 out_channels, \n                 scale=1, \n                 dropout=0.1, \n                 bn_type=None):\n        super(SpatialOCR_Module, self).__init__()\n        self.object_context_block = ObjectAttentionBlock2D(in_channels, \n                                                           key_channels, \n                                                           scale, \n                                                           bn_type)\n        _in_channels = 2 * in_channels\n\n        self.conv_bn_dropout = nn.Sequential(\n            nn.Conv2d(_in_channels, out_channels, kernel_size=1, padding=0, bias=False),\n            ModuleHelper.BNReLU(out_channels, bn_type=bn_type),\n            nn.Dropout2d(dropout)\n        )\n\n    def forward(self, feats, proxy_feats):\n        context = self.object_context_block(feats, proxy_feats)\n\n        output = self.conv_bn_dropout(torch.cat([context, feats], 1))\n\n        return output\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = out + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n                               bias=False)\n        self.bn3 = BatchNorm2d(planes * self.expansion,\n                               momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=relu_inplace)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = out + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass HighResolutionModule(nn.Module):\n    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n                 num_channels, fuse_method, multi_scale_output=True):\n        super(HighResolutionModule, self).__init__()\n        self._check_branches(\n            num_branches, blocks, num_blocks, num_inchannels, num_channels)\n\n        self.num_inchannels = num_inchannels\n        self.fuse_method = fuse_method\n        self.num_branches = num_branches\n\n        self.multi_scale_output = multi_scale_output\n\n        self.branches = self._make_branches(\n            num_branches, blocks, num_blocks, num_channels)\n        self.fuse_layers = self._make_fuse_layers()\n        self.relu = nn.ReLU(inplace=relu_inplace)\n\n    def _check_branches(self, num_branches, blocks, num_blocks,\n                        num_inchannels, num_channels):\n        if num_branches != len(num_blocks):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(\n                num_branches, len(num_blocks))\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_channels):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(\n                num_branches, len(num_channels))\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_inchannels):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(\n                num_branches, len(num_inchannels))\n            raise ValueError(error_msg)\n\n    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n                         stride=1):\n        downsample = None\n        if stride != 1 or \\\n           self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.num_inchannels[branch_index],\n                          num_channels[branch_index] * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm2d(num_channels[branch_index] * block.expansion,\n                            momentum=BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(self.num_inchannels[branch_index],\n                            num_channels[branch_index], stride, downsample))\n        self.num_inchannels[branch_index] = \\\n            num_channels[branch_index] * block.expansion\n        for i in range(1, num_blocks[branch_index]):\n            layers.append(block(self.num_inchannels[branch_index],\n                                num_channels[branch_index]))\n\n        return nn.Sequential(*layers)\n\n    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n        branches = []\n\n        for i in range(num_branches):\n            branches.append(\n                self._make_one_branch(i, block, num_blocks, num_channels))\n\n        return nn.ModuleList(branches)\n\n    def _make_fuse_layers(self):\n        if self.num_branches == 1:\n            return None\n\n        num_branches = self.num_branches\n        num_inchannels = self.num_inchannels\n        fuse_layers = []\n        for i in range(num_branches if self.multi_scale_output else 1):\n            fuse_layer = []\n            for j in range(num_branches):\n                if j > i:\n                    fuse_layer.append(nn.Sequential(\n                        nn.Conv2d(num_inchannels[j],\n                                  num_inchannels[i],\n                                  1,\n                                  1,\n                                  0,\n                                  bias=False),\n                        BatchNorm2d(num_inchannels[i], momentum=BN_MOMENTUM)))\n                elif j == i:\n                    fuse_layer.append(None)\n                else:\n                    conv3x3s = []\n                    for k in range(i-j):\n                        if k == i - j - 1:\n                            num_outchannels_conv3x3 = num_inchannels[i]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_inchannels[j],\n                                          num_outchannels_conv3x3,\n                                          3, 2, 1, bias=False),\n                                BatchNorm2d(num_outchannels_conv3x3,\n                                            momentum=BN_MOMENTUM)))\n                        else:\n                            num_outchannels_conv3x3 = num_inchannels[j]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_inchannels[j],\n                                          num_outchannels_conv3x3,\n                                          3, 2, 1, bias=False),\n                                BatchNorm2d(num_outchannels_conv3x3,\n                                            momentum=BN_MOMENTUM),\n                                nn.ReLU(inplace=relu_inplace)))\n                    fuse_layer.append(nn.Sequential(*conv3x3s))\n            fuse_layers.append(nn.ModuleList(fuse_layer))\n\n        return nn.ModuleList(fuse_layers)\n\n    def get_num_inchannels(self):\n        return self.num_inchannels\n\n    def forward(self, x):\n        if self.num_branches == 1:\n            return [self.branches[0](x[0])]\n\n        for i in range(self.num_branches):\n            x[i] = self.branches[i](x[i])\n\n        x_fuse = []\n        for i in range(len(self.fuse_layers)):\n            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n            for j in range(1, self.num_branches):\n                if i == j:\n                    y = y + x[j]\n                elif j > i:\n                    width_output = x[i].shape[-1]\n                    height_output = x[i].shape[-2]\n                    y = y + F.interpolate(\n                        self.fuse_layers[i][j](x[j]),\n                        size=[height_output, width_output],\n                        mode='bilinear', align_corners=ALIGN_CORNERS)\n                else:\n                    y = y + self.fuse_layers[i][j](x[j])\n            x_fuse.append(self.relu(y))\n\n        return x_fuse\n\n\nblocks_dict = {\n    'BASIC': BasicBlock,\n    'BOTTLENECK': Bottleneck\n}\n\n\nclass HighResolutionOCRNet(nn.Module):\n\n    def __init__(self, config, **kwargs):\n        global ALIGN_CORNERS\n        extra = config['MODEL']['EXTRA']\n        ocr_cfg = config['MODEL']['OCR']\n        super(HighResolutionOCRNet, self).__init__()\n        ALIGN_CORNERS = True\n\n        # stem net\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn1 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn2 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=relu_inplace)\n\n        self.stage1_cfg = extra['STAGE1']\n        num_channels = self.stage1_cfg['NUM_CHANNELS'][0]\n        block = blocks_dict[self.stage1_cfg['BLOCK']]\n        num_blocks = self.stage1_cfg['NUM_BLOCKS'][0]\n        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks)\n        stage1_out_channel = block.expansion*num_channels\n\n        self.stage2_cfg = extra['STAGE2']\n        num_channels = self.stage2_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage2_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition1 = self._make_transition_layer(\n            [stage1_out_channel], num_channels)\n        self.stage2, pre_stage_channels = self._make_stage(\n            self.stage2_cfg, num_channels)\n\n        self.stage3_cfg = extra['STAGE3']\n        num_channels = self.stage3_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage3_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition2 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage3, pre_stage_channels = self._make_stage(\n            self.stage3_cfg, num_channels)\n\n        self.stage4_cfg = extra['STAGE4']\n        num_channels = self.stage4_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage4_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition3 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage4, pre_stage_channels = self._make_stage(\n            self.stage4_cfg, num_channels, multi_scale_output=True)\n\n        last_inp_channels = np.int(np.sum(pre_stage_channels))\n        ocr_mid_channels = ocr_cfg['MID_CHANNELS']\n        ocr_key_channels = ocr_cfg['KEY_CHANNELS']\n\n        self.conv3x3_ocr = nn.Sequential(\n            nn.Conv2d(last_inp_channels, ocr_mid_channels,\n                      kernel_size=3, stride=1, padding=1),\n            BatchNorm2d(ocr_mid_channels),\n            nn.ReLU(inplace=relu_inplace),\n        )\n        self.ocr_gather_head = SpatialGather_Module(config['DATASET']['NUM_CLASSES'])\n\n        self.ocr_distri_head = SpatialOCR_Module(in_channels=ocr_mid_channels,\n                                                 key_channels=ocr_key_channels,\n                                                 out_channels=ocr_mid_channels,\n                                                 scale=1,\n                                                 dropout=0.05,\n                                                 )\n        self.cls_head = nn.Conv2d(\n            ocr_mid_channels, config['DATASET']['NUM_CLASSES'], kernel_size=1, stride=1, padding=0, bias=True)\n\n        self.aux_head = nn.Sequential(\n            nn.Conv2d(last_inp_channels, last_inp_channels,\n                      kernel_size=1, stride=1, padding=0),\n            BatchNorm2d(last_inp_channels),\n            nn.ReLU(inplace=relu_inplace),\n            nn.Conv2d(last_inp_channels, config['DATASET']['NUM_CLASSES'],\n                      kernel_size=1, stride=1, padding=0, bias=True)\n        )\n        \n        self.backbone = nn.ModuleList([self.conv1, self.bn1, self.conv2, self.bn2, self.layer1, self.transition1,\n                                       self.stage2, self.transition2, self.stage3, self.transition3, self.stage4])\n        self.fuse = nn.ModuleList([self.aux_head, self.conv3x3_ocr, self.ocr_gather_head, self.ocr_distri_head, self.cls_head])\n        \n    def _make_transition_layer(\n            self, num_channels_pre_layer, num_channels_cur_layer):\n        num_branches_cur = len(num_channels_cur_layer)\n        num_branches_pre = len(num_channels_pre_layer)\n\n        transition_layers = []\n        for i in range(num_branches_cur):\n            if i < num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(nn.Sequential(\n                        nn.Conv2d(num_channels_pre_layer[i],\n                                  num_channels_cur_layer[i],\n                                  3,\n                                  1,\n                                  1,\n                                  bias=False),\n                        BatchNorm2d(\n                            num_channels_cur_layer[i], momentum=BN_MOMENTUM),\n                        nn.ReLU(inplace=relu_inplace)))\n                else:\n                    transition_layers.append(None)\n            else:\n                conv3x3s = []\n                for j in range(i+1-num_branches_pre):\n                    inchannels = num_channels_pre_layer[-1]\n                    outchannels = num_channels_cur_layer[i] \\\n                        if j == i-num_branches_pre else inchannels\n                    conv3x3s.append(nn.Sequential(\n                        nn.Conv2d(\n                            inchannels, outchannels, 3, 2, 1, bias=False),\n                        BatchNorm2d(outchannels, momentum=BN_MOMENTUM),\n                        nn.ReLU(inplace=relu_inplace)))\n                transition_layers.append(nn.Sequential(*conv3x3s))\n\n        return nn.ModuleList(transition_layers)\n\n    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(inplanes, planes, stride, downsample))\n        inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_stage(self, layer_config, num_inchannels,\n                    multi_scale_output=True):\n        num_modules = layer_config['NUM_MODULES']\n        num_branches = layer_config['NUM_BRANCHES']\n        num_blocks = layer_config['NUM_BLOCKS']\n        num_channels = layer_config['NUM_CHANNELS']\n        block = blocks_dict[layer_config['BLOCK']]\n        fuse_method = layer_config['FUSE_METHOD']\n\n        modules = []\n        for i in range(num_modules):\n            # multi_scale_output is only used last module\n            if not multi_scale_output and i == num_modules - 1:\n                reset_multi_scale_output = False\n            else:\n                reset_multi_scale_output = True\n            modules.append(\n                HighResolutionModule(num_branches,\n                                     block,\n                                     num_blocks,\n                                     num_inchannels,\n                                     num_channels,\n                                     fuse_method,\n                                     reset_multi_scale_output)\n            )\n            num_inchannels = modules[-1].get_num_inchannels()\n\n        return nn.Sequential(*modules), num_inchannels\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n\n        x_list = []\n        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n            if self.transition1[i] is not None:\n                x_list.append(self.transition1[i](x))\n            else:\n                x_list.append(x)\n        y_list = self.stage2(x_list)\n\n        x_list = []\n        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n            if self.transition2[i] is not None:\n                if i < self.stage2_cfg['NUM_BRANCHES']:\n                    x_list.append(self.transition2[i](y_list[i]))\n                else:\n                    x_list.append(self.transition2[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage3(x_list)\n\n        x_list = []\n        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n            if self.transition3[i] is not None:\n                if i < self.stage3_cfg['NUM_BRANCHES']:\n                    x_list.append(self.transition3[i](y_list[i]))\n                else:\n                    x_list.append(self.transition3[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        x = self.stage4(x_list)\n\n        # Upsampling\n        x0_h, x0_w = x[0].size(2), x[0].size(3)\n        x1 = F.interpolate(x[1], size=(x0_h, x0_w),\n                        mode='bilinear', align_corners=ALIGN_CORNERS)\n        x2 = F.interpolate(x[2], size=(x0_h, x0_w),\n                        mode='bilinear', align_corners=ALIGN_CORNERS)\n        x3 = F.interpolate(x[3], size=(x0_h, x0_w),\n                        mode='bilinear', align_corners=ALIGN_CORNERS)\n\n        feats = torch.cat([x[0], x1, x2, x3], 1)\n\n        out_aux_seg = []\n\n        # ocr\n        out_aux = self.aux_head(feats)\n        # compute contrast feature\n        feats = self.conv3x3_ocr(feats)\n\n        context = self.ocr_gather_head(feats, out_aux)\n        feats = self.ocr_distri_head(feats, context)\n\n        out = self.cls_head(feats)\n\n        out_aux_seg.append(out_aux)\n        out_aux_seg.append(out)\n\n        return out_aux_seg\n\n    def init_weights(self, pretrained='',):\n        print('=> init weights from normal distribution')\n        for name, m in self.named_modules():\n            if any(part in name for part in {'cls', 'aux', 'ocr'}):\n                # print('skipped', name)\n                continue\n            if isinstance(m, nn.Conv2d):\n                nn.init.normal_(m.weight, std=0.001)\n            elif isinstance(m, BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        if os.path.isfile(pretrained):\n            pretrained_dict = torch.load(pretrained, map_location={'cuda:0': 'cpu'})\n            print('=> loading pretrained model {}'.format(pretrained))\n            model_dict = self.state_dict()\n            pretrained_dict = {k.replace('last_layer', 'aux_head').replace('model.', ''): v for k, v in pretrained_dict.items()}  \n            # print(set(model_dict) - set(pretrained_dict))            \n            # print(set(pretrained_dict) - set(model_dict))            \n            pretrained_dict = {k: v for k, v in pretrained_dict.items()\n                               if k in model_dict.keys()}\n            print('length of pretrained_dict:', len(pretrained_dict.items()))\n            # for k, _ in pretrained_dict.items():\n                # logger.info(\n                #     '=> loading {} pretrained model {}'.format(k, pretrained))\n            model_dict.update(pretrained_dict)\n            self.load_state_dict(model_dict)\n        elif pretrained:\n            raise RuntimeError('No such file {}'.format(pretrained))\n    \n    def trainable_parameters(self):\n        return list(self.backbone.parameters()), list(self.fuse.parameters())\n\n\ndef get_hr_ocr_seg_model(cfg, **kwargs):\n    cfg['MODEL']['NUM_CLASSES'] = kwargs['num_classes']\n    model = HighResolutionOCRNet(cfg, **kwargs)\n\n    # model.init_weights(cfg['MODEL']['PRETRAINED'])\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"hrnet_ocr_aspp"},{"metadata":{"trusted":true},"cell_type":"code","source":"ALIGN_CORNERS = True\nBN_MOMENTUM = 0.1\nrelu_inplace = True\n\nBatchNorm2d_class = BatchNorm2d = torch.nn.BatchNorm2d\n\nclass ModuleHelper:\n\n    @staticmethod\n    def BNReLU(num_features, bn_type=None, **kwargs):\n        return nn.Sequential(\n            BatchNorm2d(num_features, **kwargs),\n            nn.ReLU()\n        )\n\n    @staticmethod\n    def BatchNorm2d(*args, **kwargs):\n        return BatchNorm2d\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass SpatialGather_Module(nn.Module):\n    \"\"\"\n        Aggregate the context features according to the initial \n        predicted probability distribution.\n        Employ the soft-weighted method to aggregate the context.\n    \"\"\"\n    def __init__(self, cls_num=0, scale=1):\n        super(SpatialGather_Module, self).__init__()\n        self.cls_num = cls_num\n        self.scale = scale\n\n    def forward(self, feats, probs):\n        batch_size, c, h, w = probs.size(0), probs.size(1), probs.size(2), probs.size(3)\n        probs = probs.view(batch_size, c, -1)\n        feats = feats.view(batch_size, feats.size(1), -1)\n        feats = feats.permute(0, 2, 1) # batch x hw x c \n        probs = F.softmax(self.scale * probs, dim=2)# batch x k x hw\n        ocr_context = torch.matmul(probs, feats)\\\n        .permute(0, 2, 1).unsqueeze(3)# batch x k x c\n        return ocr_context\n\n\nclass _ObjectAttentionBlock(nn.Module):\n    '''\n    The basic implementation for object context block\n    Input:\n        N X C X H X W\n    Parameters:\n        in_channels       : the dimension of the input feature map\n        key_channels      : the dimension after the key/query transform\n        scale             : choose the scale to downsample the input feature maps (save memory cost)\n        bn_type           : specify the bn type\n    Return:\n        N X C X H X W\n    '''\n    def __init__(self, \n                 in_channels, \n                 key_channels, \n                 scale=1, \n                 bn_type=None):\n        super(_ObjectAttentionBlock, self).__init__()\n        self.scale = scale\n        self.in_channels = in_channels\n        self.key_channels = key_channels\n        self.pool = nn.MaxPool2d(kernel_size=(scale, scale))\n        self.f_pixel = nn.Sequential(\n            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels,\n                kernel_size=1, stride=1, padding=0, bias=False),\n            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),\n            nn.Conv2d(in_channels=self.key_channels, out_channels=self.key_channels,\n                kernel_size=1, stride=1, padding=0, bias=False),\n            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),\n        )\n        self.f_object = nn.Sequential(\n            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels,\n                kernel_size=1, stride=1, padding=0, bias=False),\n            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),\n            nn.Conv2d(in_channels=self.key_channels, out_channels=self.key_channels,\n                kernel_size=1, stride=1, padding=0, bias=False),\n            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),\n        )\n        self.f_down = nn.Sequential(\n            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels,\n                kernel_size=1, stride=1, padding=0, bias=False),\n            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),\n        )\n        self.f_up = nn.Sequential(\n            nn.Conv2d(in_channels=self.key_channels, out_channels=self.in_channels,\n                kernel_size=1, stride=1, padding=0, bias=False),\n            ModuleHelper.BNReLU(self.in_channels, bn_type=bn_type),\n        )\n\n    def forward(self, x, proxy):\n        batch_size, h, w = x.size(0), x.size(2), x.size(3)\n        if self.scale > 1:\n            x = self.pool(x)\n\n        query = self.f_pixel(x).view(batch_size, self.key_channels, -1)\n        query = query.permute(0, 2, 1)\n        key = self.f_object(proxy).view(batch_size, self.key_channels, -1)\n        value = self.f_down(proxy).view(batch_size, self.key_channels, -1)\n        value = value.permute(0, 2, 1)\n\n        sim_map = torch.matmul(query, key)\n        sim_map = (self.key_channels**-.5) * sim_map\n        sim_map = F.softmax(sim_map, dim=-1)   \n\n        # add bg context ...\n        context = torch.matmul(sim_map, value)\n        context = context.permute(0, 2, 1).contiguous()\n        context = context.view(batch_size, self.key_channels, *x.size()[2:])\n        context = self.f_up(context)\n        if self.scale > 1:\n            context = F.interpolate(input=context, size=(h, w), mode='bilinear', align_corners=ALIGN_CORNERS)\n\n        return context\n\n\nclass ObjectAttentionBlock2D(_ObjectAttentionBlock):\n    def __init__(self, \n                 in_channels, \n                 key_channels, \n                 scale=1, \n                 bn_type=None):\n        super(ObjectAttentionBlock2D, self).__init__(in_channels,\n                                                     key_channels,\n                                                     scale, \n                                                     bn_type=bn_type)\n\n\nclass SpatialOCR_Module(nn.Module):\n    \"\"\"\n    Implementation of the OCR module:\n    We aggregate the global object representation to update the representation for each pixel.\n    \"\"\"\n    def __init__(self, \n                 in_channels, \n                 key_channels, \n                 out_channels, \n                 scale=1, \n                 dropout=0.1, \n                 bn_type=None):\n        super(SpatialOCR_Module, self).__init__()\n        self.object_context_block = ObjectAttentionBlock2D(in_channels, \n                                                           key_channels, \n                                                           scale, \n                                                           bn_type)\n        _in_channels = 2 * in_channels\n\n        self.conv_bn_dropout = nn.Sequential(\n            nn.Conv2d(_in_channels, out_channels, kernel_size=1, padding=0, bias=False),\n            ModuleHelper.BNReLU(out_channels, bn_type=bn_type),\n            nn.Dropout2d(dropout)\n        )\n\n    def forward(self, feats, proxy_feats):\n        context = self.object_context_block(feats, proxy_feats)\n\n        output = self.conv_bn_dropout(torch.cat([context, feats], 1))\n\n        return output\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = out + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n                               bias=False)\n        self.bn3 = BatchNorm2d(planes * self.expansion,\n                               momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=relu_inplace)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = out + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass HighResolutionModule(nn.Module):\n    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n                 num_channels, fuse_method, multi_scale_output=True):\n        super(HighResolutionModule, self).__init__()\n        self._check_branches(\n            num_branches, blocks, num_blocks, num_inchannels, num_channels)\n\n        self.num_inchannels = num_inchannels\n        self.fuse_method = fuse_method\n        self.num_branches = num_branches\n\n        self.multi_scale_output = multi_scale_output\n\n        self.branches = self._make_branches(\n            num_branches, blocks, num_blocks, num_channels)\n        self.fuse_layers = self._make_fuse_layers()\n        self.relu = nn.ReLU(inplace=relu_inplace)\n\n    def _check_branches(self, num_branches, blocks, num_blocks,\n                        num_inchannels, num_channels):\n        if num_branches != len(num_blocks):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(\n                num_branches, len(num_blocks))\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_channels):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(\n                num_branches, len(num_channels))\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_inchannels):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(\n                num_branches, len(num_inchannels))\n            raise ValueError(error_msg)\n\n    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n                         stride=1):\n        downsample = None\n        if stride != 1 or \\\n           self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.num_inchannels[branch_index],\n                          num_channels[branch_index] * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm2d(num_channels[branch_index] * block.expansion,\n                            momentum=BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(self.num_inchannels[branch_index],\n                            num_channels[branch_index], stride, downsample))\n        self.num_inchannels[branch_index] = \\\n            num_channels[branch_index] * block.expansion\n        for i in range(1, num_blocks[branch_index]):\n            layers.append(block(self.num_inchannels[branch_index],\n                                num_channels[branch_index]))\n\n        return nn.Sequential(*layers)\n\n    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n        branches = []\n\n        for i in range(num_branches):\n            branches.append(\n                self._make_one_branch(i, block, num_blocks, num_channels))\n\n        return nn.ModuleList(branches)\n\n    def _make_fuse_layers(self):\n        if self.num_branches == 1:\n            return None\n\n        num_branches = self.num_branches\n        num_inchannels = self.num_inchannels\n        fuse_layers = []\n        for i in range(num_branches if self.multi_scale_output else 1):\n            fuse_layer = []\n            for j in range(num_branches):\n                if j > i:\n                    fuse_layer.append(nn.Sequential(\n                        nn.Conv2d(num_inchannels[j],\n                                  num_inchannels[i],\n                                  1,\n                                  1,\n                                  0,\n                                  bias=False),\n                        BatchNorm2d(num_inchannels[i], momentum=BN_MOMENTUM)))\n                elif j == i:\n                    fuse_layer.append(None)\n                else:\n                    conv3x3s = []\n                    for k in range(i-j):\n                        if k == i - j - 1:\n                            num_outchannels_conv3x3 = num_inchannels[i]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_inchannels[j],\n                                          num_outchannels_conv3x3,\n                                          3, 2, 1, bias=False),\n                                BatchNorm2d(num_outchannels_conv3x3,\n                                            momentum=BN_MOMENTUM)))\n                        else:\n                            num_outchannels_conv3x3 = num_inchannels[j]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_inchannels[j],\n                                          num_outchannels_conv3x3,\n                                          3, 2, 1, bias=False),\n                                BatchNorm2d(num_outchannels_conv3x3,\n                                            momentum=BN_MOMENTUM),\n                                nn.ReLU(inplace=relu_inplace)))\n                    fuse_layer.append(nn.Sequential(*conv3x3s))\n            fuse_layers.append(nn.ModuleList(fuse_layer))\n\n        return nn.ModuleList(fuse_layers)\n\n    def get_num_inchannels(self):\n        return self.num_inchannels\n\n    def forward(self, x):\n        if self.num_branches == 1:\n            return [self.branches[0](x[0])]\n\n        for i in range(self.num_branches):\n            x[i] = self.branches[i](x[i])\n\n        x_fuse = []\n        for i in range(len(self.fuse_layers)):\n            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n            for j in range(1, self.num_branches):\n                if i == j:\n                    y = y + x[j]\n                elif j > i:\n                    width_output = x[i].shape[-1]\n                    height_output = x[i].shape[-2]\n                    y = y + F.interpolate(\n                        self.fuse_layers[i][j](x[j]),\n                        size=[height_output, width_output],\n                        mode='bilinear', align_corners=ALIGN_CORNERS)\n                else:\n                    y = y + self.fuse_layers[i][j](x[j])\n            x_fuse.append(self.relu(y))\n\n        return x_fuse\n\n\nblocks_dict = {\n    'BASIC': BasicBlock,\n    'BOTTLENECK': Bottleneck\n}\n\n\nclass ASPPModule(nn.Module):\n    def __init__(self, in_channel=720, hidden_channel=256, out_channel=512, dilations=(3, 6, 9), **kwargs):\n        super(ASPPModule, self).__init__()\n        # ASPP\n        self.asp_conv2 = nn.Sequential(\n            nn.Conv2d(in_channel, hidden_channel, kernel_size=1, padding=0, dilation=1, bias=True),\n            BatchNorm2d(hidden_channel),\n            nn.ReLU(inplace=relu_inplace), )\n        self.asp_conv3 = nn.Sequential(\n            nn.Conv2d(in_channel, hidden_channel, kernel_size=3, padding=dilations[0], dilation=dilations[0], bias=True),\n            BatchNorm2d(hidden_channel),\n            nn.ReLU(inplace=relu_inplace), )\n        self.asp_conv4 = nn.Sequential(\n            nn.Conv2d(in_channel, hidden_channel, kernel_size=3, padding=dilations[0], dilation=dilations[0], bias=True),\n            BatchNorm2d(hidden_channel),\n            nn.ReLU(inplace=relu_inplace), )\n        self.asp_conv5 = nn.Sequential(\n            nn.Conv2d(in_channel, hidden_channel, kernel_size=3, padding=dilations[0], dilation=dilations[0], bias=True),\n            BatchNorm2d(hidden_channel),\n            nn.ReLU(inplace=relu_inplace), )\n        self.asp_fuse = nn.Sequential(\n            nn.Conv2d(hidden_channel * 4 + 512, out_channel, kernel_size=1, padding=0, dilation=1, bias=True),\n            BatchNorm2d(out_channel),\n            nn.ReLU(inplace=relu_inplace),\n            nn.Dropout2d(0.1)\n        )\n\n    def forward(self, feat_hr, feat_context):\n        feat1 = feat_context\n        feat2 = self.asp_conv2(feat_hr)\n        feat3 = self.asp_conv3(feat_hr)\n        feat4 = self.asp_conv4(feat_hr)\n        feat5 = self.asp_conv5(feat_hr)\n\n        out = torch.cat((feat1, feat2, feat3, feat4, feat5), 1)\n        output = self.asp_fuse(out)\n\n        return output\n\n\nclass HighResolutionNet_ocr_aspp(nn.Module):\n\n    def __init__(self, config, dilations=(3, 6, 9), **kwargs):\n        global ALIGN_CORNERS\n        extra = config['MODEL']['EXTRA']\n        ocr_cfg = config['MODEL']['OCR']\n        super(HighResolutionNet_ocr_aspp, self).__init__()\n        ALIGN_CORNERS = True\n\n        # stem net\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn1 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn2 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=relu_inplace)\n\n        self.stage1_cfg = extra['STAGE1']\n        num_channels = self.stage1_cfg['NUM_CHANNELS'][0]\n        block = blocks_dict[self.stage1_cfg['BLOCK']]\n        num_blocks = self.stage1_cfg['NUM_BLOCKS'][0]\n        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks)\n        stage1_out_channel = block.expansion * num_channels\n\n        self.stage2_cfg = extra['STAGE2']\n        num_channels = self.stage2_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage2_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition1 = self._make_transition_layer(\n            [stage1_out_channel], num_channels)\n        self.stage2, pre_stage_channels = self._make_stage(\n            self.stage2_cfg, num_channels)\n\n        self.stage3_cfg = extra['STAGE3']\n        num_channels = self.stage3_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage3_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition2 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage3, pre_stage_channels = self._make_stage(\n            self.stage3_cfg, num_channels)\n\n        self.stage4_cfg = extra['STAGE4']\n        num_channels = self.stage4_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage4_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition3 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage4, pre_stage_channels = self._make_stage(\n            self.stage4_cfg, num_channels, multi_scale_output=True)\n\n        # OCR\n        last_inp_channels = np.int(np.sum(pre_stage_channels))\n        ocr_mid_channels = ocr_cfg['MID_CHANNELS']\n        ocr_key_channels = ocr_cfg['KEY_CHANNELS']\n\n        self.conv3x3_ocr = nn.Sequential(\n            nn.Conv2d(last_inp_channels, ocr_mid_channels,\n                      kernel_size=3, stride=1, padding=1),\n            BatchNorm2d(ocr_mid_channels),\n            nn.ReLU(inplace=relu_inplace),\n        )\n        self.ocr_gather_head = SpatialGather_Module(config['DATASET']['NUM_CLASSES'])\n\n        self.ocr_distri_head = SpatialOCR_Module(in_channels=ocr_mid_channels,\n                                                 key_channels=ocr_key_channels,\n                                                 out_channels=ocr_mid_channels,\n                                                 scale=1,\n                                                 dropout=0.05,\n                                                 )\n        self.cls_head = nn.Conv2d(\n            ocr_mid_channels, config['DATASET']['NUM_CLASSES'], kernel_size=1, stride=1, padding=0, bias=True)\n\n        self.aux_head = nn.Sequential(\n            nn.Conv2d(last_inp_channels, last_inp_channels,\n                      kernel_size=1, stride=1, padding=0),\n            BatchNorm2d(last_inp_channels),\n            nn.ReLU(inplace=relu_inplace),\n            nn.Conv2d(last_inp_channels, config['DATASET']['NUM_CLASSES'],\n                      kernel_size=1, stride=1, padding=0, bias=True)\n        )\n\n        # ASPP module\n        self.asp_module = ASPPModule(in_channel=720, hidden_channel=256, out_channel=512, dilations=dilations)\n\n        self.backbone = nn.ModuleList([self.conv1, self.bn1, self.conv2, self.bn2, self.layer1, self.transition1,\n                                       self.stage2, self.transition2, self.stage3, self.transition3, self.stage4])\n        self.fuse = nn.ModuleList(\n            [self.aux_head, self.conv3x3_ocr, self.ocr_gather_head, self.ocr_distri_head, self.cls_head,\n             self.asp_module])\n\n    def _make_transition_layer(\n            self, num_channels_pre_layer, num_channels_cur_layer):\n        num_branches_cur = len(num_channels_cur_layer)\n        num_branches_pre = len(num_channels_pre_layer)\n\n        transition_layers = []\n        for i in range(num_branches_cur):\n            if i < num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(nn.Sequential(\n                        nn.Conv2d(num_channels_pre_layer[i],\n                                  num_channels_cur_layer[i],\n                                  3,\n                                  1,\n                                  1,\n                                  bias=False),\n                        BatchNorm2d(\n                            num_channels_cur_layer[i], momentum=BN_MOMENTUM),\n                        nn.ReLU(inplace=relu_inplace)))\n                else:\n                    transition_layers.append(None)\n            else:\n                conv3x3s = []\n                for j in range(i + 1 - num_branches_pre):\n                    inchannels = num_channels_pre_layer[-1]\n                    outchannels = num_channels_cur_layer[i] \\\n                        if j == i - num_branches_pre else inchannels\n                    conv3x3s.append(nn.Sequential(\n                        nn.Conv2d(\n                            inchannels, outchannels, 3, 2, 1, bias=False),\n                        BatchNorm2d(outchannels, momentum=BN_MOMENTUM),\n                        nn.ReLU(inplace=relu_inplace)))\n                transition_layers.append(nn.Sequential(*conv3x3s))\n\n        return nn.ModuleList(transition_layers)\n\n    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(inplanes, planes, stride, downsample))\n        inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_stage(self, layer_config, num_inchannels,\n                    multi_scale_output=True):\n        num_modules = layer_config['NUM_MODULES']\n        num_branches = layer_config['NUM_BRANCHES']\n        num_blocks = layer_config['NUM_BLOCKS']\n        num_channels = layer_config['NUM_CHANNELS']\n        block = blocks_dict[layer_config['BLOCK']]\n        fuse_method = layer_config['FUSE_METHOD']\n\n        modules = []\n        for i in range(num_modules):\n            # multi_scale_output is only used last module\n            if not multi_scale_output and i == num_modules - 1:\n                reset_multi_scale_output = False\n            else:\n                reset_multi_scale_output = True\n            modules.append(\n                HighResolutionModule(num_branches,\n                                     block,\n                                     num_blocks,\n                                     num_inchannels,\n                                     num_channels,\n                                     fuse_method,\n                                     reset_multi_scale_output)\n            )\n            num_inchannels = modules[-1].get_num_inchannels()\n\n        return nn.Sequential(*modules), num_inchannels\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n\n        x_list = []\n        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n            if self.transition1[i] is not None:\n                x_list.append(self.transition1[i](x))\n            else:\n                x_list.append(x)\n        y_list = self.stage2(x_list)\n\n        x_list = []\n        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n            if self.transition2[i] is not None:\n                if i < self.stage2_cfg['NUM_BRANCHES']:\n                    x_list.append(self.transition2[i](y_list[i]))\n                else:\n                    x_list.append(self.transition2[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage3(x_list)\n\n        x_list = []\n        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n            if self.transition3[i] is not None:\n                if i < self.stage3_cfg['NUM_BRANCHES']:\n                    x_list.append(self.transition3[i](y_list[i]))\n                else:\n                    x_list.append(self.transition3[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        x = self.stage4(x_list)\n\n        # Upsampling\n        x0_h, x0_w = x[0].size(2), x[0].size(3)\n        x1 = F.interpolate(x[1], size=(x0_h, x0_w),\n                           mode='bilinear', align_corners=ALIGN_CORNERS)\n        x2 = F.interpolate(x[2], size=(x0_h, x0_w),\n                           mode='bilinear', align_corners=ALIGN_CORNERS)\n        x3 = F.interpolate(x[3], size=(x0_h, x0_w),\n                           mode='bilinear', align_corners=ALIGN_CORNERS)\n\n        feats_hr = torch.cat([x[0], x1, x2, x3], 1)\n\n        out_aux_seg = []\n\n        # ocr\n        out_aux = self.aux_head(feats_hr)\n        # compute contrast feature\n        feats = self.conv3x3_ocr(feats_hr)\n\n        context = self.ocr_gather_head(feats, out_aux)\n        feats_context = self.ocr_distri_head(feats, context)\n\n        feats = self.asp_module(feats_hr, feats_context)\n\n        # aspp\n        out = self.cls_head(feats)\n\n        out_aux_seg.append(out_aux)\n        out_aux_seg.append(out)\n\n        return out_aux_seg\n\n    def init_weights(self, pretrained='', ):\n        print('=> init weights from normal distribution')\n        for name, m in self.named_modules():\n            if any(part in name for part in {'cls', 'aux', 'ocr'}):\n                # print('skipped', name)\n                continue\n            if isinstance(m, nn.Conv2d):\n                nn.init.normal_(m.weight, std=0.001)\n            elif isinstance(m, BatchNorm2d_class):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        if os.path.isfile(pretrained):\n            pretrained_dict = torch.load(pretrained, map_location={'cuda:0': 'cpu'})\n            print('=> loading pretrained model {}'.format(pretrained))\n            model_dict = self.state_dict()\n            pretrained_dict = {k.replace('last_layer', 'aux_head').replace('model.', ''): v for k, v in\n                               pretrained_dict.items()}\n            # print(set(model_dict) - set(pretrained_dict))\n            # print(set(pretrained_dict) - set(model_dict))\n            pretrained_dict = {k: v for k, v in pretrained_dict.items()\n                               if k in model_dict.keys()}\n            print('length of pretrained_dict:', len(pretrained_dict.items()))\n            # for k, _ in pretrained_dict.items():\n            # logger.info(\n            #     '=> loading {} pretrained model {}'.format(k, pretrained))\n            model_dict.update(pretrained_dict)\n            self.load_state_dict(model_dict)\n        elif pretrained:\n            raise RuntimeError('No such file {}'.format(pretrained))\n\n    def trainable_parameters(self):\n        return list(self.backbone.parameters()), list(self.fuse.parameters())\n\n\ndef get_hr_ocr_asp_seg_model(cfg, **kwargs):\n    cfg['MODEL']['NUM_CLASSES'] = kwargs['num_classes']\n    model = HighResolutionNet_ocr_aspp(cfg, **kwargs)\n\n    # model.init_weights(cfg['MODEL']['PRETRAINED'])\n\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"HRNet_ccnet"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.nn import Softmax\ndef INF(B, H, W):\n    return -torch.diag(torch.tensor(float(\"inf\")).cuda().repeat(H), 0).unsqueeze(0).repeat(B * W, 1, 1)\n\n\nclass CC_module(nn.Module):\n    def __init__(self, in_dim):\n        super(CC_module, self).__init__()\n        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n        self.softmax = Softmax(dim=3)\n        self.INF = INF\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        m_batchsize, _, height, width = x.size()\n        proj_query = self.query_conv(x)\n        proj_query_H = proj_query.permute(0, 3, 1, 2).contiguous().view(m_batchsize * width, -1, height).permute(0, 2,\n                                                                                                                 1)\n        proj_query_W = proj_query.permute(0, 2, 1, 3).contiguous().view(m_batchsize * height, -1, width).permute(0, 2,\n                                                                                                                 1)\n        proj_key = self.key_conv(x)\n        proj_key_H = proj_key.permute(0, 3, 1, 2).contiguous().view(m_batchsize * width, -1, height)\n        proj_key_W = proj_key.permute(0, 2, 1, 3).contiguous().view(m_batchsize * height, -1, width)\n        proj_value = self.value_conv(x)\n        proj_value_H = proj_value.permute(0, 3, 1, 2).contiguous().view(m_batchsize * width, -1, height)\n        proj_value_W = proj_value.permute(0, 2, 1, 3).contiguous().view(m_batchsize * height, -1, width)\n        energy_H = (torch.bmm(proj_query_H, proj_key_H) + self.INF(m_batchsize, height, width)).view(m_batchsize, width,\n                                                                                                     height,\n                                                                                                     height).permute(0,\n                                                                                                                     2,\n                                                                                                                     1,\n                                                                                                                     3)\n        energy_W = torch.bmm(proj_query_W, proj_key_W).view(m_batchsize, height, width, width)\n        concate = self.softmax(torch.cat([energy_H, energy_W], 3))\n\n        att_H = concate[:, :, :, 0:height].permute(0, 2, 1, 3).contiguous().view(m_batchsize * width, height, height)\n        # print(concate)\n        # print(att_H)\n        att_W = concate[:, :, :, height:height + width].contiguous().view(m_batchsize * height, width, width)\n        out_H = torch.bmm(proj_value_H, att_H.permute(0, 2, 1)).view(m_batchsize, width, -1, height).permute(0, 2, 3, 1)\n        out_W = torch.bmm(proj_value_W, att_W.permute(0, 2, 1)).view(m_batchsize, height, -1, width).permute(0, 2, 1, 3)\n        # print(out_H.size(),out_W.size())\n        return self.gamma * (out_H + out_W) + x\n\n\nclass RCCAModule(nn.Module):\n    def __init__(self, in_channels, out_channels, num_classes):\n        super(RCCAModule, self).__init__()\n        inter_channels = in_channels // 4\n        self.conva = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n                                   nn.BatchNorm2d(inter_channels))\n        self.cca = CC_module(inter_channels)\n        self.convb = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, padding=1, bias=False),\n                                   nn.BatchNorm2d(inter_channels))\n\n        self.bottleneck = nn.Sequential(\n            nn.Conv2d(in_channels+inter_channels, out_channels, kernel_size=3, padding=1, dilation=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.Dropout2d(0.1),\n            nn.Conv2d(512, num_classes, kernel_size=1, stride=1, padding=0, bias=True)\n            )\n\n    def forward(self, x, recurrence=2):\n        output = self.conva(x)\n        for i in range(recurrence):\n            output = self.cca(output)\n        output = self.convb(output)\n\n        output = self.bottleneck(torch.cat([x, output], 1))\n        return output\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=relu_inplace)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = out + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n                               bias=False)\n        self.bn3 = BatchNorm2d(planes * self.expansion,\n                               momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=relu_inplace)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = out + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass HighResolutionModule(nn.Module):\n    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n                 num_channels, fuse_method, multi_scale_output=True):\n        super(HighResolutionModule, self).__init__()\n        self._check_branches(\n            num_branches, blocks, num_blocks, num_inchannels, num_channels)\n\n        self.num_inchannels = num_inchannels\n        self.fuse_method = fuse_method\n        self.num_branches = num_branches\n\n        self.multi_scale_output = multi_scale_output\n\n        self.branches = self._make_branches(\n            num_branches, blocks, num_blocks, num_channels)\n        self.fuse_layers = self._make_fuse_layers()\n        self.relu = nn.ReLU(inplace=relu_inplace)\n\n    def _check_branches(self, num_branches, blocks, num_blocks,\n                        num_inchannels, num_channels):\n        if num_branches != len(num_blocks):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(\n                num_branches, len(num_blocks))\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_channels):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(\n                num_branches, len(num_channels))\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_inchannels):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(\n                num_branches, len(num_inchannels))\n            raise ValueError(error_msg)\n\n    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n                         stride=1):\n        downsample = None\n        if stride != 1 or \\\n                self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.num_inchannels[branch_index],\n                          num_channels[branch_index] * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm2d(num_channels[branch_index] * block.expansion,\n                            momentum=BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(self.num_inchannels[branch_index],\n                            num_channels[branch_index], stride, downsample))\n        self.num_inchannels[branch_index] = \\\n            num_channels[branch_index] * block.expansion\n        for i in range(1, num_blocks[branch_index]):\n            layers.append(block(self.num_inchannels[branch_index],\n                                num_channels[branch_index]))\n\n        return nn.Sequential(*layers)\n\n    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n        branches = []\n\n        for i in range(num_branches):\n            branches.append(\n                self._make_one_branch(i, block, num_blocks, num_channels))\n\n        return nn.ModuleList(branches)\n\n    def _make_fuse_layers(self):\n        if self.num_branches == 1:\n            return None\n\n        num_branches = self.num_branches\n        num_inchannels = self.num_inchannels\n        fuse_layers = []\n        for i in range(num_branches if self.multi_scale_output else 1):\n            fuse_layer = []\n            for j in range(num_branches):\n                if j > i:\n                    fuse_layer.append(nn.Sequential(\n                        nn.Conv2d(num_inchannels[j],\n                                  num_inchannels[i],\n                                  1,\n                                  1,\n                                  0,\n                                  bias=False),\n                        BatchNorm2d(num_inchannels[i], momentum=BN_MOMENTUM)))\n                elif j == i:\n                    fuse_layer.append(None)\n                else:\n                    conv3x3s = []\n                    for k in range(i - j):\n                        if k == i - j - 1:\n                            num_outchannels_conv3x3 = num_inchannels[i]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_inchannels[j],\n                                          num_outchannels_conv3x3,\n                                          3, 2, 1, bias=False),\n                                BatchNorm2d(num_outchannels_conv3x3,\n                                            momentum=BN_MOMENTUM)))\n                        else:\n                            num_outchannels_conv3x3 = num_inchannels[j]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_inchannels[j],\n                                          num_outchannels_conv3x3,\n                                          3, 2, 1, bias=False),\n                                BatchNorm2d(num_outchannels_conv3x3,\n                                            momentum=BN_MOMENTUM),\n                                nn.ReLU(inplace=relu_inplace)))\n                    fuse_layer.append(nn.Sequential(*conv3x3s))\n            fuse_layers.append(nn.ModuleList(fuse_layer))\n\n        return nn.ModuleList(fuse_layers)\n\n    def get_num_inchannels(self):\n        return self.num_inchannels\n\n    def forward(self, x):\n        if self.num_branches == 1:\n            return [self.branches[0](x[0])]\n\n        for i in range(self.num_branches):\n            x[i] = self.branches[i](x[i])\n\n        x_fuse = []\n        for i in range(len(self.fuse_layers)):\n            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n            for j in range(1, self.num_branches):\n                if i == j:\n                    y = y + x[j]\n                elif j > i:\n                    width_output = x[i].shape[-1]\n                    height_output = x[i].shape[-2]\n                    y = y + F.interpolate(\n                        self.fuse_layers[i][j](x[j]),\n                        size=[height_output, width_output],\n                        mode='bilinear', align_corners=ALIGN_CORNERS)\n                else:\n                    y = y + self.fuse_layers[i][j](x[j])\n            x_fuse.append(self.relu(y))\n\n        return x_fuse\n\n\nblocks_dict = {\n    'BASIC': BasicBlock,\n    'BOTTLENECK': Bottleneck\n}\n\n\nclass HighResolutionNetCC(nn.Module):\n\n    def __init__(self, config, **kwargs):\n        global ALIGN_CORNERS\n        extra = config['MODEL']['EXTRA']\n        ocr_cfg = config['MODEL']['OCR']\n        super(HighResolutionNetCC, self).__init__()\n        ALIGN_CORNERS = True\n\n        # stem net\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn1 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn2 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=relu_inplace)\n\n        self.stage1_cfg = extra['STAGE1']\n        num_channels = self.stage1_cfg['NUM_CHANNELS'][0]\n        block = blocks_dict[self.stage1_cfg['BLOCK']]\n        num_blocks = self.stage1_cfg['NUM_BLOCKS'][0]\n        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks)\n        stage1_out_channel = block.expansion * num_channels\n\n        self.stage2_cfg = extra['STAGE2']\n        num_channels = self.stage2_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage2_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition1 = self._make_transition_layer(\n            [stage1_out_channel], num_channels)\n        self.stage2, pre_stage_channels = self._make_stage(\n            self.stage2_cfg, num_channels)\n\n        self.stage3_cfg = extra['STAGE3']\n        num_channels = self.stage3_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage3_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition2 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage3, pre_stage_channels = self._make_stage(\n            self.stage3_cfg, num_channels)\n\n        self.stage4_cfg = extra['STAGE4']\n        num_channels = self.stage4_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage4_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition3 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage4, pre_stage_channels = self._make_stage(\n            self.stage4_cfg, num_channels, multi_scale_output=True)\n\n        last_inp_channels = np.int(np.sum(pre_stage_channels))\n        ocr_mid_channels = ocr_cfg['MID_CHANNELS']\n        ocr_key_channels = ocr_cfg['KEY_CHANNELS']\n\n        self.conv3x3_ocr = nn.Sequential(\n            nn.Conv2d(last_inp_channels, ocr_mid_channels,\n                      kernel_size=3, stride=1, padding=1),\n            BatchNorm2d(ocr_mid_channels),\n            nn.ReLU(inplace=relu_inplace),\n        )\n\n        self.cc = RCCAModule(512, 512, 5)\n\n        self.aux_head = nn.Sequential(\n            nn.Conv2d(last_inp_channels, last_inp_channels,\n                      kernel_size=1, stride=1, padding=0),\n            BatchNorm2d(last_inp_channels),\n            nn.ReLU(inplace=relu_inplace),\n            nn.Conv2d(last_inp_channels, config['DATASET']['NUM_CLASSES'],\n                      kernel_size=1, stride=1, padding=0, bias=True)\n        )\n\n        self.backbone = nn.ModuleList([self.conv1, self.bn1, self.conv2, self.bn2, self.layer1, self.transition1,\n                                       self.stage2, self.transition2, self.stage3, self.transition3, self.stage4])\n        self.fuse = nn.ModuleList(\n            [self.aux_head, self.conv3x3_ocr, self.cc])\n\n    def _make_transition_layer(\n            self, num_channels_pre_layer, num_channels_cur_layer):\n        num_branches_cur = len(num_channels_cur_layer)\n        num_branches_pre = len(num_channels_pre_layer)\n\n        transition_layers = []\n        for i in range(num_branches_cur):\n            if i < num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(nn.Sequential(\n                        nn.Conv2d(num_channels_pre_layer[i],\n                                  num_channels_cur_layer[i],\n                                  3,\n                                  1,\n                                  1,\n                                  bias=False),\n                        BatchNorm2d(\n                            num_channels_cur_layer[i], momentum=BN_MOMENTUM),\n                        nn.ReLU(inplace=relu_inplace)))\n                else:\n                    transition_layers.append(None)\n            else:\n                conv3x3s = []\n                for j in range(i + 1 - num_branches_pre):\n                    inchannels = num_channels_pre_layer[-1]\n                    outchannels = num_channels_cur_layer[i] \\\n                        if j == i - num_branches_pre else inchannels\n                    conv3x3s.append(nn.Sequential(\n                        nn.Conv2d(\n                            inchannels, outchannels, 3, 2, 1, bias=False),\n                        BatchNorm2d(outchannels, momentum=BN_MOMENTUM),\n                        nn.ReLU(inplace=relu_inplace)))\n                transition_layers.append(nn.Sequential(*conv3x3s))\n\n        return nn.ModuleList(transition_layers)\n\n    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(inplanes, planes, stride, downsample))\n        inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_stage(self, layer_config, num_inchannels,\n                    multi_scale_output=True):\n        num_modules = layer_config['NUM_MODULES']\n        num_branches = layer_config['NUM_BRANCHES']\n        num_blocks = layer_config['NUM_BLOCKS']\n        num_channels = layer_config['NUM_CHANNELS']\n        block = blocks_dict[layer_config['BLOCK']]\n        fuse_method = layer_config['FUSE_METHOD']\n\n        modules = []\n        for i in range(num_modules):\n            # multi_scale_output is only used last module\n            if not multi_scale_output and i == num_modules - 1:\n                reset_multi_scale_output = False\n            else:\n                reset_multi_scale_output = True\n            modules.append(\n                HighResolutionModule(num_branches,\n                                     block,\n                                     num_blocks,\n                                     num_inchannels,\n                                     num_channels,\n                                     fuse_method,\n                                     reset_multi_scale_output)\n            )\n            num_inchannels = modules[-1].get_num_inchannels()\n\n        return nn.Sequential(*modules), num_inchannels\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n\n        x_list = []\n        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n            if self.transition1[i] is not None:\n                x_list.append(self.transition1[i](x))\n            else:\n                x_list.append(x)\n        y_list = self.stage2(x_list)\n\n        x_list = []\n        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n            if self.transition2[i] is not None:\n                if i < self.stage2_cfg['NUM_BRANCHES']:\n                    x_list.append(self.transition2[i](y_list[i]))\n                else:\n                    x_list.append(self.transition2[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage3(x_list)\n\n        x_list = []\n        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n            if self.transition3[i] is not None:\n                if i < self.stage3_cfg['NUM_BRANCHES']:\n                    x_list.append(self.transition3[i](y_list[i]))\n                else:\n                    x_list.append(self.transition3[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        x = self.stage4(x_list)\n\n        # Upsampling\n        x0_h, x0_w = x[0].size(2), x[0].size(3)\n        x1 = F.interpolate(x[1], size=(x0_h, x0_w),\n                           mode='bilinear', align_corners=ALIGN_CORNERS)\n        x2 = F.interpolate(x[2], size=(x0_h, x0_w),\n                           mode='bilinear', align_corners=ALIGN_CORNERS)\n        x3 = F.interpolate(x[3], size=(x0_h, x0_w),\n                           mode='bilinear', align_corners=ALIGN_CORNERS)\n\n        feats = torch.cat([x[0], x1, x2, x3], 1)\n\n        out_aux_seg = []\n\n        # ocr\n        out_aux = self.aux_head(feats)\n        # compute contrast feature\n        feats = self.conv3x3_ocr(feats)\n\n        out = self.cc(feats)\n\n        out_aux_seg.append(out_aux)\n        out_aux_seg.append(out)\n\n        return out_aux_seg\n\n    def init_weights(self, pretrained='', ):\n        print('=> init weights from normal distribution')\n        for name, m in self.named_modules():\n            if any(part in name for part in {'cls', 'aux', 'ocr'}):\n                # print('skipped', name)\n                continue\n            if isinstance(m, nn.Conv2d):\n                nn.init.normal_(m.weight, std=0.001)\n            elif isinstance(m, BatchNorm2d_class):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        if os.path.isfile(pretrained):\n            pretrained_dict = torch.load(pretrained, map_location={'cuda:0': 'cpu'})\n            print('=> loading pretrained model {}'.format(pretrained))\n            model_dict = self.state_dict()\n            pretrained_dict = {k.replace('last_layer', 'aux_head').replace('model.', ''): v for k, v in\n                               pretrained_dict.items()}\n            # print(set(model_dict) - set(pretrained_dict))\n            # print(set(pretrained_dict) - set(model_dict))\n            pretrained_dict = {k: v for k, v in pretrained_dict.items()\n                               if k in model_dict.keys()}\n            print('length of pretrained_dict:', len(pretrained_dict.items()))\n            # for k, _ in pretrained_dict.items():\n            # logger.info(\n            #     '=> loading {} pretrained model {}'.format(k, pretrained))\n            model_dict.update(pretrained_dict)\n            self.load_state_dict(model_dict)\n        elif pretrained:\n            raise RuntimeError('No such file {}'.format(pretrained))\n\n    def trainable_parameters(self):\n        return list(self.backbone.parameters()), list(self.fuse.parameters())\n\n\ndef get_seg_cc_model(cfg, **kwargs):\n    cfg['MODEL']['NUM_CLASSES'] = kwargs['num_classes']\n    model = HighResolutionNetCC(cfg, **kwargs)\n\n    # model.init_weights(cfg['MODEL']['PRETRAINED'])\n\n    return model\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"res2net backbone"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport torch\nimport torch.nn.functional as F\n\n\nclass Bottle2neck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, baseWidth=26, scale=4, stype='normal'):\n        \"\"\" Constructor\n        Args:\n            inplanes: input channel dimensionality\n            planes: output channel dimensionality\n            stride: conv stride. Replaces pooling layer.\n            downsample: None when stride = 1\n            baseWidth: basic width of conv3x3\n            scale: number of scale.\n            type: 'normal': normal set. 'stage': first block of a new stage.\n        \"\"\"\n        super(Bottle2neck, self).__init__()\n\n        width = int(math.floor(planes * (baseWidth / 64.0)))\n        self.conv1 = nn.Conv2d(inplanes, width * scale, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(width * scale)\n\n        if scale == 1:\n            self.nums = 1\n        else:\n            self.nums = scale - 1\n        if stype == 'stage':\n            self.pool = nn.AvgPool2d(kernel_size=3, stride=stride, padding=1)\n        convs = []\n        bns = []\n        for i in range(self.nums):\n            convs.append(nn.Conv2d(width, width, kernel_size=3, stride=stride, padding=1, bias=False))\n            bns.append(nn.BatchNorm2d(width))\n        self.convs = nn.ModuleList(convs)\n        self.bns = nn.ModuleList(bns)\n\n        self.conv3 = nn.Conv2d(width * scale, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stype = stype\n        self.scale = scale\n        self.width = width\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        spx = torch.split(out, self.width, 1)\n        for i, _ in enumerate(range(self.nums)):\n            if i == 0 or self.stype == 'stage':\n                sp = spx[i]\n            else:\n                sp = sp + spx[i]\n            sp = self.convs[i](sp)\n            sp = self.relu(self.bns[i](sp))\n            if i == 0:\n                out = sp\n            else:\n                out = torch.cat((out, sp), 1)\n        if self.scale != 1 and self.stype == 'normal':\n            out = torch.cat((out, spx[self.nums]), 1)\n        elif self.scale != 1 and self.stype == 'stage':\n            out = torch.cat((out, self.pool(spx[self.nums])), 1)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Res2Net(nn.Module):\n\n    def __init__(self, block, layers, baseWidth=26, scale=4, num_classes=1000):\n        self.inplanes = 64\n        super(Res2Net, self).__init__()\n        self.baseWidth = baseWidth\n        self.scale = scale\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 32, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 64, 3, 1, 1, bias=False)\n        )\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.AvgPool2d(kernel_size=stride, stride=stride,\n                             ceil_mode=True, count_include_pad=False),\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=1, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n                            stype='stage', baseWidth=self.baseWidth, scale=self.scale))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, baseWidth=self.baseWidth, scale=self.scale))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef res2net101_v1b(pretrained=False, **kwargs):\n    \"\"\"Constructs a Res2Net-50_v1b_26w_4s model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = Res2Net(Bottle2neck, [3, 4, 23, 3], baseWidth=26, scale=4, **kwargs)\n#     if pretrained:\n#         model.load_state_dict(model_zoo.load_url(model_urls['res2net101_v1b_26w_4s']))\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"res2net + deeplabV3 plus"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Res2NetDeeplabV3P(nn.Module):\n    def __init__(self, num_classes, aspp_dilate=(12, 24, 36)):\n        super(Res2NetDeeplabV3P, self).__init__()\n        backbone = res2net101_v1b(True)\n        self.conv1 = backbone.conv1\n        self.bn1 = backbone.bn1\n        self.relu = backbone.relu\n        self.maxpool = backbone.maxpool\n        self.stage1 = backbone.layer1\n        self.stage2 = backbone.layer2\n        self.stage3 = backbone.layer3\n        self.stage4 = backbone.layer4\n\n        self.project = nn.Sequential(\n            nn.Conv2d(512, 48, 1, bias=False),\n            nn.BatchNorm2d(48),\n            nn.ReLU(inplace=True),\n        )\n\n        self.aspp = ASPP(2048, aspp_dilate)\n\n        self.classifier = nn.Sequential(\n            nn.Conv2d(304, 256, 3, padding=1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, num_classes, 1)\n        )\n\n        self.backbone = nn.ModuleList([self.conv1, self.bn1, self.stage1, self.stage2, self.stage3, self.stage4])\n        self.fuse = nn.ModuleList([self.project, self.aspp, self.classifier])\n\n        self._init_fuse_weight()\n\n    def trainable_parameters(self):\n        return list(self.backbone.parameters()), list(self.fuse.parameters())\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        low_feat = x.clone()\n\n        x = self.stage3(x)\n        out_feat = self.stage4(x)\n\n        low_level_feature = self.project(low_feat)\n        output_feature = self.aspp(out_feat)\n        output_feature = F.interpolate(output_feature, size=low_level_feature.shape[2:], mode='bilinear',\n                                       align_corners=False)\n        return self.classifier(torch.cat([low_level_feature, output_feature], dim=1))\n\n    def _init_fuse_weight(self):\n        for m in self.fuse.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n\nclass ASPPConv(nn.Sequential):\n    def __init__(self, in_channels, out_channels, dilation):\n        modules = [\n            nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        ]\n        super(ASPPConv, self).__init__(*modules)\n\n\nclass ASPPPooling(nn.Sequential):\n    def __init__(self, in_channels, out_channels):\n        super(ASPPPooling, self).__init__(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True))\n\n    def forward(self, x):\n        size = x.shape[-2:]\n        x = super(ASPPPooling, self).forward(x)\n        return F.interpolate(x, size=size, mode='bilinear', align_corners=False)\n\n\nclass ASPP(nn.Module):\n    def __init__(self, in_channels, atrous_rates):\n        super(ASPP, self).__init__()\n        out_channels = 256\n        modules = []\n        modules.append(nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)))\n\n        rate1, rate2, rate3 = tuple(atrous_rates)\n        modules.append(ASPPConv(in_channels, out_channels, rate1))\n        modules.append(ASPPConv(in_channels, out_channels, rate2))\n        modules.append(ASPPConv(in_channels, out_channels, rate3))\n        modules.append(ASPPPooling(in_channels, out_channels))\n\n        self.convs = nn.ModuleList(modules)\n\n        self.project = nn.Sequential(\n            nn.Conv2d(5 * out_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),)\n\n    def forward(self, x):\n        res = []\n        for conv in self.convs:\n            res.append(conv(x))\n        res = torch.cat(res, dim=1)\n        return self.project(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n\ndef mask_to_rle(mask):\n    '''\n    Convert a mask into RLE\n    \n    Parameters: \n    mask (numpy.array): binary mask of numpy array where 1 - mask, 0 - background\n\n    Returns: \n    sring: run length encoding \n    '''\n    pixels= mask.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"get hr pretrain model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport cv2\n\n\n# # get hr model config and create model\n# hrnet_yaml = \"/kaggle/input/model-file/hrnet.yaml\"\n# with open(hrnet_yaml, 'r', encoding='utf-8') as fo:\n#     cfg = fo.read()\n# cfg = yaml.load(cfg, Loader=yaml.FullLoader)\n# model = get_hr_seg_model(cfg)\n\n# # get hr_ocr model config and create model\n# hrnet_ocr_yaml = \"../input/hrnet-ocr-cfg/seg_hrnet_ocr_w48.yaml\"\n# with open(hrnet_ocr_yaml, 'r', encoding='utf-8') as fo:\n#     cfg = fo.read()\n# cfg = yaml.load(cfg, Loader=yaml.FullLoader)\n# model = get_hr_ocr_seg_model(cfg, num_classes=5)\n# # model = get_hr_ocr_asp_seg_model(cfg, num_classes=5, dilations=(4, 8, 12))\n\n# # # get hrCC model config and create model\n# # hrnet_yaml = \"../input/hrnet-ocr-cfg/seg_hrnet_ocr_w48.yaml\"\n# # with open(hrnet_yaml, 'r', encoding='utf-8') as fo:\n# #     cfg = fo.read()\n# # cfg = yaml.load(cfg, Loader=yaml.FullLoader)\n# # model = get_seg_cc_model(cfg, num_classes=5)\n\n# # load model pth\n# best_model_path = '../input/hr-ocr-full-all-ep110/hrnet_ocr_all_120_dice96.94_ep110.pth'\n# best_model_state_dict = torch.load(best_model_path, map_location='cpu')['state_dict']\n# model_dict = model.state_dict()\n# pretrained_dict = {k.replace('module.', '', 1): v for k, v in best_model_state_dict.items()\n#                    if k.replace('module.', '', 1) in model_dict.keys()}\n\n# print('load pretrained model..., its length is {}/{}'.format(len(pretrained_dict.items()), len(model_dict.items())))\n\n# model_dict.update(pretrained_dict)\n# model.load_state_dict(model_dict, strict=True)\n# device='cuda:0' if torch.cuda.is_available() else 'cpu'\n# model.to(device)\n# model.eval()\n\n# # get data\n# defect_root = '/kaggle/input/severstal-steel-defect-detection/test_images'\n# dataset = SteelDataset(defect_root, resize_long=(800, 800), img_normal=True, to_torch=True)\n# data_loader = DataLoader(dataset, batch_size=4, shuffle=False, num_workers=4, pin_memory=True)\n# dataset = SteelDatasetMSF(defect_root, resize_long=None, img_normal=True, to_torch=True, scales=(0.4, 0.5))\n# data_loader = DataLoader(dataset, shuffle=False, num_workers=4, pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"get res2net + deeplabv3p"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader\n# from tqdm import tqdm\n# import cv2\n\n\n\n# model = Res2NetDeeplabV3P(num_classes=5,aspp_dilate=(6, 12, 18))\n\n# # load model pth\n# best_model_path = '../input/deeplabv3p6-full-all-ep60/deeplabv3p6_dice96.29_ep60.pth'\n# best_model_state_dict = torch.load(best_model_path, map_location='cpu')['state_dict']\n# model_dict = model.state_dict()\n# pretrained_dict = {k.replace('module.', '', 1): v for k, v in best_model_state_dict.items()\n#                    if k.replace('module.', '', 1) in model_dict.keys()}\n\n# print('load pretrained model..., its length is {}/{}'.format(len(pretrained_dict.items()), len(model_dict.items())))\n\n# model_dict.update(pretrained_dict)\n# model.load_state_dict(model_dict, strict=True)\n# device='cuda:0' if torch.cuda.is_available() else 'cpu'\n# model.to(device)\n# model.eval()\n\n# # get data\n# defect_root = '/kaggle/input/severstal-steel-defect-detection/test_images'\n# dataset = SteelDataset(defect_root, resize_long=(800, 800), img_normal=True, to_torch=True)\n# data_loader = DataLoader(dataset, batch_size=4, shuffle=False, num_workers=4, pin_memory=True)\n# # dataset = SteelDatasetMSF(defect_root, resize_long=None, img_normal=True, to_torch=True, scales=(0.4, 0.5))\n# # data_loader = DataLoader(dataset, shuffle=False, num_workers=4, pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ensemble model define"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model:\n    def __init__(self, models, is_cuda=True):\n        if is_cuda:\n            self.models = [model.cuda() for model in models]\n        else:\n            self.models = models\n\n    def __call__(self, x):\n        res = []\n        with torch.no_grad():\n            for model in self.models:\n                logit = model(x)\n                if isinstance(logit, list):\n                    logit = logit[1]\n                logit = F.interpolate(logit, (64, 200), mode='bilinear', align_corners=True)\n                res.append(logit)\n        res = torch.stack(res)\n        return torch.mean(res, dim=0)\n\n    \ndef load_model(model, model_path):\n    best_model_state_dict = torch.load(model_path, map_location='cpu')['state_dict']\n    model_dict = model.state_dict()\n\n    # import re\n    # for key in best_model_state_dict.keys():\n    #     if 'asp' in key:\n    #         print(key)\n    #         print(key.replace(r'^module.', ''))\n    pretrained_dict = {k.replace('module.', '', 1): v for k, v in best_model_state_dict.items()\n                       if k.replace('module.', '', 1) in model_dict.keys()}\n\n    print('load pretrained model {}, its length is {}/{}'.format(model_path,\n                                                                 len(pretrained_dict.items()),\n                                                                 len(model_dict.items())))\n    # print(set(model_dict.keys()).difference(set(pretrained_dict.keys())))\n\n    model_dict.update(pretrained_dict)\n    model.load_state_dict(model_dict, strict=True)\n    model.eval()\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SMP model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ! pip install segmentation-models-pytorch\n!cp -r ../input/pytorch-segmentation-models/segmentation_models_pytorch-0.1.3-py3-none-any.whl /kaggle/working\n!cp -r ../input/pytorch-segmentation-models/timm-0.3.2-py3-none-any.whl /kaggle/working\n!cp -r ../input/smpwhl/efficientnet_pytorch-0.6.3-py3-none-any.whl /kaggle/working\n!cp -r ../input/smpwhl/pretrainedmodels-0.7.4-py3-none-any.whl /kaggle/working\n\n!pip install /kaggle/working/timm-0.3.2-py3-none-any.whl\n!pip install /kaggle/working/efficientnet_pytorch-0.6.3-py3-none-any.whl\n!pip install /kaggle/working/pretrainedmodels-0.7.4-py3-none-any.whl\n!pip install /kaggle/working/segmentation_models_pytorch-0.1.3-py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import segmentation_models_pytorch as smp\n# model = smp.Unet(\n#             encoder_name=\"timm-efficientnet-b5\",\n#             encoder_weights = None,\n#             in_channels=3,  # model input channels (1 for grayscale images, 3 for RGB, etc.)\n#             classes=5,  # model output channels (number of classes in your dataset)\n#         )\n# model_path = '../input/u-enetb5-full-all-ep110/enb5_net_dice96.04_ep110.pth'\n# model = load_model(model, model_path)\n# model = model.cuda()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ocr"},{"metadata":{"trusted":true},"cell_type":"code","source":"ALIGN_CORNERS = True\nBN_MOMENTUM = 0.1\n\n\nclass ModuleHelper:\n\n    @staticmethod\n    def BNReLU(num_features, bn_type=None, **kwargs):\n        return nn.Sequential(\n            BatchNorm2d(num_features, **kwargs),\n            nn.ReLU()\n        )\n\n    @staticmethod\n    def BatchNorm2d(*args, **kwargs):\n        return BatchNorm2d\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass SpatialGather_Module(nn.Module):\n    \"\"\"\n        Aggregate the context features according to the initial\n        predicted probability distribution.\n        Employ the soft-weighted method to aggregate the context.\n    \"\"\"\n    def __init__(self, cls_num=0, scale=1):\n        super(SpatialGather_Module, self).__init__()\n        self.cls_num = cls_num\n        self.scale = scale\n\n    def forward(self, feats, probs):\n        batch_size, c, h, w = probs.size(0), probs.size(1), probs.size(2), probs.size(3)\n        probs = probs.view(batch_size, c, -1)\n        feats = feats.view(batch_size, feats.size(1), -1)\n        feats = feats.permute(0, 2, 1) # batch x hw x c\n        probs = F.softmax(self.scale * probs, dim=2)# batch x k x hw\n        ocr_context = torch.matmul(probs, feats)\\\n        .permute(0, 2, 1).unsqueeze(3)# batch x k x c\n        return ocr_context\n\n\nclass _ObjectAttentionBlock(nn.Module):\n    '''\n    The basic implementation for object context block\n    Input:\n        N X C X H X W\n    Parameters:\n        in_channels       : the dimension of the input feature map\n        key_channels      : the dimension after the key/query transform\n        scale             : choose the scale to downsample the input feature maps (save memory cost)\n        bn_type           : specify the bn type\n    Return:\n        N X C X H X W\n    '''\n    def __init__(self,\n                 in_channels,\n                 key_channels,\n                 scale=1,\n                 bn_type=None):\n        super(_ObjectAttentionBlock, self).__init__()\n        self.scale = scale\n        self.in_channels = in_channels\n        self.key_channels = key_channels\n        self.pool = nn.MaxPool2d(kernel_size=(scale, scale))\n        self.f_pixel = nn.Sequential(\n            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels,\n                kernel_size=1, stride=1, padding=0, bias=False),\n            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),\n            nn.Conv2d(in_channels=self.key_channels, out_channels=self.key_channels,\n                kernel_size=1, stride=1, padding=0, bias=False),\n            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),\n        )\n        self.f_object = nn.Sequential(\n            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels,\n                kernel_size=1, stride=1, padding=0, bias=False),\n            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),\n            nn.Conv2d(in_channels=self.key_channels, out_channels=self.key_channels,\n                kernel_size=1, stride=1, padding=0, bias=False),\n            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),\n        )\n        self.f_down = nn.Sequential(\n            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels,\n                kernel_size=1, stride=1, padding=0, bias=False),\n            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),\n        )\n        self.f_up = nn.Sequential(\n            nn.Conv2d(in_channels=self.key_channels, out_channels=self.in_channels,\n                kernel_size=1, stride=1, padding=0, bias=False),\n            ModuleHelper.BNReLU(self.in_channels, bn_type=bn_type),\n        )\n\n    def forward(self, x, proxy):\n        batch_size, h, w = x.size(0), x.size(2), x.size(3)\n        if self.scale > 1:\n            x = self.pool(x)\n\n        query = self.f_pixel(x).view(batch_size, self.key_channels, -1)\n        query = query.permute(0, 2, 1)\n        key = self.f_object(proxy).view(batch_size, self.key_channels, -1)\n        value = self.f_down(proxy).view(batch_size, self.key_channels, -1)\n        value = value.permute(0, 2, 1)\n\n        sim_map = torch.matmul(query, key)\n        sim_map = (self.key_channels**-.5) * sim_map\n        sim_map = F.softmax(sim_map, dim=-1)\n\n        # add bg context ...\n        context = torch.matmul(sim_map, value)\n        context = context.permute(0, 2, 1).contiguous()\n        context = context.view(batch_size, self.key_channels, *x.size()[2:])\n        context = self.f_up(context)\n        if self.scale > 1:\n            context = F.interpolate(input=context, size=(h, w), mode='bilinear', align_corners=ALIGN_CORNERS)\n\n        return context\n\n\nclass ObjectAttentionBlock2D(_ObjectAttentionBlock):\n    def __init__(self,\n                 in_channels,\n                 key_channels,\n                 scale=1,\n                 bn_type=None):\n        super(ObjectAttentionBlock2D, self).__init__(in_channels,\n                                                     key_channels,\n                                                     scale,\n                                                     bn_type=bn_type)\n\n\nclass SpatialOCR_Module(nn.Module):\n    \"\"\"\n    Implementation of the OCR module:\n    We aggregate the global object representation to update the representation for each pixel.\n    \"\"\"\n    def __init__(self,\n                 in_channels,\n                 key_channels,\n                 out_channels,\n                 scale=1,\n                 dropout=0.1,\n                 bn_type=None):\n        super(SpatialOCR_Module, self).__init__()\n        self.object_context_block = ObjectAttentionBlock2D(in_channels,\n                                                           key_channels,\n                                                           scale,\n                                                           bn_type)\n        _in_channels = 2 * in_channels\n\n        self.conv_bn_dropout = nn.Sequential(\n            nn.Conv2d(_in_channels, out_channels, kernel_size=1, padding=0, bias=False),\n            ModuleHelper.BNReLU(out_channels, bn_type=bn_type),\n            nn.Dropout2d(dropout)\n        )\n\n    def forward(self, feats, proxy_feats):\n        context = self.object_context_block(feats, proxy_feats)\n\n        output = self.conv_bn_dropout(torch.cat([context, feats], 1))\n\n        return output\n\n\nclass Res2net_OCR(nn.Module):\n    def __init__(self, backbone='timm-res2net50_26w_4s',num_classes=5):\n        super(Res2net_OCR, self).__init__()\n        base = smp.Unet(\n            encoder_name=backbone,  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n            encoder_weights=None,  # use `imagenet` pretreined weights for encoder initialization\n            in_channels=3,  # model input channels (1 for grayscale images, 3 for RGB, etc.)\n            classes=256,  # model output channels (number of classes in your dataset)\n        )\n        \n        self.backbone = base.encoder\n        for i, m in enumerate(base.decoder.children()):\n            if i==1:\n              self.Udecoder=m[0:2]\n              # print(self.Udecoder)\n\n        last_inp_channels = 128\n        ocr_mid_channels = 256\n        ocr_key_channels = 256\n\n        self.conv3x3_ocr = nn.Sequential(\n            nn.Conv2d(last_inp_channels, ocr_mid_channels,\n                      kernel_size=3, stride=1, padding=1),\n            BatchNorm2d(ocr_mid_channels),\n            nn.ReLU(inplace=relu_inplace),\n        )\n        self.ocr_gather_head = SpatialGather_Module(num_classes)\n\n        self.ocr_distri_head = SpatialOCR_Module(in_channels=ocr_mid_channels,\n                                                 key_channels=ocr_key_channels,\n                                                 out_channels=ocr_mid_channels,\n                                                 scale=1,\n                                                 dropout=0.05,\n                                                 )\n        self.cls_head = nn.Conv2d(\n            ocr_mid_channels, num_classes, kernel_size=1, stride=1, padding=0, bias=True)\n\n        self.aux_head = nn.Sequential(\n            nn.Conv2d(last_inp_channels, last_inp_channels,\n                      kernel_size=1, stride=1, padding=0),\n            BatchNorm2d(last_inp_channels),\n            nn.ReLU(inplace=relu_inplace),\n            nn.Conv2d(last_inp_channels, num_classes,\n                      kernel_size=1, stride=1, padding=0, bias=True)\n        )\n\n        self.fuse = nn.ModuleList([self.aux_head, self.conv3x3_ocr, self.ocr_gather_head,\n                                   self.ocr_distri_head, self.cls_head, self.Udecoder])\n\n    def forward(self, x):\n        feats = self.backbone(x)\n        new_feats = feats[-1]\n        for i in range(2):\n          new_feats = F.interpolate(input=new_feats, size=feats[-2-i].shape[2:], mode='bilinear', align_corners=ALIGN_CORNERS)\n          new_feats = torch.cat((new_feats, feats[-2-i]), dim=1)\n          new_feats = self.Udecoder[i](new_feats)\n          # print(new_feats.shape)\n\n        feats = new_feats\n\n        out_aux_seg = []\n\n        # ocr\n        out_aux = self.aux_head(feats)\n        # compute contrast feature\n        feats = self.conv3x3_ocr(feats)\n\n        context = self.ocr_gather_head(feats, out_aux)\n        feats = self.ocr_distri_head(feats, context)\n\n        out = self.cls_head(feats)\n\n        out_aux_seg.append(out_aux)\n        out_aux_seg.append(out)\n\n        return out_aux_seg\n\n    def init_weights(self, pretrained='', ):\n        print('=> init weights from normal distribution')\n        for name, m in self.named_modules():\n            if any(part in name for part in {'cls', 'aux', 'ocr', 'backbone'}):\n                # print('skipped', name)\n                continue\n            if isinstance(m, nn.Conv2d):\n                nn.init.normal_(m.weight, std=0.001)\n            elif isinstance(m, BatchNorm2d_class):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        if os.path.isfile(pretrained):\n            pretrained_dict = torch.load(pretrained, map_location={'cuda:0': 'cpu'})\n            print('=> loading pretrained model {}'.format(pretrained))\n            model_dict = self.state_dict()\n            pretrained_dict = {k.replace('last_layer', 'aux_head').replace('model.', ''): v for k, v in\n                               pretrained_dict.items()}\n            # print(set(model_dict) - set(pretrained_dict))\n            # print(set(pretrained_dict) - set(model_dict))\n            pretrained_dict = {k: v for k, v in pretrained_dict.items()\n                               if k in model_dict.keys()}\n            print('length of pretrained_dict:', len(pretrained_dict.items()))\n            # for k, _ in pretrained_dict.items():\n            # logger.info(\n            #     '=> loading {} pretrained model {}'.format(k, pretrained))\n            model_dict.update(pretrained_dict)\n            self.load_state_dict(model_dict)\n        elif pretrained:\n            raise RuntimeError('No such file {}'.format(pretrained))\n\n    def trainable_parameters(self):\n        return list(self.backbone.parameters()), list(self.fuse.parameters())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"get ensemble model"},{"metadata":{"trusted":true},"cell_type":"code","source":"## get device\ndevice='cuda:0' if torch.cuda.is_available() else 'cpu'\nimport segmentation_models_pytorch as smp\n\n# hr_ocr_model\n# get hr_ocr model config and create model\nhrnet_ocr_yaml = \"../input/hrnet-ocr-cfg/seg_hrnet_ocr_w48.yaml\"\nwith open(hrnet_ocr_yaml, 'r', encoding='utf-8') as fo:\n    cfg = fo.read()\ncfg = yaml.load(cfg, Loader=yaml.FullLoader)\n\n# model1 = get_hr_ocr_seg_model(cfg, num_classes=5)\n# model1_path = '../input/hr-ocr-ohem-full-all-ep60/hrnet_ocr_ohem_60_dice96.39_ep60.pth'\n# model1 = load_model(model1, model1_path)\n\n# # deeplabv3p6\n# model2 = Res2NetDeeplabV3P(num_classes=5,aspp_dilate=(6, 12, 18))\n# model2_path = '../input/deeplabv3p6-full-all-ep60/deeplabv3p6_dice96.29_ep60.pth'\n# model2 = load_model(model2, model2_path)\n\n# # hr_cc\n# model3 = get_seg_cc_model(cfg, num_classes=5)\n# model3_path = '../input/hr-cc-full-all-ep60/hrnet_cc_all_60_ep60.pth'\n# model3 = load_model(model3, model3_path)\n\n# # hr_ocr_asp4\n# model4 = get_hr_ocr_asp_seg_model(cfg, num_classes=5, dilations=(4, 8, 12))\n# model4_path = '../input/hr-ocr-ohem-asp4-full-all-ep60/hr_ocr_ohem_asp4_dice96.43_ep60.pth'\n# model4 = load_model(model4, model4_path)\n\n# # efficient net b5\n\n# model5 = smp.Unet(\n#             encoder_name=\"timm-efficientnet-b5\",\n#             encoder_weights = None,\n#             in_channels=3,  # model input channels (1 for grayscale images, 3 for RGB, etc.)\n#             classes=5,  # model output channels (number of classes in your dataset)\n#         )\n# model5_path = '../input/u-enetb5-full-all-ep110/enb5_net_dice96.04_ep110.pth'\n# model5 = load_model(model5, model5_path)\n\n# model6 = smp.FPN(\n#             encoder_name=\"timm-efficientnet-b3\",  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n#             encoder_weights=None,\n#             in_channels=3,  # model input channels (1 for grayscale images, 3 for RGB, etc.)\n#             classes=5,  # model output channels (number of classes in your dataset)\n#         )\n# model6_path = '../input/fpn-enb/FPN_efficientnetb3_ep120.pth'\n# model6 = load_model(model6, model6_path)\n\n# model7 = Res2net_OCR()\n# model7_path = '../input/res2net-ocr/res2net_ocr_ep110.pth'\n# model7 = load_model(model7, model7_path)\n\n# model8 = Res2net_OCR(backbone='efficientnet-b5')\n# model8_path = '../input/enb5-ocr/enb5_ocr_ep120.pth'\n# model8 = load_model(model8, model8_path)\n\n# model9 = get_hr_ocr_asp_seg_model(cfg, num_classes=5, dilations=(4,8,12))\n# model9_path = '../input/hr-ocr-ohem-asp4-full-part-ep20/hr_ocr_asp_part_dice94.24NLL_syncbn_ep19.pth'\n# model9 = load_model(model9, model9_path)\n\nmodel10 = Res2net_OCR(backbone=\"timm-res2net50_26w_8s\", num_classes=5)\nmodel10_path = '../input/r2n-ocr-crop-ep120/res2net_26w_8s_ocr_ep120.pth'\nmodel10 = load_model(model10, model10_path)\n\nmodela = get_hr_ocr_seg_model(cfg, num_classes=5)\nmodela_path = '../input/hr-ocr-new/hr_ocr_new_ep60.pth'\nmodela = load_model(modela, modela_path)\n\n# ensemble model\nmodel_ls = [model10, modela]\nmodel = Model(model_ls, is_cuda=torch.cuda.is_available())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"get test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get data\ndefect_root = '/kaggle/input/severstal-steel-defect-detection/test_images'\n# dataset = SteelDataset(defect_root, resize_long=(800, 800), img_normal=True, to_torch=True)\ndataset = SteelDataset(defect_root, img_normal=True, to_torch=True)\ndata_loader = DataLoader(dataset, batch_size=4, shuffle=False, num_workers=4, pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"device='cuda:0' if torch.cuda.is_available() else 'cpu'\n# pred labels\nimage_id_class_id = []\nencoded_pixel = []\n\n# threshold_group1\nthreshold_label      = [ 0.75, 0.85, 0.50, 0.50,]\nthreshold_mask_pixel = [ 0.40, 0.40, 0.40, 0.40,]\n# threshold_mask_size  = [   40,   40,   40,   40,]\n# threshold_mask_size  = [  350,  500,  750, 1000,]  # maskthres1\n# threshold_mask_size  = [   50,  250,  1250, 1800,]  # maskthres2\nthreshold_mask_size  = [   50,  250,  1400, 1800,]  # maskthres3\n# threshold_mask_size  = [   50,  150,  900, 400,]  # maskthres4\n\n# # thres_group2\n# threshold_label = [0.65, 0.80, 0.75, 0.70, ]\n# threshold_mask_pixel = [0.30, 0.30, 0.30, 0.35, ]\n# threshold_mask_size = [100, 400, 1200, 1000, ]\n\n# threshold_label = [0.70, 0.20, 0.65, 0.50]\n# threshold_mask_pixel = [0.40, 0.40, 0.40, 0.40, ]\n# threshold_mask_size = [200, 100, 500, 400, ]\n\nwith torch.no_grad():\n    for i_iter, batch in enumerate(data_loader):\n        img_name = [name.split('/')[-1] for name in batch['name']]\n        img = batch['img'].to(device)\n        \n#         num_augment = 0\n#         probability_mask = torch.zeros((1,4,256,1600)).to(device)\n#         probability_label = torch.zeros((1, 4)).to(device)\n#         for img in batch['img']:\n#             img = img.to(device)\n\n#         scales = [0.5, 1., 1.5]\n#         h, w = img.shape[-2:]\n#         bs = img.shape[0]\n#         img_list = []\n#         for scale in scales:\n#             img_s = F.interpolate(img.detach(), (int(h*scale), int(w*scale)), mode='bilinear')\n#             img_list.append(img_s)\n        \n#         num_augment = 0\n#         probability_mask = torch.zeros((bs, 4,256,1600)).to(device)\n#         probability_label = torch.zeros((bs, 4)).to(device)\n        \n#         for img in img_list:\n#         s1 = time.time()\n        if 1: # 'null' in augment:\n            logit = model(img) #data_parallel(net,input)  #net(input)\n            if isinstance(logit, list):\n                logit = logit[1]\n            logit = F.interpolate(logit, (256, 1600), mode='bilinear', align_corners=True)\n            probability = torch.softmax(logit,1)\n\n            probability_mask  = probability[:,1:] #just drop background\n            probability_label = probability_mask_to_probability_label(probability)[:,1:]\n            num_augment = 1\n\n        if 1 : #'flip_lr' in augment:\n            logit = model(torch.flip(img,dims=[3]))\n            if isinstance(logit, list):\n                logit = logit[1]\n            logit = F.interpolate(logit, (256, 1600), mode='bilinear', align_corners=True)\n            probability  = torch.softmax(torch.flip(logit,dims=[3]),1)\n\n            probability_mask  += probability[:,1:] #just drop background\n            probability_label += probability_mask_to_probability_label(probability)[:,1:]\n            num_augment +=1\n\n        if 1 : #'flip_ud' in augment:\n            logit = model(torch.flip(img,dims=[2]))\n            if isinstance(logit, list):\n                logit = logit[1]\n            logit = F.interpolate(logit, (256, 1600), mode='bilinear', align_corners=True)\n            probability = torch.softmax(torch.flip(logit,dims=[2]),1)\n\n            probability_mask  += probability[:,1:] #just drop background\n            probability_label += probability_mask_to_probability_label(probability)[:,1:]\n            num_augment +=1\n            \n            # print(time.time()-s1)\n\n        #---\n        probability_mask  = probability_mask/num_augment\n        probability_label = probability_label/num_augment\n        \n        probability_mask  = probability_mask.data.cpu().numpy()\n        probability_label = probability_label.data.cpu().numpy()\n        \n        for b in range(img.shape[0]):\n            for c in range(4):\n                rle=''\n\n                predict_label = probability_label[b,c]>threshold_label[c]\n                if predict_label:\n                    try:\n                        predict_mask = probability_mask[b,c] > threshold_mask_pixel[c]\n                        predict_mask = post_process(predict_mask, threshold_mask_size[c])\n                        rle = run_length_encode(predict_mask)\n\n                    except:\n                        print('An exception occurred : %s'%(image_id[b]+'_%d'%(c+1)))\n\n                # print(img_name[b]+'_%d'%(c+1))\n                image_id_class_id.append(img_name[b]+'_%d'%(c+1))\n                encoded_pixel.append(rle)\n        \n        if (i_iter+1) % 100 == 0:\n            print('{}/{}'.format(i_iter+1,len(dataset)), 'time:', time.time()-s_time)\n\n    df = pd.DataFrame(zip(image_id_class_id, encoded_pixel), columns=['ImageId_ClassId', 'EncodedPixels'])\n    df.to_csv('submission.csv', index=False)\n    \n    print('total_time:', time.time()-s_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import os\n# os.remove('../input/hrnet-full-part-vflip')\n# os.remove('../input/hrnet-full-part-nocoloraug')\n# os.remove('../input/hrmodelall')\n# os.remove('../input/hrnet-full-all-40ep')\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}