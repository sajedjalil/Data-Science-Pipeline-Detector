{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThis notebook contains a detailed description of the models that I tried for the COVID19 global forecasting challenge. The challenge was to predict the number of cases and casualties in every state of every country every day for the following 6 weeks.\n\nSince the challenge require quite a bit of extrapolation, instead of using typical machine learning tools (random forests, regression, ARIMA models), that are not the best extrapolating non-seasonal data so far in the future, it is better to use some domain knowledge about epidemiology.\n\nWe will start loading the necessary libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport matplotlib.dates as mdates\nfrom datetime import timedelta\nfrom scipy.optimize import curve_fit\nfrom scipy.stats import linregress\nfrom scipy.special import erf\nfrom sklearn.metrics import mean_squared_error\nimport warnings\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preparation\n\nNow we read the files and prepare the dataframes for analysis.\n\nThis involves telling pandas how to handle the date column, how to fill missing values, small cleanup tricks, plus adding two columns with logarithmic values of the confirmed cases and fatalities. Because of the nature of the data, it is better to predict the logarithmic values than the values themselves.\n\nTo get back to the confirmed cases, we just have to exponentiate the predictions.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/covid19-global-forecasting-week-1/train.csv')\ntest_data = pd.read_csv('../input/covid19-global-forecasting-week-1/test.csv')\n\n# Change column names: '/' character may cause problems\n\ntrain_data = train_data.rename(columns={ 'Province/State' : 'State', 'Country/Region' : 'Country',\n                                         'Date' : 'DateAsString' })\n\ntest_data = test_data.rename(columns={ 'Province/State' : 'State', 'Country/Region' : 'Country',\n                                         'Date' : 'DateAsString' })\n\n# Put dates as datetime64 datatype\n\ntrain_data['Date'] = pd.to_datetime(train_data['DateAsString'], format='%Y-%m-%d')\ntest_data['Date'] = pd.to_datetime(test_data['DateAsString'], format='%Y-%m-%d')\n\n# If there are no states, there is only one state, called 'All'\n\ntrain_data['State'] = train_data['State'].fillna('All')\ntest_data['State'] = test_data['State'].fillna('All')\n\n# Take out aposthrophes in countries with aposthropes (it messes with the string definitions!)\n\ntrain_data = train_data.replace(\"Cote d'Ivoire\",\"Cote d Ivoire\")\ntest_data = test_data.replace(\"Cote d'Ivoire\",\"Cote d Ivoire\")\n\n\n# Add logaritmic values, because it is often the best metric here\n\ntrain_data[['LogConfirmed']] = train_data[['ConfirmedCases']].apply(np.log)\ntrain_data[['LogFatalities']] = train_data[['Fatalities']].apply(np.log)\n\ntest_data['ConfirmedPred'] = np.zeros(test_data.shape[0])\ntest_data['FatalitiesPred'] = np.zeros(test_data.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, a couple of variable definitions that will be necessary later...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"last_day = np.datetime64('2020-04-23')\nfirst_prediction = np.datetime64('2020-03-12')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding a model for prediction\n\n## Logistic-like model\n\nThere are several models to predict the number of infected people over time. A well-known one is the [SIR model](https://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology#The_SIR_model). Unfortunately, the model cannot be written in a [functional form easily](https://arxiv.org/abs/1403.2160), therefore, it is not the best one to fit the data to.\n\nTherefore, we will try with a simpler functional form. We need a \"test\" case, *i. e*. data from a country where the epidemic is already in a \"stable\" stage. The paradigmatic case is South Korea. Let's have a look at the evolution (in logarithmic scale) of the pandemic after 100 confirmed cases (before, the data is too noisy).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_korea = train_data.query(\"Country == 'Korea, South'\")\n\ntrain_data_korea_100 = train_data_korea.query(\"ConfirmedCases > 100\")\n\nplt.plot(train_data_korea_100['LogConfirmed'])\nplt.plot(train_data_korea_100['LogFatalities'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like a simple [logistic function](https://en.wikipedia.org/wiki/Logistic_function) would be a good fit. We are going to try it and two other functions that look similar: the [error function](https://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology#The_SIR_model) and the [log-normal function](https://en.wikipedia.org/wiki/Log-normal_distribution) (the log-normal function has [skewness](https://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology#The_SIR_model), like the SIR model function).\n\nWe are going to use the mean squared error (MSE) as the metric to decide which function fits better the logarithm of confirmed cases over time. The one with the lower MSE is the better one for the fit.\n\nOne may argue that we have to divide the data in train and test subsets, and calculate MSE over the latter. But here we are looking for a descriptive model, more than a predictive one, so using all the data is justified. Again, why may argue that we *do* want to predict future cases, so we should look at the predictive capacity of the model. And this is true, but not here. The model that predicts better future cases in a country where the epidemics is in a \"stable\" stage is not necessarily good in countries where the number of cases are increasing fast. Once again, *here* we are just looking for a descriptive model. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_to_fit(x,k,L):\n    return L/(1 + np.exp(-k*x))\n\ndef error_to_fit(x,k,L):\n    return L*(1 + erf(k*x))\n\ndef log_normal_to_fit(x,k,L,x_0):\n    return L*(1+ erf(np.log(x)-x_0)/k)\n\nx_tofit = np.arange(train_data_korea_100.shape[0])\ny_tofit = train_data_korea_100['LogConfirmed'].to_numpy()\n\nmodel_to_fit = [ logistic_to_fit, error_to_fit, log_normal_to_fit]\npopt_confirmed = []\npcov_confirmed = []\n\nfor i_model in model_to_fit:\n    i_popt, i_pcov = curve_fit(i_model, x_tofit, y_tofit)\n    popt_confirmed.append(i_popt)\n    pcov_confirmed.append(i_pcov)\n    mse = mean_squared_error(y_tofit,i_model(x_tofit,*i_popt))\n    print('For model {0} the MSE is {1}'.format(str(i_model).split()[1],mse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like the logistic function is the one that fits best! The other models don't fit the data bad either. Other kagglers have used the logistic function to fit the data, but this is not exactly what we have done here. Here we are using the logistic function on the logarithm of the number of cases. Mathematically, our fitting function is:\n\n$\\displaystyle \\ln y = \\frac{L}{1+e^{-k(x-x_{0})}} \\Rightarrow y = \\exp\\left(\\frac{L}{1+e^{-k(x-x_{0})}}\\right)$\n\nwhile other kagglers have tried:\n\n$\\displaystyle y = \\frac{L}{1+e^{-k(x-x_{0})}}$\n\n## Exponential model\n\nThe logistic equation is a good fit when the epidemics has stabilized or in countries heading towards stabilization. At the beginning of the epidemics, however, the number of confirmed cases increases exponentially (so we can fit it with a linear equation in the logarithmic space).\n\nHow do we know if logistic regression or linear regression is the best? As we did it in the previous section we can use the MSE to decide which equation fit better the results (linear or logistic).\n\nBut we can do it even better. Why choosing between linear and logistic fit? Let's use both!\n\nWait... what? The idea is easy, instead of taking only one solution, we take a weighted average of both $\\ln y = ax + b$ and $\\ln y = L/1+e^{-k(x-x_{0})}$. Which are the weights? Let's use precisely MSE as a weight for that:\n\n$\\displaystyle w_{\\text{logistic}} = \\frac{MSE_{\\text{linear}}^{2}}{MSE_{\\text{logistic}}^{2} + MSE_{\\text{linear}}^{2}}$ ; $\\displaystyle w_{\\text{linear}} = \\frac{MSE_{\\text{logistic}}^{2}}{MSE_{\\text{logistic}}^{2} + MSE_{\\text{linear}}^{2}}$\n\nIt sounds weird that the weight of the logistic regression involves the MSE of the linear regression, but it makes kind of sense. At the end, we want to give *less* weight to the logistic regression, if the linear regression performs better (*i. e.* its MSE is *lower*).\n\n**Note:** There are better ways of getting a prediction that is a mixture of two regression models (such as the [voting regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html), implemented in scikit-learn). Unfortunately, given the time constrains of this competition, I didn't have the time to implement it.\n\nOK. Enough text (and math). Let's implement a function that predict the number of cases if we give it a dataframe with the data from this Country/State.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def do_fitting(state_data):\n    x_tofit = np.arange(state_data.shape[0])\n    y_tofit = state_data['LogConfirmed'].to_numpy()\n    offset_array = state_data['LogConfirmed'].to_numpy() - state_data['LogFatalities'].to_numpy()\n    offset = offset_array.mean()\n    not_fit = False\n    warnings.filterwarnings('error')\n    try:\n        popt, pcov = curve_fit(logistic_to_fit, x_tofit, y_tofit, p0=[0.23, 9])\n    except Warning:\n        not_fit = True\n    slope, intercept, rvalue, pvalue, stderr = linregress(x_tofit, y_tofit)\n    warnings.resetwarnings()\n\n    if not_fit:\n        def linear_fitted(x):\n            y = intercept + slope * x\n            return y\n        def logistics_fitted(x):\n            y = intercept + slope * x\n            return y\n\n        mse_logistic = 0.5\n        mse_linear = 0.5\n        popt = (1e+10,1e+10)\n\n    else:\n        def logistics_fitted(x):\n            return logistic_to_fit(x,*popt)\n        def linear_fitted(x):\n            y = intercept + slope * x\n            return y\n\n        mse_logistic = mean_squared_error(y_tofit, logistics_fitted(x_tofit))\n        mse_linear = mean_squared_error(y_tofit,linear_fitted(x_tofit))\n\n    weight_logistic2 = mse_linear*mse_linear / (mse_linear*mse_linear+mse_logistic*mse_logistic)\n    weight_linear2 = mse_logistic*mse_logistic / (mse_linear * mse_linear + mse_logistic * mse_logistic)\n\n    def weighted_fitted(x):\n        y = weight_logistic2 * logistics_fitted(x) + weight_linear2 * linear_fitted(x)\n        return y\n\n    return (weighted_fitted,logistics_fitted,linear_fitted,weight_logistic2,slope,np.exp(popt[1]),offset)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of fatalities is too small and too noisy to give accurate predictions after one month. However, one can see in South Korea that both graphs are quite similar, there is just an offset between the two graphs. This offset in the logarithmic graph translates to a factor between both: the death rate.\n\nTherefore, a good way to predict the number of fatalities would be predicting the number of cases and then using a state-dependent offset.\n\nIt is also a good idea predict differently states with more and less than 100 cases. Data from states with less than 100 cases are just too noisy, so it is hard to get a model from the data. To fit the latter, we are going to just predict an exponential increase in the number of cases (it makes sense that if there are so few cases, we are at the first step of the epidemic: the exponential increase). Which exponent (*i. e.* slope in the logarithmic graph) do we use? Let's use the average of the exponents (slopes) on the states with more than 100 cases. The offset between cases an fatalities will be also the average of the exponents on the states with more than 100 cases.\n\nThe average slope and offset used here is obtained [*a posteriori*](https://www.kaggle.com/enriqueabad/exponential-and-logistic-like-fit/#slope_and_offset). Again, not best practices for producing clean code, but the time constraint of the competition was tight.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def predictions_100plus(key,dictionary,plot_it=True):\n    country_values = dictionary[key]\n    country, state = key.split('; ')\n    texto = ''\n    weighted_fitted = country_values[0]\n    logistic_fitted = country_values[1]\n    linear_fitted = country_values[2]\n    weight_logistic2 = country_values[3]\n    slope = country_values[4]\n    stabilization = round(country_values[5])\n    offset = country_values[6]\n    first_day = country_values[7]\n    first_day_100 = country_values[8]\n    if weight_logistic2 > 0.5:\n        texto = texto + 'Stabilyzing phase. Odds {0:4.2f} to 1.\\n'.format(weight_logistic2/(1-weight_logistic2))\n    else:\n        texto = texto + 'Exponential phase. Odds {0:4.2f} to 1\\n'.format((1-weight_logistic2)/weight_logistic2)\n    t_double = np.log(2) / slope\n    texto = texto + 'Duplication every {0:5.2f} days\\n'.format(t_double)\n    texto = texto + 'The state may stabilize with {0} cases\\n'.format(stabilization)\n\n    difference = first_day_100 - first_day\n    first_day_int = difference.astype(int)\n    difference = last_day - first_day_100\n    last_day_int = difference.astype(int)\n    x_tofit = np.arange(-first_day_int, last_day_int)\n    x_date = np.arange(first_day, last_day)\n    y_exponential = np.exp(linear_fitted(x_tofit))\n    y_logistic = np.exp(logistic_fitted(x_tofit))\n    y_weighted = np.exp(weighted_fitted(x_tofit))\n    y_fatalities = np.exp(weighted_fitted(x_tofit) - offset)\n\n    if plot_it:\n        fig = plt.figure()\n        plt.plot(x_date, y_exponential, label='Exponential fit')\n        # This is because the annoying way of displaying dates by matplotlib\n        plt.xticks(np.arange(np.datetime64('2020-01-30'), np.datetime64('2020-04-15'),timedelta(days=15)),\n                   labels=np.datetime_as_string(np.arange(np.datetime64('2020-01-30'), np.datetime64('2020-04-15'),timedelta(days=15)),unit='D'))\n        plt.plot(x_date, y_logistic, label='Logistic fit')\n        plt.plot(x_date, y_weighted, label='Weighted fit')\n        plt.plot(x_date, y_fatalities, label='Fatalities fit')\n        plt.title('{0} ({1})'.format(country, state))\n        plt.legend()\n        plt.text(first_day, 4.9, texto)\n        fig.autofmt_xdate()\n    else:\n        for i_x, i_date in enumerate(x_date):\n            if i_date >= first_prediction:\n                query_text = \"Country == '{0}' & State == '{1}' & Date == '{2}'\".format(country,state,i_date)\n                index = test_data.query(query_text)['ForecastId'].to_numpy()[0]\n                y_weighted = np.exp(weighted_fitted(x_tofit[i_x]))\n                y_fatalities = np.exp(weighted_fitted(x_tofit[i_x]) - offset)\n                test_data.at[index,'ConfirmedPred'] = y_weighted\n                test_data.at[index,'FatalitiesPred'] = y_fatalities\n\ndef predictions_100minus(country,state,data_this_state):\n    #print(data_this_state)\n    avg_slope = 0.19266443693901186\n    avg_offset = 4.577772921762378\n    avg_growth_rate = np.exp(avg_slope)\n    avg_fatality_rate = np.exp(-avg_offset)\n    first_date = data_this_state.head(1)['Date'].to_numpy()[0]\n    last_date = data_this_state.tail(1)['Date'].to_numpy()[0]\n    this_date = first_date\n    while this_date <= last_date:\n        query_text = \"Country == '{0}' & State == '{1}' & Date == '{2}'\".format(country, state, this_date)\n        try:\n            index_test = test_data.query(query_text)['ForecastId'].index[0]\n            index_train = train_data.query(query_text)['Id'].index[0]\n            #print(index_train,index_test)\n            test_data.at[index_test, 'ConfirmedPred'] = train_data.loc[index_train,'ConfirmedCases']\n        except IndexError:\n            pass\n        this_date = this_date + np.timedelta64(1,'D')\n    last_confirmed = data_this_state.tail(1)['ConfirmedCases'].to_numpy()[0]\n    this_date = last_date + np.timedelta64(1,'D')\n    while this_date <= last_day:\n        query_text = \"Country == '{0}' & State == '{1}' & Date == '{2}'\".format(country, state, this_date)\n        index = test_data.query(query_text)['ForecastId'].index[0]\n        this_confirmed = last_confirmed * avg_growth_rate\n        this_fatality = this_confirmed * avg_fatality_rate\n        #print(this_date,this_confirmed,this_fatality)\n        test_data.at[index, 'ConfirmedPred'] = this_confirmed\n        test_data.at[index, 'FatalitiesPred'] = this_fatality\n        this_date = this_date + np.timedelta64(1, 'D')\n        last_confirmed = this_confirmed\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction\n\nOK, we have our model defined and described, and functions to predict the number of cases in countries with more and less than 100 cases. Let's put everything together in a loop and do predictions for all countries and all states.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"large_countries = train_data['Country'].unique()\n\ncountry_confirmed_parameters = {}\n\nfor i_country in large_countries:\n    print(i_country)\n    data_this_country = train_data.query(\"Country == '{0}'\".format(i_country))\n    states = data_this_country.State.unique()\n    for i_state in states:\n        data_this_state = data_this_country.query(\"State == '{0}'\".format(i_state))\n        #print(data_this_state)\n        data_this_state_100 = data_this_state.query(\"ConfirmedCases > 100 & Fatalities > 0\")\n        if data_this_state_100.shape[0] < 2:\n            predictions_100minus(i_country, i_state, data_this_state)\n        else:\n            confirmed_parameters_this_state = do_fitting(data_this_state_100)\n            first_day = data_this_state['Date'].iloc[0].to_numpy().astype('datetime64[D]')\n            first_day_100 = data_this_state_100['Date'].iloc[0].to_numpy().astype('datetime64[D]')\n            confirmed_parameters_this_state = confirmed_parameters_this_state + (first_day, first_day_100)\n            state_key = '{0}; {1}'.format(i_country, i_state)\n            country_confirmed_parameters[state_key] = confirmed_parameters_this_state\n            predictions_100plus(state_key,country_confirmed_parameters,plot_it=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And... that's it. Everything is predicted.\n\nFor states with more than 100 confirmed cases we can even see the graph!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom ipywidgets import widgets, interactive\n\ni_country = widgets.Dropdown(options=train_data['Country'].unique().tolist(),description='Country:',value='Iran')\ni_state = widgets.Dropdown(description='State')\n#i_state = widgets.Dropdown(options=train_data['State'].unique().tolist(),description='State:')\n\ndef update(*args):\n    i_state.options = train_data.query(\"Country == '{0}'\".format(i_country.value)).State.unique()\n    \ni_country.observe(update)\n\ndef plotit(i_country,i_state):\n    try:\n        state_key = '{0}; {1}'.format(i_country, i_state)\n        if state_key == 'Iran; None':\n            predictions_100plus('Iran; All',country_confirmed_parameters)\n        else:\n            predictions_100plus(state_key,country_confirmed_parameters)\n    \n    except KeyError:\n        print('This Country/State combination has less than 100 cases')\n    \n\ninteractive(plotit,i_country=i_country,i_state=i_state)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='slope_and_offset'></a>\n\nHere we got the average slope and offset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"list_confirmed_parameters = list(country_confirmed_parameters.values())\n\nslopes = []\noffsets = []\n\nfor i_element in list_confirmed_parameters:\n    slopes.append(i_element[4])\n    offsets.append(i_element[6])\n\naverage_slope = np.array(slopes).mean()\naverage_offset = np.array(offsets).mean()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And finally, we save the results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = test_data[['ForecastId','ConfirmedPred','FatalitiesPred']]\n\nprediction.to_csv('submission.csv',\n                  header=['ForecastId','ConfirmedCases','Fatalities'],index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}