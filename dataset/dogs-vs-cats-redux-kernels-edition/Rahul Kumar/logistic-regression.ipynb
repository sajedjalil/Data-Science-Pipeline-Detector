{"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"mimetype":"text/x-python","pygments_lexer":"ipython3","version":"3.6.1","file_extension":".py","name":"python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3}}},"nbformat":4,"nbformat_minor":1,"cells":[{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"f8babf22-df7d-4138-b572-2489fc680821","_uuid":"c655d92a826af5c2dcf71f7d6c96ddf97a06e9ec"},"execution_count":null,"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom scipy import ndimage, misc\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nimport glob\nimport matplotlib.pyplot as plt\nfrom random import shuffle\n\nfrom PIL import Image\n%matplotlib inline\n# Any results you write to the current directory are saved as output.\n"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"500e12b4-7cee-48f6-a884-e59defa94d9a","_uuid":"a5fedad247bdf102ca63913802052434da66064a"},"execution_count":null,"source":"trainlist = glob.glob('../input/train/*')\n\ntrain_catlist = [imagename for imagename in trainlist if 'cat' in imagename]\ntrain_doglist = [imagename for imagename in trainlist if 'dog' in imagename]\n\n# take 200 samples of each cat and dogs\nm_trainlist = train_catlist[:1000]\nm_trainlist.extend(train_doglist[:1000])\nshuffle(m_trainlist)\n\nm_testlist = train_catlist[-200:]\nm_testlist.extend(train_doglist[-200:])\nshuffle(m_testlist)\n\n\n# shuffle each train and test set\n"},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"a486ff71-4eb6-4672-bacc-81faa484b538","_uuid":"7e530afe59e0a29667b9130d64af73e8ca15a268"},"execution_count":null,"source":"\n#trainlist = trainlist[:10]\n#testlist = testlist[:10]\nx_train_orig = np.array([np.array(misc.imresize(ndimage.imread(imagename, mode='RGB'), (256,256,3))) for imagename in m_trainlist])\nx_test_orig = np.array([np.array(misc.imresize(ndimage.imread(imagename, mode='RGB'), (256,256,3))) for imagename in m_testlist])\ny_train_orig = np.array([0 if 'cat' in imagename else 1 for imagename in m_trainlist]).reshape((-1, 1)).T\ny_test_orig = np.array([0 if 'cat' in imagename else 1 for imagename in m_testlist]).reshape((-1, 1)).T\nprint(x_train_orig.shape)\nprint(x_test_orig.shape)\nplt.imshow(x_train_orig[1])"},{"outputs":[],"cell_type":"code","metadata":{"scrolled":false,"_cell_guid":"70e57c5c-67c8-4c77-bb7d-a6626f760625","_uuid":"945c8244668a265c3df6215be9b905f844c1d851"},"execution_count":null,"source":"x_train_flatten = x_train_orig.reshape(x_train_orig.shape[0], -1).T\nx_test_flatten = x_test_orig.reshape(x_test_orig.shape[0], -1).T\nprint (\"x_train_flatten shape: \" + str(x_train_flatten.shape))\nprint (\"y_train_orig shape: \" + str(y_train_orig.shape))\nprint (\"x_test_flatten shape: \" + str(x_test_flatten.shape))\nprint (\"y_test_orig shape: \" + str(y_test_orig.shape))\nprint (\"sanity check after reshaping: \" + str(x_train_flatten[0:5,0]))\n"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"03dd614c-f1bd-4146-b351-e219cbc14fc5","_uuid":"e1a7a8b34c399ef446403e65a4203b1207355515"},"execution_count":null,"source":"x_train = x_train_flatten / 255\nx_test = x_test_flatten / 255\ny_train = y_train_orig\ny_test = y_test_orig"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"c3782d47-f2e0-444a-b96f-3b01d12a10d6","_uuid":"6ad3c24f66eddad205e233637eb3c987058edaae"},"execution_count":null,"source":"def sigmoid(z):\n    \"\"\"\n    Arguments:\n    z -- A scalar or numpy array of any size.\n\n    Return:\n    s -- sigmoid(z)\n    \"\"\"\n    s = 1/(1 + np.exp(-z))\n    return s"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"f74a7642-cad8-49bd-befb-d3043753ef52","_uuid":"b15a441a0a5512dbfecb839b5d0b3a4b970abeb9"},"execution_count":null,"source":"def initialize_with_zeros(dim):\n    \"\"\"\n    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n    \n    Argument:\n    dim -- size of the w vector we want (or number of parameters in this case)\n    \n    Returns:\n    w -- initialized vector of shape (dim, 1)\n    b -- initialized scalar (corresponds to the bias)\n    \"\"\"\n    \n    w = np.zeros((dim, 1))\n    b = 0\n    \n    assert(w.shape == (dim, 1))\n    assert(isinstance(b, float) or isinstance(b, int))\n    \n    return w, b"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"d70b6b03-a93d-486e-9cb6-46925a5dc550","_uuid":"24f1017c15f559029b24df612c18fe59743da191"},"execution_count":null,"source":"def propagate(w, b, X, Y):\n    \"\"\"\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if dog) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n    \"\"\"\n    \n    m = X.shape[1]\n    \n    # FORWARD PROPAGATION (FROM X TO COST)\n   \n    A = sigmoid(np.dot(w.T, X) + b)                                     # compute activation\n    cost = -1/m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n   \n    \n    # BACKWARD PROPAGATION (TO FIND GRAD)\n   \n    dw = 1/m * np.dot(X, (A -Y).T)\n    db = 1/m * np.sum(A-Y)\n\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads, cost"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"5fecd540-576e-477e-acaf-405be60b1c5d","_uuid":"053c16894e5381d21b426e73b027a7e954c6d0e3"},"execution_count":null,"source":"def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if dog), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        \n        # Cost and gradient calculation (≈ 1-4 lines of code)\n        grads, cost = propagate(w, b, X, Y)\n        \n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n        \n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n        \n        # Print the cost every 100 training examples\n        if print_cost and i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"f2005cf0-5b75-4a9c-ad2a-db5acaa14c77","_uuid":"ebf6ed49736d8c0b888dfd011e5657bae6a3b86d"},"execution_count":null,"source":"def predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0], 1)\n    \n    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n    ### START CODE HERE ### (≈ 1 line of code)\n    A = sigmoid(np.dot(w.T, X) + b)\n    ### END CODE HERE ###\n    for i in range(A.shape[1]):\n        \n        # Convert probabilities A[0,i] to actual predictions p[0,i]\n        ### START CODE HERE ### (≈ 4 lines of code)\n        if A[0][i] > 0.5:\n            Y_prediction[0][i] = 1\n        ### END CODE HERE ###\n    \n    assert(Y_prediction.shape == (1, m))\n    \n    return Y_prediction"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"0fb592ce-68ef-49b9-8da9-2e6e03a9cecf","_uuid":"641d1413814f4554f2502a3d2e626b0cb07f9e33"},"execution_count":null,"source":"def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    \n    ### START CODE HERE ###\n    \n    # initialize parameters with zeros (≈ 1 line of code)\n    w, b = initialize_with_zeros(X_train.shape[0])\n\n    # Gradient descent (≈ 1 line of code)\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n    \n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n    \n    # Predict test/train set examples (≈ 2 lines of code)\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    \n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d"},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"bf6818ee-def8-43c7-995e-4cde7e10026c","_uuid":"fafb64af9bbf7ae1d61b8b6a5c38b8042f621c8e"},"execution_count":null,"source":"d = model(x_train, y_train, x_test, y_test, num_iterations = 2000, learning_rate = 0.001, print_cost = True)"}]}