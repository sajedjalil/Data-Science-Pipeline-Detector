{"cells":[{"metadata":{"_uuid":"f29c2f904c5ca9e18e7c74ade7ec00c9b2c8bb13","trusted":true,"collapsed":true},"cell_type":"code","source":"import os, cv2\nimport numpy as np\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.optimizers import SGD\nfrom keras.preprocessing.image import img_to_array, array_to_img, load_img\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\nnp.random.seed(7)\n\n# fix dimension ordering issue\n# https://stackoverflow.com/questions/39547279/loading-weights-in-th-format-when-keras-is-set-to-tf-format\nfrom keras import backend as K\n# K.set_image_dim_ordering('th')\n\n# https://www.tensorflow.org/api_docs/python/tf/keras/backend/set_image_data_format\nK.set_image_data_format('channels_last')\n# print(K.image_data_format())  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import zipfile\nwith zipfile.ZipFile('../input/dogs-vs-cats-redux-kernels-edition/train.zip',\"r\") as z:\n    z.extractall('input/dogs-vs-cats-redux-kernels-edition/')\nwith zipfile.ZipFile('../input/dogs-vs-cats-redux-kernels-edition/test.zip',\"r\") as z:\n    z.extractall('input/dogs-vs-cats-redux-kernels-edition/')    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c71ad1274cfad73de939e06ba3e2c850bc2823a","collapsed":true},"cell_type":"code","source":"img_width = 150\nimg_height = 150\nTRAIN_DIR = 'input/dogs-vs-cats-redux-kernels-edition/train/'\nTEST_DIR = '.input/dogs-vs-cats-redux-kernels-edition/test/'\ntrain_images_dogs_cats = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"149a56ed0aff532981461bdc95c73ed8fea62a12"},"cell_type":"code","source":"# Sort the traning set. Use 5000 images of cats and dogs instead of all 25000 to speed up the learning process.\n# If we sort them, the top part will be cats, bottom part will be dogs.\ntrain_images_dogs_cats.sort()\ntrain_images_dogs_cats = train_images_dogs_cats[:5000] + train_images_dogs_cats[-5000:] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bc2e691b8f9a0f4a7060353ca804bd639c934477"},"cell_type":"code","source":"# Now the images have to be represented in numbers. For this, using the openCV library read and resize the image.\n# Generate labels for the supervised learning set.\n# Below is the helper function to do so.\n\ndef prepare_data(list_of_images):\n    \"\"\"\n    Returns two arrays: \n        x is an array of resized images\n        y is an array of labels\n    \"\"\"\n    x = [] # images as arrays\n    y = [] # labels\n    \n    for image in list_of_images:\n        x.append(cv2.resize(cv2.imread(image), (img_width,img_height), interpolation=cv2.INTER_CUBIC))\n    \n    for image in list_of_images:\n        image = image.replace(\"dogs-vs-cats-redux-kernels-edition/\", \"\")\n        if 'dog' in image:\n            y.append(1)\n        elif 'cat' in image:\n            y.append(0)\n            \n    return shuffle(np.array(x), np.array(y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7d93f73578393c8095144b7ea18a5adbfc08d5c4"},"cell_type":"code","source":"X, Y = prepare_data(train_images_dogs_cats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"843aff777739bba4fd46374a59603a0532b38bc1","collapsed":true},"cell_type":"code","source":"print (X.shape)\nprint (Y.shape)\nprint (Y.sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7af80dcd3a3b93656cd937465776ec335200e5d","collapsed":true},"cell_type":"code","source":"batch_size = 16\n\nsome_entry = array_to_img(X[-1])\nsome_entry","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"4623b8168a9f77c94288a130ac54e027eb902a58","collapsed":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(img_width, img_height, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nprint (model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a71fe44b9a101fb57805f6f8fabd2db1abeddef0","collapsed":true},"cell_type":"code","source":"model.fit(X, Y, batch_size=batch_size, epochs=5, validation_split=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"901ee568434f8bef80924d40e68527ef26e4da98","collapsed":true},"cell_type":"code","source":"# https://faroit.github.io/keras-docs/1.2.2/backend/\n# For 2D data (e.g. image), \"tf\" assumes (rows, cols, channels) while \"th\" assumes (channels, rows, cols).\n\n# from keras.applications.inception_v3 import InceptionV3\n# weights = '../input/inceptionv3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n# model = InceptionV3(include_top=False, weights=weights)\n\nfrom keras.applications.vgg16 import VGG16\nweights = '../input/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\nmodel = VGG16(include_top=False, weights=weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be8ad98e245d38c1a72898c5a8c38598d641f6d0","collapsed":true},"cell_type":"code","source":"from os import makedirs\nfrom os.path import join, exists, expanduser\n\ncache_dir = expanduser(join('~', '.keras'))\nif not exists(cache_dir):\n    makedirs(cache_dir)\nmodels_dir = join(cache_dir, 'models')\nif not exists(models_dir):\n    makedirs(models_dir)\n!cp  ../input/inceptionv3/* ~/.keras/models/\n!cp  ../input/vgg16/* ~/.keras/models/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa5c4d4a01ed9320c50d053422c16efd5a001aec"},"cell_type":"markdown","source":"PART 1 - Generate the output for the Convolutional Neural Network"},{"metadata":{"trusted":true,"_uuid":"258c3302d79077b9330fc9a1a4f9247194f4e14d","collapsed":true},"cell_type":"code","source":"bottleneck_features = model.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b112e749eb9ad66e2dc0f317041ca75fc0ae9b13","collapsed":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Flatten(input_shape=bottleneck_features.shape[1:]))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n\nprint (model.summary())\n\nmodel.fit(bottleneck_features, \n          Y,\n          epochs=10,\n          batch_size=batch_size,\n          validation_split=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a36a74d55b2df1ebf77aa7d1bd48c81374f9687","collapsed":true},"cell_type":"code","source":"# Fine Tuning\nbase_model = VGG16(include_top=False, weights=weights, input_shape=(150,150,3))\n\nfull_model = Model(inputs=base_model.input, outputs=model(base_model.output))\n\n# set the first 25 layers (up to the last conv block)\n# to non-trainable (weights will not be updated)\n# for layer in full_model.layers[:25]:\n#     layer.trainable = False\n    \n# compile the model with a SGD/momentum optimizer\n# and a very slow learning rate.\nfull_model.compile(loss='binary_crossentropy',\n                   optimizer=SGD(lr=1e-4, momentum=0.9),\n                   metrics=['accuracy'])\n\nprint (full_model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42ceed9af2decbf2963deebc4a73396d6b0d76ee","collapsed":true},"cell_type":"code","source":"full_model.fit(X, \n               Y,\n               epochs=15,\n               batch_size=batch_size,\n               validation_split=0.3)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}