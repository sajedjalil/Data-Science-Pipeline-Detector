{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"288010af-4aca-99f0-6e72-e133bc3e406e"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"976687aa-0e36-f1b5-c973-d787a073f5ee"},"outputs":[],"source":"#importing necessary packages\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport tensorflow as tf\nimport tflearn\nimport tensorflow as tf\nfrom PIL import Image\n%matplotlib inline\n#for writing text files\nimport glob\nimport os     \nimport random \n#reading images from a text file\nfrom tflearn.data_utils import image_preloader\nimport math"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0e22465e-85eb-6759-f0ef-4e797dfe150c"},"outputs":[],"source":"IMAGE_FOLDER = 'C:/Users/drago/DvsG/train/train'\nTRAIN_DATA = 'C:/Users/drago/DvsG/training_data.txt'\nTEST_DATA = 'C:/Users/drago/DvsG/test_data.txt'\nVALIDATION_DATA = 'C:/Users/drago/DvsG/validation_data.txt'\ntrain_proportion=0.7\ntest_proportion=0.2\nvalidation_proportion=0.1"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3a106ca5-3c1f-7d80-4d00-86e365b65d5a"},"outputs":[],"source":"#read the image directories\nfilenames_image = os.listdir(IMAGE_FOLDER)\n#shuffling the data is important otherwise the model will be fed with a single class data for a long time and \n#network will not learn properly\nrandom.shuffle(filenames_image)\n#total number of images\ntotal=len(filenames_image)\n##  *****training data******** \nfr = open(TRAIN_DATA, 'w')\ntrain_files=filenames_image[0: int(train_proportion*total)]\nfor filename in train_files:\n    if filename[0:3] == 'cat':\n        fr.write(IMAGE_FOLDER + '/'+ filename + ' 0\\n')\n    elif filename[0:3] == 'dog':\n        fr.write(IMAGE_FOLDER + '/'+ filename + ' 1\\n')\n\nfr.close()\n##  *****testing data******** \nfr = open(TEST_DATA, 'w')\ntest_files=filenames_image[int(math.ceil(train_proportion*total)):int(math.ceil((train_proportion+test_proportion)*total))]\nfor filename in test_files:\n    if filename[0:3] == 'cat':\n        fr.write(IMAGE_FOLDER + '/'+ filename + ' 0\\n')\n    elif filename[0:3] == 'dog':\n        fr.write(IMAGE_FOLDER + '/'+ filename + ' 1\\n')\nfr.close()\n\n##  *****validation data******** \nfr = open(VALIDATION_DATA, 'w')\nvalid_files=filenames_image[int(math.ceil((train_proportion+test_proportion)*total)):total]\nfor filename in valid_files:\n    if filename[0:3] == 'cat':\n        fr.write(IMAGE_FOLDER + '/'+ filename + ' 0\\n')\n    elif filename[0:3] == 'dog':\n        fr.write(IMAGE_FOLDER + '/'+ filename + ' 1\\n')\nfr.close()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4ee7754a-aa1e-1019-28da-924b18661e65"},"outputs":[],"source":"#Importing data\n#importing data\nX_train, Y_train = image_preloader(TRAIN_DATA, image_shape=(56,56),mode='file', categorical_labels=True,normalize=True)\nX_test, Y_test = image_preloader(TEST_DATA, image_shape=(56,56),mode='file', categorical_labels=True,normalize=True)\nX_val, Y_val = image_preloader(VALIDATION_DATA, image_shape=(56,56),mode='file', categorical_labels=True,normalize=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eb1b1e0b-af7d-bc91-ed29-663223b61baf"},"outputs":[],"source":"print (\"Dataset\")\nprint (\"Number of training images {}\".format(len(X_train)))\nprint (\"Number of testing images {}\".format(len(X_test)))\nprint (\"Number of validation images {}\".format(len(X_val)))\nprint (\"Shape of an image {}\" .format(X_train[1].shape))\nprint (\"Shape of label:{} ,number of classes: {}\".format(Y_train[1].shape,len(Y_train[1])))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ebc27620-692d-411b-c992-922f17008b78"},"outputs":[],"source":"#Sample Image \nplt.imshow(X_train[1])\nplt.axis('off')\nplt.title('Sample image with label {}'.format(Y_train[1]))\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"02f9b4e3-ec0c-38ef-6a1e-007142c21b89"},"outputs":[],"source":"x=tf.placeholder(tf.float32,shape=[None,56,56,3] , name='input_image') \n#input class\ny_=tf.placeholder(tf.float32,shape=[None, 2] , name='input_class')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2d259d47-1246-ca41-0866-6d273478bc53"},"outputs":[],"source":"input_layer=x\n#convolutional layer 1 --convolution+RELU activation\nconv_layer1=tflearn.layers.conv.conv_2d(input_layer, nb_filter=64, filter_size=5, strides=[1,1,1,1],\n                                        padding='same', activation='relu', regularizer=\"L2\", name='conv_layer_1')\n\n#2x2 max pooling layer\nout_layer1=tflearn.layers.conv.max_pool_2d(conv_layer1, 2)\n\n\n#second convolutional layer \nconv_layer2=tflearn.layers.conv.conv_2d(out_layer1, nb_filter=128, filter_size=5, strides=[1,1,1,1],\n                                        padding='same', activation='relu',  regularizer=\"L2\", name='conv_layer_2')\nout_layer2=tflearn.layers.conv.max_pool_2d(conv_layer2, 2)\n# third convolutional layer\nconv_layer3=tflearn.layers.conv.conv_2d(out_layer2, nb_filter=128, filter_size=5, strides=[1,1,1,1],\n                                        padding='same', activation='relu',  regularizer=\"L2\", name='conv_layer_2')\nout_layer3=tflearn.layers.conv.max_pool_2d(conv_layer3, 2)\n\n#fully connected layer1\nfcl= tflearn.layers.core.fully_connected(out_layer3, 4096, activation='relu' , name='FCL-1')\nfcl_dropout_1 = tflearn.layers.core.dropout(fcl, 0.8)\n#fully connected layer2\nfc2= tflearn.layers.core.fully_connected(fcl_dropout_1, 4096, activation='relu' , name='FCL-2')\nfcl_dropout_2 = tflearn.layers.core.dropout(fc2, 0.8)\n#softmax layer output\ny_predicted = tflearn.layers.core.fully_connected(fcl_dropout_2, 2, activation='softmax', name='output')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"29a61792-3ea0-5980-2999-ef8c3d61addb"},"outputs":[],"source":"#loss function\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_predicted+np.exp(-10)), reduction_indices=[1]))\n#optimiser -\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n#calculating accuracy of our model \ncorrect_prediction = tf.equal(tf.argmax(y_predicted,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7236c3f7-2764-92d9-a114-c94d323552fc"},"outputs":[],"source":"# session parameters\nsess = tf.InteractiveSession()\n#initialising variables\ninit = tf.initialize_all_variables()\nsess.run(init)\nsaver = tf.train.Saver()\nsave_path=\"/Users/Enkay/Documents/Viky/python/img-classification/mark2.ckpt\""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2ab61ea3-7396-c45c-213f-87b753b6cfa8"},"outputs":[],"source":"# grabbing the default graph\ng = tf.get_default_graph()\n\n# every operations in our graph\n[op.name for op in g.get_operations()]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8363d9c7-8b22-485c-6600-609f1f1d8ec3"},"outputs":[],"source":"epoch=5000\n#change batch size according to your hardware's power. For GPU's use batch size in powers of 2 like 2,4,8,16...\nbatch_size=20 \nprevious_batch=0"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"445913b7-72d2-d46a-f7cf-760e0b31b49d"},"outputs":[],"source":"for i in range(epoch):\n    #batch wise training \n    if previous_batch >= len(X_train) : #total --> total number of training images\n        previous_batch=0    \n    current_batch=previous_batch+batch_size\n    x_input=X_train[previous_batch:current_batch]\n    x_images=np.reshape(x_input,[batch_size,56,56,3])\n    y_input=Y_train[previous_batch:current_batch]\n    y_label=np.reshape(y_input,[batch_size,2])\n    previous_batch=previous_batch+batch_size\n    _,loss=sess.run([train_step, cross_entropy], feed_dict={x: x_images,y_: y_label}) \n    if i%500==0:\n        n=50 #number of test samples\n        # increase the number of test samples with higher RAM. if you have less RAM, limit your test sample or \n        # run test accross larger samples once in every 1000 epochs or so..  \n        x_test_images=np.reshape(X_test[0:n],[n,56,56,3])\n        y_test_labels=np.reshape(Y_test[0:n],[n,2])\n        Accuracy=sess.run(accuracy,\n                           feed_dict={\n                        x: x_test_images ,\n                        y_: y_test_labels\n                      })\n        print \"Iteration no :{} , Accuracy:{} , Loss : {}\" .format(i,Accuracy,loss)\n        saver.save(sess, save_path, global_step = i)\n    elif i % 100 ==0:   \n        print \"Iteration no :{} Loss : {}\" .format(i,loss)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a1759563-b6df-93bd-312c-3704bd42872c"},"outputs":[],"source":"x_input=X_val\nx_images=np.reshape(x_input,[len(X_val),56,56,3])\ny_input=Y_val\ny_label=np.reshape(y_input,[len(Y_val),2])\n\nAccuracy_validation=sess.run(accuracy,\n                           feed_dict={\n                        x: x_images ,\n                        y_: y_label\n                      })"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9de64d1a-e367-b897-a8f5-35835449638e"},"outputs":[],"source":"Accuracy_validation=round(Accuracy_validation*100,2)\nprint \"Accuracy in the validation dataset: {} %\" .format(Accuracy_validation)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f12a943-0e3a-6adb-087e-f4b3e9e95862"},"outputs":[],"source":"def process_img(img):\n        img=img.resize((56, 56), Image.ANTIALIAS) #resize the image\n        img = np.array(img)\n        img=img/np.max(img).astype(float) \n        img=np.reshape(img, [1,56,56,3])\n        return img\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"238332f6-6ce6-abe0-a5a3-4bef9c8ee5d1"},"outputs":[],"source":"#test your own images \ntest_image=Image.open('/path to file')\ntest_image= process_img(test_image)\npredicted_array= sess.run(y_predicted, feed_dict={x: test_image})\npredicted_class= np.argmax(predicted_array)\nif predicted_class==0:\n    print \"It is a cat\"  \nelse :\n    print \"It is a dog  \""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}