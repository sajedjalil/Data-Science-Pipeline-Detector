{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Understanding the data\nBefore starting to explore the data and then trying to predict, we should first try to understand what each field means and what our end goals are. It's not the easiest this to understand what each column represents. e.g. `Latitude` and `Longitude` represent a location on the map but, what location is represented by `Latitude` and `Longitude` in the dataset.\n\n### Percentile\nAs [wikipedia](https://en.wikipedia.org/wiki/Percentile) explains A percentile (or a centile) is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations falls. For example, the 20th percentile is the value (or score) below which 20% of the observations may be found.\n\nSo, in our case TotalTimeStopped_p20 represents the value for which 20% of the vehicles had to stop. (more on this using some examples).\n\n## Data columns:\n* **RowId**: Represents a unique row ID in the dataframe.\n* **IntersectionId**: Represents a **unique intersectionID** for some intersection of roads **within a city**.\n* **Latitude**: The latitude of the intersection.\n* **Longitude**: The longitude of the intersection.\n* **EntryStreetName**: The street name from which the vehicle entered towards the intersection.\n* **ExitStreetName**: The street name to which the vehicle goes from the intersection.\n* **EntryHeading**: Direction to which the car was heading while entering the intersection.\n* **ExitHeading**: Direction to which the car went after it went through the intersection.\n* **Hour**: The hour of the day.\n* **Weekend**: It's weekend or not.\n* **Month**: Which Month it is.\n* **Path**: It is a concatination in the format: `EntryStreetName_EntryHeading ExitStreetName_ExitHeading`.\n* **TotalTimeStopped_p20**: Total time for which 20% of the vehicles had to stop at an intersection.\n* **TotalTimeStopped_p40**: Total time for which 40% of the vehicles had to stop at an intersection.\n* **TotalTimeStopped_p50**: Total time for which 50% of the vehicles had to stop at an intersection.\n* **TotalTimeStopped_p60**: Total time for which 60% of the vehicles had to stop at an intersection.\n* **TotalTimeStopped_p80**: Total time for which 80% of the vehicles had to stop at an intersection.\n* **TimeFromFirstStop_p20**: Time taken for 20% of the vehicles to stop again after crossing a intersection.\n* **TimeFromFirstStop_p40**: Time taken for 40% of the vehicles to stop again after crossing a intersection.\n* **TimeFromFirstStop_p50**: Time taken for 50% of the vehicles to stop again after crossing a intersection.\n* **TimeFromFirstStop_p60**: Time taken for 60% of the vehicles to stop again after crossing a intersection.\n* **TimeFromFirstStop_p80**: Time taken for 80% of the vehicles to stop again after crossing a intersection.\n* **DistanceToFirstStop_p20**: How far before the intersection the 20% of the vehicles stopped for the first time.\n* **DistanceToFirstStop_p40**: How far before the intersection the 40% of the vehicles stopped for the first time.\n* **DistanceToFirstStop_p50**: How far before the intersection the 50% of the vehicles stopped for the first time.\n* **DistanceToFirstStop_p60**: How far before the intersection the 60% of the vehicles stopped for the first time.\n* **DistanceToFirstStop_p80**: How far before the intersection the 80% of the vehicles stopped for the first time.\n* **City**: Name of the city"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport json\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True) \nimport pandas_profiling as pp\nimport plotly.express as px\nfrom collections import defaultdict\nimport lightgbm as lgb\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/bigquery-geotab-intersection-congestion/train.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/bigquery-geotab-intersection-congestion/test.csv')\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Intersections locations in Train Test\nFrom the below four graphs we have a lot of **different intersections** in **training** and **testing** datasets. This makes the task difficult.\n\nIf the location of intersections were the same for both train and test dataset we would have easily fitted a curve on stopping duration as a function of time for each intersection.\n\nThe most challenging part of this will be predicting the metrics on the red points in the graphs below. The green ones should be easier as that represents some kind of missing data between some points which can be easily estimated using some simple regression algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_train_test_intersections(city):\n    df_draw = df[df['City'] == city]\n    df_draw = df_draw[['Latitude','Longitude']]\n    df_draw.drop_duplicates(inplace=True)\n    df_draw1 = df_test[df_test['City'] == city]\n    df_draw1 = df_draw1[['Latitude', 'Longitude']]\n\n    df_draw1.drop_duplicates(inplace=True)\n    df_common = pd.merge(df_draw, df_draw1, how='inner')\n    df_common.drop_duplicates(inplace=True)\n    df_draw['Intersection'] = 'Train'\n    df_draw1['Intersection'] = 'Test'\n    df_common['Intersection'] = 'Common'\n    df_draw = pd.concat([df_draw, df_draw1, df_common])\n    fig = px.scatter_mapbox(df_draw, lat=\"Latitude\", lon=\"Longitude\", color=\"Intersection\", zoom=10, opacity=0.7)\n    fig.update_layout(\n        mapbox_style=\"stamen-terrain\",\n        margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0},\n    )\n    return fig","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for city in df.City.unique():\n    print(city)\n    fig = draw_train_test_intersections(city)\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inference\n* From above graphs we can see that we need to add some feature that will give importance to closer points, then the points that are further.\n* We are facing a problem which requires [extrapolation](https://en.wikipedia.org/wiki/Extrapolation) So, are there macro trends in the data. (Let's explore) "},{"metadata":{},"cell_type":"markdown","source":"# Distribution of TotalTimeStopped_p80\n* Distribution is drawn for randomly selected 10 intersections for each city.\n* As we saw above we are facing a problem which involves extrapolation we want to see if there are any macro trends which might be useful like peek hours there is traffic everywhere"},{"metadata":{"trusted":true},"cell_type":"code","source":"c_names = ['pink', 'red', 'green', 'blue', 'black', 'white', 'brown', 'aqua', 'yellow', 'purple']\nfor city in df['City'].unique():\n    r = df[df['City'] == city][['Latitude', 'Longitude']].drop_duplicates().sample(10, random_state=2)\n    r_la, r_lo = r['Latitude'].to_list(), r['Longitude'].to_list()\n    df_temp = df[df['Latitude'].isin(r_la) & df['Longitude'].isin(r_lo)]\n    temp = df_temp.groupby(['Latitude', 'Longitude', 'Hour'])['TotalTimeStopped_p80'].transform(lambda x: x.mean())\n    df_temp['agg_tts'] = temp\n    fig, axs = plt.subplots(figsize=(16,8))\n    intersection_names = []\n    for i in range(10):\n        t = df_temp[(df_temp['Latitude'] == r_la[i]) & (df_temp['Longitude'] == r_lo[i])][['Hour', 'agg_tts', 'EntryStreetName', 'ExitStreetName']].sort_values('Hour')\n        x, y = t['Hour'].to_list(), t['agg_tts'].to_list()\n        en_sn, ex_sn = t['EntryStreetName'].to_list()[0], t['ExitStreetName'].to_list()[0]\n        intersection_names.append('{0} -> {1}'.format(en_sn, ex_sn))\n        axs = sns.lineplot(x=x, y=y, ax=axs, palette=c_names)\n    axs.set_xlabel('Hour')\n    axs.set_ylabel('Total time stopped p_80')\n    axs.set_title(city, fontsize=24)\n    axs.legend(intersection_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We see a few general trends like around `00:00 hrs` there is low traffic and more traffic around peek hours (`7-10`, `16-18`) but with a lot of anomalies so we can't point out that.\n* We can see that `Avenues` have a larger traffic then streets in general let's see it on a broder scale."},{"metadata":{},"cell_type":"markdown","source":"# Street type effect on the Stopping time"},{"metadata":{"trusted":true},"cell_type":"code","source":"road_encoding = {\n    \"Street\":\"Street\",\n    \"St\":\"Street\",\n    \"Avenue\":\"Avenue\",\n    \"Ave\":\"Avenue\",\n    \"Boulevard\":\"Boulevard\",\n    \"Road\":\"Road\",\n    \"Drive\":\"Drive\",\n    \"Lane\":\"Lane\",\n    \"Tunnel\":\"Tunnel\",\n    \"Highway\":\"Highway\",\n    \"Way\":\"Way\",\n    \"Parkway\":\"Parkway\",\n    \"Parking\":\"Parking\",\n    \"Oval\":\"Oval\",\n    \"Square\":\"Square\",\n    \"Place\":\"Place\",\n    \"Bridge\":\"Bridge\",\n}\n\ndef encode(x):\n    if pd.isna(x):\n        return \"Street\"\n    for road in road_encoding.keys():\n        if road in x:\n            return road_encoding[road]\n    return \"Street\"\n\nfor city in df['City'].unique():\n    df_temp = df[df['City'] == city]\n    df_temp['StreetType'] = df_temp['EntryStreetName'].apply(encode)\n    temp = df_temp.sort_values('StreetType').groupby(by=['Hour', 'StreetType'])['TotalTimeStopped_p80'].transform(lambda x: x.mean())\n    df_temp['avg_time'] = temp\n    fig, axs = plt.subplots(figsize=(16,8))\n    axs = sns.barplot(x='StreetType', y='avg_time', data=df_temp, ax=axs, ci=None)\n    axs.set_xlabel('StreetType')\n    axs.set_ylabel('Total time stopped p_80')\n    axs.set_title(city, fontsize=24)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above graph we can see that **street type** is **not a very informative feature** as in different places different type of streets have different stopping time."},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering"},{"metadata":{},"cell_type":"markdown","source":"* From the above EDA, there is one this that we can do to make our predictions better which is to penalize points that are far.\n* To do so we introduce a feature which is the **longitude and latitude** of the **4 points nearest** to any given point in the dataset. BQ can do K-fold validation so we won't create a validation split.\n* For test data though we inset **4 nearest points** that are closest to it in the training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# lon_lat_train = df[['Longitude', 'Latitude']].drop_duplicates()\n# lon_lat_test = df_test[['Longitude', 'Latitude']].drop_duplicates()\n# lon_lat = pd.concat([lon_lat_train, lon_lat_test])\n# lon_lat.drop_duplicates(inplace=True)\n\nlon_lat = df[['Longitude', 'Latitude']].drop_duplicates()\n\nneigh = NearestNeighbors(5) # 5 because first one will be the same number itself\n\nlon_lat_array = lon_lat.to_numpy()\n\nneigh.fit(lon_lat_array)\n\nlon_lat_train = df[['Longitude', 'Latitude']].drop_duplicates()\nlon_lat_test = df_test[['Longitude', 'Latitude']].drop_duplicates()\nlon_lat = pd.concat([lon_lat_train, lon_lat_test])\nlon_lat.drop_duplicates(inplace=True)\nlon_lat_array = lon_lat.to_numpy()\n\nnearest_points = defaultdict(list)\nfor lo, la in lon_lat_array:\n    nearest_points['{}-{}'.format(lo, la)] = list(neigh.kneighbors([[lo, la]], 5, return_distance=False)[0][1:])\n\nnearest_points_val = defaultdict(list)\nfor k, pt in nearest_points.items():\n    nearest_points_val[k] = [(lon_lat_array[i][0],lon_lat_array[i][1]) for i in pt]\n\nnearest_points_df = defaultdict(list)\nfor lo, la in zip(df['Longitude'].to_list(), df['Latitude'].to_list()):\n    for i in range(4):\n        nearest_points_df[\"nplo{}\".format(i)].append(nearest_points_val[\"{}-{}\".format(lo,la)][i][0])\n        nearest_points_df[\"npla{}\".format(i)].append(nearest_points_val[\"{}-{}\".format(lo,la)][i][1])\n\nnearest_points_df = pd.DataFrame(nearest_points_df)\ndf = pd.concat([df, nearest_points_df], axis=1)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nearest_points_df = defaultdict(list)\nfor lo, la in zip(df_test['Longitude'].to_list(), df_test['Latitude'].to_list()):\n    for i in range(4):\n        nearest_points_df[\"nplo{}\".format(i)].append(nearest_points_val[\"{}-{}\".format(lo,la)][i][0])\n        nearest_points_df[\"npla{}\".format(i)].append(nearest_points_val[\"{}-{}\".format(lo,la)][i][1])\n\nnearest_points_df = pd.DataFrame(nearest_points_df)\ndf_test = pd.concat([df_test, nearest_points_df], axis=1)\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing data for the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[['Latitude', 'Longitude', 'EntryHeading', 'ExitHeading', 'Hour', 'Weekend',\n       'Month', 'TotalTimeStopped_p20', 'TotalTimeStopped_p50', 'TotalTimeStopped_p80',\n       'TimeFromFirstStop_p20', 'TimeFromFirstStop_p40',\n       'TimeFromFirstStop_p50', 'TimeFromFirstStop_p60',\n       'TimeFromFirstStop_p80', 'DistanceToFirstStop_p20',\n       'DistanceToFirstStop_p50', 'DistanceToFirstStop_p80', 'City', 'nplo0',\n       'npla0', 'nplo1', 'npla1', 'nplo2', 'npla2', 'nplo3', 'npla3']]\n\ndf_test = df_test[['Latitude', 'Longitude', 'EntryHeading', 'ExitHeading', 'Hour', 'Weekend',\n       'Month', 'City', 'nplo0',\n       'npla0', 'nplo1', 'npla1', 'nplo2', 'npla2', 'nplo3', 'npla3']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"directions = {\n    'N': 0,\n    'NE': 1/4,\n    'E': 1/2,\n    'SE': 3/4,\n    'S': 1,\n    'SW': 5/4,\n    'W': 3/2,\n    'NW': 7/4\n}\n\ndf['EntryHeading'] = df['EntryHeading'].map(directions)\ndf['ExitHeading'] = df['ExitHeading'].map(directions)\n\ndf_test['EntryHeading'] = df_test['EntryHeading'].map(directions)\ndf_test['ExitHeading'] = df_test['ExitHeading'].map(directions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metric_map = json.load(open('/kaggle/input/bigquery-geotab-intersection-congestion/submission_metric_map.json'))\nprint(metric_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in df_test.columns:\n    if column == 'City':\n        continue\n    temp = pd.concat([df[column], df_test[column]])\n    mx = float(max(temp))\n    mn = float(min(temp))\n    df[column] = df[column].apply(lambda x: (x-mn)/(mx-mn))\n    df_test[column] = df_test[column].apply(lambda x: (x-mn)/(mx-mn))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# For each city train a separate model"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['Latitude', 'Longitude', 'EntryHeading', 'ExitHeading', 'Hour', 'Weekend', 'Month', 'nplo0',\n       'npla0', 'nplo1', 'npla1', 'nplo2', 'npla2', 'nplo3', 'npla3']\n\n# Thanks to https://www.kaggle.com/ragnar123/feature-engineering-and-forward-feature-selection (Source 2)\nparam = {\n    'application': 'regression', \n    'learning_rate': 0.05, \n    'metric': 'rmse', \n    'seed': 42, \n    'bagging_fraction': 0.7, \n    'feature_fraction': 0.9, \n    'lambda_l1': 0.0, \n    'lambda_l2': 5.0, \n    'max_depth': 30, \n    'min_child_weight': 50.0, \n    'min_split_gain': 0.1, \n    'num_leaves': 230\n}\n\nnfold = 5\nall_preds = {0 : {}, 1 : {}, 2 : {}, 3 : {}, 4 : {}, 5 : {}}\n\nfor metric_id, metric in metric_map.items():\n    for city in df.City.unique():\n        train = df[df['City'] == city]\n        labels = train[metric]\n        mx = float(max(labels))\n        mn = float(min(labels))\n        labels = labels.apply(lambda x: (x-mn)/(mx-mn))\n\n        train = train[cols]\n\n        test = df_test[df_test['City'] == city]\n        test = test[cols]\n        test_idx = test.index\n\n        kf = KFold(n_splits=nfold, random_state=1111, shuffle=True)\n        print('Training and predicting for target {}, {}, {}'.format(city, metric_id, metric))\n\n        oof = np.zeros(len(train))\n        preds = np.zeros(len(test))\n\n        for train_index, valid_index in kf.split(train):\n            xg_train = lgb.Dataset(train.iloc[train_index],\n                                   label=labels.iloc[train_index]\n                                   )\n            xg_valid = lgb.Dataset(train.iloc[valid_index],\n                                   label=labels.iloc[valid_index]\n                                   )\n\n            clf = lgb.train(param, xg_train, 100000, valid_sets=[xg_train, xg_valid], \n                            verbose_eval=500, early_stopping_rounds=100)\n            oof[valid_index] = clf.predict(train.iloc[valid_index], num_iteration=clf.best_iteration) \n\n            preds += clf.predict(test, num_iteration=clf.best_iteration) / nfold\n            break\n        preds = [((mx-mn) * p) + mn for p in preds]\n        all_preds[int(metric_id)].update({idx:val for idx, val in zip(test_idx, preds)})\n        print(\"\\n\\nCV RMSE: {:<0.4f}\".format(np.sqrt(mean_squared_error(labels, oof))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_preds = {0 : [], 1 : [], 2 : [], 3 : [], 4 : [], 5 : []}\nfor metric in final_preds.keys():\n    final_preds[metric] = [all_preds[metric][i] for i in sorted(all_preds[metric].keys())]\nprint(df_test.shape, len(final_preds[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/bigquery-geotab-intersection-congestion/sample_submission.csv')\ndata2 = pd.DataFrame(final_preds).stack()\ndata2 = pd.DataFrame(data2)\nsubmission['Target'] = data2[0].values\nsubmission.to_csv('lgbm.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sources\n* [Explain TimeFromFirstStop and DistanceToFirstStop](https://www.kaggle.com/c/bigquery-geotab-intersection-congestion/discussion/110519#latest-666820)\n* [Feature Engineering and forward feature selection](https://www.kaggle.com/ragnar123/feature-engineering-and-forward-feature-selection)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}