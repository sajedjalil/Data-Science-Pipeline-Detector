{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Objective\nPast model link: https://www.kaggle.com/ragnar123/eda-feature-engineer-and-baseline-lgbm (need more work (feature engineering, feature selection etc...)\n\nThis time, we will make the same model but, we are going to use a forward feature engineering tecnique to make the feature selection part."},{"metadata":{},"cell_type":"markdown","source":"# Libraries"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_columns', 999)\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nimport math\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"print('Loading trian set...')\ntrain = pd.read_csv('/kaggle/input/bigquery-geotab-intersection-congestion/train.csv')\nprint('Loading test set...')\ntest = pd.read_csv('/kaggle/input/bigquery-geotab-intersection-congestion/test.csv')\nprint('We have {} rows and {} columns in our train set'.format(train.shape[0], train.shape[1]))\nprint('We have {} rows and {} columns in our test set'.format(test.shape[0], test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# let's select the columns that we are going to use for our training\ntrain = train[['TotalTimeStopped_p80', 'IntersectionId', 'Latitude', 'Longitude', 'EntryStreetName', \n                        'ExitStreetName', 'EntryHeading', 'ExitHeading', 'Hour', 'Weekend', \n                        'Month', 'City']]\n# let's select the target variable we are going to use for feature selection\ntarget = train['TotalTimeStopped_p80']\ntrain.drop('TotalTimeStopped_p80', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing (feature engineering)\n\nFor our preprocessing we are going to use pandas to calculate some group statistics, frequencies and others like we did in the previous notebook (link in the top of the notebook). "},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"def prepro(train, test):\n    \n    # Road Mapping\n    road_encoding = {'Street': 'Street', 'St': 'Street', 'Avenue': 'Avenue', 'Ave': 'Avenue', \n                     'Boulevard': 'Boulevard', 'Road': 'Road', 'Drive': 'Drive', 'Lane': 'Lane', \n                     'Tunnel': 'Tunnel', 'Highway': 'Highway', 'Way': 'Way', 'Parkway': 'Parkway',\n                     'Parking': 'Parking', 'Oval': 'Oval', 'Square': 'Square', 'Place': 'Place', \n                     'Bridge': 'Bridge'}\n    \n    def encode(x):\n        if pd.isna(x):\n            return 'Street'\n        for road in road_encoding.keys():\n            if road in x:\n                return road_encoding[road]\n    \n    for par in [train, test]:\n        par['EntryType'] = par['EntryStreetName'].apply(encode)\n        par['ExitType'] = par['ExitStreetName'].apply(encode)\n        par['EntryType_1'] = par['EntryStreetName'].str.split().str.get(0)\n        par['ExitType_1'] = par['ExitStreetName'].str.split().str.get(0)\n        par['EntryType_2'] = par['EntryStreetName'].str.split().str.get(1)\n        par['ExitType_2'] = par['ExitStreetName'].str.split().str.get(1)\n        par.loc[par['EntryType_1'].isin(par['EntryType_1'].value_counts()[par['EntryType_1'].value_counts()<=500].index), 'EntryType_1'] = 'Other'\n        par.loc[par['ExitType_1'].isin(par['ExitType_1'].value_counts()[par['ExitType_1'].value_counts()<=500].index), 'ExitType_1'] = 'Other'\n        par.loc[par['EntryType_2'].isin(par['EntryType_2'].value_counts()[par['EntryType_2'].value_counts()<=500].index), 'EntryType_2'] = 'Other'\n        par.loc[par['ExitType_2'].isin(par['ExitType_2'].value_counts()[par['ExitType_2'].value_counts()<=500].index), 'ExitType_2'] = 'Other'\n        par['EntryType_1'].fillna('Other', inplace = True)\n        par['ExitType_1'].fillna('Other', inplace = True)\n        par['EntryType_2'].fillna('Other', inplace = True)\n        par['ExitType_2'].fillna('Other', inplace = True)\n        \n        \n    # The cardinal directions can be expressed using the equation: θ/π\n    # Where  θ  is the angle between the direction we want to encode and the north compass direction, measured clockwise.\n    directions = {'N': 0, 'NE': 1/4, 'E': 1/2, 'SE': 3/4, 'S': 1, 'SW': 5/4, 'W': 3/2, 'NW': 7/4}\n    for par in [train, test]:\n        par['EntryHeading'] = par['EntryHeading'].map(directions)\n        par['ExitHeading'] = par['ExitHeading'].map(directions)\n        \n    # EntryStreetName == ExitStreetName ?\n    # EntryHeading == ExitHeading ?\n    for par in [train, test]:\n        par[\"same_street_exact\"] = (par[\"EntryStreetName\"] ==  par[\"ExitStreetName\"]).astype(int)\n        par[\"same_heading_exact\"] = (par[\"EntryHeading\"] ==  par[\"ExitHeading\"]).astype(int)\n        \n    # We have some intersection id that are in more than one city, it is a good idea to feature cross them\n    for par in [train, test]:\n        par['Intersection'] = par['IntersectionId'].astype(str) + '_' + par['City'].astype(str)\n        \n    for par in [train, test]:\n        # Concatenating the city and month into one variable\n        par['city_month'] = par[\"City\"].astype(str) + par[\"Month\"].astype(str)\n        \n    # Add climate data\n    monthly_av = {'Atlanta1': 43, 'Atlanta5': 69, 'Atlanta6': 76, 'Atlanta7': 79, 'Atlanta8': 78, \n                  'Atlanta9': 73, 'Atlanta10': 62, 'Atlanta11': 53, 'Atlanta12': 45, 'Boston1': 30, \n                  'Boston5': 59, 'Boston6': 68, 'Boston7': 74, 'Boston8': 73, 'Boston9': 66, \n                  'Boston10': 55,'Boston11': 45, 'Boston12': 35, 'Chicago1': 27, 'Chicago5': 60, \n                  'Chicago6': 70, 'Chicago7': 76, 'Chicago8': 76, 'Chicago9': 68, \n                  'Chicago10': 56,  'Chicago11': 45, 'Chicago12': 32, 'Philadelphia1': 35, \n                  'Philadelphia5': 66, 'Philadelphia6': 76, 'Philadelphia7': 81, \n                  'Philadelphia8': 79, 'Philadelphia9': 72, 'Philadelphia10': 60, \n                  'Philadelphia11': 49, 'Philadelphia12': 40}\n  \n    monthly_rainfall = {'Atlanta1': 5.02, 'Atlanta5': 3.95, 'Atlanta6': 3.63, 'Atlanta7': 5.12, \n                        'Atlanta8': 3.67, 'Atlanta9': 4.09, 'Atlanta10': 3.11, 'Atlanta11': 4.10, \n                        'Atlanta12': 3.82, 'Boston1': 3.92, 'Boston5': 3.24, 'Boston6': 3.22, \n                        'Boston7': 3.06, 'Boston8': 3.37, 'Boston9': 3.47, 'Boston10': 3.79, \n                        'Boston11': 3.98, 'Boston12': 3.73, 'Chicago1': 1.75, 'Chicago5': 3.38, \n                        'Chicago6': 3.63, 'Chicago7': 3.51, 'Chicago8': 4.62, 'Chicago9': 3.27, \n                        'Chicago10': 2.71,  'Chicago11': 3.01, 'Chicago12': 2.43, \n                        'Philadelphia1': 3.52, 'Philadelphia5': 3.88, 'Philadelphia6': 3.29,\n                        'Philadelphia7': 4.39, 'Philadelphia8': 3.82, 'Philadelphia9':3.88 , \n                        'Philadelphia10': 2.75, 'Philadelphia11': 3.16, 'Philadelphia12': 3.31}\n\n    monthly_snowfall = {'Atlanta1': 0.6, 'Atlanta5': 0, 'Atlanta6': 0, 'Atlanta7': 0, \n                        'Atlanta8': 0, 'Atlanta9': 0, 'Atlanta10': 0, 'Atlanta11': 0, \n                        'Atlanta12': 0.2, 'Boston1': 12.9, 'Boston5': 0, 'Boston6': 0, \n                        'Boston7': 0, 'Boston8': 0, 'Boston9': 0, 'Boston10': 0, 'Boston11': 1.3, \n                        'Boston12': 9.0, 'Chicago1': 11.5, 'Chicago5': 0, 'Chicago6': 0, \n                        'Chicago7': 0, 'Chicago8': 0, 'Chicago9': 0, 'Chicago10': 0, \n                        'Chicago11': 1.3, 'Chicago12': 8.7, 'Philadelphia1': 6.5, \n                        'Philadelphia5': 0, 'Philadelphia6': 0, 'Philadelphia7': 0, \n                        'Philadelphia8': 0, 'Philadelphia9':0 , 'Philadelphia10': 0, \n                        'Philadelphia11': 0.3, 'Philadelphia12': 3.4}\n\n    monthly_daylight = {'Atlanta1': 10, 'Atlanta5': 14, 'Atlanta6': 14, 'Atlanta7': 14, \n                        'Atlanta8': 13, 'Atlanta9': 12, 'Atlanta10': 11, 'Atlanta11': 10, \n                        'Atlanta12': 10, 'Boston1': 9, 'Boston5': 15, 'Boston6': 15, \n                        'Boston7': 15, 'Boston8': 14, 'Boston9': 12, 'Boston10': 11, \n                        'Boston11': 10, 'Boston12': 9, 'Chicago1': 10, 'Chicago5': 15, \n                        'Chicago6': 15, 'Chicago7': 15, 'Chicago8': 14, 'Chicago9': 12,  \n                        'Chicago10': 11,  'Chicago11': 10, 'Chicago12': 9, 'Philadelphia1': 10, \n                        'Philadelphia5': 14, 'Philadelphia6': 15, 'Philadelphia7': 15, \n                        'Philadelphia8': 14, 'Philadelphia9':12 , 'Philadelphia10': 11, \n                        'Philadelphia11': 10, 'Philadelphia12': 9}\n\n    monthly_sunshine = {'Atlanta1': 5.3, 'Atlanta5': 9.3, 'Atlanta6': 9.5, 'Atlanta7': 8.8, 'Atlanta8': 8.3, 'Atlanta9': 7.6, \n                        'Atlanta10': 7.7, 'Atlanta11': 6.2, 'Atlanta12': 5.3, 'Boston1': 5.3, 'Boston5': 8.6, 'Boston6': 9.6, \n                        'Boston7': 9.7, 'Boston8': 8.9, 'Boston9': 7.9, 'Boston10': 6.7,'Boston11': 4.8, 'Boston12': 4.6, \n                        'Chicago1': 4.4, 'Chicago5': 9.1, 'Chicago6': 10.4, 'Chicago7': 10.3, 'Chicago8': 9.1, 'Chicago9': 7.6, \n                        'Chicago10': 6.2,  'Chicago11': 3.6, 'Chicago12': 3.4, 'Philadelphia1': 5.0, 'Philadelphia5': 7.9, \n                        'Philadelphia6': 9.0, 'Philadelphia7': 8.9, 'Philadelphia8': 8.4, 'Philadelphia9':7.9 , \n                        'Philadelphia10': 6.6,  'Philadelphia11': 5.2, 'Philadelphia12': 4.4}\n        \n        \n    for par in [train, test]:\n        # Creating a new column by mapping the city_month variable to it's corresponding average monthly temperature\n        par[\"average_temp\"] = par['city_month'].map(monthly_av)\n        # Creating a new column by mapping the city_month variable to it's corresponding average monthly rainfall\n        par[\"average_rainfall\"] = par['city_month'].map(monthly_rainfall)\n        # Creating a new column by mapping the city_month variable to it's corresponding average monthly snowfall\n        par['average_snowfall'] = par['city_month'].map(monthly_snowfall)\n        # Creating a new column by mapping the city_month variable to it's corresponding average monthly daylight\n        par[\"average_daylight\"] = par['city_month'].map(monthly_daylight)\n        # Creating a new column by mapping the city_month variable to it's corresponding average monthly sunshine\n        par[\"average_sunshine\"] = par['city_month'].map(monthly_sunshine)\n    \n    for par in [train, test]:\n        # drop city month\n        par.drop('city_month', axis=1, inplace=True)\n        # Add feature is day\n        par['is_day'] = par['Hour'].apply(lambda x: 1 if 5 < x < 20 else 0)\n        \n        # distance from the center of the city\n    def add_distance(df):\n        df_center = pd.DataFrame({\"Atlanta\":[33.753746, -84.386330], \n                                  \"Boston\":[42.361145, -71.057083], \n                                  \"Chicago\":[41.881832, -87.623177], \n                                  \"Philadelphia\":[39.952583, -75.165222]})\n        df[\"CenterDistance\"] = df.apply(lambda row: math.sqrt((df_center[row.City][0] - row.Latitude) ** 2 +\n                                                              (df_center[row.City][1] - row.Longitude) ** 2) , axis=1)\n    add_distance(train)\n    add_distance(test)\n        \n    # frequency encode\n    def encode_FE(df1, df2, cols):\n        for col in cols:\n            df = pd.concat([df1[col],df2[col]])\n            vc = df.value_counts(dropna=True, normalize=True).to_dict()\n            nm = col+'_FE'\n            df1[nm] = df1[col].map(vc)\n            df1[nm] = df1[nm].astype('float32')\n            df2[nm] = df2[col].map(vc)\n            df2[nm] = df2[nm].astype('float32')\n            print(nm,', ',end='')\n            \n    # combine features\n    def encode_CB(col1, col2 , df1 = train, df2 = test):\n        nm = col1+'_'+col2\n        df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n        df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n        print(nm,', ',end='')\n\n    # group aggregations nunique\n    def encode_AG2(main_columns, agg_col, train_df = train, test_df = test):\n        for main_column in main_columns:  \n            for col in agg_col:\n                comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n                mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n                train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n                test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n                print(col+'_'+main_column+'_ct, ',end='')\n\n    def encode_AG(main_columns, agg_col, aggregations=['mean'], train_df = train, test_df = test, fillna=True, usena=False):\n        # aggregation of main agg_cols\n        for main_column in main_columns:  \n            for col in agg_col:\n                for agg_type in aggregations:\n                    new_col_name = main_column+'_'+col+'_'+agg_type\n                    temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n                    if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n                    temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n                                                            columns={agg_type: new_col_name})\n\n                    temp_df.index = list(temp_df[col])\n                    temp_df = temp_df[new_col_name].to_dict()   \n\n                    train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n                    test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n\n                    if fillna:\n                        train_df[new_col_name].fillna(-1,inplace=True)\n                        test_df[new_col_name].fillna(-1,inplace=True)\n\n                    print(\"'\"+new_col_name+\"'\",', ',end='')\n                    \n    # Frequency encode \n    encode_FE(train, test, ['Hour', 'Month', 'EntryType', 'ExitType', 'EntryType_1', 'EntryType_2', 'ExitType_1', 'ExitType_2', 'Intersection', 'City'])\n\n    # Agreggations of main columns\n    encode_AG(['Longitude', 'Latitude', 'CenterDistance', 'EntryHeading', 'ExitHeading'], ['Hour', 'Weekend', 'Month', 'Intersection'], ['mean', 'std'])\n    \n    # bucketize lat and lon\n    temp_df = pd.concat([train[['Latitude', 'Longitude']], test[['Latitude', 'Longitude']]]).reset_index(drop = True)\n    temp_df['Latitude_B'] = pd.cut(temp_df['Latitude'], 30)\n    temp_df['Longitude_B'] = pd.cut(temp_df['Longitude'], 30)\n\n    # feature cross lat and lon\n    temp_df['Latitude_B_Longitude_B'] = temp_df['Latitude_B'].astype(str) + '_' + temp_df['Longitude_B'].astype(str)\n    train['Latitude_B'] = temp_df.loc[:(train.shape[0]), 'Latitude_B']\n    test['Latitude_B'] = temp_df.loc[(train.shape[0]):, 'Latitude_B']\n    train['Longitude_B'] = temp_df.loc[:(train.shape[0]), 'Longitude_B']\n    test['Longitude_B'] = temp_df.loc[(train.shape[0]):, 'Longitude_B']\n    train['Latitude_B_Longitude_B'] = temp_df.loc[:(train.shape[0]), 'Latitude_B_Longitude_B']\n    test['Latitude_B_Longitude_B'] = temp_df.loc[(train.shape[0]):, 'Latitude_B_Longitude_B']\n    \n    # feature crosses\n    encode_CB('Hour', 'Month')\n    \n    \n    # group aggregations nunique \n    encode_AG2(['Intersection', 'Latitude_B_Longitude_B'], ['Hour', 'Month'])\n\n    \n    # label encode\n    for i,f in enumerate(train.columns):\n        if (np.str(train[f].dtype)=='category')|(train[f].dtype=='object'): \n            df_comb = pd.concat([train[f],test[f]],axis=0)\n            df_comb,_ = df_comb.factorize(sort=True)\n            if df_comb.max()>32000: print(f,'needs int32')\n            train[f] = df_comb[:len(train)].astype('int16')\n            test[f] = df_comb[len(train):].astype('int16')\n            \n    print('After preprocessing we have {} columns'.format(train.shape[1]))\n            \n            \n            \nprepro(train, test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"Let's iterate to see if the feature increases our model score or not. This technique should be donde with our validation strategy (in this case 5 KFold). Nevertheless it's done with one train and eval partition to optimize time. I comment this part because it takes a long time."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# param = {'max_depth': 20,\n#          'learning_rate': 0.1,\n#          'objective': 'regression',\n#          'boosting_type': 'gbdt',\n#          'verbose': 1,\n#          'metric': 'rmse',\n#          'seed': 42,\n#          'n_jobs': 12}\n\n# def run_lgb(train, target):\n    \n#     original_columns = ['IntersectionId', 'Latitude', 'Longitude', 'EntryStreetName','ExitStreetName', 'EntryHeading', \n#                         'ExitHeading', 'Hour', 'Weekend', 'Month', 'City', 'EntryType', 'ExitType']\n#     train_columns = list(train.columns[13:])\n#     usefull_columns = []\n#     not_usefull_columns = []\n#     best_score = 0\n    \n#     train_tmp = train[original_columns]\n#     print('Training with {} features'.format(train_tmp.shape[1]))\n#     x_train, x_val, y_train, y_val = train_test_split(train_tmp, target, test_size = 0.2, random_state = 42)\n#     xg_train = lgb.Dataset(x_train, label = y_train)\n#     xg_valid = lgb.Dataset(x_val, label= y_val)\n#     clf = lgb.train(param, xg_train, 100000, valid_sets = [xg_train, xg_valid], verbose_eval = 3000, \n#                     early_stopping_rounds = 100)\n#     predictions = clf.predict(x_val)\n#     rmse_score = np.sqrt(mean_squared_error(y_val, predictions))\n#     print(\"RMSE baseline val score: \", rmse_score)\n#     best_score = rmse_score\n    \n#     for num, i in enumerate(train_columns):\n#         train_tmp = train[original_columns + usefull_columns + [i]]\n#         print('Training with {} features'.format(train_tmp.shape[1]))\n#         x_train, x_val, y_train, y_val = train_test_split(train_tmp, target, test_size = 0.2, random_state = 42)\n#         xg_train = lgb.Dataset(x_train, label = y_train)\n#         xg_valid = lgb.Dataset(x_val, label= y_val)   \n\n#         clf = lgb.train(param, xg_train, 100000, valid_sets = [xg_train, xg_valid], verbose_eval = 3000, \n#                         early_stopping_rounds = 100)\n#         predictions = clf.predict(x_val)\n#         rmse_score = np.sqrt(mean_squared_error(y_val, predictions))\n#         print(\"RMSE val score: \", rmse_score)\n        \n#         if rmse_score < best_score:\n#             print('Column {} is usefull'.format(i))\n#             best_score = rmse_score\n#             usefull_columns.append(i)\n#         else:\n#             print('Column {} is not usefull'.format(i))\n#             not_usefull_columns.append(i)\n            \n#         print('Best rmse score for iteration {} is {}'.format(num + 1, best_score))\n        \n#     return usefull_columns, not_usefull_columns\n            \n# usefull_columns, not_usefull_columns = run_lgb(train, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"usefull_columns = ['same_heading_exact', 'Intersection', 'is_day', 'CenterDistance', 'Intersection_FE', 'Longitude_Weekend_mean', 'CenterDistance_Month_std'] # this columns were picked with forward feature selection (run previous cell)\nfinal_features = usefull_columns + ['IntersectionId', 'Latitude', 'Longitude', 'EntryStreetName','ExitStreetName', \n                                    'EntryHeading', 'ExitHeading', 'Hour', 'Weekend', 'Month', 'City', 'EntryType', 'ExitType']\nprint('Our usefull features found with forward feature selection are {}'.format(final_features))\n\nprint('-'*50)\nprint('We have selected {} features'.format(len(final_features)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we have 20 usefull features. Let's train a model with all the features and compare it so we can be sure we are correct. Our best validation rmse score is 15.72301. Let's check the score for the full training set. Comment this part to optimize time."},{"metadata":{"trusted":false},"cell_type":"code","source":"# def run_lgb_(train, target):\n#     print('Training with {} features'.format(train.shape[1]))\n#     x_train, x_val, y_train, y_val = train_test_split(train, target, test_size = 0.2, random_state = 42)\n#     xg_train = lgb.Dataset(x_train, label = y_train)\n#     xg_valid = lgb.Dataset(x_val, label= y_val)\n#     clf = lgb.train(param, xg_train, 100000, valid_sets = [xg_train, xg_valid], verbose_eval = 3000, \n#                     early_stopping_rounds = 100)\n#     predictions = clf.predict(x_val)\n#     rmse_score = np.sqrt(mean_squared_error(y_val, predictions))\n#     print(\"RMSE baseline val score: \", rmse_score)\n    \n# run_lgb_(train, target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great, our technique works!!."},{"metadata":{},"cell_type":"markdown","source":"Let's load the train dataset again so we can get the other target variables. "},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"train_target = pd.read_csv('/kaggle/input/bigquery-geotab-intersection-congestion/train.csv')\ntarget1 = train_target['TotalTimeStopped_p20']\ntarget2 = train_target['TotalTimeStopped_p50']\ntarget3 = train_target['TotalTimeStopped_p80']\ntarget4 = train_target['DistanceToFirstStop_p20']\ntarget5 = train_target['DistanceToFirstStop_p50']\ntarget6 = train_target['DistanceToFirstStop_p80']\n\n\ndef run_lgb_f(train, test):\n    # get prediction dictonary were we are going to store predictions\n    all_preds = {0 : [], 1 : [], 2 : [], 3 : [], 4 : [], 5 : []}\n    # get a list with all the target variables\n    all_target = [target1, target2, target3, target4, target5, target6]\n    nfold = 5\n    kf = KFold(n_splits=nfold, random_state=228, shuffle=True)\n    for i in range(len(all_preds)):\n        print('Training and predicting for target {}'.format(i+1))\n        oof = np.zeros(len(train))\n        all_preds[i] = np.zeros(len(test))\n        n = 1\n        for train_index, valid_index in kf.split(all_target[i]):\n            print(\"fold {}\".format(n))\n            xg_train = lgb.Dataset(train.iloc[train_index],\n                                   label=all_target[i][train_index]\n                                   )\n            xg_valid = lgb.Dataset(train.iloc[valid_index],\n                                   label=all_target[i][valid_index]\n                                   )   \n\n            clf = lgb.train(param, xg_train, 100000, valid_sets=[xg_train, xg_valid], \n                            verbose_eval=500, early_stopping_rounds=100)\n            oof[valid_index] = clf.predict(train.iloc[valid_index], num_iteration=clf.best_iteration) \n\n            all_preds[i] += clf.predict(test, num_iteration=clf.best_iteration) / nfold\n            n = n + 1\n\n        print(\"\\n\\nCV RMSE: {:<0.4f}\".format(np.sqrt(mean_squared_error(all_target[i], oof))))\n    return all_preds\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also comment next cell to optimize time"},{"metadata":{"trusted":true},"cell_type":"code","source":"# all_preds = run_lgb_f(train[final_features], test[final_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# submission = pd.read_csv('/kaggle/input/bigquery-geotab-intersection-congestion/sample_submission.csv')\n# data2 = pd.DataFrame(all_preds).stack()\n# data2 = pd.DataFrame(data2)\n# submission['Target'] = data2[0].values\n# submission.to_csv('lgbm_baseline_fs.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We scored 64.205 public leader board, much better than our baseline model. On the other hand it train faster now (only with 20 features)."},{"metadata":{},"cell_type":"markdown","source":"# Bayesian Optimization\n\nLet's see how much we can improve with bayesian optimization. Im going to use DistanceToFirstStop_p80 because is the target variable with more variaton (in other words the target variable that makes more difference)\n\nI should have made forward feature elimination with this variable (ups, too late :)). Also comment this part to optimize time"},{"metadata":{"trusted":false},"cell_type":"code","source":"# from bayes_opt import BayesianOptimization\n\n\n\n# # Objective Function\n# def hyp_lgbm(num_leaves, feature_fraction, bagging_fraction, max_depth, min_split_gain, min_child_weight, lambda_l1, lambda_l2):\n      \n#         params = {'application':'regression',\n#                   'learning_rate':0.05,\n#                   'metric':'rmse', \n#                   'seed': 42,\n#                   'n_jobs': 12}\n#         params[\"num_leaves\"] = int(round(num_leaves))\n#         params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n#         params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n#         params['max_depth'] = int(round(max_depth))\n#         params['min_split_gain'] = min_split_gain\n#         params['min_child_weight'] = min_child_weight\n#         params['lambda_l1'] = lambda_l1\n#         params['lambda_l2'] = lambda_l2\n        \n#         oof = np.zeros(len(train))\n#         nfold = 5\n#         kf = KFold(n_splits = nfold, random_state = 228, shuffle = True)\n        \n#         for train_index, valid_index in kf.split(train):\n#             xg_train = lgb.Dataset(train[final_features].iloc[train_index], label = target6[train_index])\n#             xg_valid = lgb.Dataset(train[final_features].iloc[valid_index], label = target6[valid_index])   \n\n#             clf = lgb.train(params, xg_train, 100000, valid_sets = [xg_train, xg_valid], \n#                             verbose_eval = 2500, early_stopping_rounds = 100)\n            \n#             oof[valid_index] = clf.predict(train[final_features].iloc[valid_index], num_iteration = clf.best_iteration) \n        \n#         oof_score = np.sqrt(mean_squared_error(target6, oof))\n#         print(\"\\n\\nCV RMSE: {:<0.4f}\".format(oof_score))\n        \n        \n#         return -oof_score\n\n# pds = {'num_leaves': (30, 230),\n#        'feature_fraction': (0.3, 0.9),\n#        'bagging_fraction': (0.7, 1),\n#        'lambda_l1': (0,3),\n#        'lambda_l2': (0,5),\n#        'max_depth': (8, 30),\n#        'min_split_gain': (0.001, 0.1),\n#        'min_child_weight': (1, 50)}\n\n\n# optimizer = BayesianOptimization(hyp_lgbm, pds, random_state = 7)\n# optimizer.maximize(init_points = 6, n_iter = 20)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"# print('This were our optimal hyperparammeters found with bayesian optimization: ', optimizer.max['params'])\n# param = optimizer.max['params']\n# param['num_leaves'] = int(param['num_leaves'])\n# param['max_depth'] = int(param['max_depth'])\n# param['application'] = 'regression'\n# param['learning_rate'] = 0.05\n# param['metric'] = 'rmse'\n# param['seed'] = 42\n# param['n_jobs'] = 12\n\nparam = {'application': 'regression', \n         'learning_rate': 0.05, \n         'metric': 'rmse', \n         'seed': 42, \n         'bagging_fraction': 0.7, \n         'feature_fraction': 0.9, \n         'lambda_l1': 0.0, \n         'lambda_l2': 5.0, \n         'max_depth': 30, \n         'min_child_weight': 50.0, \n         'min_split_gain': 0.1, \n         'num_leaves': 230}\n\nall_preds = run_lgb_f(train[final_features], test[final_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/bigquery-geotab-intersection-congestion/sample_submission.csv')\ndata2 = pd.DataFrame(all_preds).stack()\ndata2 = pd.DataFrame(data2)\nsubmission['Target'] = data2[0].values\nsubmission.to_csv('lgbm_baseline_fs_bopt.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comments\n\n* We can still make a better model trying more features, apply feature selection and make a longer bayesian optimization. Also we are encourage to use other source of information, i think this is the key for this playground.\n\n* Onother step that can improve final score is to make another pipeline with another model, for example xgboost and then blend it with this model"},{"metadata":{},"cell_type":"markdown","source":"This is the end of our baseline model, hope this pipeline and techniques help others, Cheers."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}