{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Table of contents\n1. [Input features](#inputs)\n   1. [Augmentations](#augmentations)\n2. [Rewards](#rewards)\n3. [Network architecture](#architecture)\n4. [PPO algorithm explained](#ppo)\n5. [Final touches](#final_touches)\n6. [Training](#training)\n7. [Save and show](#save_and_show)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install kaggle_environments==1.7.10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# basic imports\nimport numpy as np\nimport itertools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom random import shuffle\nfrom copy import deepcopy\nfrom tqdm.notebook import tqdm\n\n# torch imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom torch.distributions import Categorical\nfrom torch.optim import Adam\n\n# hungry-geese imports\nfrom kaggle_environments import make, evaluate\n\nenv = make(\"hungry_geese\", debug=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Input features <a name=\"inputs\"></a>\n\nInput state can be generated in very different ways. Here we are going to use the following procedure:\n\nInputs have 12 channels, each one is a 7 x 11 (rows x columns) matrix.\n\n* First channel consists of ones in cells where agent is located and zeros elsewhere.\n* Next three channels are the same but with opponents coordinates (players randomly ordered).\n* In the fourth channel there are ones in cells corresponding to all agents heads.\n* In the fifth channel - the same, but with agents bodies.\n* In the sixth channel - the same, but with agents tails.\n* In the seventh channel - the same, but with agents heads previous locations.\n* In the eight channel - ones in food cells.\n* The last 3 channels each one represent only one number, broadcasted to the whole matrix:\n  * (t % 40) / 40\n  * t / 200\n  * 0 if (t + 1) is divisible by 40 and 1 otherwise,\n  \n  where t is observation step.\n  \nEach channel is rolled for agents head to be located in the center of the matrix.","metadata":{}},{"cell_type":"code","source":"def get_position_from_index(index, columns):\n    row = index // columns\n    col = index % columns\n    return row, col\n\ndef get_index_from_position(row, col, columns):\n    return row * columns + col\n\ndef find_new_head_position(head_row, head_col, action, rows, columns):\n    if action == 0: # north\n        new_row, new_col = (head_row + rows - 1) % rows, head_col\n    elif action == 1: # east\n        new_row, new_col = head_row, (head_col + 1) % columns\n    elif action == 2: # south\n        new_row, new_col = (head_row + 1) % rows, head_col\n    else: # west\n        new_row, new_col = head_row, (head_col + columns - 1) % columns\n    return new_row, new_col\n\ndef shift_head(head_id, action, rows=7, columns=11):\n    head_row, head_col = get_position_from_index(head_id, columns)\n    new_row, new_col = find_new_head_position(head_row, head_col, action, rows, columns)\n    new_head_id = get_index_from_position(new_row, new_col, columns)\n    return new_head_id\n\ndef get_previous_head(ids, last_action, rows, columns):\n    if len(ids) > 1:\n        return ids[1]\n    return shift_head(ids[0], (last_action + 2) % 4, rows, columns)\n\ndef ids2locations(ids, prev_head, step, rows, columns):\n    state = np.zeros((4, rows * columns))\n    if len(ids) == 0:\n        return state\n    state[0, ids[0]] = 1 # goose head\n    if len(ids) > 1:\n        state[1, ids[1:-1]] = 1 # goose body\n        state[2, ids[-1]] = 1 # goose tail\n    if step != 0:\n        state[3, prev_head] = 1 # goose head one step before\n    return state\n\ndef get_features(observation, config, prev_heads):\n    rows, columns = config['rows'], config['columns']\n    geese = observation['geese']\n    index = observation['index']\n    step = observation['step']\n    # convert indices to locations\n    locations = np.zeros((len(geese), 4, rows * columns))\n    for i, g in enumerate(geese):\n        locations[i] = ids2locations(g, prev_heads[i], step, rows, columns)\n    if index != 0: # swap rows for player locations to be in first channel\n        locations[[0, index]] = locations[[index, 0]]\n    # put locations into features\n    features = np.zeros((12, rows * columns))\n    for k in range(4):\n        features[k] = np.sum(locations[k][:3], 0)\n        features[k + 4] = np.sum(locations[:, k], 0)\n    features[-4, observation['food']] = 1 # food channel\n    features[-3, :] = (step % config['hunger_rate']) / config['hunger_rate'] # hunger danger channel\n    features[-2, :] = step / config['episodeSteps'] # timesteps channel\n    features[-1, :] = float((step + 1) % config['hunger_rate'] == 0) # hunger milestone indicator\n    features = torch.Tensor(features).reshape(-1, rows, columns)\n    # roll\n    head_id = geese[index][0]\n    head_row = head_id // columns\n    head_col = head_id % columns\n    features = torch.roll(features, ((rows // 2) - head_row, (columns // 2) - head_col), dims=(-2, -1))\n    return features","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"An example of input state:","metadata":{}},{"cell_type":"code","source":"def plot_features(features):\n    fig, axs = plt.subplots(3, 4, figsize=(20, 10))\n    for i in range(3):\n        for j in range(4):\n            sns.heatmap(features[i * 4 + j], ax=axs[i, j], cmap='Blues',\n                        vmin=0, vmax=1, linewidth=2, linecolor='black', cbar=False)\n\ndef get_example_features():\n    observation = {}\n    observation['step'] = 104\n    observation['index'] = 0\n    observation['geese'] = [[46, 47, 36, 37, 48, 59, 58, 69],\n                            [5, 71, 72, 6, 7, 73, 62, 61, 50, 51, 52, 63, 64, 53, 54],\n                            [12, 11, 21, 20, 19, 8, 74, 75, 76, 65, 55, 56, 67, 1],\n                            [23, 22, 32, 31, 30, 29, 28, 17, 16, 27, 26, 15, 14, 13, 24]]\n    observation['food'] = [45, 66]\n    prev_heads = [47, 71, 11, 22]\n    return get_features(observation, env.configuration, prev_heads)\n\nfeatures = get_example_features()\nplot_features(features.cpu().detach().numpy())","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Augmentations <a name=\"rewards\"></a>\n\nEach state will go through the following augmentations:\n\n* Random horizontal flip\n* Random vertical flip\n* Random opponents indices shuffle","metadata":{}},{"cell_type":"code","source":"def augment(batch):\n    # random horizontal flip\n    flip_mask = np.random.rand(len(batch['states'])) < 0.5\n    batch['states'][flip_mask] = batch['states'][flip_mask].flip(-1)\n    batch['actions'][flip_mask] = torch.where(batch['actions'][flip_mask] > 0, 4 - batch['actions'][flip_mask], 0) # 1 -> 3, 3 -> 1\n\n    # random vertical flip (and also diagonal)\n    flip_mask = np.random.rand(len(batch['states'])) < 0.5\n    batch['states'][flip_mask] = batch['states'][flip_mask].flip(-2)\n    batch['actions'][flip_mask] = torch.where(batch['actions'][flip_mask] < 3, 2 - batch['actions'][flip_mask], 3) # 0 -> 2, 2 -> 0\n\n    # shuffle opponents channels\n    permuted_axs = list(itertools.permutations([0, 1, 2]))\n    permutations = [torch.tensor(permuted_axs[i]) for i in np.random.randint(6, size=len(batch['states']))]\n    for i, p in enumerate(permutations):\n        shuffled_channels = torch.zeros(3, batch['states'].shape[2], batch['states'].shape[3])\n        shuffled_channels[p] = batch['states'][i, 1:4]\n        batch['states'][i, 1:4] = shuffled_channels\n    return batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Rewards <a name=\"rewards\"></a>\n\nWe will use two types of rewards. First reward $R_1$ is given to agent in the end of the game: -1 for getting the 4th place, -0.75 for the 3rd, -0.25 for 2nd and +1 for reaching the 1st place. Second reward $R_2$ is related to food: agent gets +0.1 for eating and -1 if it died from hunger.","metadata":{}},{"cell_type":"code","source":"def get_rank(obs, prev_obs):\n    geese = obs['geese']\n    index = obs['index']\n    player_len = len(geese[index])\n    survivors = [i for i in range(len(geese)) if len(geese[i]) > 0]\n    if index in survivors: # if our player survived in the end, its rank is given by its length in the last state\n        return sum(len(x) >= player_len for x in geese) # 1 is the best, 4 is the worst\n    # if our player is dead, consider lengths in penultimate state\n    geese = prev_obs['geese']\n    index = prev_obs['index']\n    player_len = len(geese[index])\n    rank_among_lost = sum(len(x) >= player_len for i, x in enumerate(geese) if i not in survivors)\n    return rank_among_lost + len(survivors)\n    \ndef get_rewards(env_reward, obs, prev_obs, done):\n    geese = prev_obs['geese']\n    index = prev_obs['index']\n    step  = prev_obs['step']\n    if done:\n        rank = get_rank(obs, prev_obs)\n        r1 = (1, -0.25, -0.75, -1)[rank - 1]\n        died_from_hunger = ((step + 1) % 40 == 0) and (len(geese[index]) == 1)\n        r2 = -1 if died_from_hunger else 0 # int(rank == 1) # huge penalty for dying from hunger and huge award for the win\n    else:\n        if step == 0:\n            env_reward -= 1 # somehow initial step is a special case\n        r1 = 0\n        r2 = max(0.1 * (env_reward - 1), 0) # food reward\n    return (r1, r2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Network architecture <a name=\"architecture\"></a>\n\nWe are going to train a neural network, which is structured in the following way:\n\n\n                                -------> Actor ---> Logits (ùúã)\n                              /\n    State (ùë†) ---> Encoder --- --------> Critic-1 ---> Value-1 (ùëâ-1)\n                              \\\n                                -------> Critic-2 ---> Value-2 (ùëâ-2)\n\n\nEach critic head predicts state-value $V_\\theta(s)$ (estimate of discounted return from this point onwards), where $\\theta$ stands for neural net parameters. Actor updates policy parameters for $\\pi_\\theta$, in the direction suggested by Critics.\n\nI won't go into details here, let the code speak for itself. The architecture of encoder is a relatively simple ResNet with Squeeze-Excitation blocks and Swish activation functions instead of commonly used ReLU.","metadata":{}},{"cell_type":"code","source":"class SEBlock(nn.Module):\n    \"\"\"Squeeze-Excitation Block\"\"\"\n    \n    def __init__(self, dim, reduction_ratio=4):\n        super(SEBlock, self).__init__()\n        self.f1 = nn.Linear(dim, dim // reduction_ratio)\n        self.f2 = nn.Linear(dim // reduction_ratio, dim)\n\n    def forward(self, x):\n        y = x.mean(axis=(-1, -2))\n        y = F.silu(self.f1(y))\n        y = torch.sigmoid(self.f2(y))\n        return x * y.unsqueeze(-1).unsqueeze(-1)\n\nclass BasicBlock(nn.Module):\n    \"\"\"Basic Residual Block\"\"\"\n    \n    def __init__(self, dim, downscale=False):\n        super(BasicBlock, self).__init__()\n        if downscale:\n            self.conv1 = nn.Conv2d(dim,     2 * dim, 3, stride=2, padding=1)\n            self.conv2 = nn.Conv2d(2 * dim, 2 * dim, 3, stride=1, padding=1)\n            self.bnorm1 = nn.BatchNorm2d(dim)\n            self.bnorm2 = nn.BatchNorm2d(2 * dim)\n            self.proj = nn.Conv2d(dim, 2 * dim, 1, stride=2)\n            self.se = SEBlock(2 * dim)\n        else:\n            self.conv1 = nn.Conv2d(dim, dim, 3, padding=1)\n            self.conv2 = nn.Conv2d(dim, dim, 3, padding=1)\n            self.bnorm1 = nn.BatchNorm2d(dim)\n            self.bnorm2 = nn.BatchNorm2d(dim)\n            self.proj = nn.Identity()\n            self.se = SEBlock(dim)\n\n    def forward(self, x):\n        y = self.conv1(self.bnorm1(F.silu(x)))\n        z = self.conv2(self.bnorm2(F.silu(y)))\n        return self.se(z) + self.proj(x)\n    \nclass BottleneckBlock(nn.Module):\n    \"\"\"Bottleneck Residual Block\"\"\"\n    \n    def __init__(self, dim, downscale=False):\n        super(BottleneckBlock, self).__init__()\n        if downscale:\n            self.conv1 = nn.Conv2d(dim,     dim,     1)\n            self.conv2 = nn.Conv2d(dim,     2 * dim, 3, stride=2, padding=1)\n            self.conv3 = nn.Conv2d(2 * dim, 2 * dim, 1)\n            self.bnorm1 = nn.BatchNorm2d(dim)\n            self.bnorm2 = nn.BatchNorm2d(dim)\n            self.bnorm3 = nn.BatchNorm2d(2 * dim)\n            self.proj = nn.Conv2d(dim, 2 * dim, 1, stride=2)\n            self.se = SEBlock(2 * dim)\n        else:\n            self.conv1 = nn.Conv2d(dim, dim, 1)\n            self.conv2 = nn.Conv2d(dim, dim, 3, padding=1)\n            self.conv3 = nn.Conv2d(dim, dim, 1)\n            self.bnorm1 = nn.BatchNorm2d(dim)\n            self.bnorm2 = nn.BatchNorm2d(dim)\n            self.bnorm3 = nn.BatchNorm2d(dim)\n            self.proj = nn.Identity()\n            self.se = SEBlock(dim)\n        \n    def forward(self, x):\n        y = self.conv1(self.bnorm1(F.silu(x)))\n        z = self.conv2(self.bnorm2(F.silu(y)))\n        w = self.conv3(self.bnorm3(F.silu(z)))\n        return self.se(w) + self.proj(x)\n\nclass ResLayers(nn.Module):\n    \"\"\"Sequential Residual Layers\"\"\"\n    \n    def __init__(self, block, dim, depth):\n        super(ResLayers, self).__init__()\n        self.blocks = nn.ModuleList(\n            [block(dim, downscale=False) for _ in range(depth - 1)] +\n            [block(dim, downscale=True)]\n            )\n    \n    def forward(self, x):\n        for b in self.blocks:\n            x = b(x)\n        return x\n    \nclass Encoder(nn.Module):\n    \"\"\"Res-Net Encoder\"\"\"\n    \n    def __init__(self, dim_in, depths):\n        super(Encoder, self).__init__()\n        self.gate = nn.Conv2d(12, dim_in, 1, padding=(3, 5), padding_mode='circular')\n        self.layers = nn.ModuleList([\n            ResLayers(BasicBlock,          dim_in, depths[0]),\n            ResLayers(BasicBlock,      2 * dim_in, depths[1]),\n            ResLayers(BottleneckBlock, 4 * dim_in, depths[2])\n        ])\n\n    def forward(self, x):\n        z = self.gate(x)\n        for l in self.layers:\n            z = l(z)\n        return z\n\nclass Actor(nn.Module):\n    \"\"\"Actor Head\"\"\"\n    \n    def __init__(self, dim_in, head_dim):\n        super(Actor, self).__init__()\n        self.compr = nn.Sequential(\n            nn.Conv2d(dim_in, dim_in, 3, padding=1),\n            nn.SiLU(inplace=True),\n            nn.BatchNorm2d(dim_in),\n            nn.Conv2d(dim_in, head_dim, 1),\n            nn.SiLU(inplace=True)\n        )\n        self.fc = nn.Linear(head_dim, 4)\n        \n    def forward(self, state):\n        p = self.compr(state)\n        p = p.mean(axis=(-1, -2))\n        p = self.fc(p)\n        return F.log_softmax(p, dim=1)\n    \nclass Critic(nn.Module):\n    \"\"\"Critic Head\"\"\"\n    \n    def __init__(self, dim_in, head_dim):\n        super(Critic, self).__init__()\n        self.compr = nn.Sequential(\n            nn.Conv2d(dim_in, dim_in, 3, padding=1),\n            nn.SiLU(inplace=True),\n            nn.BatchNorm2d(dim_in),\n            nn.Conv2d(dim_in, head_dim, 1),\n            nn.SiLU(inplace=True)\n        )\n        self.fc = nn.utils.weight_norm(nn.Linear(head_dim, 1))\n        \n    def forward(self, state):\n        v = self.compr(state)\n        v = v.mean(axis=(-1, -2))\n        v = self.fc(v)\n        return torch.tanh(v)\n\nclass GNet(nn.Module):\n    \"\"\"G-Net\"\"\"\n    \n    def __init__(self):\n        super(GNet, self).__init__()\n        # init hyperparameters\n        dim_in = 32\n        head_dim = 16\n        depths = (2, 2, 2)\n        # init modules\n        self.encoder = Encoder(dim_in, depths)\n        self.actor = Actor(8 * dim_in, head_dim)\n        self.critic1 = Critic(8 * dim_in, head_dim)\n        self.critic2 = Critic(8 * dim_in, head_dim)\n    \n    def forward(self, state):\n        latent = self.encoder(state)\n        logp = self.actor(latent)\n        v1 = self.critic1(latent)\n        v2 = self.critic2(latent)\n        return logp, (v1, v2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PPO algorithm explained <a name=\"ppo\"></a>\n\nFirstly, we will define policy gradient loss:\n\n$$\\mathcal{L}^{\\text{PG}}(\\theta) = \\mathbb{E}[\\log \\pi_\\theta(a|s) \\hat{A}_\\theta(s,a) ],$$\n\nwhere first term $\\log \\pi_\\theta(a|s)$ are log-probabilities from the output of policy network (actor head), and the second one is an estimate of `advantage function`, the relative value of selected action $a$. The value of $\\hat{A}_\\theta(s,a)$ is equal to `return` (or `discounted reward`) minus `baseline estimate`. Return at given time $t$ is calculated as follows:\n\n$$ V_{\\text{target}}(t) = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1},$$\n\nwhere $R^i_t$ is a reward at timestep $t$. Baseline estimate is the output of value network $V_\\theta(s)$. Therefore,\n\n$$ \\hat{A}_\\theta(t) = V_{\\text{target}}(t) - V_\\theta(s_t). $$\n\nThere also exists a generalized version of advantage estimation, that we are going to use:\n\n$$\\hat{A}_\\theta(t) = \\delta_t + (\\gamma \\lambda) \\delta_{t+1} + \\dots = \\sum_{k=0}^\\infty (\\gamma \\lambda)^k \\delta_{t+k+1},$$\n$$ \\text{where } \\delta_t = R_t + \\gamma V_\\theta(s_{t+1}) - V_\\theta(s_t),$$\n\nwhich is reduced to previous equation when $\\lambda = 1$.\n\nNow, when $\\hat{A}_\\theta$ is positive, meaning that the action agent took resulted in a better than average return, we will increase probabilities of selecting it again in the future. On the other hand, if an advantage was negative, we will reduce the likelihood of selected actions.","metadata":{}},{"cell_type":"code","source":"def inv_discount_cumsum(array, discount_factor):\n    res = [array[-1]]\n    for x in torch.flip(array, dims=[0])[1:]:\n        res.append(discount_factor * res[-1] + x)\n    return torch.flip(torch.stack(res), dims=[0])\n\ndef get_advantages_and_returns(rewards, values, gamma, lam):\n    # lists -> tensors\n    rewards = torch.tensor(rewards, dtype=torch.float)\n    values = torch.tensor(values + [0.])\n    # calculate deltas, A and R\n    deltas = rewards + gamma * values[1:] - values[:-1]\n    advs = inv_discount_cumsum(deltas, gamma * lam).cpu().detach().tolist()\n    rets = inv_discount_cumsum(rewards, gamma).cpu().detach().tolist()\n    return advs, rets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"However, as PPO-paper quotes:\n\n`While it is appealing to perform multiple steps of optimization on this loss using the same trajectory, doing so is not well-justified, and empirically it often leads to destructively large policy updates.`\n\nIn other words, we have to impose the constraint which won't allow our new policy to move too far away from an old one. Let‚Äôs denote the probability ratio between old and new policies as\n\n$$r(\\theta) = \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)}. $$\n\nThen, take a look at our new `surrogate` objective function:\n\n$$\\mathcal{L}^{\\text{CPI}}(\\theta) = \\mathbb{E}[r(\\theta) \\hat{A}_\\theta(s,a)].$$\n\nIt can be derived that maximimizing $\\mathcal{L}^{\\text{CPI}}(\\theta)$ is identical to vanilla policy gradient method, but I'll bravely skip the proof. Now, we would like to insert the aforementioned constraint into this loss function. The main objective which PPO-parer proposes is the following.\n\n$$J^{\\text{CLIP}}(\\theta) = \\mathbb{E}[\\min (r(\\theta) \\hat{A}_{\\theta_{\\text{old}}}(s,a), \\text{clip}(r(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_{\\theta_{\\text{old}}}(s,a)],$$\n\nwhere $\\epsilon$ is a `clip ratio` hyperparameter. The first term inside $min$ function, $r(\\theta) \\hat{A}_{\\theta_{\\text{old}}}(s,a)$ is a normal policy gradient objective. And the second one is its clipped version, which doesn't allow us to destroy our current policy based on a single estimate, because the value of $\\hat{A}_{\\theta_{\\text{old}}}(s,a)$ is noisy (as it is based on an output of our network).\n\nWhen applying PPO on the network architecture with shared parameters for both policy and value functions, in addition to the clipped reward, the objective function is augmented with an error term on the value estimation and an entropy term to encourage sufficient exploration. Final loss then becomes:\n\n$$\\mathcal{L}(\\theta) = \\mathbb{E}[-J(\\theta) + c  (V_\\theta(s) - V_{\\text{target}})^2 - c_{\\text{ent}}  H(s, \\pi_{\\theta}(\\cdot))], $$\n\nwhere $c$ and $c_{\\text{ent}}$ are both hyperparameter constants.","metadata":{}},{"cell_type":"code","source":"def compute_losses(net, data, c1, c2, c_ent, clip_ratio=0.2):\n    # move data to GPU\n    states = data['states'].cuda()\n    actions = data['actions'].cuda()\n    logp_old = data['log-p'].cuda()\n    returns = [data[f'ret-{i}'].float().cuda() for i in range(1, 3)]\n    advs  = data['adv-1'].float().cuda()\n    advs += data['adv-2'].float().cuda()\n    # get network outputs\n    logp_dist, (values_1, values_2) = net(states)\n    logp = torch.stack([lp[a] for lp, a in zip(logp_dist, actions)])\n    # compute actor loss\n    ratio = torch.exp(logp - logp_old)\n    clip_adv = torch.clamp(ratio, 1 - clip_ratio, 1 + clip_ratio) * advs\n    actor_loss = -(torch.min(ratio * advs, clip_adv)).mean()\n    # critic losses\n    critic_loss_1 = ((values_1.squeeze() - returns[0]) ** 2).mean()\n    critic_loss_2 = ((values_2.squeeze() - returns[1]) ** 2).mean()\n    # entropy loss\n    entropy = Categorical(probs=torch.exp(logp_dist)).entropy()\n    entropy[entropy != entropy] = torch.tensor(0.).cuda() # remove NaNs if any\n    entropy_loss = -entropy.mean()\n    return {'actor': actor_loss,\n            'critic': (c1 * critic_loss_1, \n                       c2 * critic_loss_2),\n            'entropy': c_ent * entropy_loss}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final touches <a name=\"final_touches\"></a>\n\nDefine G-Net dataset for training and Reinforcement Learning Agent which takes network as input and uses it to make actions.","metadata":{}},{"cell_type":"code","source":"class GeeseDataset(Dataset):\n    \"\"\"G-Net Dataset\"\"\"\n\n    def __init__(self, buffers):\n        self.buffers = buffers\n        \n    def __len__(self):\n        return len(self.buffers['states'])\n\n    def __getitem__(self, idx):\n        return {key: self.buffers[key][idx] for key in self.buffers}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RLAgent:\n    def __init__(self, net, stochastic):\n        self.prev_heads = [-1, -1, -1, -1]\n        self.net = net\n        self.stochastic = stochastic\n\n    def raw_outputs(self, state):\n        with torch.no_grad():\n            logits, (v1, v2) = self.net(state.cuda().unsqueeze(0))\n            logits = logits.squeeze(0)\n            v1 = v1.squeeze(0)\n            v2 = v2.squeeze(0)\n            if self.stochastic:\n                # get probabilities\n                probs = torch.exp(logits)\n                # convert 2 numpy\n                probs = probs.cpu().detach().numpy()\n                action = np.random.choice(range(4), p=probs) \n            else:\n                action = np.argmax(logits.cpu().detach().numpy())\n            return action, logits[action], (v1, v2)\n\n    def __call__(self, observation, configuration):\n        if observation['step'] == 0:\n            self.prev_heads = [-1, -1, -1, -1]\n        state = get_features(observation, configuration, self.prev_heads)\n        action, _, _ = self.raw_outputs(state)\n        self.prev_heads = [goose[0] if len(goose) > 0 else -1 for goose in observation['geese']]\n        return ['NORTH', 'EAST', 'SOUTH', 'WEST'][action]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training <a name=\"training\"></a>\n\nNow we will make a couple of functions which make our rl-agent play versus itself and collect the trajectories to be used in training. After performing training updates we run rl-agent against 3 greedy agents and save its performance.","metadata":{}},{"cell_type":"code","source":"def rollout(player, env, players, buffers, gammas, lambdas):\n    rewards = {str(i + 1): [] for i in range(2)}\n    values  = {str(i + 1): [] for i in range(2)}\n    # shuffle players indices\n    shuffle(players)\n    trainer = env.train(players)\n    observation = trainer.reset()\n    prev_obs = observation\n    done = False\n    prev_heads = [None for _ in range(4)]\n    # start rollout\n    while not done:\n        # cache previous state\n        for i, g in enumerate(observation['geese']):\n            if len(g) > 0:\n                prev_heads[i] = prev_obs['geese'][i][0]\n        prev_obs = observation\n        # transform observation to state\n        state = get_features(observation, env.configuration, prev_heads)\n        # make a move\n        action, logp, v = player.raw_outputs(state)\n        # observe\n        observation, reward, done, _ = trainer.step(['NORTH', 'EAST', 'SOUTH', 'WEST'][action])\n        # data -> buffers\n        buffers['states'].append(state)\n        buffers['actions'].append(action)\n        buffers['log-p'].append(logp.cpu().detach())\n        # save rewards and values\n        r = get_rewards(reward, observation, prev_obs, done)\n        for i in range(2):\n            rewards[str(i+1)].append(r[i])\n            values[str(i+1)].append(v[i])\n    # save advantages and returns\n    for key in ['1', '2']:\n        advs, rets = get_advantages_and_returns(rewards[key], values[key], gammas[key], lambdas[key])\n        # add them to buffer\n        buffers['adv-' + key] += advs\n        buffers['ret-' + key] += rets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def runner(net, env, samples_threshold, gammas, lambdas, progress_bar=False):\n    data_buffers = {'states': [], 'actions': [], 'log-p': [],\n                    'adv-1': [], 'ret-1': [],\n                    'adv-2': [], 'ret-2': []}\n    samples_collected = 0\n    if progress_bar:\n        samples_bar = tqdm(total=samples_threshold, desc='Collecting Samples', leave=False)\n    player = RLAgent(net, stochastic=True)\n    opponents = [RLAgent(net, stochastic=False) for _ in range(3)]\n    while True:\n        rollout(player, env, players=[None] + opponents, buffers=data_buffers,\n                gammas=gammas, lambdas=lambdas)\n        if progress_bar:\n            # update progress bar\n            samples_bar.update(len(data_buffers['states']) - samples_collected)\n        samples_collected = len(data_buffers['states'])\n        if samples_collected >= samples_threshold:\n            if progress_bar:\n                samples_bar.close()\n            return data_buffers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(net, optimizer,\n          n_episodes=25,\n          batch_size=256,\n          samples_threshold=10000,\n          n_ppo_epochs=25):\n    losses_hist = {'clip': [], 'value-1': [], 'value-2': [], 'ent': [], 'lr': []}\n    win_rates = {'Score': [], 'Rank': []}\n    gammas  = {'1': 0.8, '2': 0.8}\n    lambdas = {'1': 0.7, '2': 0.7}\n    print('-Start Training')\n    for episode in tqdm(range(n_episodes), desc='Episode', leave=False):\n        # update statistics\n        net.eval()\n        player = RLAgent(net, stochastic=False)\n        scores = evaluate(\"hungry_geese\", [player] + ['greedy'] * 3, num_episodes=30)\n        win_rates['Score'].append(np.mean([r[0] for r in scores]))\n        win_rates['Rank'].append(np.mean([sum(r[0] <= r_ for r_ in r if r_ is not None) for r in scores]))\n        # collect data\n        buffers = runner(net, env, samples_threshold, gammas=gammas, lambdas=lambdas)\n        # perform training\n        net.train()\n        dataset = GeeseDataset(buffers)\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n        for epoch in range(n_ppo_epochs):\n            for batch in dataloader:\n                losses = compute_losses(net, augment(batch),\n                                        c1=1, c2=1, c_ent=0.01)\n                loss = losses['actor']\n                losses_hist['clip'].append(losses['actor'].item())\n                for i in range(2):\n                    loss += losses['critic'][i]\n                    losses_hist[f'value-{i+1}'].append(losses['critic'][i].item())\n                loss += losses['entropy']\n                losses_hist['ent'].append(losses['entropy'].item())\n                loss.backward()\n                nn.utils.clip_grad_norm_(net.parameters(), max_norm=1)\n                optimizer.step()\n                optimizer.zero_grad()\n    return win_rates","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net = GNet().cuda()\noptimizer = Adam(net.parameters(), lr=1e-5)\n\nwin_rates = train(net, optimizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save and show <a name=\"training\"></a>","metadata":{}},{"cell_type":"code","source":"!mkdir checkpoint\ntorch.save(net.state_dict(), 'checkpoint/g.net')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t = np.arange(len(win_rates['Score']))\nfig, ax1 = plt.subplots()\n\ncolor = 'tab:red'\nax1.set_xlabel('Timesteps')\nax1.set_ylabel('Score', color=color)\nax1.plot(t, win_rates['Score'], color=color)\nax1.tick_params(axis='y', labelcolor=color)\nax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n\ncolor = 'tab:blue'\nax2.set_ylabel('Rank', color=color)  # we already handled the x-label with ax1\nax2.plot(t, win_rates['Rank'], color=color)\nax2.tick_params(axis='y', labelcolor=color)\nfig.tight_layout()  # otherwise the right y-label is slightly clipped\nplt.title('Performance of G-net vs 3 Greedy Agents')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.reset()\nnet.eval()\nenv.run([RLAgent(net, stochastic=False),\n         RLAgent(net, stochastic=False),\n         RLAgent(net, stochastic=False),\n         RLAgent(net, stochastic=False)])\nenv.render(mode=\"ipython\", width=500, height=400)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}