{"cells":[{"metadata":{},"cell_type":"markdown","source":"Started up working on a stable_baselines3 self-play implementation for snake. Still lots of work to do to get it fully functional but figured I would open source it so others can work off it/give me tips if they see anything blatantly misconfigured since I am relatively new to applying RL. \n\nNotebook is a blend of https://github.com/hardmaru/slimevolleygym/blob/master/training_scripts/train_ppo_selfplay.py and https://www.kaggle.com/kwabenantim/stable-baselines-starter"},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\nfrom kaggle_environments import evaluate, make","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install stable-baselines3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport gym\nfrom gym import spaces\nimport numpy as np\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common import logger\nfrom stable_baselines3.common.callbacks import EvalCallback\n\nfrom shutil import copyfile\nimport os\n\nfrom stable_baselines3.common.vec_env import SubprocVecEnv\nfrom stable_baselines3.common.monitor import Monitor\n\ndef transform_observation(obs, config):\n    my_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n    their_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n    food_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n    for goose in obs[0].observation.geese[0]:\n        my_board[goose] = 255\n    my_board = my_board.reshape((config.rows, config.columns, 1))\n    \n    for goose in obs[0].observation.geese[1]:\n        their_board[goose] = 255\n    their_board = their_board.reshape((config.rows, config.columns, 1))\n    \n    for goose in obs[0].observation.food:\n        food_board[goose] = 255\n    food_board = food_board.reshape((config.rows, config.columns, 1))\n    board = np.concatenate([my_board, their_board, food_board], axis = -1)\n    return board","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_actions(actions):\n    if actions == 0:\n        return \"NORTH\"\n    if actions == 1:\n        return \"EAST\"\n    if actions == 2:\n        return \"WEST\"\n    if actions == 3:\n        return \"SOUTH\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"geese_env = make(\"hungry_geese\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"REWARD_LOST = -1\nREWARD_WON = 1\nclass GeeseGym(gym.Env):\n    def __init__(self, debug = False):     \n        self.geese_env = make(\"hungry_geese\", debug = debug)\n        self.config = self.geese_env.configuration\n        self.action_space = spaces.Discrete(4)\n        \n        self.observation_space = spaces.Box(low=0, high=255, \n                                            shape=(self.config.rows, \n                                                   self.config.columns, \n                                                   3), \n                                            dtype=np.uint8)\n        \n        \n        self.reward_range = (-1, 1000)\n    def reset(self):\n        self.obs = self.geese_env.reset(num_agents = 2)\n        x_obs = transform_observation(self.obs, self.config)\n        return x_obs\n    \n    \n    def step(self, action):\n        my_actions = transform_actions(action)\n        opponent_action = transform_actions(0)\n        self.obs = self.geese_env.step([my_actions, opponent_action])        \n        x_obs = transform_observation(self.obs, self.config)\n        x_reward = self.obs[0].reward\n        done = (self.obs[0][\"status\"] != \"ACTIVE\")\n        info = self.obs[0][\"info\"]\n        return x_obs, x_reward, done, info","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Settings\nSEED = 17\nNUM_TIMESTEPS = int(1e7)\nEVAL_FREQ = int(1e4)\nEVAL_EPISODES = int(1e2)\nBEST_THRESHOLD = 0.01 # must achieve a mean score above this to replace prev best self\n\nLOGDIR = \"ppo1_selfplay\"\n\nclass GeeseSelfPlayEnv(GeeseGym):\n  # wrapper over the normal single player env, but loads the best self play model\n    def __init__(self):\n        super(GeeseSelfPlayEnv, self).__init__()\n        self.policy = self\n        self.best_model = None\n        self.best_model_filename = None\n    def predict(self, obs): # the policy\n        if self.best_model is None:\n            return self.action_space.sample() # return a random action\n        else:\n            action, _ = self.best_model.predict(obs)\n        return action\n    def reset(self):\n        # load model if it's there\n        modellist = [f for f in os.listdir(LOGDIR) if f.startswith(\"history\")]\n        modellist.sort()\n        if len(modellist) > 0:\n            filename = os.path.join(LOGDIR, modellist[-1]) # the latest best model\n            if filename != self.best_model_filename:\n                print(\"loading model: \", filename)\n                self.best_model_filename = filename\n                if self.best_model is not None:\n                    del self.best_model\n                self.best_model = PPO.load(filename, env=self)\n        return super(GeeseSelfPlayEnv, self).reset()\n\nclass SelfPlayCallback(EvalCallback):\n  # hacked it to only save new version of best model if beats prev self by BEST_THRESHOLD score\n  # after saving model, resets the best score to be BEST_THRESHOLD\n    def __init__(self, *args, **kwargs):\n        super(SelfPlayCallback, self).__init__(*args, **kwargs)\n        self.best_mean_reward = BEST_THRESHOLD\n        self.generation = 0\n    def _on_step(self) -> bool:\n        result = super(SelfPlayCallback, self)._on_step()\n        if result and self.best_mean_reward > BEST_THRESHOLD:\n            self.generation += 1\n            print(\"SELFPLAY: mean_reward achieved:\", self.best_mean_reward)\n            print(\"SELFPLAY: new best model, bumping up generation to\", self.generation)\n            source_file = os.path.join(LOGDIR, \"best_model.zip\")\n            backup_file = os.path.join(LOGDIR, \"history_\"+str(self.generation).zfill(8)+\".zip\")\n            copyfile(source_file, backup_file)\n            self.best_mean_reward = BEST_THRESHOLD\n        return result\n\ndef rollout(env, policy):\n    obs = env.reset()\n\n    done = False\n    total_reward = 0\n\n    while not done:\n        action, _states = policy.predict(obs)\n        obs, reward, done, _ = env.step(action)\n        total_reward += reward\n\n    return total_reward","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_env(rank=0):\n    def _init():\n        env = GeeseSelfPlayEnv()\n        log_file = os.path.join(LOGDIR, str(rank))\n        return env\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logger.configure(folder=LOGDIR)\n# env = GeeseSelfPlayEnv()\n# env = SubprocVecEnv([GeeseGym() for i in range(4)])\nmodel = PPO('MlpPolicy', GeeseGym(), verbose = 1, n_steps = 2048*16, batch_size = 128, n_epochs = 50, learning_rate = .01)\n# eval_callback = SelfPlayCallback(env,\n#     best_model_save_path=LOGDIR,\n#     log_path=LOGDIR,\n#     eval_freq=EVAL_FREQ,\n#     n_eval_episodes=EVAL_EPISODES,\n#     deterministic=False)\nmodel.learn(total_timesteps=NUM_TIMESTEPS)\nmodel.save(os.path.join(LOGDIR, \"final_model\")) # probably never get to this point.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_test(model):\n    env = GeeseGym(debug = True)\n    obs = env.reset()\n    done = False\n    while not done:\n        actions = model.predict(obs)[0]\n        obs, reward, done, info = env.step(actions)\n        print(reward)\n#         plt.imshow(obs)\n#         plt.show()\nrun_test(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}