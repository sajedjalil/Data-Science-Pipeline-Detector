{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Creating Gym Geese Environment","metadata":{}},{"cell_type":"markdown","source":"While working with the Stable baselines framework, we need to convert our current environment to a Gym environment.\nThese kernels helped me to grasp a better understanding of that process:\nhttps://www.kaggle.com/ryches/stable-baselines3-starter-wip &\nhttps://www.kaggle.com/victordelafuente/dqn-goose-with-stable-baselines3-pytorch","metadata":{}},{"cell_type":"code","source":"# Importing of stable-baselines3 package\n!pip install stable-baselines3","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:57:29.293502Z","iopub.execute_input":"2021-06-05T17:57:29.294013Z","iopub.status.idle":"2021-06-05T17:57:37.847706Z","shell.execute_reply.started":"2021-06-05T17:57:29.29392Z","shell.execute_reply":"2021-06-05T17:57:37.846626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"My agent will learn against three of [risk averse greedy goose](https://www.kaggle.com/ilialar/risk-averse-greedy-goose) ","metadata":{}},{"cell_type":"code","source":"%%writefile greedy-goose.py\n\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\nimport numpy as np\nimport random\n\ndef get_nearest_cells(x,y):\n    # returns all cells reachable from the current one\n    result = []\n    for i in (-1,+1):\n        result.append(((x+i+7)%7, y))\n        result.append((x, (y+i+11)%11))\n    return result\n\ndef find_closest_food(table):\n    # returns the first step toward the closest food item\n    new_table = table.copy()\n    \n    \n    # (direction of the step, axis, code)\n    possible_moves = [\n        (1, 0, 1),\n        (-1, 0, 2),\n        (1, 1, 3),\n        (-1, 1, 4)\n    ]\n    \n    # shuffle possible options to add variability\n    random.shuffle(possible_moves)\n    \n    \n    updated = False\n    for roll, axis, code in possible_moves:\n\n        shifted_table = np.roll(table, roll, axis)\n        \n        if (table == -2).any() and (shifted_table[table == -2] == -3).any(): # we have found some food at the first step\n            return code\n        else:\n            mask = np.logical_and(new_table == 0,shifted_table == -3)\n            if mask.sum() > 0:\n                updated = True\n            new_table += code * mask\n        if (table == -2).any() and shifted_table[table == -2][0] > 0: # we have found some food\n            return shifted_table[table == -2][0]\n        \n        # else - update new reachible cells\n        mask = np.logical_and(new_table == 0,shifted_table > 0)\n        if mask.sum() > 0:\n            updated = True\n        new_table += shifted_table * mask\n\n    # if we updated anything - continue reccurison\n    if updated:\n        return find_closest_food(new_table)\n    # if not - return some step\n    else:\n        return table.max()\n\nlast_step = None\n\ndef agent(obs_dict, config_dict):\n    global last_step\n    \n    observation = Observation(obs_dict)\n    configuration = Configuration(config_dict)\n    player_index = observation.index\n    player_goose = observation.geese[player_index]\n    player_head = player_goose[0]\n    player_row, player_column = row_col(player_head, configuration.columns)\n\n\n    table = np.zeros((7,11))\n    # 0 - emply cells\n    # -1 - obstacles\n    # -4 - possible obstacles\n    # -2 - food\n    # -3 - head\n    # 1,2,3,4 - reachable on the current step cell, number is the id of the first step direction\n    \n    legend = {\n        1: 'SOUTH',\n        2: 'NORTH',\n        3: 'EAST',\n        4: 'WEST'\n    }\n    \n    # let's add food to the map\n    for food in observation.food:\n        x,y = row_col(food, configuration.columns)\n        table[x,y] = -2 # food\n        \n    # let's add all cells that are forbidden\n    for i in range(4):\n        opp_goose = observation.geese[i]\n        if len(opp_goose) == 0:\n            continue\n            \n        is_close_to_food = False\n            \n        if i != player_index:\n            x,y = row_col(opp_goose[0], configuration.columns)\n            possible_moves = get_nearest_cells(x,y) # head can move anywhere\n            \n            for x,y in possible_moves:\n                if table[x,y] == -2:\n                    is_close_to_food = True\n            \n                table[x,y] = -4 # possibly forbidden cells\n        \n        # usually we ignore the last tail cell but there are exceptions\n        tail_change = -1\n        if obs_dict['step'] % 40 == 39:\n            tail_change -= 1\n        \n        # we assume that the goose will eat the food\n        if is_close_to_food:\n            tail_change += 1\n        if tail_change >= 0:\n            tail_change = None\n            \n\n        for n in opp_goose[:tail_change]:\n            x,y = row_col(n, configuration.columns)\n            table[x,y] = -1 # forbidden cells\n    \n    # going back is forbidden according to the new rules\n    x,y = row_col(player_head, configuration.columns)\n    if last_step is not None:\n        if last_step == 1:\n            table[(x + 6) % 7,y] = -1\n        elif last_step == 2:\n            table[(x + 8) % 7,y] = -1\n        elif last_step == 3:\n            table[x,(y + 10)%11] = -1\n        elif last_step == 4:\n            table[x,(y + 12)%11] = -1\n        \n    # add head position\n    table[x,y] = -3\n    \n    # the first step toward the nearest food\n    step = int(find_closest_food(table))\n    \n    # if there is not available steps try to go to possibly dangerous cell\n    if step not in [1,2,3,4]:\n        x,y = row_col(player_head, configuration.columns)\n        if table[(x + 8) % 7,y] == -4:\n            step = 1\n        elif table[(x + 6) % 7,y] == -4:\n            step = 2\n        elif table[x,(y + 12)%11] == -4:\n            step = 3\n        elif table[x,(y + 10)%11] == -4:\n            step = 4\n                \n    # else - do a random step and lose\n        else:\n            step = np.random.randint(4) + 1\n    \n    last_step = step\n    return legend[step]","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-06-05T17:57:37.849611Z","iopub.execute_input":"2021-06-05T17:57:37.849888Z","iopub.status.idle":"2021-06-05T17:57:37.857594Z","shell.execute_reply.started":"2021-06-05T17:57:37.849862Z","shell.execute_reply":"2021-06-05T17:57:37.856589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implementing PPO2 Agent","metadata":{}},{"cell_type":"markdown","source":"We convert the default env in a custom Gym env","metadata":{}},{"cell_type":"code","source":"# Importing of hungry_goose environnment\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col, greedy_agent, GreedyAgent\nfrom kaggle_environments import evaluate, make\n\n# Importing of gym and numpy\nimport gym\nimport numpy as np\nfrom gym import spaces\n\n# We create our custon environment\nclass GeeseGym(gym.Env):\n    \n    # Environnment initializaton\n    def __init__(self): \n        # We generate the default hungry_geese env & configuration\n        self.env = make(\"hungry_geese\")\n        self.config = self.env.configuration\n        \n        # We only got 4 actions\n        self.action_space = spaces.Discrete(4)\n        \n        # Normalizing obs_space from -1 to 1\n        self.observation_space = spaces.Box(low=-1,\n                                            high=1, \n                                            shape=(self.config.rows, \n                                                   self.config.columns, \n                                                   1), \n                                            dtype=np.float64)\n        self.trainer = self.env.train([None,'greedy','greedy','greedy'])\n        \n    # We need to transform the information generated by the default hungry_geese environment\n    # into a custom board normalized from -1 to 1\n    # We can also attribute specific values to some elements like the food, heads to help tha RL algorithm.\n    \n    def transform_obs(self, obs, config):\n        # Creating the board\n        board = [[0 for _ in range(self.config.columns)] for _ in range(self.config.rows)]\n        # We retrieve the useful informations from the defaut board\n        if type(obs) is list:\n            obs = obs[0].observation\n        else:\n            obs = Observation(obs)\n        my_index = obs[\"index\"]\n        \n        my_head = obs.geese[my_index][0]\n        my_body = [_ for _ in obs.geese[my_index][1:]]\n        \n        other_geese = [_ for _ in obs.geese if _ != obs.geese[my_index]]\n        other_heads = []\n        other_bodies = []\n        for goose in other_geese:\n            if goose: \n                other_heads.append(goose[0])\n            if len(goose) > 1:\n                other_bodies.append(goose[1:])\n            \n        foods = [food for food in obs.food]\n        \n        # We attribute to any of these useful informations a specific value using the row_col conversion function\n        r, c = row_col(my_head,self.config.columns)\n        board[r][c] = 0.95\n        \n        for position in my_body:\n            r, c = row_col(position,self.config.columns)\n            board[r][c] = 0.90\n        \n        for position in other_heads:\n            r, c = row_col(position,self.config.columns)\n            board[r][c] = 0.15\n        \n        for goose_body in other_bodies:\n            for position in goose_body:\n                r, c = row_col(position,self.config.columns)\n                board[r][c] = 0.1\n        \n        for position in foods:\n            r, c = row_col(position,self.config.columns)\n            board[r][c] = 1\n        \n        # We need a (7,11,1) dimensional array\n        board = np.transpose(board)    \n        board = np.array([board])\n        board = np.transpose(board)\n        \n        return board\n    \n    def reset(self):\n        # We transform the obs every time we reset the env\n        self.obs = self.env.reset(num_agents = 4)\n        self.transformed_obs = self.transform_obs(self.obs, self.config)\n        return self.transformed_obs\n    \n    def transform_action(self,agent_value):\n        # We need to transform the numerical values retruned by the algorithm \n        #into string values accepted by the default env \n        if agent_value == 0:\n            return \"NORTH\"\n        if agent_value == 1:\n            return \"EAST\"\n        if agent_value == 2:\n            return \"WEST\"\n        if agent_value == 3:\n            return \"SOUTH\"\n    \n    def step(self, agent_value):\n        # We transform the action and then pass it to the default env\n        action = self.transform_action(agent_value)\n        self.obs, reward, done, info = self.trainer.step(action)\n        reward = 1\n        if done == False:\n            self.transformed_obs = self.transform_obs(self.obs, self.config)\n        else:\n            reward = -1\n        return self.transformed_obs, reward, done, info","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:57:37.858869Z","iopub.execute_input":"2021-06-05T17:57:37.859123Z","iopub.status.idle":"2021-06-05T17:57:38.1818Z","shell.execute_reply.started":"2021-06-05T17:57:37.859099Z","shell.execute_reply":"2021-06-05T17:57:38.180997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We train the agent on 6 vectorized envs","metadata":{}},{"cell_type":"code","source":"import os\n# Importing the PPO algorithm and the make_vec_env packages\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\n\n# We create multiple vectorized environments\ngeese_env = make_vec_env(GeeseGym, n_envs = 6)\n\n# We create the PPO agent and train it\n\nmodel = PPO('MlpPolicy', geese_env, verbose = 1)\nmodel.learn(total_timesteps=((1e6)*4))\nmodel.save(\"ppo_goose\")","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:57:38.182809Z","iopub.execute_input":"2021-06-05T17:57:38.183206Z","iopub.status.idle":"2021-06-05T17:58:57.037291Z","shell.execute_reply.started":"2021-06-05T17:57:38.183177Z","shell.execute_reply":"2021-06-05T17:58:57.036426Z"},"trusted":true},"execution_count":null,"outputs":[]}]}