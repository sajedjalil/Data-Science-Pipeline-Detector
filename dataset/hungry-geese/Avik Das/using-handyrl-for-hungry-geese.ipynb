{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The content of this Code is mainly borrowed from https://www.kaggle.com/yuricat/smart-geese-trained-by-reinforcement-learning \n\nThank you for sharing this code public!","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Install libraries\n```bash\ngit clone https://github.com/DeNA/HandyRL.git\ncd HandyRL\npip3 install -r requirements.txt\npip3 install -r handyrl/envs/kaggle/requirements.txt\n```","metadata":{}},{"cell_type":"code","source":"# requires Internet Access.\n!git clone https://github.com/DeNA/HandyRL.git\n!pip install -r HandyRL/requirements.txt\n!pip install -r HandyRL/handyrl/envs/kaggle/requirements.txt","metadata":{"execution":{"iopub.status.busy":"2021-06-23T19:59:11.7325Z","iopub.execute_input":"2021-06-23T19:59:11.732894Z","iopub.status.idle":"2021-06-23T20:01:09.640818Z","shell.execute_reply.started":"2021-06-23T19:59:11.732815Z","shell.execute_reply":"2021-06-23T20:01:09.639715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Place this file where you run main.py\n\n`config.yaml`\n```config.yaml\n\nenv_args:\n    #env: 'TicTacToe'\n    #env: 'Geister'\n    env: 'HungryGeese'\n    source: 'handyrl.envs.kaggle.hungry_geese'\n    #env: 'handyrl.envs.parallel_tictactoe'  # specify by path\n\ntrain_args:\n    turn_based_training: False\n    observation: True\n    gamma: 0.8\n    forward_steps: 32\n    compress_steps: 4\n    entropy_regularization: 2.0e-3\n    entropy_regularization_decay: 0.3\n    update_episodes: 500\n    batch_size: 128\n    minimum_episodes: 10000\n    maximum_episodes: 50000\n    epochs: 10 # increase as needed\n    num_batchers: 2\n    eval_rate: 0.1\n    worker:\n        num_parallel: 6\n    lambda: 0.7\n    policy_target: 'TD' # 'UPGO' 'VTRACE' 'TD' 'MC'\n    value_target: 'TD' # 'VTRACE' 'TD' 'MC'\n    seed: 0\n    restart_epoch: 0\n\n\n```\nRefer to https://github.com/DeNA/HandyRL/blob/master/docs/parameters.md for meanings\n\nRefer to https://www.kaggle.com/c/hungry-geese/discussion/218190 for publicly available models and parameters\n\nchange parallel processing setting according to HW limitations.","metadata":{}},{"cell_type":"code","source":"%%writefile config.yaml\nenv_args:\n    #env: 'TicTacToe'\n    #env: 'Geister'\n    env: 'HungryGeese'\n    source: 'handyrl.envs.kaggle.hungry_geese'\n    #env: 'handyrl.envs.parallel_tictactoe'  # specify by path\n\ntrain_args:\n    turn_based_training: False\n    observation: True\n    gamma: 0.8\n    forward_steps: 32\n    compress_steps: 4\n    entropy_regularization: 2.0e-3\n    entropy_regularization_decay: 0.3\n    update_episodes: 500\n    batch_size: 128\n    minimum_episodes: 10000\n    maximum_episodes: 80000\n    epochs: 50 # Set more epochs to learn more. -1 for running forever.\n    num_batchers: 4\n    eval_rate: 0.1\n    worker:\n        num_parallel: 4\n    lambda: 0.7\n    policy_target: 'TD' # 'UPGO' 'VTRACE' 'TD' 'MC'\n    value_target: 'TD' # 'VTRACE' 'TD' 'MC'\n    seed: 0\n    restart_epoch: 0\n","metadata":{"execution":{"iopub.status.busy":"2021-06-23T20:01:09.643613Z","iopub.execute_input":"2021-06-23T20:01:09.643947Z","iopub.status.idle":"2021-06-23T20:01:09.650898Z","shell.execute_reply.started":"2021-06-23T20:01:09.643915Z","shell.execute_reply":"2021-06-23T20:01:09.649969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train using\n\n```bash\npython3 main.py --train\n```\n\nModels are saved at `models/*.pth`\n\nBy default, models are evaluated against random player. You may change this to GreedyAgent.\n\n","metadata":{}},{"cell_type":"code","source":"%run HandyRL/main.py --train","metadata":{"execution":{"iopub.status.busy":"2021-06-23T20:01:09.652193Z","iopub.execute_input":"2021-06-23T20:01:09.652467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport pickle\nimport bz2\nimport base64\n\nstate_dict = torch.load('models/latest.pth') # using latest.pth, you could also use i.pth (i=0, 1, 2, ...)\n# Save model parameters as base64 for submission\nPARAM = base64.b64encode(bz2.compress(pickle.dumps(state_dict)))\n\n# Save param\n# with open(\"PARAM.txt\", \"w\") as f:\n#     f.write(repr(PARAM))\n\nPARAM = repr(PARAM) # prepend `b'` and append `'`\nprint(PARAM[:10] + \"...\" + PARAM[-10:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile submission.py\n\n# This is a lightweight ML agent trained by self-play.\n# https://github.com/DeNA/HandyRL\n# copied from https://www.kaggle.com/yuricat/smart-geese-trained-by-reinforcement-learning\n\n\nimport pickle\nimport bz2\nimport base64\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n# Neural Network for Hungry Geese\n\nclass TorusConv2d(nn.Module):\n    def __init__(self, input_dim, output_dim, kernel_size, bn):\n        super().__init__()\n        self.edge_size = (kernel_size[0] // 2, kernel_size[1] // 2)\n        self.conv = nn.Conv2d(input_dim, output_dim, kernel_size=kernel_size)\n        self.bn = nn.BatchNorm2d(output_dim) if bn else None\n\n    def forward(self, x):\n        h = torch.cat([x[:,:,:,-self.edge_size[1]:], x, x[:,:,:,:self.edge_size[1]]], dim=3)\n        h = torch.cat([h[:,:,-self.edge_size[0]:], h, h[:,:,:self.edge_size[0]]], dim=2)\n        h = self.conv(h)\n        h = self.bn(h) if self.bn is not None else h\n        return h\n\n\nclass GeeseNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        layers, filters = 12, 32\n        self.conv0 = TorusConv2d(17, filters, (3, 3), True)\n        self.blocks = nn.ModuleList([TorusConv2d(filters, filters, (3, 3), True) for _ in range(layers)])\n        self.head_p = nn.Linear(filters, 4, bias=False)\n        self.head_v = nn.Linear(filters * 2, 1, bias=False)\n\n    def forward(self, x):\n        h = F.relu_(self.conv0(x))\n        for block in self.blocks:\n            h = F.relu_(h + block(h))\n        h_head = (h * x[:,:1]).view(h.size(0), h.size(1), -1).sum(-1)\n        h_avg = h.view(h.size(0), h.size(1), -1).mean(-1)\n        p = self.head_p(h_head)\n        v = torch.tanh(self.head_v(torch.cat([h_head, h_avg], 1)))\n\n        return {'policy': p, 'value': v}\n\n\n# Input for Neural Network\n\ndef make_input(obses):\n    b = np.zeros((17, 7 * 11), dtype=np.float32)\n    obs = obses[-1]\n\n    for p, pos_list in enumerate(obs['geese']):\n        # head position\n        for pos in pos_list[:1]:\n            b[0 + (p - obs['index']) % 4, pos] = 1\n        # tip position\n        for pos in pos_list[-1:]:\n            b[4 + (p - obs['index']) % 4, pos] = 1\n        # whole position\n        for pos in pos_list:\n            b[8 + (p - obs['index']) % 4, pos] = 1\n            \n    # previous head position\n    if len(obses) > 1:\n        obs_prev = obses[-2]\n        for p, pos_list in enumerate(obs_prev['geese']):\n            for pos in pos_list[:1]:\n                b[12 + (p - obs['index']) % 4, pos] = 1\n\n    # food\n    for pos in obs['food']:\n        b[16, pos] = 1\n\n    return b.reshape(-1, 7, 11)\n\n\n# Load PyTorch Model\n\nPARAM = %PARAM%\n\nstate_dict = pickle.loads(bz2.decompress(base64.b64decode(PARAM)))\nmodel = GeeseNet()\nmodel.load_state_dict(state_dict)\nmodel.eval()\n\n\n# Main Function of Agent\n\nobses = []\n\ndef agent(obs, _):\n    obses.append(obs)\n    x = make_input(obses)\n    with torch.no_grad():\n        xt = torch.from_numpy(x).unsqueeze(0)\n        o = model(xt)\n    p = o['policy'].squeeze(0).detach().numpy()\n\n    actions = ['NORTH', 'SOUTH', 'WEST', 'EAST']\n    return actions[np.argmax(p)]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('submission.py', 'r+') as f:\n    agent = f.read()\n    f.seek(0.0)\n    f.write(agent.replace('%PARAM%', PARAM))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_environments import make\nenv = make(\"hungry_geese\", debug=True)\n\nenv.reset()\nenv.run(['greedy', 'greedy', 'submission.py', 'submission.py'])\nenv.render(mode=\"ipython\", width=800, height=700)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.state","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}