{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Purpose of this notebook\n\nThis notebook is just a proof of concept on how to build a simple q-learning agent, so far it doesn't do great (it's improved from the start but still, no winner) but I'll try my best to improve it, if not at least I save someone some ground work... ;-)\n\n\n## Basic QLearner\n\n\nEquation used to update q-table can be seen on image:\n\n![Q-Learning](https://wikimedia.org/api/rest_v1/media/math/render/svg/678cb558a9d59c33ef4810c9618baf34a9577686) Source: Wikipedia\n\n\n\n### Epsilon greedy policy\n\nThe epsilon greedy policy means at each step we choose the action that yields a higher Q value (estimated reward) from the current state, but a certain epsilon-percent of times we chose a random action (to explore)."},{"metadata":{"trusted":true},"cell_type":"code","source":"import random as rand\nimport pickle\n\n#This class encapsulates a simple qlearning with epsilon-greedy policy, the states and transitions can be defined automatically as we explore (search space is too big to initialize all at once and many states won't be achievable)\nclass QLearner():\n    def __init__(self, actions, states=None, initial_value=0.1, alpha=0.05, gamma=0.8, epsilon=0.1, create_states_on_exploration=True):\n        self.actions = actions\n        self.create_states_on_exploration = create_states_on_exploration\n        self.initial_value = initial_value\n        if states!=None:\n            self.q_table = {\n                state: [initial_value for _ in self.actions] for state in states\n            }\n            self.states = states\n        else:\n            self.q_table = dict()\n            self.states = []\n            \n        self.alpha = alpha\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.previous_state = None\n        self.current_state = None\n        self.last_action = None\n        self.last_action_index = None\n\n        \n    def _check_auto_init_state(self, state):\n        if (state!=None) and (state not in self.q_table.keys()) and self.create_states_on_exploration:\n            self.q_table[state] = [self.initial_value for _ in self.actions]\n            self.states.append(state)\n    \n\n    def _epsilon_greedy(self, state):\n        #create state if needed\n        self._check_auto_init_state(state)\n        \n        if (rand.random() < self.epsilon):\n            action = rand.choice(self.actions)\n            self.last_action_index = self.actions.index(action)\n        else:\n            q_state = self.q_table[state]\n            max_val = max(q_state)\n            self.last_action_index = rand.choice([i for i,v in enumerate(q_state) if v==max_val])\n            action = self.actions[self.last_action_index]\n        return action\n\n    \n    def process_reward(self, reward, previous_state=None, last_action=None, last_action_index=None):\n        if previous_state==None:\n            previous_state = self.previous_state\n        if last_action==None:\n            last_action = self.last_action\n        if last_action_index==None:\n            last_action_index = self.last_action_index\n            \n        if (previous_state==None) or (last_action==None):\n            return\n        \n        #create state if needed\n        self._check_auto_init_state(previous_state)\n        \n        q = self.q_table\n        q_old = q[previous_state][last_action_index]\n        next_state = self.current_state\n        if next_state!=None:        \n            best_scenario = q[next_state].index(max(q[next_state]))\n            q[previous_state][last_action_index] = q_old + self.alpha * (reward + self.gamma * best_scenario - q_old)\n        else:\n            q[previous_state][last_action_index] = q_old + self.alpha * (reward + self.initial_value - q_old)\n\n            \n    def epsilon_greedy_choose_action(self, state):\n        self.previous_state = self.current_state\n        self.current_state = state\n        self.last_action = self._epsilon_greedy(state)\n        return self.last_action\n    \n    \n    def reset_internal_states(self):\n        self.previous_state = None\n        self.current_state = None\n        self.last_action = None\n        self.last_action_index = None\n          \n            \n    def save_pickle(self, name):\n        save_data = (self.actions,\n                     self.q_table,\n                     self.states,\n                     )\n        with open(f'{name}', 'wb') as handle:\n            pickle.dump(save_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n            \n    def load_pickle(self, name):\n        with open(f'{name}', 'rb') as handle:\n            data = pickle.load(handle)\n            self.actions, self.q_table, self.states = data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hungry Geese - Basic Agent Template Class (greedy risk averse)\n\nI'm using the template class I've shared before as a starting point to avoid rewriting and keeping things clear on the qlearning agent (also it's my testing opponent).\n\nNOTE:\n* The code that writes the greedy-goose.py is the same with the adition of the singlenton method (check: https://www.kaggle.com/victordelafuente/hungry-geese-basic-agent-template-class). Here is hidden just for clarity."},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col, translate, adjacent_positions, min_distance\nimport random as rand\nfrom enum import Enum, auto\n\n\ndef opposite(action):\n    if action == Action.NORTH:\n        return Action.SOUTH\n    if action == Action.SOUTH:\n        return Action.NORTH\n    if action == Action.EAST:\n        return Action.WEST\n    if action == Action.WEST:\n        return Action.EAST\n    raise TypeError(str(action) + \" is not a valid Action.\")\n\n    \n\n#Enconding of cell content to build states from observations\nclass CellState(Enum):\n    EMPTY = 0\n    FOOD = auto()\n    GOOSE = auto()\n\n\n#This class encapsulates mos of the low level Hugry Geese stuff    \nclass BornToNotMedalv2:    \n    def __init__(self):\n        self.DEBUG=True\n        self.rows, self.columns = -1, -1        \n        self.my_index = -1\n        self.my_head, self.my_tail = -1, -1\n        self.geese = []\n        self.heads = []\n        self.tails = []\n        self.food = []\n        self.cell_states = []\n        self.actions = [action for action in Action]\n        self.previous_action = None\n        self.step = 1\n\n        \n    def _adjacent_positions(self, position):\n        return adjacent_positions(position, self.columns, self.rows)\n \n\n    def _min_distance_to_food(self, position, food=None):\n        food = food if food!=None else self.food\n        return min_distance(position, food, self.columns)\n\n    \n    def _row_col(self, position):\n        return row_col(position, self.columns)\n    \n    \n    def _translate(self, position, direction):\n        return translate(position, direction, self.columns, self.rows)\n        \n        \n    def preprocess_env(self, observation, configuration):\n        observation = Observation(observation)\n        configuration = Configuration(configuration)\n        \n        self.rows, self.columns = configuration.rows, configuration.columns        \n        self.my_index = observation.index\n        self.hunger_rate = configuration.hunger_rate\n        self.min_food = configuration.min_food\n\n        self.my_head, self.my_tail = observation.geese[self.my_index][0], observation.geese[self.my_index][-1]        \n        self.my_body = [pos for pos in observation.geese[self.my_index][1:-1]]\n\n        \n        self.geese = [g for i,g in enumerate(observation.geese) if i!=self.my_index  and len(g) > 0]\n        self.geese_cells = [pos for g in self.geese for pos in g if len(g) > 0]\n        \n        self.occupied = [p for p in self.geese_cells]\n        self.occupied.extend([p for p in observation.geese[self.my_index]])\n        \n        \n        self.heads = [g[0] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 0]\n        self.bodies = [pos  for i,g in enumerate(observation.geese) for pos in g[1:-1] if i!=self.my_index and len(g) > 2]\n        self.tails = [g[-1] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 1]\n        self.food = [f for f in observation.food]\n        \n        self.adjacent_to_heads = [pos for head in self.heads for pos in self._adjacent_positions(head)]\n        self.adjacent_to_bodies = [pos for body in self.bodies for pos in self._adjacent_positions(body)]\n        self.adjacent_to_tails = [pos for tail in self.tails for pos in self._adjacent_positions(tail)]\n        self.adjacent_to_geese = self.adjacent_to_heads + self.adjacent_to_bodies\n        self.danger_zone = self.adjacent_to_geese\n        \n        #Cell occupation\n        self.cell_states = [CellState.EMPTY.value for _ in range(self.rows*self.columns)]\n        for g in self.geese:\n            for pos in g:\n                self.cell_states[pos] = CellState.GOOSE.value\n        for pos in self.heads:\n                self.cell_states[pos] = CellState.GOOSE.value\n        for pos in self.my_body:\n            self.cell_states[pos] = CellState.GOOSE.value\n                \n        #detect dead-ends\n        self.dead_ends = []\n        for pos_i,_ in enumerate(self.cell_states):\n            if self.cell_states[pos_i] != CellState.EMPTY.value:\n                continue\n            adjacent = self._adjacent_positions(pos_i)\n            adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n            num_blocked = sum(adjacent_states)\n            if num_blocked>=(CellState.GOOSE.value*3):\n                self.dead_ends.append(pos_i)\n        \n        #check for extended dead-ends\n        new_dead_ends = [pos for pos in self.dead_ends]\n        while new_dead_ends!=[]:\n            for pos in new_dead_ends:\n                self.cell_states[pos]=CellState.GOOSE.value\n                self.dead_ends.append(pos)\n            \n            new_dead_ends = []\n            for pos_i,_ in enumerate(self.cell_states):\n                if self.cell_states[pos_i] != CellState.EMPTY.value:\n                    continue\n                adjacent = self._adjacent_positions(pos_i)\n                adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n                num_blocked = sum(adjacent_states)\n                if num_blocked>=(CellState.GOOSE.value*3):\n                    new_dead_ends.append(pos_i)                                    \n        \n                \n    def strategy_random(self, observation, configuration):\n        if self.previous_action!=None:\n            action = rand.choice([action for action in Action if action!=opposite(self.previous_action)])\n        else:\n            action = rand.choice([action for action in Action])\n        self.previous_action = action\n        return action.name\n                        \n                        \n    def safe_position(self, future_position):\n        return (future_position not in self.occupied) and (future_position not in self.adjacent_to_heads) and (future_position not in self.dead_ends)\n    \n    \n    def valid_position(self, future_position):\n        return (future_position not in self.occupied) and (future_position not in self.dead_ends)    \n\n    \n    def free_position(self, future_position):\n        return (future_position not in self.occupied) \n    \n                        \n    def strategy_random_avoid_collision(self, observation, configuration):\n        dead_end_cell = False\n        free_cell = True\n        actions = [action \n                   for action in Action \n                   for future_position in [self._translate(self.my_head, action)]\n                   if self.valid_position(future_position)] \n        if self.previous_action!=None:\n            actions = [action for action in actions if action!=opposite(self.previous_action)] \n        if actions==[]:\n            dead_end_cell = True\n            actions = [action \n                       for action in Action \n                       for future_position in [self._translate(self.my_head, action)]\n                       if self.free_position(future_position)]\n            if self.previous_action!=None:\n                actions = [action for action in actions if action!=opposite(self.previous_action)] \n            #no alternatives\n            if actions==[]:\n                free_cell = False\n                actions = self.actions if self.previous_action==None else [action for action in self.actions if action!=opposite(self.previous_action)] \n\n        action = rand.choice(actions)\n        self.previous_action = action\n        if self.DEBUG:\n            aux_pos = self._row_col(self._translate(self.my_head, self.previous_action))\n            dead_ends = \"\" if not dead_end_cell else f', dead_ends={[self._row_col(p1) for p1 in self.dead_ends]}, occupied={[self._row_col(p2) for p2 in self.occupied]}'\n            if free_cell:\n                print(f'{id(self)}({self.step}): Random_ac_move {action.name} to {aux_pos} dead_end={dead_end_cell}{dead_ends}', flush=True)\n            else:\n                print(f'{id(self)}({self.step}): Random_ac_move {action.name} to {aux_pos} free_cell={free_cell}', flush=True)\n        return action.name\n    \n    \n    def strategy_greedy_avoid_risk(self, observation, configuration):        \n        actions = {  \n            action: self._min_distance_to_food(future_position)\n            for action in Action \n            for future_position in [self._translate(self.my_head, action)]\n            if self.safe_position(future_position)\n        }\n  \n        if self.previous_action!=None:\n            actions.pop(opposite(self.previous_action), None)\n        if any(actions):\n            action = min(actions.items(), key=lambda x: x[1])[0]\n            self.previous_action = action\n            if self.DEBUG:\n                aux_pos = self._row_col(self._translate(self.my_head, self.previous_action))\n                print(f'{id(self)}({self.step}): Greedy_ar_move {action.name} to {aux_pos}', flush=True)\n            self.previous_action = action\n            return action.name\n        else:\n            return self.strategy_random_avoid_collision(observation, configuration)\n    \n    \n    #Redefine this method\n    def agent_strategy(self, observation, configuration):\n        action = self.strategy_greedy_avoid_risk(observation, configuration)\n        return action\n    \n    \n    def agent_do(self, observation, configuration):\n        self.preprocess_env(observation, configuration)\n        move = self.agent_strategy(observation, configuration)\n        self.step += 1\n        #if self.DEBUG:\n        #    aux_pos = self._translate(self.my_head, self.previous_action), self._row_col(self._translate(self.my_head, self.previous_action))\n        #    print(f'{id(self)}({self.step}): Move {move} to {aux_pos} internal_vars->{vars(self)}', flush=True)\n        return move","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%writefile greedy-goose.py\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col, translate, adjacent_positions, min_distance\nimport random as rand\nfrom enum import Enum, auto\n\n\ndef opposite(action):\n    if action == Action.NORTH:\n        return Action.SOUTH\n    if action == Action.SOUTH:\n        return Action.NORTH\n    if action == Action.EAST:\n        return Action.WEST\n    if action == Action.WEST:\n        return Action.EAST\n    raise TypeError(str(action) + \" is not a valid Action.\")\n\n    \n\n#Enconding of cell content to build states from observations\nclass CellState(Enum):\n    EMPTY = 0\n    FOOD = auto()\n    GOOSE = auto()\n\n\n#This class encapsulates mos of the low level Hugry Geese stuff    \nclass BornToNotMedalv2:    \n    def __init__(self):\n        self.DEBUG=True\n        self.rows, self.columns = -1, -1        \n        self.my_index = -1\n        self.my_head, self.my_tail = -1, -1\n        self.geese = []\n        self.heads = []\n        self.tails = []\n        self.food = []\n        self.cell_states = []\n        self.actions = [action for action in Action]\n        self.previous_action = None\n        self.step = 1\n\n        \n    def _adjacent_positions(self, position):\n        return adjacent_positions(position, self.columns, self.rows)\n \n\n    def _min_distance_to_food(self, position, food=None):\n        food = food if food!=None else self.food\n        return min_distance(position, food, self.columns)\n\n    \n    def _row_col(self, position):\n        return row_col(position, self.columns)\n    \n    \n    def _translate(self, position, direction):\n        return translate(position, direction, self.columns, self.rows)\n        \n        \n    def preprocess_env(self, observation, configuration):\n        observation = Observation(observation)\n        configuration = Configuration(configuration)\n        \n        self.rows, self.columns = configuration.rows, configuration.columns        \n        self.my_index = observation.index\n        self.hunger_rate = configuration.hunger_rate\n        self.min_food = configuration.min_food\n\n        self.my_head, self.my_tail = observation.geese[self.my_index][0], observation.geese[self.my_index][-1]        \n        self.my_body = [pos for pos in observation.geese[self.my_index][1:-1]]\n\n        \n        self.geese = [g for i,g in enumerate(observation.geese) if i!=self.my_index  and len(g) > 0]\n        self.geese_cells = [pos for g in self.geese for pos in g if len(g) > 0]\n        \n        self.occupied = [p for p in self.geese_cells]\n        self.occupied.extend([p for p in observation.geese[self.my_index]])\n        \n        \n        self.heads = [g[0] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 0]\n        self.bodies = [pos  for i,g in enumerate(observation.geese) for pos in g[1:-1] if i!=self.my_index and len(g) > 2]\n        self.tails = [g[-1] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 1]\n        self.food = [f for f in observation.food]\n        \n        self.adjacent_to_heads = [pos for head in self.heads for pos in self._adjacent_positions(head)]\n        self.adjacent_to_bodies = [pos for body in self.bodies for pos in self._adjacent_positions(body)]\n        self.adjacent_to_tails = [pos for tail in self.tails for pos in self._adjacent_positions(tail)]\n        self.adjacent_to_geese = self.adjacent_to_heads + self.adjacent_to_bodies\n        self.danger_zone = self.adjacent_to_geese\n        \n        #Cell occupation\n        self.cell_states = [CellState.EMPTY.value for _ in range(self.rows*self.columns)]\n        for g in self.geese:\n            for pos in g:\n                self.cell_states[pos] = CellState.GOOSE.value\n        for pos in self.heads:\n                self.cell_states[pos] = CellState.GOOSE.value\n        for pos in self.my_body:\n            self.cell_states[pos] = CellState.GOOSE.value\n                \n        #detect dead-ends\n        self.dead_ends = []\n        for pos_i,_ in enumerate(self.cell_states):\n            if self.cell_states[pos_i] != CellState.EMPTY.value:\n                continue\n            adjacent = self._adjacent_positions(pos_i)\n            adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n            num_blocked = sum(adjacent_states)\n            if num_blocked>=(CellState.GOOSE.value*3):\n                self.dead_ends.append(pos_i)\n        \n        #check for extended dead-ends\n        new_dead_ends = [pos for pos in self.dead_ends]\n        while new_dead_ends!=[]:\n            for pos in new_dead_ends:\n                self.cell_states[pos]=CellState.GOOSE.value\n                self.dead_ends.append(pos)\n            \n            new_dead_ends = []\n            for pos_i,_ in enumerate(self.cell_states):\n                if self.cell_states[pos_i] != CellState.EMPTY.value:\n                    continue\n                adjacent = self._adjacent_positions(pos_i)\n                adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n                num_blocked = sum(adjacent_states)\n                if num_blocked>=(CellState.GOOSE.value*3):\n                    new_dead_ends.append(pos_i)                                    \n        \n                \n    def strategy_random(self, observation, configuration):\n        if self.previous_action!=None:\n            action = rand.choice([action for action in Action if action!=opposite(self.previous_action)])\n        else:\n            action = rand.choice([action for action in Action])\n        self.previous_action = action\n        return action.name\n                        \n                        \n    def safe_position(self, future_position):\n        return (future_position not in self.occupied) and (future_position not in self.adjacent_to_heads) and (future_position not in self.dead_ends)\n    \n    \n    def valid_position(self, future_position):\n        return (future_position not in self.occupied) and (future_position not in self.dead_ends)    \n\n    \n    def free_position(self, future_position):\n        return (future_position not in self.occupied) \n    \n                        \n    def strategy_random_avoid_collision(self, observation, configuration):\n        dead_end_cell = False\n        free_cell = True\n        actions = [action \n                   for action in Action \n                   for future_position in [self._translate(self.my_head, action)]\n                   if self.valid_position(future_position)] \n        if self.previous_action!=None:\n            actions = [action for action in actions if action!=opposite(self.previous_action)] \n        if actions==[]:\n            dead_end_cell = True\n            actions = [action \n                       for action in Action \n                       for future_position in [self._translate(self.my_head, action)]\n                       if self.free_position(future_position)]\n            if self.previous_action!=None:\n                actions = [action for action in actions if action!=opposite(self.previous_action)] \n            #no alternatives\n            if actions==[]:\n                free_cell = False\n                actions = self.actions if self.previous_action==None else [action for action in self.actions if action!=opposite(self.previous_action)] \n\n        action = rand.choice(actions)\n        self.previous_action = action\n        if self.DEBUG:\n            aux_pos = self._row_col(self._translate(self.my_head, self.previous_action))\n            dead_ends = \"\" if not dead_end_cell else f', dead_ends={[self._row_col(p1) for p1 in self.dead_ends]}, occupied={[self._row_col(p2) for p2 in self.occupied]}'\n            if free_cell:\n                print(f'{id(self)}({self.step}): Random_ac_move {action.name} to {aux_pos} dead_end={dead_end_cell}{dead_ends}', flush=True)\n            else:\n                print(f'{id(self)}({self.step}): Random_ac_move {action.name} to {aux_pos} free_cell={free_cell}', flush=True)\n        return action.name\n    \n    \n    def strategy_greedy_avoid_risk(self, observation, configuration):        \n        actions = {  \n            action: self._min_distance_to_food(future_position)\n            for action in Action \n            for future_position in [self._translate(self.my_head, action)]\n            if self.safe_position(future_position)\n        }\n  \n        if self.previous_action!=None:\n            actions.pop(opposite(self.previous_action), None)\n        if any(actions):\n            action = min(actions.items(), key=lambda x: x[1])[0]\n            self.previous_action = action\n            if self.DEBUG:\n                aux_pos = self._row_col(self._translate(self.my_head, self.previous_action))\n                print(f'{id(self)}({self.step}): Greedy_ar_move {action.name} to {aux_pos}', flush=True)\n            self.previous_action = action\n            return action.name\n        else:\n            return self.strategy_random_avoid_collision(observation, configuration)\n    \n    \n    #Redefine this method\n    def agent_strategy(self, observation, configuration):\n        action = self.strategy_greedy_avoid_risk(observation, configuration)\n        return action\n    \n    \n    def agent_do(self, observation, configuration):\n        self.preprocess_env(observation, configuration)\n        move = self.agent_strategy(observation, configuration)\n        self.step += 1\n        #if self.DEBUG:\n        #    aux_pos = self._translate(self.my_head, self.previous_action), self._row_col(self._translate(self.my_head, self.previous_action))\n        #    print(f'{id(self)}({self.step}): Move {move} to {aux_pos} internal_vars->{vars(self)}', flush=True)\n        return move\n\n    \n    \ndef agent_singleton(observation, configuration):\n    global gus    \n    \n    try:\n        gus\n    except NameError:\n        gus = BornToNotMedalv2()\n            \n    action = gus.agent_do(observation, configuration)\n\n    \n    return action","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Our Agent class\n\nMost of the work is done, all that's left is conecting both base classes, creating the state-space, defining transitions and rewards\n\n\n### State definition\n\nOur state will be build from a window of cells centered around our agent head \"seeing\" POV_DISTANCE=2 cells in each direction. This square has total side (POV_DISTANCE*2)+1.\n\nWe code the contents of the cells using TileState class and use join to build the current state string by concatenating. E.g. with POV_DISTANCE=1:\n* The state string = \"000020000\"\n    * Comes from the following cell contents (agents sees only its head)\n        * 000\n        * 020\n        * 000\n* The state string = \"010020020\"\n    * Comes from the following cell contents (agents is looking upward an there's food NORTH)\n        * 010\n        * 020\n        * 020\n        \n* NOTE: Here we make no diffenrentiation between my body and other agent bodies (states are the minimum to avoid exponential growth of search space).\n\n\n\n### Positive Rewards\n\n* Natural positive reward for eating food (growing)\n* Postive reward for going towards food (to help guide the learning \"eating is good\")\n\n### Common Sense knowledge rewards (penalties)\n\nAfter the first tests I've decided to add somo common sense knowledge to help guide the learning.\n\n* Colliding is bad, on move that crashes against geese process a reward of -1 and choose other random, non-colliding move\n* Same for forbidden moves\n* Same for avoiding dead-end cells (3 adjacent cells are \"blocked\")"},{"metadata":{"trusted":true},"cell_type":"code","source":"from enum import Enum, auto\nimport pickle\n\n#Enconding of cell content to build states from observations\nclass CellState(Enum):\n    EMPTY = 0\n    FOOD = auto()\n    GOOSE = auto()\n    #search space gets too big too fast... so just 3 cell states\n    #HEAD = auto()\n    #BODY = auto()\n    #TAIL = auto()\n\n    \n#This is our Q-Learning Hungry Geese Agent\nclass QGoose(BornToNotMedalv2, QLearner):    \n    def __init__(self):\n        self.POV_DISTANCE=3\n        BornToNotMedalv2.__init__(self)\n        QLearner.__init__(self, self.actions, initial_value=0.01, alpha=0.1, gamma=.8, epsilon=0.1)\n        self.world = None\n        self.previous_length = 0\n        self.last_min_distance_to_food = self.rows*self.columns #initial max value to mark no food seen so far\n       \n    \n    def title_state_from_row_col(self, row, col):\n        pos = self.columns*row+col-1\n        if pos in self.heads or pos==self.my_head:\n            return CellState.GOOSE\n        elif pos in self.bodies or pos==self.my_body:\n            return CellState.GOOSE\n        elif pos in self.tails or pos==self.my_tail:\n            return CellState.GOOSE\n        elif pos in self.food:\n            return CellState.FOOD\n        else:\n            return CellState.EMPTY\n    \n    \n    def state_from_world(self):\n        state = []\n        row_0, col_0 = self._row_col(self.my_head)\n        for col_delta in range (-self.POV_DISTANCE, self.POV_DISTANCE):\n            for row_delta in range (-self.POV_DISTANCE, self.POV_DISTANCE):\n                row_i = (row_0+row_delta)%self.rows\n                col_i = (col_0+col_delta)%self.columns\n                state.append(self.title_state_from_row_col(row_i, col_i))\n        state = \"\".join([str(s.value) for s in state])\n        return state\n    \n    \n    def common_sense_after_move_choosen(self, action):\n        future_position = self._translate(self.my_head, action)\n\n        if future_position in self.occupied:\n            return -10 \n        elif self.previous_action==opposite(self.last_action): #opposite is currently a patch until Action.opposite works...\n            return -10\n        elif self.previous_action in self.dead_ends:\n            return -1\n        else:\n            min_distance_to_food = self._min_distance_to_food(future_position)\n            aux_last = self.last_min_distance_to_food\n            self.last_min_distance_to_food=min_distance_to_food\n            \n            if min_distance_to_food<aux_last:\n                return 1\n            else:\n                return 0\n    \n    \n    def agent_strategy(self, observation, configuration):\n        state = self.state_from_world()\n        \n        #Process reward for growing\n        reward = len(self.my_body)+2*self.step #Geese really like pizza!!! 8-)\n        self.previous_length = len(self.my_body)\n        self.process_reward(reward)\n        \n        #Choose action\n        action = self.epsilon_greedy_choose_action(state)\n        \n        #Apply some common sense like colliding is bad... ;-)\n        cs_reward = self.common_sense_after_move_choosen(action)\n        if cs_reward<0:\n            #update q-table\n            self.process_reward(reward, previous_state=state, last_action=action, last_action_index=self.actions.index(action))\n\n            #choose new greedy risk averse valid action\n            random_action = self.strategy_greedy_avoid_risk(observation, configuration)                                   \n            #update internal action attributes\n            aux = [(action,index) for index,action in enumerate(Action) if action.name==random_action][0]\n            self.last_action = aux[0]\n            self.last_action_index = aux[1]\n            action = self.last_action\n        print(f'q-agent q_table{self.q_table}', flush=True)\n        \n        self.previous_action = action    \n        return Action(action).name","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now all together\n\nWe write all clases to 'qgoose.py' (see file for full source code).\n\nTo learn between different matches here on the notebook I just read a pickle at the start of the game (if it exists) and save back to disk after each update (not very efficient, I know). See \"agent_singleton\" function at the end."},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"%%writefile qgoose.py\n\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col, translate, adjacent_positions, min_distance\nimport random as rand\nfrom enum import Enum, auto\nimport pickle\nimport os\n\n\n\ndef opposite(action):\n    if action == Action.NORTH:\n        return Action.SOUTH\n    if action == Action.SOUTH:\n        return Action.NORTH\n    if action == Action.EAST:\n        return Action.WEST\n    if action == Action.WEST:\n        return Action.EAST\n    raise TypeError(str(action) + \" is not a valid Action.\")\n    \n\n    \n#Enconding of cell content to build states from observations\nclass CellState(Enum):\n    EMPTY = 0\n    FOOD = auto()\n    GOOSE = auto()\n    #search space gets too big too fast... so just 3 cell states\n    #HEAD = auto()\n    #BODY = auto()\n    #TAIL = auto()\n    \n    \n\n#This class encapsulates a simple qlearning with epsilon-greedy policy, the states and transitions can be defined automatically as we explore (search space is too big to initialize all at once and many states won't be achievable)\nclass QLearner():\n    def __init__(self, actions, states=None, initial_value=0.1, alpha=0.3, gamma=0.1, epsilon=0.9, create_states_on_exploration=True):\n        self.actions = actions\n        self.create_states_on_exploration = create_states_on_exploration\n        self.initial_value = initial_value\n        if states!=None:\n            self.q_table = {\n                state: [initial_value for _ in self.actions] for state in states\n            }\n            self.states = states\n        else:\n            self.q_table = dict()\n            self.states = []\n            \n        self.alpha = alpha\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.previous_state = None\n        self.current_state = None\n        self.last_action = None\n        self.last_action_index = None\n\n        \n    def _check_auto_init_state(self, state):\n        if (state!=None) and (state not in self.q_table.keys()) and self.create_states_on_exploration:\n            self.q_table[state] = [self.initial_value for _ in self.actions]\n            self.states.append(state)\n    \n\n    def _epsilon_greedy(self, state):\n        #create state if needed\n        self._check_auto_init_state(state)\n        \n        if (rand.random() < self.epsilon):\n            action = rand.choice(self.actions)\n            self.last_action_index = self.actions.index(action)\n        else:\n            q_state = self.q_table[state]\n            max_val = max(q_state)\n            self.last_action_index = rand.choice([i for i,v in enumerate(q_state) if v==max_val])\n            action = self.actions[self.last_action_index]\n        return action\n\n    \n    def process_reward(self, reward, previous_state=None, last_action=None, last_action_index=None):\n        if previous_state==None:\n            previous_state = self.previous_state\n        if last_action==None:\n            last_action = self.last_action\n        if last_action_index==None:\n            last_action_index = self.last_action_index\n            \n        if (previous_state==None) or (last_action==None):\n            return\n        \n        #create state if needed\n        self._check_auto_init_state(previous_state)\n        \n        q = self.q_table\n        q_old = q[previous_state][last_action_index]\n        next_state = self.current_state\n        if next_state!=None:        \n            best_scenario = q[next_state].index(max(q[next_state]))\n            q[previous_state][last_action_index] = q_old + self.alpha * (reward + self.gamma * best_scenario - q_old)\n        else:\n            q[previous_state][last_action_index] = q_old + self.alpha * (reward + self.initial_value - q_old)\n\n            \n    def epsilon_greedy_choose_action(self, state):\n        self.previous_state = self.current_state\n        self.current_state = state\n        self.last_action = self._epsilon_greedy(state)\n        return self.last_action\n    \n    \n    def reset_internal_states(self):\n        self.previous_state = None\n        self.current_state = None\n        self.last_action = None\n        self.last_action_index = None\n          \n            \n    def save_pickle(self, name):\n        save_data = (self.actions,\n                     self.q_table,\n                     self.states,\n                     )\n        with open(f'{name}', 'wb') as handle:\n            pickle.dump(save_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n            \n    def load_pickle(self, name):\n        with open(f'{name}', 'rb') as handle:\n            data = pickle.load(handle)\n            self.actions, self.q_table, self.states = data\n            \n            \n\n#This class encapsulates mos of the low level Hugry Geese stuff    \n#This class encapsulates mos of the low level Hugry Geese stuff    \nclass BornToNotMedalv2:    \n    def __init__(self):\n        self.DEBUG=True\n        self.rows, self.columns = -1, -1        \n        self.my_index = -1\n        self.my_head, self.my_tail = -1, -1\n        self.geese = []\n        self.heads = []\n        self.tails = []\n        self.food = []\n        self.cell_states = []\n        self.actions = [action for action in Action]\n        self.previous_action = None\n        self.step = 1\n\n        \n    def _adjacent_positions(self, position):\n        return adjacent_positions(position, self.columns, self.rows)\n \n\n    def _min_distance_to_food(self, position, food=None):\n        food = food if food!=None else self.food\n        return min_distance(position, food, self.columns)\n\n    \n    def _row_col(self, position):\n        return row_col(position, self.columns)\n    \n    \n    def _translate(self, position, direction):\n        return translate(position, direction, self.columns, self.rows)\n        \n        \n    def preprocess_env(self, observation, configuration):\n        observation = Observation(observation)\n        configuration = Configuration(configuration)\n        \n        self.rows, self.columns = configuration.rows, configuration.columns        \n        self.my_index = observation.index\n        self.hunger_rate = configuration.hunger_rate\n        self.min_food = configuration.min_food\n\n        self.my_head, self.my_tail = observation.geese[self.my_index][0], observation.geese[self.my_index][-1]        \n        self.my_body = [pos for pos in observation.geese[self.my_index][1:-1]]\n\n        \n        self.geese = [g for i,g in enumerate(observation.geese) if i!=self.my_index  and len(g) > 0]\n        self.geese_cells = [pos for g in self.geese for pos in g if len(g) > 0]\n        \n        self.occupied = [p for p in self.geese_cells]\n        self.occupied.extend([p for p in observation.geese[self.my_index]])\n        \n        \n        self.heads = [g[0] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 0]\n        self.bodies = [pos  for i,g in enumerate(observation.geese) for pos in g[1:-1] if i!=self.my_index and len(g) > 2]\n        self.tails = [g[-1] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 1]\n        self.food = [f for f in observation.food]\n        \n        self.adjacent_to_heads = [pos for head in self.heads for pos in self._adjacent_positions(head)]\n        self.adjacent_to_bodies = [pos for body in self.bodies for pos in self._adjacent_positions(body)]\n        self.adjacent_to_tails = [pos for tail in self.tails for pos in self._adjacent_positions(tail)]\n        self.adjacent_to_geese = self.adjacent_to_heads + self.adjacent_to_bodies\n        self.danger_zone = self.adjacent_to_geese\n        \n        #Cell occupation\n        self.cell_states = [CellState.EMPTY.value for _ in range(self.rows*self.columns)]\n        for g in self.geese:\n            for pos in g:\n                self.cell_states[pos] = CellState.GOOSE.value\n        for pos in self.heads:\n                self.cell_states[pos] = CellState.GOOSE.value\n        for pos in self.my_body:\n            self.cell_states[pos] = CellState.GOOSE.value\n                \n        #detect dead-ends\n        self.dead_ends = []\n        for pos_i,_ in enumerate(self.cell_states):\n            if self.cell_states[pos_i] != CellState.EMPTY.value:\n                continue\n            adjacent = self._adjacent_positions(pos_i)\n            adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n            num_blocked = sum(adjacent_states)\n            if num_blocked>=(CellState.GOOSE.value*3):\n                self.dead_ends.append(pos_i)\n        \n        #check for extended dead-ends\n        new_dead_ends = [pos for pos in self.dead_ends]\n        while new_dead_ends!=[]:\n            for pos in new_dead_ends:\n                self.cell_states[pos]=CellState.GOOSE.value\n                self.dead_ends.append(pos)\n            \n            new_dead_ends = []\n            for pos_i,_ in enumerate(self.cell_states):\n                if self.cell_states[pos_i] != CellState.EMPTY.value:\n                    continue\n                adjacent = self._adjacent_positions(pos_i)\n                adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n                num_blocked = sum(adjacent_states)\n                if num_blocked>=(CellState.GOOSE.value*3):\n                    new_dead_ends.append(pos_i)                                    \n        \n                \n    def strategy_random(self, observation, configuration):\n        if self.previous_action!=None:\n            action = rand.choice([action for action in Action if action!=opposite(self.previous_action)])\n        else:\n            action = rand.choice([action for action in Action])\n        self.previous_action = action\n        return action.name\n                        \n                        \n    def safe_position(self, future_position):\n        return (future_position not in self.occupied) and (future_position not in self.adjacent_to_heads) and (future_position not in self.dead_ends)\n    \n    \n    def valid_position(self, future_position):\n        return (future_position not in self.occupied) and (future_position not in self.dead_ends)    \n\n    \n    def free_position(self, future_position):\n        return (future_position not in self.occupied) \n    \n                        \n    def strategy_random_avoid_collision(self, observation, configuration):\n        dead_end_cell = False\n        free_cell = True\n        actions = [action \n                   for action in Action \n                   for future_position in [self._translate(self.my_head, action)]\n                   if self.valid_position(future_position)] \n        if self.previous_action!=None:\n            actions = [action for action in actions if action!=opposite(self.previous_action)] \n        if actions==[]:\n            dead_end_cell = True\n            actions = [action \n                       for action in Action \n                       for future_position in [self._translate(self.my_head, action)]\n                       if self.free_position(future_position)]\n            if self.previous_action!=None:\n                actions = [action for action in actions if action!=opposite(self.previous_action)] \n            #no alternatives\n            if actions==[]:\n                free_cell = False\n                actions = self.actions if self.previous_action==None else [action for action in self.actions if action!=opposite(self.previous_action)] \n\n        action = rand.choice(actions)\n        self.previous_action = action\n        if self.DEBUG:\n            aux_pos = self._row_col(self._translate(self.my_head, self.previous_action))\n            dead_ends = \"\" if not dead_end_cell else f', dead_ends={[self._row_col(p1) for p1 in self.dead_ends]}, occupied={[self._row_col(p2) for p2 in self.occupied]}'\n            if free_cell:\n                print(f'{id(self)}({self.step}): Random_ac_move {action.name} to {aux_pos} dead_end={dead_end_cell}{dead_ends}', flush=True)\n            else:\n                print(f'{id(self)}({self.step}): Random_ac_move {action.name} to {aux_pos} free_cell={free_cell}', flush=True)\n        return action.name\n    \n    \n    def strategy_greedy_avoid_risk(self, observation, configuration):        \n        actions = {  \n            action: self._min_distance_to_food(future_position)\n            for action in Action \n            for future_position in [self._translate(self.my_head, action)]\n            if self.safe_position(future_position)\n        }\n  \n        if self.previous_action!=None:\n            actions.pop(opposite(self.previous_action), None)\n        if any(actions):\n            action = min(actions.items(), key=lambda x: x[1])[0]\n            self.previous_action = action\n            if self.DEBUG:\n                aux_pos = self._row_col(self._translate(self.my_head, self.previous_action))\n                print(f'{id(self)}({self.step}): Greedy_ar_move {action.name} to {aux_pos}', flush=True)\n            self.previous_action = action\n            return action.name\n        else:\n            return self.strategy_random_avoid_collision(observation, configuration)\n    \n    \n    #Redefine this method\n    def agent_strategy(self, observation, configuration):\n        action = self.strategy_greedy_avoid_risk(observation, configuration)\n        return action\n    \n    \n    def agent_do(self, observation, configuration):\n        self.preprocess_env(observation, configuration)\n        move = self.agent_strategy(observation, configuration)\n        self.step += 1\n        #if self.DEBUG:\n        #    aux_pos = self._translate(self.my_head, self.previous_action), self._row_col(self._translate(self.my_head, self.previous_action))\n        #    print(f'{id(self)}({self.step}): Move {move} to {aux_pos} internal_vars->{vars(self)}', flush=True)\n        return move\n\n\n        \n#This is our Q-Learning Hungry Geese Agent\nclass QGoose(BornToNotMedalv2, QLearner):    \n    def __init__(self):\n        self.POV_DISTANCE=3\n        BornToNotMedalv2.__init__(self)\n        QLearner.__init__(self, self.actions, initial_value=0.01, alpha=0.1, gamma=.8, epsilon=0.1)\n        self.world = None\n        self.previous_length = 0\n        self.last_min_distance_to_food = self.rows*self.columns #initial max value to mark no food seen so far\n       \n    \n    def title_state_from_row_col(self, row, col):\n        pos = self.columns*row+col-1\n        if pos in self.heads or pos==self.my_head:\n            return CellState.GOOSE\n        elif pos in self.bodies or pos==self.my_body:\n            return CellState.GOOSE\n        elif pos in self.tails or pos==self.my_tail:\n            return CellState.GOOSE\n        elif pos in self.food:\n            return CellState.FOOD\n        else:\n            return CellState.EMPTY\n    \n    \n    def state_from_world(self):\n        state = []\n        row_0, col_0 = self._row_col(self.my_head)\n        for col_delta in range (-self.POV_DISTANCE, self.POV_DISTANCE):\n            for row_delta in range (-self.POV_DISTANCE, self.POV_DISTANCE):\n                row_i = (row_0+row_delta)%self.rows\n                col_i = (col_0+col_delta)%self.columns\n                state.append(self.title_state_from_row_col(row_i, col_i))\n        state = \"\".join([str(s.value) for s in state])\n        return state\n    \n    \n    def common_sense_after_move_choosen(self, action):\n        future_position = self._translate(self.my_head, action)\n\n        if future_position in self.occupied:\n            return -10 \n        elif self.previous_action==opposite(self.last_action): #opposite is currently a patch until Action.opposite works...\n            return -10\n        elif self.previous_action in self.dead_ends:\n            return -1\n        else:\n            min_distance_to_food = self._min_distance_to_food(future_position)\n            aux_last = self.last_min_distance_to_food\n            self.last_min_distance_to_food=min_distance_to_food\n            \n            if min_distance_to_food<aux_last:\n                return 1\n            else:\n                return 0\n    \n    \n    def agent_strategy(self, observation, configuration):\n        state = self.state_from_world()\n        \n        #Process reward for growing\n        reward = len(self.my_body)+2*self.step #Geese really like pizza!!! 8-)\n        self.previous_length = len(self.my_body)\n        self.process_reward(reward)\n        \n        #Choose action\n        action = self.epsilon_greedy_choose_action(state)\n        \n        #Apply some common sense like colliding is bad... ;-)\n        cs_reward = self.common_sense_after_move_choosen(action)\n        if cs_reward<0:\n            #update q-table\n            self.process_reward(reward, previous_state=state, last_action=action, last_action_index=self.actions.index(action))\n\n            #choose new greedy risk averse valid action\n            random_action = self.strategy_greedy_avoid_risk(observation, configuration)                                   \n            #update internal action attributes\n            aux = [(action,index) for index,action in enumerate(Action) if action.name==random_action][0]\n            self.last_action = aux[0]\n            self.last_action_index = aux[1]\n            action = self.last_action\n        print(f'q-agent q_table{self.q_table}', flush=True)\n        \n        self.previous_action = action    \n        return Action(action).name\n\n\n        \ndef agent_singleton(observation, configuration):\n    global gus    \n    saved=\"qgoose.pickle\"\n    \n    try:\n        gus\n    except NameError:\n        gus = QGoose()\n        if os.path.isfile(saved) and os.stat(saved).st_size>0:\n            if gus.DEBUG:\n                print(\"Loading agent, q-table...\")\n            gus.load_pickle(saved)\n            if gus.DEBUG:\n                print(\"Loaded!\")\n        elif gus.DEBUG:\n            print(\"No previous trained QTable found!\")\n            \n    action = gus.agent_do(observation, configuration)\n    #print(\"Saving QGoose pickle!!!\", saved)\n    gus.save_pickle(saved)\n    \n    return action","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now just to check it's working. \n\nLet the agent play some games and try to learn against greedy, this takes a while... (results not displayed)"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import kaggle_environments\nfrom kaggle_environments import make\nfrom tqdm import tqdm\n\nGAMES = 2000\nUSE_TQDM = False\n\nenv = make(\"hungry_geese\", debug=False)\ngames = tqdm(range(GAMES)) if USE_TQDM else range(GAMES)\nfor i in games:\n    env.run([\n        \"qgoose.py\",\n        \"greedy-goose.py\", \n        \"greedy-goose.py\", \n        \"greedy-goose.py\", \n    ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And see what the agent does now... **Has it learned anything?**\n\nLet's see results vs greedy-risk-averse goose:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import evaluate\n\nNUM_EPISODES = 10\nevaluate(\"hungry_geese\",\n    [\n        \"qgoose.py\",\n        \"greedy-goose.py\", \n        \"greedy-goose.py\", \n        \"greedy-goose.py\", \n    ],\n    num_episodes=NUM_EPISODES,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results aren't SO bad (ok, yeah they are...), maybe keep working! xD\n\n### **EDIT:** \n* After improving base template class, fine tuning a bit more gamma and switching to greedy risk averse fallback on the q-goose results improved a bit :-)"},{"metadata":{"trusted":true},"cell_type":"code","source":"%cp qgoose.py main.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!tar cvzf submission.tar.gz main.py qgoose.pickle ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Current conclusions\n\nOk, so far the greedy agent with hard coded rules works better and doesn't need any training time...\n\n* The q-learning agent needs to find by itself more common sense knowledge that the other agent has beforehand (move towards food, ...)\n* Already added some common sense: collision and opposite direction moves negative rewards, collision avoidance\n* Sometimes greedy just ends crashing with itself\n* Q-Agent mostly randomly browses :-(\n\n### What's next?\n* Adding information about closest food and/or agents to the state definitions?\n* Stocastic estimation from current transition to set of \"next states seen from current\"\n* Tuning hyperparameters\n* Choosing other technique? xD\n\n\n### Changelog\n* v52: updated with latest version of greedy risk-averse & dead-end avoid (my base template class)\n* v46: added reward for getting closer to food and modified the one given when the agent grows\n* v42: improved negative reward on occupied cells, also added improvements from template base class v31\n* v40: \n    * On invalid movement now the q-goose uses greedy risk averse strategy instead of random avoid collision\n    * Updated to last version of the improved base template (see link below)\n* Previous versions (no particular order):\n    * From base template class (See:https://www.kaggle.com/victordelafuente/hungry-geese-template-class-greedy-risk-averse )\n        * Improved fallback strategies and occupancy accounting\n        * Fixed couple of bugs to improve default greedy risk averse strategy\n        * Basic dead-end cell detection\n    * Higher value of gamma helped getting higher scores seems to work better for this size of search space? (maybe agent needs more time to achieve a direct reward so we take more seriously future reward)\n\n\n### To watch a game\n\nCurrently doens't save (timeout error on notebook save inside Kaggle) but you can ran the notebook yourself (just uncomment the following block of code)."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n\nfrom kaggle_environments import make\nenv = make(\"hungry_geese\", debug=False)\nenv.run(\n    [\n        \"qgoose.py\", \n        \"greedy-goose.py\",\n    ],  \n)\nenv.render(mode=\"ipython\", width=800, height=700) #takes too long and hangs firefox many times inside nootebook, no idea why... :-(\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you need to \"reset\" training while on notebook uncomment the following"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%script bash\n#rm qgoose.pickle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Thanks for checking this out!*"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}