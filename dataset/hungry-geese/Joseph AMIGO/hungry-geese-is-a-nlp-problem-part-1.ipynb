{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# The board is a sentence\nHello everyone, for several months I have been convinced that Hungry Geese is actually a sentence ... yes, a sentence with words. To be more precise, the board is a sentence composed of 77 words in 77 different positions. And what better way to model the sentences than with transformers!\n\n# Leveraging kaggle free TPU (overkilling it?)\nNow let me prove to you that it works. We are going to proceed by the simplest imitation learning possible. Attached to this notebook is a dataset of leaderboard games. In this notebook we will preprocess them. I used TPU with tensorflow to train my imitation learning model with transformers, so it is necessary to adopt a particular preprocessing to make it compatible with TPU.","metadata":{}},{"cell_type":"code","source":"!cd ../input/hungry-geese-mvp-iml-ds && ls | wc -l","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nimport json\nfrom tqdm.auto import tqdm\n\npath = \"../input/hungry-geese-mvp-iml-ds/*[!_info].json\"\ngames = []\ni = 1\nfor path_to_json in tqdm(glob.glob(path)):\n    with open(path_to_json, 'r') as json_file:\n        data = json_file.read()        \n        games.append(json.loads(data)[\"steps\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def outcome(last_obs):\n    # return terminal outcomes\n    # 1st: 1.0 2nd: 0.33 3rd: -0.33 4th: -1.00\n    rewards = {o['observation']['index']: o['reward'] for o in last_obs}\n    outcomes = {p: 0 for p in range(4)}\n    for p, r in rewards.items():\n        if r is None:\n            r = np.NINF\n        for pp, rr in rewards.items():\n            if rr is None:\n                rr = np.NINF\n            if p != pp:\n                if r > rr:\n                    outcomes[p] += 1 / (4 - 1)\n                elif r < rr:\n                    outcomes[p] -= 1 / (4 - 1)\n    return outcomes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Action\ndef preprocess_map_obs(obs, previous_obs=None, p=None):\n    if p is None:\n        p = 0\n    \n    #Thank you very much yuricat for your suggestion to use relative positions.\n    #This is really what made the model work!\n    relativ_center = obs[0]['observation']['geese'][p][0]\n    relativ_poss = np.roll(np.arange(77), relativ_center)\n    \n    sentence = []\n    positions = []\n    # Our vocabulary size is 50 :\n    # index 0 is for an empty tile\n    # then we have the vocabulary ranges for the first player : index 1 to 12 (included)\n    # for the second, third and fourth players (13 to 24, 25 to 36, 37 to 48)\n    # and index 49 is for food\n    # we then divide each player index range into 3 sub ranges\n    # which correspond to the body part\n    # for player one we have 1 to 4, 5 to 8 and 9 to 12\n    # and finally we divise these sub ranges into 4 sub sub ranges\n    # which correspond to the last action played\n    for pp, player in enumerate(obs):\n        real_player_index = pp\n        player_index = (pp - p) % 4\n        geese_length = len(obs[0]['observation']['geese'][real_player_index])\n        for goose_body_position, goose_board_position in enumerate(obs[0]['observation']['geese'][real_player_index]):\n            if goose_body_position == 0:\n                body_part = 0\n            elif goose_body_position == (geese_length-1):\n                body_part = 2\n            else:\n                body_part = 1\n\n            last_action = Action[obs[real_player_index]['action']].value\n\n            index_player = 3*4 * player_index\n            index_bodypart = 4*body_part\n            \n            word_unique_index = index_player + index_bodypart + last_action + 1\n            sentence.append(word_unique_index)\n            \n            position = relativ_poss[goose_board_position]\n            positions.append(position)\n            \n\n    for food_board_position in obs[0]['observation']['food']:\n        word_unique_index = 47 + 1 + 1\n        sentence.append(word_unique_index)\n        \n        position = relativ_poss[food_board_position]\n        positions.append(position)\n        \n    left_positions = set(range(0,77))-set(positions)\n    \n    positions = positions + list(left_positions)\n    sentence = sentence + [0]*(77-len(sentence))\n        \n    return sentence, positions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n# Converting the values into features\n# _int64 is used for numeric values\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\ndef _int64_list_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"action_encoder_dict = {\n    \"NORTH\": 0,\n    \"SOUTH\": 1,\n    \"WEST\": 2,\n    \"EAST\": 3\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import bz2\nimport pickle\n\nfile_nb = 0\n# Writing the serialized example.\nrecord_file = f'df_{file_nb}.tfrec'\nwriter = tf.io.TFRecordWriter(record_file)\nidx = 0\nfor steps in tqdm(games):\n    previous_step = None\n    winners = list(np.argsort(list(outcome(steps[-1]).values()))[3:])\n    for step_idx, step in enumerate(steps): \n        if step_idx == len(steps)-1:\n            continue;\n        \n        for p, player in enumerate(step):        \n            if not player['observation']['index'] in winners:\n                continue;\n            \n            if not player[\"status\"] == \"ACTIVE\":\n                continue;\n            \n            action = steps[step_idx+1][p]['action']\n            \n            if action is None:\n                continue;\n            \n            if (idx+1)%100000 == 0:\n                writer.close()\n                file_nb += 1\n                # Writing the serialized example.\n                record_file = f'df_{file_nb}.tfrec'\n                writer = tf.io.TFRecordWriter(record_file)\n                print(f\"incrementing file number to {file_nb}\")\n            \n            label = action_encoder_dict[action]\n            sentence, positions = preprocess_map_obs(step, previous_step, p)\n            \n            feature = {\n                'label': _int64_feature(label),\n                'sentence': _int64_list_feature(sentence),\n                'positions': _int64_list_feature(positions),\n            }\n            example = tf.train.Example(features=tf.train.Features(feature=feature))\n            writer.write(example.SerializeToString())\n            \n            idx += 1\n            \n        previous_step = step\nwriter.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -lh *","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final words regarding preprocessing\nWe need to \"Save & Run All (Commit)\" with option \"Always save output\" and then go to the commited version, then to the output and create a dataset out of it.\nWe can either create a private dataset or a public. It is important to keep in mind wether the dataset is a public or a private one, because when using TPU the ways to load the dataset on the TPU are not the same depending the two configurations.","metadata":{}}]}