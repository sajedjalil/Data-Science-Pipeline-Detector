{"cells":[{"metadata":{},"cell_type":"markdown","source":"* Learn Q function by DQN with Ape-X (3 explorers + 1 learner)\n  * Encode board\n    * Player position is always center\n    * Player length is set at center position\n    * Around enemy heads are marked as RISK\n    * Geese tails are marked as RISK (When the goose eat food, then the tail will stay there)\n  * Each explorer has 2 NN actors and 2 rule based greedy actors\n* Rule based Safe Guard to avoid Body Hit and Collision\n* Submit multiple files (code + model file) under this instruction https://www.kaggle.com/c/google-football/discussion/191257"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -U kaggle_environments cpprb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nfrom multiprocessing import set_start_method, cpu_count, Process, Event, SimpleQueue\nimport time\n\nimport numpy as np\nimport tensorflow as tf\nimport cpprb # Replay Buffer Library: https://ymd_h.gitlab.io/cpprb/\nfrom tqdm.notebook import tqdm\n\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\nfrom kaggle_environments import make\n\n# %load_ext tensorboard\n# %tensorboard --logdir logs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Global config\n#RIGHT = 0\n#GO = 1\n#LEFT = 2\n\nGOOSE = -1.0\nRISK = GOOSE/2 # Half of GOOSE (= -0.5)\nNONE = 0.0\nFOOD = 1.0\n\nact_shape = 4\n\nWIDTH = 11\nHEIGHT = 7\n\nxc = WIDTH//2 + 1\nyc = HEIGHT//2 + 1\n\nEAST_idx  = (xc+1,yc  )\nNORTH_idx = (xc  ,yc-1)\nWEST_idx  = (xc-1,yc  )\nSOUTH_idx = (xc  ,yc+1)\n\nAROUND = ([xc+1,xc  ,xc-1,xc  ],\n          [yc  ,yc-1,yc  ,yc+1])\n\ncode2dir = {0:'EAST', 1:'NORTH', 2:'WEST', 3:'SOUTH'}\n\ndir2code = {\"EAST\":0, \"NORTH\": 1, \"WEST\":2, \"SOUTH\": 3}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    model = tf.keras.Sequential([tf.keras.layers.Dense(100,activation=\"relu\",input_shape=(WIDTH*HEIGHT,)),\n                                 tf.keras.layers.Dense(100,activation=\"relu\"),\n                                 tf.keras.layers.Dense(100,activation=\"relu\"),\n                                 tf.keras.layers.Dense(act_shape)])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Q_func(model,obs,act):\n    return tf.reduce_sum(model(obs) * tf.one_hot(act,depth=act_shape), axis=1)\n\ndef Q1_func(model,next_obs,rew,done):\n    gamma = 0.99\n    return gamma*tf.reduce_max(model(next_obs),axis=1)*(1.0-done) + rew\n\n#@tf.function\ndef train_then_absTD(model,target,obs,act,rew,next_obs,done,weights):\n    with tf.GradientTape() as tape:\n        tape.watch(model.trainable_weights)\n        Q = Q_func(model,obs,act)\n        yQ1_r = Q1_func(target,next_obs,rew,done)\n        TD_square = tf.square(Q - yQ1_r)\n        weighted_loss = tf.reduce_mean(TD_square * weights)\n\n    grad = tape.gradient(weighted_loss,model.trainable_weights)\n    opt.apply_gradients(zip(grad,model.trainable_weights))\n\n    Qnew = Q_func(model,obs,act)\n    return tf.abs(Qnew - yQ1_r)\n\n#@tf.function\ndef abs_TD(model,target,obs,act,rew,next_obs,done):\n    Q = Q_func(model,obs,act)\n    yQ1_r = Q1_func(target,next_obs,rew,done)\n    return tf.abs(Q - yQ1_r)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pos(index):\n    return index%WIDTH, index//WIDTH\n\ndef centering(z,dz,Z):\n    z += dz\n    if z < 0:\n        z += Z\n    elif Z >= Z:\n        z -= Z\n    return z\n    \n\ndef encode_board(obs,act=\"NORTH\",idx=0):\n    \"\"\"\n    Player goose is always set at the center\n    \"\"\"\n    board = np.zeros((WIDTH,HEIGHT))\n\n    if len(obs[\"geese\"][idx]) == 0:\n        return board\n        \n    x0, y0 = pos(obs[\"geese\"][idx][0])\n    dx = xc - x0\n    dy = yc - y0\n    \n    for goose in obs[\"geese\"]:\n        if len(goose) == 0:\n            continue\n\n        for g in goose[:-1]:\n            x, y = pos(g)\n            x = centering(x,dx,WIDTH)\n            y = centering(y,dy,HEIGHT)       \n            board[x,y] = GOOSE\n        \n        # Tail as Risk\n        x, y = pos(goose[-1])\n        x = centering(x,dx,WIDTH)\n        y = centering(y,dy,HEIGHT)\n        board[x,y] = RISK\n        \n\n    for food in obs[\"food\"]:\n        x, y = pos(food)\n        x = centering(x,dx,WIDTH)\n        y = centering(y,dy,HEIGHT)\n        board[x,y] = FOOD\n\n    # Set RISK for around enemy geese head\n    for i, goose in enumerate(obs[\"geese\"]):\n        if (i == idx) or (len(goose) == 0):\n            continue\n        x, y = pos(goose[0])\n        if (y < HEIGHT-1) and (board[x,y+1] != GOOSE):\n            board[x,y+1] += RISK\n        if (y > 0) and (board[x,y-1] != GOOSE):\n            board[x,y-1] += RISK\n        if (x < WIDTH-1) and (board[x+1,y] != GOOSE):\n            board[x+1,y] += RISK\n        if (x > 0) and (board[x-1,y] != GOOSE):\n            board[x-1,y] += RISK\n        \n    board[xc,yc] = len(obs[\"geese\"][idx]) # self length\n\n    # Avoid Body Hit add psudo GOOSE\n    if act == \"EAST\":\n        board[WEST_idx] = GOOSE\n    elif act == \"NORTH\":\n        board[SOUTH_idx] = GOOSE\n    elif act == \"WEST\":\n        board[EAST_idx] = GOOSE\n    elif act == \"SOUTH\":\n        board[NORTH_idx] = GOOSE\n    else:\n        raise\n    \n    return board","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_obs_action(model,states,idx=0, train=False):\n    act = states[idx][\"action\"]\n\n    if states[idx][\"status\"] != \"ACTIVE\":\n        return None, act\n    \n    board = encode_board(states[0][\"observation\"],act=act,idx=idx)\n    \n    # e-greedy\n    if train:\n        if np.random.random() < 0.1:\n            new_act = np.random.randint(4)\n        else:\n            new_act = int(tf.math.argmax(tf.squeeze(model(board.reshape(1,-1)))))    \n    else:\n        Q = tf.squeeze(model(board.reshape(1,-1))).numpy()\n        OK = (board[AROUND] != GOOSE)\n        \n        new_act = 0\n        max_v = -99999\n        for i, (q,ok) in enumerate(zip(Q,OK)):\n            if (q > max_v) and ok:\n                new_act = i\n                max_v = q\n\n    return board, code2dir[new_act]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_obs_action_greedy(states,idx=0):\n    act = states[idx][\"action\"]\n    \n    if states[idx][\"status\"] != \"ACTIVE\":\n        return None, act\n    \n    board = encode_board(states[0][\"observation\"],act=act,idx=idx)\n    \n    obs = states[0][\"observation\"]\n\n    if len(obs[\"geese\"][idx]) == 0 or len(obs[\"food\"]) == 0:\n        return board, act\n    \n    x0, y0 = pos(obs[\"geese\"][idx][0])\n    \n    min_len = WIDTH + HEIGHT\n    min_i = 0\n    NG = (board[AROUND] == GOOSE)\n    for i, food in enumerate(obs[\"food\"]):\n        x, y = pos(food)\n        \n        dx = x - x0\n        dy = y - y0\n        L = abs(dx) + abs(dy)\n        \n        if dx == 0:\n            if (dy > 0) and NG[dir2code[\"SOUTH\"]]:\n                L += 2\n            elif (dy < 0) and NG[dir2code[\"NORTH\"]]:\n                L += 2\n        if dy == 0:\n            if (dx > 0) and NG[dir2code[\"EAST\"]]:\n                L += 2\n            elif (dx < 0) and NG[dir2code[\"WEST\"]]:\n                L += 2\n            \n        if L < min_len:\n            min_len = L\n            min_i = i\n\n    food = obs[\"food\"][min_i]\n    x, y = pos(food)\n\n    if (x > x0):\n        return board, \"EAST\"\n    \n    if (x < x0):\n        return board, \"WEST\"\n    \n    if (y > y0):\n        return board, \"SOUTH\"\n    \n    if (y < y0):\n        return board, \"NORTH\"\n    \n    return board, act","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_buffer(buffer_size,env_dict,alpha):\n    return cpprb.MPPrioritizedReplayBuffer(buffer_size,env_dict,alpha=alpha)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def explorer(global_rb,env_dict,is_training_done,queue):\n    local_buffer_size = int(1e+2)\n    local_rb = cpprb.ReplayBuffer(local_buffer_size+4,env_dict)\n\n    model = create_model()\n    target = tf.keras.models.clone_model(model)\n    env = make(\"hungry_geese\", debug=False)\n    \n    states = env.reset(4)\n    while not is_training_done.is_set():\n        if not queue.empty():\n            w,wt = queue.get()\n            model.set_weights(w)\n            target.set_weights(wt)\n\n        board_act = [get_obs_action(model,states,i,train=True) if i < 2 else get_obs_action_greedy(states,i)\n                     for i in range(4)]\n\n        states = env.step([a for b,a in board_act])\n\n        for i, (b, a) in enumerate(board_act):\n            if b is None:\n                continue\n\n            local_rb.add(obs=b.ravel(),\n                         act=dir2code[a],\n                         next_obs=encode_board(states[0][\"observation\"],act=a,idx=i).ravel(),\n                         rew=states[i][\"reward\"],\n                         done=(states[i][\"status\"] != \"ACTIVE\"))\n\n        if all(s[\"status\"] != \"ACTIVE\" for s in states):\n            states = env.reset(4)\n            local_rb.on_episode_end()\n\n        if local_rb.get_stored_size() >= local_buffer_size:\n            sample = local_rb.get_all_transitions()\n            global_rb.add(**sample,\n                          priorities=abs_TD(model,target,\n                                            tf.constant(sample[\"obs\"]),\n                                            tf.constant(sample[\"act\"].ravel()),\n                                            tf.constant(sample[\"rew\"].ravel()),\n                                            tf.constant(sample[\"next_obs\"]),\n                                            tf.constant(sample[\"done\"].ravel())))\n            local_rb.clear()            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# Training\nn_warming = 100\nn_train_step = int(1e+4)\nbatch_size = 64\n\nwriter = tf.summary.create_file_writer(\"./logs\")\n\n# Replay Buffer \nbuffer_size = 10e+5\nenv_dict = {\"obs\": {\"shape\": (WIDTH*HEIGHT)},\n            \"act\": {\"dtype\": int},\n            \"next_obs\": {\"shape\": (WIDTH*HEIGHT)},\n            \"rew\": {},\n            \"done\": {}}\nalpha = 0.5\nrb = create_buffer(buffer_size, env_dict,alpha)\n\n# Model\ntarget_update = 50\n\n\nmodel = create_model()\ntarget = tf.keras.models.clone_model(model)\n\nopt = tf.keras.optimizers.Adam()\n\n# Ape-X\nexplorer_update_freq = 100\nn_explorer = cpu_count() - 1\n\n\nis_training_done = Event()\nis_training_done.clear()\n\nqs = [SimpleQueue() for _ in range(n_explorer)]\nps = [Process(target=explorer,\n              args=[rb,env_dict,is_training_done,q])\n      for q in qs]\n\nfor p in ps:\n    p.start()\n\nprint(\"warm-up\")\nwhile rb.get_stored_size() < n_warming:\n    time.sleep(1)\n\n\nprint(\"training\")\n    \nepoch = 0\nfor i in tqdm(range(n_train_step)):        \n    sample = rb.sample(batch_size,beta=0.4)\n    \n    absTD = train_then_absTD(model,target,\n                             tf.constant(sample[\"obs\"]),\n                             tf.constant(sample[\"act\"].ravel()),\n                             tf.constant(sample[\"rew\"].ravel()),\n                             tf.constant(sample[\"next_obs\"]),\n                             tf.constant(sample[\"done\"].ravel()),\n                             tf.constant(sample[\"weights\"].ravel()))\n    rb.update_priorities(sample[\"indexes\"],absTD)\n        \n    if i % target_update == 0:\n        target.set_weights(model.get_weights())\n        \n    if i % explorer_update_freq == 0:\n        w = model.get_weights()\n        wt = target.get_weights()\n        for q in qs:\n            q.put((w,wt))\n\n    \nis_training_done.set()\n\n!mkdir -p sub\nmodel.save(\"sub/model\")\n\nfor p in ps:\n    p.join()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_env = make(\"hungry_geese\", debug=True)\n\nfor _ in range(4):\n    states = test_env.reset(4)\n    while any(s[\"status\"] == \"ACTIVE\" for s in states):\n        board_act = [get_obs_action(model,states,i) for i in range(4)]\n        #board_act = [get_obs_action_greedy(states,i) for i in range(4)]\n        states = test_env.step([a for b,a in board_act])\n\n    test_env.render(mode='ipython')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile sub/main.py\n\nimport sys\nimport os\n\nsys.path.append(\"/kaggle_simulations/agent\")\nworking_dir = \"/kaggle_simulations/agent\"\n\nif os.path.exists(\"sub/model\"):\n    model_f = \"sub/model\"\nelif os.path.exists(os.path.join(working_dir,\"model\")):\n    model_f = os.path.join(working_dir,\"model\")\nelse:\n    raise ValueError(\"No model file\")\n    \nprint(model_f)\n\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\n\nGOOSE = -1.0\nRISK = GOOSE/2 # Half of GOOSE (= -0.5)\nNONE = 0.0\nFOOD = 1.0\n\nact_shape = 4\n\nWIDTH = 11\nHEIGHT = 7\n\nxc = WIDTH//2 + 1\nyc = HEIGHT//2 + 1\n\nEAST_idx  = (xc+1,yc  )\nNORTH_idx = (xc  ,yc-1)\nWEST_idx  = (xc-1,yc  )\nSOUTH_idx = (xc  ,yc+1)\n\n\nAROUND = ([xc+1,xc  ,xc-1,xc  ],\n          [yc  ,yc-1,yc  ,yc+1])\n\n\ncode2dir = {0:'EAST', 1:'NORTH', 2:'WEST', 3:'SOUTH'}\ndir2code = {\"EAST\":0, \"NORTH\": 1, \"WEST\":2, \"SOUTH\": 3}\n\n\npolicy = tf.keras.models.load_model(model_f)\nLAST_ACT = \"NORTH\"\n\ndef pos(index):\n    return index%WIDTH, index//WIDTH\n\ndef centering(z,dz,Z):\n    z += dz\n    if z < 0:\n        z += Z\n    elif Z >= Z:\n        z -= Z\n    return z\n    \n\ndef encode_board(obs,idx=0):\n    \"\"\"\n    Player goose is always set at the center\n    \"\"\"\n    global LAST_ACT\n    act = LAST_ACT\n\n    board = np.zeros((WIDTH,HEIGHT))\n\n    if len(obs[\"geese\"][idx]) == 0:\n        return board\n        \n    x0, y0 = pos(obs[\"geese\"][idx][0])\n    dx = xc - x0\n    dy = yc - y0\n    \n    for goose in obs[\"geese\"]:\n        if len(goose) == 0:\n            continue\n\n        for g in goose[:-1]:\n            x, y = pos(g)\n            x = centering(x,dx,WIDTH)\n            y = centering(y,dy,HEIGHT)\n            board[x,y] = GOOSE\n            \n        # Tail as Risk\n        x, y = pos(goose[-1])\n        x = centering(x,dx,WIDTH)\n        y = centering(y,dy,HEIGHT)\n        board[x,y] = RISK\n\n            \n    for food in obs[\"food\"]:\n        x, y = pos(food)\n        x = centering(x,dx,WIDTH)\n        y = centering(y,dy,HEIGHT)\n        board[x,y] = FOOD\n        \n    # Set RISK for around enemy geese head\n    for i, goose in enumerate(obs[\"geese\"]):\n        if (i == idx) or (len(goose) == 0):\n            continue\n        x, y = pos(goose[0])\n        if (y < HEIGHT-1) and (board[x,y+1] != GOOSE):\n            board[x,y+1] += RISK\n        if (y > 0) and (board[x,y-1] != GOOSE):\n            board[x,y-1] += RISK\n        if (x < WIDTH-1) and (board[x+1,y] != GOOSE):\n            board[x+1,y] += RISK\n        if (x > 0) and (board[x-1,y] != GOOSE):\n            board[x-1,y] += RISK\n        \n    board[xc,yc] = len(obs[\"geese\"][idx]) # self length\n\n    # Avoid Body Hit add psudo GOOSE\n    if act == \"EAST\":\n        board[WEST_idx] = GOOSE\n    elif act == \"NORTH\":\n        board[SOUTH_idx] = GOOSE\n    elif act == \"WEST\":\n        board[EAST_idx] = GOOSE\n    elif act == \"SOUTH\":\n        board[NORTH_idx] = GOOSE\n    else:\n        raise\n\n    return board\n\n\ndef get_action(obs_dict,config_dict):\n    global policy\n    global LAST_ACT\n    \n    idx = Observation(obs_dict).index\n    board = encode_board(obs_dict,idx)\n\n    Q = tf.squeeze(policy(board.reshape(1,-1))).numpy()\n    OK = (board[AROUND] != GOOSE)\n\n    new_act = 0\n    max_v = -99999\n    for i, (q,ok) in enumerate(zip(Q,OK)):\n        if (q > max_v) and ok:\n            new_act = i\n            max_v = q\n    \n    LAST_ACT = code2dir[new_act]\n\n    return LAST_ACT","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test with self\n\ntest_env.run([\"sub/main.py\",\"sub/main.py\",\"sub/main.py\",\"sub/main.py\"])\ntest_env.render(mode='ipython')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test with sample agent\n\ntest_env.run([\"sub/main.py\",\"sub/main.py\",\"../input/hungry-geese/agent.py\",\"../input/hungry-geese/agent.py\"])\ntest_env.render(mode='ipython')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tarfile\nimport os.path\n\ndef make_tarfile(output_filename, source_dir):\n    with tarfile.open(output_filename, \"w:gz\") as tar:\n        tar.add(source_dir, arcname=os.path.basename(source_dir))\n\nmake_tarfile('submission.tar.gz', './sub/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}