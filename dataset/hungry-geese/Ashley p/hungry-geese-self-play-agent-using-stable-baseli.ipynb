{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This kernel attempts to train PPO agent with self play using stable-baselines3 library.\n`EvalCallback` is specified to evaluate model every 20 timesteps. And if better model is formed, it is loaded as opponent using child callback `LoadNewOpponentsFromBestModelCallback`. This callback calls `HungryGeeseEnv.load_new_opponents_from_best_model` to actually load new model as opponents.\nCode works as is. However, there is scope for performance improvement. **Please help me out to improve performance and indicating what I might be missing here.**\n\n(Action and observation transformation functions are referenced from [this](https://www.kaggle.com/ryches/stable-baselines3-starter-wip) kernel.)\n\n**TODO**\n\n- Try increasing Policy NN layers\n\n\n","metadata":{}},{"cell_type":"markdown","source":"[Original NB](https://www.kaggle.com/maheshabnave999/hungry-geese-self-play-agent-using-stable-baseli#Custom-Environment)","metadata":{}},{"cell_type":"code","source":"!pip install stable-baselines3","metadata":{"execution":{"iopub.status.busy":"2021-06-18T14:49:11.516897Z","iopub.execute_input":"2021-06-18T14:49:11.517623Z","iopub.status.idle":"2021-06-18T14:49:20.874606Z","shell.execute_reply.started":"2021-06-18T14:49:11.517505Z","shell.execute_reply":"2021-06-18T14:49:20.873458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n# Imports ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport gym\nfrom gym import spaces\nimport numpy as np\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common import logger, results_plotter\nfrom stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback\n\nfrom shutil import copyfile\nimport os\n\nfrom stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv, VecTransposeImage\nfrom stable_baselines3.common.monitor import Monitor, load_results\n\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\nfrom kaggle_environments import evaluate, make","metadata":{"execution":{"iopub.status.busy":"2021-06-18T14:49:20.876597Z","iopub.execute_input":"2021-06-18T14:49:20.877045Z","iopub.status.idle":"2021-06-18T14:49:23.567571Z","shell.execute_reply.started":"2021-06-18T14:49:20.876997Z","shell.execute_reply":"2021-06-18T14:49:23.566574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EED = 17\nNUM_TIMESTEPS = int(1e7)\nEVAL_FREQ = int(1e4)\nEVAL_EPISODES = int(1e2)\nBEST_THRESHOLD = 0.01 # must achieve a mean score above this to replace prev best self\n\nREWARD_LOST = -1\nREWARD_WON = 1\n\nN_CPU = os.cpu_count()\n\nLOGDIR = os.path.join(\".\",\"logs\",\"custom_ppo_1\")\nMONITOR_LOGS_DIR = os.path.join(LOGDIR,\"monitor_logs\")\nTB_LOGS_DIR = os.path.join(LOGDIR,\"tensorboard_logs\")\nMODEL_DIR = os.path.join(LOGDIR,\"model\")\nCHECKPOINTS_DIR = os.path.join(LOGDIR,\"checkpoints\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T14:49:23.569695Z","iopub.execute_input":"2021-06-18T14:49:23.570034Z","iopub.status.idle":"2021-06-18T14:49:23.577194Z","shell.execute_reply.started":"2021-06-18T14:49:23.570001Z","shell.execute_reply":"2021-06-18T14:49:23.575666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not os.path.exists(LOGDIR): \n    os.makedirs(LOGDIR)\nif not os.path.exists(TB_LOGS_DIR):\n    os.makedirs(TB_LOGS_DIR)\nif not os.path.exists(MONITOR_LOGS_DIR):\n    os.makedirs(MONITOR_LOGS_DIR)\nif not os.path.exists(MODEL_DIR):\n    os.makedirs(MODEL_DIR)\nif not os.path.exists(CHECKPOINTS_DIR):\n    os.makedirs(CHECKPOINTS_DIR)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T14:49:23.579231Z","iopub.execute_input":"2021-06-18T14:49:23.580343Z","iopub.status.idle":"2021-06-18T14:49:23.59314Z","shell.execute_reply.started":"2021-06-18T14:49:23.580298Z","shell.execute_reply":"2021-06-18T14:49:23.591674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom Environment ","metadata":{"tags":[]}},{"cell_type":"code","source":"class HungryGeeseEnv(gym.Env):\n    \n    def __init__(self, opponents=['random','greedy','greedy-goose.py'], debug=False, warmup_episode_count = 100, warmup_timesteps=5000):\n        super(HungryGeeseEnv, self).__init__()\n        self.opponents = opponents\n        self.opponents_old_lengths = [1 for _ in range(0,len(opponents))]\n        self.opponents_new_lengths = [1 for _ in range(0,len(opponents))]\n        self.env = make(\"hungry_geese\") #, debug=self.debug)\n        self.config = self.env.configuration\n        self.trainer = self.env.train([None, *opponents])\n        \n        self.action_space = spaces.Discrete(4)        \n        self.observation_space = spaces.Box(low=0, high=255\n                                            , shape=(self.config.rows, self.config.columns, 3)\n                                            , dtype=np.uint8) \n        self.reward_range = (-1, 1000)  #TODO why this range?\n        self.last_vert_actions_count = 0\n        self.last_horz_actions_count = 0\n        self.last_action = -1\n        \n        self.episode_count = 0\n        self.timesteps = 0\n        self.warmup_episode_count = warmup_episode_count\n        self.warmup_timesteps = warmup_timesteps\n        \n    def update_opponent_lengths(self, obs):\n        self.opponents_old_lengths = self.opponents_new_lengths\n        self.opponents_new_lengths = [len(geese) for geese in obs[0]['geese'][1:]] \n\n    def update_last_actions(self, action):\n        if self.last_action == action:\n            if action == 0 or action == 3:\n                self.last_vert_actions_count += 1 #counts last consecutive vertical actions\n            else:\n                self.last_horz_actions_count += 1 #counts last consecutive horizontal actions\n        else:\n            if action == 0 or action == 3:\n                self.last_vert_actions_count = 1\n                self.last_horz_actions_count = 0\n            else:\n                self.last_horz_actions_count = 1\n                self.last_vert_actions_count = 0   \n\n    def shape_reward(self, reward):   \n        geese_length_diff = np.array(self.opponents_new_lengths) - np.array(self.opponents_old_lengths)\n        total_geese_length_increase = (geese_length_diff > 0).sum()\n        reward -= total_geese_length_increase * 10\n\n        # prevent agent from taking straight trajectory\n        if self.last_horz_actions_count > 11:\n            reward -= 10\n        if self.last_vert_actions_count > 7:\n            reward -= 10\n\n        return reward\n    \n    def step(self, action):\n        self.update_last_actions(action)\n        my_action = self.transform_action(action)\n        \n        self.timesteps += 1        \n        \n        #opponent_actions = self.transform_actions(action[1:])  #TODO \n        #self.obs = self.env.step([my_action, *opponent_actions])  #TODO      \n        self.obs = self.trainer.step(my_action)  #TODO    \n        self.update_opponent_lengths(self.obs)       \n        x_obs = self.transform_step_observation(self.obs, self.config)\n        # x_reward = self.obs[0].reward\n        # done = (self.obs[0][\"status\"] != \"ACTIVE\")\n        # info = self.obs[0][\"info\"]\n        x_reward = self.obs[1]\n        x_reward = self.shape_reward(x_reward)\n        done = self.obs[2]\n        info = self.obs[3]\n\n        return x_obs, x_reward, done, info\n        \n    def reset(self):\n        self.episode_count += 1\n        self.obs = self.trainer.reset()\n        x_obs = self.transform_observation(self.obs, self.config)\n        return x_obs\n    \n    def load_new_opponents_from_best_model(self):\n        if self.episode_count < self.warmup_episode_count or self.timesteps < self.warmup_timesteps:\n            return True\n        \n        print(\"Loading new opponents from current best model for self play!!!\")\n        loaded_model = PPO.load(os.path.join(MODEL_DIR, \"best_model\")) \n\n        def agent_ppo(obs, config):\n            obs = self.transform_observation(obs, self.config)\n            return self.transform_action(loaded_model.predict(obs, deterministic=True)[0])\n        \n        self.opponents = [agent_ppo]*len(self.opponents)\n        self.trainer = self.env.train([None, *self.opponents])\n        self.reset()\n        \n    def transform_actions(self, actions):\n        _actions = []\n        for action in actions:\n            _actions.append(self.transform_action(action))\n        return _actions\n        \n    def transform_action(self, action):\n        if action == 0:\n            return \"NORTH\"\n        if action == 1:\n            return \"EAST\"\n        if action == 2:\n            return \"WEST\"\n        if action == 3:\n            return \"SOUTH\"\n        \n    def transform_step_observation(self, obs, config):\n        my_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n        their_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n        food_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n\n        for goose_cell in obs[0].geese[0]:\n            my_board[goose_cell] = 255\n        my_board = my_board.reshape((config.rows, config.columns, 1))\n\n        for goose in obs[0].geese[1:]:\n            for goose_cell in goose:\n                their_board[goose_cell] = 255\n        their_board = their_board.reshape((config.rows, config.columns, 1))\n        \n        for food_cell in obs[0].food:\n            food_board[food_cell] = 255\n        food_board = food_board.reshape((config.rows, config.columns, 1))\n        board = np.concatenate([my_board, their_board, food_board], axis = -1)\n        return board\n\n    def transform_observation(self, obs, config):\n        my_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n        their_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n        food_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n\n        for goose_cell in obs.geese[0]:\n            my_board[goose_cell] = 255\n        my_board = my_board.reshape((config.rows, config.columns, 1))\n\n        for goose in obs.geese[1:]:\n            for goose_cell in goose:\n                their_board[goose_cell] = 255\n        their_board = their_board.reshape((config.rows, config.columns, 1))\n        \n        for food_cell in obs.food:\n            food_board[food_cell] = 255\n        food_board = food_board.reshape((config.rows, config.columns, 1))\n        board = np.concatenate([my_board, their_board, food_board], axis = -1)\n        return board","metadata":{"execution":{"iopub.status.busy":"2021-06-18T14:49:23.594719Z","iopub.execute_input":"2021-06-18T14:49:23.595161Z","iopub.status.idle":"2021-06-18T14:49:23.628012Z","shell.execute_reply.started":"2021-06-18T14:49:23.59513Z","shell.execute_reply":"2021-06-18T14:49:23.6272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Unused. Useful for vectorised environments \n\ndef make_monitored_gym(rank =0): #TODO pass config\n    def _init():\n        env = HungryGeeseEnv() #TODO pass config\n        #LOGDIR = \"ppo_selfplay\"\n        log_file = os.path.join(LOGDIR, str(rank))\n        env = Monitor(env, log_file, allow_early_resets=True) #TODO  allow_early_resets\n        return env\n    return _init\n\ndef make_gym(rank =0): #TODO pass config\n    def _init():\n        env = HungryGeeseEnv() #TODO pass config\n        #LOGDIR = \"ppo_selfplay\"\n        #log_file = os.path.join(LOGDIR, str(rank))\n        #env = Monitor(env, log_file, allow_early_resets=True) #TODO  allow_early_resets\n        return env\n    return _init","metadata":{"execution":{"iopub.status.busy":"2021-06-18T14:49:23.629007Z","iopub.execute_input":"2021-06-18T14:49:23.629412Z","iopub.status.idle":"2021-06-18T14:49:23.647881Z","shell.execute_reply.started":"2021-06-18T14:49:23.629384Z","shell.execute_reply":"2021-06-18T14:49:23.647157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = Monitor(HungryGeeseEnv())\n\nclass LoadNewOpponentsFromBestModelCallback(BaseCallback):\n    def __init__(self, env, verbose: int = 0):\n        super(LoadNewOpponentsFromBestModelCallback, self).__init__(verbose=verbose)\n        self.env = env\n\n    #def __init_callback(self)\n\n    def _on_step(self):\n        env.load_new_opponents_from_best_model()\n        return True\n    \nload_new_opponents_from_best_model_callback = LoadNewOpponentsFromBestModelCallback(env)\n\ncheckpoint_callback = CheckpointCallback(save_freq=1000, save_path=CHECKPOINTS_DIR,\n                                         name_prefix=\"rl_model\")\n\neval_env = VecTransposeImage(DummyVecEnv([lambda:Monitor(HungryGeeseEnv())]))\neval_callback = EvalCallback(eval_env, best_model_save_path=MODEL_DIR,\n                             log_path=LOGDIR, eval_freq=20,\n                             deterministic=True, render=False\n                            , callback_on_new_best=load_new_opponents_from_best_model_callback)\n\nmodel = PPO(policy = 'MlpPolicy'\n                , env = env\n                , verbose = 1\n                , n_steps = 2048*16\n                , batch_size = 128\n                , n_epochs = 50\n                #, tb_log_name = \"ppo_vs_ppo_bfs\" #TODO check if works\n                , tensorboard_log = TB_LOGS_DIR\n                , learning_rate = .01)\n\nmodel.learn(total_timesteps=100000, callback=[checkpoint_callback, eval_callback])","metadata":{"execution":{"iopub.status.busy":"2021-06-18T14:49:23.649092Z","iopub.execute_input":"2021-06-18T14:49:23.649513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_observation(obs, config):\n    my_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n    their_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n    food_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n\n    for goose_cell in obs.geese[0]:\n        my_board[goose_cell] = 255\n    my_board = my_board.reshape((config.rows, config.columns, 1))\n\n    for goose in obs.geese[1:]:\n        for goose_cell in goose:\n            their_board[goose_cell] = 255\n    their_board = their_board.reshape((config.rows, config.columns, 1))\n\n    for food_cell in obs.food:\n        food_board[food_cell] = 255\n    food_board = food_board.reshape((config.rows, config.columns, 1))\n    board = np.concatenate([my_board, their_board, food_board], axis = -1)\n    return board\n    \ndef transform_actions(actions):\n    if actions == 0:\n        return \"NORTH\"\n    if actions == 1:\n        return \"EAST\"\n    if actions == 2:\n        return \"WEST\"\n    if actions == 3:\n        return \"SOUTH\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL = os.path.join(LOGDIR,\"model\",\"best_model\")\nSAVE_MODEL = os.path.join(LOGDIR,\"model\",\"last_model\")\nSTATE_DICT = os.path.join(LOGDIR,\"state_dict\")\nloaded_model = PPO.load(MODEL)\nloaded_model.save(SAVE_MODEL)\nprint(loaded_model.policy)\nprint(loaded_model.policy.to('cpu').state_dict())\n\nimport torch\ntorch.save(loaded_model.policy.to('cpu').state_dict(), STATE_DICT)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL = os.path.join(LOGDIR,\"model\",\"best_model\")\nprint(MODEL)\nloaded_model = PPO.load(MODEL)\n#print(loaded_model.policy)\nenv = make('hungry_geese', debug=True)\n\ndef agent_ppo(obs, config):\n    obs = transform_observation(obs, env.configuration)\n    #return directions[loaded_model.predict(obs)[0]]\n    return transform_actions(loaded_model.predict(obs, deterministic=True)[0])\n    \n# env.run([agent_ppo,'random','greedy'])\n# env.render(mode=\"ipython\")\n\nfrom kaggle_environments import evaluate\n\nevaluate(\n    \"hungry_geese\",\n    ['random',agent_ppo,'greedy'],\n    num_episodes=10\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}