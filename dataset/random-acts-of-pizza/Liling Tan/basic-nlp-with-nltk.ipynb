{"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"mimetype":"text/x-python","pygments_lexer":"ipython3","version":"3.6.3","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","file_extension":".py"}},"cells":[{"metadata":{"_uuid":"4c26dad8c106fb2eacc6d703993b524774467445","_cell_guid":"258f18fc-e0b2-4a55-8927-51c46077017b"},"cell_type":"markdown","source":"Introduction\n====\n**Natural Language Processing ** (NLP) is the task of making computers understand and produce human languages. \n\nAnd it always starts with the **corpus** i.e. *a body of text*. \n"},{"metadata":{"_uuid":"93f8697704b4f7c9a900bd26c341862d1ef82f05","_cell_guid":"2762e23e-138d-4086-af46-53aa1d7d0bcd"},"cell_type":"markdown","source":"\nWhat is a Corpus?\n====\n\nThere are many corpora (*plural of corpus*) available in NLTK, lets start with an English one call the **Brown corpus**.\n\nWhen using a new corpus in NLTK for the first time, downloads the corpus with the `nltk.download()` function, e.g. \n\n```python\nimport nltk\nnltk.download('brown')\n```"},{"metadata":{"_uuid":"6a663051c176297596bc3174d10a1f8599247621","_cell_guid":"bd910c35-dc49-4c07-9441-9d16d175580a"},"cell_type":"markdown","source":"After its downloaded, you can import it as such:"},{"outputs":[],"metadata":{"_uuid":"637fad23e6bdfd6b8188a7c56318c91b6d3227e2","_cell_guid":"2f1d4c2c-ea22-4494-a650-cfddd3aba49c","collapsed":true},"cell_type":"code","execution_count":null,"source":"from nltk.corpus import brown"},{"outputs":[],"metadata":{"_uuid":"14c6fc8bb6bdffbf6fb397c9d2c5da53d12c1e69","_cell_guid":"121fd272-19fd-498f-a3e9-5295d09a2e16","collapsed":true},"cell_type":"code","execution_count":null,"source":"brown.words() # Returns a list of strings"},{"outputs":[],"metadata":{"_uuid":"2841227d2eeba7a7629ffa690647c43dc07bea7f","_cell_guid":"ebf89848-9442-4790-9575-a7f87234eebd","collapsed":true},"cell_type":"code","execution_count":null,"source":"len(brown.words()) # No. of words in the corpus"},{"outputs":[],"metadata":{"_uuid":"2de0781902fe8662d73bda9381cab9e318cdebf8","_cell_guid":"c3ba66d4-3e26-40fc-94d3-211581fa4c5c","collapsed":true},"cell_type":"code","execution_count":null,"source":"brown.sents() # Returns a list of list of strings "},{"outputs":[],"metadata":{"_uuid":"a17b7f140a86b0541acf2796b9146955acd64201","_cell_guid":"68725143-131d-4886-851c-f932c84588aa","collapsed":true},"cell_type":"code","execution_count":null,"source":"brown.sents(fileids='ca01') # You can access a specific file with `fileids` argument."},{"metadata":{"_uuid":"4e2791d121eb8162f2031df6c126bcaa40d812d7","_cell_guid":"972700b2-ee0b-4038-ba23-573287d08fd0"},"cell_type":"markdown","source":"\n**Fast Facts:**\n\n> The Brown Corpus of Standard American English was the first of the modern, computer readable, general corpora. It was compiled by W.N. Francis and H. Kucera, Brown University, Providence, RI. The corpus consists of one million words of American English texts printed in 1961.\n\n(Source: [University of Essex Corpus Linguistics site](  https://www1.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html))\n\n>  This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on ... (for a complete list, see http://icame.uib.no/brown/bcm-los.html).\n\n![](http://)(Source: [NLTK book, Chapter 2.1.3](http://www.nltk.org/book/ch02.html))"},{"metadata":{"_uuid":"abcb302ff5d44499870e06f7f86490cf077fea2b","_cell_guid":"1ecd0a80-5a12-444a-b9da-768ffef5133f"},"cell_type":"markdown","source":"The actual `brown` corpus data is **packaged as raw text files**.  And you can find their IDs with: "},{"outputs":[],"metadata":{"_uuid":"8c979bdf698b8f7975b1216083168de0317c8ac5","_cell_guid":"9bb09e9a-574f-4c3d-97eb-2091ce989cab","collapsed":true},"cell_type":"code","execution_count":null,"source":"len(brown.fileids()) # 500 sources, each file is a source."},{"outputs":[],"metadata":{"_uuid":"30e85f007a8812090f813d8212e43a834be3de29","_cell_guid":"fca762b7-5403-446f-82ca-e91e4cbd51dd","collapsed":true},"cell_type":"code","execution_count":null,"source":"print(brown.fileids()[:100]) # First 100 sources."},{"metadata":{"_uuid":"b6572ea60d0142090d77a44fabc98ee29943a4bb","_cell_guid":"74143a0e-40e2-4450-9ab4-427686f6ac87"},"cell_type":"markdown","source":"You can access the raw files with:"},{"outputs":[],"metadata":{"_uuid":"3f73483a3d332c4be88f54115d48beaa6351e090","_cell_guid":"9adcf194-d863-4043-85d0-00743c2786c9","collapsed":true},"cell_type":"code","execution_count":null,"source":"print(brown.raw('cb01').strip()[:1000]) # First 1000 characters."},{"metadata":{"_uuid":"f293a3c986c26e82394bd58a68bf2a0843b40f80","_cell_guid":"9c58248d-6e32-496f-843b-1a07e3d4afb8"},"cell_type":"markdown","source":"<br>\nYou will see that **each word comes with a slash and a label** and unlike normal text, we see that **punctuations are separated from the word that comes before it**, e.g. \n\n> The/at General/jj-tl Assembly/nn-tl ,/, which/wdt adjourns/vbz today/nr ,/, has/hvz performed/vbn in/in an/at atmosphere/nn of/in crisis/nn and/cc struggle/nn from/in the/at day/nn it/pps convened/vbd ./.\n\n<br>\nAnd we also see that the **each sentence is separated by a newline**:\n\n> There/ex followed/vbd the/at historic/jj appropriations/nns and/cc budget/nn fight/nn ,/, in/in which/wdt the/at General/jj-tl Assembly/nn-tl decided/vbd to/to tackle/vb executive/nn powers/nns ./.\n> \n> The/at final/jj decision/nn went/vbd to/in the/at executive/nn but/cc a/at way/nn has/hvz been/ben opened/vbn for/in strengthening/vbg budgeting/vbg procedures/nns and/cc to/to provide/vb legislators/nns information/nn they/ppss need/vb ./.\n\n<br>\nThat brings us to the next point on **sentence tokenization** and **word tokenization**."},{"metadata":{"_uuid":"2275a8deeace1a0080b4194d70a0e6f751026e2d","_cell_guid":"50b08633-6665-44e1-a989-e8b5fd3f11df"},"cell_type":"markdown","source":"Tokenization\n====\n\n**Sentence tokenization** is the process of  *splitting up strings into “sentences”*\n\n**Word tokenization** is the process of  *splitting up “sentences” into “words”*\n\nLets play around with some interesting texts,  the `singles.txt` from `webtext` corpus. <br>\nThey were some  **singles ads** from  http://search.classifieds.news.com.au/\n\nFirst, downoad the data with `nltk.download()`:\n\n```python\nnltk.download('webtext')\n```\n\nThen you can import with:"},{"outputs":[],"metadata":{"_uuid":"0a0c9f31e40f7ef9f3172aad7b68dbe3d4cc1829","_cell_guid":"3636768b-77ad-4f0a-922d-1cedd2a8b11a","collapsed":true},"cell_type":"code","execution_count":null,"source":"from nltk.corpus import webtext"},{"outputs":[],"metadata":{"_uuid":"f899b6ed306c96f7731f1acc51aaba4256ec59b7","_cell_guid":"6f9fd322-e916-4b70-83c3-87078c66fb15","collapsed":true},"cell_type":"code","execution_count":null,"source":"webtext.fileids()"},{"outputs":[],"metadata":{"_uuid":"89a84224c596cf23b3498a7d5f3dbbf74428ef9e","_cell_guid":"7518ac61-756f-466b-8294-ca6056994415","collapsed":true},"cell_type":"code","execution_count":null,"source":"# Each line is one advertisement.\nfor i, line in enumerate(webtext.raw('singles.txt').split('\\n')):\n    if i > 10: # Lets take a look at the first 10 ads.\n        break\n    print(str(i) + ':\\t' + line)"},{"metadata":{"_uuid":"c1086676ea2bd10932715c1dfc5ebca5f7765389","_cell_guid":"683d51eb-41e1-41bf-9bd2-3314d435f330"},"cell_type":"markdown","source":"# Lets zoom in on candidate no. 8"},{"outputs":[],"metadata":{"_uuid":"3f117b6fa1c9dc0150800cdbd632f4cee1c3fbde","_cell_guid":"7b46c556-583a-4af2-95d4-93d4d4b55bd2","collapsed":true},"cell_type":"code","execution_count":null,"source":"single_no8 = webtext.raw('singles.txt').split('\\n')[8]\nprint(single_no8)"},{"metadata":{"_uuid":"5d8daeb0c88fa79b07730d3d95d4f66382ebb0ef","_cell_guid":"f227e306-1fc8-4d0a-bebc-55014599b588"},"cell_type":"markdown","source":"# Sentence Tokenization\n<br>\nIn NLTK, `sent_tokenize()` the default tokenizer function that you can use to split strings into \"*sentences*\". \n<br>\n\nIt is using the [**Punkt algortihm** from Kiss and Strunk (2006)](http://www.mitpressjournals.org/doi/abs/10.1162/coli.2006.32.4.485)."},{"outputs":[],"metadata":{"_uuid":"a5430c319a9f9cc66f074405d24271fec49e77c5","_cell_guid":"c49b219d-864f-4353-9535-648592f0d847","collapsed":true},"cell_type":"code","execution_count":null,"source":"from nltk import sent_tokenize, word_tokenize"},{"outputs":[],"metadata":{"_uuid":"1d1a02d8bb0892942a4a5059a46b7f8aa23b7e01","_cell_guid":"0666bda0-ebec-4f2b-ab92-2158b70e38a2","collapsed":true},"cell_type":"code","execution_count":null,"source":"sent_tokenize(single_no8)"},{"outputs":[],"metadata":{"_uuid":"f1198990be58fd7d81cb6516651f98a2716824c3","_cell_guid":"17ab6433-7166-4e87-837a-74cde568f98f","collapsed":true},"cell_type":"code","execution_count":null,"source":"for sent in sent_tokenize(single_no8):\n    print(word_tokenize(sent))"},{"metadata":{"_uuid":"0ee17d9e697179e1db2bd8ef95cae55f3b5d64b3","_cell_guid":"e11cdead-66ae-4e25-9e68-9130297e0758"},"cell_type":"markdown","source":"# Lowercasing\n\nThe CAPS in the texts are RATHER irritating although we KNOW the guy is trying to EMPHASIZE on something ;P\n\nWe can simply **lowercase them after we do `sent_tokenize()` and `word_tokenize()`**. <br>\nThe tokenizers uses the capitalization as cues to know when to split so removing them before the calling the functions would be sub-optimal."},{"outputs":[],"metadata":{"_uuid":"197cae1396cc555d02eb64676471a564e1e7f35a","_cell_guid":"665cff0c-7140-444a-8ea9-ce3e20299464","collapsed":true},"cell_type":"code","execution_count":null,"source":"sent_tokenize(single_no8)"},{"outputs":[],"metadata":{"_uuid":"9f54f59d10281ff66ec36648cff7eda1f13f5ba2","_cell_guid":"42c2c2d9-cb6b-41d4-ac89-46440b13e94c","collapsed":true},"cell_type":"code","execution_count":null,"source":"for sent in sent_tokenize(single_no8):\n    # It's a little in efficient to loop through each word,\n    # after but sometimes it helps to get better tokens.\n    print([word.lower() for word in word_tokenize(sent)])\n    # Alternatively:\n    #print(list(map(str.lower, word_tokenize(sent))))"},{"outputs":[],"metadata":{"_uuid":"b90631ac206dc4a6495729b06bcd1fc6415134d7","_cell_guid":"08dea492-753c-4dba-8ac1-90c8dce48aef","collapsed":true},"cell_type":"code","execution_count":null,"source":"print(word_tokenize(single_no8))  # Treats the whole line as one document."},{"metadata":{"_uuid":"7e5a3cc059f151e0ce4f975e56bc98205c03e90f","_cell_guid":"eebfbac8-8b29-46ad-b2e2-e5badc926e4b"},"cell_type":"markdown","source":"# Tangential Note\n\nPunkt is a statistical model so it applies the knowledge it has learnt from previous data. <br>\nGenerally, it **works for most cases on well-formed texts** but if your data is  different e.g. user-generated noisy texts, you might have to retrain a new model. \n![](http://)\nE.g. if we look at candidate no. 9, we see that it's splitting on `y.o.` (its thinking that its the end of the sentnence) and not splitting on `&c.` (its thinking that its an abbreviation, e.g. `Mr.`, `Inc.`)."},{"outputs":[],"metadata":{"_uuid":"ef345b667df7113cba651ae8edbeb0e213ee6809","_cell_guid":"e3843142-7ff7-4e6d-8430-191ee8b4ce47","collapsed":true},"cell_type":"code","execution_count":null,"source":"single_no9 = webtext.raw('singles.txt').split('\\n')[9]\nsent_tokenize(single_no9)"},{"metadata":{"_uuid":"df75cb932724a29599f66c6229ba3f324a690181","_cell_guid":"3d1a2d29-ab44-44f7-803a-ac3561368f09"},"cell_type":"markdown","source":"Stopwords\n====\n\n**Stopwords** are non-content words that primarily has only grammatical function\n\nIn NLTK, you can access them as follows:"},{"outputs":[],"metadata":{"_uuid":"501da9982fc07b1deaae173ed816caa01ac2dd79","_cell_guid":"450cc60b-4e90-491f-9dd3-92ae22fd979a","collapsed":true},"cell_type":"code","execution_count":null,"source":"from nltk.corpus import stopwords\n\nstopwords_en = stopwords.words('english')\nprint(stopwords_en)"},{"metadata":{"_uuid":"e613de7ef925754b74334b79788acf4648b9be0b","_cell_guid":"e02fc6ff-713d-4090-b7e2-3132c9415549"},"cell_type":"markdown","source":"# Often we want to remove stopwords when we want to keep the \"gist\" of the document/sentence.\n\nFor instance, lets go back to the our `single_no8`"},{"outputs":[],"metadata":{"_uuid":"ceef2a1d450726b705b22698b5b35635c942be38","_cell_guid":"17a72a56-a94e-4d0b-af61-370d5f7b3adb","collapsed":true},"cell_type":"code","execution_count":null,"source":"# Treat the multiple sentences as one document (no need to sent_tokenize)\n# Tokenize and lowercase\nsingle_no8_tokenized_lowered = list(map(str.lower, word_tokenize(single_no8)))\nprint(single_no8_tokenized_lowered)"},{"metadata":{"_uuid":"7232e7aacc09962b8b010f6463ff14566471f541","_cell_guid":"ea203327-c900-4d54-a1cd-fb3d94a9f021"},"cell_type":"markdown","source":"# Let's try to remove the stopwords using the English stopwords list in NLTK"},{"outputs":[],"metadata":{"_uuid":"3b691c536a5487f33b92eceb94b34b8d12cac22f","_cell_guid":"b97fa9b8-6366-40b6-9b0f-05a15fc3b93f","collapsed":true},"cell_type":"code","execution_count":null,"source":"stopwords_en = set(stopwords.words('english')) # Set checking is faster in Python than list.\n\n# List comprehension.\nprint([word for word in single_no8_tokenized_lowered if word not in stopwords_en])"},{"metadata":{"_uuid":"d99001efcddcd7a080d76f6064ce309e41542f86","_cell_guid":"a6ebff46-17a3-47d0-bf1f-47461ee484a4","collapsed":true},"cell_type":"markdown","source":"# Often, we want to remove the punctuations from the documents too.\n\nSince Python comes with \"batteries included\", we have string.punctuation"},{"outputs":[],"metadata":{"_uuid":"991965258d7aeffa5f55420fdf4e6f4a83dbfd9e","_cell_guid":"fd82dfe3-2221-4c0a-9d0d-1b14a5349b91","collapsed":true},"cell_type":"code","execution_count":null,"source":"from string import punctuation\n# It's a string so we have to them into a set type\nprint('From string.punctuation:', type(punctuation), punctuation)"},{"metadata":{"_uuid":"78edf4a03bd59a7753ba5a0d9a238df439711516","_cell_guid":"c83a6e53-e7f4-43c9-a0b7-522ca1e75e55"},"cell_type":"markdown","source":"# Combining the punctuation with the stopwords from NLTK."},{"outputs":[],"metadata":{"_uuid":"97c96c2b0df9d8cd1dc6c6cf6619ed7f09c3072c","_cell_guid":"ffafed92-9c4c-4fc1-aa7b-89645edf4b8c","collapsed":true},"cell_type":"code","execution_count":null,"source":"stopwords_en_withpunct = stopwords_en.union(set(punctuation))\nprint(stopwords_en_withpunct)"},{"metadata":{"_uuid":"e6b6acafbca2e1c8f88fbda2313da9d9b4961cf4","_cell_guid":"230db9d8-bb06-4e48-aad9-4c4609448c4a"},"cell_type":"markdown","source":"# Removing stopwords with punctuations from Single no. 8"},{"outputs":[],"metadata":{"_uuid":"c37dbd0c8a7658dd20b2266276baf0e162204576","_cell_guid":"8cc6b68c-29b1-48b2-baa1-bd3f9bc4e21f","collapsed":true,"scrolled":true},"cell_type":"code","execution_count":null,"source":"print([word for word in single_no8_tokenized_lowered if word not in stopwords_en_withpunct])"},{"metadata":{"_uuid":"027cd197d81df7b34048c520a84270c74781a6f0","_cell_guid":"31f5c735-c260-4a4d-870e-721ccca5cb8b"},"cell_type":"markdown","source":"# Using a stronger/longer list of stopwords\n\nFrom the previous output, we have still dangly model verbs (i.e. 'could', 'wont', etc.).\n\nWe can combine the stopwords we have in NLTK with other stopwords list we find online.\n\nPersonally, I like to use `stopword-json` because it has stopwrds in 50 languages =) <br>\nhttps://github.com/6/stopwords-json"},{"outputs":[],"metadata":{"_uuid":"376d2a04068b557bc5080353a998aa52c199dccb","_cell_guid":"adf1ed39-3116-46f0-92c0-6dfddcd904a0","collapsed":true},"cell_type":"code","execution_count":null,"source":"# Stopwords from stopwords-json\nstopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\nstopwords_json_en = set(stopwords_json['en'])\nstopwords_nltk_en = set(stopwords.words('english'))\nstopwords_punct = set(punctuation)\n# Combine the stopwords. Its a lot longer so I'm not printing it out...\nstoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct)\n\n# Remove the stopwords from `single_no8`.\nprint('With combined stopwords:')\nprint([word for word in single_no8_tokenized_lowered if word not in stoplist_combined])"},{"metadata":{"_uuid":"84c1307d0bbc465925ef9932eddcbcdfd95a17ca","_cell_guid":"273f7eb4-77ff-4cb9-abf0-4254122fa2b7"},"cell_type":"markdown","source":"# Stemming and Lemmatization\n\nOften we want to map the different forms of the same word to the same root word, e.g. \"walks\", \"walking\", \"walked\" should all be the same as \"walk\".\n\nThe stemming and lemmatization process are hand-written regex rules written find the root word.\n\n - **Stemming**: Trying to shorten a word with simple regex rules\n\n - **Lemmatization**: Trying to find the root word with linguistics rules (with the use of regexes)\n\n(See also: [Stemmers vs Lemmatizers](https://stackoverflow.com/q/17317418/610569) question on StackOverflow)\n\nThere are various stemmers and one lemmatizer in NLTK, the most common being:\n\n - **Porter Stemmer** from [Porter (1980)](https://tartarus.org/martin/PorterStemmer/index.html)\n - **Wordnet Lemmatizer** (port of the Morphy: https://wordnet.princeton.edu/man/morphy.7WN.html)"},{"outputs":[],"metadata":{"_uuid":"3dd75f548e7363aa69b9da126a886f7299e052be","_cell_guid":"f41dab63-6133-4a51-9eb6-f87414b30c30","collapsed":true},"cell_type":"code","execution_count":null,"source":"from nltk.stem import PorterStemmer\nporter = PorterStemmer()\n\nfor word in ['walking', 'walks', 'walked']:\n    print(porter.stem(word))"},{"outputs":[],"metadata":{"_uuid":"31f49462e63529ac6d25b16389ef8494343b4de4","_cell_guid":"edf939e2-bc38-4c50-92db-fd8d958ad17d","collapsed":true},"cell_type":"code","execution_count":null,"source":"from nltk.stem import WordNetLemmatizer\nwnl = WordNetLemmatizer()\n\nfor word in ['walking', 'walks', 'walked']:\n    print(wnl.lemmatize(word))"},{"metadata":{"_uuid":"d2fa9f729a413a1417461683aa667f55a112e4e2","_cell_guid":"fe9ffb81-f53a-4c58-97b6-86e0d79c51ca"},"cell_type":"markdown","source":"# Gotcha! The lemmatizer is actually pretty complicated, it needs Parts of Speech (POS) tags.\n\n\nWe won't cover what's POS today so I'll just show you how to \"whip\" the lemmatizer to do what you need.\n\nBy default, the WordNetLemmatizer.lemmatize() function will assume that the word is a Noun if there's no explict POS tag in the input.\n\nFirst you need the pos_tag function to tag a sentence and using the tag convert it into WordNet tagsets and then put it through to the WordNetLemmatizer.\n\n**Note:** Lemmatization won't really work on single words alone without context or knowledge of its POS tag (i.e. we need to know whether the word is a noun, verb, adjective, adverb)\n"},{"outputs":[],"metadata":{"_uuid":"0813197200488bb07b8e23f5281ae324b323e981","_cell_guid":"0fa94379-f1b2-4228-b30c-001d2895b993","collapsed":true},"cell_type":"code","execution_count":null,"source":"from nltk import pos_tag\nfrom nltk.stem import WordNetLemmatizer\n\nwnl = WordNetLemmatizer()\n\ndef penn2morphy(penntag):\n    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n    morphy_tag = {'NN':'n', 'JJ':'a',\n                  'VB':'v', 'RB':'r'}\n    try:\n        return morphy_tag[penntag[:2]]\n    except:\n        return 'n' # if mapping isn't found, fall back to Noun.\n    \n# `pos_tag` takes the tokenized sentence as input, i.e. list of string,\n# and returns a tuple of (word, tg), i.e. list of tuples of strings\n# so we need to get the tag from the 2nd element.\n\nwalking_tagged = pos_tag(word_tokenize('He is walking to school'))\nprint(walking_tagged)"},{"outputs":[],"metadata":{"_uuid":"e59549b59e571e2368d7c08c80250a854259d1d1","_cell_guid":"6a826e5e-1c16-46ad-ac52-916a2f4e5c41","collapsed":true},"cell_type":"code","execution_count":null,"source":"[wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) for word, tag in walking_tagged]"},{"metadata":{"_uuid":"034d83608a867c28c06b86c9dc129b2f88faf326","_cell_guid":"46b8011c-beba-4e69-8305-f5b98659e679"},"cell_type":"markdown","source":"# Now, lets create a new lemmatization function for sentences given what we learnt above."},{"outputs":[],"metadata":{"_uuid":"4f7803ff5cbc3f6c0dc97ce9c353a0bec39a5696","_cell_guid":"6fe6901c-88ca-438f-9a9a-b1f28128ffc4","collapsed":true},"cell_type":"code","execution_count":null,"source":"from nltk import pos_tag\nfrom nltk.stem import WordNetLemmatizer\n\nwnl = WordNetLemmatizer()\n\ndef penn2morphy(penntag):\n    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n    morphy_tag = {'NN':'n', 'JJ':'a',\n                  'VB':'v', 'RB':'r'}\n    try:\n        return morphy_tag[penntag[:2]]\n    except:\n        return 'n' \n    \ndef lemmatize_sent(text): \n    # Text input is string, returns lowercased strings.\n    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n            for word, tag in pos_tag(word_tokenize(text))]\n\nlemmatize_sent('He is walking to school')"},{"metadata":{"_uuid":"6ad66c3cabdf30287f2f6b0ea5ccbfdcd37cf927","_cell_guid":"346bab96-cf6c-4a91-b27d-bfced577ea15"},"cell_type":"markdown","source":"# Lets try the `lemmatize_sent()` and remove stopwords from Single no. 8"},{"outputs":[],"metadata":{"_uuid":"e591e6f136501cee25961fb1df0e68f1eb676b45","_cell_guid":"926f7ec4-12c7-4d8d-8f2b-ef66b0b3a5a4","collapsed":true},"cell_type":"code","execution_count":null,"source":"print('Original Single no. 8:')\nprint(single_no8, '\\n')\nprint('Lemmatized and removed stopwords:')\nprint([word for word in lemmatize_sent(single_no8) \n       if word not in stoplist_combined\n       and not word.isdigit() ])"},{"metadata":{"_uuid":"56629a24758169967e39e8a93576195fb7c27ee8","_cell_guid":"9c245e23-b526-4d86-9082-912c1aa87255"},"cell_type":"markdown","source":"# Combining what we know about removing stopwords and lemmatization"},{"outputs":[],"metadata":{"_uuid":"3145c9a5568eb2ed075eddf5a617d20b2286f012","_cell_guid":"358ca69f-e7b2-4b93-ad09-5cf1b0a7143a","collapsed":true},"cell_type":"code","execution_count":null,"source":"def preprocess_text(text):\n    # Input: str, i.e. document/sentence\n    # Output: list(str) , i.e. list of lemmas\n    return [word for word in lemmatize_sent(text) \n            if word not in stoplist_combined\n            and not word.isdigit()]"},{"metadata":{"_uuid":"099da61cd361f9128632758422960862742b917f","_cell_guid":"51ffeed8-ea00-479e-bc92-422f4e0bba0e"},"cell_type":"markdown","source":"# Tangential Note on Lemmatization\n\nIn English, a root word / lemma can manifest in different forms. \n\n| <img src=\"https://media1.giphy.com/media/xHHsXH7WJsWK4/giphy.gif\" align=\"left\" height=\"200\" width=\"200\"> | <img src=\"https://media1.giphy.com/media/xHHsXH7WJsWK4/giphy.gif\" align=\"left\" height=\"200\" width=\"200\"><img src=\"https://media1.giphy.com/media/xHHsXH7WJsWK4/giphy.gif\" align=\"left\" height=\"200\" width=\"200\"> |\n|:-------------:|:-------------:|\n| 1 cat  | 2 cats  |\n| 1 cat  | 2 cats  |\n\nFor instance, we use “cat” to refer to a single “cat” and we attach an “-s”  suffix to refer to more than one cat, e.g. “two cats”. \n\n| <img src=\"https://68.media.tumblr.com/b0755247c8f32f79413d34b0410ccff1/tumblr_o3q8wlGi9v1u9ia8fo1_500.gif\" align=\"left\" height=\"200\" width=\"400\"> | \n|:-------------:| \n| cats walk / cats (are) walking | \n\n<!-- | <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/ModelsCatwalk.jpg/440px-ModelsCatwalk.jpg\" align=\"left\" height=\"200\" width=\"400\"> | \n|:-------------:| \n| cat walk(s) / catwalk(s) |  --> \n\n\nAnother example, the word “walk” has different forms, e.g. “walking” and “walked” indicate the time and/or progress of the walking motion. <!-- ~~Additionally, “walk” can also refer to the act of walking which is different from the walking motion, e.g. \"John went for a walk\" (act of walking) vs \"John wanted to walk to the park\" (the walking action/motion).~~ --> We refer to these root words as ***word types*** (e.g. “cat” and “walk”) and their different forms as ***word tokens*** (e.g. “cats”, “walk”, “walking”, “walked”, “walks”). \n\nLinguists further distinguish words between their lemmas or word families. A lemma refers to the canonical root word used as a dictionary entry. A word family refers to a group of lemmas which are derived from a single root word. Even though \"walkable\" would be a separate entry in a dictionary from \"walk\", \"walkable\" can be grouped under the word family of \"walk\" together with \"walking, walked, walks\".\n\nThe distinction is subtle yet linguists go into great length to argue for what counts as a type, token, lemmas or word family. "},{"metadata":{"_uuid":"3f40b6cb3f60bc2bbfeebd27de0802d0bf7849cf","_cell_guid":"9b00681d-5757-4547-828f-5767bfe701bd"},"cell_type":"markdown","source":"# From Strings to Vectors\n\n**Vector** is an array of numbers\n\n**Vector Space Model** is conceptualizing language as a whole lot of numbers\n\n**Bag-of-Words (BoW)**: Counting each document/sentence as a vector of numbers, with each number representing the count of a word in the corpus\n\nTo count, we can use the Python `collections.Counter`"},{"outputs":[],"metadata":{"_uuid":"e5527ef7e753e6b3981c9909b50fcf39a428a85c","_cell_guid":"37963ad4-b911-4bfc-b723-f6b7927a1eeb","collapsed":true},"cell_type":"code","execution_count":null,"source":"from collections import Counter\n\nsent1 = \"The quick brown fox jumps over the lazy brown dog.\"\nsent2 = \"Mr brown jumps over the lazy fox.\"\n\n# Lemmatize and remove stopwords\nprocessed_sent1 = preprocess_text(sent1)\nprocessed_sent2 = preprocess_text(sent2)"},{"outputs":[],"metadata":{"_uuid":"cfb48c07796e6b3fbe20378dcef1f11e4880b1f9","_cell_guid":"882d59c1-e3bf-4e72-84e2-858490ec66b7","collapsed":true},"cell_type":"code","execution_count":null,"source":"print('Processed sentence:')\nprint(processed_sent1)\nprint()\nprint('Word counts:')\nprint(Counter(processed_sent1))"},{"outputs":[],"metadata":{"_uuid":"4e9bd5f5852015cf8f004d1f7a01274b76cb033d","_cell_guid":"75e94392-cf5c-40f5-bcd7-349e4861fab7","collapsed":true},"cell_type":"code","execution_count":null,"source":"print('Processed sentence:')\nprint(processed_sent2)\nprint()\nprint('Word counts:')\nprint(Counter(processed_sent2))"},{"metadata":{"_uuid":"25205be78a242dcb9b8bd653cd98244ce73ba69b","_cell_guid":"e51b4c01-2712-4ec9-852b-133716681b53"},"cell_type":"markdown","source":"# Vectorization\n\nLet's put the words and counts into a nice table:\n\n| | brown | quick | fox | jump | lazy | dog | mr | \n|:---- |:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n| Sent1 | 2 | 1 | 1 | 1 | 1 | 1 | 0 |  \n| Sent2 | 1 | 0 | 1 | 1 | 1 | 0 | 1 | \n\n\nIf we fix the positions of the vocabulary i.e. \n\n```\n[brown, quick, fox, jump, lazy, dog, mr]\n```\n\nand we do the counts for each word in each sentence, we get the sentence vectors (i.e. list of numbers to represent each sentence):\n\n```\nsent1 = [2,1,1,1,1,1,0]\nsent2 = [1,0,1,1,1,0,1]\n```"},{"metadata":{"_uuid":"5c268c3e117c595b2835cdfd7c5abe604f35e0fa","_cell_guid":"64860ccd-4cf4-46dd-89d2-3929d9b7d539"},"cell_type":"markdown","source":"# Vectorization with sklearn \n\nIn `scikit-learn`, there're pre-built functions to do the preprocessing and vectorization that we've been doing using the `CountVectorizer` object. \n\nIt will be the object that contains the vocabulary (i.e. the first row of our table above) and has the function to convert any sentence into the counts vectors we see as above.\n\nThe input that `CountVectorizer` is a textfile, so we've to do some hacking to put let it accept the string outputs.\n\nWe can \"fake it to make it\" using `io.StringIO` where we can convert any string to work like a file, e.g. "},{"outputs":[],"metadata":{"_uuid":"3834e62e0c3879b37f94e1924e80534a5899fbfd","_cell_guid":"5f92d415-5016-4405-b797-d98df2393a89","collapsed":true},"cell_type":"code","execution_count":null,"source":"from io import StringIO\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nsent1 = \"The quick brown fox jumps over the lazy brown dog.\"\nsent2 = \"Mr brown jumps over the lazy fox.\"\n\nwith StringIO('\\n'.join([sent1, sent2])) as fin:\n    # Create the vectorizer\n    count_vect = CountVectorizer()\n    count_vect.fit_transform(fin)"},{"outputs":[],"metadata":{"_uuid":"36407729e4ed2636089293545a5f2d030a4d60e8","_cell_guid":"a8a7db77-9578-4f58-8c09-8cde03f6a865","collapsed":true},"cell_type":"code","execution_count":null,"source":"# We can check the vocabulary in our vectorizer\n# It's a dictionary where the words are the keys and \n# The values are the IDs given to each word. \ncount_vect.vocabulary_"},{"metadata":{"_uuid":"e2d864d079623c6d92e2694dad75bd8edf810d48","_cell_guid":"13d2b03b-6ab9-4da4-b20e-38c3cce08243"},"cell_type":"markdown","source":"**Note:** We haven't counted anything yet just initializing our vectorizer object with the vocabulary. "},{"metadata":{"_uuid":"07b29c69132ed41c5da7f869f098694374b42cda","_cell_guid":"2249db3e-0b22-4f63-be8e-b19c951a4ea2"},"cell_type":"markdown","source":"# ちょっと待ってください ... (Wait a minute)\n\nI didn't tell the vectorizer to remove punctuation and tokenize and lowercase, how did they do it?\n\nAlso, `the` is in the vocabulary, it's a stopword, we want it gone... <br>\nAnd `jumps` isn't stemmed or lemmatized!"},{"metadata":{"_uuid":"e6393e0f95e0b0dbd92f64f22883331f9eaac919","_cell_guid":"035f1601-9e72-4d85-929e-44d8447d7253"},"cell_type":"markdown","source":"If we look at the documentation of the [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) in `sklearn`, we see:\n\n\n```python\nCountVectorizer(\n    input=’content’, encoding=’utf-8’, \n    decode_error=’strict’, strip_accents=None, \n    lowercase=True, preprocessor=None, \n    tokenizer=None, stop_words=None, \n    token_pattern=’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1), \n    analyzer=’word’, max_df=1.0, min_df=1, \n    max_features=None, vocabulary=None, \n    binary=False, dtype=<class ‘numpy.int64’>)[source]\n```\n\nAnd more specifically:\n\n> **analyzer** : string, {‘word’, ‘char’, ‘char_wb’} or callable\n> \n> Whether the feature should be made of word or character n-grams. Option ‘char_wb’ creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.\n> If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input.\n\n \n> **preprocessor** : callable or None (default)\n> \n> Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps.\n\n> **tokenizer** : callable or None (default)\n> \n> Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if analyzer == 'word'.\n\n> **stop_words** : string {‘english’}, list, or None (default)\n> \n> If ‘english’, a built-in stop word list for English is used.\n> If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if analyzer == 'word'.\nIf None, no stop words will be used. \n\n> **lowercase** : boolean, True by default\n> \n> Convert all characters to lowercase before tokenizing."},{"metadata":{"_uuid":"11d7d8b6406c5d56259d72c48f97747e119885b7","_cell_guid":"9eb50e4d-4d52-47a4-9607-fd33fe3775d1"},"cell_type":"markdown","source":"# Achso, we can override these arguments with the functions we have learnt before."},{"metadata":{"_uuid":"4875ebac4a09758236313d7ffa17fb015faac926","_cell_guid":"e9c00de2-ab64-4a35-a790-485a64b43dd3"},"cell_type":"markdown","source":"We can **override the tokenizer and stop_words**:"},{"outputs":[],"metadata":{"_uuid":"f91435b9a2d476d45d9146ceb204f64e586133c1","_cell_guid":"25630055-8410-46fe-b08c-5fc7202bf1ab","collapsed":true},"cell_type":"code","execution_count":null,"source":"from io import StringIO\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nsent1 = \"The quick brown fox jumps over the lazy brown dog.\"\nsent2 = \"Mr brown jumps over the lazy fox.\"\n\nwith StringIO('\\n'.join([sent1, sent2])) as fin:\n    # Override the analyzer totally with our preprocess text\n    count_vect = CountVectorizer(stop_words=stoplist_combined,\n                                 tokenizer=word_tokenize)\n    count_vect.fit_transform(fin)\ncount_vect.vocabulary_"},{"metadata":{"_uuid":"e23d630dd87abeae6ed1b37d64c2fe5223da7088","_cell_guid":"5f29251e-1074-4c04-a50c-d56d4ca3a97e"},"cell_type":"markdown","source":"Or just **override the analyzer** totally with our preprocess text:"},{"outputs":[],"metadata":{"_uuid":"fd10c8df7c870641807211b97140900e570c46af","_cell_guid":"40813b12-f701-4bca-a2e3-c16b928dd32e","collapsed":true},"cell_type":"code","execution_count":null,"source":"from io import StringIO\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nsent1 = \"The quick brown fox jumps over the lazy brown dog.\"\nsent2 = \"Mr brown jumps over the lazy fox.\"\n\nwith StringIO('\\n'.join([sent1, sent2])) as fin:\n    # Override the analyzer totally with our preprocess text\n    count_vect = CountVectorizer(analyzer=preprocess_text)\n    count_vect.fit_transform(fin)\ncount_vect.vocabulary_ "},{"metadata":{"_uuid":"4024585e83698a0c454b6ff332de4a32d8bfeb8b","_cell_guid":"ab69e27d-1d13-47f0-a547-9e234292d166"},"cell_type":"markdown","source":"# To vectorize any new sentences, we use  `CountVectorizer.transform()` \n\nThe function  will return a sparse matrix."},{"outputs":[],"metadata":{"_uuid":"e18dbaa0882825ffc4b1b746de13eb4d8381984d","_cell_guid":"92d41c12-7f75-406e-80ec-66a2977a79a8","collapsed":true},"cell_type":"code","execution_count":null,"source":"count_vect.transform([sent1, sent2])"},{"metadata":{"_uuid":"8815c6acca7b396407004eaf858a957576c06673","_cell_guid":"81e138ab-0b0f-4c67-abe9-b34c15cb7e8d"},"cell_type":"markdown","source":"# To view the matrix, you can output it to an array"},{"outputs":[],"metadata":{"_uuid":"ec34de51827ed00a2cdb39c000a03bb85e582713","_cell_guid":"4e4029f4-1440-4ebd-b239-c365e735e21d","collapsed":true},"cell_type":"code","execution_count":null,"source":"from operator import itemgetter\n\n# Print the words sorted by their index\nwords_sorted_by_index, _ = zip(*sorted(count_vect.vocabulary_.items(), key=itemgetter(1)))\n\nprint(preprocess_text(sent1))\nprint(preprocess_text(sent2))\nprint()\nprint('Vocab:', words_sorted_by_index)\nprint()\nprint('Matrix/Vectors:\\n', count_vect.transform([sent1, sent2]).toarray())"},{"metadata":{"_uuid":"0c271bfc242864673a862c0fc4e587ad57654528","_cell_guid":"c520f436-9d65-4950-a7aa-91eaf9382d12"},"cell_type":"markdown","source":"Naive Bayes \n====\n\n\n\n\nClassification\n====\n\nClassification simply means putting our data points into bins/box. You can also think of it as assigning label to our data points, e.g. given box of fruits, sort them in apples, oranges and others. \n\nOkay, the explanation could be more complex than that but `import this` says:\n\n> **Simple is better than complex.**"},{"metadata":{"_uuid":"5d163c46b118b580f07eeebad0a16eaf5edbb20d","_cell_guid":"1fd3db77-b764-4f26-914f-ec0ac4e4d0c9"},"cell_type":"markdown","source":"# Now that we learnt some basic NLP and vectorization, lets apply it to a fun task."},{"metadata":{"_uuid":"26ce98337b4e3c9f54278412e578117bfd92d625","_cell_guid":"0a6d1a71-4171-4384-a0b8-5cae9efbea33"},"cell_type":"markdown","source":"[Random Acts of Pizza](https://www.kaggle.com/c/random-acts-of-pizza)\n=====\n\nIn machine learning, it is often said there are [no free lunches](). How wrong we were.\n\nThis competition contains a dataset with 5671 textual requests for pizza from the Reddit community Random Acts of Pizza together with their outcome (successful/unsuccessful) and meta-data. \n\n![](https://kaggle2.blob.core.windows.net/competitions/kaggle/3949/media/pizzas.png)\n\nThe task is to create an algorithm capable of predicting which requests will garner a cheesy (but sincere!) act of kindness.\n"},{"metadata":{"_uuid":"3cae972238580fe9c4fb6a184b3661b232f77458","_cell_guid":"4c85876b-1b19-4c50-8330-a2c9dbd91f32"},"cell_type":"markdown","source":"# Lets take a look at the training data"},{"outputs":[],"metadata":{"_uuid":"efab58829e4f5082cd9ac154953f73c1f5033763","_cell_guid":"211f0cf5-8be5-482e-bc3c-d57fd089689c","collapsed":true},"cell_type":"code","execution_count":null,"source":"import json\n\nwith open('../input/random-acts-of-pizza/train.json') as fin:\n    trainjson = json.load(fin)"},{"outputs":[],"metadata":{"_uuid":"f363473d1aaaedd58eae40f7c970d1eedf6c7fa4","_cell_guid":"0bb18893-12ed-4178-89cc-06c4f01d7107","collapsed":true},"cell_type":"code","execution_count":null,"source":"trainjson[0]"},{"metadata":{"_uuid":"d231a3cdad2807893f44de861892093d02cb07a7","_cell_guid":"24d5a591-79a6-4740-8b58-c76fef459355"},"cell_type":"markdown","source":"We're only interested in the text fields:\n\n**Input**:\n - `request_id`: unique identifier for the request \n - `request_title`: title of the reddit post for pizza request\n - `request_text_edit_aware`: expository to request for pizza\n \n**Output**:\n - `requester_recieved_pizza`: whether requester gets his/her pizza\n \nFor our purpose, lets only use the `request_text` as the input to build our Naive Bayes classifier and the output is the `requester_recieved_pizza` field.\n\n**Note:** The `request_id` is only used for mapping purpose when we're submitting the results to the Kaggle task."},{"outputs":[],"metadata":{"_uuid":"9755249c3af76081f4203b75f2fbe05e860db4f6","_cell_guid":"38df138d-858e-4983-be85-37f864b8b7aa","collapsed":true},"cell_type":"code","execution_count":null,"source":"print('UID:\\t', trainjson[0]['request_id'], '\\n')\nprint('Title:\\t', trainjson[0]['request_title'], '\\n')\nprint('Text:\\t', trainjson[0]['request_text_edit_aware'], '\\n')\nprint('Tag:\\t', trainjson[0]['requester_received_pizza'], end='\\n')"},{"metadata":{"_uuid":"bf75dd208d746cb254d08576477c25c2472d8cae","_cell_guid":"2d3b4d73-784e-4aef-a1c2-d70a0592495e"},"cell_type":"markdown","source":"# Here's a neat trick to convert json to pandas DataFrame"},{"outputs":[],"metadata":{"_uuid":"a4d14ae8096e83aea7ec4fe276799632f4a244a5","_cell_guid":"afae23a2-d175-4fd8-b63e-71a34284e500","collapsed":true},"cell_type":"code","execution_count":null,"source":"import pandas as pd\ndf = pd.io.json.json_normalize(trainjson) # Pandas magic... \ndf_train = df[['request_id', 'request_title', \n               'request_text_edit_aware', \n               'requester_received_pizza']]\ndf_train.head()"},{"metadata":{"_uuid":"940b7bc059fb428bd08e6f1930b74f4e6a4d4823","_cell_guid":"0c50ef1b-d0f5-480e-81cd-ae2922211564"},"cell_type":"markdown","source":"# Lets take a look at the test data"},{"outputs":[],"metadata":{"_uuid":"d7edfb13cfc94358fdb89455626d7b8d68adf7d9","_cell_guid":"28c0923b-0ea3-456e-8311-3773a169684c","collapsed":true},"cell_type":"code","execution_count":null,"source":"import json\n\nwith open('../input/random-acts-of-pizza/test.json') as fin:\n    testjson = json.load(fin)"},{"outputs":[],"metadata":{"_uuid":"e3a07d7af63044bd190dbe630de39fccfca61740","_cell_guid":"775eec1e-2d63-4dd9-8f79-e11ae53a04e5","collapsed":true},"cell_type":"code","execution_count":null,"source":"print('UID:\\t', testjson[0]['request_id'], '\\n')\nprint('Title:\\t', testjson[0]['request_title'], '\\n')\nprint('Text:\\t', testjson[0]['request_text_edit_aware'], '\\n')\nprint('Tag:\\t', testjson[0]['requester_received_pizza'], end='\\n')"},{"metadata":{"_uuid":"df6e837ff69fbab2af2f47acffa0cb8d12f995a9","_cell_guid":"6cb14f9e-0c8e-40d3-aff7-e542790336eb"},"cell_type":"markdown","source":"# Gotcha again! \n\nIn the test data, our label (i.e. `requester_received_pizza`) **won't be known** to us since that's the thing that our classifier is predicting.\n\n**Note:** Whatever features that we're going to train our classifier with, we should have them in our test set too. In our case we need to make sure that the test set has `request_text_edit_aware` field."},{"metadata":{"_uuid":"aed441bb12db2d224cbea93c835ae1d1d27e363d","_cell_guid":"6e4c5394-7c84-47e5-b859-9a60ff97f851"},"cell_type":"markdown","source":"# Lets put the test data into a pandas DataFrame too"},{"outputs":[],"metadata":{"_uuid":"830d4b563f3fea20101083bf3cb660f405249cc1","_cell_guid":"d098e108-b1b6-4914-971a-114280869002","collapsed":true},"cell_type":"code","execution_count":null,"source":"import pandas as pd\ndf = pd.io.json.json_normalize(testjson) # Pandas magic... \ndf_test = df[['request_id', 'request_title', \n               'request_text_edit_aware']]\ndf_test.head()"},{"metadata":{"_uuid":"a2cacb2270490a92ec9a7e920e8ca17c730c1e09","_cell_guid":"b869481f-c713-4aa6-aa0d-f2e5f4b2abfc"},"cell_type":"markdown","source":"# Split training data before vectorization\n\nThe first thing to do is to split our training data into 2 parts:\n\n - **training**: Use for training our model\n - **validation**: Use to check the \"soundness\" of our model\n \n**Note:** \n\n - Splitting the data into 2 parts and holding out one part to check the model is one of method to validate the \"soundness\" of our model. It's call the **hold-out** validation. \n\n - Another popular validation method is **cross-validation**, it's out of scope here but you can take a look at `crossvalidation` in `scikit-learn`. \n "},{"outputs":[],"metadata":{"_uuid":"ac1e8766dcf5d7d81fe4fd3a95e7bc659fbf1978","_cell_guid":"0b61f317-079b-4253-b94a-a87b71bd47fb","collapsed":true},"cell_type":"code","execution_count":null,"source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split \n\n# It doesn't really matter what the function name is called\n# but the `train_test_split` is splitting up the data into \n# 2 parts according to the `test_size` argument you've set.\n\n# When we're splitting up the training data, we're spltting up \n# into train, valid split. The function name is just a name =)\ntrain, valid = train_test_split(df_train, test_size=0.2)"},{"metadata":{"_uuid":"f97576104bbabfc720fa0794ab6a8fc4e043f109","_cell_guid":"8677912b-d9e2-4fd8-b722-acb3320af4fc"},"cell_type":"markdown","source":"# Vectorize the train and validation set"},{"outputs":[],"metadata":{"_uuid":"4549079bfedbbff679ccde5f083b3c6814e73233","_cell_guid":"02efe9a8-00e4-41fa-9b6a-5fbebe751feb","collapsed":true},"cell_type":"code","execution_count":null,"source":"# Initialize the vectorizer and \n# override the analyzer totally with the preprocess_text().\n# Note: the vectorizer is just an 'empty' object now.\ncount_vect = CountVectorizer(analyzer=preprocess_text)\n\n# When we use `CounterVectorizer.fit_transform`,\n# we essentially create the dictionary and \n# vectorize our input text at the same time.\ntrain_set = count_vect.fit_transform(train['request_text_edit_aware'])\ntrain_tags = train['requester_received_pizza']\n\n# When vectorizing the validation data, we use `CountVectorizer.transform()`.\nvalid_set = count_vect.transform(valid['request_text_edit_aware'])\nvalid_tags = valid['requester_received_pizza']"},{"metadata":{"_uuid":"eb879f0ea99c654625d7e4526de4c1f5602619ac","_cell_guid":"85863807-de62-471c-a5f9-7ada4e54e0bf"},"cell_type":"markdown","source":"# Now, we need to vectorize the test data too\n\nAfter we vectorize our data, the input to train the classifier would be the vectorized text. \n<br>When we predict the label with the trained mdoel, our input needs to be vectorized too.\n"},{"outputs":[],"metadata":{"_uuid":"502788a6150bc9c7256b097a4d5c04b4822ac3eb","_cell_guid":"766b1529-5967-45e9-9736-1fb69bdc6460","collapsed":true},"cell_type":"code","execution_count":null,"source":"# When vectorizing the test data, we use `CountVectorizer.transform()`.\ntest_set = count_vect.transform(df_test['request_text_edit_aware'])"},{"metadata":{"_uuid":"3aaf138395ccdfe7a29d89d1b58da9db96ae8b36","_cell_guid":"9d7ae411-ae2c-4098-beb2-ac6c2dc267a9"},"cell_type":"markdown","source":"# Naive Bayes classifier in sklearn\n\nThere are different variants of Naive Bayes (NB) classifier in `sklearn`. <br>\nFor simplicity, lets just use the `MultinomialNB`.\n\n**Multinomial** is a big word but it just means many classes/categories/bins/boxes that needs to be classified. "},{"outputs":[],"metadata":{"_uuid":"44cee633360c7989a3d67716553ef4e2bc9eae87","_cell_guid":"8a159538-c58a-485d-bc23-d7115ee55a77","collapsed":true},"cell_type":"code","execution_count":null,"source":"from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB() \n\n# To train the classifier, simple do \nclf.fit(train_set, train_tags) "},{"metadata":{"_uuid":"fbec702a6003287ea5d49049be532ee96b6d7a14","_cell_guid":"8c2776b5-6515-4321-a7e7-f925534241d6"},"cell_type":"markdown","source":"# Before we test our classifier on the test set, we get a sense of how good it is on the validation set."},{"outputs":[],"metadata":{"_uuid":"b5f7b1ac26a30b568d3f5dec2a3f76c34a316ee5","_cell_guid":"a0bba339-cd62-46e9-9106-55ef15000dc5","collapsed":true},"cell_type":"code","execution_count":null,"source":"from sklearn.metrics import accuracy_score\n\n# To predict our tags (i.e. whether requesters get their pizza), \n# we feed the vectorized `test_set` to .predict()\npredictions_valid = clf.predict(valid_set)\n\nprint('Pizza reception accuracy = {}'.format(\n        accuracy_score(predictions_valid, valid_tags) * 100)\n     )"},{"metadata":{"_uuid":"293f22791ce16ad0071afa625f7b6efa0601da7d","_cell_guid":"24fed6a6-9b91-4192-92e1-1c6ac3264dab"},"cell_type":"markdown","source":"# Now lets use the full training data set and re-vectorize and retrain the classifier\n\nMore data == better model (in most cases)"},{"outputs":[],"metadata":{"_uuid":"4a0c2bbe6b98000d71e1b2f9cd00bb01b156522e","_cell_guid":"f738aa5b-8e1b-4885-8f3e-6d279f23d082","collapsed":true},"cell_type":"code","execution_count":null,"source":"count_vect = CountVectorizer(analyzer=preprocess_text)\n\nfull_train_set = count_vect.fit_transform(df_train['request_text_edit_aware'])\nfull_tags = df_train['requester_received_pizza']\n\n# Note: We have to re-vectorize the test set since\n#       now our vectorizer is different using the full \n#       training set.\ntest_set = count_vect.transform(df_test['request_text_edit_aware'])\n\n# To train the classifier\nclf = MultinomialNB() \nclf.fit(full_train_set, full_tags) "},{"metadata":{"_uuid":"dbf9eb2a3dde29a26ae51df05f120ee26334d0c3","_cell_guid":"6013d400-4a01-438a-9afe-b540b6ec78b9"},"cell_type":"markdown","source":"# Finally, we use the classifier to predict on the test set"},{"outputs":[],"metadata":{"_uuid":"450827a713f7eaaa2d3c3504b038a513c27b3b20","_cell_guid":"2fe3ff2b-5157-43b2-ad8d-55389ad28a0e","collapsed":true},"cell_type":"code","execution_count":null,"source":"# To predict our tags (i.e. whether requesters get their pizza), \n# we feed the vectorized `test_set` to .predict()\npredictions = clf.predict(test_set)"},{"metadata":{"_uuid":"51a804beaf030dbb8cbdfc320d6a26d86991d274","_cell_guid":"ef6b8e68-46be-4944-b809-872ddeabf386"},"cell_type":"markdown","source":"**Note:** Since we don't have the `requester_received_pizza` field in test data, we can't measure accuracy. But we can do some exploration as shown below."},{"metadata":{"_uuid":"e948c2b162d178cd674689adc1ce9dcdb426cbb5","_cell_guid":"1ac9a825-8ffa-4ebd-a223-73521a27bc10"},"cell_type":"markdown","source":"# From the training data, we had 24% pizza giving rate"},{"outputs":[],"metadata":{"_uuid":"4ad28c27f351de96d40b68aed92217b17f71c542","_cell_guid":"80300555-265f-4557-8b52-17c343a34c58","collapsed":true},"cell_type":"code","execution_count":null,"source":"success_rate = sum(df_train['requester_received_pizza']) / len(df_train) * 100\nprint(str('Of {} requests, only {} gets their pizzas,'\n          ' {}% success rate...'.format(len(df_train), \n                                        sum(df_train['requester_received_pizza']), \n                                       success_rate)\n         )\n     )"},{"metadata":{"_uuid":"9058b0225d2f7e87486ffb5e31f710c55431a39c","_cell_guid":"3301d9e0-544f-41d5-9c32-b2ea95b37871"},"cell_type":"markdown","source":"# Lolz, our classifier is rather stingy..."},{"outputs":[],"metadata":{"_uuid":"b6b08ebc7396962d4268858d27e7f1f082943891","_cell_guid":"111c7d74-3516-455d-a13d-8a0af3d1b3c6","collapsed":true},"cell_type":"code","execution_count":null,"source":"success_rate = sum(predictions) / len(predictions) * 100\nprint(str('Of {} requests, only {} gets their pizzas,'\n          ' {}% success rate...'.format(len(predictions), \n                                        sum(predictions), \n                                       success_rate)\n         )\n     )"},{"metadata":{"_uuid":"8fbd1af6bdd2f1c3e1ff0cf65976fa3252e171cc","_cell_guid":"8ed85498-5dd2-48bf-a379-1b84d237a42b"},"cell_type":"markdown","source":"# How accurate is our count vectorization naive bayes classifier on the test data?\n\nSince we don't have the `requester_received_pizza` field in the test data, we have to check that with an oracle (i.e. the person that knows). \n\nOn Kaggle, **checking with the oracle** means uploading the file in the correct format and their script will process the scores and tell you how you did.\n\n**Note:** Different tasks will use different metrics but in most cases getting as many correct predictions as possible is the thing to aim for. We won't get into the details of how classifiers are evaluated but for a start, please see [precision, recall and F1-scores](https://en.wikipedia.org/wiki/Precision_and_recall) \n"},{"metadata":{"_uuid":"9129b8f430bf2c652038a5a7316c0e9db1bb4f4a","_cell_guid":"f8054679-9d04-4563-91d5-daa29d0f7ce6"},"cell_type":"markdown","source":"# Finally, lets take a look at what format the oracle expects and create the output file for our predictions accordingly"},{"outputs":[],"metadata":{"_uuid":"66603c8033bf346a9cd418d2679c695e1973a820","_cell_guid":"a683db0e-da0f-48b0-915c-c3bc288519f9","collapsed":true},"cell_type":"code","execution_count":null,"source":"df_sample_submission = pd.read_csv('../input/patching-pizzas/sampleSubmission.csv')\ndf_sample_submission.head()"},{"outputs":[],"metadata":{"_uuid":"1410d9b66337fc2924eb156277f059e116143919","_cell_guid":"f28c1ce2-864d-4de3-926e-4dc317479216","collapsed":true},"cell_type":"code","execution_count":null,"source":"# We've kept the `request_id` previous in the `df_test` dataframe.\n# We can simply merge that column with our predictions.\ndf_output = pd.DataFrame({'request_id': list(df_test['request_id']), \n                          'requester_received_pizza': list(predictions)}\n                        )\n# Convert the predictions from boolean to integer.\ndf_output['requester_received_pizza'] = df_output['requester_received_pizza'].astype(int)\ndf_output.head()"},{"outputs":[],"metadata":{"_uuid":"e64b0d64cbea2a60b878ca0e445b27c722e368db","_cell_guid":"4053487b-e0d6-48a3-ba95-dbe2465f78f7","collapsed":true},"cell_type":"code","execution_count":null,"source":"# Create the csv file.\ndf_output.to_csv('basic-nlp-submission.csv')"},{"outputs":[],"metadata":{"_uuid":"dbbd128b7754fa488986036a55ce09ecbe52e167","_cell_guid":"a97cb294-eb6c-479e-944a-b82e63dc008f","collapsed":true},"cell_type":"code","execution_count":null,"source":""}],"nbformat":4,"nbformat_minor":1}