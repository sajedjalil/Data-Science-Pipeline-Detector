{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# deberta-v3-base Inf \n\n**Train notebook** https://www.kaggle.com/gauravbrills/jigsaw-deberta-v3-base-train-model-3?scriptVersionId=80522297","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Directory settings\n# ====================================================\nimport os\n\nOUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:41:56.020094Z","iopub.execute_input":"2021-12-23T01:41:56.020532Z","iopub.status.idle":"2021-12-23T01:41:56.046315Z","shell.execute_reply.started":"2021-12-23T01:41:56.020448Z","shell.execute_reply":"2021-12-23T01:41:56.045645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CFG","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    model_dir='../input/jigsaw-deberta-v3-base-train-model-3'\n    num_workers=4\n    model=\"../input/deberta-v3-base/deberta-v3-base\"\n    batch_size=128\n    fc_dropout=0.0000001\n    text=\"text\"\n    target=\"target\"\n    target_size=1\n    head=32\n    tail=32\n    seed=2021\n    n_fold=5\n\n\nCFG.max_len = CFG.head + CFG.tail","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:41:56.047838Z","iopub.execute_input":"2021-12-23T01:41:56.048333Z","iopub.status.idle":"2021-12-23T01:41:56.053924Z","shell.execute_reply.started":"2021-12-23T01:41:56.048296Z","shell.execute_reply":"2021-12-23T01:41:56.053331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport re\nimport sys\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport logging\nlogging.basicConfig(level=logging.ERROR)\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig \nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:41:56.056847Z","iopub.execute_input":"2021-12-23T01:41:56.057348Z","iopub.status.idle":"2021-12-23T01:42:03.001939Z","shell.execute_reply.started":"2021-12-23T01:41:56.057319Z","shell.execute_reply":"2021-12-23T01:42:03.001193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Utils\n# ====================================================\ndef get_score(df):\n    score = len(df[df['less_toxic_pred'] < df['more_toxic_pred']]) / len(df)\n    return score\n\n\ndef get_logger(filename=OUTPUT_DIR+'train'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\ndef seed_everything(seed=2021):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=2021)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:42:03.003747Z","iopub.execute_input":"2021-12-23T01:42:03.004256Z","iopub.status.idle":"2021-12-23T01:42:03.016341Z","shell.execute_reply.started":"2021-12-23T01:42:03.004219Z","shell.execute_reply":"2021-12-23T01:42:03.015704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Data Loading\n# ====================================================\ndf = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\nsub  = pd.read_csv('../input/jigsaw-toxic-severity-rating/sample_submission.csv')\n","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:42:03.017857Z","iopub.execute_input":"2021-12-23T01:42:03.018293Z","iopub.status.idle":"2021-12-23T01:42:03.129519Z","shell.execute_reply.started":"2021-12-23T01:42:03.018259Z","shell.execute_reply":"2021-12-23T01:42:03.128765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CFG.model, lowercase=True)\ntokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\nCFG.tokenizer = tokenizer","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:42:03.131466Z","iopub.execute_input":"2021-12-23T01:42:03.131712Z","iopub.status.idle":"2021-12-23T01:42:03.906337Z","shell.execute_reply.started":"2021-12-23T01:42:03.131679Z","shell.execute_reply":"2021-12-23T01:42:03.905587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input(text, cfg):\n    if cfg.tail == 0:\n        inputs = cfg.tokenizer.encode_plus(text, \n                                           return_tensors=None, \n                                           add_special_tokens=True, \n                                           max_length=cfg.max_len,\n                                           pad_to_max_length=True,\n                                           truncation=True)\n        for k, v in inputs.items():\n            inputs[k] = torch.tensor(v, dtype=torch.long)\n    else:\n        inputs = cfg.tokenizer.encode_plus(text,\n                                           return_tensors=None, \n                                           add_special_tokens=True, \n                                           truncation=True)\n        for k, v in inputs.items():\n            v_length = len(v)\n            if v_length > cfg.max_len:\n                v = np.hstack([v[:cfg.head], v[-cfg.tail:]])\n            if k == 'input_ids':\n                new_v = np.ones(cfg.max_len) * cfg.tokenizer.pad_token_id\n            else:\n                new_v = np.zeros(cfg.max_len)\n            new_v[:v_length] = v \n            inputs[k] = torch.tensor(new_v, dtype=torch.long)\n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.text = df[cfg.text].fillna(\"none\").values\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        text = str(self.text[item])\n        inputs = prepare_input(text, self.cfg)\n        return inputs","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:42:03.907777Z","iopub.execute_input":"2021-12-23T01:42:03.908019Z","iopub.status.idle":"2021-12-23T01:42:03.920976Z","shell.execute_reply.started":"2021-12-23T01:42:03.907986Z","shell.execute_reply":"2021-12-23T01:42:03.920203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:42:03.922372Z","iopub.execute_input":"2021-12-23T01:42:03.922794Z","iopub.status.idle":"2021-12-23T01:42:03.933884Z","shell.execute_reply.started":"2021-12-23T01:42:03.922713Z","shell.execute_reply":"2021-12-23T01:42:03.933178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        self.head = AttentionHead(self.config.hidden_size,self.config.hidden_size)\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(self.config.hidden_size,1)\n\n    def forward(self, xb):\n        x = self.model(**xb)[0]\n        x = self.head(x)\n        x = self.dropout(x)\n        x = self.linear(x)\n        return x\n\nclass CustomModel_Legacy(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, cfg.target_size)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        feature = torch.mean(last_hidden_states, 1)\n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:42:03.935352Z","iopub.execute_input":"2021-12-23T01:42:03.935619Z","iopub.status.idle":"2021-12-23T01:42:03.948583Z","shell.execute_reply.started":"2021-12-23T01:42:03.93557Z","shell.execute_reply":"2021-12-23T01:42:03.947855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# inference","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# inference\n# ====================================================\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:42:03.950047Z","iopub.execute_input":"2021-12-23T01:42:03.950302Z","iopub.status.idle":"2021-12-23T01:42:03.959764Z","shell.execute_reply.started":"2021-12-23T01:42:03.950272Z","shell.execute_reply":"2021-12-23T01:42:03.959004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, df)\ntest_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, \n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\nconfig_path = CFG.model_dir+\"config.pth\"\npredictions = []\nfor fold in range(CFG.n_fold):\n    model = CustomModel(CFG, config_path=config_path, pretrained=False)\n    state = torch.load(CFG.model_dir+f\"/jigsaw_fold{fold}_best.pth\", map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state; gc.collect()\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:42:03.961273Z","iopub.execute_input":"2021-12-23T01:42:03.961809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# submission","metadata":{}},{"cell_type":"code","source":"df['score'] = np.mean(predictions, axis=0)\ndf.to_csv(\"submission_nlp_raw.csv\", index=False)\ndf['score'] = df['score'].rank(method='first')\ndf[['comment_id', 'score']].to_csv(\"submission_ranked.csv\", index=False)\ndf.head()\n# 0.766  with this above logic ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Open the file\ndf_train = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\nprint('Dim Train :', df_train.columns)\n\n# If the pair has been ranked by multiple worker, we keep the order that is most unanimous\ndf_train['TEXT_ranked'] = df_train.apply(lambda row : row['less_toxic'] + ':' + row['more_toxic'], axis = 1)\ndf_train['TEXT_paire'] = df_train.apply(lambda row : min(row['less_toxic'], row['more_toxic']) + ':' + max(row['less_toxic'], row['more_toxic']), axis = 1)\ndf_train['Count_paire_ranked'] = df_train.groupby(['TEXT_ranked'])['TEXT_ranked'].transform('count')\ndf_train['Count_paire'] = df_train.groupby(['TEXT_paire'])['TEXT_ranked'].transform('count')\ndf_train['count_max'] = df_train.groupby(['TEXT_paire'])['Count_paire_ranked'].transform(max)\n\n# Selection\ndf_train = df_train[df_train['Count_paire_ranked'] == df_train['count_max']]\ndf_train = df_train[df_train['Count_paire_ranked'] == 3] # every workers agreed\n\n# Delete duplicates\ndf_train = df_train.drop(columns = ['worker'])\ndf_train = df_train.drop_duplicates()\n\n# Results\ndf_train = df_train.sort_values(by = ['TEXT_ranked'])\ndf_train = df_train.drop(columns = ['Count_paire', 'count_max', 'TEXT_ranked', 'TEXT_paire']).drop_duplicates()\nprint('Dim APRES :', df_train.shape)\ndf_train.head()\n# Add score to the validation dataset\ndf_train = df_train.merge(df[['text', 'score']], left_on = 'less_toxic', right_on = 'text', how = 'left').drop_duplicates()\ndf_train = df_train.rename(columns = {'score' : 'score_less'})\ndf_train = df_train.drop(columns = ['text'])\ndf_train = df_train.merge(df[['text', 'score']], left_on = 'more_toxic', right_on = 'text', how = 'left').drop_duplicates()\ndf_train = df_train.rename(columns = {'score' : 'score_more'})\ndf_train = df_train.drop(columns = ['text'])\n\n# Stats\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Stats\nprint(len(df_train[df_train['score_more'] < df_train['score_less']]), '/', len(df_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test\ndf_train[df_train['score_more'] < df_train['score_less']].sort_values(['less_toxic'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correction of scores\ndf_train['score_max_du_less_toxic'] = df_train.groupby(['less_toxic'])['score_more'].transform(min) # score_min des textes + toxics\ndf_train['score_min_du_more_toxic'] = df_train.groupby(['more_toxic'])['score_less'].transform(max) # score_max des textes - toxics\n\n# Join\ndf = df.merge(df_train[['less_toxic', 'score_less', 'score_max_du_less_toxic']], left_on = ['text', 'score'], right_on = ['less_toxic', 'score_less'], how = 'left')\ndf = df.drop(columns = ['less_toxic', 'score_less'])\ndf = df.merge(df_train[['more_toxic', 'score_more', 'score_min_du_more_toxic']], left_on = ['text', 'score'], right_on = ['more_toxic', 'score_more'], how = 'left')\ndf = df.drop(columns = ['more_toxic', 'score_more'])\n\n# Rename\ndf = df.rename(columns = {'score_max_du_less_toxic' : 'borne_max', 'score_min_du_more_toxic' : 'borne_min'}) # le score doit est + petit que borne_max\ndf = df[['comment_id', 'text', 'score', 'borne_min', 'borne_max']].drop_duplicates()\n\n# AperÃ§u\ndf.head()\n# CORRECTION of the scores\ndef corrige(row) :\n    score, borne_min, borne_max = row['score'], row['borne_min'], row['borne_max']\n    \n    if not(pd.isna(borne_min)) and not(pd.isna(borne_max)) :\n        if borne_max < borne_min : return (borne_max + borne_min ) // 2 # return score\n        if score < borne_min : return borne_min+1\n        if score > borne_max : return borne_max-1\n        else :\n            return score\n        \n    elif not(pd.isna(borne_min)) :\n        if score < borne_min : return borne_min+1\n        else : return score\n\n    elif not(pd.isna(borne_max)) :\n        if score > borne_max : return borne_max-1\n        else : return score\n        \n    else :\n        return score\n    \n# --------------------\n\n# Application of correction\ndf['score_corrige'] = df.apply(lambda row : corrige(row), axis=1)\ncorrections = df[df['score'] != df['score_corrige']]\nprint(\"Nb of corrections : {}/{}.\".format(len(corrections), len(df)))\n\n\n# Show\ncorrections[['comment_id', 'text', 'score', 'score_corrige']].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rank first\ndf = df[['comment_id', 'text', 'score_corrige']].drop_duplicates()\ndf['score'] = df['score_corrige'].rank(method='first')\ndf = df[['comment_id', 'text', 'score']].drop_duplicates()\n\n# Show\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[['comment_id', 'score']].drop_duplicates()\nprint(df.shape)\ndf.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}