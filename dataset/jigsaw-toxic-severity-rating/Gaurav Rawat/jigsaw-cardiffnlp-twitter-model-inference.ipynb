{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport copy\nimport time\nimport random\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# For Transformer Models\nfrom transformers import AutoTokenizer, AutoModel\n\n# Utils\nfrom tqdm import tqdm\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:38:36.945599Z","iopub.execute_input":"2021-12-01T16:38:36.945998Z","iopub.status.idle":"2021-12-01T16:38:43.883118Z","shell.execute_reply.started":"2021-12-01T16:38:36.945891Z","shell.execute_reply":"2021-12-01T16:38:43.882415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = dict(\n    seed = 42,\n    model_name = '../input/roberta-base',\n    test_batch_size = 64,\n    max_length = 128,\n    num_classes = 1,\n    dropout=0.2,\n    output_logits= 768,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n)\n \n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:38:43.886537Z","iopub.execute_input":"2021-12-01T16:38:43.886898Z","iopub.status.idle":"2021-12-01T16:38:44.075963Z","shell.execute_reply.started":"2021-12-01T16:38:43.886868Z","shell.execute_reply":"2021-12-01T16:38:44.075122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_PATHS = [\n    '../input/pytorch-w-b-jigsaw-cardiffnlp-twitter-model/Loss-Fold-0.bin',\n    '../input/pytorch-w-b-jigsaw-cardiffnlp-twitter-model/Loss-Fold-1.bin',\n    '../input/pytorch-w-b-jigsaw-cardiffnlp-twitter-model/Loss-Fold-2.bin',\n    '../input/pytorch-w-b-jigsaw-cardiffnlp-twitter-model/Loss-Fold-3.bin',\n    '../input/pytorch-w-b-jigsaw-cardiffnlp-twitter-model/Loss-Fold-4.bin'\n]","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:38:44.077294Z","iopub.execute_input":"2021-12-01T16:38:44.077621Z","iopub.status.idle":"2021-12-01T16:38:44.08344Z","shell.execute_reply.started":"2021-12-01T16:38:44.077582Z","shell.execute_reply":"2021-12-01T16:38:44.082337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG['seed'])","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:38:44.087532Z","iopub.execute_input":"2021-12-01T16:38:44.08806Z","iopub.status.idle":"2021-12-01T16:38:44.09696Z","shell.execute_reply.started":"2021-12-01T16:38:44.088021Z","shell.execute_reply":"2021-12-01T16:38:44.096213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:38:44.098468Z","iopub.execute_input":"2021-12-01T16:38:44.099076Z","iopub.status.idle":"2021-12-01T16:38:44.217009Z","shell.execute_reply.started":"2021-12-01T16:38:44.099037Z","shell.execute_reply":"2021-12-01T16:38:44.216334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.text = df['text'].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        inputs = self.tokenizer.encode_plus(\n                        text,\n                        truncation=True,\n                        add_special_tokens=True,\n                        max_length=self.max_len,\n                        padding='max_length'\n                    )\n        \n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']        \n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:38:44.218476Z","iopub.execute_input":"2021-12-01T16:38:44.218944Z","iopub.status.idle":"2021-12-01T16:38:44.226468Z","shell.execute_reply.started":"2021-12-01T16:38:44.218907Z","shell.execute_reply":"2021-12-01T16:38:44.225793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = JigsawDataset(df, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n                         num_workers=2, shuffle=False, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:38:44.227934Z","iopub.execute_input":"2021-12-01T16:38:44.228547Z","iopub.status.idle":"2021-12-01T16:38:44.242256Z","shell.execute_reply.started":"2021-12-01T16:38:44.228512Z","shell.execute_reply":"2021-12-01T16:38:44.241604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class JigsawModel_Novel(nn.Module):\n    def __init__(self, model_name):\n        super(JigsawModel_Novel, self).__init__()\n        self.bert = AutoModel.from_pretrained(model_name)\n        self.distilbert  = AutoModel.from_pretrained(\"../input/cardiffnlptwitterrobertabaseoffensive\") \n        self.drop = nn.Dropout(p=CONFIG['dropout'])\n        self.layer_norm = nn.LayerNorm(CONFIG['output_logits'])\n        self.fc = nn.Linear(CONFIG['output_logits'], CONFIG['num_classes'])\n        self.dense = nn.Sequential(\n            #nn.Dropout(p=CONFIG['dropout']),\n            nn.Linear(CONFIG['output_logits'], CONFIG['num_classes']),\n            #nn.Tanh(),\n            #nn.Dropout(p=0.1),\n            #nn.Linear(768, 3, bias=True)\n        )\n        \n    def forward(self, ids, mask): \n        out = self.bert(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=True).last_hidden_state[:, 0, :]\n        out_dist = self.distilbert(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=True).last_hidden_state[:, 0, :] \n        #concatenated_vectors = torch.cat(out , out_dist )\n        output = self.drop(out_dist)\n        #pooled_output = self.layer_norm(out[1])\n        #output = self.dense(output)\n        output = self.fc(output)\n        return output","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:38:44.243827Z","iopub.execute_input":"2021-12-01T16:38:44.244358Z","iopub.status.idle":"2021-12-01T16:38:44.254343Z","shell.execute_reply.started":"2021-12-01T16:38:44.244307Z","shell.execute_reply":"2021-12-01T16:38:44.253558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef valid_fn(model, dataloader, device):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    PREDS = []\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        \n        outputs = model(ids, mask)\n        PREDS.append(outputs.view(-1).cpu().detach().numpy()) \n    \n    PREDS = np.concatenate(PREDS)\n    gc.collect()\n    \n    return PREDS","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:38:44.257385Z","iopub.execute_input":"2021-12-01T16:38:44.257604Z","iopub.status.idle":"2021-12-01T16:38:44.266943Z","shell.execute_reply.started":"2021-12-01T16:38:44.257579Z","shell.execute_reply":"2021-12-01T16:38:44.266221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference(model_paths, dataloader, device):\n    final_preds = []\n    for i, path in enumerate(model_paths):\n        model = JigsawModel_Novel(CONFIG['model_name'])\n        model.to(CONFIG['device'])\n        model.load_state_dict(torch.load(path))\n        \n        print(f\"Getting predictions for model {i+1}\")\n        preds = valid_fn(model, dataloader, device)\n        final_preds.append(preds)\n    \n    final_preds = np.array(final_preds)\n    final_preds = np.mean(final_preds, axis=0)\n    return final_preds","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:38:44.270408Z","iopub.execute_input":"2021-12-01T16:38:44.270933Z","iopub.status.idle":"2021-12-01T16:38:44.277795Z","shell.execute_reply.started":"2021-12-01T16:38:44.270895Z","shell.execute_reply":"2021-12-01T16:38:44.277118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = inference(MODEL_PATHS, test_loader, CONFIG['device'])","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:38:44.279173Z","iopub.execute_input":"2021-12-01T16:38:44.2797Z","iopub.status.idle":"2021-12-01T16:45:24.303027Z","shell.execute_reply.started":"2021-12-01T16:38:44.279662Z","shell.execute_reply":"2021-12-01T16:45:24.302142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Total Predictiions: {preds.shape[0]}\")\nprint(f\"Total Unique Predictions: {np.unique(preds).shape[0]}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:45:24.304522Z","iopub.execute_input":"2021-12-01T16:45:24.304774Z","iopub.status.idle":"2021-12-01T16:45:24.312198Z","shell.execute_reply.started":"2021-12-01T16:45:24.304744Z","shell.execute_reply":"2021-12-01T16:45:24.31134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['score'] = preds\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:45:24.313318Z","iopub.execute_input":"2021-12-01T16:45:24.313612Z","iopub.status.idle":"2021-12-01T16:45:24.328748Z","shell.execute_reply.started":"2021-12-01T16:45:24.313524Z","shell.execute_reply":"2021-12-01T16:45:24.327826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['score'] = df['score'].rank(method='first')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:45:24.33041Z","iopub.execute_input":"2021-12-01T16:45:24.331164Z","iopub.status.idle":"2021-12-01T16:45:24.379106Z","shell.execute_reply.started":"2021-12-01T16:45:24.330959Z","shell.execute_reply":"2021-12-01T16:45:24.378311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Open the file\ndf_train = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\nprint('Dim Train :', df_train.shape)\n\n# If the pair has been ranked by multiple worker, we keep the order that is most unanimous\ndf_train['TEXT_ranked'] = df_train.apply(lambda row : row['less_toxic'] + ':' + row['more_toxic'], axis = 1)\ndf_train['TEXT_paire'] = df_train.apply(lambda row : min(row['less_toxic'], row['more_toxic']) + ':' + max(row['less_toxic'], row['more_toxic']), axis = 1)\ndf_train['Count_paire_ranked'] = df_train.groupby(['TEXT_ranked'])['TEXT_ranked'].transform('count')\ndf_train['Count_paire'] = df_train.groupby(['TEXT_paire'])['TEXT_ranked'].transform('count')\ndf_train['count_max'] = df_train.groupby(['TEXT_paire'])['Count_paire_ranked'].transform(max)\n\n# Selection\ndf_train = df_train[df_train['Count_paire_ranked'] == df_train['count_max']]\ndf_train = df_train[df_train['Count_paire_ranked'] == 3] # every workers agreed\n\n# Delete duplicates\ndf_train = df_train.drop(columns = ['worker'])\ndf_train = df_train.drop_duplicates()\n\n# Results\ndf_train = df_train.sort_values(by = ['TEXT_ranked'])\ndf_train = df_train.drop(columns = ['Count_paire', 'count_max', 'TEXT_ranked', 'TEXT_paire']).drop_duplicates()\nprint('Dim APRES :', df_train.shape)\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:45:24.380285Z","iopub.execute_input":"2021-12-01T16:45:24.381045Z","iopub.status.idle":"2021-12-01T16:45:26.515943Z","shell.execute_reply.started":"2021-12-01T16:45:24.381005Z","shell.execute_reply":"2021-12-01T16:45:26.515221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add score to the validation dataset\ndf_train = df_train.merge(df[['text', 'score']], left_on = 'less_toxic', right_on = 'text', how = 'left').drop_duplicates()\ndf_train = df_train.rename(columns = {'score' : 'score_less'})\ndf_train = df_train.drop(columns = ['text'])\ndf_train = df_train.merge(df[['text', 'score']], left_on = 'more_toxic', right_on = 'text', how = 'left').drop_duplicates()\ndf_train = df_train.rename(columns = {'score' : 'score_more'})\ndf_train = df_train.drop(columns = ['text'])\n\n# Stats\ndf_train.head()\nprint(len(df_train[df_train['score_more'] < df_train['score_less']]), '/', len(df_train))\ndf_train[df_train['score_more'] < df_train['score_less']].sort_values(['less_toxic'])\n# Correction of scores\ndf_train['score_max_du_less_toxic'] = df_train.groupby(['less_toxic'])['score_more'].transform(min) # score_min des textes + toxics\ndf_train['score_min_du_more_toxic'] = df_train.groupby(['more_toxic'])['score_less'].transform(max) # score_max des textes - toxics\n\n# Join\ndf = df.merge(df_train[['less_toxic', 'score_less', 'score_max_du_less_toxic']], left_on = ['text', 'score'], right_on = ['less_toxic', 'score_less'], how = 'left')\ndf = df.drop(columns = ['less_toxic', 'score_less'])\ndf = df.merge(df_train[['more_toxic', 'score_more', 'score_min_du_more_toxic']], left_on = ['text', 'score'], right_on = ['more_toxic', 'score_more'], how = 'left')\ndf = df.drop(columns = ['more_toxic', 'score_more'])\n\n# Rename\ndf = df.rename(columns = {'score_max_du_less_toxic' : 'borne_max', 'score_min_du_more_toxic' : 'borne_min'}) # le score doit est + petit que borne_max\ndf = df[['comment_id', 'text', 'score', 'borne_min', 'borne_max']].drop_duplicates()\n\n# AperÃ§u\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:45:26.517385Z","iopub.execute_input":"2021-12-01T16:45:26.517666Z","iopub.status.idle":"2021-12-01T16:45:26.645366Z","shell.execute_reply.started":"2021-12-01T16:45:26.51763Z","shell.execute_reply":"2021-12-01T16:45:26.644597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CORRECTION of the scores\ndef corrige(row) :\n    score, borne_min, borne_max = row['score'], row['borne_min'], row['borne_max']\n    \n    if not(pd.isna(borne_min)) and not(pd.isna(borne_max)) :\n        if borne_max < borne_min : return (borne_max + borne_min ) // 2 # return score\n        if score < borne_min : return borne_min+1\n        if score > borne_max : return borne_max-1\n        else :\n            return score\n        \n    elif not(pd.isna(borne_min)) :\n        if score < borne_min : return borne_min+1\n        else : return score\n\n    elif not(pd.isna(borne_max)) :\n        if score > borne_max : return borne_max-1\n        else : return score\n        \n    else :\n        return score\n    \n# --------------------\n\n# Application of correction\ndf['score_corrige'] = df.apply(lambda row : corrige(row), axis=1)\ncorrections = df[df['score'] != df['score_corrige']]\nprint(\"Nb of corrections : {}/{}.\".format(len(corrections), len(df)))\n\n\n# Show\ncorrections[['comment_id', 'text', 'score', 'score_corrige']]","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:45:26.646797Z","iopub.execute_input":"2021-12-01T16:45:26.647576Z","iopub.status.idle":"2021-12-01T16:45:26.855877Z","shell.execute_reply.started":"2021-12-01T16:45:26.647534Z","shell.execute_reply":"2021-12-01T16:45:26.855202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rank first\ndf = df[['comment_id', 'text', 'score_corrige']].drop_duplicates()\ndf['score'] = df['score_corrige'].rank(method='first')\ndf = df[['comment_id', 'text', 'score']].drop_duplicates()\n\n# Show\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:45:26.857042Z","iopub.execute_input":"2021-12-01T16:45:26.85749Z","iopub.status.idle":"2021-12-01T16:45:26.895069Z","shell.execute_reply.started":"2021-12-01T16:45:26.857448Z","shell.execute_reply":"2021-12-01T16:45:26.894291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[['comment_id', 'score']].drop_duplicates()\nprint(df.shape)\ndf.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:45:26.896245Z","iopub.execute_input":"2021-12-01T16:45:26.89655Z","iopub.status.idle":"2021-12-01T16:45:26.929236Z","shell.execute_reply.started":"2021-12-01T16:45:26.896512Z","shell.execute_reply":"2021-12-01T16:45:26.928496Z"},"trusted":true},"execution_count":null,"outputs":[]}]}