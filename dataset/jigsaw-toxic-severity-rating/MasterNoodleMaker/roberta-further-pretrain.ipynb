{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"Note: This Kernel implements ITPT i.e. Within-Task Pretraining. Roberta-base is further pre-trained on the cleaned data from https://www.kaggle.com/kishalmandal/fold-is-gold-cleaned-data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Code Reference: https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-itpt?scriptVersionId=63560998","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets accelerate ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport logging\nimport math\nimport os\nimport random\nimport datasets\nfrom datasets import load_dataset\nfrom tqdm.auto import tqdm\nfrom accelerate import Accelerator\nimport torch\nfrom torch.utils.data import DataLoader\nimport transformers\nfrom transformers import (\n    CONFIG_MAPPING,\n    MODEL_MAPPING,\n    AdamW,\n    AutoConfig,\n    AutoModelForMaskedLM,\n    AutoTokenizer,\n    DataCollatorForLanguageModeling,\n    SchedulerType,\n    get_scheduler,\n    set_seed\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logger = logging.getLogger(__name__)\nMODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/fold-is-gold-cleaned-data/5folds.csv')\nless_toxic = train['cleaned_less_toxic']\nmore_toxic = train['cleaned_more_toxic']\nmlm_data = less_toxic.append(more_toxic)\nmlm_data = pd.DataFrame({'text':mlm_data})\nmlm_data.to_csv('mlm_data.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainConfig:\n    train_file= 'mlm_data.csv'\n    validation_file = 'mlm_data.csv'\n    validation_split_percentage= 5\n    pad_to_max_length= True\n    model_name_or_path= 'roberta-base'        \n    config_name='roberta-base'              \n    tokenizer_name= 'roberta-base'            \n    use_slow_tokenizer= True\n    per_device_train_batch_size= 8\n    per_device_eval_batch_size= 8\n    learning_rate= 5e-5\n    weight_decay= 0.0\n    num_train_epochs= 1 \n    max_train_steps= None\n    gradient_accumulation_steps= 1\n    lr_scheduler_type= 'constant_with_warmup'\n    num_warmup_steps= 0\n    output_dir= 'output_bert'\n    seed= 2021\n    model_type= 'bert'\n    max_seq_length= 128\n    line_by_line= False\n    preprocessing_num_workers= 4\n    overwrite_cache= True\n    mlm_probability= 0.15","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = TrainConfig()\nif config.train_file is not None:\n    extension = config.train_file.split(\".\")[-1]\n    assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, json or txt file.\"\nif config.validation_file is not None:\n    extension = config.validation_file.split(\".\")[-1]\n    assert extension in [\"csv\", \"json\", \"txt\"], \"`validation_file` should be a csv, json or txt file.\"\nif config.output_dir is not None:\n    os.makedirs(config.output_dir, exist_ok=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n    args = TrainConfig()\n    accelerator = Accelerator()\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state)\n    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    data_files = {}\n    if args.train_file is not None:\n        data_files[\"train\"] = args.train_file\n    if args.validation_file is not None:\n        data_files[\"validation\"] = args.validation_file\n    extension = args.train_file.split(\".\")[-1]\n    if extension == \"txt\":\n        extension = \"text\"\n    raw_datasets = load_dataset(extension, data_files=data_files)\n\n    if args.config_name:\n        config = AutoConfig.from_pretrained(args.config_name)\n    elif config.model_name_or_path:\n        config = AutoConfig.from_pretrained(args.model_name_or_path)\n    else:\n        config = CONFIG_MAPPING[args.model_type]()\n        logger.warning(\"You are instantiating a new config instance from scratch.\")\n\n    if args.tokenizer_name:\n        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=not args.use_slow_tokenizer)\n    elif args.model_name_or_path:\n        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=not args.use_slow_tokenizer)\n    else:\n        raise ValueError(\n            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n        )\n\n    if args.model_name_or_path:\n        model = AutoModelForMaskedLM.from_pretrained(\n            args.model_name_or_path,\n            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n            config=config,\n        )\n    else:\n        logger.info(\"Training new model from scratch\")\n        model = AutoModelForMaskedLM.from_config(config)\n\n    model.resize_token_embeddings(len(tokenizer))\n\n    column_names = raw_datasets[\"train\"].column_names\n    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n\n    if args.max_seq_length is None:\n        max_seq_length = tokenizer.model_max_length\n        if max_seq_length > 1024:\n            logger.warning(\n                f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n                \"Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\"\n            )\n            max_seq_length = 1024\n    else:\n        if args.max_seq_length > tokenizer.model_max_length:\n            logger.warning(\n                f\"The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the\"\n                f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n            )\n        max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)\n\n    def tokenize_function(examples):\n        return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\n\n    tokenized_datasets = raw_datasets.map(\n        tokenize_function,\n        batched=True,\n        num_proc=args.preprocessing_num_workers,\n        remove_columns=column_names,\n        load_from_cache_file=not args.overwrite_cache,\n    )\n\n    def group_texts(examples):\n        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n        total_length = len(concatenated_examples[list(examples.keys())[0]])\n        total_length = (total_length // max_seq_length) * max_seq_length\n        result = {\n            k: [t[i: i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n            for k, t in concatenated_examples.items()\n        }\n        return result\n\n    tokenized_datasets = tokenized_datasets.map(\n        group_texts,\n        batched=True,\n        num_proc=args.preprocessing_num_workers,\n        load_from_cache_file=not args.overwrite_cache,\n    )\n    train_dataset = tokenized_datasets[\"train\"]\n    eval_dataset = tokenized_datasets[\"validation\"]\n\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=args.mlm_probability)\n    train_dataloader = DataLoader(\n        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n    )\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n\n    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader\n    )\n\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    else:\n        args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=args.num_warmup_steps,\n        num_training_steps=args.max_train_steps,\n    )\n\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n\n    for epoch in range(args.num_train_epochs):\n        model.train()\n        for step, batch in enumerate(train_dataloader):\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss = loss / args.gradient_accumulation_steps\n            accelerator.backward(loss)\n            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n                progress_bar.update(1)\n                completed_steps += 1\n\n            if completed_steps >= args.max_train_steps:\n                break\n\n        model.eval()\n        losses = []\n        for step, batch in enumerate(eval_dataloader):\n            with torch.no_grad():\n                outputs = model(**batch)\n\n            loss = outputs.loss\n            losses.append(accelerator.gather(loss.repeat(args.per_device_eval_batch_size)))\n\n        losses = torch.cat(losses)\n        losses = losses[: len(eval_dataset)]\n        perplexity = math.exp(torch.mean(losses))\n\n        logger.info(f\"epoch {epoch}: perplexity: {perplexity}\")\n\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{},"execution_count":null,"outputs":[]}]}