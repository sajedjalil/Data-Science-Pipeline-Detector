{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><center>Jigsaw: PyTorch Lightning⚡ + FP16 + GPU/TPU + W&B</center></h1>\n                                                      \n<center><img src = \"https://jigsaw.google.com/static/images/social-share.jpg?cache=df11f5c\" width = \"750\" height = \"500\"/></center>                                                                          ","metadata":{}},{"cell_type":"markdown","source":"**I have written a detailed blog on Jarvislabs AI which explaines more concepts covered in this notebook in an elaborate way. You can read it [here](https://jarvislabs.ai/blogs/jigsaw)**","metadata":{}},{"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Contents</center></h2>","metadata":{}},{"cell_type":"markdown","source":"> | S.No       |                   Heading                |\n> | :------------- | :-------------------:                |         \n> |  01 |  [**Competition Overview**](#competition-overview)  |                   \n> |  02 |  [**Libraries**](#libraries)                        |  \n> |  03 |  [**Global Config**](#global-config)                |\n> |  04 |  [**Weights and Biases**](#weights-and-biases)      |\n> |  05 |  [**Utilities**](#utilities)                |\n> |  06 |  [**Dataset**](#dataset)  |\n> |  07 |  [**Datamodule**](#datamodule)   |\n> |  08 |  [**Model**](#model)   |\n> |  09 |  [**Understanding Mixed Precision**](#understanding-mixed-precision) |\n> |  10 |  [**Understanding TPUs**](#understanding-tpus) |\n> |  11 |  [**Train**](#train) |","metadata":{}},{"cell_type":"markdown","source":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:maroon; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>If you find this notebook useful, do give me an upvote, it helps to keep up my motivation. This notebook will be updated frequently so keep checking for furthur developments.</center></h3>","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<a id=\"competition-overview\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Competition Overview</center></h2>","metadata":{}},{"cell_type":"markdown","source":"## **<span style=\"color:orange;\">Description</span>**\n\n\nIn this competition, we will be asking you to score a set of about fourteen thousand comments. Pairs of comments were presented to expert raters, who marked one of two comments more harmful — each according to their own notion of toxicity. In this contest, when you provide scores for comments, they will be compared with several hundred thousand rankings. Your average agreement with the raters will determine your individual score. In this way, we hope to focus on ranking the severity of comment toxicity from innocuous to outrageous, where the middle matters as much as the extremes.\n\nCan you build a model that produces scores that rank each pair of comments the same way as our professional raters?\n  \n---\n\n## **<span style=\"color:orange;\">Evaluation Metric</span>**\n\nSubmissions are evaluated on Average Agreement with Annotators. For the ground truth, annotators were shown two comments and asked to identify which of the two was more toxic. Pairs of comments can be, and often are, rated by more than one annotator, and may have been ordered differently by different annotators.\n\nFor each of the approximately 200,000 pair ratings in the ground truth test data, we use your predicted toxicity `score` to rank the comment pair. The pair receives a 1 if this ranking matches the annotator ranking, or `0` if it does not match.\n\nThe final score is the average across all the pair evaluations.\n\n---","metadata":{}},{"cell_type":"markdown","source":"<a id=\"libraries\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Libraries</center></h2>","metadata":{}},{"cell_type":"markdown","source":"To run the model on TPU, un-comment and run the below cell and   \nchange the `gpus = -1` argument to `tpu_cores = 1` or `tpu_cores=8` in the Trainer class.","metadata":{}},{"cell_type":"code","source":"# ! curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# ! python pytorch-xla-env-setup.py --version 1.7 --apt-packages libomp5 libopenblas-dev","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Necessities\nimport wandb\nimport pandas as pd\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\n# Transformers\nfrom transformers import AutoTokenizer, AutoModel, AdamW\n\n# PyTorch Lightning\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\n# Colored Terminal Text\nfrom colorama import Fore, Back, Style\nb_ = Fore.BLUE\ny_ = Fore.YELLOW\nsr_ = Style.RESET_ALL\n\n# Aesthetics\nimport warnings\nwarnings.simplefilter('ignore')\n\nwandb.login()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T12:17:54.293876Z","iopub.execute_input":"2021-11-26T12:17:54.294114Z","iopub.status.idle":"2021-11-26T12:18:16.274315Z","shell.execute_reply.started":"2021-11-26T12:17:54.294048Z","shell.execute_reply":"2021-11-26T12:18:16.273592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"To build this notebook I have taken inspiration from [Debarshi's](https://www.kaggle.com/debarshichanda) [starter notebook](https://www.kaggle.com/debarshichanda/pytorch-w-b-jigsaw-starter)\n\n**What will be different in this notebook?**\n1. Code has been written in PyTorch Lightning\n2. Multi-GPU Training Compatible\n3. TPU Training Compatible\n4. Uses Mixed Precision Training to reduce Training Time Significantly\n5. Uses Weights and Biases as a logger","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<a id=\"global-config\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Global Config</center></h2>","metadata":{}},{"cell_type":"code","source":"CONFIG = {\"seed\": 42,\n          \"epochs\": 2,\n          \"model_name\": \"../input/roberta-base\",\n          \"tokenizer\": AutoTokenizer.from_pretrained(\"../input/roberta-base\"),\n          \"train_file_path\": \"../input/jigsaw-folds/train_5folds.csv\",\n          \"checkpoint_directory_path\": \"./checkpoints\",\n          \"train_batch_size\": 32,\n          \"valid_batch_size\": 64,\n          \"max_length\": 128,\n          \"learning_rate\": 1e-4,\n          \"scheduler\": 'CosineAnnealingLR',\n          \"min_lr\": 1e-6,\n          \"T_max\": 500,\n          \"weight_decay\": 1e-6,\n          \"n_fold\": 5,\n          \"n_accumulate\": 1,\n          \"num_classes\": 1,\n          \"margin\": 0.5,\n          \"num_workers\": 2,\n          \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n          \"infra\" : \"Kaggle\",\n          \"competition\" : 'Jigsaw',\n          \"_wandb_kernel\" : 'neuracort',\n          \"wandb\" : True\n          }\n\n# Seed\npl.seed_everything(seed=42)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T12:18:18.432475Z","iopub.execute_input":"2021-11-26T12:18:18.432985Z","iopub.status.idle":"2021-11-26T12:18:18.654508Z","shell.execute_reply.started":"2021-11-26T12:18:18.432946Z","shell.execute_reply":"2021-11-26T12:18:18.653864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<a id=\"weights-and-biases\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Weights and Biases</center></h2>","metadata":{}},{"cell_type":"markdown","source":"<center><img src = \"https://i.imgur.com/1sm6x8P.png\" width = \"750\" height = \"500\"/></center>        ","metadata":{}},{"cell_type":"markdown","source":"**Weights & Biases** is the machine learning platform for developers to build better models faster.\n\nYou can use W&B's lightweight, interoperable tools to\n\n- quickly track experiments,\n- version and iterate on datasets,\n- evaluate model performance,\n- reproduce models,\n- visualize results and spot regressions,\n- and share findings with colleagues.\n  \nSet up W&B in 5 minutes, then quickly iterate on your machine learning pipeline with the confidence that your datasets and models are tracked and versioned in a reliable system of record.\n\nIn this notebook I will use Weights and Biases's amazing features to perform wonderful visualizations and logging seamlessly.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<a id=\"utilities\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Utilities</center></h2>","metadata":{}},{"cell_type":"code","source":"def fetch_scheduler(optimizer):\n    \n    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(\n            optimizer,\n            T_max=CONFIG['T_max'],\n            eta_min=CONFIG['min_lr']\n        )\n        \n    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(\n            optimizer,\n            T_0=CONFIG['T_0'],\n            eta_min=CONFIG['min_lr']\n        )\n        \n    elif CONFIG['scheduler'] == None:\n        return None\n        \n    return scheduler","metadata":{"execution":{"iopub.status.busy":"2021-11-26T12:18:21.77148Z","iopub.execute_input":"2021-11-26T12:18:21.772285Z","iopub.status.idle":"2021-11-26T12:18:21.779518Z","shell.execute_reply.started":"2021-11-26T12:18:21.772242Z","shell.execute_reply":"2021-11-26T12:18:21.778389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"# W&B Logger\nwandb_logger = WandbLogger(\n    project='jigsaw-lightning', \n#     group='nlp', \n    job_type='train', \n    anonymous='allow', \n    config=CONFIG\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T12:18:30.499201Z","iopub.execute_input":"2021-11-26T12:18:30.499462Z","iopub.status.idle":"2021-11-26T12:18:30.503647Z","shell.execute_reply.started":"2021-11-26T12:18:30.499431Z","shell.execute_reply":"2021-11-26T12:18:30.502672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<a id=\"dataset\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Dataset</center></h2>","metadata":{}},{"cell_type":"code","source":"class JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.more_toxic = df['more_toxic'].values\n        self.less_toxic = df['less_toxic'].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        more_toxic = self.more_toxic[index]\n        less_toxic = self.less_toxic[index]\n        inputs_more_toxic = self.tokenizer.encode_plus(\n                                more_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        inputs_less_toxic = self.tokenizer.encode_plus(\n                                less_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        target = 1\n        \n        more_toxic_ids = inputs_more_toxic['input_ids']\n        more_toxic_mask = inputs_more_toxic['attention_mask']\n        \n        less_toxic_ids = inputs_less_toxic['input_ids']\n        less_toxic_mask = inputs_less_toxic['attention_mask']\n        \n        \n        return {\n            'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n            'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n            'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n            'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n            'target': torch.tensor(target, dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2021-11-26T12:18:32.139973Z","iopub.execute_input":"2021-11-26T12:18:32.140779Z","iopub.status.idle":"2021-11-26T12:18:32.151246Z","shell.execute_reply.started":"2021-11-26T12:18:32.140731Z","shell.execute_reply":"2021-11-26T12:18:32.149836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<a id=\"datamodule\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Datamodule</center></h2>","metadata":{}},{"cell_type":"code","source":"class JigsawDataModule(pl.LightningDataModule):\n\n  def __init__(self, df_train, df_valid):\n    super().__init__()\n    self.df_train = df_train\n    self.df_valid = df_valid\n\n  def setup(self, stage=None):\n    \n    self.train_dataset = JigsawDataset(\n        self.df_train, \n        tokenizer = CONFIG['tokenizer'], \n        max_length=CONFIG['max_length']\n    )\n    \n    self.valid_dataset = JigsawDataset(\n        self.df_valid, \n        tokenizer=CONFIG['tokenizer'], \n        max_length=CONFIG['max_length']\n    )\n\n  def train_dataloader(self):\n    return DataLoader(\n      self.train_dataset,\n      batch_size=CONFIG['train_batch_size'],\n      num_workers=CONFIG[\"num_workers\"],\n      shuffle=True,\n      pin_memory=True, \n      drop_last=True\n    )\n\n  def val_dataloader(self):\n    return DataLoader(\n      self.valid_dataset,\n      batch_size=CONFIG['valid_batch_size'],\n      num_workers=CONFIG[\"num_workers\"],\n      shuffle=False, \n      pin_memory=True\n    )","metadata":{"execution":{"iopub.status.busy":"2021-11-26T12:18:44.586784Z","iopub.execute_input":"2021-11-26T12:18:44.587526Z","iopub.status.idle":"2021-11-26T12:18:44.596695Z","shell.execute_reply.started":"2021-11-26T12:18:44.587488Z","shell.execute_reply":"2021-11-26T12:18:44.594147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<a id=\"model\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Model</center></h2>","metadata":{}},{"cell_type":"markdown","source":"## **<span style=\"color:orange;\">Margin Ranking Loss</span>**\n\nCreates a criterion that measures the loss given inputs x1, x2, two 1D mini-batch Tensors, and a label 1D mini-batch tensor yy (containing 1 or -1).\n  \nIf y = 1 then it assumed the first input should be ranked higher (have a larger value) than the second input, and vice-versa for y = -1.\n\nThe loss function for each pair of samples in the mini-batch is:\n\n` loss(x1, x2, y) = max(0, -y * (x1 - x2) + margin)`\n\nRefer to [docs](https://pytorch.org/docs/stable/generated/torch.nn.MarginRankingLoss.html) to understand all the parameters.","metadata":{}},{"cell_type":"code","source":"class JigsawModel(pl.LightningModule):\n    \n    def __init__(self, model_name):\n        super(JigsawModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.2)\n        self.fc = nn.Linear(768, CONFIG['num_classes'])\n        \n    def forward(self, ids, mask):        \n        out = self.model(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        out = self.drop(out[1])\n        outputs = self.fc(out)\n                    \n        return outputs\n    \n    def training_step(self, batch, batch_idx):\n        more_toxic_ids = batch['more_toxic_ids']\n        more_toxic_mask = batch['more_toxic_mask']\n        less_toxic_ids = batch['less_toxic_ids']\n        less_toxic_mask = batch['less_toxic_mask']\n        targets = batch['target']\n        \n        more_toxic_outputs = self(more_toxic_ids, more_toxic_mask)\n        less_toxic_outputs = self(less_toxic_ids, less_toxic_mask)\n        \n        loss = self.criterion(more_toxic_outputs, less_toxic_outputs, targets)\n        \n        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n        \n        return {\"loss\": loss}\n    \n    def validation_step(self, batch, batch_idx):\n        more_toxic_ids = batch['more_toxic_ids']\n        more_toxic_mask = batch['more_toxic_mask']\n        less_toxic_ids = batch['less_toxic_ids']\n        less_toxic_mask = batch['less_toxic_mask']\n        targets = batch['target']\n        \n        more_toxic_outputs = self(more_toxic_ids, more_toxic_mask)\n        less_toxic_outputs = self(less_toxic_ids, less_toxic_mask)\n        \n        loss = self.criterion(more_toxic_outputs, less_toxic_outputs, targets)\n        \n        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n        \n        return {'val_loss': loss}      \n        \n    def configure_optimizers(self):\n        \n        optimizer = AdamW(self.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n        scheduler = fetch_scheduler(optimizer)\n        \n        return dict(\n            optimizer = optimizer,\n            lr_scheduler = scheduler\n        )\n    \n    def criterion(self, outputs1, outputs2, targets):\n        return nn.MarginRankingLoss(margin=CONFIG['margin'])(outputs1, outputs2, targets)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T12:18:47.729858Z","iopub.execute_input":"2021-11-26T12:18:47.730314Z","iopub.status.idle":"2021-11-26T12:18:47.743003Z","shell.execute_reply.started":"2021-11-26T12:18:47.730273Z","shell.execute_reply":"2021-11-26T12:18:47.742194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<a id=\"understanding-mixed-precision\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Understanding Mixed Precision</center></h2>","metadata":{}},{"cell_type":"markdown","source":"Read [NVIDIA Docs](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html) for additional examples.\n\n## **<span style=\"color:orange;\">Introduction</span>**\n\n**There are numerous benefits to using numerical formats with lower precision than 32-bit floating point**: \n\n1. They require less memory, enabling the training and deployment of larger neural networks.   \n2. They require less memory bandwidth, thereby speeding up data transfer operations.     \n3. Math operations run much faster in reduced precision, especially on GPUs with \n  \nTensor Core support for that precision. Mixed precision training achieves all these benefits while ensuring that no task-specific accuracy is lost compared to full precision training. It does so by identifying the steps that require full precision and using 32-bit floating point for only those steps while using 16-bit floating point everywhere else.\n\n---\n\n## **<span style=\"color:orange;\">Mixed Precision Training</span>**\n\nMixed precision training offers significant computational speedup by performing operations in half-precision format, while storing minimal information in single-precision to retain as much information as possible in critical parts of the network. \n  \nSince the introduction of Tensor Cores in the Volta and Turing architectures, significant training speedups are experienced by switching to mixed precision -- up to 3x overall speedup on the most arithmetically intense model architectures.\n  \n**Using mixed precision training requires two steps:**\n\n1. Porting the model to use the FP16 data type where appropriate.\n2. Adding loss scaling to preserve small gradient values.\n  \nThe ability to train deep learning networks with lower precision was introduced in the Pascal architecture and first supported in CUDA® 8 in the NVIDIA Deep Learning SDK.\n  \nMixed precision is the combined use of different numerical precisions in a computational method.\n  \n> **Half precision** **(also known as FP16)** data compared to higher precision FP32 vs FP64 reduces memory usage of the neural network, allowing training and deployment of larger networks, and FP16 data transfers take less time than FP32 or FP64 transfers.\n>   \n> **Single precision** **(also known as 32-bit)** is a common floating point format (float in C-derived programming languages), and 64-bit, known as double precision (double).\n\nDeep Neural Networks (DNNs) have led to breakthroughs in a number of areas, including image processing and understanding, language modeling, language translation, speech processing, game playing, and many others. DNN complexity has been increasing to achieve these results, which in turn has increased the computational resources required to train these networks. \n  \nOne way to lower the required resources is to use lower-precision arithmetic, which has the following benefits.\n  \n> **Decrease the required amount of memory**  \n> Half-precision floating point format (FP16) uses 16 bits, compared to 32 bits for single precision (FP32). Lowering the required memory enables training of larger models or training with larger mini-batches.\n>   \n> **Shorten the training or inference time**  \n> Execution time can be sensitive to memory or arithmetic bandwidth. Half-precision halves the number of bytes accessed, thus reducing the time spent in memory-limited layers. NVIDIA GPUs offer up to 8x more half precision arithmetic throughput when compared to single-precision, thus speeding up math-limited layers.\n\n---","metadata":{}},{"cell_type":"markdown","source":"<a id=\"understanding-tpus\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Understanding TPUs</center></h2>","metadata":{}},{"cell_type":"markdown","source":"To view the original pytorch implementation by Tanul refer [this](https://www.kaggle.com/tanulsingh077/pytorch-xla-understanding-tpu-s-and-xla) notebook.\n\n## **<span style=\"color:orange;\">What are TPU's? How they work? How are they different from a GPU?</span>**\n\nYou might be thinking why knowing how tpus work is important , well it's not a must but to exploit something fully we must know how it works right?\nTPUs are hardware accelerators specialized in deep learning tasks. For explanation of what  TPU's are and how they work please go through the following videos :\n* [video1](https://www.youtube.com/watch?v=MXxN4fv01c8)\n* [video2](https://www.youtube.com/watch?v=kBjYK3K3P6M)<br><br>\nIts important to understand the underlying concepts of Pytorch XLA's . If you want to dig even deeper [here](https://codelabs.developers.google.com/codelabs/keras-flowers-data/#2) is a article by google explaining everything about TPU's\n\n---\n\n## **<span style=\"color:orange;\">Key Takeaways</span>**\n\nFollowing are the key takeaways from the above videos and articles :-\n\n* Each TPU v3 board has 8 TPU cores and 64 GB's of memory\n* TPU's consist of two units, Matrix Multiply Unit (MXU) which runs matrix multiplications and a Vector Processing Unit (VPU) for all other tasks such as activations, softmax, etc.\n* TPU's v2/v3 use a new type of dtype called bfloat16 which combines the range of a 32-bit floating point number with just the storage space of only a 16-bit floating point number and this allows to do fit more matrices in the memory and thus more matrix multiplications. This increased speed comes at the cost of precision as bfloat16 is able to represent fewer decimal places as compared to 16-bit floating point integer but its ohk because neural networks can work at a reduced precision while maintaining their high accuracy\n* The ideal batch size for TPUs is 128 data items per TPU core but the hardware can already show good utilization from 8 data items per TPU core\n\n---\n\n**Now we move onto the final question does TPU's directly run the Python code? Or is there something else working under the hood without credits**\n\n![](https://3s81si1s5ygj3mzby34dq6qf-wpengine.netdna-ssl.com/wp-content/uploads/2018/12/bfloat.jpg)\n\n---\n\n## **<span style=\"color:orange;\">Under the Hood</span>**\n\n* We know that any deep learning framework first defines a computation graph which is then executed by any processing chip to train a neural network. Similarly, The TPU does not directly run Python code, it runs the computation graph defined by your program.However the computation graph is first converted into TPU machine code. Under the hood, a compiler called XLA (accelerated Linear Algebra compiler) transforms the graph of computation nodes into TPU machine code. This compiler also performs many advanced optimizations on your code and your memory layout. \n* In tensorflow the conversion from computation to TPU machine code automatically takes place as work is sent to the TPU, whereas there was no such support for Pytorch and thus XLA module was created to include XLA in our build chain explicitly.\n\n![](https://lh5.googleusercontent.com/NjGqp60oF_3Bu4Q63dprSivZ77BgVnaPEp0Olk1moFm8okcmMfPXs7PIJBgL9LB5QCtqlmM4WTepYxPC5Mq_i_0949sWSpq8pKvfPAkHnFJWuHjrNVLPN2_a0eggOlteV7mZB_Z9)\n\n---","metadata":{}},{"cell_type":"markdown","source":"<a id=\"train\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Train</center></h2>","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":    \n    for fold in range(0, 1): # Replace `1` with `CONFIG['n_fold']` to run for all folds\n        print(f\"{y_}====== Fold: {fold} ======{sr_}\")\n        \n        df = pd.read_csv(CONFIG['train_file_path'])        \n        \n        logger = TensorBoardLogger(\"lightning_logs\", name=\"toxic-comments\")\n\n        checkpoint_callback = ModelCheckpoint(\n          dirpath=CONFIG[\"checkpoint_directory_path\"],\n          filename= f\"fold_{fold}_roberta-base\",\n          save_top_k=1,\n          verbose=True,\n          monitor=\"val_loss\",\n          mode=\"min\"\n        )\n\n        early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)\n\n        trainer = pl.Trainer(\n          logger=wandb_logger,\n          callbacks=[checkpoint_callback, early_stopping_callback],\n          max_epochs=CONFIG['epochs'],\n          gpus=-1,\n          progress_bar_refresh_rate=30,\n          precision=16,                # Activate fp16 Training\n#           accelerator = 'dp'         # Un-comment for Multi-GPU Training\n        )\n\n        df_train = df[df.kfold != fold].reset_index(drop=True)\n        df_valid = df[df.kfold == fold].reset_index(drop=True)  \n\n        data_module = JigsawDataModule(df_train, df_valid)\n\n        model = JigsawModel(CONFIG['model_name'])    \n        trainer.fit(model, data_module)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T12:18:57.794837Z","iopub.execute_input":"2021-11-26T12:18:57.795402Z","iopub.status.idle":"2021-11-26T12:40:32.868268Z","shell.execute_reply.started":"2021-11-26T12:18:57.795362Z","shell.execute_reply":"2021-11-26T12:40:32.867361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [Check out the run page here $\\rightarrow$](https://wandb.ai/ishandutta/jigsaw-lightning/runs/23m009ta?workspace=user-ishandutta)","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:maroon; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>If you find this notebook useful, do give me an upvote, it helps to keep up my motivation. This notebook will be updated frequently so keep checking for furthur developments.</center></h3>","metadata":{}},{"cell_type":"markdown","source":"--- \n\n## **<span style=\"color:orange;\">Let's have a Talk!</span>**\n> ### Reach out to me on [LinkedIn](https://www.linkedin.com/in/ishandutta0098)\n\n---","metadata":{}}]}