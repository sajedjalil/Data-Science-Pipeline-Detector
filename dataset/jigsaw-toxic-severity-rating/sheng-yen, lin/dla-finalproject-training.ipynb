{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport copy\nimport pickle\nfrom tqdm.auto import tqdm\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport random\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom gensim.models import KeyedVectors, FastText\n\n#torch packages\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n#transformer packages\nfrom transformers import BertTokenizer, BertModel\nfrom transformers import RobertaTokenizer, RobertaModel\nfrom transformers import AdamW\nfrom transformers import get_scheduler\nfrom transformers import logging\nlogging.set_verbosity_error() #turn off bert warning\nlogging.set_verbosity_warning() #turn off bert warning","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:29:20.646076Z","iopub.execute_input":"2022-01-16T10:29:20.646384Z","iopub.status.idle":"2022-01-16T10:29:30.751583Z","shell.execute_reply.started":"2022-01-16T10:29:20.646352Z","shell.execute_reply":"2022-01-16T10:29:30.75078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \ndef generate_fold_num_for_dataset(data, num_fold):\n    skf = StratifiedKFold(n_splits=num_fold, shuffle=True)\n    for fold, ( _, val_) in enumerate(skf.split(X=data, y=data.worker)):\n        data.loc[val_ , \"kfold\"] = int(fold)\n    data[\"kfold\"] = data[\"kfold\"].astype(int)\n    return data\n\nclass JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length, use_tfidf=False, tfidf_matrix=None, use_sentence_embedding=False, embed_matrix=None):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.more_toxic = df['more_toxic'].values\n        self.less_toxic = df['less_toxic'].values\n        self.use_tfidf = use_tfidf\n        self.use_sentence_embedding = use_sentence_embedding\n        if use_tfidf:\n            self.more_toxic_sentence_idx = df['more_toxic_sentence_idx'].values\n            self.less_toxic_sentence_idx = df['less_toxic_sentence_idx'].values\n            self.tfidf_matrix = tfidf_matrix\n        elif use_sentence_embedding:\n            self.more_toxic_sentence_idx = df['more_toxic_sentence_idx'].values\n            self.less_toxic_sentence_idx = df['less_toxic_sentence_idx'].values\n            self.embed_matrix = embed_matrix\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        more_toxic = self.more_toxic[index]\n        less_toxic = self.less_toxic[index]\n        inputs_more_toxic = self.tokenizer.encode_plus(\n                                more_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        inputs_less_toxic = self.tokenizer.encode_plus(\n                                less_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        target = 1\n        \n        more_toxic_ids = inputs_more_toxic['input_ids']\n        more_toxic_mask = inputs_more_toxic['attention_mask']        \n        less_toxic_ids = inputs_less_toxic['input_ids']\n        less_toxic_mask = inputs_less_toxic['attention_mask']\n        \n        if self.use_tfidf:\n            more_toxic_sentence_idx = self.more_toxic_sentence_idx[index]\n            less_toxic_sentence_idx = self.less_toxic_sentence_idx[index]\n            more_toxic_tfidf = self.tfidf_matrix[more_toxic_sentence_idx]\n            less_toxic_tfidf = self.tfidf_matrix[less_toxic_sentence_idx]\n            return {\n                'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n                'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n                'more_toxic_tfidf': torch.tensor(more_toxic_tfidf, dtype=torch.long),\n                'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n                'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n                'less_toxic_tfidf': torch.tensor(less_toxic_tfidf, dtype=torch.long),\n                'target': torch.tensor(target, dtype=torch.long)\n            }\n        elif self.use_sentence_embedding:\n            more_toxic_sentence_idx = self.more_toxic_sentence_idx[index]\n            less_toxic_sentence_idx = self.less_toxic_sentence_idx[index]\n            more_toxic_sent_embed = self.embed_matrix[more_toxic_sentence_idx]\n            less_toxic_sent_embed = self.embed_matrix[less_toxic_sentence_idx]\n            return {\n                'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n                'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n                'more_toxic_sent_embed': torch.tensor(more_toxic_sent_embed, dtype=torch.long),\n                'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n                'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n                'less_toxic_sent_embed': torch.tensor(less_toxic_sent_embed, dtype=torch.long),\n                'target': torch.tensor(target, dtype=torch.long)\n            }\n        else:\n            return {\n                'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n                'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n                'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n                'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n                'target': torch.tensor(target, dtype=torch.long)\n            }","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:29:56.180862Z","iopub.status.idle":"2022-01-16T10:29:56.181171Z","shell.execute_reply.started":"2022-01-16T10:29:56.181004Z","shell.execute_reply":"2022-01-16T10:29:56.181019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#-> CONV/FC -> BatchNorm -> ReLu(or other activation) -> Dropout -> CONV/FC ->\nclass NN(nn.Module):\n    def __init__(self, bert_drop_out, HID_DIM=768, tfidf_len=0, use_tfidf=False, use_sentence_embedding=False, embed_len=0):\n        super().__init__()\n        if use_tfidf:\n            self.net = nn.Sequential(\n                nn.Dropout(p=bert_drop_out), #dropout for bert\n                nn.Linear(768+tfidf_len, 1)\n                \n#                 nn.Linear(768+tfidf_len, HID_DIM),\n#                 nn.BatchNorm1d(HID_DIM),\n#                 nn.ReLU(),\n#                 nn.Dropout(0.3),\n                \n#                 nn.Linear(HID_DIM, HID_DIM),\n#                 nn.BatchNorm1d(HID_DIM),\n#                 nn.ReLU(),\n#                 nn.Dropout(0.3),       \n\n#                 nn.Linear(HID_DIM, 1) \n            )\n        elif use_sentence_embedding:\n            self.net = nn.Sequential(\n                nn.Dropout(p=bert_drop_out), #dropout for bert\n                nn.Linear(768+embed_len, 1)      \n            )\n        else:\n            self.net = nn.Sequential(\n                nn.Dropout(p=bert_drop_out), #dropout for bert\n                nn.Linear(768, 1)\n#                 nn.Linear(768, HID_DIM),\n#                 nn.BatchNorm1d(HID_DIM),\n#                 nn.ReLU(),\n#                 nn.Dropout(0.3),\n                \n#                 nn.Linear(HID_DIM, HID_DIM),\n#                 nn.BatchNorm1d(HID_DIM),\n#                 nn.ReLU(),\n#                 nn.Dropout(0.3),       \n\n#                 nn.Linear(HID_DIM, 1) \n            )\n            \n    def forward(self, x):\n        score = self.net(x)\n        return score\n                \nclass JigsawModel(nn.Module):\n    def __init__(self, BERT, NN):\n        super(JigsawModel, self).__init__()\n        self.bert = BERT\n        self.fc = NN\n        \n    def forward(self, ids, mask, tfidf_vec=None, use_tfidf=False, sent_embed=None, use_sentence_embedding=False):        \n        out = self.bert(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        if use_tfidf:\n            fc_in = torch.cat(\n                (out[\"pooler_output\"], tfidf_vec), dim=1\n            )\n        elif use_sentence_embedding:\n            fc_in = torch.cat(\n                (out[\"pooler_output\"], sent_embed), dim=1\n            )        \n        else:\n            fc_in = out[\"pooler_output\"]\n        outputs = self.fc(fc_in)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:29:56.182867Z","iopub.status.idle":"2022-01-16T10:29:56.183415Z","shell.execute_reply.started":"2022-01-16T10:29:56.183125Z","shell.execute_reply":"2022-01-16T10:29:56.183152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_step_combine(\n    model, criterion, optimizer, \n    train_loader, progress_bar, device, epoch, use_tfidf = False, use_sentence_embedding=False\n):\n    y_preds = []\n    epoch_loss = 0\n    model.train()\n    for i, data in enumerate(train_loader):\n        more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n        more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n        less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n        less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n        targets = data['target'].to(device, dtype=torch.long)\n\n        optimizer.zero_grad()\n        \n        if use_tfidf:\n            more_toxic_tfidf = data['more_toxic_tfidf'].to(device, dtype = torch.long)\n            less_toxic_tfidf = data['less_toxic_tfidf'].to(device, dtype = torch.long)\n            more_out = model(more_toxic_ids, more_toxic_mask, more_toxic_tfidf, use_tfidf=True)\n            less_out = model(less_toxic_ids, less_toxic_mask, less_toxic_tfidf, use_tfidf=True)\n        elif use_sentence_embedding:\n            more_toxic_sent_embed = data['more_toxic_sent_embed'].to(device, dtype = torch.long)\n            less_toxic_sent_embed = data['less_toxic_sent_embed'].to(device, dtype = torch.long)\n            more_out = model(more_toxic_ids, more_toxic_mask, sent_embed=more_toxic_sent_embed, use_sentence_embedding=True)\n            less_out = model(less_toxic_ids, less_toxic_mask, sent_embed=less_toxic_sent_embed, use_sentence_embedding=True)           \n        else:\n            more_out = model(more_toxic_ids, more_toxic_mask)\n            less_out = model(less_toxic_ids, less_toxic_mask)\n            \n        loss = criterion(more_out, less_out, targets)\n\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        \n        for j in range(len(data['more_toxic_ids'])):\n            y_preds.append([less_out[j].item(), more_out[j].item()])\n        \n        if progress_bar is not None:\n            progress_bar.update(1)  \n            \n        print('[ Epoch {}: {}/{} ] loss:{:.3f}'.format(epoch, i+1, len(train_loader), loss.item()), end='\\r')\n    \n    df_score = pd.DataFrame(y_preds,columns=['less','more'])\n    train_accuracy = validate_accuracy(df_score)         \n    \n    return df_score, train_accuracy, epoch_loss / len(train_loader) # return loss\n\n\ndef validate_all_combine(\n    model, criterion, \n    valid_loader, device, use_tfidf=False, use_sentence_embedding=False\n):\n    epoch_loss = 0\n    y_preds = []\n    \n    model.eval()\n    with torch.no_grad():\n        for data in valid_loader:\n            more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n            more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n            less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n            less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n            targets = data['target'].to(device, dtype=torch.long)\n            \n            if use_tfidf:\n                more_toxic_tfidf = data['more_toxic_tfidf'].to(device, dtype = torch.long)\n                less_toxic_tfidf = data['less_toxic_tfidf'].to(device, dtype = torch.long)\n                more_out = model(more_toxic_ids, more_toxic_mask, more_toxic_tfidf, use_tfidf=True)\n                less_out = model(less_toxic_ids, less_toxic_mask, less_toxic_tfidf, use_tfidf=True)\n            elif use_sentence_embedding:\n                more_toxic_sent_embed = data['more_toxic_sent_embed'].to(device, dtype = torch.long)\n                less_toxic_sent_embed = data['less_toxic_sent_embed'].to(device, dtype = torch.long)\n                more_out = model(more_toxic_ids, more_toxic_mask, sent_embed=more_toxic_sent_embed, use_sentence_embedding=True)\n                less_out = model(less_toxic_ids, less_toxic_mask, sent_embed=less_toxic_sent_embed, use_sentence_embedding=True)  \n            else:\n                more_out = model(more_toxic_ids, more_toxic_mask)\n                less_out = model(less_toxic_ids, less_toxic_mask)\n            \n            loss = criterion(more_out, less_out, targets)\n\n            epoch_loss += loss.item()\n            for i in range(len(data['more_toxic_ids'])):\n                y_preds.append([less_out[i].item(), more_out[i].item()])\n        df_score = pd.DataFrame(y_preds,columns=['less','more'])\n        accuracy = validate_accuracy(df_score)\n    return df_score, accuracy, (epoch_loss / len(valid_loader))\n\ndef validate_accuracy(df_score):\n    return len(df_score[df_score['less'] < df_score['more']]) / len(df_score)\n\ndef return_wrong_text(df_score, df_valid):\n    df_score_text = pd.concat((df_valid.reset_index().drop('index',axis=1),df_score),axis=1)\n    return df_score_text[df_score_text['less'] > df_score_text['more']]\n\ndef init_weights(m):\n    if isinstance(m, nn.Linear):\n        torch.nn.init.xavier_uniform_(m.weight)\n        m.bias.data.fill_(0.01)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:29:56.185577Z","iopub.status.idle":"2022-01-16T10:29:56.186526Z","shell.execute_reply.started":"2022-01-16T10:29:56.186227Z","shell.execute_reply":"2022-01-16T10:29:56.186259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_loss_plot(train_losses, valid_losses):\n    plt.plot(train_losses,label=\"Training\")\n    plt.plot(valid_losses,label=\"Validation\")\n    plt.title(\"Loss plot\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n    \ndef plot_acc_plot(train_accs, valid_accs):\n    plt.plot(train_accs,label=\"Training\")\n    plt.plot(valid_accs,label=\"Validation\")\n    plt.title(\"Accuracy plot\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:29:56.188251Z","iopub.status.idle":"2022-01-16T10:29:56.188766Z","shell.execute_reply.started":"2022-01-16T10:29:56.188482Z","shell.execute_reply":"2022-01-16T10:29:56.188508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_duplicates(df, used_col):\n    \"\"\"Combine `less_toxic` text and `more_toxic` text,\n    then remove duplicate pair of comments while keeping the last pair\n    \"\"\"\n    df[\"combine\"] = df[\"less_toxic\"] + df[\"more_toxic\"]\n    df = df.drop_duplicates(subset=used_col, keep=\"last\")\n    return df\n\ndef create_corpus(df_train):\n    all_corpus = df_train[\"more_toxic\"].to_list()\n    all_corpus += df_train[\"less_toxic\"].to_list()\n    #remove duplicates\n    all_corpus = list(\n        set(all_corpus)\n    )\n    return all_corpus\n\ndef create_mapping_dict(corpus):\n    idx = np.arange(len(corpus))\n    sentence2idx = dict(\n        zip(corpus, idx)\n    )\n    return sentence2idx\n\ndef tokenize_by_bert_tokenizer(corpus, tokenizer):\n    corpus_tokenized = [\n        tokenizer.tokenize(sentence) for sentence in corpus\n    ]\n    return corpus_tokenized\n\ndef identity_tokenizer(text):\n    return text\n\ndef corpus2tfidf(corpus_tokenized):\n    tfidf = TfidfVectorizer(preprocessor=identity_tokenizer, tokenizer=identity_tokenizer)    \n    tfidf_matrix_sparse = tfidf.fit_transform(corpus_tokenized)\n    tfidf_matrix = tfidf_matrix_sparse.toarray()\n    return tfidf, tfidf_matrix\n    \ndef construct_tfidf_matrix(df_train, tokenizer):\n    corpus = create_corpus(df_train)\n    sentence2idx = create_mapping_dict(corpus)\n    corpus_tokenized = tokenize_by_bert_tokenizer(corpus, tokenizer)\n    tfidf_obj, tfidf_matrix = corpus2tfidf(corpus_tokenized)\n    return sentence2idx, tfidf_obj, tfidf_matrix","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:29:56.190398Z","iopub.status.idle":"2022-01-16T10:29:56.190903Z","shell.execute_reply.started":"2022-01-16T10:29:56.190613Z","shell.execute_reply":"2022-01-16T10:29:56.19064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_seed(5080)\ndata_train = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\ndata_test = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ndata_add = pd.read_csv(\"../input/additional-data/classification_data.csv\", index_col = 0)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n#load model and tokenizer\n# PRETRAINED_MODEL_NAME = \"GroNLP/hateBERT\"\n# bert_tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\nPRETRAINED_MODEL_NAME = \"roberta-base\"\nbert_tokenizer = RobertaTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:30:16.060799Z","iopub.execute_input":"2022-01-16T10:30:16.061072Z","iopub.status.idle":"2022-01-16T10:30:19.362386Z","shell.execute_reply.started":"2022-01-16T10:30:16.061044Z","shell.execute_reply":"2022-01-16T10:30:19.361292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data_add = data_add.drop([\"less_score\", \"more_score\"], axis = 1)\n# data_train = pd.concat(\n#     [data_train, data_add], axis = 0\n# )","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:29:56.195612Z","iopub.status.idle":"2022-01-16T10:29:56.196156Z","shell.execute_reply.started":"2022-01-16T10:29:56.195863Z","shell.execute_reply":"2022-01-16T10:29:56.19589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TFIDF preprocessing","metadata":{}},{"cell_type":"code","source":"#text preprocessing\ndata_train_removed = remove_duplicates(data_train, \"combine\") #(30108, 4) -> (15410, 4)\n\n#construct tfidf一定要用removed!\n#corpus_tokenized = construct_tfidf_matrix(data_train_removed, bert_tokenizer)\nsentence2idx, tfidf_obj, tfidf_matrix = construct_tfidf_matrix(data_train_removed, bert_tokenizer)\ndata_train[\"less_toxic_sentence_idx\"] = data_train[\"less_toxic\"].apply(lambda x: sentence2idx[x])\ndata_train[\"more_toxic_sentence_idx\"] = data_train[\"more_toxic\"].apply(lambda x: sentence2idx[x])\ntoken2idx = tfidf_obj.vocabulary_\ntoken_list = list(\n    tfidf_obj.vocabulary_.keys()\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:30:19.364407Z","iopub.execute_input":"2022-01-16T10:30:19.36465Z","iopub.status.idle":"2022-01-16T10:30:37.807282Z","shell.execute_reply.started":"2022-01-16T10:30:19.364621Z","shell.execute_reply":"2022-01-16T10:30:37.806239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('./tfidf_roberta_obj.pickle', 'wb') as f:\n    pickle.dump(tfidf_obj, f)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:31:49.28732Z","iopub.execute_input":"2022-01-16T10:31:49.287595Z","iopub.status.idle":"2022-01-16T10:31:49.315111Z","shell.execute_reply.started":"2022-01-16T10:31:49.287565Z","shell.execute_reply":"2022-01-16T10:31:49.313912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tfidf_obj)\nprint(tfidf_matrix.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:30:37.808991Z","iopub.execute_input":"2022-01-16T10:30:37.810552Z","iopub.status.idle":"2022-01-16T10:30:37.818376Z","shell.execute_reply.started":"2022-01-16T10:30:37.810513Z","shell.execute_reply":"2022-01-16T10:30:37.817142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentence embedding (TFIDF x FastText)","metadata":{}},{"cell_type":"code","source":"# fmodel = FastText.load(\n#     '../input/jigsaw-regression-based-data/FastText-jigsaw-256D/Jigsaw-Fasttext-Word-Embeddings-256D.bin'\n# )","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:29:55.806091Z","iopub.execute_input":"2022-01-16T10:29:55.806971Z","iopub.status.idle":"2022-01-16T10:29:55.812002Z","shell.execute_reply.started":"2022-01-16T10:29:55.806925Z","shell.execute_reply":"2022-01-16T10:29:55.81122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #build w2v\n# w2v_embed_dim = 256\n# w2v = np.zeros(\n#     (len(token2idx), w2v_embed_dim)\n# )\n# for tok in token_list:\n#     token_idx = token2idx[tok]\n#     w2v[token_idx] = fmodel.wv[tok]","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:29:55.813747Z","iopub.execute_input":"2022-01-16T10:29:55.814346Z","iopub.status.idle":"2022-01-16T10:29:55.823954Z","shell.execute_reply.started":"2022-01-16T10:29:55.814302Z","shell.execute_reply":"2022-01-16T10:29:55.823303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sentence_embedding = np.dot(\n#     tfidf_matrix, w2v\n# )","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:29:55.827966Z","iopub.execute_input":"2022-01-16T10:29:55.828351Z","iopub.status.idle":"2022-01-16T10:29:56.170094Z","shell.execute_reply.started":"2022-01-16T10:29:55.828321Z","shell.execute_reply":"2022-01-16T10:29:56.168755Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sentence_embedding.shape\n","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:29:56.17113Z","iopub.status.idle":"2022-01-16T10:29:56.171491Z","shell.execute_reply.started":"2022-01-16T10:29:56.171299Z","shell.execute_reply":"2022-01-16T10:29:56.171323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset and Dataloader","metadata":{}},{"cell_type":"code","source":"#paramters settings\ntrain_valid_ratio = 0.25\nmax_token_length = 128 #代表最多放入BERT的token長度\ntrain_batch_size = 32\nvalid_batch_size = 64\nuse_tfidf = True\nuse_sentence_embedding = False\n\ndf_train, df_valid = train_test_split(\n    data_train, test_size = train_valid_ratio\n)\n\n\nif use_tfidf:\n    tfidf_len = tfidf_matrix.shape[1]\n    train_dataset = JigsawDataset(\n        df_train, tokenizer=bert_tokenizer, \n        max_length=max_token_length , use_tfidf=True, tfidf_matrix=tfidf_matrix\n     )\n    valid_dataset = JigsawDataset(\n        df_valid, tokenizer=bert_tokenizer, \n        max_length=max_token_length , use_tfidf=True, tfidf_matrix=tfidf_matrix\n    )\nelif use_sentence_embedding:\n    embed_len = sentence_embedding.shape[1]\n    train_dataset = JigsawDataset(\n        df_train, tokenizer=bert_tokenizer, \n        max_length=max_token_length, use_sentence_embedding=True, embed_matrix=sentence_embedding\n     )\n    valid_dataset = JigsawDataset(\n        df_valid, tokenizer=bert_tokenizer, \n        max_length=max_token_length, use_sentence_embedding=True, embed_matrix=sentence_embedding\n    )\nelse:\n    train_dataset = JigsawDataset(\n        df_train, tokenizer=bert_tokenizer, \n        max_length=max_token_length \n     )\n    valid_dataset = JigsawDataset(\n        df_valid, tokenizer=bert_tokenizer, \n        max_length=max_token_length\n    )\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=train_batch_size, shuffle=True,\n    num_workers=2)\nvalid_loader = DataLoader(\n    valid_dataset, batch_size=valid_batch_size, shuffle=False,\n    num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:31:51.75147Z","iopub.execute_input":"2022-01-16T10:31:51.751791Z","iopub.status.idle":"2022-01-16T10:31:51.776169Z","shell.execute_reply.started":"2022-01-16T10:31:51.751758Z","shell.execute_reply":"2022-01-16T10:31:51.775096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"#### best settings (until 1/12 10:30)\n- PRETRAINED_MODEL_NAME = \"roberta-base\"\n- train_valid_ratio = 0.25\n- max_token_length = 128\n- batch_size = 32\n- LR = 1e-4\n- WD = 1e-6\n- bert dropout rate = 0.3\n- EPOCH = 10\n- NO-TFIDF\n- init FC weight","metadata":{}},{"cell_type":"code","source":"# LR = 1e-4\n# WD = 1e-6\n# bert_drop_out = 0.2\n# margin_list = [0.5]\n\nLR_list = [1e-4]\nWD_list = [1e-6]\nbert_dropout_list = [0.3]\nmax_patience = 3\n\nEPOCH = 10\nHID_DIM = 768\nMARGIN = 0.5\nDATE = \"0116\"\nmodel_name = \"roberta\"\n\n\nfor LR in LR_list:\n    for WD in WD_list:\n        for bert_drop_out in bert_dropout_list:\n            print(f\"LR = {LR}, WD = {WD}, bert drop out = {bert_drop_out}\")\n            \n            ####################################################MODEL SETTINGS########################################\n            bert = RobertaModel.from_pretrained(PRETRAINED_MODEL_NAME).to(device)\n            if use_tfidf:\n                dnn = NN(bert_drop_out, HID_DIM, \n                         tfidf_len, use_tfidf=True\n                        ).to(device)\n            elif use_sentence_embedding:\n                dnn = NN(bert_drop_out, HID_DIM, \n                         embed_len=embed_len, use_sentence_embedding=True\n                        ).to(device)              \n            else:\n                dnn = NN(\n                    bert_drop_out, HID_DIM\n                    ).to(device)\n                \n            dnn.apply(init_weights)\n            model = JigsawModel(bert, dnn)\n            trainable_params = list(model.parameters())\n            num_trainable_params = sum(p.numel() for p in trainable_params)\n\n            criterion = nn.MarginRankingLoss(margin=MARGIN)\n            optimizer = AdamW(\n                trainable_params\n                ,lr=LR,weight_decay=WD)\n\n            print(f\"Total trainable parameters {num_trainable_params}\")\n\n            num_training_steps = EPOCH * len(train_loader)\n            ###########################################################################################################\n        \n            ####################################################Records################################################\n            train_accs = []\n            valid_accs = []\n            train_losses = []\n            valid_losses = []\n            best_valid_loss = np.inf\n            best_valid_acc = 0\n            best_epoch = 0\n            best_model = None\n            no_update = 0 #number of non-updated epochs\n            MODEL_DIR = f\"./{DATE}_{model_name}_LR_{LR}_WD_{WD}_BDR_{bert_drop_out}.pth\"\n            ###########################################################################################################\n            \n            # start training\n            #progress_bar = None\n            progress_bar = tqdm(range(num_training_steps))\n            for epoch in range(1, EPOCH+1):\n                _, train_acc, train_loss = train_step_combine(\n                    model, criterion, optimizer, train_loader, progress_bar, \n                    device, epoch, use_tfidf=use_tfidf, use_sentence_embedding=use_sentence_embedding)\n                df_score, valid_acc, valid_loss = validate_all_combine(\n                    model, criterion, valid_loader, device, use_tfidf=use_tfidf, \n                    use_sentence_embedding=use_sentence_embedding\n                )\n                train_accs.append(train_acc)\n                valid_accs.append(valid_acc)\n                train_losses.append(train_loss)\n                valid_losses.append(valid_loss)\n\n                print(f\"Epoch {epoch}, Loss(Train/Valid) = {round(train_loss, 4)}/{round(valid_loss, 4)}, Accuracy(Train/Valid) = {round(train_acc*100, 3)}%/{round(valid_acc*100, 3)}%\")\n\n                if valid_acc > best_valid_acc:\n                    no_update = 0\n                    print(f\"Saving model...\")\n                    best_epoch = epoch\n                    best_valid_acc = valid_acc\n                    best_valid_loss = valid_loss\n                    best_model = model\n                    torch.save(\n                        {\"BERT\": best_model.bert.state_dict(),\"NN\": best_model.fc.state_dict()}\n                        ,MODEL_DIR\n                    )\n                #early stopping\n                else:\n                    no_update += 1 \n                \n                if no_update == max_patience:\n                    break\n                    \n            print(f\"Best epoch: {best_epoch}, valid loss: {round(best_valid_loss, 4)}, valid acc: {round(best_valid_acc*100, 4)}%\")\n\n            #compute acc for all dataset\n            all_dataset = JigsawDataset(\n                df_train, tokenizer=bert_tokenizer, \n                max_length=max_token_length, use_tfidf=use_tfidf, tfidf_matrix=tfidf_matrix,\n                use_sentence_embedding=use_sentence_embedding, embed_matrix=sentence_embedding\n            )\n\n\n            all_loader = DataLoader(\n                all_dataset, batch_size=valid_batch_size, \n                shuffle=True, num_workers=2\n            )\n\n            df_score, accuracy, valid_loss = validate_all_combine(\n                model, criterion, all_loader, device, \n                use_tfidf=use_tfidf, use_sentence_embedding=use_sentence_embedding\n            )\n            df_score_text = return_wrong_text(df_score, df_valid)\n            csv_out_path = f\"./wrong_text_{DATE}_LR_{LR}_WD_{WD}_BDR_{bert_drop_out}.csv\"\n            df_score_text.to_csv(csv_out_path)\n            print(f\"Accuracy in all dataset set: {round(accuracy*100, 3)}%\")\n\n            #Evaluation\n            plot_loss_plot(train_losses, valid_losses)\n            plot_acc_plot(train_accs, valid_accs)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:32:03.839776Z","iopub.execute_input":"2022-01-16T10:32:03.840408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all_dataset = JigsawDataset(\n#     data_train, tokenizer=bert_tokenizer, \n#     max_length=max_token_length)\n\n# all_loader = DataLoader(\n#     train_dataset, batch_size=batch_size, shuffle=True,\n#     num_workers=2)\n\n# df_score, accuracy, valid_loss = validate_all(bert, dnn, criterion, all_loader, device)\n# print(f\"Accuracy in all dataset set: {round(accuracy*100, 3)}%\")\n\n# df_score_text = return_wrong_text(df_score, df_valid)\n# df_score_text.to_csv(\"./wrong_text_1229_2.csv\")\n# print(df_score_text)\n\n# plot_loss_plot(train_losses, valid_losses)\n# plot_acc_plot(train_accs, valid_accs)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:29:56.179476Z","iopub.status.idle":"2022-01-16T10:29:56.179809Z","shell.execute_reply.started":"2022-01-16T10:29:56.179626Z","shell.execute_reply":"2022-01-16T10:29:56.179642Z"},"trusted":true},"execution_count":null,"outputs":[]}]}