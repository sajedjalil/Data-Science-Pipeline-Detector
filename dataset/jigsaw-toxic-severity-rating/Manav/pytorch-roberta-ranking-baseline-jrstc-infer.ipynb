{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About This Notebook\n\n<p style='font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 15px'> This implementation is based on a vanilla <b>roberta-base</b> using the Huggingface transformer model in Pytorch for the \"Jigsaw Rate Severity of Toxic Comments\" Competition.<br>\n<b>This scores around xx LB.</b></p>\n\n<p style='font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 15px'>The embeddings from both the RoBERTa model are passed through a 2 layer fully connected network and <b>MarginRankingLoss</b> is used on the validation dataset to fine tune the weights.</p>\n\n<p style='font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 15px'>\nTraining Params: -\n<ol style='font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 15px'>\n<li> <b>Dataset</b>: - validation_data.csv - pair rankings that can be used to validate models; this data includes the annotator worker id, and how that annotator ranked a given pair of comments.</li>\n<li> <b>Optimizer</b>: - AdamW </li>\n<li> <b>Scheduler</b>: - OneCycleLR </li>\n<li> <b>Loss Function</b>: - MarginRankingLoss </li>\n<li> <b>Model</b>: - roberta-base</li>\n<li> <b>Max Epochs</b>: - 3 (~35 min per epoch on P100 PCIE GPU) </li>\n<li> <b>Saved Weights</b>: - 5-fold ensemble. Weights having highest OOF loss were saved. </li>\n</ol>\n</p>\n<p style='font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 15px'>\nThis notebook only contains the inference for the model as described above.<br><br>\nIf you are looking for a starter training notebook please follow the link below: -<br></p>\n<ul style='font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 15px'>\n<li>Baseline Model Notebook:- <a href=https://www.kaggle.com/manabendrarout/pytorch-roberta-ranking-baseline-jrstc-train>Training Notebook</a></li>\n</ul>\n<p style='font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 15px'>\n<b>NB:-</b> This training notebook uses a different NN architecture. Not the exact architecture used for this notebook. But apart from the architecture, everything else (training parameters, optimizers, schedulers, etc) is same. I had to use a different architecture for demonstration because Kaggle has a timeout limit which is not possible to adhere with the transformer model.\n</p>\n\n<p style='background:MediumSeaGreen; border:0; color: white; text-align: center; font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 24px'>If you found this notebook useful or use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share such public kernels.<br>Thanks! ðŸ˜Š</p>","metadata":{}},{"cell_type":"markdown","source":"# Get GPU Info","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2021-11-11T13:50:20.333276Z","iopub.execute_input":"2021-11-11T13:50:20.334173Z","iopub.status.idle":"2021-11-11T13:50:21.050469Z","shell.execute_reply.started":"2021-11-11T13:50:20.334037Z","shell.execute_reply":"2021-11-11T13:50:21.049534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# Asthetics\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n# General\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nfrom collections import defaultdict\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nimport random\nimport gc\nimport glob\npd.set_option('display.max_columns', None)\nnp.seterr(divide='ignore', invalid='ignore')\ngc.enable()\n\n# Deep Learning\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import OneCycleLR\n# NLP\nfrom transformers import AutoTokenizer, AutoModel\n\n# Random Seed Initialize\nRANDOM_SEED = 42\n\ndef seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything()\n\n# Device Optimization\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    \nprint(f'Using device: {device}')","metadata":{"execution":{"iopub.status.busy":"2021-11-11T13:50:21.052319Z","iopub.execute_input":"2021-11-11T13:50:21.052845Z","iopub.status.idle":"2021-11-11T13:50:28.824536Z","shell.execute_reply.started":"2021-11-11T13:50:21.052804Z","shell.execute_reply":"2021-11-11T13:50:28.823713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '../input/jigsaw-toxic-severity-rating'\nmodels_dir = '../input/jrstc-models/roberta_base_2'\ntest_file_path = os.path.join(data_dir, 'comments_to_score.csv')\nprint(f'Train file: {test_file_path}')","metadata":{"execution":{"iopub.status.busy":"2021-11-11T13:50:28.825785Z","iopub.execute_input":"2021-11-11T13:50:28.826059Z","iopub.status.idle":"2021-11-11T13:50:28.831704Z","shell.execute_reply.started":"2021-11-11T13:50:28.826012Z","shell.execute_reply":"2021-11-11T13:50:28.830801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(test_file_path)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T13:50:28.834761Z","iopub.execute_input":"2021-11-11T13:50:28.836568Z","iopub.status.idle":"2021-11-11T13:50:28.949321Z","shell.execute_reply.started":"2021-11-11T13:50:28.83653Z","shell.execute_reply":"2021-11-11T13:50:28.948508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Cleaning","metadata":{}},{"cell_type":"code","source":"def text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-11-11T13:50:28.950817Z","iopub.execute_input":"2021-11-11T13:50:28.951092Z","iopub.status.idle":"2021-11-11T13:50:28.960142Z","shell.execute_reply.started":"2021-11-11T13:50:28.951056Z","shell.execute_reply":"2021-11-11T13:50:28.958189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas()\ntest_df['text'] = test_df['text'].progress_apply(text_cleaning)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T13:50:28.961841Z","iopub.execute_input":"2021-11-11T13:50:28.96217Z","iopub.status.idle":"2021-11-11T13:50:31.544491Z","shell.execute_reply.started":"2021-11-11T13:50:28.962131Z","shell.execute_reply":"2021-11-11T13:50:31.54369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.sample(10)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T13:50:31.546193Z","iopub.execute_input":"2021-11-11T13:50:31.546452Z","iopub.status.idle":"2021-11-11T13:50:31.565273Z","shell.execute_reply.started":"2021-11-11T13:50:31.546416Z","shell.execute_reply":"2021-11-11T13:50:31.564588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CFG","metadata":{}},{"cell_type":"code","source":"params = {\n    'device': device,\n    'debug': False,\n    'checkpoint': '../input/roberta-base',\n    'output_logits': 768,\n    'max_len': 256,\n    'batch_size': 32,\n    'dropout': 0.2,\n    'num_workers': 2\n}","metadata":{"execution":{"iopub.status.busy":"2021-11-11T13:50:31.566619Z","iopub.execute_input":"2021-11-11T13:50:31.567046Z","iopub.status.idle":"2021-11-11T13:50:31.571457Z","shell.execute_reply.started":"2021-11-11T13:50:31.566996Z","shell.execute_reply":"2021-11-11T13:50:31.57079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if params['debug']:\n    train_df = train_df.sample(frac=0.01)\n    print('Reduced training Data Size for Debugging purposes')","metadata":{"execution":{"iopub.status.busy":"2021-11-11T13:50:31.572721Z","iopub.execute_input":"2021-11-11T13:50:31.573128Z","iopub.status.idle":"2021-11-11T13:50:31.582328Z","shell.execute_reply.started":"2021-11-11T13:50:31.573091Z","shell.execute_reply":"2021-11-11T13:50:31.581316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class BERTDataset:\n    def __init__(self, text, max_len=params['max_len'], checkpoint=params['checkpoint']):\n        self.text = text\n        self.max_len = max_len\n        self.checkpoint = checkpoint\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n        self.num_examples = len(self.text)\n\n    def __len__(self):\n        return self.num_examples\n\n    def __getitem__(self, idx):\n        text = str(self.text[idx])\n\n        tokenized_text = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n        )\n\n        ids = tokenized_text['input_ids']\n        mask = tokenized_text['attention_mask']\n        token_type_ids = tokenized_text['token_type_ids']\n\n        return {'ids': torch.tensor(ids, dtype=torch.long),\n                'mask': torch.tensor(mask, dtype=torch.long),\n                'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)}","metadata":{"execution":{"iopub.status.busy":"2021-11-11T13:50:31.586623Z","iopub.execute_input":"2021-11-11T13:50:31.58758Z","iopub.status.idle":"2021-11-11T13:50:31.597151Z","shell.execute_reply.started":"2021-11-11T13:50:31.58754Z","shell.execute_reply":"2021-11-11T13:50:31.596336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NLP Model","metadata":{}},{"cell_type":"code","source":"class ToxicityModel(nn.Module):\n    def __init__(self, checkpoint=params['checkpoint'], params=params):\n        super(ToxicityModel, self).__init__()\n        self.checkpoint = checkpoint\n        self.bert = AutoModel.from_pretrained(checkpoint, return_dict=False)\n        self.dropout = nn.Dropout(params['dropout'])\n        self.dense = nn.Linear(params['output_logits'], 1)\n\n    def forward(self, input_ids, token_type_ids, attention_mask):\n        _, pooled_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n        pooled_output = self.dropout(pooled_output)\n        preds = self.dense(pooled_output)\n        return preds","metadata":{"execution":{"iopub.status.busy":"2021-11-11T13:50:31.599528Z","iopub.execute_input":"2021-11-11T13:50:31.600305Z","iopub.status.idle":"2021-11-11T13:50:31.609844Z","shell.execute_reply.started":"2021-11-11T13:50:31.600265Z","shell.execute_reply":"2021-11-11T13:50:31.609069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"predictions_nn = None\nfor model_name in glob.glob(models_dir + '/*.pth'):\n    model = ToxicityModel()\n    model.load_state_dict(torch.load(model_name))\n    model = model.to(params['device'])\n    model.eval()\n\n    test_dataset = BERTDataset(\n        text = test_df['text'].values\n    )\n    test_loader = DataLoader(\n        test_dataset, batch_size=params['batch_size'],\n        shuffle=False, num_workers=params['num_workers'],\n        pin_memory=True\n    )\n\n    temp_preds = None\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=f'Predicting. '):\n            ids= batch['ids'].to(device)\n            mask = batch['mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            predictions = model(ids, token_type_ids, mask).to('cpu').numpy()\n            \n            if temp_preds is None:\n                temp_preds = predictions\n            else:\n                temp_preds = np.vstack((temp_preds, predictions))\n\n    if predictions_nn is None:\n        predictions_nn = temp_preds\n    else:\n        predictions_nn += temp_preds\n        \npredictions_nn /= (len(glob.glob(models_dir + '/*.pth')))","metadata":{"execution":{"iopub.status.busy":"2021-11-11T13:50:31.611172Z","iopub.execute_input":"2021-11-11T13:50:31.611906Z","iopub.status.idle":"2021-11-11T13:50:57.577388Z","shell.execute_reply.started":"2021-11-11T13:50:31.611833Z","shell.execute_reply":"2021-11-11T13:50:57.576089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"sub_df = pd.DataFrame()\nsub_df['comment_id'] = test_df['comment_id']\nsub_df['score'] = predictions_nn\nsub_df['score'] = sub_df['score'].rank(method='first')","metadata":{"execution":{"iopub.status.busy":"2021-11-11T13:50:57.578735Z","iopub.status.idle":"2021-11-11T13:50:57.579267Z","shell.execute_reply.started":"2021-11-11T13:50:57.579013Z","shell.execute_reply":"2021-11-11T13:50:57.57905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-11T13:50:57.580791Z","iopub.status.idle":"2021-11-11T13:50:57.581436Z","shell.execute_reply.started":"2021-11-11T13:50:57.5812Z","shell.execute_reply":"2021-11-11T13:50:57.581224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T13:50:57.582807Z","iopub.status.idle":"2021-11-11T13:50:57.583564Z","shell.execute_reply.started":"2021-11-11T13:50:57.583325Z","shell.execute_reply":"2021-11-11T13:50:57.583349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style='background:MediumSeaGreen; border:0; color: white; text-align: center; font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 24px'>If you found this notebook useful or use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share such public kernels.<br>Thanks! ðŸ˜Š</p>","metadata":{}}]}