{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Toxic: TF Ranking\n---\n","metadata":{}},{"cell_type":"code","source":"# IDEA: Maybe one round with full backbone fine tuning. ","metadata":{"execution":{"iopub.status.busy":"2022-01-10T19:40:12.042413Z","iopub.execute_input":"2022-01-10T19:40:12.042975Z","iopub.status.idle":"2022-01-10T19:40:12.052315Z","shell.execute_reply.started":"2022-01-10T19:40:12.042837Z","shell.execute_reply":"2022-01-10T19:40:12.050975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install -q tensorflow-ranking\n# import tensorflow_ranking as tfr","metadata":{"execution":{"iopub.status.busy":"2022-01-10T19:40:12.056318Z","iopub.execute_input":"2022-01-10T19:40:12.056694Z","iopub.status.idle":"2022-01-10T19:40:26.351335Z","shell.execute_reply.started":"2022-01-10T19:40:12.056641Z","shell.execute_reply":"2022-01-10T19:40:26.350225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SUBMISSION = True\n\n## Model Architecture ## \n# FEATURE_HIDDEN_LAYERS = [256, 64, 16, 4]\n# FINAL_HIDDEN_LAYERS = [4, 1]\n# HIDDEN_DROPOUT = 0.10\n\n## Model Inputs ## \n# LIST_FEATURES = ['old_lb827', 'new_lb777', 'ruddit_lb785', 'fold3_lb820', 'fold3_old_lb805']\nLIST_FEATURES = ['old_lb827', 'new_lb777', 'ruddit_lb785']\nLIST_FEATURE_DIMS = [768, 768, 768, 1024, 1024]\nMAX_SCORER_LENGTH = 96\n\nUNIT_FEATURES = ['tfidf_lb864']\nMODEL_INPUTS = UNIT_FEATURES + LIST_FEATURES\n\n## Model Training ##\nBATCH_SIZE = 8","metadata":{"execution":{"iopub.status.busy":"2022-01-10T19:40:26.353873Z","iopub.execute_input":"2022-01-10T19:40:26.354492Z","iopub.status.idle":"2022-01-10T19:40:26.363726Z","shell.execute_reply.started":"2022-01-10T19:40:26.354441Z","shell.execute_reply":"2022-01-10T19:40:26.362616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Notebook Imports & Setup","metadata":{}},{"cell_type":"code","source":"import tensorflow_addons as tfa\nimport tensorflow as tf\n\nimport transformers\nimport tokenizers\nimport datasets\n\nfrom functools import partial\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nimport tensorflow as tf\nfrom time import time\nimport pandas as pd\nimport numpy as np\nimport joblib    \nimport re\n\n# Enable Mixed Precision, JIT Compilation & set random seed\ndef _enable_mixed_precision(): \n    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n    policy = mixed_precision.Policy('mixed_float16')\n    mixed_precision.set_policy(policy)\n\n# _enable_mixed_precision()\ntf.config.optimizer.set_jit(True)\ntf.random.set_seed(12)\n\n# Cache Paths\nBACKBONES_DIR = Path('../input/toxic-internet-deep-model-backbones')\nWTS = Path('../input/toxic-monster-model-internet')","metadata":{"execution":{"iopub.status.busy":"2022-01-10T19:40:26.368249Z","iopub.execute_input":"2022-01-10T19:40:26.368535Z","iopub.status.idle":"2022-01-10T19:40:27.964936Z","shell.execute_reply.started":"2022-01-10T19:40:26.368494Z","shell.execute_reply":"2022-01-10T19:40:27.963876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## âš’ Data Factory\n---","metadata":{}},{"cell_type":"code","source":"TOXIC_FEATURES = ['severe_toxic', 'identity_hate', 'threat', 'toxic', 'insult', 'obscene']","metadata":{"execution":{"iopub.status.busy":"2022-01-10T19:40:27.967027Z","iopub.execute_input":"2022-01-10T19:40:27.967408Z","iopub.status.idle":"2022-01-10T19:40:27.972848Z","shell.execute_reply.started":"2022-01-10T19:40:27.967361Z","shell.execute_reply":"2022-01-10T19:40:27.97166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"old = pd.read_csv('../input/toxic-public-dataframes/old_pseudo_label.csv')\ndf = pd.read_csv('../input/toxic-dataframes/valid.csv')\ndfc = pd.read_csv('../input/toxic-dataframes/comments.csv')\n\ndf = df[df.more_toxic.isin(old.comment_text) & df.less_toxic.isin(old.comment_text)]\ndfc = dfc[dfc.comment_text.isin(df.more_toxic) | dfc.comment_text.isin(df.less_toxic)] \n\nif SUBMISSION: \n    sub = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\n    sub['comment_text'] = sub.text\nelse: \n    sub = pd.read_csv('../input/toxic-dataframes/test_comments.csv')\n    sub['text'] = sub.comment_text\n    sub = sub.drop_duplicates('comment_text')\n    \n# Map Feature Values\nold_dict = old.set_index('comment_text').to_dict()\nfor feat in TOXIC_FEATURES: \n    sub[feat] = sub.comment_text.map(old_dict[feat])\n    df[f'MT_{feat}'] = df.more_toxic.map(old_dict[feat])\n    df[f'LT_{feat}'] = df.less_toxic.map(old_dict[feat])\n    \ndf","metadata":{"execution":{"iopub.status.busy":"2022-01-10T19:40:27.974974Z","iopub.execute_input":"2022-01-10T19:40:27.975725Z","iopub.status.idle":"2022-01-10T19:40:38.016751Z","shell.execute_reply.started":"2022-01-10T19:40:27.975676Z","shell.execute_reply":"2022-01-10T19:40:38.015686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Embedding Extractor\n---","metadata":{}},{"cell_type":"code","source":"def dataset_to_test_ds(dataset): \n    'Processed huggingface dataset to tensorflow dataset'\n    dataset.set_format(type='numpy')\n    input_ids_ds = tf.data.Dataset.from_tensor_slices(dataset['input_ids'].astype(np.int32))\n    attention_mask_ds = tf.data.Dataset.from_tensor_slices(dataset['attention_mask'].astype(np.int32))\n    ds = tf.data.Dataset.zip((input_ids_ds, attention_mask_ds))\n    ds = tf.data.Dataset.zip((ds, ds))\n    return ds.batch(1024).prefetch(tf.data.AUTOTUNE)\n\ndef load_tokenizer_and_backbone(folder): \n    print('Loading tokenizer and backbone from', folder)\n    tokenizer = transformers.AutoTokenizer.from_pretrained(str(folder))\n    with tf.device('/device:GPU:0'): \n        backbone = transformers.TFAutoModel.from_pretrained(str(folder))\n    return tokenizer, backbone","metadata":{"execution":{"iopub.status.busy":"2022-01-10T19:40:38.018867Z","iopub.execute_input":"2022-01-10T19:40:38.019651Z","iopub.status.idle":"2022-01-10T19:40:38.030358Z","shell.execute_reply.started":"2022-01-10T19:40:38.019602Z","shell.execute_reply":"2022-01-10T19:40:38.029177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# 8 min\n\ndef build_scorer_model(backbone): \n    input_ids = tf.keras.Input((MAX_SCORER_LENGTH,), dtype=tf.int32)\n    attention_mask = tf.keras.Input((MAX_SCORER_LENGTH,), dtype=tf.int32)\n    \n    backbone_outputs = backbone(\n        input_ids=input_ids, \n        attention_mask=attention_mask, \n        return_dict=True,\n    )\n    x = backbone_outputs.pooler_output\n    score_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n    return tf.keras.Model(inputs=[input_ids, attention_mask], outputs=score_layer(x))\n\nwith tf.device('/device:GPU:0'): \n    robertaB_tokenizer, robertaB_backbone = load_tokenizer_and_backbone(BACKBONES_DIR/'roberta_base')\n    robertaL_tokenizer, robertaL_backbone = load_tokenizer_and_backbone(BACKBONES_DIR/'roberta_large')\n\nraw_dataset = datasets.Dataset.from_pandas(dfc)\nprocessed_dataset = raw_dataset.map(\n    lambda ex: robertaB_tokenizer(ex['comment_text'], max_length=MAX_SCORER_LENGTH, padding='max_length', truncation=True), \n    batched=True, num_proc=4,\n)\ndf_ds = dataset_to_test_ds(processed_dataset) \nraw_dataset = datasets.Dataset.from_pandas(sub)\nprocessed_dataset = raw_dataset.map(\n    lambda ex: robertaB_tokenizer(ex['text'], max_length=MAX_SCORER_LENGTH, padding='max_length', truncation=True), \n    batched=True, num_proc=4,\n)\nsub_ds = dataset_to_test_ds(processed_dataset) \n\ndef predict(backbone, ds):\n    preds = backbone.predict(ds, verbose=1).pooler_output.astype(np.float32)\n    return np.squeeze(preds)\n\nwith tf.device('/device:GPU:0'): \n    model = build_scorer_model(robertaB_backbone)\n    \n    model.load_weights(WTS/'old_pseudo_label.h5')\n    dfc_old_lb827 = predict(robertaB_backbone, df_ds)\n    sub_old_lb827 = predict(robertaB_backbone, sub_ds)\n    \n    model.load_weights(WTS/'2019val737_robertab_scorer.h5')\n    dfc_new_lb777 = predict(robertaB_backbone, df_ds)\n    sub_new_lb777 = predict(robertaB_backbone, sub_ds)\n    \n    model.load_weights(WTS/'ruddit_val731_robertab.h5')\n    dfc_ruddit_lb785 = predict(robertaB_backbone, df_ds)\n    sub_ruddit_lb785 = predict(robertaB_backbone, sub_ds)\n    \n    import gc\n    del robertaB_backbone; gc.collect()\n#     model = build_scorer_model(robertaL_backbone)\n    \n#     model.load_weights(WTS/'comp_only_fold3_val737_robertal_sentiment_10thDec.h5')\n#     dfc_fold3_lb820 = predict(robertaL_backbone, df_ds)\n#     sub_fold3_lb820 = predict(robertaL_backbone, sub_ds)\n    \n#     model.load_weights(WTS/'fold3_old_then_comp_loss571_val750_ep1_roberta_sent_12thDec.h5')\n#     dfc_fold3_old_lb805 = predict(robertaL_backbone, df_ds)\n#     sub_fold3_old_lb805 = predict(robertaL_backbone, sub_ds)\n    \npipeline = joblib.load('../input/toxic-dataframes/pipeline_lb864.pkl')\ndfc['tfidf_lb864'] = pipeline.predict(dfc.comment_text)\nsub['tfidf_lb864'] = pipeline.predict(sub.text)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T19:40:38.03222Z","iopub.execute_input":"2022-01-10T19:40:38.03268Z","iopub.status.idle":"2022-01-10T19:48:41.514482Z","shell.execute_reply.started":"2022-01-10T19:40:38.032582Z","shell.execute_reply":"2022-01-10T19:48:41.513069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfc_comment_to_i = {c: i for i, c in enumerate(dfc.comment_text.values)}\nsub_comment_to_i = {c: i for i, c in enumerate(sub.text.values)}\n\nA, B = df.copy(), df.copy()\n\nA['A_comment'], A['B_comment'] = A.more_toxic, A.less_toxic\nB['A_comment'], B['B_comment'] = B.less_toxic, B.more_toxic\ndfc_dict = dfc.set_index('comment_text').to_dict()\nfor feat in UNIT_FEATURES: \n    df[f'MT_{feat}'] = df.more_toxic.map(dfc_dict[feat])\n    df[f'LT_{feat}'] = df.less_toxic.map(dfc_dict[feat])\n    \n    A[f'A_{feat}'], A[f'B_{feat}'], A['y'] = df[f'MT_{feat}'], df[f'LT_{feat}'], 0.0\n    B[f'A_{feat}'], B[f'B_{feat}'], B['y'] = df[f'LT_{feat}'], df[f'MT_{feat}'], 1.0\n    \n\ndf_temp = pd.concat([A, B])\ndf_temp['A_i'] = df_temp.A_comment.map(dfc_comment_to_i)\ndf_temp['B_i'] = df_temp.B_comment.map(dfc_comment_to_i)\ntrain, valid = df_temp[df_temp.fold!=3], df_temp[df_temp.fold==3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndef df_to_tfds(df, is_train=False): \n    col_to_values = {\n        'old_lb827': dfc_old_lb827,\n        'new_lb777': dfc_new_lb777,\n        'ruddit_lb785': dfc_ruddit_lb785,\n        # 'fold3_lb820': dfc_fold3_lb820,\n        # 'fold3_old_lb805': dfc_fold3_old_lb805,\n    }\n    inputs = []\n    for feat in UNIT_FEATURES: \n        inputs.append(tf.data.Dataset.from_tensor_slices(df[f'A_{feat}'].values))\n    for feat in LIST_FEATURES: \n        x = col_to_values[feat][df.A_i]\n        inputs.append(tf.data.Dataset.from_tensor_slices(x))\n    for feat in UNIT_FEATURES: \n        inputs.append(tf.data.Dataset.from_tensor_slices(df[f'B_{feat}'].values))\n    for feat in LIST_FEATURES:\n        x = col_to_values[feat][df.B_i]\n        inputs.append(tf.data.Dataset.from_tensor_slices(x))\n    input_ds = tf.data.Dataset.zip(tuple(inputs))\n    \n    label_ds = tf.data.Dataset.from_tensor_slices(df.y.values)\n    ds = tf.data.Dataset.zip((input_ds, label_ds))\n    if is_train: \n        ds = ds.shuffle(len(df), reshuffle_each_iteration=True).repeat()\n    ds = ds.batch(BATCH_SIZE)\n    if not is_train: \n        ds = ds.cache()\n    steps = len(df)//BATCH_SIZE\n    return ds.prefetch(tf.data.AUTOTUNE), steps\n\n\ndef test_df_to_tfds(df): \n    col_to_values = {\n        'old_lb827': sub_old_lb827,\n        'new_lb777': sub_new_lb777,\n        'ruddit_lb785': sub_ruddit_lb785,\n        # 'fold3_lb820': sub_fold3_lb820,\n        # 'fold3_old_lb805': sub_fold3_old_lb805,\n    }\n    \n    inputs = []\n    for feat in UNIT_FEATURES: \n        inputs.append(tf.data.Dataset.from_tensor_slices(df[feat].values))\n    for feat in LIST_FEATURES: \n        x = col_to_values[feat]\n        inputs.append(tf.data.Dataset.from_tensor_slices(x))\n    ds = tf.data.Dataset.zip(tuple(inputs))\n    ds = tf.data.Dataset.zip((ds, ds))\n    ds = ds.batch(1024)\n    return ds.prefetch(tf.data.AUTOTUNE)\n\n\ntrain_ds, train_steps = df_to_tfds(train, is_train=True)\nvalid_ds, valid_steps = df_to_tfds(valid, is_train=False)\ntest_ds = test_df_to_tfds(sub)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T20:02:42.897919Z","iopub.execute_input":"2022-01-10T20:02:42.898224Z","iopub.status.idle":"2022-01-10T20:02:43.164759Z","shell.execute_reply.started":"2022-01-10T20:02:42.898176Z","shell.execute_reply":"2022-01-10T20:02:43.163439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Cross Encoder","metadata":{}},{"cell_type":"code","source":"def build_hidden_layer(hidden_layer_units, hidden_dropout, name='hidden_layer'): \n    layers = []\n    for units in hidden_layer_units: \n        layers.append(tf.keras.layers.Dropout(hidden_dropout))\n        layers.append(tf.keras.layers.Dense(\n            units, \n            activation=tfa.activations.mish, \n        ))\n    return tf.keras.Sequential(layers, name=name)\n\ndef build_model(): \n    A_unit_inputs, B_unit_inputs = [], []\n    A_unit_embeddings, B_unit_embeddings = [], []\n    for unit_feat in UNIT_FEATURES:\n        A_unit_input = tf.keras.Input(shape=(1,), dtype=tf.float32, name=f'A_{unit_feat}')\n        B_unit_input = tf.keras.Input(shape=(1,), dtype=tf.float32, name=f'B_{unit_feat}')\n        \n        A_unit_inputs.append(A_unit_input)\n        B_unit_inputs.append(B_unit_input)\n        \n        U = tf.keras.layers.Dense(1, activation=tfa.activations.mish, name=unit_feat)\n        A_unit_embeddings.append(U(A_unit_input))\n        B_unit_embeddings.append(U(B_unit_input))\n        \n    \n    A_list_inputs, B_list_inputs = [], []\n    A_list_embeddings, B_list_embeddings = [], []\n    for list_feat, dim in zip(LIST_FEATURES, LIST_FEATURE_DIMS):\n        A_list_input = tf.keras.Input(shape=(dim,), dtype=tf.float32, name=f'A_{list_feat}')\n        B_list_input = tf.keras.Input(shape=(dim,), dtype=tf.float32, name=f'B_{list_feat}')\n        \n        A_list_inputs.append(A_list_input)\n        B_list_inputs.append(B_list_input)\n        \n        L = build_hidden_layer(LIST_FEATURE_HIDDEN_LAYERS, HIDDEN_DROPOUT, name=list_feat)\n        A_list_embeddings.append(L(A_list_input))\n        B_list_embeddings.append(L(B_list_input))\n\n    A_x = tf.concat(A_unit_embeddings+A_list_embeddings, axis=-1)\n    B_x = tf.concat(B_unit_embeddings+B_list_embeddings, axis=-1)\n    x = tf.concat([A_x, B_x], axis=-1)\n    \n    H = build_hidden_layer(FINAL_HIDDEN_LAYERS, HIDDEN_DROPOUT, name='final_hidden_layer')\n    x = H(x)\n    x = tf.keras.layers.Dense(1, activation='sigmoid', name='y')(x)\n    return tf.keras.Model(A_unit_inputs+A_list_inputs+B_unit_inputs+B_list_inputs, outputs=x)\n\ndef optimizer_factory(lr):\n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n    optimizer = tfa.optimizers.SWA(optimizer)\n    return optimizer\n\ndef loss_fn(y_true, y_pred): \n    y_true, y_pred = tf.cast(y_true, tf.float32), tf.cast(y_pred, tf.float32)\n    \n    # return tfr.keras.losses.PairwiseHingeLoss()(y_true, y_pred)\n    return tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)\n    # return tfr.keras.losses.PairwiseLogisticLoss(temperature=1.0)(y_true, y_pred)\n\ndef model_compile(): \n    optimizer = optimizer_factory(1e-2)\n    model.compile(\n        optimizer=optimizer, \n        loss=loss_fn, \n        metrics='accuracy', \n        steps_per_execution=1024, \n    )    \n\nLIST_FEATURE_HIDDEN_LAYERS = [16]\nFINAL_HIDDEN_LAYERS = [256, 64, 32, 16, 4, 2]\nHIDDEN_DROPOUT = 0.25\n\n\nwith tf.device('/device:GPU:0'): \n    model = build_model()\n    model_compile()\n    \nreduce_lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_accuracy', factor=0.5, patience=5, verbose=1, mode='max', min_lr=1e-6, \n)\naccuracy_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    'checkpoint_acc.h5', monitor='val_accuracy', mode='max', save_weights_only=True, save_best_only=True, verbose=1\n)\n\nhistory = model.fit(\n    train_ds, steps_per_epoch=train_steps, epochs=100, \n    validation_data=valid_ds, validation_steps=valid_steps, \n    callbacks=[reduce_lr_on_plateau, accuracy_checkpoint]\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T20:22:41.78997Z","iopub.execute_input":"2022-01-10T20:22:41.790616Z","iopub.status.idle":"2022-01-10T20:25:19.656244Z","shell.execute_reply.started":"2022-01-10T20:22:41.790578Z","shell.execute_reply":"2022-01-10T20:25:19.654813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 0.744 without [4], None (No Fine Tuned Layers)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T19:48:42.263891Z","iopub.status.idle":"2022-01-10T19:48:42.264514Z","shell.execute_reply.started":"2022-01-10T19:48:42.264191Z","shell.execute_reply":"2022-01-10T19:48:42.264223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference\n---","metadata":{}},{"cell_type":"code","source":"def extract_features(model): \n    unit_inputs, unit_embeddings = [], []\n    for unit_feat in UNIT_FEATURES: \n        unit_input = tf.keras.Input(shape=(1,), dtype=tf.float32, name=f'{unit_feat}_input')\n        unit_embeddings.append(model.get_layer(unit_feat)(unit_input))\n        unit_inputs.append(unit_input)\n        \n    list_inputs, list_embeddings = [], []\n    for list_feat, dim in zip(LIST_FEATURES, LIST_FEATURE_DIMS): \n        list_input = tf.keras.Input(shape=(dim,), dtype=tf.float32, name=f'{list_feat}_input')\n        L = model.get_layer(list_feat)\n        list_embeddings.append(L(list_input))\n        list_inputs.append(list_input)\n\n    x = tf.concat(unit_embeddings+list_embeddings, axis=-1)\n    return tf.keras.Model(unit_inputs+list_inputs, outputs=x)\n\n\nwith tf.device('/device:GPU:0'): \n    model.load_weights('checkpoint_acc.h5')\n    feature_model = extract_features(model)\n    test_x = feature_model.predict(test_ds, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T20:25:21.35016Z","iopub.execute_input":"2022-01-10T20:25:21.350489Z","iopub.status.idle":"2022-01-10T20:25:23.48266Z","shell.execute_reply.started":"2022-01-10T20:25:21.350447Z","shell.execute_reply":"2022-01-10T20:25:23.481603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scorer_model(model): \n    x_dim = len(UNIT_FEATURES) + len(LIST_FEATURES)*LIST_FEATURE_HIDDEN_LAYERS[-1]\n    A_x = tf.keras.Input(shape=(x_dim,), dtype=tf.float32, name='A_x')\n    B_x = tf.keras.Input(shape=(x_dim,), dtype=tf.float32, name='B_x')\n    x = tf.concat([A_x, B_x], axis=-1)\n    \n    H = model.get_layer('final_hidden_layer')\n    x = H(x)\n    score_layer = model.get_layer('y')\n    x = score_layer(x)\n    return tf.keras.Model(inputs=[A_x, B_x], outputs=x)\n\nwith tf.device('/device:GPU:0'): \n    scorer = scorer_model(model)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T20:25:48.530879Z","iopub.execute_input":"2022-01-10T20:25:48.531398Z","iopub.status.idle":"2022-01-10T20:25:48.6073Z","shell.execute_reply.started":"2022-01-10T20:25:48.531348Z","shell.execute_reply":"2022-01-10T20:25:48.60623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SPEED_UP = 2\n\ndef build_test_ds(i):\n    B_x = test_x[np.arange(0, len(sub), SPEED_UP)]\n    A_x = np.stack([test_x[i] for _ in range(len(B_x))])\n    \n    A_ds = tf.data.Dataset.from_tensor_slices(A_x)\n    B_ds = tf.data.Dataset.from_tensor_slices(B_x)\n    ds = tf.data.Dataset.zip((A_ds, B_ds))\n    ds = tf.data.Dataset.zip((ds, A_ds))\n    ds = ds.batch(4096*4)\n    return ds.prefetch(tf.data.AUTOTUNE)\n\nsub_scores = []\nfor i in tqdm(range(len(sub))): \n    test_ds = build_test_ds(i)\n    scores = scorer.predict(test_ds, verbose=0)\n    sub_scores.append(scores.mean())\n    \nsub['score'] = sub_scores","metadata":{"execution":{"iopub.status.busy":"2022-01-10T20:34:13.62028Z","iopub.execute_input":"2022-01-10T20:34:13.620631Z","iopub.status.idle":"2022-01-10T20:37:54.479918Z","shell.execute_reply.started":"2022-01-10T20:34:13.620598Z","shell.execute_reply":"2022-01-10T20:37:54.478739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.score = -sub.score\nsub.score = sub.score.rank(method='first')\nsub[['comment_id', 'score']].to_csv('submission.csv', index=False)\n\nsub.sort_values(by='score')","metadata":{"execution":{"iopub.status.busy":"2022-01-10T20:38:27.979604Z","iopub.execute_input":"2022-01-10T20:38:27.98049Z","iopub.status.idle":"2022-01-10T20:38:28.061617Z","shell.execute_reply.started":"2022-01-10T20:38:27.980452Z","shell.execute_reply":"2022-01-10T20:38:28.060625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub.sort_values(by='tfidf_lb864')","metadata":{"execution":{"iopub.status.busy":"2022-01-10T20:39:23.699641Z","iopub.execute_input":"2022-01-10T20:39:23.700721Z","iopub.status.idle":"2022-01-10T20:39:23.741228Z","shell.execute_reply.started":"2022-01-10T20:39:23.700676Z","shell.execute_reply":"2022-01-10T20:39:23.74019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}