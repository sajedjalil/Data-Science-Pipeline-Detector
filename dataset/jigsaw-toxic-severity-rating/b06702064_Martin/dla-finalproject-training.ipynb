{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport copy\nimport pickle\nfrom tqdm.auto import tqdm\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#torch packages\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n#transformer packages\nfrom transformers import BertTokenizer, BertModel\nfrom transformers import RobertaTokenizer, RobertaModel\nfrom transformers import AdamW\nfrom transformers import get_scheduler\nfrom transformers import logging\nlogging.set_verbosity_error() #turn off bert warning\nlogging.set_verbosity_warning() #turn off bert warning","metadata":{"execution":{"iopub.status.busy":"2022-01-10T12:44:11.059385Z","iopub.execute_input":"2022-01-10T12:44:11.059608Z","iopub.status.idle":"2022-01-10T12:44:18.317971Z","shell.execute_reply.started":"2022-01-10T12:44:11.059547Z","shell.execute_reply":"2022-01-10T12:44:18.3172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \ndef generate_fold_num_for_dataset(data, num_fold):\n    skf = StratifiedKFold(n_splits=num_fold, shuffle=True)\n    for fold, ( _, val_) in enumerate(skf.split(X=data, y=data.worker)):\n        data.loc[val_ , \"kfold\"] = int(fold)\n    data[\"kfold\"] = data[\"kfold\"].astype(int)\n    return data\n\nclass JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length, use_tfidf=False, tfidf_matrix=None):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.more_toxic = df['more_toxic'].values\n        self.less_toxic = df['less_toxic'].values\n        self.use_tfidf = use_tfidf\n        if use_tfidf:\n            self.more_toxic_tfidf_idx = df['more_toxic_tfidf_idx'].values\n            self.less_toxic_tfidf_idx = df['less_toxic_tfidf_idx'].values\n            self.tfidf_matrix = tfidf_matrix\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        more_toxic = self.more_toxic[index]\n        less_toxic = self.less_toxic[index]\n        inputs_more_toxic = self.tokenizer.encode_plus(\n                                more_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        inputs_less_toxic = self.tokenizer.encode_plus(\n                                less_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        target = 1\n        \n        more_toxic_ids = inputs_more_toxic['input_ids']\n        more_toxic_mask = inputs_more_toxic['attention_mask']        \n        less_toxic_ids = inputs_less_toxic['input_ids']\n        less_toxic_mask = inputs_less_toxic['attention_mask']\n        \n        if self.use_tfidf:\n            more_toxic_tfidf_idx = self.more_toxic_tfidf_idx[index]\n            less_toxic_tfidf_idx = self.less_toxic_tfidf_idx[index]\n            more_toxic_tfidf = self.tfidf_matrix[more_toxic_tfidf_idx]\n            less_toxic_tfidf = self.tfidf_matrix[less_toxic_tfidf_idx]\n            return {\n                'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n                'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n                'more_toxic_tfidf': torch.tensor(more_toxic_tfidf, dtype=torch.long),\n                'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n                'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n                'less_toxic_tfidf': torch.tensor(less_toxic_tfidf, dtype=torch.long),\n                'target': torch.tensor(target, dtype=torch.long)\n            }\n        else:\n            return {\n                'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n                'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n                'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n                'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n                'target': torch.tensor(target, dtype=torch.long)\n            }","metadata":{"execution":{"iopub.status.busy":"2022-01-10T12:44:18.320698Z","iopub.execute_input":"2022-01-10T12:44:18.321366Z","iopub.status.idle":"2022-01-10T12:44:18.340635Z","shell.execute_reply.started":"2022-01-10T12:44:18.321324Z","shell.execute_reply":"2022-01-10T12:44:18.339979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NN(nn.Module):\n    def __init__(self, bert_drop_out, HID_DIM=768, tfidf_len=0, use_tfidf=False):\n        super().__init__()\n        if use_tfidf:\n            self.net = nn.Sequential(\n                nn.Dropout(p=bert_drop_out), #dropout for bert\n                \n                nn.Linear(768+tfidf_len, 1),\n#                 nn.ReLU(),\n#                 nn.BatchNorm1d(HID_DIM),\n#                 nn.Dropout(0.3),\n                \n#                 nn.Linear(HID_DIM, HID_DIM),\n#                 nn.ReLU(),\n#                 nn.BatchNorm1d(HID_DIM),\n#                 nn.Dropout(0.4),       \n\n#                 nn.Linear(HID_DIM, 1) \n            )\n        else:\n            self.net = nn.Sequential(\n                nn.Dropout(p=bert_drop_out), #dropout for bert\n                nn.Linear(768, 1)\n#                 nn.Linear(768, HID_DIM),\n#                 nn.ReLU(),\n#                 nn.BatchNorm1d(HID_DIM),\n#                 nn.Dropout(0.3),\n                \n#                 nn.Linear(HID_DIM, HID_DIM),\n#                 nn.ReLU(),\n#                 nn.BatchNorm1d(HID_DIM),\n#                 nn.Dropout(0.4),       \n\n#                 nn.Linear(HID_DIM, 1) \n            )\n            \n    def forward(self, x):\n        score = self.net(x)\n        return score\n\nclass JigsawModel(nn.Module):\n    def __init__(self, BERT, NN):\n        super(JigsawModel, self).__init__()\n        self.bert = BERT\n        self.fc = NN\n        \n    def forward(self, ids, mask, tfidf_vec=None, use_tfidf=False):        \n        out = self.bert(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        if use_tfidf:\n            fc_in = torch.cat(\n                (out[\"pooler_output\"], tfidf_vec), dim=1\n            )\n        else:\n            fc_in = out[\"pooler_output\"]\n        outputs = self.fc(fc_in)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-01-10T12:44:18.34225Z","iopub.execute_input":"2022-01-10T12:44:18.345318Z","iopub.status.idle":"2022-01-10T12:44:18.360404Z","shell.execute_reply.started":"2022-01-10T12:44:18.345228Z","shell.execute_reply":"2022-01-10T12:44:18.359648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def train_step(\n#     bert_model, trans_model, criterion, optimizer, \n#     train_loader, progress_bar, device, epoch, use_tfidf = False\n# ):\n#     y_preds = []\n#     epoch_loss = 0\n#     trans_model.train()\n#     bert_model.train()\n#     for i, data in enumerate(train_loader):\n#         more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n#         more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n#         less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n#         less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n#         targets = data['target'].to(device, dtype=torch.long)\n\n#         optimizer.zero_grad()\n        \n#         more_in = bert_model(\n#             input_ids = more_toxic_ids, attention_mask = more_toxic_mask,\n#             output_hidden_states=False\n#         )\n#         less_in = bert_model(\n#             input_ids = less_toxic_ids, attention_mask = less_toxic_mask,\n#             output_hidden_states=False\n#         )\n        \n#         if use_tfidf:\n#             more_toxic_tfidf = data['more_toxic_tfidf'].to(device, dtype = torch.long)\n#             less_toxic_tfidf = data['less_toxic_tfidf'].to(device, dtype = torch.long)\n#             more_in = torch.cat(\n#                 (more_in[\"pooler_output\"], more_toxic_tfidf), dim=1\n#             )\n#             less_in = torch.cat(\n#                 (less_in[\"pooler_output\"], less_toxic_tfidf), dim=1\n#             )\n            \n#         more_out = trans_model(more_in)\n#         less_out = trans_model(less_in)\n#         loss = criterion(less_out, more_out, targets)\n\n#         loss.backward()\n#         optimizer.step()\n#         epoch_loss += loss.item()\n        \n#         for j in range(len(data['more_toxic_ids'])):\n#             y_preds.append([less_out[j].item(), more_out[j].item()])\n        \n#         if progress_bar is not None:\n#             progress_bar.update(1)  \n            \n#         print('[ Epoch {}: {}/{} ] loss:{:.3f}'.format(epoch, i+1, len(train_loader), loss.item()), end='\\r')\n    \n#     df_score = pd.DataFrame(y_preds,columns=['less','more'])\n#     train_accuracy = validate_accuracy(df_score)         \n    \n#     return df_score, train_accuracy, epoch_loss / len(train_loader) # return loss\n\n\n# def validate_all(\n#     bert_model, trans_model, criterion, \n#     valid_loader, device, use_tfidf=False\n# ):\n#     epoch_loss = 0\n#     y_preds = []\n    \n#     bert_model.eval()\n#     trans_model.eval()\n#     with torch.no_grad():\n#         for data in valid_loader:\n#             more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n#             more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n#             less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n#             less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n#             targets = data['target'].to(device, dtype=torch.long)\n            \n            \n#             more_in = bert_model(\n#                 input_ids = more_toxic_ids, attention_mask = more_toxic_mask,\n#                 output_hidden_states=False\n#             )\n#             less_in = bert_model(\n#                 input_ids = less_toxic_ids, attention_mask = less_toxic_mask,\n#                 output_hidden_states=False\n#             )\n\n#             if use_tfidf:\n#                 more_toxic_tfidf = data['more_toxic_tfidf'].to(device, dtype = torch.long)\n#                 less_toxic_tfidf = data['less_toxic_tfidf'].to(device, dtype = torch.long)\n#                 more_in = torch.cat(\n#                     (more_in[\"pooler_output\"], more_toxic_tfidf), dim=1\n#                 )\n#                 less_in = torch.cat(\n#                     (less_in[\"pooler_output\"], less_toxic_tfidf), dim=1\n#                 )\n\n#             more_out = trans_model(more_in)\n#             less_out = trans_model(less_in)\n#             loss = criterion(less_out, more_out, targets)\n\n#             epoch_loss += loss.item()\n#             for i in range(len(data['more_toxic_ids'])):\n#                 y_preds.append([less_out[i].item(), more_out[i].item()])\n#         df_score = pd.DataFrame(y_preds,columns=['less','more'])\n#         accuracy = validate_accuracy(df_score)\n#     return df_score, accuracy, (epoch_loss / len(valid_loader))","metadata":{"execution":{"iopub.status.busy":"2022-01-10T12:44:18.362876Z","iopub.execute_input":"2022-01-10T12:44:18.364741Z","iopub.status.idle":"2022-01-10T12:44:18.372737Z","shell.execute_reply.started":"2022-01-10T12:44:18.364701Z","shell.execute_reply":"2022-01-10T12:44:18.371954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_step_combine(\n    model, criterion, optimizer, \n    train_loader, progress_bar, device, epoch, use_tfidf = False\n):\n    y_preds = []\n    epoch_loss = 0\n    model.train()\n    for i, data in enumerate(train_loader):\n        more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n        more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n        less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n        less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n        targets = data['target'].to(device, dtype=torch.long)\n\n        optimizer.zero_grad()\n        \n        if use_tfidf:\n            more_toxic_tfidf = data['more_toxic_tfidf'].to(device, dtype = torch.long)\n            less_toxic_tfidf = data['less_toxic_tfidf'].to(device, dtype = torch.long)\n            more_out = model(more_toxic_ids, more_toxic_mask, more_toxic_tfidf, use_tfidf=True)\n            less_out = model(less_toxic_ids, less_toxic_mask, less_toxic_tfidf, use_tfidf=True)\n        else:\n            more_out = model(more_toxic_ids, more_toxic_mask)\n            less_out = model(less_toxic_ids, less_toxic_mask)\n            \n        loss = criterion(more_out, less_out, targets)\n\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        \n        for j in range(len(data['more_toxic_ids'])):\n            y_preds.append([less_out[j].item(), more_out[j].item()])\n        \n        if progress_bar is not None:\n            progress_bar.update(1)  \n            \n        #print('[ Epoch {}: {}/{} ] loss:{:.3f}'.format(epoch, i+1, len(train_loader), loss.item()), end='\\r')\n    \n    df_score = pd.DataFrame(y_preds,columns=['less','more'])\n    train_accuracy = validate_accuracy(df_score)         \n    \n    return df_score, train_accuracy, epoch_loss / len(train_loader) # return loss\n\n\ndef validate_all_combine(\n    model, criterion, \n    valid_loader, device, use_tfidf=False\n):\n    epoch_loss = 0\n    y_preds = []\n    \n    model.eval()\n    with torch.no_grad():\n        for data in valid_loader:\n            more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n            more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n            less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n            less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n            targets = data['target'].to(device, dtype=torch.long)\n            \n            if use_tfidf:\n                more_toxic_tfidf = data['more_toxic_tfidf'].to(device, dtype = torch.long)\n                less_toxic_tfidf = data['less_toxic_tfidf'].to(device, dtype = torch.long)\n                more_out = model(more_toxic_ids, more_toxic_mask, more_toxic_tfidf, use_tfidf=True)\n                less_out = model(less_toxic_ids, less_toxic_mask, less_toxic_tfidf, use_tfidf=True)\n            else:\n                more_out = model(more_toxic_ids, more_toxic_mask)\n                less_out = model(less_toxic_ids, less_toxic_mask)\n            \n            loss = criterion(more_out, less_out, targets)\n\n            epoch_loss += loss.item()\n            for i in range(len(data['more_toxic_ids'])):\n                y_preds.append([less_out[i].item(), more_out[i].item()])\n        df_score = pd.DataFrame(y_preds,columns=['less','more'])\n        accuracy = validate_accuracy(df_score)\n    return df_score, accuracy, (epoch_loss / len(valid_loader))\n\ndef validate_accuracy(df_score):\n    return len(df_score[df_score['less'] < df_score['more']]) / len(df_score)\n\ndef return_wrong_text(df_score, df_valid):\n    df_score_text = pd.concat((df_valid.reset_index().drop('index',axis=1),df_score),axis=1)\n    return df_score_text[df_score_text['less'] > df_score_text['more']]\n\ndef init_weights(m):\n    if isinstance(m, nn.Linear):\n        torch.nn.init.xavier_uniform_(m.weight)\n        m.bias.data.fill_(0.01)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T12:44:18.37537Z","iopub.execute_input":"2022-01-10T12:44:18.375984Z","iopub.status.idle":"2022-01-10T12:44:18.398636Z","shell.execute_reply.started":"2022-01-10T12:44:18.375945Z","shell.execute_reply":"2022-01-10T12:44:18.397912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_loss_plot(train_losses, valid_losses):\n    plt.plot(train_losses,label=\"Training\")\n    plt.plot(valid_losses,label=\"Validation\")\n    plt.title(\"Loss plot\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n    \ndef plot_acc_plot(train_accs, valid_accs):\n    plt.plot(train_accs,label=\"Training\")\n    plt.plot(valid_accs,label=\"Validation\")\n    plt.title(\"Accuracy plot\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-10T12:44:18.3999Z","iopub.execute_input":"2022-01-10T12:44:18.400581Z","iopub.status.idle":"2022-01-10T12:44:18.410227Z","shell.execute_reply.started":"2022-01-10T12:44:18.400544Z","shell.execute_reply":"2022-01-10T12:44:18.409527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_duplicates(df, used_col):\n    \"\"\"Combine `less_toxic` text and `more_toxic` text,\n    then remove duplicate pair of comments while keeping the last pair\n    \"\"\"\n    df[\"combine\"] = df[\"less_toxic\"] + df[\"more_toxic\"]\n    df = df.drop_duplicates(subset=used_col, keep=\"last\")\n    return df\n\ndef create_corpus(df_train):\n    all_corpus = df_train[\"more_toxic\"].to_list()\n    all_corpus += df_train[\"less_toxic\"].to_list()\n    #remove duplicates\n    all_corpus = list(\n        set(all_corpus)\n    )\n    return all_corpus\n\ndef create_mapping_dict(corpus):\n    idx = np.arange(len(corpus))\n    sentence2idx = dict(\n        zip(corpus, idx)\n    )\n    return sentence2idx\n\ndef tokenize_by_bert_tokenizer(corpus, tokenizer):\n    corpus_tokenized = [\n        tokenizer.tokenize(sentence) for sentence in corpus\n    ]\n    return corpus_tokenized\n\ndef identity_tokenizer(text):\n    return text\n\ndef corpus2tfidf(corpus_tokenized):\n    tfidf = TfidfVectorizer(preprocessor=' '.join, tokenizer=identity_tokenizer)    \n    tfidf_matrix_sparse = tfidf.fit_transform(corpus_tokenized)\n    tfidf_matrix = tfidf_matrix_sparse.toarray()\n    return tfidf, tfidf_matrix\n    \ndef construct_tfidf_matrix(df_train, tokenizer):\n    corpus = create_corpus(df_train)\n    sentence2idx = create_mapping_dict(corpus)\n    corpus_tokenized = tokenize_by_bert_tokenizer(corpus, tokenizer)\n    tfidf_obj, tfidf_matrix = corpus2tfidf(corpus_tokenized)\n    return sentence2idx, tfidf_obj, tfidf_matrix","metadata":{"execution":{"iopub.status.busy":"2022-01-10T12:44:18.411457Z","iopub.execute_input":"2022-01-10T12:44:18.411866Z","iopub.status.idle":"2022-01-10T12:44:18.422461Z","shell.execute_reply.started":"2022-01-10T12:44:18.411757Z","shell.execute_reply":"2022-01-10T12:44:18.421712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_seed(5080)\ndata_train = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\ndata_test = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n#load model and tokenizer\n# PRETRAINED_MODEL_NAME = \"GroNLP/hateBERT\"\n# bert_tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\nPRETRAINED_MODEL_NAME = \"roberta-base\"\nbert_tokenizer = RobertaTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T12:44:18.423663Z","iopub.execute_input":"2022-01-10T12:44:18.424365Z","iopub.status.idle":"2022-01-10T12:44:33.483433Z","shell.execute_reply.started":"2022-01-10T12:44:18.424333Z","shell.execute_reply":"2022-01-10T12:44:33.482705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#text preprocessing\ndata_train_removed = remove_duplicates(data_train, \"combine\") #(30108, 4) -> (15410, 4)\n\n#construct tfidf一定要用removed!\nsentence2idx, tfidf_obj, tfidf_matrix = construct_tfidf_matrix(data_train_removed, bert_tokenizer)\ndata_train[\"less_toxic_tfidf_idx\"] = data_train[\"less_toxic\"].apply(lambda x: sentence2idx[x])\ndata_train[\"more_toxic_tfidf_idx\"] = data_train[\"more_toxic\"].apply(lambda x: sentence2idx[x])","metadata":{"execution":{"iopub.status.busy":"2022-01-10T12:44:33.484709Z","iopub.execute_input":"2022-01-10T12:44:33.485051Z","iopub.status.idle":"2022-01-10T12:44:48.102697Z","shell.execute_reply.started":"2022-01-10T12:44:33.485012Z","shell.execute_reply":"2022-01-10T12:44:48.101896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('./tfidf_roberta_obj.pickle', 'wb') as f:\n    pickle.dump(tfidf_obj, f)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T12:44:48.104659Z","iopub.execute_input":"2022-01-10T12:44:48.104858Z","iopub.status.idle":"2022-01-10T12:44:48.109066Z","shell.execute_reply.started":"2022-01-10T12:44:48.104835Z","shell.execute_reply":"2022-01-10T12:44:48.10833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_obj","metadata":{"execution":{"iopub.status.busy":"2022-01-09T15:09:43.774953Z","iopub.execute_input":"2022-01-09T15:09:43.775205Z","iopub.status.idle":"2022-01-09T15:09:43.787375Z","shell.execute_reply.started":"2022-01-09T15:09:43.775168Z","shell.execute_reply":"2022-01-09T15:09:43.786269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-09T15:09:43.789065Z","iopub.execute_input":"2022-01-09T15:09:43.789574Z","iopub.status.idle":"2022-01-09T15:09:43.796171Z","shell.execute_reply.started":"2022-01-09T15:09:43.789538Z","shell.execute_reply":"2022-01-09T15:09:43.795225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset and Dataloader","metadata":{}},{"cell_type":"code","source":"#paramters settings\ntrain_valid_ratio = 0.25\nmax_token_length = 128 #代表最多放入BERT的token長度\nbatch_size = 32\nuse_tfidf = False\n\ndf_train, df_valid = train_test_split(\n    data_train, test_size = train_valid_ratio\n)\n\n\nif use_tfidf:\n    tfidf_len = tfidf_matrix.shape[1]\n    train_dataset = JigsawDataset(\n        df_train, tokenizer=bert_tokenizer, \n        max_length=max_token_length , use_tfidf=use_tfidf, tfidf_matrix=tfidf_matrix\n     )\n    valid_dataset = JigsawDataset(\n        df_valid, tokenizer=bert_tokenizer, \n        max_length=max_token_length , use_tfidf=use_tfidf, tfidf_matrix=tfidf_matrix\n    )\nelse:\n    train_dataset = JigsawDataset(\n        df_train, tokenizer=bert_tokenizer, \n        max_length=max_token_length #, use_tfidf=use_tfidf, tfidf_matrix=tfidf_matrix\n     )\n    valid_dataset = JigsawDataset(\n        df_valid, tokenizer=bert_tokenizer, \n        max_length=max_token_length #, use_tfidf=use_tfidf, tfidf_matrix=tfidf_matrix\n    )\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True,\n    num_workers=2)\nvalid_loader = DataLoader(\n    valid_dataset, batch_size=batch_size, shuffle=False,\n    num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T12:44:59.31295Z","iopub.execute_input":"2022-01-10T12:44:59.313223Z","iopub.status.idle":"2022-01-10T12:44:59.330927Z","shell.execute_reply.started":"2022-01-10T12:44:59.313193Z","shell.execute_reply":"2022-01-10T12:44:59.330252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"#### jack settings\n- PRETRAINED_MODEL_NAME = \"roberta-base\"\n- train_valid_ratio = 0.25\n- max_token_length = 128\n- batch_size = 32\n- LR = 1e-4\n- WD = 1e-6\n- EPOCH = 10","metadata":{}},{"cell_type":"code","source":"# LR = 1e-4\n# WD = 1e-6\n# bert_drop_out = 0.2\n# margin_list = [0.5]\n\nLR_list = [1e-5, 1e-4]\nWD_list = [0, 1e-6]\nbert_dropout_list = [0.2, 0.3]\nmax_patience = 2\n\nEPOCH = 10\nHID_DIM = 768\nMARGIN = 0.5\nDATE = \"0110\"\nmodel_name = \"roberta\"\n\n\nfor LR in LR_list:\n    for WD in WD_list:\n        for bert_drop_out in bert_dropout_list:\n            print(f\"LR = {LR}, WD = {WD}, bert drop out = {bert_drop_out}\")\n            bert = RobertaModel.from_pretrained(PRETRAINED_MODEL_NAME).to(device)\n            dnn = NN(bert_drop_out, HID_DIM\n                     #, tfidf_len, use_tfidf=True\n                    ).to(device)\n            dnn.apply(init_weights)\n            model = JigsawModel(bert, dnn)\n            trainable_params = list(model.parameters())\n            num_trainable_params = sum(p.numel() for p in trainable_params)\n\n            criterion = nn.MarginRankingLoss(margin=MARGIN)\n            optimizer = AdamW(\n                trainable_params\n                ,lr=LR,weight_decay=WD)\n\n            print(f\"Total trainable parameters {num_trainable_params}\")\n\n            num_training_steps = EPOCH * len(train_loader)\n\n            train_accs = []\n            valid_accs = []\n            train_losses = []\n            valid_losses = []\n            best_valid_loss = np.inf\n            best_valid_acc = 0\n            best_epoch = 0\n            best_model = None\n            no_update = 0\n            MODEL_DIR = f\"./{DATE}_{model_name}_LR_{LR}_WD_{WD}_BDR_{bert_drop_out}.pth\"\n\n            # start training\n            #progress_bar = None\n            progress_bar = tqdm(range(num_training_steps))\n            for epoch in range(1, EPOCH+1):\n                _, train_acc, train_loss = train_step_combine(model, criterion, optimizer, train_loader, progress_bar, device, epoch, use_tfidf)\n                df_score, valid_acc, valid_loss = validate_all_combine(model, criterion, valid_loader, device, use_tfidf)\n                train_accs.append(train_acc)\n                valid_accs.append(valid_acc)\n                train_losses.append(train_loss)\n                valid_losses.append(valid_loss)\n\n                print(f\"Epoch {epoch}, Loss(Train/Valid) = {round(train_loss, 4)}/{round(valid_loss, 4)}, Accuracy(Train/Valid) = {round(train_acc*100, 3)}%/{round(valid_acc*100, 3)}%\")\n\n                if valid_acc > best_valid_acc:\n                    print(f\"Saving model...\")\n                    best_epoch = epoch\n                    best_valid_acc = valid_acc\n                    best_valid_loss = valid_loss\n                    best_model = model\n                    torch.save(\n                        {\"BERT\": best_model.bert.state_dict(),\"NN\": best_model.fc.state_dict()}\n                        ,MODEL_DIR\n                    )\n                #early stopping\n                else:\n                    no_update += 1 \n                \n                if no_update == max_patience:\n                    break\n                    \n            print(f\"Best epoch: {best_epoch}, valid loss: {round(best_valid_loss, 4)}, valid acc: {round(best_valid_acc*100, 4)}%\")\n\n            #compute acc for all dataset\n            all_dataset = JigsawDataset(\n                data_train, tokenizer=bert_tokenizer, \n                max_length=max_token_length, use_tfidf=use_tfidf, tfidf_matrix=tfidf_matrix\n            )\n\n            all_loader = DataLoader(\n                all_dataset, batch_size=batch_size, shuffle=True,\n                num_workers=2)\n\n            df_score, accuracy, valid_loss = validate_all_combine(model, criterion, all_loader, device, use_tfidf)\n            df_score_text = return_wrong_text(df_score, df_valid)\n            csv_out_path = f\"./wrong_text_{DATE}_LR_{LR}_WD_{WD}_BDR_{bert_drop_out}.csv\"\n            df_score_text.to_csv(csv_out_path)\n            print(f\"Accuracy in all dataset set: {round(accuracy*100, 3)}%\")\n\n            #Evaluation\n            plot_loss_plot(train_losses, valid_losses)\n            plot_acc_plot(train_accs, valid_accs)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T12:45:04.901971Z","iopub.execute_input":"2022-01-10T12:45:04.902491Z","iopub.status.idle":"2022-01-10T13:01:04.065561Z","shell.execute_reply.started":"2022-01-10T12:45:04.902451Z","shell.execute_reply":"2022-01-10T13:01:04.063576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all_dataset = JigsawDataset(\n#     data_train, tokenizer=bert_tokenizer, \n#     max_length=max_token_length)\n\n# all_loader = DataLoader(\n#     train_dataset, batch_size=batch_size, shuffle=True,\n#     num_workers=2)\n\n# df_score, accuracy, valid_loss = validate_all(bert, dnn, criterion, all_loader, device)\n# print(f\"Accuracy in all dataset set: {round(accuracy*100, 3)}%\")\n\n# df_score_text = return_wrong_text(df_score, df_valid)\n# df_score_text.to_csv(\"./wrong_text_1229_2.csv\")\n# print(df_score_text)\n\n# plot_loss_plot(train_losses, valid_losses)\n# plot_acc_plot(train_accs, valid_accs)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:46:49.367908Z","iopub.status.idle":"2022-01-09T09:46:49.368199Z","shell.execute_reply.started":"2022-01-09T09:46:49.368055Z","shell.execute_reply":"2022-01-09T09:46:49.368071Z"},"trusted":true},"execution_count":null,"outputs":[]}]}