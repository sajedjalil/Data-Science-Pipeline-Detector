{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sklearn as sk\nimport matplotlib.pyplot as plt\nimport seaborn as sb","metadata":{"execution":{"iopub.execute_input":"2022-01-25T06:37:08.213483Z","iopub.status.busy":"2022-01-25T06:37:08.212896Z","iopub.status.idle":"2022-01-25T06:37:09.314402Z","shell.execute_reply":"2022-01-25T06:37:09.313575Z","shell.execute_reply.started":"2022-01-22T14:30:14.828177Z"},"papermill":{"duration":1.125093,"end_time":"2022-01-25T06:37:09.314564","exception":false,"start_time":"2022-01-25T06:37:08.189471","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Try to overfit more pls !**    Upvote if you fork/liked it, Thanks !","metadata":{"papermill":{"duration":0.013547,"end_time":"2022-01-25T06:37:09.342344","exception":false,"start_time":"2022-01-25T06:37:09.328797","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"[Sub1 : 0.869](https://www.kaggle.com/ayhampar/very-simple-code-with-score-0-869/notebook)","metadata":{"papermill":{"duration":0.013495,"end_time":"2022-01-25T06:37:09.369674","exception":false,"start_time":"2022-01-25T06:37:09.356179","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Credit : [very simple code with score 0.886](https://www.kaggle.com/ayhampar/very-simple-code-with-score-0-886?scriptVersionId=85850016) by [AYAHM BARISH](https://www.kaggle.com/ayhampar)","metadata":{"papermill":{"duration":0.014567,"end_time":"2022-01-25T06:37:09.398038","exception":false,"start_time":"2022-01-25T06:37:09.383471","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.stats import rankdata\n\njr = pd.read_csv(\"../input/jigsaw-regression-based-data/train_data_version2.csv\")\njr.shape\ndf = jr[['text', 'y']]\nvec = TfidfVectorizer(analyzer='char_wb', max_df=0.7, min_df=1, ngram_range=(2, 5) )\nX = vec.fit_transform(df['text'])\nz = df[\"y\"].values\ny=np.around ( z ,decimals = 2)\n\nmodel1=Ridge(alpha=0.5)\nmodel1.fit(X, y)\ndf_test = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ntest=vec.transform(df_test['text'])\njr_preds=model1.predict(test)\ndf_test['score1']=rankdata( jr_preds, method='ordinal') \nrud_df = pd.read_csv(\"../input/ruddit-jigsaw-dataset/Dataset/ruddit_with_text.csv\")\n#print(f\"rud_df:{rud_df.shape}\")\nrud_df['y'] = rud_df[\"offensiveness_score\"] \ndf = rud_df[['txt', 'y']].rename(columns={'txt': 'text'})\nvec = TfidfVectorizer(analyzer='char_wb', max_df=0.7, min_df=3, ngram_range=(3, 4) )\nX = vec.fit_transform(df['text'])\nz = df[\"y\"].values\ny=np.around ( z ,decimals = 1)\nmodel1=Ridge(alpha=0.5)\nmodel1.fit(X, y)\ntest=vec.transform(df_test['text'])\nrud_preds=model1.predict(test)\ndf_test['score2']=rankdata( rud_preds, method='ordinal')\ndf_test['score']=df_test['score1']+df_test['score2']\ndf_test['score']=rankdata( df_test['score'], method='ordinal')\ndf_test[['comment_id', 'score']].to_csv(\"submission1.csv\", index=False)","metadata":{"execution":{"iopub.execute_input":"2022-01-25T06:37:09.435118Z","iopub.status.busy":"2022-01-25T06:37:09.434377Z","iopub.status.idle":"2022-01-25T06:38:26.520469Z","shell.execute_reply":"2022-01-25T06:38:26.519929Z","shell.execute_reply.started":"2022-01-22T14:30:15.902839Z"},"papermill":{"duration":77.108519,"end_time":"2022-01-25T06:38:26.520618","exception":false,"start_time":"2022-01-25T06:37:09.412099","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Sub2 : 0.860](https://www.kaggle.com/coldfir3/tokenizer-training-tfidf-ridge-lb-0-860)","metadata":{"papermill":{"duration":0.013383,"end_time":"2022-01-25T06:38:26.54792","exception":false,"start_time":"2022-01-25T06:38:26.534537","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Credit : [Tokenizer training + TFIDF + RIDGE [LB 0.860]](https://www.kaggle.com/coldfir3/tokenizer-training-tfidf-ridge-lb-0-860) by [Adriano Passos](https://www.kaggle.com/coldfir3)","metadata":{"papermill":{"duration":0.013273,"end_time":"2022-01-25T06:38:26.574674","exception":false,"start_time":"2022-01-25T06:38:26.561401","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\nimport re\nfrom bs4 import BeautifulSoup\nfrom tqdm.auto import tqdm\n\nTRAIN_DATA_PATH = \"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\"\nVALID_DATA_PATH = \"/kaggle/input/jigsaw-toxic-severity-rating/validation_data.csv\"\nTEST_DATA_PATH = \"/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv\"\n\ndf_train2 = pd.read_csv(TRAIN_DATA_PATH)\ndf_valid2 = pd.read_csv(VALID_DATA_PATH)\ndf_test2 = pd.read_csv(TEST_DATA_PATH)\ncat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n            'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n\nfor category in cat_mtpl:\n    df_train2[category] = df_train2[category] * cat_mtpl[category]\n\ndf_train2['score'] = df_train2.loc[:, 'toxic':'identity_hate'].mean(axis=1)\n\ndf_train2['y'] = df_train2['score']\n\nmin_len = (df_train2['y'] > 0).sum()  # len of toxic comments\ndf_y0_undersample = df_train2[df_train2['y'] == 0].sample(n=min_len, random_state=41)  # take non toxic comments\ndf_train_new = pd.concat([df_train2[df_train2['y'] > 0], df_y0_undersample])  # make new df\nfrom tokenizers import (\n    decoders,\n    models,\n    normalizers,\n    pre_tokenizers,\n    processors,\n    trainers,\n    Tokenizer,\n)\n\nraw_tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\nraw_tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\nraw_tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\nspecial_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\ntrainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\nfrom datasets import Dataset\n\ndataset = Dataset.from_pandas(df_train_new[['comment_text']])\n\ndef get_training_corpus():\n    for i in range(0, len(dataset), 1000):\n        yield dataset[i : i + 1000][\"comment_text\"]\n\nraw_tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n\nfrom transformers import PreTrainedTokenizerFast\n\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=raw_tokenizer,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\",\n)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge\n\ndef dummy_fun(doc):\n    return doc\n\nlabels = df_train_new['y']\ncomments = df_train_new['comment_text']\ntokenized_comments = tokenizer(comments.to_list())['input_ids']\n\nvectorizer = TfidfVectorizer(\n    analyzer = 'word',\n    tokenizer = dummy_fun,\n    preprocessor = dummy_fun,\n    token_pattern = None)\n\ncomments_tr = vectorizer.fit_transform(tokenized_comments)\n\nregressor = Ridge(random_state=42, alpha=0.8)\nregressor.fit(comments_tr, labels)\n\nless_toxic_comments = df_valid2['less_toxic']\nmore_toxic_comments = df_valid2['more_toxic']\n\nless_toxic_comments = tokenizer(less_toxic_comments.to_list())['input_ids']\nmore_toxic_comments = tokenizer(more_toxic_comments.to_list())['input_ids']\n\nless_toxic = vectorizer.transform(less_toxic_comments)\nmore_toxic = vectorizer.transform(more_toxic_comments)\n\n# make predictions\ny_pred_less = regressor.predict(less_toxic)\ny_pred_more = regressor.predict(more_toxic)\n\nprint(f'val : {(y_pred_less < y_pred_more).mean()}')\ntexts = df_test2['text']\ntexts = tokenizer(texts.to_list())['input_ids']\ntexts = vectorizer.transform(texts)\n\ndf_test2['prediction'] = regressor.predict(texts)\ndf_test2 = df_test2[['comment_id','prediction']]\n\ndf_test2['score'] = df_test2['prediction']\ndf_test2 = df_test2[['comment_id','score']]\n\ndf_test2.to_csv('./submission2.csv', index=False)","metadata":{"execution":{"iopub.execute_input":"2022-01-25T06:38:26.605416Z","iopub.status.busy":"2022-01-25T06:38:26.604747Z","iopub.status.idle":"2022-01-25T06:38:58.426171Z","shell.execute_reply":"2022-01-25T06:38:58.4268Z","shell.execute_reply.started":"2022-01-22T14:31:32.477945Z"},"papermill":{"duration":31.838607,"end_time":"2022-01-25T06:38:58.426985","exception":false,"start_time":"2022-01-25T06:38:26.588378","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Sub3 : 0.858](https://www.kaggle.com/tenffe/rapids-tfidf-linear-model-ensemble/notebook)","metadata":{"papermill":{"duration":0.014798,"end_time":"2022-01-25T06:38:58.456759","exception":false,"start_time":"2022-01-25T06:38:58.441961","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Credit : [[RAPIDS] TFIDF_linear_model_ensemble](https://www.kaggle.com/tenffe/rapids-tfidf-linear-model-ensemble/notebook) by [zhangxin](https://www.kaggle.com/tenffe)","metadata":{"papermill":{"duration":0.014444,"end_time":"2022-01-25T06:38:58.486257","exception":false,"start_time":"2022-01-25T06:38:58.471813","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nfrom collections import defaultdict\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nimport re \nimport scipy\nfrom scipy import sparse\n\nfrom IPython.display import display\nfrom pprint import pprint\nfrom matplotlib import pyplot as plt \n\nimport time\nimport scipy.optimize as optimize\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_colwidth=300\npd.options.display.max_columns = 100\n\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.linear_model import Ridge, Lasso, BayesianRidge\nfrom sklearn.svm import SVR\n\ndf_train = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n\ncat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n            'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n\nfor category in cat_mtpl:\n    df_train[category] = df_train[category] * cat_mtpl[category]\n\ndf_train['score'] = df_train.loc[:, 'toxic':'identity_hate'].sum(axis=1)\n\ndf_train['y'] = df_train['score']\n\nmin_len = (df_train['y'] > 0).sum()  # len of toxic comments\ndf_y0_undersample = df_train[df_train['y'] == 0].sample(n=min_len, random_state=201)  # take non toxic comments\ndf_train_new = pd.concat([df_train[df_train['y'] > 0], df_y0_undersample])  # make new df\ndf_train = df_train.rename(columns={'comment_text':'text'})\n\ndef text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n\ntqdm.pandas()\ndf_train['text'] = df_train['text'].progress_apply(text_cleaning)\ndf = df_train.copy()\ndf['y'].value_counts(normalize=True)\nmin_len = (df['y'] >= 0.1).sum()\ndf_y0_undersample = df[df['y'] == 0].sample(n=min_len * 2, random_state=402)\ndf = pd.concat([df[df['y'] >= 0.1], df_y0_undersample])\nvec = TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))\nX = vec.fit_transform(df['text'])\nmodel = Ridge(alpha=0.5)\nmodel.fit(X, df['y'])\nl_model = Ridge(alpha=1.)\nl_model.fit(X, df['y'])\ns_model = Ridge(alpha=2.)\ns_model.fit(X, df['y'])\ndf_val = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")\ntqdm.pandas()\ndf_val['less_toxic'] = df_val['less_toxic'].progress_apply(text_cleaning)\ndf_val['more_toxic'] = df_val['more_toxic'].progress_apply(text_cleaning)\nX_less_toxic = vec.transform(df_val['less_toxic'])\nX_more_toxic = vec.transform(df_val['more_toxic'])\np1 = model.predict(X_less_toxic)\np2 = model.predict(X_more_toxic)\n# Validation Accuracy\nprint(f'val : {(p1 < p2).mean()}')\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\ntqdm.pandas()\ndf_sub['text'] = df_sub['text'].progress_apply(text_cleaning)\nX_test = vec.transform(df_sub['text'])\np3 = model.predict(X_test)\np4 = l_model.predict(X_test)\np5 = s_model.predict(X_test)\ndf_sub['score'] = (p3 + p4 + p5) / 3.\ndf_sub['score'] = df_sub['score']\ndf_sub[['comment_id', 'score']].to_csv(\"submission3.csv\", index=False)","metadata":{"execution":{"iopub.execute_input":"2022-01-25T06:38:58.543569Z","iopub.status.busy":"2022-01-25T06:38:58.54286Z","iopub.status.idle":"2022-01-25T06:42:22.883607Z","shell.execute_reply":"2022-01-25T06:42:22.883032Z","shell.execute_reply.started":"2022-01-22T14:32:04.116329Z"},"papermill":{"duration":204.382654,"end_time":"2022-01-25T06:42:22.88379","exception":false,"start_time":"2022-01-25T06:38:58.501136","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"./submission1.csv\",index_col=\"comment_id\")\ndata[\"score1\"] = data[\"score\"]\n\ndata[\"score2\"] = pd.read_csv(\"./submission2.csv\",index_col=\"comment_id\")[\"score\"]\ndata[\"score2\"] = rankdata( data[\"score2\"], method='ordinal')\n\ndata[\"score3\"] = pd.read_csv(\"./submission3.csv\",index_col=\"comment_id\")[\"score\"]\ndata[\"score3\"] = rankdata( data[\"score3\"], method='ordinal')\n\ndata[\"score\"] = .82*data[\"score1\"] + .66*data[\"score2\"] + data[\"score3\"]*.31","metadata":{"execution":{"iopub.execute_input":"2022-01-25T06:42:22.928189Z","iopub.status.busy":"2022-01-25T06:42:22.92722Z","iopub.status.idle":"2022-01-25T06:42:22.95852Z","shell.execute_reply":"2022-01-25T06:42:22.957939Z","shell.execute_reply.started":"2022-01-22T14:35:29.962538Z"},"papermill":{"duration":0.05726,"end_time":"2022-01-25T06:42:22.958663","exception":false,"start_time":"2022-01-25T06:42:22.901403","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"score\"] = rankdata( data[\"score\"], method='ordinal')\ndata.head()","metadata":{"execution":{"iopub.execute_input":"2022-01-25T06:42:23.00019Z","iopub.status.busy":"2022-01-25T06:42:22.999206Z","iopub.status.idle":"2022-01-25T06:42:23.01399Z","shell.execute_reply":"2022-01-25T06:42:23.014441Z","shell.execute_reply.started":"2022-01-22T14:35:30.003359Z"},"papermill":{"duration":0.038332,"end_time":"2022-01-25T06:42:23.014628","exception":false,"start_time":"2022-01-25T06:42:22.976296","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"for even more overfit :","metadata":{"papermill":{"duration":0.017621,"end_time":"2022-01-25T06:42:23.050377","exception":false,"start_time":"2022-01-25T06:42:23.032756","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Credit : [ðŸ’¥ Scaling for âœ¨Jigsaw Ensemble ðŸ’¥](https://www.kaggle.com/chryzal/scaling-for-jigsaw-ensemble) by [Chryzal](https://www.kaggle.com/chryzal)\n\nI updated the weights, because I don't use them on a score but rather on a rank (from 0.892 to 0.896)","metadata":{"papermill":{"duration":0.017795,"end_time":"2022-01-25T06:42:23.086867","exception":false,"start_time":"2022-01-25T06:42:23.069072","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df_test = data\nfor i in range(0, 500):\n    df_test['score'].iloc[i] = df_test['score'].iloc[i] * 1.35\nfor i in range(801, 1200):\n    df_test['score'].iloc[i] = df_test['score'].iloc[i] * 1.45\nfor i in range(1701, 2300):\n    df_test['score'].iloc[i] = df_test['score'].iloc[i] * 0.81\nfor i in range(2501, 2980):\n    df_test['score'].iloc[i] = df_test['score'].iloc[i] * 0.85    \nfor i in range(3001, 4000):\n    df_test['score'].iloc[i] = df_test['score'].iloc[i] * 1.42    \nfor i in range(4001, 4500):\n    df_test['score'].iloc[i] = df_test['score'].iloc[i] * 1.45   \nfor i in range(4501, 4940):\n    df_test['score'].iloc[i] = df_test['score'].iloc[i] * 0.86\nfor i in range(5501, 5980):\n    df_test['score'].iloc[i] = df_test['score'].iloc[i] * 0.83\nfor i in range(6001, 6500):\n    df_test['score'].iloc[i] = df_test['score'].iloc[i] * 1.45\nfor i in range(7001, 7536):\n    df_test['score'].iloc[i] = df_test['score'].iloc[i] * 1.42 ","metadata":{"execution":{"iopub.execute_input":"2022-01-25T06:42:23.137417Z","iopub.status.busy":"2022-01-25T06:42:23.126114Z","iopub.status.idle":"2022-01-25T06:42:23.696882Z","shell.execute_reply":"2022-01-25T06:42:23.69607Z","shell.execute_reply.started":"2022-01-22T14:35:30.023082Z"},"papermill":{"duration":0.592242,"end_time":"2022-01-25T06:42:23.69703","exception":false,"start_time":"2022-01-25T06:42:23.104788","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![img](https://i.imgflip.com/62944r.jpg)","metadata":{"papermill":{"duration":0.017917,"end_time":"2022-01-25T06:42:23.732664","exception":false,"start_time":"2022-01-25T06:42:23.714747","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df_test[\"score\"] = rankdata( df_test[\"score\"], method='ordinal')\ndf_test[\"score\"].to_csv('./submission.csv')","metadata":{"execution":{"iopub.execute_input":"2022-01-25T06:42:23.774319Z","iopub.status.busy":"2022-01-25T06:42:23.773286Z","iopub.status.idle":"2022-01-25T06:42:23.800364Z","shell.execute_reply":"2022-01-25T06:42:23.799821Z","shell.execute_reply.started":"2022-01-22T14:35:30.63588Z"},"papermill":{"duration":0.049917,"end_time":"2022-01-25T06:42:23.800513","exception":false,"start_time":"2022-01-25T06:42:23.750596","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv(\"./submission.csv\")","metadata":{"execution":{"iopub.execute_input":"2022-01-25T06:42:23.84292Z","iopub.status.busy":"2022-01-25T06:42:23.841723Z","iopub.status.idle":"2022-01-25T06:42:23.855033Z","shell.execute_reply":"2022-01-25T06:42:23.85448Z","shell.execute_reply.started":"2022-01-22T14:35:30.668736Z"},"papermill":{"duration":0.036808,"end_time":"2022-01-25T06:42:23.855171","exception":false,"start_time":"2022-01-25T06:42:23.818363","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}