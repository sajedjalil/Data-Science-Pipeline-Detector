{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h2>【0.821】TFIDF_LR_simple_baseline</h2>\n\nData from [all-in-one-dataset](https://www.kaggle.com/adldotori/all-in-one-dataset/notebook)","metadata":{}},{"cell_type":"markdown","source":"# Import Library","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nfrom collections import defaultdict\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nimport re \nimport scipy\nfrom scipy import sparse\n\nfrom IPython.display import display\nfrom pprint import pprint\nfrom matplotlib import pyplot as plt \n\nimport time\nimport scipy.optimize as optimize\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_colwidth=300\npd.options.display.max_columns = 100\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom nltk.tokenize import word_tokenize\nfrom datasets import Dataset","metadata":{"execution":{"iopub.status.busy":"2021-12-07T15:10:35.500259Z","iopub.execute_input":"2021-12-07T15:10:35.500685Z","iopub.status.idle":"2021-12-07T15:10:38.600211Z","shell.execute_reply.started":"2021-12-07T15:10:35.500547Z","shell.execute_reply":"2021-12-07T15:10:38.599199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare train data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport copy\nimport re\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom nltk import WordNetLemmatizer\n\n\nclass BaseTokenizer(object):\n    def process_text(self, text):\n        raise NotImplemented\n\n    def process(self, texts):\n        for text in texts:\n            yield self.process_text(text)\n\n\nRE_PATTERNS = {\n    ' american ':\n        [\n            'amerikan'\n        ],\n    ' adolf ':\n        [\n            'adolf'\n        ],\n    ' hitler ':\n        [\n            'hitler'\n        ],\n    ' fuck':\n        [\n            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n            'feck ', ' fux ', 'f\\*\\*', \n            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck'\n        ],\n    ' ass ':\n        [\n            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$'\n                                                           '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s'\n        ],\n    ' ass hole ':\n        [\n            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole'\n        ],\n    ' bitch ':\n        [\n            'b[w]*i[t]*ch', 'b!tch',\n            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h'\n        ],\n    ' bastard ':\n        [\n            'ba[s|z]+t[e|a]+rd'\n        ],\n    ' trans gender':\n        [\n            'transgender'\n        ],\n    ' gay ':\n        [\n            'gay'\n        ],\n    ' cock ':\n        [\n            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n        ],\n    ' dick ':\n        [\n            ' dick[^aeiou]', 'deek', 'd i c k'\n        ],\n    ' suck ':\n        [\n            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n        ],\n    ' cunt ':\n        [\n            'cunt', 'c u n t'\n        ],\n    ' bull shit ':\n        [\n            'bullsh\\*t', 'bull\\$hit'\n        ],\n    ' homo sex ual':\n        [\n            'homosexual'\n        ],\n    ' jerk ':\n        [\n            'jerk'\n        ],\n    ' idiot ':\n        [\n            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots'\n                                                                                      'i d i o t'\n        ],\n    ' dumb ':\n        [\n            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n        ],\n    ' shit ':\n        [\n            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t'\n        ],\n    ' shit hole ':\n        [\n            'shythole'\n        ],\n    ' retard ':\n        [\n            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n        ],\n    ' rape ':\n        [\n            ' raped'\n        ],\n    ' dumb ass':\n        [\n            'dumbass', 'dubass'\n        ],\n    ' ass head':\n        [\n            'butthead'\n        ],\n    ' sex ':\n        [\n            'sexy', 's3x', 'sexuality'\n        ],\n    ' nigger ':\n        [\n            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n        ],\n    ' shut the fuck up':\n        [\n            'stfu'\n        ],\n    ' pussy ':\n        [\n            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses'\n        ],\n    ' faggot ':\n        [\n            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n        ],\n    ' mother fucker':\n        [\n            ' motha ', ' motha f', ' mother f', 'motherucker',\n        ],\n    ' whore ':\n        [\n            'wh\\*\\*\\*', 'w h o r e'\n        ],\n}\n\n\nclass PatternTokenizer(BaseTokenizer):\n    def __init__(self, lower=True, initial_filters=r\"[^a-z0-9!@#\\$%\\^\\&\\*_\\-,\\.' ]\", patterns=RE_PATTERNS,\n                 remove_repetitions=True):\n        self.lower = lower\n        self.patterns = patterns\n        self.initial_filters = initial_filters\n        self.remove_repetitions = remove_repetitions\n\n    def process_text(self, text):\n        x = self._preprocess(text)\n        for target, patterns in self.patterns.items():\n            for pat in patterns:\n                x = re.sub(pat, target, x)\n        x = re.sub(r\"[^a-z' ]\", ' ', x)\n        return x.split()\n\n    def process_ds(self, ds):\n        ### ds = Data series\n\n        # lower\n        ds = copy.deepcopy(ds)\n        if self.lower:\n            ds = ds.str.lower()\n        # remove special chars\n        if self.initial_filters is not None:\n            ds = ds.str.replace(self.initial_filters, ' ')\n        # fuuuuck => fuck\n        if self.remove_repetitions:\n            pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL) \n            ds = ds.str.replace(pattern, r\"\\1\")\n\n        for target, patterns in self.patterns.items():\n            for pat in patterns:\n                ds = ds.str.replace(pat, target)\n\n        ds = ds.str.replace(r\"[^a-z' ]\", ' ')\n\n        return ds.str.split()\n\n    def _preprocess(self, text):\n        # lower\n        if self.lower:\n            text = text.lower()\n\n        # remove special chars\n        if self.initial_filters is not None:\n            text = re.sub(self.initial_filters, ' ', text)\n\n        # fuuuuck => fuck\n        if self.remove_repetitions:\n            pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n            text = pattern.sub(r\"\\1\", text)\n        return text\n    \n\ndef text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-12-07T15:10:38.603707Z","iopub.execute_input":"2021-12-07T15:10:38.604004Z","iopub.status.idle":"2021-12-07T15:10:38.776769Z","shell.execute_reply.started":"2021-12-07T15:10:38.603959Z","shell.execute_reply":"2021-12-07T15:10:38.775783Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Data:\n    def __init__(self):\n        df = pd.read_csv(\"../input/jigsaw-dataset/all_in_one_jigsaw.csv\")\n        df = self.get_text_label_df(df)\n        self.df = self.clean(df)\n        \n    def get_text_label_df(self, df):\n        df['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) > 0 ).astype(int)\n        df = df[['comment_text_processed', 'y']].rename(columns={'comment_text_processed': 'text'})\n        df = df.dropna()\n        return df\n    \n    def unsample(self, df, random_state=402):\n        min_len = (df['y'] == 1).sum()\n        df_y0_undersample = df[df['y'] == 0].sample(n=min_len, random_state=random_state)\n        df = pd.concat([df[df['y'] == 1], df_y0_undersample])\n        return df\n    \n    def clean(self, df):\n        tqdm.pandas()\n        df['text'] = df['text'].progress_apply(text_cleaning)\n        return df\n    \n    def to_dataset(self):\n        ds = Dataset.from_pandas(self.df)\n        ds.save_to_disk(\"./jigsaw_dataset\")","metadata":{"execution":{"iopub.status.busy":"2021-12-07T15:10:38.777755Z","iopub.execute_input":"2021-12-07T15:10:38.777976Z","iopub.status.idle":"2021-12-07T15:10:38.789352Z","shell.execute_reply.started":"2021-12-07T15:10:38.777949Z","shell.execute_reply":"2021-12-07T15:10:38.788427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = Data()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T15:10:38.791458Z","iopub.execute_input":"2021-12-07T15:10:38.791722Z","iopub.status.idle":"2021-12-07T15:23:54.750575Z","shell.execute_reply.started":"2021-12-07T15:10:38.791692Z","shell.execute_reply":"2021-12-07T15:23:54.749628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.to_dataset()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T15:23:54.752996Z","iopub.execute_input":"2021-12-07T15:23:54.753232Z","iopub.status.idle":"2021-12-07T15:23:57.941069Z","shell.execute_reply.started":"2021-12-07T15:23:54.753204Z","shell.execute_reply":"2021-12-07T15:23:57.940412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ./jigsaw_dataset/ -lh","metadata":{"execution":{"iopub.status.busy":"2021-12-07T15:24:58.61531Z","iopub.execute_input":"2021-12-07T15:24:58.615671Z","iopub.status.idle":"2021-12-07T15:24:59.545096Z","shell.execute_reply.started":"2021-12-07T15:24:58.615634Z","shell.execute_reply":"2021-12-07T15:24:59.543991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```python\ntokenizer = PatternTokenizer()\nfinal[\"comment_text_processed\"] = tokenizer.process_ds(final[\"comment_text\"]).str.join(sep=\" \")\n\ntqdm.pandas()\ndf['text'] = df['text'].progress_apply(text_cleaning)\n```","metadata":{}},{"cell_type":"code","source":"# %%time\n# df = pd.read_csv(\"../input/jigsaw-dataset/all_in_one_jigsaw.csv\")\n# df['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) > 0 ).astype(int)\n# df = df[['comment_text_processed', 'y']].rename(columns={'comment_text_processed': 'text'})\n# df = df.dropna()\n# df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- comment_text_processed is the text which will be used in training.\n- ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'] for label\n- we will use the same data preprocess in the [notebook](https://www.kaggle.com/adldotori/all-in-one-dataset/notebook)","metadata":{}},{"cell_type":"markdown","source":"<h3>Text Cleaning</h3>","metadata":{}},{"cell_type":"code","source":"# tqdm.pandas()\n# df_val['less_toxic'] = df_val['less_toxic'].progress_apply(text_cleaning)\n# df_val['more_toxic'] = df_val['more_toxic'].progress_apply(text_cleaning)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Undersampling","metadata":{}},{"cell_type":"code","source":"# df['y'].value_counts()\n# df['y'].value_counts(normalize=True)\n\n# min_len = (df['y'] == 1).sum()\n# df_y0_undersample = df[df['y'] == 0].sample(n=min_len, random_state=201)\n# df = pd.concat([df[df['y'] == 1], df_y0_undersample])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# TF-IDF","metadata":{}},{"cell_type":"code","source":"# vec = TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))\n# X = vec.fit_transform(df['text'])\n\n# model = LogisticRegression()\n# model.fit(X, df['y'])\n\n# df_val = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Text cleaning</h2>","metadata":{}},{"cell_type":"code","source":"# X_less_toxic = vec.transform(df_val['less_toxic'])\n# X_more_toxic = vec.transform(df_val['more_toxic'])\n\n# p1 = model.predict_proba(X_less_toxic)\n# p2 = model.predict_proba(X_more_toxic)\n\n# # Validation Accuracy\n# (p1[:, 1] < p2[:, 1]).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare submission data ","metadata":{}},{"cell_type":"code","source":"# df_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n# tqdm.pandas()\n# df_sub['text'] = df_sub['text'].progress_apply(text_cleaning)\n# X_test = vec.transform(df_sub['text'])\n# p3 = model.predict_proba(X_test)\n# df_sub['score'] = p3[:, 1]\n# df_sub['score'].count()\n# df_sub['score'] = df_sub['score'] \n# # 9 comments will fail if compared one with the other\n# df_sub['score'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}