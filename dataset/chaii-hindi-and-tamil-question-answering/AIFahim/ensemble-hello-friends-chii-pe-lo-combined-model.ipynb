{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### My ChangeLog:\n- V4: Ensemble Hello Friend Chaii pi lo & Chaii Pe Lo G\n- V5:[Error] Change fin_start_logits /= 6 -> fin_start_logits /= 10 (Forgot to change iteration 6 -> 10 for fin_start_logits )\n- V6:[Error] Change fin_start_logits /= 6 -> fin_start_logits /= 10 (Forgot to change iteration 6 -> 10 for fin_start_logits )\n- V7: Change fin_start_logits /= 6 -> fin_start_logits /= 10 (agian)\n- V8: [Error] `start_logits =0.4*fin_start_logits+ 0.6*((start_logits1 + start_logits2 + start_logits3 + start_logits4 + start_logits5) / 5)` \n      `end_logits = 0.4*fin_end_logits+0.6*((end_logits1 + end_logits2 + end_logits3 + end_logits4 +  end_logits5) / 5)` [Forgor to Turn of GPU] \n<br>\n- V9: `start_logits =0.4*fin_start_logits+ 0.6*((start_logits1 + start_logits2 + start_logits3 + start_logits4 + start_logits5) / 5)` \n      `end_logits = 0.4*fin_end_logits+0.6*((end_logits1 + end_logits2 + end_logits3 + end_logits4 +  end_logits5) / 5)` (Again)","metadata":{}},{"cell_type":"markdown","source":"Upvote these notebooks. Upvote my one if it helps :)\n\nhttps://www.kaggle.com/mihtw1/chaii-ensemble\n\nhttps://www.kaggle.com/abhishek/hello-friends-chaii-pi-lo\n\nhttps://www.kaggle.com/jillanisofttech/chaii-pe-lo-g-with-acc-0-792\n\nhttps://www.kaggle.com/nbroad/chaii-qa-torch-5-fold-with-post-processing-765\n\nhttps://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit\n\nhttps://www.kaggle.com/adldotori/how-to-qa-with-xlmr5\n\nhttps://www.kaggle.com/kishalmandal/5foldsroberta\n\n","metadata":{}},{"cell_type":"code","source":"!pip uninstall fsspec -qq -y\n!pip install --no-index --find-links ../input/hf-datasets/wheels datasets -qq","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:48:41.632907Z","iopub.execute_input":"2021-11-09T04:48:41.633487Z","iopub.status.idle":"2021-11-09T04:48:50.94579Z","shell.execute_reply.started":"2021-11-09T04:48:41.633397Z","shell.execute_reply":"2021-11-09T04:48:50.944871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/tez-lib/\")\nimport collections\nimport numpy as np\nimport transformers\nimport pandas as pd\nfrom datasets import Dataset\nfrom functools import partial\nfrom tqdm import tqdm\nimport torch\n\nfrom sklearn import metrics\nimport transformers\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport tez\nfrom string import punctuation","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:48:50.948891Z","iopub.execute_input":"2021-11-09T04:48:50.949186Z","iopub.status.idle":"2021-11-09T04:48:57.247867Z","shell.execute_reply.started":"2021-11-09T04:48:50.949149Z","shell.execute_reply":"2021-11-09T04:48:57.247051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ChaiiModel(tez.Model):\n    def __init__(self, model_name, num_train_steps, steps_per_epoch, learning_rate):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.steps_per_epoch = steps_per_epoch\n        self.model_name = model_name\n        self.num_train_steps = num_train_steps\n        self.step_scheduler_after = \"batch\"\n\n        hidden_dropout_prob: float = 0.0\n        layer_norm_eps: float = 1e-7\n\n        config = transformers.AutoConfig.from_pretrained(model_name)\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.transformer = transformers.AutoModel.from_pretrained(model_name, config=config)\n        self.output = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, ids, mask, token_type_ids=None, start_positions=None, end_positions=None):\n        transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out[0]\n        logits = self.output(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n        return (start_logits, end_logits), 0, {}","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:48:57.249272Z","iopub.execute_input":"2021-11-09T04:48:57.249533Z","iopub.status.idle":"2021-11-09T04:48:57.262457Z","shell.execute_reply.started":"2021-11-09T04:48:57.249498Z","shell.execute_reply":"2021-11-09T04:48:57.261631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ChaiiDataset:\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, item):\n        return {\n            \"ids\": torch.tensor(self.data[item][\"input_ids\"], dtype=torch.long),\n            \"mask\": torch.tensor(self.data[item][\"attention_mask\"], dtype=torch.long),\n        }","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:48:57.264921Z","iopub.execute_input":"2021-11-09T04:48:57.265372Z","iopub.status.idle":"2021-11-09T04:48:57.271845Z","shell.execute_reply.started":"2021-11-09T04:48:57.265337Z","shell.execute_reply":"2021-11-09T04:48:57.270872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_validation_features(examples, tokenizer, pad_on_right, max_length, doc_stride):\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    tokenized_examples[\"example_id\"] = []\n\n    for i in range(len(tokenized_examples[\"input_ids\"])):\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n        sample_index = sample_mapping[i]\n        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n        tokenized_examples[\"offset_mapping\"][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n        ]\n\n    return tokenized_examples","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:48:57.273417Z","iopub.execute_input":"2021-11-09T04:48:57.273744Z","iopub.status.idle":"2021-11-09T04:48:57.285417Z","shell.execute_reply.started":"2021-11-09T04:48:57.27371Z","shell.execute_reply":"2021-11-09T04:48:57.284629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def postprocess_qa_predictions(\n    examples, tokenizer, features, raw_predictions, n_best_size=20, max_answer_length=30, squad_v2=False\n):\n    all_start_logits, all_end_logits = raw_predictions\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    predictions = collections.OrderedDict()\n\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    for example_index, example in enumerate(tqdm(examples)):\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None  # Only used if squad_v2 is True.\n        valid_answers = []\n\n        context = example[\"context\"]\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n\n            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char:end_char],\n                        }\n                    )\n\n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n\n        if not squad_v2:\n            predictions[example[\"id\"]] = best_answer[\"text\"]\n        else:\n            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n            predictions[example[\"id\"]] = answer\n\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:48:57.287636Z","iopub.execute_input":"2021-11-09T04:48:57.289925Z","iopub.status.idle":"2021-11-09T04:48:57.304882Z","shell.execute_reply.started":"2021-11-09T04:48:57.289888Z","shell.execute_reply":"2021-11-09T04:48:57.304031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = transformers.AutoTokenizer.from_pretrained(\"../input/xlmrob\")","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:48:57.30642Z","iopub.execute_input":"2021-11-09T04:48:57.306914Z","iopub.status.idle":"2021-11-09T04:49:03.784803Z","shell.execute_reply.started":"2021-11-09T04:48:57.306877Z","shell.execute_reply":"2021-11-09T04:49:03.783956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pad_on_right = tokenizer.padding_side == \"right\"\nmax_length = 400\ndoc_stride = 135\n\ntest_data = pd.read_csv(\"../input/chaii-hindi-and-tamil-question-answering/test.csv\")\ntest_dataset = Dataset.from_pandas(test_data)\ntest_features = test_dataset.map(\n    partial(\n        prepare_validation_features, \n        tokenizer=tokenizer,\n        pad_on_right=pad_on_right, \n        max_length=max_length,\n        doc_stride=doc_stride\n    ),\n    batched=True,\n    remove_columns=test_dataset.column_names\n)\ntest_feats_small = test_features.map(\n    lambda example: example, remove_columns=['example_id', 'offset_mapping']\n)\n\nfin_start_logits = None\nfin_end_logits = None\n\nfor fold_ in tqdm(range(10)):\n    model = ChaiiModel(model_name=\"../input/xlmrob\", num_train_steps=0, steps_per_epoch=0, learning_rate=0)\n    model.load(f\"../input/deepsetsquad2-v2/pytorch_model_f{fold_}.bin\", weights_only=True)\n    model.to(\"cuda\")\n    model.eval()\n    data_loader = torch.utils.data.DataLoader(\n        ChaiiDataset(test_feats_small), \n        batch_size=32,\n        num_workers=4,\n        pin_memory=True,\n        shuffle=False\n    )\n    start_logits = []\n    end_logits = []\n\n    for b_idx, data in enumerate(data_loader):\n        with torch.no_grad():\n            for key, value in data.items():\n                data[key] = value.to(\"cuda\")\n            output, _, _ = model(**data)\n            start = output[0].detach().cpu().numpy()\n            end = output[1].detach().cpu().numpy()\n            start_logits.append(start)\n            end_logits.append(end)\n\n    start_logits = np.vstack(start_logits)\n    end_logits = np.vstack(end_logits)\n    \n    if fin_start_logits is None:\n        fin_start_logits = start_logits\n        fin_end_logits = end_logits\n    else:\n        fin_start_logits += start_logits\n        fin_end_logits += end_logits\n        \n    del model\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:49:03.786358Z","iopub.execute_input":"2021-11-09T04:49:03.786625Z","iopub.status.idle":"2021-11-09T04:54:36.314805Z","shell.execute_reply.started":"2021-11-09T04:49:03.786591Z","shell.execute_reply":"2021-11-09T04:54:36.313964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nimport gc\ngc.enable()\nimport math\nimport json\nimport time\nimport random\nimport multiprocessing\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm, trange\nfrom sklearn import model_selection\nfrom string import punctuation\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nimport torch.optim as optim\nfrom torch.utils.data import (\n    Dataset, DataLoader,\n    SequentialSampler, RandomSampler\n)\nfrom torch.utils.data.distributed import DistributedSampler\n\ntry:\n    from apex import amp\n    APEX_INSTALLED = True\nexcept ImportError:\n    APEX_INSTALLED = False\n\nimport transformers\nfrom transformers import (\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    logging,\n    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n)\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\n# Now Create Function\n\ndef fix_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef optimal_num_of_loader_workers():\n    num_cpus = multiprocessing.cpu_count()\n    num_gpus = torch.cuda.device_count()\n    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n    return optimal_value\n\nprint(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:54:36.316611Z","iopub.execute_input":"2021-11-09T04:54:36.316899Z","iopub.status.idle":"2021-11-09T04:54:36.450398Z","shell.execute_reply.started":"2021-11-09T04:54:36.316849Z","shell.execute_reply":"2021-11-09T04:54:36.44959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now Create Custom CLass","metadata":{}},{"cell_type":"code","source":"class Configration:\n    # model\n    model_type = 'xlm_roberta'\n    model_name_or_path = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2\"\n    config_name = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2\"\n    fp16 = True if APEX_INSTALLED else False\n    fp16_opt_level = \"O1\"\n    gradient_accumulation_steps = 2\n\n    # tokenizer\n    tokenizer_name = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2\"\n    max_seq_length = 400\n    doc_stride = 135\n\n    # train\n    epochs = 1\n    train_batch_size = 4\n    eval_batch_size = 128\n\n    # optimzer\n    optimizer_type = 'AdamW'\n    learning_rate = 1e-5\n    weight_decay = 1e-2\n    epsilon = 1e-8\n    max_grad_norm = 1.0\n\n    # scheduler\n    decay_name = 'linear-warmup'\n    warmup_ratio = 0.1\n\n    # logging\n    logging_steps = 10\n\n    # evaluate\n    output_dir = 'output'\n    seed = 2021","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:54:36.453762Z","iopub.execute_input":"2021-11-09T04:54:36.454353Z","iopub.status.idle":"2021-11-09T04:54:36.460833Z","shell.execute_reply.started":"2021-11-09T04:54:36.454323Z","shell.execute_reply":"2021-11-09T04:54:36.460037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset_Retriever class\nclass Dataset_Retriever(Dataset):\n    def __init__(self, features, mode='train'):\n        super(Dataset_Retriever, self).__init__()\n        self.features = features\n        self.mode = mode\n        \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, item):   \n        feature = self.features[item]\n        if self.mode == 'train':\n            return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n            }\n        else:\n            return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':feature['offset_mapping'],\n                'sequence_ids':feature['sequence_ids'],\n                'id':feature['example_id'],\n                'context': feature['context'],\n                'question': feature['question']\n            }","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:54:36.462194Z","iopub.execute_input":"2021-11-09T04:54:36.462955Z","iopub.status.idle":"2021-11-09T04:54:36.474732Z","shell.execute_reply.started":"2021-11-09T04:54:36.462912Z","shell.execute_reply":"2021-11-09T04:54:36.473911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now Creating Model building class","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, modelname_or_path, config):\n        super(Model, self).__init__()\n        self.config = config\n        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self._init_weights(self.qa_outputs)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n\n    def forward(\n        self, \n        input_ids, \n        attention_mask=None, \n    ):\n        outputs = self.xlm_roberta(\n            input_ids,\n            attention_mask=attention_mask,\n        )\n\n        sequence_output = outputs[0]\n        pooled_output = outputs[1]\n        \n        # sequence_output = self.dropout(sequence_output)\n        qa_logits = self.qa_outputs(sequence_output)\n        \n        start_logits, end_logits = qa_logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n    \n        return start_logits, end_logits","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:54:36.476921Z","iopub.execute_input":"2021-11-09T04:54:36.477734Z","iopub.status.idle":"2021-11-09T04:54:36.488016Z","shell.execute_reply.started":"2021-11-09T04:54:36.477698Z","shell.execute_reply":"2021-11-09T04:54:36.487122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# This making model function helping AutoConfig and AutoTokenizer the data","metadata":{}},{"cell_type":"code","source":"def Make_Model(args):\n    config = AutoConfig.from_pretrained(args.config_name)\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n    model = Model(args.model_name_or_path, config=config)\n    return config, tokenizer, model","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:54:36.489501Z","iopub.execute_input":"2021-11-09T04:54:36.489905Z","iopub.status.idle":"2021-11-09T04:54:36.499093Z","shell.execute_reply.started":"2021-11-09T04:54:36.489872Z","shell.execute_reply":"2021-11-09T04:54:36.498432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Prepare_Test_Features(args, example, tokenizer):\n    example[\"question\"] = example[\"question\"].lstrip()\n    \n    tokenized_example = tokenizer(\n        example[\"question\"],\n        example[\"context\"],\n        truncation=\"only_second\",\n        max_length=args.max_seq_length,\n        stride=args.doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    features = []\n    for i in range(len(tokenized_example[\"input_ids\"])):\n        feature = {}\n        feature[\"example_id\"] = example['id']\n        feature['context'] = example['context']\n        feature['question'] = example['question']\n        feature['input_ids'] = tokenized_example['input_ids'][i]\n        feature['attention_mask'] = tokenized_example['attention_mask'][i]\n        feature['offset_mapping'] = tokenized_example['offset_mapping'][i]\n        feature['sequence_ids'] = [0 if i is None else i for i in tokenized_example.sequence_ids(i)]\n        features.append(feature)\n    return features","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:54:36.500582Z","iopub.execute_input":"2021-11-09T04:54:36.500838Z","iopub.status.idle":"2021-11-09T04:54:36.510012Z","shell.execute_reply.started":"2021-11-09T04:54:36.500808Z","shell.execute_reply":"2021-11-09T04:54:36.509108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now import collections lib and create function","metadata":{}},{"cell_type":"code","source":"import collections","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:54:36.511649Z","iopub.execute_input":"2021-11-09T04:54:36.511928Z","iopub.status.idle":"2021-11-09T04:54:36.520892Z","shell.execute_reply.started":"2021-11-09T04:54:36.511896Z","shell.execute_reply":"2021-11-09T04:54:36.520161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n    all_start_logits, all_end_logits = raw_predictions\n    \n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    predictions = collections.OrderedDict()\n\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    for example_index, example in examples.iterrows():\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None\n        valid_answers = []\n        \n        context = example[\"context\"]\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n\n            sequence_ids = features[feature_index][\"sequence_ids\"]\n            context_index = 1\n\n            features[feature_index][\"offset_mapping\"] = [\n                (o if sequence_ids[k] == context_index else None)\n                for k, o in enumerate(features[feature_index][\"offset_mapping\"])\n            ]\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n        \n        predictions[example[\"id\"]] = best_answer[\"text\"]\n        \n        \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:54:36.523712Z","iopub.execute_input":"2021-11-09T04:54:36.523902Z","iopub.status.idle":"2021-11-09T04:54:36.538883Z","shell.execute_reply.started":"2021-11-09T04:54:36.523875Z","shell.execute_reply":"2021-11-09T04:54:36.538182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now read test dataset","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:54:36.540045Z","iopub.execute_input":"2021-11-09T04:54:36.54077Z","iopub.status.idle":"2021-11-09T04:54:36.564021Z","shell.execute_reply.started":"2021-11-09T04:54:36.540736Z","shell.execute_reply":"2021-11-09T04:54:36.563322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['context'] = test_df['context'].apply(lambda x: ' '.join(x.split()))\ntest_df['question'] = test_df['question'].apply(lambda x: ' '.join(x.split()))","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:54:36.566515Z","iopub.execute_input":"2021-11-09T04:54:36.566716Z","iopub.status.idle":"2021-11-09T04:54:36.580823Z","shell.execute_reply.started":"2021-11-09T04:54:36.566695Z","shell.execute_reply":"2021-11-09T04:54:36.580057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntokenizer = AutoTokenizer.from_pretrained(Configration().tokenizer_name)\n\ntest_features = []\nfor i, row in test_df.iterrows():\n    \n    # Now Calling Function and Calling Class\n    \n    test_features += Prepare_Test_Features(Configration(), row, tokenizer)\n\n# Now Calling Classes\n\nargs = Configration()\ntest_dataset = Dataset_Retriever(test_features, mode='test')\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=args.eval_batch_size, \n    sampler=SequentialSampler(test_dataset),\n    num_workers=optimal_num_of_loader_workers(),\n    pin_memory=True, \n    drop_last=False\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:54:36.583043Z","iopub.execute_input":"2021-11-09T04:54:36.583686Z","iopub.status.idle":"2021-11-09T04:54:37.783279Z","shell.execute_reply.started":"2021-11-09T04:54:36.583652Z","shell.execute_reply":"2021-11-09T04:54:37.782513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = '../input/5foldsroberta/output/'","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:54:37.784579Z","iopub.execute_input":"2021-11-09T04:54:37.784819Z","iopub.status.idle":"2021-11-09T04:54:37.787973Z","shell.execute_reply.started":"2021-11-09T04:54:37.784788Z","shell.execute_reply":"2021-11-09T04:54:37.787308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now Create Get Prediction Function for helping the model ","metadata":{}},{"cell_type":"code","source":"def Get_Predictions(checkpoint_path):\n    \n    # Calling Function Make_Model and Class Configration\n    \n    config, tokenizer, model = Make_Model(Configration())\n    model.cuda();\n    model.load_state_dict(\n        torch.load(base_model + checkpoint_path)\n    );\n    \n    start_logits = []\n    end_logits = []\n    for batch in test_dataloader:\n        with torch.no_grad():\n            outputs_start, outputs_end = model(batch['input_ids'].cuda(), batch['attention_mask'].cuda())\n            start_logits.append(outputs_start.cpu().numpy().tolist())\n            end_logits.append(outputs_end.cpu().numpy().tolist())\n            del outputs_start, outputs_end\n    del model, tokenizer, config\n    gc.collect()\n    return np.vstack(start_logits), np.vstack(end_logits)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:54:37.789191Z","iopub.execute_input":"2021-11-09T04:54:37.789655Z","iopub.status.idle":"2021-11-09T04:54:37.79935Z","shell.execute_reply.started":"2021-11-09T04:54:37.789618Z","shell.execute_reply":"2021-11-09T04:54:37.798636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now Calling Get Predictions Function","metadata":{}},{"cell_type":"code","source":"start_logits1, end_logits1 = Get_Predictions('checkpoint-fold-0/pytorch_model.bin')\nstart_logits2, end_logits2 = Get_Predictions('checkpoint-fold-1/pytorch_model.bin')\nstart_logits3, end_logits3 = Get_Predictions('checkpoint-fold-2/pytorch_model.bin')\nstart_logits4, end_logits4 = Get_Predictions('checkpoint-fold-3/pytorch_model.bin')\nstart_logits5, end_logits5 = Get_Predictions('checkpoint-fold-4/pytorch_model.bin')\n","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:54:37.800776Z","iopub.execute_input":"2021-11-09T04:54:37.801052Z","iopub.status.idle":"2021-11-09T04:57:53.897831Z","shell.execute_reply.started":"2021-11-09T04:54:37.801019Z","shell.execute_reply":"2021-11-09T04:57:53.896911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fin_start_logits /= 10\nfin_end_logits /= 10","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:57:53.901451Z","iopub.execute_input":"2021-11-09T04:57:53.901742Z","iopub.status.idle":"2021-11-09T04:57:53.908037Z","shell.execute_reply.started":"2021-11-09T04:57:53.901709Z","shell.execute_reply":"2021-11-09T04:57:53.90737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_logits =0.4*fin_start_logits+ 0.6*((start_logits1 + start_logits2 + start_logits3 + start_logits4 + start_logits5) / 5)\nend_logits = 0.4*fin_end_logits+0.6*((end_logits1 + end_logits2 + end_logits3 + end_logits4 + end_logits5) / 5)\n\n\n# Now Calling Function \n\nfin_preds = Postprocess_qa_predictions(test_df, test_features, (start_logits, end_logits))\n\nsubmission = []\nfor p1, p2 in fin_preds.items():\n    p2 = \" \".join(p2.split())\n    p2 = p2.strip(punctuation)\n    submission.append((p1, p2))\n    \nsample = pd.DataFrame(submission, columns=[\"id\", \"PredictionString\"])\n\ntest_data =pd.merge(left=test_df,right=sample,on='id')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:57:53.909282Z","iopub.execute_input":"2021-11-09T04:57:53.909678Z","iopub.status.idle":"2021-11-09T04:57:53.984048Z","shell.execute_reply.started":"2021-11-09T04:57:53.909646Z","shell.execute_reply":"2021-11-09T04:57:53.983243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"–\",  \",\", \";\"]\nbad_endings = [\"...\", \"-\", \"(\", \")\", \"–\", \",\", \";\"]\n\ntamil_ad = \"கி.பி\"\ntamil_bc = \"கி.மு\"\ntamil_km = \"கி.மீ\"\nhindi_ad = \"ई\"\nhindi_bc = \"ई.पू\"\n\n\ncleaned_preds = []\nfor pred, context in test_data[[\"PredictionString\", \"context\"]].to_numpy():\n    if pred == \"\":\n        cleaned_preds.append(pred)\n        continue\n    while any([pred.startswith(y) for y in bad_starts]):\n        pred = pred[1:]\n    while any([pred.endswith(y) for y in bad_endings]):\n        if pred.endswith(\"...\"):\n            pred = pred[:-3]\n        else:\n            pred = pred[:-1]\n    if pred.endswith(\"...\"):\n            pred = pred[:-3]\n    \n    if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n        pred = pred+\".\"\n        \n    cleaned_preds.append(pred)\n\ntest_data[\"PredictionString\"] = cleaned_preds\ntest_data[['id', 'PredictionString']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T04:57:53.985879Z","iopub.execute_input":"2021-11-09T04:57:53.986334Z","iopub.status.idle":"2021-11-09T04:57:54.014147Z","shell.execute_reply.started":"2021-11-09T04:57:53.986299Z","shell.execute_reply":"2021-11-09T04:57:54.013157Z"},"trusted":true},"execution_count":null,"outputs":[]}]}