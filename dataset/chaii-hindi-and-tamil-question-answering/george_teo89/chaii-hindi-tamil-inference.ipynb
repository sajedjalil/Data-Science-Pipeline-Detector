{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install openpyxl","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:33:23.301124Z","iopub.execute_input":"2021-10-18T10:33:23.3017Z","iopub.status.idle":"2021-10-18T10:33:23.319099Z","shell.execute_reply.started":"2021-10-18T10:33:23.301604Z","shell.execute_reply":"2021-10-18T10:33:23.318443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#torch libraries\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\n#tensorflow libraries\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\n#huggingface libraries\nfrom transformers import (\n    AutoTokenizer,\n    AutoConfig,\n    AutoModel,\n    AdamW,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup\n)\n\n#sklearn libraries\nfrom sklearn.model_selection import StratifiedKFold\n\n#python libraries\nimport pandas as pd\nimport numpy as np\nimport os\nimport random\nimport gc\nfrom tqdm.notebook import tqdm\nimport collections\nfrom string import punctuation","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-18T10:38:14.76388Z","iopub.execute_input":"2021-10-18T10:38:14.764176Z","iopub.status.idle":"2021-10-18T10:38:14.770494Z","shell.execute_reply.started":"2021-10-18T10:38:14.764144Z","shell.execute_reply":"2021-10-18T10:38:14.76946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CONFIG:\n    #Debug\n    debug = False\n    debug_sample = 200 #use a smaller dataset for debug\n    perform_fold = [0]\n    \n    #misc\n    seed = 42\n    num_workers = 2\n    \n    #training params \n    train_batchsize = 4\n    val_batchsize = 8\n    epochs = 1\n    n_splits = 5\n    \n    #model params\n    model = '../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2'\n    max_input_length = 384 #Hyperparameter to be tuned, following the guide from huggingface\n    doc_stride = 128  #Hyperparameter to be tuned, following the guide from huggingface\n    \n    \n    #optimizer\n    optimizer = \"AdamW\" #implemented AdamW, Adam, SGD\n    \n    max_grad_norm = 1.0 #gradient clipping to prevent exploding gradient\n    \n    if (optimizer == \"AdamW\") or (optimizer == \"Adam\"): \n        optimizer_params = dict(\n            betas = (0.9,0.999), \n            lr = 0.001,\n            eps = 1e-8,\n            weight_decay= 0.01,\n            amsgrad = False\n        )\n        \n    elif optimizer == \"SGD\":\n        optimizer_params = dict( \n            lr = 0.001,\n            momentum = 0,\n            weight_decay =0,\n            dampening = 0,\n            nesterov = False\n        )\n    \n\n    \n    #scheduler\n    #implemented  pytorch CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateu, \n    scheduler = \"cosine_with_warmup\"\n    if scheduler == \"CosineAnnealing\":\n        scheduler_params = dict(\n            T_max = 3,\n            eta_min = 0,\n            last_epoch = -1,\n            verbose=True\n        )\n    elif scheduler == \"ReduceLROnPlateu\":\n        scheduler_params = dict(\n            mode = 'min',\n            factor = 0.1,\n            patience = 10,\n            threshold = 1e-4,\n            threshold_mode= 'rel',\n            cooldown = 0,\n            min_lr = 0,\n            eps = 1e-8,\n            verbose=True\n        )\n    elif scheduler == \"CosineAnnealingWarmRestarts\":\n        scheduler_params = dict(\n            T_0  = 3,\n            T_mult  = 1,\n            eta_min = 0,\n            last_epoch = -1,\n            verbose = True\n        )\n        \n    elif scheduler == \"linear_with_warmup\": #huggingface scheduler\n        scheduler_params = dict(\n            warmup_steps_ratio =  0.1\n        )\n        \n    elif scheduler == \"cosine_with_warmup\": #huggingface scheduler\n        scheduler_params = dict(\n            warmup_steps_ratio =  0.1\n        )\n        \n    \n    #SWA stochastic weight averaging\n    SWA = False\n     \n    #FP16\n    FP16 = False\n    \n    #accelerate \n    ACCELERATE = False","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:33:31.30555Z","iopub.execute_input":"2021-10-18T10:33:31.305791Z","iopub.status.idle":"2021-10-18T10:33:31.320694Z","shell.execute_reply.started":"2021-10-18T10:33:31.305755Z","shell.execute_reply":"2021-10-18T10:33:31.319342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setting seed","metadata":{}},{"cell_type":"code","source":"def set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \n\ndef set_torch_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n#     torch.backends.cudnn.benchmark = False \n#     torch.backends.cudnn.deterministic =True\n\ndef set_tf_seed(seed):\n    tf.random.set_seed(seed)  \n    \nset_random_seed(CONFIG.seed)\nset_torch_seed(CONFIG.seed)\n# set_tf_seed(CONFIG.seed)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:33:31.322111Z","iopub.execute_input":"2021-10-18T10:33:31.322365Z","iopub.status.idle":"2021-10-18T10:33:31.341987Z","shell.execute_reply.started":"2021-10-18T10:33:31.32232Z","shell.execute_reply":"2021-10-18T10:33:31.341037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the testset","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/chaii-hindi-and-tamil-question-answering/test.csv\")\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:33:31.344767Z","iopub.execute_input":"2021-10-18T10:33:31.345237Z","iopub.status.idle":"2021-10-18T10:33:31.379561Z","shell.execute_reply.started":"2021-10-18T10:33:31.345196Z","shell.execute_reply":"2021-10-18T10:33:31.378595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Process the features","metadata":{}},{"cell_type":"code","source":"def preprocess_question(df):\n    df['question'] = df['question'].str.strip()\n    return df\n\ntest_df = preprocess_question(test_df)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:33:31.381089Z","iopub.execute_input":"2021-10-18T10:33:31.381362Z","iopub.status.idle":"2021-10-18T10:33:31.388704Z","shell.execute_reply.started":"2021-10-18T10:33:31.381328Z","shell.execute_reply":"2021-10-18T10:33:31.387809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def break_long_context(df, tokenizer, train=True):\n    if train: \n        n_examples = len(df)\n        full_set = []\n        for i in tqdm(range(n_examples)):\n            row = df.iloc[i]\n            # tokenizer parameters can be found here \n            # https://huggingface.co/transformers/internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase\n            tokenized_examples = tokenizer(row['question'],\n                                          row['context'],\n                                          padding='max_length',\n                                          max_length=CONFIG.max_input_length, \n                                          truncation='only_second',\n                                          stride=CONFIG.doc_stride,\n                                          return_overflowing_tokens=True, #returns the number of over flow\n                                          return_offsets_mapping=True     #returns the BPE mapping to the original word\n                                          ) \n            \n            # tokenized_example keys\n            #'input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'\n            sample_mappings = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n            offset_mappings = tokenized_examples.pop(\"offset_mapping\")\n            \n            final_examples = [] \n            n_sub_examples = len(sample_mappings)\n            for j in range(n_sub_examples):\n                input_ids = tokenized_examples[\"input_ids\"][j]\n                attention_mask = tokenized_examples[\"attention_mask\"][j]\n                \n                sliced_text = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids))\n                final_example = dict(input_ids = input_ids, \n                                     attention_mask = attention_mask,\n                                     sliced_text = sliced_text,\n                                     offset_mapping=offset_mappings[j],\n                                     fold=row['fold'])\n                \n                \n                \n                # Most of the time cls_index is 0\n                cls_index = input_ids.index(tokenizer.cls_token_id)\n                # None, 0, 0, .... None, None, 1, 1,.....\n                sequence_ids = tokenized_examples.sequence_ids(j)\n                \n                sample_index = sample_mappings[j]\n                offset_map = offset_mappings[j]\n                \n                if np.isnan(row[\"answer_start\"]) : # if no answer, start and end position is cls_index\n                    final_example['start_position'] = cls_index\n                    final_example['end_position'] = cls_index\n                    final_example['tokenized_answer'] = \"\"\n                    final_example['answer_text'] = \"\"\n                else:\n                    start_char  = row[\"answer_start\"]\n                    end_char  = start_char + len(row[\"answer_text\"])\n                    \n                    token_start_index = sequence_ids.index(1)\n                    token_end_index = len(sequence_ids)- 1 - (sequence_ids[::-1].index(1))\n                    \n                    if not (offset_map[token_start_index][0]<=start_char and offset_map[token_end_index][1] >= end_char):\n                        final_example['start_position'] = cls_index\n                        final_example['end_position'] = cls_index\n                        final_example['tokenized_answer'] = \"\"\n                        final_example['answer_text'] = \"\"\n                    else:\n                        #Move token_start_index to the correct context index\n                        while token_start_index < len(offset_map) and offset_map[token_start_index][0] <= start_char:\n                            token_start_index +=1\n                        final_example['start_position'] = token_start_index -1\n                        \n                        while offset_map[token_end_index][1] >= end_char: #Take note that we will want the end_index inclusively, we will need to slice properly later\n                            token_end_index -=1\n                        final_example['end_position'] = token_end_index + 1   \n                        tokenized_answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[final_example['start_position']:final_example['end_position']+1]))\n                        final_example['tokenized_answer'] = tokenized_answer\n                        final_example['answer_text'] = row['answer_text']\n                        \n                final_examples.append(final_example)\n            full_set += final_examples\n            \n    else:\n        n_examples = len(df)\n        full_set = []\n        for i in tqdm(range(n_examples)):\n            row = df.iloc[i]\n            tokenized_examples = tokenizer(row['question'], \n                                          row['context'],\n                                          padding='max_length',\n                                          max_length=CONFIG.max_input_length, \n                                          truncation='only_second',\n                                          stride=CONFIG.doc_stride,\n                                          return_overflowing_tokens=True, #returns the number of over flow\n                                          return_offsets_mapping=True     #returns the BPE mapping to the original word\n                                          )\n            \n            sample_mappings = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n            offset_mappings = tokenized_examples.pop(\"offset_mapping\")\n            n_sub_examples = len(sample_mappings)\n            \n            final_examples = []\n            for j in range(n_sub_examples):\n                input_ids = tokenized_examples[\"input_ids\"][j]\n                attention_mask = tokenized_examples[\"attention_mask\"][j]\n                \n                final_example = dict(\n                    input_ids = input_ids, \n                    attention_mask = attention_mask,\n                    offset_mapping=offset_mappings[j],\n                    example_id = row['id'],\n                    context = row['context'],\n                    question = row['question'],\n                    sequence_ids = [0 if value is None else value for value in tokenized_examples.sequence_ids(j)]  \n                )\n                \n                final_examples.append(final_example)\n            full_set += final_examples\n        return full_set","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:33:31.390525Z","iopub.execute_input":"2021-10-18T10:33:31.391112Z","iopub.status.idle":"2021-10-18T10:33:31.41629Z","shell.execute_reply.started":"2021-10-18T10:33:31.391071Z","shell.execute_reply":"2021-10-18T10:33:31.415513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CONFIG.model)\ntest_set = break_long_context(test_df, tokenizer, train=False)\n# full_test_df = pd.DataFrame.from_dict(test_set)\n# full_test_df.to_excel(\"full_test_df.xlsx\")\n\n# print(f\"Total test examples = {len(full_test_df)}\")","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:33:31.41809Z","iopub.execute_input":"2021-10-18T10:33:31.418364Z","iopub.status.idle":"2021-10-18T10:33:32.619646Z","shell.execute_reply.started":"2021-10-18T10:33:31.418328Z","shell.execute_reply":"2021-10-18T10:33:32.618798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the dataset","metadata":{}},{"cell_type":"code","source":"class ChaiDataset(Dataset):\n    def __init__(self, dataset, is_train=True):\n        super(ChaiDataset, self).__init__()\n        self.dataset = dataset #list of features\n        self.is_train= is_train\n        \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, index):\n        features = self.dataset[index]\n        if self.is_train:\n            return {\n                'input_ids': torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask': torch.tensor(features['attention_mask'], dtype=torch.long),\n                'offset_mapping':torch.tensor(features['offset_mapping'], dtype=torch.long),\n                'start_position':torch.tensor(features['start_position'], dtype=torch.long),\n                'end_position':torch.tensor(features['end_position'], dtype=torch.long)\n            }\n        else:\n            return {\n                'input_ids': torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask': torch.tensor(features['attention_mask'], dtype=torch.long),\n                'offset_mapping':torch.tensor(features['offset_mapping'], dtype=torch.long),\n                'sequence_ids':features['sequence_ids'],\n                'id':features['example_id'],\n                'context':features['context'],\n                'question':features['question']\n            }\n        ","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:33:32.621168Z","iopub.execute_input":"2021-10-18T10:33:32.621465Z","iopub.status.idle":"2021-10-18T10:33:32.632727Z","shell.execute_reply.started":"2021-10-18T10:33:32.621429Z","shell.execute_reply":"2021-10-18T10:33:32.632043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the model","metadata":{}},{"cell_type":"code","source":"class ChaiModel(nn.Module):\n    def __init__(self, model_config):\n        super(ChaiModel, self).__init__()\n        self.backbone = AutoModel.from_pretrained(CONFIG.model)\n        self.linear = nn.Linear(model_config.hidden_size, 2)\n        \n    def forward(self, input_ids, attention_mask):\n        model_output = self.backbone(input_ids, attention_mask=attention_mask)\n        sequence_output = model_output[0] # (batchsize, sequencelength, hidden_dim)\n        \n        qa_logits = self.linear(sequence_output) # (batchsize, sequencelength, 2)\n        start_logit, end_logit = qa_logits.split(1, dim=-1) #  (batchsize, sequencelength), 1), (batchsize, sequencelength, 1)\n        start_logits = start_logit.squeeze(-1) # remove last dim (batchsize, sequencelength)\n        end_logits = end_logit.squeeze(-1)    #remove last dim (batchsize, sequencelength)\n        \n        return start_logits, end_logits # (2,batchsize, sequencelength)\n        ","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:33:32.634139Z","iopub.execute_input":"2021-10-18T10:33:32.634674Z","iopub.status.idle":"2021-10-18T10:33:32.645819Z","shell.execute_reply.started":"2021-10-18T10:33:32.634637Z","shell.execute_reply":"2021-10-18T10:33:32.645063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model(model_checkpoint):\n    config = AutoConfig.from_pretrained(CONFIG.model)\n    reloaded_model = ChaiModel(config)\n    reloaded_model.load_state_dict(torch.load(model_checkpoint))\n    reloaded_model.eval()\n    return reloaded_model","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:33:32.649076Z","iopub.execute_input":"2021-10-18T10:33:32.649378Z","iopub.status.idle":"2021-10-18T10:33:32.656656Z","shell.execute_reply.started":"2021-10-18T10:33:32.649302Z","shell.execute_reply":"2021-10-18T10:33:32.655992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_loader(test_features):\n    test_dataset = ChaiDataset(test_features, is_train=False)\n    test_dataloader = DataLoader(test_dataset, \n                                CONFIG.val_batchsize, \n                                shuffle= False, \n                                num_workers= CONFIG.num_workers,\n                                drop_last=False,\n                                pin_memory=True)\n    \n    return test_dataloader","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:33:32.657872Z","iopub.execute_input":"2021-10-18T10:33:32.65847Z","iopub.status.idle":"2021-10-18T10:33:32.666243Z","shell.execute_reply.started":"2021-10-18T10:33:32.658442Z","shell.execute_reply":"2021-10-18T10:33:32.665556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_prediction(test_dataloader, model_checkpoint, device):\n    model = get_model(model_checkpoint)\n    model.eval()\n    model.to(device)\n    \n    start_logits =[]\n    end_logits=[]\n    for features in tqdm(test_dataloader, total=len(test_dataloader)):\n        input_ids = features['input_ids'].to(device)\n        attention_mask = features['attention_mask'].to(device)\n        with torch.no_grad():\n            start_logit, end_logit = model(input_ids, attention_mask) #(batch, 384,1) , (batch, 384,1)\n            start_logits.append(start_logit.to(\"cpu\").numpy())\n            end_logits.append(end_logit.to(\"cpu\").numpy())\n        \n    del model\n    gc.collect()\n    return np.vstack(start_logits),  np.vstack(end_logits)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:33:32.669442Z","iopub.execute_input":"2021-10-18T10:33:32.669638Z","iopub.status.idle":"2021-10-18T10:33:32.678176Z","shell.execute_reply.started":"2021-10-18T10:33:32.669612Z","shell.execute_reply":"2021-10-18T10:33:32.677303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set device","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:33:32.679438Z","iopub.execute_input":"2021-10-18T10:33:32.679903Z","iopub.status.idle":"2021-10-18T10:33:32.729999Z","shell.execute_reply.started":"2021-10-18T10:33:32.679869Z","shell.execute_reply":"2021-10-18T10:33:32.728834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare dataloader","metadata":{}},{"cell_type":"code","source":"test_dataloader = get_test_loader(test_set)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:33:32.733654Z","iopub.execute_input":"2021-10-18T10:33:32.733952Z","iopub.status.idle":"2021-10-18T10:33:32.741034Z","shell.execute_reply.started":"2021-10-18T10:33:32.733922Z","shell.execute_reply":"2021-10-18T10:33:32.740144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predictions","metadata":{}},{"cell_type":"code","source":"start_logits_0, end_logits_0 = get_prediction(test_dataloader,\n                                          \"../input/chai-finetuned-model-provided-data/pytorch_model_fold_0.pth\", \n                                          device)\nstart_logits_1, end_logits_1 = get_prediction(test_dataloader,\n                                          \"../input/chai-finetuned-model-provided-data/pytorch_model_fold_1.pth\", \n                                          device)\nstart_logits_2, end_logits_2 = get_prediction(test_dataloader,\n                                          \"../input/chai-finetuned-model-provided-data/pytorch_model_fold_2_epoch1.pth\", \n                                          device)\nstart_logits_3, end_logits_3 = get_prediction(test_dataloader,\n                                          \"../input/chai-finetuned-model-provided-data/pytorch_model_fold_3_epoch0.pth\", \n                                          device)\nstart_logits_4, end_logits_4 = get_prediction(test_dataloader,\n                                          \"../input/chai-finetuned-model-provided-data/pytorch_model_fold_4_epoch0.pth\", \n                                          device)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:33:32.742855Z","iopub.execute_input":"2021-10-18T10:33:32.74331Z","iopub.status.idle":"2021-10-18T10:36:46.895277Z","shell.execute_reply.started":"2021-10-18T10:33:32.74327Z","shell.execute_reply":"2021-10-18T10:36:46.894524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_logits = (start_logits_0 + start_logits_1 + start_logits_2 +start_logits_3+ start_logits_4 )/5\nend_logits = (end_logits_0 + end_logits_1 + end_logits_2 +end_logits_3 + end_logits_4)/5","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:37:40.555371Z","iopub.execute_input":"2021-10-18T10:37:40.556011Z","iopub.status.idle":"2021-10-18T10:37:40.562177Z","shell.execute_reply.started":"2021-10-18T10:37:40.555973Z","shell.execute_reply":"2021-10-18T10:37:40.561257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Post processing","metadata":{}},{"cell_type":"code","source":"def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n    all_start_logits, all_end_logits = raw_predictions\n    \n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    predictions = collections.OrderedDict()\n\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    for example_index, example in examples.iterrows():\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None\n        valid_answers = []\n        \n        context = example[\"context\"]\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n\n            sequence_ids = features[feature_index][\"sequence_ids\"]\n            context_index = 1\n\n            features[feature_index][\"offset_mapping\"] = [\n                (o if sequence_ids[k] == context_index else None)\n                for k, o in enumerate(features[feature_index][\"offset_mapping\"])\n            ]\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n        \n        predictions[example[\"id\"]] = best_answer[\"text\"]\n        \n        \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:37:42.501117Z","iopub.execute_input":"2021-10-18T10:37:42.501469Z","iopub.status.idle":"2021-10-18T10:37:42.517575Z","shell.execute_reply.started":"2021-10-18T10:37:42.501433Z","shell.execute_reply":"2021-10-18T10:37:42.516839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = postprocess_qa_predictions(test_df, test_set, (start_logits, end_logits))\n\nsubmission = []\nfor p1, p2 in predictions.items():\n    p2 = \" \".join(p2.split())\n    p2 = p2.strip(punctuation)\n    submission.append((p1, p2))\n    \nsample = pd.DataFrame(submission, columns=[\"id\", \"PredictionString\"])\n\ntest_df =pd.merge(left=test_df,right=sample,on='id')\ntest_df","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:38:20.176796Z","iopub.execute_input":"2021-10-18T10:38:20.177054Z","iopub.status.idle":"2021-10-18T10:38:20.259185Z","shell.execute_reply.started":"2021-10-18T10:38:20.177025Z","shell.execute_reply":"2021-10-18T10:38:20.258367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"–\",  \",\", \";\"]\nbad_endings = [\"...\", \"-\", \"(\", \")\", \"–\", \",\", \";\"]\n\ntamil_ad = \"கி.பி\"\ntamil_bc = \"கி.மு\"\ntamil_km = \"கி.மீ\"\nhindi_ad = \"ई\"\nhindi_bc = \"ई.पू\"\n\n\ncleaned_preds = []\nfor pred, context in test_df[[\"PredictionString\", \"context\"]].to_numpy():\n    if pred == \"\":\n        cleaned_preds.append(pred)\n        continue\n    while any([pred.startswith(y) for y in bad_starts]):\n        pred = pred[1:]\n    while any([pred.endswith(y) for y in bad_endings]):\n        if pred.endswith(\"...\"):\n            pred = pred[:-3]\n        else:\n            pred = pred[:-1]\n    if pred.endswith(\"...\"):\n            pred = pred[:-3]\n    \n    if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n        pred = pred+\".\"\n        \n    cleaned_preds.append(pred)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:38:26.193797Z","iopub.execute_input":"2021-10-18T10:38:26.194098Z","iopub.status.idle":"2021-10-18T10:38:26.210284Z","shell.execute_reply.started":"2021-10-18T10:38:26.194065Z","shell.execute_reply":"2021-10-18T10:38:26.209344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"PredictionString\"] = cleaned_preds\ntest_df","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:38:29.130729Z","iopub.execute_input":"2021-10-18T10:38:29.131006Z","iopub.status.idle":"2021-10-18T10:38:29.142454Z","shell.execute_reply.started":"2021-10-18T10:38:29.130977Z","shell.execute_reply":"2021-10-18T10:38:29.141763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = test_df[['id','PredictionString']]\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:38:36.655907Z","iopub.execute_input":"2021-10-18T10:38:36.656518Z","iopub.status.idle":"2021-10-18T10:38:36.681286Z","shell.execute_reply.started":"2021-10-18T10:38:36.656475Z","shell.execute_reply":"2021-10-18T10:38:36.680564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}