{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Optimising weights for the weighted average ensemble:\n\n\n### Important Notes:\n<ul>\n    <li> Using Optuna to optimise the weights</li>\n    <li> Only using the chaii data for optimising. I think using mlqa and xquad might give better results.</li>\n    <li> Using the public shared model only for reference. </li>\n    <li> Just for experimentation. Trying a new approach. Dont know if it will improve the scores. (Don't depend on the public LB üò∂)\n</ul>\n\n\n\n \n\n### My previous Notebooks:\n\n<ul>  \n    <li>Using the datasets : chaii, mlqa, squad, tamil_xquad </li>\n    <li>Training for 2 epochs : <a href=\"https://www.kaggle.com/kishalmandal/chaii-fit-2-epochs-mlqa-xquad-chaii/\">chaii | FIT - 2 epochs | mlqa, xquad, chaii</a> </li>\n    <li>Training for 7 epochs with tamil_xquad: <a href=\"https://www.kaggle.com/kishalmandal/chaii-fit-7-epochs-extra-tamil-data/\">chaii | FIT - 7 epochs | Extra Tamil Data</a> </li>\n    <li>Inferencing from 5 folds | fold-0 and fold-1 (7-epochs) | fold-2, fold-3 and fold-4 (2-epochs)| based on cross validation scores and a little bit of experimentation üòú: <a href=\"https://www.kaggle.com/kishalmandal/5-epochs-infer-combined-model-0-792/\">5 epochs | INFER | combined model (0.792)</a></li>\n    \n    \n</ul>\n\n","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nimport gc\ngc.enable()\nimport math\nimport json\nimport time\nimport random\nimport multiprocessing\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm, trange\nfrom sklearn import model_selection\nfrom string import punctuation\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nimport torch.optim as optim\nfrom torch.utils.data import (\n    Dataset, DataLoader,\n    SequentialSampler, RandomSampler\n)\nfrom torch.utils.data.distributed import DistributedSampler\n\ntry:\n    from apex import amp\n    APEX_INSTALLED = True\nexcept ImportError:\n    APEX_INSTALLED = False\n\nimport transformers\nfrom transformers import (\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    logging,\n    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n)\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\ndef fix_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef optimal_num_of_loader_workers():\n    num_cpus = multiprocessing.cpu_count()\n    num_gpus = torch.cuda.device_count()\n    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n    return optimal_value\n\nprint(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)","metadata":{"papermill":{"duration":10.552281,"end_time":"2021-10-03T22:37:59.394277","exception":false,"start_time":"2021-10-03T22:37:48.841996","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-11T22:47:10.3437Z","iopub.execute_input":"2021-10-11T22:47:10.344021Z","iopub.status.idle":"2021-10-11T22:47:10.377405Z","shell.execute_reply.started":"2021-10-11T22:47:10.343985Z","shell.execute_reply":"2021-10-11T22:47:10.374939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    # model\n    model_type = 'xlm_roberta'\n    model_name_or_path = \"../input/xlm-roberta-large-squad-v2\"\n    config_name = \"../input/xlm-roberta-large-squad-v2\"\n    fp16 = True if APEX_INSTALLED else False\n    fp16_opt_level = \"O1\"\n    gradient_accumulation_steps = 2\n\n    # tokenizer\n    tokenizer_name = \"../input/xlm-roberta-large-squad-v2\"\n    max_seq_length = 400\n    doc_stride = 135\n\n    # train\n    epochs = 1\n    train_batch_size = 4\n    eval_batch_size = 128\n\n    # optimzer\n    optimizer_type = 'AdamW'\n    learning_rate = 1e-9\n    weight_decay = 1e-3\n    epsilon = 1e-8\n    max_grad_norm = 1.0\n\n    # scheduler\n    decay_name = 'linear-warmup'\n    warmup_ratio = 0.1\n\n    # logging\n    logging_steps = 10\n\n    # evaluate\n    output_dir = 'output'\n    seed = 2021","metadata":{"papermill":{"duration":0.023163,"end_time":"2021-10-03T22:37:59.431991","exception":false,"start_time":"2021-10-03T22:37:59.408828","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-11T22:47:10.379259Z","iopub.execute_input":"2021-10-11T22:47:10.380289Z","iopub.status.idle":"2021-10-11T22:47:10.39384Z","shell.execute_reply.started":"2021-10-11T22:47:10.380217Z","shell.execute_reply":"2021-10-11T22:47:10.392557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DatasetRetriever(Dataset):\n    def __init__(self, features, mode='train'):\n        super(DatasetRetriever, self).__init__()\n        self.features = features\n        self.mode = mode\n        \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, item):   \n        feature = self.features[item]\n        if self.mode == 'train':\n            return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n            }\n        else:\n            return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':feature['offset_mapping'],\n                'sequence_ids':feature['sequence_ids'],\n                'id':feature['example_id'],\n                'context': feature['context'],\n                'question': feature['question']\n            }","metadata":{"papermill":{"duration":0.025081,"end_time":"2021-10-03T22:37:59.471046","exception":false,"start_time":"2021-10-03T22:37:59.445965","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-11T22:47:10.403402Z","iopub.execute_input":"2021-10-11T22:47:10.403678Z","iopub.status.idle":"2021-10-11T22:47:10.418924Z","shell.execute_reply.started":"2021-10-11T22:47:10.403645Z","shell.execute_reply":"2021-10-11T22:47:10.418047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, modelname_or_path, config):\n        super(Model, self).__init__()\n        self.config = config\n        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self._init_weights(self.qa_outputs)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n\n    def forward(\n        self, \n        input_ids, \n        attention_mask=None, \n    ):\n        outputs = self.xlm_roberta(\n            input_ids,\n            attention_mask=attention_mask,\n        )\n\n        sequence_output = outputs[0]\n        pooled_output = outputs[1]\n        \n        # sequence_output = self.dropout(sequence_output)\n        qa_logits = self.qa_outputs(sequence_output)\n        \n        start_logits, end_logits = qa_logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n    \n        return start_logits, end_logits","metadata":{"papermill":{"duration":0.024392,"end_time":"2021-10-03T22:37:59.509033","exception":false,"start_time":"2021-10-03T22:37:59.484641","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-11T22:47:10.420971Z","iopub.execute_input":"2021-10-11T22:47:10.421815Z","iopub.status.idle":"2021-10-11T22:47:10.43723Z","shell.execute_reply.started":"2021-10-11T22:47:10.421777Z","shell.execute_reply":"2021-10-11T22:47:10.436443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_model(args):\n    config = AutoConfig.from_pretrained(args.config_name)\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n    model = Model(args.model_name_or_path, config=config)\n    return config, tokenizer, model","metadata":{"papermill":{"duration":0.020297,"end_time":"2021-10-03T22:37:59.544049","exception":false,"start_time":"2021-10-03T22:37:59.523752","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-11T22:47:10.438837Z","iopub.execute_input":"2021-10-11T22:47:10.439139Z","iopub.status.idle":"2021-10-11T22:47:10.447635Z","shell.execute_reply.started":"2021-10-11T22:47:10.439106Z","shell.execute_reply":"2021-10-11T22:47:10.446741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_test_features(args, example, tokenizer):\n    example[\"question\"] = example[\"question\"].lstrip()\n    \n    tokenized_example = tokenizer(\n        example[\"question\"],\n        example[\"context\"],\n        truncation=\"only_second\",\n        max_length=args.max_seq_length,\n        stride=args.doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    features = []\n    for i in range(len(tokenized_example[\"input_ids\"])):\n        feature = {}\n        feature[\"example_id\"] = example['id']\n        feature['context'] = example['context']\n        feature['question'] = example['question']\n        feature['input_ids'] = tokenized_example['input_ids'][i]\n        feature['attention_mask'] = tokenized_example['attention_mask'][i]\n        feature['offset_mapping'] = tokenized_example['offset_mapping'][i]\n        feature['sequence_ids'] = [0 if i is None else i for i in tokenized_example.sequence_ids(i)]\n        features.append(feature)\n    return features","metadata":{"papermill":{"duration":0.02351,"end_time":"2021-10-03T22:37:59.581276","exception":false,"start_time":"2021-10-03T22:37:59.557766","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-11T22:47:10.449287Z","iopub.execute_input":"2021-10-11T22:47:10.449876Z","iopub.status.idle":"2021-10-11T22:47:10.464821Z","shell.execute_reply.started":"2021-10-11T22:47:10.449843Z","shell.execute_reply":"2021-10-11T22:47:10.463571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections\n\ndef postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n    all_start_logits, all_end_logits = raw_predictions\n    \n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    predictions = collections.OrderedDict()\n\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    for example_index, example in examples.iterrows():\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None\n        valid_answers = []\n        \n        context = example[\"context\"]\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n\n            sequence_ids = features[feature_index][\"sequence_ids\"]\n            context_index = 1\n\n            features[feature_index][\"offset_mapping\"] = [\n                (o if sequence_ids[k] == context_index else None)\n                for k, o in enumerate(features[feature_index][\"offset_mapping\"])\n            ]\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n        \n        predictions[example[\"id\"]] = best_answer[\"text\"]\n        \n        \n    return predictions","metadata":{"papermill":{"duration":0.033817,"end_time":"2021-10-03T22:37:59.629635","exception":false,"start_time":"2021-10-03T22:37:59.595818","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-11T22:47:10.602085Z","iopub.execute_input":"2021-10-11T22:47:10.60245Z","iopub.status.idle":"2021-10-11T22:47:10.633606Z","shell.execute_reply.started":"2021-10-11T22:47:10.602412Z","shell.execute_reply":"2021-10-11T22:47:10.63281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\n\n# test['context'] = test['context'].apply(lambda x: ' '.join(x.split()))\n# test['question'] = test['question'].apply(lambda x: ' '.join(x.split()))\n\n# test=test[:10]\n# test=test[:10]\ntokenizer = AutoTokenizer.from_pretrained(Config().tokenizer_name)\n\ntest_features = []\nfor i, row in test.iterrows():\n    test_features += prepare_test_features(Config(), row, tokenizer)\n\nargs = Config()\ntest_dataset = DatasetRetriever(test_features, mode='test')\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=args.eval_batch_size, \n    sampler=SequentialSampler(test_dataset),\n    num_workers=optimal_num_of_loader_workers(),\n    pin_memory=True, \n    drop_last=False\n)","metadata":{"papermill":{"duration":42.605426,"end_time":"2021-10-03T22:38:42.249767","exception":false,"start_time":"2021-10-03T22:37:59.644341","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-11T22:47:10.639025Z","iopub.execute_input":"2021-10-11T22:47:10.641063Z","iopub.status.idle":"2021-10-11T22:47:12.784018Z","shell.execute_reply.started":"2021-10-11T22:47:10.641014Z","shell.execute_reply":"2021-10-11T22:47:12.78322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = '../input/tamil-data/extra tamil/output'","metadata":{"papermill":{"duration":0.021033,"end_time":"2021-10-03T22:38:42.286094","exception":false,"start_time":"2021-10-03T22:38:42.265061","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-11T22:47:12.785421Z","iopub.execute_input":"2021-10-11T22:47:12.785696Z","iopub.status.idle":"2021-10-11T22:47:12.788967Z","shell.execute_reply.started":"2021-10-11T22:47:12.785665Z","shell.execute_reply":"2021-10-11T22:47:12.788267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_predictions(checkpoint_path):\n    config, tokenizer, model = make_model(Config())\n    model.cuda();\n    model.load_state_dict(\n        torch.load( checkpoint_path)\n    );\n    \n    start_logits = []\n    end_logits = []\n    for batch in test_dataloader:\n        with torch.no_grad():\n            outputs_start, outputs_end = model(batch['input_ids'].cuda(), batch['attention_mask'].cuda())\n            start_logits.append(outputs_start.cpu().numpy().tolist())\n            end_logits.append(outputs_end.cpu().numpy().tolist())\n            del outputs_start, outputs_end\n    del model, tokenizer, config\n    gc.collect()\n    return np.vstack(start_logits), np.vstack(end_logits)","metadata":{"papermill":{"duration":0.023212,"end_time":"2021-10-03T22:38:42.323307","exception":false,"start_time":"2021-10-03T22:38:42.300095","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-11T22:47:12.791816Z","iopub.execute_input":"2021-10-11T22:47:12.792393Z","iopub.status.idle":"2021-10-11T22:47:12.801856Z","shell.execute_reply.started":"2021-10-11T22:47:12.792354Z","shell.execute_reply":"2021-10-11T22:47:12.801079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install optuna","metadata":{"papermill":{"duration":8.699221,"end_time":"2021-10-03T22:38:51.03583","exception":false,"start_time":"2021-10-03T22:38:42.336609","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-11T22:47:12.803414Z","iopub.execute_input":"2021-10-11T22:47:12.803737Z","iopub.status.idle":"2021-10-11T22:47:12.813455Z","shell.execute_reply.started":"2021-10-11T22:47:12.803702Z","shell.execute_reply":"2021-10-11T22:47:12.812692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Objective Function","metadata":{}},{"cell_type":"code","source":"start_logits1, end_logits1 = get_predictions('../input/no-idea/output/checkpoint-fold-0/pytorch_model.bin')\nstart_logits2, end_logits2 = get_predictions('../input/no-idea/output/checkpoint-fold-1/pytorch_model.bin')\nstart_logits3, end_logits3 = get_predictions('../input/no-idea/output/checkpoint-fold-2/pytorch_model.bin')\nstart_logits4, end_logits4 = get_predictions('../input/no-idea/output/checkpoint-fold-3/pytorch_model.bin')\nstart_logits5, end_logits5 = get_predictions('../input/no-idea/output/checkpoint-fold-4/pytorch_model.bin')\n\n\n\n\ndef objective(trial):\n    \n    samples = [0.2]*5\n\n    samples[0]=trial.suggest_uniform('w0', 0, 1)\n    samples[1]=trial.suggest_uniform('w1', 0, 1)\n    samples[2]=trial.suggest_uniform('w2', 0, 1)\n    samples[3]=trial.suggest_uniform('w3', 0, 1)\n    samples[4]=trial.suggest_uniform('w4', 0, 1)\n    \n    w = [samples[i]/sum(samples) for i in range(len(samples))]\n    \n    start_logits = (w[0]*start_logits1 + w[1]*start_logits2 + w[2]*start_logits3 + w[3]*start_logits4+ w[4]*start_logits5 )\n    end_logits = (w[0]*end_logits1 + w[1]*end_logits2 + w[2]*end_logits3 + w[3]*end_logits4 + w[4]*end_logits5 )\n\n\n\n    fin_preds = postprocess_qa_predictions(test, test_features, (start_logits, end_logits))\n\n    submission = []\n    for p1, p2 in fin_preds.items():\n        p2 = \" \".join(p2.split())\n        p2 = p2.strip(punctuation)\n        submission.append((p1, p2))\n\n    sample = pd.DataFrame(submission, columns=[\"id\", \"PredictionString\"])\n\n    test_data =pd.merge(left=test,right=sample,on='id')\n\n\n    bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"‚Äì\",  \",\", \";\"]\n    bad_endings = [\"...\", \"-\", \"(\", \")\", \"‚Äì\", \",\", \";\"]\n\n    tamil_ad = \"‡Æï‡Æø.‡Æ™‡Æø\"\n    tamil_bc = \"‡Æï‡Æø.‡ÆÆ‡ØÅ\"\n    tamil_km = \"‡Æï‡Æø.‡ÆÆ‡ØÄ\"\n    hindi_ad = \"‡§à\"\n    hindi_bc = \"‡§à.‡§™‡•Ç\"\n\n\n    cleaned_preds = []\n    for pred, context in test_data[[\"PredictionString\", \"context\"]].to_numpy():\n        if pred == \"\":\n            cleaned_preds.append(pred)\n            continue\n        while any([pred.startswith(y) for y in bad_starts]):\n            pred = pred[1:]\n        while any([pred.endswith(y) for y in bad_endings]):\n            if pred.endswith(\"...\"):\n                pred = pred[:-3]\n            else:\n                pred = pred[:-1]\n        if pred.endswith(\"...\"):\n                pred = pred[:-3]\n\n        if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n            pred = pred+\".\"\n\n        cleaned_preds.append(pred)\n\n    jac=[]    \n    def jaccard(str1, str2): \n        a = set(str1.lower().split()) \n        b = set(str2.lower().split())\n        c = a.intersection(b)\n        return float(len(c)) / (len(a) + len(b) - len(c))\n\n\n    for i in range(len(test)):\n        jac.append(jaccard(cleaned_preds[i], test.answer_text.values[i]))\n\n\n    return np.mean(jac)","metadata":{"papermill":{"duration":3126.388811,"end_time":"2021-10-03T23:30:57.441865","exception":false,"start_time":"2021-10-03T22:38:51.053054","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-11T22:47:12.81648Z","iopub.execute_input":"2021-10-11T22:47:12.816683Z","iopub.status.idle":"2021-10-11T22:50:35.252869Z","shell.execute_reply.started":"2021-10-11T22:47:12.816661Z","shell.execute_reply":"2021-10-11T22:50:35.252069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)","metadata":{"papermill":{"duration":593.358833,"end_time":"2021-10-03T23:40:50.817861","exception":false,"start_time":"2021-10-03T23:30:57.459028","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-11T22:50:35.256551Z","iopub.execute_input":"2021-10-11T22:50:35.257046Z","iopub.status.idle":"2021-10-11T22:50:42.959002Z","shell.execute_reply.started":"2021-10-11T22:50:35.257001Z","shell.execute_reply":"2021-10-11T22:50:42.958144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d=study.best_params \nprint(d)","metadata":{"papermill":{"duration":0.078811,"end_time":"2021-10-03T23:40:50.966757","exception":false,"start_time":"2021-10-03T23:40:50.887946","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-11T22:50:42.960299Z","iopub.execute_input":"2021-10-11T22:50:42.96086Z","iopub.status.idle":"2021-10-11T22:50:42.965882Z","shell.execute_reply.started":"2021-10-11T22:50:42.960816Z","shell.execute_reply":"2021-10-11T22:50:42.96513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Printing the Weights","metadata":{}},{"cell_type":"code","source":"del start_logits1,start_logits2,start_logits3,start_logits4,start_logits5\ndel end_logits1,end_logits2,end_logits3,end_logits4,end_logits5\ndel test","metadata":{"execution":{"iopub.status.busy":"2021-10-11T22:50:42.966996Z","iopub.execute_input":"2021-10-11T22:50:42.967628Z","iopub.status.idle":"2021-10-11T22:50:42.97719Z","shell.execute_reply.started":"2021-10-11T22:50:42.967586Z","shell.execute_reply":"2021-10-11T22:50:42.976535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\ndef sizeof_fmt(num, suffix='B'):\n    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f %s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n\nfor name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n                         key= lambda x: -x[1])[:10]:\n    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T22:50:42.981318Z","iopub.execute_input":"2021-10-11T22:50:42.981838Z","iopub.status.idle":"2021-10-11T22:50:42.994897Z","shell.execute_reply.started":"2021-10-11T22:50:42.981809Z","shell.execute_reply":"2021-10-11T22:50:42.994178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s=[] \nfor k, v in d.items():\n    s.append(v)\n    \nw=[a/sum(s) for a in s]\nprint(w)\n","metadata":{"papermill":{"duration":0.076505,"end_time":"2021-10-03T23:40:51.111294","exception":false,"start_time":"2021-10-03T23:40:51.034789","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-11T22:50:42.99593Z","iopub.execute_input":"2021-10-11T22:50:42.996207Z","iopub.status.idle":"2021-10-11T22:50:43.008072Z","shell.execute_reply.started":"2021-10-11T22:50:42.996171Z","shell.execute_reply":"2021-10-11T22:50:43.007238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\n\ntokenizer = AutoTokenizer.from_pretrained(Config().tokenizer_name)\n\ntest_features = []\nfor i, row in test.iterrows():\n    test_features += prepare_test_features(Config(), row, tokenizer)\n\nargs = Config()\ntest_dataset = DatasetRetriever(test_features, mode='test')\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=args.eval_batch_size, \n    sampler=SequentialSampler(test_dataset),\n    num_workers=optimal_num_of_loader_workers(),\n    pin_memory=True, \n    drop_last=False\n)","metadata":{"papermill":{"duration":0.073824,"end_time":"2021-10-03T23:40:51.253633","exception":false,"start_time":"2021-10-03T23:40:51.179809","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-11T22:50:43.009517Z","iopub.execute_input":"2021-10-11T22:50:43.009793Z","iopub.status.idle":"2021-10-11T22:50:44.372351Z","shell.execute_reply.started":"2021-10-11T22:50:43.009753Z","shell.execute_reply":"2021-10-11T22:50:44.371427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_logits1, end_logits1 = get_predictions('../input/no-idea/output/checkpoint-fold-0/pytorch_model.bin')\nstart_logits2, end_logits2 = get_predictions('../input/no-idea/output/checkpoint-fold-1/pytorch_model.bin')\nstart_logits3, end_logits3 = get_predictions('../input/no-idea/output/checkpoint-fold-2/pytorch_model.bin')\nstart_logits4, end_logits4 = get_predictions('../input/no-idea/output/checkpoint-fold-3/pytorch_model.bin')\nstart_logits5, end_logits5 = get_predictions('../input/no-idea/output/checkpoint-fold-4/pytorch_model.bin')","metadata":{"papermill":{"duration":0.069318,"end_time":"2021-10-03T23:40:51.390747","exception":false,"start_time":"2021-10-03T23:40:51.321429","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-11T22:50:44.376304Z","iopub.execute_input":"2021-10-11T22:50:44.376595Z","iopub.status.idle":"2021-10-11T22:53:53.091232Z","shell.execute_reply.started":"2021-10-11T22:50:44.376561Z","shell.execute_reply":"2021-10-11T22:53:53.090429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_logits = (w[0]*start_logits1 + w[1]*start_logits2 + w[2]*start_logits3 + w[3]*start_logits4+ w[4]*start_logits5 )\nend_logits = (w[0]*end_logits1 + w[1]*end_logits2 + w[2]*end_logits3 + w[3]*end_logits4 + w[4]*end_logits5 )\n\n\n\nfin_preds = postprocess_qa_predictions(test, test_features, (start_logits, end_logits))\n\nsubmission = []\nfor p1, p2 in fin_preds.items():\n    p2 = \" \".join(p2.split())\n    p2 = p2.strip(punctuation)\n    submission.append((p1, p2))\n\nsample = pd.DataFrame(submission, columns=[\"id\", \"PredictionString\"])\n\ntest_data =pd.merge(left=test,right=sample,on='id')\n\n\nbad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"‚Äì\",  \",\", \";\"]\nbad_endings = [\"...\", \"-\", \"(\", \")\", \"‚Äì\", \",\", \";\"]\n\ntamil_ad = \"‡Æï‡Æø.‡Æ™‡Æø\"\ntamil_bc = \"‡Æï‡Æø.‡ÆÆ‡ØÅ\"\ntamil_km = \"‡Æï‡Æø.‡ÆÆ‡ØÄ\"\nhindi_ad = \"‡§à\"\nhindi_bc = \"‡§à.‡§™‡•Ç\"\n\n\ncleaned_preds = []\nfor pred, context in test_data[[\"PredictionString\", \"context\"]].to_numpy():\n    if pred == \"\":\n        cleaned_preds.append(pred)\n        continue\n    while any([pred.startswith(y) for y in bad_starts]):\n        pred = pred[1:]\n    while any([pred.endswith(y) for y in bad_endings]):\n        if pred.endswith(\"...\"):\n            pred = pred[:-3]\n        else:\n            pred = pred[:-1]\n    if pred.endswith(\"...\"):\n            pred = pred[:-3]\n\n    if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n        pred = pred+\".\"\n\n    cleaned_preds.append(pred)\n\n    \ntest_data[\"PredictionString\"] = cleaned_preds\ntest_data[['id', 'PredictionString']].to_csv('submission.csv', index=False)","metadata":{"papermill":{"duration":0.07164,"end_time":"2021-10-03T23:40:51.5329","exception":false,"start_time":"2021-10-03T23:40:51.46126","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-11T22:53:53.093956Z","iopub.execute_input":"2021-10-11T22:53:53.094237Z","iopub.status.idle":"2021-10-11T22:53:53.181958Z","shell.execute_reply.started":"2021-10-11T22:53:53.0942Z","shell.execute_reply":"2021-10-11T22:53:53.181127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data[['id', 'PredictionString']]","metadata":{"execution":{"iopub.status.busy":"2021-10-11T22:59:28.393913Z","iopub.execute_input":"2021-10-11T22:59:28.394206Z","iopub.status.idle":"2021-10-11T22:59:28.413016Z","shell.execute_reply.started":"2021-10-11T22:59:28.394175Z","shell.execute_reply":"2021-10-11T22:59:28.411968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}