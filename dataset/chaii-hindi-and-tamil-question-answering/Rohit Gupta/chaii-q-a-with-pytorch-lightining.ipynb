{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\n**THIS is adjusted fork of https://www.kaggle.com/hoshi7/chaii-pytorch-lightining-w-b**\n\nThe model building aspect was taken from: https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit\n\nOver the course of the month, I have learnt a lot about transformers and pytorch. During one such lesson, I stumbled across Pytorch-lightning and how it can create a general framework for the pytorch deep learning model that we are building. \nKeeping that in mind, I set across learning about how to structure regular pytorch code into lightning code. This is one such attempt at that, with WanDB to showcase the ML-OPS part of the training. ","metadata":{}},{"cell_type":"markdown","source":"![image.jpg](data:image/png;base64,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)","metadata":{}},{"cell_type":"markdown","source":"Also, over the course of a lot of learning, I realised that most of the tutorials that can be found easily on the web generally use a prepackaged model such as BertSequenceClassification, this made it difficult to figure out where the _forward_ step should go, or where the linear layers should have gone. Overcoming that obstacle was the main challenge of this notebook, along with finding a way to fit the model on Kaggle's less optimal memory provision. \n\nThus, this notebook highlights upon: \n1. Use of custom model (XLM Roberta) with pytorch lightning. \n2. Weights and Biases to showcase the ML-OPS\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## Installing and Importing Libraries\n\nInstalling pytorch lightning and torch to a newer version as torch 1.7.0 has an error while training with lightning. ","metadata":{}},{"cell_type":"code","source":"!pip install -qU '../input/libraries/pytorch_lightning-1.5.0rc1-py3-none-any.whl'","metadata":{"execution":{"iopub.status.busy":"2021-10-25T21:19:48.183624Z","iopub.execute_input":"2021-10-25T21:19:48.183945Z","iopub.status.idle":"2021-10-25T21:20:18.491759Z","shell.execute_reply.started":"2021-10-25T21:19:48.183861Z","shell.execute_reply":"2021-10-25T21:20:18.490944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'","metadata":{"execution":{"iopub.status.busy":"2021-10-25T21:20:18.49483Z","iopub.execute_input":"2021-10-25T21:20:18.49547Z","iopub.status.idle":"2021-10-25T21:20:18.501947Z","shell.execute_reply.started":"2021-10-25T21:20:18.495441Z","shell.execute_reply":"2021-10-25T21:20:18.501224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper libraries\nimport os\nimport gc\ngc.enable()\nimport math\nimport json\nimport time\nimport random\nimport collections\nimport multiprocessing\nfrom pathlib import Path\nimport warnings\nfrom argparse import Namespace\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom sklearn import model_selection\nfrom collections import OrderedDict\n\n#Pytorch, transformers\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport transformers\nfrom transformers import (\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    logging,\n    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n)\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\n\n#Import pytorch lightning: \nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning import Callback\nfrom pytorch_lightning.loggers import CSVLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.callbacks import LearningRateMonitor\n\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T21:20:18.503259Z","iopub.execute_input":"2021-10-25T21:20:18.503471Z","iopub.status.idle":"2021-10-25T21:20:26.190103Z","shell.execute_reply.started":"2021-10-25T21:20:18.503443Z","shell.execute_reply":"2021-10-25T21:20:26.189288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\ntest_df = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\nexternal_mlqa = pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')\nexternal_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\nexternal_train = pd.concat([external_mlqa, external_xquad], ignore_index=True)\n\ndel external_mlqa, external_xquad\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T21:20:26.191682Z","iopub.execute_input":"2021-10-25T21:20:26.191922Z","iopub.status.idle":"2021-10-25T21:20:27.825012Z","shell.execute_reply.started":"2021-10-25T21:20:26.191889Z","shell.execute_reply":"2021-10-25T21:20:27.824267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Necessary Functions\n\nThe index of necessary functions in order:\n- **optimal_num_of_loader_workers**: Find the optimal number of workers based on config. Code from: https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit","metadata":{}},{"cell_type":"code","source":"def optimal_num_of_loader_workers():\n    num_cpus = multiprocessing.cpu_count()\n    num_gpus = torch.cuda.device_count()\n    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n    return optimal_value","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-25T21:20:27.827568Z","iopub.execute_input":"2021-10-25T21:20:27.828002Z","iopub.status.idle":"2021-10-25T21:20:27.833359Z","shell.execute_reply.started":"2021-10-25T21:20:27.827961Z","shell.execute_reply":"2021-10-25T21:20:27.83244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining Configuration","metadata":{}},{"cell_type":"code","source":"config = Namespace(\n    seed = 7,\n    \n    trainer = Namespace(\n        precision = 16,\n        accumulate_grad_batches = 2,\n        max_epochs = 5,\n        weights_summary='top',\n        num_sanity_val_steps = 0,\n        gpus = 1,\n    ),\n    \n    model = Namespace(\n        model_name_or_path = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/\",\n        config_name = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/\",\n        optimizer_type = 'AdamW',\n        learning_rate = 3e-5,\n        weight_decay = 1e-2,\n        epsilon = 1e-8,\n        max_grad_norm = 1.0,\n        decay_name = 'linear-warmup',\n        warmup_ratio = 0.1,\n    ),\n    \n    data = Namespace(\n        train_batch_size = 4,\n        eval_batch_size = 8,\n        tokenizer_name = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2/\",\n        max_seq_length = 384,\n        doc_stride = 128,\n        valid_split = 0.25,\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T21:20:27.834854Z","iopub.execute_input":"2021-10-25T21:20:27.835233Z","iopub.status.idle":"2021-10-25T21:20:27.848186Z","shell.execute_reply.started":"2021-10-25T21:20:27.835182Z","shell.execute_reply":"2021-10-25T21:20:27.847361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset class","metadata":{}},{"cell_type":"code","source":"class DatasetRetriever(Dataset):\n    def __init__(self, features, mode='train'):\n        super(DatasetRetriever, self).__init__()\n        self.features = features\n        self.mode = mode\n    \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, item):   \n        feature = self.features[item]\n        res = {\n            'input_ids': torch.tensor(feature['input_ids'], dtype=torch.long),\n            'attention_mask': torch.tensor(feature['attention_mask'], dtype=torch.long),\n        }\n        if self.mode == 'train':\n            res.update({\n                'start_position': torch.tensor(feature['start_position'], dtype=torch.long),\n                'end_position': torch.tensor(feature['end_position'], dtype=torch.long)\n            })\n        else:\n            res.update({\n                'id': feature['example_id'],\n                'context': feature['context'],\n                'question': feature['question']\n            })\n            \n        return res","metadata":{"execution":{"iopub.status.busy":"2021-10-25T21:20:27.849332Z","iopub.execute_input":"2021-10-25T21:20:27.849686Z","iopub.status.idle":"2021-10-25T21:20:27.860889Z","shell.execute_reply.started":"2021-10-25T21:20:27.849637Z","shell.execute_reply":"2021-10-25T21:20:27.859927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining the `LightningDataModule` with Pytorch Lightning\n","metadata":{}},{"cell_type":"code","source":"class DataModuleFit(pl.LightningDataModule):\n    def __init__(self, df=None, **kwargs):\n        super().__init__()\n        self.save_hyperparameters(ignore=['df'])\n        self.df = df\n        \n    def _prepare_features(self, example):\n        example[\"question\"] = example[\"question\"].lstrip()\n        tokenized_example = self._tokenizer(\n            example[\"question\"],\n            example[\"context\"],\n            truncation=\"only_second\",\n            max_length=self.hparams.max_seq_length,\n            stride=self.hparams.doc_stride,\n            return_overflowing_tokens=True,\n            return_offsets_mapping=True,\n            padding=\"max_length\",\n        )\n\n        sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n        offset_mapping = tokenized_example.pop(\"offset_mapping\")\n\n        features = []\n        for i, offsets in enumerate(offset_mapping):\n            feature = {}\n\n            input_ids = tokenized_example[\"input_ids\"][i]\n            attention_mask = tokenized_example[\"attention_mask\"][i]\n\n            feature['input_ids'] = input_ids\n            feature['attention_mask'] = attention_mask\n            feature['offset_mapping'] = offsets\n\n            cls_index = input_ids.index(self._tokenizer.cls_token_id)\n            sequence_ids = tokenized_example.sequence_ids(i)\n\n            sample_index = sample_mapping[i]\n            answers = example[\"answers\"]\n\n            if len(answers[\"answer_start\"]) == 0:\n                feature[\"start_position\"] = cls_index\n                feature[\"end_position\"] = cls_index\n            else:\n                start_char = answers[\"answer_start\"][0]\n                end_char = start_char + len(answers[\"text\"][0])\n\n                token_start_index = 0\n                while sequence_ids[token_start_index] != 1:\n                    token_start_index += 1\n\n                token_end_index = len(input_ids) - 1\n                while sequence_ids[token_end_index] != 1:\n                    token_end_index -= 1\n\n                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                    feature[\"start_position\"] = cls_index\n                    feature[\"end_position\"] = cls_index\n                else:\n                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                        token_start_index += 1\n                    feature[\"start_position\"] = token_start_index - 1\n                    while offsets[token_end_index][1] >= end_char:\n                        token_end_index -= 1\n                    feature[\"end_position\"] = token_end_index + 1\n\n            features.append(feature)\n        return features\n        \n    def prepare_data(self):\n        self._tokenizer = AutoTokenizer.from_pretrained(self.hparams.tokenizer_name)\n        \n        self.df = self.df.sample(frac=1.)\n        train_split = int((1 - self.hparams.valid_split) * df.shape[0])\n        train_set = df.iloc[:train_split]\n        valid_set = df.iloc[train_split:]\n\n        train_features, valid_features = [[] for _ in range(2)]\n        for _, row in train_set.iterrows():\n            train_features += self._prepare_features(row)\n        for _, row in valid_set.iterrows():\n            valid_features += self._prepare_features(row)\n\n        self._train_features = train_features\n        self._valid_features = valid_features\n        \n    def setup(self, stage = None):\n        self._train_dset = DatasetRetriever(self._train_features)\n        self._valid_dset = DatasetRetriever(self._valid_features)\n    \n    def train_dataloader(self):\n        return DataLoader(\n            self._train_dset,\n            batch_size=self.hparams.train_batch_size,\n            num_workers=optimal_num_of_loader_workers(),\n            pin_memory=True,\n            drop_last=False,\n            shuffle=True\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self._valid_dset,\n            batch_size=self.hparams.eval_batch_size,\n            num_workers=optimal_num_of_loader_workers(),\n            pin_memory=True,\n            drop_last=False,\n            shuffle=False,\n    )\n    \nclass DataModulePredict(DataModuleFit):\n    def __init__(self, df, *args, **kwargs):\n        super().__init__()\n        self.save_hyperparameters(ignore=['df'])\n        self.df = df\n        \n    def _prepare_features(self, example):\n        example[\"question\"] = example[\"question\"].lstrip()\n\n        tokenized_example = self._tokenizer(\n            example[\"question\"],\n            example[\"context\"],\n            truncation=\"only_second\",\n            max_length=self.hparams.max_seq_length,\n            stride=self.hparams.doc_stride,\n            return_overflowing_tokens=True,\n            return_offsets_mapping=True,\n            padding=\"max_length\",\n        )\n\n        features = []\n        for i in range(len(tokenized_example[\"input_ids\"])):\n            feature = {}\n            feature[\"example_id\"] = example['id']\n            feature['context'] = example['context']\n            feature['question'] = example['question']\n            feature['input_ids'] = tokenized_example['input_ids'][i]\n            feature['attention_mask'] = tokenized_example['attention_mask'][i]\n            feature['offset_mapping'] = tokenized_example['offset_mapping'][i]\n            feature['sequence_ids'] = [0 if i is None else i for i in tokenized_example.sequence_ids(i)]\n            cls_index = feature['input_ids'].index(self._tokenizer.cls_token_id)\n            feature['cls_index'] = cls_index\n            features.append(feature)\n\n        return features\n        \n    def prepare_data(self):\n        self._tokenizer = AutoTokenizer.from_pretrained(self.hparams.tokenizer_name)\n        \n        pred_features = []\n\n        for _, row in self.df.iterrows():\n            pred_features += self._prepare_features(row)\n\n        self.pred_features = pred_features\n        \n    def setup(self, stage = None):\n        self._pred_dset = DatasetRetriever(self.pred_features, mode='predict')\n    \n    def predict_dataloader(self):\n        return DataLoader(\n            self._pred_dset,\n            batch_size=self.hparams.eval_batch_size,\n            num_workers=optimal_num_of_loader_workers(),\n            pin_memory=True,\n            drop_last=False,\n            shuffle=False\n        )    ","metadata":{"execution":{"iopub.status.busy":"2021-10-25T21:20:27.863053Z","iopub.execute_input":"2021-10-25T21:20:27.863779Z","iopub.status.idle":"2021-10-25T21:20:27.895471Z","shell.execute_reply.started":"2021-10-25T21:20:27.863723Z","shell.execute_reply":"2021-10-25T21:20:27.894622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining the `LightningModule` with Pytorch Lightning","metadata":{}},{"cell_type":"code","source":"class Model(pl.LightningModule):\n\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.save_hyperparameters()\n        self.model_config = AutoConfig.from_pretrained(self.hparams.config_name)\n        self.model = AutoModel.from_pretrained(self.hparams.model_name_or_path, config=self.model_config)\n        self.qa_outputs = nn.Linear(self.model_config.hidden_size, 2)\n        self.dropout = nn.Dropout(self.model_config.hidden_dropout_prob)\n        self._init_weights(self.qa_outputs)\n\n    def forward(self, input_ids, attention_mask):\n        \"\"\"The forward step performs the next step for the model while training.\"\"\"\n        sequence_output = self.model(input_ids, attention_mask=attention_mask)[0]\n        qa_logits = self.qa_outputs(sequence_output)\n\n        start_logits, end_logits = qa_logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits\n    \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n\n    def configure_optimizers(self):\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n                \"weight_decay_rate\": self.hparams.weight_decay\n            },\n            {\n                \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n                \"weight_decay_rate\": 0.0\n            },\n        ]\n        optimizer = AdamW(\n            optimizer_grouped_parameters,\n            lr=self.hparams.learning_rate,\n            eps = self.hparams.epsilon,\n            correct_bias=True\n        )\n\n        # Defining LR Scheduler\n        num_training_steps = len(self.trainer.datamodule.train_dataloader()) * self.trainer.max_epochs\n        num_warmup_steps = num_training_steps * self.hparams.warmup_ratio\n\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n        )\n\n        return {\n            'optimizer': optimizer,\n            'lr_scheduler': {\n                'scheduler': scheduler,\n                'monitor': 'val_loss',\n            }\n        }\n    \n    def _compute_loss(self, preds, labels):\n        start_preds, end_preds = preds\n        start_labels, end_labels = labels\n        start_loss = F.cross_entropy(start_preds, start_labels, ignore_index=-1)\n        end_loss = F.cross_entropy(end_preds, end_labels, ignore_index=-1)\n        total_loss = (start_loss + end_loss) / 2\n        return total_loss\n    \n    def training_step(self, batch, batch_idx):\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        targets_start = batch[\"start_position\"]\n        targets_end = batch['end_position']\n        \n        outputs_start, outputs_end = self(input_ids, attention_mask=attention_mask)\n        loss = self._compute_loss((outputs_start, outputs_end), (targets_start, targets_end))\n        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        targets_start = batch[\"start_position\"]\n        targets_end = batch['end_position']\n        \n        outputs_start, outputs_end = self(input_ids, attention_mask=attention_mask)\n        loss = self._compute_loss((outputs_start, outputs_end), (targets_start, targets_end))\n        self.log('val_loss', loss, prog_bar=True)\n\n    def predict_step(self, batch, batch_idx):\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        pred_start, pred_end = self(input_ids, attention_mask=attention_mask)\n        return {\n            'pred_start': pred_start,\n            'pred_end': pred_end,\n        }","metadata":{"execution":{"iopub.status.busy":"2021-10-25T21:20:27.896908Z","iopub.execute_input":"2021-10-25T21:20:27.897295Z","iopub.status.idle":"2021-10-25T21:20:27.921603Z","shell.execute_reply.started":"2021-10-25T21:20:27.897257Z","shell.execute_reply":"2021-10-25T21:20:27.920581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model with Pytorch lightning","metadata":{}},{"cell_type":"code","source":"pl.seed_everything(config.seed)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T21:20:27.925456Z","iopub.execute_input":"2021-10-25T21:20:27.925993Z","iopub.status.idle":"2021-10-25T21:20:27.938958Z","shell.execute_reply.started":"2021-10-25T21:20:27.925956Z","shell.execute_reply":"2021-10-25T21:20:27.938069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([train_df, external_train], ignore_index=True)\ndf['answers'] = df[['answer_start', 'answer_text']].apply(lambda x: {'answer_start': [x[0]], 'text': [x[1]]}, axis=1)\n\ntest_df['context'] = test_df['context'].apply(lambda x: ' '.join(x.split()))\ntest_df['question'] = test_df['question'].apply(lambda x: ' '.join(x.split()))\n\ndel train_df, external_train\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T21:20:27.940703Z","iopub.execute_input":"2021-10-25T21:20:27.941035Z","iopub.status.idle":"2021-10-25T21:20:28.253888Z","shell.execute_reply.started":"2021-10-25T21:20:27.940992Z","shell.execute_reply":"2021-10-25T21:20:28.253242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initiate callbacks\nlr_monitor = LearningRateMonitor(logging_interval='step')\nlogger = CSVLogger(save_dir='logs/')\n# Checkpoint\nckpt = ModelCheckpoint(\n    monitor=f'val_loss',\n    save_top_k=1,\n    save_last=False,\n    save_weights_only=True,\n    dirpath='checkpoints',\n    filename='{epoch:02d}-{val_loss:.4f}',\n    verbose=False,\n    mode='min',\n)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T21:20:28.256192Z","iopub.execute_input":"2021-10-25T21:20:28.256642Z","iopub.status.idle":"2021-10-25T21:20:28.279478Z","shell.execute_reply.started":"2021-10-25T21:20:28.256606Z","shell.execute_reply":"2021-10-25T21:20:28.278829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(**vars(config.model))\ndm = DataModuleFit(df, **vars(config.data))\n\ntrainer = pl.Trainer(\n    logger=logger,\n    callbacks=[ckpt, lr_monitor],\n    **vars(config.trainer)\n)\ntrainer.fit(model, datamodule=dm)\n\ntorch.cuda.empty_cache()\ndel trainer, model, dm\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T21:20:28.280792Z","iopub.execute_input":"2021-10-25T21:20:28.281042Z","iopub.status.idle":"2021-10-25T22:19:02.43323Z","shell.execute_reply.started":"2021-10-25T21:20:28.281009Z","shell.execute_reply":"2021-10-25T22:19:02.432328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict with Pytorch lightning","metadata":{}},{"cell_type":"code","source":"dm = DataModulePredict(test_df, **vars(config.data))\n\nsub_pred_start = None\nsub_pred_end = None\npred_features = None\n\ntrainer = pl.Trainer(\n    enable_checkpointing=False,\n    **vars(config.trainer),\n)\nmodel = Model.load_from_checkpoint(ckpt.best_model_path, **vars(config.model))\npreds = trainer.predict(model, datamodule=dm)\n\nsub_pred_start = np.vstack([x['pred_start'] for x in preds])\nsub_pred_end = np.vstack([x['pred_end'] for x in preds])    \ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T22:19:02.443276Z","iopub.execute_input":"2021-10-25T22:19:02.446708Z","iopub.status.idle":"2021-10-25T22:19:22.033634Z","shell.execute_reply.started":"2021-10-25T22:19:02.446662Z","shell.execute_reply":"2021-10-25T22:19:22.03248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Post process the results\nCredits: https://www.kaggle.com/kishalmandal/5-folds-infer-combined-model-0-792/","metadata":{}},{"cell_type":"code","source":"def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n    all_start_logits, all_end_logits = raw_predictions\n    \n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    predictions = []\n\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    for example_index, example in examples.iterrows():\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None\n        valid_answers = []\n        \n        context = example[\"context\"]\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n\n            sequence_ids = features[feature_index][\"sequence_ids\"]\n            context_index = 1\n\n            features[feature_index][\"offset_mapping\"] = [\n                (o if sequence_ids[k] == context_index else None)\n                for k, o in enumerate(features[feature_index][\"offset_mapping\"])\n            ]\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n            cls_index = features[feature_index][\"cls_index\"]\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n        \n        predictions.append((example[\"id\"], best_answer[\"text\"]))\n        \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-10-25T22:19:22.035264Z","iopub.execute_input":"2021-10-25T22:19:22.035538Z","iopub.status.idle":"2021-10-25T22:19:22.052834Z","shell.execute_reply.started":"2021-10-25T22:19:22.035503Z","shell.execute_reply":"2021-10-25T22:19:22.052053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_preds = postprocess_qa_predictions(test_df, dm.pred_features, (sub_pred_start, sub_pred_end))\nsub_df = pd.DataFrame(processed_preds, columns=['id', 'PredictionString'])\nsub_df = sub_df.merge(test_df, how='left', on='id')","metadata":{"execution":{"iopub.status.busy":"2021-10-25T22:19:22.054083Z","iopub.execute_input":"2021-10-25T22:19:22.054934Z","iopub.status.idle":"2021-10-25T22:19:22.103441Z","shell.execute_reply.started":"2021-10-25T22:19:22.054893Z","shell.execute_reply":"2021-10-25T22:19:22.102726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"–\",  \",\", \";\"]\nbad_endings = [\"...\", \"-\", \"(\", \")\", \"–\", \",\", \";\"]\n\ntamil_ad = \"கி.பி\"\ntamil_bc = \"கி.மு\"\ntamil_km = \"கி.மீ\"\nhindi_ad = \"ई\"\nhindi_bc = \"ई.पू\"\n\n\ncleaned_preds = []\nfor _, (pred, context) in sub_df[[\"PredictionString\", \"context\"]].iterrows():\n    if pred == \"\":\n        cleaned_preds.append(pred)\n        continue\n    while any([pred.startswith(y) for y in bad_starts]):\n        pred = pred[1:]\n    while any([pred.endswith(y) for y in bad_endings]):\n        if pred.endswith(\"...\"):\n            pred = pred[:-3]\n        else:\n            pred = pred[:-1]\n    if pred.endswith(\"...\"):\n            pred = pred[:-3]\n    \n    if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n        pred = pred+\".\"\n        \n    cleaned_preds.append(pred)\n\nsub_df[\"PredictionString\"] = cleaned_preds\nsub_df = sub_df[['id', 'PredictionString']]","metadata":{"execution":{"iopub.status.busy":"2021-10-25T22:19:22.104645Z","iopub.execute_input":"2021-10-25T22:19:22.105053Z","iopub.status.idle":"2021-10-25T22:19:22.117043Z","shell.execute_reply.started":"2021-10-25T22:19:22.105016Z","shell.execute_reply":"2021-10-25T22:19:22.115922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.to_csv('submission.csv', header=True, index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T22:19:22.119505Z","iopub.execute_input":"2021-10-25T22:19:22.120453Z","iopub.status.idle":"2021-10-25T22:19:22.132339Z","shell.execute_reply.started":"2021-10-25T22:19:22.120415Z","shell.execute_reply":"2021-10-25T22:19:22.131515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T22:19:22.134917Z","iopub.execute_input":"2021-10-25T22:19:22.13538Z","iopub.status.idle":"2021-10-25T22:19:22.148026Z","shell.execute_reply.started":"2021-10-25T22:19:22.135344Z","shell.execute_reply":"2021-10-25T22:19:22.147184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}