{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Please excuse how messy it is üò±","metadata":{}},{"cell_type":"code","source":"!pip install -U --no-build-isolation --no-deps ../input/transformers-master/ -qq","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:34:18.631462Z","iopub.execute_input":"2021-11-15T19:34:18.631883Z","iopub.status.idle":"2021-11-15T19:34:55.0567Z","shell.execute_reply.started":"2021-11-15T19:34:18.631776Z","shell.execute_reply":"2021-11-15T19:34:55.055658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/tez-lib/\")\nimport collections\nimport numpy as np\nimport transformers\nimport pandas as pd\nfrom datasets import Dataset\nfrom functools import partial\nfrom tqdm import tqdm\nimport json\nimport torch\n\nfrom sklearn import metrics\nimport transformers\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport tez\nfrom string import punctuation","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:34:55.060316Z","iopub.execute_input":"2021-11-15T19:34:55.060874Z","iopub.status.idle":"2021-11-15T19:35:01.502664Z","shell.execute_reply.started":"2021-11-15T19:34:55.06084Z","shell.execute_reply":"2021-11-15T19:35:01.501927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ChaiiModel(tez.Model):\n    def __init__(self, model_name, num_train_steps, steps_per_epoch, learning_rate):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.steps_per_epoch = steps_per_epoch\n        self.model_name = model_name\n        self.num_train_steps = num_train_steps\n        self.step_scheduler_after = \"batch\"\n\n        hidden_dropout_prob: float = 0.0\n\n        config = transformers.AutoConfig.from_pretrained(model_name)\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.transformer = transformers.AutoModelForQuestionAnswering.from_pretrained(model_name, config=config)\n\n    def forward(self, ids, mask, token_type_ids=None, start_positions=None, end_positions=None):\n        transformer_out = self.transformer(ids, mask)\n        start_logits = transformer_out.start_logits\n        end_logits = transformer_out.end_logits\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n        return (start_logits, end_logits), 0, {}","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:35:01.506061Z","iopub.execute_input":"2021-11-15T19:35:01.506266Z","iopub.status.idle":"2021-11-15T19:35:01.513338Z","shell.execute_reply.started":"2021-11-15T19:35:01.50624Z","shell.execute_reply":"2021-11-15T19:35:01.51268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_validation_features(examples, tokenizer, pad_on_right, max_length, doc_stride):\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    tokenized_examples[\"example_id\"] = []\n\n    for i in range(len(tokenized_examples[\"input_ids\"])):\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n        sample_index = sample_mapping[i]\n        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n        tokenized_examples[\"offset_mapping\"][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n        ]\n\n    return tokenized_examples","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:35:01.515332Z","iopub.execute_input":"2021-11-15T19:35:01.51579Z","iopub.status.idle":"2021-11-15T19:35:01.527692Z","shell.execute_reply.started":"2021-11-15T19:35:01.515744Z","shell.execute_reply":"2021-11-15T19:35:01.526918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"special_tokens = {\n    \"muril\": 2,\n    \"xlmr\": 3,\n    \"rembert\": 2,\n}\n\ndef postprocess_qa_predictions(\n    examples, tokenizer, features, raw_predictions, n_best_size=20, max_answer_length=30, squad_v2=False\n):\n    all_start_logits, all_end_logits = raw_predictions\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    predictions = collections.OrderedDict()\n\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n    all_answers = []\n\n    for example_index, example in enumerate(tqdm(examples)):\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None  # Only used if squad_v2 is True.\n        valid_answers = []\n\n        context = example[\"context\"]\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n\n            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": float(start_logits[start_index] + end_logits[end_index]),\n                            \"text\": context[start_char:end_char],\n                        }\n                    )\n\n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n\n        if not squad_v2:\n            predictions[example[\"id\"]] = best_answer[\"text\"]\n        else:\n            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n            predictions[example[\"id\"]] = answer\n\n        valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n        all_answers.append({\"id\": example[\"id\"], \"predictions\": valid_answers})\n    return all_answers, predictions","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:35:01.528961Z","iopub.execute_input":"2021-11-15T19:35:01.529417Z","iopub.status.idle":"2021-11-15T19:35:01.546702Z","shell.execute_reply.started":"2021-11-15T19:35:01.529381Z","shell.execute_reply":"2021-11-15T19:35:01.546019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_to_one_array(logits, max_length, stride, offset):\n    num_chunks = len(logits)\n    ctx_size = max_length-offset-1\n    max_ctx_size = num_chunks*ctx_size-(num_chunks-1)*stride\n    final_size = offset+max_ctx_size\n\n    full = np.zeros((final_size))\n    full[0:max_length-1] = logits[0][:-1]\n\n    left_idx = max_length-1-stride\n    for idx in range(1, num_chunks):\n        right_idx = left_idx+ctx_size\n\n        full[left_idx:right_idx] += logits[idx][offset:-1]\n        full[left_idx:left_idx+stride]/=2\n        \n        left_idx = right_idx-stride  \n    return full\n\ndef token_level_to_char_level(text, offsets, preds):\n    probas_char = np.ones(len(text))*-100\n    for i, offset in enumerate(offsets):\n        if offset[0] or offset[1]: # remove padding and sentiment\n            probas_char[offset[0]:offset[1]] = preds[i]\n    \n    return probas_char\n\n\ndef new_postprocess_qa_predictions(\n    examples, tokenizer, features, raw_predictions, max_seq_length, doc_stride, num_special_tokens, file_name, n_best_size=20, max_answer_length=30, squad_v2=False\n):\n    all_start_logits, all_end_logits = raw_predictions\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n        \n    new_start_features = collections.OrderedDict()\n    new_end_features = collections.OrderedDict()\n    \n    char_level_starts = {}\n    char_level_ends = {}\n    \n    predictions = collections.OrderedDict()\n\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n    all_answers = []\n\n    for example_index, example in enumerate(tqdm(examples)):\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None  # Only used if squad_v2 is True.\n        valid_answers = []\n        \n        num_q_tokens = len(tokenizer(example[\"question\"], add_special_tokens=False)[\"input_ids\"])\n        flat_input_ids = tokenizer(example[\"question\"], example[\"context\"], return_offsets_mapping=True)\n        \n        left_offset = num_q_tokens+num_special_tokens\n        \n        id_ = example[\"id\"]\n        \n        new_start_features[id_] = convert_to_one_array(all_start_logits[feature_indices], max_seq_length, doc_stride, left_offset)\n        new_end_features[id_] = convert_to_one_array(all_end_logits[feature_indices], max_seq_length, doc_stride, left_offset)        \n        \n        start_indexes = np.argsort(new_start_features[id_])[-1 : -n_best_size - 1 : -1].tolist()\n        end_indexes = np.argsort( new_end_features[id_])[-1 : -n_best_size - 1 : -1].tolist()\n        \n        offset_mapping = flat_input_ids[\"offset_mapping\"]\n        for start_index in start_indexes:\n            for end_index in end_indexes:\n                if (\n                    start_index >= len(offset_mapping)\n                    or end_index >= len(offset_mapping)\n                    or offset_mapping[start_index] is None\n                    or offset_mapping[end_index] is None\n                    or start_index < left_offset\n                ):\n                    continue\n                if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                    continue\n\n                start_char = offset_mapping[start_index][0]\n                end_char = offset_mapping[end_index][1]\n                valid_answers.append(\n                    {\n                        \"score\": float(new_start_features[id_][start_index] + new_end_features[id_][end_index]),\n                        \"text\": example[\"context\"][start_char:end_char],\n                        \"start\": start_index,\n                        \"end\": end_index\n                    }\n                )\n\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n\n        if not squad_v2:\n            predictions[example[\"id\"]] = best_answer[\"text\"]\n        else:\n            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n            predictions[example[\"id\"]] = answer\n\n        valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n        all_answers.append({\"id\": example[\"id\"], \"predictions\": valid_answers})\n\n    \n        char_level_starts[id_] = token_level_to_char_level(example[\"context\"], offset_mapping, new_start_features[id_])\n        char_level_ends[id_] = token_level_to_char_level(example[\"context\"], offset_mapping, new_end_features[id_])\n\n\n    with open(f\"char-level-start-logits-{file_name}\", \"wb\") as fp:\n        pickle.dump(char_level_starts, fp)\n\n    with open(f\"char-level-end-logits-{file_name}\", \"wb\") as fp:\n        pickle.dump(char_level_ends, fp)\n        \n        \n    return all_answers, predictions","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:35:01.549935Z","iopub.execute_input":"2021-11-15T19:35:01.550146Z","iopub.status.idle":"2021-11-15T19:35:01.576298Z","shell.execute_reply.started":"2021-11-15T19:35:01.550125Z","shell.execute_reply":"2021-11-15T19:35:01.575526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv(\"../input/chaii-hindi-and-tamil-question-answering/test.csv\")\ntest_data[\"len\"] = [len(x) for x in test_data[\"context\"]]\n\ndo_inference = len(test_data) != 5\n\nchar_threshold = 15_000\n\n# short_data = test_data.copy()\nshort_data = test_data[test_data[\"len\"]<char_threshold].reset_index(drop=True)\nlong_data = test_data[test_data[\"len\"]>=char_threshold].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:35:01.57947Z","iopub.execute_input":"2021-11-15T19:35:01.57968Z","iopub.status.idle":"2021-11-15T19:35:01.613629Z","shell.execute_reply.started":"2021-11-15T19:35:01.579655Z","shell.execute_reply":"2021-11-15T19:35:01.613035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MuRIL Large","metadata":{}},{"cell_type":"code","source":"if do_inference:\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\"../input/convert-to-pytorch-muril-large\")\n    \n    pad_on_right = tokenizer.padding_side == \"right\"\n    max_length = 384\n    doc_stride = 128\n\n    test_dataset = Dataset.from_pandas(short_data)\n    test_features = test_dataset.map(\n        partial(\n            prepare_validation_features, \n            tokenizer=tokenizer,\n            pad_on_right=pad_on_right, \n            max_length=max_length,\n            doc_stride=doc_stride\n        ),\n        batched=True,\n        remove_columns=test_dataset.column_names\n    )\n    test_feats_small = test_features.map(\n        lambda example: example, remove_columns=['example_id', 'offset_mapping']\n    )\n\n    fin_start_logits = None\n    fin_end_logits = None\n\n    chunk1 = [\"../input/convert-to-pytorch-muril-large/nbroad/flax-muril-large-chaii-f0\",]*3\n    chunk2 = [\"../input/convert-to-pytorch-muril-large-f567/nbroad/flax-muril-large-chaii-f5\"]*3\n    models = chunk1+chunk2\n\n    data_loader = torch.utils.data.DataLoader(\n        test_feats_small.with_format(\"torch\"), \n        batch_size=64,\n        num_workers=4,\n        pin_memory=True,\n        shuffle=False\n    )\n\n\n    for model_name, fold in tqdm(zip(models, [0, 8, 9, 5, 6, 7])):\n        model = ChaiiModel(model_name=model_name, num_train_steps=0, steps_per_epoch=0, learning_rate=0)\n        model.transformer.load_state_dict(torch.load(f\"{model_name[:-1]}{fold}/pytorch_model.bin\"))\n        model.to(\"cuda\")\n        model.eval()\n\n        start_logits = []\n        end_logits = []\n\n        for b_idx, data in enumerate(data_loader):\n            with torch.no_grad():\n                for key, value in data.items():\n                    data[key] = value.to(\"cuda\")\n                output, _, _ = model(ids=data[\"input_ids\"], mask=data[\"attention_mask\"])\n                start = output[0].detach().cpu().numpy()\n                end = output[1].detach().cpu().numpy()\n                start_logits.append(start)\n                end_logits.append(end)\n\n        start_logits = np.vstack(start_logits)\n        end_logits = np.vstack(end_logits)\n\n        if fin_start_logits is None:\n            fin_start_logits = start_logits\n            fin_end_logits = end_logits\n        else:\n            fin_start_logits += start_logits\n            fin_end_logits += end_logits\n            \n#         to_save, fin_preds = postprocess_qa_predictions(test_dataset, tokenizer, test_features, (start_logits, end_logits))      \n#         with open(f\"top-preds-muril-large-f{fold}.json\", \"w\") as fp:\n#             json.dump(to_save, fp)\n\n        del model\n        torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:35:01.614788Z","iopub.execute_input":"2021-11-15T19:35:01.615136Z","iopub.status.idle":"2021-11-15T19:37:18.777094Z","shell.execute_reply.started":"2021-11-15T19:35:01.6151Z","shell.execute_reply":"2021-11-15T19:37:18.776197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nif do_inference:\n    fin_start_logits /= len(models)\n    fin_end_logits /= len(models)\n    \n    # This is for voting\n#     to_save, fin_preds = postprocess_qa_predictions(test_dataset, tokenizer, test_features, (fin_start_logits, fin_end_logits))\n#     with open('muril-large-preds.json', \"w\") as fp:\n#         json.dump(to_save, fp)\n\n    all_answers, predictions = new_postprocess_qa_predictions(\n    test_dataset, tokenizer, test_features, (fin_start_logits, fin_end_logits), max_length, doc_stride, num_special_tokens=2, file_name=\"muril-large\", n_best_size=20, max_answer_length=30, squad_v2=False\n)\n\n#     short_data[\"PredictionString\"] = long_data[\"id\"].map(fin_preds)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:37:18.779338Z","iopub.execute_input":"2021-11-15T19:37:18.779643Z","iopub.status.idle":"2021-11-15T19:37:18.847531Z","shell.execute_reply.started":"2021-11-15T19:37:18.779602Z","shell.execute_reply":"2021-11-15T19:37:18.846221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MuRIL BigBird Base  (I ended up not using this)\nAll data  \n4k max length","metadata":{}},{"cell_type":"code","source":"# if do_inference:\n#     tokenizer = transformers.AutoTokenizer.from_pretrained(\"../input/bb-base-chaii\")\n#     pad_on_right = tokenizer.padding_side == \"right\"\n#     max_length = 4096\n#     doc_stride = 2048\n\n#     test_dataset = Dataset.from_pandas(test_data)\n#     test_features = test_dataset.map(\n#         partial(\n#             prepare_validation_features, \n#             tokenizer=tokenizer,\n#             pad_on_right=pad_on_right, \n#             max_length=max_length,\n#             doc_stride=doc_stride\n#         ),\n#         batched=True,\n#         remove_columns=test_dataset.column_names\n#     )\n#     test_feats_small = test_features.map(\n#         lambda example: example, remove_columns=['example_id', 'offset_mapping']\n#     )\n\n#     fin_start_logits = None\n#     fin_end_logits = None\n\n#     models = [\n#         \"../input/bb-base-chaii\",\n#         \"../input/nbroad-flax-muril-bb-base-chaii-f2\",\n#         \"../input/nbroad-flax-muril-bb-base-chaii-f3\",\n#         \"../input/nbroad-flax-muril-bb-base-chaii-f4\",\n#         \"../input/nbroad-flax-bb-base-chaii-f5\",\n#         \"../input/nbroad-flax-muril-bb-base-chaii-f6\",\n#         \"../input/nbroad-flax-muril-bb-base-chaii-f7\", \n#     ]\n\n#     data_loader = torch.utils.data.DataLoader(\n#         test_feats_small.with_format(\"torch\"), \n#         batch_size=16,\n#         num_workers=4,\n#         pin_memory=True,\n#         shuffle=False\n#     )\n\n\n#     for fold, model_name in tqdm(enumerate(models)):\n#         model = ChaiiModel(model_name=model_name, num_train_steps=0, steps_per_epoch=0, learning_rate=0)\n#         model.transformer.load_state_dict(torch.load(f\"{model_name}/pytorch_model.bin\"))\n#         model.to(\"cuda\")\n#         model.eval()\n\n#         start_logits = []\n#         end_logits = []\n\n#         for b_idx, data in enumerate(data_loader):\n#             with torch.no_grad():\n#                 for key, value in data.items():\n#                     data[key] = value.to(\"cuda\")\n#                 output, _, _ = model(ids=data[\"input_ids\"], mask=data[\"attention_mask\"])\n#                 start = output[0].detach().cpu().numpy()\n#                 end = output[1].detach().cpu().numpy()\n#                 start_logits.append(start)\n#                 end_logits.append(end)\n\n#         start_logits = np.vstack(start_logits)\n#         end_logits = np.vstack(end_logits)\n\n#         if fin_start_logits is None:\n#             fin_start_logits = start_logits\n#             fin_end_logits = end_logits\n#         else:\n#             fin_start_logits += start_logits\n#             fin_end_logits += end_logits\n            \n# #         to_save, fin_preds = postprocess_qa_predictions(test_dataset, tokenizer, test_features, (start_logits, end_logits))      \n# #         with open(f\"top-preds-bb-f{fold}.json\", \"w\") as fp:\n# #             json.dump(to_save, fp)\n\n#         del model\n#         torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:37:18.85116Z","iopub.execute_input":"2021-11-15T19:37:18.851685Z","iopub.status.idle":"2021-11-15T19:37:18.857604Z","shell.execute_reply.started":"2021-11-15T19:37:18.851653Z","shell.execute_reply":"2021-11-15T19:37:18.85674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # if do_inference:\n# #     to_save, fin_preds = postprocess_qa_predictions(test_dataset, tokenizer, test_features, (fin_start_logits, fin_end_logits))\n# if do_inference:\n#     fin_start_logits /= len(models)\n#     fin_end_logits /= len(models)\n    \n# #     to_save, fin_preds = postprocess_qa_predictions(test_dataset, tokenizer, test_features, (fin_start_logits, fin_end_logits))\n# #     with open('muril-large-preds.json', \"w\") as fp:\n# #         json.dump(to_save, fp)\n\n#     all_answers, predictions = new_postprocess_qa_predictions(\n#     test_dataset, tokenizer, test_features, (fin_start_logits, fin_end_logits), max_length, doc_stride, num_special_tokens=2, file_name=\"bb-4k\", n_best_size=20, max_answer_length=30, squad_v2=False\n# )","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:37:18.859057Z","iopub.execute_input":"2021-11-15T19:37:18.859811Z","iopub.status.idle":"2021-11-15T19:37:18.869287Z","shell.execute_reply.started":"2021-11-15T19:37:18.859769Z","shell.execute_reply":"2021-11-15T19:37:18.868372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if do_inference:\n#     long_data[\"PredictionString\"] = long_data[\"id\"].map(fin_preds)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:37:18.871699Z","iopub.execute_input":"2021-11-15T19:37:18.872193Z","iopub.status.idle":"2021-11-15T19:37:18.880932Z","shell.execute_reply.started":"2021-11-15T19:37:18.872155Z","shell.execute_reply":"2021-11-15T19:37:18.879983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MuRIL BigBird Large\n1k max length  \nALL data","metadata":{}},{"cell_type":"code","source":"if do_inference:\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\"../input/murilbasecased\")\n    \n    pad_on_right = tokenizer.padding_side == \"right\"\n    max_length = 1024\n    doc_stride = 512\n\n    test_dataset = Dataset.from_pandas(test_data)\n    test_features = test_dataset.map(\n        partial(\n            prepare_validation_features, \n            tokenizer=tokenizer,\n            pad_on_right=pad_on_right, \n            max_length=max_length,\n            doc_stride=doc_stride\n        ),\n        batched=True,\n        remove_columns=test_dataset.column_names\n    )\n    test_feats_small = test_features.map(\n        lambda example: example, remove_columns=['example_id', 'offset_mapping']\n    )\n\n    fin_start_logits = None\n    fin_end_logits = None\n\n    data_loader = torch.utils.data.DataLoader(\n        test_feats_small.with_format(\"torch\"), \n        batch_size=16,\n        num_workers=4,\n        pin_memory=True,\n        shuffle=False\n    )\n\n    \n    model_name = \"../input/muril-large-bigbird-1k-6f/nbroad/1k-shuf-squad-chaii-6f0\"\n    for fold in tqdm(range(6)):\n        model = ChaiiModel(model_name=model_name, num_train_steps=0, steps_per_epoch=0, learning_rate=0)\n        model.transformer.load_state_dict(torch.load(f\"../input/muril-large-bigbird-1k-6f/nbroad/1k-shuf-squad-chaii-6f{fold}/pytorch_model.bin\"))\n        model.to(\"cuda\")\n        model.eval()\n\n        start_logits = []\n        end_logits = []\n\n        for b_idx, data in enumerate(data_loader):\n            with torch.no_grad():\n                for key, value in data.items():\n                    data[key] = value.to(\"cuda\")\n                output, _, _ = model(ids=data[\"input_ids\"], mask=data[\"attention_mask\"])\n                start = output[0].detach().cpu().numpy()\n                end = output[1].detach().cpu().numpy()\n                start_logits.append(start)\n                end_logits.append(end)\n\n        start_logits = np.vstack(start_logits)\n        end_logits = np.vstack(end_logits)\n\n        if fin_start_logits is None:\n            fin_start_logits = start_logits\n            fin_end_logits = end_logits\n        else:\n            fin_start_logits += start_logits\n            fin_end_logits += end_logits\n            \n#         to_save, fin_preds = postprocess_qa_predictions(test_dataset, tokenizer, test_features, (start_logits, end_logits))      \n#         with open(f\"top-preds-muril-large-f{fold}.json\", \"w\") as fp:\n#             json.dump(to_save, fp)\n\n        del model\n        torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:37:18.904392Z","iopub.execute_input":"2021-11-15T19:37:18.904709Z","iopub.status.idle":"2021-11-15T19:40:02.339628Z","shell.execute_reply.started":"2021-11-15T19:37:18.904673Z","shell.execute_reply":"2021-11-15T19:40:02.338793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if do_inference:\n    \n    fin_start_logits /= len(models)\n    fin_end_logits /= len(models)\n    \n    all_answers, predictions = new_postprocess_qa_predictions(\n    test_dataset, tokenizer, test_features, (fin_start_logits, fin_end_logits), max_length, doc_stride, num_special_tokens=2, file_name=\"bb-1k\", n_best_size=20, max_answer_length=30, squad_v2=False\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:40:02.342508Z","iopub.execute_input":"2021-11-15T19:40:02.343161Z","iopub.status.idle":"2021-11-15T19:40:02.655579Z","shell.execute_reply.started":"2021-11-15T19:40:02.343127Z","shell.execute_reply":"2021-11-15T19:40:02.654728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XLMR-Large 6 folds\n\n384_128  \nShort data","metadata":{}},{"cell_type":"code","source":"if do_inference:\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\"../input/xlmrob\")\n    pad_on_right = tokenizer.padding_side == \"right\"\n    max_length = 384\n    doc_stride = 128\n\n    test_dataset = Dataset.from_pandas(short_data)\n    test_features = test_dataset.map(\n        partial(\n            prepare_validation_features, \n            tokenizer=tokenizer,\n            pad_on_right=pad_on_right, \n            max_length=max_length,\n            doc_stride=doc_stride\n        ),\n        batched=True,\n        remove_columns=test_dataset.column_names\n    )\n    test_feats_small = test_features.map(\n        lambda example: example, remove_columns=['example_id', 'offset_mapping']\n    )\n\n    fin_start_logits = None\n    fin_end_logits = None\n\n\n    data_loader = torch.utils.data.DataLoader(\n        test_feats_small.with_format(\"torch\"), \n        batch_size=16,\n        num_workers=4,\n        pin_memory=True,\n        shuffle=False\n    )\n\n    model_name = \"../input/convert-to-pytorch-xlmr-large-chaii-6f/nbroad/xlmr-large-chaii-6f0\"\n    for fold in tqdm(range(6)):\n        model = ChaiiModel(model_name=model_name, num_train_steps=0, steps_per_epoch=0, learning_rate=0)\n        model.transformer.load_state_dict(torch.load(f\"../input/convert-to-pytorch-xlmr-large-chaii-6f/nbroad/xlmr-large-chaii-6f{fold}/pytorch_model.bin\"))\n        model.to(\"cuda\")\n        model.eval()\n\n        start_logits = []\n        end_logits = []\n\n        for b_idx, data in enumerate(data_loader):\n            with torch.no_grad():\n                for key, value in data.items():\n                    data[key] = value.to(\"cuda\")\n                output, _, _ = model(ids=data[\"input_ids\"], mask=data[\"attention_mask\"])\n                start = output[0].detach().cpu().numpy()\n                end = output[1].detach().cpu().numpy()\n                start_logits.append(start)\n                end_logits.append(end)\n\n        start_logits = np.vstack(start_logits)\n        end_logits = np.vstack(end_logits)\n\n        if fin_start_logits is None:\n            fin_start_logits = start_logits\n            fin_end_logits = end_logits\n        else:\n            fin_start_logits += start_logits\n            fin_end_logits += end_logits\n            \n#         to_save, fin_preds = postprocess_qa_predictions(test_dataset, tokenizer, test_features, (start_logits, end_logits))      \n#         with open(f\"top-preds-bb-f{fold}.json\", \"w\") as fp:\n#             json.dump(to_save, fp)\n\n        del model\n        torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:40:02.677047Z","iopub.execute_input":"2021-11-15T19:40:02.677405Z","iopub.status.idle":"2021-11-15T19:43:04.768246Z","shell.execute_reply.started":"2021-11-15T19:40:02.677368Z","shell.execute_reply":"2021-11-15T19:43:04.767339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if do_inference:\n    fin_start_logits /= 6\n    fin_end_logits /= 6\n    \n    all_answers, predictions = new_postprocess_qa_predictions(\n    test_dataset, tokenizer, test_features, (fin_start_logits, fin_end_logits), max_length, doc_stride, num_special_tokens=3, file_name=\"xlmr\", n_best_size=20, max_answer_length=30, squad_v2=False\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:43:04.771152Z","iopub.execute_input":"2021-11-15T19:43:04.772951Z","iopub.status.idle":"2021-11-15T19:43:04.858093Z","shell.execute_reply.started":"2021-11-15T19:43:04.772905Z","shell.execute_reply":"2021-11-15T19:43:04.857324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RemBERT\n384_128  \nShort Data","metadata":{}},{"cell_type":"code","source":"if do_inference:\n    import tensorflow as tf\n    import tensorflow.keras.backend as K\n\n    rembert_model_path = \"../input/rembert-tf\"\n    \n    tokenizer = transformers.AutoTokenizer.from_pretrained(rembert_model_path)\n\n    strategy = tf.distribute.get_strategy()\n    AUTO     = tf.data.experimental.AUTOTUNE\n\n    max_length = 384\n    doc_stride = 128\n\n    pad_on_right = tokenizer.padding_side == \"right\"","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:43:04.859573Z","iopub.execute_input":"2021-11-15T19:43:04.859863Z","iopub.status.idle":"2021-11-15T19:43:05.89836Z","shell.execute_reply.started":"2021-11-15T19:43:04.859825Z","shell.execute_reply":"2021-11-15T19:43:05.897633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if do_inference:\n    def build_model():\n        roberta = transformers.TFAutoModel.from_pretrained(rembert_model_path)\n\n        input_ids = tf.keras.layers.Input(shape = (max_length, ), name = 'input_ids', dtype = tf.int32)\n        attention_mask = tf.keras.layers.Input(shape = (max_length, ), name = 'attention_mask', dtype = tf.int32)\n\n        embeddings = roberta(input_ids=input_ids, attention_mask=attention_mask)[0]\n\n        x1 = tf.keras.layers.Dropout(0.1)(embeddings)\n        x1 = tf.keras.layers.Dense(1, dtype=tf.float32)(x1)\n        x1 = tf.keras.layers.Flatten()(x1)\n        x1 = tf.keras.layers.Activation('softmax', name='start_positions', dtype=tf.float32)(x1)\n\n        x2 = tf.keras.layers.Dropout(0.1)(embeddings)\n        x2 = tf.keras.layers.Dense(1, dtype=tf.float32)(x2)\n        x2 = tf.keras.layers.Flatten()(x2)\n        x2 = tf.keras.layers.Activation('softmax', name='end_positions', dtype=tf.float32)(x2)\n\n        model = tf.keras.models.Model(inputs = [input_ids, attention_mask], outputs = [x1, x2])\n\n        model.compile()\n\n        return model","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:43:05.899813Z","iopub.execute_input":"2021-11-15T19:43:05.900053Z","iopub.status.idle":"2021-11-15T19:43:05.908845Z","shell.execute_reply.started":"2021-11-15T19:43:05.900021Z","shell.execute_reply":"2021-11-15T19:43:05.908061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if do_inference:\n    \n    fin_start_logits = None\n    fin_end_logits = None\n\n    batch_size = 128\n    \n    strategy = tf.distribute.get_strategy()\n    \n\n    test_dataset = Dataset.from_pandas(short_data)\n\n    test_features = test_dataset.map(\n            partial(\n                prepare_validation_features, \n                tokenizer=tokenizer,\n                pad_on_right=pad_on_right, \n                max_length=max_length,\n                doc_stride=doc_stride\n            ),\n            batched=True,\n            remove_columns=test_dataset.column_names\n        )\n\n    test_features = test_features.with_format('tensorflow')\n\n    test_x = {x: test_features[x] for x in ['input_ids', 'attention_mask']}\n    test_slices = tf.data.Dataset.from_tensor_slices((test_x)).batch(batch_size)\n\n    models = [\n            \"../input/rembert-0-3hi-v2/rembert-fit-chaii/fold0/tf_model.h5\",\n            \"../input/rembert-0-3hi-v2/rembert-fit-chaii/fold1/tf_model.h5\",\n            \"../input/rembert-0-3hi-v2/rembert-fit-chaii/fold2/tf_model.h5\",\n            \"../input/rembert-4-6-ta/rembert-fit-chaii/fold4/tf_model.h5\", \n            \"../input/rembert-4-6-ta/rembert-fit-chaii/fold5/tf_model.h5\",\n    ]\n\n    for model_name in models:\n        K.clear_session()\n        strategy = tf.distribute.get_strategy()\n\n        with strategy.scope():\n            model = build_model()\n            model.load_weights(model_name)\n\n\n        temp_start_logits, temp_end_logits = model.predict(test_slices, batch_size=batch_size, verbose=1)\n\n\n        start_logits = np.vstack(temp_start_logits)\n        end_logits = np.vstack(temp_end_logits)\n\n        if fin_start_logits is None:\n            fin_start_logits = start_logits\n            fin_end_logits = end_logits\n        else:\n            fin_start_logits += start_logits\n            fin_end_logits += end_logits\n\n\n    fin_start_logits /= 5\n    fin_end_logits /= 5\n\n    all_answers, predictions = new_postprocess_qa_predictions(\n        test_dataset, tokenizer, test_features.with_format(\"numpy\"), (fin_start_logits, fin_end_logits), max_length, doc_stride, num_special_tokens=2, file_name=\"remb\", n_best_size=20, max_answer_length=30, squad_v2=False\n    )","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:43:05.91031Z","iopub.execute_input":"2021-11-15T19:43:05.910671Z","iopub.status.idle":"2021-11-15T19:47:45.981068Z","shell.execute_reply.started":"2021-11-15T19:43:05.910636Z","shell.execute_reply":"2021-11-15T19:47:45.980306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if do_inference:\n    def move_to_boundary(start, end, context):\n        initial_start = start\n        start_offset = 0\n        end_offset = 0\n        if not context[start].isspace():\n            start_offset += 1\n            while start-start_offset > 0 and not context[start-start_offset].isspace():\n                start_offset+=1\n        if not context[end].isspace():\n            end_offset += 1\n            while end+end_offset < len(context) and not context[end+end_offset].isspace():\n                end_offset += 1\n        return context[start-start_offset:end+end_offset]","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:47:45.982662Z","iopub.execute_input":"2021-11-15T19:47:45.9831Z","iopub.status.idle":"2021-11-15T19:47:45.990215Z","shell.execute_reply.started":"2021-11-15T19:47:45.983063Z","shell.execute_reply":"2021-11-15T19:47:45.989141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Char-level averaging that never got finished","metadata":{}},{"cell_type":"code","source":"if do_inference:\n    from scipy.special import softmax   \n    \n    def fix_duplicate_scores(scores, starts=True):\n        # if starts=True, will keep the value with the lower index\n        # [1,2,2,3,4] -> [1,2,-10, 3, 4]\n        # otherwise, keep the value with higher index\n        # [1,2,2,3,4] -> [1,-10, 2, 3, 4]   \n\n        if not starts:\n            scores = scores[::-1]\n\n        prev = scores[0]\n        new_scores = [prev]\n        for val in scores[1:]:\n            if abs(val-prev) < 1e-5:\n                new_scores.append(-100)\n            else:\n                new_scores.append(val)\n            prev = val\n\n        return np.array(new_scores) if starts else np.array(new_scores[::-1])\n    \n    def process_char_level_logits(filenames, data, multipliers, n_best_size=20, max_answer_length=35):\n        all_starts = {}\n        for filename in filenames:\n            with open(f\"char-level-start-logits-{filename}\", \"rb\") as fp:\n                all_starts[filename] = pickle.load(fp)\n\n        all_ends = {}\n        for filename in filenames:\n            with open(f\"char-level-end-logits-{filename}\", \"rb\") as fp:\n                all_ends[filename] = pickle.load(fp)\n\n        predictions = collections.OrderedDict()\n        all_answers = []\n        for id_, context in data[[\"id\", \"context\"]].values:\n            start_logits = sum([softmax(logits[id_])*multipliers[filename] for filename, logits in all_starts.items()])\n            end_logits = sum([softmax(logits[id_])*multipliers[filename] for filename, logits in all_ends.items()])\n            \n            start_logits = fix_duplicate_scores(start_logits, starts=True)\n            end_logits = fix_duplicate_scores(end_logits, starts=False)\n\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n\n            valid_answers = []\n            for start_index in start_indexes:\n                if start_logits[start_index] == -100: continue\n                for end_index in end_indexes:\n                    if end_logits[end_index] == -100: continue\n                    if end_index < start_index or end_index - start_index + 1 > 30:\n                        continue\n\n                    text = context[start_index:end_index]\n                    valid_answers.append(\n                        {\n                            \"score\": float(start_logits[start_index] + end_logits[end_index]),\n                            \"text\": text,\n                            \"start\": start_index,\n                            \"end\": end_index,\n                            \"moved\": move_to_boundary(start_index, end_index, context)\n                        }\n                    )\n        \n            sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)\n\n            if len(valid_answers) > 0:\n                best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n            else:\n                best_answer = {\"text\": \"\", \"score\": 0.0}\n\n            answer = best_answer[\"moved\"]\n            predictions[id_] = answer\n\n            valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n            all_answers.append({\"id\": id_, \"predictions\": valid_answers})\n\n        return all_answers, predictions\n\n    multipliers = {\n        \"bb-4k\": 0.5,\n        \"muril-large\": 0.8,\n        \"bb-1k\": 1.3,\n        \"xlmr\": 1.1,\n        \"remb\": 1.2\n    }\n    filenames = [\n        'muril-large', \n        #\"bb-4k\",\n        \"bb-1k\", \n        \"xlmr\", \n        \"remb\"\n    ]\n    output_short = process_char_level_logits(filenames, short_data, multipliers)\n    \n    filenames = [\n#         \"bb-4k\", \n        \"bb-1k\"\n    ]\n    output_long = process_char_level_logits(filenames, long_data, multipliers)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:49:57.110596Z","iopub.execute_input":"2021-11-15T19:49:57.111106Z","iopub.status.idle":"2021-11-15T19:49:57.258142Z","shell.execute_reply.started":"2021-11-15T19:49:57.111068Z","shell.execute_reply":"2021-11-15T19:49:57.257376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if do_inference:\n    short_data[\"PredictionString\"] = short_data[\"id\"].map(output_short[1])\n    long_data[\"PredictionString\"] = long_data[\"id\"].map(output_long[1])","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:49:59.032115Z","iopub.execute_input":"2021-11-15T19:49:59.032697Z","iopub.status.idle":"2021-11-15T19:49:59.040176Z","shell.execute_reply.started":"2021-11-15T19:49:59.032656Z","shell.execute_reply":"2021-11-15T19:49:59.038102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub2 = pd.DataFrame(submission, columns=[\"id\", \"PredictionString\"])\n\n# final = pd.concat([sub1, sub2], axis=0, ignore_index=True)\n# final = final.merge(test_data[[\"context\", \"question\", \"id\"]], on=\"id\")","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:50:00.618647Z","iopub.execute_input":"2021-11-15T19:50:00.619331Z","iopub.status.idle":"2021-11-15T19:50:00.623593Z","shell.execute_reply.started":"2021-11-15T19:50:00.619294Z","shell.execute_reply":"2021-11-15T19:50:00.622771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Attempts at voting that didn't work","metadata":{}},{"cell_type":"code","source":"# # folds have been averaged and have only 1 prediction file \n# if do_inference:\n#     model_fold_preds = {}\n#     with open('xlmr-large-preds.json') as fp:\n#         model_fold_preds[\"xlmr\"] = json.load(fp)\n#     with open('muril-large-preds.json') as fp:\n#         model_fold_preds[\"muril\"] = json.load(fp)\n\n#     from collections import Counter\n\n#     voted_preds = {}\n#     top_k = 4\n#     for i in range(len(short_data)):\n#         cnt = Counter()\n#         for fold_, preds in enumerate(model_fold_preds.values()):\n#             cnt.update([text[\"text\"] for text in preds[i][\"predictions\"]][:top_k])\n            \n#         most_common = cnt.most_common(top_k)\n#         voted_preds[preds[i][\"id\"]] = most_common[0][0]\n        \n#     short_data[\"PredictionString\"] = short_data[\"id\"].map(voted_preds)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:50:00.858492Z","iopub.execute_input":"2021-11-15T19:50:00.859165Z","iopub.status.idle":"2021-11-15T19:50:00.862987Z","shell.execute_reply.started":"2021-11-15T19:50:00.859133Z","shell.execute_reply":"2021-11-15T19:50:00.86201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if do_inference:\n#     model_fold_preds = {}\n#     model = \"xlmr\"\n#     for fold_ in range(10):\n#         with open(f\"top-preds-{model}-f{fold_}.json\") as fp:\n#             model_fold_preds[f\"{model}-{fold_}\"] = json.load(fp)\n\n#     from collections import Counter\n\n#     voted_preds = {}\n#     top_k = 100\n#     for i in range(len(short_data)):\n#         cnt = Counter()\n#         for fold_, preds in enumerate(model_fold_preds.values()):\n#             cnt.update([text[\"text\"] for text in preds[i][\"predictions\"]][:top_k])\n            \n#         most_common = cnt.most_common(10)\n#         voted_preds[preds[i][\"id\"]] = most_common[0][0]\n        \n#     short_data[\"PredictionString\"] = short_data[\"id\"].map(voted_preds)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:50:01.277037Z","iopub.execute_input":"2021-11-15T19:50:01.277897Z","iopub.status.idle":"2021-11-15T19:50:01.282519Z","shell.execute_reply.started":"2021-11-15T19:50:01.277844Z","shell.execute_reply":"2021-11-15T19:50:01.281705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if do_inference:\n#     model_fold_preds = {}\n    \n#     model = \"bb\"\n#     for fold_ in range(7):\n#         with open(f\"top-preds-{model}-f{fold_}.json\") as fp:\n#             model_fold_preds[f\"{model}-{fold_}\"] = json.load(fp)\n\n#     from collections import Counter\n\n#     voted_preds = {}\n#     for i in range(len(long_data)):\n#         cnt = Counter()\n#         for fold_, preds in enumerate(model_fold_preds.values()):\n#             cnt.update([text[\"text\"] for text in preds[i][\"predictions\"]][:top_k])\n            \n#         most_common = cnt.most_common(10)\n#         voted_preds[preds[i][\"id\"]] = most_common[0][0]\n        \n#     long_data[\"PredictionString\"] = long_data[\"id\"].map(voted_preds)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:50:04.087314Z","iopub.execute_input":"2021-11-15T19:50:04.087579Z","iopub.status.idle":"2021-11-15T19:50:04.092141Z","shell.execute_reply.started":"2021-11-15T19:50:04.08755Z","shell.execute_reply":"2021-11-15T19:50:04.091045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if do_inference:\n    test_data = pd.concat([short_data, long_data], axis=0, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:50:04.992194Z","iopub.execute_input":"2021-11-15T19:50:04.993176Z","iopub.status.idle":"2021-11-15T19:50:04.999892Z","shell.execute_reply.started":"2021-11-15T19:50:04.993127Z","shell.execute_reply":"2021-11-15T19:50:04.998942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if do_inference:\n#     test_data = short_data.copy()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:50:09.144452Z","iopub.execute_input":"2021-11-15T19:50:09.145475Z","iopub.status.idle":"2021-11-15T19:50:09.149862Z","shell.execute_reply.started":"2021-11-15T19:50:09.145421Z","shell.execute_reply":"2021-11-15T19:50:09.148471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:50:09.527541Z","iopub.execute_input":"2021-11-15T19:50:09.528474Z","iopub.status.idle":"2021-11-15T19:50:09.543519Z","shell.execute_reply.started":"2021-11-15T19:50:09.528419Z","shell.execute_reply":"2021-11-15T19:50:09.542804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"‚Äì\",  \",\", \";\", \":\"]\nbad_endings = [\"-\", \"(\", \")\", \"‚Äì\", \",\", \";\", \":\"]\n\ntamil_ad = \"‡Æï‡Æø.‡Æ™‡Æø\"\ntamil_bc = \"‡Æï‡Æø.‡ÆÆ‡ØÅ\"\ntamil_km = \"‡Æï‡Æø.‡ÆÆ‡ØÄ\"\nhindi_ad = \"‡§à\"\nhindi_bc = \"‡§à.‡§™‡•Ç\"\n\ncleaned_preds = []\nif do_inference:\n    for pred, context in test_data[[\"PredictionString\", \"context\"]].values:\n        pred = pred.strip()\n        if pred == \"\":\n            cleaned_preds.append(pred)\n            continue\n\n        # I haven't check sure if this makes a difference, but there is one answer in the training set that ends like this and I think it is an annotator mistake\n        # see my notebook here for details https://www.kaggle.com/nbroad/chaii-qa-character-token-languages-eda \n        if pred.endswith(\"...\"):\n            pred = pred[:-3]\n\n        pred = pred.lstrip(\"\".join(bad_starts))\n        pred = pred.rstrip(\"\".join(bad_endings))\n\n        if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n            pred = pred+\".\"\n\n\n        cleaned_preds.append(pred)\n\n    test_data[\"PredictionString\"] = cleaned_preds","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:50:14.754957Z","iopub.execute_input":"2021-11-15T19:50:14.755454Z","iopub.status.idle":"2021-11-15T19:50:14.768291Z","shell.execute_reply.started":"2021-11-15T19:50:14.755415Z","shell.execute_reply":"2021-11-15T19:50:14.767464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if do_inference:\n    test_data[\"pred_len\"] = [len(x) for x in test_data[\"PredictionString\"]]\n# do something if the prediction is too short","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:50:15.010235Z","iopub.execute_input":"2021-11-15T19:50:15.010773Z","iopub.status.idle":"2021-11-15T19:50:15.015346Z","shell.execute_reply.started":"2021-11-15T19:50:15.010721Z","shell.execute_reply":"2021-11-15T19:50:15.01461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if do_inference:\n    test_data[[\"id\", \"PredictionString\"]].to_csv(\"submission.csv\", index=False)\nelse:\n    test_data[\"PredictionString\"] = \"lol\"\n    test_data[[\"id\", \"PredictionString\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:50:16.997196Z","iopub.execute_input":"2021-11-15T19:50:16.997942Z","iopub.status.idle":"2021-11-15T19:50:17.005846Z","shell.execute_reply.started":"2021-11-15T19:50:16.997904Z","shell.execute_reply":"2021-11-15T19:50:17.004495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if do_inference:\n    print(test_data[[\"id\", \"PredictionString\"]])","metadata":{"execution":{"iopub.status.busy":"2021-11-15T19:50:18.527422Z","iopub.execute_input":"2021-11-15T19:50:18.527687Z","iopub.status.idle":"2021-11-15T19:50:18.536219Z","shell.execute_reply.started":"2021-11-15T19:50:18.527656Z","shell.execute_reply":"2021-11-15T19:50:18.534519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}