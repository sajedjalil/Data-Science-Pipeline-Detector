{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This notebook is basically the same as @rhtsingh's [notebook](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer) but it uses RemBERT instead of XLM-R\n\nIn the future, I'll probably add more folds to see how good the ensemble will be.\n\nIf you don't know what RemBERT is, see my discussion [here](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/267827#1489286)\n\nfold | data | reinit | cv | public lb \n-----|-------|-------|--- | ---\n0 | chaii              | 0 | |0.756  \n0 | e2 chaii              | 0 | 0.674 | 0.761  \n1 | e2 chaii | 0 | 0.679 | 0.752\n2 | e2 chaii | 0 | 0.647 | 0.751\n3 | e2 chaii | 0 | 0.714 | 0.747\n1 | chaii              | 1 | |0.726  \n2 | chaii              | 1 | |0.723\n3 | chaii              | 1 | |0.731\n0 | chaii, mlqa, xquad | 0 | |0.739   \n1 | chaii, mlqa, xquad | 0 | |0.735\n2 | chaii, mlqa, xquad | 0 | |0.736\n3 | chaii, mlqa, xquad | 0 | |0.743","metadata":{}},{"cell_type":"code","source":"# since rembert is a new model, the master branch of transformers needs to be installed\n!pip install -U --no-build-isolation --no-deps ../input/transformers-master/ -qq","metadata":{"execution":{"iopub.status.busy":"2021-09-07T22:46:39.293437Z","iopub.execute_input":"2021-09-07T22:46:39.293824Z","iopub.status.idle":"2021-09-07T22:47:12.640432Z","shell.execute_reply.started":"2021-09-07T22:46:39.293743Z","shell.execute_reply":"2021-09-07T22:47:12.639414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\ngc.enable()\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import (\n    Dataset, DataLoader,\n    SequentialSampler\n)\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n    logging,\n)\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\ndef fix_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-07T22:47:12.642047Z","iopub.execute_input":"2021-09-07T22:47:12.642355Z","iopub.status.idle":"2021-09-07T22:47:17.95269Z","shell.execute_reply.started":"2021-09-07T22:47:12.642325Z","shell.execute_reply":"2021-09-07T22:47:17.951877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CFGuration","metadata":{}},{"cell_type":"code","source":"class CFG:\n    \n    test_file = \"../input/chaii-hindi-and-tamil-question-answering/test.csv\"\n    \n    # model\n    model_name_or_path = \"../input/rembert-e2-f3/output/checkpoint-fold-3\"\n    config_name = \"../input/rembert-e2-f3/output/checkpoint-fold-3\"\n\n    # tokenizer\n    tokenizer_name = \"../input/rembert-e2-f3/output/checkpoint-fold-3\"\n    max_seq_length = 384\n    doc_stride = 128\n\n    eval_batch_size = 32    \n    seed = 2021","metadata":{"execution":{"iopub.status.busy":"2021-09-07T22:49:35.280192Z","iopub.execute_input":"2021-09-07T22:49:35.28052Z","iopub.status.idle":"2021-09-07T22:49:35.286126Z","shell.execute_reply.started":"2021-09-07T22:49:35.280486Z","shell.execute_reply":"2021-09-07T22:49:35.285137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset Retriever","metadata":{}},{"cell_type":"code","source":"class DatasetRetriever(Dataset):\n    def __init__(self, features):\n        super().__init__()\n        self.features = features\n        \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, item):   \n        feature = self.features[item]\n        return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':feature['offset_mapping'],\n                'sequence_ids':feature['sequence_ids'],\n                'id':feature['example_id'],\n                'context': feature['context'],\n                'question': feature['question']\n            }","metadata":{"execution":{"iopub.status.busy":"2021-09-07T22:47:17.963195Z","iopub.execute_input":"2021-09-07T22:47:17.96368Z","iopub.status.idle":"2021-09-07T22:47:17.971484Z","shell.execute_reply.started":"2021-09-07T22:47:17.963515Z","shell.execute_reply":"2021-09-07T22:47:17.970501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, modelname_or_path, config):\n        super(Model, self).__init__()\n        self.config = config\n        self.model = AutoModel.from_pretrained(modelname_or_path, config=config)\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self._init_weights(self.qa_outputs)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n                    \n                \n    def forward(\n        self, \n        input_ids, \n        attention_mask=None, \n        # token_type_ids=None\n    ):\n        outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n        )\n\n        sequence_output = outputs[0]\n        pooled_output = outputs[1]\n        \n        # sequence_output = self.dropout(sequence_output)\n        qa_logits = self.qa_outputs(sequence_output)\n        \n        start_logits, end_logits = qa_logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n    \n        return start_logits, end_logits","metadata":{"execution":{"iopub.status.busy":"2021-09-07T22:47:17.972826Z","iopub.execute_input":"2021-09-07T22:47:17.973329Z","iopub.status.idle":"2021-09-07T22:47:17.985264Z","shell.execute_reply.started":"2021-09-07T22:47:17.973173Z","shell.execute_reply":"2021-09-07T22:47:17.984418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Utilities","metadata":{}},{"cell_type":"code","source":"def make_model(path):\n    config = AutoConfig.from_pretrained(path)\n    tokenizer = AutoTokenizer.from_pretrained(path, config=config)\n    model = Model(path, config=config)\n    return config, tokenizer, model","metadata":{"execution":{"iopub.status.busy":"2021-09-07T22:47:17.988132Z","iopub.execute_input":"2021-09-07T22:47:17.988412Z","iopub.status.idle":"2021-09-07T22:47:17.997246Z","shell.execute_reply.started":"2021-09-07T22:47:17.988387Z","shell.execute_reply":"2021-09-07T22:47:17.996453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Covert Examples to Features (Preprocess)","metadata":{}},{"cell_type":"code","source":"def prepare_test_features(args, example, tokenizer):\n    example[\"question\"] = example[\"question\"].lstrip()\n    \n    tokenized_example = tokenizer(\n        example[\"question\"],\n        example[\"context\"],\n        truncation=\"only_second\",\n        max_length=args.max_seq_length,\n        stride=args.doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    features = []\n    for i in range(len(tokenized_example[\"input_ids\"])):\n        feature = {}\n        feature[\"example_id\"] = example['id']\n        feature['context'] = example['context']\n        feature['question'] = example['question']\n        feature['input_ids'] = tokenized_example['input_ids'][i]\n        feature['attention_mask'] = tokenized_example['attention_mask'][i]\n        feature['offset_mapping'] = tokenized_example['offset_mapping'][i]\n        feature['sequence_ids'] = [0 if i is None else i for i in tokenized_example.sequence_ids(i)]\n        features.append(feature)\n    return features","metadata":{"execution":{"iopub.status.busy":"2021-09-07T22:47:17.998549Z","iopub.execute_input":"2021-09-07T22:47:17.998945Z","iopub.status.idle":"2021-09-07T22:47:18.008417Z","shell.execute_reply.started":"2021-09-07T22:47:17.998891Z","shell.execute_reply":"2021-09-07T22:47:18.00749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Postprocess QA Predictions","metadata":{}},{"cell_type":"code","source":"import collections\n\ndef postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n    all_start_logits, all_end_logits = raw_predictions\n    \n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    predictions = collections.OrderedDict()\n\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    for example_index, example in examples.iterrows():\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None\n        valid_answers = []\n        \n        context = example[\"context\"]\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n\n            sequence_ids = features[feature_index][\"sequence_ids\"]\n            context_index = 1\n\n            features[feature_index][\"offset_mapping\"] = [\n                (o if sequence_ids[k] == context_index else None)\n                for k, o in enumerate(features[feature_index][\"offset_mapping\"])\n            ]\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n        \n        predictions[example[\"id\"]] = best_answer[\"text\"]\n        \n        \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-09-07T22:47:18.011046Z","iopub.execute_input":"2021-09-07T22:47:18.011519Z","iopub.status.idle":"2021-09-07T22:47:18.026826Z","shell.execute_reply.started":"2021-09-07T22:47:18.011484Z","shell.execute_reply":"2021-09-07T22:47:18.026026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Factory","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CFG.tokenizer_name)\n\ndef get_test_data():\n    test_df = pd.read_csv(CFG.test_file)\n    test_features = []\n    for i, row in test_df.iterrows():\n        test_features += prepare_test_features(CFG, row, tokenizer)\n\n    test_dataset = DatasetRetriever(test_features)\n    dataloader = DataLoader(\n        test_dataset,\n        batch_size=CFG.eval_batch_size, \n        sampler=SequentialSampler(test_dataset),\n        num_workers=4,\n        pin_memory=True, \n        drop_last=False\n    )\n    \n    return test_df, test_features, dataloader\n\ntest_df, test_features, test_dataloader = get_test_data()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T22:52:53.428792Z","iopub.execute_input":"2021-09-07T22:52:53.429161Z","iopub.status.idle":"2021-09-07T22:52:54.19603Z","shell.execute_reply.started":"2021-09-07T22:52:53.429128Z","shell.execute_reply":"2021-09-07T22:52:54.195148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Intialize Inference","metadata":{}},{"cell_type":"code","source":"def get_predictions(checkpoint_path):\n    print(\"Getting predictions for\", checkpoint_path)\n    config, tokenizer, model = make_model(checkpoint_path)\n    model.cuda();\n    model.load_state_dict(\n        torch.load(checkpoint_path + '/pytorch_model.bin')\n    );\n    \n    start_logits = []\n    end_logits = []\n    for batch in test_dataloader:\n        with torch.no_grad():\n            o1, o2 = model(batch['input_ids'].cuda(), batch['attention_mask'].cuda())\n            \n            start_logits.append(o1.cpu().numpy().tolist())\n            end_logits.append(o2.cpu().numpy().tolist())\n    del model, tokenizer, config\n    gc.collect()\n    return np.vstack(start_logits), np.vstack(end_logits)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T22:49:10.367277Z","iopub.execute_input":"2021-09-07T22:49:10.367594Z","iopub.status.idle":"2021-09-07T22:49:10.375391Z","shell.execute_reply.started":"2021-09-07T22:49:10.367562Z","shell.execute_reply":"2021-09-07T22:49:10.374352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ensemble Folds","metadata":{}},{"cell_type":"code","source":"all_start_logits = []\nall_end_logits = []\n\n# only 1 fold for now, may add more later\nfor k in range(4):\n    path = f\"../input/rembert-e2-f{k}/output/checkpoint-fold-{k}\"\n    if k == 0:\n        path = \"../input/chaii-qa-rembert-e2-f0-chaii/output/checkpoint-fold-0\"\n    start_l, end_l = get_predictions(path)\n    all_start_logits.append(start_l)\n    all_end_logits.append(end_l)\n\nstart_logits = np.array(all_start_logits).mean(axis=0)\nend_logits = np.array(all_end_logits).mean(axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T22:49:37.589869Z","iopub.execute_input":"2021-09-07T22:49:37.59032Z","iopub.status.idle":"2021-09-07T22:50:00.024469Z","shell.execute_reply.started":"2021-09-07T22:49:37.59028Z","shell.execute_reply":"2021-09-07T22:50:00.023368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = postprocess_qa_predictions(test_df, test_features, (start_logits, end_logits))\ntest_df[\"PredictionString\"] = test_df['id'].map(predictions)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T22:53:04.154041Z","iopub.execute_input":"2021-09-07T22:53:04.154374Z","iopub.status.idle":"2021-09-07T22:53:04.193908Z","shell.execute_reply.started":"2021-09-07T22:53:04.154344Z","shell.execute_reply":"2021-09-07T22:53:04.192766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Post-process\n\nUsing some rules I made after exploring the training set answers here: https://www.kaggle.com/nbroad/chaii-qa-character-token-languages-eda","metadata":{}},{"cell_type":"code","source":"# bad_starts = [\" \", \"\\n\", \"\\t\", \".\",  \")\",  \"]\", \"-\", \"–\",  \",\", \";\", \"@\", \"#\", \"?\", \"!\", \"^\", \"&\", \"*\"]\n# bad_endings = [\" \", \"\\n\", \"\\t\", \"-\", \"(\", \"[\", \"–\", \",\", \";\", \"@\", \"#\", \"?\", \"!\", \"$\", \"%\", \"^\", \"&\", \"*\"]\n\nbad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"–\",  \",\", \";\"]\nbad_endings = [\"-\", \"(\", \")\", \"–\", \",\", \";\"]\n\ntamil_ad = \"கி.பி\"\ntamil_bc = \"கி.மு\"\ntamil_km = \"கி.மீ\"\nhindi_ad = \"ई\"\nhindi_bc = \"ई.पू\"\n\ncleaned_preds = []\nfor pred, context in test_df[[\"PredictionString\", \"context\"]].to_numpy():\n    if pred == \"\":\n        cleaned_preds.append(pred)\n        continue\n     \n    # I haven't check sure if this makes a difference, but there is one answer in the training set that ends like this and I think it is an annotator mistake\n    # see my notebook here for details https://www.kaggle.com/nbroad/chaii-qa-character-token-languages-eda \n    if pred.endswith(\"...\"):\n        pred = pred[:-3]\n    \n    pred = pred.lstrip(\"\".join(bad_starts))\n    pred = pred.rstrip(\"\".join(bad_endings))\n    \n    if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n        pred = pred+\".\"\n    \n\n    cleaned_preds.append(pred)\n\ntest_df[\"PredictionString\"] = cleaned_preds","metadata":{"execution":{"iopub.status.busy":"2021-09-07T22:53:07.320646Z","iopub.execute_input":"2021-09-07T22:53:07.321092Z","iopub.status.idle":"2021-09-07T22:53:07.345727Z","shell.execute_reply.started":"2021-09-07T22:53:07.321052Z","shell.execute_reply":"2021-09-07T22:53:07.344919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[['id', 'PredictionString']].to_csv('submission.csv', index=False)\n\nprint(test_df[['id', 'PredictionString']])","metadata":{"execution":{"iopub.status.busy":"2021-09-07T22:53:08.795312Z","iopub.execute_input":"2021-09-07T22:53:08.795635Z","iopub.status.idle":"2021-09-07T22:53:08.811217Z","shell.execute_reply.started":"2021-09-07T22:53:08.795605Z","shell.execute_reply":"2021-09-07T22:53:08.810262Z"},"trusted":true},"execution_count":null,"outputs":[]}]}