{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall fsspec -qq -y\n!pip install --no-index --find-links ../input/hf-datasets/wheels datasets -qq\n!pip install -U --no-build-isolation --no-deps ../input/transformers-master/\n","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:44:12.752471Z","iopub.execute_input":"2021-11-13T16:44:12.752882Z","iopub.status.idle":"2021-11-13T16:44:55.550744Z","shell.execute_reply.started":"2021-11-13T16:44:12.752778Z","shell.execute_reply":"2021-11-13T16:44:55.549821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"!export CUDA_HOME=/usr/local/cuda-11.0\n!env | grep CUDA","metadata":{"execution":{"iopub.status.busy":"2021-10-27T19:29:15.537438Z","iopub.execute_input":"2021-10-27T19:29:15.537722Z","iopub.status.idle":"2021-10-27T19:29:16.869568Z","shell.execute_reply.started":"2021-10-27T19:29:15.537683Z","shell.execute_reply":"2021-10-27T19:29:16.868781Z"}}},{"cell_type":"markdown","source":"!cd ../input/apex-master-10-27-2021/apex-master/ && pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-27T19:29:16.871155Z","iopub.execute_input":"2021-10-27T19:29:16.871463Z"}}},{"cell_type":"code","source":"APEX_INSTALLED = False\nprint(APEX_INSTALLED)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:44:55.553706Z","iopub.execute_input":"2021-11-13T16:44:55.554028Z","iopub.status.idle":"2021-11-13T16:44:55.563432Z","shell.execute_reply.started":"2021-11-13T16:44:55.553987Z","shell.execute_reply":"2021-11-13T16:44:55.561484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/tez-lib/\")\nfrom collections import Counter\nimport tez \nfrom functools import partial\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nimport gc\ngc.enable()\nimport math\nimport json\nimport time\nimport random\nimport multiprocessing\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm, trange\nfrom sklearn import model_selection\nfrom string import punctuation\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nimport torch.optim as optim\nfrom torch.utils.data import (\n    Dataset, DataLoader,\n    SequentialSampler, RandomSampler\n)\nfrom torch.utils.data.distributed import DistributedSampler\nimport sys \nfrom datasets import Dataset as HFDataset\nfrom transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\nfrom transformers import default_data_collator\n\nimport transformers\nfrom transformers import (\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    logging,\n    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:44:55.564833Z","iopub.execute_input":"2021-11-13T16:44:55.565303Z","iopub.status.idle":"2021-11-13T16:45:02.641566Z","shell.execute_reply.started":"2021-11-13T16:44:55.565261Z","shell.execute_reply":"2021-11-13T16:45:02.640757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fix_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef optimal_num_of_loader_workers():\n    num_cpus = multiprocessing.cpu_count()\n    num_gpus = torch.cuda.device_count()\n    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n    return optimal_value\n\n#print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-09-25T15:05:01.923913Z\",\"iopub.execute_input\":\"2021-09-25T15:05:01.924388Z\",\"iopub.status.idle\":\"2021-09-25T15:05:01.936163Z\",\"shell.execute_reply.started\":\"2021-09-25T15:05:01.924353Z\",\"shell.execute_reply\":\"2021-09-25T15:05:01.935458Z\"}}\nclass DatasetRetriever(Dataset):\n    def __init__(self, features, mode='train'):\n        super(DatasetRetriever, self).__init__()\n        self.features = features\n        self.mode = mode\n        \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, item):   \n        feature = self.features[item]\n        if self.mode == 'train':\n            return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n            }\n        else:\n            return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':feature['offset_mapping'],\n                'sequence_ids':feature['sequence_ids'],\n                'id':feature['example_id'],\n                'context': feature['context'],\n                'question': feature['question']\n            }\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-09-25T15:05:01.938192Z\",\"iopub.execute_input\":\"2021-09-25T15:05:01.938481Z\",\"iopub.status.idle\":\"2021-09-25T15:05:01.950199Z\",\"shell.execute_reply.started\":\"2021-09-25T15:05:01.938449Z\",\"shell.execute_reply\":\"2021-09-25T15:05:01.949435Z\"}}\nclass Model(nn.Module):\n    def __init__(self, modelname_or_path, config):\n        super(Model, self).__init__()\n        self.config = config\n        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self._init_weights(self.qa_outputs)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n\n    def forward(\n        self, \n        input_ids, \n        attention_mask=None, \n    ):\n        outputs = self.xlm_roberta(\n            input_ids,\n            attention_mask=attention_mask,\n        )\n\n        sequence_output = outputs[0]\n        #pooled_output = outputs[1]\n        \n        # sequence_output = self.dropout(sequence_output)\n        qa_logits = self.qa_outputs(sequence_output)\n        \n        start_logits, end_logits = qa_logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n    \n        return start_logits, end_logits\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-09-25T15:05:01.951716Z\",\"iopub.execute_input\":\"2021-09-25T15:05:01.951979Z\",\"iopub.status.idle\":\"2021-09-25T15:05:01.96162Z\",\"shell.execute_reply.started\":\"2021-09-25T15:05:01.951948Z\",\"shell.execute_reply\":\"2021-09-25T15:05:01.96087Z\"}}\ndef make_model(config):\n    model_config = AutoConfig.from_pretrained(config.config_name)\n    tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n    model = Model(config.tokenizer_name, config=model_config)\n    return config, tokenizer, model\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-09-25T15:05:01.962756Z\",\"iopub.execute_input\":\"2021-09-25T15:05:01.963103Z\",\"iopub.status.idle\":\"2021-09-25T15:05:01.975451Z\",\"shell.execute_reply.started\":\"2021-09-25T15:05:01.963068Z\",\"shell.execute_reply\":\"2021-09-25T15:05:01.97473Z\"}}\ndef prepare_test_features(args, example, tokenizer):\n    example[\"question\"] = example[\"question\"].lstrip()\n    \n    tokenized_example = tokenizer(\n        example[\"question\"],\n        example[\"context\"],\n        truncation=\"only_second\",\n        max_length=args.max_seq_length,\n        stride=args.doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    features = []\n    for i in range(len(tokenized_example[\"input_ids\"])):\n        feature = {}\n        feature[\"example_id\"] = example['id']\n        feature['context'] = example['context']\n        feature['question'] = example['question']\n        feature['input_ids'] = tokenized_example['input_ids'][i]\n        feature['attention_mask'] = tokenized_example['attention_mask'][i]\n        feature['offset_mapping'] = tokenized_example['offset_mapping'][i]\n        feature['sequence_ids'] = [0 if i is None else i for i in tokenized_example.sequence_ids(i)]\n        features.append(feature)\n    return features\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-09-25T15:05:01.978299Z\",\"iopub.execute_input\":\"2021-09-25T15:05:01.97853Z\",\"iopub.status.idle\":\"2021-09-25T15:05:01.996283Z\",\"shell.execute_reply.started\":\"2021-09-25T15:05:01.978506Z\",\"shell.execute_reply\":\"2021-09-25T15:05:01.995601Z\"}}\nimport collections\n\ndef postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30, tokenizer=None):\n    all_start_logits, all_end_logits = raw_predictions\n    \n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    \n    #predictions = collections.OrderedDict()\n    predictions1 = []\n    predictions2 = []\n    predictions3 = []\n    \n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    for example_index, example in examples.iterrows():\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None\n        valid_answers = []\n        \n        context = example[\"context\"]\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n\n            sequence_ids = features[feature_index][\"sequence_ids\"]\n            context_index = 1\n\n            features[feature_index][\"offset_mapping\"] = [\n                (o if sequence_ids[k] == context_index else None)\n                for k, o in enumerate(features[feature_index][\"offset_mapping\"])\n            ]\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    #if ')' in context[start_char: end_char] or '(' in context[start_char: end_char] or '...' in context[start_char: end_char]:\n                    #    continue\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            answer_candidates = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)\n            best_answer1 = answer_candidates[0][\"text\"]\n            try:\n                best_answer2 = answer_candidates[1][\"text\"]\n            except: \n                best_answer2 = ''\n            try:\n                best_answer3 = answer_candidates[2][\"text\"]\n            except: \n                best_answer3 = ''   \n        else:\n            best_answer1 = best_answer2 = best_answer3 = '' #{\"text\": \"\", \"score\": 0.0}\n            \n        # Multi white space removal\n        best_answer1 = \" \".join(best_answer1.split())\n        # Punc removal \n        best_answer1 = best_answer1.strip(punctuation)\n        predictions1.append(best_answer1)\n        \n        # Multi white space removal\n        best_answer2 = \" \".join(best_answer2.split())\n        # Punc removal \n        best_answer2 = best_answer2.strip(punctuation)\n        predictions2.append(best_answer2)\n        \n        \"\"\"\n        # Multi white space removal\n        best_answer3 = \" \".join(best_answer3.split())\n        # Punc removal \n        best_answer3 = best_answer3.strip(punctuation)\n        predictions3.append(best_answer3)\n        \"\"\"\n        \n    return predictions1, predictions2 #, predictions3\n\n\n\ndef get_predictions(global_config, checkpoint_path, test_dataloader):\n    config, tokenizer, model = make_model(global_config)\n    model.cuda()\n    model.load_state_dict(torch.load(checkpoint_path)) #, map_location='cuda:0'))\n    model.half()\n    model.eval()\n    print(f'Running inference for model: {checkpoint_path}') \n    start_logits = []\n    end_logits = []\n    for batch in test_dataloader:\n        with torch.no_grad():\n            outputs_start, outputs_end = model(batch['input_ids'].cuda(), batch['attention_mask'].cuda())\n            start_logits.append(outputs_start.cpu().numpy().tolist())\n            end_logits.append(outputs_end.cpu().numpy().tolist())\n            del outputs_start, outputs_end\n    del model, tokenizer, config\n    gc.collect()\n    return np.vstack(start_logits), np.vstack(end_logits)\n\n\n\nclass ChaiiModel(tez.Model):\n    def __init__(self, model_name, num_train_steps, steps_per_epoch, learning_rate):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.steps_per_epoch = steps_per_epoch\n        self.model_name = model_name\n        self.num_train_steps = num_train_steps\n        self.step_scheduler_after = \"batch\"\n\n        hidden_dropout_prob: float = 0.0\n        layer_norm_eps: float = 1e-7\n\n        config = transformers.AutoConfig.from_pretrained(model_name)\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.transformer = transformers.AutoModel.from_pretrained(model_name, config=config)\n        self.output = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, ids, mask, token_type_ids=None, start_positions=None, end_positions=None):\n        transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out[0]\n        logits = self.output(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n        return (start_logits, end_logits), 0, {}\n\n\nclass ChaiiDataset:\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, item):\n        return {\n            \"ids\": torch.tensor(self.data[item][\"input_ids\"], dtype=torch.long),\n            \"mask\": torch.tensor(self.data[item][\"attention_mask\"], dtype=torch.long),\n        }\n\n\ndef get_predictions_tez(model_path, data_loader):\n    print(f'Running inference for model {model_path}')\n    model = ChaiiModel(model_name=\"../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2\", num_train_steps=0, steps_per_epoch=0, learning_rate=0)\n    model.load(model_path, weights_only=True)\n    model.half()\n    model.to(\"cuda\")\n    model.eval()\n\n    start_logits = []\n    end_logits = []\n\n    for b_idx, data in enumerate(data_loader):\n        with torch.no_grad():\n            for key, value in data.items():\n                data[key] = value.to(\"cuda\")\n            output, _, _ = model(**data)\n            start = output[0].detach().cpu().numpy()\n            end = output[1].detach().cpu().numpy()\n            start_logits.append(start)\n            end_logits.append(end)\n\n    start_logits = np.vstack(start_logits)\n    end_logits = np.vstack(end_logits)\n\n    return start_logits, end_logits\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:45:02.643415Z","iopub.execute_input":"2021-11-13T16:45:02.643675Z","iopub.status.idle":"2021-11-13T16:45:02.715732Z","shell.execute_reply.started":"2021-11-13T16:45:02.643637Z","shell.execute_reply":"2021-11-13T16:45:02.71497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Post Processing","metadata":{}},{"cell_type":"code","source":"def clean_pred(predictions):\n  \n    bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"–\",  \",\", \";\"]\n    bad_endings = [\"...\", \"-\", \"(\", \")\", \"–\", \",\", \";\"]\n\n    tamil_ad = \"கி.பி\"\n    tamil_bc = \"கி.மு\"\n    tamil_km = \"கி.மீ\"\n    hindi_ad = \"ई\"\n    hindi_bc = \"ई.पू\"\n    hindi_ad1 = \"ए.डी\"\n\n    cleaned_preds = []\n    for pred in predictions:\n        while any([pred.startswith(y) for y in bad_starts]):\n            pred = pred[1:]\n            \n        while any([pred.endswith(y) for y in bad_endings]):\n            if pred.endswith(\"...\"):\n                pred = pred[:-3]\n            else:\n                pred = pred[:-1]\n\n            if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_ad1), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n                pred = pred+\".\"\n        \n        cleaned_preds.append(pred)\n\n    return cleaned_preds","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:45:02.718711Z","iopub.execute_input":"2021-11-13T16:45:02.719064Z","iopub.status.idle":"2021-11-13T16:45:02.728689Z","shell.execute_reply.started":"2021-11-13T16:45:02.719033Z","shell.execute_reply":"2021-11-13T16:45:02.727935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Voting Function","metadata":{}},{"cell_type":"code","source":"model1 = '../input/rembert-pt'\nmodel2 = model3 = '../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2'\nmodel4 = '../input/muril-large-pt/muril-large-cased'\nmodel_archs = [model1, model2, model3, model4]\n\ntokenizers = [\n                 AutoTokenizer.from_pretrained(model1),\n                 AutoTokenizer.from_pretrained(model2),\n                 AutoTokenizer.from_pretrained(model3),\n                 AutoTokenizer.from_pretrained(model4),\n                ]\n    \n\n    \ntk_rembert_base = '../input/d/trushk/chaii-rembert-1024-all/'\ntk_rembert_trial = [\n                tk_rembert_base+'checkpoint-fold-0',\n                tk_rembert_base+'checkpoint-fold-1',\n                #rembert_base+'checkpoint-fold-2',\n                tk_rembert_base+'checkpoint-fold-3',\n                tk_rembert_base+'checkpoint-fold-4'\n                ]\n\nrembert_base = '../input/rembert-5fold-1-epoch/rembert-5fold-1-epoch/'\nrembert_best = [\n                rembert_base+'checkpoint-fold-0',\n                rembert_base+'checkpoint-fold-2',\n                rembert_base+'checkpoint-fold-3',\n                rembert_base+'checkpoint-fold-4'\n                ]\n\nrembert_trial = [\n                rembert_base+'checkpoint-fold-2',\n                tk_rembert_base+'checkpoint-fold-3',\n                tk_rembert_base+'checkpoint-fold-4',\n                rembert_base+'checkpoint-fold-0',\n                ]\n\n\nxlmr_best =  ['../input/5foldsroberta/output/checkpoint-fold-0','../input/5foldsroberta/output/checkpoint-fold-1',\n                   '../input/5foldsroberta/output/checkpoint-fold-2', '../input/5foldsroberta/output/checkpoint-fold-3',\n                   '../input/5foldsroberta/output/checkpoint-fold-4'\n                  ]\n\nxlmr_base = '../input/public-xlm-2pochs/'\nxlmr_trial = [\n    xlmr_base + 'checkpoint-fold-0',\n    xlmr_base + 'checkpoint-fold-1',\n    xlmr_base + 'checkpoint-fold-2',\n    xlmr_base + 'checkpoint-fold-3',\n    xlmr_base + 'checkpoint-fold-4',\n\n]\n\ntez_base = '../input/xlrm-large-tez-1015-10fold/'\ntez_models = [ tez_base + 'xlm-roberta-large-squad2_fold_0.bin', tez_base + 'xlm-roberta-large-squad2_fold_1.bin',\n              tez_base + 'xlm-roberta-large-squad2_fold_2.bin', tez_base + 'xlm-roberta-large-squad2_fold_3.bin',\n              tez_base + 'xlm-roberta-large-squad2_fold_4.bin', tez_base + 'xlm-roberta-large-squad2_fold_5.bin',\n              tez_base + 'xlm-roberta-large-squad2_fold_6.bin', tez_base + 'xlm-roberta-large-squad2_fold_7.bin',\n              tez_base + 'xlm-roberta-large-squad2_fold_8.bin', tez_base + 'xlm-roberta-large-squad2_fold_9.bin',\n             ]\n    \ntez_best = [ \n              tez_base + 'xlm-roberta-large-squad2_fold_7.bin',\n              tez_base + 'xlm-roberta-large-squad2_fold_0.bin',\n              tez_base + 'xlm-roberta-large-squad2_fold_9.bin',\n              #tez_base + 'xlm-roberta-large-squad2_fold_1.bin', \n              #tez_base + 'xlm-roberta-large-squad2_fold_4.bin',\n              #tez_base + 'xlm-roberta-large-squad2_fold_3.bin',\n              #tez_base + 'xlm-roberta-large-squad2_fold_8.bin',\n             ]\n\n\ntez_trial = [ \n              tez_base + 'xlm-roberta-large-squad2_fold_7.bin',\n              tez_base + 'xlm-roberta-large-squad2_fold_0.bin',\n              tez_base + 'xlm-roberta-large-squad2_fold_9.bin',\n              tez_base + 'xlm-roberta-large-squad2_fold_1.bin', \n              #tez_base + 'xlm-roberta-large-squad2_fold_6.bin', \n              #tez_base + 'xlm-roberta-large-squad2_fold_4.bin',\n              #tez_base + 'xlm-roberta-large-squad2_fold_3.bin',\n              #tez_base + 'xlm-roberta-large-squad2_fold_8.bin',\n             ]\n\nmuril_best = [\n    '../input/chaii-muril-large-1020/checkpoint-fold-0',\n    '../input/chaii-muril-large-1020/checkpoint-fold-1',  \n    #'../input/chaii-muril-large-1020/checkpoint-fold-4',  #Drop to replace new muril\n    '../input/chaii-muril-large-1020/checkpoint-fold-3',\n    '../input/chaii-muril-large-1111/checkpoint-fold-0',\n    #'../input/chaii-muril-large-1111/checkpoint-fold-3'  Untested\n                  ]\n\nmuril_trial = [\n    '../input/chaii-muril-large-1020/checkpoint-fold-0', # val loss 1.29339.  CV 0.72\n    '../input/chaii-muril-large-1020/checkpoint-fold-3',\n    '../input/chaii-muril-large-1111/checkpoint-fold-0',\n    '../input/rembert-new/output/checkpoint-fold-0', # labeled incorrectly. muril val loss 1.21\n   # '../input/chaii-muril-large-1020/checkpoint-fold-1',  #Drop for more time for sim voter or tez. \n                  ]\n    \nmodel_paths = [ \n                    rembert_best,\n                    xlmr_best,\n                    tez_trial,\n                    muril_best\n                 ]\n\n\"\"\"\n# For debug\nmodel_paths = [ \n                [rembert_base+'checkpoint-fold-3',],\n                [xlmr_base + 'checkpoint-fold-0'],\n                #['../input/5foldsroberta/output/checkpoint-fold-0'],\n                #[ tez_base + 'xlm-roberta-large-squad2_fold_0.bin'],\n                [tez_new_base + 'deepsetxlm-roberta-large-squad2__fold_0.bin'],\n                ['../input/chaii-muril-large-1020/checkpoint-fold-0'], \n              ]\n\"\"\"\n\ntest = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')        \ntest['context'] = test['context'].apply(lambda x: ' '.join(x.split()))\ntest['question'] = test['question'].apply(lambda x: ' '.join(x.split()))\n\nall_predictions1 = []\nall_predictions2 = []\n#all_predictions3 = []\n\n# Iterate through all models and get features, predictions\nfor i in range(len(model_archs)):\n        tokenizer = tokenizers[i]\n        \n        tez_index = 2\n        # Reuse for tez xlmr \n        if i != tez_index:\n             test_features = []\n        \n        if True:\n            class Config:\n                #model\n                model_arch = model_archs[i]\n                model_name_or_path = model_arch\n                config_name = model_arch\n                tokenizer_name = model_arch\n                fp16 = True if APEX_INSTALLED else False\n                fp16_opt_level = \"O1\"\n                gradient_accumulation_steps = 2\n                # tokenizer\n                max_seq_length = 400\n                doc_stride = 135\n                if 'rembert' in model_paths[i][0]:\n                    eval_batch_size = 128\n                else:\n                    eval_batch_size = 128\n                # optimzer\n                optimizer_type = 'AdamW'\n                learning_rate = 1e-5\n                weight_decay = 1e-2\n                seed = 2021\n\n            args = Config()\n            print(f'Batch size: {args.eval_batch_size}')\n            \n            if i==tez_index:\n                #test_features = [item for sublist in test_features for item in sublist]\n                test_dataloader = torch.utils.data.DataLoader(\n                     ChaiiDataset(test_features),\n                     batch_size= args.eval_batch_size,\n                     num_workers=optimal_num_of_loader_workers(),\n                     pin_memory=True,\n                     shuffle=False\n                )\n            else:\n                for _, row in test.iterrows():\n                    test_features += prepare_test_features(Config(), row, tokenizer)                \n                test_dataset = DatasetRetriever(test_features, mode='test')\n                test_dataloader = DataLoader(\n                    test_dataset,\n                    batch_size=args.eval_batch_size,\n                    sampler=SequentialSampler(test_dataset),\n                    num_workers=optimal_num_of_loader_workers(),\n                    pin_memory=True,\n                    drop_last=False\n                )\n                \n        print(f'Running inference for model {model_archs[i]}')\n        test_shape = test.shape[0]\n                  \n        all_start_logits = []\n        all_end_logits = []\n        fin_start_logits  = None\n        for j in range(len(model_paths[i])):\n            if 'tez' in model_paths[i][0]:\n                start_logits, end_logits = get_predictions_tez(f'{model_paths[i][j]}', test_dataloader)\n            else:\n                start_logits, end_logits = get_predictions(Config(), f'{model_paths[i][j]}/pytorch_model.bin', test_dataloader)\n            if fin_start_logits is None:\n                fin_start_logits = start_logits\n                fin_end_logits = end_logits\n            else:\n                fin_start_logits += start_logits\n                fin_end_logits += end_logits\n            gc.collect()\n            torch.cuda.empty_cache()\n            start_logits = fin_start_logits / len(model_paths[i])\n            end_logits = fin_end_logits / len(model_paths[i])\n        \n        raw_predictions1, raw_predictions2 = postprocess_qa_predictions(test, test_features, (start_logits, end_logits), tokenizer=tokenizer)\n\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        predictions1 = [pred.strip() for pred in raw_predictions1]\n        cleaned_predictions1 = clean_pred(predictions1)\n        all_predictions1.append(cleaned_predictions1)\n        \n        predictions2 = [pred.strip() for pred in raw_predictions2]\n        cleaned_predictions2 = clean_pred(predictions2)\n        all_predictions2.append(cleaned_predictions2)\n        \n        \"\"\"\n        predictions3 = [pred.strip() for pred in raw_predictions3]\n        cleaned_predictions3 = clean_pred(predictions3)\n        all_predictions3.append(cleaned_predictions3)\n        \"\"\"\n        if test_shape == 5:\n            print(predictions1)\n            print(predictions2)\n            #print(predictions3)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:45:02.731411Z","iopub.execute_input":"2021-11-13T16:45:02.732087Z","iopub.status.idle":"2021-11-13T16:48:12.527195Z","shell.execute_reply.started":"2021-11-13T16:45:02.732045Z","shell.execute_reply":"2021-11-13T16:48:12.526375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Voting Serial\n","metadata":{}},{"cell_type":"code","source":"from Levenshtein import ratio\n\ndef calc_jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    jac = float(len(c)) / (len(a) + len(b) - len(c))\n    print(f'str1: {str1} str2:{str2}, jac:{jac}')\n    return jac\n\ndef check_lev(str1, str2, verbose=False):\n    l_ratio = ratio(str1,str2)\n    if verbose:\n        print(f'str1: {str1} str2:{str2}, lev:{l_ratio}')\n    return l_ratio","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:48:12.52877Z","iopub.execute_input":"2021-11-13T16:48:12.529227Z","iopub.status.idle":"2021-11-13T16:48:12.578023Z","shell.execute_reply.started":"2021-11-13T16:48:12.529188Z","shell.execute_reply":"2021-11-13T16:48:12.577316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Change vote count if votes are similar","metadata":{}},{"cell_type":"code","source":"def check_similarity(votes, verbose=False):\n    if verbose:\n        print(f'votes {votes}')\n    counter_len = len(votes)\n    adder=0\n    best_pred = votes[0][0]\n    best_vote_count = votes[0][1]\n    # If more than one vote find second best\n    if counter_len > 1:\n        second_best_pred = votes[1][0]\n        second_best_vote_count = votes[1][1]\n    else:\n        second_best_pred = None\n        second_best_vote_count = 0\n    # If more than 2 votes, check 2nd best vs rest\n    if counter_len > 2:\n        for i in range(2, counter_len):\n            score = check_lev(votes[1][0], votes[i][0], verbose=verbose)\n            if score >= 0.3:\n                adder +=1\n    \n    if adder > 0:\n        second_best_vote_count += adder\n        if verbose:\n            print(f'Increasing vote by {adder}: New second best {second_best_vote_count}')\n        if second_best_vote_count > best_vote_count:\n            return second_best_pred\n        else:\n            return best_pred\n    else:\n        return best_pred\n","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:48:12.579563Z","iopub.execute_input":"2021-11-13T16:48:12.579819Z","iopub.status.idle":"2021-11-13T16:48:12.587952Z","shell.execute_reply.started":"2021-11-13T16:48:12.579782Z","shell.execute_reply":"2021-11-13T16:48:12.5872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Iterate through all predictions and get predictions with max votes\nfinal_predictions = []\nfor i in range(len(test)):\n        # Vote top 1 \n        predictions = [all_predictions1[0][i]] + [all_predictions1[1][i]]  + [all_predictions1[2][i]]  + [all_predictions1[3][i]]\n        # Vote top 2\n        #predictions = [all_predictions1[0][i]] + [all_predictions1[1][i]]  + [all_predictions1[2][i]]  + [all_predictions1[3][i]] + [all_predictions2[0][i]] + [all_predictions2[1][i]]  + [all_predictions2[2][i]]  + [all_predictions2[3][i]]\n        #predictions = [all_predictions1[0][i]] + [all_predictions1[1][i]]  + [all_predictions2[0][i]] + [all_predictions2[1][i]] \n        x = Counter(predictions)\n        if test_shape == 5:\n            print(f'Most common: {x.most_common(1)[0]}')\n            print(f'All predictions {x}')\n        prediction, vote_count = x.most_common(1)[0]\n        # Select majority \n        if vote_count > 1:\n            final_pred = prediction\n        # If no majority \n        else:\n            #final_pred = calc_levenshtein_pred(all_predictions[0][i], all_predictions[2][i], all_predictions[1][i])\n            # Vote top 2\n            predictions = [all_predictions1[0][i]] + [all_predictions1[1][i]]  + [all_predictions1[2][i]]  + [all_predictions1[3][i]] + [all_predictions2[0][i]] + [all_predictions2[1][i]]  + [all_predictions2[2][i]]  + [all_predictions2[3][i]]\n            x = Counter(predictions)\n            if test_shape == 5:\n                print(f'Most common: {x.most_common(1)[0]}')\n                print(f'All predictions {x}')\n            votes = x.most_common()\n            # Max vote\n            prediction, vote_count = votes[0]\n            if len(votes) > 1 :\n                _, vote_count2 = votes[1]\n                if vote_count > vote_count2:\n                    final_pred = prediction\n                else:\n                    if test_shape == 5:\n                        verbose= True\n                    else:\n                        verbose = False\n                    final_pred = check_similarity(votes, verbose=verbose)\n            else:\n                # lb 792 reference\n                final_pred = all_predictions1[1][i]\n            if test_shape == 5:\n                print(f'Max votes for {final_pred} with {vote_count} num votes')\n        final_predictions.append(final_pred)\n\n\nprint(final_predictions)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:48:12.589492Z","iopub.execute_input":"2021-11-13T16:48:12.590088Z","iopub.status.idle":"2021-11-13T16:48:12.607347Z","shell.execute_reply.started":"2021-11-13T16:48:12.59005Z","shell.execute_reply":"2021-11-13T16:48:12.606555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results","metadata":{}},{"cell_type":"code","source":"cleaned_predictions = final_predictions\ntest_data = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\ntest_data[\"PredictionString\"] = cleaned_predictions\ntest_data[['id', 'PredictionString']].to_csv('submission.csv', index=False)\nprint(test_data[\"PredictionString\"])","metadata":{"execution":{"iopub.status.busy":"2021-11-13T16:48:12.608666Z","iopub.execute_input":"2021-11-13T16:48:12.608963Z","iopub.status.idle":"2021-11-13T16:48:12.646332Z","shell.execute_reply.started":"2021-11-13T16:48:12.608927Z","shell.execute_reply":"2021-11-13T16:48:12.6456Z"},"trusted":true},"execution_count":null,"outputs":[]}]}