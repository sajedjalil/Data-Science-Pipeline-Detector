{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Covid-19 ML forecasting with Linear Regression and ARIMA"},{"metadata":{},"cell_type":"markdown","source":"# 1. Introduction"},{"metadata":{},"cell_type":"markdown","source":"In this notebook we will analyze the globally spreading and development of Covid-19 which was first discovered in December 2019 in Wuhan. As of 12 April 2020, more than 1.2 million cases have been reported in 210 countries. We will try to forecast the development from 15 April 2020 to 15 May 2020 which is part of an ongoing Kaggle competition. I think it is very important to mention the idea behind this which was very well explained from my point of view by the Kaggle team:\n\n> We understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold.\n\nIn the first part of the notebook, we will explore the data in terms of information and quality. After we will clean the data and go on with feature engineering. In the third section we will have a look at different prediction methods, namely linear regression and arima. At the end, we will have a short stop and a look back before we create our submission file for the competetion itself. Even though it is a very serious topic, I hope we learn something together. Stay healthy!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nplt.style.use('fivethirtyeight')\nfrom sklearn import preprocessing\nfrom xgboost import XGBRegressor\nle = preprocessing.LabelEncoder()\nfrom sklearn import linear_model\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.io as pio\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n#pio.write_html(fig, file=\"index.html\", auto_open=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\ntest_df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\nsubmission = pd.read_csv(\"../input/covid19-global-forecasting-week-4/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Explorative data analysis\n## 2.1 Confirmed Cases"},{"metadata":{},"cell_type":"markdown","source":"Before we start, let's take a quick look at the data structure. We have two data sets. The training data set contains six columns:\n* ID: Unique identifier\n* Province_state: Provinces and states of a specific country, e.g. Washington in the United States\n* Country_region: Countries as Germany, France or Spain\n* Date: Datestamp for the respective row\n* ConfirmedCases: The number of confirmed cases of Covid-19\n* Fatalities: The number of registered deaths by Covid-19"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(train_df.isnull().sum()/len(train_df)*100)\ndisplay(test_df.isnull().sum()/len(test_df)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The lowest date in the train data set is {} and the highest {}.\".format(train_df['Date'].min(),train_df['Date'].max()))\nprint(\"The lowest date in the test data set is {} and the highest {}.\".format(test_df['Date'].min(),test_df['Date'].max()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see we have a lot of missings in the Province_State column and there is an overlapping time period in the test and train data set. Our first task is to fix both issues."},{"metadata":{"trusted":true},"cell_type":"code","source":"#just some cosmetic renaming\ntrain_df.rename(columns={'Province_State':'State','Country_Region':'Country'}, inplace=True)\ntest_df.rename(columns={'Province_State':'State','Country_Region':'Country'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function for replacing all the missings in the state column\ndef missings(state, country):\n    return country if pd.isna(state) == True else state","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#if there are no states specified for a country, the missing is replaced with the country´s name\ntrain_df['State'] = train_df.apply(lambda x: missings(x['State'],x['Country']),axis=1)\ntest_df['State'] = test_df.apply(lambda x: missings(x['State'],x['Country']),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"In our data set are {} countries and {} states.\".format(train_df['Country'].nunique(),train_df['State'].nunique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_confirmedcases = train_df.groupby(['Country','State']).max().groupby('Country').sum().sort_values(by='ConfirmedCases', ascending=False).reset_index().drop(columns='Id')\ndf_confirmedcases[:20].set_index('Country').style.background_gradient(cmap='Oranges')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most Covid-19 cases are by far in the United States. It is followed by the four largest EU member states and China, the country where Covid-19 was first registered."},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\n\ncases = 10\n\ncountries = df_confirmedcases[:cases]['Country'].unique().tolist()\nplot = train_df.loc[(train_df['Country'].isin(countries))].groupby(['Date', 'Country', 'State']).max().groupby(['Date', 'Country']).sum().sort_values(by='ConfirmedCases', ascending=False).reset_index()\nplot2 = train_df.groupby(['Date'])['ConfirmedCases'].sum().reset_index()\n\nfig = px.bar(plot, x=\"Date\", y=\"ConfirmedCases\", color=\"Country\", barmode=\"stack\")\n\nfig.add_scatter(x=plot2['Date'], y=plot2['ConfirmedCases'],name='Global Trend') # Not what is desired - need a line \n\nfig.update_layout(title='Confirmed Cases - Top {} Countries - {}'.format(cases,train_df['Date'].max()))\nfig.show()\npio.write_html(fig, file=\"diagram_1.html\", auto_open=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the diagram above we plotted the amount of confirmed cases of top 10 countries and additional the global development line. An interesting observation is that these 10 countries account for a large part of global development (see global trend line). At first glance it looks as if the curve is flattening, which is an indication that we are leaving the phase of exponential growth."},{"metadata":{"trusted":true},"cell_type":"code","source":"from plotly.subplots import make_subplots\n\n\ndevelopment = train_df.groupby(['Date'])['ConfirmedCases'].sum().sort_values(ascending=False)[:50].reset_index()\ndevelopment['ConfirmedCases_Previous'] = development['ConfirmedCases'].shift(-1)\ndevelopment['Difference'] = development['ConfirmedCases'] - development['ConfirmedCases_Previous']\ndevelopment['Differece_Previous'] = development['Difference'].shift(-1)\ndevelopment['Increase_quota'] = development['Difference'] / development['Differece_Previous']\n\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\nfig.add_trace(go.Bar(x=development[\"Date\"], y=development[\"Difference\"], name=\"Absolute increase in cases\"))\nfig.add_scatter(x=development['Date'], y=development['Increase_quota'], name=\"Increase quota in %\", secondary_y=True)\n\nfig.update_layout(title='Absolute and relative Increase per Day')\n\nfig.update_yaxes(title_text=\"<b>Absolute</b> increase\", secondary_y=False)\nfig.update_yaxes(title_text=\"<b>Relative</b> increase in %\", secondary_y=True)\n\nfig.show()\npio.write_html(fig, file=\"diagram_2.html\", auto_open=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We created two new variables: difference and increase quota. The first variable contains the difference in absolute cases from t0 and t-1. The second variable, increase quota, is the quotient of the difference t0 and t-1. As we can see, both developments flatten out over time. This can have two reasons:\n- We are entering the **next phase** of the corona epidemic and leaving the exponential growth phase...\n- The **data quality is insufficient** and only a fraction of the actual infections are counted and statistically recorded\n\nTo get a better picture, we now examine the number of fatalities."},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Fatalities"},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\n\ncases = 10\n\ncountries = df_confirmedcases[:cases]['Country'].unique().tolist()\nplot = train_df.loc[(train_df['Country'].isin(countries))].groupby(['Date', 'Country', 'State']).max().groupby(['Date', 'Country']).sum().sort_values(by='Fatalities', ascending=False).reset_index()\nplot2 = train_df.groupby('Date')['Fatalities'].sum().sort_values(ascending=False).reset_index()\n\nfig = px.bar(plot, x=\"Date\", y=\"Fatalities\", color=\"Country\", barmode=\"stack\")\n\nfig.add_scatter(x=plot2['Date'], y=plot2['Fatalities'],name='Global Trend') # Not what is desired - need a line \n\nfig.update_layout(title='Fatalities - Top {} Countries - {}'.format(cases, train_df['Date'].max()))\nfig.show()\npio.write_html(fig, file=\"diagram_3.html\", auto_open=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Basically the picture is the same as for the confirmed cases. On April 12, 2020 there are more than 114,000 reported fatalities. The majority of the deaths come from the 10 states that we have seen before in the confirmed cases. At first sight the curve looks a bit steeper than in the confirmed cases, so we will compare them."},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\n\nplot = train_df.groupby('Date')['Fatalities'].sum().sort_values(ascending=False).reset_index()\n\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\nfig.add_trace(go.Bar(x=plot[\"Date\"], y=plot[\"Fatalities\"], name=\"Fatalities\"))\nfig.add_scatter(x=development['Date'], y=development['ConfirmedCases'], name=\"Confirmed Cases\", secondary_y=True)\n\nfig.update_layout(title='Confirmed Cases and Fatalities - {}'.format(train_df['Date'].max()))\n\nfig.update_yaxes(title_text=\"Confirmed Cases\", secondary_y=False)\nfig.update_yaxes(title_text=\"Fatalities\", secondary_y=True)\n\nfig.show()\npio.write_html(fig, file=\"diagram_4.html\", auto_open=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our impression is confirmed: The curve of the confirmed cases is recently flatter than that of the fatalities, but the difference is not enormous. If the growth of confirmed cases really does slow down, this development is plausible, since people do not die directly from corona, but later."},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Geographic Spread of Covid-19"},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\n\ndf_plot = train_df.loc[: , ['Date', 'Country', 'ConfirmedCases', 'Fatalities']].groupby(['Date', 'Country']).max().reset_index()\n\n#df_plot.loc[:, 'Date'] = df_plot.Date.dt.strftime(\"%Y-%m-%d\")\ndf_plot.loc[:, 'Size'] = np.power(df_plot[\"ConfirmedCases\"]+1,0.3)-1 #np.where(df_plot['Country'].isin(['China', 'Italy']), df_plot['ConfirmedCases'], df_plot['ConfirmedCases']*300)\n\nfig = px.scatter_geo(df_plot,\n                     locations=\"Country\",\n                     locationmode = \"country names\",\n                     hover_name=\"Country\",\n                     color=\"ConfirmedCases\",\n                     animation_frame=\"Date\", \n                     size='Size',\n                     projection=\"natural earth\",\n                     title=\"Global Spread of Covid-19\",\n                    width=1500, height=800)\nfig.layout.updatemenus[0].buttons[0].args[1][\"frame\"][\"duration\"] = 5\n\nfig.show()\n\n#pio.write_html(fig, file=\"diagram_5.html\", auto_open=True)\n#py.plot(fig, filename = 'diagram_5', auto_open=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The animation shows the spread of Covid-19 in the period from 01.01.2020 to 12.04.2020. Until 20.02. there were only single, smaller spots besides China, after that there was rapid growth first in Europe, then in the USA and finally in Africa. At present, almost every region in the world is affected by Covid 19."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 2.4 Conclusion"},{"metadata":{},"cell_type":"markdown","source":"Let us briefly summarize the key findings:\n* There are around 1,8 million confirmed cases and 114,000 fatalities worldwide. The top 10 countries in terms of confirmed cases and fatalities make up the majority.\n* According to the current data, the spread seems to be slowing down somewhat.\n* Almost every region in the world is affected by Covid-19\n\nHowever, experts assume that the number of unreported cases is significantly higher and only a fraction of the cases are recorded. Therefore, our observation may also be due to the fact that testing capacities in many countries are exhausted. We cannot answer this question with the available data material. For further investigations we have to assume that the available figures reflect the actual development."},{"metadata":{},"cell_type":"markdown","source":"# 3. Feature Engineering\n## 3.1 Country, State and Date\nIn this chapter we will prepare the data in such a way that we can then make forecasts using various statistical methods. For this purpose we will clean up the data and create features. Feature Engineerung is an very interesting factor in time series analysis. If we assume that we want to forecast the confirmed cases and fatalities, we will only have the states, provinces and the date as inputs for our models. This seems a little too little to extrapolate the data points in a meaningful way, but more on this later in this chapter. Let us first take a look at the training and test data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_leak = pd.merge(train_df,test_df, how='inner', on='Date')['Date'].unique().tolist()\ndata_leak.append('2020-04-01')\ndata_leak.sort()\nprint(\"Both data sets contain the following dates: {}\".format(data_leak))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we have already seen in the first chapter, there is an overlapping period of time. The rules for [this](https://www.kaggle.com/c/covid19-global-forecasting-week-4/overview/evaluation) competition state that only data prior to 2020-04-01 may be used. For this reason we delete the data from the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing overlapping dates from our trainings data set\ntrain_df_fix = train_df.loc[~train_df['Date'].isin(data_leak)]\ndf_all = pd.concat([train_df_fix, test_df], axis = 0, sort=False)\n\n#filling up the \"new\" NAs which were created by the concat process\ndf_all['ConfirmedCases'].fillna(0, inplace=True)\ndf_all['Fatalities'].fillna(0, inplace=True)\ndf_all['Id'].fillna(-999, inplace=True)\ndf_all['ForecastId'].fillna(-999, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have already noticed that we do not have many features available. Let's first make the date meaningful. Algorithms cannot read anything from the date, so we extract various features from it, such as the day and week. From the combination you could for example read weekly trends. In Germany I noticed that on Thursday and Friday on average more cases are reported and on Saturday and Sunday significantly less. This may have something to do with the reporting process: On Saturday and Sunday there are fewer people working in the relevant offices and not all cases are reported reliably.\n\nAdditionally we use SciKit´s LabelEncoder() for states and provinces, because most algorithms can't do anything with strings. The Label Encoder assigns a number to each different case in the respective column."},{"metadata":{"trusted":true},"cell_type":"code","source":"#year is commented out because it is the same for every case\ndef create_features(df):\n    df['Day_num'] = le.fit_transform(df['Date'])\n    df['Date'] = pd.to_datetime(df['Date'])\n    df['Day'] = df['Date'].dt.day\n    df['Week'] = df['Date'].dt.week\n    df['Month'] = df['Date'].dt.month\n    #df['Year'] = df['Date'].dt.year\n    df['DayOfWeek'] = df['Date'].dt.dayofweek\n    \n    df['Country'] = le.fit_transform(df['Country'])\n    country_dict = dict(zip(le.inverse_transform(df['Country']), df['Country'])) \n    \n    df['State'] = le.fit_transform(df['State'])\n    state_dict = dict(zip(le.inverse_transform(df['State']), df['State']))\n    \n    return df, country_dict, state_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all, country_dict, state_dict = create_features(df_all)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Confirmed Cases and Fatalities - Lags and Trends"},{"metadata":{},"cell_type":"markdown","source":"If we want to predict a data point in the future, currently only Country, State and the date are known. We have no information about the number of confirmed cases or fatalities in the future. However, we do have information about the number of cases in the past and we should use this information. In time series analysis we speak of lags when the previous development of the target (e.g. Confirmed Cases) is recorded as features in the data set.\n\nIn addition, trends can be recorded, i.e. the short-term development. This can be described for a trend between two consecutive data points as (t0 -t1)/ t1."},{"metadata":{"trusted":true},"cell_type":"code","source":"def lag_feature(df,target,lags):\n    for lag in lags:\n        lag_col = target + \"_{}\".format(lag)\n        df[lag_col] = df.groupby(['Country','State'])[target].shift(lag, fill_value=0)\n    return df\n\ndef trend_feature(df,target,trends):\n    for trend in trends:\n        trend_col = \"Trend_\" + target + \"_{}\".format(trend)\n        df[trend_col] = (df.groupby(['Country','State'])[target].shift(0, fill_value=0) - df.groupby(['Country','State'])[target].shift(trend, fill_value=0))/ df.groupby(['Country','State'])[target].shift(trend, fill_value=0.0001)\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Modelling"},{"metadata":{},"cell_type":"markdown","source":"Now we have all necessary functions and the data is prepared accordingly. We will try two different approaches: 1) linear regressions and 2) ARIMA. We will start with the linear regression. You can find more information about the implementation and parameters of linear regression in SkyCit [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n\n## 4.1 Linear Regression (not used for submission)\nUnlike other machine learning projects, we can’t just draw random samples via train-test-split function, because the data points are chronologically dependent on each other. Therefore we write our own function. In the training-dataset we have data available until 31.03.2020. This is the 69th day since 22.01.2020 (the first day in our data set). We will train our data until 31.03.2020 and then we will see how close our prediction is to the actual development. We will vary in the following:\n\n* the number of lags used\n* the period of time we include for the training\n\nA few notes on our linear regression function:\n* We train the algorithm for each country individually. If there are several states in a country, we also train for each of them separately. Since the states are in different phases of the Covid-19 pandemic, we will not be able to make good predictions with an algorithm that is trained on all data points.\n* In a first step we calculate the lags for the respective country or state. The right number of lags is obtained by trial and error. Too few lags lead to an overinterpretation of the short-term trend, too many lags mean that we do not take the short-term trend into account enough.\n* Then we logarithmise the targets and lags. A linear regression is not suitable for extrapolating exponential trends. Logarithms allow our algorithm to better interpret and process the data.\n\n**Note**: The use of lags creates a new problem. These are only available for our training data. So we have only logs available for the first prediction. After that we have to write this prediction back into the training data set, recalculate the lag and make a new prediction. This process must be repeated for each data point.\n\n**Credits**: Most of the code for the implementation comes from [this great notebook](https://www.kaggle.com/saga21/covid-global-forecast-sir-model-ml-regressions). I can also generally recommend reading it for theoretical assumptions about the further process. It helped me a lot to learn about forecasting."},{"metadata":{"trusted":true},"cell_type":"code","source":"#different trainings- and testsets for confirmed cases and fatalities\ndef train_test_split_extend(df,d,day,filter_col_confirmed,filter_col_fatalities):\n    \n    df=df.loc[df['Day_num'] >= day]\n    df_train = df.loc[df['Day_num'] < d]\n    X_train = df_train\n    \n    Y_train_1 = df_train['ConfirmedCases']\n    Y_train_2 = df_train['Fatalities']\n    \n    X_train_1 = X_train.drop(columns=filter_col_fatalities).drop(columns='ConfirmedCases')\n    X_train_2 = X_train.drop(columns=filter_col_confirmed).drop(columns='Fatalities')\n    \n    df_test = df.loc[df['Day_num'] == d]\n    x_test = df_test\n    \n    x_test_1 = x_test.drop(columns=filter_col_fatalities).drop(columns='ConfirmedCases')\n    x_test_2 = x_test.drop(columns=filter_col_confirmed).drop(columns='Fatalities')\n    \n    x_test.drop(['ConfirmedCases', 'Fatalities'], axis=1, inplace=True)\n    \n    X_train_1.drop('Id', inplace=True, errors='ignore', axis=1)\n    X_train_1.drop('ForecastId', inplace=True, errors='ignore', axis=1)\n    \n    X_train_2.drop('Id', inplace=True, errors='ignore', axis=1)\n    X_train_2.drop('ForecastId', inplace=True, errors='ignore', axis=1)\n    \n    x_test_1.drop('Id', inplace=True, errors='ignore', axis=1)\n    x_test_1.drop('ForecastId', inplace=True, errors='ignore', axis=1)\n    \n    x_test_2.drop('Id', inplace=True, errors='ignore', axis=1)\n    x_test_2.drop('ForecastId', inplace=True, errors='ignore', axis=1)\n    \n    return X_train_1, X_train_2, Y_train_1, Y_train_2, x_test_1, x_test_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lin_reg(X_train, Y_train, x_test):\n    regr = linear_model.LinearRegression()\n    regr.fit(X_train, Y_train)\n    pred = regr.predict(x_test)\n    return regr, pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def country_calculation(df_all,country,date,day):\n    df_country = df_all.copy()\n    df_country = df_country.loc[df_country['Date'] >= date]\n    df_country = df_country.loc[df_country['Country'] == country_dict[country]]\n    features = ['Id', 'State', 'Country','ConfirmedCases', 'Fatalities', 'Day_num']\n    df_country = df_country[features]\n    \n    # Lags\n    df_country = lag_feature(df_country, 'ConfirmedCases',range(1, 40))\n    df_country = lag_feature(df_country, 'Fatalities', range(1,20))\n\n    filter_col_confirmed = [col for col in df_country if col.startswith('Confirmed')]\n    filter_col_fatalities= [col for col in df_country if col.startswith('Fataliti')]\n    filter_col = np.append(filter_col_confirmed, filter_col_fatalities)\n    \n    # Apply log transformation\n    df_country[filter_col] = df_country[filter_col].apply(lambda x: np.log1p(x))\n    df_country.replace([np.inf, -np.inf], 0, inplace=True)\n    df_country.fillna(0, inplace=True) ####\n    \n    # Start/end of forecast\n    start = df_country[df_country['Id']==-999].Day_num.min()\n    end = df_country[df_country['Id']==-999].Day_num.max()\n    #\n    for d in range(start,end+1):\n   \n        X_train_1, X_train_2, Y_train_1, Y_train_2, x_test_1, x_test_2 = train_test_split_extend(df_country,d,day,filter_col_confirmed,filter_col_fatalities)\n        \n        regr_1, pred_1 = lin_reg(X_train_1, Y_train_1, x_test_1)\n        df_country.loc[(df_country['Day_num'] == d) & (df_country['Country'] == country_dict[country]), 'ConfirmedCases'] = pred_1[0]\n        \n        regr_2, pred_2 = lin_reg(X_train_2, Y_train_2, x_test_2)\n        df_country.loc[(df_country['Day_num'] == d) & (df_country['Country'] == country_dict[country]), 'Fatalities'] = pred_2[0]\n        \n        df_country = lag_feature(df_country, 'ConfirmedCases',range(1, 40))\n        df_country = lag_feature(df_country, 'Fatalities', range(1,20))\n\n        df_country.replace([np.inf, -np.inf], 0, inplace=True)\n        df_country.fillna(0, inplace=True)\n        \n    print(\"Calculation done.\")\n    return df_country","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def country_state_calculation(df_all,country, state, date,day):\n    df_country = df_all.copy()\n    df_country = df_country.loc[df_country['Date'] >= date]\n    df_country = df_country.loc[df_country['Country'] == country_dict[country] & (df_country['State']==state_dict[state])]\n    features = ['Id', 'State', 'Country','ConfirmedCases', 'Fatalities', 'Day_num']\n    df_country = df_country[features]\n    \n    # Lags\n    df_country = lag_feature(df_country, 'ConfirmedCases',range(1, 40))\n    df_country = lag_feature(df_country, 'Fatalities', range(1,20))\n\n    filter_col_confirmed = [col for col in df_country if col.startswith('Confirmed')]\n    filter_col_fatalities= [col for col in df_country if col.startswith('Fataliti')]\n    filter_col = np.append(filter_col_confirmed, filter_col_fatalities)\n        \n    # Apply log transformation\n    df_country[filter_col] = df_country[filter_col].apply(lambda x: np.log1p(x))\n    df_country.replace([np.inf, -np.inf], 0, inplace=True)\n    df_country.fillna(0, inplace=True) ####\n    \n    # Start/end of forecast\n    start = df_country[df_country['Id']==-999].Day_num.min()\n    end = df_country[df_country['Id']==-999].Day_num.max()\n    #\n    for d in range(start,end+1):\n        X_train_1, X_train_2, Y_train_1, Y_train_2, x_test_1, x_test_2 = train_test_split_extend(df_country,d,day,filter_col_confirmed,filter_col_fatalities)\n        \n        regr_1, pred_1 = lin_reg(X_train_1, Y_train_1, x_test_1)\n        df_country.loc[(df_country['Day_num'] == d) & (df_country['Country'] == country_dict[country]) & (df_country['State'] == state_dict[state]), 'ConfirmedCases'] = pred_1[0]\n        \n        regr_2, pred_2 = lin_reg(X_train_2, Y_train_2, x_test_2)\n        df_country.loc[(df_country['Day_num'] == d) & (df_country['Country'] == country_dict[country]), 'Fatalities'] = pred_2[0]\n        \n        df_country = lag_feature(df_country, 'ConfirmedCases',range(1, 10))\n        df_country = lag_feature(df_country, 'Fatalities', range(1,8))\n        \n        df_country.replace([np.inf, -np.inf], 0, inplace=True)\n        df_country.fillna(0, inplace=True)\n        \n    print(\"Calculation done.\")\n    return df_country","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_check(df_check,country):\n    crosscheck = train_df[(train_df['Country'] == country) & (train_df['Date'] >= '2020-03-10')].reset_index()\n    df_check = df_check[df_check['Day_num'] >= 48]\n    df_check2 = df_check[:len(crosscheck)].reset_index()\n    df_check2['CC_Crosscheck'] = crosscheck['ConfirmedCases']\n    df_check2['Fat_Crosscheck'] = crosscheck['Fatalities']\n    df_check2['ConfirmedCases_In'] = df_check2['ConfirmedCases']\n    df_check2['Fatalities_In'] = df_check2['Fatalities']\n    df_check2['ConfirmedCases_In'] = df_check2['ConfirmedCases'].apply(lambda x: np.expm1(x)).astype(int)\n    df_check2['Fatalities_In'] = df_check2['Fatalities'].apply(lambda x: np.expm1(x)).astype(int)\n    return df_check2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_plot(df_check2,test_con):\n\n    fig = go.Figure()\n    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n    fig.add_scatter(x=df_check2['Day_num'], y=df_check2['ConfirmedCases_In'], name='Confirmed Cases - Prediction')\n    fig.add_scatter(x=df_check2['Day_num'], y=df_check2['CC_Crosscheck'], name='Confirmed Cases - Official')\n\n    fig.add_scatter(x=df_check2['Day_num'], y=df_check2['Fatalities_In'], name='Fatalities - Prediction', secondary_y=True)\n    fig.add_scatter(x=df_check2['Day_num'], y=df_check2['Fat_Crosscheck'], name='Fatalities - Official', secondary_y=True)\n\n    fig.add_annotation(\n            x=69,\n            y=0,\n            xref=\"x\",\n            yref=\"y\",\n            text=\"Split of train- and test dataset\",\n            showarrow=True,\n            font=dict(\n                #family=\"Courier New, monospace\",\n                size=12,\n                color=\"#ffffff\"\n                ),\n            align=\"center\",\n            arrowhead=0,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor=\"#636363\",\n            ax=-0,\n            ay=-345,\n            bordercolor=\"#c7c7c7\",\n            borderwidth=2,\n            borderpad=4,\n            bgcolor=\"#ff7f0e\",\n            opacity=1\n            )\n\n    fig.update_layout(title=test_con + ' Comparison of Predicted and Real Number of Cases',\n                       xaxis_title='Number of Days since 2020-01-22',\n                       yaxis_title='Confirmed Cases')\n\n    fig.update_yaxes(title_text=\"Confirmed Cases\", secondary_y=False)\n    fig.update_yaxes(title_text=\"Fatalities\", secondary_y=True)\n    \n    #py.plot(fig, filename = test_con, auto_open=True)\n\n\n    return fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will not show the results for all the different parameters now. You can test them yourself in the linked notebook. In the following we will have a look at the results for the following parameters:\n* 40 Lags for ConfirmedCases\n* 20 Lags for Fatalities\n* Start of training date 10.03.2020\n\nThe Confirmed Cases are on the left Y-axis and the Fatalities on the right Y-axis."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_con = 'Germany'\n\ndf_check = country_calculation(df_all, test_con, '2020-03-10', 48)\ndf_check2 = plot_check(df_check,test_con)\ncheck_plot(df_check2,test_con)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_con = 'Spain'\n\ndf_check = country_calculation(df_all, test_con, '2020-03-10', 48)\ndf_check2 = plot_check(df_check,test_con)\ncheck_plot(df_check2,test_con)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_con = 'Algeria'\n\ndf_check = country_calculation(df_all, test_con, '2020-03-10', 48)\ndf_check2 = plot_check(df_check,test_con)\ncheck_plot(df_check2,test_con)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_con = 'Andorra'\n\ndf_check = country_calculation(df_all, test_con, '2020-03-10', 48)\ndf_check2 = plot_check(df_check,test_con)\ncheck_plot(df_check2,test_con)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Inputs\n##day_start = 39 \n##lag_size = 30\n#\n#date = '2020-03-10'\n#day = 48\n#\n#train3 = train_df.copy()\n##train3.Province_State.fillna(\"None\", inplace=True)\n#\n#results_df = pd.DataFrame()\n#\n#import time\n#tp = time.time()\n#\n## Main loop for countries\n#for country in train3['Country'].unique():\n#\n#    # List of provinces\n#    provinces_list = train3[train3['Country']==country]['State'].unique()\n#        \n#    # If the country has several Province/State informed\n#    if len(provinces_list)>1:\n#        for province_name in provinces_list:\n#            pred_province = country_state_calculation(df_all,country,province_name,date,day)\n#            results_df = pd.concat([results_df, pred_province])\n#\n#    else:\n#        pred_country = country_calculation(df_all,country,date,day)\n#        results_df = pd.concat([results_df, pred_country])\n#        \n#results_df_submit = results_df.copy()\n#results_df_submit['ConfirmedCases'] = results_df_submit['ConfirmedCases'].apply(lambda x: np.expm1(x))\n#results_df_submit['Fatalities'] = results_df_submit['Fatalities'].apply(lambda x: np.expm1(x))\n#        \n##get_submission(results_df_submit.loc[results_df_submit['ForecastId']!=-1], 'ConfirmedCases', 'Fatalities')\n#print(\"Complete process finished in \", time.time()-tp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##submission\n#real_result = results_df_submit[(results_df_submit['Id'] == -999)].replace([np.inf, -np.inf], 0)\n#real_result['ConfirmedCases'] = real_result['ConfirmedCases'].astype(int)\n#real_result['Fatalities'] = real_result['Fatalities'].astype(int)\n#real_result_cc = real_result['ConfirmedCases'].tolist()\n#real_result_f = real_result['Fatalities'].tolist()\n#submission = pd.DataFrame({'ForecastId':test_df['ForecastId'], 'ConfirmedCases':real_result_cc, 'Fatalities': real_result_f})\n#submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Linear Regression — Conclusion\nFor Germany and Spain, the forecast initially looks good, but it can be seen that the gap between the actual figures and the forecast is widening. We have also looked at Algeria, as a country with few cases so far. Here too, the forecast and actual figures are drifting apart. I chose Andorra because there are no cases here so far. The prognosis is completely absurd and would ruin the whole score for us at Kaggle. We could intercept this phenomenon by hard-coding that countries without confirmed cases will continue to be predicted at zero. Instead, we will continue with the ARIMA method."},{"metadata":{},"cell_type":"markdown","source":"## 4.3 ARIMA (used for submission)\nARIMA stands for AutoRegressive (AR) Integrated (I) Moving Average (MA). The provided data as input must be an univariate series, since ARIMA calculates future datapoints from the past. That is exactly what we were trying to do with linear regression as well. ARIMA basically has three important parameters:\n* p: The autoregressive part of the model. Simplified one can say that the model assumes that if there were many confirmed cases yesterday and the day before, there will be many confirmed cases today and tomorrow.\n* d -> The integrated part of the model that describes the amount of differentiation. If the available data are not stationary and contain trends, ARIMA can extract this seasonality.\n* q -> The moving average part of the model. By forming moving averages, random effects can be smoothed.\n\n**An important note**: ARIMA is not able to take any external factors into account. The data points are only extrapolated based on historical data. If there are bans of going out imposed overnight in a state, our model will not provide good forecasts.\n\n**Credits**: Most of the code for the implementation comes from [this great notebook](https://www.kaggle.com/hossein2015/covid-19-week-3-sarima-x-approach)."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyramid.arima\nfrom pyramid.arima import auto_arima","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the necessary libraris #\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport warnings\nwarnings.filterwarnings(action='ignore')\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.arima_model import ARIMA\n# Define the directory for the input files (train + test + submission) #\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def RMSLE(pred,actual):\n    return np.sqrt(np.mean(np.power((np.log(pred+1)-np.log(actual+1)),2)))\npd.set_option('mode.chained_assignment', None)\n# Import the train & test data for COVID-19 (Week 3) #\ntest = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\ntrain = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\n# Replace the missing values in the train & test data sets #\ntrain['Province_State'].fillna('', inplace=True)\ntest['Province_State'].fillna('', inplace=True)\n# Convert the \"Date\" Variable in the training & test sets #\ntrain['Date'] =  pd.to_datetime(train['Date'])\n#train =  train.loc[train['Date'] <= '2020-04-01']\ntest['Date'] =  pd.to_datetime(test['Date'])\n# Sort values in the training & test sets #\ntrain = train.sort_values(['Country_Region','Province_State','Date'])\ntest = test.sort_values(['Country_Region','Province_State','Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining key dates for reference purposes #\nfeature_day = [1,5,10,15,20,30,40,50,75,100,150,200,300,400,500,750,1000,1250,1500,1750,2000,2250,2500,2750,3000,3250,3500,3750,4000]\ndef CreateInput(data):\n    feature = []\n    for day in feature_day:\n        data.loc[:,'Number day from ' + str(day) + ' case'] = 0\n        if (train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['ConfirmedCases'] < day)]['Date'].count() > 0):\n            fromday = train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['ConfirmedCases'] < day)]['Date'].max()        \n        else:\n            fromday = train[(train['Country_Region'] == country) & (train['Province_State'] == province)]['Date'].min()       \n        for i in range(0, len(data)):\n            if (data['Date'].iloc[i] > fromday):\n                day_denta = data['Date'].iloc[i] - fromday\n                data['Number day from ' + str(day) + ' case'].iloc[i] = day_denta.days \n        feature = feature + ['Number day from ' + str(day) + ' case']\n    \n    return data[feature]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_data_all = pd.DataFrame()\nfor country in train['Country_Region'].unique():\n    for province in train[(train['Country_Region'] == country)]['Province_State'].unique():\n        print(country + ' and ' + province)\n        #create dataframe for a specific country\n        df_train = train[(train['Country_Region'] == country) & (train['Province_State'] == province)]\n        df_test = test[(test['Country_Region'] == country) & (test['Province_State'] == province)]\n        #create features -> number of cases on a specific date\n        X_train = CreateInput(df_train)\n        #last 12 confirmed cases in train data set\n        y_train_confirmed = df_train['ConfirmedCases'].ravel()\n        #last 12 confirmed fatalities in train data set\n        y_train_fatalities = df_train['Fatalities'].ravel()\n        #create features in test dataset-> number of cases on a specific date\n        X_pred = CreateInput(df_test)\n        #creates reversed list of the possible features\n        for day in sorted(feature_day,reverse = True):\n            #check for the column in the list\n            feature_use = 'Number day from ' + str(day) + ' case'\n            #check the 0-dimension of the array (similiar to length of a dataframe)\n            idx = X_train[X_train[feature_use] == 0].shape[0]     \n            #if there are more than 20 values for a column, the loop will be interruped\n            if (X_train[X_train[feature_use] > 0].shape[0] >= 20):\n                break\n                \n        #[TRAIN] - cuts the value of idx from the top of the dataframe; selects the input column (e.g. Number day from 1000 case); brings it into a horizontal array\n        adjusted_X_train = X_train[idx:][feature_use].values.reshape(-1, 1)\n        #[TRAIN] - get the respective confirmed cases\n        adjusted_y_train_confirmed = y_train_confirmed[idx:]\n        #[TRAIN] - get the respective fatalities\n        adjusted_y_train_fatalities = y_train_fatalities[idx:] #.values.reshape(-1, 1)\n        \n        #[TEST] - selects for the extracted feature column and get the length (0)\n        idx = X_pred[X_pred[feature_use] == 0].shape[0]\n        \n        #[TEST] - creates a clean array also for the prediction\n        adjusted_X_pred = X_pred[idx:][feature_use].values.reshape(-1, 1)\n        \n        #[TEST] - gets the extract from the test dataset for a specific country/ region\n        pred_data = test[(test['Country_Region'] == country) & (test['Province_State'] == province)]\n        \n        #latest date from the trainings data set and earliest date from the test data set\n        max_train_date = train[(train['Country_Region'] == country) & (train['Province_State'] == province)]['Date'].max()\n        min_test_date = pred_data['Date'].min()\n        \n        if len(adjusted_y_train_confirmed) < 1:\n            adjusted_y_train_confirmed = np.zeros(3)\n        else:\n            if len(adjusted_y_train_confirmed) < 2:\n                adjusted_y_train_confirmed = np.append(adjusted_y_train_confirmed,adjusted_y_train_confirmed[len(adjusted_y_train_confirmed)-1],adjusted_y_train_confirmed[len(adjusted_y_train_confirmed)-1])\n            else:\n                if len(adjusted_y_train_confirmed) < 3:\n                    adjusted_y_train_confirmed = np.append(adjusted_y_train_confirmed,adjusted_y_train_confirmed[len(adjusted_y_train_confirmed)-1])\n                else:\n                    pass\n        \n        #[CONFIRMED CASES] - prediction and modelling\n        model = SARIMAX(adjusted_y_train_confirmed, order=(1,1,0), \n                        measurement_error=True).fit(disp=False)\n        y_hat_confirmed = model.forecast(pred_data[pred_data['Date'] > max_train_date].shape[0])\n        y_train_confirmed = train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['Date'] >=  min_test_date)]['ConfirmedCases'].values\n        y_hat_confirmed = np.concatenate((y_train_confirmed,y_hat_confirmed), axis = 0)\n\n        if len(adjusted_y_train_fatalities) < 1:\n            adjusted_y_train_fatalities = np.zeros(3)\n        else:\n            if len(adjusted_y_train_fatalities) < 2:\n                adjusted_y_train_fatalities = np.append(adjusted_y_train_fatalities,adjusted_y_train_fatalities[len(adjusted_y_train_fatalities)-1],adjusted_y_train_fatalities[len(adjusted_y_train_fatalities)-1])\n            else:\n                if len(adjusted_y_train_fatalities) < 3:\n                    adjusted_y_train_fatalities = np.append(adjusted_y_train_fatalities,adjusted_y_train_fatalities[len(adjusted_y_train_fatalities)-1])\n                else:\n                    pass\n                \n        #[FATALITIES] - prediction and modelling\n        model = SARIMAX(adjusted_y_train_fatalities, order=(1,1,0), \n                        measurement_error=True).fit(disp=False)\n        y_hat_fatalities = model.forecast(pred_data[pred_data['Date'] > max_train_date].shape[0])\n        y_train_fatalities = train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['Date'] >=  min_test_date)]['Fatalities'].values\n        y_hat_fatalities = np.concatenate((y_train_fatalities,y_hat_fatalities), axis = 0)\n        pred_data['ConfirmedCases_hat'] =  y_hat_confirmed\n        pred_data['Fatalities_hat'] = y_hat_fatalities\n        pred_data_all = pred_data_all.append(pred_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_val = pd.merge(pred_data_all,train[['Date','Country_Region','Province_State','ConfirmedCases','Fatalities']],on=['Date','Country_Region','Province_State'], how='left')\ndf_val.loc[df_val['Fatalities_hat'] < 0,'Fatalities_hat'] = 0\ndf_val.loc[df_val['ConfirmedCases_hat'] < 0,'ConfirmedCases_hat'] = 0\ndf_val_3 = df_val.copy()\nsubmission = df_val[['ForecastId','ConfirmedCases_hat','Fatalities_hat']]\nsubmission.columns = ['ForecastId','ConfirmedCases','Fatalities']\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#country = 'France'\n#province = 'Saint Pierre and Miquelon'\n#\n#\n#pred_data_all = pd.DataFrame()\n#print(country)\n##create dataframe for a specific country\n#df_train = train[(train['Country_Region'] == country) & (train['Province_State'] == province)]\n#df_test = test[(test['Country_Region'] == country) & (test['Province_State'] == province)]\n##create features -> number of cases on a specific date\n#X_train = CreateInput(df_train)\n##last 12 confirmed cases in train data set\n#y_train_confirmed = df_train['ConfirmedCases'].ravel()\n##last 12 confirmed fatalities in train data set\n#y_train_fatalities = df_train['Fatalities'].ravel()\n##create features in test dataset-> number of cases on a specific date\n#X_pred = CreateInput(df_test)\n#\n##creates reversed list of the possible features\n#for day in sorted(feature_day,reverse = True):\n#    #check for the column in the list\n#    feature_use = 'Number day from ' + str(day) + ' case'\n#    #check the 0-dimension of the array (similiar to length of a dataframe)\n#    idx = X_train[X_train[feature_use] == 0].shape[0]\n#    #print(idx)\n#    #if there are more than 20 values for a column, the loop will be interruped\n#    if (X_train[X_train[feature_use] > 0].shape[0] >= 20):\n#        break\n#        \n##[TRAIN] - cuts the value of idx from the top of the dataframe; selects the input column (e.g. Number day from 1000 case); brings it into a horizontal array\n#adjusted_X_train = X_train[idx:][feature_use].values.reshape(-1, 1)\n##[TRAIN] - get the respective confirmed cases\n#adjusted_y_train_confirmed = y_train_confirmed[idx:]\n##[TRAIN] - get the respective fatalities\n#adjusted_y_train_fatalities = y_train_fatalities[idx:]\n#\n##[TEST] - selects for the extracted feature column and get the length (0)\n#idx = X_pred[X_pred[feature_use] == 0].shape[0]\n##[TEST] - creates a clean array also for the prediction\n#adjusted_X_pred = X_pred[idx:][feature_use].values.reshape(-1, 1)\n#\n##[TEST] - gets the extract from the test dataset for a specific country/ region\n#pred_data = test[(test['Country_Region'] == country) & (test['Province_State'] == province)]\n#\n##latest date from the trainings data set\n#max_train_date = train[(train['Country_Region'] == country) & (train['Province_State'] == province)]['Date'].max()\n##earliest date from the test data set\n#min_test_date = pred_data['Date'].min()\n#\n#if len(adjusted_y_train_confirmed) < 1:\n#    adjusted_y_train_confirmed = np.zeros(3)\n#else:\n#    if len(adjusted_y_train_confirmed) < 2:\n#        adjusted_y_train_confirmed = np.append(adjusted_y_train_confirmed,adjusted_y_train_confirmed[len(adjusted_y_train_confirmed)-1],adjusted_y_train_confirmed[len(adjusted_y_train_confirmed)-1])\n#    else:\n#        if len(adjusted_y_train_confirmed) < 3:\n#            adjusted_y_train_confirmed = np.append(adjusted_y_train_confirmed,adjusted_y_train_confirmed[len(adjusted_y_train_confirmed)-1])\n#        else:\n#            pass\n#\n##[CONFIRMED CASES] - prediction and modelling\n#model = SARIMAX(adjusted_y_train_confirmed, order=(1,1,0), \n#                measurement_error=True).fit(disp=False)\n#y_hat_confirmed = model.forecast(pred_data[pred_data['Date'] > max_train_date].shape[0])\n#y_train_confirmed = train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['Date'] >=  min_test_date)]['ConfirmedCases'].values\n#y_hat_confirmed = np.concatenate((y_train_confirmed,y_hat_confirmed), axis = 0)\n#\n#if len(adjusted_y_train_fatalities) < 1:\n#    adjusted_y_train_fatalities = np.zeros(3)\n#else:\n#    if len(adjusted_y_train_fatalities) < 2:\n#        adjusted_y_train_fatalities = np.append(adjusted_y_train_fatalities,adjusted_y_train_fatalities[len(adjusted_y_train_fatalities)-1],adjusted_y_train_fatalities[len(adjusted_y_train_fatalities)-1])\n#    else:\n#        if len(adjusted_y_train_fatalities) < 3:\n#            adjusted_y_train_fatalities = np.append(adjusted_y_train_fatalities,adjusted_y_train_fatalities[len(adjusted_y_train_fatalities)-1])\n#        else:\n#            pass\n#\n##[FATALITIES] - prediction and modelling\n#model = SARIMAX(adjusted_y_train_fatalities, order=(1,1,0), \n#                measurement_error=True).fit(disp=False)\n#\n#y_hat_fatalities = model.forecast(pred_data[pred_data['Date'] > max_train_date].shape[0])\n#y_train_fatalities = train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['Date'] >=  min_test_date)]['Fatalities'].values\n#y_hat_fatalities = np.concatenate((y_train_fatalities,y_hat_fatalities), axis = 0)\n#pred_data['ConfirmedCases_hat'] =  y_hat_confirmed\n#pred_data['Fatalities_hat'] = y_hat_fatalities\n#pred_data_all = pred_data_all.append(pred_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crosscheck_sarima(country):\n    crosscheck = train_df[(train_df['Country'] == country) & (train_df['Date'] >= '2020-04-02')].reset_index()\n    arima = pred_data_all[(pred_data_all['Country_Region'] == country)].reset_index()\n    arima['ConfirmedCases_In'] = arima['ConfirmedCases_hat']\n    arima['Fatalities_In'] = arima['Fatalities_hat']\n    arima['CC_Crosscheck'] = crosscheck['ConfirmedCases']\n    arima['Fat_Crosscheck'] = crosscheck['Fatalities']\n    arima['Day_num'] = arima['Date']\n    return arima","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crosscheck_sarima_cs(country,state):\n    crosscheck = train_df[(train_df['Country'] == country) & (train_df['State'] == state) & (train_df['Date'] >= '2020-04-02')].reset_index()\n    arima = pred_data_all[(pred_data_all['Country_Region'] == country) & (pred_data_all['Province_State'] == state)].reset_index()\n    arima['ConfirmedCases_In'] = arima['ConfirmedCases_hat']\n    arima['Fatalities_In'] = arima['Fatalities_hat']\n    arima['CC_Crosscheck'] = crosscheck['ConfirmedCases']\n    arima['Fat_Crosscheck'] = crosscheck['Fatalities']\n    arima['Day_num'] = arima['Date']\n    return arima","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_plot_2(df_check2,test_con,state=''):\n\n    fig = go.Figure()\n    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n    fig.add_scatter(x=df_check2['Day_num'], y=df_check2['ConfirmedCases_In'], name='Confirmed Cases - Prediction')\n    fig.add_scatter(x=df_check2['Day_num'], y=df_check2['CC_Crosscheck'], name='Confirmed Cases - Official')\n\n    fig.add_scatter(x=df_check2['Day_num'], y=df_check2['Fatalities_In'], name='Fatalities - Prediction', secondary_y=True)\n    fig.add_scatter(x=df_check2['Day_num'], y=df_check2['Fat_Crosscheck'], name='Fatalities - Official', secondary_y=True)\n\n    if state=='':\n        fig.update_layout(title='ARIMA '+ test_con + ' Forecast',\n                           xaxis_title='Number of Days since 2020-01-22',\n                           yaxis_title='Confirmed Cases')\n    else:\n        fig.update_layout(title='Arima_'+ test_con + ', ' + state + ' Forecast',\n                       xaxis_title='Number of Days since 2020-01-22',\n                       yaxis_title='Confirmed Cases')    \n\n    fig.update_yaxes(title_text=\"Confirmed Cases\", secondary_y=False)\n    fig.update_yaxes(title_text=\"Fatalities\", secondary_y=True)\n    \n    #if state=='':\n    #    py.plot(fig, filename = 'SARIMA_' + test_con, auto_open=True)\n    #else:\n    #    py.plot(fig, filename = 'SARIMA_' + test_con + '_' + state, auto_open=True)\n        \n\n    return fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pred_data_all[(pred_data_all['Country_Region'] == country_dict[country]) & (pred_data_all['Date'] <= '2020-04-02')].reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import statsmodels.api as sm\n#\n#decomposition = sm.tsa.seasonal_decompose(train_df['value'], model='additive', \n#                            extrapolate_trend='freq') #additive or multiplicative is data specific\n#fig = decomposition.plot()\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Confirmed Cases are on the left Y-axis and the Fatalities on the right Y-axis."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_con = 'Germany'\ndf_check2 = crosscheck_sarima(test_con)\ncheck_plot_2(df_check2,test_con)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_con = 'Spain'\ndf_check2 = crosscheck_sarima(test_con)\ncheck_plot_2(df_check2,test_con)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_con = 'Italy'\ndf_check2 = crosscheck_sarima(test_con)\ncheck_plot_2(df_check2,test_con)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_con = 'Algeria'\ndf_check2 = crosscheck_sarima(test_con)\ncheck_plot_2(df_check2,test_con)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_con = 'Andorra'\ndf_check2 = crosscheck_sarima(test_con)\ncheck_plot_2(df_check2,test_con)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_con = 'Iran'\ndf_check2 = crosscheck_sarima(test_con)\ncheck_plot_2(df_check2,test_con)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_con = 'Russia'\ndf_check2 = crosscheck_sarima(test_con)\ncheck_plot_2(df_check2,test_con)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_con = 'US'\ntest_st = 'New York'\ndf_check2 = crosscheck_sarima_cs(test_con,test_st)\ncheck_plot_2(df_check2,test_con,test_st)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_con = 'US'\ntest_st = 'Washington'\ndf_check2 = crosscheck_sarima_cs(test_con,test_st)\ncheck_plot_2(df_check2,test_con,test_st)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_con = 'China'\ntest_st = 'Shanghai'\ndf_check2 = crosscheck_sarima_cs(test_con,test_st)\ncheck_plot_2(df_check2,test_con,test_st)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_con = 'China'\ntest_st = 'Beijing'\ndf_check2 = crosscheck_sarima_cs(test_con,test_st)\ncheck_plot_2(df_check2,test_con,test_st)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_con = 'Korea, South'\ndf_check2 = crosscheck_sarima(test_con)\ncheck_plot_2(df_check2,test_con)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_con = 'South Africa'\ndf_check2 = crosscheck_sarima(test_con)\ncheck_plot_2(df_check2,test_con)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_con = 'Ghana'\ndf_check2 = crosscheck_sarima(test_con)\ncheck_plot_2(df_check2,test_con)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Country'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.4 ARIMA — Conclusion\nThe forecasts for the individual countries look much more plausible over time. Data points for countries with more confirmed cases and a longer history such as Germany can be extrapolated well. If the figures are reported erratically, as in Shanghai, our algorithm will not be able to make good predictions.\nThe implementation process compared to linear regression was also much easier. The first test submission to Kaggle had an RMLSE of about 0.4. The competition for week 4 ends today and the evaluation phase begins. I will add the final results."},{"metadata":{},"cell_type":"markdown","source":"# 5. Summary\nFirst we looked at the developments around Covid-19, cleaned up the data and prepared it accordingly. We then tried out two different methods for predicting time series: 1) linear regression and 2) ARIMA. The results of the linear regression were very volatile and the predictions did not seem particularly accurate. With ARIMA, we achieved significantly better and more plausible results. A final evaluation of the results is possible on 15.05.2020. However, the submission for the test period achieved a good score.\nI hope that despite the seriousness of the topic you were able to take something with you. Thanks for reading! Stay at home and healthy!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}