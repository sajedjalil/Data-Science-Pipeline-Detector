{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import RandomizedSearchCV\nimport pickle\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom sklearn.utils import shuffle\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/adeconvid19/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/adeconvid19/train.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/adeconvid19/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.shape,\"\\n\",df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Province_State\"].fillna(\"state\", inplace = True)    \ndf[\"Country_Region\"] = [country_name.replace(\"'\",\"\") for country_name in df[\"Country_Region\"]]\nprint(df.shape,\"\\n\",df.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now preparing our dataset**\n\nWe are going to consider the past 7 days data to forecast the cases and also the fatalities on the 8th day."},{"metadata":{"trusted":true},"cell_type":"code","source":"data=[]\ncountries=df.Country_Region.unique()\nfor country in countries:\n    provinces=df[df.Country_Region==country].Province_State.unique()\n    for province in provinces:\n        temp_df=df[(df['Country_Region'] == country) & (df['Province_State']==province)]\n        for i in range(0,74):\n            Iday1=float(temp_df.iloc[i].ConfirmedCases)\n            Iday2=float(temp_df.iloc[i+1].ConfirmedCases)\n            Iday3=float(temp_df.iloc[i+2].ConfirmedCases)\n            Iday4=float(temp_df.iloc[i+3].ConfirmedCases)\n            Iday5=float(temp_df.iloc[i+4].ConfirmedCases)\n            Iday6=float(temp_df.iloc[i+5].ConfirmedCases)\n            Iday7=float(temp_df.iloc[i+6].ConfirmedCases)\n            Fday1=float(temp_df.iloc[i].Fatalities)\n            Fday2=float(temp_df.iloc[i+1].Fatalities)\n            Fday3=float(temp_df.iloc[i+2].Fatalities)\n            Fday4=float(temp_df.iloc[i+3].Fatalities)\n            Fday5=float(temp_df.iloc[i+4].Fatalities)\n            Fday6=float(temp_df.iloc[i+5].Fatalities)\n            Fday7=float(temp_df.iloc[i+6].Fatalities)\n            target_infection=float(temp_df.iloc[i+7].ConfirmedCases)\n            target_fatal=float(temp_df.iloc[i+7].Fatalities)\n            data.append({\"Iday1\":Iday1,\"Iday2\":Iday2,\"Iday3\":Iday3,\"Iday4\":\n                         Iday4,\"Iday5\":Iday5,\n                         \"Iday6\":Iday6,\"Iday7\":Iday7,\"Fday1\":Fday1,\"Fday2\":\n                         Fday2,\"Fday3\":Fday3,\n                         \n                         \"Fday4\":Fday4,\"Fday5\":Fday5,\"Fday6\":Fday6,\"Fday7\":Fday7,\n                         \"target_infection\":target_infection,\"target_fatal\":target_fatal})        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data=pd.DataFrame(data)\nprint(\"The shape of new dataFrame:\",new_data.shape,\"\\nThe columns are:\",new_data.columns)\nprint(new_data.head(-5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NOW, SPLITTING THE DATASET INTO TRAIN AND TEST**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_y=shuffle(new_data)\ny_cases=X_y['target_infection']\ny_fatal=X_y['target_fatal']\nX=X_y.drop(['target_infection','target_fatal'],axis=1)\nX_train_cases, X_test_cases, y_train_cases, y_test_cases = train_test_split(X, y_cases, test_size=0.33)\nX_train_fatal, X_test_fatal, y_train_fatal, y_test_fatal = train_test_split(X, y_fatal, test_size=0.33)\nprint(\"Shape of infection train dataset:\",(X_train_cases.shape,y_train_cases.shape))\nprint(\"Shape of infection test dataset:\",(X_test_cases.shape,y_test_cases.shape))\nprint(\"Shape of fatal train dataset:\",(X_train_fatal.shape,y_train_fatal.shape))\nprint(\"Shape of fatal test dataset:\",(X_test_fatal.shape,y_test_fatal.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now, using Elastic Net to train the CONVID19 data (after using RandomSearch to find out the best set of parameters).**"},{"metadata":{},"cell_type":"markdown","source":"Note: that we don't have good result when applying scaling. Hence, the original data was chosen to be used through this process."},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_case=ElasticNet(random_state=42,l1_ratio=0.1,max_iter=2200)\nparams = [{'alpha': [10**-4,10**-3, 10**-2,10**-1, 10**0,10**1, 10**2,10**3,10**4]}]\nclf = RandomizedSearchCV(reg_case, params, cv=4, scoring='neg_root_mean_squared_error',return_train_score=True)\nsearch=clf.fit(X_train_cases, y_train_cases)\nresults = pd.DataFrame.from_dict(clf.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_alpha=10\nbest_itr=2400\nfinal_reg_case=ElasticNet(random_state=42,alpha=best_alpha,l1_ratio=0.1,max_iter=best_itr)\nfinal_reg_case.fit(X_train_cases,y_train_cases)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RMSE VALUE.\n\nNOTE: That the RMSE value will definately big due to the fact that, no scaling was done."},{"metadata":{"trusted":true},"cell_type":"code","source":"pred=final_reg_case.predict(X_test_cases)\nprint(\"The RMSE value\",(mean_squared_error(y_test_cases,pred))**0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also with the same procedure. \n\nWe are going to train the fatality data as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_fatal=ElasticNet(random_state=42,l1_ratio=0.1,max_iter=3500)\nparams = [{'alpha': [10**-4,10**-3, 10**-2,10**-1, 10**0,10**1, 10**2,10**3,10**4]}]\nclf = RandomizedSearchCV(reg_fatal, params, cv=4, scoring='neg_root_mean_squared_error',return_train_score=True)\nsearch=clf.fit(X_train_fatal, y_train_fatal)\nresults = pd.DataFrame.from_dict(clf.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_alpha=100\nbest_iter=3500\nfinal_reg_fatal = ElasticNet(random_state=42,alpha=best_alpha,l1_ratio=0.1,max_iter=best_iter)\nfinal_reg_fatal.fit(X_train_fatal, y_train_fatal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred=final_reg_fatal.predict(X_test_fatal)\nprint(\"The RMSE value\",(mean_squared_error(y_test_fatal,pred))**0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**FEATURE ENGINEERING:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=[]\ncountries=df.Country_Region.unique()\nfor country in countries:\n    provinces=df[df.Country_Region==country].Province_State.unique()\n    for province in provinces:\n        temp_df=df[(df['Country_Region'] == country) & (df['Province_State']==province)]\n        for i in range(0,74):\n            Iday1=float(temp_df.iloc[i].ConfirmedCases)\n            Iday2=float(temp_df.iloc[i+1].ConfirmedCases)\n            Iday3=float(temp_df.iloc[i+2].ConfirmedCases)\n            Iday4=float(temp_df.iloc[i+3].ConfirmedCases)\n            Iday5=float(temp_df.iloc[i+4].ConfirmedCases)\n            Iday6=float(temp_df.iloc[i+5].ConfirmedCases)\n            Iday7=float(temp_df.iloc[i+6].ConfirmedCases)\n            Fday1=float(temp_df.iloc[i].Fatalities)\n            Fday2=float(temp_df.iloc[i+1].Fatalities)\n            Fday3=float(temp_df.iloc[i+2].Fatalities)\n            Fday4=float(temp_df.iloc[i+3].Fatalities)\n            Fday5=float(temp_df.iloc[i+4].Fatalities)\n            Fday6=float(temp_df.iloc[i+5].Fatalities)\n            Fday7=float(temp_df.iloc[i+6].Fatalities)\n            if Iday6==0 :\n                iavg=1\n            else:\n                iavg=Iday7/(Iday6)\n            if Fday6==0:\n                favg=1\n            else:    \n                favg=Fday7/(Fday6)        \n            target_infection=float(temp_df.iloc[i+7].ConfirmedCases)\n            target_fatal=float(temp_df.iloc[i+7].Fatalities)\n            data.append({\"Iday1\":Iday1,\"Iday2\":Iday2,\"Iday3\":Iday3,\"Iday4\":Iday4,\"Iday5\":Iday5,\n                         \"Iday6\":Iday6,\"Iday7\":Iday7,\"Fday1\":Fday1,\"Fday2\":Fday2,\"Fday3\":Fday3,\n                         \"Fday4\":Fday4,\"Fday5\":Fday5,\"Fday6\":Fday6,\"Fday7\":Fday7,'iratio':iavg,\"fratio\":favg,\"target_infection\":target_infection,\"target_fatal\":target_fatal})        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"HOWEVER, THE SUBSEQUENT METHODS WILL REMAIN THE SAME"},{"metadata":{"trusted":true},"cell_type":"code","source":"featured=pd.DataFrame(data)\nX_y_f=shuffle(featured)\ny_cases_f=X_y_f['target_infection']\ny_fatal_f=X_y_f['target_fatal']\nX_f=X_y_f.drop(['target_infection','target_fatal'],axis=1)\nX_train_cases_f, X_test_cases_f, y_train_cases_f, y_test_cases_f = train_test_split(X_f, y_cases_f, test_size=0.33)\nX_train_fatal_f, X_test_fatal_f, y_train_fatal_f, y_test_fatal_f = train_test_split(X_f, y_fatal_f, test_size=0.33)\nprint(\"Shape of featurized infection train dataset:\",(X_train_cases_f.shape,y_train_cases_f.shape))\nprint(\"Shape of featurized infection test dataset:\",(X_test_cases_f.shape,y_test_cases_f.shape))\nprint(\"Shape of featurized fatal train dataset:\",(X_train_fatal_f.shape,y_train_fatal_f.shape))\nprint(\"Shape of featurized fatal test dataset:\",(X_test_fatal_f.shape,y_test_fatal_f.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_case_f=ElasticNet(random_state=42,l1_ratio=0.1,max_iter=2200)\nparams = [{'alpha': [10**-4,10**-3, 10**-2,10**-1, 10**0,10**1, 10**2,10**3,10**4]}]\nclf_f= RandomizedSearchCV(reg_case_f, params, cv=4, scoring='neg_root_mean_squared_error',return_train_score=True)\nsearch_f=clf_f.fit(X_train_cases_f, y_train_cases_f)\nresults_f = pd.DataFrame.from_dict(clf_f.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_alpha=10000\nbest_itr=4200\nfinal_reg_case_f=ElasticNet(random_state=42,alpha=best_alpha,l1_ratio=0.1,max_iter=best_itr)\nfinal_reg_case_f.fit(X_train_cases_f,y_train_cases_f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_f=final_reg_case_f.predict(X_test_cases_f)\nprint(\"RMSE is:\",(mean_squared_error(y_test_cases_f,pred_f))**0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_fatal_f=ElasticNet(random_state=42,alpha=best_alpha,l1_ratio=0.1,max_iter=2200)\nparams = [{'alpha': [10**-4,10**-3, 10**-2,10**-1, 10**0,10**1, 10**2,10**3,10**4]}]\nclf_f= RandomizedSearchCV(reg_fatal_f, params, cv=4, scoring='neg_root_mean_squared_error',return_train_score=True)\nsearch_f=clf_f.fit(X_train_fatal_f, y_train_fatal_f)\nresults_f = pd.DataFrame.from_dict(clf_f.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_alpha=100\nbest_itr=2400\nfinal_reg_fatal_f=ElasticNet(random_state=42,alpha=best_alpha,l1_ratio=0.1,max_iter=best_itr)\nfinal_reg_fatal_f.fit(X_train_fatal_f,y_train_fatal_f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_f=final_reg_fatal_f.predict(X_test_fatal_f)\nprint(\"RMSE is:\",(mean_squared_error(y_test_fatal_f,pred_f))**0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NOW, MAKING OF CASES AND FATALITIES PROJECTION:\nBY INITIALIZING THE LIST WITH PREVIOUS SEVEN DAYS DATA AND MAKE A PREDICTION VALUE FOR THE FOLLOWING DAY.\nALSO, APPENDING THIS VALUE TO THIS LIST AND AMKE THIS LATEST DATA(UPDATED DATA) TO MAKE A PREDICTION FOR THE FOLLOWING ONE ETC."},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"Province_State\"].fillna(\"state\", inplace = True)    \ntest[\"Country_Region\"] = [country_name.replace(\"'\",\"\") for country_name in test[\"Country_Region\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport random\npredicted_case=[]\npredicted_fatal=[]\ncountries=df.Country_Region.unique()\nfor country in countries:\n    provinces=df[df.Country_Region==country].Province_State.unique()\n    for province in provinces:\n        temp_df=df[(df['Country_Region'] == country) & (df['Province_State']==province)&(df['Date']>='2020-04-02')]\n        ongoingCases=list(temp_df.ConfirmedCases.values)\n        ongoingFatal=list(temp_df.Fatalities.values)\n        predicted_case.extend(ongoingCases)\n        predicted_fatal.extend(ongoingFatal)\n        for _ in range(1,34):  \n            if ongoingCases[-2]==0:\n                iavg=ongoingCases[-1]\n            else:\n                iavg=ongoingCases[-1]/ongoingCases[-2]\n            if ongoingFatal[-2]==0:\n                favg=ongoingFatal[-1]\n            else:    \n                favg=ongoingFatal[-1]/ongoingFatal[-2]\n            point=ongoingCases[len(ongoingCases)-7:]+ongoingFatal[len(ongoingFatal)-7:]+[iavg,favg]\n            # print(point)\n            # print()\n            randF=random.random()\n            randI=random.random()\n            predC=final_reg_case_f.predict([point])\n            predF=final_reg_fatal_f.predict([point])\n            predicted_case.append(int(predC[0]-(randI*predC[0]*0.002)))\n            predicted_fatal.append(abs(int(predF[0]-(randF*predF[0]*0.0005))))\n            ongoingCases.append(predC[0]-(randI*predC[0]*0.002))\n            ongoingFatal.append(abs(predF[0]-(randF*predF[0]*0.0005)))    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LASTLY, LET'S UPDATE THE VALUE IN OUR TEST DATASET**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test['ConfirmedCases'] = list(map(int,predicted_case))\ntest['Fatalities'] = list(map(int,predicted_fatal))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file=test[['ForecastId','ConfirmedCases','Fatalities']]\nsubmission_file=shuffle(submission_file)\nsubmission_file.to_csv(\"submission_convid19.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}