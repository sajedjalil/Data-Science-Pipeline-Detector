{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#!/usr/bin/env python\n# coding: utf-8\n\n# In[1]:\n\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\nget_ipython().run_line_magic('matplotlib', 'inline')\nget_ipython().run_line_magic('config', \"InlineBackend.figure_format = 'retina'\")\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport xgboost as xgb\n\nfrom tensorflow.keras.optimizers import Nadam\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport tensorflow.keras.layers as KL\nfrom datetime import timedelta\nimport numpy as np\nimport pandas as pd\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nimport datetime\nimport gc\nfrom tqdm import tqdm\ndef RMSLE(pred,actual):\n    return np.sqrt(np.mean(np.power((np.log(pred+1)-np.log(actual+1)),2)))\ntesting_result=False\ndateck='2020-04-03'\n\n\n# In[2]:\n\n\ndef get_cpmp_sub(save_oof=False, save_public_test=False):\n    train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\n    if testing_result :train=train[train.Date<=dateck]\n    train['Province_State'].fillna('', inplace=True)\n    train['Date'] = pd.to_datetime(train['Date'])\n    train['day'] = train.Date.dt.dayofyear\n    #train = train[train.day <= 85]\n    train['geo'] = ['_'.join(x) for x in zip(train['Country_Region'], train['Province_State'])]\n    train\n\n    test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')\n    test['Province_State'].fillna('', inplace=True)\n    test['Date'] = pd.to_datetime(test['Date'])\n    test['day'] = test.Date.dt.dayofyear\n    test['geo'] = ['_'.join(x) for x in zip(test['Country_Region'], test['Province_State'])]\n    test\n\n    day_min = train['day'].min()\n    train['day'] -= day_min\n    test['day'] -= day_min\n\n    min_test_val_day = test.day.min()\n    max_test_val_day = train.day.max()\n    max_test_day = test.day.max()\n    num_days = max_test_day + 1\n\n    min_test_val_day, max_test_val_day, num_days\n\n    train['ForecastId'] = -1\n    test['Id'] = -1\n    test['ConfirmedCases'] = 0\n    test['Fatalities'] = 0\n\n    debug = False\n\n    data = pd.concat([train,\n                      test[test.day > max_test_val_day][train.columns]\n                     ]).reset_index(drop=True)\n    if debug:\n        data = data[data['geo'] >= 'France_'].reset_index(drop=True)\n    #del train, test\n    gc.collect()\n\n    dates = data[data['geo'] == 'France_'].Date.values\n\n    if 0:\n        gr = data.groupby('geo')\n        data['ConfirmedCases'] = gr.ConfirmedCases.transform('cummax')\n        data['Fatalities'] = gr.Fatalities.transform('cummax')\n\n    geo_data = data.pivot(index='geo', columns='day', values='ForecastId')\n    num_geo = geo_data.shape[0]\n    geo_data\n\n    geo_id = {}\n    for i,g in enumerate(geo_data.index):\n        geo_id[g] = i\n\n\n    ConfirmedCases = data.pivot(index='geo', columns='day', values='ConfirmedCases')\n    Fatalities = data.pivot(index='geo', columns='day', values='Fatalities')\n\n    if debug:\n        cases = ConfirmedCases.values\n        deaths = Fatalities.values\n    else:\n        cases = np.log1p(ConfirmedCases.values)\n        deaths = np.log1p(Fatalities.values)\n\n\n    def get_dataset(start_pred, num_train, lag_period):\n        days = np.arange( start_pred - num_train + 1, start_pred + 1)\n        lag_cases = np.vstack([cases[:, d - lag_period : d] for d in days])\n        lag_deaths = np.vstack([deaths[:, d - lag_period : d] for d in days])\n        target_cases = np.vstack([cases[:, d : d + 1] for d in days])\n        target_deaths = np.vstack([deaths[:, d : d + 1] for d in days])\n        geo_ids = np.vstack([geo_ids_base for d in days])\n        country_ids = np.vstack([country_ids_base for d in days])\n        return lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days\n\n    def update_valid_dataset(data, pred_death, pred_case):\n        lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days = data\n        day = days[-1] + 1\n        new_lag_cases = np.hstack([lag_cases[:, 1:], pred_case])\n        new_lag_deaths = np.hstack([lag_deaths[:, 1:], pred_death]) \n        new_target_cases = cases[:, day:day+1]\n        new_target_deaths = deaths[:, day:day+1] \n        new_geo_ids = geo_ids  \n        new_country_ids = country_ids  \n        new_days = 1 + days\n        return new_lag_cases, new_lag_deaths, new_target_cases, new_target_deaths, new_geo_ids, new_country_ids, new_days\n\n    def fit_eval(lr_death, lr_case, data, start_lag_death, end_lag_death, num_lag_case, fit, score):\n        lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days = data\n\n        X_death = np.hstack([lag_cases[:, -start_lag_death:-end_lag_death], country_ids])\n        X_death = np.hstack([lag_deaths[:, -num_lag_case:], country_ids])\n        X_death = np.hstack([lag_cases[:, -start_lag_death:-end_lag_death], lag_deaths[:, -num_lag_case:], country_ids])\n        y_death = target_deaths\n        y_death_prev = lag_deaths[:, -1:]\n        if fit:\n            if 0:\n                keep = (y_death > 0).ravel()\n                X_death = X_death[keep]\n                y_death = y_death[keep]\n                y_death_prev = y_death_prev[keep]\n            lr_death.fit(X_death, y_death)\n        y_pred_death = lr_death.predict(X_death)\n        y_pred_death = np.maximum(y_pred_death, y_death_prev)\n\n        X_case = np.hstack([lag_cases[:, -num_lag_case:], geo_ids])\n        X_case = lag_cases[:, -num_lag_case:]\n        y_case = target_cases\n        y_case_prev = lag_cases[:, -1:]\n        if fit:\n            lr_case.fit(X_case, y_case)\n        y_pred_case = lr_case.predict(X_case)\n        y_pred_case = np.maximum(y_pred_case, y_case_prev)\n\n        if score:\n            death_score = val_score(y_death, y_pred_death)\n            case_score = val_score(y_case, y_pred_case)\n        else:\n            death_score = 0\n            case_score = 0\n\n        return death_score, case_score, y_pred_death, y_pred_case\n\n    def train_model(train, valid, start_lag_death, end_lag_death, num_lag_case, num_val, score=True):\n        alpha = 3\n        lr_death = Ridge(alpha=alpha, fit_intercept=False)\n        lr_case = Ridge(alpha=alpha, fit_intercept=True)\n\n        (train_death_score, train_case_score, train_pred_death, train_pred_case,\n        ) = fit_eval(lr_death, lr_case, train, start_lag_death, end_lag_death, num_lag_case, fit=True, score=score)\n\n        death_scores = []\n        case_scores = []\n\n        death_pred = []\n        case_pred = []\n\n        for i in range(num_val):\n\n            (valid_death_score, valid_case_score, valid_pred_death, valid_pred_case,\n            ) = fit_eval(lr_death, lr_case, valid, start_lag_death, end_lag_death, num_lag_case, fit=False, score=score)\n\n            death_scores.append(valid_death_score)\n            case_scores.append(valid_case_score)\n            death_pred.append(valid_pred_death)\n            case_pred.append(valid_pred_case)\n\n            if 0:\n                print('val death: %0.3f' %  valid_death_score,\n                      'val case: %0.3f' %  valid_case_score,\n                      'val : %0.3f' %  np.mean([valid_death_score, valid_case_score]),\n                      flush=True)\n            valid = update_valid_dataset(valid, valid_pred_death, valid_pred_case)\n\n        if score:\n            death_scores = np.sqrt(np.mean([s**2 for s in death_scores]))\n            case_scores = np.sqrt(np.mean([s**2 for s in case_scores]))\n            if 0:\n                print('train death: %0.3f' %  train_death_score,\n                      'train case: %0.3f' %  train_case_score,\n                      'val death: %0.3f' %  death_scores,\n                      'val case: %0.3f' %  case_scores,\n                      'val : %0.3f' % ( (death_scores + case_scores) / 2),\n                      flush=True)\n            else:\n                print('%0.4f' %  case_scores,\n                      ', %0.4f' %  death_scores,\n                      '= %0.4f' % ( (death_scores + case_scores) / 2),\n                      flush=True)\n        death_pred = np.hstack(death_pred)\n        case_pred = np.hstack(case_pred)\n        return death_scores, case_scores, death_pred, case_pred\n\n    countries = [g.split('_')[0] for g in geo_data.index]\n    countries = pd.factorize(countries)[0]\n\n    country_ids_base = countries.reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    country_ids_base = 0.2 * ohe.fit_transform(country_ids_base)\n    country_ids_base.shape\n\n    geo_ids_base = np.arange(num_geo).reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    geo_ids_base = 0.1 * ohe.fit_transform(geo_ids_base)\n    geo_ids_base.shape\n\n    def val_score(true, pred):\n        pred = np.log1p(np.round(np.expm1(pred) - 0.2))\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\n    def val_score(true, pred):\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\n\n\n    start_lag_death, end_lag_death = 14, 6,\n    num_train = 5\n    num_lag_case = 14\n    lag_period = max(start_lag_death, num_lag_case)\n\n    def get_oof(start_val_delta=0):   \n        start_val = min_test_val_day + start_val_delta\n        last_train = start_val - 1\n        num_val = max_test_val_day - start_val + 1\n        print(dates[start_val], start_val, num_val)\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, \n                                                            start_lag_death, end_lag_death, num_lag_case, num_val)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = train[['Date', 'Id', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        #sub = sub.fillna(0)\n        sub = sub[sub.day >= start_val]\n        sub = sub[['Id', 'ConfirmedCases', 'Fatalities']].copy()\n        return sub\n\n\n    if save_oof:\n        for start_val_delta, date in zip(range(3, -8, -3),\n                                  ['2020-03-22', '2020-03-19', '2020-03-16', '2020-03-13']):\n            print(date, end=' ')\n            oof = get_oof(start_val_delta)\n            oof.to_csv('../submissions/cpmp-%s.csv' % date, index=None)\n\n    def get_sub(start_val_delta=0):   \n        start_val = min_test_val_day + start_val_delta\n        last_train = start_val - 1\n        num_val = max_test_val_day - start_val + 1\n        print(dates[last_train], start_val, num_val)\n        num_lag_case = 14\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, \n                                                            start_lag_death, end_lag_death, num_lag_case, num_val)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = test[['Date', 'ForecastId', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        sub = sub.fillna(0)\n        sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n        return sub\n        return sub\n\n\n    known_test = train[['geo', 'day', 'ConfirmedCases', 'Fatalities']\n              ].merge(test[['geo', 'day', 'ForecastId']], how='left', on=['geo', 'day'])\n    known_test = known_test[['ForecastId', 'ConfirmedCases', 'Fatalities']][known_test.ForecastId.notnull()].copy()\n    known_test\n\n    unknow_test = test[test.day > max_test_val_day]\n    unknow_test\n\n    def get_final_sub():   \n        start_val = max_test_val_day + 1\n        last_train = start_val - 1\n        num_val = max_test_day - start_val + 1\n        print(dates[last_train], start_val, num_val)\n        num_lag_case = num_val + 3\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        (_, _, val_death_preds, val_case_preds\n        ) = train_model(train_data, valid_data, start_lag_death, end_lag_death, num_lag_case, num_val, score=False)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n        print(unknow_test.shape, pred_deaths.shape, pred_cases.shape)\n\n        sub = unknow_test[['Date', 'ForecastId', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        #sub = sub.fillna(0)\n        sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n        sub = pd.concat([known_test, sub])\n        return sub\n\n    if save_public_test:\n        sub = get_sub()\n    else:\n        sub = get_final_sub()\n    return sub\n\n\n# In[3]:\n\n\nsub1 = get_cpmp_sub()\nsub1['ForecastId'] = sub1['ForecastId'].astype('int')\nsub1.sort_values(\"ForecastId\", inplace=True)\n\n\n# In[4]:\n\n\n\n\ntrain = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\ndacheckv='2020-04-01'\ntrain['Province_State'].fillna('', inplace=True)\ntrain['Date'] = pd.to_datetime(train['Date'])\ntrain['day'] = train.Date.dt.dayofyear\n#train = train[train.day <= 85]\ntrain['geo'] = ['_'.join(x) for x in zip(train['Country_Region'], train['Province_State'])]\ntrain=train[train.Date>dacheckv]\n\ntest = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')\ntest['Province_State'].fillna('', inplace=True)\ntest['Date'] = pd.to_datetime(test['Date'])\ntest['day'] = test.Date.dt.dayofyear\n#train = train[train.day <= 85]\ntest['geo'] = ['_'.join(x) for x in zip(test['Country_Region'], test['Province_State'])]\ntest1=test.merge(train,on=['Country_Region','Province_State','Date'],how='inner')\nprint(RMSLE(sub1[sub1.ForecastId.isin(test1.ForecastId.values)]['ConfirmedCases'].values,test1['ConfirmedCases'].values))\n\nprint(RMSLE(sub1[sub1.ForecastId.isin(test1.ForecastId.values)]['Fatalities'].values,test1['Fatalities'].values))\n\n\n# In[5]:\n\n\n\n# Parameters - can be changed\nBAGS = 3\nSEED = 123\nSET_FRAC = 0.01  \n\nTRUNCATED = False\n\n\nDROPS = True\nPRIVATE = True\nUSE_PRIORS = False\n\n\nSUP_DROP = 0.0\nACTIONS_DROP = 0.0\nPLACE_FRACTION = 1.0  # 0.4 \n\nLT_DECAY_MAX = 0.3\nLT_DECAY_MIN = -0.4\n\nSINGLE_MODEL = False\nMODEL_Y = 'agg_dff' # 'slope'  # 'slope' or anything else for difference/aggregate log gain\n\n\n\nimport pandas as pd\nimport numpy as np\nimport os\n\n# %% [code]\nfrom collections import Counter\nfrom random import shuffle\nimport math\n\n# %% [code]\nfrom scipy.stats.mstats import gmean\n\n\n# %% [code]\nimport datetime\n\n# %% [code]\nimport matplotlib.pyplot as plt\nimport matplotlib as matplotlib\nimport seaborn as sns\n\n# %% [code]\npd.options.display.float_format = '{:.8}'.format\n\n\n# %% [code]\nplt.rcParams[\"figure.figsize\"] = (12, 4.75)\n \n# %% [code]\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n \n\n    \npd.options.display.max_rows = 999\n    \n\npath = '/kaggle/input/c19week3'\ninput_path = '/kaggle/input/covid19-global-forecasting-week-4'\n\n# %% [code]\ntrain = pd.read_csv(input_path + '/train.csv')\nif testing_result :train=train[train.Date<=dateck]\ntest = pd.read_csv(input_path  + '/test.csv')\nsub = pd.read_csv(input_path + '/submission.csv')\n\n\ntt = pd.merge(train, test, on=['Country_Region', 'Province_State', 'Date'], \n              how='right', validate=\"1:1\")\\\n                    .fillna(method = 'ffill')\npublic = tt[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n      \n\ntrain.Date.max()\n\n# %% [code]\ntest_dates = test.Date.unique()\ntest_dates\n\n# %% [raw]\n# # simulate week 1 sort of \n# test = test[ test.Date >=  '2020-03-25']\n\n# %% [raw]\n# test\n\n# %% [code]\npp = 'public'\n\n# %% [code]\n#FINAL_PUBLIC_DATE = datetime.datetime(2020, 4, 8)\n\nif PRIVATE:\n    test = test[ pd.to_datetime(test.Date) >  train.Date.max()]\n    pp = 'private'\n\n# %% [code]\ntest.Date.unique()\n\n# %% [markdown]\n# ### Train Fix\n\n# %% [markdown]\n# #### Supplement Missing US Data\n\n# %% [code]\nrevised = pd.read_csv(path + '/outside_data' + \n                          '/covid19_train_data_us_states_before_march_09_new.csv')\n\n\n# %% [raw]\n# revised.Date = pd.to_datetime(revised.Date)\n# revised.Date = revised.Date.apply(datetime.datetime.strftime, args= ('%Y-%m-%d',))\n\n# %% [code]\nrevised = revised[['Province_State', 'Country_Region', 'Date', 'ConfirmedCases', 'Fatalities']]\n\n# %% [code]\ntrain.tail()\n\n# %% [code]\nrevised.head()\n\n# %% [code]\ntrain.Date = pd.to_datetime(train.Date)\nrevised.Date = pd.to_datetime(revised.Date)\n\n# %% [code]\nrev_train = pd.merge(train, revised, on=['Province_State', 'Country_Region', 'Date'],\n                            suffixes = ('', '_r'), how='left')\n\n# %% [code]\n\n\n# %% [code]\nrev_train[~rev_train.ConfirmedCases_r.isnull()].head()\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\nrev_train.ConfirmedCases =     np.where( (rev_train.ConfirmedCases == 0) & ((rev_train.ConfirmedCases_r > 0 )) &\n                 (rev_train.Country_Region == 'US'),\n        \n        rev_train.ConfirmedCases_r,\n            rev_train.ConfirmedCases)\n\n\n# %% [code]\nrev_train.Fatalities =     np.where( ~rev_train.Fatalities_r.isnull() & \n                (rev_train.Fatalities == 0) & ((rev_train.Fatalities_r > 0 )) &\n                 (rev_train.Country_Region == 'US')\n             ,\n        \n        rev_train.Fatalities_r,\n            rev_train.Fatalities)\n\n\n# %% [code]\nrev_train.drop(columns = ['ConfirmedCases_r', 'Fatalities_r'], inplace=True)\n\n# %% [code]\ntrain = rev_train\n\n# %% [raw]\n# train[train.Province_State == 'California']\n\n# %% [raw]\n# import sys\n# def sizeof_fmt(num, suffix='B'):\n#     ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n#     for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n#         if abs(num) < 1024.0:\n#             return \"%3.1f %s%s\" % (num, unit, suffix)\n#         num /= 1024.0\n#     return \"%.1f %s%s\" % (num, 'Yi', suffix)\n# \n# for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n#                          key= lambda x: -x[1])[:10]:\n#     print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n# \n\n# %% [markdown]\n# ### Oxford Actions Database\n\n# %% [code]\n# contain_data = pd.read_excel(path + '/outside_data' + \n#                           '/OxCGRT_Download_latest_data.xlsx')\n\ncontain_data = pd.read_csv(path + '/outside_data' + \n                          '/OxCGRT_Download_070420_160027_Full.csv')\n\n# %% [code] {\"scrolled\":true}\ncontain_data = contain_data[[c for c in contain_data.columns if \n                      not any(z in c for z in ['_Notes','Unnamed', 'Confirmed',\n                                               'CountryCode',\n                                                      'S8', 'S9', 'S10','S11',\n                                              'StringencyIndexForDisplay'])] ]\\\n        \n\n# %% [code]\ncontain_data.rename(columns = {'CountryName': \"Country\"}, inplace=True)\n\n# %% [code]\ncontain_data.Date = contain_data.Date.astype(str)    .apply(datetime.datetime.strptime, args=('%Y%m%d', ))\n\n# %% [code]\n\n\n# %% [code]\ncontain_data_orig = contain_data.copy()\n\n# %% [code]\ncontain_data.columns\n\n# %% [raw]\n# contain_data.columns\n\n# %% [code]\n\n\n# %% [code]\ncds = []\nfor country in contain_data.Country.unique():\n    cd = contain_data[contain_data.Country==country]\n    cd = cd.fillna(method = 'ffill').fillna(0)\n    cd.StringencyIndex = cd.StringencyIndex.cummax()  # for now\n    col_count = cd.shape[1]\n    \n    # now do a diff columns\n    # and ewms of it\n    for col in [c for c in contain_data.columns if 'S' in c]:\n        col_diff = cd[col].diff()\n        cd[col+\"_chg_5d_ewm\"] = col_diff.ewm(span = 5).mean()\n        cd[col+\"_chg_20_ewm\"] = col_diff.ewm(span = 20).mean()\n        \n    # stringency\n    cd['StringencyIndex_5d_ewm'] = cd.StringencyIndex.ewm(span = 5).mean()\n    cd['StringencyIndex_20d_ewm'] = cd.StringencyIndex.ewm(span = 20).mean()\n    \n    cd['S_data_days'] =  (cd.Date - cd.Date.min()).dt.days\n    for s in [1, 10, 20, 30, 50, ]:\n        cd['days_since_Stringency_{}'.format(s)] =                 np.clip((cd.Date - cd[(cd.StringencyIndex > s)].Date.min()).dt.days, 0, None)\n    \n    \n    cds.append(cd.fillna(0)[['Country', 'Date'] + cd.columns.to_list()[col_count:]])\ncontain_data = pd.concat(cds)\n\n# %% [raw]\n# contain_data.columns\n\n# %% [raw]\n# dataset.groupby('Country').S_data_days.max().sort_values(ascending = False)[-30:]\n\n# %% [raw]\n# contain_data.StringencyIndex.cummax()\n\n# %% [raw]\n# contain_data.groupby('Date').count()[90:]\n\n# %% [code]\ncontain_data.Date.max()\n\n# %% [code]\ncontain_data.columns\n\n# %% [code]\ncontain_data[contain_data.Country == 'Australia']\n\n# %% [code]\ncontain_data.shape\n\n# %% [raw]\n# contain_data.groupby('Country').Date.max()[:50]\n\n# %% [code]\ncontain_data.Country.replace({ 'United States': \"US\",\n                                 'South Korea': \"Korea, South\",\n                                    'Taiwan': \"Taiwan*\",\n                              'Myanmar': \"Burma\", 'Slovak Republic': \"Slovakia\",\n                                  'Czech Republic': 'Czechia',\n\n}, inplace=True)\n\n# %% [code]\nset(contain_data.Country) - set(test.Country_Region)\n\n# %% [code]\n\n\n# %% [markdown]\n# #### Load in Supplementary Data\n\n# %% [code]\nsup_data = pd.read_excel(path + '/outside_data' + \n                          '/Data Join - Copy1.xlsx')\n\n\n# %% [code]\nsup_data.columns = [c.replace(' ', '_') for c in sup_data.columns.to_list()]\n\n# %% [code]\nsup_data.drop(columns = [c for c in sup_data.columns.to_list() if 'Unnamed:' in c], inplace=True)\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [raw]\n# sup_data.drop(columns = ['longitude', 'temperature', 'humidity',\n#                         'latitude'], inplace=True)\n\n# %% [raw]\n# sup_data.columns\n\n# %% [raw]\n# sup_data.drop(columns = [c for c in sup_data.columns if \n#                                  any(z in c for z in ['state', 'STATE'])], inplace=True)\n\n# %% [raw]\n# sup_data = sup_data[['Province_State', 'Country_Region',\n#                      'Largest_City',\n#                      'IQ', 'GDP_region', \n#                      'TRUE_POPULATION', 'pct_in_largest_city', \n#                    'Migrant_pct',\n#                     'Avg_age',\n#                      'latitude', 'longitude',\n#                 'abs_latitude', #  'Personality_uai', 'Personality_ltowvs',\n#               'Personality_pdi',\n# \n#                  'murder',  'real_gdp_growth'\n#                     ]]\n\n# %% [raw]\n# sup_data = sup_data[['Province_State', 'Country_Region',\n#                      'Largest_City',\n#                      'IQ', 'GDP_region', \n#                      'TRUE_POPULATION', 'pct_in_largest_city', \n#                    #'Migrant_pct',\n#                     # 'Avg_age',\n#                      # 'latitude', 'longitude',\n#              #    'abs_latitude', #  'Personality_uai', 'Personality_ltowvs',\n#             #   'Personality_pdi',\n# \n#                  'murder', # 'real_gdp_growth'\n#                     ]]\n\n# %% [code]\nsup_data.drop(columns = [ 'Date', 'ConfirmedCases',\n       'Fatalities', 'log-cases', 'log-fatalities', 'continent'], inplace=True)\n\n# %% [raw]\n# sup_data.drop(columns = [ 'Largest_City',  \n#                         'continent_gdp_pc', 'continent_happiness', 'continent_generosity',\n#        'continent_corruption', 'continent_Life_expectancy', 'TRUE_CHINA',\n#                          'Happiness', 'Logged_GDP_per_capita',\n#        'Social_support','HDI', 'GDP_pc', 'pc_GDP_PPP', 'Gini',\n#                          'state_white', 'state_white_asian', 'state_black',\n#        'INNOVATIVE_STATE','pct_urban', 'Country_pop', \n#                         \n#                         ], inplace=True)\n\n# %% [raw]\n# sup_data.columns\n\n# %% [raw]\n# \n\n# %% [code]\nsup_data['Migrants_in'] = np.clip(sup_data.Migrants, 0, None)\nsup_data['Migrants_out'] = -np.clip(sup_data.Migrants, None, 0)\nsup_data.drop(columns = 'Migrants', inplace=True)\n\n# %% [raw]\n# sup_data.loc[:, 'Largest_City'] = np.log(sup_data.Largest_City + 1)\n\n# %% [code]\nsup_data.head()\n\n# %% [code]\n\n\n# %% [code]\nsup_data.shape\n\n# %% [raw]\n# sup_data.loc[4][:50]\n\n# %% [code]\n\n\n# %% [markdown]\n# #### Revise Columns\n\n# %% [code]\ntrain.Date = pd.to_datetime(train.Date)\ntest.Date = pd.to_datetime(test.Date)\n#contain_data.Date = pd.to_datetime(contain_data.Date)\n\n# %% [code]\ntrain.rename(columns={'Country_Region': 'Country'}, inplace=True)\ntest.rename(columns={'Country_Region': 'Country'}, inplace=True)\nsup_data.rename(columns={'Country_Region': 'Country'}, inplace=True)\n\n\n# %% [code]\ntrain['Place'] = train.Country + train.Province_State.fillna(\"\")\ntest['Place'] = test.Country +  test.Province_State.fillna(\"\")\n\n\n\n\n\n\n\n\n\n# %% [code]\nsup_data['Place'] = sup_data.Country +  sup_data.Province_State.fillna(\"\")\n\n# %% [code]\nlen(train.Place.unique())\n\n# %% [code]\nsup_data = sup_data[    \n    sup_data.columns.to_list()[2:]]\n\n# %% [code]\nsup_data = sup_data.replace('N.A.', np.nan).fillna(-0.5)\n\n# %% [code]\nfor c in sup_data.columns[:-1]:\n    m = sup_data[c].max() #- sup_data \n    \n    if m > 300 and c!='TRUE_POPULATION':\n        print(c)\n        sup_data[c] = np.log(sup_data[c] + 1)\n        assert sup_data[c].min() > -1\n\n# %% [code]\nfor c in sup_data.columns[:-1]:\n    m = sup_data[c].max() #- sup_data \n    \n    if m > 300:\n        print(c)\n\n# %% [code]\n\n\n# %% [code]\nDEATHS = 'Fatalities'\n\n# %% [code]\n\n\n# %% [code]\nlen(train.Place.unique())\n\n# %% [code]\n\n\n# %% [markdown]\n# #### Correct Drop-Offs with interpolation\n\n# %% [raw]\n# \n# train[(train.ConfirmedCases.shift(1) > train.ConfirmedCases) & \n#          (train.Place == train.Place.shift(1)) & (train.ConfirmedCases == 0)]\n\n# %% [code]\n\n\n# %% [code]\ntrain.ConfirmedCases =     np.where(\n        (train.ConfirmedCases.shift(1) > train.ConfirmedCases) & \n        (train.ConfirmedCases.shift(1) > 0) & (train.ConfirmedCases.shift(-1) > 0) &\n         (train.Place == train.Place.shift(1)) & (train.Place == train.Place.shift(-1)) & \n        ~train.ConfirmedCases.shift(-1).isnull(),\n        \n        np.sqrt(train.ConfirmedCases.shift(1) * train.ConfirmedCases.shift(-1)),\n        \n        train.ConfirmedCases)\n\n\n\n# %% [code]\ntrain.Fatalities =     np.where(\n        (train.Fatalities.shift(1) > train.Fatalities) & \n        (train.Fatalities.shift(1) > 0) & (train.Fatalities.shift(-1) > 0) &\n         (train.Place == train.Place.shift(1)) & (train.Place == train.Place.shift(-1)) & \n        ~train.Fatalities.shift(-1).isnull(),\n        \n        np.sqrt(train.Fatalities.shift(1) * train.Fatalities.shift(-1)),\n        \n        train.Fatalities)\n\n\n\n# %% [code]\n\n\n# %% [code]\nfor i in [0, -1]:\n    train.ConfirmedCases =         np.where(\n            (train.ConfirmedCases.shift(2+ i ) > train.ConfirmedCases) & \n            (train.ConfirmedCases.shift(2+ i) > 0) & (train.ConfirmedCases.shift(-1+ i) > 0) &\n         (train.Place == train.Place.shift(2+ i)) & (train.Place == train.Place.shift(-1+ i)) & \n            ~train.ConfirmedCases.shift(-1+ i).isnull(),\n\n            np.sqrt(train.ConfirmedCases.shift(2+ i) * train.ConfirmedCases.shift(-1+ i)),\n\n            train.ConfirmedCases)\n\n\n\n# %% [code]\n\n\n\n# %% [code]\n\n\n# %% [code]\ntrain[train.Place=='USVirgin Islands'][-10:]\n\n# %% [code] {\"scrolled\":true}\n\ntrain[(train.ConfirmedCases.shift(2) > 2* train.ConfirmedCases) & \n         (train.Place == train.Place.shift(2)) & (train.ConfirmedCases < 100000)]\n\n# %% [code]\n\ntrain[(train.Fatalities.shift(1) > train.Fatalities) & \n\n      (train.Place == train.Place.shift(1)) & (train.Fatalities < 10000)]\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Use Training Set that is Old Predictions\n\n# %% [code]\n\n# %% [code]\ntrain_bk = train.copy()\n\n# %% [raw]\n# train.Date.unique()\n\n# %% [markdown]\n# #### Possible Truncation for Test Set Prediction\n\n# %% [code]\nfull_train = train.copy()\n\n# %% [raw]\n# full_train[full_train.Place =='USVirgin Islands']\n\n# %% [markdown]\n# ### Graphs\n\n# %% [code]\ntrain_c = train[train.Country == 'China']\ntrain_nc = train[train.Country != 'China']\ntrain_us = train[train.Country == 'US']\n# train_nc = train[train.Country != 'China']\n\n# %% [raw]\n# data.shape\n# data[data.ConfirmedCases > 0].shape\n# data.ConfirmedCases\n\n# %% [code]\ndef lplot(data, minDate = datetime.datetime(2000, 1, 1), \n              columns = ['ConfirmedCases', 'Fatalities']):\n    return\n        \n\n# %% [code]\nREAL = datetime.datetime(2020, 2, 10)\n\n\n# %% [code]\ndataset = train.copy()\n\n\nif TRUNCATED:\n    dataset = dataset[dataset.Country.isin(\n        ['Italy', 'Spain', 'Germany', 'Portugal', 'Belgium', 'Austria', 'Switzerland' ])]\n\n# %% [code]\ndataset.head()\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Create Lagged Growth Rates (4, 7, 12, 20 day rates)\n\n# %% [code]\ndef rollDates(df, i, preserve=False):\n    df = df.copy()\n    if preserve:\n        df['Date_i'] = df.Date\n    df.Date = df.Date + datetime.timedelta(i)\n    return df\n\n# %% [code]\nWINDOWS = [1, 2,  4, 7, 12, 20, 30]\n\n# %% [code]\nfor window in WINDOWS:\n    csuffix = '_{}d_prior_value'.format(window)\n    \n    base = rollDates(dataset, window)\n    dataset = pd.merge(dataset, base[['Date', 'Place',\n                'ConfirmedCases', 'Fatalities']], on = ['Date', 'Place'],\n            suffixes = ('', csuffix), how='left')\n#     break;\n    for c in ['ConfirmedCases', 'Fatalities']:\n        dataset[c+ csuffix].fillna(0, inplace=True)\n        dataset[c+ csuffix] = np.log(dataset[c + csuffix] + 1)\n        dataset[c+ '_{}d_prior_slope'.format(window)] =                     (np.log(dataset[c] + 1)                          - dataset[c+ csuffix]) / window\n        dataset[c+ '_{}d_ago_zero'.format(window)] = 1.0*(dataset[c+ csuffix] == 0)     \n    \n    \n    \n\n# %% [code]\nfor window1 in WINDOWS:\n    for window2 in WINDOWS:\n        for c in ['ConfirmedCases', 'Fatalities']:\n            if window1 * 1.3 < window2 and window1 * 5 > window2:\n                dataset[ c +'_{}d_{}d_prior_slope_chg'.format(window1, window2) ] =                         dataset[c+ '_{}d_prior_slope'.format(window1)]                                 - dataset[c+ '_{}d_prior_slope'.format(window2)]\n                \n                \n\n# %% [raw]\n# dataset.tail()\n\n# %% [raw]\n# dataset\n\n# %% [markdown]\n# #### First Case Etc.\n\n# %% [code]\nfirst_case = dataset[dataset.ConfirmedCases >= 1].groupby('Place').min() \ntenth_case = dataset[dataset.ConfirmedCases >= 10].groupby('Place').min()\nhundredth_case = dataset[dataset.ConfirmedCases >= 100].groupby('Place').min()\nthousandth_case = dataset[dataset.ConfirmedCases >= 1000].groupby('Place').min()\n\n# %% [code]\nfirst_fatality = dataset[dataset.Fatalities >= 1].groupby('Place').min()\ntenth_fatality = dataset[dataset.Fatalities >= 10].groupby('Place').min()\nhundredth_fatality = dataset[dataset.Fatalities >= 100].groupby('Place').min()\nthousandth_fatality = dataset[dataset.Fatalities >= 1000].groupby('Place').min()\n\n\n# %% [raw]\n# np.isinf(dataset.days_since_hundredth_case).sum()\n\n# %% [raw]\n# (dataset.Date - hundredth_case.loc[dataset.Place].Date.values).dt.days\n\n# %% [code]\ndataset['days_since_first_case'] =         np.clip((dataset.Date - first_case.loc[dataset.Place].Date.values).dt.days                            .fillna(-1), -1, None)\ndataset['days_since_tenth_case'] =         np.clip((dataset.Date - tenth_case.loc[dataset.Place].Date.values).dt.days                            .fillna(-1), -1, None)\ndataset['days_since_hundredth_case'] =         np.clip((dataset.Date - hundredth_case.loc[dataset.Place].Date.values).dt.days                            .fillna(-1), -1, None)\ndataset['days_since_thousandth_case'] =         np.clip((dataset.Date - thousandth_case.loc[dataset.Place].Date.values).dt.days                            .fillna(-1), -1, None)\n\n\n# %% [code]\ndataset['days_since_first_fatality'] =         np.clip((dataset.Date - first_fatality.loc[dataset.Place].Date.values).dt.days                    .fillna(-1), -1, None)\ndataset['days_since_tenth_fatality'] =         np.clip((dataset.Date - tenth_fatality.loc[dataset.Place].Date.values).dt.days                    .fillna(-1), -1, None)\ndataset['days_since_hundredth_fatality'] =         np.clip((dataset.Date - hundredth_fatality.loc[dataset.Place].Date.values).dt.days                    .fillna(-1), -1, None)\ndataset['days_since_thousandth_fatality'] =         np.clip((dataset.Date - thousandth_fatality.loc[dataset.Place].Date.values).dt.days                    .fillna(-1), -1, None)\n\n# %% [code]\n\n\n# %% [code]\ndataset['case_rate_since_first_case'] =     np.clip((np.log(dataset.ConfirmedCases + 1)              - np.log(first_case.loc[dataset.Place].ConfirmedCases.fillna(0).values + 1))                     / (dataset.days_since_first_case+0.01), 0, 1)\ndataset['case_rate_since_tenth_case'] =     np.clip((np.log(dataset.ConfirmedCases + 1)              - np.log(tenth_case.loc[dataset.Place].ConfirmedCases.fillna(0).values + 1))                     / (dataset.days_since_tenth_case+0.01), 0, 1)\ndataset['case_rate_since_hundredth_case'] =     np.clip((np.log(dataset.ConfirmedCases + 1)              - np.log(hundredth_case.loc[dataset.Place].ConfirmedCases.fillna(0).values + 1))                     / (dataset.days_since_first_case+0.01), 0, 1)\ndataset['case_rate_since_thousandth_case'] =     np.clip((np.log(dataset.ConfirmedCases + 1)              - np.log(thousandth_case.loc[dataset.Place].ConfirmedCases.fillna(0).values + 1))                     / (dataset.days_since_first_case+0.01), 0, 1)\n\n# %% [code]\ndataset['fatality_rate_since_first_case'] =     np.clip((np.log(dataset.Fatalities + 1)              - np.log(first_case.loc[dataset.Place].Fatalities.fillna(0).values + 1))                     / (dataset.days_since_first_case+0.01), 0, 1)\ndataset['fatality_rate_since_tenth_case'] =     np.clip((np.log(dataset.Fatalities + 1)              - np.log(tenth_case.loc[dataset.Place].Fatalities.fillna(0).values + 1))                     / (dataset.days_since_first_case+0.01), 0, 1)\ndataset['fatality_rate_since_hundredth_case'] =     np.clip((np.log(dataset.Fatalities + 1)              - np.log(hundredth_case.loc[dataset.Place].Fatalities.fillna(0).values + 1))                     / (dataset.days_since_first_case+0.01), 0, 1)\ndataset['fatality_rate_since_thousandth_case'] =     np.clip((np.log(dataset.Fatalities + 1)              - np.log(thousandth_case.loc[dataset.Place].Fatalities.fillna(0).values + 1))                     / (dataset.days_since_first_case+0.01), 0, 1)\n\n\n#.plot(kind='hist', bins = 150)\n\n# %% [code]\ndataset['fatality_rate_since_first_fatality'] =     np.clip((np.log(dataset.Fatalities + 1)              - np.log(first_fatality.loc[dataset.Place].Fatalities.fillna(0).values + 1))                     / (dataset.days_since_first_fatality+0.01), 0, 1)\ndataset['fatality_rate_since_tenth_fatality'] =     np.clip((np.log(dataset.Fatalities + 1)              - np.log(tenth_fatality.loc[dataset.Place].Fatalities.fillna(0).values + 1))                     / (dataset.days_since_tenth_fatality+0.01), 0, 1)\ndataset['fatality_rate_since_hundredth_fatality'] =     np.clip((np.log(dataset.Fatalities + 1)              - np.log(hundredth_fatality.loc[dataset.Place].Fatalities.fillna(0).values + 1))                     / (dataset.days_since_hundredth_fatality+0.01), 0, 1)\ndataset['fatality_rate_since_thousandth_fatality'] =     np.clip((np.log(dataset.Fatalities + 1)              - np.log(thousandth_fatality.loc[dataset.Place].Fatalities.fillna(0).values + 1))                     / (dataset.days_since_thousandth_fatality+0.01), 0, 1)\n \n#.plot(kind='hist', bins = 150)\n\n# %% [code]\n\n\n# %% [code]\ndataset['first_case_ConfirmedCases'] =        np.log(first_case.loc[dataset.Place].ConfirmedCases.values + 1)\ndataset['first_case_Fatalities'] =        np.log(first_case.loc[dataset.Place].Fatalities.values + 1)\n\n# %% [code]\n\n\n# %% [code]\ndataset['first_fatality_ConfirmedCases'] =        np.log(first_fatality.loc[dataset.Place].ConfirmedCases.fillna(0).values + 1)             * (dataset.days_since_first_fatality >= 0 )\ndataset['first_fatality_Fatalities'] =        np.log(first_fatality.loc[dataset.Place].Fatalities.fillna(0).values + 1)             * (dataset.days_since_first_fatality >= 0 )\n\n# %% [code]\ndataset['first_fatality_cfr'] =     np.where(dataset.days_since_first_fatality < 0,\n            -8,\n        (dataset.first_fatality_Fatalities) -\n               (dataset.first_fatality_ConfirmedCases )   )\n\n# %% [code]\ndataset['first_fatality_lag_vs_first_case'] =     np.where(dataset.days_since_first_fatality >= 0,\n                 dataset.days_since_first_case - dataset.days_since_first_fatality , -1)\n\n# %% [code]\n\n\n# %% [markdown]\n# #### Update Frequency, MAs of Change Rates, etc.\n\n# %% [code]\ndataset['case_chg'] =     np.clip(np.log(dataset.ConfirmedCases + 1 )            - np.log(dataset.ConfirmedCases.shift(1) +1), 0, None).fillna(0)\n\n# %% [code]\ndataset['case_chg_ema_3d'] = dataset.case_chg.ewm(span = 3).mean()                                 * np.clip( (dataset.Date - dataset.Date.min() ).dt.days/3, 0, 1)\ndataset['case_chg_ema_10d'] = dataset.case_chg.ewm(span = 10).mean()                              * np.clip( (dataset.Date - dataset.Date.min() ).dt.days/10, 0, 1)\n\n# %% [code]\ndataset['case_chg_stdev_5d'] = dataset.case_chg.rolling(5).std()                                 * np.clip( (dataset.Date - dataset.Date.min() ).dt.days/5, 0, 1)\ndataset['case_chg_stdev_15d'] = dataset.case_chg.rolling(15).std()                                 * np.clip( (dataset.Date - dataset.Date.min() ).dt.days/15, 0, 1)\n\n# %% [raw]\n# dataset['max_case_chg_3d'] = dataset.case_chg.rolling(3).max() \\\n#                                  * np.clip( (dataset.Date - dataset.Date.min() ).dt.days/3, 0, 1)\n# dataset['max_case_chg_10d'] = dataset.case_chg.rolling(10).max() \\\n#                                  * np.clip( (dataset.Date - dataset.Date.min() ).dt.days/10, 0, 1)\n\n# %% [code]\ndataset['case_update_pct_3d_ewm'] = (dataset.case_chg > 0).ewm(span = 3).mean()                      * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days/3, 0, 1), 2)\ndataset['case_update_pct_10d_ewm'] = (dataset.case_chg > 0).ewm(span = 10).mean()                      * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days/10, 0, 1), 2)\ndataset['case_update_pct_30d_ewm'] = (dataset.case_chg > 0).ewm(span = 30).mean()                      * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days/30, 0, 1), 2)\n\n \n\n# %% [code]\n\n\n# %% [code]\ndataset['fatality_chg'] =     np.clip(np.log(dataset.Fatalities + 1 )            - np.log(dataset.Fatalities.shift(1) +1), 0, None).fillna(0)\n\n# %% [code]\ndataset['fatality_chg_ema_3d'] = dataset.fatality_chg.ewm(span = 3).mean()                     * np.clip( (dataset.Date - dataset.Date.min() ).dt.days/33, 0, 1)\ndataset['fatality_chg_ema_10d'] = dataset.fatality_chg.ewm(span = 10).mean()                     * np.clip( (dataset.Date - dataset.Date.min() ).dt.days/10, 0, 1)\n\n# %% [code]\ndataset['fatality_chg_stdev_5d'] = dataset.fatality_chg.rolling(5).std()                                 * np.clip( (dataset.Date - dataset.Date.min() ).dt.days/5, 0, 1)\ndataset['fatality_chg_stdev_15d'] = dataset.fatality_chg.rolling(15).std()                                 * np.clip( (dataset.Date - dataset.Date.min() ).dt.days/15, 0, 1)\n\n# %% [code]\ndataset['fatality_update_pct_3d_ewm'] = (dataset.fatality_chg > 0).ewm(span = 3).mean()                      * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days/3, 0, 1), 2)\ndataset['fatality_update_pct_10d_ewm'] = (dataset.fatality_chg > 0).ewm(span = 10).mean()                      * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days/10, 0, 1), 2)\ndataset['fatality_update_pct_30d_ewm'] = (dataset.fatality_chg > 0).ewm(span = 30).mean()                      * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days/30, 0, 1), 2)\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\ndataset.tail()\n\n# %% [code]\n\n\n# %% [markdown]\n# #### Add Supp Data\n\n# %% [code]\n# lag containment data as one week behind\ncontain_data.Date = contain_data.Date + datetime.timedelta(7)\n\n# %% [code]\ncontain_data.Date.max()\n\n# %% [code]\nassert set(dataset.Place.unique()) == set(dataset.Place.unique())\ndataset = pd.merge(dataset, sup_data, on='Place', how='left', validate='m:1')\ndataset = pd.merge(dataset, contain_data, on = ['Country', 'Date'], how='left', validate='m:1')\n\n# %% [code]\ndataset['log_true_population'] =   np.log(dataset.TRUE_POPULATION + 1)\n\n# %% [code]\ndataset['ConfirmedCases_percapita'] = np.log(dataset.ConfirmedCases + 1)                                        - np.log(dataset.TRUE_POPULATION + 1)\ndataset['Fatalities_percapita'] = np.log(dataset.Fatalities + 1)                                        - np.log(dataset.TRUE_POPULATION + 1)\n\n# %% [code]\n\n\n# %% [markdown]\n# ##### CFR\n\n# %% [raw]\n# np.log( 0 + 0.015/1)\n\n# %% [raw]\n# BLCFR = -4.295015257684252\n\n# %% [code] {\"scrolled\":true}\n# dataset['log_cfr_bad'] = np.log(dataset.Fatalities + 1) - np.log(dataset.ConfirmedCases + 1)\ndataset['log_cfr'] = np.log(    (dataset.Fatalities                                          + np.clip(0.015 * dataset.ConfirmedCases, 0, 0.3))                             / ( dataset.ConfirmedCases + 0.1) )\n\n# %% [code]\ndef cfr(case, fatality):\n    cfr_calc = np.log(    (fatality                                          + np.clip(0.015 * case, 0, 0.3))                             / ( case + 0.1) )\n#     cfr_calc =np.array(cfr_calc)\n    return np.where(np.isnan(cfr_calc) | np.isinf(cfr_calc),\n                           BLCFR, cfr_calc)\n\n# %% [code]\nBLCFR = np.median(dataset[dataset.ConfirmedCases==1].log_cfr[::10])\ndataset.log_cfr.fillna(BLCFR, inplace=True)\ndataset.log_cfr = np.where(dataset.log_cfr.isnull() | np.isinf(dataset.log_cfr),\n                           BLCFR, dataset.log_cfr)\nBLCFR\n\n# %% [code]\ndataset['log_cfr_3d_ewm'] = BLCFR +                 (dataset.log_cfr - BLCFR).ewm(span = 3).mean()                       * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days/3, 0, 1), 2)\n                     \ndataset['log_cfr_8d_ewm'] = BLCFR +                 (dataset.log_cfr - BLCFR).ewm(span = 8).mean()                       * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days/8, 0, 1), 2)\n\ndataset['log_cfr_20d_ewm'] = BLCFR +                 (dataset.log_cfr - BLCFR).ewm(span = 20).mean()                       * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days/20, 0, 1), 2)\n\ndataset['log_cfr_3d_20d_ewm_crossover'] = dataset.log_cfr_3d_ewm - dataset.log_cfr_20d_ewm\n\n\n# %% [code]\ndataset.drop(columns = 'log_cfr', inplace=True)\n\n\n\n# %% [code]\n\n\n# %% [markdown]\n# ##### Per Capita vs. World and Similar Countries\n\n# %% [code]\ndate_totals = dataset.groupby('Date').sum()\n\n# %% [code]\nmean_7d_c_slope = dataset.groupby('Date')[['ConfirmedCases_7d_prior_slope']].apply(lambda x:\n                                        np.mean(x[x > 0]) ).ewm(span = 3).mean() \nmean_7d_f_slope = dataset.groupby('Date')[['Fatalities_7d_prior_slope']].apply(lambda x:\n                                        np.mean(x[x > 0]) ).ewm(span = 7).mean()\n\n# %% [raw]\n# mean_7d_c_slope.plot()\n\n# %% [raw]\n# dataset.columns[:100]\n\n# %% [raw]\n# mean_7d_c_slope.plot()\n\n# %% [raw]\n# date_totals.Fatalities_7d_prior_slope.plot()\n\n# %% [raw]\n# date_counts = dataset.groupby('Date').apply(lambda x:  x > 0)\n\n# %% [raw]\n# date_counts\n\n# %% [raw]\n# date_totals['world_cases_chg'] = (np.log(date_totals.ConfirmedCases + 1 )\\\n#                                     - np.log(date_totals.ConfirmedCases.shift(1) + 1) )\\\n#                                     .fillna(method='bfill')\n# date_totals['world_fatalities_chg'] = (np.log(date_totals.Fatalities + 1 )\\\n#                                     - np.log(date_totals.Fatalities.shift(1) + 1) )\\\n#                                     .fillna(method='bfill')\n# date_totals['world_cases_chg_10d_ewm'] = \\\n#         date_totals.world_cases_chg.ewm(span=10).mean()\n# date_totals['world_fatalities_chg_10d_ewm'] = \\\n#         date_totals.world_fatalities_chg.ewm(span=10).mean()  \n\n# %% [raw]\n# \n# dataset['world_cases_chg_10d_ewm'] = \\\n#         date_totals.loc[dataset.Date].world_cases_chg_10d_ewm.values\n# \n# dataset['world_fatalities_chg_10d_ewm'] = \\\n#         date_totals.loc[dataset.Date].world_fatalities_chg_10d_ewm.values\n# \n\n# %% [raw]\n# dataset.continent\n\n# %% [raw]\n# date_totals\n\n# %% [code]\ndataset['ConfirmedCases_percapita_vs_world'] = np.log(dataset.ConfirmedCases + 1)                                        - np.log(dataset.TRUE_POPULATION + 1)                                    -  (\n                                   np.log(date_totals.loc[dataset.Date].ConfirmedCases + 1)  \n                                       -np.log(date_totals.loc[dataset.Date].TRUE_POPULATION + 1)\n                                        ).values\n\ndataset['Fatalities_percapita_vs_world'] = np.log(dataset.Fatalities + 1)                                            - np.log(dataset.TRUE_POPULATION + 1)                                     -  (\n                                   np.log(date_totals.loc[dataset.Date].Fatalities + 1)  \n                                       -np.log(date_totals.loc[dataset.Date].TRUE_POPULATION + 1)\n                                        ).values\ndataset['cfr_vs_world'] = dataset.log_cfr_3d_ewm                             -    np.log(    date_totals.loc[dataset.Date].Fatalities                               /   date_totals.loc[dataset.Date].ConfirmedCases ).values\n\n# %% [code]\n\n\n# %% [markdown]\n# #### Nearby Countries\n\n# %% [code]\ncont_date_totals = dataset.groupby(['Date', 'continent_generosity']).sum()\n\n# %% [raw]\n# cont_date_totals.iloc[dataset.Date]\n\n# %% [code]\nlen(dataset)\n\n# %% [raw]\n# dataset.columns\n\n# %% [raw]\n# dataset.TRUE_POPULATION\n\n# %% [raw]\n# dataset\n\n# %% [raw]\n# dataset\n\n# %% [code]\ndataset['ConfirmedCases_percapita_vs_continent_mean'] = 0\ndataset['Fatalities_percapita_vs_continent_mean'] = 0\ndataset['ConfirmedCases_percapita_vs_continent_median'] = 0\ndataset['Fatalities_percapita_vs_continent_median'] = 0\n\nfor cg in dataset.continent_generosity.unique():\n    ps = dataset.groupby(\"Place\").last()\n    tp = ps[ps.continent_generosity==cg].TRUE_POPULATION.sum()\n    print(tp / 1e9)\n    for Date in dataset.Date.unique():\n        cd =  dataset[(dataset.Date == Date) &\n                    (dataset.continent_generosity == cg)]\\\n                               [['ConfirmedCases', 'Fatalities', 'TRUE_POPULATION']]\n#         print(cd)\n        cmedian = np.median(np.log(cd.ConfirmedCases + 1)                                              - np.log(cd.TRUE_POPULATION+1))\n        cmean = np.log(cd.ConfirmedCases.sum() + 1) - np.log(tp + 1)\n        fmedian = np.median(np.log(cd.Fatalities + 1)                                              - np.log(cd.TRUE_POPULATION+1))\n        fmean = np.log(cd.Fatalities.sum() + 1) - np.log(tp + 1)\n        cfrmean = cfr( cd.ConfirmedCases.sum(),  cd.Fatalities.sum()   ) \n#         print(cmean)\n        \n#         break;\n        \n        dataset.loc[(dataset.Date == Date) &\n                    (dataset.continent_generosity == cg), \n                    'ConfirmedCases_percapita_vs_continent_mean'] = \\\n                                dataset['ConfirmedCases_percapita'] \\\n                                     - (cmean)\n        dataset.loc[(dataset.Date == Date) &\n                    (dataset.continent_generosity == cg), \n                    'ConfirmedCases_percapita_vs_continent_median'] = \\\n                                dataset['ConfirmedCases_percapita'] \\\n                                     - (cmedian)\n        \n        dataset.loc[(dataset.Date == Date) &\n                    (dataset.continent_generosity == cg), \n                    'Fatalities_percapita_vs_continent_mean'] = \\\n                                dataset['Fatalities_percapita']\\\n                                    - (fmean)\n        dataset.loc[(dataset.Date == Date) &\n                    (dataset.continent_generosity == cg), \n                    'Fatalities_percapita_vs_continent_median'] = \\\n                                dataset['Fatalities_percapita']\\\n                                    - (fmedian)\n        \n        dataset.loc[(dataset.Date == Date) &\n                    (dataset.continent_generosity == cg), \n                    'cfr_vs_continent'] = \\\n                                dataset.log_cfr_3d_ewm \\\n                            -    cfrmean\n#       \n#         r.ConfirmedCases\n#         r.Fatalities\n#         print(continent)\n    \n\n# %% [code]\n\n\n# %% [raw]\n# dataset[dataset.Country=='China'][['Place', 'Date', \n#                'ConfirmedCases_percapita_vs_continent_mean',\n#                'Fatalities_percapita_vs_continent_mean']][1000::10]\n\n# %% [raw]\n# dataset[['Place', 'Date', \n#                'cfr_vs_continent']][10000::5]\n\n# %% [code]\n\n\n# %% [code]\nall_places = dataset[['Place', 'latitude', 'longitude']].drop_duplicates().set_index('Place',\n                                                                                    drop=True)\nall_places.head()\n\n# %% [code]\ndef surroundingPlaces(place, d = 10):\n    dist = (all_places.latitude - all_places.loc[place].latitude)**2                     + (all_places.longitude - all_places.loc[place].longitude) ** 2 \n    return all_places[dist < d**2][1:n+1]\n\n# %% [raw]\n# surroundingPlaces('Afghanistan', 5)\n\n# %% [code]\ndef nearestPlaces(place, n = 10):\n    dist = (all_places.latitude - all_places.loc[place].latitude)**2                     + (all_places.longitude - all_places.loc[place].longitude) ** 2\n    ranked = np.argsort(dist) \n    return all_places.iloc[ranked][1:n+1]\n\n# %% [code]\n\n\n# %% [raw]\n# dataset.ConfirmedCases_percapita\n\n# %% [code]\ndgp = dataset.groupby('Place').last()\nfor n in [5, 10, 20]:\n#     dataset['ConfirmedCases_percapita_vs_nearest{}'.format(n)] = 0\n#     dataset['Fatalities_percapita_vs_nearest{}'.format(n)] = 0\n    \n    for place in dataset.Place.unique():\n        nps = nearestPlaces(place, n)\n        tp = dgp.loc[nps.index].TRUE_POPULATION.sum()\n#         print(tp)\n        \n        \n        dataset.loc[dataset.Place==place, \n                    'ratio_population_vs_nearest{}'.format(n)] = \\\n            np.log(dataset.loc[dataset.Place==place].TRUE_POPULATION.mean() + 1)\\\n                - np.log(tp+1)\n         \n#         dataset.loc[dataset.Place==place, \n#                     'avg_distance_to_nearest{}'.format(n)] = \\\n#             (dataset.loc[dataset.Place==place].latitude.mean() + 1)\\\n#                 - np.log(tp+1)\n        \n\n        nbps =  dataset[(dataset.Place.isin(nps.index))]                            .groupby('Date')[['ConfirmedCases', 'Fatalities']].sum()\n\n        nppc = (np.log( nbps.loc[dataset[dataset.Place==place].Date]                                          .fillna(0).ConfirmedCases + 1) - np.log(tp + 1))\n        nppf = (np.log( nbps.loc[dataset[dataset.Place==place].Date]                                          .fillna(0).Fatalities + 1) - np.log(tp + 1))\n        npp_cfr = cfr( nbps.loc[dataset[dataset.Place==place].Date]                                          .fillna(0).ConfirmedCases,\n                      nbps.loc[dataset[dataset.Place==place].Date]\\\n                                          .fillna(0).Fatalities)\n#         print(npp_cfr)\n#         continue;\n        \n        dataset.loc[\n                (dataset.Place == place),\n                    'ConfirmedCases_percapita_vs_nearest{}'.format(n)] = \\\n            dataset[(dataset.Place == place)].ConfirmedCases_percapita \\\n                            - nppc.values\n        dataset.loc[ \n                (dataset.Place == place),\n                    'Fatalities_percapita_vs_nearest{}'.format(n)] = \\\n            dataset[(dataset.Place == place)].Fatalities_percapita \\\n                            - nppf.values\n        dataset.loc[ \n                (dataset.Place == place),\n                    'cfr_vs_nearest{}'.format(n)] = \\\n            dataset[(dataset.Place == place)].log_cfr_3d_ewm \\\n                            - npp_cfr   \n        \n        dataset.loc[\n                (dataset.Place == place),\n                    'ConfirmedCases_nearest{}_percapita'.format(n)] = nppc.values\n        dataset.loc[ \n                (dataset.Place == place),\n                    'Fatalities_nearest{}_percapita'.format(n)] = nppf.values\n        dataset.loc[ \n                (dataset.Place == place),\n                    'cfr_nearest{}'.format(n)] = npp_cfr\n        \n        dataset.loc[\n                (dataset.Place == place),\n                    'ConfirmedCases_nearest{}_10d_slope'.format(n)] =   \\\n                               ( nppc.ewm(span = 1).mean() - nppc.ewm(span = 10).mean() ).values\n        dataset.loc[\n                (dataset.Place == place),\n                    'Fatalities_nearest{}_10d_slope'.format(n)] =   \\\n                               ( nppf.ewm(span = 1).mean() - nppf.ewm(span = 10).mean() ).values\n        \n        npp_cfr_s = pd.Series(npp_cfr)\n        dataset.loc[ \n                (dataset.Place == place),\n                    'cfr_nearest{}_10d_slope'.format(n)] = \\\n                            ( npp_cfr_s.ewm(span = 1).mean()\\\n                                     - npp_cfr_s.ewm(span = 10).mean() ) .values\n        \n#         print(( npp_cfr_s.ewm(span = 1).mean()\\\n#                                      - npp_cfr_s.ewm(span = 10).mean() ).values)\n        \n\n# %% [code]\n\n\n# %% [code]\ndgp = dataset.groupby('Place').last()\nfor d in [5, 10, 20]:\n#     dataset['ConfirmedCases_percapita_vs_nearest{}'.format(n)] = 0\n#     dataset['Fatalities_percapita_vs_nearest{}'.format(n)] = 0\n    \n    for place in dataset.Place.unique():\n        nps = surroundingPlaces(place, d)\n        dataset.loc[dataset.Place==place, 'num_surrounding_places_{}_degrees'.format(d)] =             len(nps)\n        \n        \n        tp = dgp.loc[nps.index].TRUE_POPULATION.sum()\n        \n        dataset.loc[dataset.Place==place, \n                    'ratio_population_vs_surrounding_places_{}_degrees'.format(d)] = \\\n            np.log(dataset.loc[dataset.Place==place].TRUE_POPULATION.mean() + 1)\\\n                - np.log(tp+1)\n        \n        if len(nps)==0:\n            continue;\n            \n#         print(place)\n#         print(nps)\n#         print(tp)\n        nbps =  dataset[(dataset.Place.isin(nps.index))]                            .groupby('Date')[['ConfirmedCases', 'Fatalities']].sum()\n\n#         print(nbps)\n        nppc = (np.log( nbps.loc[dataset[dataset.Place==place].Date]                                          .fillna(0).ConfirmedCases + 1) - np.log(tp + 1))\n        nppf = (np.log( nbps.loc[dataset[dataset.Place==place].Date]                                          .fillna(0).Fatalities + 1) - np.log(tp + 1))\n#         break;\n        npp_cfr = cfr( nbps.loc[dataset[dataset.Place==place].Date]                                          .fillna(0).ConfirmedCases,\n                      nbps.loc[dataset[dataset.Place==place].Date]\\\n                                          .fillna(0).Fatalities)\n        dataset.loc[\n                (dataset.Place == place),\n                    'ConfirmedCases_percapita_vs_surrounding_places_{}_degrees'.format(d)] = \\\n            dataset[(dataset.Place == place)].ConfirmedCases_percapita \\\n                            - nppc.values\n        dataset.loc[ \n                (dataset.Place == place),\n                    'Fatalities_percapita_vs_surrounding_places_{}_degrees'.format(d)] = \\\n            dataset[(dataset.Place == place)].Fatalities_percapita \\\n                            - nppf.values\n        dataset.loc[ \n                (dataset.Place == place),\n                    'cfr_vs_surrounding_places_{}_degrees'.format(d)] = \\\n            dataset[(dataset.Place == place)].log_cfr_3d_ewm \\\n                            - npp_cfr   \n        \n        \n        dataset.loc[\n                (dataset.Place == place),\n                    'ConfirmedCases_surrounding_places_{}_degrees_percapita'.format(d)] = nppc.values\n        dataset.loc[ \n                (dataset.Place == place),\n                    'Fatalities_surrounding_places_{}_degrees_percapita'.format(d)] = nppf.values\n        dataset.loc[ \n                (dataset.Place == place),\n                    'cfr_surrounding_places_{}_degrees'.format(d)] = npp_cfr\n        \n        dataset.loc[\n                (dataset.Place == place),\n                    'ConfirmedCases_surrounding_places_{}_degrees_10d_slope'.format(d)] =   \\\n                               ( nppc.ewm(span = 1).mean() - nppc.ewm(span = 10).mean() ).values\n        dataset.loc[\n                (dataset.Place == place),\n                    'Fatalities_surrounding_places_{}_degrees_10d_slope'.format(d)] =   \\\n                               ( nppf.ewm(span = 1).mean() - nppf.ewm(span = 10).mean() ).values\n        npp_cfr_s = pd.Series(npp_cfr)\n        dataset.loc[ \n                (dataset.Place == place),\n                    'cfr_surrounding_places_{}_degrees_10d_slope'.format(d)] = \\\n                            ( npp_cfr_s.ewm(span = 1).mean()\\\n                                     - npp_cfr_s.ewm(span = 10).mean() ) .values\n        \n\n# %% [code]\n\n\n# %% [code]\nfor col in [c for c in dataset.columns if 'surrounding_places' in c and 'num_sur' not in c]:\n    dataset[col] = dataset[col].fillna(0)\n    n_col = 'num_surrounding_places_{}_degrees'.format(col.split('degrees')[0]                                                           .split('_')[-2])\n\n    print(col)\n#     print(n_col)\n    dataset[col + \"_times_num_places\"] = dataset[col] * np.sqrt(dataset[n_col])\n#     print('num_surrounding_places_{}_degrees'.format(col.split('degrees')[0][-2:-1]))\n\n# %% [code]\ndataset[dataset.Country=='US'][['Place', 'Date']                                      + [c for c in dataset.columns if 'ratio_p' in c]]                [::50]\n\n# %% [code]\n\n\n# %% [raw]\n# dataset[dataset.Country==\"US\"].groupby('Place').last()\\\n#         [[c for c in dataset.columns if 'cfr' in c]].iloc[:10, 8:]\n\n# %% [code]\n\n\n# %% [raw]\n# dataset[dataset.Place=='USAlabama'][['Place', 'Date'] \\\n#                                      + [c for c in dataset.columns if 'places_5_degree' in c]]\\\n#                [40::5]\n\n# %% [code]\n\n\n# %% [code]\ndataset.TRUE_POPULATION\n\n# %% [code]\ndataset.TRUE_POPULATION.sum()\n\n# %% [code]\ndataset.groupby('Date').sum().TRUE_POPULATION\n\n# %% [code]\n\n\n# %% [raw]\n# dataset[dataset.ConfirmedCases>0]['log_cfr'].plot(kind='hist', bins = 250)\n\n# %% [raw]\n# dataset.log_cfr.isnull().sum()\n\n# %% [code]\ndataset['first_case_ConfirmedCases_percapita'] =        np.log(dataset.first_case_ConfirmedCases + 1)           - np.log(dataset.TRUE_POPULATION + 1)\n\ndataset['first_case_Fatalities_percapita'] =        np.log(dataset.first_case_Fatalities + 1)           - np.log(dataset.TRUE_POPULATION + 1)\n\ndataset['first_fatality_Fatalities_percapita'] =        np.log(dataset.first_fatality_Fatalities + 1)           - np.log(dataset.TRUE_POPULATION + 1)\n\ndataset['first_fatality_ConfirmedCases_percapita'] =         np.log(dataset.first_fatality_ConfirmedCases + 1)            - np.log(dataset.TRUE_POPULATION + 1)\n\n# %% [code]\n\n\n# %% [code]\n \ndataset['days_to_saturation_ConfirmedCases_4d'] =                                 ( - np.log(dataset.ConfirmedCases + 1)                                        + np.log(dataset.TRUE_POPULATION + 1))                             / dataset.ConfirmedCases_4d_prior_slope         \ndataset['days_to_saturation_ConfirmedCases_7d'] =                                 ( - np.log(dataset.ConfirmedCases + 1)                                        + np.log(dataset.TRUE_POPULATION + 1))                             / dataset.ConfirmedCases_7d_prior_slope         \n\n    \ndataset['days_to_saturation_Fatalities_20d_cases'] =                                 ( - np.log(dataset.Fatalities + 1)                                        + np.log(dataset.TRUE_POPULATION + 1))                             / dataset.ConfirmedCases_20d_prior_slope         \ndataset['days_to_saturation_Fatalities_12d_cases'] =                                 ( - np.log(dataset.Fatalities + 1)                                        + np.log(dataset.TRUE_POPULATION + 1))                             / dataset.ConfirmedCases_12d_prior_slope         \n \n\n# %% [code]\ndataset['days_to_3pct_ConfirmedCases_4d'] =                                 ( - np.log(dataset.ConfirmedCases + 1)                                        + np.log(dataset.TRUE_POPULATION + 1) - 3.5)                             / dataset.ConfirmedCases_4d_prior_slope         \ndataset['days_to_3pct_ConfirmedCases_7d'] =                                 ( - np.log(dataset.ConfirmedCases + 1)                                        + np.log(dataset.TRUE_POPULATION + 1) - 3.5)                             / dataset.ConfirmedCases_7d_prior_slope         \n\n    \ndataset['days_to_0.3pct_Fatalities_20d_cases'] =                                 ( - np.log(dataset.Fatalities + 1)                                        + np.log(dataset.TRUE_POPULATION + 1) - 5.8)                             / dataset.ConfirmedCases_20d_prior_slope         \ndataset['days_to_0.3pct_Fatalities_12d_cases'] =                                 ( - np.log(dataset.Fatalities + 1)                                        + np.log(dataset.TRUE_POPULATION + 1) - 5.8)                             / dataset.ConfirmedCases_12d_prior_slope         \n \n\n# %% [code]\n\n\n# %% [raw]\n# \n\n# %% [code]\n\n\n# %% [code]\ndataset.tail()\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Build Intervals into Future\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\ndataset = dataset[dataset.ConfirmedCases > 0]\n\nlen(dataset)\n\n# %% [code]\ndatas = []\nfor window in range(1, 35):\n    base = rollDates(dataset, window, True)\n    datas.append(pd.merge(dataset[['Date', 'Place',\n                 'ConfirmedCases', 'Fatalities']], base, on = ['Date', 'Place'],\n                          how = 'right', \n            suffixes = ('_f', '')))\ndata = pd.concat(datas, axis =0).astype(np.float32, errors ='ignore')\n\n# %% [code]\nlen(data)\n\n# %% [raw]\n# data[data.Place=='USNew York']\n\n# %% [code]\ndata['Date_f'] = data.Date\ndata.Date = data.Date_i\n\n# %% [code]\ndata['elapsed'] = (data.Date_f - data.Date_i).dt.days\n\n# %% [code]\ndata['CaseChgRate'] = (np.log(data.ConfirmedCases_f + 1) - np.log(data.ConfirmedCases + 1))                            / data.elapsed;\ndata['FatalityChgRate'] = (np.log(data.Fatalities_f + 1) - np.log(data.Fatalities + 1))                            / data.elapsed;\n\n\n# %% [code]\n\n\n# %% [code]\ndata.elapsed\n\n# %% [code]\n\n\n# %% [raw]\n# data[slope_cols]\n\n# %% [raw]\n# [c for c in data.columns if any(z in c for z in [ 'rate']) ]\n\n# %% [code]\nfalloff_hash = {}\n\n# %% [code]\n\n\n# %% [code]\ndef true_agg(rate_i, elapsed, bend_rate):\n#     print(elapsed); \n    elapsed = int(elapsed)\n#     ar = 0\n#     rate = rate_i\n#     for i in range(0, elapsed):\n#         rate *= bend_rate\n#         ar += rate\n#     return ar\n\n    if (bend_rate, elapsed) not in falloff_hash:\n        falloff_hash[(bend_rate, elapsed)] =             np.sum( [  np.power(bend_rate, e) for e in range(1, elapsed+1)] )\n    return falloff_hash[(bend_rate, elapsed)] * rate_i\n     \n\n# %% [code]\ntrue_agg(0.3, 30, 0.9)\n\n# %% [raw]\n# %timeit true_agg(0.3, 30, 0.9)\n\n# %% [code]\nslope_cols = [c for c in data.columns if \n                      any(z in c for z in ['prior_slope', 'chg', 'rate'])\n           and not any(z in c for z in ['bend', 'prior_slope_chg', 'Country', 'ewm', \n                                        ]) ] # ** bid change; since rate too stationary\nprint(slope_cols)\nbend_rates = [1, 0.95, 0.90]\nfor bend_rate in bend_rates:\n    bend_agg = data[['elapsed']].apply(lambda x: true_agg(1, *x, bend_rate), axis=1)\n     \n    for sc in slope_cols:\n        if bend_rate < 1:\n            data[sc+\"_slope_bend_{}\".format(bend_rate)] =  data[sc]                                      * np.power((bend_rate + 1)/2, data.elapsed)\n         \n            data[sc+\"_true_slope_bend_{}\".format(bend_rate)] =                           bend_agg *  data[sc] / data.elapsed\n            \n        data[sc+\"_agg_bend_{}\".format(bend_rate)] =  data[sc] * data.elapsed                                 * np.power((bend_rate + 1)/2, data.elapsed)\n         \n        data[sc+\"_true_agg_bend_{}\".format(bend_rate)] =                         bend_agg *  data[sc]\n#                       data[[sc, 'elapsed']].apply(lambda x: true_agg(*x, bend_rate), axis=1) \n        \n         \n#         print(data[sc+\"_true_agg_bend_{}\".format(bend_rate)])\n\n# %% [raw]\n# data[[c for c in data.columns if 'Fatalities_7d_prior_slope' in c and 'true_agg' in c]]\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [raw]\n# data[data.Place=='USNew York'][['elapsed'] +[c for c in data.columns if 'ses_4d_prior_slope' in c]]\n\n# %% [code]\nslope_cols[:5]\n\n# %% [raw]\n# data\n\n# %% [code]\nfor col in [c for c in data.columns if any(z in c for z in \n                               ['vs_continent', 'nearest', 'vs_world', 'surrounding_places'])]:\n#     print(col)\n    data[col + '_times_days'] = data[col] * data.elapsed\n\n# %% [code]\ndata['saturation_slope_ConfirmedCases'] = (- np.log(data.ConfirmedCases + 1)                                                        + np.log(data.TRUE_POPULATION + 1))                                                     / data.elapsed\ndata['saturation_slope_Fatalities'] = (- np.log(data.Fatalities + 1)                                                + np.log(data.TRUE_POPULATION + 1))                                                     / data.elapsed\n\ndata['dist_to_ConfirmedCases_saturation_times_days'] = (- np.log(data.ConfirmedCases + 1)                                                        + np.log(data.TRUE_POPULATION + 1))                                                     * data.elapsed\ndata['dist_to_Fatalities_saturation_times_days'] = (- np.log(data.Fatalities + 1)                                                + np.log(data.TRUE_POPULATION + 1))                                                     * data.elapsed\n        \n\n\ndata['slope_to_1pct_ConfirmedCases'] = (- np.log(data.ConfirmedCases + 1)                                                        + np.log(data.TRUE_POPULATION + 1) - 4.6)                                                     / data.elapsed\ndata['slope_to_0.1pct_Fatalities'] = (- np.log(data.Fatalities + 1)                                                + np.log(data.TRUE_POPULATION + 1) - 6.9)                                                     / data.elapsed\n\ndata['dist_to_1pct_ConfirmedCases_times_days'] = (- np.log(data.ConfirmedCases + 1)                                                        + np.log(data.TRUE_POPULATION + 1) - 4.6)                                                     * data.elapsed\ndata['dist_to_0.1pct_Fatalities_times_days'] = (- np.log(data.Fatalities + 1)                                                + np.log(data.TRUE_POPULATION + 1) - 6.9)                                                     * data.elapsed\n\n# %% [raw]\n# data.ConfirmedCases_12d_prior_slope.plot(kind='hist')\n\n# %% [code]\ndata['trendline_per_capita_ConfirmedCases_4d_slope'] = ( np.log(data.ConfirmedCases + 1)                                                        - np.log(data.TRUE_POPULATION + 1))                                        + (data.ConfirmedCases_4d_prior_slope * data.elapsed)\ndata['trendline_per_capita_ConfirmedCases_7d_slope'] = ( np.log(data.ConfirmedCases + 1)                                                        - np.log(data.TRUE_POPULATION + 1))                                        + (data.ConfirmedCases_7d_prior_slope * data.elapsed)\n \n\ndata['trendline_per_capita_Fatalities_12d_slope'] = ( np.log(data.Fatalities + 1)                                                        - np.log(data.TRUE_POPULATION + 1))                                        + (data.ConfirmedCases_12d_prior_slope * data.elapsed)\ndata['trendline_per_capita_Fatalities_20d_slope'] = ( np.log(data.Fatalities + 1)                                                        - np.log(data.TRUE_POPULATION + 1))                                        + (data.ConfirmedCases_20d_prior_slope * data.elapsed)\n\n \n\n# %% [code]\n\n\n# %% [raw]\n# data[data.Place == 'USNew York']\n\n# %% [code]\nlen(data)\n\n# %% [raw]\n# data.CaseChgRate.plot(kind='hist', bins = 250);\n\n# %% [code]\n\n\n# %% [raw]\n# data_bk = data.copy()\n\n# %% [code]\n\n\n# %% [code]\ndata.groupby('Place').last()\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [raw]\n# # data['log_days_since_first_case'] =  np.log(data.days_since_first_case + 1)\n# # data['log_days_since_first_fatality'] = np.log(data.days_since_first_fatality + 1)\n# \n# data['sqrt_days_since_first_case'] = np.sqrt(data.days_since_first_case)\n# data['sqrt_days_since_first_fatality'] = np.sqrt(data.days_since_first_fatality)\n# \n# \n# \n\n \n# %% [code]\ndef logHist(x, b = 150):\n    return\n\n# %% [raw]\n# np.std(x.log_cases)\n\n# %% [raw]\n# np.std(x.log_fatalities)\n\n# %% [code]\n\n\n# %% [code]\ndata['log_fatalities'] = np.log(data.Fatalities + 1) #  + 0.4 * np.random.normal(0, 1, len(data))\ndata['log_cases'] = np.log(data.ConfirmedCases + 1) # + 0.2 *np.random.normal(0, 1, len(data))\n\n\n\n# %% [raw]\n# data.log_cases.plot(kind='hist', bins = 250)\n\n# %% [code]\ndata['is_China'] = (data.Country=='China') & (~data.Place.isin(['Hong Kong', 'Macau']))\n\n# %% [code]\nfor col in [c for c in data.columns if 'd_ewm' in c]:\n    data[col] += np.random.normal(0, 1, len(data)) * np.std(data[col]) * 0.2\n    \n\n# %% [raw]\n# data[data.log_cfr>-11].log_fatalities.plot(kind='hist', bins = 150)\n\n# %% [code]\ndata['is_province'] = 1.0* (~data.Province_State.isnull() )\n\n# %% [code]\ndata['log_elapsed'] = np.log(data.elapsed + 1)\n\n# %% [code]\ndata.columns\n\n# %% [code]\ndata.columns[::19]\n\n# %% [code]\ndata.shape\n\n# %% [code]\nlogHist(data.ConfirmedCases)\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Data Cleanup\n\n# %% [code]\ndata.drop(columns = ['TRUE_POPULATION'], inplace=True)\n\n# %% [code]\ndata['final_day_of_week'] = data.Date_f.apply(datetime.datetime.weekday)\n\n# %% [code]\ndata['base_date_day_of_week'] = data.Date.apply(datetime.datetime.weekday)\n\n# %% [code]\ndata['date_difference_modulo_7_days'] = (data.Date_f - data.Date).dt.days % 7\n\n# %% [raw]\n# for c in data.columns.to_list():\n#     if 'days_since' in c:\n#         data[c] = np.log(data[c]+1)\n\n# %% [code]\n\n\n# %% [code]\nfor c in data.columns.to_list():\n    if 'days_to' in c:\n#         print(c)\n        data[c] = data[c].where(~np.isinf(data[c]), 1e3)\n        data[c] = np.clip(data[c], 0, 365)\n        data[c] = np.sqrt(data[c])\n\n\n        \n        \nnew_places = train[(train.Date == test.Date.min() - datetime.timedelta(1)) &\n      (train.ConfirmedCases == 0)\n     ].Place\n\n        \n        \n        # %% [code]\n\n\n# %% [markdown]\n# ## II. Modeling\n\n# %% [markdown]\n# ### Data Prep\n\n# %% [code]\nmodel_data = data[ (( len(test) ==0 ) | (data.Date_f < test.Date.min()))\n                  & \n                  (data.ConfirmedCases > 0) &\n                 (~data.ConfirmedCases_f.isnull())].copy()\n\n# %% [raw]\n# data.Date_f\n\n# %% [code]\ntest.Date.min()\n\n# %% [code]\nmodel_data.Date_f.max()\n\n# %% [code]\nmodel_data.Date_f.max()\n\n# %% [code]\nmodel_data.Date.max()\n\n# %% [code]\nmodel_data.Date_f.min()\n\n# %% [code]\n\n\n# %% [code]\nmodel_data = model_data[~( \n                            ( np.random.rand(len(model_data)) < 0.8 )  &\n                          ( model_data.Country == 'China') &\n                              (model_data.Date < datetime.datetime(2020, 2, 15)) )]\n\n# %% [code]\nx_dates = model_data[['Date_i', 'Date_f', 'Place']]\n\n# %% [code]\nx = model_data[    \n    model_data.columns.to_list()[\n            model_data.columns.to_list().index('ConfirmedCases_1d_prior_value'):]]\\\n            .drop(columns = ['Date_i', 'Date_f', 'CaseChgRate', 'FatalityChgRate'])\n\n# %% [raw]\n# x.columns\n\n# %% [raw]\n# x\n\n\n\n\ntest.Date\n\n# %% [code]\nif PRIVATE:\n    data_test = data[ (data.Date_i == train.Date.max() ) & \n                     (data.Date_f.isin(test.Date.unique() ) ) ].copy()\nelse:\n    data_test = data[ (data.Date_i == test.Date.min() - datetime.timedelta(1) ) & \n                     (data.Date_f.isin(test.Date.unique() ) ) ].copy()\n\n# %% [code]\ndata_test.Date.unique()\n\n# %% [code]\ntest.Date.unique()\n\n# %% [raw]\n# data_test.Date_f\n\n# %% [code]\nx_test =  data_test[x.columns].copy()\n\n# %% [code]\ntrain.Date.max()\n\n# %% [code]\ntest.Date.max()\n\n# %% [raw]\n# data_test[data_test.Place=='San Marino'].Date_f\n\n# %% [raw]\n# data_test.groupby('Place').Date_f.count().sort_values()\n\n# %% [raw]\n# x_test\n\n# %% [code]\n\n\n# %% [raw]\n# x.columns\n\n# %% [code]\n\n\n# %% [code]\nif MODEL_Y is 'slope':\n    y_cases = model_data.CaseChgRate \n    y_fatalities = model_data.FatalityChgRate \nelse:\n    y_cases = model_data.CaseChgRate * model_data.elapsed\n    y_fatalities = model_data.FatalityChgRate * model_data.elapsed\n    \ny_cfr = np.log(    (model_data.Fatalities_f                                          + np.clip(0.015 * model_data.ConfirmedCases_f, 0, 0.3))                             / ( model_data.ConfirmedCases_f + 0.1) )\n\n# %% [code]\ngroups = model_data.Country\nplaces = model_data.Place\n\n# %% [raw]\n# y_cfr\n\n# %% [code]\n\n\n# %% [markdown]\n# #### Model Setup\n\n# %% [code]\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GroupKFold, GroupShuffleSplit, PredefinedSplit\nfrom sklearn.model_selection import ParameterSampler\nfrom sklearn.metrics import make_scorer\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import HuberRegressor, ElasticNet\nimport lightgbm as lgb\n\n\n# %% [code]\nnp.random.seed(SEED)\n\n# %% [code]\nenet_params = { 'alpha': [   3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 1e-3,  ],\n                'l1_ratio': [  0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 0.8, 0.9, 0.97, 0.99 ]}\n\n# %% [code]\net_params = {        'n_estimators': [50, 70, 100, 140],\n                    'max_depth': [3, 5, 7, 8, 9, 10],\n                      'min_samples_leaf': [30, 50, 70, 100, 130, 165, 200, 300, 600],\n                     'max_features': [0.4, 0.5, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85],\n                    'min_impurity_decrease': [0, 1e-5 ], #1e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2],\n                    'bootstrap': [ True, False], # False is clearly worse          \n                 #   'criterion': ['mae'],\n                   }\n\n# %% [code]\nlgb_params = {\n                'max_depth': [5, 12],\n                'n_estimators': [ 100, 200, 300, 500],   # continuous\n                'min_split_gain': [0, 0, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2],\n                'min_child_samples': [ 7, 10, 14, 20, 30, 40, 70, 100, 200, 400, 700, 1000, 2000],\n                'min_child_weight': [0], #, 1e-3],\n                'num_leaves': [5, 10, 20, 30],\n                'learning_rate': [0.05, 0.07, 0.1],   #, 0.1],       \n                'colsample_bytree': [0.1, 0.2, 0.33, 0.5, 0.65, 0.8, 0.9], \n                'colsample_bynode':[0.1, 0.2, 0.33, 0.5, 0.65, 0.81],\n                'reg_lambda': [1e-5, 3e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100, 1000,   ],\n                'reg_alpha': [1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 30, 1000,], # 1, 10, 100, 1000, 10000],\n                'subsample': [  0.8, 0.9, 1],\n                'subsample_freq': [1],\n                'max_bin': [ 7, 15, 31, 63, 127, 255],\n  #               'extra_trees': [True, False],\n#                 'boosting': ['gbdt', 'dart'],\n    #     'subsample_for_bin': [200000, 500000],\n               }    \n\n# %% [code]\nMSE = 'neg_mean_squared_error'\nMAE = 'neg_mean_absolute_error'\n\n# %% [code]\n\n\n# %% [code]\ndef trainENet(x, y, groups, cv = 0, **kwargs):\n    return trainModel(x, y, groups, \n                      clf = ElasticNet(normalize = True, selection = 'random', \n                                       max_iter = 3000),\n                      params = enet_params, \n                      cv = cv, **kwargs)\n\n# %% [code]\ndef trainETR(x, y, groups, cv = 0, n_jobs = 5,  **kwargs):\n    clf = ExtraTreesRegressor(n_jobs = 1)\n    params = et_params\n    return trainModel(x, y, groups, clf, params, cv, n_jobs, **kwargs)\n\n# %% [code]\ndef trainLGB(x, y, groups, cv = 0, n_jobs = 4, **kwargs):\n    clf = lgb.LGBMRegressor(verbosity=-1, hist_pool_size = 1000,  \n                      )\n    params = lgb_params\n    \n    return trainModel(x, y, groups, clf, params, cv, n_jobs,  **kwargs)\n\n# %% [code]\ndef trainModel(x, y, groups, clf, params, cv = 0, n_jobs = None, \n                   verbose=0, splits=None, **kwargs):\n#     if cv is 0:\n#         param_sets = list(ParameterSampler(params, n_iter=1))\n#         clf = clf.set_params(**param_sets[0] )\n#         if n_jobs is not None:\n#             clf = clf.set_params(** {'n_jobs': n_jobs } )\n#         f = clf.fit(x, y)\n#         return clf \n#     else:\n        if n_jobs is None:\n            n_jobs = 4\n        if np.random.rand() < 0.8: # all shuffle, don't want overfit models, just reasonable\n            folds = GroupShuffleSplit(n_splits=4, \n                                                   test_size= 0.2 + 0.10 * np.random.rand())\n        else:\n            folds = GroupKFold(4)\n        clf = RandomizedSearchCV(clf, params, \n                            cv=  folds, \n#                                  cv = GroupKFold(4),\n                                 n_iter=12, \n                                verbose = 0, n_jobs = n_jobs, scoring = MSE)\n        f = clf.fit(x, y, groups)\n        #if verbose > 0:\n        print(pd.DataFrame(clf.cv_results_['mean_test_score'])); print();  \n     #   print(pd.DataFrame(clf.cv_results_).to_string()); print();  \n        \n        \n        best = clf.best_estimator_;  print(best)\n        print(\"Best Score: {}\".format(np.round(clf.best_score_,4)))\n        \n        return best\n\n# %% [code] {\"scrolled\":true}\nnp.mean(y_cases)\n\n# %% [code]\n\n\n# %% [code]\ndef getSparseColumns(x, verbose = 0):\n    sc = []\n    for c in x.columns.to_list():\n        u = len(x[c].unique())\n        if u > 10 and u < 0.01*len(x) :\n            sc.append(c)\n            if verbose > 0:\n                print(\"{}: {}\".format(c, u))\n\n    return sc\n\n# %% [code]\ndef noisify(x, noise = 0.1):\n    x = x.copy()\n   # cols = x.columns.to_list()\n    cols = getSparseColumns(x)\n    for c in cols:\n        u = len(x[c].unique())\n        if u > 50:\n            x[c].values[:] = x[c].values + np.random.normal(0, noise, len(x)) * np.std(x[c])\n    return x;\n\n# %% [raw]\n# cols = getSparseColumns(x)\n# for c in cols:\n#     u = len(x[c].unique())\n#     if u > 50:\n#         print(\"{}: {}\".format(c, u)) #x[c].values[:] = x[c].values + np.random.normal(0, noise, len(x)) * np.std(x[c])\n# # return x;\n\n# %% [raw]\n# [c for c in x.columns if any(z in c for z in \n#                                  ['prior_slope', 'prior_value'])]\n\n# %% [raw]\n# getSparseColumns(x, verbose = 0)\n\n# %% [raw]\n# x.columns[::19]\n\n# %% [code]\ndef getMaxOverlap(row, df):\n#     max_overlap_frac = 0\n\n    df_place = df[df.Place == row.Place]\n    if len(df_place)==0:\n        return 0\n#     print(df_place)\n    overlap =         (np.clip( df_place.Date_f, None, row.Date_f)                             - np.clip( df_place.Date_i, row.Date_i, None) ).dt.days\n    overlap = np.clip(overlap, 0, None)\n    length = np.clip(  (df_place.Date_f - df_place.Date_i).dt.days, \n                        (row.Date_f - row.Date_i).days,  None)\n#     print(overlap)\n#     print(length)\n#     print(overlap)\n#     print(length)\n    return np.amax(overlap / length) \n#     print(row)\n#     print(df_place)\n#     return\n    \n#     for i in range(0, len(df_place)):\n#         selected = df_place.iloc[i]\n#        # if row.Place == selected.Place:\n#         overlap = (np.min((row.Date_f, selected.Date_f))\\\n#                      - np.max((row.Date_i, selected.Date_i )) ).days\n#         overlap_frac = overlap / (selected.Date_f - selected.Date_i).days \n#         if overlap_frac > max_overlap_frac:\n#             max_overlap_frac = overlap_frac\n#     return max_overlap_frac\n     \n\n# %% [raw]\n# \n\n# %% [code]\ndef getSampleWeight(x, groups):\n \n    \n    counter = Counter(groups)\n    median_count = np.median( [counter[group] for group in groups.unique()])\n#     print(median_count)\n    c_count = [counter[group] for group in groups]\n    \n    e_decay = np.round(LT_DECAY_MIN + np.random.rand() * ( LT_DECAY_MAX - LT_DECAY_MIN), 1) \n    print(\"LT weight decay: {:.2f}\".format(e_decay));\n    ssr =  np.power(  1 / np.clip( c_count / median_count , 0.1,  30) , \n                        0.1 + np.random.rand() * 0.6) \\\n                /   np.power(x.elapsed / 3, e_decay) \\\n                    *  SET_FRAC * np.exp(  -    np.random.rand()  )\n    \n#     print(np.power(  1 / np.clip( c_count / median_count , 1,  10) , \n#                         0.1 + np.random.rand() * 0.3))\n#     print(np.power(x.elapsed / 3, e_decay))\n#     print(np.exp(  1.5 * (np.random.rand() - 0.5) ))\n        \n    # drop % of groups at random\n    group_drop = dict([(group, np.random.rand() < 0.15) for group in groups.unique()])\n    ssr = ssr * (  [ 1 -group_drop[group] for group in groups])\n#     print(ssr[::171])\n#     print(np.array([ 1 -group_drop[group] for group in groups]).sum() / len(groups))\n\n#     pd.Series(ssr).plot(kind='hist', bins = 100)\n    return ssr;\n\n# %% [raw]\n# group_drop = dict([(group, np.random.rand() < 0.20) for group in groups.unique()])\n#      \n# np.array([ 1 -group_drop[group] for group in groups]).sum() / len(groups)\n# \n\n# %% [raw]\n# [c for c in x.columns if 'continent' in c]\n\n# %% [raw]\n# x.columns[::10]\n\n# %% [raw]\n# x.shape\n\n# %% [raw]\n# contain_data.columns\n\n# %% [code]\ndef runBags(x, y, groups, cv, bags = 3, model_type = trainLGB, \n            noise = 0.1, splits = None, weights = None, **kwargs):\n    models = []\n    for bag in range(bags):\n        print(\"\\nBAG {}\".format(bag+1))\n        \n        x = x.copy()  # copy X to modify it with noise\n        \n        if DROPS:\n            # drop 0-70% of the bend/slope/prior features, just for speed and model diversity\n            for col in [c for c in x.columns if any(z in c for z in ['bend', 'slope', 'prior'])]:\n                if np.random.rand() < np.sqrt(np.random.rand()) * 0.7:\n                    x[col].values[:] = 0\n            \n        # 00% of the time drop all 'rate_since' features \n#         if np.random.rand() < 0.00:\n#             print('dropping rate_since features')\n#             for col in [c for c in x.columns if 'rate_since' in c]:    \n#                 x[col].values[:] = 0\n        \n        # 20% of the time drop all 'world' features \n#         if np.random.rand() < 0.00:\n#             print('dropping world features')\n#             for col in [c for c in x.columns if 'world' in c]:    \n#                 x[col].values[:] = 0\n        \n        # % of the time drop all 'nearest' features \n        if DROPS and (np.random.rand() < 0.30):\n            print('dropping nearest features')\n            for col in [c for c in x.columns if 'nearest' in c]:    \n                x[col].values[:] = 0\n        \n        #  % of the time drop all 'surrounding_places' features \n        if DROPS and (np.random.rand() < 0.25):\n            print('dropping \\'surrounding places\\' features')\n            for col in [c for c in x.columns if 'surrounding_places' in c]:    \n                x[col].values[:] = 0\n        \n        \n        # 20% of the time drop all 'continent' features \n#         if np.random.rand() < 0.20:\n#             print('dropping continent features')\n#             for col in [c for c in x.columns if 'continent' in c]:    \n#                 x[col].values[:] = 0\n        \n        # drop 0-50% of all features\n#         if DROPS:\n        col_drop_frac = np.sqrt(np.random.rand()) * 0.5\n        for col in [c for c in x.columns if 'elapsed' not in c ]:\n            if np.random.rand() < col_drop_frac:\n                x[col].values[:] = 0\n\n        \n        x = noisify(x, noise)\n        \n        \n        if DROPS and (np.random.rand() < SUP_DROP):\n            print(\"Dropping supplemental country data\")\n            for col in x[[c for c in x.columns if c in sup_data.columns]]:  \n                x[col].values[:] = 0\n                \n        if DROPS and (np.random.rand() < ACTIONS_DROP): \n            for col in x[[c for c in x.columns if c in contain_data.columns]]:  \n                x[col].values[:] = 0\n#             print(x.StringencyIndex_20d_ewm[::157])\n        else:\n            print(\"*using containment data\")\n            \n        if np.random.rand() < 0.6: \n            x.S_data_days = 0\n            \n        ssr = getSampleWeight(x, groups)\n        \n        date_falloff = 0 + (1/30) * np.random.rand()\n        if weights is not None:\n            ssr = ssr * np.exp(-weights * date_falloff)\n        \n        ss = ( np.random.rand(len(y)) < ssr  )\n        print(\"n={}\".format(len(x[ss])))\n        \n        p1 =x.elapsed[ss].plot(kind='hist', bins = int(x.elapsed.max() - x.elapsed.min() + 1))\n        p1 = plt.figure();\n#         break\n#        print(Counter(groups[ss]))\n        print((ss).sum())\n        models.append(model_type(x[ss], y[ss], groups[ss], cv,   **kwargs))\n    return models\n\n# %% [code]\nx = x.astype(np.float32)\n\n# %% [raw]\n# x.elapsed\n\n# %% [code]\n\n\n# %% [code]\nBAG_MULT = 1\n\n# %% [code]\n\n\n# %% [code]\nx.shape\n\n# %% [code]\n\n\n# %% [code]\nlgb_c_clfs = []; lgb_c_noise = []\n\n# %% [code] {\"scrolled\":true}\ndate_weights =  np.abs((model_data.Date_f - test.Date.min()).dt.days) \n\n# %% [code]\nfor iteration in range(0, int(math.ceil(1.1 * BAGS))):\n    for noise in [ 0.05, 0.1, 0.2, 0.3, 0.4  ]:\n        print(\"\\n---\\n\\nNoise of {}\".format(noise));\n        num_bags = 1 * BAG_MULT;\n        if np.random.rand() < PLACE_FRACTION:\n            cv_group = places\n            print(\"CV by Place\")\n        else:\n            cv_group = groups\n            print(\"CV by Country\")\n             \n        \n        lgb_c_clfs.extend(runBags(x, y_cases, \n                          cv_group, #groups\n                          MSE, num_bags, trainLGB, verbose = 0, \n                                          noise = noise, weights = date_weights\n\n                                 ))\n        lgb_c_noise.extend([noise] * num_bags)\n        if SINGLE_MODEL:\n            break;\n\n# %% [code]\n\n\n# %% [raw]\n# np.isinf(x).sum().sort_values()\n\n# %% [code]\n\n\n# %% [raw]\n# enet_c_clfs = runBags(x, y_cases, groups, MSE, 1, trainENet, verbose = 1)\n\n# %% [code]\n\n\n# %% [code]\nlgb_f_clfs = []; lgb_f_noise = []\n\n# %% [code]\nfor iteration in range(0, int(np.ceil(np.sqrt(BAGS)))):\n    for noise in [  0.5,  1, 2, 3,  ]:\n        print(\"\\n---\\n\\nNoise of {}\".format(noise));\n        num_bags = 1 * int(np.ceil(np.sqrt(BAG_MULT)))\n        if np.random.rand() < PLACE_FRACTION  :\n            cv_group = places\n            print(\"CV by Place\")\n        else:\n            cv_group = groups\n            print(\"CV by Country\")\n            \n   \n        lgb_f_clfs.extend(runBags(x, y_fatalities, \n                                  cv_group, #places, # groups, \n                                  MSE, num_bags, trainLGB, \n                                  verbose = 0, noise = noise,\n                                  weights = date_weights\n                                 ))\n        lgb_f_noise.extend([noise] * num_bags)\n        if SINGLE_MODEL:\n            break;\n\n# %% [raw]\n# lgb_f_noise = lgb_f_noise[0:3]\n# lgb_f_clfs = lgb_f_clfs[0:3]\n\n# %% [raw]\n# lgb_f_noise = lgb_f_noise[2:]\n# lgb_f_clfs = lgb_f_clfs[2:]\n\n# %% [raw]\n# et_f_clfs = runBags(x, y_fatalities, groups, MSE, 1, trainETR, verbose = 1)\n# \n# \n\n# %% [raw]\n# enet_f_clfs = runBags(x, y_fatalities, groups, MSE, 1, trainENet, verbose = 1)\n# \n# \n\n# %% [raw]\n# y_cfr.plot(kind='hist', bins = 250)\n\n# %% [code]\nlgb_cfr_clfs = []; lgb_cfr_noise = [];\n\n# %% [code]\nfor iteration in range(0, int(np.ceil(np.sqrt(BAGS)))):\n    for noise in [    0.4, 1, 2, 3]:\n        print(\"\\n---\\n\\nNoise of {}\".format(noise));\n        num_bags = 1 * BAG_MULT;\n        if np.random.rand() < 0.5 * PLACE_FRACTION :\n            cv_group = places\n            print(\"CV by Place\")\n        else:\n            cv_group = groups\n            print(\"CV by Country\")\n \n        lgb_cfr_clfs.extend(runBags(x, y_cfr, \n                          cv_group, #groups\n                          MSE, num_bags, trainLGB, verbose = 0, \n                                          noise = noise, \n                                          weights = date_weights\n\n                                 ))\n        lgb_cfr_noise.extend([noise] * num_bags)\n        if SINGLE_MODEL:\n            break;\n\n# %% [raw]\n# x_test\n\n# %% [code]\nlgb_cfr_clfs[0].predict(x_test)\n\n# %% [raw]\n# \n\n# %% [code]\n# full sample, through 03/28 (avail on 3/30), lgb only: 0.0097 / 0.0036;   0.0092 / 0.0042\n#                                                       \n\n# %% [markdown]\n# ##### Feature Importance\n\n# %% [code]\ndef show_FI(model, featNames, featCount):\n   # show_FI_plot(model.feature_importances_, featNames, featCount)\n    fis = model.feature_importances_\n    fig, ax = plt.subplots(figsize=(6, 5))\n    indices = np.argsort(fis)[::-1][:featCount]\n    g = sns.barplot(y=featNames[indices][:featCount],\n                    x = fis[indices][:featCount] , orient='h' )\n    g.set_xlabel(\"Relative importance\")\n    g.set_ylabel(\"Features\")\n    g.tick_params(labelsize=12)\n    g.set_title( \" feature importance\")\n    \n\n# %% [code]\ndef avg_FI(all_clfs, featNames, featCount):\n    # 1. Sum\n    clfs = []\n    for clf_set in all_clfs:\n        for clf in clf_set:\n            clfs.append(clf);\n    print(\"{} classifiers\".format(len(clfs)))\n    fi = np.zeros( (len(clfs), len(clfs[0].feature_importances_)) )\n    for idx, clf in enumerate(clfs):\n        fi[idx, :] = clf.feature_importances_\n    avg_fi = np.mean(fi, axis = 0)\n\n    # 2. Plot\n    fis = avg_fi\n    fig, ax = plt.subplots(figsize=(6, 5))\n    indices = np.argsort(fis)[::-1]#[:featCount]\n    #print(indices)\n    g = sns.barplot(y=featNames[indices][:featCount],\n                    x = fis[indices][:featCount] , orient='h' )\n    g.set_xlabel(\"Relative importance\")\n    g.set_ylabel(\"Features\")\n    g.tick_params(labelsize=12)\n    g.set_title( \" feature importance\")\n    \n    return pd.Series(fis[indices], featNames[indices])\n\n# %% [code]\n\ndef linear_FI_plot(fi, featNames, featCount):\n   # show_FI_plot(model.feature_importances_, featNames, featCount)\n    fig, ax = plt.subplots(figsize=(6, 5))\n    indices = np.argsort(np.absolute(fi))[::-1]#[:featCount]\n    g = sns.barplot(y=featNames[indices][:featCount],\n                    x = fi[indices][:featCount] , orient='h' )\n    g.set_xlabel(\"Relative importance\")\n    g.set_ylabel(\"Features\")\n    g.tick_params(labelsize=12)\n    g.set_title( \" feature importance\")\n    return pd.Series(fi[indices], featNames[indices])\n\n# %% [code]\n\n\n# %% [raw]\n# fi_list = []\n# for clf in enet_c_clfs:\n#     fi = clf.coef_ * np.std(x, axis=0).values \n#     fi_list.append(fi)\n# fis = np.mean(np.array(fi_list), axis = 0)\n# fis = linear_FI_plot(fis, x.columns.values,25)\n\n# %% [raw]\n# lgb_c_clfs\n\n# %% [code]\nf = avg_FI([lgb_c_clfs], x.columns, 25)\n\n# %% [code]\nfor feat in ['bend', 'capita', 'cfr', 'slope', 'since', 'chg', 'ersonal', \n             'world', 'continent', 'nearest', 'surrounding']:\n    print(\"{}: {:.2f}\".format(feat, f.filter(like=feat).sum() / f.sum()))\n\n# %% [code]\nf[:100:3]\n\n# %% [code]\nprint(\"{}: {:.2f}\".format('sup_data', \n                       f[[c for c in f.index if c in sup_data.columns]].sum() / f.sum()))\nprint(\"{}: {:.2f}\".format('contain_data', \n                   f[[c for c in f.index if c in contain_data.columns]].sum() / f.sum()))\n\n# %% [raw]\n# I used a very simple Week 2 model like many. For Week 3:\n# \n# The right target is total change in the logged counts. This exactly mirrors the final evaluation metric, and using change rather than raw logged countes keeps it stationary.\n# \n# These can be put into a regressor for all windows from 1-30 days; ideally lightgbm or xgboost.  \n# \n# Cross-validation works well by place (country-level struggles to understand China's magic numbers; time-series would be too 1-2 week centric and couldn't be done for a full month). Each place is it's own outbreak so this works reasonably well.\n# \n# Feature Importance:\n# ~30-40%: current and past outbreak information (slopes and rates calculated *many* ways)\n# ~20-30%: nearby outbreak information, e.g. per capita rates vs. nearest 5, 10, 20 regions or within a specified latitude and longitude range--indicates not just spread but propensity to be tracking and reporting, severity, gov't management, likelihood of flattening, etc.\n# ~10-20%: place attributes (average age, personality, tfr, percent in largest city, etc)\n# ~10%: comparisons with world or continent, typically per capita prevalance compared with world or continent figures\n# ~5%: containment actions taken \n# ~5%: other \n# \n# The models started to get good once I put in world and continent and then proximity information--this 'state of the world' information gives a clue to where the country is compared to others and its likely pace that may mirror recent trends for similar countries. \n# \n# It might be possible to get better 1-10 day figures with time series models, but a lot of the error is in long-term drift, so these 1-30 day interval total aggregate change models are best suited to the competition overall.\n# \n# \n\n# %% [code]\nf = avg_FI([lgb_f_clfs], x.columns, 25)\n\n# %% [code]\nfor feat in ['bend', 'capita', 'cfr', 'slope', 'since', 'chg', 'ersonal', \n            'world', 'continent', 'nearest', 'surrounding']:\n    print(\"{}: {:.2f}\".format(feat, f.filter(like=feat).sum() / f.sum()))\n\n# %% [code]\nprint(\"{}: {:.2f}\".format('sup_data', \n                       f[[c for c in f.index if c in sup_data.columns]].sum() / f.sum()))\nprint(\"{}: {:.2f}\".format('contain_data', \n                   f[[c for c in f.index if c in contain_data.columns]].sum() / f.sum()))\n\n\n# %% [raw]\n# x.days_since_Stringency_1.plot(kind='hist', bins = 100)\n\n# %% [raw]\n# len(x.log_fatalities.unique())\n\n# %% [code]\nf = avg_FI([lgb_cfr_clfs], x.columns, 25)\n\n# %% [code]\nfor feat in ['bend', 'capita', 'cfr', 'slope', 'since', 'chg', 'ersonal', \n            'world', 'continent', 'nearest', 'surrounding']:\n    print(\"{}: {:.2f}\".format(feat, f.filter(like=feat).sum() / f.sum()))\n\n# %% [code]\nprint(\"{}: {:.2f}\".format('sup_data', \n                       f[[c for c in f.index if c in sup_data.columns]].sum() / f.sum()))\nprint(\"{}: {:.2f}\".format('contain_data', \n                   f[[c for c in f.index if c in contain_data.columns]].sum() / f.sum()))\n\n\n\n\n# %% [code]\nall_c_clfs = [lgb_c_clfs, ]#  enet_c_clfs]\nall_f_clfs = [lgb_f_clfs] #, enet_f_clfs]\nall_cfr_clfs = [lgb_cfr_clfs]\n\n\n# %% [code]\nall_c_noise = [lgb_c_noise]\nall_f_noise = [lgb_f_noise]\nall_cfr_noise = [lgb_cfr_noise]\n\n# %% [code]\n\n\n# %% [code]\nNUM_TEST_RUNS = 1\n\n# %% [code]\nc_preds = np.zeros((NUM_TEST_RUNS * sum([len(x) for x in all_c_clfs]), len(x_test)))\nf_preds = np.zeros((NUM_TEST_RUNS * sum([len(x) for x in all_f_clfs]), len(x_test)))\ncfr_preds = np.zeros((NUM_TEST_RUNS * sum([len(x) for x in all_cfr_clfs]), len(x_test)))\n\n\n# %% [code]\ndef avg(x):\n    return (np.mean(x, axis=0) + np.median(x, axis=0))/2\n\n# %% [code]\ncount = 0\n\nfor idx, clf in enumerate(lgb_c_clfs):\n    for i in range(0, NUM_TEST_RUNS):\n        noise = lgb_c_noise[idx]\n        c_preds[count,:] = np.clip(clf.predict(noisify(x_test, noise)), -1 , 10)\n        count += 1\n#y_cases_pred_blended_full = avg(c_preds)\n\n# %% [code]\ncount = 0\n\nfor idx, clf in enumerate(lgb_f_clfs):\n    for i in range(0, NUM_TEST_RUNS):\n        noise = lgb_f_noise[idx]\n        f_preds[count,:] = np.clip(clf.predict(noisify(x_test, noise)), -1 , 10)\n        count += 1\n#y_fatalities_pred_blended_full = avg(f_preds)\n\n# %% [code]\ncount = 0\n\nfor idx, clf in enumerate(lgb_cfr_clfs):\n    for i in range(0, NUM_TEST_RUNS):\n        noise = lgb_cfr_noise[idx]\n        cfr_preds[count,:] = np.clip(clf.predict(noisify(x_test, noise)), -10 , 10)\n        count += 1\n#y_cfr_pred_blended_full = avg(cfr_preds)\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\ndef qPred(preds, pctile, simple=False):\n    q = np.percentile(preds, pctile, axis = 0)\n    if simple:\n        return q;\n    resid = preds - q\n    resid_wtg = 2/100/len(preds)* ( np.clip(resid, 0, None) * (pctile)                         + np.clip(resid, None, 0) * (100- pctile) )\n    adj = np.sum(resid_wtg, axis = 0)\n#     print(q)\n#     print(adj)\n#     print(q+adj)\n    return q + adj\n\n# %% [code]\n\n\n# %% [code]\nq = 50\n\n# %% [code]\ny_cases_pred_blended_full = qPred(c_preds, q) #avg(c_preds)\ny_fatalities_pred_blended_full = qPred(f_preds, q) # avg(f_preds)\ny_cfr_pred_blended_full = qPred(cfr_preds, q) #avg(cfr_preds)\n\n# %% [code]\n\n\n# %% [raw]\n# cfr_preds\n\n# %% [raw]\n# lgb_cfr_noise\n\n# %% [raw]\n# lgb_cfr_clfs[0].predict(noisify(x_test, 0.4))\n\n# %% [raw]\n# cfr_preds[0][0:500]\n\n# %% [raw]\n# x.log_cfr.plot(kind='hist', bins = 250)\n\n# %% [code]\n\n\n# %% [code]\nprint(np.mean(np.corrcoef(c_preds[::NUM_TEST_RUNS]),axis=0))\n\n# %% [code]\nprint(np.mean(np.corrcoef(f_preds[::NUM_TEST_RUNS]), axis=0))\n\n# %% [code]\nprint(np.mean(np.corrcoef(cfr_preds[::NUM_TEST_RUNS]), axis = 0))\n\n# %% [raw]\n# cfr_preds\n\n# %% [code]\npd.Series(np.std(c_preds, axis = 0)).plot(kind='hist', bins = 50)\n\n# %% [code]\npd.Series(np.std(f_preds, axis = 0)).plot(kind='hist', bins = 50)\n\n# %% [code]\npd.Series(np.std(cfr_preds, axis = 0)).plot(kind='hist', bins = 50)\n\n# %% [code]\ny_cfr\n\n# %% [code]\n(groups == 'Sierra Leone').sum()\n\n# %% [code]\npred = pd.DataFrame(np.hstack((np.transpose(c_preds),\n                              np.transpose(f_preds))), index=x_test.index)\npred['Place'] = data_test.Place\n\n\npred['Date'] = data_test.Date\npred['Date_f'] = data_test.Date_f\n\n# %% [code]\npred[(pred.Date == pred.Date.max()) & (pred.Date_f == pred.Date_f.max())][30: 60]\n\n# %% [code]\n(pred.Place=='Sierra Leone').sum()\n\n# %% [code]\nnp.round(pred[(pred.Date == pred.Date.max()) & (pred.Date_f == pred.Date_f.max())], 2)[190:220:]\n\n# %% [code] {\"scrolled\":false}\nnp.round(pred[(pred.Date == pred.Date.max()) & (pred.Date_f == pred.Date_f.max())][220:-20],2)\n\n# %% [code]\nc_preds.shape\nx_test.shape\n\n# %% [raw]\n# \n# data_test.shape\n\n# %% [raw]\n# pd.DataFrame({'c_mean': np.mean(c_preds, axis =0 ),\n#                   'c_median': np.median(c_preds, axis =0 ),\n#              }, index=data_test.Place)[::7]\n\n# %% [raw]\n# np.median(c_preds, axis =0 )[::71]\n\n# %% [code]\n\n\n# %% [markdown]\n# ### III. Other\n\n# %% [code]\n\n\n# %% [raw]\n# MAX_DATE = np.max(train.Date)\n\n# %% [raw]\n# final = train[train.Date == MAX_DATE]\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [raw]\n# train.groupby('Place')[['ConfirmedCases','Fatalities']].apply(lambda x: np.sum(x >0))\n\n# %% [raw]\n# num_changes = train.groupby('Place')[['ConfirmedCases','Fatalities']].apply(lambda x: np.sum(x - x.shift(1) >0))\n\n# %% [raw]\n# num_changes.Fatalities.plot(kind='hist', bins = 50);\n\n# %% [raw]\n# num_changes.ConfirmedCases.plot(kind='hist', bins = 50);\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Rate Calculation\n\n# %% [raw]\n# def getRate(train, window = 5):\n#     joined = pd.merge(train[train.Date == \n#                                     np.max(train.Date) - datetime.timedelta(window)], \n#                       final,  on=['Place'])\n#     joined['FatalityRate'] = (np.log(joined.Fatalities_y + 1)\\\n#                                   - np.log(joined.Fatalities_x + 1)) / window\n#     joined['CasesRate'] = (np.log(joined.ConfirmedCases_y + 1)\\\n#                                    - np.log(joined.ConfirmedCases_x + 1)) / window\n#     joined.set_index('Place', inplace=True)\n# \n#     rates = joined[[c for c in joined.columns.to_list() if 'Rate' in c]] \n#     return rates\n\n# %% [raw]\n# ltr = getRate(train, 14)\n\n# %% [raw]\n# lm = pd.merge(ltr, num_changes, on='Place')\n\n# %% [raw]\n# lm.filter(like='China', axis='rows')\n\n# %% [raw]\n# \n\n# %% [raw]\n# flat = lm[\n#     (lm.CasesRate < 0.01) & (lm.ConfirmedCases > 5)]\n\n# %% [raw]\n# flat\n\n# %% [raw]\n# \n\n# %% [raw]\n# c_rate = pd.Series(\n#     np.where(num_changes.ConfirmedCases >= 0, \n#          getRate(train, 7).CasesRate, \n#          getRate(train, 5).CasesRate),\n#     index = num_changes.index, name = 'CasesRate')\n# \n# f_rate = pd.Series(\n#     np.where(num_changes.Fatalities >= 0, \n#          getRate(train, 7).FatalityRate, \n#          getRate(train, 4).CasesRate),\n#     index = num_changes.index, name = 'FatalityRate')\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Plot of Changes\n\n# %% [raw]\n# def rollDates(df, i):\n#     df = df.copy()\n#     df.Date = df.Date + datetime.timedelta(i)\n#     return df\n\n# %% [raw]\n# m = pd.merge(rollDates(train, 7), train, on=['Place', 'Date'])\n# m['CaseChange'] = (np.log(m.ConfirmedCases_y + 1) - np.log(m.ConfirmedCases_x + 1))/7\n\n# %% [raw]\n# m[m.Place=='USMaine']\n\n# %% [code]\n\n\n# %% [markdown]\n# #### Histograms of Case Counts\n\n# %% [code]\n\n\n# %% [raw]\n# m = pd.merge(rollDates(full_train, 1), full_train, on=['Place', 'Date'])\n# \n\n# %% [code]\n\n\n# %% [markdown]\n# ##### CFR Charts\n\n# %% [raw]\n# joined.Fatalities_y\n\n# %% [raw]\n# withcases = joined[joined.ConfirmedCases_y > 300]\n\n# %% [raw]\n# withcases.sort_values(by = ['Fatalities_y'])\n\n# %% [raw]\n# (withcases.Fatalities_y / withcases.ConfirmedCases_x).plot(kind='hist', bins = 150);\n\n# %% [raw]\n# (final.Fatalities / final.ConfirmedCases).plot(kind='hist', bins = 250);\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Predict on Test Set\n\n# %% [code]\ndata_wp = data_test.copy()\n\n# %% [code]\nif MODEL_Y is 'slope':\n    data_wp['case_slope'] = y_cases_pred_blended_full \n    data_wp['fatality_slope'] = y_fatalities_pred_blended_full \nelse:\n    data_wp['case_slope'] = y_cases_pred_blended_full / x_test.elapsed\n    data_wp['fatality_slope'] = y_fatalities_pred_blended_full / x_test.elapsed\n\ndata_wp['cfr_pred'] = y_cfr_pred_blended_full\n\n# %% [raw]\n# data_wp.head()\n\n# %% [raw]\n# data_wp.shape\n\n# %% [raw]\n# data_wp.Date_f.unique()\n\n# %% [code]\ntrain.Date.max()\n\n# %% [raw]\n# data_wp.Date\n\n# %% [code]\ntest.Date.min()\n\n# %% [raw]\n# test\n\n# %% [code]\nif len(test) > 0:\n    base_date = test.Date.min() - datetime.timedelta(1)\nelse:\n    base_date = train.Date.max()\n\n# %% [raw]\n# train\n\n# %% [raw]\n# len(test)\n\n# %% [code]\nbase_date\n\n# %% [code]\ndata_wp_ss = data_wp[data_wp.Date == base_date]\ndata_wp_ss = data_wp_ss.drop(columns='Date').rename(columns = {'Date_f': 'Date'})\n\n# %% [raw]\n# base_date\n\n# %% [raw]\n# data_wp_ss.head()\n\n# %% [raw]\n# test\n\n# %% [raw]\n# data_wp_ss.columns\n\n# %% [code]\n\n\n# %% [raw]\n# len(test);\n# len(x_test)\n\n# %% [code]\ntest_wp = pd.merge(test, data_wp_ss[['Date', 'Place', 'case_slope', 'fatality_slope', 'cfr_pred',\n                                    'elapsed']], \n            how='left', on = ['Date', 'Place'])\n\n# %% [raw]\n# test_wp[test_wp.Country == 'US']\n\n# %% [raw]\n# test_wp\n\n# %% [code]\nfirst_c_slope = test_wp[~test_wp.case_slope.isnull()].groupby('Place').first()\nlast_c_slope = test_wp[~test_wp.case_slope.isnull()].groupby('Place').last()\n\nfirst_f_slope = test_wp[~test_wp.fatality_slope.isnull()].groupby('Place').first()\nlast_f_slope = test_wp[~test_wp.fatality_slope.isnull()].groupby('Place').last()\n\nfirst_cfr_pred = test_wp[~test_wp.cfr_pred.isnull()].groupby('Place').first()\nlast_cfr_pred = test_wp[~test_wp.cfr_pred.isnull()].groupby('Place').last()\n\n# %% [raw]\n# test_wp\n\n# %% [raw]\n# first_c_slope\n\n# %% [raw]\n# test_wp\n\n# %% [raw]\n# test_wp\n\n# %% [code]\ntest_wp.case_slope = np.where(  test_wp.case_slope.isnull() & \n                     (test_wp.Date < first_c_slope.loc[test_wp.Place].Date.values),\n                   \n                  first_c_slope.loc[test_wp.Place].case_slope.values,\n                     test_wp.case_slope\n                  )\n\ntest_wp.case_slope = np.where(  test_wp.case_slope.isnull() & \n                     (test_wp.Date > last_c_slope.loc[test_wp.Place].Date.values),\n                   \n                  last_c_slope.loc[test_wp.Place].case_slope.values,\n                     test_wp.case_slope\n                  )\n\n# %% [code]\ntest_wp.fatality_slope = np.where(  test_wp.fatality_slope.isnull() & \n                     (test_wp.Date < first_f_slope.loc[test_wp.Place].Date.values),\n                   \n                  first_f_slope.loc[test_wp.Place].fatality_slope.values,\n                     test_wp.fatality_slope\n                  )\n\ntest_wp.fatality_slope = np.where(  test_wp.fatality_slope.isnull() & \n                     (test_wp.Date > last_f_slope.loc[test_wp.Place].Date.values),\n                   \n                  last_f_slope.loc[test_wp.Place].fatality_slope.values,\n                     test_wp.fatality_slope\n                  )\n\n# %% [code]\ntest_wp.cfr_pred = np.where(  test_wp.cfr_pred.isnull() & \n                     (test_wp.Date < first_cfr_pred.loc[test_wp.Place].Date.values),\n                   \n                  first_cfr_pred.loc[test_wp.Place].cfr_pred.values,\n                     test_wp.cfr_pred\n                  )\n\ntest_wp.cfr_pred = np.where(  test_wp.cfr_pred.isnull() & \n                     (test_wp.Date > last_cfr_pred.loc[test_wp.Place].Date.values),\n                   \n                  last_cfr_pred.loc[test_wp.Place].cfr_pred.values,\n                     test_wp.cfr_pred\n                  )\n\n# %% [code]\n\n\n# %% [code]\ntest_wp.case_slope = test_wp.case_slope.interpolate('linear')\ntest_wp.fatality_slope = test_wp.fatality_slope.interpolate('linear')\ntest_wp.cfr_pred = test_wp.cfr_pred.interpolate('linear')\n\n# %% [code]\ntest_wp.case_slope = test_wp.case_slope.fillna(0)\ntest_wp.fatality_slope = test_wp.fatality_slope.fillna(0)\n\n# test_wp.fatality_slope = test_wp.fatality_slope.fillna(0)\n\n# %% [raw]\n# test_wp.cfr_pred.isnull().sum()\n\n# %% [markdown]\n# #### Convert Slopes to Aggregate Counts\n\n# %% [code]\nLAST_DATE = test.Date.min() - datetime.timedelta(1)\n\n# %% [code]\nfinal = train_bk[train_bk.Date == LAST_DATE  ]\n\n# %% [raw]\n# train\n\n# %% [raw]\n# final\n\n# %% [code]\ntest_wp = pd.merge(test_wp, final[['Place', 'ConfirmedCases', 'Fatalities']], on='Place', \n                   how ='left', validate='m:1')\n\n# %% [raw]\n# test_wp\n\n# %% [code]\nLAST_DATE\n\n# %% [raw]\n# test_wp\n\n# %% [code]\ntest_wp.ConfirmedCases = np.exp( \n                            np.log(test_wp.ConfirmedCases + 1) \\\n                                + test_wp.case_slope * \n                                   (test_wp.Date - LAST_DATE).dt.days )- 1\n\ntest_wp.Fatalities = np.exp(\n                            np.log(test_wp.Fatalities + 1) \\\n                              + test_wp.fatality_slope * \n                                   (test_wp.Date - LAST_DATE).dt.days )  -1\n\n# test_wp.Fatalities = np.exp(\n#                             np.log(test_wp.ConfirmedCases + 1) \\\n#                               + test_wp.cfr_pred  )  -1\n                                     \n\n# %% [code]\nLAST_DATE\n\n# %% [raw]\n# final[final.Place=='Italy']\n\n# %% [code]\ntest_wp[ (test_wp.Country == 'Italy')].groupby('Date').sum()[:10]\n\n\n# %% [code]\ntest_wp[ (test_wp.Country == 'US')].groupby('Date').sum().iloc[-5:]\n\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Final Merge\n\n# %% [code]\nfinal = train_bk[train_bk.Date == test.Date.min() - datetime.timedelta(1) ]\n\n# %% [code]\nfinal.head()\n\n# %% [code]\ntest['elapsed'] = (test.Date - final.Date.max()).dt.days \n\n# %% [raw]\n# test.Date\n\n# %% [code]\ntest.elapsed\n\n# %% [code]\n\n\n# %% [markdown]\n# ### CFR Caps\n\n# %% [code]\nfull_bk = test_wp.copy()\n\n# %% [code]\nfull = test_wp.copy()\n\n# %% [code]\n\n\n# %% [code]\nBASE_RATE = 0.01\n\n# %% [code]\nCFR_CAP = 0.13\n\n# %% [code]\n\n\n# %% [code]\nlplot(full_bk)\n\n# %% [code]\nlplot(full_bk, columns = ['case_slope', 'fatality_slope'])\n\n# %% [code]\n\n\n# %% [code]\nfull['cfr_imputed_fatalities_low'] = full.ConfirmedCases * np.exp(full.cfr_pred) / np.exp(0.5)\nfull['cfr_imputed_fatalities_high'] = full.ConfirmedCases * np.exp(full.cfr_pred) * np.exp(0.5)\nfull['cfr_imputed_fatalities'] = full.ConfirmedCases * np.exp(full.cfr_pred)  \n\n# %% [code]\n\n\n# %% [raw]\n# full[(full.case_slope > 0.02) & \n#           (full.Fatalities < full.cfr_imputed_fatalities_low    ) &\n#                 (full.cfr_imputed_fatalities_low > 0.3) &\n#                 ( full.Fatalities < 100 ) &\n#     (full.Country!='China')] \\\n#      .groupby('Place').count()\\\n#     .sort_values('ConfirmedCases', ascending=False).iloc[:, 9:]\n\n# %% [code]\nfull[(full.case_slope > 0.02) & \n                   (full.Fatalities < full.cfr_imputed_fatalities_low    ) &\n                (full.cfr_imputed_fatalities_low > 0.3) &\n                ( full.Fatalities < 100000 ) &\n    (full.Country!='China') &\n     (full.Date == datetime.datetime(2020, 4,15))] \\\n     .groupby('Place').last()\\\n    .sort_values('Fatalities', ascending=False).iloc[:, 9:]\n\n# %% [code]\n(np.log(full.Fatalities + 1) -np.log(full.cfr_imputed_fatalities) ).plot(kind='hist', bins = 250)\n\n# %% [raw]\n# full[  \n#                    (np.log(full.Fatalities + 1) < np.log(full.cfr_imputed_fatalities_high + 1) -0.5    ) \n#     & (~full.Country.isin(['China', 'Korea, South']))\n#                 ][full.Date==train.Date.max()]\\\n#      .groupby('Place').first()\\\n#     .sort_values('cfr_imputed_fatalities', ascending=False).iloc[:, 9:]\n\n# %% [code]\nfull[(full.case_slope > 0.02) & \n                   (full.Fatalities < full.cfr_imputed_fatalities_low    ) &\n                (full.cfr_imputed_fatalities_low > 0.3) &\n                ( full.Fatalities < 100000 ) &\n    (~full.Country.isin(['China', 'Korea, South']))][full.Date==train.Date.max()]\\\n     .groupby('Place').first()\\\n    .sort_values('cfr_imputed_fatalities', ascending=False).iloc[:, 9:]\n\n# %% [code]\nfull.Fatalities = np.where(   \n    (full.case_slope > 0.02) & \n                   (full.Fatalities <= full.cfr_imputed_fatalities_low    ) &\n                (full.cfr_imputed_fatalities_low > 0.3) &\n                ( full.Fatalities < 100000 ) &\n    (~full.Country.isin(['China', 'Korea, South'])) ,\n                        \n                        (full.cfr_imputed_fatalities_high + full.cfr_imputed_fatalities)/2,\n                                    full.Fatalities)\n    \n\n# %% [raw]\n# assert len(full) == len(data_wp)\n\n# %% [raw]\n# x_test.shape\n\n# %% [code]\nfull['elapsed'] = (test_wp.Date - LAST_DATE).dt.days\n\n# %% [code]\nfull[ (full.case_slope > 0.02) & \n          (np.log(full.Fatalities + 1) < np.log(full.ConfirmedCases * BASE_RATE + 1) - 0.5) &\n                           (full.Country != 'China')]\\\n            [full.Date == datetime.datetime(2020, 4, 5)] \\\n            .groupby('Place').last().sort_values('ConfirmedCases', ascending=False).iloc[:,8:]\n\n# %% [raw]\n# full.Fatalities.max()\n\n# %% [code]\nfull.Fatalities = np.where((full.case_slope > 0.02) & \n                      (full.Fatalities < full.ConfirmedCases * BASE_RATE) &\n                           (full.Country != 'China'), \n                                            \n            np.exp(   \n                    np.log( full.ConfirmedCases * BASE_RATE + 1) \\\n                           * np.clip(   0.5* (full.elapsed - 1) / 30, 0, 1) \\\n                           \n                     +  np.log(full.Fatalities +1 ) \\\n                           * np.clip(1 - 0.5* (full.elapsed - 1) / 30, 0, 1)\n            ) -1\n                           \n                           ,\n                                               full.Fatalities)  \n\n# %% [raw]\n# full.elapsed\n\n# %% [code]\nfull[(full.case_slope > 0.02) & \n                   (full.Fatalities > full.cfr_imputed_fatalities_high   ) &\n                (full.cfr_imputed_fatalities_low > 0.4) &\n    (full.Country!='China')]\\\n     .groupby('Place').count()\\\n    .sort_values('ConfirmedCases', ascending=False).iloc[:, 8:]\n\n# %% [raw]\n# full[full.Place=='United KingdomTurks and Caicos Islands']\n\n# %% [code]\nfull[(full.case_slope > 0.02) & \n                   (full.Fatalities > full.cfr_imputed_fatalities_high * 2   ) &\n                (full.cfr_imputed_fatalities_low > 0.4) &\n    (full.Country!='China')  ]\\\n     .groupby('Place').last()\\\n    .sort_values('ConfirmedCases', ascending=False).iloc[:, 8:]\n\n# %% [code]\nfull[(full.case_slope > 0.02) & \n                   (full.Fatalities > full.cfr_imputed_fatalities_high * 1.5   ) &\n                (full.cfr_imputed_fatalities_low > 0.4) &\n    (full.Country!='China')][full.Date==train.Date.max()]\\\n     .groupby('Place').first()\\\n    .sort_values('ConfirmedCases', ascending=False).iloc[:, 8:]\n\n# %% [code]\n\n\n# %% [code]\nfull.Fatalities =  np.where(  (full.case_slope > 0.02) & \n                   (full.Fatalities > full.cfr_imputed_fatalities_high      * 2   ) &\n                (full.cfr_imputed_fatalities_low > 0.4) &\n                (full.Country!='China') ,\n                            \n                     full.cfr_imputed_fatalities,\n                            \n                            full.Fatalities)\n\nfull.Fatalities =  np.where(  (full.case_slope > 0.02) & \n                   (full.Fatalities > full.cfr_imputed_fatalities_high   ) &\n                (full.cfr_imputed_fatalities_low > 0.4) &\n                (full.Country!='China') ,\n                    np.exp(        \n                            0.6667 * np.log(full.Fatalities + 1) \\\n                        + 0.3333 * np.log(full.cfr_imputed_fatalities + 1)\n                                ) - 1,\n                            \n                            full.Fatalities)\n\n# %% [code]\n\n\n# %% [code]\nfull[(full.Fatalities > full.ConfirmedCases * CFR_CAP) &\n                                          (full.ConfirmedCases > 1000)\n\n    ]                        .groupby('Place').last().sort_values('Fatalities', ascending=False)\n\n# %% [raw]\n# full.Fatalities =  np.where( (full.Fatalities > full.ConfirmedCases * CFR_CAP) &\n#                                           (full.ConfirmedCases > 1000)\n#                                         , \n#                              full.ConfirmedCases * CFR_CAP\\\n#                                            * np.clip((full.elapsed - 5) / 15, 0, 1) \\\n#                                  +  full.Fatalities * np.clip(1 - (full.elapsed - 5) / 15, 0, 1)\n#                             , \n#                                                full.Fatalities)\n\n# %% [raw]\n# train[train.Country=='Italy']\n\n# %% [raw]\n# final[final.Country=='US'].sum()\n\n# %% [code]\n(np.log(full.Fatalities + 1) -np.log(full.cfr_imputed_fatalities) ).plot(kind='hist', bins = 250)\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Fix Slopes now\n\n# %% [raw]\n# final\n\n# %% [code]\nassert len(pd.merge(full, final, on='Place', suffixes = ('', '_i'), validate='m:1')) == len(full)\n\n# %% [code]\nffm = pd.merge(full, final, on='Place', suffixes = ('', '_i'), validate='m:1')\nffm['fatality_slope'] = (np.log(ffm.Fatalities + 1 )                             - np.log(ffm.Fatalities_i + 1 ) )                                  / ffm.elapsed\nffm['case_slope'] = (np.log(ffm.ConfirmedCases + 1 )                              - np.log(ffm.ConfirmedCases_i + 1 ) )                                  / ffm.elapsed\n\n# %% [markdown]\n# #### Fix Upward Slopers\n\n# %% [raw]\n# final_slope = (ffm.groupby('Place').last().case_slope)\n# final_slope.sort_values(ascending=False)\n# \n# high_final_slope = final_slope[final_slope > 0.1].index\n\n# %% [raw]\n# slope_change = (ffm.groupby('Place').last().case_slope - ffm.groupby('Place').first().case_slope)\n# slope_change.sort_values(ascending = False)\n# high_slope_increase = slope_change[slope_change > 0.05].index\n\n# %% [code]\n\n\n# %% [raw]\n# test.Date.min()\n\n# %% [raw]\n# set(high_slope_increase) & set(high_final_slope)\n\n# %% [raw]\n# ffm.groupby('Date').case_slope.median()\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Fix Drop-Offs\n\n# %% [code]\nffm[np.log(ffm.Fatalities+1) < np.log(ffm.Fatalities_i+1) - 0.2]    [['Place', 'Date', 'elapsed', 'Fatalities', 'Fatalities_i']]\n\n# %% [code]\nffm[np.log(ffm.ConfirmedCases + 1) < np.log(ffm.ConfirmedCases_i+1) - 0.2]    [['Place', 'elapsed', 'ConfirmedCases', 'ConfirmedCases_i']]\n\n# %% [code]\n\n\n# %% [raw]\n# (ffm.groupby('Place').last().fatality_slope - ffm.groupby('Place').first().fatality_slope)\\\n#     .sort_values(ascending = False)[:10]\n\n# %% [markdown]\n# ### Display\n\n# %% [raw]\n# full[full.Country=='US'].groupby('Date').agg(\n#     {'ForecastId': 'count',\n#      'case_slope': 'mean',\n#         'fatality_slope': 'mean',\n#             'ConfirmedCases': 'sum',\n#                 'Fatalities': 'sum',\n#                     })\n\n# %% [code]\nfull_bk[(full_bk.Date == test.Date.max() ) & \n   (~full_bk.Place.isin(new_places))].groupby('Country').agg(\n    {'ForecastId': 'count',\n     'case_slope': 'last',\n        'fatality_slope': 'last',\n            'ConfirmedCases': 'sum',\n                'Fatalities': 'sum',\n                    }\n).sort_values('ConfirmedCases', ascending=False)\n\n# %% [raw]\n# full[full.Country=='China'].groupby('Date').agg(\n#     {'ForecastId': 'count',\n#      'case_slope': 'mean',\n#         'fatality_slope': 'mean',\n#             'ConfirmedCases': 'sum',\n#                 'Fatalities': 'sum',\n#                     })[::5]\n\n# %% [code]\n\n\n# %% [raw]\n# ffc = pd.merge(final, full, on='Place', validate = '1:m')\n# ffc[(np.log(ffc.Fatalities_x) - np.log(ffc.ConfirmedCase_x)) / ffc.elapsed_y ]\n\n# %% [raw]\n# ffm.groupby('Place').case_slope.last().sort_values(ascending = False)[:30]\n\n# %% [raw]\n# lplot(test_wp)\n\n# %% [raw]\n# lplot(test_wp, columns = ['case_slope', 'fatality_slope'])\n\n# %% [code]\n\n# %% [code]\n\n\n# %% [code]\nlplot(ffm[~ffm.Place.isin(new_places)])\n\n# %% [code]\n\n\n# %% [code]\nlplot(ffm[~ffm.Place.isin(new_places)], columns = ['case_slope', 'fatality_slope'])\n\n# %% [code]\n\n\n# %% [raw]\n# test.Date.min()\n\n# %% [code]\nffm.fatality_slope = np.clip(ffm.fatality_slope, None, 0.5)\n\n# %% [raw]\n# ffm.case_slope = np.clip(ffm.case_slope, None, 0.25)\n\n# %% [code]\n\n\n# %% [raw]\n# for lr in [0.05, 0.02, 0.01, 0.007, 0.005, 0.003]:\n# \n#     ffm.loc[ (ffm.Place==ffm.Place.shift(1) )\n#          & (ffm.Place==ffm.Place.shift(-1) ) &\n#      ( np.abs ( (ffm.case_slope.shift(-1) + ffm.case_slope.shift(1) ) / 2\n#                        - ffm.case_slope).fillna(0)\n#                     > lr ), 'case_slope'] = \\\n#                      ( ffm.case_slope.shift(-1) + ffm.case_slope.shift(1) ) / 2\n# \n\n# %% [code]\nfor lr in [0.2, 0.14, 0.1, 0.07, 0.05, 0.03, 0.01 ]:\n\n    ffm.loc[ (ffm.Place==ffm.Place.shift(4) )\n         & (ffm.Place==ffm.Place.shift(-4) ), 'fatality_slope'] = \\\n         ( ffm.fatality_slope.shift(-2) * 0.25 \\\n              + ffm.fatality_slope.shift(-1) * 0.5 \\\n                + ffm.fatality_slope \\\n                  + ffm.fatality_slope.shift(1) * 0.5 \\\n                    + ffm.fatality_slope.shift(2) * 0.25 ) / 2.5\n\n\n# %% [code]\n\n\n# %% [code]\nffm.ConfirmedCases = np.exp( \n                            np.log(ffm.ConfirmedCases_i + 1) \\\n                                + ffm.case_slope * \n                                   ffm.elapsed ) - 1\n\nffm.Fatalities = np.exp(\n                            np.log(ffm.Fatalities_i + 1) \\\n                              + ffm.fatality_slope * \n                                   ffm.elapsed ) - 1\n# test_wp.Fatalities = np.exp(\n#                             np.log(test_wp.ConfirmedCases + 1) \\\n#                               + test_wp.cfr_pred  )  -1\n                                     \n\n# %% [code]\n\n\n# %% [code]\nlplot(ffm[~ffm.Place.isin(new_places)])\n\n# %% [code]\n\n\n# %% [code]\nlplot(ffm[~ffm.Place.isin(new_places)], columns = ['case_slope', 'fatality_slope'])\n\n# %% [code]\n\n\n# %% [code]\nffm[(ffm.Date == test.Date.max() ) & \n   (~ffm.Place.isin(new_places))].groupby('Country').agg(\n    {'ForecastId': 'count',\n     'case_slope': 'last',\n        'fatality_slope': 'last',\n            'ConfirmedCases': 'sum',\n                'Fatalities': 'sum',\n                    }\n).sort_values('ConfirmedCases', ascending=False)\n\n# %% [code]\n\n\n# %% [code]\nffm_bk = ffm.copy()\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\nffm = ffm_bk.copy()\n\n# %% [code]\ncounter = Counter(data.Place)\n# counter.most_common()\nmedian_count = np.median([ counter[group] for group in ffm.Place])\n# [ (group, np.round( np.power(counter[group] / median_count, -1),3) ) for group in \n#      counter.keys()]\nc_count = [ np.clip(\n            np.power(counter[group] / median_count, -1.5), None, 2.5) for group in ffm.Place]\n \n\n# %% [code]\nRATE_MULT = 0.00\nRATE_ADD = 0.003\nLAG_FALLOFF = 15\n\nma_factor = np.clip( ( ffm.elapsed - 14) / 14 , 0, 1)\n\nffm.case_slope = np.where(ffm.elapsed > 0,\n    0.7 * ffm.case_slope * (1+ ma_factor * RATE_MULT) \\\n         + 0.3 * (  ffm.case_slope.ewm(span=LAG_FALLOFF).mean()\\\n                                                      * np.clip(ma_factor, 0, 1)\n                      + ffm.case_slope    * np.clip( 1 - ma_factor, 0, 1)) \n                          \n                          + RATE_ADD * ma_factor * c_count,\n         ffm.case_slope)\n\n# --\n\nRATE_MULT = 0\nRATE_ADD = 0.015\nLAG_FALLOFF = 15\n\nma_factor = np.clip( ( ffm.elapsed - 10) / 14 , 0, 1)\n\n\nffm.fatality_slope = np.where(ffm.elapsed > 0,\n    0.3 * ffm.fatality_slope * (1+ ma_factor * RATE_MULT) \\\n         + 0.7* (  ffm.fatality_slope.ewm(span=LAG_FALLOFF).mean()\\\n                                                              * np.clip( ma_factor, 0, 1)\n                      + ffm.fatality_slope    * np.clip( 1 - ma_factor, 0, 1)   )\n                              \n                              + RATE_ADD * ma_factor * c_count \\\n                              \n                              \n                              * (ffm.Country != 'China')\n                              ,\n         ffm.case_slope)\n\n# %% [code]\nffm.ConfirmedCases = np.exp( \n                            np.log(ffm.ConfirmedCases_i + 1) \\\n                                + ffm.case_slope * \n                                   ffm.elapsed ) - 1\n\nffm.Fatalities = np.exp(\n                            np.log(ffm.Fatalities_i + 1) \\\n                              + ffm.fatality_slope * \n                                   ffm.elapsed ) - 1\n# test_wp.Fatalities = np.exp(\n#                             np.log(test_wp.ConfirmedCases + 1) \\\n#                               + test_wp.cfr_pred  )  -1\n                                     \n\n# %% [code]\n\n\n# %% [code]\nlplot(ffm[~ffm.Place.isin(new_places)])\n\n# %% [code]\n\n\n# %% [code]\nlplot(ffm[~ffm.Place.isin(new_places)], columns = ['case_slope', 'fatality_slope'])\n\n# %% [code]\n\n\n# %% [raw]\n# LAST_DATE\n\n# %% [code]\nffm_bk[(ffm_bk.Date == test.Date.max() ) & \n   (~ffm_bk.Place.isin(new_places))].groupby('Country').agg(\n    {'ForecastId': 'count',\n     'case_slope': 'last',\n        'fatality_slope': 'last',\n            'ConfirmedCases': 'sum',\n                'Fatalities': 'sum',\n                    }\n).sort_values('ConfirmedCases', ascending=False)[:15]\n\n# %% [code]\nffm[(ffm.Date == test.Date.max() ) & \n   (~ffm.Place.isin(new_places))].groupby('Country').agg(\n    {'ForecastId': 'count',\n     'case_slope': 'last',\n        'fatality_slope': 'last',\n            'ConfirmedCases': 'sum',\n                'Fatalities': 'sum',\n                    }\n).sort_values('ConfirmedCases', ascending=False)[:15]\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\nffm_bk[(ffm_bk.Date == test.Date.max() ) & \n   (~ffm_bk.Place.isin(new_places))].groupby('Country').agg(\n    {'ForecastId': 'count',\n     'case_slope': 'last',\n        'fatality_slope': 'last',\n            'ConfirmedCases': 'sum',\n                'Fatalities': 'sum',\n                    }\n).sort_values('ConfirmedCases', ascending=False)[-50:]\n\n# %% [code]\nffm[(ffm.Date == test.Date.max() ) & \n   (~ffm.Place.isin(new_places))].groupby('Country').agg(\n    {'ForecastId': 'count',\n     'case_slope': 'last',\n        'fatality_slope': 'last',\n            'ConfirmedCases': 'sum',\n                'Fatalities': 'sum',\n                    }\n).loc[ffm_bk[(ffm_bk.Date == test.Date.max() ) & \n   (~ffm_bk.Place.isin(new_places))].groupby('Country').agg(\n    {'ForecastId': 'count',\n     'case_slope': 'last',\n        'fatality_slope': 'last',\n            'ConfirmedCases': 'sum',\n                'Fatalities': 'sum',\n                    }\n).sort_values('ConfirmedCases', ascending=False)[-50:].index]\n\n# %% [code]\n\n\n# %% [code]\n# use country-specific CFR !!!!  helps cap US and raise up Italy !\n# could also use lagged CFR off cases as of 2 weeks ago...\n # ****  keep everything within ~0.5 order of magnitude of its predicted CFR.. !!\n\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Join\n\n# %% [raw]\n# assert len(test_wp) == len(full)\n# \n\n# %% [raw]\n# full = pd.merge(test_wp, full[['Place', 'Date', 'Fatalities']], on = ['Place', 'Date'],\n#             validate='1:1')\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Fill in New Places with Ramp Average\n\n# %% [code]\nNUM_TEST_DATES = len(test.Date.unique())\n\nbase = np.zeros((2, NUM_TEST_DATES))\nbase2 = np.zeros((2, NUM_TEST_DATES))\n\n# %% [code]\nfor idx, c in enumerate(['ConfirmedCases', 'Fatalities']):\n    for n in range(0, NUM_TEST_DATES):\n        base[idx,n] = np.mean(\n            np.log(  train[((train.Date < test.Date.min())) & \n              (train.ConfirmedCases > 0)].groupby('Country').nth(n)[c]+1))\n\n# %% [code]\nbase = np.pad( base, ((0,0), (6,0)), mode='constant', constant_values = 0)\n\n# %% [code]\nfor n in range(0, base2.shape[1]):\n    base2[:, n] = np.mean(base[:, n+0: n+7], axis = 1)\n\n# %% [code]\nnew_places = train[(train.Date == test.Date.min() - datetime.timedelta(1)) &\n      (train.ConfirmedCases == 0)\n     ].Place\n\n# %% [code]\n# fill in new places \nffm.ConfirmedCases =     np.where(   ffm.Place.isin(new_places),\n          base2[ 0, (ffm.Date - test.Date.min()).dt.days],\n                 ffm.ConfirmedCases)\nffm.Fatalities =     np.where(   ffm.Place.isin(new_places),\n          base2[ 1, (ffm.Date - test.Date.min()).dt.days],\n                 ffm.Fatalities)\n\n# %% [code]\n\n\n# %% [code]\nffm[ffm.Country=='US'].groupby('Date').agg(\n    {'ForecastId': 'count',\n     'case_slope': 'mean',\n        'fatality_slope': 'mean',\n            'ConfirmedCases': 'sum',\n                'Fatalities': 'sum',\n                    })\n\n# %% [raw]\n# train[train.Country == 'US'].Province_State.unique()\n\n# %% [markdown]\n# ### Save\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\nsub = pd.read_csv(input_path + '/submission.csv')\n\n# %% [code]\nscl = sub.columns.to_list()\n\n# %% [code]\n\nprint(full_bk.groupby('Place').last()[['Date', 'ConfirmedCases', 'Fatalities']])\nprint(ffm.groupby('Place').last()[['Date', 'ConfirmedCases', 'Fatalities']])\n\n\n# %% [code]\nif ffm[scl].isnull().sum().sum() == 0:\n    out = full_bk[scl] * 0.7 + ffm[scl] * 0.3\nelse:\n    print('using full-bk')\n    out = full_bk[scl]\n\n\nout = out[sub.columns.to_list()]\nout.ForecastId = np.round(out.ForecastId, 0).astype(int) \nout = np.round(out, 2)\nprivate = out\n  \n\n\nfull_pred = pd.concat((private, public[~public.ForecastId.isin(private.ForecastId)]),\n     ignore_index=True).sort_values('ForecastId')\n#full_pred.to_csv('submission.csv', index=False)\n\n\n# In[6]:\n\n\nsub1.sort_values(\"ForecastId\", inplace=True)\n\n\n# In[7]:\n\n\n\nsub4=full_pred\nsub4.sort_values(\"ForecastId\", inplace=True)\ntrain = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\ndacheckv='2020-04-01'\ntrain['Province_State'].fillna('', inplace=True)\ntrain['Date'] = pd.to_datetime(train['Date'])\ntrain['day'] = train.Date.dt.dayofyear\n#train = train[train.day <= 85]\ntrain['geo'] = ['_'.join(x) for x in zip(train['Country_Region'], train['Province_State'])]\ntrain=train[train.Date>dacheckv]\n\ntest = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')\ntest['Province_State'].fillna('', inplace=True)\ntest['Date'] = pd.to_datetime(test['Date'])\ntest['day'] = test.Date.dt.dayofyear\n#train = train[train.day <= 85]\ntest['geo'] = ['_'.join(x) for x in zip(test['Country_Region'], test['Province_State'])]\ntest1=test.merge(train,on=['Country_Region','Province_State','Date'],how='inner')\nprint(RMSLE(sub1[sub1.ForecastId.isin(test1.ForecastId.values)]['ConfirmedCases'].values,test1['ConfirmedCases'].values))\n\nprint(RMSLE(sub1[sub1.ForecastId.isin(test1.ForecastId.values)]['Fatalities'].values,test1['Fatalities'].values))\n\n\n# In[8]:\n\n\nprint(RMSLE(sub1[sub1.ForecastId.isin(test1.ForecastId.values)]['ConfirmedCases'].values,test1['ConfirmedCases'].values))\n\nprint(RMSLE(sub1[sub1.ForecastId.isin(test1.ForecastId.values)]['Fatalities'].values,test1['Fatalities'].values))\n\n\nprint(RMSLE(sub4[sub4.ForecastId.isin(test1.ForecastId.values)]['ConfirmedCases'].values,test1['ConfirmedCases'].values))\n\nprint(RMSLE(sub4[sub4.ForecastId.isin(test1.ForecastId.values)]['Fatalities'].values,test1['Fatalities'].values))\n\n\n# In[9]:\n\n\nsub1=sub1.reset_index(drop=True)\nsub4=sub4.reset_index(drop=True)\n\nsub_df = sub1.copy()\nTARGETS = [\"ConfirmedCases\", \"Fatalities\"]\nfor t in TARGETS:\n    if t=='ConfirmedCases':\n        sub_df[t] = sub1[t]*0.01 + sub4[t]*0.99\n    else:\n        sub_df[t] = sub1[t]*0.01 + sub4[t]*0.99\n    \nsub_df.to_csv(\"submission.csv\", index=False)\nprint(RMSLE(sub_df[sub_df.ForecastId.isin(test1.ForecastId.values)]['ConfirmedCases'].values,test1['ConfirmedCases'].values))\n\nprint(RMSLE(sub_df[sub_df.ForecastId.isin(test1.ForecastId.values)]['Fatalities'].values,test1['Fatalities'].values))\n\n\n# In[ ]:\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}