{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\ntest = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')\nsamplesub = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# from pandas_profiling import ProfileReport\n# ProfileReport(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"samplesub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# [code: 103]\ntrain.Province_State.fillna(train.Country_Region, inplace=True)\ntrain.Province_State.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['Province_State', 'Country_Region', 'Id']] = train[['Province_State', 'Country_Region', 'Id']].apply(lambda x: x.astype('category'))\ntrain[['ConfirmedCases', 'Fatalities']] = train[['ConfirmedCases', 'Fatalities']].astype('int')\n\ntrain['Date']= pd.to_datetime(train['Date'])\n# Or, train.Date = train.Date.apply(pd.to_datetime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum() / train.shape[0] * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# [code: 103] one line code above for same code below\n# train['Province_State'] = train['Province_State'].astype('str')\n# train['Country_Region'] = train['Country_Region'].astype('str')\n\n# # train.loc[train['Province_State'] == 'nan', ['Province_State']].shape\n# # train.loc[train['Province_State'] == 'nan', ['Country_Region']].shape\n# # train['Province_State'].isna().sum()\n\n# train.loc[train['Province_State'] == 'nan', ['Province_State']] = train.loc[train['Province_State'] == 'nan', ['Country_Region']]\n# train['Province_State'].fillna(train['Country_Region'], inplace=True) # category not present\n\n# # train['Province_State'] = train['Province_State'].astype('category')\n# train['Province_State'] = pd.Categorical(train['Province_State'])\n# train['Country_Region'] = pd.Categorical(train['Country_Region'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['days_since'] = train['Date'].apply(lambda x: (x - pd.to_datetime('2020-01-21')).days )\ntrain[train['days_since']==1].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train['month_date'] = train.Date.dt.strftime(\"%m%d\").astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding feature continent and sub_region\n\ncoucon = pd.read_csv('/kaggle/input/country-to-continent/countryContinent.csv', encoding='latin-1')\n\n# ss = train.merge(coucon[['country', 'continent', 'sub_region']],how='left', left_on='Country_Region', right_on='country').fillna(np.nan)\n# train.drop(['country'],axis=1,inplace=True)\n# ss[ss['continent'].isna()]['Country_Region'].unique()\n\ndef get_continent(x):\n    if coucon['country'].str.contains(x).any():\n        return coucon.loc[coucon['country'].str.contains(x), 'continent'].iloc[0] \n    else:\n        np.nan\n\ntrain['continent'] = train['Country_Region'].apply(get_continent)\n\ntrain.loc[train['Country_Region']== 'Burma', ['continent']] = 'Asia'\ntrain.loc[train['Country_Region'].isin(['Congo (Brazzaville)', 'Congo (Kinshasa)']), ['continent']] = 'Africa'\ntrain.loc[train['Country_Region']== \"Cote d'Ivoire\", ['continent']] = 'Africa'\ntrain.loc[train['Country_Region']== \"Czechia\", ['continent']] = 'Europe'\ntrain.loc[train['Country_Region']== \"Diamond Princess\", ['continent']] = 'Asia'\ntrain.loc[train['Country_Region']== \"Eswatini\", ['continent']] = 'Africa'\ntrain.loc[train['Country_Region']== \"India\", ['continent']] = 'Asia'\ntrain.loc[train['Country_Region']== \"Korea, South\", ['continent']] = 'Asia'\ntrain.loc[train['Country_Region']== \"Kosovo\", ['continent']] = 'Europe'\ntrain.loc[train['Country_Region']== \"Laos\", ['continent']] = 'Asia'\ntrain.loc[train['Country_Region']== \"MS Zaandam\", ['continent']] = 'Americas'\ntrain.loc[train['Country_Region']== \"North Macedonia\", ['continent']] = 'Europe'\ntrain.loc[train['Country_Region']== \"US\", ['continent']] = 'Americas'\ntrain.loc[train['Country_Region']== \"Vietnam\", ['continent']] = 'Asia'\ntrain.loc[train['Country_Region']== \"West Bank and Gaza\", ['continent']] = 'Asia'\n\ntrain.continent = train.continent.astype('category')\n\ntrain['continent'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainc = train.copy()\n# y = trainc.loc[:, ['ConfirmedCases']]\n# y = trainc.loc[:, ['Fatalities']]\ny = trainc.loc[:, ['ConfirmedCases', 'Fatalities']]\n\n# X = trainc.loc[:, ['Province_State', 'Country_Region', 'Date', 'days_since']]\nX = trainc.loc[:, ['Province_State', 'Country_Region', 'days_since',]]\n\n# split dataset in train and test\nfrom sklearn.model_selection import train_test_split\n# 80/20 split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, OneHotEncoder\n# Merge less frequent levels. OHE on limited levels won't generate large no. of features.\n\n# Define which columns should be encoded vs scaled\nnumeric_ftrs  = X_train.select_dtypes(include=['float64','int64'])\ncategorical_ftrs = X_train.select_dtypes(include=['category'])\n\n# Instantiate encoder/scaler\nscaler = StandardScaler()\nohe    = OneHotEncoder(sparse=False)\n\n# auto\nscaled_data = pd.DataFrame(scaler.fit_transform(numeric_ftrs), columns = ['days_since'])\n\nencoder = OneHotEncoder(handle_unknown=\"ignore\")\n# encoder.fit(X_train)\nencoder.fit(categorical_ftrs)\n# encoded_ftrs = pd.DataFrame(encoder.fit_transform(categorical_ftrs).toarray())\nencoded_ftrs = pd.DataFrame(encoder.fit_transform(categorical_ftrs).toarray(), columns = encoder.get_feature_names(categorical_ftrs.columns.to_list()))\n\nX_train = pd.concat([scaled_data, encoded_ftrs], axis = 1)\nprint(\"ran\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"te_numeric_ftrs  = X_test.select_dtypes(include=['float64','int64'])\nte_categorical_ftrs = X_test.select_dtypes(include=['category'])\n\nte_scaled_data = pd.DataFrame(scaler.fit_transform(te_numeric_ftrs), columns = numeric_ftrs.columns.to_list())\n\n# te_encoded_ftrs = pd.DataFrame(encoder.fit_transform(te_categorical_ftrs).toarray())\nte_encoded_ftrs = pd.DataFrame(encoder.fit_transform(te_categorical_ftrs).toarray(), columns = encoder.get_feature_names(te_categorical_ftrs.columns.to_list()))\n\nX_test = pd.concat([te_scaled_data, te_encoded_ftrs], axis = 1)\nprint(\"ran\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_y_train = y_train.copy()\nrf_y_test = y_test.copy()\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(n_jobs=-1, n_estimators=100, oob_score=True, max_features='sqrt')\n# rf = RandomForestRegressor(n_jobs=-1,n_estimators=50,oob_score=True,max_features=0.8,min_samples_leaf=5)\n# rf = RandomForestRegressor(n_jobs=-1,n_estimators=50,min_samples_leaf=7,max_features=0.5,oob_score=True,max_depth=40)\n\nrf.fit(X_train, rf_y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_cc = y_train['ConfirmedCases']\ny_train_fat = y_train['Fatalities']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # XGBoost\n\n# from sklearn.model_selection import ShuffleSplit, cross_val_score\n# skfold = ShuffleSplit(random_state=7)\n\n# import xgboost as xgb\n\n# reg_xgb_cc = xgb.XGBRegressor(n_estimators = 400)\n# reg_xgb_fat = xgb.XGBRegressor(n_estimators = 200)\n\n# xgb_acc = cross_val_score(reg_xgb_cc, X_train, y_train_cc, cv = skfold)\n# xgb_acc_fat = cross_val_score(reg_xgb_fat, X_train, y_train_fat, cv = skfold)\n\n# # print (xgb_acc.mean())\n# print (xgb_acc.mean(), xgb_acc_fat.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reg_xgb_cc.fit(X_train, Y_train_cc)\n# y_pred_cc = reg_xgb_cc.predict(X_test) \n\n# reg_xgb_fat.fit(X_train, Y_train_fat)\n# y_pred_fat = reg_xgb_fat.predict(X_test) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # BaggingRegressor\n# y_train_cc = y_train['ConfirmedCases']\n# y_train_fat = y_train['Fatalities']\n\n# from sklearn.ensemble import BaggingRegressor\n# from sklearn.tree import DecisionTreeRegressor\n\n# reg_bgr_cc = BaggingRegressor(base_estimator = DecisionTreeRegressor(), n_estimators = 100)\n# reg_bgr_fat = BaggingRegressor(base_estimator = DecisionTreeRegressor(), n_estimators = 70)\n\n# bgr_acc = cross_val_score(reg_bgr_cc, X_train, y_train_cc, cv = skfold)\n# bgr_acc_fat = cross_val_score(reg_bgr_fat, X_train, y_train_fat, cv = skfold)\n# print (bgr_acc.mean(), bgr_acc_fat.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reg_bgr_cc.fit(X_train, y_train_cc)\n# y_pred_cc = clf_bgr_cc.predict(X_test)\n\n# reg_bgr_fat.fit(X_train, y_train_cc)\n# y_pred_cc = reg_bgr_fat.predict(X_test) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train Score: ', rf.score(X_train, rf_y_train), \". \", end = '')\nprint('test Score: ', rf.score(X_test, rf_y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Chained Models for Each Output (RegressorChain)\n# https://machinelearningmastery.com/multi-output-regression-models-with-python/\n# Another approach to using single-output regression models for multioutput regression is to create a linear \n# sequence of models.\n\n# The first model in the sequence uses the input and predicts one output; the second model uses the input and \n# the output from the first model to make a prediction; the third model uses the input and output from the \n# first two models to make a prediction, and so on.\n\nfrom sklearn.multioutput import RegressorChain\nwrapper = RegressorChain(rf)\nwrapper.fit(X_train, y_train)\n\nrf_y_test_pred = wrapper.predict(X_test)\n# summarize prediction\nprint(rf_y_test_pred[0:5])\nprint(rf_y_test_pred.astype('int')[0:5])\nrf_y_test_pred = rf_y_test_pred.astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the R forest's predict method on the test data\nrf_y_test_pred = rf.predict(X_test)\nprint(rf_y_test_pred[0:5])\nprint(rf_y_test_pred.astype('int')[0:5])\nrf_y_test_pred = rf_y_test_pred.astype('int')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RMSLE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSLE\nfrom sklearn.metrics import mean_squared_log_error\n# np.sqrt(mean_squared_log_error(y_test, predictions ))\nrf_y_test\nrf_y_test_pred2 = pd.DataFrame(rf_y_test_pred, columns = ['ConfirmedCases', 'Fatalities'])\n\nnp.sqrt(mean_squared_log_error(rf_y_test['ConfirmedCases'], rf_y_test_pred2['ConfirmedCases']))\nnp.sqrt(mean_squared_log_error(rf_y_test['Fatalities'], rf_y_test_pred2['Fatalities']))\n\n# mean of individual rmsle\nres = (np.sqrt(mean_squared_log_error(rf_y_test['ConfirmedCases'], rf_y_test_pred2['ConfirmedCases'])) +\nnp.sqrt(mean_squared_log_error(rf_y_test['Fatalities'], rf_y_test_pred2['Fatalities'])) )/2\nprint(\"mean of rmsle individual (func) = {}\".format(res))\n\n\nimport math\n#A function to calculate Root Mean Squared Logarithmic Error (RMSLE)\ndef rmsle(y, y_pred):\n    assert len(y) == len(y_pred)\n    terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n    return (sum(terms_to_sum) * (1.0/len(y)) ) ** 0.5\n\nres = (rmsle(rf_y_test['ConfirmedCases'].to_numpy(), rf_y_test_pred2['ConfirmedCases'].to_numpy()) +\nrmsle(rf_y_test['Fatalities'].to_numpy(), rf_y_test_pred2['Fatalities'].to_numpy()) )/2\n\nprint(\"mean of rmsle individual(manual) = {}\".format(res))\n\n\n# RMSLE combined for target columns\n# first sum of squared errors, then mean and root\n# manual\n# y,y_pred = rf_y_test['ConfirmedCases'].to_numpy(), rf_y_test_pred2['ConfirmedCases'].to_numpy()\n# conf_sle2 = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n\n# y,y_pred = rf_y_test['Fatalities'].to_numpy(), rf_y_test_pred2['Fatalities'].to_numpy()\n# fata_sle2 = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n\n# print( ( (sum(conf_sle2) + sum(fata_sle2)) * (1.0/(2 * len(y))) )** 0.5 )\n\n\ndef rmsle_2col(y1, y_pred1,y2, y_pred2):\n    assert len(y1) == len(y_pred1)\n    terms_to_sum1 = [(math.log(y_pred1[i] + 1) - math.log(y1[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred1)]\n    assert len(y2) == len(y_pred2)\n    terms_to_sum2 = [(math.log(y_pred2[i] + 1) - math.log(y2[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred2)]\n    \n    return ( (sum(terms_to_sum1) + sum(terms_to_sum2) ) * (1.0/(2 * len(y1))) ) ** 0.5\n\nres = rmsle_2col(rf_y_test['ConfirmedCases'].to_numpy(), rf_y_test_pred2['ConfirmedCases'].to_numpy(),rf_y_test['Fatalities'].to_numpy(), rf_y_test_pred2['Fatalities'].to_numpy())\nprint(\"RMSLE combined = {}\".format(res))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MTR  both  RF-100 estimators\n# mean of rmsle individual (func) = 0.368470396754244\n# mean of rmsle individual(manual) = 0.3684703967542432\n# RMSLE combined = 0.3749458290992757\n\n# MTR chained regression RF-100 estimators\n# mean of rmsle individual (func) = 0.347249992389988\n# mean of rmsle individual(manual) = 0.347249992389988\n# RMSLE combined = 0.36388949014719607\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Score final test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# [code 103]\ntest.Province_State.fillna(train.Country_Region, inplace=True)\n\ntest[['Province_State', 'Country_Region', 'ForecastId']] = test[['Province_State', 'Country_Region', 'ForecastId']].apply(lambda x: x.astype('category'))\ntest['Date']= pd.to_datetime(test['Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replaced [code 103]\n\n# test['Province_State'] = test['Province_State'].astype('str')\n# test['Country_Region'] = test['Country_Region'].astype('str')\n\n# # test.loc[test['Province_State'] == 'nan', ['Province_State']].shape\n# # test.loc[test['Province_State'] == 'nan', ['Country_Region']].shape\n# # test['Province_State'].isna().sum()\n\n# test.loc[test['Province_State'] == 'nan', ['Province_State']] = test.loc[test['Province_State'] == 'nan', ['Country_Region']]\n# test['Province_State'].fillna(test['Country_Region'], inplace=True) # category not present\n\n# # test['Province_State'] = test['Province_State'].astype('category')\n# test['Province_State'] = pd.Categorical(test['Province_State'])\n# test['Country_Region'] = pd.Categorical(test['Country_Region'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['days_since'] = test['Date'].apply(lambda x: (x - pd.to_datetime('2020-01-21')).days )\ntest[test['days_since']>1].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testc = test.copy()\n# y = testc.loc[:, ['ConfirmedCases']]\nX_score = testc.loc[:, ['Province_State', 'Country_Region', 'days_since']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_numeric_ftrs  = X_score.select_dtypes(include=['float64','int64'])\nscore_categorical_ftrs = X_score.select_dtypes(include=['category'])\n\nscore_scaled_data = pd.DataFrame(scaler.fit_transform(score_numeric_ftrs), columns = ['days_since'])\n\n# score_encoded_ftrs = pd.DataFrame(encoder.fit_transform(score_categorical_ftrs).toarray())\nscore_encoded_ftrs = pd.DataFrame(encoder.fit_transform(score_categorical_ftrs).toarray(), columns = encoder.get_feature_names(score_categorical_ftrs.columns.to_list()))\n\nX_score = pd.concat([score_scaled_data, score_encoded_ftrs], axis = 1)\nprint(\"ran\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the R forest's predict method on the test data\nrf_y_score = rf.predict(X_score)\nprint(rf_y_score[0:5])\nprint(rf_y_score.astype('int')[0:5])\nrf_y_score = rf_y_score.astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sub = pd.concat([test['ForecastId'], pd.DataFrame(rf_y_score, columns = ['ConfirmedCases']), ], axis = 1)\n# sub = pd.concat([test['ForecastId'], pd.DataFrame(rf_y_score, columns = ['Fatalities']), ], axis = 1)\nsub = pd.concat([test['ForecastId'], pd.DataFrame(rf_y_score, columns = ['ConfirmedCases', 'Fatalities']), ], axis = 1)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sub.to_csv('/kaggle/working/sub_conf.csv', index = False)\n# sub.to_csv('/kaggle/working/sub_fata.csv', index = False)\n# sub.to_csv('/kaggle/working/sub_both.csv', index = False)\n\nos.listdir('/kaggle/working/')\n\n# conf = pd.read_csv('/kaggle/working/sub_conf.csv')\n# fata = pd.read_csv('/kaggle/working/sub_fata.csv')\n# both = pd.read_csv('/kaggle/working/sub_both.csv')\n\n# both.to_csv('/kaggle/working/samplesubmission.csv', index = False)\n\n# all = pd.concat([conf, both, fata], axis = 1)\n\n# sub = pd.read_csv('/kaggle/working/sub_both.csv')\n# sub.to_csv('samplesubmission.csv', index = False)\n\n# print(\"End..\")\n# print(os.listdir('/kaggle/working/'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}