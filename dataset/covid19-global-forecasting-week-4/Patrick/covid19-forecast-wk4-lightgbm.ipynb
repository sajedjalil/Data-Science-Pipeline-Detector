{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Summary and observations\nThis is part of the [COVID-19 Global Forecasting Challenge](https://www.kaggle.com/c/covid19-global-forecasting-week-4) that predicts the number of COVID-19 confirmed cases and fatalities in the future.\n\nI used LightGBM, a tree based gradient boosting framework to learn and predict daily confirmed cases and daily fatalities.\n\nAmong the features I tried (temperature, population density, age, number of nurses and doctors, strain info, etc.), those features that include Dates (first day of school closure, first day of policy intervention, etc.) are the most informative (see [Display feature importance](https://www.kaggle.com/bitsnpieces/covid19-forecast-wk4-lightgbm#Display-feature-importance)).\n\nFrance, US (New York), Spain, Germany and China (Hubei) showed the greatest difference in predictions which can be attributed to the high number of COVID-19 confirmed cases (see [Validation results](https://www.kaggle.com/bitsnpieces/covid19-forecast-wk4-lightgbm#Validation-results)). It is interesting to note that there's a massive surge of cases and fatalities in France around April 2-April 4.\n\n# References\n* https://www.kaggle.com/covid19\n* https://coronavirus.jhu.edu/map.html\n* https://www.kaggle.com/osciiart/covid-19-lightgbm-with-weather-2/data#Model-training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/vinhnguyen/gpu-acceleration-for-lightgbm/notebook\n# !rm -r /opt/conda/lib/python3.6/site-packages/lightgbm\n# !git clone --recursive https://github.com/Microsoft/LightGBM\n# !apt-get install -y -qq libboost-all-dev\n\n# %%bash\n# cd LightGBM\n# rm -r build\n# mkdir build\n# cd build\n# cmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ ..\n# make -j$(nproc)\n\n# !cd LightGBM/python-package/;python3 setup.py install --precompile\n\n# !mkdir -p /etc/OpenCL/vendors && echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd\n# !rm -r LightGBM","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Params"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n\n\n# DEBUG = True\nDEBUG = False\n\n# shift params\n# window = 3\nMAX_LAG = 3\nshift_offset = 0\nhalflife = 6\n\n# train params\n# log_transform = True\nlog_transform = False\nSEED = 42\nimpute = True\nnum_round = 500\n# num_round = 1000\n# num_round = 2000\n# num_round = 15000\n# early_stopping_rounds = 300   # the smaller the less chances of overfit\n# early_stopping_rounds = 500\nearly_stopping_rounds = 1000\n# early_stopping_rounds = 2000\n\nparams = {\n          'num_leaves': 10,  # 8,  default 31\n          'min_data_in_leaf': 10, #5,  # 42,  default 20\n          'objective': 'regression',\n          'max_depth': 8, #8,  # default no limit -1\n          'max_bin': 50, # default 255\n          'learning_rate': 0.02,\n#            'device_type': 'gpu',\n          'boosting': 'gbdt',   #  traditional Gradient Boosting Decision Tree, \n#             'boosting': 'dart',   # , Dropouts meet Multiple Additive Regression Trees\n          'bagging_freq': 5,  # 5\n          'bagging_fraction': 0.8,  # 0.5,\n          'feature_fraction': 0.8201,\n          'bagging_seed': SEED,\n#           'reg_alpha': 1,  # 1.728910519108444,\n#           'reg_lambda': 4.9847051755586085,\n          'random_state': SEED,\n          'metric': 'mse',\n#             'metric': {'l2','l1'},\n          'verbosity': 100,\n#           'min_gain_to_split': 0.02,  # 0.01077313523861969,\n#           'min_child_weight': 5,  # 19.428902804238373,\n#           'num_threads': 4,\n#             'extra_trees': True,\n          }\n\ncol_cat = []\nf_list = ['DailyFatalities_%d' % d for d in range(1,MAX_LAG+1)]\nc_list = ['DailyConfirmedCases_%d' % d for d in range(1,MAX_LAG+1)]\ncol_var = f_list + c_list + [\n    'FirstFatalitiesDays','FirstConfirmedCasesDays',\n    'Days',\n#     'Month',\n    \n    'AirportRestrictionDays',\n    \n    'first_school_closure_days', 'MeasureImplementDays',\n    \n    'DailyFCRatio_1',\n    'ConfirmedLessFatalities_1',\n    \n#     'DailyFatalities_1', 'DailyFatalities_2', 'DailyFatalities_3', 'DailyFatalities_4', 'DailyFatalities_5', 'DailyFatalities_6',\n#     'DailyConfirmedCases_1', 'DailyConfirmedCases_2', 'DailyConfirmedCases_3', 'DailyConfirmedCases_4', 'DailyConfirmedCases_5', 'DailyConfirmedCases_6',\n    \n#     'DailyFatalitiesSlope_1_2','DailyFatalitiesSlope_2_3',           # difference eventually becomes really small and ends up becoming zero\n#     'DailyConfirmedCasesSlope_1_2','DailyConfirmedCasesSlope_2_3',\n    \n#     'latitude', 'longitude',\n#     'CountryCode', \n#     'RegionCode',\n    \n#     'age_over_65_years_percent',\n#     'Median_age', \n#     'sex_male_to_female_over_65',\n    \n#     'Population_2020',\n    \n#     'Density_KM2m',\n    \n    'Precip', 'Temp',\n    \n#     'Flu_pneumonia_death_rate_per_100000',\n    \n#     'A1a', 'A2', 'A2a', 'A3', 'A6', 'A7', 'B', 'B1', 'B2', 'B4',  # strain clades\n    \n#     'ICU-CCB_beds_per_100000', \n#     'NursesPer1000', 'DrPer1000',\n]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install --upgrade --force-reinstall lightgbm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# imports\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom scipy import stats\n\nimport matplotlib.pyplot as plt\nfrom scipy import interpolate\nimport json\nimport requests\nimport io\nfrom tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import lognorm\nfrom scipy.optimize import curve_fit\nimport string\nfrom scipy.integrate import quad\n\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.linear_model import SGDRegressor, LinearRegression, Lasso, Ridge, LogisticRegression\nfrom sklearn.base import clone\nfrom sklearn.pipeline import Pipeline, make_pipeline\n\n# https://images.plot.ly/plotly-documentation/images/python_cheat_sheet.pdf\n# https://www.apsnet.org/edcenter/disimpactmngmnt/topc/EpidemiologyTemporal/Pages/ModellingProgress.aspx\n\n# https://www.kaggle.com/c/ashrae-energy-prediction/discussion/114614\n# !pip install --upgrade numpy==1.17.3\nimport numpy as np\n\n# !pip install --upgrade --force-reinstall lightgbm -DUSE_GPU=1\n#https://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n# https://www.kaggle.com/osciiart/covid-19-lightgbm-with-weather-2/data#Model-training\nimport lightgbm as lgb  # LightGBM is a gradient boosting framework that uses tree based learning algorithms. \n\npd.set_option('display.max_columns', 100)\npd.options.display.float_format = '{:.4f}'.format\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"def country_slice(df, country='China', province=''):\n    if province is None or pd.isna(province):\n        return df[(df['Country_Region']==country) & (pd.isna(df['Province_State']) == True) ]\n    else:\n        return df[(df['Country_Region']==country) & (df['Province_State']==province)]\n\ndef preprocess_df(df, index_date=pd.to_datetime('2020-01-22')):\n    try:\n        df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n    except:\n        df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%y')\n    \n#     df['ConfirmedCases'] = df['ConfirmedCases'].astype({'ConfirmedCases': 'float32'})\n#     df['Fatalities'] = df['Fatalities'].astype({'Fatalities': 'float32'})\n        \n    df['Days'] = (df['Date'] - index_date).dt.days\n    df = df.sort_values(by=['Country_Region','Province_State','Date'], ascending=True)\n    df = df.rename(columns={'Country/Region':'Country_Region', 'Province/State':'Province_State'})\n    df['Province_State'] = df['Province_State'].apply(lambda x: '' if pd.isna(x) else x)\n    if 'ConfirmedCases' in df:\n        df['DailyConfirmedCases'] = df['ConfirmedCases'].diff()\n        df['DailyConfirmedCases'] = df['DailyConfirmedCases'].clip(0).fillna(0)\n    if 'Fatalities' in df:\n        df['DailyFatalities'] = df['Fatalities'].diff()\n        df['DailyFatalities'] = df['DailyFatalities'].clip(0).fillna(0)\n    if 'Recovered' in df:\n        df['DailyRecovered'] = df['Recovered'].diff()\n        df['DailyRecovered'] = df['DailyRecovered'].clip(0).fillna(0)\n    \n    df['Country_Province'] = df['Country_Region'] + '/' + df['Province_State']\n    \n    le = preprocessing.LabelEncoder()\n    le.fit(df['Country_Region'])\n    df['CountryCode'] = le.transform(df['Country_Region'])\n    \n    return df\n\ntrain = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\ntest = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')\nsubmission = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/submission.csv')\ntrain = preprocess_df(train)\ntest = preprocess_df(test)\n\n# print(list(zip(range(84),train['Date'].drop_duplicates().strftime('%Y-%m-%d').values)))\n\n# TODO\n# test = test.query('Date < \"2020-04-20\"')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sel_countries = [c for c in countries_lots_cases.tolist() if c != 'China/Hubei']\n# sel_countries = [\"Italy/\",\"Spain/\",\"US/New York\",\"Germany/\",\"China/Hubei\",\"France/\",\"Iran/\",\"United Kingdom/\",\"US/New Jersey\",\"Switzerland/\",\n#                  \"Turkey/\",\"Belgium/\",\"Netherlands/\",\"Austria/\",\"Korea, South/\",\"US/California\",\"US/Michigan\",\"Portugal/\",\n#                  \"US/Massachusetts\",\"US/Illinois\",\"US/Florida\",\"Brazil/\",\"US/Louisiana\",\"Israel/\",\"US/Pennsylvania\",\"US/Washington\",\n#                  \"Sweden/\",\"Norway/\",\"US/Georgia\",\"Canada/Quebec\"]   # countries with lots of cases, model separately\nsel_countries = [\"Italy/\",\"Spain/\",\"US/New York\",\"Germany/\",\"France/\",\"Iran/\",\"United Kingdom/\"]\nif DEBUG:\n    train = train[train['Country_Province'].isin(sel_countries)]\n    test = test[test['Country_Province'].isin(sel_countries)]\n    print(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# group countries by number of cases\nmax_cases = train.groupby(['Country_Province']).agg({'ConfirmedCases':'max'}).reset_index().sort_values(by='ConfirmedCases')\nmax_50, min_90 = max_cases['ConfirmedCases'].quantile([.5, .9])\n\n\n# big countries, model separtely\n\n\nall_countries   = train['Country_Province'].drop_duplicates().values\nbig_countries   = all_countries\n# big_countries   = max_cases.query(f'ConfirmedCases > {min_90}')['Country_Province'].drop_duplicates().values\n# small_countries = max_cases.query(f'ConfirmedCases < {min_90}')['Country_Province'].drop_duplicates().values\n# small_countries = max_cases.query(f'ConfirmedCases < {max_50}')['Country_Province'].drop_duplicates().values\n# med_countries   = set(train['Country_Province'].values).difference(set(big_countries).union(set(small_countries)))\nprint(f'\\n{len(big_countries)} big_countries {big_countries}')\n# print(f'\\n{len(med_countries)} med_countries {med_countries}')\n# print(f'\\n{len(small_countries)} small_countries {small_countries}')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Add Lat/Long and Recovered info to training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fetch data covid\n# note some Countries have no province data like Canada!\n# import requests\n# import io\n\n# def get_df_from_url(url):\n#     s = requests.get(url).content\n#     return pd.read_csv(io.StringIO(s.decode('utf-8')))\n\n# covid_url_prefix = 'https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/'\n# # df_covid_confirmed = get_df_from_url(covid_url_prefix + 'time_series_covid19_confirmed_global.csv')\n# # df_covid_deaths = get_df_from_url(covid_url_prefix + 'time_series_covid19_deaths_global.csv')\n# df_covid_recovered = get_df_from_url(covid_url_prefix + 'time_series_covid19_recovered_global.csv')\n\n# df_covid_recovered.info()\n\n# df_covid_recovered = df_covid_recovered.rename(columns={'Province/State':'Province_State', 'Country/Region':'Country_Region'})\n# df_covid_recovered\n# df_covid_recovered.columns\n# start_date_index = list(df_covid_recovered.columns).index('1/22/20')\n# value_vars = df_covid_recovered.columns[start_date_index:].values\n# df_recovered = pd.melt(df_covid_recovered, id_vars=\"Province_State\tCountry_Region\tLat\tLong\".split('\\t'), value_vars=value_vars)\n# df_recovered = df_recovered.rename(columns={'variable':'Date', 'value':'Recovered'})\n# df_recovered['Date'] = pd.to_datetime(df_recovered['Date'], format='%m/%d/%y')\n# df_recovered = preprocess_df(df_recovered)\n\n# # add Recovered info to training data\n# train = pd.merge(train, df_recovered, on=['Province_State','Country_Region','Date','Days'], how='left').reset_index()\n# del train['index']\n# test = pd.merge(test, df_recovered, on=['Province_State','Country_Region','Date','Days'], how='left').reset_index()\n# del test['index']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# first confirmed case / fatality\nout = []\nmin_dates = dict()\nfor metric in ['Fatalities','ConfirmedCases']:\n    tmp = train.query(metric + '>0').groupby(['Country_Region','Province_State']).agg({'Date':'min'}).reset_index()\n    tmp = tmp.rename(columns={'Date':'First'+metric+'Date'})\n#     print(tmp)\n    train = pd.merge(train, tmp, how='left')\n    test = pd.merge(test, tmp, how='left')\n    train['First'+metric+'Days'] = 0\n    test['First'+metric+'Days'] = 0\n    train['First'+metric+'Days'] = (train['Date'] - train['First'+metric+'Date']).dt.days\n    test['First'+metric+'Days'] = (test['Date'] - test['First'+metric+'Date']).dt.days\n\n# fatality / confirmed ratio\ntrain['DailyFCRatio'] = train['DailyFatalities'] / (train['DailyConfirmedCases'] + 0.00000000001)\ntest['DailyFCRatio'] = 0\n\n# month\ntrain['Month'] = train['Date'].dt.month\ntest['Month'] = test['Date'].dt.month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# time lag, t-1, t-2\n\nout = []\nfor key, tmp in list(train.groupby(['Country_Region','Province_State'])):\n    country, province = key[0], key[1]\n    \n#     tmp['AvgConfirmedCases'] = tmp['DailyConfirmedCases'].rolling(window=window).mean()\n#     tmp['AvgFatalities']     = tmp['DailyFatalities'].rolling(window=window).mean()\n    tmp['AvgConfirmedCases'] = tmp['DailyConfirmedCases'].ewm(halflife=halflife).mean()  # weighted rolling window with half-life decay\n    tmp['AvgFatalities']     = tmp['DailyFatalities'].ewm(halflife=halflife).mean()\n    tmp['ConfirmedLessFatalities'] = (tmp['DailyConfirmedCases'] - tmp['DailyFatalities'])/(tmp['DailyConfirmedCases'] + 0.000000000001)\n#     https://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html#exponentially-weighted-windows\n\n#     for j in range(1, MAX_LAG+1):\n#         tmp['DailyConfirmedCases_'+str(j)] = tmp['DailyConfirmedCases'].shift(j, fill_value=0)\n#         tmp['DailyFatalities_'+str(j)] = tmp['DailyFatalities'].shift(j, fill_value=0)\n    \n    for j in range(1, MAX_LAG+1):\n#         tmp['DailyConfirmedCases_'+str(j)] = tmp['AvgConfirmedCases'].shift(j, fill_value=0)\n#         tmp['DailyFatalities_'+str(j)] = tmp['AvgFatalities'].shift(j, fill_value=0)\n        tmp['DailyConfirmedCases_'+str(j)] = tmp['AvgConfirmedCases'].shift(j+shift_offset, fill_value=0)\n        tmp['DailyFatalities_'+str(j)] = tmp['AvgFatalities'].shift(j+shift_offset, fill_value=0)\n    \n    tmp['DailyFCRatio_1'] = tmp['DailyFCRatio'].shift(1, fill_value=0)\n    tmp['ConfirmedLessFatalities_1'] = tmp['ConfirmedLessFatalities'].shift(1, fill_value=0)\n  \n    out.append(tmp)\n    \ntrain = pd.concat(out)\ntmp = country_slice(train,'Italy')\ntmp[['Country_Region','Province_State', 'Date', 'Fatalities', 'DailyFatalities','DailyFatalities_1','DailyFatalities_2']].tail(10)\n\ntest['DailyFatalities_1'] = 0  # t-1 lag\ntest['DailyFatalities_2'] = 0  # t-2 lag\ntest['DailyFatalities_3'] = 0  # t-3 lag\ntest['DailyFatalities_4'] = 0  # t-4 lag\ntest['DailyFatalities_5'] = 0  # t-5 lag\ntest['DailyFatalities_6'] = 0  # t-6 lag\ntest['DailyConfirmedCases_1'] = 0  # t-1 lag\ntest['DailyConfirmedCases_2'] = 0  # t-2 lag\ntest['DailyConfirmedCases_3'] = 0  # t-3 lag\ntest['DailyConfirmedCases_4'] = 0  # t-4 lag\ntest['DailyConfirmedCases_5'] = 0  # t-5 lag\ntest['DailyConfirmedCases_6'] = 0  # t-6 lag\ntest['DailyFCRatio_1'] = 0\ntest['ConfirmedLessFatalities_1'] = 0\nmin_test_date = min(test['Date'])\nfor i, row in list(test.iterrows()):\n    country, province, test_date = row['Country_Region'], row['Province_State'], row['Date']\n    if test_date == min_test_date:\n        test.loc[i,'DailyFCRatio_1'] = train.query(f'Country_Region==\"{country}\" & Date==\"{test_date}\"')['DailyFCRatio_1'].values[0]\n        test.loc[i,'ConfirmedLessFatalities_1'] = train.query(f'Country_Region==\"{country}\" & Date==\"{test_date}\"')['ConfirmedLessFatalities_1'].values[0]\n        \n        for j in range(1, MAX_LAG+1):\n            test.loc[i,'DailyConfirmedCases_%d' % j] = train.query(f'Country_Region==\"{country}\" & Date==\"{test_date}\"')['DailyConfirmedCases_%d' % j].values[0]\n            test.loc[i,'DailyFatalities_%d' % j] = train.query(f'Country_Region==\"{country}\" & Date==\"{test_date}\"')['DailyFatalities_%d' % j].values[0]\n    elif test_date == min_test_date + pd.DateOffset(1):\n        for j in range(2, MAX_LAG+1):\n            test.loc[i,'DailyConfirmedCases_%d' % j] = train.query(f'Country_Region==\"{country}\" & Date==\"{test_date}\"')['DailyConfirmedCases_%d' % j].values[0]\n            test.loc[i,'DailyFatalities_%d' % j] = train.query(f'Country_Region==\"{country}\" & Date==\"{test_date}\"')['DailyFatalities_%d' % j].values[0]\n    elif test_date == min_test_date + pd.DateOffset(2):\n        for j in range(3, MAX_LAG+1):\n            test.loc[i,'DailyConfirmedCases_%d' % j] = train.query(f'Country_Region==\"{country}\" & Date==\"{test_date}\"')['DailyConfirmedCases_%d' % j].values[0]\n            test.loc[i,'DailyFatalities_%d' % j] = train.query(f'Country_Region==\"{country}\" & Date==\"{test_date}\"')['DailyFatalities_%d' % j].values[0]\n    elif test_date == min_test_date + pd.DateOffset(3):\n        for j in range(4, MAX_LAG+1):\n            test.loc[i,'DailyConfirmedCases_%d' % j] = train.query(f'Country_Region==\"{country}\" & Date==\"{test_date}\"')['DailyConfirmedCases_%d' % j].values[0]\n            test.loc[i,'DailyFatalities_%d' % j] = train.query(f'Country_Region==\"{country}\" & Date==\"{test_date}\"')['DailyFatalities_%d' % j].values[0]\n    elif test_date == min_test_date + pd.DateOffset(4):\n        for j in range(5, MAX_LAG+1):\n            test.loc[i,'DailyConfirmedCases_%d' % j] = train.query(f'Country_Region==\"{country}\" & Date==\"{test_date}\"')['DailyConfirmedCases_%d' % j].values[0]\n            test.loc[i,'DailyFatalities_%d' % j] = train.query(f'Country_Region==\"{country}\" & Date==\"{test_date}\"')['DailyFatalities_%d' % j].values[0]\n    elif test_date == min_test_date + pd.DateOffset(5):\n        for j in range(6, MAX_LAG+1):\n            test.loc[i,'DailyConfirmedCases_%d' % j] = train.query(f'Country_Region==\"{country}\" & Date==\"{test_date}\"')['DailyConfirmedCases_%d' % j].values[0]\n            test.loc[i,'DailyFatalities_%d' % j] = train.query(f'Country_Region==\"{country}\" & Date==\"{test_date}\"')['DailyFatalities_%d' % j].values[0]\n\nfor metric in ['ConfirmedCases','Fatalities']:\n    train[f'Daily{metric}Slope_1_2'] = (train[f'Daily{metric}_1']-train[f'Daily{metric}_2'])/2\n    train[f'Daily{metric}Slope_2_3'] = (train[f'Daily{metric}_2']-train[f'Daily{metric}_3'])/2\n    test[f'Daily{metric}Slope_1_2'] = (test[f'Daily{metric}_1']-test[f'Daily{metric}_2'])/2\n    test[f'Daily{metric}Slope_2_3'] = (test[f'Daily{metric}_2']-test[f'Daily{metric}_3'])/2\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Add country info"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_merged = pd.read_csv('/kaggle/input/covid19-country-data/covid19_merged.csv')\ndel df_merged['Unnamed: 0']\ndf_merged['country'] = df_merged['country'].apply(lambda x:x.replace('South Korea','Korea, South').replace('Taiwan','Taiwan*').replace('United States','US'))\nprint('in merged but not train', set(df_merged['country'].values).difference(set(train['Country_Region'].values)))\nprint('in train but not merged', set(train['Country_Region'].values).difference(set(df_merged['country'].values)))\ncountry_with_missing_values = list(set(train['Country_Region'].values).difference(set(df_merged['country'].values)))\ncountry_with_missing_values    # {'Kosovo', 'Botswana', 'MS Zaandam', 'West Bank and Gaza', 'Burundi', 'Sierra Leone', 'South Sudan', 'Sao Tome and Principe', 'Malawi', 'Western Sahara', 'Burma'}\ndf_merged = df_merged.rename(columns={'country':'Country_Region'})\ndf_merged['Density_KM2m'] = df_merged['Density_KM2m'].apply(lambda x:str(x).replace(',','')).astype('float32')\ndf_merged['Median_age'] = df_merged['Median_age'].replace('N.A.',None).apply(lambda x:str(x).replace(',','')).astype('float32')\ndf_merged['first_school_closure_date'] = pd.to_datetime(df_merged['first_school_closure_date'])\ndf_merged['Population_2020'] = np.log(df_merged['Population_2020']+1)\n\ntrain = pd.merge(train, df_merged, how='left', on='Country_Region').reset_index()\ndel train['index']\ntest = pd.merge(test, df_merged, how='left', on='Country_Region').reset_index()\ndel test['index']\n\ntrain['first_school_closure_days'] = (train['Date'] - train['first_school_closure_date']).dt.days.clip(0).fillna(0)\ntest['first_school_closure_days'] = (test['Date'] - test['first_school_closure_date']).dt.days.clip(0).fillna(0)\n\n\n# temp and precip\ntemp_cols = [ c for c in df_merged.columns if '_temp' in c]\nprecip_cols = [c for c in df_merged.columns if '_precip' in c]\ndf_temp = df_merged[['Country_Region']+temp_cols]\ndf_precip = df_merged[['Country_Region']+precip_cols]\ndf_temp\ndel df_temp['annual_temp']\ndf_precip\ndel df_precip['Annual_precip']\n\nmonths = 'jan\tfeb\tmar\tapr\tmay\tjun\tjuly\taug\tsept\toct\tnov\tdec'.replace('\\t',' ').split(' ')\nmonths_dict = dict(zip(months,range(1,len(months)+1)))\nmonths_dict\n\n# df_temp.columns = ['Country_Region'] + ['temp_' + c.replace('_temp','') for c in df_merged.columns if '_temp' in c and c != 'annual_temp']\ndf_temp = pd.melt(df_temp, id_vars='Country_Region').rename(columns={'variable':'Month','value':'Temp'})\ndf_temp['Month'] = [months_dict[m.replace('_temp','')] for m in df_temp['Month'] ]\ndf_temp\n\n# df_temp.columns = ['Country_Region'] + ['temp_' + c.replace('_temp','') for c in df_merged.columns if '_temp' in c and c != 'annual_temp']\ndf_precip = pd.melt(df_precip, id_vars='Country_Region').rename(columns={'variable':'Month','value':'Precip'})\ndf_precip['Month'] = [months_dict[m.lower().replace('_precip','')] for m in df_precip['Month'] ]\ndf_precip\n\n\ntrain = pd.merge(train, df_temp, how='left', on=['Country_Region','Month']).reset_index()\ndel train['index']\ntest = pd.merge(test, df_temp, how='left', on=['Country_Region','Month']).reset_index()\ndel test['index']\n\ntrain = pd.merge(train, df_precip, how='left', on=['Country_Region','Month']).reset_index()\ndel train['index']\ntest = pd.merge(test, df_precip, how='left', on=['Country_Region','Month']).reset_index()\ndel test['index']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Add airport restriction"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_air = pd.read_csv('/kaggle/input/uncover/UNCOVER/un_world_food_programme/world-travel-restrictions.csv')\ndf_air = df_air.rename(columns={'published':'AirportRestrictionDate','iso3':'code_3digit_x'})[['code_3digit_x','AirportRestrictionDate']].dropna()\ndf_air['AirportRestrictionDate'] = pd.to_datetime(df_air['AirportRestrictionDate'])\ntrain = pd.merge(train, df_air, how='left', on=['code_3digit_x']).reset_index()\ndel train['index']\ntrain['AirportRestrictionDays'] = (train['Date'] - train['AirportRestrictionDate']).dt.days.clip(0).fillna(0)\ntest = pd.merge(test, df_air, how='left', on=['code_3digit_x']).reset_index()\ndel test['index']\ntest['AirportRestrictionDays'] = (test['Date'] - test['AirportRestrictionDate']).dt.days.clip(0).fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Add strain info"},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_strain = pd.read_csv('/kaggle/input/covid19-country-data/covid19_data - covid19_strains.csv')\n# df_strain_clade = df_strain.groupby(['Country','Clade']).count().reset_index()\n# df_strain_clade['Count'] = 1\n# df_strain_clade = df_strain_clade[['Country','Clade','Count']]\n\n# tmp = df_strain_clade.pivot_table(index='Country',columns=['Clade'], aggfunc=np.sum).fillna(0).reset_index()\n# # print(tmp.columns.to_frame()['Clade'].values)\n# # print(tmp.columns)\n# # print(tmp.shape)\n# tmp.columns = tmp.columns.to_frame()['Clade'].values\n# tmp['Country_Region'] = tmp['']\n# del tmp['']\n# df_strain_clade = tmp\n# print(df_strain_clade.columns)\n\n\n# train = pd.merge(train, df_strain_clade, how='left', on=['Country_Region']).reset_index()\n# del train['index']\n# test = pd.merge(test, df_strain_clade, how='left', on=['Country_Region']).reset_index()\n# del test['index']\n\n# del df_strain_clade\n# del tmp\n# del df_strain\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Add Nurse and doctor info"},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_nurses = pd.read_csv('/kaggle/input/doctors-and-nurses-per-1000-people-by-country/Nurses_Per_Capital_By_Country.csv')\n# df_dr = pd.read_csv('/kaggle/input/doctors-and-nurses-per-1000-people-by-country/Doctors_Per_Capital_By_Country.csv')\n\n# df_nurses = df_nurses.query('TIME == \"2017\"')\n# df_nurses = df_nurses.rename(columns={'LOCATION':'code_3digit_x', 'Value':'NursesPer1000'})\n# df_nurses = df_nurses[['code_3digit_x','NursesPer1000']]\n# # df_nurses\n\n\n# df_dr = df_dr.query('TIME == \"2017\"')\n# df_dr = df_dr.rename(columns={'LOCATION':'code_3digit_x', 'Value':'DrPer1000'})\n# df_dr = df_dr[['code_3digit_x','DrPer1000']]\n# # df_dr\n\n\n# train = pd.merge(train, df_nurses, how='left', on=['code_3digit_x']).reset_index()\n# del train['index']\n# test = pd.merge(test, df_nurses, how='left', on=['code_3digit_x']).reset_index()\n# del test['index']\n\n# train = pd.merge(train, df_dr, how='left', on=['code_3digit_x']).reset_index()\n# del train['index']\n# test = pd.merge(test, df_dr, how='left', on=['code_3digit_x']).reset_index()\n# del test['index']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Add MeasureImplementDate dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test.shape, train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_acaps = pd.read_csv('/kaggle/input/demographic-factors-for-explaining-covid19/acaps_covid19_database/acaps_covid19_database.csv')\ndf_acaps['DATE_IMPLEMENTED'] = pd.to_datetime(df_acaps['DATE_IMPLEMENTED'].replace('3/28/2020','28/3/2020'), format='%d/%m/%Y' )\nprint(min(df_acaps['DATE_IMPLEMENTED']), max(df_acaps['DATE_IMPLEMENTED']))\ndf_acaps_agg = df_acaps.groupby(['ISO']).min().reset_index()\ndf_acaps_agg = df_acaps_agg.rename(columns={'DATE_IMPLEMENTED':'MeasureImplementDate', 'REGION':'Region', 'ISO':'code_3digit_x'})\nle = preprocessing.LabelEncoder()\nle.fit(df_acaps_agg['Region'])\ndf_acaps_agg['RegionCode'] = le.transform(df_acaps_agg['Region'])\ndel df_acaps_agg['COUNTRY']\ndf_acaps_agg = df_acaps_agg.drop_duplicates()\nprint(df_acaps_agg.shape)\n\ntrain = pd.merge(train, df_acaps_agg, how='left', on=['code_3digit_x']).reset_index()\ndel train['index']\ntest = pd.merge(test, df_acaps_agg, how='left', on=['code_3digit_x']).reset_index()\ndel test['index']\n\ntrain['MeasureImplementDays'] = (train['Date'] - train['MeasureImplementDate']).dt.days.clip(0).fillna(0)\ntest['MeasureImplementDays'] = (test['Date'] - test['MeasureImplementDate']).dt.days.clip(0).fillna(0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model training"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plt_country(df, country, province=''):\n    nrows = 2\n    ncols = 1\n    index = 1\n\n    plt.subplots_adjust(hspace=1.)\n\n    for index, metric in enumerate(['DailyConfirmedCases','DailyFatalities']):\n        tmp = country_slice(df, country, province)\n        if tmp.shape[0] == 0:\n            raise Exception(f'Not found {province} {country}')\n#         metric = 'DailyFatalities'\n        metric_pred = metric + 'Predicted'\n        y = np.array(tmp[metric])\n        x = np.array(range(len(y)))\n        y_pred = np.array(tmp[metric_pred])\n        dates = sorted(tmp['Date'].drop_duplicates().tolist())\n        min_test_date = test['Date'].min()\n        min_test_date_index = dates.index(min_test_date)\n        min_test_date_str = str(min_test_date).replace('00:00:00','')\n\n        ax = plt.subplot(nrows, ncols, index+1)\n        plt.title(f\"{province} {country}\")\n        plt.plot(x, y, label='Actual')\n        plt.plot(x, y_pred, label='Predicted')\n        plt.ylabel(metric)\n        plt.xlabel('Time')\n        plt.axvline(min_test_date_index, 0, 10000,label=f'Test {min_test_date_str}',linestyle='--')\n#         ax = plt.gca()\n        ax.legend()\n    \n# plt_country(train, 'China', 'Hubei')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from https://www.kaggle.com/osciiart/covid-19-lightgbm-with-weather-2/data#Model-training\n\ndef calc_score(y_true, y_pred):\n    y_true[y_true<0] = 0\n    print(stats.describe(y_true))\n    print(stats.describe(y_pred))\n    if log_transform:\n        score = metrics.mean_squared_error(np.log(y_true.clip(0, 1e10)+1), np.log(y_pred[:]+1))**0.5\n    else:\n        score = metrics.mean_squared_error(y_true.clip(0, 1e10), y_pred[:].clip(0, 1e10))**0.5\n    return score\n\n\ndef train_model(df, min_test_date, col_target, col_var, init_model=None):\n    \n    # filter training data upto the test date\n    df_valid = df[df['Date']>=min_test_date]\n    df_train = df[df['Date']<min_test_date]\n#     print(f\"min_test_date={min(test['Date'])}\")\n#     print(f\"df_valid min_date={min(df_valid['Date'])} max={max(df_valid['Date'])}\")\n#     print(f'train days={max(train[\"Days\"])}, min_date={min(train[\"Date\"])}, max_date={max(train[\"Date\"])}')\n#     print(f'test days={max(test[\"Days\"])}, min_date={min(test[\"Date\"])}, max_date={max(test[\"Date\"])}')\n\n    X_train = df_train[col_var]\n    X_valid = df_valid[col_var]\n\n    if impute:\n        from sklearn.impute import SimpleImputer\n        # Some Countries like Canada don't have Recovery broken down by province! so impute nan's\n        # np.argwhere(np.isnan(X_train))\n        my_imputer = SimpleImputer()\n        X_train = my_imputer.fit_transform(X_train)\n        X_valid = my_imputer.fit_transform(X_valid)\n\n\n    print(f'log transform? {log_transform}')\n#     col_target = 'DailyFatalities'\n    if log_transform:\n        y_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\n        y_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\n    else:\n        # y_train = df_train[col_target].values.clip(0, 1e10)\n        # y_valid = df_valid[col_target].values.clip(0, 1e10)\n        y_train = df_train[col_target].values\n        y_valid = df_valid[col_target].values\n    train_data = lgb.Dataset(X_train, label=y_train)   # categorical_feature=col_cat\n    valid_data = lgb.Dataset(X_valid, label=y_valid)\n    model = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                      verbose_eval = 100,\n                      init_model = init_model,\n                      early_stopping_rounds=early_stopping_rounds,\n    #                   early_stopping_rounds=150,\n    #                   early_stopping_rounds=500,\n                     )\n\n    best_itr = model.best_iteration\n    y_true = df_valid[col_target].values\n\n    if log_transform:\n        y_pred = np.exp(model.predict(X_valid))-1\n    else:\n        y_pred = model.predict(X_valid)\n\n    score = calc_score(y_true, y_pred)\n    print(f\"{col_target} score {round(score,4)}\")\n    \n    return model\n\n\n\n\n# exclude countries with lots of cases\nmin_test_date = min(test[\"Date\"])\n# df_max_cases = train.query(f'Date < \"{min_test_date}\"').groupby(['Country_Province']).agg({'ConfirmedCases':'max'}).reset_index().sort_values(by='ConfirmedCases')\n# df_max_cases.tail(30).plot.bar('Country_Province','ConfirmedCases')\n# min_cases = 3e4\n# countries_lots_cases = df_max_cases.query(f'ConfirmedCases > {min_cases}')['Country_Province'].values\n# countries_lots_cases\nall_countries = train['Country_Region'].drop_duplicates().values\nprint(len(all_countries))\n\nif big_countries is not None:\n    print(f'training {sel_countries}')\n#     df_test = test[test['Country_Province'].isin(sel_countries)]\n    df_train_large = train[train['Country_Province'].isin(big_countries)]\n#     df_train_med   = train[train['Country_Province'].isin(med_countries)]\n#     df_train_small = train[train['Country_Province'].isin(small_countries)]\n    model_large_fatalities = train_model(df_train_large, min_test_date, 'DailyFatalities', col_var)\n    model_large_cc         = train_model(df_train_large, min_test_date, 'DailyConfirmedCases', col_var)\n#     model_med_fatalities = train_model(df_train_med, min_test_date, 'DailyFatalities', col_var)\n#     model_med_cc         = train_model(df_train_med, min_test_date, 'DailyConfirmedCases', col_var)\n#     model_small_fatalities       = train_model(df_train_small, min_test_date, 'DailyFatalities', col_var)\n#     model_small_cc               = train_model(df_train_small, min_test_date, 'DailyConfirmedCases', col_var)\nelse:\n    df_train = train\n    model_fatalities       = train_model(df_train, min_test_date, 'DailyFatalities', col_var)\n    model_cc               = train_model(df_train, min_test_date, 'DailyConfirmedCases', col_var)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(df, model):\n    \n    if type(df)==pd.core.series.Series:\n        # if we predict test\n        x = np.array(df[col_var]).reshape(1, -1)\n    else:\n        x = df[col_var]\n    \n    y_pred = model.predict(x)\n    \n    if log_transform:\n        y_pred = np.exp(y_pred)-1\n\n    \n    if y_pred.shape[0] == 1:\n        # if we predict test\n        y_pred = y_pred[0]\n        \n    return y_pred\n\n\n\n# calculate predictions\ndf_train_large.loc[:,'DailyFatalitiesPredicted']             = predict(df_train_large, model_large_fatalities)\ndf_train_large.loc[:,'DailyConfirmedCasesPredicted']         = predict(df_train_large, model_large_cc)\n# df_train_med.loc[:,'DailyFatalitiesPredicted']             = predict(df_train_med, model_med_fatalities)\n# df_train_med.loc[:,'DailyConfirmedCasesPredicted']         = predict(df_train_med, model_med_cc)\n# df_train_small.loc[:,'DailyFatalitiesPredicted']             = predict(df_train_small, model_small_fatalities)\n# df_train_small.loc[:,'DailyConfirmedCasesPredicted']         = predict(df_train_small, model_small_cc)\n\ncols = ['Country_Province', 'Date', 'DailyFatalitiesPredicted','DailyConfirmedCasesPredicted']\nif 'DailyFatalitiesPredicted' in train:\n    print('DailyFatalitiesPredicted in train')\n    del train['DailyFatalitiesPredicted']\nif 'DailyConfirmedCasesPredicted' in train:\n    del train['DailyConfirmedCasesPredicted']\n# train = pd.merge(train, pd.concat([df_train_small[cols], df_train_med[cols], df_train_large[cols]]), how='left', on=['Country_Province','Date'])\ntrain = pd.merge(train, pd.concat([df_train_large[cols]]), how='left', on=['Country_Province','Date'])\nprint(f'train {tmp.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot\nfor i, c in list(enumerate(big_countries))[:10]:\n    country, province = c.split('/')\n    plt.figure(i)\n    plt_country(df_train_large, country, province)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot\n# for i, c in enumerate(['Taiwan*/', 'Japan/', 'Philippines/', 'Canada/British Columbia']):\n# for i, c in list(enumerate(small_countries))[:10]:\n#     country, province = c.split('/')\n#     plt.figure(i)\n#     plt_country(df_train_small, country, province)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Display feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = pd.DataFrame()\ntmp[\"feature\"] = col_var\ntmp[\"importance_ConfirmedCases\"]   = model_large_cc.feature_importance()\ntmp[\"importance_Fatalities\"]       = model_large_fatalities.feature_importance()\ntmp = tmp.sort_values('importance_ConfirmedCases', ascending=False)\ntmp.to_csv('var_importance_large.csv',index=False)\ntmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# train model to predict fatalities/day\n# train_bak = train\n\n# # exclude countries with lots of cases\n# min_test_date = min(test[\"Date\"])\n# df_max_cases = train.query(f'Date < \"{min_test_date}\"').groupby(['Country_Province']).agg({'ConfirmedCases':'max'}).reset_index().sort_values(by='ConfirmedCases')\n# df_max_cases.tail(30).plot.bar('Country_Province','ConfirmedCases')\n# min_cases = 3e4\n# countries_lots_cases = df_max_cases.query(f'ConfirmedCases > {min_cases}')['Country_Province'].values\n# countries_lots_cases\n# all_countries = train['Country_Region'].drop_duplicates().values\n# print(len(all_countries))\n# sel_countries = countries_lots_cases\n# if sel_countries:\n#     print(len(sel_countries))\n#     test = test[test['Country_Region'].isin(sel_countries)]\n#     train = train[train['Country_Region'].isin(sel_countries)]\n\n# col_target = 'DailyFatalities'\n# model_fatalities = train_model(train, min_test_date, 'DailyFatalities', col_var)\n# model_cc = train_model(train, min_test_date, 'DailyConfirmedCases', col_var)\n\n# # train.to_csv('train_merged.csv', index=False)\n\n# print(f\"Max train date {max(df_train['Date'])}\")\n\n# X_train = df_train[col_var]\n# X_valid = df_valid[col_var]\n\n\n# if impute:\n#     from sklearn.impute import SimpleImputer\n#     # Some Countries like Canada don't have Recovery broken down by province! so impute nan's\n#     # np.argwhere(np.isnan(X_train))\n#     my_imputer = SimpleImputer()\n#     X_train = my_imputer.fit_transform(X_train)\n#     X_valid = my_imputer.fit_transform(X_valid)\n\n\n# col_target = 'DailyFatalities'\n# if log_transform:\n#     y_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\n#     y_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\n# else:\n#     # y_train = df_train[col_target].values.clip(0, 1e10)\n#     # y_valid = df_valid[col_target].values.clip(0, 1e10)\n#     y_train = df_train[col_target].values\n#     y_valid = df_valid[col_target].values\n# train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\n# valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\n# model = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n#                   verbose_eval=100,\n#                   early_stopping_rounds=early_stopping_rounds,\n# #                   early_stopping_rounds=150,\n# #                   early_stopping_rounds=500,\n#                  )\n\n# best_itr = model.best_iteration\n# y_true = df_valid[col_target].values\n\n# if log_transform:\n#     y_pred = np.exp(model.predict(X_valid))-1\n# else:\n#     y_pred = model.predict(X_valid)\n    \n# score = calc_score(y_true, y_pred)\n# print(f\"{col_target} score {round(score,4)}\")\n# model_fatalities = model\n\n\n# col_target = 'DailyConfirmedCases'\n# if log_transform:\n#     y_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\n#     y_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\n# else:\n#     # y_train = df_train[col_target].values.clip(0, 1e10)\n#     # y_valid = df_valid[col_target].values.clip(0, 1e10)\n#     y_train = df_train[col_target].values\n#     y_valid = df_valid[col_target].values\n# train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\n# valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\n# model = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n#                   verbose_eval=100,\n#                   early_stopping_rounds=early_stopping_rounds,\n# #                   early_stopping_rounds=150,\n# #                   early_stopping_rounds=500,\n#                  )\n\n# best_itr = model.best_iteration\n# y_true = df_valid[col_target].values\n\n# if log_transform:\n#     y_pred = np.exp(model.predict(X_valid))-1\n# else:\n#     y_pred = model.predict(X_valid)\n    \n# score = calc_score(y_true, y_pred)\n# print(f\"{col_target} score {round(score,4)}\")\n# model_cc = model\n\n\n# col_target = 'DailyRecovered'  \n# y_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\n# y_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\n# train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\n# valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\n# model = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n#                   verbose_eval=100,\n#                   early_stopping_rounds=150,)\n\n# best_itr = model.best_iteration\n# y_true = df_valid[col_target].values\n# y_pred = np.exp(model.predict(X_valid))-1\n# score = calc_score(y_true, y_pred)\n# print(f\"{col_target} score {round(score,4)}\")\n# model_recovered = model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build features for test prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"def assign_predictions_as_features(df, idx, country_province, pred, metrics):\n    \"\"\"\n    assign predictions as features for future predictions\n    \"\"\"\n    for j,m in enumerate(metrics, 1):\n        try:\n            if(df.loc[idx+j,'Country_Province']==country_province):\n#                 print(f'assign {m} {j} {pred}')\n                df.loc[idx+j, m] = pred\n        except:\n            pass\n    \n    \ny_fatalities, y_cases = [], []\ncols = ['Date','DailyFatalities','DailyConfirmedCases'] + col_var\nX_test = test[col_var]\nX_train = train.query(f'Date < \"{min_test_date}\"')[cols]\n\ntrain_freq = np.ceil(test.shape[0] / len(all_countries))\nprint(f'train_freq={train_freq}')\n\nfor i in tqdm(list(range(test.shape[0]))):\n# for i in list(range(test.shape[0]))[:3]:\n    country = test.loc[i, 'Country_Region']\n    province = test.loc[i, 'Province_State']\n    country_province = test.loc[i, 'Country_Province']\n    \n#     x = np.array(test.loc[i, col_var]).reshape(1, -1)\n    \n    # pick the model\n    m_f = model_large_fatalities\n    m_c = model_large_cc\n#     if country_province in big_countries:\n#         m_f = model_large_fatalities\n#         m_c = model_large_cc\n#     elif country_province in med_countries:\n#         m_f = model_med_fatalities\n#         m_c = model_med_cc\n#     else:\n#         m_f = model_small_fatalities\n#         m_c = model_small_cc\n    \n    # predict\n    y_f = predict(test.loc[i], m_f)\n    y_c = predict(test.loc[i], m_c)\n    if y_c < 0:\n        y_c = 0\n    if y_f < 0:\n        y_f = 0\n    if y_f > y_c:\n        y_f = y_c\n    y_fatalities.append(y_f)\n    y_cases.append(y_c)\n    y_fc_ratio = y_f / (y_c + 0.00000000001)\n    y_c_less_f = (y_c - y_f) / (y_c + 0.00000000001)\n    \n    # assign predictions as features for future predictions\n#     print(f'====== test_{i} before {test.loc[i][col_var]} ======')\n    assign_predictions_as_features(test, i, country_province, y_f, [f'DailyFatalities_{i}' for i in range(1,MAX_LAG+1)] )\n#     print(f'====== test_{i} after {test.loc[i][col_var]} ======')\n    assign_predictions_as_features(test, i, country_province, y_c, [f'DailyConfirmedCases_{i}' for i in range(1,MAX_LAG+1)] )\n    assign_predictions_as_features(test, i, country_province, y_fc_ratio, ['DailyFCRatio_1'] )\n    assign_predictions_as_features(test, i, country_province, y_c_less_f, ['ConfirmedLessFatalities_1'] )\n    \n    test.loc[i,'DailyFatalities']   = y_f\n    test.loc[i,'DailyConfirmedCases']   = y_c\n    \n        \n    # update our models occasionally to speedup\n    try:\n#         if i % 14 == 0:\n        if i % 100 == 0:\n#         if test.loc[i,'Date'] in [pd.to_datetime(d) for d in ['2020-04-12']]:\n            X_valid_date = test.loc[i, 'Date'] - pd.Timedelta('2 days')\n            \n            test_slice = test.loc[i, cols]\n#             X_train = pd.concat([X_train, test_slice])\n            X_train = X_train.append(test_slice)\n            \n            print('X_train_shape',X_train.shape)\n            model_large_fatalities = train_model(X_train, X_valid_date, 'DailyFatalities', col_var, model_large_fatalities)\n            model_large_cc         = train_model(X_train, X_valid_date, 'DailyConfirmedCases', col_var, model_large_cc)\n#             if country_province in big_countries:\n#                 model_large_fatalities = train_model(X_train, X_valid_date, 'DailyFatalities', col_var, model_large_fatalities)\n#                 model_large_cc = train_model(X_train, X_valid_date, 'DailyConfirmedCases', col_var, model_large_cc)\n#             elif country_province in med_countries:\n#                 model_med_fatalities = train_model(X_train, X_valid_date, 'DailyFatalities', col_var, model_med_fatalities)\n#                 model_med_cc = train_model(X_train, X_valid_date, 'DailyConfirmedCases', col_var, model_med_cc)\n#             else:\n#                 model_small_fatalities = train_model(X_train, X_valid_date, 'DailyFatalities', col_var, model_small_fatalities)\n#                 model_small_cc = train_model(X_train, X_valid_date, 'DailyConfirmedCases', col_var, model_small_cc)\n    except Exception as e:\n        print(f'Error {e}')\n        pass\n        \n        \ntest['DailyFatalitiesPredicted']     = y_fatalities\ntest['DailyConfirmedCasesPredicted'] = y_cases","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot predictions in test\ncols = ['Country_Region','Province_State', 'Date', 'DailyConfirmedCases','DailyFatalities', 'DailyConfirmedCasesPredicted','DailyFatalitiesPredicted'] \ntest['DailyFatalities'] = None\ntest['DailyConfirmedCases'] = None\ntrain_test = pd.concat( [ train[cols], test[test['Date'] > max(train['Date'])][cols] ] ).sort_values(by=['Country_Region','Province_State', 'Date'])\n\n# plt_dict = {'China':'Hubei', 'France':'', 'Japan':'', 'Italy':'', 'Spain':'', 'Germany':'', 'US':'New York', 'Taiwan*':'', 'Korea, South':'', 'United Kingdom':'', 'Canada':'British Columbia'}\n# for country, province in plt_dict.items():\nfor i, cp in enumerate(['Taiwan*/', 'Japan/', 'Philippines/', 'Canada/British Columbia'], 1):\n    country, province = cp.split('/')\n    if province is None:\n            province = ''\n    plt.figure(i)\n    try:\n        plt_country(train_test, country, province)\n    except Exception as e:\n        print(f'Exception in plt_country {e}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, cp in enumerate(sel_countries, 1):\n    country, province = cp.split('/')\n    if province is None:\n            province = ''\n    plt.figure(i)\n    plt_country(train_test, country, province)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# country_slice(train,'Canada','British Columbia').query('Date < \"2020-04-02\"').tail(5)[['Date', 'DailyFatalities','DailyFatalitiesPredicted','DailyConfirmedCases','DailyConfirmedCasesPredicted']+ col_var]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# country_slice(test,'Canada','British Columbia').head(5)[['Date', 'DailyFatalities','DailyFatalitiesPredicted','DailyConfirmedCases','DailyConfirmedCasesPredicted']+ col_var]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation results"},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train = train[col_var]\n\n# if log_transform:\n#     y_fatalities = np.exp(model_fatalities.predict(X_train))-1\n#     y_cases = np.exp(model_cc.predict(X_train))-1\n# else:\n#     y_fatalities = model_fatalities.predict(X_train)\n#     y_cases = model_cc.predict(X_train)\n    \n# train['DailyFatalitiesPredicted'] = y_fatalities\n# train['DailyConfirmedCasesPredicted'] = y_cases\ntrain['diff_fatalities'] = abs(train['DailyFatalitiesPredicted']-train['DailyFatalities'])\ntrain['diff_cases'] = abs(train['DailyConfirmedCasesPredicted']-train['DailyConfirmedCases'])\n\ntmp = train[['Province_State','Country_Region', 'Date', 'DailyFatalities', 'DailyFatalitiesPredicted', 'DailyConfirmedCases', 'DailyConfirmedCasesPredicted']]\n# country_slice(tmp,'Spain')\ncountry_slice(tmp,'Italy').tail(5)\n\ntrain_diff = train.groupby(['Country_Region','Province_State']).agg({'diff_fatalities':'sum','diff_cases':'sum'}).reset_index()\ntrain_diff.sort_values(by='diff_fatalities',ascending=False).head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_diff.sort_values(by='diff_cases',ascending=False).head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculate cumulative sum"},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate cumulative sum\n# get the last training fatalities count and add the y_pred and calculate cumsum\nout = []\n# for i, rows in list(train[train['Date']==max(train['Date'])].iterrows())[:2]:\nfor i, rows in list(train[train['Date']==max(train['Date'])].iterrows()):\n    country, province, fatalities, confirmed_cases = rows['Country_Region'], rows['Province_State'], rows['Fatalities'], rows['ConfirmedCases']\n    tmp = country_slice(test, country, province).sort_values(by='Date')\n#     print(country, province, fatalities, confirmed_cases)\n    tmp['Fatalities'] = np.cumsum([fatalities] + tmp['DailyFatalitiesPredicted'].tolist())[1:]\n    tmp['ConfirmedCases'] = np.cumsum([confirmed_cases] + tmp['DailyConfirmedCasesPredicted'].tolist())[1:]\n    out.append(tmp)\n\nresults = pd.concat(out).sort_values(by='ForecastId')\n\ntmp = results[['ForecastId', 'Province_State','Country_Region', 'Date', 'Fatalities', 'DailyFatalitiesPredicted', 'ConfirmedCases', 'DailyConfirmedCasesPredicted'] + col_var]\ntmp.head()\n# train[['Province_State','Country_Region', 'Date', 'Fatalities']]\n\ntmp.to_csv('results.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results[submission.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"results[submission.columns].to_csv('submission.csv', index=False)\ncols = ['Country_Region','Province_State','Date','DailyFatalitiesPredicted','DailyConfirmedCases'] + col_var\ntest[cols].to_csv('test_out.csv', index=False)\ntrain[cols].to_csv('train_out.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Done!')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}