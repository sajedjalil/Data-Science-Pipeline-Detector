{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"\n{\n   \"schemaVersion\": 2,\n   \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n   \"config\": {\n      \"mediaType\": \"application/vnd.docker.container.image.v1+json\",\n      \"size\": 27512,\n      \"digest\": \"sha256:8ef19b5397d8b13638e69746589be8265f4e9f565ee5af4d64f43f5d14a68a64\"\n   },\n   \"layers\": [\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 45339314,\n         \"digest\": \"sha256:c5e155d5a1d130a7f8a3e24cee0d9e1349bff13f90ec6a941478e558fde53c14\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 95104141,\n         \"digest\": \"sha256:86534c0d13b7196a49d52a65548f524b744d48ccaf89454659637bee4811d312\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 1571501372,\n         \"digest\": \"sha256:5764e90b1fae3f6050c1b56958da5e94c0d0c2a5211955f579958fcbe6a679fd\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 1083072,\n         \"digest\": \"sha256:ba67f7304613606a1d577e2fc5b1e6bb14b764bcc8d07021779173bcc6a8d4b6\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 526,\n         \"digest\": \"sha256:36c8cee5dcabe015f8e5b00d9e5f26f3dc43c685616a9ff657aeac32dcb0dec7\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 456,\n         \"digest\": \"sha256:fbde6884bcec90a734814ab616cc8abcf34cde78a99498df8da757431c6c28fd\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 13117845,\n         \"digest\": \"sha256:4aceba2705e51efc04a48b7883d097f3c89d00a2f96b2fb16b54a7d5fc410e53\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 558997840,\n         \"digest\": \"sha256:690778d6efe115dbba1239a78693944fe179985f5a5d31078d376731eb900635\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 718986208,\n         \"digest\": \"sha256:cfc8fe521bf9c7e028edea60d6f3cbd2a50f56751c0e8d7415d6d364453b41d0\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 33502401,\n         \"digest\": \"sha256:5a2d591ac4f68ab561f030733f354b722051f02fb7114a632a980d4095e9f6a5\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 95925401,\n         \"digest\": \"sha256:b720a0e96c3024ee325ea8e1874a33d66d097c990ac50e8229b1c76076ae869a\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 142490052,\n         \"digest\": \"sha256:a661d11e464bd9abfabe3ec4b4b4e22b01c228481ba20d5dd6c066ab512e26c2\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 1038014421,\n         \"digest\": \"sha256:555cc8cba1c97f86ef332cee16e11b952f18352f55b915df4a9a776d81edd234\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 75007872,\n         \"digest\": \"sha256:7ed8a9307f830dad6f7b8b273c80b0a820bdf9f9db7ad1c762282ef8b63e4122\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 80474425,\n         \"digest\": \"sha256:29eb9237adacfa8ff7974c2ce5e9f1ffc5047e625347bbb03b5a170d397153fb\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 89451858,\n         \"digest\": \"sha256:6b054a59f9fd46636ecb9f0c31a837127ae856fd44e5b998286f5f1111bf1d30\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 415384338,\n         \"digest\": \"sha256:19c088e1afee706e063eaff6a2d259efb55b962f4da47927f9461a83d904c8a1\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 44168262,\n         \"digest\": \"sha256:838b7e776f75e4fdce36596b5ee8e250ebb50cdc2717033290df2bff0e70a7dc\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 107758705,\n         \"digest\": \"sha256:1ad83d09763421093849a7abef397f8610f79e07767f51e3248ab9ef52679705\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 958342282,\n         \"digest\": \"sha256:569c6fda9d84413ea844c2f25799b7449b2fd6ac486bcbb8be2eb1ca65b6c51e\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 588654554,\n         \"digest\": \"sha256:d8ad2accaf088624da0281224e625ff49da8212cf9e21423898f50f648542d40\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 21654895,\n         \"digest\": \"sha256:1754ab792f2a4062623d4f461f9196ed41ec1bf9eb81b45ea05fe0fe6a4df3c0\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 227370423,\n         \"digest\": \"sha256:153e0f49aeb357a372ddccfffe487d5b431fc81143728d1a65805a38454c477a\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 20307,\n         \"digest\": \"sha256:6f8920d2e4f3aa5274096bbbf084a9587d1ff438026fac3ee7931e3f75008de3\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 85317368,\n         \"digest\": \"sha256:5857d6464a2ed88c711915621eb005d178603b9daabbf65ccfe2fd2e72d7be36\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 3264,\n         \"digest\": \"sha256:5347c992f3b56e47242dd8a5694638c839cad94e9635876f2bfe9e8dd36dd62c\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 2163,\n         \"digest\": \"sha256:dd6f840a7b975737ae3f11a10036c7501bd6796ca86befd2596712365a9fd073\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 1269,\n         \"digest\": \"sha256:a12c0432261d580586748b11db6bbfe798f5957a9ad57a71230c0f9986826114\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 641,\n         \"digest\": \"sha256:112b56a741fa6492ba1a4f9eda937bcb52f02f7c31265e142a592824bf830c36\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 2052,\n         \"digest\": \"sha256:bcd81def64e80646bbebb0cd99ecfe423c0ec3df21c607fceb2f9c3a2b782e1e\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 876,\n         \"digest\": \"sha256:daf7bad905212cda27468f9f136e888189f0cde90182e6eb488937740a70ac38\"\n      },\n      {\n         \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 210,\n         \"digest\": \"sha256:37f94f1dfe09302f5ab426ed04a71a4bad5cc9585d65611518efb8ebc1ea5ba5\"\n      }\n   ]\n}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ARG BASE_TAG=latest\nARG TENSORFLOW_VERSION=2.1.0\n \nFROM gcr.io/kaggle-images/python-tensorflow-whl:${TENSORFLOW_VERSION}-py37-2 as tensorflow_whl\nFROM gcr.io/deeplearning-platform-release/base-cpu:${BASE_TAG}\n \nADD clean-layer.sh  /tmp/clean-layer.sh\nADD patches/nbconvert-extensions.tpl /opt/kaggle/nbconvert-extensions.tpl\n \n# This is necessary for apt to access HTTPS sources\nRUN apt-get update && \\\n    apt-get install apt-transport-https && \\\n    /tmp/clean-layer.sh\n \n    # Use a fixed apt-get repo to stop intermittent failures due to flaky httpredir connections,\n    # as described by Lionel Chan at http://stackoverflow.com/a/37426929/5881346\nRUN sed -i \"s/httpredir.debian.org/debian.uchicago.edu/\" /etc/apt/sources.list && \\\n    apt-get update && \\\n    # Needed by vowpalwabbit & lightGBM (GPU build).\n    # https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Python#installing\n    # https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html#build-lightgbm\n    apt-get install -y build-essential unzip cmake && \\\n    apt-get install -y libboost-dev libboost-program-options-dev libboost-system-dev libboost-thread-dev libboost-math-dev libboost-test-dev libboost-python-dev libboost-filesystem-dev zlib1g-dev && \\\n    pip install --upgrade pip && \\\n    # enum34 is a backport of the Python 3.4 enum class to Python < 3.4.\n    # No need since we are using Python 3.7. This is causing errors for packages\n    # expecting the 3.7 version of enum. e.g. AttributeError: module 'enum' has no attribute 'IntFlag'\n    pip uninstall -y enum34 && \\\n    /tmp/clean-layer.sh\n \n# Make sure the dynamic linker finds the right libstdc++\nENV LD_LIBRARY_PATH=/opt/conda/lib\n# b/128333086: Set PROJ_LIB to points to the proj4 cartographic library.\nENV PROJ_LIB=/opt/conda/share/proj\n \n# Install conda packages not available on pip.\n# When using pip in a conda environment, conda commands should be ran first and then\n# the remaining pip commands: https://www.anaconda.com/using-pip-in-a-conda-environment/\nRUN conda install -c conda-forge matplotlib basemap cartopy python-igraph imagemagick pysal && \\\n    # b/142337634#comment22 pin required to avoid torchaudio downgrade.\n    conda install -c pytorch pytorch torchvision \"torchaudio>=0.4.0\" cpuonly && \\\n    /tmp/clean-layer.sh\n \n# The anaconda base image includes outdated versions of these packages. Update them to include the latest version.\n# b/150498764 distributed 2.11.0 fails at import while trying to reach out to 8.8.8.8 since the network is disabled in our hermetic tests.\nRUN pip install distributed==2.10.0 && \\\n    pip install seaborn python-dateutil dask && \\\n    pip install pyyaml joblib pytagcloud husl geopy ml_metrics mne pyshp && \\\n    pip install pandas && \\\n    # Install h2o from source.\n    # Use `conda install -c h2oai h2o` once Python 3.7 version is released to conda.\n    apt-get install -y default-jre-headless && \\\n    pip install -f https://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o && \\\n    /tmp/clean-layer.sh\n \n# Install tensorflow from a pre-built wheel\nCOPY --from=tensorflow_whl /tmp/tensorflow_cpu/*.whl /tmp/tensorflow_cpu/\nRUN pip install /tmp/tensorflow_cpu/tensorflow*.whl && \\\n    rm -rf /tmp/tensorflow_cpu && \\\n    /tmp/clean-layer.sh\n \nRUN apt-get install -y libfreetype6-dev && \\\n    apt-get install -y libglib2.0-0 libxext6 libsm6 libxrender1 libfontconfig1 --fix-missing && \\\n    pip install gensim && \\\n    pip install textblob && \\\n    pip install wordcloud && \\\n    pip install xgboost && \\\n    # Pinned to match GPU version. Update version together.\n    pip install lightgbm==2.3.1 && \\\n    pip install git+git://github.com/Lasagne/Lasagne.git && \\\n    pip install keras && \\\n    pip install flake8 && \\\n    #neon\n    cd /usr/local/src && \\\n    git clone --depth 1 https://github.com/NervanaSystems/neon.git && \\\n    cd neon && pip install . && \\\n    #nolearn\n    pip install nolearn && \\\n    pip install Theano && \\\n    pip install pybrain && \\\n    pip install python-Levenshtein && \\\n    pip install hep_ml && \\\n    # chainer\n    pip install chainer && \\\n    # NLTK Project datasets\n    mkdir -p /usr/share/nltk_data && \\\n    # NLTK Downloader no longer continues smoothly after an error, so we explicitly list\n    # the corpuses that work\n    # \"yes | ...\" answers yes to the retry prompt in case of an error. See b/133762095.\n    yes | python -m nltk.downloader -d /usr/share/nltk_data abc alpino averaged_perceptron_tagger \\\n    basque_grammars biocreative_ppi bllip_wsj_no_aux \\\n    book_grammars brown brown_tei cess_cat cess_esp chat80 city_database cmudict \\\n    comtrans conll2000 conll2002 conll2007 crubadan dependency_treebank \\\n    europarl_raw floresta gazetteers genesis gutenberg \\\n    ieer inaugural indian jeita kimmo knbc large_grammars lin_thesaurus mac_morpho machado \\\n    masc_tagged maxent_ne_chunker maxent_treebank_pos_tagger moses_sample movie_reviews \\\n    mte_teip5 names nps_chat omw opinion_lexicon paradigms \\\n    pil pl196x porter_test ppattach problem_reports product_reviews_1 product_reviews_2 propbank \\\n    pros_cons ptb punkt qc reuters rslp rte sample_grammars semcor senseval sentence_polarity \\\n    sentiwordnet shakespeare sinica_treebank smultron snowball_data spanish_grammars \\\n    state_union stopwords subjectivity swadesh switchboard tagsets timit toolbox treebank \\\n    twitter_samples udhr2 udhr unicode_samples universal_tagset universal_treebanks_v20 \\\n    vader_lexicon verbnet webtext word2vec_sample wordnet wordnet_ic words ycoe && \\\n    # Stop-words\n    pip install stop-words && \\\n    pip install scikit-image && \\\n    /tmp/clean-layer.sh\n \nRUN pip install ibis-framework && \\\n    pip install mxnet && \\\n    pip install gluonnlp && \\\n    pip install gluoncv && \\    \n    /tmp/clean-layer.sh\n \n# scikit-learn dependencies\nRUN pip install scipy && \\\n    pip install scikit-learn && \\\n    # HDF5 support\n    pip install h5py && \\\n    pip install biopython && \\\n    # PUDB, for local debugging convenience\n    pip install pudb && \\\n    pip install imbalanced-learn && \\\n    # Convex Optimization library\n    # Latest version fails to install, see https://github.com/cvxopt/cvxopt/issues/77\n    #    and https://github.com/cvxopt/cvxopt/issues/80\n    # pip install cvxopt && \\\n    # Profiling and other utilities\n    pip install line_profiler && \\\n    pip install orderedmultidict && \\\n    pip install smhasher && \\\n    pip install bokeh && \\\n    pip install numba && \\\n    pip install datashader && \\\n    # Boruta (python implementation)\n    pip install Boruta && \\\n    apt-get install -y graphviz && pip install graphviz && \\\n    # Pandoc is a dependency of deap\n    apt-get install -y pandoc && \\\n    pip install git+git://github.com/scikit-learn-contrib/py-earth.git@issue191 && \\\n    pip install essentia && \\\n    /tmp/clean-layer.sh\n \n# vtk with dependencies\nRUN apt-get install -y libgl1-mesa-glx && \\\n    pip install vtk && \\\n    # xvfbwrapper with dependencies\n    apt-get install -y xvfb && \\\n    pip install xvfbwrapper && \\\n    /tmp/clean-layer.sh\n \nRUN pip install mpld3 && \\\n    pip install mplleaflet && \\\n    pip install gpxpy && \\\n    pip install arrow && \\\n    pip install nilearn && \\\n    pip install nibabel && \\\n    pip install pronouncing && \\\n    pip install markovify && \\\n    pip install imgaug && \\\n    pip install preprocessing && \\\n    pip install Baker && \\\n    pip install path.py && \\\n    pip install Geohash && \\\n    # https://github.com/vinsci/geohash/issues/4\n    sed -i -- 's/geohash/.geohash/g' /opt/conda/lib/python3.7/site-packages/Geohash/__init__.py && \\\n    pip install deap && \\\n    pip install tpot && \\\n    pip install scikit-optimize && \\\n    pip install haversine && \\\n    pip install toolz cytoolz && \\\n    pip install sacred && \\\n    pip install plotly && \\\n    pip install hyperopt && \\\n    pip install fitter && \\\n    pip install langid && \\\n    # Delorean. Useful for dealing with datetime\n    pip install delorean && \\\n    pip install trueskill && \\\n    pip install heamy && \\\n    # Useful data exploration libraries (for missing data and generating reports)\n    pip install missingno && \\\n    pip install pandas-profiling && \\\n    pip install s2sphere && \\\n    pip install git+https://github.com/fmfn/BayesianOptimization.git && \\\n    pip install matplotlib-venn && \\\n    pip install pyldavis && \\\n    pip install mlxtend && \\\n    pip install altair && \\\n    pip install pystan && \\\n    pip install ImageHash && \\\n    pip install ecos && \\\n    pip install CVXcanon && \\\n    pip install fancyimpute && \\\n    pip install pymc3 && \\\n    pip install tifffile && \\\n    pip install spectral && \\\n    pip install descartes && \\\n    pip install geojson && \\\n    pip install terminalplot && \\\n    pip install pydicom && \\\n    pip install wavio && \\\n    pip install SimpleITK && \\\n    pip install hmmlearn && \\\n    pip install bayespy && \\\n    pip install gplearn && \\\n    pip install PyAstronomy && \\\n    pip install squarify && \\\n    pip install fuzzywuzzy && \\\n    pip install python-louvain && \\\n    pip install pyexcel-ods && \\\n    pip install sklearn-pandas && \\\n    pip install stemming && \\\n    # b/148383434 remove pip install for holidays once fbprophet is compatible with latest version of holidays.\n    pip install holidays==0.9.12 && \\\n    pip install fbprophet && \\\n    pip install holoviews && \\\n    # 1.6.2 is not currently supported by the version of matplotlib we are using.\n    # See other comments about why matplotlib is pinned.\n    pip install geoviews==1.6.1 && \\\n    pip install hypertools && \\\n    pip install py_stringsimjoin && \\\n    pip install nibabel && \\\n    pip install mlens && \\\n    pip install scikit-multilearn && \\\n    pip install cleverhans && \\\n    pip install leven && \\\n    pip install catboost && \\\n    # fastFM doesn't support Python 3.7 yet: https://github.com/ibayer/fastFM/issues/151\n    # pip install fastFM && \\\n    pip install lightfm && \\\n    pip install folium && \\\n    pip install scikit-plot && \\\n    # dipy requires the optional fury dependency for visualizations.\n    pip install fury dipy && \\\n    # plotnine 0.5 is depending on matplotlib >= 3.0 which is not compatible with basemap.\n    # once basemap support matplotlib, we can unpin this package.\n    pip install plotnine==0.4.0 && \\\n    pip install scikit-surprise && \\\n    pip install pymongo && \\\n    pip install geoplot && \\\n    pip install eli5 && \\\n    pip install implicit && \\\n    pip install dask-ml[xgboost] && \\\n    /tmp/clean-layer.sh\n \nRUN pip install kmeans-smote --no-dependencies && \\\n    # Add google PAIR-code Facets\n    cd /opt/ && git clone https://github.com/PAIR-code/facets && cd facets/ && jupyter nbextension install facets-dist/ --user && \\\n    export PYTHONPATH=$PYTHONPATH:/opt/facets/facets_overview/python/ && \\\n    pip install tensorpack && \\\n    pip install pycountry && pip install iso3166 && \\\n    pip install pydash && \\\n    pip install kmodes --no-dependencies && \\\n    pip install librosa && \\\n    pip install polyglot && \\\n    pip install mmh3 && \\\n    pip install fbpca && \\\n    pip install sentencepiece && \\\n    pip install cufflinks && \\\n    pip install lime && \\\n    pip install memory_profiler && \\\n    /tmp/clean-layer.sh\n \n# install cython & cysignals before pyfasttext\nRUN pip install --upgrade cython && \\\n    pip install --upgrade cysignals && \\\n    pip install pyfasttext && \\\n    # ktext has an explicit dependency on Keras 2.2.4 which is not\n    # compatible with TensorFlow 2.0 (support was added in Keras 2.3.0).\n    # Add the package back once it is fixed upstream.\n    # pip install ktext && \\\n    pip install fasttext && \\\n    apt-get install -y libhunspell-dev && pip install hunspell && \\\n    pip install annoy && \\\n    # Need to use CountEncoder from category_encoders before it's officially released\n    pip install git+https://github.com/scikit-learn-contrib/categorical-encoding.git && \\\n    pip install google-cloud-automl && \\\n    # Newer version crashes (latest = 1.14.0) when running tensorflow.\n    # python -c \"from google.cloud import bigquery; import tensorflow\". This flow is common because bigquery is imported in kaggle_gcp.py\n    # which is loaded at startup.\n    pip install google-cloud-bigquery==1.12.1 && \\\n    pip install google-cloud-storage && \\\n    pip install ortools && \\\n    pip install scattertext && \\\n    # Pandas data reader\n    pip install pandas-datareader && \\\n    pip install wordsegment && \\\n    pip install pyahocorasick && \\\n    pip install wordbatch && \\\n    pip install emoji && \\\n    # Add Japanese morphological analysis engine\n    pip install janome && \\\n    pip install wfdb && \\\n    pip install vecstack && \\\n    # Doesn't support Python 3.7 yet. Last release on pypi is from 2017.\n    # Add back once this PR is released: https://github.com/scikit-learn-contrib/lightning/pull/133\n    # pip install sklearn-contrib-lightning && \\\n    # yellowbrick machine learning visualization library\n    pip install yellowbrick && \\\n    pip install mlcrate && \\\n    /tmp/clean-layer.sh\n \nRUN pip install bcolz && \\\n    pip install bleach && \\\n    pip install certifi && \\\n    pip install cycler && \\\n    pip install decorator && \\\n    pip install entrypoints && \\\n    pip install html5lib && \\\n    # Latest version breaks nbconvert: https://github.com/ipython/ipykernel/issues/422\n    pip install ipykernel==5.1.1 && \\\n    pip install ipython && \\\n    pip install ipython-genutils && \\\n    pip install ipywidgets && \\\n    pip install isoweek && \\\n    pip install jedi && \\\n    pip install Jinja2 && \\\n    pip install jsonschema && \\\n    pip install jupyter && \\\n    pip install jupyter-client && \\\n    pip install jupyter-console && \\\n    pip install jupyter-core && \\\n    pip install MarkupSafe && \\\n    pip install mistune && \\\n    pip install nbconvert && \\\n    pip install nbformat && \\\n    pip install notebook==5.5.0 && \\\n    pip install olefile && \\\n    pip install opencv-python && \\\n    pip install pandas_summary && \\\n    pip install pandocfilters && \\\n    pip install pexpect && \\\n    pip install pickleshare && \\\n    pip install Pillow && \\\n    # Install openslide and its python binding\n    apt-get install -y openslide-tools && \\\n    # b/152402322 install latest from pip once is in: https://github.com/openslide/openslide-python/pull/76\n    pip install git+git://github.com/rosbo/openslide-python.git@fix-setup && \\\n    pip install ptyprocess && \\\n    pip install Pygments && \\\n    pip install pyparsing && \\\n    pip install pytz && \\\n    pip install PyYAML && \\\n    pip install pyzmq && \\\n    pip install qtconsole && \\\n    pip install six && \\\n    pip install terminado && \\\n    # Latest version (6.0) of tornado breaks Jupyter notebook:\n    # https://github.com/jupyter/notebook/issues/4439\n    pip install tornado==5.0.2 && \\\n    pip install tqdm && \\\n    pip install traitlets && \\\n    pip install wcwidth && \\\n    pip install webencodings && \\\n    pip install widgetsnbextension && \\\n    pip install pyarrow && \\\n    pip install feather-format && \\\n    pip install fastai && \\\n    pip install torchtext && \\\n    pip install allennlp && \\\n    # b/149359379 remove once allennlp 1.0 is released which won't cause a spacy downgrade.\n    pip install spacy==2.2.3 && python -m spacy download en && python -m spacy download en_core_web_lg && \\\n    apt-get install -y ffmpeg && \\ \n    /tmp/clean-layer.sh\n \n    ###########\n    #\n    #      NEW CONTRIBUTORS:\n    # Please add new pip/apt installs in this block. Don't forget a \"&& \\\" at the end\n    # of all non-final lines. Thanks!\n    #\n    ###########\n \nRUN pip install flashtext && \\\n    pip install wandb && \\\n    pip install marisa-trie && \\\n    pip install pyemd && \\\n    pip install pyupset && \\\n    pip install pympler && \\\n    pip install s3fs && \\\n    pip install featuretools && \\\n    pip install -e git+https://github.com/SohierDane/BigQuery_Helper#egg=bq_helper && \\\n    pip install hpsklearn && \\\n    pip install git+https://github.com/Kaggle/learntools && \\\n    pip install kmapper && \\\n    pip install shap && \\\n    pip install ray && \\\n    pip install gym && \\\n    pip install tensorforce && \\\n    pip install pyarabic && \\\n    pip install conx && \\\n    pip install pandasql && \\\n    pip install tensorflow_hub && \\\n    pip install jieba  && \\\n    pip install git+https://github.com/SauceCat/PDPbox && \\\n    pip install ggplot && \\\n    pip install cesium && \\\n    pip install rgf_python && \\\n    # b/145404107: latest version force specific version of numpy and torch.\n    pip install pytext-nlp==0.1.2 && \\\n    pip install tsfresh && \\\n    pip install pykalman && \\\n    pip install optuna && \\\n    pip install chainercv && \\\n    pip install chainer-chemistry && \\\n    pip install plotly_express && \\\n    pip install albumentations && \\\n    pip install catalyst && \\\n    # b/145133331: latest version is causing issue with gcloud.\n    pip install rtree==0.8.3 && \\\n    # b/145133331 osmnx 0.11 requires rtree >= 0.9 which is causing issue with gcloud.\n    pip install osmnx==0.10 && \\\n    apt-get -y install libspatialindex-dev && \\\n    pip install pytorch-ignite && \\\n    pip install qgrid && \\\n    pip install bqplot && \\\n    pip install earthengine-api && \\\n    pip install transformers && \\\n    pip install dlib && \\\n    pip install kaggle-environments && \\\n    # b/149905611 The geopandas tests are broken with the version 0.7.0\n    pip install geopandas==0.6.3 && \\\n    pip install nnabla && \\\n    pip install vowpalwabbit && \\\n    /tmp/clean-layer.sh\n \n# Tesseract and some associated utility packages\nRUN apt-get install tesseract-ocr -y && \\\n    pip install pytesseract && \\\n    pip install wand==0.5.3 && \\\n    pip install pdf2image && \\\n    pip install PyPDF && \\\n    pip install pyocr && \\\n    /tmp/clean-layer.sh\nENV TESSERACT_PATH=/usr/bin/tesseract\n \n# For Facets\nENV PYTHONPATH=$PYTHONPATH:/opt/facets/facets_overview/python/\n# For Theano with MKL\nENV MKL_THREADING_LAYER=GNU\n \n# Temporary fixes and patches\n    # Temporary patch for Dask getting downgraded, which breaks Keras\nRUN pip install --upgrade dask && \\\n    # Stop jupyter nbconvert trying to rewrite its folder hierarchy\n    mkdir -p /root/.jupyter && touch /root/.jupyter/jupyter_nbconvert_config.py && touch /root/.jupyter/migrated && \\\n    mkdir -p /.jupyter && touch /.jupyter/jupyter_nbconvert_config.py && touch /.jupyter/migrated && \\\n    # Stop Matplotlib printing junk to the console on first load\n    sed -i \"s/^.*Matplotlib is building the font cache using fc-list.*$/# Warning removed by Kaggle/g\" /opt/conda/lib/python3.7/site-packages/matplotlib/font_manager.py && \\\n    # Make matplotlib output in Jupyter notebooks display correctly\n    mkdir -p /etc/ipython/ && echo \"c = get_config(); c.IPKernelApp.matplotlib = 'inline'\" > /etc/ipython/ipython_config.py && \\\n    # Temporary patch for broken libpixman 0.38 in conda-forge, symlink to system libpixman 0.34 untile conda package gets updated to 0.38.5 or higher.\n    ln -sf /usr/lib/x86_64-linux-gnu/libpixman-1.so.0.34.0 /opt/conda/lib/libpixman-1.so.0.38.0 && \\\n    /tmp/clean-layer.sh\n \n# gcloud SDK https://cloud.google.com/sdk/docs/quickstart-debian-ubuntu\nRUN echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" \\\n    | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && \\\n    curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | \\\n    apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - && \\\n    apt-get update -y && apt-get install google-cloud-sdk -y && \\\n    /tmp/clean-layer.sh\n \n# Add BigQuery client proxy settings\nENV PYTHONUSERBASE \"/root/.local\"\nADD patches/kaggle_gcp.py /root/.local/lib/python3.7/site-packages/kaggle_gcp.py\nADD patches/kaggle_secrets.py /root/.local/lib/python3.7/site-packages/kaggle_secrets.py\nADD patches/kaggle_web_client.py /root/.local/lib/python3.7/site-packages/kaggle_web_client.py\nADD patches/kaggle_datasets.py /root/.local/lib/python3.7/site-packages/kaggle_datasets.py\nADD patches/log.py /root/.local/lib/python3.7/site-packages/log.py\nADD patches/sitecustomize.py /root/.local/lib/python3.7/site-packages/sitecustomize.py\n# Override default imagemagick policies\nADD patches/imagemagick-policy.xml /etc/ImageMagick-6/policy.xml\n \n# TensorBoard Jupyter extension. Should be replaced with TensorBoard's provided magic once we have\n# worker tunneling support in place.\n# b/139212522 re-enable TensorBoard once solution for slowdown is implemented.\n# ENV JUPYTER_CONFIG_DIR \"/root/.jupyter/\"\n# RUN pip install jupyter_tensorboard && \\\n#     jupyter serverextension enable jupyter_tensorboard && \\\n#     jupyter tensorboard enable\n# ADD patches/tensorboard/notebook.py /opt/conda/lib/python3.7/site-packages/tensorboard/notebook.py\n \n# Set backend for matplotlib\nENV MPLBACKEND \"agg\"\n \n# We need to redefine TENSORFLOW_VERSION here to get the default ARG value defined above the FROM instruction.\n# See: https://docs.docker.com/engine/reference/builder/#understand-how-arg-and-from-interact\nARG TENSORFLOW_VERSION\nARG GIT_COMMIT=unknown\nARG BUILD_DATE=unknown\n \nLABEL git-commit=$GIT_COMMIT\nLABEL build-date=$BUILD_DATE\nLABEL tensorflow-version=$TENSORFLOW_VERSION\n# Used in the Jenkins `Docker GPU Build` step to restrict the images being pruned.\nLABEL kaggle-lang=python\n \n# Correlate current release with the git hash inside the kernel editor by running `!cat /etc/git_commit`.\nRUN echo \"$GIT_COMMIT\" > /etc/git_commit && echo \"$BUILD_DATE\" > /etc/build_date\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!/bin/bash\nset -e\n \nusage() {\ncat << EOF\nUsage: $0 [OPTIONS] [LABEL]\nPush a newly-built image with the given LABEL to gcr.io and DockerHub.\n \nOptions:\n    -g, --gpu                   Push the image with GPU support.\n    -s, --source-image IMAGE    Tag for the source image. \nEOF\n}\n \nSOURCE_IMAGE_TAG='kaggle/python-build:latest'\nSOURCE_IMAGE_TAG_OVERRIDE=''\nTARGET_IMAGE='gcr.io/kaggle-images/python'\n \nwhile :; do\n    case \"$1\" in \n        -h|--help)\n            usage\n            exit\n            ;;\n        -g|--gpu)\n            SOURCE_IMAGE_TAG='kaggle/python-gpu-build:latest'\n            TARGET_IMAGE='gcr.io/kaggle-private-byod/python'\n            ;;\n        -s|--source-image)\n            if [[ -z $2 ]]; then\n                usage\n                printf 'ERROR: No IMAGE specified after the %s flag.\\n' \"$1\" >&2\n                exit\n            fi\n            SOURCE_IMAGE_TAG_OVERRIDE=$2\n            shift # skip the flag value\n            ;;\n        -?*)\n            usage\n            printf 'ERROR: Unknown option: %s\\n' \"$1\" >&2\n            exit\n            ;;\n        *)            \n            break\n    esac\n \n    shift\ndone\n \nLABEL=${1:-testing}\n \nif [[ -n \"$SOURCE_IMAGE_TAG_OVERRIDE\" ]]; then\n    SOURCE_IMAGE_TAG=\"$SOURCE_IMAGE_TAG_OVERRIDE\"\nfi\n \nreadonly SOURCE_IMAGE_TAG\nreadonly TARGET_IMAGE\nreadonly LABEL\n \nset -x\ndocker tag \"${SOURCE_IMAGE_TAG}\" \"${TARGET_IMAGE}:${LABEL}\"\ngcloud docker -- push \"${TARGET_IMAGE}:${LABEL}\"\n ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data input**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"p1 = \"/kaggle/input/covid19-global-forecasting-week-4/\"\np2 = \"/kaggle/input/world-bank-wdi-212-health-systems/\"\np3 = \"/kaggle/input/covid19inf/\"\ntrain = pd.read_csv(p1 + \"train.csv\")\ntest =  pd.read_csv(p1 + \"test.csv\")\nsubmission =  pd.read_csv(p1 + \"submission.csv\")\nhealth = pd.read_csv(p2 + \"2.12_Health_systems.csv\")\ncountry = pd.read_csv(p3 + \"covid19countryinfo.csv\")\npollution = pd.read_csv(p3 + \"region_pollution.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some data munging in the contry dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"country.drop(columns= country.columns[range(22,54)], inplace=True)\ncountry[\"pop\"] = country[\"pop\"].str.replace(\",\", \"\").astype('float64')\ncountry[\"quarantine\"] = pd.to_datetime(country.quarantine)\ncountry[\"schools\"] = pd.to_datetime(country.schools)\ncountry[\"restrictions\"] = pd.to_datetime(country.restrictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data munging and merging"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Date\"] = pd.to_datetime(train.Date)\ntrain[\"country_province\"] = train[\"Province_State\"]\ntrain.country_province.fillna(train[\"Country_Region\"], inplace=True)\ntest[\"Date\"] = pd.to_datetime(test.Date)\ntest[\"country_province\"] = test[\"Province_State\"]\ntest.country_province.fillna(test[\"Country_Region\"], inplace=True)\ntrain = train.merge(country, how='left', left_on = [\"country_province\"], right_on = [\"country\"])\ntrain = train.merge(pollution, how='left', left_on = [\"country_province\"], right_on = [\"Region\"])\ntrain = train.merge(health, how='left', left_on = [\"Country_Region\", \"Province_State\"], right_on = [\"Country_Region\", \"Province_State\"])\ntest = test.merge(country, how='left', left_on = [\"country_province\"], right_on = [\"country\"])\ntest = test.merge(pollution, how='left', left_on = [\"country_province\"], right_on = [\"Region\"])\ntest = test.merge(health, how='left', left_on = [\"Country_Region\", \"Province_State\"], right_on = [\"Country_Region\", \"Province_State\"])\ntrain[\"days\"] = (train.Date - train.Date[0]).dt.days\ntest[\"days\"] = (test.Date - train.Date[0]).dt.days\n\n#The columns with region/state names in the different csvs are not longer needed\ncolumns_to_drop = [\"country\", \"Region\", \"Province_State\", \"Country_Region\", \"World_Bank_Name\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"More data preparation, this time when it is in quarnatine, school stops and restriction. Those data are not complete and this is a pity but they are valuable."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"in_quarantine\"] = 0\ntrain[\"in_schools\"] = 0\ntrain[\"in_restrictions\"] = 0\ntest[\"in_quarantine\"] = 0\ntest[\"in_schools\"] = 0\ntest[\"in_restrictions\"] = 0\nfor cp in train.country_province.unique():\n    quarantine = country.loc[country.country == cp, \"quarantine\"]\n    schools = country.loc[country.country == cp, \"schools\"]\n    restrictions = country.loc[country.country == cp, \"restrictions\"]\n    if (len(quarantine) > 0) and (quarantine.values[0] is not np.nan):\n        date1 = pd.to_datetime(quarantine.values[0])\n        train.loc[(train.country_province == cp) & (train.Date > date1), \"in_quarantine\"] = (train.Date - date1).dt.days\n        test.loc[(test.country_province == cp) & (test.Date > date1), \"in_quarantine\"] = (test.Date - date1).dt.days\n        \n    if (len(schools) > 0) and (schools.values[0] is not np.nan):\n        date1 = pd.to_datetime(schools.values[0])\n        train.loc[(train.country_province == cp) & (train.Date > date1), \"in_schools\"] = (train.Date - date1).dt.days\n        test.loc[(test.country_province == cp) & (test.Date > date1), \"in_schools\"] = (test.Date - date1).dt.days\n\n    if (len(restrictions) > 0) and (restrictions.values[0] is not np.nan):\n        date1 = pd.to_datetime(restrictions.values[0])\n        train.loc[(train.country_province == cp) & (train.Date > date1), \"in_restrictions\"] = (train.Date - date1).dt.days\n        test.loc[(test.country_province == cp) & (test.Date > date1), \"in_restrictions\"] = (test.Date - date1).dt.days\n\ncolumns_to_drop += [\"quarantine\", \"schools\", \"restrictions\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Label encode the countries and/or regions"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\ntrain[\"country_province\"] = lb.fit_transform(train.country_province)\ntest[\"country_province\"] = lb.transform(test.country_province)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generate three new variables, days from first death, first reported case and 100th reported case. The virus arrives one place by travelling that is not easy to track for the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"days_from_first_case\"] = 0\ntest[\"days_from_first_case\"] = 0\ntrain[\"days_from_first_death\"] = 0\ntrain[\"days_from_case_100\"] = 0\ntest[\"days_from_case_100\"] = 0\ntest[\"days_from_first_death\"] = 0\n\ndates = list(train.Date.unique())\nfor province in train.country_province.unique():\n    #print(province)\n    mask1 = train.country_province == province\n    mask2 = train.ConfirmedCases > 1.0\n    mask3 = train.ConfirmedCases > 100.0\n    mask4 = train.Fatalities > 1.0\n    try:\n        idx1 = train.loc[mask1 & mask2 ,[\"ConfirmedCases\"]].idxmin()[0]\n        dateidx1 = train.iloc[idx1][\"Date\"]\n    except:\n        dateidx1 = test.Date.max()\n        pass\n    #print(dateidx1)\n    train.loc[mask1 & (train.Date >= dateidx1), \"days_from_first_case\"] = (train.Date - dateidx1).dt.days\n    test.loc[mask1 & (test.Date >= dateidx1), \"days_from_first_case\"] = (test.Date - dateidx1).dt.days\n    \n    try:\n        idx1 = train.loc[mask1 & mask3 ,[\"ConfirmedCases\"]].idxmin()[0]\n        dateidx1 = train.iloc[idx1][\"Date\"]\n    except:\n        dateidx1 = test.Date.max()\n        pass\n    train.loc[mask1 & (train.Date >= dateidx1), \"days_from_case_100\"] = (train.Date - dateidx1).dt.days\n    test.loc[mask1 & (test.Date >= dateidx1), \"days_from_case_100\"] = (test.Date - dateidx1).dt.days    \n\n        \n    try:\n        idx1 = train.loc[mask1 & mask4 ,[\"Fatalities\"]].idxmin()[0]\n        dateidx1 = train.iloc[idx1][\"Date\"]\n    except:\n        dateidx1 = test.Date.max()\n        pass\n    train.loc[mask1 & (train.Date >= dateidx1), \"days_from_first_death\"] = (train.Date - dateidx1).dt.days\n    test.loc[mask1 & (test.Date >= dateidx1), \"days_from_first_death\"] = (test.Date - dateidx1).dt.days    \n\ntrain.fillna(value = 0, inplace = True)\ntest.fillna(value = 0, inplace = True)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Definition of lagged variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Construction of laged variables \nlag_number = 3\nfor lag in range(1, lag_number + 1):\n    var_name = \"cases_lag%d\" % lag\n    train[var_name] = train.ConfirmedCases.shift(periods = lag)\n    train.loc[train.Date <= train.Date[lag - 1] , var_name] = 0\n    var_name = \"fatalities_lag%d\" % lag\n    train[var_name] = train.Fatalities.shift(periods = 1)\n    train.loc[train.Date <= train.Date[lag - 1] , var_name] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally the train and test data can be prepared for the models eliminating the object variables and repited ones. From train data a small sample is taken for validation, the days that coincide in train and test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Days that coincide in train and test\nprint(train.loc[train.Date.isin(test.Date.unique()), \"Date\"].unique())\n# The smallest of those days will be the separation between train and validation\nsep_date = train.loc[train.Date.isin(test.Date.unique()), \"Date\"].unique().min()\n\nresult_columns = [\"ConfirmedCases\", \"Fatalities\"]\nX_train = train.loc[(train.Date<sep_date),].drop(columns = columns_to_drop + [\"Id\", \"Date\"] + result_columns)\ny_train_cases = train.loc[(train.Date<sep_date),].ConfirmedCases\ny_train_fatalities = train.loc[(train.Date<sep_date),].Fatalities\n\nX_val = train.loc[(train.Date>=sep_date),].drop(columns = columns_to_drop + [\"Id\", \"Date\"] + result_columns)\ny_val_cases = train.loc[(train.Date>=sep_date),].ConfirmedCases\ny_val_fatalities = train.loc[(train.Date>=sep_date),].Fatalities\n\nX_test = test.drop(columns = columns_to_drop + [\"ForecastId\", \"Date\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before starting the models a function is defined to reduce code to check errors"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\ndef validate_models(model_cases, model_fatalities):\n    predict_train_cases = model_cases.predict(X_train)\n    predict_val_cases = model_cases.predict(X_val)\n    print(\"RMSE in train detected cases: \", np.sqrt(mean_squared_error(y_train_cases, predict_train_cases)))\n    print(\"RMSE in validation detected cases: \", np.sqrt(mean_squared_error(y_val_cases, predict_val_cases)))\n    predict_train_fatalities = model_fatalities.predict(X_train)\n    predict_val_fatalities = model_fatalities.predict(X_val)\n    print(\"RMSE in train fatalities: \", np.sqrt(mean_squared_error(y_train_fatalities, predict_train_fatalities)))\n    print(\"RMSE in validation fatalities: \", np.sqrt(mean_squared_error(y_val_fatalities, predict_val_fatalities)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the models, let's start with a Random Forest and after a lightgbm"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlm_cases = LinearRegression()\nlm_cases.fit(X_train, y_train_cases)\n\nlm_fatalities = LinearRegression()\nlm_fatalities.fit(X_train, y_train_fatalities)\n\nvalidate_models(lm_cases, lm_fatalities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RandomForest\nfrom sklearn.ensemble import RandomForestRegressor\nrf_cases = RandomForestRegressor(n_estimators= 400, max_depth=6, random_state=0, verbose=0, n_jobs=-1)\nrf_cases.fit(X_train, y_train_cases)\n\nrf_fatalities = RandomForestRegressor(n_estimators= 400, max_depth=6, random_state=0, verbose=0, n_jobs=-1)\nrf_fatalities.fit(X_train, y_train_fatalities)\n\nvalidate_models(rf_cases, rf_fatalities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nlgb_params = {\n               'feature_fraction': 0.8,\n               'metric': 'rmse',\n               'nthread':-1, \n               'min_data_in_leaf': 2**4,\n               'bagging_fraction': 0.75, \n               'learning_rate': 0.5, \n               'objective': 'mse', \n               'bagging_seed': 2**5, \n               'num_leaves': 2**6,\n               'bagging_freq':1,\n               'verbose':1 \n              }\nlgbm_cases = lgb.train(lgb_params, \n                       train_set=lgb.Dataset(X_train, label=y_train_cases), \n                       valid_sets=lgb.Dataset(X_val, label=y_val_cases), \n                       num_boost_round=500)\nlgbm_fatalities = lgb.train(lgb_params, \n                            train_set=lgb.Dataset(X_train, label=y_train_fatalities), \n                            valid_sets=lgb.Dataset(X_val, label=y_val_fatalities), \n                            num_boost_round=500)\nvalidate_models(lgbm_cases, lgbm_fatalities)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both model worked well in train set but much worse in validation, seems overfitting. Let's see the features importances"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb.plot_importance(lgbm_fatalities)\nplt.rcParams['figure.figsize'] = [10, 10]\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally the calculation for the test set requires a rolling forecast, going one by one to calculate the lags with the previous values"},{"metadata":{"trusted":true},"cell_type":"code","source":"lags = {}\npredict_test_cases, predict_test_fatalities = [] , []\ntest_min_day = test.days.min()\nfor i in range(1, lag_number + 1):\n    lags[\"caseslag%d\" % i] = 0\n    lags[\"fatalitieslag%d\" % i] = 0\n    \nfor ind in tqdm_notebook(range(len(X_test))):\n    #print(\"case: {} of {}\".format(ind, len(X_test)))\n    #First lag data are obtained either from previous calculations or train data\n    if X_test.iloc[ind].days == test_min_day:\n        #print(test_min_day)\n        for i in range(1, lag_number + 1):\n            mask1 = train.days == (test_min_day - i)\n            mask2 = train.country_province == X_test.iloc[ind].country_province\n            lags[\"caseslag%d\" % i] = train.loc[mask1 & mask2, \"ConfirmedCases\"].values[0]\n            lags[\"fatalitieslag%d\" % i] = train.loc[mask1 & mask2, \"Fatalities\"].values[0]\n    else:\n        lags[\"caseslag1\"] = pred_cases\n        lags[\"fatalitieslag1\"] = pred_fatalities\n        for i in range(2, lag_number + 1):\n            lags[\"caseslag%d\" % i] = lags[\"caseslag%d\" % (i-1)]\n            lags[\"fatalitieslag%d\" % i] = lags[\"fatalitieslag%d\" % (i-1)]\n    x_test = X_test.iloc[ind].copy()\n    x_test =pd.DataFrame(x_test).transpose()\n    for i in range(1, lag_number + 1):\n        x_test[\"cases_lag%d\" % i] = lags[\"caseslag%d\" % i]\n        x_test[\"fatalities_lag%d\" % i] = lags[\"fatalitieslag%d\" % i]\n        \n    pred_cases = rf_cases.predict(x_test)[0]\n    pred_fatalities = rf_fatalities.predict(x_test)[0]\n    predict_test_cases.append(pred_cases)\n    predict_test_fatalities.append(pred_fatalities)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.ConfirmedCases = predict_test_cases\nsubmission.Fatalities = predict_test_fatalities\nsubmission.to_csv(\"/kaggle/working/submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}