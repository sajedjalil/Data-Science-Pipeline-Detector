{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.layers import GRU\nfrom keras.initializers import random_uniform\nfrom keras.optimizers import Adagrad\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\nimport tensorflow as tf\nimport datetime\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nfont = {'family' : 'meiryo'}\nplt.rc('font', **font)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random as rn\nimport os\nos.environ['PYTHONHASHSEED'] = '0'\nnp.random.seed(42)\nrn.seed(12345)\nsession_conf =  tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nfrom keras import backend as K\ntf.random.set_seed(1234)\nsess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\ntf.compat.v1.keras.backend.set_session(sess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\ntrain_df = train_df.fillna(\"No State\")\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_rate = 0.1\ntime_series_len = 18\ntrain_date_count = len(set(train_df[\"Date\"]))\n\nX, Y = [],[]\n\nscaler = StandardScaler()\ntrain_df[\"ConfirmedCases_std\"] = scaler.fit_transform(train_df[\"ConfirmedCases\"].values.reshape(len(train_df[\"ConfirmedCases\"].values),1))\n\n#Formatting the train data for a time series model\nfor state,country in train_df.groupby([\"Province_State\",\"Country_Region\"]).sum().index:\n    df = train_df[(train_df[\"Country_Region\"] == country) & (train_df[\"Province_State\"] == state)]\n    \n    #Areas with zero patients cannot be predicted ⇒ Artificially predicted to be zero\n    if df[\"ConfirmedCases\"].sum() != 0:\n        for i in range(len(df) - time_series_len):\n            X.append(df[['ConfirmedCases_std']].iloc[i:(i+time_series_len)].values)\n            Y.append(df[['ConfirmedCases_std']].iloc[i+time_series_len].values)\n\nX=np.array(X)\nY=np.array(Y)\n    \nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_rate, shuffle = True ,random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmedCases_std_min = train_df[\"ConfirmedCases_std\"].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def huber_loss(y_true, y_pred, clip_delta=1.0):\n  error = y_true - y_pred\n  cond  = tf.keras.backend.abs(error) < clip_delta\n\n  squared_loss = 0.5 * tf.keras.backend.square(error)\n  linear_loss  = clip_delta * (tf.keras.backend.abs(error) - 0.5 * clip_delta)\n\n  return tf.where(cond, squared_loss, linear_loss)\n\ndef huber_loss_mean(y_true, y_pred, clip_delta=1.0):\n  return tf.keras.backend.mean(huber_loss(y_true, y_pred, clip_delta))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs_num = 20\nn_in = 1\n\nmodel = Sequential()\nmodel.add(GRU(100,\n               batch_input_shape=(None, time_series_len, n_in),\n               kernel_initializer=random_uniform(seed=1),\n               return_sequences=False\n             ))\nmodel.add(Dense(50))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(n_in, kernel_initializer=random_uniform(seed=1)))\nmodel.add(Activation(\"linear\"))\n\nopt = Adagrad(lr=0.01, epsilon=1e-08, decay=1e-4)\nmodel.compile(loss = huber_loss_mean, optimizer=opt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = [ReduceLROnPlateau(monitor='loss', patience=4, verbose=1, factor=0.6),\n             EarlyStopping(monitor='loss', patience=10)]\n\nhist = model.fit(X_train, Y_train, batch_size=20, epochs=epochs_num,\n                 callbacks=callbacks,shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_std = model.predict(X_test)\nresult_std= pd.DataFrame(predicted_std)\nresult_std.columns = ['predict']\nresult_std['actual'] = Y_test\nresult_std.plot(figsize=(25,6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = hist.history['loss']\nepochs = len(loss)\nfig = plt.figure()\nplt.plot(range(epochs), loss, marker='.', label='loss(training data)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = scaler.inverse_transform(predicted_std)\nY_test2 = scaler.inverse_transform(Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sqrt(mean_squared_log_error(predicted, Y_test2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result= pd.DataFrame(predicted)\nresult.columns = ['predict']\nresult['actual'] = Y_test2\nresult.plot(figsize=(25,6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/covid19-global-forecasting-week-4/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = (datetime.datetime.strptime(\"2020-04-01\", '%Y-%m-%d') - datetime.timedelta(days=time_series_len)).strftime('%Y-%m-%d')\ntest_df = train_df[train_df[\"Date\"] > temp]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\").query(\"Date>'2020-04-01'and Date<='2020-04-14'\")\ncheck_df[\"ConfirmedCases_std\"] = scaler.transform(check_df[\"ConfirmedCases\"].values.reshape(len(check_df[\"ConfirmedCases\"].values),1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmedCases_pred = []\nfor i in range(0,313*time_series_len,time_series_len):\n    temp_array = np.array(test_df[\"ConfirmedCases_std\"][i:i+time_series_len])\n    for j in range(43):\n        if j<13:\n            temp_array = np.append(temp_array,np.array(check_df[\"ConfirmedCases_std\"])[int(i*13/time_series_len)+j])\n        elif np.array(test_df[\"ConfirmedCases\"][i:i+time_series_len]).sum() == 0:\n            temp_array = np.append(temp_array,temp_array[-1])\n        else:\n            temp_array = np.append(temp_array,model.predict(temp_array[-time_series_len:].reshape(1,time_series_len,1)))\n    confirmedCases_pred.append(temp_array[-43:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[\"ConfirmedCases\"] = np.abs(scaler.inverse_transform(np.array(confirmedCases_pred).reshape(313*43)))\nsubmission[\"ConfirmedCases_std\"] = np.array(confirmedCases_pred).reshape(313*43)\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('./submission_c.csv')\nsubmission.to_csv('..\\output\\kaggle\\working\\submission_c.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_rate = 0.1\ntime_series_len = 16\ntrain_date_count = len(set(train_df[\"Date\"]))\n\nX, Y = [],[]\n\nscaler = StandardScaler()\ntrain_df[\"Fatalities_std\"] = scaler.fit_transform(train_df[\"Fatalities\"].values.reshape(len(train_df[\"Fatalities\"].values),1))\n\nss = StandardScaler()\ntrain_df[\"ConfirmedCases_std\"] = ss.fit_transform(train_df[\"ConfirmedCases\"].values.reshape(len(train_df[\"ConfirmedCases\"].values),1))\n\n#Formatting the train data for a time series model\nfor state,country in train_df.groupby([\"Province_State\",\"Country_Region\"]).sum().index:\n    df = train_df[(train_df[\"Country_Region\"] == country) & (train_df[\"Province_State\"] == state)]\n    \n    #Areas with zero patients cannot be predicted ⇒ Artificially predicted to be zero\n    if df[\"Fatalities\"].sum() != 0 or df[\"ConfirmedCases\"].sum() != 0:\n        for i in range(len(df) - time_series_len):\n            X.append(df[['Fatalities_std','ConfirmedCases_std']].iloc[i:(i+time_series_len)].values)\n            Y.append(df[['Fatalities_std']].iloc[i+time_series_len].values)\n\nX=np.array(X)\nY=np.array(Y)\n    \nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_rate, shuffle = True ,random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fatalities_std_min = train_df[\"Fatalities_std\"].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs_num = 21\nn_in = 2\n\nmodel = Sequential()\nmodel.add(GRU(100,\n               batch_input_shape=(None, time_series_len, n_in),\n               kernel_initializer=random_uniform(seed=1),\n               return_sequences=False))\nmodel.add(Dense(50))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(1, kernel_initializer=random_uniform(seed=1)))\nmodel.add(Activation(\"linear\"))\n\nopt = Adagrad(lr=0.01, epsilon=1e-08, decay=1e-4)\nmodel.compile(loss = huber_loss_mean, optimizer=opt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = [ReduceLROnPlateau(monitor='loss', patience=4, verbose=1, factor=0.6),\n             EarlyStopping(monitor='loss', patience=10)]\nhist = model.fit(X_train, Y_train, batch_size=16, epochs=epochs_num,\n                 callbacks=callbacks,shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The expected result may be negative because the loss function is MSE.\n\nBecause,restore by taking an absolute value."},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_std = model.predict(X_test)\nresult_std= pd.DataFrame(predicted_std)\nresult_std.columns = ['predict']\nresult_std['actual'] = Y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_std.plot(figsize=(25,6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = hist.history['loss']\nepochs = len(loss)\nfig = plt.figure()\nplt.plot(range(epochs), loss, marker='.', label='loss(training data)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = scaler.inverse_transform(predicted_std)\nY_test = scaler.inverse_transform(Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_ = scaler.inverse_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sqrt(mean_squared_log_error(predicted, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = (datetime.datetime.strptime(\"2020-04-01\", '%Y-%m-%d') - datetime.timedelta(days=time_series_len)).strftime('%Y-%m-%d')\ntest_df = train_df[train_df[\"Date\"] > temp]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_df[\"Fatalities_std\"] = scaler.transform(check_df[\"Fatalities\"].values.reshape(len(check_df[\"Fatalities\"].values),1))\ncheck_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fatalities_pred = []\nfor i in range(0,313*time_series_len,time_series_len):\n    temp_array = np.array(test_df[[\"Fatalities_std\",\"ConfirmedCases_std\"]][i:i+time_series_len])\n    for j in range(43):\n        if j<13:\n            temp_array = np.append(temp_array,np.append(np.array(check_df[\"Fatalities_std\"])[int(i*13/time_series_len)+j],np.array(check_df[\"ConfirmedCases_std\"])[int(i*13/time_series_len)+j]).reshape(1,2),axis=0)\n        elif np.array(test_df[[\"Fatalities\",\"ConfirmedCases\"]][i:i+time_series_len]).sum() == 0:\n            temp_array = np.append(temp_array,np.array(temp_array[-1]).reshape(1,2),axis=0)\n        else:\n            temp_array = np.append(temp_array,np.append(model.predict(temp_array[-time_series_len:].reshape(1,time_series_len,2)),submission[\"ConfirmedCases_std\"][i/time_series_len*43+j]).reshape(1,2),axis=0)\n    fatalities_pred.append(temp_array[-43:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[\"Fatalities\"] = np.abs(scaler.inverse_transform([i[0] for i in np.array(fatalities_pred).reshape(313*43,2)]))\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[[\"ConfirmedCases\",\"Fatalities\"]] = submission[[\"ConfirmedCases\",\"Fatalities\"]].round().astype(int)\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = submission.drop(\"ConfirmedCases_std\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = submission.set_index('ForecastId')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(313):\n    for j in range(2,44):\n        if submission[\"ConfirmedCases\"][i*43+j] < submission[\"ConfirmedCases\"][i*43+j-1]:\n            submission[\"ConfirmedCases\"][i*43+j] = submission[\"ConfirmedCases\"][i*43+j-1]\n        if submission[\"Fatalities\"][i*43+j] < submission[\"Fatalities\"][i*43+j-1]:\n            submission[\"Fatalities\"][i*43+j] = submission[\"Fatalities\"][i*43+j-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}