{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.utils import shuffle\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom sklearn.metrics import mean_squared_error\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom sklearn import preprocessing\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load training data and preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/covid19-with-population/update_train_processed.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The character ' will make later query function report an error,so it's replaced by a space\ntrain.Country.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\ntrain.Province.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\n\n# There are few infinite values in the weather data,it will cause the training loss become NAN.Since the amount of np.inf is very few,it's simply replace by 0.\ntrain.replace(np.inf,0,inplace=True)\n\n# Transform percentage data to float\ndef get_percent(x):\n    x = str(x)\n    x = x.strip('%')\n    x = int(x)/100\n    return x\n\ntrain.UrbanPopRate = train.UrbanPopRate.apply(lambda x:get_percent(x))\n\n# Transform date type\ndef get_dt(x):\n    return datetime.strptime(x,'%Y-%m-%d')\n\ntrain.Date = train.Date.apply(lambda x:get_dt(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Outliers handling\n#### The confirmedcases and fatalities are cumulative amount,but there are some values smaller than last dates."},{"metadata":{"trusted":true},"cell_type":"code","source":"for index,row in train.iterrows():\n    if train.iloc[index].Province == train.iloc[index - 1].Province and train.iloc[index].ConfirmedCases < train.iloc[index-1].ConfirmedCases:\n        train.iloc[index,4] = train.iloc[index-1,4]\n    if train.iloc[index].Province == train.iloc[index - 1].Province and train.iloc[index].Fatalities < train.iloc[index-1].Fatalities:\n        train.iloc[index,5] = train.iloc[index-1,5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_exam = train[['Country','Province','Date','ConfirmedCases','Fatalities']]\ndiff_df = pd.DataFrame(columns = ['Country','Province','Date','ConfirmedCases','Fatalities'])\nfor country in train_exam.Country.unique():\n    for province in train_exam[train_exam.Country == country].Province.unique():\n        province_df = train_exam.query(f\"Country == '{country}' and Province == '{province}'\")\n        conf = province_df.ConfirmedCases\n        fata = province_df.Fatalities\n        diff_conf = conf.diff()\n        diff_fata = fata.diff()\n        province_df.ConfirmedCases = diff_conf\n        province_df.Fatalities = diff_fata\n        diff_df = pd.concat([diff_df,province_df],0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sum(diff_df.ConfirmedCases < 0),sum(diff_df.Fatalities<0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MinMax normalization of confirmedcases and fatalities for each province"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('mode.chained_assignment', None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scale_train = pd.DataFrame(columns = ['Id_x', 'Province', 'Country', 'Date', 'ConfirmedCases', 'Fatalities',\n       'Days_After_1stJan', 'Dayofweek', 'Month', 'Day', 'Population',\n       'Density', 'Land_Area', 'Migrants', 'MedAge', 'UrbanPopRate', 'Id_y',\n       'Lat', 'Long', 'temp', 'min', 'max', 'stp', 'slp', 'dewp', 'rh', 'ah',\n       'wdsp', 'prcp', 'fog', 'API_beds'])\nfor country in train.Country.unique():\n    for province in train.query(f\"Country=='{country}'\").Province.unique():\n        province_df = train.query(f\"Country=='{country}' and Province=='{province}'\")\n        province_confirm = province_df.ConfirmedCases\n        province_fatalities = province_df.Fatalities\n        province_confirm = np.array(province_confirm).reshape(-1,1)\n        province_fatalities = np.array(province_confirm).reshape(-1,1)\n        scaler1= preprocessing.MinMaxScaler()\n        scaled_confirm = scaler1.fit_transform(province_confirm)\n        scaler2 = preprocessing.MinMaxScaler()\n        scaled_fata = scaler2.fit_transform(province_fatalities)\n        province_df['ConfirmedCases'] = scaled_confirm\n        province_df['Fatalities'] = scaled_fata\n        scale_train = pd.concat((scale_train,province_df),axis = 0,sort=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create input data for LSTM\n\n#### This part referenced the code of the great notebook: https://www.kaggle.com/frlemarchand/covid-19-forecasting-with-an-rnn#5.-Generate-predictions-using-the-model"},{"metadata":{"trusted":true},"cell_type":"code","source":"trend_df = pd.DataFrame(columns={\"infection_trend\",\"fatality_trend\",\"quarantine_trend\",\"school_trend\",\"total_population\",\"expected_cases\",\"expected_fatalities\"})\n\ntrain_df = scale_train\ndays_in_sequence = 14\n\ntrend_list = []\n\nwith tqdm(total=len(list(train_df.Country.unique()))) as pbar:\n    for country in train_df.Country.unique():\n        for province in train_df.query(f\"Country=='{country}'\").Province.unique():\n            province_df = train_df.query(f\"Country=='{country}' and Province=='{province}'\")\n            \n\n            for i in range(0,len(province_df)):\n                if i+days_in_sequence<=len(province_df):\n                    #prepare all the trend inputs\n                    infection_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].ConfirmedCases.values]\n                    fatality_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].Fatalities.values]\n\n                    #preparing all the stable inputs\n                    days_after_1stJan = float(province_df.iloc[i].Days_After_1stJan)\n                    dayofweek = float(province_df.iloc[i].Dayofweek)\n                    month = float(province_df.iloc[i].Month)\n                    day= float(province_df.iloc[i].Day)\n                    population = float(province_df.iloc[i].Population)\n                    density = float(province_df.iloc[i].Density)\n                    land_area = float(province_df.iloc[i].Land_Area)\n                    migrants = float(province_df.iloc[i].Migrants)\n                    medage = float(province_df.iloc[i].MedAge)\n                    urbanpoprate = float(province_df.iloc[i].UrbanPopRate)\n                    beds = float(province_df.iloc[i].API_beds)\n\n                    #True cases in i+days_in_sequence-1 day\n                    expected_cases = float(province_df.iloc[i+days_in_sequence-1].ConfirmedCases)\n                    expected_fatalities = float(province_df.iloc[i+days_in_sequence-1].Fatalities)\n\n                    trend_list.append({\"infection_trend\":infection_trend,\n                                     \"fatality_trend\":fatality_trend,\n                                     \"stable_inputs\":[population,density,land_area,migrants,medage,urbanpoprate,beds],\n                                     \"expected_cases\":expected_cases,\n                                     \"expected_fatalities\":expected_fatalities})\n        pbar.update(1)\ntrend_df = pd.DataFrame(trend_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trend_df[\"temporal_inputs\"] = [np.asarray([trends[\"infection_trend\"],trends[\"fatality_trend\"]]) for idx,trends in trend_df.iterrows()]\ntrend_df = shuffle(trend_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trend_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Only keeping 25 sequences where the number of cases stays at 0, as there were way too many of these samples in our dataset.\ni=0\ntemp_df = pd.DataFrame()\nfor idx,row in trend_df.iterrows():\n    if sum(row.infection_trend)>0:\n        temp_df = temp_df.append(row)\n    else:\n        if i<25:\n            temp_df = temp_df.append(row)\n            i+=1\ntrend_df = temp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence_length = 13\ntraining_percentage = 0.9\n# The purpose of '-2'and'+2' is to make the number of samples in the training test set divisible by batchsize\ntraining_item_count = int(len(trend_df)*training_percentage)\nvalidation_item_count = len(trend_df)-int(len(trend_df)*training_percentage)\ntraining_df = trend_df[:training_item_count-2]\nvalidation_df = trend_df[training_item_count+2:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_temporal_train = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in training_df[\"temporal_inputs\"].values]),(training_item_count-2,2,sequence_length)),(0,2,1) )).astype(np.float32)\nX_stable_train = np.asarray([np.asarray(x) for x in training_df[\"stable_inputs\"]]).astype(np.float32)\nY_cases_train = np.asarray([np.asarray(x) for x in training_df[\"expected_cases\"]]).astype(np.float32)\nY_fatalities_train = np.asarray([np.asarray(x) for x in training_df[\"expected_fatalities\"]]).astype(np.float32)\n\nX_temporal_test = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in validation_df[\"temporal_inputs\"]]),(validation_item_count-2,2,sequence_length)),(0,2,1)) ).astype(np.float32)\nX_stable_test = np.asarray([np.asarray(x) for x in validation_df[\"stable_inputs\"]]).astype(np.float32)\nY_cases_test = np.asarray([np.asarray(x) for x in validation_df[\"expected_cases\"]]).astype(np.float32)\nY_fatalities_test = np.asarray([np.asarray(x) for x in validation_df[\"expected_fatalities\"]]).astype(np.float32)\n\n# Transform to tensor type\nX_temporal_train = torch.from_numpy(X_temporal_train)\nX_stable_train = torch.from_numpy(X_stable_train)\nY_cases_train = torch.from_numpy(Y_cases_train)\nY_fatalities_train = torch.from_numpy(Y_fatalities_train)\n\nX_temporal_test = torch.from_numpy(X_temporal_test)\nX_stable_test = torch.from_numpy(X_stable_test)\nY_cases_test = torch.from_numpy(Y_cases_test)\nY_fatalities_test = torch.from_numpy(Y_fatalities_test)\n\n# Merge two objective values\nY_train = torch.cat((Y_cases_train.reshape(14770,1),Y_fatalities_train.reshape(14770,1)),1)\nY_test = torch.cat((Y_cases_test.reshape(1640,1),Y_fatalities_test.reshape(1640,1)),1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(X_temporal_train),len(X_temporal_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create train,test loader for training\nclass MyDataset(Dataset):\n    def __init__(self, data1,data2, labels):\n        self.trend= data1\n        self.stable= data2\n        self.labels = labels  \n\n    def __getitem__(self, index):    \n        trend,stable, labels = self.trend[index], self.stable[index], self.labels[index]\n        return trend,stable,labels\n\n    def __len__(self):\n        return len(self.trend) \n    \ntrain_ds = MyDataset(data1 = X_temporal_train,data2 = X_stable_train,labels = Y_train)\ntest_ds =MyDataset(data1 = X_temporal_test,data2 = X_stable_test,labels = Y_test)\ntrain_loader = torch.utils.data.DataLoader(train_ds,batch_size = 10,shuffle=False)\ntest_loader = torch.utils.data.DataLoader(test_ds,batch_size = 10,shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define Model\nclass Net(nn.Module):\n    def __init__(self):\n            super(Net, self).__init__()\n            self.lstm = nn.LSTM(2,32,1,dropout = 0.5)\n            \n            self.stable_full = nn.Linear(7,16)\n            nn.init.kaiming_normal_(self.stable_full.weight)\n            self.BN1 = nn.BatchNorm1d(16)\n            self.stable_dropout = nn.Dropout(0.5)\n            \n            self.merge_full = nn.Linear(16+13*32,64)# stable:（5*16）  lstm:（13，5，32）\n            nn.init.kaiming_normal_(self.merge_full.weight)\n            self.BN2 = nn.BatchNorm1d(64)\n            self.merge_dropout = nn.Dropout(0.3)\n            self.merge_full2 = nn.Linear(64,2)\n            nn.init.kaiming_normal_(self.merge_full2.weight)\n\n    def reset_hidden(self):\n        self.hidden = (torch.zeros(self.hidden[0].shape), torch.zeros(self.hidden[1].shape))\n        \n    def forward(self, x_trend,x_stable):\n        batch_size = x_trend.reshape(13,-1,2).size(1)\n        x_trend = x_trend.reshape(13,batch_size,2)\n        x_trend, self.hidden = self.lstm(x_trend)\n        \n        x_stable = self.stable_dropout(F.relu(self.BN1(self.stable_full(x_stable))))\n        \n        s, b, h = x_trend.shape  #(seq, batch, hidden)\n        x_trend = x_trend.view(b, s*h)\n        x_merge = torch.cat((x_trend,x_stable),axis = 1)\n        x_merge = F.relu(self.merge_full2(self.merge_dropout(F.relu(self.BN2(self.merge_full(x_merge))))))\n        return x_merge","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training Settings\nmodel = Net()\noptimizer = torch.optim.Adam(model.parameters(),lr=0.001)\ncriterion = nn.MSELoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training process\ndef train_model(epoch):\n    model.train()\n    for batch_idx, (trend, stable, target) in enumerate(train_loader):\n        trend, stable, target = Variable(trend), Variable(stable),Variable(target)\n        optimizer.zero_grad()\n        output = model(trend,stable)\n        loss = criterion(output, target)  \n        loss.backward()\n        optimizer.step()\n        if batch_idx % 300 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(trend), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.data))\n\ndef test_model(epoch):\n    model.eval()\n    test_loss = 0\n    y_pred = []\n    y_true = []\n    for trend, stable, target in test_loader:\n        trend,stable, target = Variable(trend),Variable(stable),Variable(target)\n        output = model(trend,stable)\n        test_loss += criterion(output, target).data\n        y_pred.append(output)\n        y_true.append(target)\n\n    y_pred = torch.cat(y_pred, dim=0)\n    y_true = torch.cat(y_true, dim=0)\n    test_loss = test_loss\n    test_loss /= len(test_loader) # loss function already averages over batch size\n    MSE = mean_squared_error(y_true.detach().numpy(), y_pred.detach().numpy())\n    print('\\nTest set: Average loss: {:.4f}, MSE: {} \\n'.format(\n        test_loss, MSE \n        ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(1, 21):\n    train_model(epoch)\n    test_model(epoch)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make prediction base on model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# In order to use query function,transform datetime to string\ndef get_str_date(x):\n    x = str(x)[0:10]\n    return x\n\nscale_train.Date = scale_train.Date.apply(lambda x: get_str_date(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del scale_train['Id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read test_df and a new train_df\ntest_df = pd.read_csv('/kaggle/input/covid-with-weather-and-population/test_processed.csv')\ntrain_df2 =  pd.read_csv('/kaggle/input/covid19-with-population/update_train_processed.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = test_df.query(\"Date > '2020-04-25'\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# same preprocess as before\ntrain_df2.Country.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\ntrain_df2.Province.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\ntrain_df2.replace(np.inf,0,inplace=True)\ntrain_df2.UrbanPopRate = train_df2.UrbanPopRate.apply(lambda x:get_percent(x))\n\ntest_df.Country.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\ntest_df.Province.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\ntest_df.replace(np.inf,0,inplace=True)\ntest_df.UrbanPopRate = test_df.UrbanPopRate.apply(lambda x:get_percent(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make train dataframe and test dataframe have same columns\ntest_df['ConfirmedCases'] = np.NAN\ntest_df['Fatalities'] = np.NAN\ntest_df['Id_x'] = 0\ntest_df['Id_y'] = 0\ntest_df = test_df[list(scale_train.columns)]\n# merge scale_train and test\ntotal_df = pd.concat((scale_train,test_df),axis = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### prepare prediction input"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_conf_scaler(country,province):\n    train_df2_province = train_df2.query(f\"Country == '{country}' and Province =='{province}'\")\n    train_df2_province_conf = train_df2_province['ConfirmedCases']\n    train_df2_province_fata = train_df2_province['Fatalities']\n    province_conf_scaler = preprocessing.MinMaxScaler()\n    province_fata_scaler = preprocessing.MinMaxScaler()\n    province_conf_scaler.fit(np.array(train_df2_province_conf).reshape(-1,1))\n    province_fata_scaler.fit(np.array(train_df2_province_fata).reshape(-1,1))\n    return province_conf_scaler,province_fata_scaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_initial_input(country,province,start,end):\n    input_province = total_df.query(f\"Country =='{country}' and Province == '{province}' and Date>='{start}' and Date<='{end}'\")\n    input_list_province = []\n    #prepare all the trend inputs\n    infection_trend = [float(x) for x in input_province[:-1].ConfirmedCases.values]\n    fatality_trend = [float(x) for x in input_province[:-1].Fatalities.values]\n\n    #preparing all the stable inputs\n    ##date inputs\n    days_after_1stJan = float(input_province.iloc[-1].Days_After_1stJan)\n    dayofweek = float(input_province.iloc[-1].Dayofweek)\n    month = float(input_province.iloc[-1].Month)\n    day= float(input_province.iloc[-1].Day)\n    ##population inputs\n    'Population','Density', 'Land_Area', 'Migrants', 'MedAge', 'UrbanPopRate'\n    population = float(input_province.iloc[-1].Population)\n    density = float(input_province.iloc[-1].Density)\n    land_area = float(input_province.iloc[-1].Land_Area)\n    migrants = float(input_province.iloc[-1].Migrants)\n    medage = float(input_province.iloc[-1].MedAge)\n    urbanpoprate = float(input_province.iloc[-1].UrbanPopRate)\n    beds = float(input_province.iloc[-1].API_beds)\n\n    input_list_province.append({\"infection_trend\":infection_trend,\n                     \"fatality_trend\":fatality_trend,\n                     \"stable_inputs\":[population,density,land_area,migrants,medage,urbanpoprate,beds],})\n    input_df_province = pd.DataFrame(input_list_province)\n    input_df_province[\"temporal_inputs\"] = [np.asarray([input_df_province[\"infection_trend\"],input_df_province[\"fatality_trend\"]])]\n    \n    province_temporal_train = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in input_df_province[\"temporal_inputs\"].values]),(1,2,sequence_length)),(2,0,1) )).astype(np.float32)\n    province_stable_train = np.asarray([np.asarray(x) for x in input_df_province[\"stable_inputs\"]]).astype(np.float32)\n    province_temporal_train = torch.from_numpy(province_temporal_train)\n    province_stable_train = torch.from_numpy(province_stable_train)\n    return province_temporal_train,province_stable_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_pred(country,province,trend,stable):\n    conf_scaler,fata_scaler = get_conf_scaler(country,province)\n    output = model(trend,stable)\n    conf_output = conf_scaler.inverse_transform(output[0][0].detach().numpy().reshape(-1,1))\n    fata_output = fata_scaler.inverse_transform(output[0][1].detach().numpy().reshape(-1,1))\n    original_output = [conf_output,fata_output]\n    return output,original_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_pred_for_province(country,province):\n    start_date = datetime.strptime('2020-04-13','%Y-%m-%d')\n    end_date = datetime.strptime('2020-04-26','%Y-%m-%d')\n    pred = []\n    trend_input,stable_input = get_initial_input(country,province,str(start_date)[0:10],str(end_date)[0:10])\n    for i in range(0,19):\n        start = str(start_date+timedelta(days = i))[0:10]\n        end = str(end_date+timedelta(days = i))[0:10]\n        output,original_output = get_pred(country,province,trend_input,stable_input)\n        pred.append([end,original_output[0],original_output[1]])\n        trend_input = trend_input[1:]\n        output_tensor = torch.as_tensor(output)\n        new = torch.as_tensor(output_tensor.reshape(1,1,2))\n        trend_input = torch.cat((trend_input,new),0)\n    pred_for_province = pd.DataFrame(pred,columns=['Date','confirmed_pred','fata_pred'])\n    pred_for_province['Province'] = province\n    pred_for_province['Country'] = country\n    pred_for_province = pred_for_province[['Country','Province','Date','confirmed_pred','fata_pred']]\n    return pred_for_province","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_table = pd.DataFrame(columns = ['Country','Province','Date','confirmed_pred','fata_pred'])\nfor country in test_df.Country.unique():\n    for province in test_df.query(f\"Country == '{country}'\")['Province'].unique():\n        province_pred = get_pred_for_province(country,province)\n        pred_table = pd.concat((pred_table,province_pred),0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_table","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Merge result for submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"original_train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_dt(x):\n    x = datetime.strptime(x,'%Y-%m-%d')\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_train.Date = original_train.Date.apply(lambda x:get_dt(x))\noriginal_train.rename(columns = {'Province_State':'Province','Country_Region':'Country'},inplace = True)\nfor i in range(len(original_train)):\n    if original_train.Province[i] is np.NaN:\n        original_train.Province[i] = original_train.Country[i]\n        \nfor i in range(len(original_train)):\n    if original_train.Date[i]<datetime.strptime('2020-04-02','%Y-%m-%d') or original_train.Date[i]>datetime.strptime('2020-04-25','%Y-%m-%d'):\n        original_train.drop(i,inplace=True)\n        \ndel original_train['Id']\n#del original_train['Country']\n\noriginal_train.Province.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\n#del pred_table['Country']\noriginal_train.Date = original_train.Date.apply(lambda x:get_str_date(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_number(x):\n    return x[0][0]\npred_table.confirmed_pred = pred_table.confirmed_pred.apply(lambda x:get_number(x))\npred_table.fata_pred = pred_table.fata_pred.apply(lambda x:get_number(x))\n\npred_table.rename(columns = {'confirmed_pred':'ConfirmedCases','fata_pred':'Fatalities'},inplace = True)\npred_table.Country.replace('Cote d Ivoire','Cote d\\'Ivoire',inplace = True)\npred_table.Province.replace('Cote d Ivoire','Cote d\\'Ivoire',inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')\nfor i in range(len(original_test)):\n    if original_test.iloc[i]['Province_State'] is np.NaN:\n        original_test.iloc[i,1] = original_test.iloc[i,2]\n        \noriginal_test.rename(columns = {'Country_Region':'Country','Province_State':'Province'},inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final = pd.concat([pred_table,original_train],axis = 0,sort = True)\nfinal_submit = pd.merge(original_test,final,on = ['Country','Province','Date'],how = 'left')\nsubmission = final_submit[['ForecastId','ConfirmedCases','Fatalities']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('/kaggle/working/submission.csv',index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}