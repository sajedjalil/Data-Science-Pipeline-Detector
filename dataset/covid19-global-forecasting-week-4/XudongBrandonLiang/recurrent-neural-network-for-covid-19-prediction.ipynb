{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport datetime\nimport os\nimport glob\nimport pickle\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom IPython.display import Image\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Competition Overview\nhttps://www.kaggle.com/c/covid19-global-forecasting-week-4/overview\n"},{"metadata":{},"cell_type":"markdown","source":"# Part 1 Visualize and Understand the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read both training and test data as Pandas Dataframe to inspect the data\nforecasting_location = \"/kaggle/input/covid19-global-forecasting-week-4/\"\ntraining_file = forecasting_location + \"train.csv\"\ntest_file = forecasting_location + \"test.csv\"\ntraining_df = pd.read_csv(training_file)\ntest_df = pd.read_csv(test_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_df.info","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.info","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_countries = training_df[\"Country_Region\"].unique()\ntest_countries = test_df[\"Country_Region\"].unique()\nprint(\"Number of countries in Training File: {0}\".format(len(training_countries)))\nprint(\"Number of countries in Test File: {0}\".format(len(test_countries)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nThe provided training data (train.csv) and test data (test.csv) have the same 184 countries (field \"**Country_Region**\", where some countries have multiple states/provinces (field \"**Province_State**\"). For the countries that don't have data to the state/province level, their entries under the field \"**Province_State**\" are Null/blank.\n\nThe key metrics we are concerned about and want to learn and predict are \"**ConfirmedCases**\" and \"**Fatalities**\", since the core objective of this competition is to predict future **ConfirmedCases** and **Fatalities** by learning and understanding past **ConfirmedCases** and **Fatalities**.\n\nThe training data have data for each of these (\"**Country_Region**\", \"**Province_State**\") combination from 01-22-2020 to 04-29-2020 (99 days). Here are the top 20 countries by count of **Province_State**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distritbution of Entries (# of Occurrences) by country in training file (Top Countries)\ndef top_countries_by_entries(df, title, top_k = 20):\n    df_country_groupby_size = df.groupby(\"Country_Region\").count().rename(columns = {\"Date\": \"Count\"})[[\"Count\"]].sort_values(by = [\"Count\"], ascending = False)\n    df_country_groupby_size.head(top_k).plot.bar(title = title)\n    \ntop_countries_by_entries(training_df, \"Top 20 Countries in Training File\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize the trend of **ConfirmedCases** and **Fatalities** in some countries that have suffered the most from COVID-19"},{"metadata":{"trusted":true},"cell_type":"code","source":"def filter_by_country(df, country):\n    return df[df[\"Country_Region\"] == country]\n\ntraining_df_usa = filter_by_country(training_df, \"US\")\ntraining_df_china = filter_by_country(training_df, \"China\")\ntraining_df_italy = filter_by_country(training_df, \"Italy\")\ntraining_df_spain = filter_by_country(training_df, \"Spain\")\ntraining_df_france = filter_by_country(training_df, \"France\")\ntraining_df_germany = filter_by_country(training_df, \"Germany\")\ntraining_df_iran = filter_by_country(training_df, \"Iran\")\ntraining_df_turkey = filter_by_country(training_df, \"Turkey\")\ntraining_df_japan = filter_by_country(training_df, \"Japan\")\ntraining_df_singapore = filter_by_country(training_df, \"Singapore\")\ntraining_df_australia = filter_by_country(training_df, \"Australia\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def trend_visualization_per_country(df_country, country, ms = 4):\n    df_country = df_country.sort_values(by = [\"Date\"])\n    \n    # need to sum numbers of all provinces/states together for each date\n    df_country = df_country.groupby(\"Date\")[\"ConfirmedCases\", \"Fatalities\"].agg(\"sum\")\n    \n    dates = df_country.index\n    dates = [datetime.datetime(int(date[:4]), int(date[5:7]), int(date[8:10]), 0, 0) for date in dates]\n    confirmed_cases = df_country[\"ConfirmedCases\"]\n    fatalities = df_country[\"Fatalities\"]\n    \n    \n    fig, ax = plt.subplots(figsize=(20,8))\n    ax.plot_date(dates, confirmed_cases, ms = ms, label = \"Confirmed Cases\")\n    ax.plot_date(dates, fatalities, ms = ms, label = \"Fatalities\")\n    ax.legend()\n    ax.set_title(\"Confirmed Cases & Fatalities in {0}\".format(country))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trend in USA\ntrend_visualization_per_country(training_df_usa, \"USA\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trend in China\ntrend_visualization_per_country(training_df_china, \"China\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trend in Italy\ntrend_visualization_per_country(training_df_italy, \"Italy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trend in Spain\ntrend_visualization_per_country(training_df_spain, \"Spain\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trend in France\ntrend_visualization_per_country(training_df_france, \"France\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trend in Germany\ntrend_visualization_per_country(training_df_germany, \"Germany\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trend in Iran\ntrend_visualization_per_country(training_df_iran, \"Iran\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trend in Turkey\ntrend_visualization_per_country(training_df_turkey, \"Turkey\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trend in Japan\ntrend_visualization_per_country(training_df_germany, \"Japan\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trend in Singapore\ntrend_visualization_per_country(training_df_germany, \"Singapore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trend in Australia\ntrend_visualization_per_country(training_df_australia, \"Australia\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize the trends of these countries in one graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"# since there are 185 countries, I choose the following 11 countries\ncountries = [\"China\", \"US\", \"Italy\", \"Spain\", \"France\", \"Germany\", \"Iran\", \"Turkey\", \"Japan\", \"Singapore\", \"Australia\"]\nmetrics = [\"ConfirmedCases\", \"Fatalities\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def trend_visualization_per_metric(df, metrics, countries, ms = 1):\n    df = df.sort_values(by = [\"Date\"])\n    \n    for metric in metrics:\n        fig, ax = plt.subplots(figsize=(20,8))\n        \n        for country in countries:\n            df_country = df[df[\"Country_Region\"] == country]\n        \n            # need sum all provinces together for each date\n            df_country = df_country.groupby(\"Date\")[\"ConfirmedCases\", \"Fatalities\"].agg(\"sum\")\n    \n            #print(df_country.tail())\n    \n            dates = df_country.index\n            dates = [datetime.datetime(int(date[:4]), int(date[5:7]), int(date[8:10]), 0, 0) for date in dates]\n            values = df_country[metric]\n    \n            ax.plot(dates, values, ms = ms, label = country)\n            \n        ax.legend()\n        ax.set_title(\"Trend of {0}\".format(metric))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trend_visualization_per_metric(training_df, metrics, countries)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 2 Build a Recurrent Neural Network Encoder-Decoder model to learn the data"},{"metadata":{},"cell_type":"markdown","source":"# Partition Data"},{"metadata":{},"cell_type":"markdown","source":"In the context of this COVID-19 dataset, each (\"**Country_Region**\", \"**Province_State**\") combination acts as a base unit, because each location's COVID-19 development is unique and independent of each other. In other words, we need to first partition the training data by each (\"**Country_Region**\", \"**Province_State**\") combination to the location level and model the COVID-19 cases in each location independently."},{"metadata":{"trusted":true},"cell_type":"code","source":"# construct a country-to-state dictionary\nall_countries = training_df[\"Country_Region\"].unique().tolist()\nall_locations = {}\nfor country in all_countries:\n    df_country = training_df[training_df[\"Country_Region\"] == country]\n    all_states = df_country[\"Province_State\"].unique().tolist()\n    for state in all_states:\n        if country in all_locations:\n            tmp = all_locations[country]\n            tmp.append(str(state))\n            all_locations[country] = tmp\n        else:\n            all_locations[country] = [str(state)]\n            \n# partition training set by each unique (Country_Region, Province_State)\nfor country in all_locations:\n    os.system(\"mkdir -p \\\"partition/{0}\\\"\".format(country))\n    states = all_locations[country]\n    country_df = training_df[training_df[\"Country_Region\"] == country]\n    for state in states:\n        location = \"partition/{0}/{1}.csv\".format(country, state)\n        if state != \"nan\":\n            location_df = country_df[country_df[\"Province_State\"] == state]\n            location_df.to_csv(location, index = False)\n        else:\n            country_df.to_csv(location, index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Develop Model - Brain Storm\nNow, we develop a model to learn and understand the development of **ConfimedCases** and **Fatalities** for each (**Country_Region**, **Province_State**) location.\n\nFor each state, we have data from 01-22-2020 to 04-29-2020 in 2 dimensions: **ConfimedCases** and **Fatalities**. The numbers of **ConfirmedCases** and **Fatalities** depend on the numbers in the preceding days only. So our model is essentially a time series forecasting model. Moreover, other than these 2 metrics, we have no other information/features. Therefore, for each state, we are developing 2 separate models that takes in the **accumulation** of the numbers of **ConfirmedCases** (model 1) or **Fatalities** (model 2) up to a given date as input and predict the number of **ConfirmedCases** (model 1) or **Fatalities** (model 2) for the next day.\n\nIn other words, for each model, the only features are\n* values of **ConfirmedCases** or **Fatalities** (one day at a time)\n* the underlying order of inputs - which is discussed below\n\n(A further study could be to link these 2 metrics and models together into one model based on their intrinsic connections, i.e.: how number of confirmed cases affects mortality rate and vice versa.)\n\n# A Recurrent Neural Network Approach\n\n# Motivation\n\nAs mentioned above, this dataset and challenge is essentially a time series data since we need preserve the underlying chronological order of inputs. **Recurrent Neural Network** is a classic approach to model time series data for prediction and the model I deploy here is inspired by the **Transformer-based Sequence-to-Sequence Encoder-Decoder** architecture (of which common applications include Machine Translation, Language Generation, Q&A, etc.). This is a pretty good read and illustration of this architecture to get yourself familiarized: http://jalammar.github.io/illustrated-transformer/.\n\nThe Transformer-based Seq-to-Seq Encoder-Decoder architecture is commonly used in NLP as texts are considered as sequences of characters, words, sentences of paragraphs (the granularity is arbitrary, though most applications use text as sequences of words) that preserve temporal relationships. In other words, words need to be parsed in order to preserve the meaning and semantics of the entire text. The same encoder takes in each word as input sequentially and encodes the accumulation of these inputs as one single encoded vector, which serves as an input to the decoder to produce predictions.\n\n# Architecture\nSimilarly, in this problem, the numbers of **ConfirmedCases** and **Fatalities** need to be parsed **Sequentially** into our model to mimic the effect of **accumulation** in real world, because every day's new development depends heavily and closely on all historic developments (maybe not uniformly, which I will cover in the choice of base RNN unit and loss calculation). The encoder and the decoder are constructed using base RNN unit as they are the core mechanism to accumulate and decode historical data. The encoder will sequentially **accumulate** the numbers from day 1 to a specific date into an **encoded vector** (the preserves historical information) and pass it to the decoder as input to predict the number of the next day. In training, this process will iterate (n-1) times, where n stands for the number of training days; in the i-th iteration, the encoder will **accumulate** the numbers from day 1 to the i-th day into an encoded vector and have the decoder use the encoded vector to predict the number on the (i+1)-th day, which will be compared to the actual number on the (i+1)-th day to compute the loss for that iteration. Essentially, in each epoch, we are training both the encoder and the decoder.\n\nHere is a graph of this architecture: "},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"../input/images/architecture.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Procedure for Prediction/Testing & Sequence-to-One\nIt's important to mention that, the model deployed for this dataset is not a Sequence-to-Sequence one, but rather a **Sequence-to-One** model, because we are only predicting for one day at a time. (I will elaborate more in the last section about this.) Then, in testing, the idea is to use the **accumulated** encoded vector of the entire training period to predict the number for the very next day using the decoder, encode that predicted number on top of the **accumulated** encoded vector to predict the next day using the same decoder, and so on.\n"},{"metadata":{},"cell_type":"markdown","source":"# Implementation"},{"metadata":{},"cell_type":"markdown","source":"I use PyTorch to construct my encoder and decoder, as well as the training procedure.\n\nHere, for encoder and decoder, I choose GRU (Gated Recurrent Unit) as the base RNN unit due to its ability to fulfill Long Short-Term Memory but with less parameters and memory than LSTM. I expect GRU to learn which protion of history to forget and which to rely heavily on."},{"metadata":{"trusted":true},"cell_type":"code","source":"# define encoder & decoder\n\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size)\n        \n        # use glorot initialization to avoid loss plateauing\n        nn.init.xavier_normal_(self.linear.weight)\n\n    def forward(self, input_data, hidden):\n        # GRU requires 3-dimensional inputs\n        input_data_transformed = self.linear(input_data).view(-1, 1, self.hidden_size)\n        hidden = hidden.view(-1, 1, self.hidden_size)\n        \n        output, hidden = self.gru(input_data_transformed, hidden)\n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(1, self.hidden_size)\n\n    \nclass DecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size):\n        super(DecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.gru = nn.GRU(hidden_size, hidden_size)\n        self.out = nn.Linear(hidden_size, output_size)\n        \n        # use glorot initialization to avoid loss plateauing\n        nn.init.xavier_normal_(self.out.weight)\n\n    def forward(self, input_data, hidden):\n        # GRU requires 3-dimensional inputs\n        input_data = input_data.view(-1, 1, self.hidden_size)\n        hidden = hidden.view(-1, 1, self.hidden_size)\n        \n        output, hidden = self.gru(input_data, hidden)\n        output = F.relu(self.out(output))\n        return output, hidden","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we define the training function. Please refer to the in-line comments for a detailed documentation. The ideas have already been covered above."},{"metadata":{"trusted":true},"cell_type":"code","source":"# a function to train an encoder-decoder model for a given (Country_Region, Province_State)\n\n# Arguments:\n# - train_csv: the csv file for a (Country_Region, Province_State)\n# - epochs: number of epochs to train\n# - frequency: how often (in terms of epochs) to plot the predicted values vs true numbers\n# - which_feature: 0 means ConfirmedCases, 1 means Fatalities\n# - val_frac: fraction of the training data as validation data\n# - hidden_size: the hidden size for the encoder and the decoder\n# - lr: learning rate for the SGD optimizer\n# - remove_zero: whether to remove 0 values in the training data\n# - normalize: whether to z-score normalize the training data\n# - log: a log file location\n\n# Return:\n# - True: if the model performance plateaus, so the client code can retry\n# - False: the model performance doesn't plateau, so the client code can proceed to the next model\n\ndef train_encoder_decoder(train_csv, epochs = 10, frequency = 5, which_feature = 0, val_frac = 0.1, hidden_size = 20, lr = 1e-3, remove_zero = False, normalize = False, log = \"encoder_decoder/training.log\"):\n    country = train_csv.split(\"/\")[-2]\n    state = train_csv.split(\"/\")[-1][:-4]\n    \n    if which_feature == 0:\n        feature = \"ConfirmedCases\"\n    else:\n        feature = \"Fatalities\"\n        \n    # serializing the models\n    encoder_location = \"encoder_decoder/{0}/{1}/{2}_encoder.mdl\".format(country, state, feature)\n    decoder_location = \"encoder_decoder/{0}/{1}/{2}_decoder.mdl\".format(country, state, feature)\n    \n    # serializing the encoder and decoder hidden states after training\n    decoder_hidden_location = \"encoder_decoder/{0}/{1}/{2}_decoder_hidden.p\".format(country, state, feature)\n    encoder_hidden_location = \"encoder_decoder/{0}/{1}/{2}_encoder_hidden.p\".format(country, state, feature)\n    \n    # serializing the mean and std-dev of the data for prediction, if normalize = True\n    stats_location = \"encoder_decoder/{0}/{1}/{2}_stats.p\".format(country, state, feature)\n    \n    # make directory for saving\n    os.system(\"mkdir -p \\\"encoder_decoder/{0}/{1}\\\"\".format(country, state))\n    \n    df = pd.read_csv(train_csv)\n    \n    x_mean = None\n    x_std = None\n    \n    x = df[feature].values\n    \n    if remove_zero:\n        x = x[x != 0]\n        \n    if normalize:\n        # z-score normalize\n        # normalize because we are interested in the relative trend, not the absolute numbers\n        # normalization turns out to be more effective than no normalization as the optimization is more sensitive and thus less stable to larger numbers\n        x_mean = x.mean()\n        x_std = x.std()\n        x = (x - x_mean) / x_std\n        \n    if (x_mean == 0) and (x_std == 0):\n        print(\"Skipping this feature because all data are 0\")\n        return\n        \n    # save normalization stats for prediction    \n    stats = {\"mean\": x_mean, \"std\": x_std}\n    with open(stats_location, 'wb') as fp:\n        pickle.dump(stats, fp, protocol=pickle.HIGHEST_PROTOCOL)\n\n    size = len(x)\n    \n    train_size = int(size * (1 - val_frac))\n    val_size = size - train_size\n    \n    x_train = x[: train_size]\n    x_val = x[train_size :]\n    \n    # initialize encoder and decoder\n    encoder = EncoderRNN(input_size = 1, hidden_size = hidden_size)\n    decoder = DecoderRNN(hidden_size = hidden_size, output_size = 1)\n    \n    # use mean squared error as loss\n    criterion = nn.MSELoss()\n    \n    # optimizers\n    encoder_optimizer = optim.SGD(encoder.parameters(), lr = lr)\n    decoder_optimizer = optim.SGD(decoder.parameters(), lr = lr)\n    \n    losses = []\n    plateau = None\n    \n    for epoch in range(epochs):\n        # zero the gradients of the optimizers\n        encoder_optimizer.zero_grad()\n        decoder_optimizer.zero_grad()\n    \n        # loss\n        loss = 0\n        val_loss = 0\n        \n        # get an initial hidden state (0 vector) for the encoder at the beginning of each epoch\n        encoder_hidden = encoder.initHidden()\n        \n        # for storing the predictions\n        decoder_outputs_train = torch.zeros(train_size - 1)\n        decoder_outputs_val = torch.zeros(val_size)\n    \n        # training mode\n        encoder.train()\n        decoder.train()\n        for index, target in enumerate(x_train[1:]):\n            \n            # encode the input (previous day's number) into the encoded vector\n            encoder_output, encoder_hidden = encoder(torch.tensor(x[index - 1]).view(1, 1), encoder_hidden)\n            \n            # create an artifact (0 vector) for the decoder - since we don't have other input features\n            decoder_input = torch.zeros(1, encoder.hidden_size)\n    \n            # set the decoder hidden state to the final encoder hidden state (encoded vector) (accumulated over all inputs up till this point)\n            decoder_hidden = encoder_hidden\n    \n            # pass through decoder\n            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n            \n            decoder_output = decoder_output.squeeze()\n    \n            loss += criterion(decoder_output, torch.tensor(target)) * (index + 1)\n            # more weights/emphasis for recent dates\n            # this is the key for emphasizing on the most recent developments\n        \n            decoder_outputs_train[index] = decoder_output\n        \n        loss /= (train_size - 1) # since we didn't encoder the number from the last day\n        \n        losses.append(loss.detach().numpy().tolist())\n        \n        if (epoch + 1) % frequency == 0:\n            \n            plateau = np.array(losses).std()\n            \n            # the loss of the model depends heavily on weight initialization, sometimes the loss never improves and the prediction curve is flat,\n            # in which case we need to start over\n            if (plateau <= 1e-4):\n                print(\"Loss Standard Deviation for the last {} Training Epochs is {}. The model performance plateaus, will retry!\".format(frequency, plateau))\n\n                # lets the client code know that this model performance plateaus, so it can retry\n                return True\n            \n            # print training loss at this epoch\n            print(\"Epoch {0} - Training Loss: {1}\".format(epoch + 1, loss))\n            \n            # plot prediction vs true value\n            plot_pred_vs_true(decoder_outputs_train.squeeze(), torch.tensor(x_train[1:]).squeeze(), epoch, title = \"encoder_decoder/{0}/{1}/{2}_prediction_vs_target_epoch_{3}_training.jpg\".format(country, state, feature, epoch + 1), country = country, state = state, feature = feature)\n            \n            losses = []\n        \n        # optimize\n        loss.backward()\n        encoder_optimizer.step()\n        decoder_optimizer.step()\n        \n        # eval mode\n        encoder.eval()\n        decoder.eval()\n        \n        # using the last day's number in training as the starting input for validation\n        previous_number = x_train[-1]\n        \n        for index, target in enumerate(x_val):\n            \n            # encode the input into the encoded vector\n            encoder_output, encoder_hidden = encoder(torch.tensor(previous_number).view(1, 1), encoder_hidden)\n            \n            # create an artifact (0 vector) for the decoder - since we don't have other input features\n            decoder_input = torch.zeros(1, encoder.hidden_size)\n    \n            # set the decoder hidden state to the final encoder hidden state (encoded vector) (accumulated over all inputs up till this point)\n            decoder_hidden = encoder_hidden\n    \n            # pass through decoder\n            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n            \n            decoder_output = decoder_output.squeeze()\n    \n            val_loss += criterion(decoder_output, torch.tensor(target))\n        \n            decoder_outputs_val[index] = decoder_output\n            \n            # update the input to always be the previous day's number\n            previous_number = target\n        \n        val_loss /= val_size\n        \n        if (epoch + 1) % frequency == 0:\n            print(\"Epoch {0} - Validation Loss: {1}\".format(epoch + 1, val_loss))\n            # plot prediction vs true value\n            plot_pred_vs_true(decoder_outputs_val.squeeze(), torch.tensor(x_val).squeeze(), epoch, title = \"encoder_decoder/{0}/{1}/{2}_prediction_vs_target_epoch_{3}_val.jpg\".format(country, state, feature, epoch + 1), country = country, state = state, mode = \"Validation\", feature = feature)\n    \n    with open(decoder_hidden_location, 'wb') as fp:\n        pickle.dump(decoder_hidden, fp, protocol=pickle.HIGHEST_PROTOCOL)\n            \n    # save encoded vector for testing/prediction\n    with open(encoder_hidden_location, 'wb') as fp:\n        pickle.dump(encoder_hidden, fp, protocol=pickle.HIGHEST_PROTOCOL)\n        \n    if (epoch + 1 == epochs):\n        if (state != \"nan\"):\n            os.system(\"echo \\\"Training on {} for {} {}\\\" >> {}\".format(feature, country, state, log))\n        else:\n            os.system(\"echo \\\"Training on {} for {}\\\" >> {}\".format(feature, country, log))\n        os.system(\"echo \\\"Training Loss for Epoch {}: {}\\\" >> {}\".format(epoch + 1, loss, log))\n        os.system(\"echo \\\"Validation Loss for Epoch {}: {}\\\" >> {}\".format(epoch + 1, val_loss, log))\n        \n    torch.save(encoder, encoder_location)\n    torch.save(decoder, decoder_location)\n    \n    os.system(\"echo >> {}\".format(log))\n    \n    # reaches the end, so the model performance doesn't plateau and the client code can proceed to the next model\n    return False\n    \n# a function to plot the predicted numbers vs the real numbers in an given epoch during training\ndef plot_pred_vs_true(pred, true, epoch, title, country, state, feature = \"ConfirmedCases\", mode = \"Training\", ms = 5):\n    pred = pred.squeeze().detach().numpy()\n    true = true.squeeze()\n    size = len(pred)\n    x = list(range(size))\n    fig, ax = plt.subplots(figsize=(10,7))\n    \n    ax.plot(x, pred, ms = ms, label = \"prediction\")\n    ax.plot(x, true, ms = ms, label = \"true\")\n    \n    if (state == \"nan\"):\n        state = \"\"\n    else:\n        state = \", {}\".format(state)\n            \n    ax.legend()\n    ax.set_title(\"{} Epoch {} for {} in {}{}\".format(mode, epoch + 1, feature, country, state))\n    plt.savefig(title)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Key components in this training architecture:\n* I **normalize** all the data in the chosen feature per location for each model using **z-score normalization**, because the loss function (L2-Norm/MSE) is senstive to large numbers and would throw off the gradient computation. Furthermore, normalization is a always a good transformation/practice prior to pouring the data into neural network and we can always revert/un-normalize the data to recover the original/authentic numeric range (which will be used in actual testing/prediction). This is also why I save the mean and standard deviation of the data for each model.\n* In the loss calculation for each training date, I multiply each loss by the index position of that date in order to put proportionally more weight on losses based on the recentness/proximity to the prediction date, because we care more about the more recent developments, compared to the less recent developments.\n* In training, the decoder predicts the i-th day's number for the loss computation after encoding the previous (i-1) days into the encoder. In the next iteration, the encoder reads in the actual number for the i-th day to accumulate the encoded vector for the prediction of the (i+1)-th day's number.\n* Since it's a time series data, we need to make sure the validation data are **after** the training data, chronologically. That being said, the sampling of the validation data is essentially to take the last **val_frac** of the entire training data."},{"metadata":{},"cell_type":"markdown","source":"For display purpose, here I only show the training progress and results for a few specific locations instead of all locations. The locations I choose are some of the hardest hit states/countries in this pandemic."},{"metadata":{"trusted":true},"cell_type":"code","source":"# train encoder and decoder for these 4 (Country_Region, Province_State) locations\n\nlocations = [(\"US\", \"New York\"), (\"China\", \"Hubei\"), (\"US\", \"New Jersey\"), (\"Italy\", \"nan\")]\n\npath = \"/kaggle/working/partition\"\n\nfeatures = {0: \"ConfirmedCases\", 1: \"Fatalities\"}\n\nos.system(\"rm -fr encoder_decoder/\")\n\nfor (country, state) in locations:\n    if (state != \"nan\"):\n        print(\"Modeling for {}, {}\".format(country, state))\n    else:\n        print(\"Modeling for {}\".format(country))\n    csv_file = path + \"/{}/{}.csv\".format(country, state)\n    for which_feature in [0, 1]:\n        print(\">> Metric/Feature: {}\".format(features[which_feature]))\n        trial = 0\n        plateau = True\n        while plateau:\n            trial += 1\n            print(\">> >> Trial {}\".format(trial))\n            plateau = train_encoder_decoder(csv_file, lr = 1e-3, epochs = 100, frequency = 10, normalize = True, which_feature = which_feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Performance of the 4 models (by the end of training - 100 Epochs)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat encoder_decoder/training.log","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's worth mentioning that the relatively high training loss value for each model is because of poor predictions in the early stage of the development (as shown in the graph comparison) which is the least of our concern, and that we are multiply each individual date's training loss by the date index (to put more weight on the more recents dates).\n\n# Visualizing the model performances (by the end of training - 100 Epochs)\nHere are the graphs between predictions and true numbers for the final training and validation epoch for each model, where the x-axis is the number of days from the first training date and the y-axis is the normalized value of the feature (**ConfirmedCases** or **Fatalities**):"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"encoder_decoder/US/New York/ConfirmedCases_prediction_vs_target_epoch_100_training.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"encoder_decoder/US/New York/ConfirmedCases_prediction_vs_target_epoch_100_val.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"encoder_decoder/US/New York/Fatalities_prediction_vs_target_epoch_100_training.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"encoder_decoder/US/New York/Fatalities_prediction_vs_target_epoch_100_val.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"encoder_decoder/China/Hubei/ConfirmedCases_prediction_vs_target_epoch_100_training.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"encoder_decoder/China/Hubei/ConfirmedCases_prediction_vs_target_epoch_100_val.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"encoder_decoder/China/Hubei/Fatalities_prediction_vs_target_epoch_100_training.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"encoder_decoder/China/Hubei/Fatalities_prediction_vs_target_epoch_100_val.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"encoder_decoder/US/New Jersey/ConfirmedCases_prediction_vs_target_epoch_100_training.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"encoder_decoder/US/New Jersey/ConfirmedCases_prediction_vs_target_epoch_100_val.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"encoder_decoder/US/New Jersey/Fatalities_prediction_vs_target_epoch_100_training.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"encoder_decoder/US/New Jersey/Fatalities_prediction_vs_target_epoch_100_val.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"encoder_decoder/Italy/nan/ConfirmedCases_prediction_vs_target_epoch_100_training.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"encoder_decoder/Italy/nan/ConfirmedCases_prediction_vs_target_epoch_100_val.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"encoder_decoder/Italy/nan/Fatalities_prediction_vs_target_epoch_100_training.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"encoder_decoder/Italy/nan/Fatalities_prediction_vs_target_epoch_100_val.jpg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reflection based on training\nFrom the training output and graphs, we can see that each model does a terrific job adapting to the development trend in training by the end of the 100 training epochs (with minimal gaps toward the end as we focus the most on recent developments) and also a good job in validation as the validation loss decreases for every 10 epochs and the predicted trend follows closely the actual trend in validation by the end of the 100 training epochs.\n\nMoreover, from the graphs, we can also see that the margins between predicted trends and actual trends for both training and validation in each model keep shrinking as training progresses, which shows that the models are able to effectively and efficiently adapt to and learn the data in a comfortable pace, using the proposed RNN architecture and the given parameters.\n\nOf course there is still room for improvement, one being the volume of training data, since for each location/model, we have only 98 days of data (90% for training, 10% for validation)."},{"metadata":{},"cell_type":"markdown","source":"# Part 3 Testing/Prediction\n\nNow let's predict the future numbers for these 4 locations\n\nNote that we do not have the ground truth for the testing data (after 04-29-2020) so we can only use intuition to judge the prediction results."},{"metadata":{"trusted":true},"cell_type":"code","source":"locations = [(\"US\", \"New York\"), (\"China\", \"Hubei\"), (\"US\", \"New Jersey\"), (\"Italy\", \"nan\")]\n\ncsv_path = \"/kaggle/working/partition/{}/{}.csv\"\nencoder_decoder_path = \"/kaggle/working/encoder_decoder/{}/{}\"\n\nfeatures = {0: \"ConfirmedCases\", 1: \"Fatalities\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training data stops on 04-29-2020, so testing starts on 04-30-2020\n# but the last encoded date in training is actually 04-28-2020 (04-29-2020 is never encoded in training so we treat 04-29-2020 also as unseen data) so testing actually starts on 04-29-2020 and ends on 2020-05-14 (16 days)\n\nindex_to_date = {}\nfor index in range(2):\n    index_to_date[index] = \"2020-04-{}\".format(index + 29)\nfor index in range(2, 11):\n    index_to_date[index] = \"2020-05-0{}\".format(index - 1)\nfor index in range(11, 16):\n    index_to_date[index] = \"2020-05-{}\".format(index - 1)\nprint(index_to_date)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# configurations from training\nnormalize = True\nremove_zero = False\nfrequency = 10\nepochs = 100\nhidden_size = 20\n\nos.system(\"rm -fr prediction\")\n\n# recover the original/authentic numeric range\ndef unnormalize(x, mean, std):\n    return x * std + mean\n\nfor (country, state) in locations:\n    for feature in features:\n        feature = features[feature]\n        \n        training_csv = csv_path.format(country, state)\n    \n        df = pd.read_csv(training_csv)\n    \n        if (state != \"nan\"):\n            print(\"Predicting {} for {}, {}\".format(feature, country, state))\n        else:\n            print(\"Predicting {} for {}\".format(feature, country))\n    \n        encoder = encoder_decoder_path.format(country, state) + \"/{}_encoder.mdl\".format(feature)\n        decoder = encoder_decoder_path.format(country, state) + \"/{}_decoder.mdl\".format(feature)\n    \n        encoder_hidden = encoder.replace(\".mdl\", \"_hidden.p\")\n        stats = decoder.replace(\"_decoder.mdl\", \"_stats.p\")\n    \n        prediction_csv = \"prediction/{0}/{1}/{2}.csv\".format(country, state, feature)\n        graph_location = \"prediction/{0}/{1}/{2}.jpg\".format(country, state, feature)\n        os.system(\"mkdir -p \\\"prediction/{0}/{1}\\\"\".format(country, state))\n    \n        # load trained models from training\n        decoder = torch.load(decoder)\n        encoder = torch.load(encoder)\n    \n        # load encoded vector from training\n        with open(encoder_hidden, 'rb') as fp:\n            encoder_hidden = pickle.load(fp)\n        \n        # load mean and std-dev for un-normalization\n        with open(stats, 'rb') as fp:\n            stats = pickle.load(fp)\n        \n        # encode 04-29 data from training csv as it's never been encoded in training\n        if (state != \"nan\"):\n            encoder_input = df[(df[\"Country_Region\"] == country) & (df[\"Province_State\"] == state) & (df[\"Date\"] == \"2020-04-29\")][feature].values\n        else:\n            encoder_input = df[(df[\"Country_Region\"] == country) & (df[\"Province_State\"].isnull()) & (df[\"Date\"] == \"2020-04-29\")][feature].values\n        encoder_input = torch.tensor(encoder_input).view(1, -1).float()\n    \n        encoder_output, encoder_hidden = encoder(encoder_input, encoder_hidden)\n        \n        decoder_hidden = encoder_hidden\n        \n        values = []\n    \n        for index in index_to_date:\n            if index == 0:\n                # 2020-04-29 already encoded above\n                continue\n        \n            # create a start-of-sequence tensor for the decoder\n            decoder_input = torch.zeros(1, hidden_size)\n    \n            # pass through decoder\n            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n        \n            # decoded/predicted outputs\n            values.append(decoder_output.squeeze().detach().numpy().tolist())\n        \n            # accumulate the history by encoding the predicted number\n            encoder_hidden = decoder_hidden\n            encoder_output, encoder_hidden = encoder(decoder_output, encoder_hidden)\n            decoder_hidden = encoder_hidden\n    \n        values = np.array(values)\n    \n        # un-normalize\n        if normalize:\n            values = unnormalize(values, stats[\"mean\"], stats[\"std\"])\n    \n        # plot prediction and save graph\n        fig, ax = plt.subplots(figsize=(10,7))\n    \n        dates = [datetime.datetime(int(date[:4]), int(date[5:7]), int(date[8:10]), 0, 0) for date in list(index_to_date.values())[1:]]   \n        ax.plot_date(dates, values, ms = 5)\n        if (state != \"nan\"):\n            ax.set_title(\"Predicting {} for {}, {}\".format(feature, country, state))\n        else:\n            ax.set_title(\"Predicting {} for {}\".format(feature, country))\n        plt.savefig(graph_location)\n        #plt.show()\n        plt.close()\n    \n        df = pd.DataFrame(list(index_to_date.values())[1:], columns = [\"Date\"]) # skip the 1st day (04-29-2020) since it's afterall in the training data\n        df[\"Province_State\"] = state\n        df[\"Country_Region\"] = country\n        df[feature] = values\n    \n        df.to_csv(prediction_csv, index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here are the predictions for each location"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"prediction/US/New York/ConfirmedCases.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"prediction/US/New York/Fatalities.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"prediction/China/Hubei/ConfirmedCases.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"prediction/China/Hubei/Fatalities.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"prediction/US/New Jersey/ConfirmedCases.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"prediction/US/New Jersey/Fatalities.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"prediction/Italy/nan/ConfirmedCases.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"prediction/Italy/nan/Fatalities.jpg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction Summary\nWe can see that almost every prediction identically displays the same trend that mirrors a reciprocal function, essentially predicting that both numbers will drop rapidly in the short run and eventually converge. However, in reality, we know that can not be true because the confirmed cases and fatalities don't just drop so quickly in a short span of time. Therefore, the predictions are not reliable at all."},{"metadata":{},"cell_type":"markdown","source":"# Why it doesn't work well in prediction & More Reflection\n\nSo this leads me to thinking what went wrong and how to improve that.\n\nA key issue is that the model we are training is a **Sequence-to-One** transformer, where it uses all historical data to predict the number for just the next day. More detailedly, in training validation, I use up till the i-th day's real numbers as the input to predict just the (i+1)-th day's number and in the next iteratoin, then use up till the (i+1)-th day's real numbers (instead of my predicted number for the (i+1)-th day) to predict just the (i+2)-th day's number, and so on. This is why it works well in training and validation as we have sufficient historical data for the next-day prediction and the accumulation of history keeps enriching the model incrementally.\n\nHowever, in real-world testing, we are expected to perform a **Sequence-to-Sequence** transformer where it uses all historical data to predict an extended period of time into the future **all at once**. That is, we don't know the ground truth for the entirety after the last day in the training data. When predicting the very first day in testing, the procedure follows that of validation (and should perform well). But, starting from the second day in testing, I can only use my predicted number from the first testing date (along with everything the model has learned from the entirety of the training dates, which is encoded in the encoded vector) to predict the second day, and so on. In other words, my only knowledge for the preceding testing days' numbers comes from my prediction, a collection of inference rather than facts. Because this is a time series forecasting that takes input sequentially and accumulates information chronologically, a small deviation in the early stage of testing may grow drastically or exponentially later in testing. In other words, a small poor prediction early on may snowball into a much more misleading prediction later in the testing stage. And unfortunately, we have no control over that since we can't foresee the future.\n\n# Alternative Approach\n\nA modified/alternative approach would be to train such Sequence-to-Sequence Encoder-Decoder transformer. Frankly, I also tried that. The idea was to define a variable **days_to_predict** to be the number of days to predict from the testing set and train each batch of training data consisting of a variable-sized number of feature values as the input sequence to the model and (days_to_predict) number of feature values as the output sequence to the model for evaluation. Unfortunately, the training performance is nowhere close to that of this model. I welcome any suggestion and advice on how we can approach this to build a reliable **Sequence-to-Sequence Encoder-Decoder transformer** for time series forecasting."},{"metadata":{},"cell_type":"markdown","source":"# Conclusion & Future Work\nI propose and experiment an adventurous approach to learn and predict time series data of COVID-19 using a **Transformer-based Recurrent Neural Network Encoder-Decoder Sequence-to-One** model. The training and validation processes yield good performance but the prediction (testing) does not seem reliable (based on real-world understanding of COVID-19). Part of the reason is that the real-world forecasting of COVID-19 challenges is in nature a **Sequence-to-Sequence** prediction, rather than **Sequence-to-One**. Future work involves building a reliable **Sequence-to-Sequence Transformed-based RNN Encoder-Decoder** architecture for such time series forecasting."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}