{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID19 Global Forecasting"},{"metadata":{},"cell_type":"markdown","source":"## Import libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os, traceback\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom scipy.optimize import curve_fit\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.svm import LinearSVR\nfrom sklearn.utils import shuffle\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pre processing\n### Load files"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        path = os.path.join(dirname, filename)\n        if 'train' in filename:\n            train = pd.read_csv(path)\n        elif 'test' in filename:\n            test = pd.read_csv(path)\n\n# Scale X\nscaler = StandardScaler()\n            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check df"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train.sample(frac=1).head(5)\ntest.sample(frac=1).head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocess DFs"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rename columns\ntrain.rename(columns={'Country_Region':'Country', \n                         'Province_State': 'State'},\n                inplace=True)\ntest.rename(columns={'Country_Region':'Country', \n                         'Province_State': 'State'},\n                inplace=True)\n\n# Convert date\ntrain['Date'] = pd.to_datetime(train['Date'], infer_datetime_format=True)\ntest['Date'] = pd.to_datetime(test['Date'], infer_datetime_format=True)\nsubmission = pd.DataFrame(columns=['ForecastId','ConfirmedCases','Fatalities'], dtype='int32')\n\n# Concat both DFs\ndf = pd.concat([train,test],sort=False)\n\n# Fill N/A for one which missing\ndf['Country'] = df.Country.fillna(\"None\")\ndf['State'] = df.State.fillna(\"None\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualization\nHere, I define the function to plot the final result. It shows top 10 countries' trend of fatalities."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_data(train, test=None, submission=None):\n    past_n_day = 30\n    top_ctry_num = 10\n\n    if test is not None:\n        train_n = len(train); test_n = len(test)\n        # Merge test + submission\n        df = pd.merge(test, submission, on='ForecastId')\n        # Adjust ID\n        df['Id'] = pd.Series(range(train_n+1, train_n+test_n+1)).astype(int)\n        # Concat. train + (test + subm.)\n        df = pd.concat([train,df],sort=True)\n        df = df.drop_duplicates(subset=['Date','Country','State'],keep='first')\n    else:\n        df = train\n\n    # First/Last date in the DF\n    first_date = train.Date.min(); last_date = train.Date.max()\n    _,first_ctry = list(df.groupby('Country'))[0]\n    last_idx = np.where(first_ctry['Date'] == last_date)[0][0]\n\n    # Find countries with most fatalities\n    ctry = df.groupby(['Country','Date']).sum()\n    top_ctry = ctry.groupby(['Country'])\\\n                      .sum()\\\n                      .sort_values('Fatalities', ascending=False)[:top_ctry_num]['Fatalities']\n\n    # Plot settings\n    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(8,6))\n    fig.add_subplot(111, frameon=False)\n    plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n    plt.grid(False)\n    fig.suptitle(\"\\nFatality Growth\")\n    plt.xlabel(\"Day Since {}\".format(str(first_date).split(' ')[0]))\n    plt.ylabel(\"Number of Fatalities\",labelpad=30)\n    ax2.set_yscale(\"log\")\n    ax1.grid(which=\"both\", alpha=0.75, linestyle='dashed', linewidth=0.5)\n    ax2.grid(which=\"both\", alpha=0.75, linestyle='dashed', linewidth=0.5)\n\n    labels = {}\n    for idx,(name, country) in enumerate(ctry.groupby('Country')):\n        #print(country)\n        if name not in top_ctry.index.values:\n            continue\n        y = country.Fatalities.values\n        x = range(0,len(y))\n        #print(y)\n        l = ax1.plot(x,y,'-o',label=name,\n                 linewidth=2, markersize=3,markevery=7)\n        ax2.plot(x,y,'-o',label=name,\n                 linewidth=2, markersize=3,markevery=7)\n        labels[name] = l[0]\n\n    # Plot settings\n    fig.legend(list(labels.values()), list(labels.keys()),\n               loc='center right')\n    plt.subplots_adjust(right=0.77)\n    #print(last_date)\n    if test is not None:\n        ax1.axvline(x=last_idx, color='k',linewidth='1',linestyle='--')\n        ax2.axvline(x=last_idx, color='k',linewidth='1',linestyle='--')\n        \n    fig = plt.gcf()\n    fig.savefig(\"result.png\")\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Try out to see how does it look like."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bell-shaped regression\nHere, I define the model which fit with a normalized curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass BellModel():\n    def __init__(self,streak_threshold=7,power=10):\n        self.streak_threshold = streak_threshold\n        self.power = power\n    def __str__(self):\n        return \"BellModel(streak_threshold={},power={},\\npopt={})\".format(\n            self.streak_threshold,\n            self.power,\n            self.popt\n        )\n\n    def func(self,X,a,b,c):\n        result = a*np.exp(-(((X-b)**2) / (2*(c**2+0.1))))\n        return result.flatten()\n\n    def _find_streak(self, arr):\n        pos = np.clip(arr, 0, 1).astype(bool).cumsum()\n        neg = np.clip(arr, -1, 0).astype(bool).cumsum()\n        streaks = np.where(arr >= 0,\n                           pos-np.maximum.accumulate(np.where(arr <= 0, pos, 0)),\n                           -neg+np.maximum.accumulate(np.where(arr >= 0, neg, 0)))\n        return streaks\n\n    def fit(self,x,y):\n        self.streak_threshold = -self.streak_threshold\n        # First non-zero index\n        x = scaler.inverse_transform(x)\n        y = y.T.flatten(); x = x.T.flatten()\n        # Sort array\n        c = np.argsort(x[:]); y = y[c]\n        x = np.sort(x)\n\n        # Find the first index of population > 0\n        nonzero_index, *_ = np.where(np.sign(y).cumsum() == 1)\n\n        if not isinstance(nonzero_index,np.ndarray) and not nonzero_index:\n            # If no real value, a = 0.\n            self.popt = [0,0,0]; return\n        else:\n            try:\n                nonzero_index = nonzero_index.item(0)\n            except IndexError:\n                self.popt = [0,0,0]; return\n                \n        zeros, y_sp = y[:nonzero_index],y[nonzero_index:]\n\n        a = b = d = 0; c = 1\n        a_min = b_min = 0; c_min = 1\n\n        # The peak of the curve\n        a_min = np.max(y_sp)\n        a_max = a_min*self.power+1\n        # Slide length\n        d = len(zeros)\n\n        # First Derivative\n        y1 = np.diff(y_sp)\n        y1_streaks = self._find_streak(y1)\n        if self.streak_threshold in y1_streaks:\n            # The peak is over!\n            a = np.max(y_sp)\n        else:\n            b_min = len(y_sp) + d\n\n        # Second Derivative\n        y2 = np.diff(y1)\n        y2_streaks = self._find_streak(y2)\n        if self.streak_threshold in y2_streaks:\n            # Near half of the peak\n            b = np.where(y2_streaks==self.streak_threshold)[0][0] * 2\n            c = b/3\n        else:\n            # Not even near the half of the peak\n            # b_min: already assigned\n            c_min = len(y_sp)*2/3\n        a = a_min if a < a_min else a\n        b = 300 if b < b_min else b\n        c = 100 if c < c_min else c\n        import traceback\n        try: \n            self.popt, self.pconv = curve_fit(\n                self.func, x, y,\n                maxfev=100000,\n                check_finite=False,\n                p0=[a,b,c],\n                bounds=([a_min,b_min,c_min],[a_max,300.,100.])\n            )\n        except Exception as e:\n            traceback.print_exc()\n            self.popt = [0,0,0]\n        return self\n\n    def get_params(self, deep=False):\n            return { \n                'streak_threshold': self.streak_threshold,\n                'power': self.power,\n            }\n    def set_params(self, **params):\n        for param, value in params.items():\n            setattr(self, param, value)\n        return self\n\n    def get_estimated_params(self):\n        return [self.streak_threshold, self.power]\n\n    def set_curve_params(self, streak_threshold, power):\n        self.streak_threshold = streak_threshold\n        self.power = power\n        return self\n\n    def predict(self,x):\n        x = scaler.inverse_transform(x).T.flatten()\n        return self.func(x,*self.popt)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"##################################################\n#\n# Fit Model\n#\n##################################################\n\ndef model_fit(X_train, Y_train, X_test):\n    \"\"\"Create a model, find the best fit, predict, and ensemble to predict the best result.\"\"\"\n    X_train = scaler.fit_transform(X_train.reshape(-1,1))\n    X_test = scaler.transform(X_test.reshape(-1,1))\n    X_train_sh, Y_train_sh = shuffle(X_train,Y_train, random_state=0)\n    remove_neg = lambda x: np.rint(x).astype(int).clip(min=0)\n    metric = \"neg_mean_squared_error\"\n\n    ##################################################\n    # Bell-Shaped\n    ##################################################\n\n    param_grid = {\n        #'streak_threshold': [3,4,5,6,7],\n        'streak_threshold': [5,6,7,14,21],\n        'power': [2,3,5,10,20,30,50]\n    }\n    bell_grid = GridSearchCV(BellModel(),\n                             param_grid,\n                             cv=3,\n                             scoring=metric,\n                             verbose=0\n    )\n    bell_grid.fit(X_train_sh, Y_train_sh)\n    bell_model = bell_grid.best_estimator_\n    ##################################################\n    # Ridge\n    ##################################################\n    param_grid = {\n        'polynomialfeatures__degree': np.arange(1,5),\n        'ridge__alpha':[1e2, 1e3,1e4],\n        'ridge__fit_intercept': [True, False],\n        'ridge__normalize': [True, False],\n        'ridge__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg']\n    }\n    def PolynomialRidgeRegression(degree=2, **kwargs):\n        return make_pipeline(PolynomialFeatures(degree), Ridge(**kwargs))\n    \n    ridge_grid = GridSearchCV(PolynomialRidgeRegression(),\n                                 param_grid,\n                                 cv=3,\n                                 scoring=metric,\n                                 verbose=0 )\n    ridge_grid.fit(X_train_sh, Y_train_sh)\n    ridge_model = ridge_grid.best_estimator_\n    ##################################################\n    # SVR\n    ##################################################\n    param_grid = {\n        'polynomialfeatures__degree': np.arange(1,5),\n        'linearsvr__C' : np.logspace(0,1,5),\n        'linearsvr__epsilon' : np.logspace(-1,1,5),\n        'linearsvr__fit_intercept': [True, False],\n    }\n\n    def PolynomialSVRRegression(degree=2, **kwargs):\n        return make_pipeline(PolynomialFeatures(degree), LinearSVR(**kwargs,max_iter=10000))\n    \n    svr_grid = GridSearchCV(PolynomialSVRRegression(),\n                                 param_grid,\n                                 cv=3,\n                                 scoring=metric,\n                                 verbose=0 )\n    svr_grid.fit(X_train_sh, Y_train_sh)\n    svr_model = svr_grid.best_estimator_\n\n    ##################################################\n    # second feature matrix\n    X_train2 = pd.DataFrame( {'Bell': remove_neg(bell_model.predict(X_train)),\n                              'Ridge': remove_neg(ridge_model.predict(X_train)),\n                              'SVR': remove_neg(svr_model.predict(X_train)),\n    })\n    X_test2 = pd.DataFrame( { 'Bell': remove_neg(bell_model.predict(X_test)),\n                              'Ridge': remove_neg(ridge_model.predict(X_test)),\n                              'SVR': remove_neg(svr_model.predict(X_test)),\n    })\n\n\n    # second-feature modeling using linear regression\n    reg = LinearRegression()\n    reg.fit(X_train2, Y_train)\n\n    Y_test = reg.predict(X_test2)\n    Y_test = remove_neg(Y_test)\n\n    return Y_test\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The main section "},{"metadata":{"trusted":true},"cell_type":"code","source":"##################################################\n# Iterate along Country/State\n##################################################\n\nfor name,state in df.groupby(['Country','State']):\n    # Save Train/Test overrap\n    mask = state.duplicated(subset=['Date'],keep='first')\n    df_tmp = state.loc[~mask]\n    df_tmp_dropped = state.loc[mask]\n\n    # Duplicate number\n    drop_num = len(state) - len(df_tmp)\n\n    try:\n        df_tmp.insert(0,'Index', range(1,len(df_tmp)+1))\n    except:\n        pass\n\n    # Training Data\n    tmp_train = df_tmp.dropna(subset=['ConfirmedCases'])\n    X_tr = tmp_train['Index'].values\n\n    # Testing Data\n    tmp_test = df_tmp[ df_tmp['ConfirmedCases'].isna() ]\n    X_te = tmp_test.Index.values\n\n    for cat in ('ConfirmedCases', 'Fatalities'):\n        # Training Data\n        Y_tr = tmp_train[cat].values\n        Y_te = model_fit(X_tr, Y_tr, X_te)\n\n        # Save to DF\n        tmp_test.loc[:,cat] = Y_te\n        \n    # Merge train(last 13) + test\n    tmp_train = pd.merge(tmp_train.tail(drop_num)[['Date','ConfirmedCases','Fatalities']],\n                     df_tmp_dropped[['Date','ForecastId']],\n                     on='Date')\n    col = ['ForecastId','ConfirmedCases','Fatalities']\n    tmp_train = tmp_train[col]\n    tmp_test = tmp_test[col]\n    tmp = pd.concat([tmp_train,tmp_test]) \n    submission = pd.concat([submission, tmp])\n\nos.chdir(\"/kaggle/working/\")\nsubmission = submission.fillna(0)\nsubmission = submission.astype(int)\nsubmission.to_csv('submission.csv', index=False)\nplot_data(train,test,submission) \n ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}