{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **INTRODUCTION**\n\nIn this notebook I've used LSTM deep learning network in order to model Covid-19 spread across all countries.\nFor each country, fine tuning on the number of lags (time steps to be used for the predictions) has been performed.\nPredictions are computed as follows:\n\n* Public Leaderboard: data for training up to 2020-04-01.\n* Private Leaderboard: all data available for training-"},{"metadata":{"trusted":true},"cell_type":"code","source":"###\n### LIBRARIES\n###\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport random as rn\nimport datetime as dt\nimport time \nimport gc\n\n\n# matplotlib\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport matplotlib.dates as mdates\nfrom matplotlib import colors \n\n#seaborn\nimport seaborn as sns\n\nfrom scipy.optimize import curve_fit, fsolve\nimport tensorflow as tf\n\n#########################################################################################\n#In order to keep the results of deep learning models as much \n# reproducibles as possible\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\n# The below is necessary for starting Numpy generated random numbers\n# in a well-defined initial state.\nnp.random.seed(42)\n\n# The below is necessary for starting core Python generated random numbers\n# in a well-defined state.\nrn.seed(12345)\n\n# Force TensorFlow to use single thread.\n# Multiple threads are a potential source of non-reproducible results.\nsession_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n                              inter_op_parallelism_threads=1)\n\n# The below tf.random.set_seed() will make random number generation\n# in the TensorFlow backend have a well-defined initial state.\n# For further details, see:\ntf.random.set_seed(1234)\n\nsess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\ntf.compat.v1.keras.backend.set_session(sess)\n##########################################################################################\n\n\n# Deep learning library for LSTM\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import Bidirectional\nfrom keras.layers import Dropout\nfrom keras.layers import Concatenate\nfrom keras.layers import Masking\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras import initializers\nfrom keras import backend as K\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport itertools\nfrom copy import deepcopy\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###\n### FUNCTIONS\n###\n\n\n### DATA PREPARATION \n\ndef train_test_creation(timeseries, train_test_days_split = 9):\n\n    \"\"\"\n    This functions split timeseries in input as train and test data:\n    Input:\n        - timeseries: could be univariate or multivariate.\n        - train_test_days_split: days of test data.\n    \"\"\"     \n\n    n = len(timeseries)\n    train, test = timeseries[0:(n-train_test_days_split)], timeseries[(n-train_test_days_split):]\n    \n    return train, test\n\ndef scale(n_features, train, test = None):\n    \n    \"\"\"\n    Defines scale transform for data in input.\n    \"\"\" \n    \n    if test is not None:\n        # fit scaler\n        scaler = MinMaxScaler(feature_range=(0, 1))\n        train = train.reshape(-1, n_features)\n        test = test.reshape(-1, n_features)\n        scaler = scaler.fit(train)\n        # transform train\n        train_scaled = scaler.transform(train)\n        # transform test\n        test_scaled = scaler.transform(test)\n        return scaler, train_scaled, test_scaled\n    else:\n        # fit scaler\n        scaler = MinMaxScaler(feature_range=(0, 1))\n        train = train.reshape(-1, n_features)\n        scaler = scaler.fit(train)\n        # transform train\n        train_scaled = scaler.transform(train)\n        \n        return scaler, train_scaled\n    \n\ndef invert_scale(scaler, value):\n    \n    \"\"\"\n    Returns inverse scale transform for value in input.\n    \"\"\"     \n\n    array = np.array(value)\n    array = array.reshape(-1, 1)\n    inverted = scaler.inverse_transform(array)\n    return inverted\n       \n\ndef split_univariate(sequences, n_steps):\n    \n    \"\"\"\n    Transform univariate sequence to supervised sequence for LSTM.\n    \"\"\" \n    \n    n = len(sequences)\n    X, y = list(), list()\n    \n    for i in range(n - n_steps):\n        aux_x = sequences[i:(i+n_steps)]\n        aux_y = sequences[i+n_steps]\n        \n        X.append(aux_x)\n        y.append(aux_y)\n    \n    return np.array(X), np.array(y)\n\n\ndef prepare_data(data, train_test_days_split, n_steps, n_features, Normalization = True):\n\n    # creation of train and test set: small test set since we have little data in this case.\n    train, test = train_test_creation(data, train_test_days_split=train_test_days_split)\n    \n    if train_test_days_split != 0:\n        \n        # we have to create data to feed LSTM... \n        if Normalization:\n\n            # univariate time sequences\n            if n_features == 1:\n                \n                # normalization to [-1,1]\n                scaler, train_scaled, test_scaled = scale(n_features, train, test)\n                X_train, y_train = split_univariate(train_scaled, n_steps)\n                X_test, y_test = split_univariate(test_scaled, n_steps)\n                # ... and then reshape into correct dimension for the network [samples, time_steps, n_features]\n                X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],n_features))\n                X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1],n_features))\n\n            return X_train, y_train, X_test, y_test, scaler\n\n\n        else:\n\n            if n_features == 1:\n                \n                X_train, y_train = split_univariate(train, n_steps)\n                X_test, y_test = split_univariate(test, n_steps)\n                # ... and then reshape into correct dimension for the network [samples, time_steps, n_features]\n                X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],n_features))\n                X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1],n_features))              \n\n            return X_train, y_train, X_test, y_test\n    \n    ### ONLY TRAINING\n    else:\n        # we have to create data to feed LSTM... \n        if Normalization:\n\n            # univariate time sequences\n            if n_features == 1:\n                # normalization\n                scaler, train_scaled = scale(n_features, train)\n                X_train, y_train = split_univariate(train_scaled, n_steps)\n                # ... and then reshape into correct dimension for the network [samples, time_steps, n_features]\n                X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],n_features))\n\n            return X_train, y_train, scaler\n\n\n        else:\n\n            if n_features == 1:\n                X_train, y_train = split_univariate(train, n_steps)\n                # ... and then reshape into correct dimension for the network [samples, time_steps, n_features]\n                X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],n_features))\n\n            return X_train, y_train       \n        \n\n### FORECASTING\n\ndef forecast_point_by_point_multiple_LSTM(model, X_test_1, X_test_2, scaler_1, scaler_2, n_steps, n_features, window):\n\n    predictions_cases_new = list()\n    predictions_fatalities_new = list()\n    \n    # start sequence for total cases\n    start_sequence_1 = X_test_1[-n_steps:,]\n    print(n_steps)\n    print(scaler_1.inverse_transform(start_sequence_1))\n    # start sequence for total fatalities\n    start_sequence_2 = X_test_2[-n_steps:,]\n\n    for i in range(window):\n        \n        # reshape for predictions\n        start_sequence_1 = start_sequence_1.reshape(1,n_steps,1)\n        start_sequence_2 = start_sequence_2.reshape(1,n_steps,1)\n        \n        # predictions\n        pred_cases_scaled, pred_fatalities_scaled = model.predict([start_sequence_1,start_sequence_2])\n        \n        # update start sequences for next prediction: insert last prediction and slice it using n_steps\n        start_sequence_1 = np.insert(start_sequence_1,n_steps,pred_cases_scaled)\n        start_sequence_1 = start_sequence_1[-n_steps:]\n        start_sequence_2 = np.insert(start_sequence_2,n_steps,pred_fatalities_scaled)\n        start_sequence_2 = start_sequence_2[-n_steps:]\n        \n        # rescale predictions...\n        pred_cases = scaler_1.inverse_transform(pred_cases_scaled)[0,0]\n        pred_fatalities = scaler_2.inverse_transform(pred_fatalities_scaled)[0,0]\n        \n        # ... and append the results of predictions\n        predictions_cases_new.append(pred_cases)\n        predictions_fatalities_new.append(pred_fatalities)\n    \n    return np.array(predictions_cases_new), np.array(predictions_fatalities_new)\n\n\n### PLOTTING\n\ndef plot_forecasting_LSTM(df, country, variable, predictions):\n    \n    aux = df[df['IDRegion']==country][variable]\n    date = df[df['IDRegion']==country]['Date']\n    FMT = '%Y-%m-%d'\n    start_date_acquistion = dt.datetime.strptime(date.min(),FMT)\n    forecast_window = len(predictions)\n    end_date = dt.datetime.strptime(date.max(),FMT) + dt.timedelta(days=forecast_window + 1)\n\n    delta = end_date - start_date_acquistion       # as timedelta\n    times = list()\n    for i in range(delta.days + 1):\n        day = start_date_acquistion + dt.timedelta(days=i)\n        times.append(day)\n\n    fig, ax = plt.subplots(figsize = (8,6))\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n    ax.xaxis.set_major_locator(mdates.DayLocator(interval=5))\n\n    ax.plot(times[0:len(aux)], aux, '-ko', label='True', color = 'red')\n    ax.plot(times[len(aux):(len(aux) + len(predictions))], predictions, '-ko', label='Predicted', color = 'blue')\n    # position legend\n    ax.set_xlabel('Date')\n    ax.set_ylabel(variable)\n\n    ax.legend(loc='upper left')\n    \n    fig.autofmt_xdate()\n    fig.suptitle('{}'.format(country))\n    plt.show()\n\n### MODEL DEPLOYMENT    \n\n\ndef get_default_configuration(structure = 'classic',\n                               n_steps = 1,\n                               n_steps_out = None, \n                               n_features = 1,\n                               neurons = 1,\n                               activation_hidden = 'relu',\n                               activation_out = 'linear',\n                               loss_cases = 'mse',\n                               loss_fatalities = 'mse',\n                               optimizer = 'adam',\n                               metrics = 'RMSE',\n                               epochs = 2):\n    \n    \"\"\"\n    Defines default network configuration for LSTM\n    \"\"\" \n\n    defaults = {'structure': structure, \n                'n_steps': n_steps,\n                'n_steps_out': n_steps_out, \n                'n_features': n_features,\n                'neurons': neurons,\n                'activation_hidden': activation_hidden, \n                'activation_out' : activation_out,\n                'loss_cases': loss_cases,\n                'loss_fatalities': loss_fatalities,\n                'optimizer': optimizer, \n                'metrics': metrics, \n                'epochs': epochs}\n  \n    return defaults\n\ndef get_tuning_model(options):\n    \n    \"\"\"\n    Returns a dict with all possible configuration given by options in input.\n    \"\"\"     \n\n    keys = options.keys()\n    values = (options[key] for key in keys)\n    tuning_model = [dict(zip(keys, combination)) for combination in itertools.product(*values)]\n\n    return tuning_model\n\n\ndef run(cases, fatalities, train_test_days_split, param_dict, tuning_model = None, Validation = False):\n    \n    \"\"\"\n    This functions takes in input train and test data, along with the default configuration for the newtork and one dictionary:\n    tuning model. \n    Tuning model is a dictionary with alla possible configurations to run.\n    It works in the following way:\n        - Take the default configuration and substitute the configuration in input from the dictionary.\n        - run the model and save the metric.\n    \"\"\"\n    \n    list_of_models = list()\n    histories = list()\n\n    # Itero sul numero di configurazioni da testare\n    for i in range(len(tuning_model)):\n        param_dict_temp = deepcopy(param_dict)\n        dict_aux = deepcopy(tuning_model[i])\n\n        # modifico i parametri della configurazione di default\n        for parameter, options in dict_aux.items():\n            param_dict_temp[parameter] = options\n\n        # valuto modello e salvo metrica di accuratezza\n        n_steps = param_dict_temp['n_steps']\n        n_features = param_dict_temp['n_features']\n        \n        # check if needs log transformation\n        transformation = param_dict_temp['transformation']\n        \n        # verify if padding is needed...\n        if transformation == 'Log':\n            cases_log = np.log(cases[cases > 0])\n            fatalities_log= np.log(fatalities[fatalities > 0])\n            len_cases_log = len(cases_log)\n            len_fatalities_log = len(fatalities_log)\n            \n            max_len = np.max([len_cases_log, len_fatalities_log])\n            \n            # .. padding to have same length of sequences\n            Xpad = padding(max_len, cases_log, fatalities_log)\n            cases_final = Xpad[:,0]\n            fatalities_final = Xpad[:,1]\n        \n        elif len(cases)!= len(fatalities):\n\n            len_cases = len(cases)\n            len_fatalities = len(fatalities)\n            max_len = np.max([len_cases, len_fatalities])\n\n            # .. function for padding padding()\n            Xpad = padding(max_len, cases, fatalities)\n            cases_final = Xpad[:,0]\n            fatalities_final = Xpad[:,1]\n            \n        else:\n            \n            cases_final = cases\n            fatalities_final = fatalities\n        \n        # prepare data\n        X_train_1, y_train_1, X_test_1, y_test_1, _ =  prepare_data(cases_final, train_test_days_split = train_test_days_split, n_steps = n_steps, \n                                    n_features = n_features, Normalization = True)\n        X_train_2, y_train_2, X_test_2, y_test_2, _ =  prepare_data(fatalities_final, train_test_days_split = train_test_days_split, n_steps = n_steps, \n                                    n_features = n_features, Normalization = True)\n        \n        # update metrics\n        tuning_model[i]['metric'], model, history = evaluation(X_train_1, y_train_1, X_test_1, y_test_1,\n                                                      X_train_2, y_train_2, X_test_2, y_test_2,\n                                                      param_dict_temp, Validation = Validation)\n\n        list_of_models.append(model)\n        histories.append(history)\n        K.clear_session()\n\n    return pd.DataFrame(tuning_model), list_of_models, histories\n\n\n    \ndef evaluation(X_train_1, y_train_1, X_test_1, y_test_1, X_train_2, y_train_2, X_test_2, y_test_2, param_dict, Validation = False):\n    \n    \"\"\"\n    Builds a LSTM model using the given params.\n    \"\"\"\n    \n    aux = multiple_LSTM(structure = param_dict['structure'],\n                        n_steps = param_dict['n_steps'],\n                        n_features = param_dict['n_features'],\n                        neurons = param_dict['neurons'], \n                        activation_hidden = param_dict['activation_hidden'],\n                        activation_out = param_dict['activation_out'],\n                        loss_cases = param_dict['loss_cases'],\n                        loss_fatalities = param_dict['loss_fatalities'],\n                        optimizer = param_dict['optimizer'],\n                        metrics = param_dict['metrics'],\n                        epochs = param_dict['epochs'])\n    \n    aux.model_compile()\n    metric, history = aux.evaluate_model(X_train_1, y_train_1, X_test_1, y_test_1,X_train_2, y_train_2, X_test_2, y_test_2, Validation = Validation)\n    \n    ### save txt of the results\n    \n    return metric, aux, history\n\ndef overall_fine_tuning(data, default_model_configuration, options, train_test_days_split, forecasting_window,\n                          Public = True, Validation = False):\n    \n    \"\"\"\n    Fine tuning over a set of options and predictions with the best model found.\n    Input:\n        - data: train dataset.\n        - default_model_configuration: model with default parameters.\n        - options: dictionary with parameters for tuning.\n        - end_training_date: date to split dataset between training and validation\n        - transform: possible transformation to log (if so, padding is needed for sequences of cases and fatalities)\n        - train_test_days_split: derives from end_training_date.\n        - window: forecasting window\n    Output:\n        - list_of_countries: list of all countries analyzed.\n        - predictions_cases_new: \n        - predictions_fatalities_new:\n    \"\"\"\n    \n    print(\"### STARTING FINE TUNING ###\")\n    \n    # countries\n    countries = np.unique(data['IDRegion'])\n    \n    # dictionary of parameters for fine tuning\n    tuning_model = get_tuning_model(options)\n\n    # lists in output\n    list_of_countries = list()\n    predictions_cases_new = list()\n    predictions_fatalities_new = list()  \n\n    count = 1\n    total_time = 0\n    \n    for country in countries:\n        \n        print(\"--- {}) Starting with {}... ---\".format(count, country))\n        \n        t0 = time.time()\n        \n        # variables for LSTM network\n        cases = data[data['IDRegion']==country]['ConfirmedCases']\n        fatalities = data[data['IDRegion']==country]['Fatalities']\n        cases = np.array(cases)\n        fatalities = np.array(fatalities)\n        \n        # if there's too much zero in the sequences, splitting training also in validation could\n        # create more problem of convergence.\n        len_cases_greater_than_zero = len(cases[cases > 0])       \n        perc = round(len_cases_greater_than_zero/len(cases),2)\n        if Validation:\n            if (perc < 0.6):\n                Validation = False\n        \n        # run of fine tuning\n        df_results, list_of_models, list_of_histories = run(cases, fatalities, train_test_days_split,\n                                         default_model_configuration,\n                                         tuning_model=tuning_model,\n                                         Validation = Validation)\n        \n        # extract the index of top model\n        df_results = df_results.sort_values('metric')\n        index = df_results.index[df_results['metric'] == df_results['metric'].min()].tolist()\n        top_model = list_of_models[index[0]]\n        \n        \n        n_steps = df_results['n_steps'].values[0]\n        transformation = df_results['transformation'].values[0]\n        predictions_cases, predictions_fatalities = single_predictions(top_model = top_model,\n                                                                                     data = data,\n                                                                                     country = country,\n                                                                                     train_test_days_split = train_test_days_split,\n                                                                                     n_steps = n_steps,\n                                                                                     n_features = 1,\n                                                                                     window = forecasting_window,\n                                                                                     transform = transformation,\n                                                                                     Public = Public,\n                                                                                     Normalization = True)   \n        # collect the predictions\n        list_of_countries.append(country)\n        predictions_cases_new.append(predictions_cases)\n        predictions_fatalities_new.append(predictions_fatalities)\n\n        \n        print(\"--- {} in {} seconds ---\".format(country,round((time.time() - t0))))\n        \n        count = count + 1\n        total_time = total_time + round((time.time() - t0))\n        gc.collect()\n    \n    print('### FINE TUNING LASTS {} MINUTES'.format(round(total_time/60)))\n    \n    return list_of_countries, predictions_cases_new, predictions_fatalities_new\n\n\ndef single_predictions(top_model, data, country, train_test_days_split, n_steps, n_features,  window, transform = None, \n                         Public = True, Normalization = True):\n    \n    cases = data[data['IDRegion']==country]['ConfirmedCases']\n    fatalities = data[data['IDRegion']==country]['Fatalities']\n    cases = np.array(cases)\n    fatalities = np.array(fatalities)\n\n    # transformation to log...\n    if transform == 'Log':\n        \n        cases_log = np.log(cases[cases > 0])\n        fatalities_log= np.log(fatalities[fatalities > 0])\n        len_cases_log = len(cases_log)\n        len_fatalities_log = len(fatalities_log)\n        max_len = np.max([len_cases_log, len_fatalities_log])\n\n        # .. function for padding padding()\n        Xpad = padding(max_len, cases_log, fatalities_log)\n        cases = Xpad[:,0]\n        fatalities = Xpad[:,1]\n        \n    elif len(cases)!= len(fatalities):\n        \n        len_cases = len(cases)\n        len_fatalities = len(fatalities)\n        max_len = np.max([len_cases, len_fatalities])\n\n        # .. function for padding padding()\n        Xpad = padding(max_len, cases, fatalities)\n        cases = Xpad[:,0]\n        fatalities = Xpad[:,1]\n\n    train_cases, test_cases = train_test_creation(cases, train_test_days_split=train_test_days_split)\n    train_fatalities, test_fatalities = train_test_creation(fatalities, train_test_days_split=train_test_days_split)    \n    \n    scaler_1, train_cases_scaled, test_cases_scaled = scale(1,train_cases, test_cases)\n    scaler_2, train_fatalities_scaled, test_fatalities_scaled = scale(1,train_fatalities, test_fatalities)        \n\n    # new predictions\n    if Public:\n        predictions_cases_new, predictions_fatalities_new = forecast_point_by_point_multiple_LSTM(top_model.model, train_cases_scaled, \n                                                                                                  train_fatalities_scaled,\n                                                                                                  scaler_1, scaler_2,\n                                                                                                  n_steps, n_features, window)\n    else: \n        predictions_cases_new, predictions_fatalities_new = forecast_point_by_point_multiple_LSTM(top_model.model, test_cases_scaled, \n                                                                                                  test_fatalities_scaled,\n                                                                                                  scaler_1, scaler_2,\n                                                                                                  n_steps, n_features, window)\n        \n        \n    return predictions_cases_new, predictions_fatalities_new\n\ndef padding(max_len, cases, fatalities):\n    \n    Xpad = np.full((max_len, 2), fill_value=-10, dtype = 'float')\n    n_cases = len(cases)\n    n_fatalities = len(fatalities)\n    Xpad[-n_cases:,0] = cases\n    Xpad[-n_fatalities:,1] = fatalities\n    \n    return Xpad\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **MODEL ARCHITECTURE**\n\nIn the following class there's the implementation of the LSTM model.\nI've used functional API of Keras in order to write a multi-inputs and multi-outputs model. That is:\n\n1. **INPUT**:\n\n    * Temporal branch for total cases.\n    * Temporal branch for total fatalities.\n    \n\n2. **OUTPUT**:\n    \n    * Output for total cases.\n    * Output for total fatalities."},{"metadata":{"trusted":true},"cell_type":"code","source":"###\n### CLASSES\n###\n\n# LSTM Class with in input only time sequences that need to be predicted: i.e. total cases and fatalities\nclass multiple_LSTM():\n    \n    \"\"\" \n    PARAMETERS:\n        structure: string, which LSTM structure to use\n        n_steps: number of time steps used\n        n_steps_out: number of prediction time steps\n        n_features: number of features in input (1 if just univariate time series)\n        num_layers: int, number of hidden layers (excluding the input layer) \n        neurons: array of units/nodes in each layer\n        activation: str, activation function in all layers except output\n        loss: str, loss function\n        optimizer: str, optimizer\n        metrics: list of strings, metrics used\n        epochs: int, number of epochs to train for\n        batch_size: int, number of samples per batch\n    \"\"\"\n    \n    def __init__(self, structure, n_steps, n_features, \n                 neurons, activation_hidden, activation_out, loss_cases, loss_fatalities, optimizer, metrics,\n                 epochs):\n        \n        self.structure = structure\n        self.n_steps = n_steps\n        self.n_features = n_features\n        self.neurons = neurons\n        self.activation_hidden = activation_hidden\n        self.activation_out = activation_out\n        self.loss_cases = loss_cases\n        self.loss_fatalities = loss_fatalities\n        self.optimizer = optimizer\n        self.metrics = metrics\n        self.epochs = epochs\n        \n        \n    def model_compile(self):\n        \n        # call specific architecture base on kind of LSTM\n        if self.structure == 'classic':\n            self.classic_lstm()\n    \n        if self.structure == 'bidirectional':\n            self.bidirectional_lstm()\n            \n            \n    def evaluate_model(self, X_train_1, y_train_1, X_test_1, y_test_1, X_train_2, y_train_2, X_test_2, y_test_2, Validation = False):\n        \n        # Fit the model: fixed approach, fit once on the training data and then predict\n        # each new time step one at a time from the test data.\n        \n        if Validation:\n            history_callback = self.model.fit([X_train_1,X_train_2], [y_train_1,y_train_2],\\\n            epochs = self.epochs, validation_split = 0.1,\n                                              verbose = 0, shuffle = False)\n        else: \n            history_callback = self.model.fit([X_train_1,X_train_2], [y_train_1,y_train_2],\\\n            epochs = self.epochs, validation_data=([X_test_1, X_test_2],  [y_test_1, y_test_2]),\n                                              verbose = 0, shuffle = False)\n\n        # evaluate on test data.\n        if self.metrics == 'RMSE':\n            y_pred_cases, y_pred_fatalities = self.model.predict([X_test_1,X_test_2])\n            pred = np.array([y_pred_cases,y_pred_fatalities]).reshape(-1)\n            actual = np.array([y_test_1,y_test_2]).reshape(-1)\n            metric= np.sqrt(mean_squared_error(pred, actual))\n        elif self.metrics == 'RMSLE':\n            y_pred_cases, y_pred_fatalities = self.model.predict([X_test_1,X_test_2])\n            pred = np.array([y_pred_cases,y_pred_fatalities]).reshape(-1)\n            actual = np.array([y_test_1,y_test_2]).reshape(-1)\n            metric= np.sqrt(mean_squared_log_error(pred, actual,))                \n            \n            \n        return metric, history_callback\n        \n            \n    def classic_lstm(self):\n      \n        input_cases = Input(shape=(self.n_steps, self.n_features))\n        input_fatalities = Input(shape=(self.n_steps, self.n_features))\n        \n        #LSTM cases\n        MASKING_1_cases = Masking(mask_value=-10)(input_cases) # special value negative since with log transform in this case we take from >= 1\n        LSTM_1_cases = LSTM(self.neurons)(MASKING_1_cases)\n        DENSE_1_cases = Dense(self.neurons)(LSTM_1_cases)\n        DROPOUT_1_cases = Dropout(0.3)(DENSE_1_cases)\n        DENSE_final_cases = Dense(1, activation=self.activation_out, name = 'Cases')(DROPOUT_1_cases)\n        \n        #LSTM fatalities\n        MASKING_1_fatalities = Masking(mask_value=-10)(input_fatalities) # special value negative since with log transform in this case we take from >= 1\n        LSTM_1_fatalities = LSTM(self.neurons)(MASKING_1_fatalities)\n        DENSE_1_fatalities = Dense(self.neurons)(LSTM_1_fatalities)\n        DROPOUT_1_fatalities = Dropout(0.3)(DENSE_1_fatalities)\n        DENSE_final_fatalities = Dense(1, activation=self.activation_out, name = 'Fatalities')(DROPOUT_1_fatalities)\n        \n        self.model = Model(inputs=[input_cases, input_fatalities], outputs=[DENSE_final_cases,DENSE_final_fatalities])\n        \n        optimizer = Adam(\n        learning_rate=0.001)\n        \n        self.model.compile(loss=[self.loss_cases,self.loss_fatalities], \n                           optimizer=optimizer)\n        \n    def bidirectional_lstm(self):\n      \n        input_cases = Input(shape=(self.n_steps, self.n_features))\n        input_fatalities = Input(shape=(self.n_steps, self.n_features))\n        \n        #LSTM cases\n        MASKING_1_cases = Masking(mask_value=-10)(input_fatalities) # special value negative since with log transform in this case we take from >= 1\n        LSTM_1_cases = Bidirectional(LSTM(self.neurons))(MASKING_1_cases)\n        DENSE_1_cases = Dense(self.neurons)(LSTM_1_cases)\n        # DROPOUT_1_cases = Dropout(0.3)(DENSE_1_cases)\n        DENSE_final_cases = Dense(1, activation=self.activation_out, name = 'Cases')(DENSE_1_cases)\n        \n        #LSTM fatalities\n        MASKING_1_fatalities = Masking(mask_value=-10)(input_fatalities) # special value negative since with log transform in this case we take from >= 1\n        LSTM_1_fatalities = Bidirectional(LSTM(self.neurons))(MASKING_1_fatalities)\n        DENSE_1_fatalities = Dense(self.neurons)(LSTM_1_fatalities)\n        # DROPOUT_1_fatalities = Dropout(0.3)(DENSE_1_fatalities)\n        DENSE_final_fatalities = Dense(1, activation=self.activation_out, name = 'Fatalities')(DENSE_1_fatalities)\n        \n        self.model = Model(inputs=[input_cases, input_fatalities], outputs=[DENSE_final_cases,DENSE_final_fatalities])\n        \n        optimizer = Adam(\n        learning_rate=0.001)\n        \n        self.model.compile(loss=[self.loss_cases,self.loss_fatalities], \n                           optimizer=optimizer)\n        \n# LSTM Class with multi inputs: time sequences that need to be predicted and also auxiliary data.\nclass multi_inputs_LSTM():\n    \n    \"\"\" \n    PARAMETERS:\n        structure: string, which LSTM structure to use\n        n_steps: number of time steps used\n        n_steps_out: number of prediction time steps\n        n_features: number of features in input (1 if just univariate time series)\n        n_auxiliary_input: number of auxiliary variables used.\n        num_layers: int, number of hidden layers (excluding the input layer) \n        neurons: array of units/nodes in each layer\n        activation: str, activation function in all layers except output\n        loss: str, loss function\n        optimizer: str, optimizer\n        metrics: list of strings, metrics used\n        epochs: int, number of epochs to train for\n        batch_size: int, number of samples per batch\n    \"\"\"\n    \n    def __init__(self, structure, n_steps, n_features, n_auxiliary_input,\n                 neurons, activation_hidden, activation_out, loss_cases, loss_fatalities, optimizer, metrics,\n                 epochs):\n        \n        self.structure = structure\n        self.n_steps = n_steps\n        self.n_features = n_features\n        self.n_auxiliary_input = n_auxiliary_input\n        self.neurons = neurons\n        self.activation_hidden = activation_hidden\n        self.activation_out = activation_out\n        self.loss_cases = loss_cases\n        self.loss_fatalities = loss_fatalities\n        self.optimizer = optimizer\n        self.metrics = metrics\n        self.epochs = epochs\n        \n        \n    def model_compile(self):\n        \n        # call specific architecture base on kind of LSTM\n        if self.structure == 'classic':\n            self.classic_lstm()\n            \n            \n    def evaluate_model(self, X_train_1, y_train_1, X_test_1, y_test_1, X_train_2, y_train_2, X_test_2, y_test_2,\n                       X_train_3, X_test_3, Validation = False):\n        \n        # Fit the model: fixed approach, fit once on the training data and then predict\n        # each new time step one at a time from the test data.\n        \n        if Validation:\n            history_callback = self.model.fit([X_train_1,X_train_2,X_train_3], [y_train_1,y_train_2],\\\n            epochs = self.epochs, validation_split = 0.1,\n                                              verbose = 0, shuffle = False)\n        else: \n            history_callback = self.model.fit([X_train_1,X_train_2, X_train_3], [y_train_1,y_train_2],\\\n            epochs = self.epochs, validation_data=([X_test_1, X_test_2, X_test_3],  [y_test_1, y_test_2]),\n                                              verbose = 0, shuffle = False)\n\n        # evaluate on test data.\n        if self.metrics == 'RMSE':\n            y_pred_cases, y_pred_fatalities = self.model.predict([X_test_1,X_test_2,X_test_3])\n            pred = np.array([y_pred_cases,y_pred_fatalities]).reshape(-1)\n            actual = np.array([y_test_1,y_test_2]).reshape(-1)\n            metric= np.sqrt(mean_squared_error(pred, actual))\n        elif self.metrics == 'RMSLE':\n            y_pred_cases, y_pred_fatalities = self.model.predict([X_test_1,X_test_2,X_test_3])\n            pred = np.array([y_pred_cases,y_pred_fatalities]).reshape(-1)\n            actual = np.array([y_test_1,y_test_2]).reshape(-1)\n            metric= np.sqrt(mean_squared_log_error(pred, actual,))                \n            \n            \n        return metric, history_callback\n        \n            \n    def classic_lstm(self):\n        \n        # main inputs\n        input_cases = Input(shape=(self.n_steps, self.n_features))\n        input_fatalities = Input(shape=(self.n_steps, self.n_features))\n        \n        # auxiliary inputs\n        input_auxiliary = Input(shape = (self.n_auxiliary_input,self.n_features))\n        \n        LSTM_auxiliary = LSTM(self.neurons)(input_auxiliary)\n        DENSE_auxiliary = Dense(self.neurons)(LSTM_auxiliary)\n        DROPOUT_auxiliary = Dropout(0.2)(DENSE_auxiliary)\n        \n        #LSTM cases\n        MASKING_1_cases = Masking(mask_value=-10)(input_cases) # special value negative since with log transform in this case we take from >= 1\n        LSTM_1_cases = LSTM(self.neurons)(MASKING_1_cases)\n        MERGE_1_cases = Concatenate(axis=-1)([LSTM_1_cases,DROPOUT_auxiliary])\n        DENSE_1_cases = Dense(self.neurons)(MERGE_1_cases)\n        DROPOUT_1_cases = Dropout(0.3)(DENSE_1_cases)\n        DENSE_final_cases = Dense(1, activation=self.activation_out, name = 'Cases')(DROPOUT_1_cases)\n        \n        #LSTM fatalities\n        MASKING_1_fatalities = Masking(mask_value=-10)(input_fatalities) # special value negative since with log transform in this case we take from >= 1\n        LSTM_1_fatalities = LSTM(self.neurons)(MASKING_1_fatalities)\n        MERGE_1_fatalities = Concatenate(axis=-1)([LSTM_1_fatalities, DROPOUT_auxiliary])\n        DENSE_1_fatalities = Dense(self.neurons)(LSTM_1_fatalities)\n        DROPOUT_1_fatalities = Dropout(0.3)(DENSE_1_fatalities)\n        DENSE_final_fatalities = Dense(1, activation=self.activation_out, name = 'Fatalities')(DROPOUT_1_fatalities)\n        \n        self.model = Model(inputs=[input_cases, input_fatalities, input_auxiliary], outputs=[DENSE_final_cases,DENSE_final_fatalities])\n        \n        optimizer = Adam(\n        learning_rate=0.001)\n        \n        self.model.compile(loss=[self.loss_cases,self.loss_fatalities], \n                           optimizer=optimizer)\n        \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **LOADING DATA**"},{"metadata":{"trusted":true},"cell_type":"code","source":"###\n### LOADING DATA\n###\n\nfilename_train = '../input/covid19-global-forecasting-week-4/train.csv'\nfilename_test = '../input/covid19-global-forecasting-week-4/test.csv'\nfilename_submission = '../input/covid19-global-forecasting-week-4/submission.csv'\n\nfilename_enriched = '../input/covid-19-enriched-dataset-week-2/enriched_covid_19_week_2.csv'\n\n### 1) Covid_Data: \ndata_train = pd.read_csv(filename_train)\n\ndata_train['Province_State'] = data_train['Province_State'].fillna('Nation')\ndata_train['IDRegion'] = data_train['Country_Region'] + ' ' + data_train['Province_State']\n\n# data_train['ConfirmedCases'] = data_train['ConfirmedCases'].astype('float32')\n# data_train['Fatalities'] = data_train['Fatalities'].astype('float32')\n\ndata_test = pd.read_csv(filename_test)\n\ndata_test['Province_State'] = data_test['Province_State'].fillna('Nation')\ndata_test['IDRegion'] = data_test['Country_Region'] + ' ' + data_test['Province_State']\n\ndata_submission = pd.read_csv(filename_submission)\n\n\n# Enriched Data\ndata_enriched = pd.read_csv(filename_enriched)\n\ndata_enriched.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **PUBLIC LEADERBOARD PERIOD**\n\nWe use data for training until 2020-04-01, in order to predict from 2020-04-02 to 2020-04-15."},{"metadata":{"trusted":true},"cell_type":"code","source":"###\n### TRAINING AND VALIDATION PERIOD: PUBLIC LEADERBOARD\n###\n\ntotal_length = data_train.shape[0]/len(np.unique(data_train['IDRegion']))\n\nend_training_date_public = '2020-04-01'\npublic_leaderboard = '2020-04-15'\nend_test_date = data_train['Date'].max()\n\nFMT = '%Y-%m-%d'\n\ndiff = np.int((dt.datetime.strptime(end_test_date,FMT) - dt.datetime.strptime(end_training_date_public,FMT)).days)\npublic_leaderboard_window = np.int((dt.datetime.strptime(public_leaderboard,FMT) - dt.datetime.strptime(end_training_date_public,FMT)).days)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###\n### FINE TUNING - PUBLIC\n###\n\n# For each country we run fine tuning algorithm to find the best lag parameter\noptions = {\n    'transformation': ['Normal'],\n    'structure': ['classic'],\n    \"neurons\": [16],\n    \"n_steps\": [1,2,3],\n    \"epochs\": [350]\n}\n\n# other parameters...\ndefault_model_configuration = get_default_configuration(structure = 'classic',\n                        n_steps = 1, \n                        n_features = 1,\n                        neurons = 20, \n                        activation_hidden = 'relu',\n                        activation_out = 'linear',\n                        loss_cases = tf.keras.losses.MeanSquaredLogarithmicError(),\n                        loss_fatalities = tf.keras.losses.MeanSquaredLogarithmicError(), \n                        optimizer = 'adam',\n                        metrics = 'RMSE',\n                        epochs = 400)\n\n# predictions\nlist_of_countries, predictions_cases_new, predictions_fatalities_new = overall_fine_tuning(data = data_train,\n                                                                                           default_model_configuration = default_model_configuration, \n                                                                                           options = options,\n                                                                                           train_test_days_split = diff,\n                                                                                           forecasting_window = public_leaderboard_window,\n                                                                                           Public = True,\n                                                                                           Validation = True)\n\n# relate predictions to country with a dict\nzip_cases = zip(list_of_countries, predictions_cases_new)\nzip_fatalities = zip(list_of_countries, predictions_fatalities_new)\n\n# Create a dictionary from zip object\ndict_of_predictions_cases_public = dict(zip_cases)\ndict_of_predictions_fatalities_public = dict(zip_fatalities)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###\n### PLOT FORECASTING CASES: PUBLIC\n###\n\ncountry = 'France Nation'\n\ndate_for_predictions_public = data_train[data_train['Date'] <= end_training_date_public]\nvariable = 'ConfirmedCases'\npredictions_cases_public = dict_of_predictions_cases_public[country]\nplot_forecasting_LSTM(date_for_predictions_public, country, variable, predictions_cases_public)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###\n### PLOT FORECASTING FATALITIES: PUBLIC\n###\n\nvariable = 'Fatalities'\npredictions_fatalities_public = dict_of_predictions_fatalities_public[country]\nplot_forecasting_LSTM(date_for_predictions_public, country, variable, predictions_fatalities_public)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **PRIVATE LEADERBOARD PERIOD**\n\nIn order to train the model, I use all available data and afterwards I make predictions from 2020-04-16 to 2020-05-14."},{"metadata":{"trusted":true},"cell_type":"code","source":"###\n### TRAINING AND VALIDATION PERIOD: PRIVATE LEADERBOARD\n###\n\ntotal_length = data_train.shape[0]/len(np.unique(data_train['IDRegion']))\n\nend_training_date_private = data_train['Date'].max()\npublic_leaderboard = '2020-04-15'\nprivate_leaderboard = '2020-05-14'\n\nFMT = '%Y-%m-%d'\n\ndiff = np.int((dt.datetime.strptime(public_leaderboard,FMT) - dt.datetime.strptime(end_training_date_private,FMT)).days) - 1\nprivate_leaderboard_window = np.int((dt.datetime.strptime(private_leaderboard,FMT) - dt.datetime.strptime(public_leaderboard,FMT)).days)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###\n### FINE TUNING - PRIVATE\n###\n\n# For each country we run fine tuning algorithm to find the best lag parameter\noptions = {\n    'transformation': ['Normal'],\n    'structure': ['classic'],\n    \"neurons\": [8, 16],\n    \"n_steps\": [1,2,3],\n    \"epochs\": [350]\n}\n\n# other parameters...\ndefault_model_configuration = get_default_configuration(structure = 'classic',\n                        n_steps = 1, \n                        n_features = 1,\n                        neurons = 20, \n                        activation_hidden = 'relu',\n                        activation_out = 'linear',\n                        loss_cases = tf.keras.losses.MeanSquaredLogarithmicError(),\n                        loss_fatalities = tf.keras.losses.MeanSquaredLogarithmicError(), \n                        optimizer = 'adam',\n                        metrics = 'RMSE',\n                        epochs = 400)\n\n# predictions\nlist_of_countries, predictions_cases_new, predictions_fatalities_new = overall_fine_tuning(data = data_train,\n                                                                                           default_model_configuration = default_model_configuration, \n                                                                                           options = options,\n                                                                                           train_test_days_split = 5,\n                                                                                           forecasting_window = private_leaderboard_window + diff,\n                                                                                           Public = False,\n                                                                                           Validation = True)\n\n# relate predictions to country with a dict\nzip_cases = zip(list_of_countries, predictions_cases_new)\nzip_fatalities = zip(list_of_countries, predictions_fatalities_new)\n\n# Create a dictionary from zip object\ndict_of_predictions_cases_private = dict(zip_cases)\ndict_of_predictions_fatalities_private = dict(zip_fatalities)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###\n### PLOT FORECASTING CASES: PRIVATE\n###\n\ncountry = 'Germany Nation'\n\ndate_for_predictions_private = data_train[data_train['Date'] <= end_training_date_private]\nvariable = 'ConfirmedCases'\npredictions_cases_private = dict_of_predictions_cases_private[country]\nplot_forecasting_LSTM(date_for_predictions_private, country, variable, predictions_cases_private)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###\n### PLOT FORECASTING FATALITIES: PRIVATE\n###\n\ndate_for_fatalities_private = data_train[data_train['Date'] <= end_training_date_private]\nvariable = 'Fatalities'\npredictions_fatalities_private = dict_of_predictions_fatalities_private[country]\nplot_forecasting_LSTM(date_for_predictions_private, country, variable, predictions_fatalities_private)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **SUBMISSION FILE**\n\nAt the end we collect the two periods in the final submission file."},{"metadata":{"trusted":true},"cell_type":"code","source":"###\n### SUBMISSION FILE\n###\n\n### PUBLIC\n# assing predictions to submission file\nfor key in dict_of_predictions_fatalities_public:\n  \n    # cases\n    cases = dict_of_predictions_cases_public[key]\n    \n    # fatalities\n    fatalities = dict_of_predictions_fatalities_public[key]\n    \n    # id\n    fid = data_test[data_test['IDRegion']==key]['ForecastId'].values\n    fid = fid[0:public_leaderboard_window]\n    \n    data_submission[data_submission.ForecastId.isin(fid)] = data_submission[data_submission.ForecastId.isin(fid)].assign(ConfirmedCases=cases)\n    data_submission[data_submission.ForecastId.isin(fid)] = data_submission[data_submission.ForecastId.isin(fid)].assign(Fatalities=fatalities)\n\n\n### PRIVATE\n\n# # assing predictions to submission file\nfor key in dict_of_predictions_cases_private:\n  \n    # cases\n    cases = dict_of_predictions_cases_private[key]\n    cases = cases[diff:]\n    \n    # fatalities\n    fatalities = dict_of_predictions_fatalities_private[key]\n    fatalities = fatalities[diff:]\n    \n    # id\n    fid = data_test[data_test['IDRegion']==key]['ForecastId'].values\n    fid = fid[public_leaderboard_window:]\n    \n    data_submission[data_submission.ForecastId.isin(fid)] = data_submission[data_submission.ForecastId.isin(fid)].assign(ConfirmedCases=cases)\n    data_submission[data_submission.ForecastId.isin(fid)] = data_submission[data_submission.ForecastId.isin(fid)].assign(Fatalities=fatalities)\n        \ndata_submission.to_csv('submission.csv',index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}