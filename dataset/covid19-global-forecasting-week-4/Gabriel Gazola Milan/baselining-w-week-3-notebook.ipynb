{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nOn last week's competition, I ran out of time with my predictions and did no data exploration nor adding secondary datasets.\nThis week I intend to focus more on exploration."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\nFULL_RUN = True\n\n# Listing files\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Loading data\nif FULL_RUN:\n    df_train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\n    df_test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')\nelse:\n    df_train = pd.read_csv('/kaggle/input/covid19-week-3-data/df_train.csv')\n    df_test = pd.read_csv('/kaggle/input/covid19-week-3-data/df_test.csv')\nsubmission_example = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/submission.csv')\ndf_population = pd.read_csv('/kaggle/input/population-by-country-2020/population_by_country_2020.csv')\n#df_containment = pd.read_csv('/kaggle/input/covid19-containment-and-mitigation-measures/COVID 19 Containment measures data.csv')\ndf_health_systems = pd.read_csv('/kaggle/input/world-bank-wdi-212-health-systems/2.12_Health_systems.csv')\ndf_lat_lon = pd.read_csv('/kaggle/input/coronavirus-latlon-dataset/CV_LatLon_21Jan_12Mar.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Merging competition data with the Lat/Lon dataset\n\nOn this section, I'll merge the train and test datasets with the latitude/longitude dataset (coronavirus-latlon-dataset).\n\nAs some values on Province_State and Country_Region were not filled (as two of them are boats and few others were not on the coronavirus-latlon-dataset), I needed to get these values myself.\n\nAfter that, I'll plot a heatmap of the confirmed cases for the most recent date on the train dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    # Getting countries with states\n    import math\n    has_states = set(df_train[~(df_train['Province_State'].isna())]['Country_Region'])\n    common_states = set(df_train['Province_State']) & set(df_lat_lon[df_lat_lon['country'].isin(has_states)]['state'])\n    common_countries = set(df_train['Country_Region']) & set(df_lat_lon['country'])\n    remaining_countries = {\n        'Angola' : [-11.1799566,13.2833794],\n        'Antigua and Barbuda' : [17.3257025,-62.2903859],\n        'Bahamas' : [24.417926,-78.2102651],\n        'Barbados' : [13.1881671,-59.6052954],\n        'Belize' : [17.187683,-89.4413417],\n        'Benin' : [9.3003431,0.0654059],\n        'Botswana' : [-22.3273954,22.4434759],\n        'Burma' : [18.7811838,87.6460721],\n        'Burundi' : [-3.3893677,29.3648016],\n        'Cabo Verde' : [16.0202145,-25.1098509],\n        'Central African Republic' : [6.6154729,18.6926929],\n        'Chad' : [15.4008548,14.2402013],\n        'Congo (Brazzaville)' : [-4.2471919,15.1571824,12],\n        'Diamond Princess' : [35.4526321,139.4550321], # Diamond Princess is a ship, seen in Yokohama, Japan (Feb 27th)\n        'Djibouti' : [11.8127758,42.0669243],\n        'Dominica' : [15.4263293,-61.4975892],\n        'El Salvador' : [13.7483455,-89.4906972],\n        'Equatorial Guinea' : [1.1431229,6.1935546],\n        'Eritrea' : [15.1764605,37.5884248],\n        'Eswatini' : [-26.516566,30.9023408],\n        'Ethiopia' : [9.1215001,36.00375],\n        'Fiji' : [-16.5421848,177.2178571],\n        'Gabon' : [-0.9230372,9.2299158],\n        'Gambia' : [13.4168603,-15.9293406],\n        'Ghana' : [7.8984804,-3.2743994],\n        'Grenada' : [12.259767,-61.7303844],\n        'Guatemala' : [15.719987,-91.3560049],\n        'Guinea' : [9.92542,-13.7038879],\n        'Guinea-Bissau' : [11.7002291,-15.8496604],\n        'Haiti' : [19.0343549,-73.6754192],\n        'Kazakhstan' : [47.6548578,57.9392984],\n        'Kenya' : [0.1544419,35.6643364],\n        'Kosovo' : [42.5612976,20.3416721],\n        'Kyrgyzstan' : [41.2010445,72.4968368],\n        'Laos' : [18.1963416,101.615389],\n        'Liberia' : [6.4096257,-10.573663],\n        'Libya' : [26.2900748,12.8375989],\n        'MS Zaandam' : [26.1410956,-80.2156069], # It's a ship located at Fort Lauderdale (April 3rd)\n        'Madagascar' : [-18.771976,42.373469],\n        'Mali' : [17.5237177,-8.4809037],\n        'Mauritania' : [20.959589,-15.444754],\n        'Mauritius' : [-20.2030942,56.5543186],\n        'Montenegro' : [42.6928556,18.832956],\n        'Mozambique' : [-18.5836828,31.3118067],\n        'Namibia' : [-22.9037659,13.8724459],\n        'Nicaragua' : [12.866514,-86.1389169],\n        'Niger' : [17.5460918,3.5859574],\n        'Papua New Guinea' : [-6.3567909,145.9055506],\n        'Rwanda' : [-1.9435638,29.3199833],\n        'Saint Kitts and Nevis' : [16.249782,-62.284578],\n        'Saint Lucia' : [13.9128128,-61.1106006],\n        'Saint Vincent and the Grenadines' : [12.9714329,-61.5635867],\n        'Seychelles' : [-7.0850076,48.9440464],\n        'Sierra Leone' : [8.4206974,-12.9587225],\n        'Somalia' : [5.2310437,41.808129],\n        'Sudan' : [15.7399293,25.7594752],\n        'Suriname' : [3.9826927,-57.1279423],\n        'Syria' : [34.7943312,36.7594245],\n        'Tanzania' : [-6.3533765,30.4940155],\n        'Timor-Leste' : [-8.7889361,125.1685995],\n        'Trinidad and Tobago' : [10.6962001,-61.7721494],\n        'Uganda' : [1.3671063,30.059196],\n        'Uruguay' : [-32.600568,-58.0278336],\n        'Uzbekistan' : [41.2939152,60.0857832],\n        'Venezuela' : [6.6368125,-71.1105344],\n        'West Bank and Gaza' : [31.9461203,34.6667392],\n        'Zambia' : [-13.101327,23.3590343],\n        'Zimbabwe' : [-19.0020825,26.9090619]\n    }\n\n    def fillLat (state, country):\n        if state in common_states:\n            return df_lat_lon[df_lat_lon['state'] == state]['lat'].unique()[0]\n        elif country in common_countries:\n            return df_lat_lon[df_lat_lon['country'] == country]['lat'].unique()[0]\n        elif country in remaining_countries:\n            return remaining_countries[country][0]\n        else:\n            return float('NaN')\n\n    def fillLon (state, country):\n        if state in common_states:\n            return df_lat_lon[df_lat_lon['state'] == state]['lon'].unique()[0]\n        elif country in common_countries:\n            return df_lat_lon[df_lat_lon['country'] == country]['lon'].unique()[0]\n        elif country in remaining_countries:\n            return remaining_countries[country][1]\n        else:\n            return float('NaN')\n\n    df_train['Lat'] = df_train.loc[:, ['Country_Region', 'Province_State']].apply(lambda x : fillLat(x['Province_State'], x['Country_Region']), axis=1)\n    df_train['Lon'] = df_train.loc[:, ['Country_Region', 'Province_State']].apply(lambda x : fillLon(x['Province_State'], x['Country_Region']), axis=1)\n    df_test['Lat'] = df_test.loc[:, ['Country_Region', 'Province_State']].apply(lambda x : fillLat(x['Province_State'], x['Country_Region']), axis=1)\n    df_test['Lon'] = df_test.loc[:, ['Country_Region', 'Province_State']].apply(lambda x : fillLon(x['Province_State'], x['Country_Region']), axis=1)\n\n    # Filling Province_State column\n    # Following the idea at\n    # https://www.kaggle.com/ranjithks/25-lines-of-code-results-better-score#Fill-NaN-from-State-feature\n    # Filling NaN states with the Country\n    EMPTY_VAL = \"EMPTY_VAL\"\n    def fillState(state, country):\n        if state == EMPTY_VAL: return country\n        return state\n    def replaceGeorgiaState (state, country):\n        if (state == 'Georgia') and (country == 'US'):\n            return 'Georgia_State'\n        else:\n            return state\n    df_train['Province_State'].fillna(EMPTY_VAL, inplace=True)\n    df_train['Province_State'] = df_train.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\n    df_train['Province_State'] = df_train.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : replaceGeorgiaState(x['Province_State'], x['Country_Region']), axis=1)\n    df_test['Province_State'].fillna(EMPTY_VAL, inplace=True)\n    df_test['Province_State'] = df_test.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\n    df_test['Province_State'] = df_test.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : replaceGeorgiaState(x['Province_State'], x['Country_Region']), axis=1)\n\n    # Checking for missing values\n    missing_values = df_train.isnull().sum()\n    print(\"Train missing values:\\n{}\".format(missing_values[missing_values>0].sort_values(ascending = False)))\n    missing_values = df_test.isnull().sum()\n    print(\"Test missing values:\\n{}\".format(missing_values[missing_values>0].sort_values(ascending = False)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    import folium\n    from folium.plugins import HeatMap\n    geo_df = df_train.groupby([\"Province_State\"]).max()[['ConfirmedCases', 'Fatalities', 'Lat', 'Lon']]\n    max_amount = float(geo_df['ConfirmedCases'].max())\n    hmap = folium.Map(location = [0, 0], zoom_start = 2.4, tiles=\"CartoDB positron\")\n    title_html = '<h3 align=\"center\" style=\"font-size:20px\"><b>COVID-19 ConfirmedCases Heatmap ({})</b></h3>'.format(df_train['Date'].max())\n    hmap.get_root().html.add_child(folium.Element(title_html))\n    hm_wide = HeatMap (list (zip (geo_df.Lat.values, geo_df.Lon.values, geo_df.ConfirmedCases.values)),\n                       min_opacity = 0.4,\n                       max_val = max_amount,\n                           radius = 17,\n                       blur = 15,\n                       max_zoom = 1\n                      )\n    hmap.add_child(hm_wide)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Merging population by country\n\nOn this section, I'll merge the train and test datasets with the population dataset.\n\nAs some values on Country_Region were not filled (as two of them are boats and few others were not on the dataset), I needed to get these values myself.\n\nAfter that, do some plotting!"},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    common_countries = set (df_train['Country_Region']) & set(df_population['Country (or dependency)'])\n    remaining_countries = {\n        'Czechia': {\n            'population' : 10650000,\n            'density' : 134,\n            'area' : 78865,\n            'medage' : 41,\n        },\n        'Saint Vincent and the Grenadines': {\n            'population' : 109897,\n            'density' : 284,\n            'area' : 389,\n            'medage' : 33,\n        },\n        'West Bank and Gaza': {\n            'population' : 3340143,\n            'density' : 13,\n            'area' : 5655,\n            'medage' : 17,\n        },\n        'Kosovo': {\n            'population' : 1831000,\n            'density' : 159,\n            'area' : 168,\n            'medage' : 29,\n        },\n        'MS Zaandam': {\n            'population' : 1400,\n            'density' : 18178,\n            'area' : 0.0770148,    # Approximation\n            'medage' : 40,         # Couldn't find this\n        },\n        'Burma': {\n            'population' : 54410000,\n            'density' : 79,\n            'area' : 676578,\n            'medage' : 29,\n        },\n        'Diamond Princess': {\n            'population' : 3711,\n            'density' : 20073,     # It's large 'cause it's a boat\n            'area' : 0.184875,     # Approximation\n            'medage' : 30,         # Couldn't find this\n        },\n        'Saint Kitts and Nevis': {\n            'population' : 53199,\n            'density' : 204,\n            'area' : 261,\n            'medage' : 36,\n        },\n        \"Cote d'Ivoire\": {\n            'population' : 24290000,\n            'density' : 83,\n            'area' : 322463,\n            'medage' : 20,\n        },\n    }\n    def fillPopulation (country):\n        if country == \"US\":\n            return df_population[df_population['Country (or dependency)'] == \"United States\"]['Population (2020)'].unique()[0]\n        elif country.startswith(\"Congo\"):\n            return df_population[df_population['Country (or dependency)'] == \"Congo\"]['Population (2020)'].unique()[0]\n        elif country == \"Taiwan*\":\n            return df_population[df_population['Country (or dependency)'] == \"Taiwan\"]['Population (2020)'].unique()[0]\n        elif country == \"Korea, South\":\n            return df_population[df_population['Country (or dependency)'] == \"South Korea\"]['Population (2020)'].unique()[0]\n        elif country in common_countries:\n            return df_population[df_population['Country (or dependency)'] == country]['Population (2020)'].unique()[0]\n        elif country in remaining_countries:\n            return remaining_countries[country]['population']\n        else:\n            return float('NaN')\n\n    def fillDensity (country):\n        if country == \"US\":\n            return df_population[df_population['Country (or dependency)'] == \"United States\"]['Density (P/Km²)'].unique()[0]\n        elif country.startswith(\"Congo\"):\n            return df_population[df_population['Country (or dependency)'] == \"Congo\"]['Density (P/Km²)'].unique()[0]\n        elif country == \"Taiwan*\":\n            return df_population[df_population['Country (or dependency)'] == \"Taiwan\"]['Density (P/Km²)'].unique()[0]\n        elif country == \"Korea, South\":\n            return df_population[df_population['Country (or dependency)'] == \"South Korea\"]['Density (P/Km²)'].unique()[0]\n        elif country in common_countries:\n            return df_population[df_population['Country (or dependency)'] == country]['Density (P/Km²)'].unique()[0]\n        elif country in remaining_countries:\n            return remaining_countries[country]['density']\n        else:\n            return float('NaN')\n\n    def fillArea (country):\n        if country == \"US\":\n            return df_population[df_population['Country (or dependency)'] == \"United States\"]['Land Area (Km²)'].unique()[0]\n        elif country.startswith(\"Congo\"):\n            return df_population[df_population['Country (or dependency)'] == \"Congo\"]['Land Area (Km²)'].unique()[0]\n        elif country == \"Taiwan*\":\n            return df_population[df_population['Country (or dependency)'] == \"Taiwan\"]['Land Area (Km²)'].unique()[0]\n        elif country == \"Korea, South\":\n            return df_population[df_population['Country (or dependency)'] == \"South Korea\"]['Land Area (Km²)'].unique()[0]\n        elif country in common_countries:\n            return df_population[df_population['Country (or dependency)'] == country]['Land Area (Km²)'].unique()[0]\n        elif country in remaining_countries:\n            return remaining_countries[country]['area']\n        else:\n            return float('NaN')\n\n    def fillMedAge (country):\n        if country == \"Andorra\":\n            return 45\n        elif country == \"Dominica\":\n            return 34\n        elif country == \"Holy See\":\n            return 60\n        elif country == \"Liechtenstein\":\n            return 41\n        elif country == \"Monaco\":\n            return 52\n        elif country == \"San Marino\":\n            return 45\n        elif country == \"US\":\n            return df_population[df_population['Country (or dependency)'] == \"United States\"]['Med. Age'].unique()[0]\n        elif country.startswith(\"Congo\"):\n            return df_population[df_population['Country (or dependency)'] == \"Congo\"]['Med. Age'].unique()[0]\n        elif country == \"Taiwan*\":\n            return df_population[df_population['Country (or dependency)'] == \"Taiwan\"]['Med. Age'].unique()[0]\n        elif country == \"Korea, South\":\n            return df_population[df_population['Country (or dependency)'] == \"South Korea\"]['Med. Age'].unique()[0]\n        elif country in common_countries:\n            return df_population[df_population['Country (or dependency)'] == country]['Med. Age'].unique()[0]\n        elif country in remaining_countries:\n            return remaining_countries[country]['medage']\n        else:\n            return float('NaN')\n\n    df_train['Population'] = df_train.loc[:, ['Country_Region']].apply(lambda x : fillPopulation(x['Country_Region']), axis=1)\n    df_test['Population'] = df_test.loc[:, ['Country_Region']].apply(lambda x : fillPopulation(x['Country_Region']), axis=1)\n    df_train['Density'] = df_train.loc[:, ['Country_Region']].apply(lambda x : fillDensity(x['Country_Region']), axis=1)\n    df_test['Density'] = df_test.loc[:, ['Country_Region']].apply(lambda x : fillDensity(x['Country_Region']), axis=1)\n    df_train['Area'] = df_train.loc[:, ['Country_Region']].apply(lambda x : fillArea(x['Country_Region']), axis=1)\n    df_test['Area'] = df_test.loc[:, ['Country_Region']].apply(lambda x : fillArea(x['Country_Region']), axis=1)\n    df_train['MedAge'] = df_train.loc[:, ['Country_Region']].apply(lambda x : fillMedAge(x['Country_Region']), axis=1)\n    df_test['MedAge'] = df_test.loc[:, ['Country_Region']].apply(lambda x : fillMedAge(x['Country_Region']), axis=1)\n\n    # Checking for missing values\n    missing_values = df_train.isnull().sum()\n    print(\"Train missing values:\\n{}\".format(missing_values[missing_values>0].sort_values(ascending = False)))\n    missing_values = df_test.isnull().sum()\n    print(\"Test missing values:\\n{}\".format(missing_values[missing_values>0].sort_values(ascending = False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that our data is merged and no column has any missing values, we'll plot the ratio of confirmed cases by the country density, which may give us the idea of the criticality of COVID-19 on the country, since countries with larger density are more likely to have a greater number of confirmed cases, right?\n\nBut before doing that, I'll do a similar plot with the absolute number of confirmed cases so I can compare them after.\n\n**My guess** is that this feature may have a good correlation with the \"Fatalities\" feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    # Creating Cases/Density column\n    cases_density_df = df_train.groupby([\"Province_State\"]).max()[['ConfirmedCases', 'Density']]\n    cases_density_df['Cases_Density'] = cases_density_df.loc[:, ['ConfirmedCases', 'Density']].apply(lambda x : x['ConfirmedCases'] / x['Density'], axis=1)\n\n    # Making plot\n    plt.figure(figsize=(16,8))\n    plt.title(\"Number of COVID-19 confirmed cases\")\n    plt.xlabel(\"Confirmed Cases\")\n    plt.barh(cases_density_df.sort_values(by='ConfirmedCases', ascending = False).iloc[:10].sort_values(by='ConfirmedCases', ascending = True).index, cases_density_df.sort_values(by='ConfirmedCases', ascending = False).iloc[:10].sort_values(by='ConfirmedCases', ascending = True)['ConfirmedCases'], color='#ffb3b3')\n    for i, v in enumerate(cases_density_df.sort_values(by='ConfirmedCases', ascending = False).iloc[:10].sort_values(by='ConfirmedCases', ascending = True)['ConfirmedCases']):\n        plt.text(v + 100, i-0.1, str(int(v)), color='#880000')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    # Making plot\n    plt.figure(figsize=(16,8))\n    plt.title(\"Number of COVID-19 confirmed cases per country density\")\n    plt.xlabel(\"Confirmed Cases / Density\")\n    plt.barh(cases_density_df.sort_values(by='Cases_Density', ascending = False).iloc[:10].sort_values(by='Cases_Density', ascending = True).index, cases_density_df.sort_values(by='Cases_Density', ascending = False).iloc[:10].sort_values(by='Cases_Density', ascending = True)['Cases_Density'], color='#ffb3b3')\n    for i, v in enumerate(cases_density_df.sort_values(by='Cases_Density', ascending = False).iloc[:10].sort_values(by='Cases_Density', ascending = True)['Cases_Density']):\n        plt.text(v + 50, i-0.1, str(np.round(v,2)), color='#880000')\n    plt.show()\n\n    # Filling df_train and df_test\n    df_train['Cases_Density'] = df_train.loc[:, ['Province_State']].apply(lambda x : cases_density_df.loc[x['Province_State']]['Cases_Density'], axis=1)\n    df_test['Cases_Density'] = df_test.loc[:, ['Province_State']].apply(lambda x : cases_density_df.loc[x['Province_State']]['Cases_Density'], axis=1)\n\n    # Checking for missing values\n    missing_values = df_train.isnull().sum()\n    print(\"Train missing values:\\n{}\".format(missing_values[missing_values>0].sort_values(ascending = False)))\n    missing_values = df_test.isnull().sum()\n    print(\"Test missing values:\\n{}\".format(missing_values[missing_values>0].sort_values(ascending = False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By comparing the two of them (and considering the ConfirmedCases/Density ratio a criticality indicator) it's possible to see that Italy and Spain are not as critical as they seem to be on the previous plot. Germany is not even on the second plot, even though is on 4th place on the first one.\n\nBut before making any further assumptions on criticality, maybe we should take a look at the health systems data.\n\nNow I'll build a FatalRatio feature, which is the Fatalities/Cases ratio and plot it against median age, trying to see a correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    def getFatalRatio (country):\n        df = df_train[df_train['Country_Region'] == country]\n        cases = df['ConfirmedCases'].max()\n        fatal = df['Fatalities'].max()\n        if cases <= 0:\n            return 0\n        else:\n            return fatal/cases\n\n    df_train['FatalRatio'] = df_train.loc[:, ['Country_Region']].apply(lambda x : getFatalRatio(x['Country_Region']), axis=1)\n    df_test['FatalRatio'] = df_test.loc[:, ['Country_Region']].apply(lambda x : getFatalRatio(x['Country_Region']), axis=1)\n\n    plot_df = df_train.groupby([\"Province_State\", 'Country_Region']).max()[['FatalRatio', 'MedAge']]\n    plot_df = plot_df[plot_df['FatalRatio'] > 0]\n    plt.figure(figsize=(16,8))\n    plt.title(\"Fatality ratio vs. Median age\")\n    plt.xlabel(\"Median age\")\n    plt.ylabel(\"Fatality ratio\")\n    plt.scatter(plot_df['MedAge'].astype(int), plot_df['FatalRatio'].values)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can tell me I'm crazy, but I kinda see that the middle of this plot (around 30 to 35 median age) seems lower than the rest.\n\nMaybe that's a good feature for Fatalities, but I have no idea yet."},{"metadata":{},"cell_type":"markdown","source":"## Visualizing the numbers\n\nOn this section, I'll try to plot **the most critical countries (according to the \"Cases_Density\" feature)** evolution using the competition dataset.\n\nNew features \"ConfirmedCases_Hat\", \"Fatalities_Hat\", \"ConfirmedCases_Hat_Hat\" and \"Fatalities_Hat_Hat\" will be added, being:\n* ConfirmedCases_Hat: The first derivative of \"ConfirmedCases\"\n* Fatalities_Hat: The first derivative of \"Fatalities\"\n* ConfirmedCases_Hat_Hat: The second derivative of \"ConfirmedCases\"\n* Fatalities_Hat_Hat: The second derivative of \"Fatalities\""},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    # Generating new features\n    df_train['ConfirmedCases_Hat'] = df_train['ConfirmedCases'] - df_train['ConfirmedCases'].shift(1)\n    df_train['ConfirmedCases_Hat'] = df_train['ConfirmedCases_Hat'].apply(lambda x: 0 if x < 0 else x)\n    df_train['ConfirmedCases_Hat'] = df_train['ConfirmedCases_Hat'].fillna(0)\n    df_train['ConfirmedCases_Hat_Hat'] = df_train['ConfirmedCases_Hat'] - df_train['ConfirmedCases_Hat'].shift(1)\n    df_train['ConfirmedCases_Hat_Hat'] = df_train['ConfirmedCases_Hat_Hat'].fillna(0)\n    df_train['Fatalities_Hat'] = df_train['Fatalities'] - df_train['Fatalities'].shift(1)\n    df_train['Fatalities_Hat'] = df_train['Fatalities_Hat'].apply(lambda x: 0 if x < 0 else x)\n    df_train['Fatalities_Hat'] = df_train['Fatalities_Hat'].fillna(0)\n    df_train['Fatalities_Hat_Hat'] = df_train['Fatalities_Hat'] - df_train['Fatalities_Hat'].shift(1)\n    df_train['Fatalities_Hat_Hat'] = df_train['Fatalities_Hat_Hat'].fillna(0)\n\n    # Getting the most critical\n    most_critical = set(df_train.groupby([\"Province_State\"]).max().sort_values(by='Cases_Density', ascending = False).iloc[:8].index)\n    xticks = df_train['Date'].unique()\n    i = 0\n    for tick in xticks:\n        if i == 1:\n            xticks[xticks==tick] = ''\n            i = 0\n        else:\n            i = 1\n\n    plot_df = df_train[df_train['Province_State'].isin(most_critical)]\n    plt.figure(figsize=(16,10))\n    plt.title(\"COVID-19 confirmed cases over time\")\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Confirmed cases\")\n    for state in most_critical:\n        plt.plot(plot_df[plot_df['Province_State'] == state]['Date'], plot_df[plot_df['Province_State'] == state]['ConfirmedCases'])\n    plt.legend(most_critical)\n    plt.xticks(xticks, rotation=45)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    plot_df = df_train[df_train['Province_State'].isin(most_critical)]\n    plt.figure(figsize=(16,10))\n    plt.title(\"COVID-19 fatalities over time\")\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Fatalities\")\n    for state in most_critical:\n        plt.plot(plot_df[plot_df['Province_State'] == state]['Date'], plot_df[plot_df['Province_State'] == state]['Fatalities'])\n    plt.legend(most_critical)\n    plt.xticks(xticks, rotation=45)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the worldwide visualization, as I intend to plot both cases and its smoothed first derivatives, I decided to take the y-axis scale to log.\n\nIt gets better visualization of the relations between them"},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    from scipy.interpolate import make_interp_spline, BSpline\n    from datetime import datetime\n\n    plot_df = df_train.groupby([\"Date\"]).sum()\n    plt.figure(figsize=(16,10))\n    plt.title(\"Worldwide COVID-19 Confirmed Cases\")\n    plt.xlabel(\"Date\")\n    x = plot_df.index\n    y = plot_df['ConfirmedCases']\n    plt.plot(x, y)\n    x = pd.to_datetime(plot_df.index)\n    y = plot_df['ConfirmedCases_Hat']\n    xnew = np.linspace(0, len(x), 300)\n    spl = make_interp_spline(range(len(x)), y, k=2)  # type: BSpline\n    power_smooth = spl(xnew)\n    plt.plot(xnew, power_smooth)\n    plt.yscale('log')\n    plt.xticks(xticks, rotation=45)\n    plt.legend(['Confirmed cases', 'First derivative (smoothed)'])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    plt.figure(figsize=(16,10))\n    plt.title(\"Worldwide COVID-19 Fatalities\")\n    plt.xlabel(\"Date\")\n    x = plot_df.index\n    y = plot_df['Fatalities']\n    plt.plot(x, y)\n    x = pd.to_datetime(plot_df.index)\n    y = plot_df['Fatalities_Hat']\n    xnew = np.linspace(0, len(x), 300)\n    spl = make_interp_spline(range(len(x)), y, k=2)  # type: BSpline\n    power_smooth = spl(xnew)\n    plt.plot(xnew, power_smooth)\n    plt.yscale('log')\n    plt.xticks(xticks, rotation=45)\n    plt.legend(['Fatalities', 'First derivative (smoothed)'])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adding season of the year data\n\nIt has been disclosed that cold and dry weather may positively influence in COVID-19 spreading.\n\nSeasons say a lot about the weather. I'll now add season data based on latitude and date."},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    # == North:\n    #  - Spring runs from March 1 to May 31;\n    #  - Summer runs from June 1 to August 31;\n    #  - Fall (autumn) runs from September 1 to November 30; and\n    #  - Winter runs from December 1 to February 28 (February 29 in a leap year).\n    #\n    # == South:\n    #  - Spring starts September 1 and ends November 30;\n    #  - Summer starts December 1 and ends February 28 (February 29 in a Leap Year);\n    #  - Fall (autumn) starts March 1 and ends May 31; and\n    #  - Winter starts June 1 and ends August 31;\n    def getSeason (latitude, date):\n        month = pd.to_datetime(date).month\n        # North\n        if latitude >= 0:\n            # Spring\n            if ((month >= 3) and (month <= 5)):\n                return \"spring\"\n            # Summer\n            elif ((month >= 6) and (month <= 8)):\n                return \"summer\"\n            # Fall\n            elif ((month >= 9) and (month <= 11)):\n                return \"fall\"\n            # Winter\n            else:\n                return \"winter\"\n        # South\n        else:\n            # Fall\n            if ((month >= 3) and (month <= 5)):\n                return \"fall\"\n            # Winter\n            elif ((month >= 6) and (month <= 8)):\n                return \"winter\"\n            # Spring\n            elif ((month >= 9) and (month <= 11)):\n                return \"spring\"\n            # Summer\n            else:\n                return \"summer\"\n\n    #getSeason(1, df_train['Date'].unique()[0])\n    df_train['Season'] = df_train.loc[:, ['Lat', 'Date']].apply(lambda x : getSeason(x['Lat'], x['Date']), axis=1)\n    df_test['Season'] = df_test.loc[:, ['Lat', 'Date']].apply(lambda x : getSeason(x['Lat'], x['Date']), axis=1)\n\n    # Checking for missing values\n    missing_values = df_train.isnull().sum()\n    print(\"Train missing values:\\n{}\".format(missing_values[missing_values>0].sort_values(ascending = False)))\n    missing_values = df_test.isnull().sum()\n    print(\"Test missing values:\\n{}\".format(missing_values[missing_values>0].sort_values(ascending = False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adding health systems data\n\nThe [World Bank WDI 2.12 - Health Systems](https://www.kaggle.com/danevans/world-bank-wdi-212-health-systems) has called my attention. I'll try to understand it and add its data into my dataset.\n\nOn the description of the dataset, [Dan Evans](https://www.kaggle.com/danevans) says that there are some countries/regions in the Covid-19 data with no World Bank data. Knowing that and considering the difficulty of looking for this data, I'll just set the values as the average of the column.\n\nAs the names of the columns aren't intuitive, I'll leave the description here:\n* Country_Region: the region as used in Kaggle Covid-19 spread data challenges.\n* Province_State: the region as used in Kaggle Covid-19 spread data challenges.\n* WorldBankName: the name of the country used by the World Bank\n* HealthexppctGDP2016: Level of current health expenditure expressed as a percentage of GDP. Estimates of current health expenditures include healthcare goods and services consumed during each year. This indicator does not include capital health expenditures such as buildings, machinery, IT and stocks of vaccines for emergency or outbreaks.\n* Healthexppublicpct2016: Share of current health expenditures funded from domestic public sources for health. Domestic public sources include domestic revenue as internal transfers and grants, transfers, subsidies to voluntary health insurance beneficiaries, non-profit institutions serving households (NPISH) or enterprise financing schemes as well as compulsory prepayment and social health insurance contributions. They do not include external resources spent by governments on health.\n* Healthexpoutofpocketpct2016: Share of out-of-pocket payments of total current health expenditures. Out-of-pocket payments are spending on health directly out-of-pocket by households.\n* HealthexppercapitaUSD_2016: Current expenditures on health per capita in current US dollars. Estimates of current health expenditures include healthcare goods and services consumed during each year.\n* percapitaexpPPP2016: Current expenditures on health per capita expressed in international dollars at purchasing power parity (PPP).\n* Externalhealthexppct2016: Share of current health expenditures funded from external sources. External sources compose of direct foreign transfers and foreign transfers distributed by government encompassing all financial inflows into the national health system from outside the country. External sources either flow through the government scheme or are channeled through non-governmental organizations or other schemes.\n* Physiciansper1000_2009-18: Physicians include generalist and specialist medical practitioners.\n* Nursemidwifeper10002009-18: Nurses and midwives include professional nurses, professional midwives, auxiliary nurses, auxiliary midwives, enrolled nurses, enrolled midwives and other associated personnel, such as dental nurses and primary care nurses.\n* Specialistsurgicalper10002008-18: Specialist surgical workforce is the number of specialist surgical, anaesthetic, and obstetric (SAO) providers who are working in each country per 100,000 population.\n* Completenessofbirthreg2009-18: Completeness of birth registration is the percentage of children under age 5 whose births were registered at the time of the survey. The numerator of completeness of birth registration includes children whose birth certificate was seen by the interviewer or whose mother or caretaker says the birth has been registered.\n* Completenessofdeathreg2008-16: Completeness of death registration is the estimated percentage of deaths that are registered with their cause of death information in the vital registration system of a country."},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    # Getting missing countries\n    missing_countries = set(df_train['Country_Region']) - set(df_health_systems['Country_Region'])\n    print (\"Missing countries are: {}\".format(missing_countries))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    desired_columns = [\n        'Health_exp_pct_GDP_2016',              # Gives the idea on how much is spent on health/GDP\n        'Health_exp_public_pct_2016',           # Tells how much comes from public sources\n        'Health_exp_per_capita_USD_2016',       # Expenditures on health per capita\n        'Physicians_per_1000_2009-18',          # Number of physicians/10K\n        'Nurse_midwife_per_1000_2009-18',       # Number of nurses/midwives/10K\n        'Specialist_surgical_per_1000_2008-18', # Number of surgeons/10K\n        'Completeness_of_death_reg_2008-16',    # Percentage of death correctly registered\n    ]\n    desired_names = [\n        'Health_GDP',\n        'Health_Public',\n        'Health_USD',\n        'Physicians',\n        'Nurses',\n        'Surgeons',\n        'DeathCompleteness'\n    ]\n    df_health_systems = pd.read_csv('/kaggle/input/world-bank-wdi-212-health-systems/2.12_Health_systems.csv')\n    df_health_systems = df_health_systems[['Country_Region', 'Province_State'] + desired_columns]\n    missing_countries = set(missing_countries)\n    health_states = set(df_health_systems['Province_State'])\n    health_countries = set(df_health_systems['Country_Region'])\n\n    def getFeature (country, state, feature_name):\n        if country in set(missing_countries):\n            return df_health_systems[feature_name].mean()\n        elif state in health_states:\n            return df_health_systems[df_health_systems['Province_State'] == state][feature_name].mean()\n        elif country in health_countries:\n            return df_health_systems[df_health_systems['Country_Region'] == country][feature_name].mean()\n        else:\n            return float('NaN')\n\n    for i in range(len(desired_columns)):\n        feature_name = desired_columns[i]\n        desired_name = desired_names[i]\n        print (\"Getting feature {}\".format(desired_name))\n        df_train[desired_name] = df_train.loc[:, ['Country_Region', 'Province_State']].apply(lambda x : getFeature(x['Country_Region'], x['Province_State'], feature_name), axis=1)\n        df_test[desired_name] = df_test.loc[:, ['Country_Region', 'Province_State']].apply(lambda x : getFeature(x['Country_Region'], x['Province_State'], feature_name), axis=1)\n\n    # Checking for missing values\n    missing_values = df_train.isnull().sum()\n    print(\"Train missing values:\\n{}\".format(missing_values[missing_values>0].sort_values(ascending = False)))\n    missing_values = df_test.isnull().sum()\n    print(\"Test missing values:\\n{}\".format(missing_values[missing_values>0].sort_values(ascending = False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now filling missing with the mean of the column"},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    # Filling missing with column mean\n    for i in range(len(desired_columns)):\n        df_train[desired_names[i]] = df_train[desired_names[i]].fillna(df_health_systems[desired_columns[i]].mean())\n        df_test[desired_names[i]] = df_test[desired_names[i]].fillna(df_health_systems[desired_columns[i]].mean())\n\n    # Checking for missing values\n    missing_values = df_train.isnull().sum()\n    print(\"Train missing values:\\n{}\".format(missing_values[missing_values>0].sort_values(ascending = False)))\n    missing_values = df_test.isnull().sum()\n    print(\"Test missing values:\\n{}\".format(missing_values[missing_values>0].sort_values(ascending = False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice! Let's plot some stuff!\n\nI'll begin by making a pairplot for ease of visualization of every health feature vs. confirmed cases and fatalities."},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    import seaborn as sns\n    sns.pairplot(df_train,\n                 y_vars = ['ConfirmedCases', 'Fatalities', 'FatalRatio'],\n                 x_vars = desired_names,\n                 diag_kind=\"kde\",\n                 palette=\"husl\"\n                )\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That WAS a huge plot on previous version, but now I found a way of making it shorter (sorry for newbieness).\n\nIf I tried to draw a straight line (\\\\(\\alpha x + \\beta\\\\)) on each chart, I could **kinda** see a negative \\\\(\\alpha\\\\) on \"Nurses\" (cases + fatalities), \"Health_USD\" (fatalities) and \"Health_GDP\" (fatalities) features, or at least I find them easier to see than others. Maybe they're good features.\n\nFor continuing the analysis of criticality, I'll plot bar charts with \"Nurses\" feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    # Making Nurse plot\n    plot_df = df_train.groupby([\"Province_State\"]).max()[['Nurses', 'Health_GDP']].sort_values(by='Nurses', ascending = False).iloc[:10].sort_values(by='Nurses', ascending = True)\n    fig, ax = plt.subplots(figsize=(16,8))\n    plt.title(\"Nurses per 1000 people\")\n    plt.xlabel(\"Nurses/1K\")\n    ax.barh(plot_df.index, plot_df['Nurses'], color='#99e699')\n    for i, v in enumerate(plot_df['Nurses']):\n        plt.text(v + 0.1, i-0.1, str(np.round(v, 1)), color='#1f7a1f')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Referring to the absolute confirmed cases chart and assuming everything I've assumed is correct, Germany seems to be in a good scenario, if you compare it with other countries.\n\nFor reasons of curiosity, I'll check my beloved country, Brazil, numbers."},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    df = df_train.groupby([\"Province_State\"]).max()\n    brazil = df.loc['Brazil']\n    brazil","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, it seems we're getting close to the top Cases_Density ratio but our Nurses/1K number is really low. Our numbers are not that optimistic as I wanted them to be.\n\nI can't even imagine how pessimistic they would be if I added news about COVID-19 on the dataset. Maybe do that in the future.\n\nNow it's time for me to add the \"Lag\" features for time series analysis, which was my only feature of interest on Week 2.\n\nBut before doing that, I must turn Date into numbers. For \"Date\" I'll just turn it into an integer in the interval \\\\([0, n]\\\\) being \\\\(0\\\\) the initial date and \\\\(n\\\\) the last date on the test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    # Making Date become number\n    import time\n    from datetime import datetime\n    df_train['Date'] = pd.to_datetime(df_train['Date'])\n    df_test['Date'] = pd.to_datetime(df_test['Date'])\n    df_train['Date'] = df_train['Date'].apply(lambda s: time.mktime(s.timetuple()))\n    df_test['Date'] = df_test['Date'].apply(lambda s: time.mktime(s.timetuple()))\n    min_timestamp = np.min(df_train['Date'])\n    df_train['Date'] = df_train['Date'].apply(lambda s: (s - min_timestamp) / 86400.0)\n    df_test['Date'] = df_test['Date'].apply(lambda s: (s - min_timestamp) / 86400.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    import time\n    from tqdm import tqdm\n\n    start_time = time.time()\n    lag_range = np.arange(1,15,1)\n    states = set(df_train['Province_State'])\n\n    with tqdm(total = len(list(states))) as pbar:\n        for state in states:\n            for d in df_train['Date'].drop_duplicates().astype('int'):\n                mask = (df_train['Date'] == d) & (df_train['Province_State'] == state)\n                for lag in lag_range:\n                    mask_org = (df_train['Date'] == (d - lag)) & (df_train['Province_State'] == state)\n                    try:\n                        df_train.loc[mask, 'ConfirmedCases_Lag_' + str(lag)] = df_train.loc[mask_org, 'ConfirmedCases'].values\n                    except:\n                        df_train.loc[mask, 'ConfirmedCases_Lag_' + str(lag)] = 0\n                    try:\n                        df_train.loc[mask, 'Fatalities_Lag_' + str(lag)] = df_train.loc[mask_org, 'Fatalities'].values\n                    except:\n                        df_train.loc[mask, 'Fatalities_Lag_' + str(lag)] = 0\n            pbar.update(1)\n    print('Time spent for building features is {} minutes'.format(round((time.time()-start_time)/60,1)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    # Never forget to add'em into your test dataset\n    missing_cols = set(df_train.columns) - set(df_test.columns) - set(['Id'])\n    print (\"Do not forget to add these columns into your test dataset: {}\".format(missing_cols))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature encoding\n\nBefore choosing features, modeling and making predictions, all features must be turned into numerical features. Otherwise, models just won't work.\n\nBut before that, I'll add the missing columns on the test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    for col in missing_cols:\n        df_test[col] = -1\n    missing_cols = set(df_train.columns) - set(df_test.columns) - set(['Id'])\n    if (missing_cols == set()):\n        print (\"No remaining missing columns on test dataset!\")\n    else:\n        print (\"Something's gone wrong, these are missing: {}\".format(missing_cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64', 'uint8']\n\n    all_columns = set(df_train.columns)\n    numeric_columns = set(df_train.select_dtypes(include=numerics).columns)\n    remaining_columns = all_columns - numeric_columns\n    print (\"Non-numerical columns: {}\".format(remaining_columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wait, what? Why is \"MedAge\" feature here? After checking, I realized some of the values were numbers in strings, so I'll just turn'em into numbers."},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    df_train['MedAge'] = df_train['MedAge'].astype(int)\n    df_test['MedAge'] = df_test['MedAge'].astype(int)\n    numeric_columns = set(df_train.select_dtypes(include=numerics).columns)\n    remaining_columns = all_columns - numeric_columns\n    print (\"Non-numerical columns: {}\".format(remaining_columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, so that is expected. For the sake of simplicity, I'll turn \"Country_Region\" and \"Province_State\" features into a single feature called \"Location\" and then drop'em. After that, I'll do One-Hot Encoding for them."},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    df_train['Location'] = ['_'.join(x) for x in zip(df_train['Country_Region'], df_train['Province_State'])]\n    df_test['Location'] = ['_'.join(x) for x in zip(df_test['Country_Region'], df_test['Province_State'])]\n    df_train.drop(columns=['Country_Region', 'Province_State'], inplace=True)\n    df_test.drop(columns=['Country_Region', 'Province_State'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    # One-hot encoding\n    df_train = pd.concat([df_train,pd.get_dummies(df_train['Location'], prefix='Location',dummy_na=False)],axis=1).drop(['Location'],axis=1)\n    df_test = pd.concat([df_test,pd.get_dummies(df_test['Location'], prefix='Location',dummy_na=False)],axis=1).drop(['Location'],axis=1)\n    df_train.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    all_columns = set(df_train.columns)\n    numeric_columns = set(df_train.select_dtypes(include=numerics).columns)\n    remaining_columns = all_columns - numeric_columns\n    print (\"Non-numerical columns: {}\".format(remaining_columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's Season encoding time! Nothing new here, I'll just do One-hot encoding again."},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    df_train = pd.concat([df_train,pd.get_dummies(df_train['Season'], prefix='Season',dummy_na=False)],axis=1).drop(['Season'],axis=1)\n    df_test = pd.concat([df_test,pd.get_dummies(df_test['Season'], prefix='Season',dummy_na=False)],axis=1).drop(['Season'],axis=1)\n    df_train.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shapes are different! Will find out what columns are missing and fill'em with zeros."},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    missing_seasons = (set(df_train.columns) - set(df_test.columns))\n    for col in missing_seasons:\n        if col.startswith('Season'):\n            df_test[col] = 0\n    df_train.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    # Double checking for test dataset columns\n    all_columns = set(df_test.columns)\n    numeric_columns = set(df_test.select_dtypes(include=numerics).columns)\n    remaining_columns = all_columns - numeric_columns\n    print (\"Non-numerical columns: {}\".format(remaining_columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    # Saving DFs\n    df_train.to_csv('df_train.csv')\n    df_test.to_csv('df_test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now everything is done! Going after feature selection!\n\n# Feature selection\n\nI wanted to use [Stability Selection](https://stat.ethz.ch/~nicolai/stability.pdf) for this, but I couldn't figure it out on how to import [this](https://github.com/scikit-learn-contrib/stability-selection).\n\nFor now, I'll just start with the SelectFromModel stuff."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LassoCV\n\nall_columns = list(df_train.columns)\nfor c in ['Id', 'ConfirmedCases', 'Fatalities']:\n    all_columns.remove(c)\nfor c in [x for x in df_train.columns if 'Hat' in x]:\n    all_columns.remove(c)    \n\nX = df_train[all_columns]\ny_cases = df_train['ConfirmedCases']\ny_fatal = df_train['Fatalities']\n\nX_scaler = StandardScaler()\nX = X_scaler.fit_transform(X)\n\ncases_cols = ['ConfirmedCases_Lag_1']\nfatal_cols = ['ConfirmedCases_Lag_1']\n# print (\" * Fitting ConfirmedCases\")\n# threshold = 0.25\n# clf = LassoCV()\n# sfm = SelectFromModel(clf, threshold=threshold)\n# sfm.fit(X, y_cases)\n# n_features = sfm.transform(X).shape[1]\n# print (\"   - Got {} features from threshold {}\".format(n_features, threshold))\n# while ((threshold < .95) and (n_features > 10)):\n#     threshold += .05\n#     sfm = SelectFromModel(clf, threshold=threshold)\n#     sfm.fit(X, y_cases)\n#     X_cases = sfm.transform(X)\n#     n_features = sfm.transform(X).shape[1]\n#     print (\"   - Got {} features from threshold {}\".format(n_features, threshold))\n# cases_cols = []\n# mask = sfm.get_support()\n# for i in range(len(all_columns)):\n#     if mask[i]:\n#         cases_cols.append(all_columns[i])\n# print (\"   - For ConfirmedCases, you'll want {}\".format(cases_cols))\n    \n# print (\" * Fitting Fatalities\")\n# threshold = 0.25\n# clf = LassoCV()\n# sfm = SelectFromModel(clf, threshold=threshold)\n# sfm.fit(X, y_fatal)\n# n_features = sfm.transform(X).shape[1]\n# print (\"   - Got {} features from threshold {}\".format(n_features, threshold))\n# while ((threshold < .95) and (n_features > 10)):\n#     threshold += .05\n#     sfm = SelectFromModel(clf, threshold=threshold)\n#     sfm.fit(X, y_fatal)\n#     X_fatal = sfm.transform(X)\n#     n_features = sfm.transform(X).shape[1]\n#     print (\"   - Got {} features from threshold {}\".format(n_features, threshold))\n# fatal_cols = []\n# mask = sfm.get_support()\n# for i in range(len(all_columns)):\n#     if mask[i]:\n#         fatal_cols.append(all_columns[i])\n# print (\"   - For Fatalities, you'll want {}\".format(fatal_cols))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After this first round on feature selection, I found out that I wanted fewer features for my training. For this, I'll now apply RFE."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFECV\nfrom sklearn.linear_model import BayesianRidge\n\nX_cases = df_train[all_columns]\nX_fatal = df_train[all_columns]\n\nX_cases_scaler = StandardScaler()\nX_fatal_scaler = StandardScaler()\nX_cases = X_cases_scaler.fit_transform(X_cases)\nX_fatal = X_fatal_scaler.fit_transform(X_fatal)\n\nmodel = BayesianRidge()\n\n# print (\" * Fitting ConfirmedCases...\")\n# selector_cases = RFECV (model)\n# selector_cases.fit(X_cases, y_cases)\n# cases_mask = selector_cases.support_\n# new_cases_cols = []\n# for i in range(len(cases_mask)):\n#     if cases_mask[i]:\n#         new_cases_cols.append(all_columns[i])\n# print (\"   - Wanted features are: {}\".format(new_cases_cols))\nnew_cases_cols = ['ConfirmedCases_Lag_1', 'ConfirmedCases_Lag_2', 'Fatalities_Lag_2', 'ConfirmedCases_Lag_3', 'Fatalities_Lag_3', 'ConfirmedCases_Lag_4', 'Fatalities_Lag_4']\n        \n# print (\" * Fitting Fatalities...\")\n# selector_fatal = RFECV (model)\n# selector_fatal.fit(X_fatal, y_fatal)\n# fatal_mask = selector_fatal.support_\n# new_fatal_cols = []\n# for i in range(len(fatal_mask)):\n#     if fatal_mask[i]:\n#         new_fatal_cols.append(all_columns[i])\n# print (\"   - Wanted features are: {}\".format(new_fatal_cols))\nnew_fatal_cols = ['ConfirmedCases_Lag_1', 'Fatalities_Lag_1', 'ConfirmedCases_Lag_2', 'Fatalities_Lag_2', 'Fatalities_Lag_3', 'ConfirmedCases_Lag_4', 'Fatalities_Lag_4', 'Fatalities_Lag_5', 'Fatalities_Lag_7', 'Fatalities_Lag_8', 'Fatalities_Lag_10', 'Fatalities_Lag_11', 'ConfirmedCases_Lag_13', 'Fatalities_Lag_13', 'ConfirmedCases_Lag_14', 'Fatalities_Lag_14']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well this was a huge reduction. For I'm not sure this is good, I'll try to choose which set of features to use by training and validating models.\n\nI'll now plot the features correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Heatmap of positive correlation features\nimport seaborn as sns\ncorrelation = df_train.corr()\nk = len([i for i in correlation['ConfirmedCases'] if abs(i) >= 0.75])\ncols = correlation.nlargest(k,'ConfirmedCases')['ConfirmedCases'].index\ncm = np.corrcoef(df_train[cols].values.T)\nf , ax = plt.subplots(figsize = (18,16))\nsns.heatmap(cm, vmax=.8, linewidths=0.01,square=True,annot=True,cmap='viridis',\n            linecolor=\"white\",xticklabels = cols.values ,annot_kws = {'size':12},yticklabels = cols.values)\nax.set_title('ConfirmedCases correlation heatmap')\nplt.show()\nmy_cases_cols = list(cols)\nfor c in ['Id', 'ConfirmedCases', 'Fatalities']:\n    try:\n        my_cases_cols.remove(c)\n    except ValueError:\n        pass\nfor c in [x for x in df_train.columns if 'Hat' in x]:\n    try:\n        my_cases_cols.remove(c)\n    except ValueError:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = len([i for i in correlation['Fatalities'] if abs(i) >= 0.75])\ncols = correlation.nlargest(k,'Fatalities')['Fatalities'].index\ncm = np.corrcoef(df_train[cols].values.T)\nf , ax = plt.subplots(figsize = (18,16))\nsns.heatmap(cm, vmax=.8, linewidths=0.01,square=True,annot=True,cmap='viridis',\n            linecolor=\"white\",xticklabels = cols.values ,annot_kws = {'size':12},yticklabels = cols.values)\nax.set_title('Fatalities correlation heatmap')\nplt.show()\nmy_fatal_cols = list(cols)\nfor c in ['Id', 'ConfirmedCases', 'Fatalities']:\n    try:\n        my_fatal_cols.remove(c)\n    except ValueError:\n        pass\nfor c in [x for x in df_train.columns if 'Hat' in x]:\n    try:\n        my_fatal_cols.remove(c)\n    except ValueError:\n        pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generating validation set\n\nAs some of the training data intersects with some test data, there's the possibility of building a validation dataset. For that, I'll crop the intersection to a new dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"index = df_test[df_test['Date'] <= np.max(df_train['Date'])].index\ndf_intersection = df_train[df_train['Date'] >= np.min(df_test['Date'])]\ndf_intersection.set_index(index)\ndf_intersection['ForecastId'] = df_test[df_test['Date'] <= np.max(df_train['Date'])]['ForecastId'].values\ndf_intersection.drop(columns=['Id'], inplace=True)\ndf_intersection.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train[df_train['Date'] < np.min(df_test['Date'])]\ndf_train.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create scalers dict\nscaler = {}\n# Set cases train, valid\nX_cases = df_train[cases_cols]\n# Set fatal train, valid\nX_fatal = df_train[fatal_cols]\n# Set scalers\nscaler['cases'] = []\nscaler['cases'].append(StandardScaler())\nscaler['fatal'] = []\nscaler['fatal'].append(StandardScaler())\n# Scaling cases\nX_cases_one = scaler['cases'][0].fit_transform(X_cases)\n# Scaling fatal\nX_fatal_one = scaler['fatal'][0].fit_transform(X_fatal)\n# Set cases train, valid\nX_cases = df_train[new_cases_cols]\n# Set fatal train, valid\nX_fatal = df_train[new_fatal_cols]\n# Scaling cases\nscaler['cases'].append(StandardScaler())\nX_cases_two = scaler['cases'][1].fit_transform(X_cases)\n# Scaling fatal\nscaler['fatal'].append(StandardScaler())\nX_fatal_two = scaler['fatal'][1].fit_transform(X_fatal)\n# Set cases train, valid\nX_cases = df_train[my_cases_cols]\n# Set fatal train, valid\nX_fatal = df_train[my_fatal_cols]\n# Scaling cases\nscaler['cases'].append(StandardScaler())\nX_cases_three = scaler['cases'][2].fit_transform(X_cases)\n# Scaling fatal\nscaler['fatal'].append(StandardScaler())\nX_fatal_three = scaler['fatal'][2].fit_transform(X_fatal)\n# Getting y\ny_cases = df_train['ConfirmedCases']\ny_fatal = df_train['Fatalities']\n\nX_train = {\n    'cases' : [X_cases_one, X_cases_two, X_cases_three],\n    'fatal' : [X_cases_two, X_fatal_two, X_fatal_three]\n}\n\ny_train = {\n    'cases' : y_cases,\n    'fatal' : y_fatal\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling with linear regressors\n\nOn this section, I'll try a few linear regressors for predicting the test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, BayesianRidge, Lasso, LassoLars, ElasticNet, TheilSenRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_log_error\nimport keras.backend as K\n\ndef root_mean_squared_log_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(K.log(y_pred + 1) - K.log(y_true + 1)))) \n\ndef rmsle(estimator, X, y0):\n    y = estimator.predict(X)\n    if len(y[y<=-1]) != 0:\n        y[y<=-1] = 0.0\n    assert len(y) == len(y0)\n    r = np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(y0), 2)))\n    if math.isnan(r):\n        print(\"this is a nan\")\n        print(scipy.stats.describe(y))\n        plt.hist(y, bins=10, color='blue')\n        plt.show()\n    return r\n\nmodels = [\n    LinearRegression(),\n    Ridge(),\n    SGDRegressor(),\n    BayesianRidge(),\n    Lasso(),\n    LassoLars(),\n    ElasticNet(),\n    TheilSenRegressor()\n]\n\nfor pred_type in X_train:\n    print (\" * Predicting {}...\".format(pred_type))\n    for model in models:\n        print (\"   - {}\".format(model))\n        print (\"      . Dataset 1: \", end='')\n        scores = cross_val_score(model, X_train[pred_type][0], y_train[pred_type], cv=5, scoring=rmsle)\n        print (scores.mean())\n        print (\"      . Dataset 2: \", end='')\n        scores = cross_val_score(model, X_train[pred_type][1], y_train[pred_type], cv=5, scoring=rmsle)\n        print (scores.mean())\n        print (\"      . Dataset 3: \", end='')\n        scores = cross_val_score(model, X_train[pred_type][2], y_train[pred_type], cv=5, scoring=rmsle)\n        print (scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So based on these scores, I'll remove the following models from my future predictions:\n\nFor ConfirmedCases:\n* SGDRegressor\n* LassoLars\n* ElasticNet\n\nFor Fatalities:\n* SGDRegressor\n* LassoLars"},{"metadata":{},"cell_type":"markdown","source":"# Modeling with XGBoost / LightGBM\n\nAs I saw many people using these particular two tree-based models for predicting, I'll try them myself too!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nmodels = [\n    XGBRegressor(),\n    LGBMRegressor()\n]\n\nfor pred_type in X_train:\n    print (\" * Predicting {}...\".format(pred_type))\n    for model in models:\n        print (\"   - {}\".format(model))\n        print (\"      . Dataset 1: \", end='')\n        scores = cross_val_score(model, X_train[pred_type][0], y_train[pred_type], cv=5, scoring=rmsle)\n        print (scores.mean())\n        print (\"      . Dataset 2: \", end='')\n        scores = cross_val_score(model, X_train[pred_type][1], y_train[pred_type], cv=5, scoring=rmsle)\n        print (scores.mean())\n        print (\"      . Dataset 3: \", end='')\n        scores = cross_val_score(model, X_train[pred_type][2], y_train[pred_type], cv=5, scoring=rmsle)\n        print (scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well the results seems good!!! Can't wait to get to the bottom of this notebook!"},{"metadata":{},"cell_type":"markdown","source":"# Model ensembling\n\nOn this section, the whole idea is to ensemble some of the models in order to reach a better result.\n\nI'll ensemble using a CustomEnsembler I wrote [here](https://www.kaggle.com/gabrielmilan/ames-iowa-house-prices-to-be-improved), on my Iowa House Prices Notebook.\n\nFor the TheilSenRegressor takes TOO LONG to fit, I'll remove it from the combinations, run everything and then try some combinations with the TheilSenRegressor myself, as it delivers good results."},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nfrom tqdm import tqdm\nfrom sklearn.model_selection import KFold\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n\n########################################################################################################################################\nclass CustomEnsemble (BaseEstimator, RegressorMixin, TransformerMixin):\n    \n    def __init__(self, models, meta_model):\n        self.models = models\n        self.modelsNames = [a.__str__().split(\"(\")[0] for a in self.models]\n        self.meta_model = meta_model\n        \n    def fit(self,X,y):\n        predictions = np.zeros((X.shape[0], len(self.models)))\n        for i, model in enumerate(self.models):\n            model.fit (X, y)\n            predictions[:,i] = model.predict(X)\n        self.meta_model.fit(predictions, y)\n            \n    def predict(self,X):\n        predictions = np.zeros((X.shape[0], len(self.models)))\n        for i, model in enumerate(self.models):\n            predictions[:,i] = model.predict(X)\n        return self.meta_model.predict(predictions)\n    \n    def __str__ (self):\n        return \"<CustomEnsemble (meta={}, models={})>\".format(self.meta_model.__str__().split(\"(\")[0], self.modelsNames)\n    \n    def __repr__ (self):\n        return self.__str__()\n########################################################################################################################################\n\n# Defining function for making models combinations\ndef make_combinations (iterable):\n    from itertools import combinations\n    my_combs = []\n    for item in iterable.copy():\n        iterable.remove(item)\n        for i in range(len(iterable)):\n            for comb in combinations(iterable, i+1):\n                my_combs.append((item, comb))\n        iterable.append(item)\n    return my_combs\n\ncases_models = [\n    #TheilSenRegressor(),\n    XGBRegressor(),\n    LGBMRegressor()\n]\n\nfatal_models = [\n    LinearRegression(),\n    Ridge(),\n    BayesianRidge(),\n    Lasso(),\n    #TheilSenRegressor(),\n    XGBRegressor(),\n    LGBMRegressor()\n]\n\ncases_combs = make_combinations(cases_models)\nfatal_combs = make_combinations(fatal_models)\n\nmodels = {}\nmodels['cases'] = []\nmodels['fatal'] = []\nfor comb in cases_combs:\n    models['cases'].append(CustomEnsemble(meta_model=comb[0], models=comb[1]))\nfor comb in fatal_combs:\n    models['fatal'].append(CustomEnsemble(meta_model=comb[0], models=comb[1]))\n\nbest_score = {}\nbest_score['cases'] = 10e3\nbest_score['fatal'] = 10e3\nbest_model = {}\nbest_model['cases'] = None\nbest_model['fatal'] = None\nbest_dataset = {}\nbest_dataset['cases'] = None\nbest_dataset['fatal'] = None\n\nif False:\n    print (\" --> I'll test {} models! :D\".format(len(models['cases']) + len(models['fatal'])))\n    with tqdm(total = len(models['cases']) + len(models['fatal'])) as pbar:\n        for pred_type in X_train:\n            #print (\" * Predicting {}...\".format(pred_type))\n            for model in models[pred_type]:\n                ##\n                score = cross_val_score(model, X_train[pred_type][0], y_train[pred_type], cv=5, scoring=rmsle).mean()\n                if (score < best_score[pred_type]):\n                    best_score[pred_type] = score\n                    best_model[pred_type] = model\n                    best_dataset[pred_type] = X_train[pred_type][0]\n                ##\n                score = cross_val_score(model, X_train[pred_type][1], y_train[pred_type], cv=5, scoring=rmsle).mean()\n                if (score < best_score[pred_type]):\n                    best_score[pred_type] = score\n                    best_model[pred_type] = model\n                    best_dataset[pred_type] = X_train[pred_type][1]\n                ##\n                score = cross_val_score(model, X_train[pred_type][2], y_train[pred_type], cv=5, scoring=rmsle).mean()\n                if (score < best_score[pred_type]):\n                    best_score[pred_type] = score\n                    best_model[pred_type] = model\n                    best_dataset[pred_type] = X_train[pred_type][2]\n                ##\n                pbar.update(1)\nelse:\n    best_model['cases'] = CustomEnsemble(meta_model=LGBMRegressor(), models=[XGBRegressor()])\n    best_score['cases'] = 0.23218969104329368\n    best_dataset['cases'] = X_train['cases'][2]\n    best_model['fatal'] = CustomEnsemble(meta_model=XGBRegressor(), models=[LGBMRegressor(), Lasso()])\n    best_score['fatal'] = 0.1117526321099606\n    best_dataset['fatal'] = X_train['fatal'][2]\n    \nprint (\"Cases:\\n=> Best model: {}\\n=> Best score: {:.4f}\\n\\nFatalities:\\n=> Best model: {}\\n=> Best score: {:.4f}\".format(best_model['cases'], best_score['cases'], best_model['fatal'], best_score['fatal']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I'll test a few models myself"},{"metadata":{"trusted":true},"cell_type":"code","source":"if FULL_RUN:\n    models['cases'] = [\n        CustomEnsemble(meta_model=TheilSenRegressor(), models=[XGBRegressor(), LGBMRegressor(), Lasso()]),\n        CustomEnsemble(meta_model=LGBMRegressor(), models=[XGBRegressor(), TheilSenRegressor()]),\n        CustomEnsemble(meta_model=XGBRegressor(), models=[LGBMRegressor(), TheilSenRegressor()]),\n        LGBMRegressor(),\n    ]\n\n    models['fatal'] = [\n        CustomEnsemble(meta_model=TheilSenRegressor(), models=[LGBMRegressor(), Lasso(), XGBRegressor()]),\n        CustomEnsemble(meta_model=LGBMRegressor(), models=[TheilSenRegressor(), XGBRegressor(), Lasso()]),\n        CustomEnsemble(meta_model=XGBRegressor(), models=[TheilSenRegressor(), LGBMRegressor(), Lasso()]),\n        LGBMRegressor(),\n    ]\n\n    for pred_type in X_train:\n        print (\" * Predicting {}...\".format(pred_type))\n        for model in models[pred_type]:\n            score = cross_val_score(model, X_train[pred_type][2], y_train[pred_type], cv=5, scoring=rmsle).mean()\n            print (\"-> Score: {}\".format(score))\n            if (score < best_score[pred_type]):\n                best_score[pred_type] = score\n                best_model[pred_type] = model\n                best_dataset[pred_type] = X_train[pred_type][2]\n                print (\"Score got better for model {}\".format(model))\nelse:\n    best_model['cases'] = CustomEnsemble(meta_model=TheilSenRegressor(), models=[XGBRegressor(), LGBMRegressor(), Lasso()])\n    best_score['cases'] = 0.22054589547642367","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting\n\nBefore we do this, I'll set a few helping functions."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotStatus (location):\n    plt.figure(figsize=(14,8))\n    plt.title('COVID-19 cases on {}'.format(location))\n    df = df_train[df_train[location] == 1]\n    test = df_test[df_test[location] == 1]\n    intersection = df_intersection[df_intersection[location] == 1]\n    idx = df_test[df_test[location] == 1].index\n    legend = []\n    plt.xlabel('#Days since dataset')\n    plt.ylabel('Number')\n    plt.plot(df['Date'], df['ConfirmedCases'])\n    plt.plot(test['Date'], test['ConfirmedCases'])\n    plt.plot(intersection['Date'], intersection['ConfirmedCases'])\n    legend.append('{} confirmed cases'.format(location))\n    legend.append('{} predicted cases'.format(location))\n    legend.append('{} actual cases'.format(location))\n    plt.legend(legend)\n    plt.show()\n    legend = []\n    plt.figure(figsize=(14,8))\n    plt.title('COVID-19 fatalities on {}'.format(location))\n    plt.xlabel('#Days since dataset')\n    plt.ylabel('Number')\n    plt.plot(df['Date'], df['Fatalities'])\n    plt.plot(test['Date'], test['Fatalities'])\n    plt.plot(intersection['Date'], intersection['Fatalities'])\n    legend.append('{} fatalities'.format(location))\n    legend.append('{} predicted fatalities'.format(location))\n    legend.append('{} actual fatalities'.format(location))\n    plt.show()\n\ndef rmsle (location):\n    idx = df_test[(df_test[location] == 1) & (df_test['Date'] <= df_intersection['Date'].max())].index\n    my_sub = df_test.loc[idx][['ConfirmedCases', 'Fatalities']]\n    cases_pred = my_sub['ConfirmedCases'].values\n    fatal_pred = my_sub['Fatalities'].values\n    idx = df_intersection[df_intersection[location] == 1].index\n    cases_targ = df_intersection.loc[idx]['ConfirmedCases'].values\n    fatal_targ = df_intersection.loc[idx]['Fatalities'].values\n    cases = np.sqrt(mean_squared_log_error( cases_targ, cases_pred ))\n    fatal = np.sqrt(mean_squared_log_error( fatal_targ, fatal_pred ))\n    return cases, fatal\n\ndef avg_rmsle():\n    idx = df_intersection.index\n    my_sub = df_test.loc[idx][['ConfirmedCases', 'Fatalities']]\n    cases_pred = my_sub['ConfirmedCases'].values\n    fatal_pred = my_sub['Fatalities'].values\n    cases_targ = df_intersection.loc[idx]['ConfirmedCases'].values\n    fatal_targ = df_intersection.loc[idx]['Fatalities'].values\n    cases_score = np.sqrt(mean_squared_log_error( cases_targ, cases_pred ))\n    fatal_score = np.sqrt(mean_squared_log_error( fatal_targ, fatal_pred ))\n    score = (cases_score + fatal_score)/2\n    return score\n\ndef handle_predictions (predictions, lowest = 0):\n    #predictions = np.round(predictions, 0)\n    # Predictions can't be negative\n    predictions[predictions < 0] = 0\n    # Predictions can't decrease from greatest value on train dataset\n    predictions[predictions < lowest] = lowest\n    # Predictions can't decrease over time\n    for i in range(1, len(predictions)):\n        if predictions[i] < predictions[i - 1]:\n            predictions[i] = predictions[i - 1]\n    #return predictions.astype(int)\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now I need to fill the test dataset with the data I have on the intersection.\n\nI set a flag \"use_predictions\" in order to set if I want it to predict for the private leaderboard or to the public one."},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = []\nlag_range = np.arange(1,15,1)\nfor lag in lag_range:\n    cols.append(\"ConfirmedCases_Lag_{}\".format(lag))\n    cols.append(\"Fatalities_Lag_{}\".format(lag))\ntest_intersection_mask = (df_test['Date'] <= df_intersection['Date'].max())\ntrain_intersection_mask = (df_intersection['Date'] >= df_test['Date'].min())\ndf_test.loc[test_intersection_mask, cols] = df_intersection.loc[train_intersection_mask, cols].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now c'mon, I can't take it anymore, I NEED TO TEST THIS!"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_cases = best_model['cases']\nmodel_fatal = best_model['fatal']\n\ninput_cols = list(set(my_cases_cols + my_fatal_cols))\n\nmodel_cases.fit(X_train['cases'][2], y_train['cases'])\nmodel_fatal.fit(X_train['fatal'][2], y_train['fatal'])\n\nuse_predictions = False\npred_dt_range = range(int(df_test['Date'].min()), int(df_test['Date'].max()) + 1)\nlocations = [col for col in df_train.columns if col.startswith('Location')]\nrandom_validation_set = ['Location_Brazil_Brazil', 'Location_US_New York', 'Location_Afghanistan_Afghanistan', 'Location_China_Zhejiang', 'Location_Italy_Italy']#random.sample(states, 10)\npred_input = locations\n\nstart_time = time.time()\nwith tqdm(total = len(list(pred_input))) as pbar:\n    for location in pred_input:\n        for d in pred_dt_range:\n            mask = (df_test['Date'] == d) & (df_test[location] == 1)\n            if (d > df_intersection['Date'].max()):\n                for lag in lag_range:\n                    mask_org = (df_test['Date'] == (d - lag)) & (df_test[location] == 1)\n                    try:\n                        df_test.loc[mask, 'ConfirmedCases_Lag_' + str(lag)] = df_test.loc[mask_org, 'ConfirmedCases'].values\n                    except:\n                        df_test.loc[mask, 'ConfirmedCases_Lag_' + str(lag)] = 0\n                    try:\n                        df_test.loc[mask, 'Fatalities_Lag_' + str(lag)] = df_test.loc[mask_org, 'Fatalities'].values\n                    except:\n                        df_test.loc[mask, 'Fatalities_Lag_' + str(lag)] = 0\n            X_test  = df_test.loc[mask, input_cols]\n            # Cases\n            X_test_cases = X_test[my_cases_cols].values\n            X_test_cases = scaler['cases'][2].transform(X_test_cases)\n            next_cases = model_cases.predict(X_test_cases)\n            # Fatal\n            X_test_fatal = X_test[my_fatal_cols].values\n            X_test_fatal = scaler['fatal'][2].transform(X_test_fatal)\n            next_fatal = model_fatal.predict(X_test_fatal)\n            # Update df_test\n            if (d > np.max(df_train['Date'].values)):\n                if (next_cases < 0):\n                    next_cases = 0\n                if (next_cases < X_test['ConfirmedCases_Lag_1'].values[0]):\n                    next_cases = X_test['ConfirmedCases_Lag_1'].values[0]\n                df_test.loc[mask, 'ConfirmedCases'] = next_cases\n                if (next_fatal < 0):\n                    next_fatal = 0\n                if (next_fatal < X_test['Fatalities_Lag_1'].values[0]):\n                    next_fatal = X_test['Fatalities_Lag_1'].values[0]\n                df_test.loc[mask, 'Fatalities'] = next_fatal\n            else:\n                if use_predictions:\n                    if (next_cases < 0):\n                        next_cases = 0\n                    if (next_cases < X_test['ConfirmedCases_Lag_1'].values[0]):\n                        next_cases = X_test['ConfirmedCases_Lag_1'].values[0]\n                    df_test.loc[mask, 'ConfirmedCases'] = next_cases\n                    if (next_fatal < 0):\n                        next_fatal = 0\n                    if (next_fatal < X_test['Fatalities_Lag_1'].values[0]):\n                        next_fatal = X_test['Fatalities_Lag_1'].values[0]\n                    df_test.loc[mask, 'Fatalities'] = next_fatal\n        # Fill cases\n        lowest_pred = np.max(df_train[df_train[location] == 1]['ConfirmedCases'].values)\n        cases = handle_predictions (df_test[df_test[location] == 1]['ConfirmedCases'].values, lowest_pred)\n        # Fill fatal\n        lowest_pred = np.max(df_train[df_train[location] == 1]['Fatalities'].values)\n        cases = handle_predictions (df_test[df_test[location] == 1]['Fatalities'].values, lowest_pred)\n        # Update progress bar\n        pbar.update(1)\n        \nprint('Time spent for predicting everything was {} minutes'.format(round((time.time()-start_time)/60,1)))\n#avg_rmsle()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sanity Check"},{"metadata":{"trusted":true},"cell_type":"code","source":"cases = []\nfatal = []\nfor a in random_validation_set:\n    score = rmsle(a)\n    cases.append(score[0])\n    fatal.append(score[1])\n    print(score)\nprint (\"Average = {}, {}\".format(np.average(cases), np.average(fatal)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for a in random_validation_set:\n    plotStatus(a)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TO-DO (Week 4):\n\n## EDA\n* Maybe add more weather data?\n* Get the \"Recovered\" data from coronavirus-latlon-dataset (not sure if I'll do this)\n* Merge the \"covid19-containment-and-mitigation-measures\" dataset (implies in hard work with text processing)\n* Why do the first derivatives of cases and fatalities go low on 2020-02-10?\n\n## Modeling\n* Hyper-parameters tuning\n* Check the Gompertz model as seen on [sadiakhalil's notebook](https://www.kaggle.com/sadiakhalil/covid-19-global-eda-forecast-2#Final-Submission-Using-Gompertz-Model) using [this](https://arxiv.org/ftp/arxiv/papers/2003/2003.05447.pdf);\n* Apply the best LSTM model I could reach on [Week 2](https://www.kaggle.com/gabrielmilan/covid-19-forecasting-with-lstm?scriptVersionId=31277554);\n* Attempt to reach a good model using the approach I saw [here](https://www.kaggle.com/aerdem4/covid19-w2-final-v2), which I just loved."},{"metadata":{"trusted":true},"cell_type":"code","source":"output_cols = ['ConfirmedCases', 'Fatalities']\nsubmission = df_test[['ForecastId'] + output_cols]\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}