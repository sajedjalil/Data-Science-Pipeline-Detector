{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## DEBUG LOG\n\n- Version 9: Used RMSE as a loss function. Gradcam looks generally on point.\n- Version 10: Used BCE as a loss function. Gradcam looks horrendous.","metadata":{}},{"cell_type":"markdown","source":"## Flags","metadata":{}},{"cell_type":"code","source":"# Whether this notebook is inference mode or not\nis_inference = False\n\n# Whether to normalize target from [0,100] to [0,1]\nis_normalize_target = True","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:28:10.458366Z","iopub.execute_input":"2021-12-17T11:28:10.458702Z","iopub.status.idle":"2021-12-17T11:28:10.484087Z","shell.execute_reply.started":"2021-12-17T11:28:10.45863Z","shell.execute_reply":"2021-12-17T11:28:10.483379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dependencies","metadata":{}},{"cell_type":"code","source":"if not is_inference:\n    !pip install -q torch-summary==1.4.5\n    !pip install -q timm\n    !pip install -q --upgrade wandb\n    !pip install -q grad-cam==1.3.3\n    !pip install -q torch-lr-finder==0.2.1\nelse:\n    import sys\n    sys.path.append(\"../input/timm-pytorch-image-models/pytorch-image-models-master\")\n    sys.path.append(\"../input/reighns-gradcam/pytorch-grad-cam-master\")\n    sys.path.append(\"../input/reighns-ttach/ttach-master\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:28:10.485814Z","iopub.execute_input":"2021-12-17T11:28:10.486139Z","iopub.status.idle":"2021-12-17T11:29:07.932434Z","shell.execute_reply.started":"2021-12-17T11:28:10.486104Z","shell.execute_reply":"2021-12-17T11:29:07.931522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections\nimport gc\nimport os\nimport random\nimport warnings\nfrom collections import defaultdict\nfrom dataclasses import asdict, dataclass, field\nfrom typing import *\n\nimport numpy as np\nimport pandas as pd\nimport timm\nimport torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:07.936101Z","iopub.execute_input":"2021-12-17T11:29:07.936369Z","iopub.status.idle":"2021-12-17T11:29:14.529991Z","shell.execute_reply.started":"2021-12-17T11:29:07.936323Z","shell.execute_reply":"2021-12-17T11:29:14.529132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configurations","metadata":{}},{"cell_type":"markdown","source":"### Config class\n\nWe create a `config` class to create directories and store some initialization parameters. Note this `config` class does not store any information of the training process.\n\n---\n\nCalling `config` will then create the folders. You can simply go check if the folders are indeed created in the `output` directory of Kaggle.\n\nEven if you do not call `config` explicitly, it will also create the folders for you if you are using a script. \n```python\nimport config # will initialize the process\n```","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport pathlib\n\nclass config:\n    warnings.filterwarnings(\"ignore\", category=UserWarning)\n    # Need to change in local path\n    DATA_DIR = Path(\"../input/petfinder-pawpularity-score\")\n    OUTPUT_DIR = Path(\"./\")\n    LOGS_DIR = Path(OUTPUT_DIR, \"logs\")\n    MODEL_DIR = Path(OUTPUT_DIR, \"model\")\n    OOF_DIR = Path(OUTPUT_DIR, \"oof\")\n    LOGS_DIR.mkdir(parents=True, exist_ok=True)\n    MODEL_DIR.mkdir(parents=True, exist_ok=True)\n    OOF_DIR.mkdir(parents=True, exist_ok=True)\n    \nconfig","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:14.532466Z","iopub.execute_input":"2021-12-17T11:29:14.532732Z","iopub.status.idle":"2021-12-17T11:29:14.546055Z","shell.execute_reply.started":"2021-12-17T11:29:14.532696Z","shell.execute_reply":"2021-12-17T11:29:14.545193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logger\n\nInstead of simply printing, we can create a logger to log your bugs, info into a file. This is important when debugging too!","metadata":{}},{"cell_type":"code","source":"from logging import INFO, FileHandler, Formatter, StreamHandler, getLogger\nimport logging\n\ndef init_logger(log_file: str = Path(config.LOGS_DIR, \"info.log\")) -> logging.Logger:\n    \"\"\"Initialize logger and save to file.\n\n    Consider having more log_file paths to save, eg: debug.log, error.log, etc.\n\n    Args:\n        log_file (str, optional): [description]. Defaults to Path(LOGS_DIR, \"info.log\").\n\n    Returns:\n        logging.Logger: [description]\n    \"\"\"\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    stream_handler = StreamHandler()\n    stream_handler.setFormatter(\n        Formatter(\"%(asctime)s: %(message)s\", \"%Y-%m-%d %H:%M:%S\")\n    )\n    file_handler = FileHandler(filename=log_file)\n    file_handler.setFormatter(\n        Formatter(\"%(asctime)s: %(message)s\", \"%Y-%m-%d %H:%M:%S\")\n    )\n    logger.addHandler(stream_handler)\n    logger.addHandler(file_handler)\n\n    return logger","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:14.547494Z","iopub.execute_input":"2021-12-17T11:29:14.547779Z","iopub.status.idle":"2021-12-17T11:29:14.564113Z","shell.execute_reply.started":"2021-12-17T11:29:14.547742Z","shell.execute_reply":"2021-12-17T11:29:14.563288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config.logger = init_logger()","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:29:14.565481Z","iopub.execute_input":"2021-12-17T11:29:14.565806Z","iopub.status.idle":"2021-12-17T11:29:14.576744Z","shell.execute_reply.started":"2021-12-17T11:29:14.565741Z","shell.execute_reply":"2021-12-17T11:29:14.575788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Device\n\nWhether we are using GPU or CPU?","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nconfig.logger.info(f\"We are using {device}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:29:14.578132Z","iopub.execute_input":"2021-12-17T11:29:14.578589Z","iopub.status.idle":"2021-12-17T11:29:14.630142Z","shell.execute_reply.started":"2021-12-17T11:29:14.578547Z","shell.execute_reply":"2021-12-17T11:29:14.629324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Global Parameters\n\nThis part is where one store all their parameters, I partitioned parameters into different dataclasses object.\n\nAs an example, `ModelParams`, just like its name, handles primarily the parameters inside a model, be it default or tunable params. Note that there might be overlap in terms of the parameters.\n\n---\n\n**PLEASE UNHIDE CELL BELOW TO MAKE CHANGES TO THE GLOBAL PARAMETERS!**","metadata":{}},{"cell_type":"code","source":"from dataclasses import dataclass, field, asdict\nimport pathlib\nfrom typing import Any, Dict, List\nimport wandb\n\n\n@dataclass\nclass FilePaths:\n    \"\"\"Class to keep track of the files.\"\"\"\n\n    train_images: pathlib.Path = pathlib.Path(config.DATA_DIR, \"train\")\n    test_images: pathlib.Path = pathlib.Path(config.DATA_DIR, \"test\")\n    train_csv: pathlib.Path = pathlib.Path(config.DATA_DIR, \"train.csv\")\n    test_csv: pathlib.Path = pathlib.Path(config.DATA_DIR, \"test.csv\")\n    sub_csv: pathlib.Path = pathlib.Path(\n        config.DATA_DIR, \"sample_submission.csv\"\n    )\n    folds_csv: pathlib.Path = pathlib.Path(config.OUTPUT_DIR, \"df_folds.csv\")\n    weight_path: pathlib.Path = pathlib.Path(config.MODEL_DIR)\n    oof_csv: pathlib.Path = pathlib.Path(config.OOF_DIR)\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return asdict(self)\n\n\n@dataclass\nclass DataLoaderParams:\n    \"\"\"Class to keep track of the data loader parameters.\"\"\"\n\n    train_loader: Dict[str, Any] = field(\n        default_factory=lambda: {\n            \"batch_size\": 32,\n            \"num_workers\": 4,\n            \"pin_memory\": False,\n            \"drop_last\": True,\n            \"shuffle\": True,\n            \"collate_fn\": None,\n        }\n    )\n    valid_loader: Dict[str, Any] = field(\n        default_factory=lambda: {\n            \"batch_size\": 32,\n            \"num_workers\": 4,\n            \"pin_memory\": False,\n            \"drop_last\": False,\n            \"shuffle\": False,\n            \"collate_fn\": None,\n        }\n    )\n\n    test_loader: Dict[str, Any] = field(\n        default_factory=lambda: {\n            \"batch_size\": 8,\n            \"num_workers\": 0,\n            \"pin_memory\": True,\n            \"drop_last\": False,\n            \"shuffle\": False,\n            \"collate_fn\": None,\n        }\n    )\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return asdict(self)\n\n    def get_len_train_loader(self):\n        total_rows = pd.read_csv(FilePaths().train_csv).shape[\n            0\n        ]  # get total number of rows/images\n        total_rows_per_fold = total_rows / (MakeFolds().num_folds)\n        total_rows_per_training = total_rows_per_fold * (\n            MakeFolds().num_folds - 1\n        )  # if got 1000 images, 10 folds, then train on 9 folds = 1000/10 * (10-1) = 100 * 9 = 900\n        len_of_train_loader = (\n            total_rows_per_training // self.train_loader[\"batch_size\"]\n        )  # if 900 rows, bs is 16, then 900/16 = 56.25, but we drop last if dataloader, so become 56 steps. if not 57 steps.\n        return int(len_of_train_loader)\n\n\n@dataclass\nclass MakeFolds:\n    \"\"\"A class to keep track of cross-validation schema.\n    seed (int): random seed for reproducibility.\n    num_folds (int): number of folds.\n    cv_schema (str): cross-validation schema.\n    class_col_name (str): name of the target column.\n    image_col_name (str): name of the image column.\n    folds_csv (str): path to the folds csv.\n    \"\"\"\n\n    seed: int = 1992\n    num_folds: int = 5\n    cv_schema: str = \"StratifiedKFold\"\n    class_col_name: str = \"Pawpularity\"\n    image_col_name: str = \"Id\"\n    folds_csv: pathlib.Path = FilePaths().folds_csv\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return asdict(self)\n\n\n@dataclass\nclass AugmentationParams:\n    \"\"\"Class to keep track of the augmentation parameters.\"\"\"\n\n    mean: List[float] = field(default_factory=lambda: [0.485, 0.456, 0.406])\n    std: List[float] = field(default_factory=lambda: [0.229, 0.224, 0.225])\n    image_size: int = 224\n    mixup: bool = True\n    mixup_params: Dict[str, Any] = field(\n        default_factory=lambda: {\"mixup_alpha\": 0.2, \"use_cuda\": True}\n    )\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return asdict(self)\n\n\n@dataclass\nclass CriterionParams:\n    \"\"\"A class to track loss function parameters.\"\"\"\n\n    train_criterion_name: str = \"BCEWithLogitsLoss\"\n    valid_criterion_name: str = \"BCEWithLogitsLoss\"\n    train_criterion_params: Dict[str, Any] = field(\n        default_factory=lambda: {\n            \"weight\": None,\n            \"reduction\": \"mean\",\n            \"pos_weight\": None,\n        }\n    )\n    valid_criterion_params: Dict[str, Any] = field(\n        default_factory=lambda: {\n            \"weight\": None,\n            \"reduction\": \"mean\",\n            \"pos_weight\": None,\n        }\n    )\n\n\n@dataclass\nclass ModelParams:\n    \"\"\"A class to track model parameters.\n    model_name (str): name of the model.\n    pretrained (bool): If True, use pretrained model.\n    input_channels (int): RGB image - 3 channels or Grayscale 1 channel\n    output_dimension: Final output neuron.\n                      It is the number of classes in classification.\n                      Caution: If you use sigmoid layer for Binary, then it is 1.\n    \"\"\"\n\n    model_name: str = \"swin_large_patch4_window7_224\"\n    pretrained: bool = True\n    input_channels: int = 3\n    output_dimension: int = 1\n    classification_type: str = \"\"\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return asdict(self)\n\n    def check_dimension(self) -> None:\n        \"\"\"Check if the output dimension is correct.\"\"\"\n        if (\n            self.classification_type == \"binary\"\n            and CriterionParams().train_criterion_name == \"BCEWithLogitsLoss\"\n        ):\n            assert self.output_dimension == 1, \"Output dimension should be 1\"\n        elif self.classification_type == \"multilabel\":\n            config.logger.info(\n                \"Check on output dimensions as we are likely using BCEWithLogitsLoss\"\n            )\n\n\n@dataclass\nclass GlobalTrainParams:\n    debug: bool = False\n    debug_multipler: int = 16\n    epochs: int = 6  # 6\n    mixup: bool = AugmentationParams().mixup\n    patience: int = 3\n    model_name: str = ModelParams().model_name\n    num_classes: int = ModelParams().output_dimension\n    classification_type: str = ModelParams().classification_type\n    use_amp: bool = True\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return asdict(self)\n\n\n@dataclass\nclass OptimizerParams:\n    \"\"\"A class to track optimizer parameters.\n    optimizer_name (str): name of the optimizer.\n    lr (float): learning rate.\n    weight_decay (float): weight decay.\n    \"\"\"\n\n    optimizer_name: str = \"AdamW\"\n    optimizer_params: Dict[str, Any] = field(\n        default_factory=lambda: {\n            \"lr\": 5e-5,\n            \"betas\": (0.9, 0.999),\n            \"amsgrad\": False,\n            \"weight_decay\": 1e-3,\n            \"eps\": 1e-08,\n        }\n    )\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return asdict(self)\n\n\n@dataclass\nclass SchedulerParams:\n    \"\"\"A class to track Scheduler Params.\"\"\"\n\n    scheduler_name: str = \"OneCycleLR\"\n    if scheduler_name == \"OneCycleLR\":\n        scheduler_params: Dict[str, Any] = field(\n            default_factory=lambda: {\n                \"max_lr\": 3e-5,\n                \"steps_per_epoch\": DataLoaderParams().get_len_train_loader(),\n                \"epochs\": GlobalTrainParams().epochs,\n                \"pct_start\": 0.3,\n                \"anneal_strategy\": \"cos\",\n                \"div_factor\": 25, # default is 25\n                \"three_phase\": False,\n                \"last_epoch\": -1,\n            }\n        )\n\n    elif scheduler_name == \"CosineAnnealingWarmRestarts\":\n        scheduler_params: Dict[str, Any] = field(\n            default_factory=lambda: {\n                \"T_0\": 10,\n                \"T_mult\": 1,\n                \"eta_min\": 1e-4,\n                \"last_epoch\": -1,\n            }\n        )\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return asdict(self)\n\n\n@dataclass\nclass WandbParams:\n    \"\"\"A class to track wandb parameters.\"\"\"\n\n    project: str = \"Petfinder\"\n    entity: str = \"reighns\"\n    save_code: bool = True\n    job_type: str = \"Train\"\n    # add an unique group id behind group name.\n    group: str = f\"{GlobalTrainParams().model_name}_{MakeFolds().num_folds}_folds_{wandb.util.generate_id()}\"\n    # dir: str = \"./wandb\"\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return asdict(self)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:14.632716Z","iopub.execute_input":"2021-12-17T11:29:14.633329Z","iopub.status.idle":"2021-12-17T11:29:14.676207Z","shell.execute_reply.started":"2021-12-17T11:29:14.633286Z","shell.execute_reply":"2021-12-17T11:29:14.675425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We call these global variables so we can easily use them across scripts/notebooks.","metadata":{}},{"cell_type":"code","source":"FILES = FilePaths()\nFOLDS = MakeFolds()\nLOADER_PARAMS = DataLoaderParams()\nTRAIN_PARAMS = GlobalTrainParams()\nTRANSFORMS = AugmentationParams()\nCRITERION_PARAMS = CriterionParams()\nOPTIMIZER_PARAMS = OptimizerParams()\nSCHEDULER_PARAMS = SchedulerParams()\nMODEL_PARAMS = ModelParams()\nWANDB_PARAMS = WandbParams()","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:29:14.677811Z","iopub.execute_input":"2021-12-17T11:29:14.678105Z","iopub.status.idle":"2021-12-17T11:29:14.723045Z","shell.execute_reply.started":"2021-12-17T11:29:14.678067Z","shell.execute_reply":"2021-12-17T11:29:14.722424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"markdown","source":"### Seeding and Reproducibility\n\nThis is very important, and I tend to have an OCD over it. Everytime I made changes to my code base, I will run the whole script on `DEBUG` mode just to check if the training loss changes. If my code change should not affect the training results, yet the loss changes, then it either means that my code is wrong, or a hidden pseudo number generator is called. See here[^seeding_issue].\n\n[^seeding_issue]: https://discuss.pytorch.org/t/why-cannot-i-call-dataloader-or-model-object-twice/137761/2","metadata":{}},{"cell_type":"code","source":"def seed_all(seed: int = 1992) -> None:\n    \"\"\"Seed all random number generators.\"\"\"\n    print(f\"Using Seed Number {seed}\")\n\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)  # set PYTHONHASHSEED env var at fixed value\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)  # pytorch (both CPU and CUDA)\n    np.random.seed(seed)  # for numpy pseudo-random generator\n    # set fixed value for python built-in pseudo-random generator\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    torch.backends.cudnn.enabled = True\n\n\ndef seed_worker(_worker_id) -> None:\n    \"\"\"Seed a worker with the given ID.\"\"\"\n    worker_seed = torch.initial_seed() % 2 ** 32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n    \nseed_all()","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:29:14.727379Z","iopub.execute_input":"2021-12-17T11:29:14.727567Z","iopub.status.idle":"2021-12-17T11:29:14.740317Z","shell.execute_reply.started":"2021-12-17T11:29:14.727544Z","shell.execute_reply":"2021-12-17T11:29:14.739459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Delete Memory\n\nWe create a function to delete objects to free up GPU.","metadata":{}},{"cell_type":"code","source":"import torch\nimport gc\n\n\ndef free_gpu_memory(\n    *args,\n) -> None:\n    \"\"\"Delete all variables from the GPU. Clear cache.\n    Args:\n        model ([type], optional): [description]. Defaults to None.\n        optimizer (torch.optim, optional): [description]. Defaults to None.\n        scheduler (torch.optim.lr_scheduler, optional): [description]. Defaults to None.\n    \"\"\"\n\n#     if args is not None:\n#         # Delete all other variables\n#         for arg in args:\n#             for var_name, unique_var_id in globals().items():\n#                 if id(arg) == id(unique_var_id):\n#                     del globals()[var_name]\n\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:14.743399Z","iopub.execute_input":"2021-12-17T11:29:14.743586Z","iopub.status.idle":"2021-12-17T11:29:14.74989Z","shell.execute_reply.started":"2021-12-17T11:29:14.743563Z","shell.execute_reply":"2021-12-17T11:29:14.748934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Show GPU Usage\n\nThis is especially useful when trying to debug a memory leak - this become useful when CUDA out of memory in scenarios where it should not.","metadata":{}},{"cell_type":"code","source":"def show_gpu_usage():\n    \"\"\"For debugging GPU memory leaks.\n    We divide by 1e+9 to convert bytes to gigabytes.\n    See here https://discuss.pytorch.org/t/memory-leak-debugging-and-common-causes/67339 for tips on how to debug.\"\"\"\n\n    config.logger.info(\n        f\"Current CUDA memory allocated: {torch.cuda.memory_allocated() / 1e9}\"\n    )\n\n    config.logger.info(\n        f\"Max CUDA memory allocated: {torch.cuda.max_memory_allocated() / 1e9}\"\n    )\n\n    config.logger.info(\n        f\"Percentage of CUDA memory allocated: {torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() }\"\n    )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:14.752922Z","iopub.execute_input":"2021-12-17T11:29:14.753617Z","iopub.status.idle":"2021-12-17T11:29:14.759632Z","shell.execute_reply.started":"2021-12-17T11:29:14.753577Z","shell.execute_reply":"2021-12-17T11:29:14.758831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A peek at the data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/petfinder-pawpularity-score/train.csv\")\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:29:14.760783Z","iopub.execute_input":"2021-12-17T11:29:14.76118Z","iopub.status.idle":"2021-12-17T11:29:14.802477Z","shell.execute_reply.started":"2021-12-17T11:29:14.761142Z","shell.execute_reply":"2021-12-17T11:29:14.801673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA\n\nWe will pass this section in another notebook, as the purpose of this notebook is to show the modularity of each code block.\n\nTODO: Plot image pixels histograms etc. Note that this is non-trivial, simply knowing the color distributions of images can give you a good idea on what augmentations to use. To give a funny example, imagine you are classifying whether an object is apple or pear. Assuming all apples are either green or red, and **most** pears are green, with some exceptions (red). Then you realized that those red pears are often misclassified to be an apple, in this scenario, it may be a good idea to grayscale the images to reduce noise caused by colors.","metadata":{}},{"cell_type":"markdown","source":"## Prepare Data\n\nWe create a few functions to prepare our data. For the most part, the docstring explains what each function does, if there is anything that requires clarification, I will emphasize here.\n\n\nSomething custom:\n- Sturges Rule.\n","metadata":{}},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nfrom typing import Union\n\nimport pandas as pd\nimport torch\n\ndef return_filepath(image_id: str, folder: Path = FILES.train_images) -> str:\n    \"\"\"Add a new column image_path to the train and test csv.\n    We can call the images easily in __getitem__ in Dataset.\n    We need to be careful if the image_id has extension already.\n    In this case, there is no need to add the extension.\n    Args:\n        image_id (str): The unique image id: 1000015157.jpg\n        folder (Path, optional): The train folder. Defaults to FILES().train_images.\n    Returns:\n        image_path (str): The path to the image: \"c:\\\\users\\\\reighns\\\\kaggle_projects\\\\cassava\\\\data\\\\train\\\\1000015157.jpg\"\n    \"\"\"\n    image_path = os.path.join(folder, f\"{image_id}.jpg\")\n    return image_path\n\n\ndef prepare_data(\n    image_col_name: str = FOLDS.image_col_name,\n) -> pd.DataFrame:\n    \"\"\"Call a sequential number of steps to prepare the data.\n    Args:\n        image_col_name (str): The column name of the unique image id.\n                        In Cassava, it is \"image_id\".\n    Returns:\n        df_train (pd.DataFrame): The train dataframe.\n        df_test (pd.DataFrame): The test dataframe.\n        df_folds (pd.DataFrame): The folds dataframe with an additional column \"fold\".\n        df_sub (pd.DataFrame): The submission dataframe.\n    \"\"\"\n\n    df_train = pd.read_csv(FILES.train_csv)\n    df_test = pd.read_csv(FILES.test_csv)\n    df_sub = pd.read_csv(FILES.sub_csv)\n\n    df_train[\"image_path\"] = df_train[image_col_name].apply(\n        lambda x: return_filepath(image_id=x, folder=FILES.train_images)\n    )\n    df_test[\"image_path\"] = df_test[image_col_name].apply(\n        lambda x: return_filepath(x, folder=FILES.test_images)\n    )\n    \n    ##################### CUSTOM Sturges' rule ####################################\n    num_bins = int(np.floor(1+np.log2(len(df_train))))\n    # Cut the target Pawpularity into `num_bins` using pd.cut\n    df_train['bins'] = pd.cut(df_train['Pawpularity'], bins=num_bins, labels=False, ordered=True)\n    df_train['bins'].hist()\n    ################################################################################\n\n    df_folds = make_folds(train_csv=df_train, cv_params=FOLDS)\n\n    return df_train, df_test, df_folds, df_sub\n\n\ndef prepare_loaders(\n    df_folds: pd.DataFrame,\n    fold: int,\n) -> Union[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n    \"\"\"Prepare Data Loaders.\"\"\"\n\n    if TRAIN_PARAMS.debug:\n        df_train = df_folds[df_folds[\"fold\"] != fold].sample(\n            LOADER_PARAMS.train_loader[\"batch_size\"]\n            * TRAIN_PARAMS.debug_multipler, \n            random_state = FOLDS.seed\n        )\n        df_valid = df_folds[df_folds[\"fold\"] == fold].sample(\n            LOADER_PARAMS.train_loader[\"batch_size\"]\n            * TRAIN_PARAMS.debug_multipler, \n            random_state = FOLDS.seed\n        )\n        # TODO: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html\n        # TODO: Consider adding stratified sampling to avoid df_valid having 0 num samples of minority class, causing issues when computing roc.\n        df_oof = df_valid.copy()\n    else:\n        df_train = df_folds[df_folds[\"fold\"] != fold].reset_index(drop=True)\n        df_valid = df_folds[df_folds[\"fold\"] == fold].reset_index(drop=True)\n        # Initiate OOF dataframe for this fold (same as df_valid).\n        df_oof = df_valid.copy()\n\n    dataset_train = CustomDataset(\n        df_train, transforms=get_train_transforms(), mode=\"train\"\n    )\n    dataset_valid = CustomDataset(\n        df_valid, transforms=get_valid_transforms(), mode=\"train\"\n    )\n\n    # Seeding workers for reproducibility.\n    g = torch.Generator()\n    g.manual_seed(0)\n\n    train_loader = torch.utils.data.DataLoader(\n        dataset_train,\n        **LOADER_PARAMS.train_loader,\n        worker_init_fn=seed_worker,\n        generator=g,\n    )\n    valid_loader = torch.utils.data.DataLoader(\n        dataset_valid,\n        **LOADER_PARAMS.valid_loader,\n        worker_init_fn=seed_worker,\n        generator=g,\n    )\n\n    # TODO: consider decoupling the oof and loaders, and consider add test loader here for consistency.\n    return train_loader, valid_loader, df_oof","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:14.803984Z","iopub.execute_input":"2021-12-17T11:29:14.804237Z","iopub.status.idle":"2021-12-17T11:29:14.821859Z","shell.execute_reply.started":"2021-12-17T11:29:14.804204Z","shell.execute_reply":"2021-12-17T11:29:14.821082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross-Validation Strategy\n\nWhen choosing a CV strategy, it is often good to ask ourselves the following questions:\n\n- Is the dataset $\\mathcal{X}$ imbalanced?\n- Is the dataset $\\mathcal{X}$ generated in a **i.i.d** manner, more specifically, if I split $\\mathcal{X}$ to $\\mathcal{X}{train}$ and $\\mathcal{X}{val}$, can we ensure that $\\mathcal{X}{val}$ has no dependency on $\\mathcal{X}{train}$?\n\n---\n\nHowever, this is an interesting problem, we can use either regression or classification to do it. In this specific tutorial, we will use classification, although we do not actually care about the target's distribution, we will use `StratifiedKFold` to be on the safe side.\n\n---\n\nSomething custom here:\n- When calling `StratifiedKFold`, we are not stratifying vs the targets, but the bins we created using Sturge's Rule.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# from IPython.display import display\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\n\n\ndef make_folds(\n    train_csv: pd.DataFrame, cv_params: MakeFolds()\n) -> pd.DataFrame:\n    \"\"\"Split the given dataframe into training folds.\"\"\"\n\n    if cv_params.cv_schema == \"StratifiedKFold\":\n        df_folds = train_csv.copy()\n        skf = StratifiedKFold(\n            n_splits=cv_params.num_folds,\n            shuffle=True,\n            random_state=cv_params.seed,\n        )\n        ######################## Custom Stratification ############\n        for fold, (_train_idx, val_idx) in enumerate(\n            skf.split(\n                X=df_folds[cv_params.image_col_name],\n                y=df_folds['bins'],\n            )\n        ):\n        ##########################################################\n            df_folds.loc[val_idx, \"fold\"] = int(fold + 1)\n        df_folds[\"fold\"] = df_folds[\"fold\"].astype(int)\n        print(df_folds.groupby([\"fold\", cv_params.class_col_name]).size())\n\n    elif cv_params.cv_schema == \"GroupKfold\":\n        df_folds = train_csv.copy()\n        gkf = GroupKFold(n_splits=cv_params.num_folds)\n        groups = df_folds[cv_params.group_kfold_split].values\n        for fold, (_train_index, val_index) in enumerate(\n            gkf.split(\n                X=df_folds, y=df_folds[cv_params.class_col_name], groups=groups\n            )\n        ):\n            df_folds.loc[val_index, \"fold\"] = int(fold + 1)\n        df_folds[\"fold\"] = df_folds[\"fold\"].astype(int)\n\n        print(df_folds.groupby([\"fold\", cv_params.class_col_name]).size())\n\n    else:\n        config.logger.error(\n            f\"Unknown CV schema: {cv_params.cv_schema}, are you using custom split?\"\n        )\n        df_folds = train_csv.copy()\n        print(df_folds.groupby([\"fold\", cv_params.class_col_name]).size())\n\n    df_folds.to_csv(cv_params.folds_csv, index=False)\n\n    return df_folds","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:14.823452Z","iopub.execute_input":"2021-12-17T11:29:14.823925Z","iopub.status.idle":"2021-12-17T11:29:15.426098Z","shell.execute_reply.started":"2021-12-17T11:29:14.823886Z","shell.execute_reply":"2021-12-17T11:29:15.425332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train, df_test, df_folds, df_sub = prepare_data()","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:29:15.428334Z","iopub.execute_input":"2021-12-17T11:29:15.428821Z","iopub.status.idle":"2021-12-17T11:29:15.865259Z","shell.execute_reply.started":"2021-12-17T11:29:15.428778Z","shell.execute_reply":"2021-12-17T11:29:15.864552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normalize Target\n\nWe set a flag to check if we want to normalize the targets or not.","metadata":{}},{"cell_type":"code","source":"if is_normalize_target:\n    df_folds[\"Pawpularity\"] = df_folds[\"Pawpularity\"] / 100","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:29:15.866624Z","iopub.execute_input":"2021-12-17T11:29:15.867018Z","iopub.status.idle":"2021-12-17T11:29:15.873902Z","shell.execute_reply.started":"2021-12-17T11:29:15.866981Z","shell.execute_reply":"2021-12-17T11:29:15.873276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset\n\nWe create a custom `Dataset` here.","metadata":{}},{"cell_type":"code","source":"from typing import Dict, Union\nimport albumentations\n\nimport cv2\nimport pandas as pd\nimport torch\n\n\nclass CustomDataset(torch.utils.data.Dataset):\n    \"\"\"Dataset class for the {insert competition/project name} dataset.\"\"\"\n\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        transforms: albumentations.core.composition.Compose = None,\n        mode: str = \"train\",\n    ):\n        \"\"\"Constructor for the dataset class.\n\n        Args:\n            df (pd.DataFrame): Dataframe for either train, valid or test.\n            transforms (albumentations.core.composition.Compose): albumentations transforms to apply to the images.\n            mode (str, optional): Defaults to \"train\". One of ['train', 'valid', 'test', 'gradcam']\n        \"\"\"\n\n        # \"image_path\" is hardcoded, as that is always defined in prepare_data.\n        self.image_path = df[\"image_path\"].values\n        self.image_ids = df[FOLDS.image_col_name].values\n        self.df = df\n        self.targets = (\n            torch.from_numpy(df[FOLDS.class_col_name].values)\n            if mode != \"test\"\n            else None\n        )\n\n        self.transforms = transforms\n        self.mode = mode\n\n        if self.mode not in [\"train\", \"valid\", \"test\", \"gradcam\"]:\n            raise ValueError(\n                f\"Mode {self.mode} not in accepted list of modes {['train', 'valid', 'test', 'gradcam']}\"\n            )\n\n    def __len__(self) -> int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.df)\n\n    @staticmethod\n    def return_dtype(\n        X: torch.Tensor, y: torch.Tensor, original_image: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"Return the dtype of the dataset.\n        Args:\n            X (torch.Tensor): Image tensor.\n            y (torch.Tensor): Target tensor.\n            original_image  (torch.Tensor): Original image tensor.\n        Returns:\n            X (torch.Tensor): Image tensor.\n            y (torch.Tensor): Target tensor.\n            original_image  (torch.Tensor): Original image tensor.\n        \"\"\"\n\n        # TODO: To check if defining float32 here will affect how Mixed Precision works. If so, then change from float32 to float.\n        if CRITERION_PARAMS.train_criterion_name == \"BCEWithLogitsLoss\":\n            # Make changes to reshape rather than in Trainer.\n            y = torch.as_tensor(y, dtype=torch.float32).view(-1, 1)\n        else:\n            y = torch.as_tensor(y, dtype=torch.long)\n\n        X = torch.as_tensor(X, dtype=torch.float32)\n        original_image = torch.as_tensor(original_image, dtype=torch.float32)\n\n        return X, y, original_image\n    \n\n    def check_shape(self):\n        \"\"\"Check the shape of the dataset.\n\n        Add a tensor transpose if transformation is None since most images is HWC but ToTensorV2 transforms them to CHW.\"\"\"\n\n        raise NotImplementedError\n\n    def __getitem__(\n        self, index: int\n    ) -> Union[\n        Dict[str, torch.FloatTensor],\n        Dict[str, Union[torch.FloatTensor, torch.LongTensor]],\n    ]:\n        \"\"\"Implements the getitem method: https://www.geeksforgeeks.org/__getitem__-and-__setitem__-in-python/\n\n        Be careful of Targets:\n            BCEWithLogitsLoss expects a target.float()\n            CrossEntropyLoss expects a target.long()\n\n        Args:\n            index (int): index of the dataset.\n\n        Returns:\n            Dict[str, torch.FloatTensor]:{\"X\": image_tensor}\n            Dict[str, Union[torch.FloatTensor, torch.LongTensor]]: {\"y\": target_tensor} If BCEwithLogitsLoss then FloatTensor, else LongTensor\n        \"\"\"\n        image_path = self.image_path[index]\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # needed for gradcam.\n        original_image = cv2.resize(\n            image, (TRANSFORMS.image_size, TRANSFORMS.image_size)\n        ).copy()\n\n        # Get target for all modes except for test, if test, replace target with dummy ones to pass through return_dtype.\n        target = self.targets[index] if self.mode != \"test\" else torch.ones(1)\n\n        if self.transforms:\n            image = self.transforms(image=image)[\"image\"]\n\n        # TODO: Consider not returning original image if we don't need it, may cause more memory usage and speed issues?\n        X, y, original_image = self.return_dtype(image, target, original_image)\n        \n        if self.mode in [\"train\", \"valid\"]:\n            \n            return {\"X\": X, \"y\": y}\n\n        if self.mode == \"test\":\n            return {\"X\": X}\n\n        if self.mode == \"gradcam\":\n\n            return {\n                \"X\": X,\n                \"y\": y,\n                \"original_image\": original_image,\n                \"image_id\": self.image_ids[index],\n            }\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:15.875353Z","iopub.execute_input":"2021-12-17T11:29:15.875651Z","iopub.status.idle":"2021-12-17T11:29:17.483836Z","shell.execute_reply.started":"2021-12-17T11:29:15.87561Z","shell.execute_reply":"2021-12-17T11:29:17.483128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Augmentations\n\nNotice that in `get_inference_transforms` I defined many TTAs, in which all of them were copied verbatim from `get_train_transforms`, except for `VerticalFlip`. Since I put very low probability for `Vflip`, I reckon I will just not include it in TTA.\n\n---\n\nOne other note is that in order to make TTA results deterministic, I will use `p=1.` for all TTAs. However, if you have `RandomResizedCrops`, then there will be some randomness.","metadata":{}},{"cell_type":"code","source":"from albumentations.pytorch.transforms import ToTensorV2\nimport albumentations\n\n\ndef get_train_transforms(image_size: int = TRANSFORMS.image_size):\n    \"\"\"Performs Augmentation on training data.\n\n    Args:\n        image_size (int, optional): [description]. Defaults to AUG.image_size.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    return albumentations.Compose(\n        [  albumentations.RandomResizedCrop(height=image_size, width=image_size),\n            albumentations.HorizontalFlip(p=0.5),\n            albumentations.VerticalFlip(p=0.1),\n            albumentations.Rotate(limit=180, p=0.5),\n            albumentations.ShiftScaleRotate(\n                shift_limit=0.1, scale_limit=0.1, rotate_limit=45, p=0.5\n            ),\n            albumentations.HueSaturationValue(\n                hue_shift_limit=0.2,\n                sat_shift_limit=0.2,\n                val_shift_limit=0.2,\n                p=0.5,\n            ),\n            albumentations.RandomBrightnessContrast(\n                brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5\n            ),\n            # albumentations.Resize(image_size, image_size),\n            albumentations.Normalize(\n                mean=TRANSFORMS.mean,\n                std=TRANSFORMS.std,\n                max_pixel_value=255.0,\n                p=1.0,\n            ),\n            ToTensorV2(p=1.0),\n        ]\n    )\n\n\ndef get_valid_transforms(image_size: int = TRANSFORMS.image_size):\n    \"\"\"Performs Augmentation on validation data.\n\n    Args:\n        image_size (int, optional): [description]. Defaults to AUG.image_size.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    return albumentations.Compose(\n        [\n            albumentations.Resize(image_size, image_size),\n            albumentations.Normalize(\n                mean=TRANSFORMS.mean,\n                std=TRANSFORMS.std,\n                max_pixel_value=255.0,\n                p=1.0,\n            ),\n            ToTensorV2(p=1.0),\n        ]\n    )\n\n\ndef get_gradcam_transforms(image_size: int = TRANSFORMS.image_size):\n    \"\"\"Performs Augmentation on gradcam data.\n\n    Args:\n        image_size (int, optional): [description]. Defaults to AUG.image_size.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    return albumentations.Compose(\n        [\n            albumentations.Resize(image_size, image_size),\n            albumentations.Normalize(\n                mean=TRANSFORMS.mean,\n                std=TRANSFORMS.std,\n                max_pixel_value=255.0,\n                p=1.0,\n            ),\n            ToTensorV2(p=1.0),\n        ]\n    )\n\n\ndef get_inference_transforms(image_size: int = TRANSFORMS.image_size) -> Dict[str, albumentations.Compose]:\n    \"\"\"Performs Augmentation on test dataset.\n    Returns the transforms for inference in a dictionary which can hold TTA transforms.\n\n    Args:\n        image_size (int, optional): [description]. Defaults to AUG.image_size.\n\n    Returns:\n        Dict[str, albumentations.Compose]: [description]\n    \"\"\"\n\n    transforms_dict = {\n        \"transforms_test\": albumentations.Compose(\n            [\n                albumentations.Resize(image_size, image_size),\n                albumentations.Normalize(\n                    mean=TRANSFORMS.mean,\n                    std=TRANSFORMS.std,\n                    max_pixel_value=255.0,\n                    p=1.0,\n                ),\n                ToTensorV2(p=1.0),\n            ]\n        ),\n        \"tta_flip\": albumentations.Compose(\n            [\n                albumentations.HorizontalFlip(p=1),\n                albumentations.Resize(image_size, image_size),\n                albumentations.Normalize(\n                    mean=TRANSFORMS.mean,\n                    std=TRANSFORMS.std,\n                    max_pixel_value=255.0,\n                    p=1.0,\n                ),\n                ToTensorV2(p=1.0),\n            ]\n        ),\n        \"tta_rotate\": albumentations.Compose(\n            [\n                albumentations.Rotate(limit=180, p=1),\n                albumentations.Resize(image_size, image_size),\n                albumentations.Normalize(\n                    mean=TRANSFORMS.mean,\n                    std=TRANSFORMS.std,\n                    max_pixel_value=255.0,\n                    p=1.0,\n                ),\n                ToTensorV2(p=1.0),\n            ]\n        ),\n        \"tta_shift_scale_rotate\": albumentations.Compose(\n            [\n                albumentations.ShiftScaleRotate(\n                    shift_limit=0.1, scale_limit=0.1, rotate_limit=45, p=1\n                ),\n                albumentations.Resize(image_size, image_size),\n                albumentations.Normalize(\n                    mean=TRANSFORMS.mean,\n                    std=TRANSFORMS.std,\n                    max_pixel_value=255.0,\n                    p=1.0,\n                ),\n                ToTensorV2(p=1.0),\n            ]\n        ),\n        \"tta_hue_saturation_value\": albumentations.Compose(\n            [\n                albumentations.HueSaturationValue(\n                    hue_shift_limit=0.2,\n                    sat_shift_limit=0.2,\n                    val_shift_limit=0.2,\n                    p=1,\n                ),\n                albumentations.Resize(image_size, image_size),\n                albumentations.Normalize(\n                    mean=TRANSFORMS.mean,\n                    std=TRANSFORMS.std,\n                    max_pixel_value=255.0,\n                    p=1.0,\n                ),\n                ToTensorV2(p=1.0),\n            ]\n        ),\n        \"tta_random_brightness_contrast\": albumentations.Compose(\n            [\n                albumentations.RandomBrightnessContrast(\n                    brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=1\n                ),\n                albumentations.Resize(image_size, image_size),\n                albumentations.Normalize(\n                    mean=TRANSFORMS.mean,\n                    std=TRANSFORMS.std,\n                    max_pixel_value=255.0,\n                    p=1.0,\n                ),\n                ToTensorV2(p=1.0),\n            ]\n        ),\n    }\n\n    return transforms_dict","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:17.4851Z","iopub.execute_input":"2021-12-17T11:29:17.48553Z","iopub.status.idle":"2021-12-17T11:29:17.525597Z","shell.execute_reply.started":"2021-12-17T11:29:17.48549Z","shell.execute_reply":"2021-12-17T11:29:17.523872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Mixup is all we need?","metadata":{}},{"cell_type":"code","source":"def mixup_data(\n    x: torch.Tensor,\n    y: torch.Tensor,\n    params: TRANSFORMS.mixup_params,\n) -> torch.Tensor:\n    \"\"\"Implements mixup data augmentation.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n        y (torch.Tensor): The target tensor.\n        params (TRANSFORMS, optional): [description]. Defaults to TRANSFORMS.mixup_params.\n\n    Returns:\n        torch.Tensor: [description]\n    \"\"\"\n    assert params[\"mixup_alpha\"] > 0, \"Mixup alpha must be greater than 0.\"\n    assert (\n        x.size(0) > 1\n    ), \"Mixup requires more than one sample as at least two samples are needed to mix.\"\n\n    if params[\"mixup_alpha\"] > 0:\n        lambda_ = np.random.beta(params[\"mixup_alpha\"], params[\"mixup_alpha\"])\n    else:\n        lambda_ = 1\n\n    batch_size = x.size()[0]\n    if params[\"use_cuda\"] and torch.cuda.is_available():\n        index = torch.randperm(batch_size).cuda()\n    else:\n        index = torch.randperm(batch_size)\n\n    mixed_x = lambda_ * x + (1 - lambda_) * x[index, :]\n    y_a, y_b = y, y[index]\n\n    return mixed_x, y_a, y_b, lambda_\n\n\ndef mixup_criterion(\n    criterion: Union[torch.nn.BCEWithLogitsLoss, torch.nn.CrossEntropyLoss],\n    logits: torch.Tensor,\n    y_a: torch.Tensor,\n    y_b: torch.Tensor,\n    lambda_: float,\n) -> torch.Tensor:\n    \"\"\"Implements mixup criterion.\n\n    Args:\n        criterion (Union[torch.nn.BCEWithLogitsLoss, torch.nn.CrossEntropyLoss]): [description]\n        logits (torch.Tensor): [description]\n        y_a (torch.Tensor): [description]\n        y_b (torch.Tensor): [description]\n        lambda_ (float): [description]\n\n    Returns:\n        torch.Tensor: [description]\n    \"\"\"\n    return lambda_ * criterion(logits, y_a) + (1 - lambda_) * criterion(\n        logits, y_b\n    )","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:29:17.530494Z","iopub.execute_input":"2021-12-17T11:29:17.530778Z","iopub.status.idle":"2021-12-17T11:29:17.549625Z","shell.execute_reply.started":"2021-12-17T11:29:17.530742Z","shell.execute_reply":"2021-12-17T11:29:17.548614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sanity Plots\n\nIt is often a good idea to check how your dataloader is performing with the augmentation images. After all, these images will be fed to the model during training phase.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport torch\nfrom typing import List\nimport torchvision\n\ndef unnormalize(\n    normalized_img, mean, std, max_pixel_value=255.0\n) -> torch.Tensor:\n    \"\"\"TODO: Use https://discuss.pytorch.org/t/simple-way-to-inverse-transform-normalization/4821/7 code and make it a class to include both Normalize and Unnormalize Method.\n    Args:\n        normalized_img ([type]): [description]\n        mean ([type]): [description]\n        std ([type]): [description]\n        max_pixel_value (float, optional): [description]. Defaults to 255.0.\n    Returns:\n        torch.Tensor: [description]\n    \"\"\"\n    # normalized_img = (unnormalized_img - mean * max_pixel_value) / (std * max_pixel_value)\n    # unnormalized_img = normalized_img * (std * max_pixel_values) + mean * max_pixel_values\n\n    unnormalized = torch.zeros(normalized_img.size(), dtype=torch.float64)\n    unnormalized[0, :, :] = (\n        normalized_img[0, :, :] * (std[0] * max_pixel_value)\n        + mean[0] * max_pixel_value\n    )\n    unnormalized[1, :, :] = (\n        normalized_img[1, :, :] * (std[1] * max_pixel_value)\n        + mean[1] * max_pixel_value\n    )\n    unnormalized[2, :, :] = (\n        normalized_img[2, :, :] * (std[2] * max_pixel_value)\n        + mean[2] * max_pixel_value\n    )\n    print(unnormalized)\n    return unnormalized\n\n\ndef show_image(\n    loader: torch.utils.data.DataLoader,\n    nrows: int = 3,\n    ncols: int = 4,\n    mean: List[float] = [0.485, 0.456, 0.406],\n    std: List[float] = [0.229, 0.224, 0.225],\n    one_channel: bool = False,\n):\n    \"\"\"Plot a grid of image from Dataloader.\n    Args:\n        train_dataset (torch.utils.data.Dataset): [description]\n        nrows (int, optional): [description]. Defaults to 3.\n        ncols (int, optional): [description]. Defaults to 4.\n        mean (List[float], optional): [description]. Defaults to None.\n        std (List[float], optional): [description]. Defaults to None.\n    \"\"\"\n\n    dataiter = iter(loader)\n    \n    one_batch_images, one_batch_targets = (\n        dataiter.next()[\"X\"],\n        dataiter.next()[\"y\"],\n    )\n    # TODO: FIX UNNORMALIZE not showing properly.\n    # one_batch_images = [unnormalize(image, mean, std, max_pixel_value=255.0) for image in one_batch_images]\n\n    # create grid of images\n    image_grid = torchvision.utils.make_grid(one_batch_images)\n    plt.figure(figsize = (30,15))\n    if one_channel:\n        pass\n\n    image_grid = image_grid.numpy()\n    if one_channel:\n        plt.imshow(image_grid, cmap=\"Greys\")\n    else:\n        \n        plt.imshow(np.transpose(image_grid, (1, 2, 0)))\n    plt.show()\n\n    return image_grid","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:17.554506Z","iopub.execute_input":"2021-12-17T11:29:17.554889Z","iopub.status.idle":"2021-12-17T11:29:17.578194Z","shell.execute_reply.started":"2021-12-17T11:29:17.554852Z","shell.execute_reply":"2021-12-17T11:29:17.577303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not is_inference:\n    train_dataset = CustomDataset(\n        df=df_folds, transforms=get_train_transforms(), mode=\"train\"\n    )\n    valid_dataset = CustomDataset(\n        df=df_folds, transforms=get_valid_transforms(), mode=\"train\"\n    )\n\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size = 4,\n        shuffle=True,\n        worker_init_fn=seed_worker,\n    )\n\n    valid_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size = 4,\n        shuffle=True,\n        worker_init_fn=seed_worker,\n    )\n\n\n    _ = show_image(\n        loader=valid_loader,\n        nrows=1,\n        ncols=1,\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n    )\n    # YASSS Free the GPU Memory!\n    free_gpu_memory(train_loader, valid_loader)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:17.583455Z","iopub.execute_input":"2021-12-17T11:29:17.585679Z","iopub.status.idle":"2021-12-17T11:29:19.031237Z","shell.execute_reply.started":"2021-12-17T11:29:17.585638Z","shell.execute_reply":"2021-12-17T11:29:19.030254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"OK, I am unsure why the images are looking so weird, did I mess up the Hue too much? #FIXME","metadata":{}},{"cell_type":"markdown","source":"## Models","metadata":{}},{"cell_type":"markdown","source":"### Simple Custom Model\n\nHere we create a simple model. To change to a custom layer with custom head, one need to look at how to change `self.single_head_fc`.","metadata":{}},{"cell_type":"code","source":"class CustomNeuralNet(torch.nn.Module):\n    def __init__(\n        self,\n        model_name: str = MODEL_PARAMS.model_name,\n        out_features: int = MODEL_PARAMS.output_dimension,\n        in_channels: int = MODEL_PARAMS.input_channels,\n        pretrained: bool = MODEL_PARAMS.pretrained,\n    ):\n        \"\"\"Construct a new model.\n        Args:\n            model_name ([type], str): The name of the model to use. Defaults to MODEL_PARAMS.model_name.\n            out_features ([type], int): The number of output features, this is usually the number of classes, but if you use sigmoid, then the output is 1. Defaults to MODEL_PARAMS.output_dimension.\n            in_channels ([type], int): The number of input channels; RGB = 3, Grayscale = 1. Defaults to MODEL_PARAMS.input_channels.\n            pretrained ([type], bool): If True, use pretrained model. Defaults to MODEL_PARAMS.pretrained.\n        \"\"\"\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.pretrained = pretrained\n\n        self.backbone = timm.create_model(\n            model_name, pretrained=self.pretrained, in_chans=self.in_channels\n        )\n        config.logger.info(\n            f\"\\nModel: {model_name} \\nPretrained: {pretrained} \\nIn Channels: {in_channels}\"\n        )\n\n        # removes head from backbone\n        self.backbone.reset_classifier(num_classes=0, global_pool=\"avg\")\n\n        # get the last layer's number of features in backbone (feature map)\n        self.in_features = self.backbone.num_features\n        self.out_features = out_features\n\n        # Custom Head\n        self.single_head_fc = torch.nn.Sequential(\n            torch.nn.Linear(self.in_features, self.out_features),\n        )\n        \n        self.architecture: Dict[str, Callable] = {\n            \"backbone\": self.backbone,\n            \"bottleneck\": None,\n            \"head\": self.single_head_fc,\n        }\n\n    def extract_features(self, image: torch.FloatTensor) -> torch.FloatTensor:\n        \"\"\"Extract the features mapping logits from the model.\n        This is the output from the backbone of a CNN.\n        Args:\n            image (torch.FloatTensor): The input image.\n        Returns:\n            feature_logits (torch.FloatTensor): The features logits.\n        \"\"\"\n        feature_logits = self.architecture[\"backbone\"](image)\n        return feature_logits\n\n    def forward(self, image: torch.FloatTensor) -> torch.FloatTensor:\n        \"\"\"The forward call of the model.\n        Args:\n            image (torch.FloatTensor): The input image.\n        Returns:\n            classifier_logits (torch.FloatTensor): The output logits of the classifier head.\n        \"\"\"\n\n        feature_logits = self.extract_features(image)\n        classifier_logits = self.architecture[\"head\"](feature_logits)\n\n        return classifier_logits\n\n    def get_last_layer(self):\n        # TODO: Implement this properly.\n        \"\"\"Get the last layer information of TIMM Model.\n        Returns:\n            [type]: [description]\n        \"\"\"\n        last_layer_name = None\n        for name, _param in self.model.named_modules():\n            last_layer_name = name\n\n        last_layer_attributes = last_layer_name.split(\".\")  # + ['in_features']\n        linear_layer = functools.reduce(\n            getattr, last_layer_attributes, self.model\n        )\n        # reduce applies to a list recursively and reduce\n        in_features = functools.reduce(\n            getattr, last_layer_attributes, self.model\n        ).in_features\n        return last_layer_attributes, in_features, linear_layer","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:19.032434Z","iopub.execute_input":"2021-12-17T11:29:19.032679Z","iopub.status.idle":"2021-12-17T11:29:19.051843Z","shell.execute_reply.started":"2021-12-17T11:29:19.032649Z","shell.execute_reply":"2021-12-17T11:29:19.051049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DOLG","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6, requires_grad=False):\n        super(GeM, self).__init__()\n        self.p = nn.Parameter(torch.ones(1)*p, requires_grad=requires_grad)\n        self.eps = eps\n\n    def forward(self, x):\n        return self.gem(x, p=self.p, eps=self.eps)\n\n    def gem(self, x, p=3, eps=1e-6):\n        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:29:19.053339Z","iopub.execute_input":"2021-12-17T11:29:19.053989Z","iopub.status.idle":"2021-12-17T11:29:19.068208Z","shell.execute_reply.started":"2021-12-17T11:29:19.053951Z","shell.execute_reply":"2021-12-17T11:29:19.067565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiAtrous(nn.Module):\n    def __init__(self, in_channel, out_channel, size, dilation_rates=[3, 6, 9]):\n        super().__init__()\n        self.dilated_convs = [\n            nn.Conv2d(in_channel, int(out_channel/4),\n                      kernel_size=3, dilation=rate, padding=rate)\n            for rate in dilation_rates\n        ]\n        self.gap_branch = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channel, int(out_channel/4), kernel_size=1),\n            nn.ReLU(),\n            nn.Upsample(size=(size, size), mode='bilinear')\n        )\n        self.dilated_convs.append(self.gap_branch)\n        self.dilated_convs = nn.ModuleList(self.dilated_convs)\n\n    def forward(self, x):\n        local_feat = []\n        for dilated_conv in self.dilated_convs:\n            local_feat.append(dilated_conv(x))\n        local_feat = torch.cat(local_feat, dim=1)\n        return local_feat\n\n\nclass DolgLocalBranch(nn.Module):\n    def __init__(self, in_channel, out_channel, hidden_channel=2048):\n        super().__init__()\n        self.multi_atrous = MultiAtrous(in_channel, hidden_channel, size=int(TRANSFORMS.image_size/8))\n        self.conv1x1_1 = nn.Conv2d(hidden_channel, out_channel, kernel_size=1)\n        self.conv1x1_2 = nn.Conv2d(\n            out_channel, out_channel, kernel_size=1, bias=False)\n        self.conv1x1_3 = nn.Conv2d(out_channel, out_channel, kernel_size=1)\n\n        self.relu = nn.ReLU()\n        self.bn = nn.BatchNorm2d(out_channel)\n        self.softplus = nn.Softplus()\n\n    def forward(self, x):\n        local_feat = self.multi_atrous(x)\n\n        local_feat = self.conv1x1_1(local_feat)\n        local_feat = self.relu(local_feat)\n        local_feat = self.conv1x1_2(local_feat)\n        local_feat = self.bn(local_feat)\n\n        attention_map = self.relu(local_feat)\n        attention_map = self.conv1x1_3(attention_map)\n        attention_map = self.softplus(attention_map)\n\n        local_feat = F.normalize(local_feat, p=2, dim=1)\n        local_feat = local_feat * attention_map\n\n        return local_feat\n\n\nclass OrthogonalFusion(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, local_feat, global_feat):\n        global_feat_norm = torch.norm(global_feat, p=2, dim=1)\n        projection = torch.bmm(global_feat.unsqueeze(1), torch.flatten(\n            local_feat, start_dim=2))\n        projection = torch.bmm(global_feat.unsqueeze(\n            2), projection).view(local_feat.size())\n        projection = projection / \\\n            (global_feat_norm * global_feat_norm).view(-1, 1, 1, 1)\n        orthogonal_comp = local_feat - projection\n        global_feat = global_feat.unsqueeze(-1).unsqueeze(-1)\n        return torch.cat([global_feat.expand(orthogonal_comp.size()), orthogonal_comp], dim=1)\n\n\nclass DolgNet(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, num_of_classes):\n        super().__init__()\n        self.cnn = timm.create_model(\n            'resnet101',\n            pretrained=True,\n            features_only=True,\n            in_chans=input_dim,\n            out_indices=(2, 3)\n        )\n        self.orthogonal_fusion = OrthogonalFusion()\n        self.local_branch = DolgLocalBranch(512, hidden_dim) # 512 \n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.gem_pool = GeM()\n        self.fc_1 = nn.Linear(1024, hidden_dim)\n        self.fc_2 = nn.Linear(int(2*hidden_dim), output_dim)\n\n\n\n    def forward(self, x):\n        output = self.cnn(x)\n\n        local_feat = self.local_branch(output[0])  # ,hidden_channel,16,16\n        global_feat = self.fc_1(self.gem_pool(output[1]).squeeze())  # ,1024\n\n        feat = self.orthogonal_fusion(local_feat, global_feat)\n        feat = self.gap(feat).squeeze()\n        feat = self.fc_2(feat)\n\n        return feat","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:29:19.06996Z","iopub.execute_input":"2021-12-17T11:29:19.070632Z","iopub.status.idle":"2021-12-17T11:29:19.101575Z","shell.execute_reply.started":"2021-12-17T11:29:19.07059Z","shell.execute_reply":"2021-12-17T11:29:19.100809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dognet = DolgNet(input_dim=3, hidden_dim=128, output_dim=1, num_of_classes=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:29:19.104475Z","iopub.execute_input":"2021-12-17T11:29:19.105291Z","iopub.status.idle":"2021-12-17T11:29:20.03598Z","shell.execute_reply.started":"2021-12-17T11:29:19.105222Z","shell.execute_reply":"2021-12-17T11:29:20.035197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### With Custom Head","metadata":{}},{"cell_type":"code","source":"# class CustomNeuralNet(torch.nn.Module):\n#     def __init__(\n#         self,\n#         model_name: str = MODEL_PARAMS.model_name,\n#         out_features: int = MODEL_PARAMS.output_dimension,\n#         in_channels: int = MODEL_PARAMS.input_channels,\n#         pretrained: bool = MODEL_PARAMS.pretrained,\n#     ):\n#         \"\"\"Construct a new model.\n#         Args:\n#             model_name ([type], str): The name of the model to use. Defaults to MODEL_PARAMS.model_name.\n#             out_features ([type], int): The number of output features, this is usually the number of classes, but if you use sigmoid, then the output is 1. Defaults to MODEL_PARAMS.output_dimension.\n#             in_channels ([type], int): The number of input channels; RGB = 3, Grayscale = 1. Defaults to MODEL_PARAMS.input_channels.\n#             pretrained ([type], bool): If True, use pretrained model. Defaults to MODEL_PARAMS.pretrained.\n#         \"\"\"\n#         super().__init__()\n\n#         self.in_channels = in_channels\n#         self.pretrained = pretrained\n\n#         self.backbone = timm.create_model(\n#             model_name, pretrained=self.pretrained, in_chans=self.in_channels\n#         )\n#         config.logger.info(\n#             f\"\\nModel: {model_name} \\nPretrained: {pretrained} \\nIn Channels: {in_channels}\"\n#         )\n\n#         # removes head from backbone\n#         self.backbone.reset_classifier(num_classes=0, global_pool=\"avg\")\n\n#         # get the last layer's number of features in backbone (feature map)\n#         self.in_features = self.backbone.num_features\n#         self.out_features = out_features\n\n#         # Custom Head\n\n#         self.dense1 = torch.nn.Linear(self.in_features, 128)\n#         self.dropout = torch.nn.Dropout(0.1)\n#         self.dense2 = torch.nn.Linear(128, 64)\n#         self.single_head_fc = torch.nn.Linear(64, 1)\n\n        \n#         self.architecture: Dict[str, Callable] = {\n#             \"backbone\": self.backbone,\n#             \"bottleneck\": None,\n#             \"head\": self.single_head_fc,\n#         }\n\n#     def extract_features(self, image: torch.FloatTensor) -> torch.FloatTensor:\n#         \"\"\"Extract the features mapping logits from the model.\n#         This is the output from the backbone of a CNN.\n#         Args:\n#             image (torch.FloatTensor): The input image.\n#         Returns:\n#             feature_logits (torch.FloatTensor): The features logits.\n#         \"\"\"\n#         feature_logits = self.architecture[\"backbone\"](image)\n#         return feature_logits\n\n#     def forward(self, image: torch.FloatTensor) -> torch.FloatTensor:\n#         \"\"\"The forward call of the model.\n#         Args:\n#             image (torch.FloatTensor): The input image.\n#         Returns:\n#             classifier_logits (torch.FloatTensor): The output logits of the classifier head.\n#         \"\"\"\n\n#         feature_logits = self.extract_features(image)\n#         x = self.dropout(feature_logits)\n#         x = self.dense1(x)\n#         x = self.dense2(x)\n#         classifier_logits = self.single_head_fc(x)\n#         # classifier_logits = self.architecture[\"head\"](feature_logits)\n\n#         return classifier_logits\n\n#     def get_last_layer(self):\n#         # TODO: Implement this properly.\n#         \"\"\"Get the last layer information of TIMM Model.\n#         Returns:\n#             [type]: [description]\n#         \"\"\"\n#         last_layer_name = None\n#         for name, _param in self.model.named_modules():\n#             last_layer_name = name\n\n#         last_layer_attributes = last_layer_name.split(\".\")  # + ['in_features']\n#         linear_layer = functools.reduce(\n#             getattr, last_layer_attributes, self.model\n#         )\n#         # reduce applies to a list recursively and reduce\n#         in_features = functools.reduce(\n#             getattr, last_layer_attributes, self.model\n#         ).in_features\n#         return last_layer_attributes, in_features, linear_layer","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:29:20.040972Z","iopub.execute_input":"2021-12-17T11:29:20.043346Z","iopub.status.idle":"2021-12-17T11:29:20.056013Z","shell.execute_reply.started":"2021-12-17T11:29:20.043301Z","shell.execute_reply":"2021-12-17T11:29:20.055226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not is_inference:\n    !pip install -q torchsummary\n    import torchsummary\n\n    def torchsummary_wrapper(\n        model: CustomNeuralNet, image_size: Tuple[int, int, int]\n    ):\n        \"\"\"A torch wrapper to print out layers of a Model.\n        Args:\n            model (CustomNeuralNet): Model.\n            image_size (Tuple[int, int, int]): Image size as a tuple of (channels, height, width).\n        Returns:\n            model_summary (torchsummary.model_statistics.ModelStatistics): Model summary.\n        \"\"\"\n\n        model_summary = torchsummary.summary(model, image_size)\n        return model_summary\n\n\n    def forward_pass(\n        loader: torch.utils.data.DataLoader,\n        model: CustomNeuralNet,\n    ) -> Union[\n        torch.FloatTensor,\n        torch.LongTensor,\n        # model_statistics.ModelStatistics,\n    ]:\n        \"\"\"Performs a forward pass of a tensor through the model.\n        Args:\n            model (CustomNeuralNet): Model to be used for the forward pass.\n        Returns:\n            X (torch.FloatTensor): The input tensor.\n            y (torch.LongTensor): The output tensor.\n        \"\"\"\n        seed_all()\n        model.to(device)\n\n        batch_size, channel, height, width = iter(train_loader).next()[\"X\"].shape\n        image_size = (channel, height, width)\n\n        try:\n            config.logger.info(\"Model Summary: Uncomment me to see the long model summary!\")\n            torchsummary.summary(model, image_size)\n        except RuntimeError:\n            config.logger.debug(\"Check the channel number.\")\n\n        X = torch.randn((batch_size, *image_size)).to(device)\n        y = model(X)\n        config.logger.info(\"Forward Pass Successful!\")\n        config.logger.info(f\"x: {X.shape} \\ny: {y.shape}\")\n        config.logger.info(f\"x[0][0][0]: {X[0][0][0][0]} \\ny[0][0][0]: {y[0][0]}\")\n\n        free_gpu_memory(model, X, y)\n        return X, y","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:20.064647Z","iopub.execute_input":"2021-12-17T11:29:20.065086Z","iopub.status.idle":"2021-12-17T11:29:28.129621Z","shell.execute_reply.started":"2021-12-17T11:29:20.065049Z","shell.execute_reply":"2021-12-17T11:29:28.128634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Forward Pass","metadata":{}},{"cell_type":"markdown","source":"Performing a forward pass is a good habit to check if your model is working or not, any shape errors can be detected here!","metadata":{}},{"cell_type":"code","source":"if not is_inference:\n    # forward_X, forward_y = forward_pass(loader=train_loader, model=CustomNeuralNet())\n    forward_X, forward_y = forward_pass(loader=train_loader, model=dognet)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:29:28.131273Z","iopub.execute_input":"2021-12-17T11:29:28.131577Z","iopub.status.idle":"2021-12-17T11:29:41.047352Z","shell.execute_reply.started":"2021-12-17T11:29:28.131538Z","shell.execute_reply":"2021-12-17T11:29:41.046606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Learning Rate Finder\n\nWe use LR finder to find the best learning rate, this is especially useful when you are using `OneCycleLR`.\n\nFor more information, read the explaination here[^one_cycle_lr_kaggle] and the paper here[^superconvergence].\n\nWe will be using the library called pytorch-lr-finder[^pytorch-lr-finder] with references to fastai's documentation[^fastai-onecycle].\n\nMost examples and explanations are taken from the following links and check out [pytorch-lr-finder](https://github.com/davidtvs/pytorch-lr-finder) for working examples.\n\n[^one_cycle_lr_kaggle]: https://www.kaggle.com/residentmario/one-cycle-learning-rate-schedulers\n[^superconvergence]: https://arxiv.org/pdf/1803.09820.pdf\n[^pytorch-lr-finder]: https://github.com/davidtvs/pytorch-lr-finder\n[^fastai-onecycle]: https://fastai1.fast.ai/callbacks.one_cycle.html\n[^how-to-use-onecyclelr]: https://sgugger.github.io/the-1cycle-policy.html\n\n","metadata":{}},{"cell_type":"code","source":"import copy\nimport os\nimport torch\nimport numpy as np\nfrom tqdm.autonotebook import tqdm\nfrom torch.optim.lr_scheduler import _LRScheduler\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\n\nfrom packaging import version\n\nPYTORCH_VERSION = version.parse(torch.__version__)\n\n\nclass DataLoaderIter(object):\n    def __init__(self, data_loader):\n        self.data_loader = data_loader\n        self._iterator = iter(data_loader)\n\n    @property\n    def dataset(self):\n        return self.data_loader.dataset\n\n    def inputs_labels_from_batch(self, batch_data):\n        if not isinstance(batch_data, list) and not isinstance(\n            batch_data, tuple\n        ):\n            raise ValueError(\n                \"Your batch type is not supported: {}. Please inherit from \"\n                \"`TrainDataLoaderIter` or `ValDataLoaderIter` and override the \"\n                \"`inputs_labels_from_batch` method.\".format(type(batch_data))\n            )\n\n        inputs, labels, *_ = batch_data\n\n        return inputs, labels\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        batch = next(self._iterator)\n        return self.inputs_labels_from_batch(batch)\n\n\nclass TrainDataLoaderIter(DataLoaderIter):\n    def __init__(self, data_loader, auto_reset=True):\n        super().__init__(data_loader)\n        self.auto_reset = auto_reset\n\n    def __next__(self):\n        try:\n            batch = next(self._iterator)\n            inputs, labels = self.inputs_labels_from_batch(batch)\n        except StopIteration:\n            if not self.auto_reset:\n                raise\n            self._iterator = iter(self.data_loader)\n            batch = next(self._iterator)\n            inputs, labels = self.inputs_labels_from_batch(batch)\n\n        return inputs, labels\n\n\nclass ValDataLoaderIter(DataLoaderIter):\n    \"\"\"This iterator will reset itself **only** when it is acquired by\n    the syntax of normal `iterator`. That is, this iterator just works\n    like a `torch.data.DataLoader`. If you want to restart it, you\n    should use it like:\n        ```\n        loader_iter = ValDataLoaderIter(data_loader)\n        for batch in loader_iter:\n            ...\n        # `loader_iter` should run out of values now, you can restart it by:\n        # 1. the way we use a `torch.data.DataLoader`\n        for batch in loader_iter:        # __iter__ is called implicitly\n            ...\n        # 2. passing it into `iter()` manually\n        loader_iter = iter(loader_iter)  # __iter__ is called by `iter()`\n        ```\n    \"\"\"\n\n    def __init__(self, data_loader):\n        super().__init__(data_loader)\n        self.run_limit = len(self.data_loader)\n        self.run_counter = 0\n\n    def __iter__(self):\n        if self.run_counter >= self.run_limit:\n            self._iterator = iter(self.data_loader)\n            self.run_counter = 0\n        return self\n\n    def __next__(self):\n        self.run_counter += 1\n        return super(ValDataLoaderIter, self).__next__()\n\n\nclass LRFinder(object):\n    \"\"\"Learning rate range test.\n    The learning rate range test increases the learning rate in a pre-training run\n    between two boundaries in a linear or exponential manner. It provides valuable\n    information on how well the network can be trained over a range of learning rates\n    and what is the optimal learning rate.\n    Arguments:\n        model (torch.nn.Module): wrapped model.\n        optimizer (torch.optim.Optimizer): wrapped optimizer where the defined learning\n            is assumed to be the lower boundary of the range test.\n        criterion (torch.nn.Module): wrapped loss function.\n        device (str or torch.device, optional): a string (\"cpu\" or \"cuda\") with an\n            optional ordinal for the device type (e.g. \"cuda:X\", where is the ordinal).\n            Alternatively, can be an object representing the device on which the\n            computation will take place. Default: None, uses the same device as `model`.\n        memory_cache (boolean, optional): if this flag is set to True, `state_dict` of\n            model and optimizer will be cached in memory. Otherwise, they will be saved\n            to files under the `cache_dir`.\n        cache_dir (string, optional): path for storing temporary files. If no path is\n            specified, system-wide temporary directory is used. Notice that this\n            parameter will be ignored if `memory_cache` is True.\n    Example:\n        >>> lr_finder = LRFinder(net, optimizer, criterion, device=\"cuda\")\n        >>> lr_finder.range_test(dataloader, end_lr=100, num_iter=100)\n        >>> lr_finder.plot() # to inspect the loss-learning rate graph\n        >>> lr_finder.reset() # to reset the model and optimizer to their initial state\n    Reference:\n    Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n    fastai/lr_find: https://github.com/fastai/fastai\n    \"\"\"\n\n    def __init__(\n        self,\n        model,\n        optimizer,\n        criterion,\n        device=None,\n        memory_cache=True,\n        cache_dir=None,\n        use_amp=False,\n    ):\n        # Check if the optimizer is already attached to a scheduler\n        self.optimizer = optimizer\n        self._check_for_scheduler()\n\n        self.model = model\n        self.criterion = criterion\n        self.history = {\"lr\": [], \"loss\": []}\n        self.best_loss = None\n        self.memory_cache = memory_cache\n        self.cache_dir = cache_dir\n        self.use_amp = use_amp\n\n        # Save the original state of the model and optimizer so they can be restored if\n        # needed\n        self.model_device = next(self.model.parameters()).device\n        self.state_cacher = StateCacher(memory_cache, cache_dir=cache_dir)\n        self.state_cacher.store(\"model\", self.model.state_dict())\n        self.state_cacher.store(\"optimizer\", self.optimizer.state_dict())\n\n        # If device is None, use the same as the model\n        if device:\n            self.device = device\n        else:\n            self.device = self.model_device\n\n        if self.use_amp:\n            self.scaler = torch.cuda.amp.GradScaler()\n\n    def reset(self):\n        \"\"\"Restores the model and optimizer to their initial states.\"\"\"\n\n        self.model.load_state_dict(self.state_cacher.retrieve(\"model\"))\n        self.optimizer.load_state_dict(self.state_cacher.retrieve(\"optimizer\"))\n        self.model.to(self.model_device)\n\n    def range_test(\n        self,\n        train_loader,\n        val_loader=None,\n        start_lr=None,\n        end_lr=10,\n        num_iter=100,\n        step_mode=\"exp\",\n        smooth_f=0.05,\n        diverge_th=5,\n        accumulation_steps=1,\n        non_blocking_transfer=True,\n    ):\n        \"\"\"Performs the learning rate range test.\n        Arguments:\n            train_loader (`torch.utils.data.DataLoader`\n                or child of `TrainDataLoaderIter`, optional):\n                the training set data loader.\n                If your dataset (data loader) returns a tuple (inputs, labels,*) then\n                Pytorch data loader object can be provided. However, if a dataset\n                returns different outputs e.g. dicts, then you should inherit\n                from `TrainDataLoaderIter` class and redefine `inputs_labels_from_batch`\n                method so that it outputs (inputs, labels).\n            val_loader (`torch.utils.data.DataLoader`\n                or child of `ValDataLoaderIter`, optional): if `None` the range test\n                will only use the training loss. When given a data loader, the model is\n                evaluated after each iteration on that dataset and the evaluation loss\n                is used. Note that in this mode the test takes significantly longer but\n                generally produces more precise results.\n                Similarly to `train_loader`, if your dataset outputs are not standard\n                you should inherit from `ValDataLoaderIter` class and\n                redefine method `inputs_labels_from_batch` so that\n                it outputs (inputs, labels). Default: None.\n            start_lr (float, optional): the starting learning rate for the range test.\n                Default: None (uses the learning rate from the optimizer).\n            end_lr (float, optional): the maximum learning rate to test. Default: 10.\n            num_iter (int, optional): the number of iterations over which the test\n                occurs. Default: 100.\n            step_mode (str, optional): one of the available learning rate policies,\n                linear or exponential (\"linear\", \"exp\"). Default: \"exp\".\n            smooth_f (float, optional): the loss smoothing factor within the [0, 1[\n                interval. Disabled if set to 0, otherwise the loss is smoothed using\n                exponential smoothing. Default: 0.05.\n            diverge_th (int, optional): the test is stopped when the loss surpasses the\n                threshold:  diverge_th * best_loss. Default: 5.\n            accumulation_steps (int, optional): steps for gradient accumulation. If it\n                is 1, gradients are not accumulated. Default: 1.\n            non_blocking_transfer (bool, optional): when non_blocking_transfer is set,\n                tries to convert/move data to the device asynchronously if possible,\n                e.g., moving CPU Tensors with pinned memory to CUDA devices. Default: True.\n        Example (fastai approach):\n            >>> lr_finder = LRFinder(net, optimizer, criterion, device=\"cuda\")\n            >>> lr_finder.range_test(dataloader, end_lr=100, num_iter=100)\n        Example (Leslie Smith's approach):\n            >>> lr_finder = LRFinder(net, optimizer, criterion, device=\"cuda\")\n            >>> lr_finder.range_test(trainloader, val_loader=val_loader, end_lr=1, num_iter=100, step_mode=\"linear\")\n        Gradient accumulation is supported; example:\n            >>> train_data = ...    # prepared dataset\n            >>> desired_bs, real_bs = 32, 4         # batch size\n            >>> accumulation_steps = desired_bs // real_bs     # required steps for accumulation\n            >>> dataloader = torch.utils.data.DataLoader(train_data, batch_size=real_bs, shuffle=True)\n            >>> acc_lr_finder = LRFinder(net, optimizer, criterion, device=\"cuda\")\n            >>> acc_lr_finder.range_test(dataloader, end_lr=10, num_iter=100, accumulation_steps=accumulation_steps)\n        If your DataLoader returns e.g. dict, or other non standard output, intehit from TrainDataLoaderIter,\n        redefine method `inputs_labels_from_batch` so that it outputs (inputs, lables) data:\n            >>> import torch_lr_finder\n            >>> class TrainIter(torch_lr_finder.TrainDataLoaderIter):\n            >>>     def inputs_labels_from_batch(self, batch_data):\n            >>>         return (batch_data['user_features'], batch_data['user_history']), batch_data['y_labels']\n            >>> train_data_iter = TrainIter(train_dl)\n            >>> finder = torch_lr_finder.LRFinder(model, optimizer, partial(model._train_loss, need_one_hot=False))\n            >>> finder.range_test(train_data_iter, end_lr=10, num_iter=300, diverge_th=10)\n        Reference:\n        [Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU & Distributed setups](\n        https://medium.com/huggingface/ec88c3e51255)\n        [thomwolf/gradient_accumulation](https://gist.github.com/thomwolf/ac7a7da6b1888c2eeac8ac8b9b05d3d3)\n        \"\"\"\n\n        # Reset test results\n        self.history = {\"lr\": [], \"loss\": []}\n        self.best_loss = None\n\n        # Move the model to the proper device\n        self.model.to(self.device)\n\n        # Check if the optimizer is already attached to a scheduler\n        self._check_for_scheduler()\n\n        # Set the starting learning rate\n        if start_lr:\n            self._set_learning_rate(start_lr)\n\n        # Initialize the proper learning rate policy\n        if step_mode.lower() == \"exp\":\n            lr_schedule = ExponentialLR(self.optimizer, end_lr, num_iter)\n        elif step_mode.lower() == \"linear\":\n            lr_schedule = LinearLR(self.optimizer, end_lr, num_iter)\n        else:\n            raise ValueError(\n                \"expected one of (exp, linear), got {}\".format(step_mode)\n            )\n\n        if smooth_f < 0 or smooth_f >= 1:\n            raise ValueError(\"smooth_f is outside the range [0, 1[\")\n\n        # Create an iterator to get data batch by batch\n        if isinstance(train_loader, DataLoader):\n            train_iter = TrainDataLoaderIter(train_loader)\n        elif isinstance(train_loader, TrainDataLoaderIter):\n            train_iter = train_loader\n        else:\n            raise ValueError(\n                \"`train_loader` has unsupported type: {}.\"\n                \"Expected types are `torch.utils.data.DataLoader`\"\n                \"or child of `TrainDataLoaderIter`.\".format(type(train_loader))\n            )\n\n        if val_loader:\n            if isinstance(val_loader, DataLoader):\n                val_iter = ValDataLoaderIter(val_loader)\n            elif isinstance(val_loader, ValDataLoaderIter):\n                val_iter = val_loader\n            else:\n                raise ValueError(\n                    \"`val_loader` has unsupported type: {}.\"\n                    \"Expected types are `torch.utils.data.DataLoader`\"\n                    \"or child of `ValDataLoaderIter`.\".format(type(val_loader))\n                )\n\n        for iteration in tqdm(range(num_iter)):\n            # Train on batch and retrieve loss\n            loss = self._train_batch(\n                train_iter,\n                accumulation_steps,\n                non_blocking_transfer=non_blocking_transfer,\n            )\n            if val_loader:\n                loss = self._validate(\n                    val_iter, non_blocking_transfer=non_blocking_transfer\n                )\n\n            # Update the learning rate\n            self.history[\"lr\"].append(lr_schedule.get_lr()[0])\n            lr_schedule.step()\n\n            # Track the best loss and smooth it if smooth_f is specified\n            if iteration == 0:\n                self.best_loss = loss\n            else:\n                if smooth_f > 0:\n                    loss = (\n                        smooth_f * loss\n                        + (1 - smooth_f) * self.history[\"loss\"][-1]\n                    )\n                if loss < self.best_loss:\n                    self.best_loss = loss\n\n            # Check if the loss has diverged; if it has, stop the test\n            self.history[\"loss\"].append(loss)\n            if loss > diverge_th * self.best_loss:\n                print(\"Stopping early, the loss has diverged\")\n                break\n\n        print(\n            \"Learning rate search finished. See the graph with {finder_name}.plot()\"\n        )\n\n    def _set_learning_rate(self, new_lrs):\n        if not isinstance(new_lrs, list):\n            new_lrs = [new_lrs] * len(self.optimizer.param_groups)\n        if len(new_lrs) != len(self.optimizer.param_groups):\n            raise ValueError(\n                \"Length of `new_lrs` is not equal to the number of parameter groups \"\n                + \"in the given optimizer\"\n            )\n\n        for param_group, new_lr in zip(self.optimizer.param_groups, new_lrs):\n            param_group[\"lr\"] = new_lr\n\n    def _check_for_scheduler(self):\n        for param_group in self.optimizer.param_groups:\n            if \"initial_lr\" in param_group:\n                raise RuntimeError(\n                    \"Optimizer already has a scheduler attached to it\"\n                )\n\n    def _train_batch(\n        self, train_iter, accumulation_steps, non_blocking_transfer=True\n    ):\n        self.model.train()\n        total_loss = None  # for late initialization\n\n        self.optimizer.zero_grad()\n        for i in range(accumulation_steps):\n            inputs, labels = next(train_iter)\n            inputs, labels = self._move_to_device(\n                inputs, labels, non_blocking=non_blocking_transfer\n            )\n\n            # Forward pass\n\n            if self.use_amp:\n                with torch.cuda.amp.autocast():\n                    outputs = self.model(inputs)\n                    loss = self.criterion(outputs, labels)\n                self.scaler.scale(loss).backward()\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n            else:\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, labels)\n                loss.backward()\n                self.optimizer.step()\n\n            # Loss should be averaged in each step\n            loss /= accumulation_steps\n\n            if total_loss is None:\n                total_loss = loss\n            else:\n                total_loss += loss\n\n        return total_loss.item()\n\n    def _move_to_device(self, inputs, labels, non_blocking=True):\n        def move(obj, device, non_blocking=True):\n            if hasattr(obj, \"to\"):\n                return obj.to(device, non_blocking=non_blocking)\n            elif isinstance(obj, tuple):\n                return tuple(move(o, device, non_blocking) for o in obj)\n            elif isinstance(obj, list):\n                return [move(o, device, non_blocking) for o in obj]\n            elif isinstance(obj, dict):\n                return {\n                    k: move(o, device, non_blocking) for k, o in obj.items()\n                }\n            else:\n                return obj\n\n        inputs = move(inputs, self.device, non_blocking=non_blocking)\n        labels = move(labels, self.device, non_blocking=non_blocking)\n        return inputs, labels\n\n    def _validate(self, val_iter, non_blocking_transfer=True):\n        # Set model to evaluation mode and disable gradient computation\n        running_loss = 0\n        self.model.eval()\n        with torch.no_grad():\n            for inputs, labels in val_iter:\n                # Move data to the correct device\n                inputs, labels = self._move_to_device(\n                    inputs, labels, non_blocking=non_blocking_transfer\n                )\n\n                # Forward pass and loss computation\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, labels)\n                running_loss += loss.item() * len(labels)\n\n        return running_loss / len(val_iter.dataset)\n\n    def plot(\n        self,\n        skip_start=10,\n        skip_end=5,\n        log_lr=True,\n        show_lr=None,\n        ax=None,\n        suggest_lr=True,\n    ):\n        \"\"\"Plots the learning rate range test.\n        Arguments:\n            skip_start (int, optional): number of batches to trim from the start.\n                Default: 10.\n            skip_end (int, optional): number of batches to trim from the start.\n                Default: 5.\n            log_lr (bool, optional): True to plot the learning rate in a logarithmic\n                scale; otherwise, plotted in a linear scale. Default: True.\n            show_lr (float, optional): if set, adds a vertical line to visualize the\n                specified learning rate. Default: None.\n            ax (matplotlib.axes.Axes, optional): the plot is created in the specified\n                matplotlib axes object and the figure is not be shown. If `None`, then\n                the figure and axes object are created in this method and the figure is\n                shown . Default: None.\n            suggest_lr (bool, optional): suggest a learning rate by\n                - 'steepest': the point with steepest gradient (minimal gradient)\n                you can use that point as a first guess for an LR. Default: True.\n        Returns:\n            The matplotlib.axes.Axes object that contains the plot,\n            and the suggested learning rate (if set suggest_lr=True).\n        \"\"\"\n\n        if skip_start < 0:\n            raise ValueError(\"skip_start cannot be negative\")\n        if skip_end < 0:\n            raise ValueError(\"skip_end cannot be negative\")\n        if show_lr is not None and not isinstance(show_lr, float):\n            raise ValueError(\"show_lr must be float\")\n\n        # Get the data to plot from the history dictionary. Also, handle skip_end=0\n        # properly so the behaviour is the expected\n        lrs = self.history[\"lr\"]\n        losses = self.history[\"loss\"]\n        if skip_end == 0:\n            lrs = lrs[skip_start:]\n            losses = losses[skip_start:]\n        else:\n            lrs = lrs[skip_start:-skip_end]\n            losses = losses[skip_start:-skip_end]\n\n        # Create the figure and axes object if axes was not already given\n        fig = None\n        if ax is None:\n            fig, ax = plt.subplots()\n\n        # Plot loss as a function of the learning rate\n        ax.plot(lrs, losses)\n\n        # Plot the suggested LR\n        if suggest_lr:\n            # 'steepest': the point with steepest gradient (minimal gradient)\n            print(\"LR suggestion: steepest gradient\")\n            min_grad_idx = None\n            try:\n                min_grad_idx = (np.gradient(np.array(losses))).argmin()\n            except ValueError:\n                print(\n                    \"Failed to compute the gradients, there might not be enough points.\"\n                )\n            if min_grad_idx is not None:\n                print(\"Suggested LR: {:.2E}\".format(lrs[min_grad_idx]))\n                ax.scatter(\n                    lrs[min_grad_idx],\n                    losses[min_grad_idx],\n                    s=75,\n                    marker=\"o\",\n                    color=\"red\",\n                    zorder=3,\n                    label=\"steepest gradient\",\n                )\n                ax.legend()\n\n        if log_lr:\n            ax.set_xscale(\"log\")\n        ax.set_xlabel(\"Learning rate\")\n        ax.set_ylabel(\"Loss\")\n\n        if show_lr is not None:\n            ax.axvline(x=show_lr, color=\"red\")\n\n        # Show only if the figure was created internally\n        if fig is not None:\n            plt.show()\n\n        if suggest_lr and min_grad_idx is not None:\n            return ax, lrs[min_grad_idx]\n        else:\n            return ax\n\n\nclass LinearLR(_LRScheduler):\n    \"\"\"Linearly increases the learning rate between two boundaries over a number of\n    iterations.\n    Arguments:\n        optimizer (torch.optim.Optimizer): wrapped optimizer.\n        end_lr (float): the final learning rate.\n        num_iter (int): the number of iterations over which the test occurs.\n        last_epoch (int, optional): the index of last epoch. Default: -1.\n    \"\"\"\n\n    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n        self.end_lr = end_lr\n\n        if num_iter <= 1:\n            raise ValueError(\"`num_iter` must be larger than 1\")\n        self.num_iter = num_iter\n\n        super(LinearLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        # In earlier Pytorch versions last_epoch starts at -1, while in recent versions\n        # it starts at 0. We need to adjust the math a bit to handle this. See\n        # discussion at: https://github.com/davidtvs/pytorch-lr-finder/pull/42\n        if PYTORCH_VERSION < version.parse(\"1.1.0\"):\n            curr_iter = self.last_epoch + 1\n            r = curr_iter / (self.num_iter - 1)\n        else:\n            r = self.last_epoch / (self.num_iter - 1)\n\n        return [\n            base_lr + r * (self.end_lr - base_lr) for base_lr in self.base_lrs\n        ]\n\n\nclass ExponentialLR(_LRScheduler):\n    \"\"\"Exponentially increases the learning rate between two boundaries over a number of\n    iterations.\n    Arguments:\n        optimizer (torch.optim.Optimizer): wrapped optimizer.\n        end_lr (float): the final learning rate.\n        num_iter (int): the number of iterations over which the test occurs.\n        last_epoch (int, optional): the index of last epoch. Default: -1.\n    \"\"\"\n\n    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n        self.end_lr = end_lr\n\n        if num_iter <= 1:\n            raise ValueError(\"`num_iter` must be larger than 1\")\n        self.num_iter = num_iter\n\n        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        # In earlier Pytorch versions last_epoch starts at -1, while in recent versions\n        # it starts at 0. We need to adjust the math a bit to handle this. See\n        # discussion at: https://github.com/davidtvs/pytorch-lr-finder/pull/42\n        if PYTORCH_VERSION < version.parse(\"1.1.0\"):\n            curr_iter = self.last_epoch + 1\n            r = curr_iter / (self.num_iter - 1)\n        else:\n            r = self.last_epoch / (self.num_iter - 1)\n\n        return [\n            base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs\n        ]\n\n\nclass StateCacher(object):\n    def __init__(self, in_memory, cache_dir=None):\n        self.in_memory = in_memory\n        self.cache_dir = cache_dir\n\n        if self.cache_dir is None:\n            import tempfile\n\n            self.cache_dir = tempfile.gettempdir()\n        else:\n            if not os.path.isdir(self.cache_dir):\n                raise ValueError(\"Given `cache_dir` is not a valid directory.\")\n\n        self.cached = {}\n\n    def store(self, key, state_dict):\n        if self.in_memory:\n            self.cached.update({key: copy.deepcopy(state_dict)})\n        else:\n            fn = os.path.join(\n                self.cache_dir, \"state_{}_{}.pt\".format(key, id(self))\n            )\n            self.cached.update({key: fn})\n            torch.save(state_dict, fn)\n\n    def retrieve(self, key):\n        if key not in self.cached:\n            raise KeyError(\"Target {} was not cached.\".format(key))\n\n        if self.in_memory:\n            return self.cached.get(key)\n        else:\n            fn = self.cached.get(key)\n            if not os.path.exists(fn):\n                raise RuntimeError(\n                    \"Failed to load state in {}. File doesn't exist anymore.\".format(\n                        fn\n                    )\n                )\n            state_dict = torch.load(\n                fn, map_location=lambda storage, location: storage\n            )\n            return state_dict\n\n    def __del__(self):\n        \"\"\"Check whether there are unused cached files existing in `cache_dir` before\n        this instance being destroyed.\"\"\"\n\n        if self.in_memory:\n            return\n\n        for k in self.cached:\n            if os.path.exists(self.cached[k]):\n                os.remove(self.cached[k])\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:41.049215Z","iopub.execute_input":"2021-12-17T11:29:41.049499Z","iopub.status.idle":"2021-12-17T11:29:41.129235Z","shell.execute_reply.started":"2021-12-17T11:29:41.049464Z","shell.execute_reply":"2021-12-17T11:29:41.128325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First we need to define the model, criterion and optimizer that we will be using. With them defined, we call `LRFinder`.","metadata":{}},{"cell_type":"markdown","source":"Subsequently, we create `DataLoaders` to be passed in. However, because our loader returns a dictionary instead of inputs, labels, * format, we need to use a custom data iterator to convert them. The author of the repo has provided examples and we need to follow them by calling additional wrappers `CustomTrainIter` and `CustomValIter`.","metadata":{}},{"cell_type":"markdown","source":"\nIf a dataloader is passed to LRFinder.range_test() through the val_loader parameter the model is evaluated on that dataset after each iteration. The evaluation loss is more sensitive to instability therefore it provides a more precise view of when the divergence occurs. The disadvantage is that it takes significantly longer to run.","metadata":{}},{"cell_type":"markdown","source":"In our case, we can choose MAX_LR = 4.5e-4 and MIN_LR = 5e-5. In the [link](https://github.com/pytorch/pytorch/issues/36133), we can use `AdamW` as the optimizer.","metadata":{}},{"cell_type":"code","source":"if not is_inference:\n    # from torch_lr_finder import LRFinder, TrainDataLoaderIter, ValDataLoaderIter\n\n    use_valid = False\n    use_lr_finder = False\n\n    if use_lr_finder:\n        # model = CustomNeuralNet(pretrained=True)\n        model = dognet\n        criterion = torch.nn.BCEWithLogitsLoss()\n\n        # Use back the same optimizer params you will use in training!\n        optimizer = getattr(torch.optim, OPTIMIZER_PARAMS.optimizer_name)(\n            model.parameters(), **OPTIMIZER_PARAMS.optimizer_params\n        )\n\n\n        lr_finder = LRFinder(model, optimizer, criterion, device=device, use_amp=True)\n\n        train_dataset = CustomDataset(\n            df=df_folds, transforms=get_train_transforms(), mode=\"train\"\n        )\n        valid_dataset = CustomDataset(\n            df=df_folds, transforms=get_valid_transforms(), mode=\"train\"\n        )\n\n        train_loader = torch.utils.data.DataLoader(\n            train_dataset,\n            **LOADER_PARAMS.train_loader,\n            worker_init_fn=seed_worker,\n        )\n\n        valid_loader = torch.utils.data.DataLoader(\n            valid_dataset,\n            **LOADER_PARAMS.valid_loader,\n            worker_init_fn=seed_worker,\n        )\n\n        class CustomTrainIter(TrainDataLoaderIter):\n            def inputs_labels_from_batch(self, batch_data):\n                return batch_data[\"X\"], batch_data[\"y\"].view(-1, 1)\n\n        class CustomValIter(ValDataLoaderIter):\n            def inputs_labels_from_batch(self, batch_data):\n                return batch_data[\"X\"], batch_data[\"y\"].view(-1, 1)\n\n        custom_train_iter = CustomTrainIter(train_loader)\n        custom_valid_iter = CustomValIter(valid_loader)\n\n        ##################\n\n        lr_finder.reset()\n        if use_valid:\n            lr_finder.range_test(\n                custom_train_iter,\n                val_loader=custom_valid_iter,\n                start_lr=1e-7,\n                end_lr=3e-2,\n                num_iter=100,\n                step_mode=\"exp\",\n            )  # [\"exp\", \"linear\"]\n        else:\n            lr_finder.range_test(\n                custom_train_iter,\n                start_lr=1e-7,\n                end_lr=3e-2,\n                num_iter=100,\n                step_mode=\"exp\",\n            )  # [\"exp\", \"linear\"]\n\n        lr_finder.plot()\n\n        free_gpu_memory(model, optimizer, criterion)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:41.130873Z","iopub.execute_input":"2021-12-17T11:29:41.131158Z","iopub.status.idle":"2021-12-17T11:29:41.146128Z","shell.execute_reply.started":"2021-12-17T11:29:41.13112Z","shell.execute_reply":"2021-12-17T11:29:41.145337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The learning rate and loss history can be accessed through `lr_finder.history` which returns a dictionary with lr and loss keys.","metadata":{}},{"cell_type":"code","source":"# lr_finder.history","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:29:41.147513Z","iopub.execute_input":"2021-12-17T11:29:41.14803Z","iopub.status.idle":"2021-12-17T11:29:41.158012Z","shell.execute_reply.started":"2021-12-17T11:29:41.147989Z","shell.execute_reply":"2021-12-17T11:29:41.157219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Choosing the learning rate from plot\n\nThe learning rate range test is a test that provides valuable information about the optimal learning rate. During a pre-training run, the learning rate is increased linearly or exponentially between two boundaries. The low initial learning rate allows the network to start converging and as the learning rate is increased it will eventually be too large and the network will diverge.\n\nTypically, a good static learning rate can be found half-way on the descending loss curve. In the plot below that would be lr = 0.002.\n\nFor cyclical learning rates (also detailed in Leslie Smith's paper) where the learning rate is cycled between two boundaries (start_lr, end_lr), the author advises the point at which the loss starts descending and the point at which the loss stops descending or becomes ragged for start_lr and end_lr respectively. In the plot below, start_lr = 0.0002 and end_lr=0.2 for `OneCycleLR`.\n\nTo explain a bit more, the suggested MAX LR should be around 0.2, which is close to the minimum of the plot, but not necessarily so, in fastai, the tutorial mentioned that you can even cut the max lr by one-tenth [here](https://fastai1.fast.ai/callbacks.one_cycle.html). The MIN LR can be around 0.0002, where the descending of loss happens.\n\n![image.png](attachment:81432d11-f515-4557-8420-f51ebfbd2009.png)","metadata":{},"attachments":{"81432d11-f515-4557-8420-f51ebfbd2009.png":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEOCAYAAABmVAtTAAAgAElEQVR4nO3dd3xV9f3H8de9NzcJGSSEJKwwE0aRvYco4t4iDrTugVarUq229tfW1mqttbXugVhR60atCipaRQTCCluWJGwZWZDJvbm5N78/zk2IMSEJ5N5z7s37+Xjch+eee3LPO5Hkcz9nfL8gIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIScDazAzRX+/btq3r06GF2DBGRkLJy5cp8IKWx7SICmCEa+BaI8u9nNvBAA9tO8b8+Esg62pv26NGDrKyjbiIiInXYbLadTdkukEXBDUwCSgEnsAj4DFhaZ7t44C5gWQCziIhIE9gD+N5VGAUBjKLg9K+r6y/Ao4ArgFlERKQJAlkUABzAGiAX+JKfdgPDgK7A3ADnEBGRJgh0UfACQ4A0YBQwoM6+HwfuacL7TMM415CVl5fX0hlFRMQv0EWh2iFgPnBWrXXxGEXiG2AHMAb4GBhRz9fP8K8fkZLS6MlzERE5RoEsCilAon+5DXA6sLnW60VAMtDD/1gKXEAjVx+JiEjgBLIodMLoDtYBKzDOKcwBHsT44y8iIk00b8N+dhWUB3w/gbwkdR0wtJ71f2xg+4kBzCIiErIOV3i5483VXDe+B78752cB3VewzimIiMgxWrnzIBVeH2PT2wd8XyoKIiIWt2RbPg67jZE9kgK+LxUFERGLy8wpYHBaAnFRgTzib1BREBGxsFJ3Jev2FAXl0BGoKIiIWNqK7YV4fVWMS08Oyv5UFERELGzJtgIiHXaGd28XlP2pKIiIWFhmTj5DuyUS7XQEZX8qCiIiFlVU7mHD3uKgnU8AFQUREctaur2AqiqCdj4BVBRERCxrSU4B0U47g7smBG2fKgoiIha1JKeAkT2SiIoIzvkEUFEQEbGk/FI3Ww6UMKZX8M4ngIqCiIglLd1WAMC4IJ5kBhUFERFLWpJTQFxUBAO7BO98AqgoiIhY0pKcAkb1TCLCEdw/0yoKIiIWc6DYxbb8MsYG+XwCqCiIiFjO1gOlAAwI8qEjUFEQEbEcl8cLQGxU8C5FraaiICJiMa5KoygEa7yj2lQUREQsxu3xARAdxJvWqqkoiIhYTHWnEOUM/p9oFQUREYtRpyAiIjXUKYiISI3qTiEqQkVBRKTVc1V6iYywY7PZgr5vFQUREYtxe3xEm9AlgIqCiIjluCu9RJlwjwKoKIiIWI7L4yPahJPMoKIgImI57kqvKZejgoqCiIjluDw+Uy5HhcAWhWhgObAW2AD8uZ5t7gY2AuuAr4DuAcwjIhISwrVTcAOTgMHAEOAsYEydbVYDI4BBwGzg7wHMIyISEsK1U6gCSv3LTv+jqs4284Fy//JSIC2AeUREQoLLE56dAoADWAPkAl8Cy46y7Y3AZwHOIyJiee7K8OwUALwYh47SgFHAgAa2uwrjMNJjDbw+DcgCsvLy8lo6o4iIpYRzp1DtEMahorPqee004P+ACzDOQ9RnBkbRGJGSkhKQgCIiVmF0CuFXFFKARP9yG+B0YHOdbYYCL2IUhNwAZhERCRkuj9eUwfAAIgL43p2AVzHOK9iBd4E5wIMYh4I+xjhcFAe85/+aXRgFQkSk1XJX+kyZihMCWxTWYXQCdf2x1vJpAdy/iEjI8fmqqKj0mdYp6I5mERELqfD6Z10Lw3MKIiLSTC6Pf9Y1dQoiIuLyqFMQERE/d6U6BRER8VOnICIiNao7BU2yIyIiNZ1CVJgPcyEiIk2gTkFERGqoUxARkRrqFEREpIY6BRERqVF9R7M6BRERwV3p7xR0n4KIiGjsIxERqVHTKagoiIiI2z/rms1mM2X/KgoiIhZi5qxroKIgImIpZs7PDCoKIiKW4vJ41SmIiIjBbeL8zKCiICJiKeoURESkhnGiWZ2CiIhQfaJZnYKIiKBOQUREalGnICIiNdyVPqLUKYiICBjzKahTEBERwBj7SOcUREQEMH/so4gAvnc08C0Q5d/PbOCBOttEAa8Bw4EC4HJgRyDCZGbn89DcTXRLiqFb+xi6JsXQLSmGwWkJJMZEBmKXIiLN4vVVUeE1947mQBYFNzAJKAWcwCLgM2BprW1uBA4CGcBU4FGMwtDiIhx2OrSNYmtuCV9vyaXCP2a53QYD0xKZkJHMib2T6d4+hjJ3JWVuL2UVlQBkpMSREh9l2lC2ItI6VP9dCtdOoQqjIIBRFJz+dbVdCPzJvzwbeAaw1bPdcRvVM4lRPUcB4PNVkVviZlt+Kcu2FbIoO5/nF+TwzPzsBr8+McZJn9R40lNjsdtsVHqr8Hh9VHh9JMdFkZEaV/NoHxupAiIizWb2rGsQ2KIA4ABWYnQCzwLL6rzeBdjtX64EioD2QH4gQ9ntNjomRNMxIZpx6cn86vQ+FLs8LM0poKCsgtioCGIjHcRERuD1VbE1t4TvD5Sy9UAJX2w4AECEw4bTYSfCbiO3xE15hbfm/aOdduKinMRHRxAb5SA+yklq2yg6tI0mNd74b8/kWDJS437yicDl8ZKdW0rRYQ+D0hKIj3YG8kchIhbiDvNOAcALDAESgQ+BAcB3x/A+0/wP8vLyWixcbW2jnZxxQsd6Xzuxd/JRv7aqqop9RS6yc0vJzi1l76HDlFVUUur2UuaupOiwh9W7DrG/2FXTHoJx6Kp7+1j6dIgD4PsDpewsKMNXdeT1AV0SGN0zieHd2+GrgoPlFRwq93CovIKU+CgGpyUyMC2BmMgf/6/0+qpwebzERgX6f7GItJTW0ClUOwTMB87ix0XhB6ArsMefJQHjhHNdM/wPUlJSWvzQ0vGy2Wx0TmxD58Q2nNQnpcHtqqqqKD5cyb7iw+TklrHlQAlbD5Sw5UAJNqBfx3guGNyZPh3iiYuOIGtHIcu2FfJq5k5eWrj9R+8VFWGv+VRht0GfDvF0Sogmr9RNbrGbgrIKvL4q0tq1YXDXRIakJTK4ayKD0hJM/RQiIg0L904hBfBgFIQ2wOkYJ5Jr+xi4FlgCXAJ8TQDOJ1iFzWYjIcZJQoyTfh3bci6djrr9yf4C4/J42bSvmGing3YxkSTGOIl2OsgvdbNuzyHW7C5ize5D5Ja4SYmPon+ntqTGR9Mm0sHGvcWs2XWIuev2ARAZYWdE93aMz0hmbHp7BnZJwOnQlckiVhDunUIn4FWM8wp24F1gDvAgkIVREF4GXgeygUKMK5Ckjming6Hd2v1kfXJcFJP6dWBSvw6NvkdeiZs1uw+xdFsBi7PzeWzeFsDoMjoltKFLuzZ0bRdDr5RYJvROZkDnBOx2nSwXCabqohCuncI6YGg96/9Ya9kFXBrADOKXEh/F6f07cHp/o4AUlLpZsq2A7/eXsPvgYfYcLCczJ5/3V+3hsXlbSI6L5KQ+KUzsm0q/jvF0bRdDm0gddhIJpCOHj8KzUxALax8XxXmDOsOgH68vKHXz7dY85m/O4+vNuXyw6oea11Lio+iWFMOgtAQm9k1ldM8knZ8QaUFHDh+FZ6cgIah9XBSTh6YxeWgaXl8VG/cWs72gjN2F5ewqKGdHQRlvLtvFK4t3EO20My49mUn9UjnjhA6kxkebHV8kpKlTEEtz2G0MTEtgYFrCj9a7PF6WbCtgwZY85m/J5evNufzho+8Y0b0dZw/oxFkDOtI5sY1JqUVClzoFCUnRTgen9E3llL6pPFDVn625pXy6fh+ff7efB+ds5C9zN3Jm/47ccnKvek+Qi0j9qjsFM+dTUFGQ42Kz2ejTIZ4+HeKZfloftuWV8v6qPby+ZCefb9jPqJ5J3HpyLyb2SdXVTCKNsEKnoAvUpUX1Sonj3jP7kXn/qfzhvP7sKSznhllZnPHEt7y1fFfNP3oR+SkrnFNQUZCAiIuK4MYTe7LgvlP41+WDiXTYuf+D9Yz729c8/uX35Je6zY4oYjlujxebDSJNvKFURUECyumwM3loGnPvPJG3bh7DsG6JPPXVVib94xveXbGbqqqwvYFdpNlclcZcCmaOsqyiIEFhs9kYm96emdeO5MtfnUS/Tm257/11XPXyMnYVlJsdT8QSjKk4zb33R0VBgq53h3jevnkMD08ewNrdRZzxxAJmLtxGpdfX+BeLhDGXx9xZ10BFQUxit9v4+ejufHn3SYxPT+ahuZuY/Fwm3/1QZHY0EdO4K9UpSCvXKaENM68dwTNXDmVfkYsLn13MI59u4nCFrlKS1ieUOoV0IMq/PBG4E2PiHJHjZrPZOG9QZ766+2QuHZ7Gi99u44wnFjBvw36diJZWJZQ6hfcxZlHLwJjspivwZqBCSeuUEOPkb1MG8fa0MURFOLjl9ZVc9uISVu86aHY0kaAIpU7BhzGH8mTgaeBeaGSGGJFjNKZXez6/awIPTx7A9vxyJj+Xye1vrmJ3oa5SkvDmCqFOwQNcgTFL2hz/Os0oLwET4bDz89Hd+ebeidx5am++3pTLmU98y3+W7tQhJQlbbo/P1CEuoOlF4XpgLPAwsB3oiTFjmkhAxUVFcPfpffjfPSczvHs7fv/f77j65eXsOaiuQcKPq9Jr6mB40PSisBHj5PJbQDsgnp/OtywSMF0S2/DaDaP46+SBrN51kLOeWMi7K3abHUukRbk9PqJDpFP4BmgLJAGrgJeAxwMVSqQ+NpuNK0d34/PpJzGwSwL3vb+O3324nopK3fQm4cEdQp1CAlAMXAy8BowGTgtUKJGj6ZoUw39uGs2tJ6fz5rJdXDVzGQUaYE/CQCh1ChEYVxtdxpETzSKmcdht/Pbsfjw5dQhr9xzigmcWs3FvsdmxRI5LKJ1TeBCYB+QAK4BewNZAhRJpqguHdOG9W8fi9VUx5flM5m3Yb3YkkWPi9VXh8VaFTKfwHjAI+IX/+TZgSkASiTTToLREPr5jPH07xnPrf1by4oIcXbYqIcddaQztYuYEO9D0opAGfAjk+h/v+9eJWEJqfDRvTxvDOQM78chnm7n/g/V4NOqqhBCXxz8/c4jc0fwK8DHQ2f/4xL9OxDKinQ6enjqUOyZl8PaK3Vz77+UUlXvMjiXSJEc6hdA4fJSCUQQq/Y9Z/nUilmK327jnjL7889LBrNhRyJUzl3KwrMLsWCKNqukUQuTwUQFwFeDwP67yrxOxpCnD03jpmhFszS3lipeW6pJVsbyaTiFETjTfgHE56n5gH3AJcF2gQom0hIl9U3n52hFszy/jypeWka/CIBYWap3CTuACjENGqcBF6OojCQETeqfwynUj2VlYxhUzlpJb4jI7kki93J7Q6hTqc3eLpRAJoHEZycy6fhQ/HDrMpS8sISev1OxIIj/hqgytTqE+tkZe7wrMxxhMbwNwVz3bJGBcybTWv831x5FHpEFjerXnPzeNptRVycXPZZKZk292JJEfcfk7hVAZOrs+jd0dVAncA/QHxgC3+5drux2jaAzGmObzn0DkcWQSadCwbu347+3jSYmP4pqXl2uUVbEUt79TsPolqSUYA+HVfZRg3K9wNPswRlStfp9NQJc621RhDMNtA+KAQoxiIhIQXZNieP8X4xib3p773l/HI59tolI3uYkFHOkUrH34KB5jyOy6j3iMQfKaqgcwFFhWZ/0zwM+AvcB6jENM+g2VgEpo4+Tf143k56O78eKCbUydsVST9ojpQqVTaAlxGMNiTMfoMmo7E1iD0XUMwSgSbet5j2lAFpCVl5cXuKTSajgddh6ePJAnLh/C5v0lnP3kQj5eu9fsWNKKVV99FMonmpvCiVEQ3gA+qOf16/3rq4BsjKk++9Wz3QxgBDAiJUU3UkvLuWhoFz69cwIZqXHc+dZq7nl3LeUVOoIpwVfTKYTwiebG2ICXMc4lNDRL2y7gVP9yB6AvxgisIkHTrX0M794yljsnZfDh6j3cMGuFCoMEncvjxWYDp6OxCzsDK5BFYTxwNTAJ4xDRGuAc4Fb/A+AvwDiM8wlfAb8BdK2gBJ3TYefuM/ryr8uHsHx7Ide/osIgweWuNGZds9nMLQrNOVncXIto/F6GvcAZAcwg0iwXDjEukPvVO2u47pUVzLp+JDGRgfw1ETG4PF7T51KA4JxoFgkpFw7pwr8uH0LWjkKue2UFZW51DBJ4Lo/X9BvXQEVBpF4XDunCE1OHkrWjkCtnLiOvRIPpSWC5K33qFESs7ILBnXnhquFs2V/M5OcWk51bYnYkCWPqFERCwBkndOSdaWNxeXwaM0kCSp2CSIgY3DWRD28bR4e20Vzz8nJmr9xjdiQJQ+oUREJI16QYZv9iHKN7JfHr99byauYOsyNJmHFX+ky/mxlUFESarHrMpNP7d+CBjzcw49scsyNJGHF5fOoUREJNVISD534+jHMHdeKvn27m6a+2mh1JwoS70hr3KeiuHJFmcjrsPHn5EKIcdv755fe4K33cc0Yf0+9EldDm9vhMHyEVVBREjkmEw85jlw4mMsLOM/OzKShz8+CFA3A6zP+kJ6HJONFs/r8fFQWRY+Sw2/jr5IG0j4vk2fk5/HDIxbNXDiU+2ml2NAlBxiWp5ncK5pclkRBmt9u498x+/O3igSzOzufSF5awr+iw2bEkBFmlUzA/gUgYmDqqG69cN5I9Bw9z0bOL+e6HIrMjSQip9Pqo9FWpUxAJJyf1SWH2L8bisNmY8nwm72btNjuShIjqCXbUKYiEmX4d2/LJHScyvHs77pu9jvs/WI+70mt2LLE4q8zPDCoKIi2ufVwUr90wil9MTOet5bu47IUl/HBI5xmkYS7//MxWuE/B/AQiYSjCYec3Z/XjhauGk5NXxoXPLGb9Hp1nkPodOXykTkEkrJ01oCP/vX0cURF2LntxCfM355odSSxInYJIK5KRGs+Ht40jPTWWm17L4q3lu8yOJBZTXRTUKYi0Eqlto3ln2lgm9E7m/g/W8495W6iqqjI7llhEzeEjdQoirUdsVAQzrxnBFaO68sz8bH7/3+/w+VQYxFqdgoa5EAmiCIedv04eSEKbSF5YkMPhCi9/v2QQERozqVU7ckmq+f8OVBREgsxms/Hbs/sRHx3BY/O2UF7h5ckrhljiU6KYw0qdgvllSaSVuv2UDB44vz+fb9jPza+tpMxdaXYkMYmVOgXzE4i0YteP78nfpwxi0dY8zn96kcZMaqXcNZekqlMQafUuG9mV/9w0mvIKL5OfW8zMhdt0ArqVcXk09pGI1DIuPZnP7prAKX1TeWjuJq6btYLcEpfZsSRIqsfHUqcgIjXaxUby4tXDeeiiASzbVsA5Ty5i4dY8s2NJELg8Puw2iLCbP6WrioKIhdhsNq4a051P7jiRdjFOrvn3ch6bt5lKr8/saBJAxS4PsVERlpjnO5BFoSswH9gIbADuamC7icAa/zYLAphHJGT06RDPx788kcuGd+XZ+TlMnbFUI62Gse35ZfRMjjU7BhDYolAJ3AP0B8YAt/uXa0sEngMuAE4ALg1gHpGQ0ibSwaOXDOLJqUPYtK+Yc59ayNebD5gdSwIgJ7eU9JQ4s2MAgS0K+4BV/uUSYBPQpc42VwIfANUjhGkISZE6LhzShTl3TqBTQhtumJXFo5/rcFI4KXNXsrfIRUZq+BeF2noAQ4Flddb3AdoB3wArgWuClEckpPRMjuXD28ZxxahuPP9NDlfOXMaBYl2dFA625ZUBkJ4S/oePqsUB7wPTgeI6r0UAw4FzgTOBP2AUirqmAVlAVl6ersaQ1ina6eCRiwfyr8sHs35PEec+tZD/bdThpFCXnVcC0Go6BSdGQXgD4zBRXXuAeUAZkA98CwyuZ7sZwAhgREpKSmCSioSIyUPT+PiX40mOi+Km17K49721FLs8ZseSY5STW4bDbqNbUvh3CjbgZYxzCY83sM1HwIkYHUMMMNq/vYgcRW//1Um/PCWD91ft4ewnFrI4O9/sWHIMsnNL6d4+hkgL3M0MgS0K44GrgUkYl5yuAc4BbvU/wCgAnwPrgOXATOC7AGYSCRuREXZ+fWZf3v+FMd3nz2cu45FPN+kkdIjJybPOlUcQ2KGzF2F0C415zP8QkWMwtFs75t45gYfmbuTFb7fx3d4inr5iGEmxkWZHk0ZUen3sKCjj1J91MDtKDWv0KyJyXNpEOnh48kD+fskgVuw4qBFXQ8SuwnI83irLnGQGFQWRsHLZiK68d8tYfFVVTHk+k9kr95gdSY4ix2KXo4KKgkjYGdw1kU/uOJGh3RL59Xtruevt1RQd1tVJVpSdWwpAujoFEQmk5Lgo3rhpDPec3oc56/ZxzpMLWbGj0OxYUkdOXimp8VG0jXaaHaWGioJImHLYbdxxam9m3zqWCIeNy19cwj+/2IJXE/hYRnZuqaXOJ4CKgkjYq7466eJhaTz9dTY3zFpBUbkOJ5mtqqrKcpejgoqCSKsQFxXBPy4dzCMXDyQzJ58Ln11Edm6J2bFatbwSNyWuSkudZAYVBZFW5YpR3Xjr5jGUur1c9Gymxk4yUXaecZI5IzXe5CQ/pqIg0sqM6JHEJ3eMp1dKLDe9lsWVLy3l9SU7yNWoq0GVU3PlkToFETFZp4Q2vHvLWKaf1pv9xS7+8NEGRj/yFZe+kMmcdXupqtLJ6EDLySsjNtJBx7bRZkf5kUAOcyEiFhbtdDD9tD7cdWpvtuaW8un6fXyydi+/fHM1H/Xfy0MXDaCDxf5ghZPs3FLSU+MsMS9zbeoURFo5m81Gnw7xTD+tD/Omn8TvzunHt9/ncfrjC3gva7e6hgDJySslw2JXHoGKgojUEuGwM+2kdD6ffhL9Orbl3tnruGHWCs3X0MJK3ZXsK3JZ6k7maioKIvITPZNjeXvaGP50fn8WZecz5blMdheWmx0rbGzzX3lktXsUQEVBRBpgt9u4bnxPXr1hFAeKXUx+bjFrdh8yO1ZYyKm5HNVaVx6BioKINGJcejIf3DaONpEOLn9xCZ+t32d2pJCXnVtqqSk4a1NREJFGZaTG8+Ft4+nfuS23vbmKmQu36QT0ccjJLbPUFJy1WS+RiFhSclwUb908hrNO6MhDczfx5082anC9Y1BeUUnWzoP0sdidzNVUFESkyaKdDp69chg3ntiTWZk7uO2Nlbg8XrNjhZQXFmwjv9TNTRN6mh2lXioKItIsdruNP5zXnz+c158vNh7gypeWUlhWYXaskPDDocO8uCCH8wd3ZkSPJLPj1EtFQUSOyY0n9uS5K4fx3d5iLnhGc0I3xd8+24zNBr89u5/ZURqkoiAix+zsgZ1495axVHqNOaE/XK05oRuyYkchn6zdy7ST0umS2MbsOA1SURCR4zLEPyf04K6J/OqdtTz4yUY8Xp/ZsSzF56viwU820rFtNLee3MvsOEeloiAixy0lPoo3bhrN9eN78O/F27n65WUUlLrNjmUZs1ftYf0PRfz27H7ERFp7HFIVBRFpEU6HnQfOP4HHLxvM6l2HuOCZxa3+PEOZu5J3V+zm0c82M7RbIhcO6Wx2pEZZu2SJSMi5eFgavVPjueX1LKY8n8mjUwZx0dAuZscKqpU7C3lnxW7mrNtHeYWX9JRYHrl4oOWGya6PioKItLiBaQl8fMeJ3P7GKqa/s4Y1uw9x16m9aRcbaXa0gHs1cwcPfLyBmEgH5w/qzGUj0xjWrV1IFASA0EhZy/Dhw6uysrLMjiEiTeDx+nh47iZmZe4g2mln8tA0rh/fgz4drHk37/EqLKvg5MfmMygtgRlXjyA2yjqfu20220pgRGPbWSexiIQdp8POny44gStGdWNW5nY+WLWHt5bvYkLvZG45KZ3xGe1D5hN0Uzz+5RbKK7z86fwTLFUQmkMnmkUk4Pp2jOeRiwex5P5TuffMvmzZX8JVLy/joucy+XLjAXxhMIbSlv0lvLlsF1eN7kbvEO6EAlkUugLzgY3ABuCuo2w7EqgELglgHhExWVJsJLefksG3953Cw5MHUFjm5ubXsjjnqYV8snZvyBaHqqoq/jJnI/HRTqaf1sfsOMclkEWhErgH6A+MAW73L9flAB4FvghgFhGxkGing5+P7s78eyby+GWD8Xh93PHWas568lvmrtsXcsXhq025LMrOZ/ppoX8yPZBFYR+wyr9cAmwC6rsu7Q7gfSA3gFlExIIiHHYuHpbGF786mSenDsHrq+L2N1dxzlML+XD1npAYaK+i0sfDn24iPSWWq8Z0NzvOcQvWmZAewFBgWZ31XYDJwCkYh5BEpBVy2G1cOKQL5w3qzJx1e3nyq6386p21APTv1JbxGe0Zn5HM+IxknA7rnArdc7Cc57/JYXt+Ga9cP9JS2Y5VMIpCHEYnMB0orvPaE8BvgMYGSpnmf5CXl9fS+UTEImoXh7V7DpGZnc+i7HxezdzJSwu3kxwXxSXD07h8ZFd6JpszlWVhWQVz1+3lozV7ydp5EIApw9I4pW+qKXlaWqCvBXMCc4B5wOP1vL69VoZkoBzjj/9/G3pD3acg0vocrvCycGse72btYf6WXLy+Kkb3TOKWk3txSt/UoF3WunFvMVNnLKHYVUnfDvFcMKQzFwzuTNekmKDs/3g09T6FQP4kbcCrQCFGl9CYWRgFZPbRNlJREGndcotdzPbf77C78DDjM9rzf+f0p3/ntgHd766Ccqa8kEmE3cZL14xgQJeEgO6vpTW1KATyANh44GpgErDG/zgHuNX/EBFpttS20dw2MYOv7p7IA+f3Z8PeYs59eiH3vreWA8WugOwzr8TN1f9ehsfr4/UbR4VcQWiOkLuVUJ2CiNRWVO7hmflbmZW5A4fdxrVje3DryektdmloicvD1BlL2ZZXxhs3j2ZYt3Yt8r7BZoXDRwGhoiAi9dlVUM4T//ueD9f8QFxkBDdN6MWNE3oSdxzDTZS6K7n51SxW7Chk5rUjmBjCJ5NVFESkVdqyv4THv9zCvA0HaBsdwaR+qZzWvwMn9UmhbbSzye+z4Ps8fvfBevYWHeZflw0J+eG/VRREpFVbs/sQr2XuYP6WXA6We4iw2xjdK4lBaYn0To2jd2o86amxP5kJ7VB5BQ/O2aHHTx4AAAbNSURBVMgHq34gIzWOR6cMYnj30DxkVJtGSRWRVm1I10SGXG7cJb1q10H+t+kAC7bkMXPhNjzeI8NoJMVGkhjjpF1MJO1inKzZXcSh8grumJTBLydlEBXhMPG7CD51CiLSqni8PnYWlJOdW8LWA6XsL3ZxqNzDwfIKCssqaBcTye/P+xkndA6vK4zUKYiI1MPpsJORGkdGahxnDTA7jfWE/kAdIiLSYlQURESkhoqCiIjUUFEQEZEaKgoiIlJDRUFERGqoKIiISA0VBRERqRFydzQDecBOIAEo8q+rvVz7eX3bJAP5x7DfuvtozjYN5avveX3LtdcFKn9Ts9e3rqn5Q/FnX3s52Pmt8LNvKEdTXm8sf7j/3tZetsLvbW//+rA1o4Hl2s/r2+ZYx8iou4/mbNNQvvqe17dce12g8jc1e33rmpo/FH/2tZeDnd8KP/uGcjTl9cbyh/vvbe1lK/7e1ivUR3r6voHl2s/rbjONJv5wGtlfc7dpKF99z+tbrv5vIPM3NXt965qSP1R/9tXLZuS3ws++oRxNeb2x/OH+e1u9bNXfW+H4PjFZQSjnD+XsENr5Qzk7KH/QhHqncKxWmh3gOIVy/lDODqGdP5Szg/KLiIiIiIiIiIiIiIhYRms90dwQO/AQMBlIBdaaG6dZJgKvA6OBUmCHuXGOSSywBNhLaF069zOMfzfXAe0JoStN/C4C7gGuAoqBHHPjNFsv4B/ANcC7JmdpilhgJnAu0BZYb26c8PVvIBf4rs76s4AtQDbw20beYzLwKvA4cGpLBzyKlsh+MvAZMAvIaOmAjWiJ/AAPAvcB57VouqNrqexgfKj4T8tFa5KWzN8OeLnlojVJS+af3YK5mqs538fVwPn+5XeCkq6VOgkYxo//pzgwPvX0AiIxPvn3BwYCc+o8UjH+p93i/9pg/gNriezV41h1AN4ISuojWiL/6cBUjE/bwSwKLZEd4AKMonxlUFIf0VL5Af7pf69gasn8ZhaF5nwf9wND/Nu8GcSMrVIPfvw/ZSwwr9bz+/2PhlwFXOZfDnYFP97s1SIx55fjePM/DDwBfAF8RHAHa2ypnz3A3JYK1QzHm98GPAqc1vLRmqSlfv5mFgVo+vdxNUc++LwdnGhNF2F2gADrAuyu9XwPxjH3hnwAPA1MAL4NYK6maG72i4EzgUTgmQDmaqrm5v8//3+vwxg4zBegXE3R3OwTMX7+UcCnAczVVM3NfwdGQUjAOPT4QuCiNUlz87fH+FAxFOOP7iOBi9YsDX0fT2H8jp4LfGJCrqMK96LQXOXAjWaHOEYf+B+hbpbZAY7BN/5HqHrK/whVBcCtZodohjLgerNDNCTc51P4Aeha63maf10oCOXsENr5Qzk7KL9VhMv3EdLqHtOLALYBPTlyoucEE3I1RShnh9DOH8rZQfmtIly+j7DxFrAP8GAcu6s+DHQOxjXvORw5bm01oZwdQjt/KGcH5beKcPk+REREREREREREREREREREREREREREROSI0iDvbybGqJfBNB2ICfI+RURCUksXBTPGBrNx9OFndgDJQcoiIhLS6isKKcD7wAr/Y7x//SiMWd5WA5lAX//664CPga+BBRgjoH6DMSzzZoy5Kmz+bb8BRtTa98MYQxksxZjXAiDd/3w9xuxs9WXsgTERy2vABqA78DzGDG4bgD/7t7sTqPC/13z/ujP838cq4D0grr4fjIhIa1TfH9w3gRP9y92ATf7lthzpBE7DKBxgFIU9QJL/+USgCGMwMzvGH+Dq96tdFKo4MpvW34Hf+5fnAFf4l29tIGMPjKHCx9RaV71/h38/g/zPa3cKyRhDvMf6n/8G+GM97y/SZBo6W8Ldafz4uH9bjE/TCRhTr/bG+IPurLXNl0BhrefLMQoFwBqMP+KL6uynAqMAAKzEmEkOjIlWLvIvv4kxl3B9dmJ0FNUuA6Zh/I528n8P6+p8zRj/+sX+55EYRUvkmKkoSLizY/zxdNVZ/wzGIZjJGH/ka8+HUFZnW3etZS/1/954MIrL0bY5mtr77An8GhgJHMSYYyK6nq+xYRSwK+p5TeSYhPt8CiJfYMwsVq16btwEjoxtf10A978UmOJfntrEr2mLUSSKMM5NnF3rtRIgvtZ7j8eYLQ2Mw0h9jiesiIqChJMYjMM81Y+7MU7OjsA49LKRIzN0/R1j2sbVBLZjnu7PsQ7jj3dRE75mrT/XZoxDTotrvTYD+Byjy8nDKGhv+d9/CdCvpYKLiEjLi+HI1UpTgY9MzCIiIiabgPHJfx3GlUIZR99cREREREREREREREREREREREREREREJKT9P/2QZ+frvPo2AAAAAElFTkSuQmCC"}}},{"cell_type":"markdown","source":"### Useful for other Schedulers\n\nI think the `LRFinder` is also useful for other types of schedulers. Like the plot above, the `lr_finder.plot()` says the suggested LR to be the below.\n\n```markdown\nLR suggestion: steepest gradient\nSuggested LR: 4.48E-04\n```\n\nNotice that they are suggesting the initial LR to be somewhere in between the descent, so do not be fooled and start the LR from the point where it gives the lowest loss >.<|| Doing so will cause your loss to decrease. Intuitively, the LR finder plot just wants you to find a good initial LR, in this case it is indicated by a red dot below, and then as the training progresses (i.e. iter increases can be understood as training progresses), the loss decrease steadily until a point where it is going to diverge, we can choose the end_lr right before that divergence point.\n\n\n![lr_finder.png](attachment:ca443c95-9560-4622-97cc-aafeed6c6332.png) ![Inkedlr_finder_LI.jpg](attachment:f3c5f5ca-e185-46d3-ae56-dcdb9225177d.jpg)\n","metadata":{},"attachments":{"ca443c95-9560-4622-97cc-aafeed6c6332.png":{"image/png":"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"},"f3c5f5ca-e185-46d3-ae56-dcdb9225177d.jpg":{"image/jpeg":"/9j/4AAQSkZJRgABAQEAAAAAAAD/4RCyRXhpZgAATU0AKgAAAAgAAodpAAQAAAABAAAIMuocAAcAAAgMAAAAJgAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFkAMAAgAAABQAABCAkAQAAgAAABQAABCUkpEAAgAAAAMzMgAAkpIAAgAAAAMzMgAA6hwABwAACAwAAAh0AAAAABzqAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMjAyMToxMToyNiAxMzowNjo1MwAyMDIxOjExOjI2IDEzOjA2OjUzAAAA/+EKfWh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8APD94cGFja2V0IGJlZ2luPSfvu78nIGlkPSdXNU0wTXBDZWhpSHpyZVN6TlRjemtjOWQnPz4NCjx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iPjxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczpNaWNyb3NvZnRQaG90bz0iaHR0cDovL25zLm1pY3Jvc29mdC5jb20vcGhvdG8vMS4wLyI+PE1pY3Jvc29mdFBob3RvOkl0ZW1TdWJUeXBlPkx1bWlhLkxpdmluZ0ltYWdlPC9NaWNyb3NvZnRQaG90bzpJdGVtU3ViVHlwZT48L3JkZjpEZXNjcmlwdGlvbj48cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0idXVpZDpmYWY1YmRkNS1iYTNkLTExZGEtYWQzMS1kMzNkNzUxODJmMWIiIHhtbG5zOnhtcD0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wLyI+PHhtcDpDcmVhdGVEYXRlPjIwMjEtMTEtMjZUMTM6MDY6NTMuMzE2PC94bXA6Q3JlYXRlRGF0ZT48L3JkZjpEZXNjcmlwdGlvbj48L3JkZjpSREY+PC94OnhtcG1ldGE+DQogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgIDw/eHBhY2tldCBlbmQ9J3cnPz7/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/2wBDAQICAgICAgUDAwUKBwYHCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgr/wAARCAEKAZIDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD+f+iiigAooooAKKKKACiv2w/4N7/+De/9hT/gqL+wpqH7SH7SHiH4hWfiCz+IWoaJHH4W8QW1rbG2htrOVCUltZW37p3ydwBAXgYJP3P/AMQaP/BJb/odvjN/4WFj/wDK+gD+Wiiv6l/+INH/AIJLf9Dt8Zv/AAsLH/5X0f8AEGj/AMElv+h2+M3/AIWFj/8AK+gD+Wiiv6l/+INH/gkt/wBDt8Zv/Cwsf/lfR/xBo/8ABJb/AKHb4zf+FhY//K+gD+Wiiv6l/wDiDR/4JLf9Dt8Zv/Cwsf8A5X0f8QaP/BJb/odvjN/4WFj/APK+gD+Wiiv6l/8AiDR/4JLf9Dt8Zv8AwsLH/wCV9H/EGj/wSW/6Hb4zf+FhY/8AyvoA/loor+pf/iDR/wCCS3/Q7fGb/wALCx/+V9H/ABBo/wDBJb/odvjN/wCFhY//ACvoA/loor+pf/iDR/4JLf8AQ7fGb/wsLH/5X0f8QaP/AASW/wCh2+M3/hYWP/yvoA/loor+pf8A4g0f+CS3/Q7fGb/wsLH/AOV9H/EGj/wSW/6Hb4zf+FhY/wDyvoA/loor+pf/AIg0f+CS3/Q7fGb/AMLCx/8AlfR/xBo/8Elv+h2+M3/hYWP/AMr6AP5aKK/qX/4g0f8Agkt/0O3xm/8ACwsf/lfR/wAQaP8AwSW/6Hb4zf8AhYWP/wAr6AP5aKK/qX/4g0f+CS3/AEO3xm/8LCx/+V9H/EGj/wAElv8AodvjN/4WFj/8r6AP5aKK/qX/AOINH/gkt/0O3xm/8LCx/wDlfR/xBo/8Elv+h2+M3/hYWP8A8r6AP5aKK/qX/wCINH/gkt/0O3xm/wDCwsf/AJX0f8QaP/BJb/odvjN/4WFj/wDK+gD+Wiiv6l/+INH/AIJLf9Dt8Zv/AAsLH/5X0f8AEGj/AMElv+h2+M3/AIWFj/8AK+gD+Wiiv6l/+INH/gkt/wBDt8Zv/Cwsf/lfR/xBo/8ABJb/AKHb4zf+FhY//K+gD+Wiiv6l/wDiDR/4JLf9Dt8Zv/Cwsf8A5X0f8QaP/BJb/odvjN/4WFj/APK+gD+Wiiv6l/8AiDR/4JLf9Dt8Zv8AwsLH/wCV9H/EGj/wSW/6Hb4zf+FhY/8AyvoA/loor+pf/iDR/wCCS3/Q7fGb/wALCx/+V9H/ABBo/wDBJb/odvjN/wCFhY//ACvoA/loor+pf/iDR/4JLf8AQ7fGb/wsLH/5X0f8QaP/AASW/wCh2+M3/hYWP/yvoA/loor+pf8A4g0f+CS3/Q7fGb/wsLH/AOV9H/EGj/wSW/6Hb4zf+FhY/wDyvoA/loor+pf/AIg0f+CS3/Q7fGb/AMLCx/8AlfR/xBo/8Elv+h2+M3/hYWP/AMr6AP5aKK9T/bm+DPhL9nL9tn4xfs9eAJ72TQfAfxT8QeHdFk1KZZLh7Sy1K4toTK6qqtIUiXcwVQTkgDpXllABRRRQAUUUUAFFFFABRRRQB/Ut/wAGZ6In/BJjXGRFBb40awWIHU/YNMGT+AH5V+s1fk1/wZo/8olta/7LNrH/AKQ6bX6y0AeLeJvjv+1tpniPUNF8M/sJX+qWtveyxafq8nxG0m3t7uEORHMVLGWPcoDFShK5xyRVD/hZX/BRDT/+Jtefss/DvULeYZj0nT/idPHd2/8AvyS2HlOe3y457gV7xRXoRxuHirfV4ffU1/8AJ/ysebLA4iUr/Wan3U9P/KevzueEr+1T+0Za/wDEt1f/AIJ8ePl1LrssfEWiz2hX1Fx9rUZ9ioOKP+Gi/wBsOX57b/gnlrQVv9X9o+JGiIw92Albb+BY+1e7UU/ruF/6BoffU/8AlgfUcX/0FT+6n/8AK/8AI8J/4aE/bOX/AFn/AAT3vjjg+X8T9IOfcZYcfXB9jS/8NB/tnJ88n/BPi+Zf7sPxP0gt+TFR+te60UfXsN/0DQ++r/8ALA+o4r/oKqfdS/8AlZ4T/wANG/tgTH7Pbf8ABPLXFmXlmuviNoiQ47YdJnJPtt49aB8c/wBumUb4f2DNLRT91bj4vWiuPqEtGAP0JHvXu1FH17C/9A0Pvqf/ACwPqOK/6Cqn3Uv/AJWeE/8AC7/27k+eT9hDRGUfeWH4xW5c/QNZKufqwoHx9/bYm4tv2APLx977Z8VdNXP08tHz+OK92oo+vYb/AKBoffV/+WB9RxX/AEFVPupf/Kzwn/hfH7cP3h+wNa7V+8v/AAtqx3H/AHR5GD+JFL/w0X+2HH8s3/BO/XGbubf4kaGy/hunU/oK91oo+vYb/oGh99X/AOWB9RxX/QVU+6l/8rPCv+GjP2wZflt/+Cd+vKf4jdfEjQ1H4bJ3yfwFJ/w0R+2SvL/8E8tWO77vl/EzRSR/vZkGPwzXu1FH17C/9A0Pvq//ACwPqOK/6Cqn3Uv/AJWeE/8ADRn7YXX/AId3a9t/7KRoW7/0fjH4/hR/w0/+07ZfLrX/AATz8bK3/UP8XaHcr+Yu1/lXu1FH13C/9A0Pvqf/ACwPqOK/6Cp/dS/+VnhP/DYnxRsfm139gj4vxr3NhDpF0fyW/BoH7fXgnTj/AMVr+z98ZfDo7yal8ML6WMf8DtVmU/ga92oo+tYCXxYe3+GUl/6VzB9VzCPw4i/+KEX/AOk8p4z4a/4KFfsa+JdUXQn+OumaLftwtj4st59GlJ9At9HFnn0zntXr+m6npus2EWqaPqEF1azLuhuLaYSRyL6qykgj6VV8T+EPCXjbS20Pxn4X07V7KTiSz1SxjuIm+qOCD+VeMa5+wL8OPDN7L4t/ZZ8T6p8I/ETMX87wm2dLum7Lc6bITbTJ7KsbejCmo5VW0i5U3/etNfNpRaXpGT8hc2bUNZKFRf3bwfyUnJN+sorzPeKK8Q+Fn7S3jrw18RLb9nz9rbw7pvh/xZfIzeGfEWkyP/YvilEGW+ztJ80Fyo5e2kJbHzIzqePzq/4Kr/8ABcrxP4o1rVP2dv2KfFM2maLaySWevePLFgtxqLDKvHZP1ihBz+/XDuRlCq4Z4eW4qNZU2t1dNaprumt/zvdOzTPs+C+G8y47x31fLl8P8SUrpU/8XW76Jay3Xu6n6seNfj18DPhtqkeifEX4z+E9AvZuYbPWvEVrayvwTwkrqTwCenauos7y01C0iv7C6jnt541khmhkDJIhGQykcEEcgjgiv5QdR1HUNXvptU1a+murm4kLz3FxKXkkY9WZjkk+5r93P+DfJPjSn7BY/wCFqi+/stvFNyfBH9pCTf8A2Z5MH+r3/wDLDz/O2Y4zvxxirxWXrDUefmufovHXhTR4NyGOPjjPaS5lFxcVG97/AA+89rap9Lu+lj7morzX46ftifsufs0FIvjt8dvDfhu4kXfFp99qCm7kX+8tum6Vl5HIUivL9L/4LI/8Ez9YuFtrT9q3R0ZpAgN1pV/Auc45aS3UAe5OK4Y0a0o3UW/kz8ywvD+fY2h7bD4SrOH80ac5L70mj6aornfhh8W/hd8a/Csfjn4QfEPRfE2jyyGNdS0PUo7qHeMZQtGSFcZGVOCM8gV0VZtNaM8upTqUajp1IuMlo01Zp9mnsFFFFBmFFFFABRRRQAUUUUAFFFFABRRRQB/EH/wVi/5Sm/tLf9nAeMv/AE+XlfP9fQH/AAVi/wCUpv7S3/ZwHjL/ANPl5Xz/AEAFFFFABRRRQAUUUUAFFFFAH9S//Bmj/wAolta/7LNrH/pDptfrLX5M/wDBme4b/gkxrigN8vxo1gHKkf8ALhph49evav1moAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKK+U/+CkX/BVn4M/sCeHn8NR+T4k+Il9ZmTR/CsE3y24I+S4vGH+qizyF+/JjC4GXW6dOdWXLFXZ6WU5TmWeY6GDwNNzqS2S/Ft7JLq3oup4//wAHBn7Xvw++GH7OUH7NNnHY6h408XXUF3bxsoaXRLSGTcb1SOYpXZfKjPBKtMR93n8WvAPgTxb8UfG+k/DnwHok2pa1rmoRWWl2NuuXnnkYKqj05PU8AcnAFafxq+NHxJ/aF+J+r/GH4t+JZtW17W7oz313NwB2VEUcJGqgKqDAVQAK+9P+CRf7Ivjf9nbxD8O/+ChXxm8K26eF9e8TLoGj299bnz7KK+heCDWOeEjM5jhVj1SYuOCrH6jD0ZYfDqlF3erSvu7XsvW1l3Z/V1Knk3grwFUxWIalWdnJpfxKr0jHo+SHV9IqUrJto+3P2D/+CLX7MH7LngK1uvjB4E0L4heOLjZPqWr69piXVraSDkRWkMwKoqn/AJaFfMY8kqNqr0n/AAV4/a+8ZfsM/saSeM/g7FDY+INW1i20Dw/d/wBnrJDprPFLIZQhHl5WKBwisCu7blWAIr6qrz79p79mP4Sfte/B7UPgh8atHuLvRdQeOXdZ3TQz206HMc0Tjo6npkFSMhlYEg/ORre0rqdbVX1P5sw/EtTMuJqOYZ/KVempqU4vVct9Uov3bf3VZNKx/MP4o8VeJvG/iG88W+MvEF7q2qahO019qWo3TzT3Eh6u7uSzH3JqhX77fsif8EMP2Qv2VviH/wALQ1OfU/HmqWwU6OviyKFrbT5OcyrCiBZJORhn3BMZUBsMPrHxr8Jfhh8RvC994K8deANJ1TStSs3tb2xvLBGSWFl2lOnHHpgjqMEV6082pRlaEbr7j99zLx4yHB4qNHAYWVWkkryv7O3lGLi72Xdx1021P5tP2M/20vjR+xB8X7P4q/CXWXMSuE1rQbiZvserW2fmhlUd8fdcDcjYI7g/0T/sp/tPfDP9sD4G6L8d/hVes2narFi4s5yPPsLpeJbaUDo6NxxwwKsMqwJ/Br/gqf8A8E9Ne/YE+PbaNo0V3deBfERkuvBurXHzERg/PZyN3lh3KCeNysjdyB3n/BDX9t/W/wBmT9q3Tvg74g1Nm8F/Eq+h0zULeWQ7LPUGylpdICQqkyFYXPGUfJyY1FVjKFPFUfbU97fejo8ROF8r484ZjxBlNnVjDmTW84L4oy/vR1t1TTj10/e6iiivnz+TQooooAKKKKACiiigAooooAKKKKAP4g/+CsX/AClN/aW/7OA8Zf8Ap8vK+f6+gP8AgrF/ylN/aW/7OA8Zf+ny8r5/oAKKKKACiiigAooooAKKKKAP6l/+DNH/AJRLa1/2WbWP/SHTa/WWvya/4M0f+US2tf8AZZtY/wDSHTa/WWgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigD57/4KjftT+L/ANjr9i/xR8Zvh7ZLLr6SW2n6PNNbmSK2muJlj89wOyIXZc8F9inriv5zfGPjLxZ8Q/FN/wCOPHXiO81fWNUumuNR1LULhpZrmVjku7sSSa/qO+NXwY+G/wC0N8LdZ+DPxd8Nx6t4d162EOpWMkjJvCusiMGUhldXRHVgQVZQR0r5e/Zo/wCCF/7D37NXxPT4r2lj4g8X39nKsujWvjK8guLbTplfcsyRxQxB3HGDJvC4yAGwR6mBxdDDUnzL3j9u8M+PuGODckxCxNGTxMpXTik+eNlyx5m/dSd276a3V3ovjj/glN/wQ61/4i3mkftH/tleH2sfDI2Xeh+BbyMrcaryGSW7X/llAevlEbpAfmCrw/6tfHv4LaD8b/gR4i+CF0kdpa6zoslnZyRJtWylC5t5UA6GKRY3XHQoK7amySxwxtNNIqqq5ZmOAB6muaeNxEsRGsnZxaa8mtUfm/GXFWaccYyVXMHeDTjGC+GMXukurfVvV+iSXm37H/xd1b43fs6+G/HPiiHydeW2k0/xNbtjdFqdpI1tdAjtmaJ2A/usK9Lr81vhf/wWe/Yf/Zz/AGlfil8M5/Euuap4P8RePP7Y0XxRoukiextbie3iXUN+XWVovtMburRRyB/MZhkEE/oz4V8VeHPHPhnT/Gfg7W7bUtJ1azju9N1CzlEkVzBIoZJEYcFSpBB964amZZTmGOqrA1YzSd7Rd+Xm1t8r8t9ro58y4H434SyvC1c/wFXD+1XuyqQcefl0bV1u1aXK7NJptamhRRRVngny3/wWR/Z4sf2hv+Cf/ji1TTI5tW8J2J8SaLN5IZ4pLMGSYLxnL2/npgcksOvQ/wA8el6pqGianb61pF5Jb3VncJPa3ETYaKRWDKwPYggGv6svFGgWXivw1qPhbU41e21KxmtbhWGQ0ciFGH5E1/KbqumXmi6pc6NqEXl3FpcPDOn911YqR+Yr3cpnzU5QfT9f+GP6i8AsyniMpxmAm7qnKMkn2mmmvS8L+r8z+oz9nL4s2fx3+APgv4z2K7U8UeF7HU2j3AmKSaBHeMkADKsWU4AGRXaV8p/8ETfGkvjb/gmf8Nri6uGkuNNh1DTptzZ2iC/uEjX6CLy+O3SvqyvGrR9nVlHs2fzpxBgY5XnuKwcVZU6k4r0jJpfggooorM8gKKKKACiiigAooooAKKKKAP4g/wDgrF/ylN/aW/7OA8Zf+ny8r5/r6A/4Kxf8pTf2lv8As4Dxl/6fLyvn+gAooooAKKKKACiiigAooooA/qX/AODNH/lEtrX/AGWbWP8A0h02v1lr8mf+DM8yH/gkxrm9VA/4XRrG3DdR9g0zrxxzn1/pX6zUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABXIftB+A9e+KnwE8cfDDwrq/8AZ+qeJPCGpaXpuobiv2a4uLWSKOXI5G1nDcc8V19FRUpxqU3CWzVvvOjB4qtgcXTxNL4oSUldXV4tNXXXVbH8snjX9n743fD74rN8EPGHws16z8XfbfssWgSabI11cSliqiJFBMwYj5WTcHGCpIINf0T/APBNH4I/EP8AZz/YX+Hfwd+K7v8A8JBpWkyvqUMkm9rVp7ma4W2JBIJiSVYjgkfu+OMVc/b98J6xrn7M2seNPCcO7XvAV1beL/D57/adNkFyVHrviSaLHcSYr1bwZ4r0jx54O0nxx4fl8yw1rTYL6xk/vQzRrIh/75YV85w/wTh+HebHQrObm5Qs1blS5Za6u7fR6bPQ/efGL6SGbeMGWYbI6+Ahho0HGtKUZubqT5ZwvG8Y8kYpyvG82+aLctDSooor6Y/n4K/mL/bj8DP8Nf2yfil4HaMqun+PtVSAFcfujdSNGfoUZSPY1/TpX84H/BWgg/8ABR34uY/6Glv/AETHXrZS/wB9JeR++eANacc+xdJbOkn81JJf+lM/Tn/g2+8ct4h/Yg17wdOT5nh/x/dxx+nkzW1tKv8A4+Zf8mv0Gr8zP+DZXP8Awz/8Sz/1ONt/6SCv0zrjxy5cVP1Pz3xMowo8eY+Mf57/ADlFN/i2FFFFcp8KFFFFABRRRQAUUUUAFFFFAH8Qf/BWL/lKb+0t/wBnAeMv/T5eV8/19Af8FYv+Upv7S3/ZwHjL/wBPl5Xz/QAUUUUAFFFFABRRRQAUUUUAf1L/APBmj/yiW1r/ALLNrH/pDptfrLX5Nf8ABmj/AMolta/7LNrH/pDptfrLQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAEOpadZavp1xpOp2yzW11C0NxC4+V0YEMp9iCRXif/BO/Ur20/Zug+E+tXLS6l8Ntf1LwhqDN1xY3DJbn6G1Nuw9m44r3KvBvhN/xbn9vL4ofDk/Ja+NvDek+MdNhXhRNHu069OP7xMVox/38nrXpYX97ga9LtyzXrF8rX3TbfoeZi/3OPoVu7lB+klzJ/wDgUEl/iPeaKKK809MK/nT/AOCyXhLUvB3/AAUp+KVnqKP/AKZq1vf28jR7Q8c9nBKCOTkDcVz6qcgHIH9Flfjt/wAHLX7Pc2ifFXwL+05pNo/2XXtJk0HWHVBsjurZmlgZjnJaSKWRemALX3r0srqKOJs+q/4J+w+COZ08Dxn7Cb/jU5QX+JNTX4Ra9Wjt/wDg2O+ImlS+Evip8JpBsvrfUtP1eHJ/1sUkckL4/wBxo0z/ANdRX6pV/Ot/wSE/a2039kD9tjw/4v8AF2rfY/C/iGGTQvE88jERw285Uxzt6LHOkLsccIHx1r+ihHWRQ6MGVhlWXvU5lTcMTzdzLxoyatlvGU8Vb3MRGMk+l0lGS9U0m/KSFooorzz8jCiiigAooooAKKKKACiiigD+IP8A4Kxf8pTf2lv+zgPGX/p8vK+f6+gP+CsX/KU39pb/ALOA8Zf+ny8r5/oAKKKKACiiigAooooAKKKKAP6l/wDgzR/5RLa1/wBlm1j/ANIdNr9Za/Jn/gzPEg/4JMa5vZSP+F0axtwvQfYNM688859P61+s1ABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFfP/wC1zfWvwe+NPwn/AGor25W10rS9aufC/i68dtscOm6nGBHNK54WOO8gtiWPA8w/UfQFZfjXwd4d+Ifg/VPAni/TI7zS9YsJbPULWVQVlhkUqw59j+FdWBxEcNiFOavGzTXlJOLt52bt5nHjsPLFYVwg7STTi+nNFqSv5XSv5XNNXV1DowZWGVYd6WvE/wBgbxVrl78BV+FXjO8abxF8M9YuvCGtSP8AelNkwW3m9SJLVreTcepc9a9sqcVh5YXEzot35Xa/ddGvJrVGmDxEcXhYVkrcyTt2fVPzT0fmgrx/9u79k/w9+2n+zB4k+A2tNDDeX1uLjw/qE0e77DqMWWgm9QM5RscmORx3r2CisYylCSkt0elgcZicuxlPFYeXLOnJSi+zTuj+U3x54F8XfDHxpqnw78e6DcaXrWi38tlqmn3S4kgmjYqyn8R1HBHIyDX7Yf8ABCj/AIKK2n7Rfwci/Zj+K3iSM+OvBdmselSXdwTNrWlKMJIC335YRiNxkkoI35JcjhP+C8f/AATE1b4n2k37a3wE8NLPrWmWOPH2j2cJ86/tYlwt+gH35IkG1xjJjVSP9WQfyO+HXxE8cfCTxzpfxK+G3ia60bXtFvFutL1Oyk2yQSr0I7EEZBUgqykgggkH6JqnmWF7P8mf19Wp5P4xcExcJKFaOvd06qWqfXkl+MWn8S0/quor8r/hb/wcw+BrP4V6dB8Y/wBn7XLzxnBCsepS6DdQR6fdMODMpkbfEWHPl7WAPAbFfZX/AAT9/wCCkXwT/wCChfhLVtU+HGnaho+t+H3iGveHdVUGS3SVpBDKki/LKjCNuRgqRhgMqW8OphMRRi5Sjoj+Z864B4t4fw08TjcK40oOzmmnHeyejbs3azaSu0t3Y+hqKKK5z48KKKKACiiigAooooA/iD/4Kxf8pTf2lv8As4Dxl/6fLyvn+voD/grF/wApTf2lv+zgPGX/AKfLyvn+gAooooAKKKKACiiigAooooA/qX/4M0f+US2tf9lm1j/0h02v1lr8mv8AgzR/5RLa1/2WbWP/AEh02v1loAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA8C0yWH4Qf8FDtS0mSRbfTfjB4IjvrfzGwJdZ0lxDKqj+81nPCx7kW/tXvteS/tqfBzVPi98CdSfwVbKvjLwyy694FvkX99b6panzYlQ9R5u0wsOhWVga7D4H/FfQPjp8IPDfxf8Mn/AEPxDpEN5HHuyYWZfnib/aR9yN6FTXpYv/aMJTxK3VoS9Yr3X846Lzizy8H/ALNjKmGezvOPpJ++vVSd35TR1VFFFeaeoFfil/wXz/YG+AX7MviPQfj58HboaHP461aeG+8F29oPsqyRx+ZNdwMCPJXc0YaHaV3SZUqBtr9ra/Pr/g4m/Zu8U/F39k/RfjF4QsJLyb4c6xLc6pbQws7jTrlFjmmAVScRvHAzZwqx+YxOFrswFR08VHWyejP0bwpzitlfGuGj7Zwp1XyTXSV0+VNPT47We6vo0fiZ4W8K+JvHHiOz8IeDfD95quq6lcLBYadp9u001xIxwERFBLE+gFftt/wQ7/4Jo/GT9jDT/E/xh/aCht9N8QeLLG3stP8ADlveCaTT7VHaR2uGQmMyO3l4VS2xUOWy5Vfxd+EHxd+IfwG+Jej/ABf+FPiOTSfEOg3X2jTNQiRWMbbSpBVgVZWVmVlIIZWIIwa/Zj/gnT/wXi+Fv7Q8lp8Kf2qxpvgnxlI8cFhrCyFNK1dyAPvOT9klJ/hdijcbXBISvWzL6xKjaC069z988YYcXYjIZUMtpKeGkv3rV3U0d9I/yaJtq8t72im3+hlFIrK6h0YMrDII70tfOn8ehRRRQAUUUUAFFFFAH8Qf/BWL/lKb+0t/2cB4y/8AT5eV8/19Af8ABWL/AJSm/tLf9nAeMv8A0+XlfP8AQAUUUUAFFFFABRRRQAUUUUAf1L/8GaP/ACiW1r/ss2sf+kOm1+stfkz/AMGZ6Bf+CTGuMC3zfGjWCcsT/wAuGmDj06dq/WagAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAr52+HGjyfstfter8EdF1i8HgP4m6TqWt+GtJunV4tK12G48++t7Y7QyQzRT+eIySFaKTbgEgfRNeQ/tjfDDx1418F+H/H/wk0iO/wDGHw/8V2niHQtPa5WA6gke6O6svMPCedbSzIM8FtmcdR6OW1I+0dCo0o1E077J2fLLXa0rXfa62bPNzKnL2UcRTTc6bTVt3G654pLe8b2X81nukevUVyvwS+L/AIT+Pfwr0X4u+CPtC6brVr5scN3GEmt5AxSSCVQSFkjkV42GThlPJ611VcNSnUo1HCas07Ndmt0d1OpTrU41IO8Wk0+6eqYVHd2lrqFpLYX9rHNBNG0c0M0YZJEIwVYHggjgg9akoqDTbVH5i/8ABQv/AIN+PCPj+S++LH7Ectr4e1Z9r3PgO6by9OnwMM1rJybdj18tsoTnBjGBX5E+Pvh/44+Fni+/8AfEfwpf6HrWl3Bh1DS9StmhmgcdirDv1B6EEEZBBr+rCvzL/wCDmH4a+EZ/gF4A+L/9j2y67aeMjpH25YQJpLWa0nmMbMBllV7dSAT8pZsfeNexgcdUdRUp6369T+hvC3xOzqtmlDJMxftYTfLCbfvxaTaTf2lpbX3le92tDh/+Dfn/AIKJeOdY8Z/8MO/GHxTNqVjNpstx8P7q9cNNbPCGkmsd5O54zEGkRTnYImUfKVC/rZX813/BMHxHqXhb/goT8H9T0qeSOWXx3Y2bNE5UmO4k+zyDjsUkYEdwSDwa/pRrHM6UaeIvHqvxPm/GzI8HlPFUa+GioqvDnkloudNpu3mkm+7u+oUUUV5p+OhRRXjv7XHxh8Y+GNN0f4G/BOZD8RfiFNJY+H5WXcukWqgG61WUf887eM5UH78jRqAckVvhsPUxVZUoder2SWrb8krt+SOfFYinhKDqz6dFu29El3bdkl3Z6Z4U8d+DfHS6g/g3xNZamNJ1SbTdTNlcCT7LeRY8yB8fddcjKnkZrWrlPgj8HfB/wC+F2kfCfwNBIthpNvs8+dt013MxLS3ErfxSySFnZu7Me3FdXUVvYqtJUm3G+je7Xd9r9uhdB1nRi6qSlbVLZPsu9u/U/iD/AOCsX/KU39pb/s4Dxl/6fLyvn+voD/grF/ylN/aW/wCzgPGX/p8vK+f6zNQooooAKKKKACiiigAooooA/qX/AODNH/lEtrX/AGWbWP8A0h02v1lr8mv+DNH/AJRLa1/2WbWP/SHTa/WWgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA+d/DFhL+yt+1/b/DzTNTmXwH8YP7S1HTdPnw0eleJoys9xHE2AUiuoTJKIySBLE+3AYivoivL/wBr/wCDWsfGr4I32leC5xb+LNDuIdd8E33GbbV7RvNtzk9A5Bib/Ylatn9nX44eHP2h/hBo/wAUvD37l7y3Catpr5E2mXyfLcWcqn5kkik3KQQDwD0IJ9TFXxWFhivtL3Z+v2ZP/FHTzcW3qzycHbB4yeE+y/fh6bSiv8MteyU0lojt6KKK8s9YK/M//g5o8V6Zafs7/DfwPLdILzUPGk99DBuG5ore0eN2x1wDcxjPbcPWv0wr8T/+Dlbxfr2pftf+DfBVzDImmaT8Po7mz8yAKHnuLy5EzK38S7YYV9ircc5Pdl8ebFx8tT9M8IcD9e48wzvpTUpv5RaX4tfK58y/8EsdDn8Q/wDBRD4Q2FvHuaPxpa3JGP4YczMfwEZNf0lV+HP/AAblfB9fG/7bOqfFK9tpGt/BPhG4mt5lU7VvLpltkUnpzC11weu3jocfuNWuaz5sQl2R73jrj6eJ4tp4eP8Ay6pRT9ZNy/JxCiiivMPxUy/HHjXwx8N/BuqeP/GurR2Gk6LYS3mpXk33YoY1LM3vwOAOSeBzXjv7IHgjxR4z1TWP2yPi5pElp4m8e28cfh/Sbpfn8P8Ah1DutLL/AGZJM/aJsYzJIAQNlZvxw/4yr/aI0/8AZVsf33g3wY1r4g+Kki8x3k2fM07R29Q7KLmVSMeXHGMjfX0QOOAK9Sf+w4Lk+3VSb8obxXrL4n/dUe7R5MP9vx3tP+XdJtLzntJ+kdYr+85dkwoooryz1j+IP/grF/ylN/aW/wCzgPGX/p8vK+f6+gP+CsX/AClN/aW/7OA8Zf8Ap8vK+f6ACiiigAooooAKKKKACiiigD+pb/gzPkR/+CTGuKjqSvxo1gMAeh+waYcH8CPzr9Zq/Jr/AIM0f+US2tf9lm1j/wBIdNr9ZaACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAr5v/ao+G0H7OOpXH7cHwStLrTb/AEu+gu/idoulsfsviTRt4W6llg+4bqGMmZJwFfEbhiwbA+kKr6vpOm69pV1oes2UdzZ3tu8F1bzLuSWN1KsjDuCCQfrXXgsVLCV1LeL0kuko9U/Xp2dmtUjjx2EjjMO4bSWsZdYy6NenXurp6NhpeqafremW+taReR3FpeW6T2txC25JY2UMrqe4IIIqxXhP7BOrX/hv4f69+zN4kvpJtW+EviSbQI2uGzLNpJAn0yc/7LWkkaD3hb0r3apxmH+q4qVK90no+6eqfzTT+ZWCxP1vCwq2s2tV2a0kvk018grwn9tr/gnP+zX+3zo2m2Xxu0nUrfUtHYjS/EXh+8S3vreMnLwhpEkjaNj1V0bB5XaSTXu1FYRnKnLmi7M9bL8wx2V4uOKwdR06kdpRdmun4rRrZrRnjv7Gn7Cv7Pn7CXgS78DfAjQbxG1OZJda1nVrrz73UnQMIzK4VVwgdgqoiKNzHGWYn2KiiiUpTlzSd2TjcdjMyxU8TiqjnUk7uUndv1fpouy0QVw37R/xu0v9nv4Pat8Tb6we+urdEt9F0iH/AFup6hMwitrSMDktJKyLwDgZboDXc1876dIn7WH7Xp1qM/aPAPwVvHhs26w6p4sdCsjjs62ULFB6TTtjOyuzAUKdSo6lX+HBc0vPtH1k7Lyu3smeNmGIqUqSp0v4k3yx8u8vSKvLzslu0d3+yZ8EtV+CHwmjsvGl+l/4w8Q302ueONVX/l71W5IabB/55xjbCgGAEiXgV6bRRXPiK9TE1pVZ7yd/+G7JdF0R04ehTwtCNKntFWX/AAe7e7fV6hRRRWJsfxB/8FYv+Upv7S3/AGcB4y/9Pl5Xz/X0B/wVi/5Sm/tLf9nAeMv/AE+XlfP9ABRRRQAUUUUAFFFFABRRRQB/Uv8A8GaP/KJbWv8Ass2sf+kOm1+stfk1/wAGaP8AyiW1r/ss2sf+kOm1+stABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAfPv7TeneIP2efina/tveCdJnvtJtdJGlfFjRrOPdLcaOjNJFqUSj781ozOzDktC8gBG0V71o+r6Z4g0i117Rb2O5s763juLS4iOVlidQyuPYggj60mtaPpviHR7vQNas0uLO+tpLe7t5B8ssTqVZT7EEivhj4kft065/wSi+B198C/j1o1vr2q+HPDsKfCHUbKSQR+JbUSNBHDcbgPIltVEXmhScxlSvJGfWjGWYYNJfxKdl5yh+rg9O/K10jpy5dl+MrZ5TwWFg5fWJWil0qfoprVt2SabbvLX3v9uD/AIKH/s7fsE+ELfXPi/rU11rGoKf7G8K6Pskv70c/vNjMojhBBBlYhc8Dc3y18VfD7/g5s8Daz8RLXSfiR+y3faH4ZuLry59Z0/xQL65tIzwJDbm2jEmDywVwQOgcjDflh8dvjr8UP2k/ipq3xl+MPiebVtd1m4MtxcSE7Yl/ghiXpHEgwqoOFAxX0r+wx/wRc/ak/bGsdD+JWrWsHg/4f6pNvPiDVGBubi2HWS2tc7pAx4VnKIfvAkDnojgcJQo3rPXvf8j+saHhbwHwvw/7biOonUa1m5yilK1+WnFNczXS6k5WvZLQ/oAtLu1v7WO+sbiOaGaNZIZo2DK6kZDAjqCOc1JVLw34f0nwl4esPCug2ogsdMsorSyhXpHDGgRF/BQBV2vBP5aly8z5duh5/wDtR/Gq3+APwO134iRxmfVEt/snhvTo498moarP+6tLZEHLM8zIMAHC7j0Bo/ZW+Dp+Af7PPhP4UXDB77S9Ij/ti43bvtF/JmW6lJ7lp3kbPPXrXncXl/tOftqyTM32jwb8EBtjA+aG98V3MR3HuGNnatj1SW5Pda+hK9PFf7Lg4Yb7UrTl817i+UW3/wBvW6Hk4X/a8ZPE/ZjeEfk/ffzklH/ty/UKKKK8w9QKKKKAP4g/+CsX/KU39pb/ALOA8Zf+ny8r5/r6A/4Kxf8AKU39pb/s4Dxl/wCny8r5/oAKKKKACiiigAooooAKKKKAP6lv+DM9w3/BJjXFAb5fjRrAOVI/5cNMPHr17V+s1fk1/wAGaP8AyiW1r/ss2sf+kOm1+stABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAeW/tcftifA79in4VzfFb43eJfssBLR6Xpdth7zVLgDIhgjyNzdMsSFQHLFRzX4Af8FB/28viN+3/8b2+J/jCwXS9J06BrPwx4ehmLx6fa7i2WY/fmc4LvgZIUABVUC7/wVH+Lvxo+K/7cXxDj+NGp3jT+H/FN/pWi6XcTlotNsIrhxBFEOgUptcsAN5cv/FXYf8E2f+CTPxj/AG+NYTxjqMs3hj4c2d1s1DxPNBmS9Kt88FkjcSP1BkP7uM9dzDYfoMLh6ODpe1m9e/8Akf1xwVwlw54d5Is+zOrF1ZRTc38MVJXUaa6ya0uruWysr36z/giv/wAE2I/2y/i7J8W/i5oBm+Gvg+5X7ZDKcLrOoAB47Pp80Sgh5f8AZKJ/y0JH7SftG/GfT/2b/hC3iHQ/DC6lqk9zbaN4N8NWu2P+0dSnYRWtqnQKueWI+7GjkdMV4/8As5eDNM/4J6fE+z/ZLsdMkT4ZeNdQkn+GerS3PmNp2peR5lzpdyzckyGOSeFzksS6ckDHWfHSGPxL+3H8DvC86CaHS9N8Ta9Nbvyqyx29rawy46blN1JtPUHpROl7bMISq+9T5XNW2ajFya8m3FxfVP5H8z+I3iFjONsfVxFO9OMJKlTg94KUlHna1TlJNT6pqyvZHmtv4K/aS/bZ+JvhzRvj14T8Y/CuH4b6beyanrnhPUPscepeIXnSGGbTpX3mW1Fskz5dSB9pMbbutdALr9vj4YaHqH7M3hXwvdeKrq5vDD4P+M2salBLHp+myucyalE7LJLeWykquxWWchCcYYH6iorGWcylaPsYci+GLTaTu3dO9929G2mnZp2R8HDJIxvP20/aS+KaaTkrJWatZaJapKSaumrs4/4D/BTwn+z78MdP+GPhCS4uIrXzJr3Ur6Tfc6jdyuZJ7qd/45ZJGZie2cDAAA7CiivJq1Klao6k3dt3b7tnsUqVOhTjTpq0UrJdkgooorM0CiiigD+IP/grF/ylN/aW/wCzgPGX/p8vK+f6+gP+CsX/AClN/aW/7OA8Zf8Ap8vK+f6ACiiigAooooAKKKKACiiigD+pf/gzR/5RLa1/2WbWP/SHTa/WWvya/wCDNH/lEtrX/ZZtY/8ASHTa/WWgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAPK/jL+xD+yR+0L4vg8f8Axq/Z+8NeItat4VhXUtQ08NM0anKo5GPMUdg2QASBwTXpWjaLo/h3S4ND8P6Ta2NlaxhLazs7dYool/uqigBR7AVaoqnKUkk3sdVbHYzEUYUatSUoQ+GLk2o+ibsvkee/tRfBBv2gPgzqXgTTdUGm65DJFqPhXWv4tM1a2cS2twCASAsigNjkozjvXg/7NHxs1T9ob9u1r7xd4Tm0PxL4B+Elxo3i/RZWDJY6tNqsRk8pwSHjdLZJEcHmORe+a+uq5Dw38Dfhx4T+MPiT466FozQ+IvFlhZWmuXIkO2dLUOsTbezbWCk9xGnpz6mDzCnRwdWjUV3Z8jXRyspJ+Tivk0rbs+dxuXVK2OpV6Tsk1zruo3cWvNSfzTd9kdfRRRXknsBRRRQAUUUUAFFFFAH8Qf8AwVi/5Sm/tLf9nAeMv/T5eV8/19Af8FYv+Upv7S3/AGcB4y/9Pl5Xz/QAUUUUAFFFFABRRRQAUUUUAfqZ/wAEaP8Ag5Z/4dHfsl337Ln/AAxZ/wALA+2eM7zX/wC3P+Fjf2Ts8+C2i8nyf7OuM7fs+d+8Z342jGT9Zf8AEc5/1i6/8zZ/95a/AGigD9/v+I5z/rF1/wCZs/8AvLR/xHOf9Yuv/M2f/eWvwBooA/f7/iOc/wCsXX/mbP8A7y0f8Rzn/WLr/wAzZ/8AeWvwBooA/f7/AIjnP+sXX/mbP/vLR/xHOf8AWLr/AMzZ/wDeWvwBooA/f7/iOc/6xdf+Zs/+8tH/ABHOf9Yuv/M2f/eWvwBooA/f7/iOc/6xdf8AmbP/ALy0f8Rzn/WLr/zNn/3lr8AaKAP3+/4jnP8ArF1/5mz/AO8tH/Ec5/1i6/8AM2f/AHlr8AaKAP3+/wCI5z/rF1/5mz/7y0f8Rzn/AFi6/wDM2f8A3lr8AaKAP3+/4jnP+sXX/mbP/vLR/wARzn/WLr/zNn/3lr8AaKAP3+/4jnP+sXX/AJmz/wC8tH/Ec5/1i6/8zZ/95a/AGigD9/v+I5z/AKxdf+Zs/wDvLR/xHOf9Yuv/ADNn/wB5a/AGigD9/v8AiOc/6xdf+Zs/+8tH/Ec5/wBYuv8AzNn/AN5a/AGigD9/v+I5z/rF1/5mz/7y0f8AEc5/1i6/8zZ/95a/AGigD9/v+I5z/rF1/wCZs/8AvLR/xHOf9Yuv/M2f/eWvwBooA/f7/iOc/wCsXX/mbP8A7y0f8Rzn/WLr/wAzZ/8AeWvwBooA/f7/AIjnP+sXX/mbP/vLR/xHOf8AWLr/AMzZ/wDeWvwBooA/f7/iOc/6xdf+Zs/+8tH/ABHOf9Yuv/M2f/eWvwBooA/f7/iOc/6xdf8AmbP/ALy0f8Rzn/WLr/zNn/3lr8AaKAP3+/4jnP8ArF1/5mz/AO8tH/Ec5/1i6/8AM2f/AHlr8AaKAP3+/wCI5z/rF1/5mz/7y0f8Rzn/AFi6/wDM2f8A3lr8AaKAP3+/4jnP+sXX/mbP/vLR/wARzn/WLr/zNn/3lr8AaKAPQP2sfjp/w1B+1P8AEv8AaX/4Rb+w/wDhYnxA1nxP/Yv277V/Z/2++muvs/nbI/N8vzdm/Ym7bnaucDz+iigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAP/2VBLAwQtAAgAAAD1aHpTAAAAAAAAAAAAAAAAGQAAAGZvcm1hdHMvbGl2aW5nL2xpdmluZy5qcGf/2P/gABBKRklGAAEBAQBIAEgAAP/bAEMAAwICAwICAwMDAwQDAwQFCAUFBAQFCgcHBggMCgwMCwoLCw0OEhANDhEOCwsQFhARExQVFRUMDxcYFhQYEhQVFP/bAEMBAwQEBQQFCQUFCRQNCw0UFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFP/AABEIAQoBkgMBIgACEQEDEQH/xAAfAAABBQEBAQEBAQAAAAAAAAAAAQIDBAUGBwgJCgv/xAC1EAACAQMDAgQDBQUEBAAAAX0BAgMABBEFEiExQQYTUWEHInEUMoGRoQgjQrHBFVLR8CQzYnKCCQoWFxgZGiUmJygpKjQ1Njc4OTpDREVGR0hJSlNUVVZXWFlaY2RlZmdoaWpzdHV2d3h5eoOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4eLj5OXm5+jp6vHy8/T19vf4+fr/xAAfAQADAQEBAQEBAQEBAAAAAAAAAQIDBAUGBwgJCgv/xAC1EQACAQIEBAMEBwUEBAABAncAAQIDEQQFITEGEkFRB2FxEyIygQgUQpGhscEJIzNS8BVictEKFiQ04SXxFxgZGiYnKCkqNTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqCg4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2dri4+Tl5ufo6ery8/T19vf4+fr/2gAMAwEAAhEDEQA/APyqooooAKKKKACiiigAor9H/wDgnv8A8E9/hj+1F8Cbvxh4wu/ENvqsOtXGnqul3sUMRjSKF1JVomO7Mjc59OK+m/8AhzR8B/8AoJeM/wDwZwf/ACPQB+ItFft1/wAOaPgP/wBBLxn/AODOD/5Ho/4c0fAf/oJeM/8AwZwf/I9AH4i0V+3X/Dmj4D/9BLxn/wCDOD/5Ho/4c0fAf/oJeM//AAZwf/I9AH4i0V+3X/Dmj4D/APQS8Z/+DOD/AOR6P+HNHwH/AOgl4z/8GcH/AMj0AfiLRX7df8OaPgP/ANBLxn/4M4P/AJHo/wCHNHwH/wCgl4z/APBnB/8AI9AH4i0V+3X/AA5o+A//AEEvGf8A4M4P/kej/hzR8B/+gl4z/wDBnB/8j0AfiLRX7df8OaPgP/0EvGf/AIM4P/kej/hzR8B/+gl4z/8ABnB/8j0AfiLRX7df8OaPgP8A9BLxn/4M4P8A5Ho/4c0fAf8A6CXjP/wZwf8AyPQB+ItFft1/w5o+A/8A0EvGf/gzg/8Akej/AIc0fAf/AKCXjP8A8GcH/wAj0AfiLRX7df8ADmj4D/8AQS8Z/wDgzg/+R6P+HNHwH/6CXjP/AMGcH/yPQB+ItFft1/w5o+A//QS8Z/8Agzg/+R6P+HNHwH/6CXjP/wAGcH/yPQB+ItFft1/w5o+A/wD0EvGf/gzg/wDkej/hzR8B/wDoJeM//BnB/wDI9AH4i0V+3X/Dmj4D/wDQS8Z/+DOD/wCR6P8AhzR8B/8AoJeM/wDwZwf/ACPQB+ItFft1/wAOaPgP/wBBLxn/AODOD/5Ho/4c0fAf/oJeM/8AwZwf/I9AH4i0V+3X/Dmj4D/9BLxn/wCDOD/5Ho/4c0fAf/oJeM//AAZwf/I9AH4i0V+3X/Dmj4D/APQS8Z/+DOD/AOR6P+HNHwH/AOgl4z/8GcH/AMj0AfiLRX7df8OaPgP/ANBLxn/4M4P/AJHo/wCHNHwH/wCgl4z/APBnB/8AI9AH4i0V+3X/AA5o+A//AEEvGf8A4M4P/kej/hzR8B/+gl4z/wDBnB/8j0AfiLRX7df8OaPgP/0EvGf/AIM4P/kej/hzR8B/+gl4z/8ABnB/8j0AfiLRX7df8OaPgP8A9BLxn/4M4P8A5Ho/4c0fAf8A6CXjP/wZwf8AyPQB+ItFft1/w5o+A/8A0EvGf/gzg/8Akej/AIc0fAf/AKCXjP8A8GcH/wAj0AfiLRXb/HPwZYfDn42fEHwnpTTPpeg+IdQ0u0a5YNKYYLmSJC5AALbUGSAOewriKACiiigAooooAKKKKACiiigD9uf+CM6hf2S9TIABPim8J9/9Htq+76+Ef+CNH/Jpepf9jTef+iLavu6gDznU/Hnj211K7trL4YXF7BHK6Q3ja5aRxzIGIV8EllyMHBXIz3qr/wAJL8W7f9/J4I8O3cT9LS3190mj/wB5mt9p/CvUKK61Wgv+XUf/ACb/AOSON0Jt39rL/wAl/wDkTzAfFTxfF+5uPhPr4vOu2C+spIcf9dPOH5Yo/wCFjfEFuU+Et6FP3fM12yU/iA5x+Ga9Poo9tT/59L75f/JB7Cp/z+l90f8A5E8w/wCFhfEUdfhPOf8Ad8QWZ/LJHFL/AMLC+Iq8n4TzkeieILQn9SB+tenUUe3p/wDPqP8A5N/8kHsKn/P6X/kv/wAieYf8LG+ID/InwlvlkHJMuuWSpj2Ickn8KP8AhOvic3K/C61UdhJ4miDfjiEj9TXp9FHt6f8Az6j/AOTf/JB7Cp/z+l90f/kTzD/hOPievJ+F9iV7hPE8Zb8MwAfrR/wn3xIf7nwq2evneIrYfltVv6V6fRR7en/z6j/5N/8AJB7Cp/z+l/5L/wDInmH/AAnnxL6/8KsiwOo/4SSDP4fu8fmRS/8ACxviCvDfCS+Y/wDTPXbEj9XFenUUe3p/8+o/+Tf/ACQewqf8/pf+S/8AyJ5j/wALG+IDcJ8JL9T383XbED9JDSf8LE+IY6/CW7Oem3XrL9csP0zXp9FHt6f/AD6j/wCTf/JB7Cp/z+l/5L/8ieYf8LG+IP8A0SS/x/2HbDP/AKMo/wCFoeNIeLn4Ta2p/wCnfUrGUfn5wr0+ij21P/n1H75f/JB7Cp/z+l90f/kTzD/hcWtwc3Xwt8YIP+ndbSb9BPR/wv3Tbf8A5CXhTxlpI/vXPh+d1/76iDj9a9Poo9rRe9P7m/1uHsq62q/el+ljzrTf2hfh5qV0LU+J7XTro9INWSSxf8p1TNegW11DeW6T28sc8LjKSRsGVh6gjrUGqaPYa3atbajZW9/bN1huolkQ/gwIrzq++AWj6ZM9/wCCL268A6sTu36Sc2kp7CW1b926/QKfenbDz0TcfXVfhZ/gxXxNPVpS9NH+Laf3o9RorzXwr8StU0/xHB4S8eWdvpevXAJ0/ULMt9h1UAc+UW5SUd4m57gkV6VWFSlKk7S/4D9DelVjVV4/PuvUKKKKyNgooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAP5rP2sf+TpvjJ/2Oes/+l01eVV6r+1j/wAnTfGT/sc9Z/8AS6avKqACiiigAooooAKKKKACiiigD9uv+CNH/Jpepf8AY03n/oi2r7ur4Q/4Iztn9kvUxzx4pvB0/wCne2r7voAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAOd8e+B9O+Inhm60bUlZUkw8NxGcS20y8pNG38LqeQfw6E1h/CHxhqGvaXf6L4g2r4s8Pz/YdTCjAm4zFcqP7sqYYe+4dq76vKfiIv/CB/Ezwv42i/d2OoOvh7WccDZK2bWZu3yS/KSe0tdtF+1i6D9V69vmtPWxw1/3M1XXo/Tv8nr6XPVqKKK4juCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA/ms/ax/5Om+Mn/Y56z/6XTV5VXqv7WP/ACdN8ZP+xz1n/wBLpq8qoAKKKKACiiigAooooAKKKKAP26/4I0f8ml6l/wBjTef+iLavu6vhH/gjR/yaXqX/AGNN5/6Itq+7qACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArnfiF4Pg8feCNa8PXDbFv7Zokk/55SdY5B7q4Vh9K6KkZgilmIVQMknoKqMnCSlHdEygqkXCSunocd8H/F0/jf4daPqd6vl6oI2tb+M9VuoWMUwx2+dGP0Irsq+OfC/7Z/w0+HPxK8baM15fXvh/UdY+3WuqWVsJLeKSSNRc7vmDlfNUsCitncSPU/XelarZ65plpqOn3Md5Y3cSzwXELBkkRhlWUjqCDWcsThsRVn9WmmvLpfp8tj1sTkebZTh6U8zw86fOtHJNXt/V7b2ZboooqjywooooAKKKKACiiigAooooAKKKKACiiigAooooA/ms/ax/5Om+Mn/Y56z/AOl01eVV6r+1j/ydN8ZP+xz1n/0umryqgAooooAKKKKACiiigAooooA/br/gjR/yaXqX/Y03n/oi2r7ur4Q/4Iz5/wCGS9TyAB/wlN5jn/p3tq+76ACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArn/iFoN14q8A+JdFsbj7Je6lplzZwXGSPLkkiZFbI9CwP4V0FFTKKknF9TajVlQqRqw3i016rU/ELWvh/wCJfD/itvDWoaHf2+veb5K6e0DGWRs4GxQPnyehXIPbNfrf+zR4I1b4c/Avwj4e10t/a1rbO06M24xGSV5BGTk8oHCccfLxVj4/aTcX3wz1DUbBc6poMkeuWX/XS2YSEf8AAkDr/wACrudF1a317R7DU7Rt9rewR3MTeqOoZT+RFeRl+SQy6+IjPm5rra1tn87/AC2Z+ocY+JGJ4ww9LLqmHVJU2ptp83NKzjpouVK7013Wpdooor2T8pCiiigAooooAKKKKACiiigAooooAKKKKACiiigD+az9rH/k6b4yf9jnrP8A6XTV5VXqv7WP/J03xk/7HPWf/S6avKqACiiigAooooAKKKKACiiigD9uv+CNH/Jpepf9jTef+iLavu6vhH/gjR/yaXqX/Y03n/oi2r7uoAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigCK5t47y3lgmQSQyoUdG6MpGCPyrzf9ne5kh+G8WhXLl7zw3eXOhzE/8ATCQrGfxiMR/GvTa8u8J/8U58efG2kfdg1uwtNegQdA65tpz9TshJ+vvXZS96lUh6P7tPyd/kcVb3K1OfrH79fzVvmeo0UUVxnaFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAH81n7WP8AydN8ZP8Asc9Z/wDS6avKq9V/ax/5Om+Mn/Y56z/6XTV5VQAUUUUAFFFFABRRRQAUUUUAft1/wRo/5NL1L/sabz/0RbV93V8If8EZ8/8ADJep5II/4Sm8xx/0721fd9ABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFeVfFydPB/jTwJ42kcQWNrdyaPqczHCpbXSgK7noFWaOLk9N1eq1S1rR7TxFo97peoQrcWV5C8E0TDIZGBBH5Gt6FRU5qUttn6PRnPXpurTcY76Neqd0XAwYAg5B6Glrzf4B6rczeAl0PUZDJq3hm5l0O7Zur+ScRP/wACiMTZ75NekVNWm6VRwfQqjUVanGouv9WCiiisjYKKKKACiiigAooooAKKKKACiiigAooooA/ms/ax/wCTpvjJ/wBjnrP/AKXTV5VXqv7WP/J03xk/7HPWf/S6avKqACiiigAooooAKKKKACiiigD9uv8AgjR/yaXqX/Y03n/oi2r7ur4R/wCCNH/Jpepf9jTef+iLavu6gAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDyu1ZfB/wC0PeQEiKz8YaStym44D3todjge5hkQ/wDbOvVK4P41eDZ/F/gS8OmoB4h0wjUtInUfPHdRHegU/wC1gofUOa6DwP4stfHXg/R/EFl/x76hbJOFzkoSPmQ+6tlT7g12Vv3lONVbr3X8tvvWnyZxUf3dWVF7P3l89/uevzRuUUUVxnaFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAfzWftY/8AJ03xk/7HPWf/AEumryqvVf2sf+TpvjJ/2Oes/wDpdNXlVABRRRQAUUUUAFFFFABRRRQB+3X/AARo/wCTS9S/7Gm8/wDRFtX3dXwh/wAEZ1x+yXqZ558U3h6/9O9tX3fQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAV5H4bsz8Lfi8vhq2uJh4X8TW1zqFhaSkMlrfJJvuI4jjKo6Sb9pJAKtivXK4D4x+F9T1rRdK1XQbdbrxB4f1GHVLO3MgjNwFys0O49N8TuvPGcfWuvDSXM6cnpLT59H8n+FzjxMXyqrFe9HX5dV81072O/orD8E+MLDx94V07X9N8wWd7HvVJl2yRsCVZHHZlYFSPUGtyuaUXCTjJWaOqMlOKlF3TCiiipKCiiigAooooAKKKKACiivPfi54w1DS7bT/DPhtlPi7xC7W9kxGRaRAfvrtx/djU5HqxUc1rTpurNQj/Xn8jKrUVGDnL+vL5nZ6Tr2na6Ls6dew3otLh7S48lw3lTJjdG2OjDIyPer9YXgjwdp/gHwvYaFpisLW0Tb5khy8rk5eRz3ZmJYn1NbtTPl5nybFU+blXPv1P5rP2sf+TpvjJ/2Oes/+l01eVV6r+1j/wAnTfGT/sc9Z/8AS6avKqgsKKKKACiiigAooooAKKKKAP26/wCCNH/Jpepf9jTef+iLavu6vhH/AII0f8ml6l/2NN5/6Itq+7qACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDyTS7dvhX8YItJhmceF/GH2m6gt5MFbTU1xJIqHssqFm2/3kbGM163XFfGDwbceNfBF1BpzeTr1i6ajpM/eO8hO6M/RjlD7Oa0Ph144s/iJ4P0/W7T920ybbm2bh7adeJIXB5DK2QQfr3rtq/vaca3VaP8AR/NafJnDR/c1ZUej1X6r5PX5o6WiiiuI7gooooAKKKKACiiigCjrmtWXhvRr3VdSnW1sLKFp55n6IijJP5CvPvg/od7rV1qHxD1+3aDWdeRVsrSUfNp+nA5hh9mbPmP/ALTAfw1T8cf8XW+Ilp4Gi/eeHtFMWp+ImH3ZnzutrM+u4jzXH91VHevW67ZfuKXL9qW/kui+e/pbucMf9orc32Ybecur+W3rfsgoooriO4/ms/ax/wCTpvjJ/wBjnrP/AKXTV5VXqv7WP/J03xk/7HPWf/S6avKqACiiigAooooAKKKKACiiigD9uf8AgjOwb9kvUwCCR4pvAfb/AEe2r7vr4R/4I0f8ml6l/wBjTef+iLavu6gAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAK8e+KnhtfhxczfEvw3HLZ3VrMk2v2Vqf3WpWeQJnePp5qKS4cYPykHOa9hqG8tIb+1mtriNZreZGjkjcZV1IwQfYg1vRqujO/Tqu67HPXoqtDl2fR9n3FtbqK+tYbm3kWaCZBJHIhyrKRkEH0IqWvMPgJdy6b4f1TwZeStJfeEr59MUyH53tD89rIfYxMq/8ANen0q1P2VRw7fl0fzQ6NT21NT2v+D6r5MKKKKxNwooooAK5n4keN4Ph74Pv9alia6njAitbNPv3Nw5CxRKPVnIHHQZPaumryS3YfFj4vG5H73wt4KlaOI9UutWYYZh6iBCR7O59K6cPTUpOU/hjq/8vm9PxOXEVJRiow+KWi/z+S1/A6f4S+CZ/A/hNY9RlW68QahM+o6vdD/lrdycvj/ZXhF/2UFdpRRWVSpKpNzluzanTjSgoR2QUUUVmaH81n7WP/J03xk/7HPWf/S6avKq9V/ax/5Om+Mn/Y56z/6XTV5VQAUUUUAFFFFABRRRQAUUUUAft1/wRo/5NL1L/sabz/0RbV93V8I/8EaP+TS9S/7Gm8/9EW1fd1ABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAeUfE63u/h54qg+JWmwSXVjFbfYvEdnCMvJZqSyXKDu8JLE+qMw4xXqNneQ6hZwXVtIs1vOiyxyLyGVhkEexBovbOHULOe1uY1lt542ikjboysMEH6g15V8JNS1PwHqkHwx8RNDLNY2Al0TUombF/ZoxQowI4ljGwEAnIINd38elf7UPxj/AMD8vJHB/u9a32Z/hL/7b8/NnrdFFFcJ3hRRRQBynxS8ap4A8D6nq4Hm3qp5NhbqNzXF0/ywxKO5Zyv4ZPaj4V+Dv+EB+HmhaE53XNrbL9qkznzLhvnlfPfLsx/GuRXHxO+NTMT53h7wPwvdJtVkXk+h8mI49ml9q9Zrtq/uqUaXV6v9F92vz8jhpfvqsq3Re6v1f3q3y8woooriO4KKKKAP5rP2sf8Ak6b4yf8AY56z/wCl01eVV6r+1j/ydN8ZP+xz1n/0umryqgAooooAKKKKACiiigAooooA/bn/AIIztn9kvUxzx4pvB0/6d7avu+vhH/gjR/yaXqX/AGNN5/6Itq+7qACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAK4H4z+D7rxL4UGoaOoTxRoUg1PSJgPm85OTFx1WRcxkdDuGeld9RWtOo6U1OPQyq01Wg4S2Zi+C/FVp448JaRr9if8ARdRtkuEUnJTcOUPupyD7g1nfEzx0vw/8KyahHanUNSnljstOsFOGurqQ7YowewJ5J7KGPauN8MSS/B/4gSeFrmP/AIpTxJeS3Wh3Ib5bS6YGSazYdgxDyR/Vlq746Qal8cvhpZMPMjtYNT1Jo25AZY4oo2x6jzmwe1dioQVfvCzkvNJN2/Cz8zhdebodp3UX5NtK/wCN15HHR6L4x+NnibSLfxTYax4Hj8NwTvcX2kz+QlzqDSKiPbOd2+Lylc5IP+sKnPWtTzfin4XsbrwZY2UuuTSS7NM8Z3k8brb2zsctdISGeaIcDAIf5Scc59tooeMbsuRcq2XZ7+vy2COBSvLnfM931a29PmtTn/Afgmw+H3hi00XTzJKkW55bmc7pbiZiWklkbuzMST9cdBXQUUVwSlKcnKTu2ejGMacVGKskFFFFSUFFFFAH81n7WP8AydN8ZP8Asc9Z/wDS6avKq9V/ax/5Om+Mn/Y56z/6XTV5VQAUUUUAFFFFABRRRQAUUUUAft1/wRo/5NL1L/sabz/0RbV93V8I/wDBGj/k0vUv+xpvP/RFtX3dQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQByfxR8Dn4geDLzS4Z/sepIVutOve9tdxtvhkH0YDOOxI715d8M/G0/xD+Oxlv7B9N1nQPDclhqdkxBWC7e7TdsYcMrLEGU91YV77XP6b4G0fSfGGseJ7W3Mer6tDDBdybuHEQIQ49cEAn0VfSu6jiIwpTpyXR28m7J/Jr8fmefWw8p1YVIPqr+aV2vmn+HyOgooorhPQCiiigAooooAKKKKAP5rP2sf+TpvjJ/2Oes/+l01eVV6r+1j/wAnTfGT/sc9Z/8AS6avKqACiiigAooooAKKKKACiiigD7d/Yz/4KWf8Mj/CW58E/wDCuf8AhK/O1SbUvt39ufY8eZHEmzZ9mk6eVnO7+LpxXu//AA/O/wCqJ/8Al1//AHFX5V0UAfqp/wAPzv8Aqif/AJdf/wBxUf8AD87/AKon/wCXX/8AcVflXRQB+qn/AA/O/wCqJ/8Al1//AHFR/wAPzv8Aqif/AJdf/wBxV+VdFAH6qf8AD87/AKon/wCXX/8AcVH/AA/O/wCqJ/8Al1//AHFX5V0UAfqp/wAPzv8Aqif/AJdf/wBxUf8AD87/AKon/wCXX/8AcVflXRQB+qn/AA/O/wCqJ/8Al1//AHFR/wAPzv8Aqif/AJdf/wBxV+VdFAH6qf8AD87/AKon/wCXX/8AcVH/AA/O/wCqJ/8Al1//AHFX5V0UAfqp/wAPzv8Aqif/AJdf/wBxUf8AD87/AKon/wCXX/8AcVflXRQB+qn/AA/O/wCqJ/8Al1//AHFR/wAPzv8Aqif/AJdf/wBxV+VdFAH6qf8AD87/AKon/wCXX/8AcVH/AA/O/wCqJ/8Al1//AHFX5V0UAfqp/wAPzv8Aqif/AJdf/wBxUf8AD87/AKon/wCXX/8AcVflXRQB+qn/AA/O/wCqJ/8Al1//AHFR/wAPzv8Aqif/AJdf/wBxV+VdFAH6qf8AD87/AKon/wCXX/8AcVH/AA/O/wCqJ/8Al1//AHFX5V0UAfqp/wAPzv8Aqif/AJdf/wBxUf8AD87/AKon/wCXX/8AcVflXRQB+qn/AA/O/wCqJ/8Al1//AHFR/wAPzv8Aqif/AJdf/wBxV+VdFAH6qf8AD87/AKon/wCXX/8AcVH/AA/O/wCqJ/8Al1//AHFX5V0UAfqp/wAPzv8Aqif/AJdf/wBxUf8AD87/AKon/wCXX/8AcVflXRQB+qn/AA/O/wCqJ/8Al1//AHFR/wAPzv8Aqif/AJdf/wBxV+VdFAH6qf8AD87/AKon/wCXX/8AcVH/AA/O/wCqJ/8Al1//AHFX5V0UAfqp/wAPzv8Aqif/AJdf/wBxUf8AD87/AKon/wCXX/8AcVflXRQB+qn/AA/O/wCqJ/8Al1//AHFR/wAPzv8Aqif/AJdf/wBxV+VdFAHVfFjx1/wtD4qeMvGX2H+zP+Ei1q81f7F5vm/Z/tE7y+Xv2ru278btozjOB0rlaKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//9lQSwcIzTcDatYgAAAAAAAA1iAAAAAAAABQSwMELQAIAAAA9Wh6UwAAAAAAAAAAAAAAABkAAABmb3JtYXRzL2xpdmluZy9saXZpbmcubXA0AAAAGGZ0eXBtcDQyAAAAAG1wNDFpc29tAAAAKHV1aWRcpwj7Mo5CBahhZQ7KCpWWAAAADDEwLjAuMTkwNDMuMAAARYVtZGF0AAAAAAAAABAAAAACCRAAABg9ZbgEBrxGKAAP/scAAzHAAMyYoAAsJyf/xD8EEABAABAAAAMCgJBYaKyKeTyimAFQwBfC+EAMO+++++++++++++++++++//1h+CCAAgAAgCgAKAOAERhXqcj29GRjt0FSSGapJmGMBwABBEAAECgADAABHbTAcAA+AAIEDGDgAHwABAgY+DgAHwABAgYg4AB8AAQIGPjQACkaAAUhduZO3MLtzJ25kxQABYTk5P//+HoACDYKazP4glimk80MP4VD/w1wAEABgPAAI8A6AyRyYtFArMQG06AOfABA5mCQSHKtJg4CbxhQf+AARjY4gaCTrGiwtCk0eiT4WONsVPL2vRP+CBMsgAIAAEBt9TPipAZ9v5M8AA4D5gAkABZ7VZVkDJdzGMIQABA+UAAQIAiAAIIKAACBGAA5i5SCQAFcpXNgXa2wBGt4AdewE1bi5w/AAcQn2sgABvb21kGG1kAIWnAA3FwKLHFzh//w/oNQ+AYAotHkQAHA9vPoEwQoD//7gJ8CEQeHBSgiFK7Bjf8wJPoIEJhC6CBEGbPB3/4WAAZAU5h38y0/7DpCwCdg/0M3Av/Kge+f4WAInmdcm4X/v////oNcCQhVU0X5uMMoSj/3gsAAuBRNmb3xBP++FkgBAAKVGAJRT/uBvPgABADAAEAoJjsOcQP/+8uEeHQv///6DXA1OAnEA3RcaSQQv3gIwBAYDguQVaATucOAd8wFEBEUKGhT8kQKDnngxf/SiBpJBZg+8RIB+fXN8Lf/BNs/7L///0GuG5BpILsr/eBUBUIAkUdyZuLIz9/uwfAHgWgA6Vwc6Hx/qBsASQMwAH8pICvsD//7iJG5f///+g1wJIL0F4dm0IpCP/AevgABARYAHf0CP/XgrvMAAQDQABAuBcegKIj//7iBAS0N1PzlCR75f///6DXDIvwTSj4DcAXoIcAFsWsBlHE//+7DBABKDd+2/vzgFBlFtfADrVjAFvIYv+IkInHKVcv///+g17IA9A/CrVk9Y//+4LZgDAQudCOQlfMFh/orgAKgPJPf/9xuQBhSQDB5Y9a4A1AI66I1H8/4AmARETLYL/f///6DXBLtCOBaPBxyD//7gQI9VQGAkyJi5shn//OweAFXAAEEYAAQb5BOAaQxxc4BVBFgEABoYhhJwanSDE/yRBiAAjtmKIvXvGFAAQAAgOi7AQ9fvz/4ZQAEEZE2fmveGQAA+AAIFAyMMgd+17//+uvWv/GAAIBgniQLkQZQ4EJHHUxRJfoAAQA3DBwTAdZKNGIqCmysZ0/JGJWIL2+Q8hrww/AkABigEgcehtzmw7dsencPEPgAAQBQIPcCgJ5tC7FingNI87v/94QACAsiAiKsrFynV2F64ZPIf9U0NLiozoEAAICwAosOBAACA2AYUHwNKlUACoRGkgBAF7CAbKbEsxUdgJvIMg/ThbCHnIAHKA4DphC+OWrgRSQNcEwACLTPEAB4HLSAhNGkZMxKigQ5DI4yY8ppLuYACsAIAbQJ2QRV9ieR/1z+C34ADGFZbBjoS4oF31DRbaYHoAAIGgHIDwLLQKW6TMXWhEc/34BQABALAxYAon9BfVg1OUOWYIAEAzAsIAIB2hQfgAAQDRxmW1rD7KjuzF38GH4AAEBAYfh5nIQa18V5OkIZOTk5OTk5OTk69PJycn///BBwAEAGEIU1RP+SzWdzQycnJycnJycnJyeJ6UD0Skd/wAAQGhkgEgcphViaFXlVJTp5h+AACAaABQCDISzjops1PKxE3uIfgAGAAIA4QAEAxvwp5S28ijTikC3Hm79vAABABAY8A5iOcgyoTkkCe3zxAACAYAAIDwNMFrbeZBdxBQrqD4AAEAsA4AAgbAinoSE8wFE2nQX5YDgAHwABAxAFBgAHgABAyAG9LA06BxnXBXJUvYBSYAKHCcMaHyeznFOVfgNwAC2cpACSAusgb/vtoCKigg/z4tmtYIBRuACAsWgkRf1W9ZJiv8BMAAIAGCd5GELhjC4/fJ5OTk5OTk5OTrp5OTk6wycnJycnJycnJyf9J/6DnwGEDwKprZP3WQttQjU+//fIyCKyiP5pi3kNhwAEYRDjrQUUh4n6SXwgACYYsoIAAoClFwXgA2KAIJMH7FBE5+BlAARZTADiDIlSnZECttvlNlKfQAXPH/BAngFFZbZBj++aFOMCYH4GAKB4B0g8moajZ5doVEdYYfgAAgIFDAACCACKnIyUikGrzbQG1PD4AAEBYAAQMgAsAm53DnaHY1jpRAbKoA0/vCAA+tB4ADCgLOlDNYFQ8KUD8BAgSIMaoPXyNbtTDPUGEAASAQGjIQAAg6gACDyAAIGwAHANkGO1Yt4WWzoyr54EgAgBnkcOiidmsD4EX4AWIxFZDmUoyVLBOIMxg4mNrqEhb8/ggABADAlaiYMyOlSuIB8ARV/gMKQeMXRKFUHBHxKkJycnJycnJyddPJycnh//4IOAAg2Cmsz+IJYppPNDJycnJycnJycnJ/2p/wWfgYAGB4Jye3zhYKAU0jkJPsavAIFdglxV7MdM5hH/ggAisaEAAIBIAQEmIAFZgmXFDkDWLJlIcBSOMGs+28NOT4t5dDWHIDBLY0CngiD8AAECAAAQKA0idWRAsxsjLR5RXSv4AgAosoGDRyFa/dThCZCSWXLwAAQAQoNAjIwrrEQ7El83CKJY0B+AACAwAAIAwFPjSrTh3ZofzFu2AYfgEQAAQEgAMEc4owEgigXBvgQA1WvgMwFBpCFVRg0DnjMvCodQ/gCABKAAoypdsAo/T50H4OhyDCAAEDcAAQXxpcIAAQlAABEyAAEDIACmgHKBNNQEwY6QM4OeCBzLsQTPh1fojPwFRgBDCYy9l8crnuEfXBtaQdDjeeVyAAEAxWAAEA84BhUHpS1gI3zPwggBQDB2qgtFBOGglfWgL3BsDvGQn/BAI2TYiblGE+xRktM9xJOTk5OTk5OTrp5OTk6wycnJycnJycnJycn09PpRI78jQlZWFQ8j2kPh8AACAKBxAARJEE/6rkALRKihR7f/g/AABAIABwAWApDbUoAWmwIiBVuB1vh+gABAAAAEA54CRHi4iA3ORTmvi4Y81713jwMgBQk5K5U8B4HPOxiGY++BAAzAAChPK1eChhU5cFxEQKwRABgACASBASABAACAYBfdFcYuhcz6SAMBDZSKaHII5xU+DBcACAH6t4A0HelC/7ztFrI5i6px4AgHOiFDXQjHJILw6GAAwAm3yQBQPdq1/3hOTk5OTk5OTrp5OTk8P//BBAAQThTGb3EksW0jmhmAACBIAAULGEzJycnJycnJycnJyf6afPhz5GiGykKhRPSpPAABgFIAAEEsGPo65egDk/Qgt3P/4PiAAwABBWAAEBcK+BRb+C4t9V5VW32wYBEdJoHihyo6vMmsMIAAmGY0IAAkHayBJgOHLWjUbdKfopS8gLkM5nQUhLj1EFwJQAAgCAAou8KDj9V/DIYEBYbGFR7i0PEdTS6fSgew3D+po0ht1IrOofhABAEBGb6FLc1D3VJmvd0PwBABQDwAgl5qRAnVBgpUagl1wwgABALADgw4EAAIBgAIDTv+KAAwGHzmm149fPwIB/4bAAIAoVSiTqInJRcJQycnJycnJyddPJycnXk5OTk5OTk5OTk5OT0RP0ppj/gAwAzgABAP6XQ6BJu3e6OgfFvgiC4JjndVhwnLQwJn/7wjqARTlkgsQKVV3pY78AyAACAQAGjDCh6+OQCsqaYTa6B+wABALAAECQsQ3IfARLfenfyjBD+BsHAc1jrodfF7bT9l35EYishzKUZKlpD5gACAIAAIDQlokZCzkJUajLnqX4MIgAMAEACAhAAQAOAF6OEAAIDtTcNAA09CNnvAkIoCICZIQRNF7lgRfAPkCqmWBKRA3oiYwEcAAQIovJcAAg/GF73gSDMA/hU0apHzFi4KPgWYMt4UqRPbJGThOTk5OTk5Ounk5OTw//8EEABABBSFtWR/Esxn80MOTk5OTk5OTk5OTk5P9ET9Jw58AAMAAQJQBkwDJUzEozE51KZMwuCb94AT+lAIWEmrkDJHliAJ9XQJaJkE1v//uAAwAEAAEAEw10uaNgACASFkWST1aD8IAKA0ANPf7GAb+gjVqto92/gAAgCgAwAsEzBmkSoGMY2oEEsGEJGA5QgABoD/4AIEBAqkilkWZEJNHNx4wgCBhtHhBuiSargd8BQAwKDxpTHJKt1JCuKfnwgwDh5xDpNyaqij4e/4T+wWw/oIAAQNAANBoiwUnyIEI6ZMZ1aAwgACIUggIAAgEJJD9ZArLsgLSIH/FTAYflBgz8osMVJiF6RWGTk5OT//9sF2AA4AAgJAA4AfIELqDBgHoVcoQATA6/AAIxMcVfEEc01pBDfAAEAAUAAQBQNgQzqLAOEwIQHbFrEsE8IAAQBgBw8sEAAIBIAQEGeABwAAQBAAUDYVJGOUSRKhk4AHQABAHABgOjIub/NNIsGj/tL7DfAAoAAIB7gACCFQCLgACALMB9MJ4SJY34PwABsAAIDgBQHupmRXqKRHpPchsD/BSiKQsiYMIFwMKCCwHFgA5kGUazSDXbtfpCDM7DAhB4QwTKjwAdjCIPRhJj92v54wV/6aFpkRHFOrDXTycnJ1ycnJycnJycnJycnJyLoaJ9KJ4/4BgACA2ABWQdfKSsBEpaU1iusl/gAQEDSx2GSDPzag6yIP7hBjONcsxqpfwSQ/7DABhwABAZFMFPQJWqAbbT/74MARHAAEC6Bc5XADxGSFsPgu8+GQCAAEAwuWABZgqJT8z6fWg0QOooTRNZJz/AABAWjAACACAAxnIoPbsuPN6X7YYQABICggFggACoEBIM8ABBBA+kilk80JyaGSwPQI4NA2SGETBnJYMvwE0A5eYBygoTqVtG4AAwcWFksMk/+5UTwS1h4AxBL00+pHTmDIa7AT4BLMOEJlRa1Y1YnJycn+xNs0/+AA4ABAcWM2UJh8nGMc94nBCGP7wABoAAIDACDnfA2AvtuhWySZzDwGKJEUMer9S5c2pWaX9wAAICLAAYABgElVxFmNyfaWcqV/g/AAAgBKGAAEDEAZ0gVYCIRKdlv1/wACAYDAAsGjDWL9b7e7yeGRTwAAICAAEDAr1dSNb+Nxn/9iR+8iIRGQxkIMhSkBhwgABAnBwAOYCAAECgGAAS2AA8NxGI8ktJAjiavAAIaAezSxEL2y335FmMQAAgEjCWYYHoWJOr//AwAHYRBXrQUUgkX9JTgABh5Api+SAYskwuzL8QMAAQDh5XGQnA0JoFn78GTDXTycnJ4f/+E+AAgnCmM3uJJYtpHNDMAAECQAAoWMJnycnJycnJycnJycnJyf5nT6cOfAMAAQFpMBg3w6vj6CJRbZkPwAAQCgELJAMUVAoFtRssvkRDaw/AAgGAwAzKxqSirjly1rzJ1+EcYg9yzi8lRerVDCAAOgIIZCAANAca54CgACAKwByHkYCgTvjbQdEPnMyNDbodGfwFAAEAgoAhJT/BoMYROY/STy0jTE+Uiu3T+gew3D6Ew5cJ9BwurJot2/gzigACA28tcqQAFT6akP7H4PwAAQBBQwAYP1ORkgLwHyfh9XUMMIAAQDQAUChIIAAQDgAYDRXnpk2J8hUdg8ABgACAEJnPY5PtVLOIh/KaNIbdSKzh6wAgABAFFSlwIzb2yTCVeTk/7W9rHDmACsfeJnFUafoAgAMBQgAGhhWhP4UdtO92VATgaQAHBjBxMVjUKLKde6K7v3gAAgBggAWAhZ1OUATToPK0gjq5cBEAAECzTgSsHQXpBGrQ4yH/3hnFPNX5oSi2VRbdXCAAPAOAPDwQAAgAgEACARgAeyQBVwHLDBSZW39QQAB8wAAgEBU8wDJTDN/96ABaHQ+TwUoXF51jh6CwABAAQUAWVhiIcKpf/vP//wV9MAAQFQABAxWD5+AAwmscSBTJ/fB6wABADAAEAsCb3NhvP/+d8RP+8IAA6FBYuEAAcCAoW4Awo25G8UaaIGrmAsGH4AQYdIlaCGekz9jYXDDk66eTk5OuTk5OTk5OTk5OTk5OTk9EE9NPNP4AAIAIDA8UbQV/l6pyYYNeJfBkAAECoBocwkjnureWrqZg/AbgACCA0AH8h0ABTIoAb4mpgE8PYxjL8Qi2QYJtnq9/yMgE7KQHceIumsjD9AACAaFAGgQh+oQloGLEkqBLgB+ACAMG5AlY7mwJBPiR87Yi+AAw4CCItJVNPRe2lbPXYQAAgWAcSDgQAAgXgYUDoGgBkAALxGHADXRSuR39wEdGRsT5Do7ARmZNDIrC4hOgAAgEgoYleJcEpq1mNpQ+AkABHAAEADydEghD43qlePgITIyQ26mVnAhojJifKZXbgAAQDgYEp4nAwCtNC/CwpJyfa3tb6G4ADgxAINhOzVMm++vYjV4/74ABgBQUABAIL1FtHvDh1aoRG9+BABJAAEAzpj0yUgH55EgS6Q7DBUpHotkbrtpueAAgMwAQNFYJUyaqO3eDXjwAAQBQwBYBrxJA6D693g2IrjB8GBwGASEFet9M+OnKlQHTL4DDvsSymzGnrJ4caDAAEBcABgAGAQAAgLQAAoADwAH0ITYuRa5o9IyP0wBWQAwwSYJiA+9IAA4AAQFjLmQA0IwN+8DAAuzAaXAMIOicKogf1oAOWwAUeKs05GP/AsAlAAEB02IigAQEod/eBk5Ounk5OTw//8PcABABhCFNUT/ks1nc0MnJycnJycnJycnJycnJ/0p/oFkPwAYAAgQgCgGB3fjcxIUcBFr2ztj+AAwQ8ApyW0w60ctKu7IggABAFAEDSgQAAgCAChhUMBQIcMYOwgt2rz+SgIww9AGMCWRCPmyRCgoTmD78KeSTO2/Ne9d4/gAAQBgABAUSIKWbCTkY9I8evqBwAEAwY4aVdWPHli46nXLk/5AAEHwAAQChCZAAmABallYhYtIXDL4GHFgBoIcsr1NpND3LwZ/YIvAAGBj8UujATw+D25ibFvgAAgDABXgACA2ANWaQjVcwjRG0jW5eAMAEFxCXVNoNFhAurzuz97oADq7ADBAuzjTETfQkAAEACboHABEO7MLXE8aCAAIgACALIAAgPBAAEAABAEgAAQIesgVVywek8Z0+W0AwjsJb2GtCBJa7f/tAEAAweclTYlMzLpY/laDbdAABAsABuYQGXwBwwRtQSFJkTnSRngBB3+Q5NnOCRWFc64APgAHDhGWWhvSyMZIv8eCX7gACBmADsRmNfkHsrI//DXwQABlqAASOTnYWSXVwjQFWHwAAIAQAOCgGCTME4YI1pAAsHsMIA9+8AfBAABAYAAdQsQHr+TuBLKgyfCd4RigACAKtiNAHnRkljXrZfAABAkQUAPAKLEgqP6c2dCw4iTf8AAEBoAAQCQEvCCKvyzHcPeZcJRR8ADAAMLREmPUkmdR52duHfkAAIDIcE4AWD4fDBbMRmHS3YOIQAB8AAQNh4NBAAGAABA1DAKgZwBgAmgpQrAkqDbfB//gSwABAjAmSaYwBF+PATjf/4f6SACAA9A9ssVYB5xrA/wEkAAIBg2tFlfjtFv4ohP/8P+f/+Cv4AAIFrgEAA4B5RUOLUZFMP57pqwfqRCTEJmMxCMGEAAICIAFAkRCAAEBAACAQId4AAk0gQcg8JXKFwYfowDLeFCUSIrirXhhycnXTycnJ1ycnJycnJycnJycnJycnJ9KZ/RuOh8AAIIAFAIydsQTb5OmtKatf74AQDiuGTD5rN1tkeCa7wIBBqgjt89HVY57OL1wOeoBGDbuSAHjatWWCa/94wIAAQWQHNMDFIBSaDPCnWBIDvbwAGqgBwCANOVoRNR6veRAACAuAAUBHcHzofRB1mbbvb1IDTZgUse7RcyRDDCAAOgEJOCAAPAClCQJEcxxeuhCZgTVP3/7gAIEEmnfaRrgMvZ1Lj+BnBQ9mmtGSJVRg0hoMxzCfVi1DIotIH/7gAMCij7yfUKXDhBMqd/wEIcCnK92aJFFXTaJPycnJ108nJyeEAwD/kffffffffffffffffffffhD//Ink5OTk5OTk5OTk5OTk5OTk5OTk5OTk5OTk6eGMBwABBEAAECkAAQugABBDlv8HAAPgACBAZBwAD4AAgQGeDgAHwABAgMg4AB8AAQIDOIAkQgCRYXZZOy8LssnZbW1tbW1tbW1tbW1tbW1tbW1tbW1tbW1tYWcABAjHAEWpSwwqVKbKFSv+AIgYr3Aw8fNtXlgAHgABAhY+uMXPGLnAAAAAIJMAAAAaFh4CAY/AQHE8MieO4cwAEBAACAwECejPwEYkrXz78A6AYAAWfgIECiot1oUORgbEvt5GAAARcwgACQcsoIAAQSgABAvAAWBrgAhg2KQhnOAFe4ADiFLDZe2a94UfLycBHNx1vzBvwAWQoHjhthaMV96WF++AEMoYTyglTnldUVn1JP+AjAoxGucLKo78njwfgIwIMQne6yiGeo36GH8AYOcADgzfcpN3IYQrhFMucPwECBAEQjwXaOZJMzF+mzhRDDAQiAIAAQGRgAc4AKC5CLw8pt9BgPKNAJwAyDqAB6DEFq8y0YFdU+AbCnkcjRl31+3s23AAAIAQMNYByBPAAkpBnhr8EAAIDVlAACAUEad2SSsAmYQJoeAJsMJTQQABMMWVBW4yyBoQof8MCOOcLWOkutDqRoxhzC+/w/DWmAIA8QDFSRE6Q9/UAJAJTQw4CpEkv4LgpP/or+H4GUOJ/TGk0/8QU0gGHAwAB4KAAkIAAQSQABCpUAAIIo4AUpAwyweB/oYxgDHL1EBgXtCfdiqSkSS8MM89f6MArvPXJDkhgAAAACCTAAAAE6YeBAGPBS8PBcOQAEO6GCNHrxSNKPkVvQAAg8ArgeO1gjS9FNwFSGYgo+AACB8AAyvBfah/fE7MOQu+xg0ICUpOAD/slxo+v7gAWAMDplVNHLeJ6sVf9J77qe4jo///S4ZJ4JgbBAF4J4J4VwAEAAEBABgeyDyui5iIhNZk9DEZj6E4EmJAk8HSKBmJEvcHAUw4rZ1oQspkJfYiah4ADAAEAQDugIyXJkWvx81Iw7cAgXdwgABAKAGBRYAD7DYrCsn0Q4/zJl6Gvp/gAPDBpD+ZEpk5Fun0FOgClB55Q6iPb8SA/8MwAHAAEBMAAQEhTa4AXE+eFYCYjAQhZA/8H4DA4kyJsVJuXoyEIUdraVUMKkggADAGENAjxup5s+M3v/zAMeQk+ZHZKPOGOz4MOGeev9GAV3nrkhyQwAAAACCTAAAAFbYeBgGPBTw34ADZGNcRkU6kZ178ZQeetgL+GTCjMXaiAxET/AAAgCDAACBYukmG2otdnVgBwJIACGgDB5eEyAhpyp14un8AwogLJJ4VRhhCs+0fv6+//528MABAAGBDgIrDZ8BUjCYYOQhjSIXgCIBgBV2X1uAF1aPgTv2IPxQAAgIgFAxNIT1Z8CjMrwoHgGFISQkBMIymsHjHlx8AAEAeAAGgBT2iqoJUuiMQPazwAAIDQAVICT64HhJ/iKvIiySl2jeAAIUwAAgcg+u4AACEQA7hy/v74IAF6ACBAACH6AAIAwAAhfTAB9DdDjO9wNX3cBReSIz0AIA9GVjcLHoGnuEMFgVKmWXvlt3wAA4EUYGnVGzBTAkdWjtD4MxhyWqOOFJvorADHDAwCBZK+umWQI1MWx7oGgAAgJODeLCpPPfOCYCQ/7z1wJOHD/DPPXw8P6MArvPXJDkhgAAAACCTAAAAFKYeCAGPBTz17/562AvwwsGAOQ5gAIAGAZArZl7B8qOE0rk1IvGwABAfKA5Z4FSFl0zhK9//gEXgBhtMVVDnAA8MaJej+9XLj3Sf4OAKeH+AAgTi9XqO60gW2JLFYeg2xhUysTbDn6X3h8ABwCCAW2KUI0iyt8djgj0fBlEITBGQCgpqm27NVX7+AAujF7kX3LH7+AYcnxKvV6wpL9EuFoDhVIKhyVbwZlFBqAB8N6RO4Q/DGv//Vhg0AAEBexIzNRDfVM/v7wFMDpk930JVrRCvP+QCeoe4ImCRSF6Pk//7/2UmFfgAOACHCuBWyrDpfVxffv///AAgBSmhJ5owouSdJAyvGPNh0hMV3Xf28DDlnjV0y5dN0R9Tp/Qw4S2wgACAZxgC5YtYUrHaKEDD4ABgE1WBtL+Lxar23BpKRJI/hhkTwCic9ckOSGAAAAAgkwAAABQ2HgoBjwU89e/+etgL8MLCQRDmAAgABgIETT7iasXBqRu9ZAAIAIFCUmroPWuqB1WLmhl8AAEAcAFUQIXiIQSoY2UztSgH4COyaTq4AHkYpDnEkumHo/PPVAL0jixw9DhkngmD4JgQ00NwALWBKqEOciL7v/4ChZCKvwAOCawi0U/uIKHYMQ5a22xe//wYf4AAYaPeWIVvnciWnPw/SVEAAIPNfygACAcOgL+AACAqD6nUT3G6OjGi9RAekHTGueBoFgAwBCLV3rp/RMEIqu7r///z5qkccAAGECLiePGtanO//w3X+gtIcBIYxJJS+WQ1V5L//0Cc2K7X0RSmP+8M4ACQ1CTEcYWSuScMAAQAQAFDSS47kDZUjo5CHTcFcFUMAIQAEZzgDLjkOKT1eHAXMEHVCMG4aN2Jmgw4BXeeuSHJDAAAAAAgkwAAABeWHgwBjwU89e/+etgL8Mc9eUR5L89UAtI4scPeGeeugUaBq4CGA4Hz1GhdfS8Bqm+r4Twf8ABAABAuAgouCZOmHZzZlDU3AZAJQADSAAkA3IDLFIP3YVMi2CntCo1sXRe8UIkKceugsk95PgAAgJAACAiRdgbAA2YSOBLHNtcS/iCAgGluYEjVO/FuZVUf/wAAwABAWlQ4Z2YQXrHhC3Tog/AgpJwxZEqVRYPOFSbXkAPUcCGS2J+ZT4XS9gAkBgYuifCmjYxUlpKQgYAAiAOACggAIAAgUgACDGAAIfQAP5ZmQAmCk/OAx6IJZPcAB8cBdCWxMwezAl/t2RI8n4UFWcAA4AwxU/HhbsyAGkwlY+AEDp8Q8Oe1Uc34AHtzgDDCDRP+ClUCwH+Roe+BBV6UAgkJzVZ9iMbOU2n/9huHwgNIwOwwbRdpQ9/vMAAQHS/igA23AGaT/CwMOSEsTfaMqz7+8r05Gmg+UQmTGE5RHMYGAh+euSHJDAAAAAAgkwAAABcGHg4BjwU89e/+etgL8Mc9ej/z1QCsjixw9wzz10C6BfPX0uA1SeME4eDY0w3gAsUtFWxUJaoYBQQIVrHFnALQb8u+e//4ALApQd46C8Fn1v8/fwKQIOcEtIMCBY8ZLgYEiA5scP9OrzsdIv9n1gXvDOtD/aAEAYHEAq60+sM+NUE5AHZdDDAAvJBOFbPCsf7ZAAIEAAIECUABoCZijTT3gCSwF4J4KPgAgAIdBy8D2aus4FqXAgKIMAAQA0BoAey+rfDd43T069AK9pDx8pv+wAFgGYEX/JX9IYj3/8BuOTdtfGf9+B6BjtG1V0itfm4GB2PDP7cJ8UF9kLDz1//PEVywzwr4AAEAsAAQFAWFo15VLZ6AREsPI4BV7gAAQAAAMA5g8qCjhkpIicGoAlroBd/Bh+GK0k0lo13qpVV11cMOERMwQAAgEABgQaBeeF3VRirT4P4C4A1cBSwOfL1dLgoLr840qjsH4G7nrkhyQwAAAAAgkwAAABk2HhABjwU89e/+etgL8Mc9ej/z1QCsjixw9wzz10C6BfPX0uA1OHMABAGBhRM2TaGHHSspVuHCvwBAAQAoneC4B96lQoguACAACXBEzEEAAIBAAUEluALCcphFk57gAWGZgNdpA0mPHlRaJX6T/ECcbwAE0jGqEMji+hH8IABuADijEocIYoZYI8AAiMMRE5jUo1LMSH8xGAGChd5VOkTpMmHoHgVbuhUleTyffgw4cABwACAW/WQ0YlOnVWWv/4MTQgB5DI5V95n/++wiALUqfPcD6+fNPfctBjtaSv2mzG25//Hsi1gZ4a/AYEAAPuGrk8XbgTnqKI4FIDXDCC7OEAAID4AAgBAPHoGhiNsheL1WLI/z8BQABAAAHGKvvJyHEpkEsR2LMOTwsG/wrDAAcAAQFwEDZuaCVECasz2L2QABwwABAEBoIWfpMHodnLH8BTAt3Mzv+KSl4vaq/7xjcoqfe/1lre9YYcIAiKUE7+DqHE21CfIP2XeBGH4AGAjRk7QO8u+8y5NMb6DwN3PXJDkhgAAAACCTAAAAFSYeEgGPBTz17/562Avwxz16P/PVAKyOLHD3DPPXQLoF89fS4DU4c8ABAABAmoHABIT9mg1Jhh46ZNr2hAE5zcAKlavv//+4AFgYgh9Od1lkVPOovvPJTi3ZhL8Py/EMk/DAZw0HgvAAqAAIAyAACBsTwAkAAQNCMIXRPWhkAP0HoYEQNpWbh0TyeeUvgIYEVghZXrKShb3+AIANBYAHGgAn4CSiQIwuhJhCXDCAAjlFhAJAAKBEdYBqBSQ09zQSxNDytenyBpABFKU80OiySUIn/+BJXjCVjKk0BdIyVknhgP4Z4WwAEAAEAYAdgEjskomVJEGP76t5K0YAPuACc96i1EDiv7vB0oLdZRpZ5OLNb4DAposuXfH2e6cxZkb1iGHiEAJ2Yrv0buOHv32/e2BgSDRwxS815N8sY/48Rd/BGMMU5QhjSRBJhgMDdz1yQ5IYAAAAACCTAAAAFYYeFAGPBTz17/562Avwxz16P/PVAKyOLHD3DPPXQLoF89fS4DU5686/+eoNYMPpeGSeGAqDAH6HyDcABwxmvZ8WQtCauv/3gMEE9KkCAhd+b9lTxruMAw53XQYgJ3aufMv94DCklmlki3eqlVXW1Qw/kGMB20k6G2PuVX5uH4ADAHIWKnXqaCyU+ZzwAAgCnuKcLlndO+nj//4aGcCjEczW5PT/a4QABgACAIDiUweAn7JZOSYQJIH4EAARMgUIAAwFFoAAlk4eU1KTTYfj+vW1rg5g0Fy48G0a89/2AApg6GqRMoRqlKFlK0Q1Xf//O4ycQ0fo6JzWIGrG4FggKq3//3eFocABAAQcEiHlunCFHopStECkPgAAQHAAwPAHiTYGV5nT5awmlk1gw5WEACOYVqIw++Le3sf/DAcYvbN/0XFkMPEEQMUQwIIeaFOlSoMDdz1yQ5IYAAAAACCTAAAAEjYeFgGPBTz17/562Avwxz16P/PVAKyOLHD3DPPXQLoF89fS4DV56lBA6XhknhgJ0HgyBCAqjfAAcAAQCQJdosOHVXskipDf/4ABwBhW+FX5ZqrsZ7Lf8HgGc7ndf1tr///x4ACAhdejJaYwLRxoWU4GBdiNEzXmnGZ/gwJAAEAJjhuUeeuyFjf9gwAHACF8tbn8BCx8uoWX//ACAwAEAwEzspVlBo+YrhgFdJHbwAEIvzfNCUIOEDthTwMW0GAATCaABdY5zV2LWEUeIy7zd6Gx4xdHgALGYCB7qHU/N4LipLjf/uoBQBPzDbGlonLp+gBxRhUt3haT5rp4W4ADsgnWd4gRBb3f+D4CElbnNSjFplSsGHVwM9xjuL54cDdz1yQ5IYAAAAAgkwAAACC2HhgBjwU89e/+etgL8Mc9ej/z1QCsjixw9/C+ABYAAgBAAOAJNA4g+hdgGaqHoOwIf2gCAMDhYZz3vWMwsDB06JAD8AAIOvwAAQJAABAJUSAEISAbHUawMXwCWBiAMcMIFgpQQAAgFgAgAGgWgA+cWwIa2jS0EacNqguKWEABxHBvH8y8iuGjrM3BL0QwUosgFnEywfSERGTOcPEv4bgCAAwHFz6bAQX2RWDEQBCe3TgAAQJgAQfAW9TiYkz1LkSlUR//g/DIAtAReQXpPUmjjMPDf6ZkJAFSneYHgANVgZ+AACBWOFpDSrQ8EUzpPvNaqVQwwgAIBBmAgABAkAAEGoAAQHBIC7cx1D4cSk/34WpmOKMoK9+fwAMENJceC8wcCuRuFrYHwAEKI8yRlMzYiCz4CBcDC/PXQLoF89fS4DV56lBA6XhkngeAEDhIIixAf8ABgrOUUgJauhRdpEABAAgAoYfR5pofKlXkM8jkPgAgg0T9jpl7UYTRa///BwAN5MfuMh3ZgAHAD4BAvyV22mFmUlf/aAAgEERItvx5NP1lr/hHAQABMggeaOQBuebEsLgAeqjDZFRd08F7GpVfeif9uA4ADBGLZeoDa0xnn99xj/hlhIIkEhWGAB1icAFi+nA3kmtv2ZGNsxlYjH/7/Q/oGGPn/w6v3+j1uomY4Vzvu1/gVeeuSHJDAAAAACCTAAAAFvYeGgGPBTz17/562Av5P8PhQCCFgxwAKDDMaRw1FaXcXq8AAEAIcJrULFhKky9tSKU0H4AAIAODCOhGRJUhlaKIyHfRhjLK2E0OuaJmf3JRwEbVh7hSmrLL/3wAdgCHN2mMnU+BFD4KNYt17sn9Bfnr0f+eqAVkcWOHv4awAHCAXl2BwjXlDzZlNP/0NY6DP4AUbWp79+r8njCgzMEAiCUQN/ACcGiBE2U6IU3nxBswqvctDXNeQ+wwCJ8M70lVj/XPBiMWS9RhCnmvIMJDAEAGIPCsUkqJxJEh721u4tiTDzP8UtNf+8ABOBChNry5mZHUroAKjASzPgukXRPvAY3V4HJtDNW6opJTScMPx4Z+YgbU2I4kl4MOJSDABAxSL0IYa0L7HnjXfmHu2zBfVj/YQE4v2K9ZWU6n+B3DxW7fZT/1bGW0Vb0j8F+eugXQL56+lwGrz1KCB0vDPPwnO1kEf64Qzz19nwKvPXJDkhgAAAAAIJMAAAAYNh4cAY8FPPXv/nrYC/tQqgxwALkaGqVTGC+xACAMZ1NCE2e94ryznfREegABAJAAEEcARRaxYZz/0BOAnUrgCgnkpjvyj/eXGnmFWU/WAGajeiNf94DpcBaajRmZJ9glhvgKBoA3AgIa7o9Gb2Z5VBr/pNs+FuevR/56oBWRxY4e/nrR/yfh3gIUQHIALzMZwCt1YGhVqf+tgIQAEQAPYBvSC47Lyh73hoB5WXqiX/wAXpphW4xE/cmU5h1YZfpTB3AAOAIAEA8hMh4LF97hW4RLxn/0U4y0PIuD+DHYEKJ/cEmJaRbNBKmh+AACAkADATaPBdgSyOqzh0tCHwAAQAsCUA0CPZ9F8U2DYKLg0Bh+pERITZDIzIGHCFAAQQIABACAACBsAWAA3WRwwnkkJ1jaQKW9YIDJTYMAJAYxOdzRxHhkCSM9EHXGTxFRjoUmmAAIBlQBiUi6F/BoBxSfgwtz10C6BfPX0uA1eepQQOl4Z5+HWl/XCGeevs+BV565IckMAAAAACCTAAAAGDYeHgGPBTz17/562Av/PXjiVjgQbC/PXo/89UArI4scPfz1o/5Pwz8OYACBBWQ9y1ROL1hLofVUmX8Iyv0k9wyGwuHwmGRsABMSqYqCqvm8hOYCUKUJpzDKz9zH94MARCqY9yyi5gxpNb/QACEVzKWSSala05/gINAE6hMYP2R4DfoWzMSg+AABANwAFtQZgioN3EBYsQf3gMFVpA0PbUxSWgVgoRgogCAMItMf3TtqtbH8AACAGAo4st+smH64okh/+D8BsAOGCfWksLpVVZ16rMgwBiWogEjXe/AD7UxM578TLXqTefABp9NN3/bg0IdCiUI/p/7/gYBCxi1YzZKGi47fcIwDAfIimW9kaoLc9dAugXz19LtgxByC0GoVh+iGxo/E515dV1u8PwAIAHKoSHhFGiRZ1mlHqz9ADKKm+7tTqKj/7B+cQlt20/rGqlj1/AkxPtxPEomty3//w4ZcCFCKXl5SlvDAaXPUoIHS8M8/DrS/rhDPPX2fAq89ckOSGAAAAAAgkwAAABU2HiABjwU89e/+etgL8Mc9ej/z1QCsjixw9/PWj/89V/yf+LHhyABYYiCUb/bkf36BAAZ8kIUoKg2D+33gCKAAQAouywgN10o630FKIyKiwA9NbV16DuubfHgBMGDi/WjekleYj4Lc9dAugXz19Ls1KUDHV+OwAEFAgAHVZvgsNuMaVggl+MQFPCt4MrGA7P4OmP/+D8DDLggSkvOmZIT1kPwjnGKcsSztIgkyykAAgAAgIYAAQCAiwI7PyqI2QqQxuvfgDADPwAh60A9U6PUYYsJJDDCAALhENCAAEGsAAQNgKDoAXfQwi1De3hhJ4AL44KOsGKhfgBawAICpUgwCweJXwf9/tdHYrly1jwIABzAC8BWj8CvgQFErC4Owz4CDVohDvPtsZULu4MOhCZn/CC0xnlH/d7wZwGjz1KCB0vDPPw60v64Qzz19nwKvPXJDkhgAAAAAIJMAAAAcFh4iAY8FPPXv/nrYC/DHPXo/89UArI4scPfz1o//PVf/hjgAWI/xFOn5IYu/ADCchyMOR7SmkPbAApAGCG1D3Jek0IIf+D0jDhpasI5yf+kwlRfCXxzAIXNyd3+7Atz10C6BfPX0uyfYkMsIjwJ43+AAkgZS/hutlD//XwBQ4Ato9QEMFxm1Q6Y+BBti+bkLWO2DD4CcFhI7SpfDzy70/8AFoQUXrBPijbix+/q+hCFzARDeY6ZRn2HV4M4Apq2zCaK+socuRQhcYs81Csmf9+iygoSH0Fgt8BgrpOGrvRnWa0b1D8FAUUj9GSS/b22goH4DPwMTikyAMDRHGRvxCH4eAUQehOg2CivNv7ONYMB0IBLalF+DNiVJ1YII5NGF1Na0f32VjbgYZa1vfzwy41BENQJAAEAIAA5/1IT55K3qRvACZGUvsIQWclXgaAIHMIN7Yer5l///58AYEesUXGf9J7L/aJ7kJTUwBQz8/wAkEzVDAC9ExUCJQ+EQz1gACA4AAICV3Y/Fk08kHL94dCW3A1rz4/+kgjCQhXVGTr9/wz4a8NeGl8Chz1KCB0vDPPw60v64Qzz19nwKvPXJDkhgAAAAIJMAAAATth4kAY8FPPXv/nrYC/DHPXo/89UArI4scPfz1o//PVf/nqVF+WoW566BdAvnr6XZP/mDnAA5YB1MB6oLbhD+96jsd+zQP4b4ADhFoAkeiBYqmx84U//g05w+ln+AAWK2Zb54e8Mk/EgQ8WBRC8EQIkhAh2FTL+eAYAYEfO04Wmnhm2g/AJuAQr0weAElf5hzgdEAYITMSZUSnAIAT0KoAGq7p7ISUXE6IJYwiwDf7duuegB0PpoZte2gRwAhTM3VlnqxZ9CQIdhbQGHRhE0hJNr394A4qLH1JRg12DBgiH4AEChZxykIkedjXCzfGOa3Yc2Khv7gAAQCQL4TOPuoFXMdW//+DWRp7jZtqv5nzAABZAlIqeePfoRDnO2KqEOzP+8Bb89SggdLwzz8OtL+uEM89fZ8Crz1yQ5IYAAAACCTAAAAGzYeJgGPBTz17/562Avwxz16P/PVAKyOLHD389aP/z1X/56lRfmEwtz10C6BfPX0u+ev2aB/PX8oIYZJ/5wJ4X4ACbBs8yIADhs7VWKB2HQAXOJAFiq5kMiMer3gKwYHyOQPDXCCFwnYg0CifAOI7amgEv0nc9G4gACAWAClknhvTB//2G4ACCfgACANF9AGgCowwweiQ5h/TAAMngoOXZHaFwbFMNQAAoAWYBREOwL7NB7wlgjBAAEw4BwWAFlA3vTPeQAAVHRcw0AAPUGALE790Dy95AAAgsgACBCAq5hlQKbAshfwa8AAQBbrALBUh+gZKH38HgABolC9VJ5CGv/08AUwht3k+zVsZ6NCFaMM1nznTSGDrB+gEyKPrrYGKep/ngade1e7n51EBRIIrhqxG1qJsWzN/l/YbgDkQyugiq16VdofgYYJLCnORK/mtMGj/C0PkAAIFRwUDAp5DwuaqS+a//gw4QABQENNCAAEFQAAQOAACQEdgdAoFISGpkIAFP4NSRsni+vYHwQyiBDSxwkOVvEjIBKGAneepQQOl4Z5+HWl/XCGeevs+BV565IckMAAAAAgkwAAABb2HigBjwU89e/+etgL8Mc9ej/z1QCsjixw9/PWj/89V/+epUX8Lc9dAugXz19Lvnr9mgcGiseDsL8AB4AGAdaDHAaEb5J2n13wfAhQWOrKp7fvm54LSwT//ysCNCq+vARwjW5eGHiF6+V9oADDjM6jHXMUtif74A4QXVJx+AKg+3SWfwAdm0jNbN+gRwwB/iIL8kpSroSAi2NC0AFkDLYF54eyT/hz/AB4BWgMeFlovHyZmAmAAEAgAAQOwQst4Pq9RVAJHhC/wAYOAAIB1a2SH4hMJECRF5iSSEJiYDQGY2sbClnn5HgaNiPvgADzfWMvfobHAdE4VkiMkevvDMokNQwJAAEBRAVwmlaZz6j1b/31trXH/+f/hoxp52VLP//8rDMAQMBBcYFx2meqKROklvsAAEAYCHICq+c53Pz/3wH6AF+NUv5/8COkF2k2juR/X/DInxPA089SggdLwzz8OtL+uEM89fZ8Crz1yQ5IYAAAACCTAAAAF9YeKgGPBTz17/562Avwxz16P/PVAKyOLHD389aP/z1X/56lRfwtz10C6BfPX0u+ev2aBwacOcABsMOzcLHItGePE2vAhlyScQJnbU04+ws8/gAtT/nUv378MYAG4B2QccxqSH17gwBWAgBgnqAW+AxZhxQRyDsDD/1Mhpik7mchXDCWkEAARDlGYALyUx1DRyj34EGAwEOYsWbRICmSliDMMx4fDUPgAgABAYBxqdURDSuflDLI+AJQxV+hvu9bFbiAT8EIye1pq/7/+TwQg7BKBDKVxsMPQhCMoZDa+Wrpot/gAAgKANYB6lOHLlygLVDxy9B8AgAwaL53UbBPI0RuKV/BgCAAIC5ADi4+RY06lJne5eQH6EQMvaV621ZcMPwAAQDAZirYrVGHX5vLDD0Ol8AMq725W4YlyNH3+URNq5sv37ASDFdf2/phrX0wGXw1IWwVFJf+wJiMZszJpv/4ZE+J4GnnqUEDpeGefh1pf1whnnr7PgVeeuSHJDAAAAAgkwAAABP2HiwBjwU89e/+etgL8Mc9ej/z1QCsjixw9/PWj/89V/+epUX8Lc9dAugXz19Lvnr9mgcGnPWl8wYhsn4XBGGQThMJgggAOAAICQQU6tphwQ1dXeMCL/g9AbB7RNT+I1N/f4AQAMAogqpOT67XBxKRN3IAD5B3HIEWcMVdT5/gwwAHAAEBQAgCFcRf8SRRjMFj7///wIVRBvkC60xyCrH9wEEao0yRKWeqYr3//8AIFpsTJUK6hZUh/wfgIAJxUQBbL7O/uQDQbF52uBFrvQfEABwADpBC61ZVUmn0AB4BgC0mEBtaRhBW91gCo9yYmkTKWhp4KQahrwBAEyTACAbXRRx2FbO///cAACAKAAUSbJpuowquqve/BhoBPoW/oMqIzqmuT74GnnqUEDpeGefh1pf1whnnr7PgVeeuSHJDAAAAACCTAAAAFyYeLgGPBTz17/562Avwxz16P/PVAKyOLHD389aP/z1X/56lRfwtz10C6BfPX0u+ev2aBwac9aXzBiGyf+JEhjwAEAYjEic2UUtsjtZZmAAgCBTGHq3E9JMHGnBhkAICcpqBlHAlu9b+IJHh3pAEKpfzYEFV18tBbI9v/U3+cz9gt8A4wHcgGkoovN2rjB8AACAMORIqomui094pqS/vAADA8ApUmVqCCV6iupvyCAAR3OCAAEDcAAQGwA4FsCqAMv0gehbTv4Cgxl4AnGVVtIAb0DkqBhmCkFIagAIAAIHIBw2IMwTq2NcovipaO0hhgAIAAIAwSpAKNC9eGCGqho00RFjFhjXBfnjF9fHbAwAkR9oBHa0ox/+/9iXhmHwAY9xeKoHlayqsS/3mTiG79TQACjkbaAGBX68tdzbnMZ/6QHXdPnP/n/+BsaFAxQ2c6UT5AMOGfDXhp/DfPUoIHS8M8/DrS/rhDPPX2fAq89ckOSGAAAAAIJMAAAAYph4wAY8FPPXv/nrYC/DHPXo/89UArI4scPfz1o//PVf/nqVF/C3PXQLoF89fS756/ZoHBpz1pfMGIb58GgZRzw+1jC03+dX1SHYADg4wHcgGkoovN2rjB+AuCCqstF8qXE1ThOq8HRSbKADHq9mWAJAqvEL8AAEBYAAQIR0jOuAfKNmQQohBkjnIxohxWTAggE8JCAAEBAAAQfgJABvABu8jioAAQFUzwHJppcADYCmfrAAcDhWAFJDn44KY0AAIEf3ABAIJUZQkpfvgHUgTvQZ4a8ABAABAVADF09DQBBW8eH3j4eQuv0iLvAOl6ZoTF7+bUYOX+g1DD8AEMeqwKkE7ZaB8mCa+mrwOkSAACAuAgIQUwqFvpCsDBACAAOKJA7fyN7S7fsueGBrBY0VOODJq6dwErVmGEAAaHFfCAAEHIAAQEQABAaDsLwMbU9JRtDKvCMWGP/f7hDoiYOAEo0/gAIEQLbQFXaHeQ+Oi0CjwwNHPUoIHS8M8/DrS/rhDPPX2fAq89ckOSGAAAAAgkwAAABeGHjIBjwU89e/+etgL8Mc9ej/z1QCsjixw9/PWj/89V/+epUX8Lc9dAugXz19Lvnr9mgcGnPWl8wYhvnrQMu+OBh77DHgAtBGDmD8GSBx8gbtYEiAVKBl+4pnF4p9+9KPKDsAB5/JuvUsCzz36fff4GTRzCnz0HIADgACASAAUZNeHQymmptjbwf/8MMABAMBQjYWlqeRDIhEywYp2noAAIBQYdIca3EZo2sFPJ3/7fCAAOgIGlAgABCBAAEGQAAQVwAPQAfbUI5GBA9fcgW6aG1Q7EKYAFwAI4PxmAofleJB4K7/oCsCOHOEqGyn3gikbNZPDQRoFwIw2IDuC3YjifGL4wMCRoUyKAvg4xFcZRPBgAuJCZkIIRzD//7gCAAIDIKPKXUuYiYvSmU1QwBABgDJYwoJLq2mdemCGNhY/U/A1DGGj2B+jz+9ajiu5Of9/MfXY/4zQSLUV1bTe/AzA0c9SggdLwzz8OtL+uEM89fZ8Crz1yQ5IYAAAACCTAAAAF/YeNAGPBTz17/562Avwxz16P/PVAKyOLHD389aP/z1X/56lRfwtz10C6BfPX0u+ev2aBwac9aXzBiG+etAy7/568N2o8MmjmH/SHIADgACASAAUZNeHQymmptjbwf/8MMABMbgAMtPtADAGAdN0tWHI9fAwtC6Lo4NsGBmZW0UeAADBgIDBDHmJsidT3UdnP+CAIKIJhAACA4AAIGYAAgIAAzAB8PIWUzBgSpQSisjwz0gAPaAA7AAEADABZFRJCNV5IwFwv8BgACABm4wF+HlN7gB6JPRcCyAoQQAXw/wwAECDzFITf3xXFluD8AGN4ZbCMnTD/6tKvBhhwAEENmIKcpBY3mMF3Q0f/CFd3f///7OixxaT0PyqxtgEKxRLPYvtr//8auNxrmDLxAg9AJ/wGV2nMm1DsHwFrDMAB0AIcwEi2HK1TPzjrE8QwewsMbfU6SYcpcMww1Iql/ozQAp7QTpT4P4E7nqUEDpeGefh1pf1whnnr7PgVeeuSHJDAAAAACCTAAAAG4YeNgGPBTz17/562Avwxz16P/PVAKyOLHD389aP/z1X/56lRfwtz10C6BfPX0u+ev2aBwac9aXzBiG+etAy7+Gl0HMAhBwxUwH/+LgCMAGAROnW+xfTTJoNTWYED8AUvVZp9J/0BjATYY+AA4IFTKBzNLZvGlI/wYfgAMPAMDK5QCVmmVIDapn4Zg9AAYMT6m9dukkov+AKAAIDgAhIZAuYBOllY2VMy3hYwgFiLhxFtQD23GY+j82FGTTpvv+oEgAHQj3lK/BbtCa7/gzEGPQrThC3PB3TF+RXZPvzGYrNX/4agAIAAwABAPEdJGEQBWT0hNKQW9zZnn5B5UV/oGBIAAgXDgAOC6wpUXUmiwrChtYPQ2OKqWLVMzM/ABAAJgOtEh8hXRlcut3j8AAEAAAzngACAKH2KpN2UAuIwnjeYAgABABAzCwgABCFAAED4AcAuB5SzDfdwCF+/ZkEoodFdh7/4AYAE6HuIABRteCvgACA70gWUAoUka0fXCpwifB/4ZgCGFg6jycDmzIhImaqQw6AHoPgLnzGqybeIGHAm89SggdLwzz8OtL+uEM89fZ8Crz1yQ5IYAAAACCTAAAAGPYeOAGPBTz17/562Avwxz16P/PVAKyOLHD389aP/z1X/56lRfwtz10C6BfPX0u+ev2aBwac9aXzBiG+etAy7+GueswIH/w54ACADACWwBJVAPWQ3CvG2JjfQrZY+P4BN+W182gYADz0QlFTeOU9uJnoAv/3Q01smmeGXgrBGHvAAQCBxFzLiotVyotV1N1XhA2I8QqWWxf3gEc5gZtjGf4I0z6oegBACqgelniOZVjD/vgJAAEABAAFAFCeAKhRxEpb0goawKf+AQAAgCGrAAEAwZjUurrwCZDB9BMAphExsEAAIAAAoLLdtEqgIheC+OP/+5AEndIolE15M5/398DQAwbhT/YBLuDYhPf/qgSghhcLfAAHCAixtlVlFCPzvRXwYqi2NSo0ggvnuYH4EFIAAn1mLP3ODkTwgUB8ADYYC7c3pjzazrcP/8HwABBlNTpLUTWscZKA+vuHz++eXp6n3/gEA07ds2q6nHv43oEIU/cuyWUGBN56lBA6Xhnn4daX9cIZ56+z4FXnrkhyQwAAAAAgkwAAABxWHjoBjwU89e/+etgL8Mc9ej/z1QCsjixw9/PWj/89V/+epUX8Lc9dAugXz19Lvnr9mgcGnPWl8wYhvnrQMu/hrnrMCB/89f0DhrhzAA9CBDD3eeBS54XX5AEAGAAIAPQAAIB5yCgqD4F4KPYHpYAAghBd2CAAEAgAKCTHABvWeQFgpI/3AAcIK9YpDpfrkQU+NTGT+zhuIC3gAWDCiOvWQyvyjrGf+94CgCAIK9pUyY4gg8N27wfkA0ImRkAAEA0BAJ4njXxCWkNCazdyfYCR6z5+v4sEZdSimEwz//4ZN8P57DnAGGCi/aJt09AEABAIDTROk0MPxAn8K1pe+AACAWAAwur1UzCGWDgRJaP4YQABMMQSEAAIJYAAgXAAKA3GPXe/81AplxozHkAGPBKKxV5yZjVonwYi3mGkBLTJsywQAAIWwLrCoW/5gAQDBSERFKHhNDtZmldMBLDwMowBN5C0nsGeaJ/3uEBpRuAAEA5D2bXynH/7TAAEAOkBRvfw1KM5f/0PwAhAAEgznNUcb0b2WJU/zfiAIWa1DLBeixGvo2wAQIKj5SFN2vBnz1KCB0vDPPw60v64Qzz19nwKvPXJDkhgAAAAIJMAAAAUph48AY8FPPXv/nrYC/DHPXo/89UArI4scPfz1o//PVf/nqVF/C3PXQLoF89fS756/ZoHBpz1pfMGIb560DLv4a56zAgf/PX9A4a56/zhHz1w3JP8MpBUIAQQvgF4AAgFQAAQHACE7IAiGaBCaiDIefkokpp3/eAAmgYAAgClXAkGrmh3kQAIRUD5IAX+oXpiqeQDAqwgEhjwA46q+z2BJNe/IAD8If4u2HVnLboIDVJg5Ts0+nKs5gt+GAA4gHA704cJ1gymUhOAHxiRf/bEAMAAQOmqAAiDgrgb9s/AAkzCJJtXT1pd+/uAhBIAnnRNGE9PRhgHAyf/iEAXKZY4BX2TGl8eo5M/8b7CoADbYTJP+vJbnhmY4ahgAOW2PNpBE9juhr/8BeY56/5Ghv6+GYRAgHqUEDpeGefh1pf1whnnr7PgVeeuSHJDAAAAM6bW9vdgAAAGxtdmhkAAAAAN3GHB3dxhwdAAB1MAAAeRcAAQAAAQAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgAAAoF0cmFrAAAAXHRraGQAAAAB3cYcHd3GHB0AAAABAAAAAAAAeRcAAAAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAABAAAAAAZIAAAEKAAAAAAIdbWRpYQAAACBtZGhkAAAAAN3GHB3dxhwdAAB1MAAAeRdVxAAAAAAALWhkbHIAAAAAAAAAAHZpZGUAAAAAAAAAAAAAAABWaWRlb0hhbmRsZXIAAAAByG1pbmYAAAAUdm1oZAAAAAEAAAAAAAAAAAAAACRkaW5mAAAAHGRyZWYAAAAAAAAAAQAAAAx1cmwgAAAAAQAAAYhzdGJsAAAAlHN0c2QAAAAAAAAAAQAAAIRhdmMxAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAZIBCgBIAAAASAAAAAAAAAABCkFWQyBDb2RpbmcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGP//AAAALmF2Y0MBQsAe/+EAF2dCwB6VoGgj4iSaIAAAfQAAHUwB44TUAQAEaM48gAAAABhzdHRzAAAAAAAAAAEAAAAfAAAD6AAAABxzdHNjAAAAAAAAAAEAAAABAAAAHwAAAAEAAACQc3RzegAAAAAAAAAAAAAAHwAAGEcAAAGrAAABRAAAAWUAAAFUAAABTQAAAYMAAAF6AAABnQAAAVwAAAFiAAABLQAAAhUAAAF5AAABjQAAAY0AAAFdAAABywAAAUUAAAG9AAABeQAAAYcAAAFJAAABfAAAAZQAAAGCAAABiQAAAcIAAAGZAAABzwAAAVQAAAAUc3RjbwAAAAAAAAABAAAAUAAAABRzdHNzAAAAAAAAAAEAAAABAAAARXVkdGEAAAA1bWV0YQAAAAAAAAAhaGRscgAAAAAAAAAAbWRpcgAAAAAAAAAAAAAAAAAAAAAIaWxzdAAAAAhYdHJhUEsHCDiRihf/SAAAAAAAAP9IAAAAAAAAUEsDBC0ACAAIAPVoelMAAAAAAAAAAAAAAAATAAAAW0NvbnRlbnRfVHlwZXNdLnhtbG2Oyw6CMBBFf6WZPQwaY4yhsPDxBfgBTRlKlT5CK8G/t8DOuJwz98ydsp7NwCYag3aWwy4vgJGVrtVWcXg09+wEdVU2H0+BpagNHPoY/RkxyJ6MCLnzZNOmc6MRMY2jQi/kSyjCfVEcUTobycYsLjegKq/UifcQ2W1OeKt9egXssuWWKg7aLP7K8a9i/OFHmXRLDleeFFxfrr5QSwcIiHoWxJoAAAAAAAAA4AAAAAAAAABQSwECLQAtAAgAAAD1aHpTzTcDav//////////GQAcAAAAAAAAAAAAAAD/////Zm9ybWF0cy9saXZpbmcvbGl2aW5nLmpwZwEAGADWIAAAAAAAANYgAAAAAAAA51IAAAAAAABQSwECLQAtAAgAAAD1aHpTOJGKF///////////GQAcAAAAAAAAAAAAAAD/////Zm9ybWF0cy9saXZpbmcvbGl2aW5nLm1wNAEAGAD/SAAAAAAAAP9IAAAAAAAADHQAAAAAAABQSwECLQAtAAgACAD1aHpTiHoWxP//////////EwAcAAAAAAAAAAAAAAD/////W0NvbnRlbnRfVHlwZXNdLnhtbAEAGADgAAAAAAAAAJoAAAAAAAAAWr0AAAAAAABQSwYGLAAAAAAAAAAtAC0AAAAAAAAAAAADAAAAAAAAAAMAAAAAAAAAIwEAAAAAAAA9vgAAAAAAAFBLBgcAAAAAYL8AAAAAAAABAAAAUEsFBv////////////////////8AAA=="}}},{"cell_type":"markdown","source":"## Metrics","metadata":{}},{"cell_type":"markdown","source":"### MetricMonitor\n\nCourtesy of https://www.kaggle.com/manabendrarout/transformers-classifier-method-starter-train.","metadata":{}},{"cell_type":"code","source":"# Courtesy of https://www.kaggle.com/manabendrarout/transformers-classifier-method-starter-train\nclass MetricMonitor:\n    \"\"\"Monitor Metrics\"\"\"\n\n    def __init__(self, float_precision=3):\n        self.float_precision = float_precision\n        self.reset()\n\n    def reset(self):\n        self.metrics = defaultdict(\n            lambda: {\"metric_score\": 0, \"count\": 0, \"average_score\": 0}\n        )\n\n    def update(self, metric_name, metric_score):\n        metric = self.metrics[metric_name]\n\n        metric[\"metric_score\"] += metric_score\n        metric[\"count\"] += 1\n        metric[\"average_score\"] = metric[\"metric_score\"] / metric[\"count\"]\n\n    def __str__(self):\n        return \" | \".join(\n            [\n                \"{metric_name}: {avg:.{float_precision}f}\".format(\n                    metric_name=metric_name,\n                    avg=metric[\"average_score\"],\n                    float_precision=self.float_precision,\n                )\n                for (metric_name, metric) in self.metrics.items()\n            ]\n        )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:41.161123Z","iopub.execute_input":"2021-12-17T11:29:41.161456Z","iopub.status.idle":"2021-12-17T11:29:41.171375Z","shell.execute_reply.started":"2021-12-17T11:29:41.161426Z","shell.execute_reply":"2021-12-17T11:29:41.168801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RMSE","metadata":{}},{"cell_type":"markdown","source":"Our metric to optimize is a regression metric RMSE.","metadata":{}},{"cell_type":"code","source":"import torchmetrics\n\ndef mse_torch(y_true: torch.Tensor, y_pred: torch.Tensor, is_rmse: bool = True) -> torch.Tensor:\n    \"\"\"Compute r/mse score for regression.\n\n    Args:\n        y_true (torch.Tensor): True labels.\n        y_pred (torch.Tensor): Predicted labels.\n        is_rmse (bool): Whether return mse or rmse.\n\n    Returns:\n        squared_error (torch.Tensor): The squared error\n    \"\"\"\n    \n    squared = True if is_rmse is False else False\n    squared_error = torchmetrics.MeanSquaredError(squared=squared)(y_pred, y_true)\n    \n    return squared_error\n\n\n# ====================================================\n# Loss\n# ====================================================\nclass RMSELoss(torch.nn.Module):\n    def __init__(self, eps=1e-6):\n        super().__init__()\n        self.mse = torch.nn.MSELoss()\n        self.eps = eps\n\n    def forward(self, yhat, y):\n        loss = torch.sqrt(self.mse(yhat, y) + self.eps)\n        return loss","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:41.174309Z","iopub.execute_input":"2021-12-17T11:29:41.174523Z","iopub.status.idle":"2021-12-17T11:29:41.294957Z","shell.execute_reply.started":"2021-12-17T11:29:41.174491Z","shell.execute_reply":"2021-12-17T11:29:41.294246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CV Metrics","metadata":{}},{"cell_type":"markdown","source":"## Trainer","metadata":{}},{"cell_type":"markdown","source":"### Optimizer, Scheduler and Criterion\n\nThe 3 brothers are always together. We can put them in our big boy `Trainer` class and since `criterion` does not invoke `self` inside these methods, we can put them as `staticmethod`.","metadata":{}},{"cell_type":"markdown","source":"### The Trainer\n\nAlthough very common to see this class named as Trainer, it still reminds me of Pokemon Trainer everytime I invoke this class.","metadata":{}},{"cell_type":"code","source":"import time\nfrom pathlib import Path\nfrom typing import DefaultDict, Dict, List, Union\n\nimport numpy as np\nimport torch\n\nfrom pytorch_grad_cam import GradCAM\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\nfrom tqdm.auto import tqdm\nimport os\n\n\n\nclass Trainer:\n    \"\"\"Object used to facilitate training.\"\"\"\n\n    def __init__(\n        self,\n        params,\n        model,\n        device=torch.device(\"cpu\"),\n        wandb_run=None,\n        early_stopping = None,\n    ):\n        # Set params\n        self.params = params\n        self.model = model\n        self.device = device\n\n\n        self.wandb_run = wandb_run\n        self.early_stopping = early_stopping\n\n        self.optimizer = self.get_optimizer(model=self.model, optimizer_params=OPTIMIZER_PARAMS)\n        self.scheduler = self.get_scheduler(optimizer=self.optimizer, scheduler_params=SCHEDULER_PARAMS)\n        \n        if self.params.use_amp:\n            # https://pytorch.org/docs/stable/notes/amp_examples.html\n            self.scaler = torch.cuda.amp.GradScaler()\n            \n        self.monitored_metric = {\n        \"metric_name\": \"valid_rmse\",\n        \"metric_score\": None,\n        \"mode\": \"min\",\n                    }\n        # Metric to optimize, either min or max.\n        self.best_valid_score = (\n            -np.inf if self.monitored_metric[\"mode\"] == \"max\" else np.inf\n        )\n        \n        self.patience_counter = self.params.patience  # Early Stopping Counter\n        \n        # list to contain various train metrics\n        # TODO: how to add more metrics? wandb log too. Maybe save to model artifacts?\n        self.history = DefaultDict(list)\n         \n            \n    def get_optimizer(self,\n        model: CustomNeuralNet,\n        optimizer_params: OptimizerParams(),\n    ):\n        \"\"\"Get the optimizer for the model.\n\n        Args:\n            model (models.CustomNeuralNet): [description]\n            optimizer_params (global_params.OptimizerParams): [description]\n\n        Returns:\n            [type]: [description]\n        \"\"\"\n        return getattr(torch.optim, optimizer_params.optimizer_name)(\n            self.model.parameters(), **optimizer_params.optimizer_params\n        )\n\n\n    def get_scheduler(self,\n        optimizer: torch.optim,\n        scheduler_params: SchedulerParams(),\n    ):\n        \"\"\"Get the scheduler for the optimizer.\n\n        Args:\n            optimizer (torch.optim): [description]\n            scheduler_params (global_params.SchedulerParams(), optional): [description]. Defaults to SCHEDULER_PARAMS.scheduler_params.\n\n        Returns:\n            [type]: [description]\n        \"\"\"\n\n        return getattr(torch.optim.lr_scheduler, scheduler_params.scheduler_name)(\n            optimizer=self.optimizer, **scheduler_params.scheduler_params\n        )\n\n    @staticmethod\n    def train_criterion(y_true, y_logits, criterion: CriterionParams()):\n        \"\"\"Train Loss Function.\n        Note that we can evaluate train and validation fold with different loss functions.\n\n        The below example applies for CrossEntropyLoss.\n\n        Args:\n            y_true ([type]): Input - N,C) where N = number of samples and C = number of classes.\n            y_logits ([type]): If containing class indices, shape (N) where each value is 0 \\leq \\text{targets}[i] \\leq C-10targets[i]C1\n                                If containing class probabilities, same shape as the input.\n\n        Returns:\n            [type]: [description]\n        \"\"\"\n\n        loss_fn = getattr(torch.nn, criterion.train_criterion_name)(\n            **criterion.train_criterion_params\n        )\n        loss = loss_fn(y_logits, y_true)\n        return loss\n\n\n    @staticmethod\n    def valid_criterion(y_true, y_logits, criterion: CriterionParams()):\n        \"\"\"Validation Loss Function.\n\n        Args:\n            y_true ([type]): [description]\n            y_logits ([type]): [description]\n\n        Returns:\n            [type]: [description]\n        \"\"\"\n        loss_fn = getattr(torch.nn, criterion.valid_criterion_name)(\n            **criterion.valid_criterion_params\n        )\n        loss = loss_fn(y_logits, y_true)\n        return loss\n\n    def get_classification_metrics(\n        self,\n        y_trues: torch.Tensor,\n        y_preds: torch.Tensor,\n        y_probs: torch.Tensor,\n    ):\n        \"\"\"[summary]\n\n        Args:\n            y_trues (torch.Tensor): dtype=[torch.int64], shape=(num_samples, 1); (May be float if using BCEWithLogitsLoss)\n            y_preds (torch.Tensor): dtype=[torch.int64], shape=(num_samples, 1);\n            y_probs (torch.Tensor): dtype=[torch.float32], shape=(num_samples, num_classes);\n\n        Returns:\n            [type]: [description]\n        \"\"\"\n\n        torchmetrics_accuracy = accuracy_score_torch(\n            y_trues,\n            y_preds,\n            num_classes=TRAIN_PARAMS.num_classes,\n            threshold=0.5,\n        )\n\n        auroc_dict = multiclass_roc_auc_score_torch(\n            y_trues,\n            y_probs,\n            num_classes=TRAIN_PARAMS.num_classes,\n        )\n\n        _auroc_all_classes, macro_auc = (\n            auroc_dict[\"auroc_per_class\"],\n            auroc_dict[\"macro_auc\"],\n        )\n\n        # TODO: To check robustness of the code for confusion matrix.\n        # macro_cm = metrics.tp_fp_tn_fn_binary(\n        #     y_true=y_trues, y_prob=y_probs, class_labels=[0, 1, 2, 3, 4]\n        # )\n\n        return {\"accuracy\": torchmetrics_accuracy, \"macro_auroc\": macro_auc}\n    \n    def get_regression_metrics(\n        self,\n        y_trues: torch.Tensor,\n        y_preds: torch.Tensor,\n        y_probs: torch.Tensor,\n    ):\n        ### ONLY FOR THIS COMP YOU NEED TO DENORMALIZE ###\n        y_trues = y_trues * 100\n        y_probs = y_probs * 100\n        mse = mse_torch(y_trues, y_probs, is_rmse=False)\n        rmse = mse_torch(y_trues, y_probs, is_rmse=True)\n        \n        return {\"mse\": mse, \"rmse\": rmse}\n\n    def get_lr(self, optimizer) -> float:\n        \"\"\"Get the learning rate of the current epoch.\n        Note learning rate can be different for different layers, hence the for loop.\n        Args:\n            self.optimizer (torch.optim): [description]\n        Returns:\n            float: [description]\n        \"\"\"\n        for param_group in optimizer.param_groups:\n            return param_group[\"lr\"]\n\n    def fit(\n        self,\n        train_loader: torch.utils.data.DataLoader,\n        valid_loader: torch.utils.data.DataLoader,\n        fold: int = None,\n    ):\n        \"\"\"[summary]\n\n        Args:\n            train_loader (torch.utils.data.DataLoader): [description]\n            val_loader (torch.utils.data.DataLoader): [description]\n            fold (int, optional): [description]. Defaults to None.\n\n        Returns:\n            [type]: [description]\n        \"\"\"\n\n        # To automatically log gradients\n        # self.wandb_run.watch(self.model, log_freq=100)\n        self.best_valid_loss = np.inf\n\n        config.logger.info(\n            f\"\\nTraining on Fold {fold} and using {self.params.model_name}\\n\"\n        )\n\n        for _epoch in range(1, self.params.epochs + 1):\n\n            # get current epoch's learning rate\n            curr_lr = self.get_lr(self.optimizer)\n\n            ############################ Start of Training #############################\n\n            train_start_time = time.time()\n\n            train_one_epoch_dict = self.train_one_epoch(train_loader)\n            train_loss = train_one_epoch_dict[\"train_loss\"]\n\n            # total time elapsed for this epoch\n            train_time_elapsed = time.strftime(\n                \"%H:%M:%S\", time.gmtime(time.time() - train_start_time)\n            )\n\n            config.logger.info(\n                f\"[RESULT]: Train. Epoch {_epoch} | Avg Train Summary Loss: {train_loss:.3f} | \"\n                f\"Learning Rate: {curr_lr:.10f} | Time Elapsed: {train_time_elapsed}\\n\"\n            )\n\n            ########################### End of Training #################################\n\n            ########################### Start of Validation #############################\n\n            val_start_time = time.time()  # start time for validation\n            valid_one_epoch_dict = self.valid_one_epoch(valid_loader)\n\n            (\n                valid_loss,\n                valid_trues,\n                valid_logits,\n                valid_preds,\n                valid_probs,\n            ) = (\n                valid_one_epoch_dict[\"valid_loss\"],\n                valid_one_epoch_dict[\"valid_trues\"],\n                valid_one_epoch_dict[\"valid_logits\"],\n                valid_one_epoch_dict[\"valid_preds\"],\n                valid_one_epoch_dict[\"valid_probs\"],\n            )\n\n            # total time elapsed for this epoch\n            valid_elapsed_time = time.strftime(\n                \"%H:%M:%S\", time.gmtime(time.time() - val_start_time)\n            )\n\n            valid_metrics_dict = self.get_regression_metrics(\n                valid_trues,\n                valid_preds,\n                valid_probs,\n            )\n            \n            valid_rmse = valid_metrics_dict[\"rmse\"]\n            \n            # TODO: Still need save each metric for each epoch into a list history. Rename properly\n            # TODO: Log each metric to wandb and log file.\n            \n            config.logger.info(\n                f\"[RESULT]: Validation. Epoch {_epoch} | Avg Val Summary Loss: {valid_loss:.3f} | \"\n                f\"Valid RMSE: {valid_rmse:.3f} | \"\n                f\"Time Elapsed: {valid_elapsed_time}\\n\"\n            )\n        \n            ########################### End of Validation ##############################\n\n            ########################### Start of Wandb #################################\n            self.history[\"epoch\"].append(_epoch)\n            self.history[\"valid_loss\"].append(valid_loss)\n            self.history[\"valid_rmse\"].append(valid_rmse)\n            \n            self.log_metrics(_epoch, self.history)\n            \n            ########################### End of Wandb ###################################\n\n            ########################## Start of Early Stopping ##########################\n            ########################## Start of Model Saving ############################\n\n            # User has to choose a few metrics to monitor.\n            # Here I chose valid_loss and valid_macro_auroc.\n\n            self.monitored_metric[\"metric_score\"] = torch.clone(valid_rmse)\n\n\n            if self.early_stopping is not None:\n                best_score, early_stop = self.early_stopping.should_stop(\n                    curr_epoch_score=valid_loss\n                )\n                self.best_valid_loss = best_score\n\n                if early_stop:\n                    config.logger.info(\"Stopping Early!\")\n                    break\n                # TODO: Add save_model_artifacts here as well.\n            else:\n                if valid_loss < self.best_valid_loss:\n                    self.best_valid_loss = valid_loss\n\n                if self.monitored_metric[\"mode\"] == \"max\":\n                    if (\n                        self.monitored_metric[\"metric_score\"]\n                        > self.best_valid_score\n                    ):\n                        self.best_valid_score = self.monitored_metric[\n                            \"metric_score\"\n                        ]\n                else:\n                    if (\n                        self.monitored_metric[\"metric_score\"]\n                        < self.best_valid_score\n                    ):\n                        self.best_valid_score = self.monitored_metric[\n                            \"metric_score\"\n                        ]\n                        # Reset patience counter as we found a new best score\n                        patience_counter_ = self.patience_counter\n\n                        model_path = Path(\n                            FILES.weight_path, f\"{self.params.model_name}\"\n                        )\n                        # create model directory if not exist\n                        os.makedirs(model_path, exist_ok=True)\n\n                        self.save_model_artifacts(\n                            Path(\n                                model_path,\n                                f\"{self.params.model_name}_best_{self.monitored_metric['metric_name']}_fold_{fold}.pt\",\n                            ),\n                            valid_trues,\n                            valid_logits,\n                            valid_preds,\n                            valid_probs,\n                        )\n\n                        config.logger.info(\n                            f\"\\nSaving model with best valid {self.monitored_metric['metric_name']} score: {self.best_valid_score}\"\n                        )\n                    else:    \n                        patience_counter_ -= 1\n                        config.logger.info(\n                            f\"Patience Counter {patience_counter_}\"\n                        )\n                        if patience_counter_ == 0:\n                            config.logger.info(\n                                f\"\\n\\nEarly Stopping, patience reached!\\n\\nbest valid {self.monitored_metric['metric_name']} score: {self.best_valid_score}\"\n                            )\n                            break\n                            \n            ########################## End of Early Stopping ############################\n            ########################## End of Model Saving ##############################\n\n            ########################## Start of Scheduler ###############################\n\n            if self.scheduler is not None:\n                # Special Case for ReduceLROnPlateau\n                if isinstance(\n                    self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau\n                ):\n                    self.scheduler.step(self.monitored_metric[\"metric_score\"])\n                    \n                elif isinstance(self.scheduler, torch.optim.lr_scheduler.OneCycleLR):\n                    config.logger.info(f\"Using OneCycleLR\")\n                    pass\n                else:\n                    self.scheduler.step()\n\n            ########################## End of Scheduler #################################\n\n        ########################## Load Best Model ##################################\n        # Load current checkpoint so we can get model's oof predictions, often in the form of probabilities.\n        del self.optimizer\n        del self.scheduler\n        del valid_trues\n        del valid_logits\n        del valid_preds\n        del valid_probs\n        \n        curr_fold_best_checkpoint = self.load(\n            Path(\n                model_path,\n                f\"{self.params.model_name}_best_{self.monitored_metric['metric_name']}_fold_{fold}.pt\",\n            )\n        )\n        ########################## End of Load Best Model ###########################\n\n        return curr_fold_best_checkpoint\n\n    def train_one_epoch(\n        self, train_loader: torch.utils.data.DataLoader\n    ) -> Dict[str, float]:\n        \"\"\"Train one epoch of the model.\"\"\"\n\n        metric_monitor = MetricMonitor()\n\n        # set to train mode\n        self.model.train()\n        average_cumulative_train_loss: float = 0.0\n        train_bar = tqdm(train_loader)\n\n        # Iterate over train batches\n        \n        for step, data in enumerate(train_bar, start=1):\n            is_mixup = np.random.randint(1, 10) >= 5\n            \n            if self.params.use_amp:\n                if self.params.mixup and is_mixup:\n                    # TODO: Implement MIXUP logic. Refer here: https://www.kaggle.com/ar2017/pytorch-efficientnet-train-aug-cutmix-fmix and my https://colab.research.google.com/drive/1sYkKG8O17QFplGMGXTLwIrGKjrgxpRt5#scrollTo=5y4PfmGZubYp\n                    inputs, targets = data[\"X\"], data[\"y\"].view(-1, 1)\n\n                    inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, params = TRANSFORMS.mixup_params)\n                    inputs, targets_a, targets_b = inputs.to(self.device, non_blocking=True), targets_a.to(self.device, non_blocking=True), targets_b.to(self.device, non_blocking=True)\n                else:\n                    # unpack\n                    inputs = data[\"X\"].to(self.device, non_blocking=True)\n                    # .view(-1, 1) if BCELoss\n                    targets = data[\"y\"].to(self.device, non_blocking=True).view(-1, 1)\n                    \n                self.optimizer.zero_grad()\n                with torch.cuda.amp.autocast():\n                    logits = self.model(inputs) # Forward pass logits\n                    batch_size = inputs.shape[0]\n                    if self.params.mixup and is_mixup:\n                        curr_batch_train_loss=mixup_criterion(torch.nn.BCEWithLogitsLoss(), logits, targets_a, targets_b, lam)\n                    else:\n                        #criterion = RMSELoss()\n                        #curr_batch_train_loss = criterion(logits, targets)\n                        curr_batch_train_loss = self.train_criterion(\n                            targets,\n                            logits,\n                            CRITERION_PARAMS,\n                        )\n                self.scaler.scale(curr_batch_train_loss).backward()\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n                average_cumulative_train_loss += (\n                    curr_batch_train_loss.detach().item()\n                    - average_cumulative_train_loss\n                ) / (step)\n                \n            else:\n                # unpack\n                inputs = data[\"X\"].to(self.device, non_blocking=True)\n                # .view(-1, 1) if BCELoss\n                targets = data[\"y\"].to(self.device, non_blocking=True)\n                batch_size = inputs.shape[0]\n                logits = self.model(inputs)  # Forward pass logits\n                self.optimizer.zero_grad()  # reset gradients\n                curr_batch_train_loss = self.train_criterion(\n                    targets,\n                    logits,\n                    CRITERION_PARAMS,\n                )\n                curr_batch_train_loss.backward()  # Backward pass\n                # Update loss metric\n                metric_monitor.update(\"Loss\", curr_batch_train_loss.item())\n                train_bar.set_description(f\"Train. {metric_monitor}\")\n\n                self.optimizer.step()  # Update weights using the optimizer\n\n                # Cumulative Loss\n                # Batch/Step 1: curr_batch_train_loss = 10 -> average_cumulative_train_loss = (10-0)/1 = 10\n                # Batch/Step 2: curr_batch_train_loss = 12 -> average_cumulative_train_loss = 10 + (12-10)/2 = 11 (Basically (10+12)/2=11)\n                # Essentially, average_cumulative_train_loss = loss over all batches / batches\n                average_cumulative_train_loss += (\n                    curr_batch_train_loss.detach().item()\n                    - average_cumulative_train_loss\n                ) / (step)\n                \n                \n            y_train_prob = logits.sigmoid()\n            self.scheduler.step()\n\n        # self.log_weights(step)\n        # running loss\n        # self.log_scalar(\"running_train_loss\", curr_batch_train_loss.data.item(), step)\n        # TODO: Consider enhancement that returns the same dict as valid_one_epoch.\n        return {\"train_loss\": average_cumulative_train_loss}\n\n    # @torch.no_grad\n    def valid_one_epoch(\n        self, valid_loader: torch.utils.data.DataLoader\n    ) -> Dict[str, Union[float, np.ndarray]]:\n        \"\"\"Validate the model on the validation set for one epoch.\n\n        Args:\n            valid_loader (torch.utils.data.DataLoader): The validation set dataloader.\n\n        Returns:\n            Dict[str, np.ndarray]:\n                valid_loss (float): The validation loss for each epoch.\n                valid_trues (np.ndarray): The ground truth labels for each validation set. shape = (num_samples, 1)\n                valid_logits (np.ndarray): The logits for each validation set. shape = (num_samples, num_classes)\n                valid_preds (np.ndarray): The predicted labels for each validation set. shape = (num_samples, 1)\n                valid_probs (np.ndarray): The predicted probabilities for each validation set. shape = (num_samples, num_classes)\n        \"\"\"\n\n        self.model.eval()  # set to eval mode\n        metric_monitor = MetricMonitor()\n        average_cumulative_valid_loss: float = 0.0\n        valid_bar = tqdm(valid_loader)\n\n        valid_logits, valid_trues, valid_preds, valid_probs = [], [], [], []\n\n        with torch.no_grad():\n            for step, data in enumerate(valid_bar, start=1):\n                # unpack\n                inputs = data[\"X\"].to(self.device, non_blocking=True)\n                targets = data[\"y\"].to(self.device, non_blocking=True).view(-1,1)\n\n                self.optimizer.zero_grad()  # reset gradients\n\n                logits = self.model(inputs)  # Forward pass logits\n                # print(f\"valid_logits.shape: {logits.shape}\")\n\n                # get batch size, may not be same as params.batch_size due to whether drop_last in loader is True or False.\n                batch_size = inputs.shape[0]\n                # assert targets.size() == (batch_size,)\n                # assert logits.size() == (batch_size, TRAIN_PARAMS.num_classes)\n\n                # TODO: Refer to my RANZCR notes on difference between Softmax and Sigmoid with examples.\n                y_valid_prob = logits.sigmoid()\n                y_valid_pred = torch.argmax(y_valid_prob, axis=1)\n             \n                #criterion = RMSELoss()\n                #curr_batch_val_loss = criterion(logits, targets)\n                curr_batch_val_loss = self.valid_criterion(targets, logits, CRITERION_PARAMS)\n                \n                average_cumulative_valid_loss += (\n                    curr_batch_val_loss.item() - average_cumulative_valid_loss\n                ) / (step)\n                valid_bar.set_description(f\"Validation. Loss: {metric_monitor}\")\n\n                # For OOF score and other computation.\n                # TODO: Consider giving numerical example. Consider rolling back to targets.cpu().numpy() if torch fails.\n                valid_trues.extend(targets.cpu())\n                valid_logits.extend(logits.cpu())\n                valid_preds.extend(y_valid_pred.cpu())\n                valid_probs.extend(y_valid_prob.cpu())\n        \n        # We should work with numpy arrays.\n        # vstack here to stack the list of numpy arrays.\n        # a = [np.asarray([1,2,3]), np.asarray([4,5,6])]\n        # np.vstack(a) -> array([[1, 2, 3], [4, 5, 6]])\n        valid_trues, valid_logits, valid_preds, valid_probs = (\n            torch.vstack(valid_trues),\n            torch.vstack(valid_logits),\n            torch.vstack(valid_preds),\n            torch.vstack(valid_probs),\n        )\n        num_valid_samples = len(valid_trues)\n        assert valid_trues.shape == valid_preds.shape == (num_valid_samples, 1)\n        assert (\n            valid_logits.shape\n            == valid_probs.shape\n            == (num_valid_samples, TRAIN_PARAMS.num_classes)\n        )\n\n        return {\n            \"valid_loss\": average_cumulative_valid_loss,\n            \"valid_trues\": valid_trues,\n            \"valid_logits\": valid_logits,\n            \"valid_preds\": valid_preds,\n            \"valid_probs\": valid_probs,\n        }\n\n    def log_metrics(\n        self, epoch: int, history: Dict[str, Union[float, np.ndarray]]\n    ):\n        \"\"\"Log a scalar value to both MLflow and TensorBoard\n        Args:\n            history (Dict[str, Union[float, np.ndarray]]): A dictionary of metrics to log.\n        \"\"\"\n        for metric_name, metric_values in history.items():\n            self.wandb_run.log(\n                {metric_name: metric_values[epoch - 1]}, step=epoch\n            )\n        # self.wandb_run.log(history)\n\n\n    def save_model(self, path: str) -> None:\n        \"\"\"Save the trained model.\"\"\"\n        self.model.eval()\n        torch.save(self.model.state_dict(), path)\n\n    def save_model_artifacts(\n        self,\n        path: str,\n        valid_trues: torch.Tensor,\n        valid_logits: torch.Tensor,\n        valid_preds: torch.Tensor,\n        valid_probs: torch.Tensor,\n    ) -> None:\n        \"\"\"Save the weight for the best evaluation metric and also the OOF scores.\n        valid_trues -> oof_trues: np.array of shape [num_samples, 1] and represent the true labels for each sample in current fold.\n                                i.e. oof_trues.flattened()[i] = true label of sample i in current fold.\n        valid_logits -> oof_logits: np.array of shape [num_samples, num_classes] and represent the logits for each sample in current fold.\n                                i.e. oof_logits[i] = [logit_of_sample_i_in_current_fold_for_class_0, logit_of_sample_i_in_current_fold_for_class_1, ...]\n        valid_preds -> oof_preds: np.array of shape [num_samples, 1] and represent the predicted labels for each sample in current fold.\n                                i.e. oof_preds.flattened()[i] = predicted label of sample i in current fold.\n        valid_probs -> oof_probs: np.array of shape [num_samples, num_classes] and represent the probabilities for each sample in current fold. i.e. first row is the probabilities of the first class.\n                                i.e. oof_probs[i] = [probability_of_sample_i_in_current_fold_for_class_0, probability_of_sample_i_in_current_fold_for_class_1, ...]\n        \"\"\"\n        # self.model.eval()\n\n        torch.save(\n            {\n                \"model_state_dict\": self.model.state_dict(),\n                \"optimizer_state_dict\": self.optimizer.state_dict(),\n                \"scheduler_state_dict\": self.scheduler.state_dict(),\n                \"oof_trues\": valid_trues,\n                \"oof_logits\": valid_logits,\n                \"oof_preds\": valid_preds,\n                \"oof_probs\": valid_probs,\n            },\n            path,\n        )\n\n    @staticmethod\n    def load(path: str):\n        \"\"\"Load a model checkpoint from the given path.\n        Reason for using a static method: https://stackoverflow.com/questions/70052073/am-i-using-static-method-correctly/70052107#70052107\n        \"\"\"\n        checkpoint = torch.load(path, map_location=torch.device(\"cpu\"))\n        return checkpoint\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:41.296782Z","iopub.execute_input":"2021-12-17T11:29:41.297149Z","iopub.status.idle":"2021-12-17T11:29:41.395062Z","shell.execute_reply.started":"2021-12-17T11:29:41.297109Z","shell.execute_reply":"2021-12-17T11:29:41.394331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Weights & Biases\n\n> Not an ambassador, but I guess marketing down right is how I would describe this company. There are many MLOps platform, but W&B has so many example notebooks online so I conveniently copy what others do. After all, it is much easier to stand on the shoulders of giants :)","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# wandb\n# ====================================================\n\nif not is_inference:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    wandb_api = user_secrets.get_secret(\"Petfinder\")\n\n    import wandb\n    wandb.login(key=wandb_api)\n    \n\ndef wandb_init(fold: int):\n    config = {\n        \"Train_Params\": TRAIN_PARAMS.to_dict(),\n        \"Model_Params\": MODEL_PARAMS.to_dict(),\n    }\n    wandb_run = wandb.init(\n        config=config,\n        name=f\"{TRAIN_PARAMS.model_name}_fold_{fold}\",\n        **WANDB_PARAMS.to_dict(),\n    )\n    return wandb_run","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:41.397936Z","iopub.execute_input":"2021-12-17T11:29:41.398138Z","iopub.status.idle":"2021-12-17T11:29:43.395884Z","shell.execute_reply.started":"2021-12-17T11:29:41.398113Z","shell.execute_reply":"2021-12-17T11:29:43.395127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_one_fold(df_folds: pd.DataFrame, fold: int, is_plot: bool = False):\n    \"\"\"Train the model on the given fold.\"\"\"\n\n    ################################## W&B #####################################\n    # wandb.login()\n    wandb_run = wandb_init(fold=fold)\n\n    train_loader, valid_loader, df_oof = prepare_loaders(df_folds, fold)\n\n    if is_plot:\n        image_grid = plot.show_image(\n            loader=train_loader,\n            nrows=1,\n            ncols=1,\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n        )\n        # writer.add_image(\"name\", image_grid)\n\n    # Model, cost function and optimizer instancing\n    # model = CustomNeuralNet().to(device)\n    model = dognet\n\n    reighns_trainer: Trainer = Trainer(\n        params=TRAIN_PARAMS,\n        model=model,\n        device=device,\n        wandb_run=wandb_run,\n    )\n\n    curr_fold_best_checkpoint = reighns_trainer.fit(\n        train_loader, valid_loader, fold\n    )\n\n    # TODO: Note that for sigmoid on one class, the OOF score is the positive class.\n    df_oof[[str(c) + \"_oof\" for c in range(TRAIN_PARAMS.num_classes)]] = (\n        curr_fold_best_checkpoint[\"oof_probs\"].detach().numpy()\n    )\n    df_oof[\"oof_trues\"] = curr_fold_best_checkpoint[\"oof_trues\"]\n    # df_oof['error_analysis'] = todo - sort the dataframe by the ones that the model got wrong to see where they are focusing.\n    # df_oof[\"oof_preds\"] = curr_fold_best_checkpoint[\"oof_preds\"].argmax(1)\n\n    df_oof.to_csv(Path(FILES.oof_csv, f\"oof_fold_{fold}.csv\"), index=False)\n    del model\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    wandb_run.finish()  # Finish the run to start next fold.\n\n    return df_oof\n\n\ndef train_loop(df_folds: pd.DataFrame, is_plot: bool = False):\n    \"\"\"Perform the training loop on all folds. Here The CV score is the average of the validation fold metric.\n    While the OOF score is the aggregation of all validation folds.\"\"\"\n\n    cv_score_list = []\n    df_oof = pd.DataFrame()\n\n    for fold in range(1, FOLDS.num_folds + 1):\n        if fold == 1:\n            _df_oof = train_one_fold(df_folds=df_folds, fold=fold, is_plot=is_plot)\n            df_oof = pd.concat([df_oof, _df_oof])\n\n        # TODO: populate the cv_score_list using a dataframe like breast cancer project.\n        # curr_fold_best_score_dict, curr_fold_best_score = get_oof_roc(config, _oof_df)\n        # cv_score_list.append(curr_fold_best_score)\n        # print(\"\\n\\n\\nOOF Score for Fold {}: {}\\n\\n\\n\".format(fold, curr_fold_best_score))\n\n    # print(\"CV score\", np.mean(cv_score_list))\n    # print(\"Variance\", np.var(cv_score_list))\n    # print(\"Five Folds OOF\", get_oof_roc(config, oof_df))\n\n    df_oof.to_csv(Path(FILES.oof_csv, \"oof.csv\"), index=False)\n    return df_oof","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:43.397199Z","iopub.execute_input":"2021-12-17T11:29:43.397494Z","iopub.status.idle":"2021-12-17T11:29:43.409786Z","shell.execute_reply.started":"2021-12-17T11:29:43.397457Z","shell.execute_reply":"2021-12-17T11:29:43.409093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The idea of normalizing the targets to between 0 and 1 is to conform to the idea of using BinaryCrossEntropy Loss. Technically, our target should only be 0 or 1 in a \"binary setting\", but we are pulling a sleight of hands to still treat the problem as a classification problem, and although if you predict a label with 100% probability and the label is indeed 1, your BCE loss will be 0, but it will not be the same here, but regardless, if your target is 0.6, and your prediction is 0.6, then the loss will be guaranteed to be the lowest if it predicts anything other than 0.6, though it won't be 0.","metadata":{}},{"cell_type":"code","source":"# Examples:\n\nloss = torch.nn.BCEWithLogitsLoss()\ninput = torch.tensor([0.6])\ntarget = torch.tensor([0.6])\noutput = loss(input, target)\nprint(output) # -> this should be the minimum of the loss function if the prediction==target if I am not wrong.","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:29:43.411202Z","iopub.execute_input":"2021-12-17T11:29:43.41213Z","iopub.status.idle":"2021-12-17T11:29:43.449084Z","shell.execute_reply.started":"2021-12-17T11:29:43.41209Z","shell.execute_reply":"2021-12-17T11:29:43.448257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"def inference_all_folds(\n    model: CustomNeuralNet,\n    state_dicts: List[collections.OrderedDict],\n    test_loader: torch.utils.data.DataLoader,\n) -> np.ndarray:\n    \"\"\"Inference the model on all K folds.\n    Args:\n        model (models.CustomNeuralNet): The model to be used for inference. Note that pretrained should be set to False.\n        state_dicts (List[collections.OrderedDict]): The state dicts of the models. Generally, K Fold means K state dicts.\n        test_loader (torch.utils.data.DataLoader): The dataloader for the test set.\n    Returns:\n        mean_cv_preds (np.ndarray): The mean of the predictions of all folds.\n    \"\"\"\n\n    model.to(device)\n    model.eval()\n\n    with torch.no_grad():\n        all_folds_preds = []\n\n        for _fold_num, state in enumerate(state_dicts):\n            if \"model_state_dict\" not in state:\n                model.load_state_dict(state)\n            else:\n                model.load_state_dict(state[\"model_state_dict\"])\n\n            current_fold_preds = []\n\n            for data in tqdm(test_loader, position=0, leave=True):\n                images = data[\"X\"].to(device)\n                logits = model(images)\n                y_test_prob = logits.sigmoid().cpu().numpy()\n                # Need to multiply the predictions by 100 since we divide by 100 in the training script.\n                y_test_prob = y_test_prob * 100\n\n                current_fold_preds.append(y_test_prob)\n\n            current_fold_preds = np.concatenate(current_fold_preds, axis=0)\n            all_folds_preds.append(current_fold_preds)\n        mean_cv_preds = np.mean(all_folds_preds, axis=0)\n    return mean_cv_preds\n\n\ndef inference(\n    df_test: pd.DataFrame,\n    model_dir: str,\n    df_sub: pd.DataFrame = None,\n) -> Dict[str, np.ndarray]:\n    \"\"\"Inference the model and perform TTA, if any.\n    Dataset and Dataloader are constructed within this function because of TTA.\n    Args:\n        df_test (pd.DataFrame): The test dataframe.\n        model_dir (str): model directory for the model.\n        df_sub (pd.DataFrame, optional): The submission dataframe. Defaults to None.\n    Returns:\n        all_preds (Dict[str, np.ndarray]): {\"normal\": normal_preds, \"tta\": tta_preds}\n    \"\"\"\n    # Model, cost function and optimizer instancing\n\n    model = CustomNeuralNet(pretrained=False).to(device)\n\n    all_preds = {}\n    transform_dict = get_inference_transforms()\n\n    # weights = [model_path for model_path in glob.glob(model_dir + \"/*.pt\")]\n    weights = [\n        \"../input/petfinder-flexible-pytorch-pipeline/model/swin_large_patch4_window7_224/swin_large_patch4_window7_224_best_valid_rmse_fold_1.pt\",\n        \"../input/petfinder-flexible-pytorch-pipeline/model/swin_large_patch4_window7_224/swin_large_patch4_window7_224_best_valid_rmse_fold_2.pt\",\n        \"../input/petfinder-flexible-pytorch-pipeline/model/swin_large_patch4_window7_224/swin_large_patch4_window7_224_best_valid_rmse_fold_3.pt\",\n        \"../input/petfinder-flexible-pytorch-pipeline/model/swin_large_patch4_window7_224/swin_large_patch4_window7_224_best_valid_rmse_fold_4.pt\",\n        \"../input/petfinder-flexible-pytorch-pipeline/model/swin_large_patch4_window7_224/swin_large_patch4_window7_224_best_valid_rmse_fold_5.pt\"\n    ]\n    state_dicts = [torch.load(path)[\"model_state_dict\"] for path in weights]\n\n\n    for aug, aug_param in transform_dict.items():\n        test_dataset = CustomDataset(\n            df=df_test, transforms=aug_param, mode=\"test\"\n        )\n        test_loader = torch.utils.data.DataLoader(\n            test_dataset, **LOADER_PARAMS.test_loader\n        )\n        predictions = inference_all_folds(\n            model=model, state_dicts=state_dicts, test_loader=test_loader\n        )\n\n        all_preds[aug] = predictions\n\n        df_sub[FOLDS.class_col_name] = predictions # np.argmax(predictions, axis=1)\n        \n        df_sub[[FOLDS.image_col_name, FOLDS.class_col_name]].to_csv(\n            f\"submission.csv\", index=False\n        )\n        \n        df_sub.head()\n\n        plt.figure(figsize=(12, 6))\n        plt.hist(df_sub[FOLDS.class_col_name], bins=100)\n    \n#     df_sub[FOLDS.class_col_name] = all_preds[\"transforms_test\"] * 0.5 + all_preds[\"tta_hflip\"] * 0.5\n#     df_sub[[FOLDS.image_col_name, FOLDS.class_col_name]].to_csv(\n#     f\"submission.csv\", index=False\n#     )\n        \n    return all_preds","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:43.45064Z","iopub.execute_input":"2021-12-17T11:29:43.451101Z","iopub.status.idle":"2021-12-17T11:29:43.467691Z","shell.execute_reply.started":"2021-12-17T11:29:43.45106Z","shell.execute_reply":"2021-12-17T11:29:43.466852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference with SVR","metadata":{}},{"cell_type":"code","source":"# def inference_all_folds(\n#     model: CustomNeuralNet,\n#     state_dicts: List[collections.OrderedDict],\n#     test_loader: torch.utils.data.DataLoader,\n#     svr_state_dicts = None\n# ) -> np.ndarray:\n#     \"\"\"Inference the model on all K folds.\n#     Args:\n#         model (models.CustomNeuralNet): The model to be used for inference. Note that pretrained should be set to False.\n#         state_dicts (List[collections.OrderedDict]): The state dicts of the models. Generally, K Fold means K state dicts.\n#         test_loader (torch.utils.data.DataLoader): The dataloader for the test set.\n#     Returns:\n#         mean_cv_preds (np.ndarray): The mean of the predictions of all folds.\n#     \"\"\"\n\n#     model.to(device)\n#     model.eval()\n\n#     with torch.no_grad():\n#         all_folds_preds = []\n#         all_folds_svr_preds = []\n\n#         for _fold_num, (state, svr_state) in enumerate(zip(state_dicts, svr_state_dicts)):\n#             if \"model_state_dict\" not in state:\n#                 model.load_state_dict(state)\n#             else:\n#                 model.load_state_dict(state[\"model_state_dict\"])\n            \n\n#             ##################\n#             # LOAD RAPIDS SVR \n#             print('Loading SVR...')\n#             svr = pickle.load(open(svr_state, \"rb\"))\n\n#             current_fold_preds = []\n#             current_fold_svr_preds = []\n\n#             for data in tqdm(test_loader, position=0, leave=True):\n#                 images = data[\"X\"].to(device)\n#                 # Additional steps for SVR\n#                 embeddings = model.extract_features(images)\n#                 svr_preds = svr.predict(embeddings.cpu().numpy())\n#                 logits = model(images)\n#                 y_test_prob = logits.sigmoid().cpu().numpy()\n#                 # Need to multiply the predictions by 100 since we divide by 100 in the training script.\n#                 y_test_prob = y_test_prob * 100\n\n#                 current_fold_preds.append(y_test_prob)\n#                 current_fold_svr_preds.append(svr_preds)\n\n#             current_fold_preds = np.concatenate(current_fold_preds, axis=0)\n#             current_fold_svr_preds = np.concatenate(current_fold_svr_preds, axis=0)\n#             all_folds_preds.append(current_fold_preds)\n#             all_folds_svr_preds.append(current_fold_svr_preds)\n#         mean_cv_preds = np.mean(all_folds_preds, axis=0)\n#         mean_cv_svr_preds = np.mean(all_folds_svr_preds, axis=0)\n#     return mean_cv_preds, mean_cv_svr_preds\n\n\n# def inference(\n#     df_test: pd.DataFrame,\n#     model_dir: str,\n#     df_sub: pd.DataFrame = None,\n# ) -> Dict[str, np.ndarray]:\n#     \"\"\"Inference the model and perform TTA, if any.\n#     Dataset and Dataloader are constructed within this function because of TTA.\n#     Args:\n#         df_test (pd.DataFrame): The test dataframe.\n#         model_dir (str): model directory for the model.\n#         df_sub (pd.DataFrame, optional): The submission dataframe. Defaults to None.\n#     Returns:\n#         all_preds (Dict[str, np.ndarray]): {\"normal\": normal_preds, \"tta\": tta_preds}\n#     \"\"\"\n#     # Model, cost function and optimizer instancing\n\n#     model = CustomNeuralNet(pretrained=False).to(device)\n\n#     all_preds = {}\n#     transform_dict = get_inference_transforms()\n\n#     # weights = [model_path for model_path in glob.glob(model_dir + \"/*.pt\")]\n    \n#     weights = [\"../input/swin-large-petfinder-hn/swin_large_patch4_window7_224_best_valid_rmse_fold_1.pt\",\n#               \"../input/swin-large-petfinder-hn/swin_large_patch4_window7_224_best_valid_rmse_fold_2.pt\",\n#               \"../input/swin-large-petfinder-hn/swin_large_patch4_window7_224_best_valid_rmse_fold_3.pt\",\n#               \"../input/swin-large-petfinder-hn/swin_large_patch4_window7_224_best_valid_rmse_fold_4.pt\",\n#               \"../input/swin-large-petfinder-hn/swin_large_patch4_window7_224_best_valid_rmse_fold_5.pt\"]\n\n#     SVR_weights = [\"../input/swin-large-petfinder-hn/SVR_fold_1.pkl\", \n#                    \"../input/swin-large-petfinder-hn/SVR_fold_2.pkl\", \n#                    \"../input/swin-large-petfinder-hn/SVR_fold_3.pkl\", \n#                    \"../input/swin-large-petfinder-hn/SVR_fold_4.pkl\", \n#                    \"../input/swin-large-petfinder-hn/SVR_fold_5.pkl\"]\n    \n#     state_dicts = [torch.load(path)[\"model_state_dict\"] for path in weights]\n\n#     for aug, aug_param in transform_dict.items():\n#         test_dataset = CustomDataset(\n#             df=df_test, transforms=aug_param, mode=\"test\"\n#         )\n#         test_loader = torch.utils.data.DataLoader(\n#             test_dataset, **LOADER_PARAMS.test_loader\n#         )\n#         predictions, svr_predictions = inference_all_folds(\n#             model=model, \n#             state_dicts=state_dicts,\n#             test_loader=test_loader, \n#             svr_state_dicts=SVR_weights\n#         )\n        \n        \n#         all_preds[aug] = predictions * 0.8 + svr_predictions * 0.2\n\n#         df_sub[FOLDS.class_col_name] = predictions * 0.8 + svr_predictions * 0.2 # np.argmax(predictions, axis=1)\n\n#         df_sub[[FOLDS.image_col_name, FOLDS.class_col_name]].to_csv(\n#             f\"submission.csv\", index=False\n#         )\n#         df_sub.head()\n\n#         plt.figure(figsize=(12, 6))\n#         plt.hist(df_sub[FOLDS.class_col_name], bins=100)\n#     return all_preds","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:43.469436Z","iopub.execute_input":"2021-12-17T11:29:43.470028Z","iopub.status.idle":"2021-12-17T11:29:43.481838Z","shell.execute_reply.started":"2021-12-17T11:29:43.469988Z","shell.execute_reply":"2021-12-17T11:29:43.480871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVR Head Fitting/Training","metadata":{}},{"cell_type":"code","source":"import cuml, pickle\nfrom cuml.svm import SVR\nprint('RAPIDS version',cuml.__version__,'\\n')\n\n# NUM_DENSE = 0\n\n# LOAD_SVR_FROM_PATH = \"../input/swin-large-petfinder-hn/\"\n# # If training define LOAD_SVR_FROM_PATH = None\n\n\n# super_final_predictions = []\n# super_final_predictions2 = []\n# super_final_oof_predictions = []\n# super_final_oof_predictions2 = []\n# super_final_oof_true = []\n# batch_size = 4\n# # weights = [model_path for model_path in glob.glob(model_dir + \"/*.pt\")]\n\n# weights = [\"../input/swin-large-petfinder-hn/swin_large_patch4_window7_224_best_valid_rmse_fold_1.pt\",\n#           \"../input/swin-large-petfinder-hn/swin_large_patch4_window7_224_best_valid_rmse_fold_2.pt\",\n#           \"../input/swin-large-petfinder-hn/swin_large_patch4_window7_224_best_valid_rmse_fold_3.pt\",\n#           \"../input/swin-large-petfinder-hn/swin_large_patch4_window7_224_best_valid_rmse_fold_4.pt\",\n#           \"../input/swin-large-petfinder-hn/swin_large_patch4_window7_224_best_valid_rmse_fold_5.pt\"]\n    \n\n# # Utimately we want embeddings to be shape (n_samples, n_embeddings)\n# # SO in my case if n_embeddings=1536, then if 100 samples, our final shape is (100, 1536)\n# for index, fold_ in enumerate(range(1, FOLDS.num_folds+1)):\n#     svr_name = f\"SVR_fold_{fold_}.pkl\"\n#     print('#'*25)\n#     print('### FOLD',fold_)\n#     print('#'*25)\n#     if LOAD_SVR_FROM_PATH is None:\n#         df_valid = df_folds[df_folds[\"fold\"] != fold_].reset_index(drop=True)\n#         # Here hard code the embeddings first, as ultimately we want (n_samples, 1536)\n#         embeddings = torch.zeros(df_valid.shape[0], 1536)\n\n#         model = CustomNeuralNet(pretrained=False).to(device)\n\n#         valid_transforms = get_inference_transforms()[\"transforms_test\"]\n\n#         state_dict = torch.load(weights[index])[\"model_state_dict\"]\n#         model.load_state_dict(state_dict)\n#         model.eval() # Think if u extract embeddings should set to eval too\n\n#         valid_dataset = CustomDataset(\n#             df=df_valid, transforms=valid_transforms, mode=\"train\"\n#         )\n#         valid_loader = torch.utils.data.DataLoader(\n#             valid_dataset, shuffle=False, batch_size = batch_size\n#         )\n#         row, col = 0, batch_size\n#         with torch.no_grad():\n#             for data in tqdm(valid_loader, position=0, leave=True):\n#                 images = data[\"X\"].to(device)\n#                 targets = data[\"y\"].to(device)\n#                 # SWIN returns 1536 as embeddings, depends on how you take it it can be flattened or not yet\n#                 curr_batch_embeds = model.extract_features(images)\n#                 embeddings[row:col] = curr_batch_embeds\n#                 row, col = row + batch_size, col + batch_size\n\n\n#         ##################\n#         # FIT RAPIDS SVR\n#         print('Fitting SVR...')\n#         clf = SVR(C=20.0)\n#         clf.fit(embeddings.cpu().detach().numpy().astype('float32'), df_valid.Pawpularity.values.astype('int32'))\n\n#         ##################\n#         # SAVE RAPIDS SVR \n#         pickle.dump(clf, open(svr_name, \"wb\"))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T11:29:43.484414Z","iopub.execute_input":"2021-12-17T11:29:43.484725Z","iopub.status.idle":"2021-12-17T11:29:46.156195Z","shell.execute_reply.started":"2021-12-17T11:29:43.484686Z","shell.execute_reply":"2021-12-17T11:29:46.1554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\n\ndef calc_oof(y_true, y_pred):\n    \"\"\"\n    Calculate out of fold metric\n    \"\"\"\n    rmse = metrics.mean_squared_error(y_true, y_pred, squared=False)\n    return rmse","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:29:46.15766Z","iopub.execute_input":"2021-12-17T11:29:46.15809Z","iopub.status.idle":"2021-12-17T11:29:46.163337Z","shell.execute_reply.started":"2021-12-17T11:29:46.158051Z","shell.execute_reply":"2021-12-17T11:29:46.162042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    if not is_inference:\n        df_oof = train_loop(df_folds=df_folds, is_plot=False)\n        oof_cv = calc_oof(df_oof[\"oof_trues\"].values, df_oof[\"0_oof\"].values)\n        config.logger.info(f\"OOF RMSE: {oof_cv}\")\n        torch.cuda.empty_cache()\n    else:\n        model_dir = r\"C:\\Users\\reighns\\kaggle_projects\\cassava\\model\\tf_efficientnet_b0_ns\"\n        predictions = inference(df_test, model_dir, df_sub)\n        # _ = inference.show_gradcam()","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:29:46.164933Z","iopub.execute_input":"2021-12-17T11:29:46.165334Z","iopub.status.idle":"2021-12-17T11:39:39.380812Z","shell.execute_reply.started":"2021-12-17T11:29:46.165295Z","shell.execute_reply":"2021-12-17T11:39:39.377884Z"},"trusted":true},"execution_count":null,"outputs":[]}]}