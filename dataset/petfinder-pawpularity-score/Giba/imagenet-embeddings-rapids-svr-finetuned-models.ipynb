{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport os\n\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\nimport timm\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nimport pandas as pd\nimport numpy as np\nimport glob\nimport cv2\nimport matplotlib.pyplot as plt\nimport joblib\nimport gc\nfrom glob import glob\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\nimport timm\nfrom PIL import Image\nimport PIL\n\nfrom tqdm import tqdm\nimport joblib\nimport time\nfrom tqdm.notebook import tqdm\nimport joblib\nfrom sklearn.model_selection import StratifiedKFold\n\nimport cuml\n\nprint(np.__version__)\nprint(pd.__version__)\nprint(torch.__version__)\nprint(timm.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-18T16:49:29.453743Z","iopub.execute_input":"2022-01-18T16:49:29.454116Z","iopub.status.idle":"2022-01-18T16:49:43.200874Z","shell.execute_reply.started":"2022-01-18T16:49:29.454019Z","shell.execute_reply":"2022-01-18T16:49:43.199729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Install OpenAI CLIP","metadata":{}},{"cell_type":"code","source":"!pip install ../input/openaiclipweights/python-ftfy-master/python-ftfy-master\n!pip install ../input/openaiclipweights/clip/CLIP\n!cp ../input/openaiclipweights/CLIP-main/CLIP-main/clip/bpe_simple_vocab_16e6.txt /opt/conda/lib/python3.7/site-packages/clip/.\n!gzip -k /opt/conda/lib/python3.7/site-packages/clip/bpe_simple_vocab_16e6.txt\n\nimport clip","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:49:46.764827Z","iopub.execute_input":"2022-01-18T16:49:46.765664Z","iopub.status.idle":"2022-01-18T16:51:24.891067Z","shell.execute_reply.started":"2022-01-18T16:49:46.765615Z","shell.execute_reply":"2022-01-18T16:51:24.889744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Train and Test","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/petfinderdata/train-folds-1.csv')\ntest = pd.read_csv('../input/petfinder-pawpularity-score/test.csv')\nsub = pd.read_csv('../input/petfinder-pawpularity-score/sample_submission.csv')\n\ntrain['path'] = train['Id'].map(lambda x: '../input/petfinder-pawpularity-score/train/'+x+'.jpg')\ntest['path'] = test['Id'].map(lambda x: '../input/petfinder-pawpularity-score/test/'+x+'.jpg')\n\n# If its Public LB run, then augment Testset to chack batch size memory consumption.\nif test.shape[0]<10:\n    test = pd.concat([\n        test, test, test, test, test, \n    ])\n    test = test.reset_index(drop=True)\n\nprint(train.shape, test.shape, sub.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:51:24.894698Z","iopub.execute_input":"2022-01-18T16:51:24.895591Z","iopub.status.idle":"2022-01-18T16:51:25.018685Z","shell.execute_reply.started":"2022-01-18T16:51:24.895532Z","shell.execute_reply":"2022-01-18T16:51:25.017749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Just create K Folds and check target consistency accross folds","metadata":{}},{"cell_type":"code","source":"train['bins'] = (train['Pawpularity']//5).round()\n\ntrain['fold0'] = -1\nskf = StratifiedKFold(n_splits = 20, shuffle=True, random_state = 1)\nfor i, (_, test_index) in enumerate(skf.split(train.index, train['bins'])):\n    train.iloc[test_index, -1] = i\n\ntrain['fold0'] = train['fold0'].astype('int')\ngc.collect()\n\ntrain.groupby(['fold0'])['Pawpularity'].agg(['mean','std','count'])","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:51:25.023695Z","iopub.execute_input":"2022-01-18T16:51:25.026374Z","iopub.status.idle":"2022-01-18T16:51:25.402876Z","shell.execute_reply.started":"2022-01-18T16:51:25.026328Z","shell.execute_reply":"2022-01-18T16:51:25.401887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:51:25.405778Z","iopub.execute_input":"2022-01-18T16:51:25.406102Z","iopub.status.idle":"2022-01-18T16:51:25.425218Z","shell.execute_reply.started":"2022-01-18T16:51:25.406061Z","shell.execute_reply":"2022-01-18T16:51:25.424079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:51:25.427169Z","iopub.execute_input":"2022-01-18T16:51:25.427922Z","iopub.status.idle":"2022-01-18T16:51:25.449153Z","shell.execute_reply.started":"2022-01-18T16:51:25.427865Z","shell.execute_reply":"2022-01-18T16:51:25.448006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lest check all models available in timm library","metadata":{}},{"cell_type":"code","source":"avail_pretrained_models = timm.list_models(pretrained=True)\nlen(avail_pretrained_models), avail_pretrained_models","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:51:25.45115Z","iopub.execute_input":"2022-01-18T16:51:25.451796Z","iopub.status.idle":"2022-01-18T16:51:25.476115Z","shell.execute_reply.started":"2022-01-18T16:51:25.451752Z","shell.execute_reply":"2022-01-18T16:51:25.475082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# As we can see there are 575 pretrained model architectures available in timm library.\n# The first part of the solution is basically extract the features from the last layer of that models and run a SVR on that extracted features.\n# Most of the models in timm are trained using 1000 classes in imagenet, so output shape is 1000 for each model.\n# Extracting features from all 575 models is something crazy and unthinkable, specially taking into account submission time of 9h. So the idea is to find a subset of models (from that 575) that performs well in terms of RMSE.\n# To do that it was used a forward models selection algorithm, following by RMSE hill climbing logic. Starting with one model, then keep adding models until it stop increasing RMSE performance.\n","metadata":{}},{"cell_type":"markdown","source":"# Now lets start to extract imagenet pretrained models features","metadata":{}},{"cell_type":"markdown","source":"# The pretrained models found by the forward model selection algorithm used in this solution are listed above.","metadata":{}},{"cell_type":"code","source":"names = [\n    'deit_base_distilled_patch16_384',\n    #'fbnetc_100',\n    #'ig_resnext101_32x8d',\n    'ig_resnext101_32x48d',\n    'repvgg_b0',\n    'resnetv2_152x4_bitm',\n    #'rexnet_200',\n    #'resnest269e',\n    'swsl_resnext101_32x8d',\n    #'tf_efficientnet_b6_ns',\n    #'tf_efficientnet_b7_ns',\n    #'tf_efficientnet_b8_ap',\n    'tf_efficientnet_l2_ns_475',\n    'vit_base_patch16_384',\n    #'vit_large_patch16_384',\n    'vit_large_r50_s32_384',\n]\n\nnames_hflip_crop = [\n    'tf_efficientnet_l2_ns_hflip_384',\n    'deit_base_distilled_patch16_384_hflip_384',\n    'ig_resnext101_32x48d_hflip_384',\n    'tf_efficientnet_l2_ns_512',\n]\n\nnames_orig = [\n    'ig_resnext101_32x48d',\n    'vit_large_r50_s32_384',\n    'clip_RN50x4',\n    'clip_ViT-B-16',\n    'clip_RN50x16',\n    'clip_ViT-B-32',\n]\n","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:51:25.477746Z","iopub.execute_input":"2022-01-18T16:51:25.478456Z","iopub.status.idle":"2022-01-18T16:51:25.488032Z","shell.execute_reply.started":"2022-01-18T16:51:25.478396Z","shell.execute_reply":"2022-01-18T16:51:25.486919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create a dictionary with the path of all pretrained weights available in Kaggle datasets","metadata":{}},{"cell_type":"code","source":"modelpath = { m.split('/')[-1].split('.')[0] :m for m in glob('../input/pytorch-pretrained-0/*.pt')+glob('../input/pytorch-pretrained-1/*.pt')+glob('../input/pytorch-pretrained-2/*.pt')+glob('../input/pytorch-pretrained-3/*.pt')}\nmodelpath","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:51:25.489486Z","iopub.execute_input":"2022-01-18T16:51:25.490056Z","iopub.status.idle":"2022-01-18T16:51:25.523206Z","shell.execute_reply.started":"2022-01-18T16:51:25.490011Z","shell.execute_reply":"2022-01-18T16:51:25.52192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now interactively extract the TESTSET features from each imagenet pretrained model and append to a dictionary","metadata":{}},{"cell_type":"code","source":"class PawpularDataset:\n    def __init__(self, images, base_path='../input/petfinder-pawpularity-score/train/', modelcfg=None, aug=0 ):\n        \n        self.images = images.copy()\n        self.base_path = base_path\n        self.transform = create_transform(**modelcfg)\n        self.aug=aug\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, item):\n        img = Image.open(self.base_path + self.images[item] + '.jpg').convert('RGB')\n        img = self.transform(img)\n        return img\n\n\nEMB_TEST = {}\nfor arch in names:\n    starttime = time.time()\n\n    model = timm.create_model(arch, pretrained=False).to('cuda')\n    model.load_state_dict(torch.load(modelpath[arch]))\n    model.eval()\n\n    train_dataset = PawpularDataset(\n        images = test.Id.values,\n        base_path='../input/petfinder-pawpularity-score/test/',\n        modelcfg = resolve_data_config({}, model=model),\n        aug = 0,\n    )\n    BS = 10 if arch in ['tf_efficientnet_l2_ns'] else 16\n    train_dataloader = DataLoader(train_dataset, batch_size=BS, num_workers= 2, shuffle=False)\n    \n    with torch.no_grad():\n        res = [model(img.to('cuda')).cpu().numpy() for img in train_dataloader]\n    res = np.concatenate(res, 0)\n    EMB_TEST[arch] = res\n    \n    print( arch, ', Done in:', int(time.time() - starttime), 's' )\n    \n    del model, res\n    torch.cuda.empty_cache() # PyTorch thing to clean RAM\n    gc.collect()\n\nprint(time.time() )    \nlen(EMB_TEST), EMB_TEST.keys()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T13:20:47.059128Z","iopub.execute_input":"2022-01-18T13:20:47.059468Z","iopub.status.idle":"2022-01-18T13:24:59.986487Z","shell.execute_reply.started":"2022-01-18T13:20:47.05943Z","shell.execute_reply":"2022-01-18T13:24:59.985802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extract features using Horizontal Flip and small crop","metadata":{}},{"cell_type":"code","source":"class PawpularDataset_HFLIP:\n    def __init__(self, images, base_path='../input/petfinder-pawpularity-score/train/', modelcfg=None, doflip=False ):\n        \n        self.images = images.copy()\n        self.base_path = base_path\n        self.transform = modelcfg\n        self.doflip=doflip\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, item):\n        img = Image.open(self.base_path + self.images[item] + '.jpg').convert('RGB')\n        \n        if self.doflip==True:\n            img = img.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n            width, height = img.size\n            img = img.crop((0.0*width, 0.02*height, 0.98*width, 0.98 * height))  \n        \n        img = self.transform(img)\n        return img\n\n\nfor arch in names_hflip_crop:\n    starttime = time.time()\n\n    archname = arch.split('_hflip_')[0]\n    if arch == 'tf_efficientnet_l2_ns_512':\n        archname = 'tf_efficientnet_l2_ns'\n    model = timm.create_model(archname, pretrained=False).to('cuda')\n    model.load_state_dict(torch.load(modelpath[archname]))\n    model.eval()\n\n    # Get model default transforms\n    transf = resolve_data_config({}, model=model)\n    sz = int(arch.split('_')[-1])\n    transf['input_size'] = (3, sz, sz)\n    transf['crop_pct'] = 1.0        \n    transf = create_transform(**transf)\n\n    doflip = True if arch.split('_')[-2] == 'hflip' else False\n    train_dataset = PawpularDataset_HFLIP(\n        images = test.Id.values,\n        base_path='../input/petfinder-pawpularity-score/test/',\n        modelcfg = transf,\n        doflip = doflip,\n    )\n\n    BS = 10 if archname in ['tf_efficientnet_l2_ns'] else 16\n    train_dataloader = DataLoader(train_dataset, batch_size=BS, num_workers= 2, shuffle=False)\n\n    with torch.no_grad():\n        res = [model(img.to('cuda')).cpu().numpy() for img in train_dataloader]\n    res = np.concatenate(res, 0)\n    EMB_TEST[arch] = res\n\n    print( arch, 'imge size:', sz, 'Hflip:', doflip, ',Done in:', int(time.time() - starttime), 's' )\n\n    del model, res\n    torch.cuda.empty_cache() # PyTorch thing to clean RAM\n    gc.collect()\n\nprint(time.time() )    \nlen(EMB_TEST), EMB_TEST.keys()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T13:38:12.912744Z","iopub.execute_input":"2022-01-18T13:38:12.913042Z","iopub.status.idle":"2022-01-18T13:40:10.330487Z","shell.execute_reply.started":"2022-01-18T13:38:12.913009Z","shell.execute_reply":"2022-01-18T13:40:10.329701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now extract TESTSET features from CLIP architecture","metadata":{}},{"cell_type":"code","source":"class CustomDataset:\n    def __init__(self, data, base_path='../input/petfinder-pawpularity-score/test/', preprocess=None):\n        \n        self.data = data.copy()\n        self.base_path = base_path\n        if 'Pawpularity' not in self.data.columns:\n            self.data['Pawpularity'] = 0\n        self.preprocess=preprocess\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        img = Image.open(self.base_path + self.data.Id[item] + '.jpg').convert(\"RGB\")\n        img = self.preprocess(img)\n        return img\n\n    \nfor m in ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'ViT-B-16', 'ViT-B-32']:\n    starttime = time.time()\n    model, preprocess = clip.load(\"../input/openaiclipweights/clip/CLIP/models/\"+m+\".pt\")\n    model.cuda().eval()\n    \n    EMB = []\n    with torch.no_grad():\n        test_dataset = CustomDataset(data = test, base_path='../input/petfinder-pawpularity-score/test/', preprocess=preprocess)\n        test_data_loader = DataLoader(test_dataset, batch_size=64,num_workers=2,shuffle=False,pin_memory=True,)\n        for batch in test_data_loader:\n            image_features = model.encode_image(batch.to('cuda'))\n            #image_features /= image_features.norm(dim=-1, keepdim=True)\n            logits = image_features.cpu().numpy()\n            EMB.append(logits)\n    EMB = np.concatenate(EMB, 0)\n    EMB = EMB.astype('float32')\n    gc.collect()\n    \n    EMB_TEST['clip_'+m] = EMB\n    print( m, ', Done in:', int(time.time() - starttime), 's' )\n    \n    del model\n    torch.cuda.empty_cache() # PyTorch thing to clean RAM\n    gc.collect()\n    \ngc.collect()\nprint(EMB_TEST.keys())","metadata":{"execution":{"iopub.status.busy":"2022-01-18T13:40:10.333863Z","iopub.execute_input":"2022-01-18T13:40:10.334595Z","iopub.status.idle":"2022-01-18T13:41:02.846803Z","shell.execute_reply.started":"2022-01-18T13:40:10.334554Z","shell.execute_reply":"2022-01-18T13:41:02.845999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load TRAINSET extracted features (made offline)","metadata":{}},{"cell_type":"code","source":"EMB_TRAIN = joblib.load('../input/petfinderdata/train-embeddings-direct-1.joblib')\ngc.collect()\n\nresclip = joblib.load('../input/openai-clip/train-embeddings-openai-clip-1.joblib')\nfor m in resclip.keys():\n    EMB_TRAIN[m] = resclip[m]\ndel resclip\ngc.collect()\n\nhflipmodels = joblib.load('../input/petfinder-extracted-pretrained-1/extracted-pretrained-1.joblib')\nfor col in names_hflip_crop:\n    EMB_TRAIN[col] = hflipmodels[col]\ndel hflipmodels\ngc.collect()\n\nprint( len(EMB_TRAIN) )\nprint(EMB_TRAIN.keys())","metadata":{"execution":{"iopub.status.busy":"2022-01-18T13:30:49.535134Z","iopub.execute_input":"2022-01-18T13:30:49.535393Z","iopub.status.idle":"2022-01-18T13:31:21.088371Z","shell.execute_reply.started":"2022-01-18T13:30:49.535365Z","shell.execute_reply":"2022-01-18T13:31:21.087589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check the shape of the features","metadata":{}},{"cell_type":"code","source":"for m in EMB_TEST.keys():\n    print(EMB_TRAIN[m].shape, EMB_TEST[m].shape, m)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T13:41:02.848409Z","iopub.execute_input":"2022-01-18T13:41:02.848705Z","iopub.status.idle":"2022-01-18T13:41:02.863539Z","shell.execute_reply.started":"2022-01-18T13:41:02.848667Z","shell.execute_reply":"2022-01-18T13:41:02.862824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# names0 = [\n#     'clip_RN50x16',\n#     'clip_ViT-B-32',\n#     'clip_ViT-B-16',\n#     'clip_RN50x4',\n#     'deit_base_distilled_patch16_384',\n#     'ig_resnext101_32x48d',\n#     'repvgg_b0',\n#     'resnetv2_152x4_bitm',\n#     'swsl_resnext101_32x8d',\n#     'tf_efficientnet_l2_ns_475',\n#     'vit_base_patch16_384',\n#     'vit_large_r50_s32_384',\n# ]\n\n# names1 = [\n#     'clip_RN50x16',\n#     'clip_RN101', \n#     'clip_RN50',\n#     'fbnetc_100',\n#     'ig_resnext101_32x8d',\n#     'rexnet_200',\n#     'resnest269e',\n#     'tf_efficientnet_b6_ns',\n#     'tf_efficientnet_b8_ap',\n#     'tf_efficientnet_b7_ns',\n#     'vit_large_patch16_384',\n# ]\n\nnames2 = [\n    'tf_efficientnet_l2_ns_hflip_384',\n    'deit_base_distilled_patch16_384_hflip_384',\n    'ig_resnext101_32x48d_hflip_384',\n    'tf_efficientnet_l2_ns_512',\n    'ig_resnext101_32x48d',\n    'vit_large_r50_s32_384',\n    'clip_RN50x4',\n    'clip_ViT-B-16',\n    'clip_RN50x16',\n    'clip_ViT-B-32',\n]\n\n#names = np.unique(names0 + names1 + names2)\nnames = np.unique(names2)\nlen(names), names","metadata":{"execution":{"iopub.status.busy":"2022-01-18T13:41:37.578941Z","iopub.execute_input":"2022-01-18T13:41:37.579476Z","iopub.status.idle":"2022-01-18T13:41:37.5904Z","shell.execute_reply.started":"2022-01-18T13:41:37.57944Z","shell.execute_reply":"2022-01-18T13:41:37.589587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Just clean memory of offline trainset features not going to be used here","metadata":{}},{"cell_type":"code","source":"feats = list(EMB_TRAIN.keys())\nfor n in feats:\n    if n not in names:\n        del EMB_TRAIN[n]\n        gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T13:41:41.547969Z","iopub.execute_input":"2022-01-18T13:41:41.548513Z","iopub.status.idle":"2022-01-18T13:41:47.722184Z","shell.execute_reply.started":"2022-01-18T13:41:41.548475Z","shell.execute_reply":"2022-01-18T13:41:47.721449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now its time to fit a GPU accelerated SVR using cuml ","metadata":{}},{"cell_type":"code","source":"from cuml.svm import SVR\nfrom sklearn.preprocessing import StandardScaler\n\ndef fit_gpu_svr(TRAIN, TEST, kfoldcol='fold0'):\n    \n    ypredtrain_ = np.zeros(train.shape[0])\n    ypredtest_ = np.zeros(test.shape[0])\n\n    for fold in range(train[kfoldcol].max()+1):\n        ind_train = train[kfoldcol] != fold\n        ind_valid = train[kfoldcol] == fold\n\n        model = SVR(C=16.0, kernel='rbf', degree=3, max_iter=4000, output_type='numpy')\n        model.fit(TRAIN[ind_train], train.Pawpularity[ind_train].clip(1, 85)  )\n\n        ypredtrain_[ind_valid] = np.clip(model.predict(TRAIN[ind_valid]), 1 , 100)\n        ypredtest_ += np.clip(model.predict(TEST), 1, 100)\n\n        del model\n        gc.collect()\n\n    ypredtest_ /= (train[kfoldcol].max()+1)\n\n    return ypredtrain_, ypredtest_\n\ndef rmse(ytrue, ypred):\n    return np.sqrt(np.mean((ytrue-ypred)**2))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:09:02.310601Z","iopub.execute_input":"2022-01-18T17:09:02.31093Z","iopub.status.idle":"2022-01-18T17:09:02.322158Z","shell.execute_reply.started":"2022-01-18T17:09:02.310885Z","shell.execute_reply":"2022-01-18T17:09:02.320863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# First, lets fit one SVR for each architecture independently","metadata":{}},{"cell_type":"code","source":"for col in names:\n    \n    TRAIN = EMB_TRAIN[col].copy()\n    TEST = EMB_TEST[col].copy()\n\n    scaler = StandardScaler()\n    scaler.fit( np.vstack((TRAIN, TEST)) )\n    TRAIN = scaler.transform(TRAIN)\n    TEST = scaler.transform(TEST)\n    \n    ypredtrain, ypredtest = fit_gpu_svr(TRAIN, TEST, 'fold0')\n    print(rmse(train.Pawpularity,ypredtrain), col)    ","metadata":{"execution":{"iopub.status.busy":"2022-01-17T12:52:12.257423Z","iopub.execute_input":"2022-01-17T12:52:12.257761Z","iopub.status.idle":"2022-01-17T12:59:08.233241Z","shell.execute_reply.started":"2022-01-17T12:52:12.257728Z","shell.execute_reply":"2022-01-17T12:59:08.232116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# As you can see above, features extracted from individual architectures have SVR RMSE ranging from 17.56 to 18.52.\n# But what happens if we stack some architecture features side by side before fitting the SVR?","metadata":{}},{"cell_type":"markdown","source":"# Concatenate some features and standardize","metadata":{}},{"cell_type":"markdown","source":"# Fit the SVR A using all K Folds.\n# I noticed cliping the target in 85 slightly boosts RMSE.","metadata":{}},{"cell_type":"code","source":"# print('Concatenating:', names0)\n\n# TRAIN = np.concatenate([EMB_TRAIN[k] for k in names0], 1)\n# TEST = np.concatenate([EMB_TEST[k] for k in names0], 1)\n# scaler = StandardScaler()\n# scaler.fit( np.vstack((TRAIN, TEST)) )\n# gc.collect()\n\n# TRAIN = scaler.transform(TRAIN)\n# TEST = scaler.transform(TEST)\n# gc.collect()\n\n# # Check the output shape\n# print(TRAIN.shape, TEST.shape)\n\n# ypredtrainA, ypredtestA = fit_gpu_svr(TRAIN, TEST, 'fold0')\n# print(rmse(train.Pawpularity, ypredtrainA))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del TRAIN, TEST\ngc.collect()\ntorch.cuda.empty_cache() # PyTorch thing","metadata":{"execution":{"iopub.status.busy":"2022-01-17T13:02:41.667999Z","iopub.execute_input":"2022-01-17T13:02:41.66866Z","iopub.status.idle":"2022-01-17T13:04:58.236188Z","shell.execute_reply.started":"2022-01-17T13:02:41.668624Z","shell.execute_reply":"2022-01-17T13:04:58.235001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Also I noticed that using a multiplier of 1.032 boosts both CV and LB. It may be by the fact that SRV optimizes mean squared error and not RMSE.","metadata":{}},{"cell_type":"code","source":"# print('RMSE:', rmse(train.Pawpularity, 1.032*ypredtrainA))","metadata":{"execution":{"iopub.status.busy":"2022-01-17T13:04:58.238328Z","iopub.execute_input":"2022-01-17T13:04:58.239364Z","iopub.status.idle":"2022-01-17T13:04:58.247305Z","shell.execute_reply.started":"2022-01-17T13:04:58.239314Z","shell.execute_reply":"2022-01-17T13:04:58.246054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now fit a SVR B using a second subset of features. The idea of fitting more subsets is to add diversity in posterior model ensemble and avoid the curse of dimensionality increasing too much the number of features.","metadata":{}},{"cell_type":"code","source":"# print('Concatenating:', names1)\n\n# TRAIN = np.concatenate([EMB_TRAIN[k] for k in names1], 1)\n# TEST = np.concatenate([EMB_TEST[k] for k in names1], 1)\n# scaler = StandardScaler()\n# scaler.fit( np.vstack((TRAIN, TEST)) )\n# gc.collect()\n\n# TRAIN = scaler.transform(TRAIN)\n# TEST = scaler.transform(TEST)\n# gc.collect()\n\n# print( TRAIN.shape, TEST.shape )\n\n# ypredtrainB, ypredtestB = fit_gpu_svr(TRAIN, TEST, 'fold0')\n# print('RMSE:', rmse(train.Pawpularity, ypredtrainB))","metadata":{"execution":{"iopub.status.busy":"2022-01-17T13:04:58.250141Z","iopub.execute_input":"2022-01-17T13:04:58.250886Z","iopub.status.idle":"2022-01-17T13:05:54.108937Z","shell.execute_reply.started":"2022-01-17T13:04:58.250836Z","shell.execute_reply":"2022-01-17T13:05:54.107959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del TRAIN, TEST\ngc.collect()\ntorch.cuda.empty_cache() # PyTorch thing","metadata":{"execution":{"iopub.status.busy":"2022-01-17T13:05:54.111408Z","iopub.execute_input":"2022-01-17T13:05:54.111827Z","iopub.status.idle":"2022-01-17T13:07:25.992305Z","shell.execute_reply.started":"2022-01-17T13:05:54.111768Z","shell.execute_reply":"2022-01-17T13:07:25.991147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVR C","metadata":{}},{"cell_type":"code","source":"print('Concatenating:', names2)\n\nTRAIN = np.concatenate([EMB_TRAIN[k] for k in names2], 1)\nTEST = np.concatenate([EMB_TEST[k] for k in names2], 1)\nscaler = StandardScaler()\nscaler.fit( np.vstack((TRAIN, TEST)) )\ngc.collect()\n\nTRAIN = scaler.transform(TRAIN)\nTEST = scaler.transform(TEST)\ngc.collect()\n\nprint( TRAIN.shape, TEST.shape )\n\nypredtrainC, ypredtestC = fit_gpu_svr(TRAIN, TEST, 'fold0')\nprint('RMSE:', rmse(train.Pawpularity, ypredtrainC))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T13:42:05.744305Z","iopub.execute_input":"2022-01-18T13:42:05.74487Z","iopub.status.idle":"2022-01-18T13:43:39.146377Z","shell.execute_reply.started":"2022-01-18T13:42:05.744828Z","shell.execute_reply":"2022-01-18T13:43:39.145406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Free RAM and GPU memory","metadata":{}},{"cell_type":"code","source":"del TRAIN, TEST\ndel EMB_TRAIN, EMB_TEST\ngc.collect()\n\ntorch.cuda.empty_cache() # PyTorch thing to free GPU memory\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T13:07:25.994442Z","iopub.execute_input":"2022-01-17T13:07:25.995096Z","iopub.status.idle":"2022-01-17T13:07:26.763033Z","shell.execute_reply.started":"2022-01-17T13:07:25.995029Z","shell.execute_reply":"2022-01-17T13:07:26.76209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now run inference using Deep Learning finetuned image models.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport albumentations as A\n\ndevice = torch.device('cuda')\nclass Config:\n    model_name = \"swin_large_patch4_window7_224\"\n    base_dir = \"../input/petfinder-pawpularity-score\"\n    data_dir = base_dir\n    model_dir = \"exp\"\n    output_dir = model_dir\n    img_test_dir = os.path.join(data_dir, \"test\")\n    model_path = \"swin_large_patch4_window7_224\"\n    im_size =  384\n    batch_size = 16\n\n\nclass PetDataset(Dataset):\n    def __init__(self, image_filepaths, targets, transform=None):\n        self.image_filepaths = image_filepaths\n        self.targets = targets\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.image_filepaths)\n\n    def __getitem__(self, idx):\n        image_filepath = self.image_filepaths[idx]\n        with open(image_filepath, 'rb') as f:\n            image = Image.open(f)\n            image_rgb = image.convert('RGB')\n        image = np.array(image_rgb)\n\n        if self.transform is not None:\n            image = self.transform(image = image)[\"image\"]\n        \n        image = image / 255\n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        target = self.targets[idx]\n\n        image = torch.tensor(image, dtype = torch.float)\n        target = torch.tensor(target, dtype = torch.float)\n        return image, target    \n\n\ndef get_inference_fixed_transforms(mode=0, dim = 224):\n    if mode == 0: # do not original aspects, colors and angles\n        return A.Compose([\n                A.SmallestMaxSize(max_size=dim, p=1.0),\n                A.CenterCrop(height=dim, width=dim, p=1.0),\n            ], p=1.0)\n    elif mode == 1:\n        return A.Compose([\n                A.SmallestMaxSize(max_size=dim+16, p=1.0),\n                A.CenterCrop(height=dim, width=dim, p=1.0),\n                A.HorizontalFlip(p = 1.0)\n            ], p=1.0)\n\n\nclass PetNet(nn.Module):\n    def __init__(\n        self,\n        model_name = Config.model_path,\n        out_features = 1,\n        inp_channels = 3,\n        pretrained = False,\n    ):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=False, in_chans=3, num_classes = 1)\n    \n    def forward(self, image):\n        output = self.model(image)\n        return output    \n\n\ndef tta_fn(filepaths, model, ttas=[0, 1]):\n    print('Image Size:', Config.im_size)\n    model.eval()\n    tta_preds = []\n    for tta_mode in ttas:#range(Config.tta_times):\n        print(f'tta mode:{tta_mode}')\n        test_dataset = PetDataset(\n          image_filepaths = filepaths,\n          targets = np.zeros(len(filepaths)),\n          transform = get_inference_fixed_transforms(tta_mode, dim = Config.im_size )\n        )\n        test_loader = DataLoader(\n          test_dataset,\n          batch_size = Config.batch_size,\n          shuffle = False,\n          num_workers = 2,\n          pin_memory = True\n        )\n        #stream = tqdm(test_loader)\n        tta_pred = []\n        for images, target in test_loader:#enumerate(stream, start = 1):\n            images = images.to(device, non_blocking = True).float()\n            target = target.to(device, non_blocking = True).float().view(-1, 1)\n            with torch.no_grad():\n                output = model(images)\n\n            pred = (torch.sigmoid(output).detach().cpu().numpy() * 100).ravel().tolist()\n            tta_pred.extend(pred)\n        tta_preds.append(np.array(tta_pred))\n    \n    fold_preds = tta_preds[0]\n    for n in range(1, len(tta_preds)):\n        fold_preds += tta_preds[n]\n    fold_preds /= len(tta_preds)\n        \n    del test_loader, test_dataset\n    gc.collect()\n    torch.cuda.empty_cache()\n    return fold_preds    ","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:05:41.355484Z","iopub.execute_input":"2022-01-18T17:05:41.355899Z","iopub.status.idle":"2022-01-18T17:05:41.38701Z","shell.execute_reply.started":"2022-01-18T17:05:41.355863Z","shell.execute_reply":"2022-01-18T17:05:41.385898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List all test files\nfilepaths = test['path'].values.copy()\nlen(filepaths)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:05:42.20417Z","iopub.execute_input":"2022-01-18T17:05:42.204781Z","iopub.status.idle":"2022-01-18T17:05:42.212485Z","shell.execute_reply.started":"2022-01-18T17:05:42.204748Z","shell.execute_reply":"2022-01-18T17:05:42.21122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n\n# class Config:\n#     model_dir = \"exp53\"\n#     output_dir = \"exp53\"\n#     model_name = \"swin_large_patch4_window7_224\"\n#     im_size =  224\n#     model_path = model_name\n#     base_dir = \"../input/petfinder-pawpularity-score\"\n#     data_dir = base_dir\n#     img_test_dir = os.path.join(data_dir, \"test\")\n#     batch_size = 16\n\n\n# test_preds = []\n# test_preds_model = []\n# modelfiles = glob('../input/petfinder-'+Config.model_dir+'/*.pth')\n# for mi, model_path in enumerate(modelfiles):\n#     print(f'inference: {model_path}')\n#     test_preds_fold = []\n#     model = PetNet(\n#         model_name = Config.model_path,\n#         out_features = 1,\n#         inp_channels = 3,\n#         pretrained=False\n#     )\n#     model.load_state_dict(torch.load(model_path))\n#     model = model.to(device)\n#     model = model.float()\n#     model.eval()\n#     test_preds_fold = tta_fn(filepaths, model, [1] )        \n#     test_preds_model.append(test_preds_fold)\n    \n# oof53 = pd.read_csv('../input/petfinder-'+Config.model_dir+'/oof_tta.csv')\n# final_predictions53 = np.mean(np.array(test_preds_model), axis=0)\n# final_predictions53","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:52:11.29874Z","iopub.execute_input":"2022-01-18T16:52:11.299105Z","iopub.status.idle":"2022-01-18T16:54:17.243496Z","shell.execute_reply.started":"2022-01-18T16:52:11.299055Z","shell.execute_reply":"2022-01-18T16:54:17.242562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# class Config:\n#     model_dir = \"exp55\"\n#     output_dir = \"exp55\"\n#     model_name = \"beit_large_patch16_224\"\n#     im_size =  224\n#     model_path = model_name\n#     base_dir = \"../input/petfinder-pawpularity-score\"\n#     data_dir = base_dir\n#     img_test_dir = os.path.join(data_dir, \"test\")\n#     batch_size = 16\n\n\n# test_preds = []\n# test_preds_model = []\n# modelfiles = glob('../input/petfinder-'+Config.model_dir+'/*.pth')\n# for mi, model_path in enumerate(modelfiles):\n#     print(f'inference: {model_path}')\n#     test_preds_fold = []\n#     model = PetNet(\n#         model_name = Config.model_path,\n#         out_features = 1,\n#         inp_channels = 3,\n#         pretrained = False\n#     )\n#     model.load_state_dict(torch.load(model_path))\n#     model = model.to(device)\n#     model = model.float()\n#     model.eval()\n#     test_preds_fold = tta_fn(filepaths, model, [0] )        \n#     test_preds_model.append(test_preds_fold)\n\n\n# oof55 = pd.read_csv('../input/petfinder-'+Config.model_dir+'/oof_tta.csv')\n# final_predictions55 = np.mean(np.array(test_preds_model), axis=0)\n# final_predictions55","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:56:34.353533Z","iopub.execute_input":"2022-01-18T16:56:34.353839Z","iopub.status.idle":"2022-01-18T16:58:55.844171Z","shell.execute_reply.started":"2022-01-18T16:56:34.353806Z","shell.execute_reply":"2022-01-18T16:58:55.842868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nclass Config:\n    model_dir = \"exp66\"\n    output_dir = \"exp66\"\n    model_name = \"swin_large_patch4_window12_384_in22k\"\n    im_size =  384\n    model_path = model_name\n    base_dir = \"../input/petfinder-pawpularity-score\"\n    data_dir = base_dir\n    img_test_dir = os.path.join(data_dir, \"test\")\n    batch_size = 16\n\n    \ntest_preds = []\ntest_preds_model = []\nmodelfiles = glob('../input/petfinder-'+Config.model_dir+'/*.pth')\nfor mi, model_path in enumerate(modelfiles):\n    print(f'inference: {model_path}')\n    test_preds_fold = []\n    model = PetNet(\n        model_name = Config.model_path,\n        out_features = 1,\n        inp_channels = 3,\n        pretrained = False\n    )\n    model.load_state_dict(torch.load(model_path))\n    model = model.to(device)\n    model = model.float()\n    model.eval()\n    test_preds_fold = tta_fn(filepaths, model, [0] )        \n    test_preds_model.append(test_preds_fold)\n    \noof66 = pd.read_csv('../input/petfinder-'+Config.model_dir+'/oof_tta.csv')\nfinal_predictions66 = np.mean(np.array(test_preds_model), axis=0)\nfinal_predictions66","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:05:48.210152Z","iopub.execute_input":"2022-01-18T17:05:48.210909Z","iopub.status.idle":"2022-01-18T17:07:37.710037Z","shell.execute_reply.started":"2022-01-18T17:05:48.210831Z","shell.execute_reply":"2022-01-18T17:07:37.708992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nclass Config:\n    model_dir = \"exp77\"\n    output_dir = \"exp77\"\n    model_name = \"beit_large_patch16_224\"\n    im_size =  224\n    model_path = model_name\n    base_dir = \"../input/petfinder-pawpularity-score\"\n    data_dir = base_dir\n    img_test_dir = os.path.join(data_dir, \"test\")\n    batch_size = 16\n\n    \nclass PetNet(nn.Module):\n    def __init__(\n        self,\n        model_name = Config.model_path,\n        out_features = 1,\n        inp_channels = 3,\n        pretrained = False\n    ):\n        super().__init__()\n        NC = 1000\n        self.model = timm.create_model(model_name, pretrained=False)\n        self.dropout = nn.Dropout(0.05)\n        self.head = nn.Linear(NC, 1)\n    \n    def forward(self, image):\n        output = self.model(image)\n        output = self.dropout(output)\n        output = self.head(output)\n        return output    \n    \ntest_preds = []\ntest_preds_model = []\nmodelfiles = glob('../input/petfinder-'+Config.model_dir+'/*.pth')\nfor mi, model_path in enumerate(modelfiles):\n    print(f'inference: {model_path}')\n    test_preds_fold = []\n    model = PetNet(\n        model_name = Config.model_path,\n        out_features = 1,\n        inp_channels = 3,\n        pretrained = False\n    )\n    model.load_state_dict(torch.load(model_path))\n    model = model.to(device)\n    model = model.float()\n    model.eval()\n    test_preds_fold = tta_fn(filepaths, model, [0] )        \n    test_preds_model.append(test_preds_fold)\n    \noof77 = pd.read_csv('../input/petfinder-'+Config.model_dir+'/oof_tta.csv')\nfinal_predictions77 = np.mean(np.array(test_preds_model), axis=0)\nfinal_predictions77","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:59:50.740546Z","iopub.execute_input":"2022-01-18T16:59:50.740843Z","iopub.status.idle":"2022-01-18T17:02:17.895271Z","shell.execute_reply.started":"2022-01-18T16:59:50.740813Z","shell.execute_reply":"2022-01-18T17:02:17.89413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# class Config:\n#     model_dir = \"exp82\"\n#     output_dir = \"exp82\"\n#     model_name = \"tf_efficientnet_b6_ns\"\n#     im_size =  528\n#     model_path = model_name\n#     base_dir = \"../input/petfinder-pawpularity-score\"\n#     data_dir = base_dir\n#     img_test_dir = os.path.join(data_dir, \"test\")\n#     batch_size = 16\n\n\n# class PetNet(nn.Module):\n#     def __init__(\n#         self,\n#         model_name = Config.model_path,\n#         out_features = 1,\n#         inp_channels = 3,\n#         pretrained = False\n#     ):\n#         super().__init__()\n#         NC = 1000\n#         self.model = timm.create_model(model_name, pretrained=False)\n#         self.dropout = nn.Dropout(0.15)\n#         self.head = nn.Linear(NC, out_features)\n    \n#     def forward(self, image):\n#         output = self.model(image)\n#         output = self.dropout(output)\n#         output = self.head(output)\n#         return output\n    \n# test_preds = []\n# test_preds_model = []\n# modelfiles = glob('../input/petfinder-'+Config.model_dir+'/*.pth')\n# for mi, model_path in enumerate(modelfiles):\n#     print(f'inference: {model_path}')\n#     test_preds_fold = []\n#     model = PetNet(\n#         model_name = Config.model_path,\n#         out_features = 1,\n#         inp_channels = 3,\n#         pretrained = False\n#     )\n#     model.load_state_dict(torch.load(model_path))\n#     model = model.to(device)\n#     model = model.float()\n#     model.eval()\n#     test_preds_fold = tta_fn(filepaths, model, [0] )        \n#     test_preds_model.append(test_preds_fold)\n    \n# oof82 = pd.read_csv('../input/petfinder-'+Config.model_dir+'/oof_tta.csv')\n# final_predictions82 = np.mean(np.array(test_preds_model), axis=0)\n# final_predictions82","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:02:17.897572Z","iopub.execute_input":"2022-01-18T17:02:17.898111Z","iopub.status.idle":"2022-01-18T17:02:59.958592Z","shell.execute_reply.started":"2022-01-18T17:02:17.898065Z","shell.execute_reply":"2022-01-18T17:02:59.957457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Weighted average image models","metadata":{}},{"cell_type":"code","source":"# oof = oof77.copy()\n# oof['pred'] = (\n#     3*oof53['pred'] +\n#     #0*oof55['pred'] +\n#     #3*oof66['pred'] +\n#     4*oof77['pred']\n#     #2*oof82['pred']  \n# ) / (0+0+3+4+0) # 3+4+3+4+2 \n\n# final_train_predictions = train.merge(oof, on='Id', how='left')['pred'].values.copy()\n#oof53_predictions = train.merge(oof53, on='Id', how='left')['pred'].values.copy()\noof66_predictions = train.merge(oof66, on='Id', how='left')['pred'].values.copy()\noof77_predictions = train.merge(oof77, on='Id', how='left')['pred'].values.copy()\n\n# rmse(train.Pawpularity.values, final_train_predictions)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:09:07.704216Z","iopub.execute_input":"2022-01-18T17:09:07.705056Z","iopub.status.idle":"2022-01-18T17:09:07.735056Z","shell.execute_reply.started":"2022-01-18T17:09:07.705022Z","shell.execute_reply":"2022-01-18T17:09:07.733995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final_test_predictions = (\n#     3*final_predictions53 +\n#     #0*final_predictions55 +\n#     #3*final_predictions66 +\n#     4*final_predictions77\n#     #2*final_predictions82 \n# ) / (0+0+3+4+0) # 3+4+3+4+2","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:09:13.536953Z","iopub.execute_input":"2022-01-18T17:09:13.537645Z","iopub.status.idle":"2022-01-18T17:09:13.544096Z","shell.execute_reply.started":"2022-01-18T17:09:13.53761Z","shell.execute_reply":"2022-01-18T17:09:13.542848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optimize the overall RMSE using OOF prediction of cuML SVR A, B, C, and Image models ensemble.","metadata":{}},{"cell_type":"code","source":"from scipy.optimize import minimize\n\ndef min_func(K):\n    #ypredtrain = K[0]*ypredtrainA + K[1]*ypredtrainB + K[2]*ypredtrainC + K[3]*final_train_predictions\n    #ypredtrain = K[0]*ypredtrainA + K[1]*ypredtrainC + K[2]*final_train_predictions\n    #ypredtrain = K[0]*ypredtrainC + K[1]*oof53_predictions + K[2]*oof77_predictions\n    ypredtrain = K[0]*ypredtrainC + K[1]*oof66_predictions + K[2]*oof77_predictions\n    return rmse(train.Pawpularity, ypredtrain)\n   \nres = minimize(min_func, [1/3]*3, method='Nelder-Mead', tol=1e-6)\nK = res.x\nres","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:09:15.881145Z","iopub.execute_input":"2022-01-18T17:09:15.881483Z","iopub.status.idle":"2022-01-18T17:09:15.940422Z","shell.execute_reply.started":"2022-01-18T17:09:15.881453Z","shell.execute_reply":"2022-01-18T17:09:15.938908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ypredtrain = K[0]*ypredtrainA + K[1]*ypredtrainB + K[2]*ypredtrainC + K[3]*final_train_predictions\n#ypredtrain = K[0]*ypredtrainA + K[1]*ypredtrainC + K[2]*final_train_predictions\n#ypredtrain = K[0]*ypredtrainC + K[1]*oof53_predictions + K[2]*oof77_predictions\nypredtrain = K[0]*ypredtrainC + K[1]*oof66_predictions + K[2]*oof77_predictions\n\n#test['Pawpularity'] = K[0]*ypredtestA + K[1]*ypredtestB + K[2]*ypredtestC + K[3]*final_test_predictions\n#test['Pawpularity'] = K[0]*ypredtestA + K[1]*ypredtestC + K[2]*final_test_predictions\n#test['Pawpularity'] = K[0]*ypredtestC + K[1]*final_predictions53 + K[2]*final_predictions77\ntest['Pawpularity'] = K[0]*ypredtestC + K[1]*final_predictions66 + K[2]*final_predictions77\n\nprint('Ensemble weights:', K)\nprint('Final RMSE:', rmse(train.Pawpularity, ypredtrain) )","metadata":{"execution":{"iopub.status.busy":"2022-01-15T21:18:33.772177Z","iopub.execute_input":"2022-01-15T21:18:33.772748Z","iopub.status.idle":"2022-01-15T21:18:33.780427Z","shell.execute_reply.started":"2022-01-15T21:18:33.77271Z","shell.execute_reply":"2022-01-15T21:18:33.779675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(8)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T21:18:39.882837Z","iopub.execute_input":"2022-01-15T21:18:39.883647Z","iopub.status.idle":"2022-01-15T21:18:39.898767Z","shell.execute_reply.started":"2022-01-15T21:18:39.883605Z","shell.execute_reply":"2022-01-15T21:18:39.8978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[['Id','Pawpularity']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T16:34:49.722164Z","iopub.execute_input":"2022-01-12T16:34:49.722461Z","iopub.status.idle":"2022-01-12T16:34:49.736168Z","shell.execute_reply.started":"2022-01-12T16:34:49.72242Z","shell.execute_reply":"2022-01-12T16:34:49.735266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}