{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Abstraction\n\n- **[About Competition]**: In the given [cat-dog pet dataset](https://www.kaggle.com/c/petfinder-pawpularity-score/data), both image files and also meta informations are provided which can be used in training. The overall task is to analyze this raw images and metadata to predict the **Pawpularity** of pet photos.\n- **[About Hybrid External Multi-Head Transformer]**: \n    - In this notebook, we will be implementing the **External Self-Attention Transformer, EAT in `TensorFlow.Keras` ([paper](https://arxiv.org/pdf/2105.02358.pdf) 2021)**. Also implementation will be maintained in this repo [External-Attention-TensorFlow](https://github.com/innat/External-Attention-TensorFlow). The official PyTorch implementation is [here](https://github.com/MenghaoGuo/EANet). \n    - Specifically, in this notebook, we will use the **EAT** and build a **hybrid model** with ImageNet mdoels (diagram below, we choose `resnet` here, but you can use any). From the existing `tf.keras.applications`, we will import `resnet` and further we'll build a new feature extraction model with 2 output. One of the layer is `top_activation` and another is `conv2_block3_out` layer from `resnet`. \n    - In the next call, the output of the `conv2_block3_out` layer will be passed to the **EAT** model for further training. At the end, according to the model architecture, we will have 3 output to merge followed by classificaiton layer. \n- **[About Input Format]**: As in this competition, we have both **image data** and **structure data**, we'll be building a multi-input model in order to utilize both information. In the above diagram, the `input 1` is the raw image and `input 2` refer the structure data. And for structure data, a simple mlp model will be used. FYI, for structure data, we can also try [TensorFlow Decision Forests](https://blog.tensorflow.org/2021/05/introducing-tensorflow-decision-forests.html).\n\n![up](https://user-images.githubusercontent.com/17668390/141291222-b3184730-11ba-4e12-bd87-30a85d82e854.png)\n\n- **[About Training]**: We'll train the model on **TPU** and next we'll inference with **GPU**. For training details with **TPU** settings, check **Version 2**; we've trained already and saved the weight file. And lastly, we'll try to inspect the **activaiton feature maps** from the **External-Transformer** blocks.\n- **[About Inference]**: Next, in the **Inference** section, we'll compute out-of-fold validation and do inference with all trained folds and followed by simple **Ensemble**. \n- **[(Optional): About RAPIDS SVR]**: Added RAPIDS SVR for second stage training approach. Reference [kernel](https://www.kaggle.com/cdeotte/rapids-svr-boost-17-8) - [Discussion](https://www.kaggle.com/c/petfinder-pawpularity-score/discussion/276724). ","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os, random\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport random as python_random\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport tensorflow as tf; print(tf.__version__)\nimport tensorflow_hub as hub\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import Input, Model, Sequential, layers\n\n# control gpu ram growth \ntf.config.optimizer.set_jit(True)\nphysical_devices = tf.config.list_physical_devices('GPU')\ntry: tf.config.experimental.set_memory_growth(physical_devices[0], True)\nexcept: pass \n\n# for reproducibiity\ndef seed_all(s):\n    random.seed(s)\n    python_random.seed(s)\n    tf.random.set_seed(s)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n    os.environ['PYTHONHASHSEED'] = str(0)\n    \n# seed all\nSEED  = 1994\nsns.set(style=\"darkgrid\")\nseed_all(SEED)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-23T01:37:23.158009Z","iopub.execute_input":"2021-12-23T01:37:23.158389Z","iopub.status.idle":"2021-12-23T01:37:29.014365Z","shell.execute_reply.started":"2021-12-23T01:37:23.158299Z","shell.execute_reply":"2021-12-23T01:37:29.013611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set DEVICE = 'TPU' for training. Please check version 2. \nDEVICE = 'GPU' \n\nif DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        DEVICE = \"GPU\"\n\nif DEVICE != \"TPU\":\n    print(\"Using default strategy for CPU and single GPU\")\n    strategy = tf.distribute.get_strategy()\n\nif DEVICE == \"GPU\":\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n    physical_devices = tf.config.list_physical_devices('GPU')\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n    \n    \nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-23T01:37:29.018097Z","iopub.execute_input":"2021-12-23T01:37:29.018306Z","iopub.status.idle":"2021-12-23T01:37:29.248869Z","shell.execute_reply.started":"2021-12-23T01:37:29.01828Z","shell.execute_reply":"2021-12-23T01:37:29.247746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE   = 896 \nBATCH_SIZE = 10\nEPOCHS     = 5\n\n# Folders\nif DEVICE == \"TPU\":\n    from kaggle_datasets import KaggleDatasets\n    DATA_DIR = '/kaggle/input/petfinder-pawpularity-score/'\n    GCS_PATH  = KaggleDatasets().get_gcs_path('petfinder-pawpularity-score')\n    TRAIN_DIR = GCS_PATH + '/train/'\n    TEST_DIR  = GCS_PATH + '/test/'\nelse:\n    DATA_DIR  = '/kaggle/input/petfinder-pawpularity-score/'\n    TRAIN_DIR = DATA_DIR + 'train/'\n    TEST_DIR  = DATA_DIR + 'test/'","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:37:29.250055Z","iopub.execute_input":"2021-12-23T01:37:29.250388Z","iopub.status.idle":"2021-12-23T01:37:29.255908Z","shell.execute_reply.started":"2021-12-23T01:37:29.250342Z","shell.execute_reply":"2021-12-23T01:37:29.255122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configured Competition Data","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\nFOLDS = 10\n# Load Train Data\ntrain_df = pd.read_csv(f'{DATA_DIR}train.csv')\ntrain_df['Id'] = train_df['Id'].apply(lambda x: f'{TRAIN_DIR}{x}.jpg')\n# Set a specific label to be able to perform stratification\ntrain_df['stratify_label'] = pd.qcut(train_df['Pawpularity'], q = 30, labels = range(30))\n# Label value to be used for feature model 'classification' training.\ntrain_df['target_value'] = train_df['Pawpularity'] / 100.\n# list of feature that will be second input along with image input\ndense_features = [\n    'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n    'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'\n]\nkfold = StratifiedKFold(n_splits = FOLDS,  shuffle = True, random_state = SEED)\ntrain_df['kfold'] = -1\nfor fold, (train_index, val_index) in enumerate(kfold.split(train_df.index,\n                                                            train_df['stratify_label'])):\n    train_df.loc[val_index, 'kfold'] = fold \n    \n# save or not\ntrain_df.to_csv(\"train_df_folds.csv\", index=False)\ndisplay(train_df.head(3))\n\n# test set \ntest_df = pd.read_csv(f'{DATA_DIR}test.csv')\ntest_df['Id'] = test_df['Id'].apply(lambda x: f'{TEST_DIR}{x}.jpg')\ntest_df['Pawpularity'] = 0\ndisplay(test_df.head(3))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-23T01:37:29.258464Z","iopub.execute_input":"2021-12-23T01:37:29.25908Z","iopub.status.idle":"2021-12-23T01:37:29.572186Z","shell.execute_reply.started":"2021-12-23T01:37:29.259044Z","shell.execute_reply":"2021-12-23T01:37:29.571534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# `tf.data` API for Multi-Input","metadata":{}},{"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE  \ndef build_augmenter(is_labelled):\n    def augment(img):\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_flip_up_down(img)\n        img = tf.image.random_saturation(img, 0.95, 1.05)\n        img = tf.image.random_brightness(img, 0.05)\n        img = tf.image.random_contrast(img, 0.95, 1.05)\n        img = tf.image.random_hue(img, 0.05)\n        return img\n    \n    def augment_with_labels(path, label):\n        image, feature = path \n        augment_image = augment(image)\n        return (augment_image, feature), label\n    \n    def augment_without_labels(path, _):\n        image, feature = path \n        augment_image = augment(image)\n        return (augment_image, feature), None\n    \n    return augment_with_labels if is_labelled else augment_without_labels","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:37:29.573572Z","iopub.execute_input":"2021-12-23T01:37:29.573819Z","iopub.status.idle":"2021-12-23T01:37:29.581037Z","shell.execute_reply.started":"2021-12-23T01:37:29.573785Z","shell.execute_reply":"2021-12-23T01:37:29.580204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_decoder(is_labelled):\n    def decode(path):\n        file_bytes = tf.io.read_file(path)\n        img = tf.image.decode_jpeg(file_bytes, channels = 3)\n        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE)) \n        return tf.divide(img, 255.)\n    \n    def decode_with_labels(path, label):\n        image, feature = path \n        decode_image = decode(image)\n        return (decode_image, feature), label \n    \n    def decode_without_labels(path, feature):\n        decode_image = decode(path)\n        return (decode_image, feature), None\n    \n    return decode_with_labels if is_labelled else decode_without_labels\n\ndef create_dataset(df, \n                   batch_size  = 32, \n                   is_labelled = False, \n                   augment     = False,\n                   repeat      = False, \n                   shuffle     = False):\n    \n    decode_fn    = build_decoder(is_labelled)\n    augmenter_fn = build_augmenter(is_labelled)\n    \n    # Create Dataset\n    if is_labelled:\n        dataset_sample = tf.data.Dataset.from_tensor_slices((df['Id'].values, df[dense_features].values))\n        dataset_labels = tf.data.Dataset.from_tensor_slices(df['target_value'].values)\n        dataset = tf.data.Dataset.zip((dataset_sample, dataset_labels))\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices((df['Id'].values, df[dense_features].values))\n        \n    dataset = dataset.map(decode_fn, num_parallel_calls = AUTOTUNE)\n    dataset = dataset.map(augmenter_fn, num_parallel_calls = AUTOTUNE) if augment else dataset\n    dataset = dataset.repeat() if repeat else dataset\n    dataset = dataset.shuffle(1024 * REPLICAS, reshuffle_each_iteration = True) if shuffle else dataset\n    dataset = dataset.batch(batch_size * REPLICAS, drop_remainder=shuffle)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:37:29.582727Z","iopub.execute_input":"2021-12-23T01:37:29.583026Z","iopub.status.idle":"2021-12-23T01:37:29.596512Z","shell.execute_reply.started":"2021-12-23T01:37:29.582957Z","shell.execute_reply":"2021-12-23T01:37:29.595685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check Dataloader**","metadata":{}},{"cell_type":"code","source":"training_dataset = create_dataset(train_df, \n                                  batch_size  = BATCH_SIZE, \n                                  is_labelled = True, \n                                  augment     = True,\n                                  repeat      = False, \n                                  shuffle     = False)\n(sample_images, sample_feature), sample_labels = next(iter(training_dataset))\nprint(sample_images.shape, sample_feature.shape, sample_labels.shape)\n\nimport matplotlib.pyplot as plt \nplt.figure(figsize=(16, 10))\nfor i, (image, label) in enumerate(zip(sample_images[:8], sample_labels[:8])):\n    ax = plt.subplot(3, 4, i + 1)\n    plt.title(f'{label.numpy()} , Raw: {image.numpy().shape}')\n    plt.imshow(image.numpy().squeeze())\n    plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:37:29.598064Z","iopub.execute_input":"2021-12-23T01:37:29.598327Z","iopub.status.idle":"2021-12-23T01:37:35.054181Z","shell.execute_reply.started":"2021-12-23T01:37:29.598292Z","shell.execute_reply":"2021-12-23T01:37:35.053479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling\n\nAs described at the beginning, we'll implement [External Attention](https://arxiv.org/pdf/2105.02358.pdf) in `TensorFlow.Keras` and later we'll integrate it into a ImageNet model (here, we pick `resnet`). Describing the model is beyond the scope of this code example. [Here](https://github.com/MenghaoGuo/EANet) is the official PyTorch implementation. And we'll try to rebuild based on that.  \n\n<img width=\"756\" alt=\"ea\" src=\"https://user-images.githubusercontent.com/17668390/141291708-7c3cd892-d508-4cca-8306-a8b06a38c158.png\">\n","metadata":{}},{"cell_type":"markdown","source":"## Implement the patch extraction and encoding layer","metadata":{}},{"cell_type":"code","source":"class PatchEmbed(tf.keras.layers.Layer):\n    def __init__(self, img_size=(224, 224), patch_size=(4, 4),  embed_dim=96):\n        super().__init__(name='patch_embed')\n        patches_resolution = [img_size[0] // patch_size[0], \n                              img_size[1] // patch_size[1]]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n        self.embed_dim = embed_dim\n        self.proj = layers.Conv2D(embed_dim, \n                                  kernel_size=patch_size, \n                                  strides=patch_size, name='proj')\n        self.norm = layers.LayerNormalization(epsilon=1e-5, name='norm')\n     \n    def call(self, x):\n        B, H, W, C = x.get_shape().as_list()\n        x = self.proj(x)\n        x = tf.reshape(\n            x, shape=[-1, \n                      (H // self.patch_size[0]) * (W // self.patch_size[0]), \n                      self.embed_dim]\n        )\n        x = self.norm(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:37:35.055232Z","iopub.execute_input":"2021-12-23T01:37:35.055587Z","iopub.status.idle":"2021-12-23T01:37:35.073728Z","shell.execute_reply.started":"2021-12-23T01:37:35.055548Z","shell.execute_reply":"2021-12-23T01:37:35.072916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Implement the External Attention Block","metadata":{}},{"cell_type":"code","source":"class ExternalAttention(layers.Layer):\n    def __init__(self, dim, num_heads, dim_coefficient = 4, \n                 attention_dropout = 0,  projection_dropout = 0, \n                 **kwargs):\n        super(ExternalAttention, self).__init__(name= 'ExternalAttention', **kwargs)\n        self.dim       = dim \n        self.num_heads = num_heads \n        self.dim_coefficient    = dim_coefficient\n        self.attention_dropout  = attention_dropout\n        self.projection_dropout = projection_dropout\n        \n        k = 256 // dim_coefficient\n        self.trans_dims = layers.Dense(dim * dim_coefficient)\n        self.linear_0 = layers.Dense(k)\n        self.linear_1 = layers.Dense(dim * dim_coefficient // num_heads)\n        self.proj = layers.Dense(dim)\n    \n        self.attn_drop  = layers.Dropout(attention_dropout)\n        self.proj_drop  = layers.Dropout(projection_dropout)\n        \n    def call(self, inputs, return_attention_scores=False, training=None):\n        num_patch = tf.shape(inputs)[1]\n        channel   = tf.shape(inputs)[2]\n        x = self.trans_dims(inputs)\n        x = tf.reshape(x, shape=(-1, \n                                 num_patch, \n                                 self.num_heads,\n                                 self.dim * self.dim_coefficient // self.num_heads))\n        x = tf.transpose(x, perm=[0, 2, 1, 3])\n        \n        # a linear layer M_k\n        attn = self.linear_0(x)\n        # normalize attention map\n        attn = layers.Softmax(axis=2)(attn)\n        # dobule-normalization\n        attn = attn / (1e-9 + tf.reduce_sum(attn, axis=-1, keepdims=True))\n        attn_drop = self.attn_drop(attn, training=training)\n        \n        # a linear layer M_v\n        attn_dense = self.linear_1(attn_drop)\n        x = tf.transpose(attn_dense, perm=[0, 2, 1, 3])\n        x = tf.reshape(x, [-1, num_patch, self.dim * self.dim_coefficient])\n        # a linear layer to project original dim\n        x = self.proj(x)\n        x = self.proj_drop(x, training=training)\n  \n        if return_attention_scores:\n            return x, attn\n        else:\n            return x \n        \n    def get_config(self):\n        config = {\n            'dim'                : self.dim,\n            'num_heads'          : self.num_heads,\n            'dim_coefficient'    : self.dim_coefficient,\n            'attention_dropout'  : self.attention_dropout,\n            'projection_dropout' : self.projection_dropout\n        }\n        base_config = super(ExternalAttention, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:37:35.075061Z","iopub.execute_input":"2021-12-23T01:37:35.075354Z","iopub.status.idle":"2021-12-23T01:37:35.109207Z","shell.execute_reply.started":"2021-12-23T01:37:35.07532Z","shell.execute_reply":"2021-12-23T01:37:35.108322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Implement the MLP block","metadata":{}},{"cell_type":"code","source":"class MLP(layers.Layer):\n    def __init__(self, mlp_dim, embedding_dim=None, \n                 act_layer=tf.nn.gelu, drop_rate=0.2, **kwargs):\n        super(MLP, self).__init__(name='MLP', **kwargs)\n        self.fc1  = layers.Dense(mlp_dim, activation=act_layer)\n        self.fc2  = layers.Dense(embedding_dim)\n        self.drop = layers.Dropout(drop_rate)\n\n    def call(self, inputs, training=None):\n        x = self.fc1(inputs)\n        x = self.drop(x, training=training)\n        x = self.fc2(x)\n        x = self.drop(x, training=training)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:37:35.113611Z","iopub.execute_input":"2021-12-23T01:37:35.114244Z","iopub.status.idle":"2021-12-23T01:37:35.122992Z","shell.execute_reply.started":"2021-12-23T01:37:35.114205Z","shell.execute_reply":"2021-12-23T01:37:35.122262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Implement the Transformer block","metadata":{}},{"cell_type":"code","source":"patch_size       = 4\nnum_heads        = 4\nembedding_dim    = 128\nmlp_dim          = 64\ndim_coefficient  = 4\nnum_patches      = (IMG_SIZE // patch_size) ** 2\nattention_dropout   = 0.2\nprojection_dropout  = 0.2\nnum_ext_transformer_blocks = 10","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:37:35.124565Z","iopub.execute_input":"2021-12-23T01:37:35.125086Z","iopub.status.idle":"2021-12-23T01:37:35.131294Z","shell.execute_reply.started":"2021-12-23T01:37:35.125051Z","shell.execute_reply":"2021-12-23T01:37:35.130622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionEncoder(layers.Layer):\n    def __init__(self, embedding_dim, \n                 mlp_dim, num_heads, \n                 dim_coefficient,  \n                 attention_dropout,  \n                 projection_dropout, \n                 get_attention_matrix=False,\n                 **kwargs):\n        super(AttentionEncoder, self).__init__(**kwargs)\n        self.embedding_dim = embedding_dim\n        self.mlp_dim   = mlp_dim\n        self.num_heads = num_heads\n        self.dim_coefficient    = dim_coefficient\n        self.attention_dropout  = attention_dropout\n        self.projection_dropout = projection_dropout\n        self.get_attention_matrix = get_attention_matrix\n        self.mlp = MLP(mlp_dim, embedding_dim)\n        \n        self.etn = ExternalAttention(\n            embedding_dim,\n            num_heads,\n            dim_coefficient,\n            attention_dropout,\n            projection_dropout\n        )\n    \n    def call(self, inputs):\n        residual_1 = inputs \n        x, ext_attention_scores = self.etn(inputs, return_attention_scores=True) \n        x = layers.add([x, residual_1])\n        residual_2 = x\n        x = self.mlp(x)\n        x = layers.add([x, residual_2])\n        \n        if self.get_attention_matrix:\n            return x, ext_attention_scores\n        else:\n            return x \n    \n    def get_config(self):\n        config = {\n            'embedding_dim'     : self.embedding_dim,\n            'mlp_dim'           : self.mlp_dim,\n            'num_heads'         : self.num_heads,\n            'dim_coefficient'   : self.dim_coefficient,\n            'attention_dropout' : self.attention_dropout,\n            'projection_dropout': self.projection_dropout,\n            'get_attention_matrix': self.get_attention_matrix\n        }\n        base_config = super(AttentionEncoder, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:37:35.133016Z","iopub.execute_input":"2021-12-23T01:37:35.13359Z","iopub.status.idle":"2021-12-23T01:37:35.148663Z","shell.execute_reply.started":"2021-12-23T01:37:35.133555Z","shell.execute_reply":"2021-12-23T01:37:35.147863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ImageNet-Model + External Attention Transformer \n\n![up](https://user-images.githubusercontent.com/17668390/141291222-b3184730-11ba-4e12-bd87-30a85d82e854.png)","metadata":{}},{"cell_type":"code","source":"def get_model(plot_model, print_summary, with_compile):\n    # multi-input \n    image_inputs  = layers.Input((IMG_SIZE, IMG_SIZE, 3))\n    feature_input =  layers.Input((len(dense_features),))\n\n    # base mdoel: image-net (replace with yours)\n    # multi-output \n    backbone = tf.keras.applications.ResNet50(\n        include_top=False,\n        weights=None,\n        input_tensor=layers.Input((IMG_SIZE, IMG_SIZE, 3)),\n    )\n    multi_op_backbone = Model(\n        backbone.input, \n        [\n            backbone.get_layer('conv2_block3_out').output, # for transformer blocks \n            backbone.output # for cnn blocks \n        ]\n    )\n    mid_y, last_y = multi_op_backbone(image_inputs)\n    \n    # Tranformer Blocks \n    patchedx = PatchEmbed(\n        img_size=(224, 224),\n        patch_size=(patch_size, patch_size),\n        embed_dim=embedding_dim\n    )(mid_y)\n    \n    x = patchedx\n    for _ in range(num_ext_transformer_blocks):\n        x, attn_weight_matrix = AttentionEncoder(\n            embedding_dim,\n            mlp_dim,\n            num_heads,\n            dim_coefficient,\n            attention_dropout,\n            projection_dropout,\n            get_attention_matrix = True\n        )(x)\n\n    # end layers : for transformer head \n    tail_1 = Sequential(\n        [\n            layers.GlobalAveragePooling1D(),\n            layers.Dropout(0.5),\n            layers.BatchNormalization()\n        ], name='tail_1'\n    )\n    \n    # end layers : for cnn head \n    tail_2 = Sequential(\n        [\n            layers.GlobalAveragePooling2D(),\n            layers.Dropout(0.5),\n        ], name='tail_2'\n    )\n    \n    # end layers : head layers for feature input (simple mlp)\n    tail_3 = Sequential(\n        [\n            layers.Dense(32,  activation='selu'),\n            layers.Dense(64,  activation='selu'),\n            layers.Dense(128, activation='selu'),\n            layers.Dropout(0.2),\n        ], name='tail_3'\n    )\n    \n    # bring all together \n    cating = tf.concat(\n        [\n            tail_1(x), \n            tail_2(last_y), \n            tail_3(feature_input)\n        ], \n        axis=-1\n    )\n    classifier = layers.Dense(1, activation = 'sigmoid')(cating)\n    \n    # construct the DAG graph \n    model = Model([image_inputs, feature_input], classifier)\n\n    # plotting \n    if plot_model:\n        display(tf.keras.utils.plot_model(model, \n                                          show_shapes=True, \n                                          show_layer_names=True,  \n                                          expand_nested=False))\n    # overal summary  \n    if print_summary:\n        print(model.summary())\n        \n    # compiling \n    if with_compile:\n        model.compile(\n            optimizer = optimizers.Adam(), \n            loss = losses.BinaryCrossentropy(), \n            metrics = [metrics.RootMeanSquaredError('rmse')])  \n\n    return model ","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:37:35.150188Z","iopub.execute_input":"2021-12-23T01:37:35.15072Z","iopub.status.idle":"2021-12-23T01:37:35.171491Z","shell.execute_reply.started":"2021-12-23T01:37:35.150684Z","shell.execute_reply":"2021-12-23T01:37:35.170655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model(plot_model    = True,  \n                  print_summary = True, \n                  with_compile  = False)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:37:35.172985Z","iopub.execute_input":"2021-12-23T01:37:35.173513Z","iopub.status.idle":"2021-12-23T01:37:39.248856Z","shell.execute_reply.started":"2021-12-23T01:37:35.173478Z","shell.execute_reply":"2021-12-23T01:37:39.24756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Modules","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import losses, optimizers , metrics\nfrom tensorflow.keras import callbacks\n\ndef get_lr_callback(batch_size=8):\n    lr_start   = 0.000005\n    lr_max     = 0.00000125 * batch_size * REPLICAS\n    lr_min     = 0.000001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n        return lr\n    return callbacks.LearningRateScheduler(lrfn, verbose=True)\n\n\n# Set Callbacks\ndef model_checkpoint(fold):\n    return callbacks.ModelCheckpoint(f'feature_model_{fold}.h5',\n                                              verbose = 1, \n                                              monitor = 'val_rmse', \n                                              mode  = 'min', \n                                              save_weights_only = True,\n                                              save_best_only    = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:37:39.250777Z","iopub.execute_input":"2021-12-23T01:37:39.251062Z","iopub.status.idle":"2021-12-23T01:37:39.259085Z","shell.execute_reply.started":"2021-12-23T01:37:39.251023Z","shell.execute_reply":"2021-12-23T01:37:39.258297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_fold = 0 # 1st fold training: total fold 10\nprint(f'\\nFold {training_fold}\\n')\n\ndf = pd.read_csv(\"./train_df_folds.csv\")\ndf_train = df[df.kfold != training_fold].reset_index(drop=True)\ndf_valid = df[df.kfold == training_fold].reset_index(drop=True)\n\nwith strategy.scope():\n    # Create Model\n    model = get_model(plot_model=False, print_summary=False, with_compile=True)\n\ntraining_dataset = create_dataset(df_train, \n                                  batch_size  = BATCH_SIZE, \n                                  is_labelled = True, \n                                  repeat      = True, \n                                  shuffle     = True)\nvalidation_dataset = create_dataset(df_valid, \n                                    batch_size  = BATCH_SIZE, \n                                    is_labelled = True,\n                                    repeat      = True, \n                                    shuffle     = False)\n\n# Only True for training. \n# Current Image_Size is too big for other device.\nif DEVICE == \"TPU\":\n    # Fit Model\n    history = model.fit(training_dataset,\n                        epochs = EPOCHS,\n                        steps_per_epoch  = df_train.shape[0] / batch_size // REPLICAS,\n                        validation_steps = df_valid.shape[0] / batch_size // REPLICAS,\n                        callbacks = [model_checkpoint(training_fold),  get_lr_callback(batch_size)],\n                        validation_data = validation_dataset,\n                        verbose = 1)   \n    # Validation Information\n    best_val_rmse = min(history.history['val_rmse'])\n    print(f'\\nValidation RMSE: {best_val_rmse}\\n')\nelse:\n    # load trained weights \n    model.load_weights(f'../input/pet-test-wg/ext_attn_wg/feature_model_{training_fold}.h5')","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:37:39.260711Z","iopub.execute_input":"2021-12-23T01:37:39.261303Z","iopub.status.idle":"2021-12-23T01:37:45.243584Z","shell.execute_reply.started":"2021-12-23T01:37:39.261244Z","shell.execute_reply":"2021-12-23T01:37:45.242814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot Learning Curve","metadata":{}},{"cell_type":"code","source":"if DEVICE == \"TPU\":\n    plt.figure(figsize=(19,6))\n\n    plt.subplot(131)\n    plt.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n    plt.plot(history.epoch, history.history[\"val_loss\"], label=\"Valid loss\")\n    plt.legend()\n\n    plt.subplot(132)\n    plt.plot(history.epoch, history.history[\"rmse\"], label=\"Train RMSE\")\n    plt.plot(history.epoch, history.history[\"val_rmse\"], label=\"Valid RMSE\")\n    plt.legend()\n\n    plt.subplot(133)\n    rng = [i for i in range(epochs)]\n    lr = history.history[\"lr\"]\n    plt.plot(rng, lr, '-o')\n    plt.xlabel('Epoch',size=14)\n    plt.ylabel('Learning Rate',size=14)\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:37:45.245013Z","iopub.execute_input":"2021-12-23T01:37:45.245254Z","iopub.status.idle":"2021-12-23T01:37:45.253118Z","shell.execute_reply.started":"2021-12-23T01:37:45.24522Z","shell.execute_reply":"2021-12-23T01:37:45.252397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Activaiton Maps of External MHA Transformer Blocks ","metadata":{}},{"cell_type":"code","source":"# build model with transformer blocks only \ntransformer_feature_blocks = [layer.output for layer in model.layers if isinstance(layer, AttentionEncoder)]\ntrans_act_model = Model(inputs=model.inputs, outputs=transformer_feature_blocks)\n\n# only transformer blocks\ntf.keras.utils.plot_model(trans_act_model, show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:37:45.254332Z","iopub.execute_input":"2021-12-23T01:37:45.254752Z","iopub.status.idle":"2021-12-23T01:37:45.45589Z","shell.execute_reply.started":"2021-12-23T01:37:45.254715Z","shell.execute_reply":"2021-12-23T01:37:45.455061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(sample_img, sample_feat), sample_gt = next(iter(validation_dataset))\nprint(sample_img.shape, sample_feat.shape)\n\nplt.figure(figsize=(10, 10))\nplt.axis('off')\nplt.imshow(sample_img[5])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:37:45.457556Z","iopub.execute_input":"2021-12-23T01:37:45.45797Z","iopub.status.idle":"2021-12-23T01:37:46.398786Z","shell.execute_reply.started":"2021-12-23T01:37:45.457931Z","shell.execute_reply":"2021-12-23T01:37:46.397322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_maps = trans_act_model.predict(\n    (\n        tf.expand_dims(sample_img[5],  axis=0),\n        tf.expand_dims(sample_feat[5], axis=0)\n    )\n)\n\nfeats_maps  = np.array(list(zip(*feature_maps))[0]).squeeze(axis=1) ; print(feats_maps.shape)\nattn_weight = np.array(list(zip(*feature_maps))[1]).squeeze(axis=1) ; print(attn_weight.shape)\n\nfor i, feature_map in enumerate(feats_maps):\n    print('ExtTransformerBlock ', i)\n    num_sequence, channel = feature_map.shape\n    height = width = int(np.sqrt(num_sequence))\n    feature_map = tf.reshape(feature_map, shape=(-1, height, width, channel))\n    ix = 1\n    plt.figure(figsize=(25, 25))\n    for _ in range(64):\n        ax = plt.subplot(10, 10, ix)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        plt.imshow(feature_map[0, :, :, ix - 1], cmap=\"viridis\")\n        ix += 1\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:37:46.400057Z","iopub.execute_input":"2021-12-23T01:37:46.402602Z","iopub.status.idle":"2021-12-23T01:38:30.946797Z","shell.execute_reply.started":"2021-12-23T01:37:46.402556Z","shell.execute_reply":"2021-12-23T01:38:30.944517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# OOF + Ensemble Inference \n\n- **Classification Head Models**\n- **Let's try TTA later.**\n- **We'll try [RAPIDS SVR](https://www.kaggle.com/cdeotte/rapids-svr-boost-17-8) next.**","metadata":{}},{"cell_type":"code","source":"all_weit = '../input/pet-test-wg/ext_attn_wg'\nall_rmse = []\nall_pred = []\n\nfor fold_ in range(df.kfold.nunique())[:len(os.listdir(all_weit))]:\n    print('FOLD ', fold_)\n    # Get fold-wise samples \n    df_valid = df[df.kfold == fold_].reset_index(drop=True)\n    \n    # load trained model \n    model = get_model(plot_model = False,  print_summary = False,  with_compile  = True)\n    model.load_weights(f'{all_weit}/feature_model_{fold_}.h5')\n    \n    # get validaiton data set to compute Out-of-fold \n    cb_val_set = create_dataset(df_valid, \n                                 batch_size  = BATCH_SIZE,\n                                 is_labelled = True, \n                                 augment     = False,\n                                 repeat      = False, \n                                 shuffle     = False)\n    loss, rmse = model.evaluate(cb_val_set, verbose=1)\n    all_rmse.append(rmse)\n    \n    # Inference on test set \n    test_dataset = create_dataset(test_df, \n                                 batch_size  = BATCH_SIZE,\n                                 is_labelled = False, \n                                 augment     = False, \n                                 repeat      = False, \n                                 shuffle     = False)\n    fold_wise_pred = model.predict(test_dataset, verbose=1)\n    fold_wise_pred = [x * 100 for x in fold_wise_pred]\n    all_pred.append(fold_wise_pred)\n    del model","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:38:30.948024Z","iopub.execute_input":"2021-12-23T01:38:30.948726Z","iopub.status.idle":"2021-12-23T01:46:35.745173Z","shell.execute_reply.started":"2021-12-23T01:38:30.948685Z","shell.execute_reply":"2021-12-23T01:46:35.744365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Mean of all RMSE ', np.mean(all_rmse))\ntest_df['Pawpularity'] = np.mean(np.column_stack(all_pred), axis=1)\ntest_df = test_df[[\"Id\", \"Pawpularity\"]]\ntest_df['Id'] = test_df.Id.apply(lambda x: x.split('/')[-1].split('.')[0])\ntest_df.to_csv(\"submission.csv\", index=False)\ntest_df.head(15)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:46:35.748404Z","iopub.execute_input":"2021-12-23T01:46:35.748629Z","iopub.status.idle":"2021-12-23T01:46:35.767181Z","shell.execute_reply.started":"2021-12-23T01:46:35.748603Z","shell.execute_reply":"2021-12-23T01:46:35.766412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [Optional]: RAPIDS SVR \n\n[Reference.](https://www.kaggle.com/cdeotte/rapids-svr-boost-17-8) - [Discussion](https://www.kaggle.com/c/petfinder-pawpularity-score/discussion/276724). ","metadata":{}},{"cell_type":"code","source":"import cuml, pickle\nfrom cuml.svm import SVR\nprint('RAPIDS version',cuml.__version__,'\\n')","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:46:35.76834Z","iopub.execute_input":"2021-12-23T01:46:35.768881Z","iopub.status.idle":"2021-12-23T01:46:39.074055Z","shell.execute_reply.started":"2021-12-23T01:46:35.768832Z","shell.execute_reply":"2021-12-23T01:46:39.073275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LOAD_SVR_FROM_PATH = None\ndf = pd.read_csv('../input/pet-test-wg/train_df_folds.csv')\nprint('Train shape:', df.shape )\n\ntest_df = pd.read_csv(f'{DATA_DIR}test.csv')\ntest_df['Id'] = test_df['Id'].apply(lambda x: f'{TEST_DIR}{x}.jpg')\ntest_df['Pawpularity'] = 0\nprint('Train shape:', test_df.shape )","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:46:39.075405Z","iopub.execute_input":"2021-12-23T01:46:39.075793Z","iopub.status.idle":"2021-12-23T01:46:39.145356Z","shell.execute_reply.started":"2021-12-23T01:46:39.075752Z","shell.execute_reply":"2021-12-23T01:46:39.144508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dnn_test_preds = []\nsvr_test_preds = []\n\ndnn_val_preds = []\nsvr_val_preds = []\n\nval_true = []\n\nfor fold_ in range(df.kfold.nunique())[:len(os.listdir(all_weit))]:\n    print('-'*10)\n    print('FOLD',fold_)\n    print('-'*10)\n    \n    # build model \n    model = get_model(plot_model = False,   print_summary = False,  with_compile  = True)\n    model.load_weights(f'../input/pet-test-wg/ext_attn_wg/feature_model_{fold_}.h5')\n    \n    # move out the embedding and predictin layer \n    new_model = Model(model.input, [model.layers[-2].output, model.output])\n    \n    # get validation fold on current fold \n    df_valid = df[df.kfold == fold_].reset_index(drop=True)\n\n    name = f\"SVR_fold_{fold_}.pkl\" \n    if LOAD_SVR_FROM_PATH is None:\n        # extract embeddings \n        # get training fold on current fold \n        df_train = df[df.kfold != fold_].reset_index(drop=True)\n        training_dataset = create_dataset(df_train,\n                                          batch_size  = BATCH_SIZE, \n                                          is_labelled = False, \n                                          augment     = False,\n                                          repeat      = False, \n                                          shuffle     = False)\n        print('Extracting train embedding...')\n        embed, _ = new_model.predict(training_dataset, verbose=1)\n        \n        print('Fitting SVR...')\n        clf = SVR(C=20.0)\n        clf.fit(embed.astype('float32'), df_train.Pawpularity.values.astype('int32'))\n        pickle.dump(clf, open(name, \"wb\"))\n    else:\n        # LOAD RAPIDS SVR \n        print('Loading SVR...',LOAD_SVR_FROM_PATH+name)\n        clf = pickle.load(open(LOAD_SVR_FROM_PATH+name, \"rb\"))\n  \n    # test set \n    test_dataset = create_dataset(test_df, \n                                 batch_size  = BATCH_SIZE,\n                                 is_labelled = False, \n                                 augment     = False, \n                                 repeat      = False, \n                                 shuffle     = False)\n\n    print('Predicting test...')\n    # Step 1: Get Classification Prediction and Preceding layer feature / Embeddings \n    dnn_embed, dnn_test_pred = new_model.predict(test_dataset, verbose=1)\n    dnn_test_pred = [x * 100 for x in dnn_test_pred]\n    \n    # Step 2: Pass The Embeddings to RAPIDS-SVR Model \n    svr_test_pred = clf.predict(dnn_embed)\n    \n    # Step 3: Save \n    dnn_test_preds.append(dnn_test_pred)\n    svr_test_preds.append(svr_test_pred)\n\n    # OOF \n    valid_dataset = create_dataset(df_valid,\n                                   batch_size  = BATCH_SIZE, \n                                   is_labelled = False, \n                                   augment     = False,\n                                   repeat      = False, \n                                   shuffle     = False)\n    print('Predicting Out-of-Fold...')\n    # Step 1: Get Classification Prediction and Preceding layer feature / Embeddings \n    dnn_embed, dnn_val_pred = new_model.predict(valid_dataset, verbose=1)\n    dnn_val_pred = [x * 100 for x in dnn_val_pred]\n    \n    # Step 2: Pass The Embeddings to RAPIDS-SVR Model \n    svr_val_pred = clf.predict(dnn_embed)    \n    \n    # Step 3: Save \n    dnn_val_preds.append(dnn_val_pred)\n    svr_val_preds.append(svr_val_pred)\n    \n    # Step 4: Save GT for computing OOF \n    val_true.append(df_valid['Pawpularity'].values)\n \n    ##################\n    # COMPUTE RSME\n    rsme = np.sqrt( np.mean( (val_true[-1] - np.array(dnn_val_preds[-1]))**2.0 ) )\n    print('NN RSME =',rsme)\n    \n    rsme = np.sqrt( np.mean( (val_true[-1] - np.array(svr_val_preds[-1]))**2.0 ) )\n    print('SVR RSME =',rsme)\n    \n    w = 0.5\n    oof2 = (1-w)*np.array(dnn_val_preds[-1]) + w*np.array(svr_val_preds[-1])\n    rsme = np.sqrt( np.mean( (val_true[-1] - oof2)**2.0 ) )\n    print('Ensemble RSME =',rsme,'\\n')","metadata":{"execution":{"iopub.status.busy":"2021-12-23T01:46:39.146771Z","iopub.execute_input":"2021-12-23T01:46:39.147122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Additional Resources\n1. How to use it on my own dataset?\n    - First, understand the competition task and its data format. And try to relate with yours.\n    - Second, run this notebook successfully on the competition data.\n    - Lastly, replace the dataset with yours.\n2. More Code Exampels.\n    - [TF.Keras: EfficientNet Hybrid Swin Transformer TPU](https://www.kaggle.com/ipythonx/tf-keras-efficientnet-hybrid-swin-transformer-tpu) - [Discussion](https://www.kaggle.com/c/petfinder-pawpularity-score/discussion/280531).\n    - [[TF.Keras]:Learning to Resize Images for ViT Model](https://www.kaggle.com/ipythonx/tf-keras-learning-to-resize-images-for-vit-model) - [Discussion](https://www.kaggle.com/c/petfinder-pawpularity-score/discussion/280438).\n    - [Discussion: DOLG Models in TensorFlow 2 (Keras) Implementation](https://www.kaggle.com/c/petfinder-pawpularity-score/discussion/281914) - [TF.Keras Code](https://github.com/innat/DOLG-TensorFlow)","metadata":{}}]}