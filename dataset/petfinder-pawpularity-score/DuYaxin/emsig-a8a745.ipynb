{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install python-box timm pytorch-lightning==1.4.0 grad-cam ttach torchmetrics","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:51:48.708898Z","iopub.execute_input":"2022-01-29T00:51:48.709637Z","iopub.status.idle":"2022-01-29T00:52:17.431301Z","shell.execute_reply.started":"2022-01-29T00:51:48.709503Z","shell.execute_reply":"2022-01-29T00:52:17.430067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport warnings\nfrom pprint import pprint\nfrom glob import glob\nfrom tqdm import tqdm\nimport torchmetrics\n\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as T\nfrom box import Box\nfrom timm import create_model\nfrom sklearn.model_selection import StratifiedKFold\nfrom torchvision.io import read_image\nfrom torch.utils.data import DataLoader, Dataset\nfrom pytorch_grad_cam import GradCAMPlusPlus\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\n\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.utilities.seed import seed_everything\nfrom pytorch_lightning import callbacks\nfrom pytorch_lightning.callbacks.progress import ProgressBarBase\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning import LightningDataModule, LightningModule\n\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:52:17.434093Z","iopub.execute_input":"2022-01-29T00:52:17.434503Z","iopub.status.idle":"2022-01-29T00:52:25.314851Z","shell.execute_reply.started":"2022-01-29T00:52:17.434449Z","shell.execute_reply":"2022-01-29T00:52:25.313809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:52:25.316675Z","iopub.execute_input":"2022-01-29T00:52:25.317048Z","iopub.status.idle":"2022-01-29T00:52:25.32228Z","shell.execute_reply.started":"2022-01-29T00:52:25.316994Z","shell.execute_reply":"2022-01-29T00:52:25.321095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import timm\ntimm.list_models(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:52:25.326253Z","iopub.execute_input":"2022-01-29T00:52:25.326961Z","iopub.status.idle":"2022-01-29T00:52:25.358336Z","shell.execute_reply.started":"2022-01-29T00:52:25.326919Z","shell.execute_reply":"2022-01-29T00:52:25.357107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## config","metadata":{}},{"cell_type":"code","source":"# !rm ./convit_base -r","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:52:25.360327Z","iopub.execute_input":"2022-01-29T00:52:25.360674Z","iopub.status.idle":"2022-01-29T00:52:25.368373Z","shell.execute_reply.started":"2022-01-29T00:52:25.360609Z","shell.execute_reply":"2022-01-29T00:52:25.367361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {'seed': 42,\n          'root': '/kaggle/input/emsig-spectrum/', \n          'n_splits': 5,\n          'epoch': 20,\n          'trainer': {\n              'gpus': 1,\n              'accumulate_grad_batches': 4,\n              'progress_bar_refresh_rate': 1,\n              'fast_dev_run': False,\n              'num_sanity_val_steps': 0,\n              'resume_from_checkpoint': None,\n          },\n          'transform':{\n              'name': 'get_default_transforms',\n              'image_size': 384\n          },\n          'train_loader':{\n              'batch_size': 4,\n              'shuffle': True,\n              'num_workers': 4,\n              'pin_memory': False,\n              'drop_last': True,\n          },\n          'val_loader': {\n              'batch_size': 8,\n              'shuffle': False,\n              'num_workers': 4,\n              'pin_memory': False,\n              'drop_last': False\n         },\n          'model':{\n              'name': 'vit_large_patch16_384',\n              'output_dim': 6\n          },\n          'optimizer':{\n              'name': 'optim.AdamW',\n              'params':{\n                  'lr': 1e-5\n              },\n          },\n          'scheduler':{\n              'name': 'optim.lr_scheduler.CosineAnnealingWarmRestarts',\n              'params':{\n                  'T_0': 20,\n                  'eta_min': 1e-4,\n              }\n          },\n          'loss': 'nn.CrossEntropyLoss',\n}\n\nconfig = Box(config)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-29T00:52:25.370344Z","iopub.execute_input":"2022-01-29T00:52:25.370678Z","iopub.status.idle":"2022-01-29T00:52:25.383368Z","shell.execute_reply.started":"2022-01-29T00:52:25.370639Z","shell.execute_reply":"2022-01-29T00:52:25.3823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pprint(config)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:52:25.38543Z","iopub.execute_input":"2022-01-29T00:52:25.386233Z","iopub.status.idle":"2022-01-29T00:52:25.414297Z","shell.execute_reply.started":"2022-01-29T00:52:25.386141Z","shell.execute_reply":"2022-01-29T00:52:25.413419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## dataset","metadata":{}},{"cell_type":"code","source":"class PetfinderDataset(Dataset):\n    def __init__(self, df, image_size=384):\n        self._X = df[\"id\"].values\n        self._y = None\n        if \"target\" in df.keys():\n            self._y = df[\"target\"].values\n        self._transform = T.Resize([image_size, image_size])\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._X[idx]\n        image = read_image(image_path)\n        image = self._transform(image)\n        if self._y is not None:\n            label = self._y[idx]\n            return image, label\n        return image\n\nclass PetfinderDataModule(LightningDataModule):\n    def __init__(\n        self,\n        train_df,\n        val_df,\n        cfg,\n    ):\n        super().__init__()\n        self._train_df = train_df\n        self._val_df = val_df\n        self._cfg = cfg\n\n    def __create_dataset(self, train=True):\n        return (\n            PetfinderDataset(self._train_df, self._cfg.transform.image_size)\n            if train\n            else PetfinderDataset(self._val_df, self._cfg.transform.image_size)\n        )\n\n    def train_dataloader(self):\n        dataset = self.__create_dataset(True)\n        return DataLoader(dataset, **self._cfg.train_loader)\n\n    def val_dataloader(self):\n        dataset = self.__create_dataset(False)\n        return DataLoader(dataset, **self._cfg.val_loader)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:52:25.416087Z","iopub.execute_input":"2022-01-29T00:52:25.416476Z","iopub.status.idle":"2022-01-29T00:52:25.429233Z","shell.execute_reply.started":"2022-01-29T00:52:25.416433Z","shell.execute_reply":"2022-01-29T00:52:25.427818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## visualize data","metadata":{}},{"cell_type":"code","source":"torch.autograd.set_detect_anomaly(True)\nseed_everything(config.seed)\n\ndf = pd.read_csv('../input/emsig-train-label/train_labels.csv')\ndf[\"id\"] = df[\"id\"].apply(lambda x: os.path.join(config.root, \"crop_spectrum_train_pic\", x))","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:52:25.431674Z","iopub.execute_input":"2022-01-29T00:52:25.432086Z","iopub.status.idle":"2022-01-29T00:52:25.481573Z","shell.execute_reply.started":"2022-01-29T00:52:25.432031Z","shell.execute_reply":"2022-01-29T00:52:25.480427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['target'] = df['target'] - 1","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:52:25.483117Z","iopub.execute_input":"2022-01-29T00:52:25.483932Z","iopub.status.idle":"2022-01-29T00:52:25.505952Z","shell.execute_reply.started":"2022-01-29T00:52:25.483892Z","shell.execute_reply":"2022-01-29T00:52:25.504972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_dataloader = PetfinderDataModule(df, df, config).val_dataloader()\nimages, labels = iter(sample_dataloader).next()\n\nplt.figure(figsize=(12, 12))\nfor it, (image, label) in enumerate(zip(images[:16], labels[:16])):\n    plt.subplot(4, 4, it+1)\n    plt.imshow(image.permute(1, 2, 0))\n    plt.axis('off')\n    plt.title(f'target: {int(label)}')","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:52:25.510567Z","iopub.execute_input":"2022-01-29T00:52:25.510837Z","iopub.status.idle":"2022-01-29T00:52:27.502497Z","shell.execute_reply.started":"2022-01-29T00:52:25.510809Z","shell.execute_reply":"2022-01-29T00:52:27.501577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## augmentation","metadata":{}},{"cell_type":"code","source":"IMAGENET_MEAN = [0.485, 0.456, 0.406]  # RGB\nIMAGENET_STD = [0.229, 0.384, 0.225]  # RGB\n\n\ndef get_default_transforms():\n    transform = {\n        \"train\": T.Compose(\n            [\n#                 T.RandomHorizontalFlip(),\n#                 T.RandomVerticalFlip(),\n#                 T.RandomAffine(15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n#                 T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n                T.ConvertImageDtype(torch.float),\n                T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n            ]\n        ),\n        \"val\": T.Compose(\n            [\n                T.ConvertImageDtype(torch.float),\n                T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n            ]\n        ),\n    }\n    return transform\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:52:27.503844Z","iopub.execute_input":"2022-01-29T00:52:27.505153Z","iopub.status.idle":"2022-01-29T00:52:27.514929Z","shell.execute_reply.started":"2022-01-29T00:52:27.505072Z","shell.execute_reply":"2022-01-29T00:52:27.513758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## model","metadata":{}},{"cell_type":"code","source":"def mixup(x: torch.Tensor, y: torch.Tensor, alpha: float = 1.0):\n    assert alpha > 0, \"alpha should be larger than 0\"\n    assert x.size(0) > 1, \"Mixup cannot be applied to a single instance.\"\n\n    lam = np.random.beta(alpha, alpha)\n    rand_index = torch.randperm(x.size()[0])\n    mixed_x = lam * x + (1 - lam) * x[rand_index, :]\n    target_a, target_b = y, y[rand_index]\n    return mixed_x, target_a, target_b, lam\n\nclass Model(pl.LightningModule):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.__build_model()\n        self._criterion = eval(self.cfg.loss)()\n        self.transform = get_default_transforms()\n        self.save_hyperparameters(cfg)\n\n    def __build_model(self):\n        self.backbone = create_model(\n            self.cfg.model.name, pretrained=True, num_classes=0, in_chans=3\n        )\n        self.fc = nn.Sequential(\n            nn.Dropout(0.5), nn.Linear(self.backbone.num_features, self.cfg.model.output_dim)\n        )\n        \n\n    def forward(self, x):\n        f = self.backbone(x)\n        out = self.fc(f)\n        return out\n\n    def training_step(self, batch, batch_idx):\n        loss, pred, labels = self.__share_step(batch, 'train')\n        # print('train loss: ', loss)\n        return {'loss': loss, 'pred': pred, 'labels': labels}\n        \n    def validation_step(self, batch, batch_idx):\n        loss, pred, labels = self.__share_step(batch, 'val')\n        # print('valid loss: ', loss)\n        return {'pred': pred, 'labels': labels}\n    \n    def __share_step(self, batch, mode):\n        images, labels = batch\n#         labels = labels.float() / 100.0\n        images = self.transform[mode](images)\n        \n        pred = self.forward(images)\n        loss = self._criterion(pred, labels)\n        \n        pred_cpu = pred.detach().cpu()# * 100.\n        labels = labels.detach().cpu()# * 100.\n        return loss, pred_cpu, labels\n        \n    def training_epoch_end(self, outputs):\n        print('-----train-----')\n        self.__share_epoch_end(outputs, 'train')\n\n    def validation_epoch_end(self, outputs):\n        print('-----valid-----')\n        self.__share_epoch_end(outputs, 'val')    \n        \n    def __share_epoch_end(self, outputs, mode):\n        preds = []\n        labels = []\n        for out in outputs:\n            pred, label = out['pred'], out['labels']\n            preds.append(pred)\n            labels.append(label)\n        preds = torch.cat(preds)\n        labels = torch.cat(labels)\n        loss = self._criterion(preds, labels)\n        metrics = torchmetrics.functional.accuracy(preds, labels)\n        sfmx = nn.Softmax(dim = 1)\n        preds_prob = sfmx(preds)\n        cm = torchmetrics.functional.confusion_matrix(preds_prob, labels, num_classes=6, normalize=None, threshold=0.5, multilabel=False)\n        print(cm)\n        print('Accuracyï¼š ', metrics)\n        print('epoch loss: ', loss)\n#         print('preds: ',preds)\n#         print('labels: ',labels)\n        self.log(f'{mode}_loss', metrics)\n    \n    def check_gradcam(self, dataloader, target_layer, target_category=None, reshape_transform=None):\n        cam = GradCAMPlusPlus(\n            model=self,\n            target_layers = [target_layer], \n            use_cuda=self.cfg.trainer.gpus, \n            reshape_transform=reshape_transform)\n        \n        org_images, labels = iter(dataloader).next()\n        cam.batch_size = len(org_images)\n        images = self.transform['val'](org_images)\n        images = images.to(self.device)\n        logits = self.forward(images).squeeze(1)\n        pred = logits.sigmoid().detach().cpu().numpy() * 100\n        labels = labels.cpu().numpy()\n        \n        grayscale_cam = cam(input_tensor=images, eigen_smooth=True) #target_category=None, \n        org_images = org_images.detach().cpu().numpy().transpose(0, 2, 3, 1) / 255.\n        return org_images, grayscale_cam, pred, labels\n\n    def configure_optimizers(self):\n        optimizer = eval(self.cfg.optimizer.name)(\n            self.parameters(), **self.cfg.optimizer.params\n        )\n        scheduler = eval(self.cfg.scheduler.name)(\n            optimizer,\n            **self.cfg.scheduler.params\n        )\n        return [optimizer], [scheduler]","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:52:27.517057Z","iopub.execute_input":"2022-01-29T00:52:27.518049Z","iopub.status.idle":"2022-01-29T00:52:27.549661Z","shell.execute_reply.started":"2022-01-29T00:52:27.517933Z","shell.execute_reply":"2022-01-29T00:52:27.548622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## train","metadata":{}},{"cell_type":"code","source":"skf = StratifiedKFold(\n    n_splits=config.n_splits, shuffle=True, random_state=config.seed\n)\n\nFOLD = 0\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(df[\"id\"], df[\"target\"])):\n    print('FOLD: ', FOLD)\n    train_df = df.loc[train_idx].reset_index(drop=True)\n    val_df = df.loc[val_idx].reset_index(drop=True)\n    datamodule = PetfinderDataModule(train_df, val_df, config)\n    model = Model(config)\n    earystopping = EarlyStopping(monitor=\"val_loss\", patience=4)\n    lr_monitor = callbacks.LearningRateMonitor()\n    loss_checkpoint = callbacks.ModelCheckpoint(\n        filename=\"best_loss\",\n        monitor=\"val_loss\",\n        save_top_k=1,\n        mode=\"min\",\n        save_last=False,\n    )\n    logger = TensorBoardLogger(config.model.name)\n    \n    trainer = pl.Trainer(\n        logger=logger,\n        max_epochs=config.epoch,\n        callbacks=[lr_monitor, loss_checkpoint, earystopping],\n        **config.trainer,\n    )\n    trainer.fit(model, datamodule=datamodule)\n    PATH = './' + str(fold)\n    torch.save(model.state_dict(), PATH)\n    FOLD += 1","metadata":{"execution":{"iopub.status.busy":"2022-01-29T00:52:27.551618Z","iopub.execute_input":"2022-01-29T00:52:27.552552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# class activation map","metadata":{}},{"cell_type":"code","source":"# gradcam reshape_transform for vit\ndef reshape_transform(tensor, height=7, width=7):\n    result = tensor.reshape(tensor.size(0),\n                            height, width, tensor.size(2))\n\n    # like in CNNs.\n    result = result.permute(0, 3, 1, 2)\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # import torch, gc\n# # gc.collect()\n# # torch.cuda.empty_cache()\n\n# model = Model(config) \n# model.load_state_dict(torch.load(f'{config.model.name}/default/version_0/checkpoints/best_loss.ckpt')['state_dict'])\n# model = model.cuda().eval()\n# config.val_loader.batch_size = 16\n# datamodule = PetfinderDataModule(train_df, val_df, config)\n# target_category=None\n# images, grayscale_cams, preds, labels = model.check_gradcam(\n#                                             datamodule.val_dataloader(), \n#                                             target_layer=model.backbone.layers[-1].blocks[-1].norm1,\n# #                                             target_category=target_category,\n#                                             reshape_transform=reshape_transform)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(12, 12))\n# for it, (image, grayscale_cam, pred, label) in enumerate(zip(images, grayscale_cams, preds, labels)):\n#     plt.subplot(4, 4, it + 1)\n#     visualization = show_cam_on_image(image, grayscale_cam)\n#     plt.imshow(visualization)\n#     plt.title(f'pred: {pred} label: {label}')\n#     plt.axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# visualize result","metadata":{}},{"cell_type":"code","source":"from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n\npath = glob(f'./{config.model.name}/default/version_0/events*')[0]\nevent_acc = EventAccumulator(path, size_guidance={'scalars': 0})\nevent_acc.Reload()\n\nscalars = {}\nfor tag in event_acc.Tags()['scalars']:\n    events = event_acc.Scalars(tag)\n    scalars[tag] = [event.value for event in events]","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}