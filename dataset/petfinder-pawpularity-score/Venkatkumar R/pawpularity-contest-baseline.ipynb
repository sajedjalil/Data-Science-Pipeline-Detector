{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## ***About this Competition***\n\n### ***In this competition, your task is to predict engagement with a pet's profile based on the photograph for that profile. You are also provided with hand-labelled metadata for each photo. The dataset for this competition therefore comprises both images and tabular data.***\n","metadata":{}},{"cell_type":"markdown","source":"# Import Necessary library","metadata":{}},{"cell_type":"code","source":"#mathematical analysis\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os.path\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport os\nimport cv2\n#preprocessing\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn import preprocessing\n#Model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\n","metadata":{"execution":{"iopub.status.busy":"2021-09-26T04:22:01.406487Z","iopub.execute_input":"2021-09-26T04:22:01.406764Z","iopub.status.idle":"2021-09-26T04:22:01.415469Z","shell.execute_reply.started":"2021-09-26T04:22:01.406736Z","shell.execute_reply":"2021-09-26T04:22:01.414545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read data","metadata":{}},{"cell_type":"code","source":"#\ntrain_data = pd.read_csv('../input/petfinder-pawpularity-score/train.csv')\ntest_data = pd.read_csv('../input/petfinder-pawpularity-score/test.csv')\nsample = pd.read_csv('../input/petfinder-pawpularity-score/sample_submission.csv')\nprint('Shape of input data :',train_data.shape,test_data.shape,sample.shape)\nprint('Identify the Null values : ',train_data.isnull().sum())\ntrain_data.sample(2)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-26T04:22:01.539455Z","iopub.execute_input":"2021-09-26T04:22:01.539715Z","iopub.status.idle":"2021-09-26T04:22:01.584589Z","shell.execute_reply.started":"2021-09-26T04:22:01.539688Z","shell.execute_reply":"2021-09-26T04:22:01.583935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KFOLD","metadata":{}},{"cell_type":"code","source":"#add extra one columns\ntrain_data['kfold']=-1\n#Distributing the data 5 shares\nkfold = model_selection.KFold(n_splits=10, shuffle= True, random_state = 42)\nfor fold, (train_indicies, valid_indicies) in enumerate(kfold.split(X=train_data)):\n    #print(fold,train_indicies,valid_indicies)\n    train_data.loc[valid_indicies,'kfold'] = fold\n\n    \nprint(train_data.kfold.value_counts()) #total data 300000 = kfold split :5 * 60000\n\n#output of train folds data\ntrain_data.to_csv(\"trainfold_10.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T04:22:01.72676Z","iopub.execute_input":"2021-09-26T04:22:01.727368Z","iopub.status.idle":"2021-09-26T04:22:01.79107Z","shell.execute_reply.started":"2021-09-26T04:22:01.727322Z","shell.execute_reply":"2021-09-26T04:22:01.79034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize to Null data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"./trainfold_10.csv\")\n# Plot dataframe\nheat = train_data.corr().round(5)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(heat)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(16,16))\nax = sns.heatmap(heat, annot=False, mask=mask, cmap=\"RdYlGn\", annot_kws={\"weight\": \"bold\", \"fontsize\":13})\nax.set_title(\"Feature correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\",\n         rotation_mode=\"anchor\", rotation=0, ha=\"right\")\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-09-26T04:22:01.974605Z","iopub.execute_input":"2021-09-26T04:22:01.97488Z","iopub.status.idle":"2021-09-26T04:22:02.399197Z","shell.execute_reply.started":"2021-09-26T04:22:01.974844Z","shell.execute_reply":"2021-09-26T04:22:02.398478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"execution":{"iopub.status.busy":"2021-09-26T04:22:02.400699Z","iopub.execute_input":"2021-09-26T04:22:02.401051Z","iopub.status.idle":"2021-09-26T04:22:02.407448Z","shell.execute_reply.started":"2021-09-26T04:22:02.401013Z","shell.execute_reply":"2021-09-26T04:22:02.406771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBRegressor","metadata":{}},{"cell_type":"code","source":"#store the final_prediction data and score\nfinal_predictions = []\nscore= []\n\n#features(categorical and numerical datas separate)\nuseful_features = [c for c in train.columns if c not in (\"Id\",\"Pawpularity\",\"kfold\")]\nobject_cols = [col for col in useful_features]\n#numerical_cols = [col for col in useful_features]\ntest = test_data[useful_features]\n\nfor fold in range(10):\n    xtrain = train[train.kfold != fold].reset_index(drop=True)\n    xvalid = train[train.kfold == fold].reset_index(drop=True)\n    xtest = test.copy()\n    \n    ytrain = xtrain.Pawpularity\n    yvalid = xvalid.Pawpularity\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    #ordinal encode categorical colums and standardscaler is applied (mean0,sd=1)\n    ordinal_encoder = OrdinalEncoder()\n    \n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n\n    #Model hyperparameter of XGboostRegressor\n    xgb_params = {\n        'learning_rate': 0.2113303692287,\n        'subsample': 0.12703520389320402,\n        'colsample_bytree': 0.2566392406542389,\n        'max_depth': 2,\n        'booster': 'gbtree', \n        'reg_lambda': 0.0005172374569093787,\n        'reg_alpha': 0.001273145009879541,\n        'random_state':256,\n        'n_estimators':30000\n        \n        \n    }\n    \n    model= XGBRegressor(**xgb_params,\n                       tree_method='gpu_hist',\n                       predictor='gpu_predictor',\n                       gpu_id=0)\n    model.fit(xtrain,ytrain,early_stopping_rounds=100,eval_set=[(xvalid,yvalid)],verbose=False)\n    preds_valid = model.predict(xvalid)\n    \n    #Training model apply the test data and predict the output\n    test_pre = model.predict(xtest)\n    final_predictions.append(test_pre)\n    \n    #Rootmeansquared output\n    rms = mean_squared_error(yvalid,preds_valid,squared=False)\n    \n    score.append(rms)\n    #way of output is display\n    print(f\"fold:{fold},rmse:{rms}\")\n\n#mean of repeation of fold data and identify the  mean and standard deviation \nprint(np.mean(score),np.std(score))","metadata":{"execution":{"iopub.status.busy":"2021-09-26T04:22:02.586299Z","iopub.execute_input":"2021-09-26T04:22:02.586834Z","iopub.status.idle":"2021-09-26T04:22:05.160318Z","shell.execute_reply.started":"2021-09-26T04:22:02.586797Z","shell.execute_reply":"2021-09-26T04:22:05.15953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#store the final_prediction data and score\nfinal_predictions = []\nscore= []\n\n#features(categorical and numerical datas separate)\nuseful_features = [c for c in train.columns if c not in (\"Id\",\"Pawpularity\",\"kfold\")]\nobject_cols = [col for col in useful_features]\n#numerical_cols = [col for col in useful_features]\ntest = test_data[useful_features]\n\nfor fold in range(10):\n    xtrain = train[train.kfold != fold].reset_index(drop=True)\n    xvalid = train[train.kfold == fold].reset_index(drop=True)\n    xtest = test.copy()\n    \n    ytrain = xtrain.Pawpularity\n    yvalid = xvalid.Pawpularity\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    #ordinal encode categorical colums and standardscaler is applied (mean0,sd=1)\n    ordinal_encoder = OrdinalEncoder()\n    \n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n\n    #Model hyperparameter of XGboostRegressor\n    #lgb parameters\n    params_lgb = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    'subsample': 0.95312,\n    \"metric\": \"rmse\",\n    'learning_rate': 0.11635,\n    \"max_depth\": 2,\n    \"feature_fraction\": 0.2256038826485174,\n    \"bagging_fraction\": 0.7705303688019942,\n    \"min_child_samples\": 290,\n    \"reg_alpha\": 14.68267919457715,\n    \"reg_lambda\": 66.156,\n    \"max_bin\": 772,\n    \"min_data_per_group\": 177,\n    \"bagging_freq\": 1,\n    \"cat_smooth\": 96,\n    \"cat_l2\": 17,\n    \"verbosity\": -1,\n    'random_state':42,\n    'n_estimators':5000,\n    'colsample_bytree':0.1107,\n    'njobs':4\n    }\n    \n    lgb_train = lgb.Dataset(xtrain, ytrain)\n    lgb_val = lgb.Dataset(xvalid, yvalid)\n    \n    model = lgb.train(params=params_lgb,\n                      train_set=lgb_train,\n                      valid_sets=lgb_val,\n                      early_stopping_rounds=300,\n                      verbose_eval=1000)\n    \n   \n    preds_valid = model.predict(xvalid,num_iteration=model.best_iteration)\n    test_pre = model.predict(xtest,num_iteration=model.best_iteration)\n    final_predictions.append(test_pre)\n    \n    #Rootmeansquared output\n    rms = mean_squared_error(yvalid,preds_valid,squared=False)\n    \n    score.append(rms)\n    #way of output is display\n    print(f\"fold:{fold},rmse:{rms}\")\n\n#mean of repeation of fold data and identify the  mean and standard deviation \nprint(np.mean(score),np.std(score))","metadata":{"execution":{"iopub.status.busy":"2021-09-26T04:22:49.225484Z","iopub.execute_input":"2021-09-26T04:22:49.225873Z","iopub.status.idle":"2021-09-26T04:22:53.412329Z","shell.execute_reply.started":"2021-09-26T04:22:49.225838Z","shell.execute_reply":"2021-09-26T04:22:53.411024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostRegressor\n#store the final_prediction data and score\nfinal_predictions = []\nscore= []\n\n#features(categorical and numerical datas separate)\nuseful_features = [c for c in train.columns if c not in (\"Id\",\"Pawpularity\",\"kfold\")]\nobject_cols = [col for col in useful_features]\n#numerical_cols = [col for col in useful_features]\ntest = test_data[useful_features]\n\nfor fold in range(10):\n    xtrain = train[train.kfold != fold].reset_index(drop=True)\n    xvalid = train[train.kfold == fold].reset_index(drop=True)\n    xtest = test.copy()\n    \n    ytrain = xtrain.Pawpularity\n    yvalid = xvalid.Pawpularity\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    #ordinal encode categorical colums and standardscaler is applied (mean0,sd=1)\n    ordinal_encoder = OrdinalEncoder()\n    \n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n\n    #Model hyperparameter of XGboostRegressor\n    #catboost model\n    catpara={\n        'subsample': 0.95312,\n        'learning_rate': 0.0011356,\n        \"max_depth\": 6,\n        \"min_data_in_leaf\":77,\n        'random_state':42,\n        'n_estimators':8000,\n        'rsm':0.5,\n        'l2_leaf_reg': 0.02247766515106271\n    }\n    \n    model=CatBoostRegressor(**catpara)\n    model.fit(xtrain,ytrain,early_stopping_rounds=100,eval_set=[(xvalid,yvalid)],verbose=1000)\n    model.fit(xtrain,ytrain,early_stopping_rounds=100,eval_set=[(xvalid,yvalid)],verbose=False)\n    preds_valid = model.predict(xvalid)\n    \n    #Training model apply the test data and predict the output\n    test_pre = model.predict(xtest)\n    final_predictions.append(test_pre)\n    \n    #Rootmeansquared output\n    rms = mean_squared_error(yvalid,preds_valid,squared=False)\n    \n    score.append(rms)\n    #way of output is display\n    print(f\"fold:{fold},rmse:{rms}\")\n\n#mean of repeation of fold data and identify the  mean and standard deviation \nprint(np.mean(score),np.std(score))","metadata":{"execution":{"iopub.status.busy":"2021-09-26T04:35:12.767002Z","iopub.execute_input":"2021-09-26T04:35:12.767704Z","iopub.status.idle":"2021-09-26T04:35:58.373421Z","shell.execute_reply.started":"2021-09-26T04:35:12.767666Z","shell.execute_reply":"2021-09-26T04:35:58.372565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction output","metadata":{}},{"cell_type":"code","source":"#prediction of data\npreds = np.mean(np.column_stack(final_predictions),axis=1)\nprint(preds)\nsample.Pawpularity = preds\nsample.to_csv(\"submission.csv\",index=False)\nprint(\"success\")","metadata":{"execution":{"iopub.status.busy":"2021-09-26T04:36:15.348874Z","iopub.execute_input":"2021-09-26T04:36:15.349446Z","iopub.status.idle":"2021-09-26T04:36:15.358471Z","shell.execute_reply.started":"2021-09-26T04:36:15.34941Z","shell.execute_reply":"2021-09-26T04:36:15.357499Z"},"trusted":true},"execution_count":null,"outputs":[]}]}