{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install libraries\n!pip install '../input/offline-packages/Keras_Applications-1.0.8-py3-none-any.whl'\n!pip install '../input/offline-packages/efficientnet-1.1.1-py3-none-any.whl'","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:05:00.198801Z","iopub.execute_input":"2021-12-23T09:05:00.1992Z","iopub.status.idle":"2021-12-23T09:05:19.99785Z","shell.execute_reply.started":"2021-12-23T09:05:00.199136Z","shell.execute_reply":"2021-12-23T09:05:19.996605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport efficientnet.tfkeras as efn\n\nimport os\nfilenames = os.listdir('../input/petfinder-pawpularity-score')\nprint(filenames)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-23T09:05:20.001898Z","iopub.execute_input":"2021-12-23T09:05:20.002245Z","iopub.status.idle":"2021-12-23T09:05:20.012913Z","shell.execute_reply.started":"2021-12-23T09:05:20.002191Z","shell.execute_reply":"2021-12-23T09:05:20.011692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"Q = 30\nfeature_folds = 5\nbatch_size = 16\nepochs  = 10\nseed  = 4261\nverbose = 1\nLR  = 0.0005\nCHANNELS = 3\nIMG_SIZE = 384\n# SetAutoTune\nAUTOTUNE = tf.data.experimental.AUTOTUNE  \n\nroot_dir = '../input/petfinder-pawpularity-score/'\ntrain_meta = pd.read_csv(root_dir + 'train.csv')\ntest_meta = pd.read_csv(root_dir + 'test.csv')\ntrain_dir = root_dir + 'train/'\ntest_dir = root_dir + 'test/'","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:05:20.014205Z","iopub.execute_input":"2021-12-23T09:05:20.016644Z","iopub.status.idle":"2021-12-23T09:05:20.051978Z","shell.execute_reply.started":"2021-12-23T09:05:20.016591Z","shell.execute_reply":"2021-12-23T09:05:20.050956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_meta.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:05:20.05345Z","iopub.execute_input":"2021-12-23T09:05:20.054509Z","iopub.status.idle":"2021-12-23T09:05:20.074459Z","shell.execute_reply.started":"2021-12-23T09:05:20.054446Z","shell.execute_reply":"2021-12-23T09:05:20.073376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_meta['Id'] = train_meta['Id'].apply(lambda x: train_dir + x + '.jpg')\n\n# Set a specific label to be able to perform stratification\ntrain_meta['stratify_label'] = pd.qcut(train_meta['Pawpularity'], q = Q, labels = range(Q))\n\n# Label value to be used for feature model 'classification' training.\ntrain_meta['target_value'] = train_meta['Pawpularity'] / 100.\n\n# Summary\nprint('train_meta:{}'.format(train_meta.shape))\ntrain_meta.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:05:20.07794Z","iopub.execute_input":"2021-12-23T09:05:20.079584Z","iopub.status.idle":"2021-12-23T09:05:20.116665Z","shell.execute_reply.started":"2021-12-23T09:05:20.07954Z","shell.execute_reply":"2021-12-23T09:05:20.11549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_meta['Id'] = test_meta['Id'].apply(lambda x: test_dir + x + '.jpg')\ntest_meta['Pawpularity'] = 0\n\nprint('test_meta:{}'.format(test_meta.shape))\ntest_meta.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:05:20.118319Z","iopub.execute_input":"2021-12-23T09:05:20.118792Z","iopub.status.idle":"2021-12-23T09:05:20.141017Z","shell.execute_reply.started":"2021-12-23T09:05:20.118752Z","shell.execute_reply":"2021-12-23T09:05:20.139616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define strategy","metadata":{}},{"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    # On google colab (tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR'], pass tpu_address as param in below fn)\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    #If no TPU, uncomment below to check for GPU\n    #strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:05:20.151909Z","iopub.execute_input":"2021-12-23T09:05:20.153078Z","iopub.status.idle":"2021-12-23T09:05:20.165467Z","shell.execute_reply.started":"2021-12-23T09:05:20.153033Z","shell.execute_reply":"2021-12-23T09:05:20.164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Tf Dataset","metadata":{}},{"cell_type":"code","source":"def build_augmenter(is_labelled):\n    def augment(img):\n        # Only use basic augmentations...too much augmentation hurts performance\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_flip_up_down(img)\n        img = tf.image.random_saturation(img, 0.95, 1.05)\n        img = tf.image.random_brightness(img, 0.05)\n        img = tf.image.random_contrast(img, 0.95, 1.05)\n        img = tf.image.random_hue(img, 0.05)\n        \n        return img\n    \n    def augment_with_labels(img, label):\n        return augment(img), label\n    \n    return augment_with_labels if is_labelled else augment\n\ndef build_decoder(is_labelled):\n    def decode(path):\n        # Read Image\n        file_bytes = tf.io.read_file(path)\n        img = tf.image.decode_jpeg(file_bytes, channels = CHANNELS)\n        \n        # Normalize and Resize\n        img = tf.cast(img, tf.float32) / 255.0\n        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n        \n        return img\n    \n    def decode_with_labels(path, label):\n        return decode(path), label\n    \n    return decode_with_labels if is_labelled else decode\n\ndef create_dataset(df, batch_size = 32, is_labelled = False, augment = False, repeat = False, shuffle = False):\n    decode_fn = build_decoder(is_labelled)\n    augmenter_fn = build_augmenter(is_labelled)\n    \n    # Create Dataset\n    if is_labelled:\n        dataset = tf.data.Dataset.from_tensor_slices((df['Id'].values, df['target_value'].values))\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices((df['Id'].values))\n    dataset = dataset.map(decode_fn, num_parallel_calls = AUTOTUNE)\n    dataset = dataset.map(augmenter_fn, num_parallel_calls = AUTOTUNE) if augment else dataset\n    dataset = dataset.repeat() if repeat else dataset\n    dataset = dataset.shuffle(1024, reshuffle_each_iteration = True) if shuffle else dataset\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTOTUNE)\n    \n    return dataset","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:05:20.167655Z","iopub.execute_input":"2021-12-23T09:05:20.168128Z","iopub.status.idle":"2021-12-23T09:05:20.186294Z","shell.execute_reply.started":"2021-12-23T09:05:20.168081Z","shell.execute_reply":"2021-12-23T09:05:20.185116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Model","metadata":{}},{"cell_type":"code","source":"def unfreeze_model(model):\n    # Unfreeze layers while leaving BatchNorm layers frozen\n    for layer in model.layers:\n        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n            layer.trainable = True\n        else:\n            layer.trainable = False\n\ndef create_model(): \n    # Create and Compile Model and show Summary\n#     effnet_model = tf.keras.applications.EfficientNetB1(weights='imagenet', \n#                                                             include_top=False, \n#                                                             input_shape = (IMG_SIZE, IMG_SIZE, CHANNELS), pooling='avg')\n    \n    effnet_model = efn.EfficientNetB2(include_top = False, \n                                      classes = None, \n                                      input_shape = (IMG_SIZE, IMG_SIZE, CHANNELS), \n                                      weights = '../input/weights/efficientnet-b2_noisy-student_notop.h5',\n                                      pooling = 'avg')\n\n    # Set all layers to Trainable except BN layers\n    unfreeze_model(effnet_model)\n    \n    X = tf.keras.layers.Dropout(0.25)(effnet_model.output)\n    output = tf.keras.layers.Dense(1, activation = 'sigmoid')(X)\n    \n    # Create Final Model\n    model = tf.keras.Model(inputs = effnet_model.input, outputs = output)\n\n    # Compile\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = LR), \n                  loss = tf.keras.losses.BinaryCrossentropy(), \n                  metrics = [tf.keras.metrics.RootMeanSquaredError('rmse')])        \n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:05:20.188557Z","iopub.execute_input":"2021-12-23T09:05:20.18898Z","iopub.status.idle":"2021-12-23T09:05:20.202939Z","shell.execute_reply.started":"2021-12-23T09:05:20.188922Z","shell.execute_reply":"2021-12-23T09:05:20.20179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_dataset = create_dataset(train_meta,\n                                  batch_size  = batch_size, \n                                  is_labelled = True, \n                                  augment = True,\n                                  repeat  = False, \n                                  shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:05:20.218265Z","iopub.execute_input":"2021-12-23T09:05:20.218803Z","iopub.status.idle":"2021-12-23T09:05:20.575455Z","shell.execute_reply.started":"2021-12-23T09:05:20.218742Z","shell.execute_reply":"2021-12-23T09:05:20.574496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define callbacks","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import losses, optimizers , metrics\nfrom tensorflow.keras import callbacks\n\ndef get_lr_callback(batch_size=8):\n    lr_start   = 0.000005\n    lr_max     = 0.00000125 * batch_size \n    lr_min     = 0.000001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n        return lr\n    return callbacks.LearningRateScheduler(lrfn, verbose=True)\n\ndef model_callback(fold):\n    ckpt = tf.keras.callbacks.ModelCheckpoint(f'feature_model_{fold}.h5',\n                                              verbose = 1, \n                                              monitor = 'val_rmse',\n                                              mode = 'min', \n                                              save_weights_only = True,\n                                              save_best_only = True)\n    \n    return [ckpt, get_lr_callback(batch_size)]","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:05:20.596308Z","iopub.execute_input":"2021-12-23T09:05:20.597431Z","iopub.status.idle":"2021-12-23T09:05:20.608007Z","shell.execute_reply.started":"2021-12-23T09:05:20.597371Z","shell.execute_reply":"2021-12-23T09:05:20.606717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check images in dataset","metadata":{}},{"cell_type":"code","source":"sample_images, _ = next(iter(training_dataset))\n\nimport matplotlib.pyplot as plt \n\nplt.figure(figsize=(16, 10))\nfor i, image in enumerate(sample_images[:6]):\n    print(image.shape)\n    ax = plt.subplot(3, 4, 2 * i + 1)\n    plt.title(\"Input Image\")\n    plt.imshow(image.numpy().squeeze())\n    plt.axis(\"off\")\n\n#     ax = plt.subplot(3, 4, 2 * i + 2)\n#     resized_image = learnable_resizer(image[None, ...])\n#     plt.title(\"Resized Image\")\n#     plt.imshow(resized_image.numpy().squeeze())\n#     plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:05:20.620506Z","iopub.execute_input":"2021-12-23T09:05:20.620835Z","iopub.status.idle":"2021-12-23T09:05:21.536995Z","shell.execute_reply.started":"2021-12-23T09:05:20.620793Z","shell.execute_reply":"2021-12-23T09:05:21.535047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create and train models","metadata":{}},{"cell_type":"code","source":"import gc\nfrom sklearn.model_selection import StratifiedKFold\n\n# OOF RMSE Placeholder\nall_val_loss = []\nkfold = StratifiedKFold(n_splits = feature_folds, \n                        shuffle = True, random_state = seed)\nfor fold, (train_index, val_index) in enumerate(kfold.split(train_meta.index,\n                                                            train_meta['stratify_label'])):\n#     if fold == 1:\n    print(f'\\nFold {fold}\\n')\n    # Pre model.fit cleanup\n    tf.keras.backend.clear_session()\n    gc.collect()\n\n    # Create Model\n    model = create_model()\n#     for i in range(len(model.weights)):\n#         model.weights[i]._handle_name = model.weights[i].name + str(i)\n\n    # Create TF Datasets\n    trn = train_meta.iloc[train_index]\n    val = train_meta.iloc[val_index]\n    training_dataset = create_dataset(trn, \n                                      batch_size  = batch_size, \n                                      is_labelled = True, \n                                      augment     = True, \n                                      repeat      = True, \n                                      shuffle     = True)\n    validation_dataset = create_dataset(val, \n                                        batch_size  = batch_size, \n                                        is_labelled = True,\n                                        augment     = False, \n                                        repeat      = True,\n                                        shuffle     = False)\n    # Fit Model\n    history = model.fit(training_dataset,\n                        epochs = epochs,\n                        steps_per_epoch  = trn.shape[0] // batch_size,\n                        validation_steps = val.shape[0] // batch_size,\n                        callbacks = model_callback(fold),\n                        validation_data = validation_dataset,\n                        verbose = 1)   \n\n#         # Validation Information\n#         best_val_loss = min(history.history['val_rmse'])\n#         all_val_loss.append(best_val_loss)\n#         print(f'\\nValidation RMSE: {best_val_loss}\\n')","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:05:21.539005Z","iopub.execute_input":"2021-12-23T09:05:21.539662Z","iopub.status.idle":"2021-12-23T13:08:23.258207Z","shell.execute_reply.started":"2021-12-23T09:05:21.539605Z","shell.execute_reply":"2021-12-23T13:08:23.256067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create submission file\nWe will calculate average of predictions from all 5 models to get our final prediction.","metadata":{}},{"cell_type":"code","source":"submission_df = pd.read_csv('{}sample_submission.csv'.format(root_dir))\npred = 0\n\nfor fold_index in range(feature_folds):\n    model = create_model()\n    model.load_weights('feature_model_{}.h5'.format(fold_index))\n    \n    cb_test_set = create_dataset(test_meta, \n                             batch_size  = batch_size,\n                             is_labelled = False,\n                             repeat      = False, \n                             shuffle     = False)\n    pred = pred + model.predict(cb_test_set)*100\n    \nsubmission_df['Pawpularity'] = pred/5\nsubmission_df.to_csv('submission.csv', index = False)\nsubmission_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T13:22:01.01151Z","iopub.execute_input":"2021-12-23T13:22:01.011792Z","iopub.status.idle":"2021-12-23T13:22:32.295355Z","shell.execute_reply.started":"2021-12-23T13:22:01.011764Z","shell.execute_reply":"2021-12-23T13:22:32.294388Z"},"trusted":true},"execution_count":null,"outputs":[]}]}