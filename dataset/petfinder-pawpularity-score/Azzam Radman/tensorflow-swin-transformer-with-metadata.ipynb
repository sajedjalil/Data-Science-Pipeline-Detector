{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport math\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import (Input, Conv2D, BatchNormalization, Dropout, Dense, MaxPooling2D, \n                                     ReLU, Flatten, Softmax, GlobalAveragePooling2D, Concatenate, Add)\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-04T08:30:16.889657Z","iopub.execute_input":"2021-12-04T08:30:16.88999Z","iopub.status.idle":"2021-12-04T08:30:22.248338Z","shell.execute_reply.started":"2021-12-04T08:30:16.889917Z","shell.execute_reply":"2021-12-04T08:30:22.24763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading Train Images","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/petfinder-pawpularity-score/train.csv')\ntrain_df['Id'] = train_df['Id'].apply(lambda x: x + '.jpg')\nprint(train_df.shape)\ndirs_df = pd.DataFrame(columns=['dirs'])\ndirs_df['dirs'] = train_df[['Id']]\ntrain_labels = train_df['Pawpularity']","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:22.251722Z","iopub.execute_input":"2021-12-04T08:30:22.251924Z","iopub.status.idle":"2021-12-04T08:30:22.30821Z","shell.execute_reply.started":"2021-12-04T08:30:22.251897Z","shell.execute_reply":"2021-12-04T08:30:22.307518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_splits = 10\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2021)\ndirs_df['fold'] = -1\ndirs_df['label'] = train_labels\ntrain_df['fold'] = -1\n\nfor fold, (tr, val) in enumerate(skf.split(dirs_df, train_labels)):\n    dirs_df.loc[val, 'fold'] = fold\n    train_df.loc[val, 'fold'] = fold","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:22.309398Z","iopub.execute_input":"2021-12-04T08:30:22.309794Z","iopub.status.idle":"2021-12-04T08:30:22.338197Z","shell.execute_reply.started":"2021-12-04T08:30:22.309756Z","shell.execute_reply":"2021-12-04T08:30:22.337503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dirs = dirs_df[~dirs_df['fold'].isin([5, 6, 9])].drop(['fold', 'label'], axis=1)\ntrain_labs = dirs_df[~dirs_df['fold'].isin([5, 6, 9])].drop(['fold', 'dirs'], axis=1)\nX_train = train_df[~train_df['fold'].isin([5, 6, 9])].drop(['fold', 'Pawpularity', 'Id'], axis=1)\ny_train = train_df[~train_df['fold'].isin([5, 6, 9])]['Pawpularity']\n\n\nvalid_dirs = dirs_df[dirs_df['fold'].isin([5, 6])].drop(['fold', 'label'], axis=1)\nvalid_labs = dirs_df[dirs_df['fold'].isin([5, 6])].drop(['fold', 'dirs'], axis=1)\nX_valid = train_df[~train_df['fold'].isin([5, 6, 9])].drop(['fold', 'Pawpularity', 'Id'], axis=1)\ny_valid = train_df[~train_df['fold'].isin([5, 6, 9])]['Pawpularity']\n\n\ntest_dirs = dirs_df[dirs_df['fold'] == 9].drop(['fold', 'label'], axis=1)\ntest_labs = dirs_df[dirs_df['fold'] == 9].drop(['fold', 'dirs'], axis=1)\nX_test = train_df[~train_df['fold'].isin([5, 6, 9])].drop(['fold', 'Pawpularity', 'Id'], axis=1)\ny_test = train_df[~train_df['fold'].isin([5, 6, 9])]['Pawpularity']","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:22.340255Z","iopub.execute_input":"2021-12-04T08:30:22.340735Z","iopub.status.idle":"2021-12-04T08:30:22.372907Z","shell.execute_reply.started":"2021-12-04T08:30:22.340698Z","shell.execute_reply":"2021-12-04T08:30:22.372293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_tensor = tf.data.Dataset.from_tensor_slices(X_train)\ny_train_tensor = tf.data.Dataset.from_tensor_slices(y_train)\n\nX_valid_tensor = tf.data.Dataset.from_tensor_slices(X_valid)\ny_valid_tensor = tf.data.Dataset.from_tensor_slices(y_valid)\n\nX_test_tensor = tf.data.Dataset.from_tensor_slices(X_test)\ny_test_tensor = tf.data.Dataset.from_tensor_slices(y_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:22.374479Z","iopub.execute_input":"2021-12-04T08:30:22.374921Z","iopub.status.idle":"2021-12-04T08:30:24.881016Z","shell.execute_reply.started":"2021-12-04T08:30:22.374885Z","shell.execute_reply":"2021-12-04T08:30:24.88009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir = '../input/petfinder-pawpularity-score/train/'\ntrain_dirs = [train_dir+branch for branch in train_dirs.dirs.tolist()]\nvalid_dirs = [train_dir+branch for branch in valid_dirs.dirs.tolist()]\ntest_dirs = [train_dir+branch for branch in test_dirs.dirs.tolist()]","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:24.882482Z","iopub.execute_input":"2021-12-04T08:30:24.88299Z","iopub.status.idle":"2021-12-04T08:30:24.891306Z","shell.execute_reply.started":"2021-12-04T08:30:24.882951Z","shell.execute_reply":"2021-12-04T08:30:24.890523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = pd.read_csv('../input/petfinder-pawpularity-score/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:24.892588Z","iopub.execute_input":"2021-12-04T08:30:24.892833Z","iopub.status.idle":"2021-12-04T08:30:24.90493Z","shell.execute_reply.started":"2021-12-04T08:30:24.892798Z","shell.execute_reply":"2021-12-04T08:30:24.904235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Tesorflow Dataset","metadata":{}},{"cell_type":"code","source":"train_ds = tf.data.Dataset.list_files(train_dirs, shuffle=False)\nvalid_ds = tf.data.Dataset.list_files(valid_dirs, shuffle=False)\ntest_ds = tf.data.Dataset.list_files(test_dirs, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:24.9064Z","iopub.execute_input":"2021-12-04T08:30:24.906676Z","iopub.status.idle":"2021-12-04T08:30:30.512831Z","shell.execute_reply.started":"2021-12-04T08:30:24.906641Z","shell.execute_reply":"2021-12-04T08:30:30.51185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_image_counts = len(train_ds)\nprint(\"Number of images =\", train_image_counts)\n\nvalid_image_counts = len(valid_ds)\nprint(\"Number of images =\", valid_image_counts)\n\ntest_image_counts = len(test_ds)\nprint(\"Number of images =\", test_image_counts)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:30.514588Z","iopub.execute_input":"2021-12-04T08:30:30.515005Z","iopub.status.idle":"2021-12-04T08:30:30.525192Z","shell.execute_reply.started":"2021-12-04T08:30:30.514967Z","shell.execute_reply":"2021-12-04T08:30:30.524071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for path in train_ds.take(5):\n    print(path.numpy())","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:30.528957Z","iopub.execute_input":"2021-12-04T08:30:30.529247Z","iopub.status.idle":"2021-12-04T08:30:30.569437Z","shell.execute_reply.started":"2021-12-04T08:30:30.529205Z","shell.execute_reply":"2021-12-04T08:30:30.568734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing ","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 32\nEPOCHS = 100\nVERBOSE = 1","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:30.570675Z","iopub.execute_input":"2021-12-04T08:30:30.571032Z","iopub.status.idle":"2021-12-04T08:30:30.57467Z","shell.execute_reply.started":"2021-12-04T08:30:30.570999Z","shell.execute_reply":"2021-12-04T08:30:30.573934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_image(file_path):\n    # to read the file in tensorflow we need this function\n    img = tf.io.read_file(file_path)\n    # decode the image\n    img = tf.image.decode_jpeg(img)\n    # resize the image to have the same size for all images\n    img = tf.image.resize(img, [224, 224])\n    img = img/255.0\n    return img","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:30.576004Z","iopub.execute_input":"2021-12-04T08:30:30.576556Z","iopub.status.idle":"2021-12-04T08:30:30.583263Z","shell.execute_reply.started":"2021-12-04T08:30:30.576517Z","shell.execute_reply":"2021-12-04T08:30:30.582612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE\ntrain_ds_images = train_ds.map(process_image)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:30.584535Z","iopub.execute_input":"2021-12-04T08:30:30.585131Z","iopub.status.idle":"2021-12-04T08:30:30.66525Z","shell.execute_reply.started":"2021-12-04T08:30:30.585094Z","shell.execute_reply":"2021-12-04T08:30:30.664391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_image(file_path):\n    # to read the file in tensorflow we need this function\n    img = tf.io.read_file(file_path)\n    # decode the image\n    img = tf.image.decode_jpeg(img)\n    # resize the image to have the same size for all images\n    img = tf.image.resize(img, [224, 224])\n    img = img/255.0\n    return img","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:30.667468Z","iopub.execute_input":"2021-12-04T08:30:30.667904Z","iopub.status.idle":"2021-12-04T08:30:30.673216Z","shell.execute_reply.started":"2021-12-04T08:30:30.667853Z","shell.execute_reply":"2021-12-04T08:30:30.672466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_ds_images = valid_ds.map(process_image)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:30.67561Z","iopub.execute_input":"2021-12-04T08:30:30.676244Z","iopub.status.idle":"2021-12-04T08:30:30.692874Z","shell.execute_reply.started":"2021-12-04T08:30:30.676198Z","shell.execute_reply":"2021-12-04T08:30:30.692124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_image(file_path):\n    # to read the file in tensorflow we need this function\n    img = tf.io.read_file(file_path)\n    # decode the image\n    img = tf.image.decode_jpeg(img)\n    # resize the image to have the same size for all images\n    img = tf.image.resize(img, [224, 224])\n    img = img/255.0\n    return img","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:30.694266Z","iopub.execute_input":"2021-12-04T08:30:30.694783Z","iopub.status.idle":"2021-12-04T08:30:30.701801Z","shell.execute_reply.started":"2021-12-04T08:30:30.694742Z","shell.execute_reply":"2021-12-04T08:30:30.700861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds_images = test_ds.map(process_image)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:30.703223Z","iopub.execute_input":"2021-12-04T08:30:30.704713Z","iopub.status.idle":"2021-12-04T08:30:30.720612Z","shell.execute_reply.started":"2021-12-04T08:30:30.704672Z","shell.execute_reply":"2021-12-04T08:30:30.719888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tensor = tf.data.Dataset.zip(({'img': train_ds_images, 'table': X_train_tensor}, \n                                    y_train_tensor)).shuffle(1000).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\nvalid_tensor = tf.data.Dataset.zip(({'img': valid_ds_images, 'table': X_valid_tensor}, \n                                    y_valid_tensor)).shuffle(1000).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\ntest_tensor = tf.data.Dataset.zip(({'img': test_ds_images, 'table': X_test_tensor}, \n                                    y_test_tensor)).shuffle(1000).batch(BATCH_SIZE).prefetch(AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:30.721964Z","iopub.execute_input":"2021-12-04T08:30:30.722431Z","iopub.status.idle":"2021-12-04T08:30:30.740006Z","shell.execute_reply.started":"2021-12-04T08:30:30.722396Z","shell.execute_reply":"2021-12-04T08:30:30.739321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(5):\n    for x, y in train_tensor:\n        img = x['img']\n        table = x['table']\n        print(img.shape)\n        print(table.shape)\n        print(y.shape)\n        break\n    break","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:30.741281Z","iopub.execute_input":"2021-12-04T08:30:30.741737Z","iopub.status.idle":"2021-12-04T08:30:35.962127Z","shell.execute_reply.started":"2021-12-04T08:30:30.741702Z","shell.execute_reply":"2021-12-04T08:30:35.961271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(1):\n    for x, y in valid_tensor:\n        img = x['img']\n        table = x['table']\n        print(img.shape)\n        print(table.shape)\n        print(y.shape)\n        break\n    break","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:35.963557Z","iopub.execute_input":"2021-12-04T08:30:35.963861Z","iopub.status.idle":"2021-12-04T08:30:40.653051Z","shell.execute_reply.started":"2021-12-04T08:30:35.963826Z","shell.execute_reply":"2021-12-04T08:30:40.652261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(1):\n    for x, y in test_tensor:\n        img = x['img']\n        table = x['table']\n        print(img.shape)\n        print(table.shape)\n        print(y.shape)\n        break\n    break","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:40.654282Z","iopub.execute_input":"2021-12-04T08:30:40.654814Z","iopub.status.idle":"2021-12-04T08:30:45.421681Z","shell.execute_reply.started":"2021-12-04T08:30:40.654775Z","shell.execute_reply":"2021-12-04T08:30:45.421013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading test images","metadata":{}},{"cell_type":"code","source":"def test_process_image(file_path):\n    # to read the file in tensorflow we need this function\n    img = tf.io.read_file(file_path)\n    # decode the image\n    img = tf.image.decode_jpeg(img)\n    # resize the image to have the same size for all images\n    img = tf.image.resize(img, [224, 224])\n    img = img/255.0\n    return img","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:45.426424Z","iopub.execute_input":"2021-12-04T08:30:45.428925Z","iopub.status.idle":"2021-12-04T08:30:45.435304Z","shell.execute_reply.started":"2021-12-04T08:30:45.428876Z","shell.execute_reply":"2021-12-04T08:30:45.434618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_images_ds = tf.data.Dataset.list_files('../input/petfinder-pawpularity-score/test/*', shuffle=False)\ntest_images_ds = test_images_ds.map(test_process_image)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:45.436426Z","iopub.execute_input":"2021-12-04T08:30:45.436756Z","iopub.status.idle":"2021-12-04T08:30:45.506531Z","shell.execute_reply.started":"2021-12-04T08:30:45.436717Z","shell.execute_reply":"2021-12-04T08:30:45.505892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('../input/petfinder-pawpularity-score/test.csv').drop('Id', axis=1)\ntest_ds = tf.data.Dataset.from_tensor_slices(test_df)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:45.507817Z","iopub.execute_input":"2021-12-04T08:30:45.508055Z","iopub.status.idle":"2021-12-04T08:30:45.538768Z","shell.execute_reply.started":"2021-12-04T08:30:45.508023Z","shell.execute_reply":"2021-12-04T08:30:45.538153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds_tensor = tf.data.Dataset.zip(({'img': test_images_ds, 'table': test_ds})).batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:45.54002Z","iopub.execute_input":"2021-12-04T08:30:45.54026Z","iopub.status.idle":"2021-12-04T08:30:45.545304Z","shell.execute_reply.started":"2021-12-04T08:30:45.540227Z","shell.execute_reply":"2021-12-04T08:30:45.544614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Swin Transformer","metadata":{}},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    BATCH_SIZE = tpu_strategy.num_replicas_in_sync * 64\n    print(\"Running on TPU:\", tpu.master())\n    print(f\"Batch Size: {BATCH_SIZE}\")\n    \nexcept ValueError:\n    strategy = tf.distribute.get_strategy()\n    BATCH_SIZE = BATCH_SIZE\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    print(f\"Batch Size: {BATCH_SIZE}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:45.546643Z","iopub.execute_input":"2021-12-04T08:30:45.547044Z","iopub.status.idle":"2021-12-04T08:30:45.556206Z","shell.execute_reply.started":"2021-12-04T08:30:45.54701Z","shell.execute_reply":"2021-12-04T08:30:45.555465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape = (224, 224, 3)\npatch_size = (4, 4)  # 2-by-2 sized patches\ndropout_rate = 0.03  # Dropout rate\nnum_heads = 8  # Attention heads\nembed_dim = 64  # Embedding dimension\nnum_mlp = 256  # MLP layer size\nqkv_bias = True  # Convert embedded patches to query, key, and values with a learnable additive value\nwindow_size = 2  # Size of attention window\nshift_size = 1  # Size of shifting window\nimage_dimension = 224  # Initial image size\n\nnum_patch_x = input_shape[0] // patch_size[0]\nnum_patch_y = input_shape[1] // patch_size[1]\n\nlearning_rate = 3e-4\nbatch_size = 32\nnum_epochs = 40\nvalidation_split = 0.1\nweight_decay = 1e-5\nlabel_smoothing = 0.1","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:45.557275Z","iopub.execute_input":"2021-12-04T08:30:45.557513Z","iopub.status.idle":"2021-12-04T08:30:45.566022Z","shell.execute_reply.started":"2021-12-04T08:30:45.557482Z","shell.execute_reply":"2021-12-04T08:30:45.565252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def window_partition(x, window_size):\n    _, height, width, channels = x.shape\n    patch_num_y = height // window_size\n    patch_num_x = width // window_size\n    x = tf.reshape(\n        x, shape=(-1, patch_num_y, window_size, patch_num_x, window_size, channels)\n    )\n    x = tf.transpose(x, (0, 1, 3, 2, 4, 5))\n    windows = tf.reshape(x, shape=(-1, window_size, window_size, channels))\n    return windows\n\n\ndef window_reverse(windows, window_size, height, width, channels):\n    patch_num_y = height // window_size\n    patch_num_x = width // window_size\n    x = tf.reshape(\n        windows,\n        shape=(-1, patch_num_y, patch_num_x, window_size, window_size, channels),\n    )\n    x = tf.transpose(x, perm=(0, 1, 3, 2, 4, 5))\n    x = tf.reshape(x, shape=(-1, height, width, channels))\n    return x\n\n\nclass DropPath(layers.Layer):\n    def __init__(self, drop_prob=None, **kwargs):\n        super(DropPath, self).__init__(**kwargs)\n        self.drop_prob = drop_prob\n\n    def call(self, x):\n        input_shape = tf.shape(x)\n        batch_size = input_shape[0]\n        rank = x.shape.rank\n        shape = (batch_size,) + (1,) * (rank - 1)\n        random_tensor = (1 - self.drop_prob) + tf.random.uniform(shape, dtype=x.dtype)\n        path_mask = tf.floor(random_tensor)\n        output = tf.math.divide(x, 1 - self.drop_prob) * path_mask\n        return output","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:45.572293Z","iopub.execute_input":"2021-12-04T08:30:45.572504Z","iopub.status.idle":"2021-12-04T08:30:45.586127Z","shell.execute_reply.started":"2021-12-04T08:30:45.572479Z","shell.execute_reply":"2021-12-04T08:30:45.585264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WindowAttention(layers.Layer):\n    def __init__(\n        self, dim, window_size, num_heads, qkv_bias=True, dropout_rate=0.0, **kwargs\n    ):\n        super(WindowAttention, self).__init__(**kwargs)\n        self.dim = dim\n        self.window_size = window_size\n        self.num_heads = num_heads\n        self.scale = (dim // num_heads) ** -0.5\n        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias)\n        self.dropout = layers.Dropout(dropout_rate)\n        self.proj = layers.Dense(dim)\n\n    def build(self, input_shape):\n        num_window_elements = (2 * self.window_size[0] - 1) * (\n            2 * self.window_size[1] - 1\n        )\n        self.relative_position_bias_table = self.add_weight(\n            shape=(num_window_elements, self.num_heads),\n            initializer=tf.initializers.Zeros(),\n            trainable=True,\n        )\n        coords_h = np.arange(self.window_size[0])\n        coords_w = np.arange(self.window_size[1])\n        coords_matrix = np.meshgrid(coords_h, coords_w, indexing=\"ij\")\n        coords = np.stack(coords_matrix)\n        coords_flatten = coords.reshape(2, -1)\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n        relative_coords = relative_coords.transpose([1, 2, 0])\n        relative_coords[:, :, 0] += self.window_size[0] - 1\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)\n\n        self.relative_position_index = tf.Variable(\n            initial_value=tf.convert_to_tensor(relative_position_index), trainable=False\n        )\n\n    def call(self, x, mask=None):\n        _, size, channels = x.shape\n        head_dim = channels // self.num_heads\n        x_qkv = self.qkv(x)\n        x_qkv = tf.reshape(x_qkv, shape=(-1, size, 3, self.num_heads, head_dim))\n        x_qkv = tf.transpose(x_qkv, perm=(2, 0, 3, 1, 4))\n        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n        q = q * self.scale\n        k = tf.transpose(k, perm=(0, 1, 3, 2))\n        attn = q @ k\n\n        num_window_elements = self.window_size[0] * self.window_size[1]\n        relative_position_index_flat = tf.reshape(\n            self.relative_position_index, shape=(-1,)\n        )\n        relative_position_bias = tf.gather(\n            self.relative_position_bias_table, relative_position_index_flat\n        )\n        relative_position_bias = tf.reshape(\n            relative_position_bias, shape=(num_window_elements, num_window_elements, -1)\n        )\n        relative_position_bias = tf.transpose(relative_position_bias, perm=(2, 0, 1))\n        attn = attn + tf.expand_dims(relative_position_bias, axis=0)\n\n        if mask is not None:\n            nW = mask.get_shape()[0]\n            mask_float = tf.cast(\n                tf.expand_dims(tf.expand_dims(mask, axis=1), axis=0), tf.float32\n            )\n            attn = (\n                tf.reshape(attn, shape=(-1, nW, self.num_heads, size, size))\n                + mask_float\n            )\n            attn = tf.reshape(attn, shape=(-1, self.num_heads, size, size))\n            attn = keras.activations.softmax(attn, axis=-1)\n        else:\n            attn = keras.activations.softmax(attn, axis=-1)\n        attn = self.dropout(attn)\n\n        x_qkv = attn @ v\n        x_qkv = tf.transpose(x_qkv, perm=(0, 2, 1, 3))\n        x_qkv = tf.reshape(x_qkv, shape=(-1, size, channels))\n        x_qkv = self.proj(x_qkv)\n        x_qkv = self.dropout(x_qkv)\n        return x_qkv","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:45.587553Z","iopub.execute_input":"2021-12-04T08:30:45.588144Z","iopub.status.idle":"2021-12-04T08:30:45.612722Z","shell.execute_reply.started":"2021-12-04T08:30:45.588102Z","shell.execute_reply":"2021-12-04T08:30:45.61191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SwinTransformer(layers.Layer):\n    def __init__(\n        self,\n        dim,\n        num_patch,\n        num_heads,\n        window_size=7,\n        shift_size=0,\n        num_mlp=1024,\n        qkv_bias=True,\n        dropout_rate=0.0,\n        **kwargs,\n    ):\n        super(SwinTransformer, self).__init__(**kwargs)\n\n        self.dim = dim  # number of input dimensions\n        self.num_patch = num_patch  # number of embedded patches\n        self.num_heads = num_heads  # number of attention heads\n        self.window_size = window_size  # size of window\n        self.shift_size = shift_size  # size of window shift\n        self.num_mlp = num_mlp  # number of MLP nodes\n\n        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n        self.attn = WindowAttention(\n            dim,\n            window_size=(self.window_size, self.window_size),\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            dropout_rate=dropout_rate,\n        )\n        self.drop_path = DropPath(dropout_rate)\n        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n\n        self.mlp = keras.Sequential(\n            [\n                layers.Dense(num_mlp),\n                layers.Activation(keras.activations.gelu),\n                layers.Dropout(dropout_rate),\n                layers.Dense(dim),\n                layers.Dropout(dropout_rate),\n            ]\n        )\n\n        if min(self.num_patch) < self.window_size:\n            self.shift_size = 0\n            self.window_size = min(self.num_patch)\n\n    def build(self, input_shape):\n        if self.shift_size == 0:\n            self.attn_mask = None\n        else:\n            height, width = self.num_patch\n            h_slices = (\n                slice(0, -self.window_size),\n                slice(-self.window_size, -self.shift_size),\n                slice(-self.shift_size, None),\n            )\n            w_slices = (\n                slice(0, -self.window_size),\n                slice(-self.window_size, -self.shift_size),\n                slice(-self.shift_size, None),\n            )\n            mask_array = np.zeros((1, height, width, 1))\n            count = 0\n            for h in h_slices:\n                for w in w_slices:\n                    mask_array[:, h, w, :] = count\n                    count += 1\n            mask_array = tf.convert_to_tensor(mask_array)\n\n            # mask array to windows\n            mask_windows = window_partition(mask_array, self.window_size)\n            mask_windows = tf.reshape(\n                mask_windows, shape=[-1, self.window_size * self.window_size]\n            )\n            attn_mask = tf.expand_dims(mask_windows, axis=1) - tf.expand_dims(\n                mask_windows, axis=2\n            )\n            attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)\n            attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n            self.attn_mask = tf.Variable(initial_value=attn_mask, trainable=False)\n\n    def call(self, x):\n        height, width = self.num_patch\n        _, num_patches_before, channels = x.shape\n        x_skip = x\n        x = self.norm1(x)\n        x = tf.reshape(x, shape=(-1, height, width, channels))\n        if self.shift_size > 0:\n            shifted_x = tf.roll(\n                x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2]\n            )\n        else:\n            shifted_x = x\n\n        x_windows = window_partition(shifted_x, self.window_size)\n        x_windows = tf.reshape(\n            x_windows, shape=(-1, self.window_size * self.window_size, channels)\n        )\n        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n\n        attn_windows = tf.reshape(\n            attn_windows, shape=(-1, self.window_size, self.window_size, channels)\n        )\n        shifted_x = window_reverse(\n            attn_windows, self.window_size, height, width, channels\n        )\n        if self.shift_size > 0:\n            x = tf.roll(\n                shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2]\n            )\n        else:\n            x = shifted_x\n\n        x = tf.reshape(x, shape=(-1, height * width, channels))\n        x = self.drop_path(x)\n        x = x_skip + x\n        x_skip = x\n        x = self.norm2(x)\n        x = self.mlp(x)\n        x = self.drop_path(x)\n        x = x_skip + x\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:45.613957Z","iopub.execute_input":"2021-12-04T08:30:45.614492Z","iopub.status.idle":"2021-12-04T08:30:45.639245Z","shell.execute_reply.started":"2021-12-04T08:30:45.614456Z","shell.execute_reply":"2021-12-04T08:30:45.638506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PatchExtract(layers.Layer):\n    def __init__(self, patch_size, **kwargs):\n        super(PatchExtract, self).__init__(**kwargs)\n        self.patch_size_x = patch_size[0]\n        self.patch_size_y = patch_size[0]\n\n    def call(self, images):\n        batch_size = tf.shape(images)[0]\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=(1, self.patch_size_x, self.patch_size_y, 1),\n            strides=(1, self.patch_size_x, self.patch_size_y, 1),\n            rates=(1, 1, 1, 1),\n            padding=\"VALID\",\n        )\n        patch_dim = patches.shape[-1]\n        patch_num = patches.shape[1]\n        return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))\n\n\nclass PatchEmbedding(layers.Layer):\n    def __init__(self, num_patch, embed_dim, **kwargs):\n        super(PatchEmbedding, self).__init__(**kwargs)\n        self.num_patch = num_patch\n        self.proj = layers.Dense(embed_dim)\n        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n\n    def call(self, patch):\n        pos = tf.range(start=0, limit=self.num_patch, delta=1)\n        return self.proj(patch) + self.pos_embed(pos)\n\n\nclass PatchMerging(tf.keras.layers.Layer):\n    def __init__(self, num_patch, embed_dim):\n        super(PatchMerging, self).__init__()\n        self.num_patch = num_patch\n        self.embed_dim = embed_dim\n        self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False)\n\n    def call(self, x):\n        height, width = self.num_patch\n        _, _, C = x.get_shape().as_list()\n        x = tf.reshape(x, shape=(-1, height, width, C))\n        x0 = x[:, 0::2, 0::2, :]\n        x1 = x[:, 1::2, 0::2, :]\n        x2 = x[:, 0::2, 1::2, :]\n        x3 = x[:, 1::2, 1::2, :]\n        x = tf.concat((x0, x1, x2, x3), axis=-1)\n        x = tf.reshape(x, shape=(-1, (height // 2) * (width // 2), 4 * C))\n        return self.linear_trans(x)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:45.64076Z","iopub.execute_input":"2021-12-04T08:30:45.641268Z","iopub.status.idle":"2021-12-04T08:30:45.657665Z","shell.execute_reply.started":"2021-12-04T08:30:45.641231Z","shell.execute_reply":"2021-12-04T08:30:45.657005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def swin_transformer_with_mlp_model():\n    \n    # Images Swin Transformer\n    input_1 = layers.Input(input_shape, name='img')\n    x = PatchExtract(patch_size)(input_1)\n    x = PatchEmbedding(num_patch_x * num_patch_y, embed_dim)(x)\n    x = SwinTransformer(\n        dim=embed_dim,\n        num_patch=(num_patch_x, num_patch_y),\n        num_heads=num_heads,\n        window_size=window_size,\n        shift_size=0,\n        num_mlp=num_mlp,\n        qkv_bias=qkv_bias,\n        dropout_rate=dropout_rate,\n    )(x)\n    x = SwinTransformer(\n        dim=embed_dim,\n        num_patch=(num_patch_x, num_patch_y),\n        num_heads=num_heads,\n        window_size=window_size,\n        shift_size=shift_size,\n        num_mlp=num_mlp,\n        qkv_bias=qkv_bias,\n        dropout_rate=dropout_rate,\n    )(x)\n    x = PatchMerging((num_patch_x, num_patch_y), embed_dim=embed_dim)(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x_1 = tf.keras.layers.LeakyReLU()(x)\n    \n    # Metadata MLP\n    input_2 = Input(shape=(12,), name='table')\n    x = Dense(256)(input_2)\n    x = tf.keras.layers.LeakyReLU()(x)\n    x = Dense(128)(x)\n    x = tf.keras.layers.LeakyReLU()(x)\n    x = Dense(128)(x)\n    x_2 = tf.keras.layers.LeakyReLU()(x)\n\n    concat = Concatenate()([x_1, x_2])\n\n    x = Dense(64)(concat)\n    x = tf.keras.layers.LeakyReLU()(x)\n    output = layers.Dense(1, activation='relu')(x)\n    \n    model = Model(inputs=[input_1, input_2], outputs=output)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:45.659094Z","iopub.execute_input":"2021-12-04T08:30:45.65937Z","iopub.status.idle":"2021-12-04T08:30:45.672179Z","shell.execute_reply.started":"2021-12-04T08:30:45.659336Z","shell.execute_reply":"2021-12-04T08:30:45.671227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = tfa.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\nloss_fn = tf.keras.losses.MeanSquaredError()\nacc_metric = tf.keras.metrics.RootMeanSquaredError()\nacc_metric_valid = tf.keras.metrics.RootMeanSquaredError()","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:45.673671Z","iopub.execute_input":"2021-12-04T08:30:45.673954Z","iopub.status.idle":"2021-12-04T08:30:45.70487Z","shell.execute_reply.started":"2021-12-04T08:30:45.673916Z","shell.execute_reply":"2021-12-04T08:30:45.704218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = swin_transformer_with_mlp_model()\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:45.705881Z","iopub.execute_input":"2021-12-04T08:30:45.706113Z","iopub.status.idle":"2021-12-04T08:30:47.0388Z","shell.execute_reply.started":"2021-12-04T08:30:45.706081Z","shell.execute_reply":"2021-12-04T08:30:47.038024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the model\ntf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:30:47.040011Z","iopub.execute_input":"2021-12-04T08:30:47.040247Z","iopub.status.idle":"2021-12-04T08:30:48.019008Z","shell.execute_reply.started":"2021-12-04T08:30:47.040213Z","shell.execute_reply":"2021-12-04T08:30:48.017367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 10\nfor epoch in range(num_epochs):\n    print(f\"\\n Start of Training Epoch {epoch+1}\")\n    for batch_idx, ((x_batch, y_batch), (x_batch_valid, y_batch_valid)) in enumerate(zip(train_tensor, valid_tensor)):\n        img = x_batch['img']\n        table = x_batch['table']\n        \n        img_valid = x_batch_valid['img']\n        table_valid = x_batch_valid['table']\n        \n        # tape forward pass of training data\n        with tf.GradientTape() as tape:\n            y_pred = model((img, table), training=True)\n            loss = loss_fn(y_batch, y_pred)\n            \n        # calculate grads of loss with respect to trainable parameters\n        gradients = tape.gradient(loss, model.trainable_weights)\n        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n        acc_metric.update_state(y_batch, y_pred)\n        \n        # tape forward pass validation data\n        with tf.GradientTape() as tape_valid:\n            y_pred_valid = model((img_valid, table_valid), training=False)\n            loss_valid = loss_fn(y_batch_valid, y_pred_valid)\n            \n        acc_metric_valid.update_state(y_batch_valid, y_pred_valid)\n        \n    train_acc = acc_metric.result()\n    valid_acc = acc_metric_valid.result()\n    print(f\"Train RMSE of epoch {train_acc}\")\n    print(f\"Valid RMSE of epoch {valid_acc}\")\n    # reset the accuracy metrics to start from 0 for the next epoch\n    acc_metric.reset_states()\n    acc_metric_valid.reset_states()\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-04T08:31:54.735536Z","iopub.execute_input":"2021-12-04T08:31:54.736127Z","iopub.status.idle":"2021-12-04T08:37:08.942379Z","shell.execute_reply.started":"2021-12-04T08:31:54.736075Z","shell.execute_reply":"2021-12-04T08:37:08.941627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict the test dataset labels\ny_test_preds_array = np.zeros(len(sample))\nrow = 0\nfor batch_idx, x_batch in enumerate(test_ds_tensor):\n    print(f\"Batch No. {batch_idx+1}\")\n    img = x_batch['img']\n    table = x_batch['table']\n              \n    y_test_pred = model((img, table), training=False)\n    \n    try:            \n        y_test_preds_array[row:row+BATCH_SIZE] = y_test_pred.numpy().flatten()\n    except:\n        y_test_preds_array[row:] = y_test_pred.numpy().flatten()\n    \n    row += BATCH_SIZE","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:37:08.944278Z","iopub.execute_input":"2021-12-04T08:37:08.944539Z","iopub.status.idle":"2021-12-04T08:37:09.013082Z","shell.execute_reply.started":"2021-12-04T08:37:08.944504Z","shell.execute_reply":"2021-12-04T08:37:09.012373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample.iloc[:, 1] = y_test_preds_array\nsample.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T08:37:09.014251Z","iopub.execute_input":"2021-12-04T08:37:09.01491Z","iopub.status.idle":"2021-12-04T08:37:09.023316Z","shell.execute_reply.started":"2021-12-04T08:37:09.014863Z","shell.execute_reply":"2021-12-04T08:37:09.022613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}