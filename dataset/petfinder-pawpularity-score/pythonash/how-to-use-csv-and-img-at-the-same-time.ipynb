{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Read me\n\nHello, This notebook for users who want to know how you can use the csv dataset and image dataset at the same time.\n\nIf you has a problem that it is hard to use two dataset, you will get some ideas from my notebook.\n\nThe data handling procedure will be skipped. If you want to know how it worked in detail, just clik on my previous notebook (https://www.kaggle.com/pythonash/how-to-handle-dataset-for-beginners).\n\nIf you have any questions, please leave the comments.\n\nI hope you to gain more imformation about data handling, DNN, CNN, and etc..\n\n## **Knowledge can be improved by being shared.**\n\nPlease upvote!!\n\n\n## [You can learn more skills for handling dataset or neural network.]\n\n### [Parallel combination DNN with CNN] - Pawpularity Contest\n - https://www.kaggle.com/pythonash/parallel-dnn-and-cnn-network-for-beginners\n \n### [Image data handling without memory exploded] - Pawpularity Contest\n - https://www.kaggle.com/pythonash/how-to-handle-dataset-for-beginners\n\n### [Data handling & Deep learning] - Titanic competition (best score!!)\n - https://www.kaggle.com/pythonash/how-to-handle-raw-dataset-and-analyze-with-dl\n \n### [Deep learning model with SeLU activation function] - Titanic competition\n- https://www.kaggle.com/pythonash/selu-activation-function-in-dl\n\n### [Preparing a completed dataset with proper imputation method] - Titanic competition\n - https://www.kaggle.com/pythonash/making-completed-dataset\n\n**Let's start!**","metadata":{}},{"cell_type":"markdown","source":"# Just run this code before you set your model.\n\n- This code is for preparing dataset to input at your model.\n\n- The details are described as in my notebook, \"Image data handling without memory exploded\".\n\n\n## The difference from previous notebook is\n\n> we will use sigmoid function as setting the output like binary classification.\n\n> That is, we will normalize the target variable by dividing with 100 (the maximum value).\n\n> So, the result will be distributed like probability.\n\n> Finally, you have to transform the predict value by multiplying with 100.","metadata":{}},{"cell_type":"code","source":"##################################JUST RUN THIS CODE FOR PREPARING DATASET##################################\nimport pandas as pd\nimport tensorflow as tf\nimport cv2\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\ntrain_csv = pd.read_csv('../input/petfinder-pawpularity-score/train.csv')\ntest_csv = pd.read_csv('../input/petfinder-pawpularity-score/test.csv')\nsubmission = pd.read_csv('../input/petfinder-pawpularity-score/sample_submission.csv')\n\nos.chdir('../input/petfinder-pawpularity-score/train')\n\nsize_data = pd.DataFrame()\nfor file in os.listdir():\n    imgg = cv2.imread(file)\n    w,h,c = imgg.shape\n    size_data=size_data.append([[w,h,c,imgg.size/3]])\n    \n\ntrain_img = []\nfor i in os.listdir():\n    file = cv2.imread(i)\n    file=cv2.resize(file,(64,64), interpolation=cv2.INTER_AREA)\n    train_img.append(file/255)\n\n\ntrain_img_name = []\nfor i in os.listdir():\n    train_img_name.append(i)\n\n\ntrain_csv_data = pd.DataFrame()\nfor img, name in zip(train_img, train_img_name):\n    name=name[:-4]\n    location = train_csv[train_csv['Id'] == name].index[0]\n    train_csv_data= train_csv_data.append([train_csv.loc[location]])\n\nos.chdir('../test')\n\ntest_img = []\nfor i in os.listdir():\n    file = cv2.imread(i)\n    file=cv2.resize(file,(64,64), interpolation=cv2.INTER_AREA)\n    test_img.append(file/255)\n\ntest_img_name = []\nfor i in os.listdir():\n    test_img_name.append(i)\n\ntest_csv_data = pd.DataFrame()\nfor img, name in zip(test_img, test_img_name):\n    name=name[:-4]\n    location = test_csv[test_csv['Id'] == name].index[0]\n    test_csv_data= test_csv_data.append([test_csv.loc[location]])\ntest_csv_data=test_csv_data.reset_index().drop(['index'],axis=1)\n\ntrain_csv_x = train_csv_data.drop(['Id','Pawpularity'],axis=1)\ntrain_y = train_csv_data['Pawpularity']/100\ntest_csv_x = test_csv_data.drop(['Id'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-28T17:31:36.39613Z","iopub.execute_input":"2021-12-28T17:31:36.396497Z","iopub.status.idle":"2021-12-28T17:36:03.95293Z","shell.execute_reply.started":"2021-12-28T17:31:36.396373Z","shell.execute_reply":"2021-12-28T17:36:03.952156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(test_img[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-28T17:36:03.95708Z","iopub.execute_input":"2021-12-28T17:36:03.95767Z","iopub.status.idle":"2021-12-28T17:36:04.170677Z","shell.execute_reply.started":"2021-12-28T17:36:03.95763Z","shell.execute_reply":"2021-12-28T17:36:04.170047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The comnibation of DNN and CNN\n\nWe will use a combination structure which consists of DNN for CSV and CNN for IMAGE.\n\nThe idea is very simple.\n\n1. Set the DNN model and CNN model, respectively.\n\n2. Combinate two models with one outputs.\n\n> This model will output one results through using CSV result and IMG result.\n\nHow can it work??\n\nLet's start!","metadata":{}},{"cell_type":"markdown","source":"## Set your model\n\nIn this part, please read the codes carefully, we will use two input layers.","metadata":{}},{"cell_type":"code","source":"##################### CSV FILE INPUT & IMG FILE INPUT LAYER ###########################\ncsv_input = tf.keras.Input(shape = train_csv_x.shape[1:], name = 'CSV_Input')        ##\nimg_input = tf.keras.Input(shape = np.array(train_img).shape[1:], name = 'IMG_Input')##\n#######################################################################################\n                                        ##\n                                        ##\n                                        ##\n##################### CSV FILE HIDDEN LAYER STRUCTURE  ######################################\ncsv_hidden1 = tf.keras.layers.Dense(8, activation='relu', name='CSV_Hidden1')(csv_input)   ##\ncsv_hidden2 = tf.keras.layers.Dense(30, activation='relu', name='CSV_Hidden2')(csv_hidden1)##\ncsv_hidden3 = tf.keras.layers.Dense(50, activation='relu', name='CSV_Hidden3')(csv_hidden2)##\ncsv_dropout = tf.keras.layers.Dropout(0.5, name ='CSV_Dropout')(csv_hidden3)               ##\ncsv_hidden4 = tf.keras.layers.Dense(30, activation='relu', name='CSV_Hidden4')(csv_dropout)##\ncsv_hidden5 = tf.keras.layers.Dense(10, activation='relu', name='CSV_Hidden5')(csv_hidden4)##\n#############################################################################################\n                                         #\n                                         #\n                                         #\n##################### IMG FILE CONVOLUTIONAL LAYER STRUCTURE  ######################################\nimg_conv1 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 5, strides= 1, padding = 'same',   ##\n                                   activation = 'relu', name='IMG_Conv1')(img_input)              ##\nimg_pool1 = tf.keras.layers.MaxPool2D(3, name = 'IMG_Pool1')(img_conv1)                           ##\nimg_conv2 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 4, strides= 1, padding = 'same',  ##\n                                   activation = 'relu', name='IMG_Conv2')(img_pool1)              ##\nimg_conv3 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 4, strides =1, padding = 'same',  ##\n                                   activation = 'relu', name='IMG_Conv3')(img_conv2)              ##\nimg_pool2 = tf.keras.layers.MaxPool2D(3, name = 'IMG_Pool2')(img_conv3)                           ##\nimg_conv4 = tf.keras.layers.Conv2D(filters = 256, kernel_size = 3, strides =1, padding = 'same',  ##\n                                   activation = 'relu', name='IMG_Conv4')(img_pool2)              ##\nimg_pool3 = tf.keras.layers.MaxPool2D(3, name = 'IMG_Pool3')(img_conv4)                           ##\nimg_conv5 = tf.keras.layers.Conv2D(filters = 512, kernel_size = 3, strides =1, padding = 'same',  ##\n                                   activation = 'relu', name='IMG_Conv5')(img_pool3)              ##\nimg_conv6 = tf.keras.layers.Conv2D(filters = 512, kernel_size = 3, strides =1, padding = 'same',  ##\n                                   activation = 'relu', name='IMG_Conv6')(img_conv5)              ##\nimg_pool4 = tf.keras.layers.MaxPool2D(2, name = 'IMG_Pool4')(img_conv6)                           ##\nimg_flatten = tf.keras.layers.Flatten(name = 'IMG_Flatten')(img_pool4)                            ##\nimg_dense1 = tf.keras.layers.Dense(3000, activation = 'relu', name='IMG_Dense1')(img_flatten)     ##\nimg_dropout1 = tf.keras.layers.Dropout(0.5, name='IMG_Dropout1')(img_dense1)                      ##\nimg_dense2 = tf.keras.layers.Dense(3000, activation = 'relu', name='IMG_Dense2')(img_dropout1)    ##\nimg_dropout2 = tf.keras.layers.Dropout(0.5, name='IMG_Dropout2')(img_dense2)                      ##\n####################################################################################################\n                                        ##\n                                        ##\n                                        ##\n######################################### IMG & CSV DATA CONNETION #####################################\nimg_csv_concat = tf.keras.layers.Concatenate(name = 'connection', axis=1)([csv_hidden5, img_dropout2])##\n########################################################################################################\n                                        ##\n                                        ##\n                                        ##\n################################# IMG & CSV DENSE LAYER AND DROPOUT #######################################\nimg_csv_dense1 = tf.keras.layers.Dense(100, name = 'IMG_CSV_Dense1', activation = 'relu')(img_csv_concat)##\nimg_csv_dropout = tf.keras.layers.Dropout(0.5, name = 'IMG_CSV_Dropout')(img_csv_dense1)                 ##\nimg_csv_dense2 = tf.keras.layers.Dense(50, name = 'IMG_CSV_Dense2', activation = 'relu')(img_csv_dropout)##\n###########################################################################################################\n                                        ##\n                                        ##\n                                        ##\n################################# IMG & CSV DENSE LAYER AND DROPOUT ########################################\nimg_csv_output = tf.keras.layers.Dense(1, name = 'IMG_CSV_Output', activation = 'sigmoid')(img_csv_dense2)##\n############################################################################################################\n                                        ##\n                                        ##\n                                        ##\n############################################# MODEL SETTING  ####################################################\nmodel = tf.keras.Model(inputs=[csv_input, img_input], outputs=[img_csv_output], name='Pythonash_model')##\n#################################################################################################################","metadata":{"execution":{"iopub.status.busy":"2021-12-28T17:36:04.171848Z","iopub.execute_input":"2021-12-28T17:36:04.172085Z","iopub.status.idle":"2021-12-28T17:36:05.771625Z","shell.execute_reply.started":"2021-12-28T17:36:04.172052Z","shell.execute_reply":"2021-12-28T17:36:05.770888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## model summary\n\nIt shows that your model structure, simply.","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-28T17:36:05.773833Z","iopub.execute_input":"2021-12-28T17:36:05.774263Z","iopub.status.idle":"2021-12-28T17:36:05.792511Z","shell.execute_reply.started":"2021-12-28T17:36:05.774224Z","shell.execute_reply":"2021-12-28T17:36:05.791789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## plot your model\n\nThis plot shows your model and you can figure out you model structure, intuitively.\n\nYou can see that DNN model and CNN model are parallel and only one output layer is in this structure.","metadata":{}},{"cell_type":"code","source":"# move you current directory to back.\nos.chdir('../')\nos.chdir('../')\nos.chdir('../')\ntf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True, rankdir='TB')","metadata":{"execution":{"iopub.status.busy":"2021-12-28T17:36:05.793862Z","iopub.execute_input":"2021-12-28T17:36:05.794109Z","iopub.status.idle":"2021-12-28T17:36:06.30457Z","shell.execute_reply.started":"2021-12-28T17:36:05.794075Z","shell.execute_reply":"2021-12-28T17:36:06.303795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compile and fit your model.","metadata":{}},{"cell_type":"code","source":"# We will use the learning schedule that decrease step by step.\n\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=0.001,\n    decay_steps=500,\n    decay_rate=0.3\n)\nopt = tf.keras.optimizers.Adam(learning_rate = lr_schedule)\nmodel.compile(loss='binary_crossentropy', optimizer = opt, metrics = tf.keras.metrics.RootMeanSquaredError())\n\nepoch_number = 100\n\n\n# It is used for preventing this model from overfitting.\n# It remembers the best model in terms of validation loss.\n# So, you have to load your best model when you attempt to predict.\ncheck_1 = tf.keras.callbacks.ModelCheckpoint('pythonash_model.h5', save_best_only=True, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2021-12-28T17:36:06.306414Z","iopub.execute_input":"2021-12-28T17:36:06.307888Z","iopub.status.idle":"2021-12-28T17:36:06.336622Z","shell.execute_reply.started":"2021-12-28T17:36:06.307845Z","shell.execute_reply":"2021-12-28T17:36:06.336002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit( \n    x= [train_csv_x, np.array(train_img)], y = train_y, epochs=epoch_number,\n    validation_split=0.2, verbose =2, workers=3, batch_size = 20, validation_batch_size = 20,\n    callbacks = [check_1])","metadata":{"execution":{"iopub.status.busy":"2021-12-28T17:36:06.337997Z","iopub.execute_input":"2021-12-28T17:36:06.338451Z","iopub.status.idle":"2021-12-28T17:36:35.988811Z","shell.execute_reply.started":"2021-12-28T17:36:06.338414Z","shell.execute_reply":"2021-12-28T17:36:35.98806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading best model & submission","metadata":{}},{"cell_type":"code","source":"best_model = tf.keras.models.load_model('pythonash_model.h5')\nimg_result = best_model.predict([test_csv_x,np.array(test_img)])\nfinal_result = pd.DataFrame(img_result)\nfinal_result.columns =['Pawpularity']\nfinal_result","metadata":{"execution":{"iopub.status.busy":"2021-12-28T17:36:35.990145Z","iopub.execute_input":"2021-12-28T17:36:35.990933Z","iopub.status.idle":"2021-12-28T17:36:36.989447Z","shell.execute_reply.started":"2021-12-28T17:36:35.990894Z","shell.execute_reply":"2021-12-28T17:36:36.988663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submit your result.\n\nIt has done!\n\nI think it might be easy to you who already know about DNN and CNN.","metadata":{}},{"cell_type":"code","source":"for ids, paw in zip(test_csv_data['Id'], final_result['Pawpularity']):\n    location = submission[submission['Id'] == ids].index[0]\n    submission['Pawpularity'].loc[location] = paw * 100\nsubmission.to_csv('./working/submission.csv',index=False)\nsubmission\n","metadata":{"execution":{"iopub.status.busy":"2021-12-28T17:36:36.990557Z","iopub.execute_input":"2021-12-28T17:36:36.990806Z","iopub.status.idle":"2021-12-28T17:36:37.013311Z","shell.execute_reply.started":"2021-12-28T17:36:36.99077Z","shell.execute_reply":"2021-12-28T17:36:37.012647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# It's your turn!!\n\nYou have many opportunities that you can change this model parameters and get your submission score.\n\nI recommend that you change the hyper parameters such as learning_rate, batch_size, activation function, the number of neurons, layers, and so on...\n\nIf you get any helps from my notebook, please upvote!!\n\nFingers crossed!!","metadata":{}}]}