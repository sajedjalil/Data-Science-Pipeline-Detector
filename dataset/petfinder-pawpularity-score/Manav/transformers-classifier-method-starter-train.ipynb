{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![SETI](https://www.petfinder.my/images/cuteness_meter.jpg)  \n\n# Problem Statement\n* Millions of stray animals suffer on the streets or are euthanized in shelters every day around the world. You might expect pets with attractive photos to generate more interest and be adopted faster.\n* With the help of data science, we will accurately determine a pet photoâ€™s appeal to give these rescue animals a higher chance of loving homes.\n* Currently, PetFinder.my uses a basic Cuteness Meter to rank pet photos. It analyzes picture composition and other factors compared to the performance of thousands of pet profiles.\n\n## Why this competition?\nAs evident from the problem statement, this competition presents an interesting challenge for a good cause.  \nAlso (if successful) the solution can be adapted into tools that will can shelters and rescuers around the world to improve the appeal of their pet profiles, automatically enhancing photo quality and consequently helping animals find a suitable hjome much faster.\n\n## Expected Outcome\nGiven a photo a pet animal and some basic information about the photo as dense features, we should be able to estimate the 'pawpularity' score of the pet.\n\n## Data Description\nImage data is stored in a jpg image format in training folder and the dense features and target scores are mentioned in the `train.csv` file where the Id of each row corresponds to an unique image in the training folder.\nThere are also some basic info on the photograph as dense features on the `train.csv` file.\n\n## Grading Metric\nSubmissions are evaluated on **RMSE** between the predicted value and the observed target.\n\n## Problem Category\nFrom the data and objective its is evident that this is a **Regression Problem** in the Computer Vision domain.\n\n**If you found this notebook useful and use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share these public kernels.** ðŸ˜Š","metadata":{}},{"cell_type":"markdown","source":"# About This Notebook:-\n* This notebook tried to demonstrate the use of Transfer learning using Pytorch and how to combine image features with dense features for various tasks.\n* We use a vanilla **vit_large_patch32_384** model for extracting image embeddings and concatenate them with the dense features on the last layer on a NN.\n* Refer [this link](https://www.kaggle.com/c/petfinder-pawpularity-score/discussion/275094) for description regarding using this particular methodology.\n* This notebook only covers the training part. Inference can be found in the notebook link below.\n\nInference Notebook:- https://www.kaggle.com/manabendrarout/transformers-classifier-method-starter-infer  \n\n<p style='color: #fc0362; font-family: Segoe UI; font-size: 1.5em; font-weight: 300; font-size: 24px'>TLDR:- We treat this problem as a classification problem by scaling all targets between [0, 1] and use cross entropy loss as loss-function. It is known that transformer based models are performing better than classic CNN based models on this dataset.</p>","metadata":{}},{"cell_type":"markdown","source":"# Get GPU Info","metadata":{"id":"BINjp9x4_S1v"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"7JFphgy7-Bnh","outputId":"9726f4ca-f7a2-45c7-8faa-de794e82430d","execution":{"iopub.status.busy":"2021-10-03T17:54:14.637881Z","iopub.execute_input":"2021-10-03T17:54:14.638596Z","iopub.status.idle":"2021-10-03T17:54:15.411285Z","shell.execute_reply.started":"2021-10-03T17:54:14.638488Z","shell.execute_reply":"2021-10-03T17:54:15.410521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Installations","metadata":{"id":"6I-oEtuP_kQ4","_kg_hide-input":false,"_kg_hide-output":false}},{"cell_type":"code","source":"!pip install -qq timm\n!pip install -qq albumentations==1.0.3\n!pip install -qq grad-cam\n!pip install -qq ttach","metadata":{"id":"9iIVyfJS_Vmb","_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-03T17:54:15.414864Z","iopub.execute_input":"2021-10-03T17:54:15.415114Z","iopub.status.idle":"2021-10-03T17:54:56.969027Z","shell.execute_reply.started":"2021-10-03T17:54:15.415086Z","shell.execute_reply":"2021-10-03T17:54:56.968116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{"id":"OOStUXXcAgDa"}},{"cell_type":"code","source":"# Asthetics\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n# General\nfrom tqdm.auto import tqdm\nfrom collections import defaultdict\nimport pandas as pd\nimport numpy as np\nimport os\nimport random\nimport gc\nimport cv2\nimport glob\ngc.enable()\npd.set_option('display.max_columns', None)\n\n# Visialisation\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Image Aug\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n# Deep Learning\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, OneCycleLR, CosineAnnealingLR\nimport torch\nimport torchvision\nimport timm\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n#Metrics\nfrom sklearn.metrics import mean_squared_error\n\n# Random Seed Initialize\nRANDOM_SEED = 42\n\ndef seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything()\n\n# Device Optimization\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    \nprint(f'Using device: {device}')","metadata":{"id":"fewdVIGwAfpU","outputId":"68c049ba-a995-4305-9788-ee9e6e6fb14e","execution":{"iopub.status.busy":"2021-10-03T17:54:56.970658Z","iopub.execute_input":"2021-10-03T17:54:56.970909Z","iopub.status.idle":"2021-10-03T17:55:03.98923Z","shell.execute_reply.started":"2021-10-03T17:54:56.970873Z","shell.execute_reply":"2021-10-03T17:55:03.988464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_dir = '../input/petfinder-pawpularity-score'\ntrain_dir = '../input/petfinder-pawpularity-score/train'\ntest_dir = '../input/petfinder-pawpularity-score/test'\n\ntrain_file_path = '../input/pawpular-folds/train_5folds.csv'\nsample_sub_file_path = os.path.join(csv_dir, 'sample_submission.csv')\n\nprint(f'Train file: {train_file_path}')\nprint(f'Train file: {sample_sub_file_path}')","metadata":{"id":"cscftms-CyMn","outputId":"150ac493-d079-4950-85b5-b69f1f3ea8bd","execution":{"iopub.status.busy":"2021-10-03T17:55:03.990957Z","iopub.execute_input":"2021-10-03T17:55:03.991233Z","iopub.status.idle":"2021-10-03T17:55:03.996947Z","shell.execute_reply.started":"2021-10-03T17:55:03.991197Z","shell.execute_reply":"2021-10-03T17:55:03.99623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(train_file_path)\ntest_df = pd.read_csv(sample_sub_file_path)","metadata":{"id":"B1aBWdnoDWWX","execution":{"iopub.status.busy":"2021-10-03T17:55:03.999262Z","iopub.execute_input":"2021-10-03T17:55:03.999669Z","iopub.status.idle":"2021-10-03T17:55:04.056372Z","shell.execute_reply.started":"2021-10-03T17:55:03.999637Z","shell.execute_reply":"2021-10-03T17:55:04.05574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def return_filpath(name, folder=train_dir):\n    path = os.path.join(folder, f'{name}.jpg')\n    return path","metadata":{"id":"vy3X2yUODZP1","execution":{"iopub.status.busy":"2021-10-03T17:55:04.057447Z","iopub.execute_input":"2021-10-03T17:55:04.057699Z","iopub.status.idle":"2021-10-03T17:55:04.062533Z","shell.execute_reply.started":"2021-10-03T17:55:04.057664Z","shell.execute_reply":"2021-10-03T17:55:04.061868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['image_path'] = train_df['Id'].apply(lambda x: return_filpath(x))\ntest_df['image_path'] = test_df['Id'].apply(lambda x: return_filpath(x, folder=test_dir))","metadata":{"id":"4cQlB9XUD31w","execution":{"iopub.status.busy":"2021-10-03T17:55:04.064111Z","iopub.execute_input":"2021-10-03T17:55:04.064628Z","iopub.status.idle":"2021-10-03T17:55:04.0999Z","shell.execute_reply.started":"2021-10-03T17:55:04.064593Z","shell.execute_reply":"2021-10-03T17:55:04.098968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"id":"7cZ3OJc8D80E","outputId":"3b7f823a-6c3b-484b-b822-03d016b43e17","execution":{"iopub.status.busy":"2021-10-03T17:55:04.10176Z","iopub.execute_input":"2021-10-03T17:55:04.102136Z","iopub.status.idle":"2021-10-03T17:55:04.125362Z","shell.execute_reply.started":"2021-10-03T17:55:04.102098Z","shell.execute_reply":"2021-10-03T17:55:04.124597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"id":"UI-XjtOiD-T7","outputId":"e896c973-20c6-4030-aca5-eaad2fa53be6","execution":{"iopub.status.busy":"2021-10-03T17:55:04.12661Z","iopub.execute_input":"2021-10-03T17:55:04.126854Z","iopub.status.idle":"2021-10-03T17:55:04.137761Z","shell.execute_reply.started":"2021-10-03T17:55:04.126823Z","shell.execute_reply":"2021-10-03T17:55:04.136635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = ['Pawpularity']\nnot_features = ['Id', 'kfold', 'image_path', 'Pawpularity']\ncols = list(train_df.columns)\nfeatures = [feat for feat in cols if feat not in not_features]\nprint(features)","metadata":{"execution":{"iopub.status.busy":"2021-10-03T17:55:04.139414Z","iopub.execute_input":"2021-10-03T17:55:04.13974Z","iopub.status.idle":"2021-10-03T17:55:04.147037Z","shell.execute_reply.started":"2021-10-03T17:55:04.139674Z","shell.execute_reply":"2021-10-03T17:55:04.145988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CFG","metadata":{"id":"GJPLd0HKELDF"}},{"cell_type":"code","source":"TRAIN_FOLDS = [0, 1, 2, 3, 4]","metadata":{"id":"qYYZYyotM7WA","execution":{"iopub.status.busy":"2021-10-03T17:55:04.148832Z","iopub.execute_input":"2021-10-03T17:55:04.149531Z","iopub.status.idle":"2021-10-03T17:55:04.154169Z","shell.execute_reply.started":"2021-10-03T17:55:04.149495Z","shell.execute_reply":"2021-10-03T17:55:04.153372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    'model': 'vit_large_patch32_384',\n    'dense_features': features,\n    'pretrained': True,\n    'inp_channels': 3,\n    'im_size': 384,\n    'device': device,\n    'lr': 1e-5,\n    'weight_decay': 1e-6,\n    'batch_size': 32,\n    'num_workers' : 0,\n    'epochs': 10,\n    'out_features': 1,\n    'dropout': 0.2,\n    'num_fold': len(TRAIN_FOLDS),\n    'mixup': False,\n    'mixup_alpha': 1.0,\n    'scheduler_name': 'CosineAnnealingWarmRestarts',\n    'T_0': 5,\n    'T_max': 5,\n    'T_mult': 1,\n    'min_lr': 1e-7,\n    'max_lr': 1e-4\n}","metadata":{"execution":{"iopub.status.busy":"2021-10-03T17:55:04.15594Z","iopub.execute_input":"2021-10-03T17:55:04.156622Z","iopub.status.idle":"2021-10-03T17:55:04.164634Z","shell.execute_reply.started":"2021-10-03T17:55:04.156532Z","shell.execute_reply":"2021-10-03T17:55:04.163719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Augmentations\n\nThere a well known concept called **image augmentations** in CNN. What augmentation generally does is, it artificially increases the dataset size by subtly modifying the existing images to create new ones (while training). One added advantage of this is:- The model becomes more generalized and focuses to finding features and representations rather than completely overfitting to the training data. It also sometimes helps the model train on more noisy data as compared to conventional methods.  \n\nExample:-  \n![](https://www.researchgate.net/publication/319413978/figure/fig2/AS:533727585333249@1504261980375/Data-augmentation-using-semantic-preserving-transformation-for-SBIR.png)  \nSource:- https://www.researchgate.net/publication/319413978/figure/fig2/AS:533727585333249@1504261980375/Data-augmentation-using-semantic-preserving-transformation-for-SBIR.png\n\nOne of the most popular image augmentation libraries is **Albumentations**. It has an extensive list of image augmentations, the full list can be found in their [documentation](https://albumentations.ai/docs/).  \n\n*Tip:- Not all augmentations are applicable in all conditions. It really depends on the dataset and the problem. Example:- If your task is to identify if a person is standing or sleeping, applying a rotational augmentation can make the model worse.*  \n\nWith that in mind, let's define our augmentations:-","metadata":{"id":"DsXs0lrZG6MY"}},{"cell_type":"markdown","source":"## 1. Train Augmentations","metadata":{"id":"ocxEJDymG9-9"}},{"cell_type":"code","source":"def get_train_transforms(DIM = params['im_size']):\n    return albumentations.Compose(\n        [\n            albumentations.Resize(DIM,DIM),\n            albumentations.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            albumentations.HorizontalFlip(p=0.5),\n            albumentations.VerticalFlip(p=0.5),\n            albumentations.Rotate(limit=180, p=0.7),\n            albumentations.ShiftScaleRotate(\n                shift_limit = 0.1, scale_limit=0.1, rotate_limit=45, p=0.5\n            ),\n            albumentations.HueSaturationValue(\n                hue_shift_limit=0.2, sat_shift_limit=0.2,\n                val_shift_limit=0.2, p=0.5\n            ),\n            albumentations.RandomBrightnessContrast(\n                brightness_limit=(-0.1, 0.1),\n                contrast_limit=(-0.1, 0.1), p=0.5\n            ),\n            ToTensorV2(p=1.0),\n        ]\n    )","metadata":{"id":"VAgcHrtSG53X","execution":{"iopub.status.busy":"2021-10-03T17:55:04.166635Z","iopub.execute_input":"2021-10-03T17:55:04.167002Z","iopub.status.idle":"2021-10-03T17:55:04.176353Z","shell.execute_reply.started":"2021-10-03T17:55:04.166956Z","shell.execute_reply":"2021-10-03T17:55:04.175384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Mixup","metadata":{"id":"_7Jgmmh0HCI-"}},{"cell_type":"code","source":"def mixup_data(x, z, y, params):\n    if params['mixup_alpha'] > 0:\n        lam = np.random.beta(\n            params['mixup_alpha'], params['mixup_alpha']\n        )\n    else:\n        lam = 1\n\n    batch_size = x.size()[0]\n    if params['device'].type == 'cuda':\n        index = torch.randperm(batch_size).cuda()\n    else:\n        index = torch.randperm(batch_size)\n\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    mixed_z = lam * z + (1 - lam) * z[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, mixed_z, y_a, y_b, lam\n\ndef mixup_criterion(criterion, pred, y_a, y_b, lam):\n    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)","metadata":{"id":"cSI6LYDxHCoy","execution":{"iopub.status.busy":"2021-10-03T17:55:04.180649Z","iopub.execute_input":"2021-10-03T17:55:04.180904Z","iopub.status.idle":"2021-10-03T17:55:04.190538Z","shell.execute_reply.started":"2021-10-03T17:55:04.18084Z","shell.execute_reply":"2021-10-03T17:55:04.189536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Valid Augmentations","metadata":{"id":"GkfBqo6cHHCZ"}},{"cell_type":"code","source":"def get_valid_transforms(DIM = params['im_size']):\n    return albumentations.Compose(\n        [\n          albumentations.Resize(DIM,DIM),\n          albumentations.Normalize(\n              mean=[0.485, 0.456, 0.406],\n              std=[0.229, 0.224, 0.225],\n          ),\n          ToTensorV2(p=1.0)\n        ]\n    )","metadata":{"id":"vyYpi2CJHHZr","execution":{"iopub.status.busy":"2021-10-03T17:55:04.192123Z","iopub.execute_input":"2021-10-03T17:55:04.192788Z","iopub.status.idle":"2021-10-03T17:55:04.199211Z","shell.execute_reply.started":"2021-10-03T17:55:04.192751Z","shell.execute_reply":"2021-10-03T17:55:04.198409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{"id":"1XzssvuYHNI0"}},{"cell_type":"code","source":"class CuteDataset(Dataset):\n    def __init__(self, images_filepaths, dense_features, targets, transform=None):\n        self.images_filepaths = images_filepaths\n        self.dense_features = dense_features\n        self.targets = targets\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images_filepaths)\n\n    def __getitem__(self, idx):\n        image_filepath = self.images_filepaths[idx]\n        image = cv2.imread(image_filepath)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.transform is not None:\n            image = self.transform(image=image)['image']\n        \n        dense = self.dense_features[idx, :]\n        label = torch.tensor(self.targets[idx]).float()\n        return image, dense, label","metadata":{"id":"7b4_Zu73HK62","execution":{"iopub.status.busy":"2021-10-03T17:55:04.200684Z","iopub.execute_input":"2021-10-03T17:55:04.201027Z","iopub.status.idle":"2021-10-03T17:55:04.209295Z","shell.execute_reply.started":"2021-10-03T17:55:04.200993Z","shell.execute_reply":"2021-10-03T17:55:04.208405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Visualize Some Examples","metadata":{"id":"z4LHNKAcTsi7"}},{"cell_type":"code","source":"X_train = train_df['image_path']\nX_train_dense = train_df[params['dense_features']]\ny_train = train_df['Pawpularity']\ntrain_dataset = CuteDataset(\n    images_filepaths=X_train.values,\n    dense_features=X_train_dense.values,\n    targets=y_train.values,\n    transform=get_train_transforms()\n)","metadata":{"id":"-WJ54a68TxVS","execution":{"iopub.status.busy":"2021-10-03T17:55:04.210675Z","iopub.execute_input":"2021-10-03T17:55:04.211139Z","iopub.status.idle":"2021-10-03T17:55:04.221523Z","shell.execute_reply.started":"2021-10-03T17:55:04.211106Z","shell.execute_reply":"2021-10-03T17:55:04.22083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_image(train_dataset=train_dataset, inline=4):\n    plt.figure(figsize=(20,10))\n    for i in range(inline):\n        rand = random.randint(0, len(train_dataset))\n        image, dense, label = train_dataset[rand]\n        plt.subplot(1, inline, i%inline +1)\n        plt.axis('off')\n        plt.imshow(image.permute(2, 1, 0))\n        plt.title(f'Pawpularity: {label}')","metadata":{"id":"z8r6mz2TVQMA","execution":{"iopub.status.busy":"2021-10-03T17:55:04.222979Z","iopub.execute_input":"2021-10-03T17:55:04.223645Z","iopub.status.idle":"2021-10-03T17:55:04.231014Z","shell.execute_reply.started":"2021-10-03T17:55:04.22361Z","shell.execute_reply":"2021-10-03T17:55:04.230252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(3):\n    show_image(inline=4)","metadata":{"id":"IVDGvn6uX9du","outputId":"bdf5a8a9-52af-4f7d-ee85-81f537d7ade5","execution":{"iopub.status.busy":"2021-10-03T17:55:04.232284Z","iopub.execute_input":"2021-10-03T17:55:04.233098Z","iopub.status.idle":"2021-10-03T17:55:06.328017Z","shell.execute_reply.started":"2021-10-03T17:55:04.233004Z","shell.execute_reply":"2021-10-03T17:55:06.327241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X_train, X_train_dense, y_train, train_dataset","metadata":{"id":"ffF9yW2JUqpq","execution":{"iopub.status.busy":"2021-10-03T17:55:06.329238Z","iopub.execute_input":"2021-10-03T17:55:06.329709Z","iopub.status.idle":"2021-10-03T17:55:06.333915Z","shell.execute_reply.started":"2021-10-03T17:55:06.329671Z","shell.execute_reply":"2021-10-03T17:55:06.333148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metrics","metadata":{"id":"GMHo2KKAHw-3"}},{"cell_type":"code","source":"def usr_rmse_score(output, target):\n    y_pred = torch.sigmoid(output).cpu()\n    y_pred = y_pred.detach().numpy()*100\n    target = target.cpu()*100\n    \n    return mean_squared_error(target, y_pred, squared=False)","metadata":{"id":"nHiDAFprHtD-","execution":{"iopub.status.busy":"2021-10-03T17:55:06.335641Z","iopub.execute_input":"2021-10-03T17:55:06.336108Z","iopub.status.idle":"2021-10-03T17:55:06.345638Z","shell.execute_reply.started":"2021-10-03T17:55:06.336069Z","shell.execute_reply":"2021-10-03T17:55:06.344796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MetricMonitor:\n    def __init__(self, float_precision=3):\n        self.float_precision = float_precision\n        self.reset()\n\n    def reset(self):\n        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n\n    def update(self, metric_name, val):\n        metric = self.metrics[metric_name]\n\n        metric[\"val\"] += val\n        metric[\"count\"] += 1\n        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n\n    def __str__(self):\n        return \" | \".join(\n            [\n                \"{metric_name}: {avg:.{float_precision}f}\".format(\n                    metric_name=metric_name, avg=metric[\"avg\"],\n                    float_precision=self.float_precision\n                )\n                for (metric_name, metric) in self.metrics.items()\n            ]\n        )","metadata":{"id":"KqMs6f0iIv2e","execution":{"iopub.status.busy":"2021-10-03T17:55:06.347092Z","iopub.execute_input":"2021-10-03T17:55:06.347392Z","iopub.status.idle":"2021-10-03T17:55:06.356088Z","shell.execute_reply.started":"2021-10-03T17:55:06.347356Z","shell.execute_reply":"2021-10-03T17:55:06.355269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scheduler\n\nScheduler is essentially an function that changes our learning rate over epochs/steps. But why do we need to do that?\n1. The first reason is that our network may become stuck in either saddle points or local minima, and the low learning rate may not be sufficient to break out of the area and descend into areas of the loss landscape with lower loss.\n2. Secondly, our model and optimizer may be very sensitive to our initial learning rate choice. If we make a poor initial choice in learning rate, our model may be stuck from the very start.\n\nInstead, we can use Schedulers and specifically Cyclical Learning Rates(CLR) to oscillate our learning rate between upper and lower bounds, enabling us to:\n* Have more freedom in our initial learning rate choices.\n* Break out of saddle points and local minima.\n\nIn practice, using CLRs leads to far fewer learning rate tuning experiments along with near identical accuracy to exhaustive hyperparameter tuning.","metadata":{"id":"0RLKnQ_XI5fy"}},{"cell_type":"code","source":"def get_scheduler(optimizer, scheduler_params=params):\n    if scheduler_params['scheduler_name'] == 'CosineAnnealingWarmRestarts':\n        scheduler = CosineAnnealingWarmRestarts(\n            optimizer,\n            T_0=scheduler_params['T_0'],\n            eta_min=scheduler_params['min_lr'],\n            last_epoch=-1\n        )\n    elif scheduler_params['scheduler_name'] == 'OneCycleLR':\n        scheduler = OneCycleLR(\n            optimizer,\n            max_lr=scheduler_params['max_lr'],\n            steps_per_epoch=int(((scheduler_params['num_fold']-1) * train_df.shape[0]) / (scheduler_params['num_fold'] * scheduler_params['batch_size'])) + 1,\n            epochs=scheduler_params['epochs'],\n        )\n\n    elif scheduler_params['scheduler_name'] == 'CosineAnnealingLR':\n        scheduler = CosineAnnealingLR(\n            optimizer,\n            T_max=scheduler_params['T_max'],\n            eta_min=scheduler_params['min_lr'],\n            last_epoch=-1\n        )\n    return scheduler","metadata":{"id":"ikY8Rw2vI3fx","execution":{"iopub.status.busy":"2021-10-03T17:55:06.357187Z","iopub.execute_input":"2021-10-03T17:55:06.357392Z","iopub.status.idle":"2021-10-03T17:55:06.367749Z","shell.execute_reply.started":"2021-10-03T17:55:06.357369Z","shell.execute_reply":"2021-10-03T17:55:06.366881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN Model\n\nWe will inherit from the nn.Module class to define our model. This is a easy as well as effective way of defining the model as it allows very granular control over the complete NN. We are not using the full capability of it here since it is a starter model, but practicing similar definitions will help if/when you decide to play around a little more with the NN layers and functions.  \n\nAlso we are using timm for instancing a pre-trained model.  \nThe complete list of Pytorch pre-trained image models through timm can be found [here](https://rwightman.github.io/pytorch-image-models/)  ","metadata":{"id":"ZG2lASSbJ7dj"}},{"cell_type":"code","source":"class PetNet(nn.Module):\n    def __init__(self, model_name=params['model'], out_features=params['out_features'], inp_channels=params['inp_channels'],\n                 pretrained=params['pretrained'], num_dense=len(params['dense_features'])):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels)\n        n_features = self.model.head.in_features\n        self.model.head = nn.Linear(n_features, 128)\n        self.fc = nn.Sequential(\n            nn.Linear(128 + num_dense, 64),\n            nn.ReLU(),\n            nn.Linear(64, out_features)\n        )\n        self.dropout = nn.Dropout(params['dropout'])\n    \n    def forward(self, image, dense):\n        embeddings = self.model(image)\n        x = self.dropout(embeddings)\n        x = torch.cat([x, dense], dim=1)\n        output = self.fc(x)\n        return output","metadata":{"id":"WvxyQ04RJ0Ll","execution":{"iopub.status.busy":"2021-10-03T17:55:06.369233Z","iopub.execute_input":"2021-10-03T17:55:06.369726Z","iopub.status.idle":"2021-10-03T17:55:06.379147Z","shell.execute_reply.started":"2021-10-03T17:55:06.369682Z","shell.execute_reply":"2021-10-03T17:55:06.378516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train and Validation Functions","metadata":{"id":"4t9QFMaWL8UB"}},{"cell_type":"markdown","source":"## 1. Train Function","metadata":{"id":"ICwkAc81L-gl"}},{"cell_type":"code","source":"def train_fn(train_loader, model, criterion, optimizer, epoch, params, scheduler=None):\n    metric_monitor = MetricMonitor()\n    model.train()\n    stream = tqdm(train_loader)\n    \n    for i, (images, dense, target) in enumerate(stream, start=1):\n        if params['mixup']:\n            images, dense, target_a, target_b, lam = mixup_data(images, dense, target.view(-1, 1), params)\n            images = images.to(params['device'], dtype=torch.float)\n            dense = dense.to(params['device'], dtype=torch.float)\n            target_a = target_a.to(params['device'], dtype=torch.float)\n            target_b = target_b.to(params['device'], dtype=torch.float)\n        else:\n            images = images.to(params['device'], non_blocking=True)\n            dense = dense.to(params['device'], non_blocking=True)\n            target = target.to(params['device'], non_blocking=True).float().view(-1, 1)\n            \n        output = model(images, dense)\n        \n        if params['mixup']:\n            loss = mixup_criterion(criterion, output, target_a, target_b, lam)\n        else:\n            loss = criterion(output, target)\n            \n        rmse_score = usr_rmse_score(output, target)\n        metric_monitor.update('Loss', loss.item())\n        metric_monitor.update('RMSE', rmse_score)\n        loss.backward()\n        optimizer.step()\n            \n        if scheduler is not None:\n            scheduler.step()\n        \n        optimizer.zero_grad()\n        stream.set_description(f\"Epoch: {epoch:02}. Train. {metric_monitor}\")","metadata":{"id":"lPx4HwTbL3_N","execution":{"iopub.status.busy":"2021-10-03T17:55:06.380512Z","iopub.execute_input":"2021-10-03T17:55:06.380824Z","iopub.status.idle":"2021-10-03T17:55:06.39411Z","shell.execute_reply.started":"2021-10-03T17:55:06.380779Z","shell.execute_reply":"2021-10-03T17:55:06.393357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Validate Function","metadata":{"id":"wKVjdmIbMrN4"}},{"cell_type":"code","source":"def validate_fn(val_loader, model, criterion, epoch, params):\n    metric_monitor = MetricMonitor()\n    model.eval()\n    stream = tqdm(val_loader)\n    final_targets = []\n    final_outputs = []\n    with torch.no_grad():\n        for i, (images, dense, target) in enumerate(stream, start=1):\n            images = images.to(params['device'], non_blocking=True)\n            dense = dense.to(params['device'], non_blocking=True)\n            target = target.to(params['device'], non_blocking=True).float().view(-1, 1)\n            output = model(images, dense)\n            loss = criterion(output, target)\n            rmse_score = usr_rmse_score(output, target)\n            metric_monitor.update('Loss', loss.item())\n            metric_monitor.update('RMSE', rmse_score)\n            stream.set_description(f\"Epoch: {epoch:02}. Valid. {metric_monitor}\")\n            \n            targets = (target.detach().cpu().numpy()*100).tolist()\n            outputs = (torch.sigmoid(output).detach().cpu().numpy()*100).tolist()\n            \n            final_targets.extend(targets)\n            final_outputs.extend(outputs)\n    return final_outputs, final_targets","metadata":{"id":"d07oClh-MnzD","execution":{"iopub.status.busy":"2021-10-03T17:55:06.395494Z","iopub.execute_input":"2021-10-03T17:55:06.395744Z","iopub.status.idle":"2021-10-03T17:55:06.407366Z","shell.execute_reply.started":"2021-10-03T17:55:06.395713Z","shell.execute_reply":"2021-10-03T17:55:06.406748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run","metadata":{"id":"dlPu51_fMysB"}},{"cell_type":"code","source":"best_models_of_each_fold = []\nrmse_tracker = []","metadata":{"id":"hmgk5aQ1MwmG","execution":{"iopub.status.busy":"2021-10-03T17:55:06.4086Z","iopub.execute_input":"2021-10-03T17:55:06.409184Z","iopub.status.idle":"2021-10-03T17:55:06.418738Z","shell.execute_reply.started":"2021-10-03T17:55:06.40915Z","shell.execute_reply":"2021-10-03T17:55:06.417958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold in TRAIN_FOLDS:\n    print(''.join(['#']*50))\n    print(f\"{''.join(['=']*15)} TRAINING FOLD: {fold+1}/{train_df['kfold'].nunique()} {''.join(['=']*15)}\")\n    # Data Split to train and Validation\n    train = train_df[train_df['kfold'] != fold]\n    valid = train_df[train_df['kfold'] == fold]\n    \n    X_train = train['image_path']\n    X_train_dense = train[params['dense_features']]\n    y_train = train['Pawpularity']/100\n    X_valid = valid['image_path']\n    X_valid_dense = valid[params['dense_features']]\n    y_valid = valid['Pawpularity']/100\n    \n    # Pytorch Dataset Creation\n    train_dataset = CuteDataset(\n        images_filepaths=X_train.values,\n        dense_features=X_train_dense.values,\n        targets=y_train.values,\n        transform=get_train_transforms()\n    )\n\n    valid_dataset = CuteDataset(\n        images_filepaths=X_valid.values,\n        dense_features=X_valid_dense.values,\n        targets=y_valid.values,\n        transform=get_valid_transforms()\n    )\n    \n    # Pytorch Dataloader creation\n    train_loader = DataLoader(\n        train_dataset, batch_size=params['batch_size'], shuffle=True,\n        num_workers=params['num_workers'], pin_memory=True\n        )\n\n    val_loader = DataLoader(\n        valid_dataset, batch_size=params['batch_size'], shuffle=False,\n        num_workers=params['num_workers'], pin_memory=True\n        )\n    \n    # Model, cost function and optimizer instancing\n    model = PetNet()\n    model = model.to(params['device'])\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=params['lr'],\n                                  weight_decay=params['weight_decay'],\n                                  amsgrad=False)\n    scheduler = get_scheduler(optimizer)\n    \n    # Training and Validation Loop\n    best_rmse = np.inf\n    best_epoch = np.inf\n    best_model_name = None\n    for epoch in range(1, params['epochs'] + 1):\n        train_fn(train_loader, model, criterion, optimizer, epoch, params, scheduler)\n        predictions, valid_targets = validate_fn(val_loader, model, criterion, epoch, params)\n        rmse = round(mean_squared_error(valid_targets, predictions, squared=False), 3)\n        if rmse < best_rmse:\n            best_rmse = rmse\n            best_epoch = epoch\n            if best_model_name is not None:\n                os.remove(best_model_name)\n            torch.save(model.state_dict(),\n                       f\"{params['model']}_{epoch}_epoch_f{fold+1}_{rmse}_rmse.pth\")\n            best_model_name = f\"{params['model']}_{epoch}_epoch_f{fold+1}_{rmse}_rmse.pth\"\n\n    # Print summary of this fold\n    print('')\n    print(f'The best RMSE: {best_rmse} for fold {fold+1} was achieved on epoch: {best_epoch}.')\n    print(f'The Best saved model is: {best_model_name}')\n    best_models_of_each_fold.append(best_model_name)\n    rmse_tracker.append(best_rmse)\n    print(''.join(['#']*50))\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n\nprint('')\nprint(f'Average RMSE of all folds: {round(np.mean(rmse_tracker), 4)}')","metadata":{"execution":{"iopub.status.busy":"2021-10-03T17:55:06.419839Z","iopub.execute_input":"2021-10-03T17:55:06.420731Z","iopub.status.idle":"2021-10-03T18:07:46.798273Z","shell.execute_reply.started":"2021-10-03T17:55:06.420694Z","shell.execute_reply":"2021-10-03T18:07:46.796978Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, name in enumerate(best_models_of_each_fold):\n    print(f'Best model of fold {i+1}: {name}')","metadata":{"id":"DLW11PItOY0Z","execution":{"iopub.status.busy":"2021-10-03T18:07:46.79938Z","iopub.status.idle":"2021-10-03T18:07:46.799883Z","shell.execute_reply.started":"2021-10-03T18:07:46.799638Z","shell.execute_reply":"2021-10-03T18:07:46.799662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a simple starter kernel on implementation of Transfer Learning using Pytorch for this problem. Pytorch has many SOTA Image models which you can try out using the guidelines in this notebook.\n\nI hope you have learnt something from this notebook. I have created this notebook as a baseline model, which you can easily fork and paly-around with to get much better results. I might update parts of it down the line when I get more GPU hours and some interesting ideas.\n\n**If you liked this notebook and use parts of it in you code, please show some support by upvoting this kernel. It keeps me inspired to come-up with such starter kernels and share it with the community.**\n\nThanks and happy kaggling!","metadata":{}}]}