{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Learning to Resize Images for Vision Transformer\n\n\nIn this notebook, it's shown how we can train a higher resolution image for **Vision Transformer** model. Usually transformer based models with high resolution may not fit into GPU. In such cases, we can adopt a **Trainable Resizer** mechanism as a backbone of the transformer models and perform as a joint learning of the image resizer and recognition models simultaneously.","metadata":{}},{"cell_type":"markdown","source":"[**Learning to Resize Images for Computer Vision Tasks** - Google Research](https://arxiv.org/pdf/2103.09950v1.pdf). For a given image resolution and a model, this research work answer how to best resize that image in a target resolution. Off-the-shelf image resizers for example: bilinear, bicubic methods are commonly used in most of the machine learning softwares. But this may limit the on-task performance of the trained model. In the above work, it's shown that typical linear resizer can be replaced with the **Learned Resizer**. \n\nIn the paper, they showed that the proposed resizer mechanism improve the classificaiton mdoels. They added the resizer mechanism to the classification models such as `DenseNet`, `InceptionNet` etc. In this way, we can input very large image size and the resizer mechanism will downscale the image appropriately for the actual mdoel. \n\n![rtee](https://user-images.githubusercontent.com/17668390/138254072-f87daa13-12cc-4c6a-9145-a567f644cb12.png)","metadata":{}},{"cell_type":"markdown","source":"[**Vision Transformer** - Google Research](https://arxiv.org/pdf/2010.11929.pdf). We know that the transformer models are computationally expensive. And thus limits the input size roughly around `224`, `384`. So, the idea is to use this **resizer mechanism** as a bacbone of the **vision transformer**, so that we can input enough large image for **joint learning**. So, the overall model architecture could be \n\n![Presentation2](https://user-images.githubusercontent.com/17668390/138256285-c24f98db-ce35-4877-8741-221fd57d895e.jpg)","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport tensorflow as tf \nfrom tensorflow import keras \nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras import layers, losses, optimizers, metrics\nimport tensorflow_hub as hub\n\ntry:\n    physical_devices = tf.config.list_physical_devices('GPU')\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\nexcept:\n    pass \n\n# enable mixed_precision and jit compiler \ntf.keras.mixed_precision.set_global_policy('mixed_float16')\ntf.config.optimizer.set_jit(True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-19T13:41:32.647138Z","iopub.execute_input":"2022-02-19T13:41:32.647664Z","iopub.status.idle":"2022-02-19T13:41:38.270855Z","shell.execute_reply.started":"2022-02-19T13:41:32.647597Z","shell.execute_reply":"2022-02-19T13:41:38.270104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INP_SIZE      = (512, 512) # Input size of the Image Resizer Module (IRM)\nTARGET_SIZE   = (224, 224) # Output size of IRM and Input size of the Vision Transformer \nINTERPOLATION = \"bilinear\"","metadata":{"execution":{"iopub.status.busy":"2022-02-19T13:41:59.201137Z","iopub.execute_input":"2022-02-19T13:41:59.201418Z","iopub.status.idle":"2022-02-19T13:41:59.206759Z","shell.execute_reply.started":"2022-02-19T13:41:59.201388Z","shell.execute_reply":"2022-02-19T13:41:59.205308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare tf.data object","metadata":{}},{"cell_type":"code","source":"NUM_FOLDS  = 10\nBATCH_SIZE = 24\nSEED       = INP_SIZE[0]\n\nDATA_DIR  = '../input/petfinder-pawpularity-score/'\nTRAIN_DIR = DATA_DIR + 'train/'\nTEST_DIR  = DATA_DIR + 'test/'\n\n# SetAutoTune\nAUTOTUNE = tf.data.AUTOTUNE  \n\ndef build_augmenter(is_labelled):\n    def augment(img):\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_flip_up_down(img)\n        img = tf.image.random_saturation(img, 0.65, 1.05)\n        img = tf.image.random_brightness(img, 0.05)\n        img = tf.image.random_contrast(img, 0.75, 1.05)\n        img = tf.image.random_hue(img, 0.05)\n        return img\n    \n    def augment_with_labels(img, label):\n        return augment(img), label\n    return augment_with_labels if is_labelled else augment\n\ndef build_decoder(is_labelled):\n    def decode(path):\n        file_bytes = tf.io.read_file(path)\n        img = tf.image.decode_jpeg(file_bytes, channels = 3)\n        img = tf.image.resize(img, (INP_SIZE))\n        return tf.divide(img, 255.)\n    \n    def decode_with_labels(path, label):\n        return decode(path), label\n    \n    return decode_with_labels if is_labelled else decode\n\ndef create_dataset(df, \n                   batch_size  = 32, \n                   is_labelled = False, \n                   augment     = False, \n                   repeat      = False, \n                   shuffle     = False):\n    decode_fn    = build_decoder(is_labelled)\n    augmenter_fn = build_augmenter(is_labelled)\n    \n    # Create Dataset\n    if is_labelled:\n        dataset = tf.data.Dataset.from_tensor_slices((df['Id'].values, df['target_value'].values))\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices((df['Id'].values))\n        \n    dataset = dataset.map(decode_fn, num_parallel_calls = AUTOTUNE)\n    dataset = dataset.map(augmenter_fn, num_parallel_calls = AUTOTUNE) if augment else dataset\n    dataset = dataset.repeat() if repeat else dataset\n    dataset = dataset.shuffle(1024, reshuffle_each_iteration = True) if shuffle else dataset\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-02-19T13:42:08.701992Z","iopub.execute_input":"2022-02-19T13:42:08.702277Z","iopub.status.idle":"2022-02-19T13:42:08.716342Z","shell.execute_reply.started":"2022-02-19T13:42:08.702225Z","shell.execute_reply":"2022-02-19T13:42:08.715202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Train Data\ntrain_df = pd.read_csv(f'{DATA_DIR}train.csv')\ntrain_df['Id'] = train_df['Id'].apply(lambda x: f'{TRAIN_DIR}{x}.jpg')\n\n# Set a specific label to be able to perform stratification\ntrain_df['stratify_label'] = pd.qcut(train_df['Pawpularity'], q = 30, labels = range(30))\n\n# Though it's a regression problem, but it's been informed that if we treat it as a -\n# classification problmen, it would be useful. So, here we normalize the target labels.\n# Label value to be used for feature model 'classification' training.\ntrain_df['target_value'] = train_df['Pawpularity'] / 100.\n\n# Summary\nprint(f'train_df: {train_df.shape}')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-19T13:43:20.684519Z","iopub.execute_input":"2022-02-19T13:43:20.685037Z","iopub.status.idle":"2022-02-19T13:43:20.727429Z","shell.execute_reply.started":"2022-02-19T13:43:20.685Z","shell.execute_reply":"2022-02-19T13:43:20.726728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Test Data\ntest_df = pd.read_csv(f'{DATA_DIR}test.csv')\ntest_df['Id'] = test_df['Id'].apply(lambda x: f'{TEST_DIR}{x}.jpg')\ntest_df['Pawpularity'] = 0\n\n# Summary\nprint(f'test_df: {test_df.shape}')\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-19T13:43:27.649539Z","iopub.execute_input":"2022-02-19T13:43:27.650078Z","iopub.status.idle":"2022-02-19T13:43:27.67496Z","shell.execute_reply.started":"2022-02-19T13:43:27.650039Z","shell.execute_reply":"2022-02-19T13:43:27.674177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implementation of Learning to Resize Mechanism\n\nQuoting from the paper: \n\n> Our proposed resizer architecture is shown in below figure. Perhaps the most important characteristics of this model are\n(1) the bilinear feature resizing, and (2) the skip connection that accommodates combining the bilinearly resized image\nand the CNN features. The former factor allows for incorporation of features computed at original resolution into\nthe model. Also, the skip connection accommodates for an easier learning process, because the resizer model can directly pass the bilinearly resized image into the baseline task. Note that the bilinear feature resizer shown in Figure 3 acts as a feed-forward bottleneck (down-scaling), but in principle it can also act as an inverse bottleneck as well (up-scaling). It is worth noting that unlike typical encoderdecoder architectures, the proposed architecture allows for resizing an image to any target size and aspect ratio. It is also important to highlight that performance of the learned resizer is barely dependent on the bilinear resizer choice, meaning it can be safely replaced with o\n\n![image resizer](https://user-images.githubusercontent.com/17668390/138250657-29995830-b903-447f-8729-09b72b90ab3c.png)","metadata":{}},{"cell_type":"code","source":"# ref: https://keras.io/examples/vision/learnable_resizer/\ndef residual_block(x):\n    shortcut = x\n    def conv_bn_leaky(inputs, filters, kernel_size, strides):\n        x = layers.Conv2D(filters, \n                          kernel_size, \n                          strides=strides, \n                          use_bias=False, \n                          padding='same')(inputs)\n        x = layers.BatchNormalization()(x)\n        x = layers.LeakyReLU()(x)\n        return x \n    \n    def conv_bn(inputs, filters, kernel_size, strides):\n        x = layers.Conv2D(filters, \n                          kernel_size, \n                          strides, \n                          padding='same')(inputs)\n        x = layers.BatchNormalization()(x)\n        return x \n    \n    x = conv_bn_leaky(x, 16, 3, 1)\n    x = conv_bn(x, 16, 3, 1)\n    x = layers.add([shortcut, x])\n    return x\n\n\ndef get_learnable_resizer(filters=16,\n                          num_res_blocks=1, \n                          interpolation=INTERPOLATION):\n    \n    inputs = layers.Input(shape=[None, None, 3])\n    \n    # First, perform naive resizing.\n    naive_resize = layers.Resizing(\n        *TARGET_SIZE, interpolation=interpolation\n    )(inputs)\n\n    # First convolution block without batch normalization.\n    x = layers.Conv2D(filters=filters, kernel_size=7, strides=1, padding='same')(inputs)\n    x = layers.LeakyReLU(0.2)(x)\n\n    # Second convolution block with batch normalization.\n    x = layers.Conv2D(filters=filters, kernel_size=1, strides=1, padding='same')(x)\n    x = layers.LeakyReLU(0.2)(x)\n    x = layers.BatchNormalization()(x)\n\n    # Intermediate resizing as a bottleneck.\n    bottleneck = layers.Resizing(\n        *TARGET_SIZE, interpolation=interpolation\n    )(x)\n    \n    # Residual passes.\n    x = residual_block(bottleneck)\n    for i in range(1, num_res_blocks):\n        x = residual_block(x)\n        \n    # Projection.\n    x = layers.Conv2D(\n        filters=filters, kernel_size=3, strides=1, padding=\"same\", use_bias=False\n    )(x)\n    x = layers.BatchNormalization()(x)\n\n    # Skip connection.\n    x = layers.Add()([bottleneck, x])\n\n    # Final resized image.\n    x = layers.Conv2D(filters=3, kernel_size=7, strides=1, padding=\"same\")(x)\n    final_resize = layers.Add()([naive_resize, x])\n    return keras.Model(inputs, final_resize, name=\"learnable_resizer\")","metadata":{"execution":{"iopub.status.busy":"2022-02-19T13:43:36.355619Z","iopub.execute_input":"2022-02-19T13:43:36.35621Z","iopub.status.idle":"2022-02-19T13:43:36.369383Z","shell.execute_reply.started":"2022-02-19T13:43:36.356173Z","shell.execute_reply":"2022-02-19T13:43:36.368505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The number of residual block in image resizer modules \nNUM_RESIDUAL_BLOCK = 10\nLEARNABLE_RESIZER  = get_learnable_resizer(num_res_blocks=NUM_RESIDUAL_BLOCK)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T13:43:42.115614Z","iopub.execute_input":"2022-02-19T13:43:42.116187Z","iopub.status.idle":"2022-02-19T13:43:44.352318Z","shell.execute_reply.started":"2022-02-19T13:43:42.116144Z","shell.execute_reply":"2022-02-19T13:43:44.351566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check**\n\nLet's check how raw image get tansformed with this resizer blocks with initial states.","metadata":{}},{"cell_type":"code","source":"training_dataset = create_dataset(train_df,\n                                  batch_size  = BATCH_SIZE, \n                                  is_labelled = True, \n                                  augment = True,\n                                  repeat  = False, \n                                  shuffle = False)\nsample_images, _ = next(iter(training_dataset))","metadata":{"execution":{"iopub.status.busy":"2022-02-19T13:44:47.213098Z","iopub.execute_input":"2022-02-19T13:44:47.213566Z","iopub.status.idle":"2022-02-19T13:44:48.147969Z","shell.execute_reply.started":"2022-02-19T13:44:47.213526Z","shell.execute_reply":"2022-02-19T13:44:48.147106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 10))\nfor i, image in enumerate(sample_images[:6]):\n    ax = plt.subplot(3, 4, 2 * i + 1)\n    plt.title(\"Input Image\")\n    plt.imshow(image.numpy().squeeze())\n    plt.axis(\"off\")\n\n    ax = plt.subplot(3, 4, 2 * i + 2)\n    resized_image = LEARNABLE_RESIZER(image[None, ...])\n    resized_image = tf.cast(resized_image, dtype=tf.float32)\n    plt.title(\"Resized Image\")\n    plt.imshow(resized_image.numpy().squeeze())\n    plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2022-02-19T13:44:52.843772Z","iopub.execute_input":"2022-02-19T13:44:52.844033Z","iopub.status.idle":"2022-02-19T13:44:59.789313Z","shell.execute_reply.started":"2022-02-19T13:44:52.844003Z","shell.execute_reply":"2022-02-19T13:44:59.780077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Learned Resizer + Vision Transformer \n\nWe'll use vision transformer from [tf.hub](https://tfhub.dev/sayakpaul/collections/vision_transformer/1). However, initially we enabled mixed precision technique. But in `hub.KerasLayer`, the data types used by a saved model have been fixed at saving time, [source-link](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer). So, we may need to set global policy back to `float32` right before loading the `hub.KerasLayer` and reset to `float16` after that for mixed precision training. Directly quoting form the source:\n\n> Note: The data types used by a saved model have been fixed at saving time. Using tf.keras.mixed_precision etc. has no effect on the saved model that gets loaded by a hub.KerasLayer.","metadata":{}},{"cell_type":"code","source":"EPOCHS       = 20\nVERBOSE      = 1\nlr           = 0.0003\nLABEL_SMOOTH = 0.01","metadata":{"execution":{"iopub.status.busy":"2022-02-19T13:45:20.812191Z","iopub.execute_input":"2022-02-19T13:45:20.812486Z","iopub.status.idle":"2022-02-19T13:45:20.816704Z","shell.execute_reply.started":"2022-02-19T13:45:20.812455Z","shell.execute_reply":"2022-02-19T13:45:20.816022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"handle = \"../input/vitjax2tf\"\ndef get_model(plot_modal, \n              print_summary, \n              with_compile):\n    # Input layer + Resizer Blocks\n    inputs = layers.Input((INP_SIZE[0], INP_SIZE[1], 3))\n    x = LEARNABLE_RESIZER(inputs)\n    \n    # Loading Vision Transformer from tf.hub\n    keras.mixed_precision.set_global_policy('float32')\n    hub_layer = hub.KerasLayer(handle, trainable=True)\n    backbone = keras.Sequential(\n        [\n            layers.InputLayer((TARGET_SIZE[0], TARGET_SIZE[1], 3)),\n            hub_layer\n        ], name='vit'\n    )\n    keras.mixed_precision.set_global_policy('mixed_float16')\n    outputs = backbone(x)\n    \n    # End layers with classifier. \n    tail = keras.Sequential(\n            [\n                layers.Dropout(0.5),\n                layers.BatchNormalization(),\n                layers.Dense(1, activation = 'sigmoid', dtype='float32')\n            ], name='head'\n        )\n    \n    # bind all: complete model \n    model = keras.Model(inputs, tail(outputs))\n    \n    if plot_modal:\n        display(keras.utils.plot_model(model, \n                                       show_shapes=True, \n                                       show_layer_names=True, \n                                       expand_nested=True))\n    if print_summary:\n        print(model.summary())\n        \n    if with_compile:\n        model.compile(\n            optimizer = keras.optimizers.Adam(learning_rate = lr),  \n            loss = losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTH), \n            metrics = [metrics.RootMeanSquaredError('rmse')]\n        )\n    return model ","metadata":{"execution":{"iopub.status.busy":"2022-02-19T13:45:23.571096Z","iopub.execute_input":"2022-02-19T13:45:23.571635Z","iopub.status.idle":"2022-02-19T13:45:23.58117Z","shell.execute_reply.started":"2022-02-19T13:45:23.571595Z","shell.execute_reply":"2022-02-19T13:45:23.580477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_model(plot_modal=True, print_summary=True, with_compile=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T13:45:30.69201Z","iopub.execute_input":"2022-02-19T13:45:30.692282Z","iopub.status.idle":"2022-02-19T13:45:43.146997Z","shell.execute_reply.started":"2022-02-19T13:45:30.692233Z","shell.execute_reply":"2022-02-19T13:45:43.146287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_lr_callback(batch_size=8):\n    lr_start   = 0.000005\n    lr_max     = 0.00000125 * batch_size \n    lr_min     = 0.000001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n        return lr\n    return callbacks.LearningRateScheduler(lrfn, verbose=True)\n\ndef model_callback(fold):\n    ckpt = callbacks.ModelCheckpoint(f'feature_model_{fold}.h5',\n                                     verbose = 1, \n                                     monitor = 'val_rmse',\n                                     mode = 'min',\n                                     save_weights_only = True,\n                                     save_best_only = True)\n    \n    return [ckpt, get_lr_callback(BATCH_SIZE)]","metadata":{"execution":{"iopub.status.busy":"2022-02-19T13:46:02.692671Z","iopub.execute_input":"2022-02-19T13:46:02.693044Z","iopub.status.idle":"2022-02-19T13:46:02.701787Z","shell.execute_reply.started":"2022-02-19T13:46:02.692997Z","shell.execute_reply":"2022-02-19T13:46:02.701026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nfrom sklearn.model_selection import StratifiedKFold\n\n# OOF RMSE Placeholder\nall_val_loss = []\nkfold = StratifiedKFold(n_splits = NUM_FOLDS, \n                        shuffle = True, random_state = SEED)\nfor fold, (train_index, val_index) in enumerate(kfold.split(train_df.index,\n                                                            train_df['stratify_label'])):\n    if fold == 1:\n        print(f'\\nFold {fold}\\n')\n        # Pre model.fit cleanup\n        tf.keras.backend.clear_session()\n        gc.collect()\n\n        # Create Model\n        model = get_model(plot_modal    = False, \n                          print_summary = False,\n                          with_compile  = True)\n        for i in range(len(model.weights)):\n            model.weights[i]._handle_name = model.weights[i].name + str(i)\n    \n        # Create TF Datasets\n        trn = train_df.iloc[train_index]\n        val = train_df.iloc[val_index]\n        training_dataset = create_dataset(trn, \n                                          batch_size  = BATCH_SIZE, \n                                          is_labelled = True, \n                                          augment     = True, \n                                          repeat      = True, \n                                          shuffle     = True)\n        validation_dataset = create_dataset(val, \n                                            batch_size  = BATCH_SIZE, \n                                            is_labelled = True,\n                                            augment     = False, \n                                            repeat      = True,\n                                            shuffle     = False)\n        # Fit Model\n        history = model.fit(training_dataset,\n                            epochs = EPOCHS,\n                            steps_per_epoch  = trn.shape[0] // BATCH_SIZE,\n                            validation_steps = val.shape[0] // BATCH_SIZE,\n                            callbacks = model_callback(fold),\n                            validation_data = validation_dataset,\n                            verbose = VERBOSE)   \n\n        # Validation Information\n        best_val_loss = min(history.history['val_rmse'])\n        all_val_loss.append(best_val_loss)\n        print(f'\\nValidation RMSE: {best_val_loss}\\n')","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:30:33.306136Z","iopub.execute_input":"2022-02-16T15:30:33.307101Z","iopub.status.idle":"2022-02-16T15:30:34.707369Z","shell.execute_reply.started":"2022-02-16T15:30:33.307062Z","shell.execute_reply":"2022-02-16T15:30:34.705072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check**\n\nSample output of Trained Resizer.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16, 10))\nfor i, image in enumerate(sample_images[:6]):\n    ax = plt.subplot(3, 4, 2 * i + 1)\n    plt.title(\"Input Image\")\n    plt.imshow(image.numpy().squeeze())\n    plt.axis(\"off\")\n\n    ax = plt.subplot(3, 4, 2 * i + 2)\n    resized_image = LEARNABLE_RESIZER(image[None, ...])\n    resized_image = tf.cast(resized_image, dtype=tf.float32)\n    plt.title(\"Resized Image\")\n    plt.imshow(resized_image.numpy().squeeze())\n    plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2022-02-19T13:46:09.953003Z","iopub.execute_input":"2022-02-19T13:46:09.953727Z","iopub.status.idle":"2022-02-19T13:46:11.236702Z","shell.execute_reply.started":"2022-02-19T13:46:09.953684Z","shell.execute_reply":"2022-02-19T13:46:11.23608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inference**","metadata":{}},{"cell_type":"code","source":"cb_test_set = create_dataset(test_df, \n                             batch_size  = BATCH_SIZE,\n                             is_labelled = False,\n                             repeat      = False, \n                             shuffle     = False)\n\nsubmission_df = pd.read_csv(f'{DATA_DIR}sample_submission.csv')\n# we train only first fold.\nmodel.load_weights(\"./feature_model_1.h5\")\nsubmission_df['Pawpularity'] = model.predict(cb_test_set)*100\nsubmission_df.to_csv('submission.csv', index = False)\nsubmission_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T13:46:18.342765Z","iopub.execute_input":"2022-02-19T13:46:18.343025Z","iopub.status.idle":"2022-02-19T13:46:18.689816Z","shell.execute_reply.started":"2022-02-19T13:46:18.342996Z","shell.execute_reply":"2022-02-19T13:46:18.687969Z"},"trusted":true},"execution_count":null,"outputs":[]}]}