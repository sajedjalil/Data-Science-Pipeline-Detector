{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# やまぴーさん part","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport torch\nimport importlib\nimport cv2 \nimport pandas as pd\n\nfrom PIL import Image\nfrom IPython.display import display","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:55:14.481719Z","iopub.execute_input":"2022-01-04T06:55:14.481992Z","iopub.status.idle":"2022-01-04T06:55:16.011644Z","shell.execute_reply.started":"2022-01-04T06:55:14.481956Z","shell.execute_reply":"2022-01-04T06:55:16.010855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n%cp -r /kaggle/input/yolox-pet2 /kaggle/working/\n%cd /kaggle/working/yolox-pet2/yolox-dep","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:55:16.01427Z","iopub.execute_input":"2022-01-04T06:55:16.014721Z","iopub.status.idle":"2022-01-04T06:55:32.457902Z","shell.execute_reply.started":"2022-01-04T06:55:16.014682Z","shell.execute_reply":"2022-01-04T06:55:32.457145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n!pip install pip-21.3.1-py3-none-any.whl -f ./ --no-index\n!pip install loguru-0.5.3-py3-none-any.whl -f ./ --no-index\n!pip install ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl -f ./ --no-index\n!pip install onnx-1.8.1-cp37-cp37m-manylinux2010_x86_64.whl -f ./ --no-index\n!pip install onnxruntime-1.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl -f ./ --no-index\n!pip install onnxoptimizer-0.2.6-cp37-cp37m-manylinux2014_x86_64.whl -f ./ --no-index\n!pip install thop-0.0.31.post2005241907-py3-none-any.whl -f ./ --no-index\n!pip install tabulate-0.8.9-py3-none-any.whl -f ./ --no-index","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-04T06:55:32.460311Z","iopub.execute_input":"2022-01-04T06:55:32.460766Z","iopub.status.idle":"2022-01-04T06:56:45.006222Z","shell.execute_reply.started":"2022-01-04T06:55:32.460724Z","shell.execute_reply":"2022-01-04T06:56:45.005403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n%cd /kaggle/working/yolox-pet2/YOLOX\n!pip install -r requirements.txt\n!pip install -v -e . ","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-04T06:56:45.007707Z","iopub.execute_input":"2022-01-04T06:56:45.007979Z","iopub.status.idle":"2022-01-04T06:57:59.824527Z","shell.execute_reply.started":"2022-01-04T06:56:45.007945Z","shell.execute_reply":"2022-01-04T06:57:59.823627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Install CocoAPI tool\n%cd /kaggle/working/yolox-pet2/yolox-dep/cocoapi/PythonAPI\n\n!make\n!make install\n!python setup.py install","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-04T06:57:59.826283Z","iopub.execute_input":"2022-01-04T06:57:59.826717Z","iopub.status.idle":"2022-01-04T06:58:18.493167Z","shell.execute_reply.started":"2022-01-04T06:57:59.826677Z","shell.execute_reply":"2022-01-04T06:58:18.492216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pycocotools","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:18.495008Z","iopub.execute_input":"2022-01-04T06:58:18.495481Z","iopub.status.idle":"2022-01-04T06:58:18.501427Z","shell.execute_reply.started":"2022-01-04T06:58:18.49543Z","shell.execute_reply":"2022-01-04T06:58:18.500675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\nfrom glob import glob\nimport pandas as pd\nfrom pandarallel import pandarallel\npandarallel.initialize(progress_bar=True)\nimport torchvision.transforms as T\nfrom torchvision.io import ImageReadMode, read_image, write_jpeg\nimport gc","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:18.502756Z","iopub.execute_input":"2022-01-04T06:58:18.503178Z","iopub.status.idle":"2022-01-04T06:58:18.760335Z","shell.execute_reply.started":"2022-01-04T06:58:18.503143Z","shell.execute_reply":"2022-01-04T06:58:18.759647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:18.761637Z","iopub.execute_input":"2022-01-04T06:58:18.761882Z","iopub.status.idle":"2022-01-04T06:58:18.769572Z","shell.execute_reply.started":"2022-01-04T06:58:18.761848Z","shell.execute_reply":"2022-01-04T06:58:18.768026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/petfinder-pawpularity-score/test.csv\")\n# df = pd.read_csv(\"../input/petfinder-pawpularity-score/train.csv\").sample(6800)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:18.773013Z","iopub.execute_input":"2022-01-04T06:58:18.773689Z","iopub.status.idle":"2022-01-04T06:58:18.790832Z","shell.execute_reply.started":"2022-01-04T06:58:18.773646Z","shell.execute_reply":"2022-01-04T06:58:18.790157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = pd.concat([df]*900, axis=\"index\").reset_index(drop=True)\n# df.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:18.791897Z","iopub.execute_input":"2022-01-04T06:58:18.792222Z","iopub.status.idle":"2022-01-04T06:58:18.796384Z","shell.execute_reply.started":"2022-01-04T06:58:18.792187Z","shell.execute_reply":"2022-01-04T06:58:18.795533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:18.797508Z","iopub.execute_input":"2022-01-04T06:58:18.798352Z","iopub.status.idle":"2022-01-04T06:58:18.804429Z","shell.execute_reply.started":"2022-01-04T06:58:18.798316Z","shell.execute_reply":"2022-01-04T06:58:18.803255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tqdm import tqdm_notebook as tqdm\n# import shutil\n\n# !mkdir /kaggle/working/copy_train\n# for file in tqdm(df[\"Id\"]):\n#     shutil.copyfile(f\"../input/petfinder-pawpularity-score/train/{file}.jpg\", f\"/kaggle/working/copy_train/{file}.jpg\")","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:18.805917Z","iopub.execute_input":"2022-01-04T06:58:18.806459Z","iopub.status.idle":"2022-01-04T06:58:18.813007Z","shell.execute_reply.started":"2022-01-04T06:58:18.806424Z","shell.execute_reply":"2022-01-04T06:58:18.812236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Id\"] = \"../input/petfinder-pawpularity-score/test/\" + df[\"Id\"] + \".jpg\"\n# df[\"Id\"] = \"../input/petfinder-pawpularity-score/train/\" + df[\"Id\"] + \".jpg\"\n# df[\"Id\"].head()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:18.81391Z","iopub.execute_input":"2022-01-04T06:58:18.815642Z","iopub.status.idle":"2022-01-04T06:58:18.828924Z","shell.execute_reply.started":"2022-01-04T06:58:18.815602Z","shell.execute_reply":"2022-01-04T06:58:18.828095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/test_center_crop/","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:18.831276Z","iopub.execute_input":"2022-01-04T06:58:18.831841Z","iopub.status.idle":"2022-01-04T06:58:19.489364Z","shell.execute_reply.started":"2022-01-04T06:58:18.831804Z","shell.execute_reply":"2022-01-04T06:58:19.48845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def center_crop(Id: str):\n    image = read_image(Id, mode=ImageReadMode.RGB)\n    write_jpeg(\n        T.CenterCrop(min(image.shape[1:]))(image),\n        f'/kaggle/working/test_center_crop/{Id.replace(\"../input/petfinder-pawpularity-score/test/\", \"\")}',\n#         f'/kaggle/working/test_center_crop/{Id.replace(\"../input/petfinder-pawpularity-score/train/\", \"\")}',\n        quality=100\n    )","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:19.491819Z","iopub.execute_input":"2022-01-04T06:58:19.492119Z","iopub.status.idle":"2022-01-04T06:58:19.49914Z","shell.execute_reply.started":"2022-01-04T06:58:19.492077Z","shell.execute_reply":"2022-01-04T06:58:19.497515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndf[\"Id\"].parallel_apply(center_crop)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:19.500869Z","iopub.execute_input":"2022-01-04T06:58:19.501213Z","iopub.status.idle":"2022-01-04T06:58:19.685893Z","shell.execute_reply.started":"2022-01-04T06:58:19.501175Z","shell.execute_reply":"2022-01-04T06:58:19.685142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:19.688638Z","iopub.execute_input":"2022-01-04T06:58:19.688857Z","iopub.status.idle":"2022-01-04T06:58:19.810642Z","shell.execute_reply.started":"2022-01-04T06:58:19.688825Z","shell.execute_reply":"2022-01-04T06:58:19.809524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/yolox-pet2/YOLOX","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:19.812702Z","iopub.execute_input":"2022-01-04T06:58:19.813488Z","iopub.status.idle":"2022-01-04T06:58:19.820648Z","shell.execute_reply.started":"2022-01-04T06:58:19.813444Z","shell.execute_reply":"2022-01-04T06:58:19.819683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import argparse\nimport os\nimport time\n\nimport numpy as np\nfrom loguru import logger\n# import jpeg4py as jpeg\n\nimport cv2\n\nimport torch\n\nfrom yolox.data.data_augment import ValTransform\nfrom yolox.data.datasets import COCO_CLASSES\nfrom yolox.exp import get_exp\nfrom yolox.utils import fuse_model, get_model_info, postprocess, vis\n\nIMAGE_EXT = [\".jpg\", \".jpeg\", \".webp\", \".bmp\", \".png\"]","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:19.822742Z","iopub.execute_input":"2022-01-04T06:58:19.823153Z","iopub.status.idle":"2022-01-04T06:58:19.888495Z","shell.execute_reply.started":"2022-01-04T06:58:19.823114Z","shell.execute_reply":"2022-01-04T06:58:19.887832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_parser():\n    parser = argparse.ArgumentParser(\"YOLOX Demo!\")\n    parser.add_argument(\n        \"demo\", default=\"image\", help=\"demo type, eg. image, video and webcam\"\n    )\n    parser.add_argument(\"-expn\", \"--experiment-name\", type=str, default=None)\n    parser.add_argument(\"-n\", \"--name\", type=str, default=None, help=\"model name\")\n\n    parser.add_argument(\n        \"--path\", default=\"./assets/dog.jpg\", help=\"path to images or video\"\n    )\n    parser.add_argument(\"--camid\", type=int, default=0, help=\"webcam demo camera id\")\n    parser.add_argument(\n        \"--save_result\",\n        action=\"store_true\",\n        help=\"whether to save the inference result of image/video\",\n    )\n\n    # exp file\n    parser.add_argument(\n        \"-f\",\n        \"--exp_file\",\n        default=None,\n        type=str,\n        help=\"pls input your experiment description file\",\n    )\n    parser.add_argument(\"-c\", \"--ckpt\", default=None, type=str, help=\"ckpt for eval\")\n    parser.add_argument(\n        \"--device\",\n        default=\"cpu\",\n        type=str,\n        help=\"device to run our model, can either be cpu or gpu\",\n    )\n    parser.add_argument(\"--conf\", default=0.3, type=float, help=\"test conf\")\n    parser.add_argument(\"--nms\", default=0.3, type=float, help=\"test nms threshold\")\n    parser.add_argument(\"--tsize\", default=None, type=int, help=\"test img size\")\n    parser.add_argument(\n        \"--fp16\",\n        dest=\"fp16\",\n        default=False,\n        action=\"store_true\",\n        help=\"Adopting mix precision evaluating.\",\n    )\n    parser.add_argument(\n        \"--legacy\",\n        dest=\"legacy\",\n        default=False,\n        action=\"store_true\",\n        help=\"To be compatible with older versions\",\n    )\n    parser.add_argument(\n        \"--fuse\",\n        dest=\"fuse\",\n        default=False,\n        action=\"store_true\",\n        help=\"Fuse conv and bn for testing.\",\n    )\n    parser.add_argument(\n        \"--trt\",\n        dest=\"trt\",\n        default=False,\n        action=\"store_true\",\n        help=\"Using TensorRT model for testing.\",\n    )\n    return parser\n\n\ndef get_image_list(path):\n    image_names = []\n    for maindir, subdir, file_name_list in os.walk(path):\n        for filename in file_name_list:\n            apath = os.path.join(maindir, filename)\n            ext = os.path.splitext(apath)[1]\n            if ext in IMAGE_EXT:\n                image_names.append(apath)\n    return image_names\n\n\nclass Predictor(object):\n    def __init__(\n        self,\n        model,\n        exp,\n        cls_names=COCO_CLASSES,\n        trt_file=None,\n        decoder=None,\n        device=\"cpu\",\n        fp16=False,\n        legacy=False,\n    ):\n        self.model = model\n        self.cls_names = cls_names\n        self.decoder = decoder\n        self.num_classes = exp.num_classes\n        self.confthre = exp.test_conf\n        self.nmsthre = exp.nmsthre\n        self.test_size = exp.test_size\n        self.device = device\n        self.fp16 = fp16\n        self.preproc = ValTransform(legacy=legacy)\n        if trt_file is not None:\n            from torch2trt import TRTModule\n\n            model_trt = TRTModule()\n            model_trt.load_state_dict(torch.load(trt_file))\n\n            x = torch.ones(1, 3, exp.test_size[0], exp.test_size[1]).cuda()\n            self.model(x)\n            self.model = model_trt\n\n    def inference(self, img):\n        img_info = {\"id\": 0}\n        if isinstance(img, str):\n            img_info[\"file_name\"] = os.path.basename(img)\n            img = cv2.imread(img)\n#             img = jpeg.JPEG(img).decode()[:, :, ::-1]\n        else:\n            img_info[\"file_name\"] = None\n\n        height, width = img.shape[:2]\n        img_info[\"height\"] = height\n        img_info[\"width\"] = width\n        img_info[\"raw_img\"] = img\n\n        ratio = min(self.test_size[0] / img.shape[0], self.test_size[1] / img.shape[1])\n        img_info[\"ratio\"] = ratio\n\n        img, _ = self.preproc(img, None, self.test_size)\n        img = torch.from_numpy(img).unsqueeze(0)\n        img = img.float()\n        if self.device == \"gpu\":\n            img = img.cuda()\n            if self.fp16:\n                img = img.half()  # to FP16\n\n        with torch.no_grad():\n            t0 = time.time()\n            outputs = self.model(img)\n            if self.decoder is not None:\n                outputs = self.decoder(outputs, dtype=outputs.type())\n            outputs = postprocess(\n                outputs, self.num_classes, self.confthre,\n                self.nmsthre, class_agnostic=True\n            )\n            logger.info(\"Infer time: {:.4f}s\".format(time.time() - t0))\n        return outputs, img_info\n\n    def visual(self, output, img_info, cls_conf=0.35):\n        ratio = img_info[\"ratio\"]\n        img = img_info[\"raw_img\"]\n        if output is None:\n            return img\n        output = output.cpu()\n\n        bboxes = output[:, 0:4]\n\n        # preprocessing: resize\n        bboxes /= ratio\n\n        cls = output[:, 6]\n        scores = output[:, 4] * output[:, 5]\n\n        vis_res = vis(img, bboxes, scores, cls, cls_conf, self.cls_names)\n        return vis_res\n\n    def crop_max_dog_or_cat(self, output, img_info):\n        ratio = img_info[\"ratio\"]\n        img = img_info[\"raw_img\"]\n        if output is None:\n            return img, None\n        output = output.cpu()\n\n        bboxes = output[:, 0:4]\n\n        # preprocessing: resize\n        bboxes /= ratio\n        bboxes = bboxes.numpy()\n\n        cls = output[:, 6]\n        cls = cls.numpy().astype(int)\n        cls_names = [self.cls_names[clsid] for clsid in cls]\n        scores = (output[:, 4] * output[:, 5]).numpy()\n\n        img_bbox_infos = dict()\n        img_bbox_infos[\"file_name\"] = img_info[\"file_name\"]\n        img_bbox_infos[\"height\"] = img_info[\"height\"]\n        img_bbox_infos[\"width\"] = img_info[\"width\"]\n        img_bbox_infos[\"cls_names\"] = cls_names\n        img_bbox_infos[\"scores\"] = scores\n        img_bbox_infos[\"cls_ids\"] = cls\n        img_bbox_infos[\"bboxes\"] = bboxes\n\n        dog_or_cat_idx = [i for i, x in enumerate(cls_names) if x in [\"dog\", \"cat\"]]\n        if len(dog_or_cat_idx) < 1:\n            return img, img_bbox_infos\n\n        dog_or_cat_scores = scores[dog_or_cat_idx]\n        dog_or_cat_bboxes = bboxes[dog_or_cat_idx]\n\n        max_score_idx = np.argmax(dog_or_cat_scores)\n        max_score_bbox = dog_or_cat_bboxes[max_score_idx]\n        x0 = max(0, int(max_score_bbox[0]))\n        y0 = max(0, int(max_score_bbox[1]))\n        x1 = min(int(max_score_bbox[2]), img_info[\"width\"])\n        y1 = min(int(max_score_bbox[3]), img_info[\"height\"])\n\n        res_img = img[y0:y1, x0:x1]\n\n        return res_img, img_bbox_infos","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:19.890269Z","iopub.execute_input":"2022-01-04T06:58:19.890736Z","iopub.status.idle":"2022-01-04T06:58:19.923849Z","shell.execute_reply.started":"2022-01-04T06:58:19.890702Z","shell.execute_reply":"2022-01-04T06:58:19.923013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nimport sys\n\ndef save_pickle(obj, file_path):\n    max_bytes = 2 ** 31 - 1\n    bytes_out = pickle.dumps(obj, protocol=pickle.HIGHEST_PROTOCOL)\n    n_bytes = sys.getsizeof(bytes_out)\n    with open(file_path, \"wb\") as f_out:\n        for idx in range(0, n_bytes, max_bytes):\n            f_out.write(bytes_out[idx : idx + max_bytes])\n\ndef load_pickle(file_path):\n    max_bytes = 2 ** 31 - 1\n    input_size = os.path.getsize(file_path)\n    bytes_in = bytearray(0)\n    with open(file_path, \"rb\") as f_in:\n        for _ in range(0, input_size, max_bytes):\n            bytes_in += f_in.read(max_bytes)\n    obj = pickle.loads(bytes_in)\n    return obj","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:19.926001Z","iopub.execute_input":"2022-01-04T06:58:19.926584Z","iopub.status.idle":"2022-01-04T06:58:19.936586Z","shell.execute_reply.started":"2022-01-04T06:58:19.926545Z","shell.execute_reply":"2022-01-04T06:58:19.935864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def image_crop(predictor, vis_folder, path, current_time, save_result):\n    if os.path.isdir(path):\n        files = get_image_list(path)\n    else:\n        files = [path]\n    files.sort()\n    for image_name in files:\n        outputs, img_info = predictor.inference(image_name)\n        result_image, bboxes_info = predictor.crop_max_dog_or_cat(outputs[0], img_info)\n        if save_result:\n            save_folder = os.path.join(\n                vis_folder, \"test_images\"\n            )\n            os.makedirs(save_folder, exist_ok=True)\n            save_file_name = os.path.join(save_folder, os.path.basename(image_name))\n            logger.info(\"Saving detection result in {}\".format(save_file_name))\n            cv2.imwrite(save_file_name, result_image)\n            if bboxes_info is not None:\n                save_pickle(bboxes_info, f\"{str(save_file_name)}.pkl\")\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:19.9384Z","iopub.execute_input":"2022-01-04T06:58:19.938925Z","iopub.status.idle":"2022-01-04T06:58:19.949033Z","shell.execute_reply.started":"2022-01-04T06:58:19.938871Z","shell.execute_reply":"2022-01-04T06:58:19.948156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pwd","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:19.952312Z","iopub.execute_input":"2022-01-04T06:58:19.95255Z","iopub.status.idle":"2022-01-04T06:58:20.080575Z","shell.execute_reply.started":"2022-01-04T06:58:19.952523Z","shell.execute_reply":"2022-01-04T06:58:20.079794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# args = make_parser().parse_args(args=[\"image_clop\", \"-n\", \"yolox-l\", \"-c\", \"weights/yolox_l.pth\", \"--path\", \"/kaggle/working/test_center_crop/\", \"--conf\", \"0.25\", \"--nms\", \"0.45\", \"--tsize\", \"640\", \"--save_result\", \"--device\", \"gpu\"])\nargs = make_parser().parse_args(args=[\"image_clop\", \"-n\", \"yolox-x\", \"-c\", \"weights/yolox_x.pth\", \"--path\", \"/kaggle/input/petfinder-pawpularity-score/test/\", \"--conf\", \"0.01\", \"--nms\", \"0.4\", \"--tsize\", \"640\", \"--save_result\", \"--device\", \"gpu\"])\n# args = make_parser().parse_args(args=[\"image_clop\", \"-n\", \"yolox-x\", \"-c\", \"weights/yolox_x.pth\", \"--path\", \"/kaggle/working/copy_train/\", \"--conf\", \"0.01\", \"--nms\", \"0.4\", \"--tsize\", \"640\", \"--save_result\", \"--device\", \"gpu\"])\nargs","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:20.082508Z","iopub.execute_input":"2022-01-04T06:58:20.082799Z","iopub.status.idle":"2022-01-04T06:58:20.092018Z","shell.execute_reply.started":"2022-01-04T06:58:20.082746Z","shell.execute_reply":"2022-01-04T06:58:20.091014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp = get_exp(args.exp_file, args.name)\nexp.seed = 1031\nexp.output_dir = \"/kaggle/working/\"\nexp.exp_name = \"yolox_x/test_images/\"\nexp","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:20.093713Z","iopub.execute_input":"2022-01-04T06:58:20.094334Z","iopub.status.idle":"2022-01-04T06:58:20.111094Z","shell.execute_reply.started":"2022-01-04T06:58:20.094274Z","shell.execute_reply":"2022-01-04T06:58:20.11029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not args.experiment_name:\n    args.experiment_name = exp.exp_name\n\nfile_name = os.path.join(exp.output_dir, args.experiment_name)\nos.makedirs(file_name, exist_ok=True)\n\nvis_folder = None\nif args.save_result:\n    vis_folder = os.path.join(file_name, \"vis_res\")\n    os.makedirs(vis_folder, exist_ok=True)\n\nif args.trt:\n    args.device = \"gpu\"\n\nlogger.info(\"Args: {}\".format(args))\n\nif args.conf is not None:\n    exp.test_conf = args.conf\nif args.nms is not None:\n    exp.nmsthre = args.nms\nif args.tsize is not None:\n    exp.test_size = (args.tsize, args.tsize)\n\nmodel = exp.get_model()\nlogger.info(\"Model Summary: {}\".format(get_model_info(model, exp.test_size)))\n\nif args.device == \"gpu\":\n    model.cuda()\n    if args.fp16:\n        model.half()  # to FP16\nmodel.eval()\n\nif not args.trt:\n    if args.ckpt is None:\n        ckpt_file = os.path.join(file_name, \"best_ckpt.pth\")\n    else:\n        ckpt_file = args.ckpt\n    logger.info(\"loading checkpoint\")\n    ckpt = torch.load(ckpt_file, map_location=\"cpu\")\n    # load the model state dict\n    model.load_state_dict(ckpt[\"model\"])\n    logger.info(\"loaded checkpoint done.\")\n\nif args.fuse:\n    logger.info(\"\\tFusing model...\")\n    model = fuse_model(model)\n\nif args.trt:\n    assert not args.fuse, \"TensorRT model is not support model fusing!\"\n    trt_file = os.path.join(file_name, \"model_trt.pth\")\n    assert os.path.exists(\n        trt_file\n    ), \"TensorRT model is not found!\\n Run python3 tools/trt.py first!\"\n    model.head.decode_in_inference = False\n    decoder = model.head.decode_outputs\n    logger.info(\"Using TensorRT to inference\")\nelse:\n    trt_file = None\n    decoder = None\n\npredictor = Predictor(\n    model, exp, COCO_CLASSES, trt_file, decoder,\n    args.device, args.fp16, args.legacy,\n)\ncurrent_time = time.localtime()\nimage_crop(predictor, vis_folder, args.path, current_time, args.save_result)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:20.112587Z","iopub.execute_input":"2022-01-04T06:58:20.112878Z","iopub.status.idle":"2022-01-04T06:58:30.38799Z","shell.execute_reply.started":"2022-01-04T06:58:20.112841Z","shell.execute_reply":"2022-01-04T06:58:30.387265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from glob import glob\n\nbbox_df = pd.DataFrame().from_dict(\n    [\n        load_pickle(filepath) for filepath in glob(f\"/kaggle/working/yolox_x/test_images/vis_res/test_images/*.jpg.pkl\")\n    ]\n)\nbbox_df[\"Id\"] = bbox_df[\"file_name\"].str.replace(\".jpg\", \"\")\nbbox_df[\"aspect_ratio\"] = bbox_df[\"height\"] / bbox_df[\"width\"]\nbbox_df[\"area\"] = bbox_df[\"height\"] * bbox_df[\"width\"]\nbbox_df[\"num_dog\"] = bbox_df[\"cls_names\"].astype(str).str.count(\"dog\")\nbbox_df[\"num_cat\"] = bbox_df[\"cls_names\"].astype(str).str.count(\"cat\")\nbbox_df[\"num_teddy_bear\"] = bbox_df[\"cls_names\"].astype(str).str.count(\"teddy bear\")\nbbox_df[\"num_person\"] = bbox_df[\"cls_names\"].astype(str).str.count(\"person\")\nbbox_df[\"num_dog_cat\"] = bbox_df[\"num_dog\"] + bbox_df[\"num_cat\"]\nbbox_df[\"num_dog_cat_teddy_bear\"] = bbox_df[\"num_dog\"] + bbox_df[\"num_cat\"] + bbox_df[\"num_teddy_bear\"]\nbbox_df","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:30.393642Z","iopub.execute_input":"2022-01-04T06:58:30.393853Z","iopub.status.idle":"2022-01-04T06:58:30.444963Z","shell.execute_reply.started":"2022-01-04T06:58:30.393826Z","shell.execute_reply":"2022-01-04T06:58:30.444265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\n\nbbox_df.to_csv(\"/kaggle/working/bbox_info.csv\", index=False)\ndel bbox_df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:30.446179Z","iopub.execute_input":"2022-01-04T06:58:30.446564Z","iopub.status.idle":"2022-01-04T06:58:30.591974Z","shell.execute_reply.started":"2022-01-04T06:58:30.44653Z","shell.execute_reply":"2022-01-04T06:58:30.591253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/yolox_x/test_images/vis_res/test_images/*.jpg.pkl","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:30.593239Z","iopub.execute_input":"2022-01-04T06:58:30.593647Z","iopub.status.idle":"2022-01-04T06:58:30.766771Z","shell.execute_reply.started":"2022-01-04T06:58:30.593611Z","shell.execute_reply":"2022-01-04T06:58:30.7658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:30.768575Z","iopub.execute_input":"2022-01-04T06:58:30.768909Z","iopub.status.idle":"2022-01-04T06:58:30.77519Z","shell.execute_reply.started":"2022-01-04T06:58:30.768862Z","shell.execute_reply":"2022-01-04T06:58:30.774486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nimport gc\nimport os\nimport sys\nimport warnings\nfrom pathlib import Path\nfrom pprint import pprint\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nimport torchvision.transforms as T\nfrom torchvision.io import ImageReadMode, read_image\nfrom pytorch_lightning import callbacks, seed_everything, LightningDataModule\nfrom torch.utils.data import DataLoader, Dataset\n\n!pip install ../input/omegaconf/omegaconf-2.0.5-py3-none-any.whl\nfrom omegaconf import OmegaConf\nsys.path.append('../input/timm-3monthsold/pytorch-image-models-master 2')\nimport timm\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:58:30.776496Z","iopub.execute_input":"2022-01-04T06:58:30.776919Z","iopub.status.idle":"2022-01-04T06:59:03.580652Z","shell.execute_reply.started":"2022-01-04T06:58:30.776884Z","shell.execute_reply":"2022-01-04T06:59:03.579754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n!pip install ../input/python-box/python_box-5.4.1-py3-none-any.whl\nfrom box import Box","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:59:03.582142Z","iopub.execute_input":"2022-01-04T06:59:03.582439Z","iopub.status.idle":"2022-01-04T06:59:30.372433Z","shell.execute_reply.started":"2022-01-04T06:59:03.582397Z","shell.execute_reply":"2022-01-04T06:59:30.371516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GBDT_EXP_NUM = \"065\"","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:59:30.373966Z","iopub.execute_input":"2022-01-04T06:59:30.374531Z","iopub.status.idle":"2022-01-04T06:59:30.381434Z","shell.execute_reply.started":"2022-01-04T06:59:30.374486Z","shell.execute_reply":"2022-01-04T06:59:30.380711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbdt_config = OmegaConf.load(\n    f\"../input/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}/output.json\"\n)\n# pprint(gbdt_config)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-04T06:59:30.383013Z","iopub.execute_input":"2022-01-04T06:59:30.383383Z","iopub.status.idle":"2022-01-04T06:59:33.159498Z","shell.execute_reply.started":"2022-01-04T06:59:30.383321Z","shell.execute_reply":"2022-01-04T06:59:33.158744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbdt_config[\"features\"]","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:59:33.164116Z","iopub.execute_input":"2022-01-04T06:59:33.164383Z","iopub.status.idle":"2022-01-04T06:59:33.176109Z","shell.execute_reply.started":"2022-01-04T06:59:33.164336Z","shell.execute_reply":"2022-01-04T06:59:33.175381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PAW_EXP_NUM = gbdt_config[\"features\"][\"paw_embed\"].replace(\"exp_\", \"\")\nBIN_PAW_EXP_NUM = gbdt_config[\"features\"][\"bin_paw\"].replace(\"exp_\", \"\")\nAGE_EXP_NUM = gbdt_config[\"features\"][\"age\"].replace(\"exp_\", \"\")\nBREED_EXP_NUM = gbdt_config[\"features\"][\"breed\"].replace(\"exp_\", \"\")\nADOPTION_SPEED_EXP_NUM = gbdt_config[\"features\"][\"adoption_speed\"].replace(\"exp_\", \"\")\nGENDER_EXP_NUM = gbdt_config[\"features\"][\"gender\"].replace(\"exp_\", \"\")\nMATURITY_SIZE_EXP_NUM = gbdt_config[\"features\"][\"maturity_size\"].replace(\"exp_\", \"\")\nBIN_SWIN_EXP_NUM = \"109\"\nCENTER_CROP_SWIN_EXP_NUM = \"101\"","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:59:33.177533Z","iopub.execute_input":"2022-01-04T06:59:33.177944Z","iopub.status.idle":"2022-01-04T06:59:33.185988Z","shell.execute_reply.started":"2022-01-04T06:59:33.177906Z","shell.execute_reply":"2022-01-04T06:59:33.185205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paw_config = OmegaConf.load(\n    f\"../input/petfinder-pawpularity-score-exp-{PAW_EXP_NUM}/exp_{PAW_EXP_NUM}.yaml\"\n)\npaw_config.val_loader.batch_size = 128\npprint(paw_config)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:59:33.187646Z","iopub.execute_input":"2022-01-04T06:59:33.187977Z","iopub.status.idle":"2022-01-04T06:59:33.220258Z","shell.execute_reply.started":"2022-01-04T06:59:33.187879Z","shell.execute_reply":"2022-01-04T06:59:33.219489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bin_paw_config = OmegaConf.load(\n    f\"../input/petfinder-pawpularity-score-exp-{BIN_PAW_EXP_NUM}/exp_{BIN_PAW_EXP_NUM}.yaml\"\n)\nbin_paw_config.val_loader.batch_size = 128\npprint(bin_paw_config)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:59:33.221521Z","iopub.execute_input":"2022-01-04T06:59:33.221744Z","iopub.status.idle":"2022-01-04T06:59:33.253222Z","shell.execute_reply.started":"2022-01-04T06:59:33.221714Z","shell.execute_reply":"2022-01-04T06:59:33.252313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"age_config = OmegaConf.load(\n    f\"../input/petfinder-pawpularity-score-exp-{AGE_EXP_NUM}/exp_{AGE_EXP_NUM}.yaml\"\n)\nage_config.val_loader.batch_size = 128\npprint(age_config)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:59:33.254529Z","iopub.execute_input":"2022-01-04T06:59:33.254847Z","iopub.status.idle":"2022-01-04T06:59:33.284548Z","shell.execute_reply.started":"2022-01-04T06:59:33.254812Z","shell.execute_reply":"2022-01-04T06:59:33.283831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"breed_config = OmegaConf.load(\n    f\"../input/petfinder-pawpularity-score-exp-{BREED_EXP_NUM}/exp_{BREED_EXP_NUM}.yaml\"\n)\nbreed_config.val_loader.batch_size = 128\npprint(breed_config)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:59:33.285892Z","iopub.execute_input":"2022-01-04T06:59:33.286085Z","iopub.status.idle":"2022-01-04T06:59:33.314311Z","shell.execute_reply.started":"2022-01-04T06:59:33.286061Z","shell.execute_reply":"2022-01-04T06:59:33.313647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adoption_speed_config = OmegaConf.load(\n    f\"../input/petfinder-pawpularity-score-exp-{ADOPTION_SPEED_EXP_NUM}/exp_{ADOPTION_SPEED_EXP_NUM}.yaml\"\n)\nadoption_speed_config.val_loader.batch_size = 128\npprint(adoption_speed_config)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:59:33.315466Z","iopub.execute_input":"2022-01-04T06:59:33.315695Z","iopub.status.idle":"2022-01-04T06:59:33.345183Z","shell.execute_reply.started":"2022-01-04T06:59:33.315661Z","shell.execute_reply":"2022-01-04T06:59:33.344519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gender_config = OmegaConf.load(\n    f\"../input/petfinder-pawpularity-score-exp-{GENDER_EXP_NUM}/exp_{GENDER_EXP_NUM}.yaml\"\n)\ngender_config.val_loader.batch_size = 128\npprint(gender_config)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:59:33.346282Z","iopub.execute_input":"2022-01-04T06:59:33.346537Z","iopub.status.idle":"2022-01-04T06:59:33.376376Z","shell.execute_reply.started":"2022-01-04T06:59:33.346504Z","shell.execute_reply":"2022-01-04T06:59:33.375673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maturity_size_config = OmegaConf.load(\n    f\"../input/petfinder-pawpularity-score-exp-{MATURITY_SIZE_EXP_NUM}/exp_{MATURITY_SIZE_EXP_NUM}.yaml\"\n)\nmaturity_size_config.val_loader.batch_size = 128\npprint(maturity_size_config)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:59:33.37753Z","iopub.execute_input":"2022-01-04T06:59:33.377942Z","iopub.status.idle":"2022-01-04T06:59:33.406747Z","shell.execute_reply.started":"2022-01-04T06:59:33.377905Z","shell.execute_reply":"2022-01-04T06:59:33.40607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bin_swin_config = OmegaConf.load(\n    f\"../input/petfinder-pawpularity-score-exp-{BIN_SWIN_EXP_NUM}/exp_{BIN_SWIN_EXP_NUM}.yaml\"\n)\nbin_swin_config.val_loader.batch_size = 128\npprint(bin_swin_config)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:59:33.408037Z","iopub.execute_input":"2022-01-04T06:59:33.40829Z","iopub.status.idle":"2022-01-04T06:59:33.438829Z","shell.execute_reply.started":"2022-01-04T06:59:33.408257Z","shell.execute_reply":"2022-01-04T06:59:33.438135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"center_crop_swin_config = OmegaConf.load(\n    f\"../input/petfinder-pawpularity-score-exp-{CENTER_CROP_SWIN_EXP_NUM}/exp_{CENTER_CROP_SWIN_EXP_NUM}.yaml\"\n)\ncenter_crop_swin_config.val_loader.batch_size = 128\npprint(center_crop_swin_config)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:59:33.440162Z","iopub.execute_input":"2022-01-04T06:59:33.440435Z","iopub.status.idle":"2022-01-04T06:59:33.47014Z","shell.execute_reply.started":"2022-01-04T06:59:33.440397Z","shell.execute_reply":"2022-01-04T06:59:33.469402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(paw_config.seed)\ntorch.autograd.set_detect_anomaly(True)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:59:33.471399Z","iopub.execute_input":"2022-01-04T06:59:33.471633Z","iopub.status.idle":"2022-01-04T06:59:35.073179Z","shell.execute_reply.started":"2022-01-04T06:59:33.471601Z","shell.execute_reply":"2022-01-04T06:59:35.072403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(Path(\"../input/petfinder-pawpularity-score\")/ \"test.csv\")\n# test_df = pd.concat([test_df]*900, axis=\"index\").reset_index(drop=True)\ntest_df[\"org_Id\"] = test_df[\"Id\"].copy()\ntest_df[\"filepath\"] = (\"/kaggle/working/yolox_x/test_images/vis_res/test_images/\" + test_df[\"Id\"] + \".jpg\")\ntest_df[\"Id\"] = (\"/kaggle/working/yolox_x/test_images/vis_res/test_images/\" + test_df[\"Id\"] + \".jpg\")\ntest_df[\"center_Id\"] = (\"/kaggle/working/test_center_crop/\" + test_df[\"org_Id\"] + \".jpg\")\ndisplay(test_df.head())","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:59:35.074909Z","iopub.execute_input":"2022-01-04T06:59:35.075182Z","iopub.status.idle":"2022-01-04T06:59:35.099964Z","shell.execute_reply.started":"2022-01-04T06:59:35.075145Z","shell.execute_reply":"2022-01-04T06:59:35.099224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:59:35.101167Z","iopub.execute_input":"2022-01-04T06:59:35.101443Z","iopub.status.idle":"2022-01-04T06:59:35.107943Z","shell.execute_reply.started":"2022-01-04T06:59:35.101407Z","shell.execute_reply":"2022-01-04T06:59:35.107236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pawpularity\nclass PetfinderDataset(Dataset):\n    def __init__(self, df, image_size=224, output_dim=1, over_100_or_not=False):\n        self._X = df[\"Id\"].values\n        self._y = None\n        if \"Pawpularity\" in df.keys():\n            pawpularity = df[\"Pawpularity\"].values\n\n            if output_dim == 100:\n                self._y = np.zeros((len(df), output_dim)).astype(np.float32)\n                for i in range(len(df)):\n                    self._y[i, : pawpularity[i]] = 1\n            else:\n                if over_100_or_not:\n                    self._y = (pawpularity == 100).astype(np.float32)\n                else:\n                    self._y = pawpularity\n\n        self._transform = T.Resize([image_size, image_size])\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._X[idx]\n        image = read_image(image_path)\n        image = self._transform(image)\n        if self._y is not None:\n            label = self._y[idx]\n            return image, label\n        return image, 0\n\nclass PetfinderInferenceDataModule(LightningDataModule):\n    def __init__(\n        self,\n        test_df,\n        cfg,\n    ):\n        super().__init__()\n        self._test_df = test_df\n        self._cfg = cfg\n\n    def predict_dataloader(self):\n        dataset = PetfinderDataset(\n            self._test_df,\n            self._cfg.transform.image_size,\n            self._cfg.model.output_dim,\n            self._cfg.model.over_100_or_not,\n        )\n        return DataLoader(dataset, **self._cfg.val_loader)\n    \n# centercrop Pawpularity\nclass PetfinderCenterDataset(Dataset):\n    def __init__(self, df, image_size=224, output_dim=1, over_100_or_not=False):\n        self._X = df[\"center_Id\"].values\n        self._y = None\n        if \"Pawpularity\" in df.keys():\n            pawpularity = df[\"Pawpularity\"].values\n\n            if output_dim == 100:\n                self._y = np.zeros((len(df), output_dim)).astype(np.float32)\n                for i in range(len(df)):\n                    self._y[i, : pawpularity[i]] = 1\n            else:\n                if over_100_or_not:\n                    self._y = (pawpularity == 100).astype(np.float32)\n                else:\n                    self._y = pawpularity\n\n        self._transform = T.Resize([image_size, image_size])\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._X[idx]\n        image = read_image(image_path)\n        image = self._transform(image)\n        if self._y is not None:\n            label = self._y[idx]\n            return image, label\n        return image, 0\n\nclass PetfinderCenterInferenceDataModule(LightningDataModule):\n    def __init__(\n        self,\n        test_df,\n        cfg,\n    ):\n        super().__init__()\n        self._test_df = test_df\n        self._cfg = cfg\n\n    def predict_dataloader(self):\n        dataset = PetfinderCenterDataset(\n            self._test_df,\n            self._cfg.transform.image_size,\n            self._cfg.model.output_dim,\n            self._cfg.model.over_100_or_not,\n        )\n        return DataLoader(dataset, **self._cfg.val_loader)\n\n\n# Age\nclass PetfinderAgeDataset(Dataset):\n    def __init__(self, df, image_size=224, output_dim=1):\n        self._X = df[\"filepath\"].values\n        self._y = None\n        if \"Age\" in df.keys():\n            self._y = df[\"Age\"].clip(0, 100).values\n\n        self._transform = T.Resize([image_size, image_size])\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._X[idx]\n        image = read_image(image_path, mode=ImageReadMode.RGB)\n        image = self._transform(image)\n        if self._y is not None:\n            label = self._y[idx]\n            return image, label\n        return image, 0\n\nclass PetfinderAgeInferenceDataModule(LightningDataModule):\n    def __init__(\n        self,\n        test_df,\n        cfg,\n    ):\n        super().__init__()\n        self._test_df = test_df\n        self._cfg = cfg\n\n    def predict_dataloader(self):\n        dataset = PetfinderAgeDataset(\n            self._test_df,\n            self._cfg.transform.image_size,\n            self._cfg.model.output_dim,\n        )\n        return DataLoader(dataset, **self._cfg.val_loader)\n\n# Breed\n\nclass PetfinderBreedDataset(Dataset):\n    def __init__(self, df, image_size=224, output_dim=100):\n        self._X = df[\"filepath\"].values\n        self._y = None\n        if \"Breed1\" in df.keys():\n            self._y = np.identity(output_dim)[df[\"Breed1\"].values]\n\n        self._transform = T.Resize([image_size, image_size])\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._X[idx]\n        image = read_image(image_path, mode=ImageReadMode.RGB)\n        image = self._transform(image)\n        if self._y is not None:\n            label = self._y[idx]\n            return image, label\n        return image, 0\n\nclass PetfinderBreedInferenceDataModule(LightningDataModule):\n    def __init__(\n        self,\n        test_df,\n        cfg,\n    ):\n        super().__init__()\n        self._test_df = test_df\n        self._cfg = cfg\n\n    def predict_dataloader(self):\n        dataset = PetfinderBreedDataset(\n            self._test_df,\n            self._cfg.transform.image_size,\n            self._cfg.model.output_dim,\n        )\n        return DataLoader(dataset, **self._cfg.val_loader)\n\n\n# Gender\nclass PetfinderGenderDataset(Dataset):\n    def __init__(self, df, image_size=224, output_dim=3):\n        self._X = df[\"filepath\"].values\n        self._y = None\n        if \"Gender\" in df.keys():\n            self._y = np.identity(output_dim)[df[\"Gender\"].values]\n\n        self._transform = T.Resize([image_size, image_size])\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._X[idx]\n        image = read_image(image_path, mode=ImageReadMode.RGB)\n        image = self._transform(image)\n        if self._y is not None:\n            label = self._y[idx]\n            return image, label\n        return image, 0\n\n\nclass PetfinderGenderInferenceDataModule(LightningDataModule):\n    def __init__(\n        self,\n        test_df,\n        cfg,\n    ):\n        super().__init__()\n        self._test_df = test_df\n        self._cfg = cfg\n\n    def predict_dataloader(self):\n        dataset = PetfinderGenderDataset(self._test_df, self._cfg.transform.image_size)\n        return DataLoader(dataset, **self._cfg.val_loader)\n\n    \n# Adoption Speed\nclass PetfinderAdoptionSpeedDataset(Dataset):\n    def __init__(self, df, image_size=224):\n        self._X = df[\"filepath\"].values\n        self._y = None\n        if \"AdoptionSpeed\" in df.keys():\n            adoption_speed = df[\"AdoptionSpeed\"].values\n\n            self._y = np.zeros((len(df), 4)).astype(np.float32)\n            for i in range(len(df)):\n                self._y[i, : adoption_speed[i]] = 1\n\n        self._transform = T.Resize([image_size, image_size])\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._X[idx]\n        image = read_image(image_path)\n        image = self._transform(image)\n        if self._y is not None:\n            label = self._y[idx]\n            return image, label\n        return image, 0\n\n\nclass PetfinderAdoptionSpeedInferenceDataModule(LightningDataModule):\n    def __init__(\n        self,\n        test_df,\n        cfg,\n    ):\n        super().__init__()\n        self._test_df = test_df\n        self._cfg = cfg\n\n    def predict_dataloader(self):\n        dataset = PetfinderAdoptionSpeedDataset(\n            self._test_df,\n            self._cfg.transform.image_size,\n        )\n        return DataLoader(dataset, **self._cfg.val_loader)\n\n# bin paw\nclass PetfinderBinPawpularityDataset(Dataset):\n    def __init__(self, df, image_size=224, num_bins=14):\n        self._X = df[\"filepath\"].values\n        self._y = None\n        if \"bins_paw\" in df.keys():\n            adoption_speed = df[\"bins_paw\"].values\n\n            self._y = np.zeros((len(df), num_bins)).astype(np.float32)\n            for i in range(len(df)):\n                self._y[i, : adoption_speed[i]] = 1\n\n        self._transform = T.Resize([image_size, image_size])\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._X[idx]\n        image = read_image(image_path)\n        image = self._transform(image)\n        if self._y is not None:\n            label = self._y[idx]\n            return image, label\n        return image, 0\n\nclass PetfinderBinPawpularityInferenceDataModule(LightningDataModule):\n    def __init__(\n        self,\n        test_df,\n        cfg,\n    ):\n        super().__init__()\n        self._test_df = test_df\n        self._cfg = cfg\n\n    def predict_dataloader(self):\n        dataset = PetfinderBinPawpularityDataset(\n            self._test_df,\n            self._cfg.transform.image_size,\n        )\n        return DataLoader(dataset, **self._cfg.val_loader)\n    \n    \n# Maturity Size\nclass PetfinderMaturitySizeDataset(Dataset):\n    def __init__(self, df, image_size=224):\n        self._X = df[\"filepath\"].values\n        self._y = None\n        if \"MaturitySize\" in df.keys():\n            maturity_size = df[\"MaturitySize\"].values\n\n            self._y = np.zeros((len(df), 3)).astype(np.float32)\n            for i in range(len(df)):\n                self._y[i, : maturity_size[i]] = 1\n\n        self._transform = T.Resize([image_size, image_size])\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._X[idx]\n        image = read_image(image_path)\n        image = self._transform(image)\n        if self._y is not None:\n            label = self._y[idx]\n            return image, label\n        return image, 0\n\nclass PetfinderMaturitySizeInferenceDataModule(LightningDataModule):\n    def __init__(\n        self,\n        test_df,\n        cfg,\n    ):\n        super().__init__()\n        self._test_df = test_df\n        self._cfg = cfg\n\n    def predict_dataloader(self):\n        dataset = PetfinderMaturitySizeDataset(\n            self._test_df,\n            self._cfg.transform.image_size,\n        )\n        return DataLoader(dataset, **self._cfg.val_loader)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:59:35.109407Z","iopub.execute_input":"2022-01-04T06:59:35.110104Z","iopub.status.idle":"2022-01-04T06:59:35.165328Z","shell.execute_reply.started":"2022-01-04T06:59:35.110065Z","shell.execute_reply":"2022-01-04T06:59:35.164315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.logits = logits\n        self.reduce = reduce\n\n    def forward(self, inputs, targets):\n        if self.logits:\n            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n        else:\n            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n\n        if self.reduce:\n            return torch.mean(F_loss)\n        else:\n            return F_loss\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:59:35.166501Z","iopub.execute_input":"2022-01-04T06:59:35.167209Z","iopub.status.idle":"2022-01-04T06:59:35.178121Z","shell.execute_reply.started":"2022-01-04T06:59:35.167171Z","shell.execute_reply":"2022-01-04T06:59:35.177402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMAGENET_MEAN = [0.485, 0.456, 0.406]  # RGB\nIMAGENET_STD = [0.229, 0.224, 0.225]  # RGB\n\ndef get_default_transforms():\n    transform = {\n        \"train\": T.Compose(\n            [\n                T.RandomHorizontalFlip(),\n                T.RandomVerticalFlip(),\n                T.RandomAffine(15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n                T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n                T.ConvertImageDtype(torch.float),\n                T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n            ]\n        ),\n        \"val\": T.Compose(\n            [\n                T.ConvertImageDtype(torch.float),\n                T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n            ]\n        ),\n    }\n    return transform\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:59:35.181226Z","iopub.execute_input":"2022-01-04T06:59:35.181701Z","iopub.status.idle":"2022-01-04T06:59:35.190611Z","shell.execute_reply.started":"2022-01-04T06:59:35.181667Z","shell.execute_reply":"2022-01-04T06:59:35.18987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1.0 - lam)\n    cut_w = np.int(W * cut_rat)\n    cut_h = np.int(H * cut_rat)\n\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\n\ndef rand_region(target_size, source_size):\n    t_h, t_w = target_size[2:]\n    s_h, s_w = source_size[2:]\n    cut_h = s_h // 2\n    cut_w = s_w // 2\n\n    cx = np.random.randint(cut_w, t_w - cut_w)\n    cy = np.random.randint(cut_h, t_h - cut_h)\n    x1 = cx - cut_w\n    x2 = x1 + s_w\n    y1 = cy - cut_h\n    y2 = y1 + s_h\n    return x1, y1, x2, y2\n\nimport numpy as np\nimport torch\nfrom torchvision.transforms import Resize\n\ndef cutmix(x, y, alpha):\n    assert alpha > 0, \"alpha should be larger than 0\"\n    lam = np.random.beta(alpha, alpha)\n    rand_index = torch.randperm(x.size()[0]).cuda()\n    target_a = y\n    target_b = y[rand_index]\n    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n    x[:, :, bbx1:bbx2, bby1:bby2] = x[rand_index, :, bbx1:bbx2, bby1:bby2]\n    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))\n    return x, target_a, target_b, lam\n\n\ndef mixup(x, y, alpha):\n    assert alpha > 0, \"alpha should be larger than 0\"\n\n    lam = np.random.beta(alpha, alpha)\n    rand_index = torch.randperm(x.size()[0]).cuda()\n    mixed_x = lam * x + (1 - lam) * x[rand_index, :]\n    target_a, target_b = y, y[rand_index]\n    return mixed_x, target_a, target_b, lam\n\n\ndef resizemix(x, y, alpha=0.1, beta=0.8):\n    assert alpha > 0, \"alpha should be larger than 0\"\n    assert beta < 1, \"beta should be smaller than 1\"\n\n    rand_index = torch.randperm(x.size()[0]).cuda()\n    tau = np.random.uniform(alpha, beta)\n    lam = tau ** 2\n\n    H, W = x.size()[2:]\n    resize_transform = Resize((int(H * tau), int(W * tau)))\n    resized_x = resize_transform(x[rand_index])\n\n    target_a = y[rand_index]\n    target_b = y\n    x1, y1, x2, y2 = rand_region(x.size(), resized_x.size())\n    x[:, :, y1:y2, x1:x2] = resized_x\n    return x, target_a, target_b, lam\n\n\ndef get_strong_transforms(cfg):\n    return eval(cfg.strong_transform.name)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:59:35.192505Z","iopub.execute_input":"2022-01-04T06:59:35.193533Z","iopub.status.idle":"2022-01-04T06:59:35.21267Z","shell.execute_reply.started":"2022-01-04T06:59:35.193493Z","shell.execute_reply":"2022-01-04T06:59:35.211829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Copyright 2021 Sea Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nVision OutLOoker (VOLO) implementation\n\"\"\"\n\n# https://github.com/sail-sg/volo/blob/main/models/volo.py\n\nimport math\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.models.helpers import load_state_dict\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\nfrom timm.models.registry import register_model\n\n\ndef _cfg(url=\"\", **kwargs):\n    return {\n        \"url\": url,\n        \"num_classes\": 1000,\n        \"input_size\": (3, 224, 224),\n        \"pool_size\": None,\n        \"crop_pct\": 0.96,\n        \"interpolation\": \"bicubic\",\n        \"mean\": IMAGENET_DEFAULT_MEAN,\n        \"std\": IMAGENET_DEFAULT_STD,\n        \"first_conv\": \"patch_embed.proj\",\n        \"classifier\": \"head\",\n        **kwargs,\n    }\n\n\ndefault_cfgs = {\n    \"volo\": _cfg(crop_pct=0.96),\n    \"volo_large\": _cfg(crop_pct=1.15),\n}\n\n\nclass OutlookAttention(nn.Module):\n    \"\"\"\n    Implementation of outlook attention\n    --dim: hidden dim\n    --num_heads: number of heads\n    --kernel_size: kernel size in each window for outlook attention\n    return: token features after outlook attention\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        kernel_size=3,\n        padding=1,\n        stride=1,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n        super().__init__()\n        head_dim = dim // num_heads\n        self.num_heads = num_heads\n        self.kernel_size = kernel_size\n        self.padding = padding\n        self.stride = stride\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n        self.attn = nn.Linear(dim, kernel_size ** 4 * num_heads)\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        self.unfold = nn.Unfold(kernel_size=kernel_size, padding=padding, stride=stride)\n        self.pool = nn.AvgPool2d(kernel_size=stride, stride=stride, ceil_mode=True)\n\n    def forward(self, x):\n        B, H, W, C = x.shape\n\n        v = self.v(x).permute(0, 3, 1, 2)  # B, C, H, W\n\n        h, w = math.ceil(H / self.stride), math.ceil(W / self.stride)\n        v = (\n            self.unfold(v)\n            .reshape(\n                B,\n                self.num_heads,\n                C // self.num_heads,\n                self.kernel_size * self.kernel_size,\n                h * w,\n            )\n            .permute(0, 1, 4, 3, 2)\n        )  # B,H,N,kxk,C/H\n\n        attn = self.pool(x.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)\n        attn = (\n            self.attn(attn)\n            .reshape(\n                B,\n                h * w,\n                self.num_heads,\n                self.kernel_size * self.kernel_size,\n                self.kernel_size * self.kernel_size,\n            )\n            .permute(0, 2, 1, 3, 4)\n        )  # B,H,N,kxk,kxk\n        attn = attn * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (\n            (attn @ v)\n            .permute(0, 1, 4, 3, 2)\n            .reshape(B, C * self.kernel_size * self.kernel_size, h * w)\n        )\n        x = F.fold(\n            x,\n            output_size=(H, W),\n            kernel_size=self.kernel_size,\n            padding=self.padding,\n            stride=self.stride,\n        )\n\n        x = self.proj(x.permute(0, 2, 3, 1))\n        x = self.proj_drop(x)\n\n        return x\n\n\nclass Outlooker(nn.Module):\n    \"\"\"\n    Implementation of outlooker layer: which includes outlook attention + MLP\n    Outlooker is the first stage in our VOLO\n    --dim: hidden dim\n    --num_heads: number of heads\n    --mlp_ratio: mlp ratio\n    --kernel_size: kernel size in each window for outlook attention\n    return: outlooker layer\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        kernel_size,\n        padding,\n        stride=1,\n        num_heads=1,\n        mlp_ratio=3.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        qkv_bias=False,\n        qk_scale=None,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = OutlookAttention(\n            dim,\n            num_heads,\n            kernel_size=kernel_size,\n            padding=padding,\n            stride=stride,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop=attn_drop,\n        )\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer\n        )\n\n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n\nclass Mlp(nn.Module):\n    \"Implementation of MLP\"\n\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    \"Implementation of self-attention\"\n\n    def __init__(\n        self,\n        dim,\n        num_heads=8,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, H, W, C = x.shape\n\n        qkv = (\n            self.qkv(x)\n            .reshape(B, H * W, 3, self.num_heads, C // self.num_heads)\n            .permute(2, 0, 3, 1, 4)\n        )\n        q, k, v = (\n            qkv[0],\n            qkv[1],\n            qkv[2],\n        )  # make torchscript happy (cannot use tensor as tuple)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, H, W, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n\n        return x\n\n\nclass Transformer(nn.Module):\n    \"\"\"\n    Implementation of Transformer,\n    Transformer is the second stage in our VOLO\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop=attn_drop,\n        )\n\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer\n        )\n\n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n\nclass ClassAttention(nn.Module):\n    \"\"\"\n    Class attention layer from CaiT, see details in CaiT\n    Class attention is the post stage in our VOLO, which is optional.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        num_heads=8,\n        head_dim=None,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        if head_dim is not None:\n            self.head_dim = head_dim\n        else:\n            head_dim = dim // num_heads\n            self.head_dim = head_dim\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.kv = nn.Linear(dim, self.head_dim * self.num_heads * 2, bias=qkv_bias)\n        self.q = nn.Linear(dim, self.head_dim * self.num_heads, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(self.head_dim * self.num_heads, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n\n        kv = (\n            self.kv(x)\n            .reshape(B, N, 2, self.num_heads, self.head_dim)\n            .permute(2, 0, 3, 1, 4)\n        )\n        k, v = kv[0], kv[1]  # make torchscript happy (cannot use tensor as tuple)\n        q = self.q(x[:, :1, :]).reshape(B, self.num_heads, 1, self.head_dim)\n        attn = (q * self.scale) @ k.transpose(-2, -1)\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        cls_embed = (\n            (attn @ v).transpose(1, 2).reshape(B, 1, self.head_dim * self.num_heads)\n        )\n        cls_embed = self.proj(cls_embed)\n        cls_embed = self.proj_drop(cls_embed)\n        return cls_embed\n\n\nclass ClassBlock(nn.Module):\n    \"\"\"\n    Class attention block from CaiT, see details in CaiT\n    We use two-layers class attention in our VOLO, which is optional.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        head_dim=None,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = ClassAttention(\n            dim,\n            num_heads=num_heads,\n            head_dim=head_dim,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop=attn_drop,\n            proj_drop=drop,\n        )\n        # NOTE: drop path for stochastic depth\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop,\n        )\n\n    def forward(self, x):\n        cls_embed = x[:, :1]\n        cls_embed = cls_embed + self.drop_path(self.attn(self.norm1(x)))\n        cls_embed = cls_embed + self.drop_path(self.mlp(self.norm2(cls_embed)))\n        return torch.cat([cls_embed, x[:, 1:]], dim=1)\n\n\ndef get_block(block_type, **kargs):\n    \"\"\"\n    get block by name, specifically for class attention block in here\n    \"\"\"\n    if block_type == \"ca\":\n        return ClassBlock(**kargs)\n\n\ndef rand_bbox(size, lam, scale=1):\n    \"\"\"\n    get bounding box as token labeling (https://github.com/zihangJiang/TokenLabeling)\n    return: bounding box\n    \"\"\"\n    W = size[1] // scale\n    H = size[2] // scale\n    cut_rat = np.sqrt(1.0 - lam)\n    cut_w = np.int(W * cut_rat)\n    cut_h = np.int(H * cut_rat)\n\n    # uniform\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"\n    Image to Patch Embedding.\n    Different with ViT use 1 conv layer, we use 4 conv layers to do patch embedding\n    \"\"\"\n\n    def __init__(\n        self,\n        img_size=224,\n        stem_conv=False,\n        stem_stride=1,\n        patch_size=8,\n        in_chans=3,\n        hidden_dim=64,\n        embed_dim=384,\n    ):\n        super().__init__()\n        assert patch_size in [4, 8, 16]\n\n        self.stem_conv = stem_conv\n        if stem_conv:\n            self.conv = nn.Sequential(\n                nn.Conv2d(\n                    in_chans,\n                    hidden_dim,\n                    kernel_size=7,\n                    stride=stem_stride,\n                    padding=3,\n                    bias=False,\n                ),  # 112x112\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(\n                    hidden_dim,\n                    hidden_dim,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1,\n                    bias=False,\n                ),  # 112x112\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(\n                    hidden_dim,\n                    hidden_dim,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1,\n                    bias=False,\n                ),  # 112x112\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU(inplace=True),\n            )\n\n        self.proj = nn.Conv2d(\n            hidden_dim,\n            embed_dim,\n            kernel_size=patch_size // stem_stride,\n            stride=patch_size // stem_stride,\n        )\n        self.num_patches = (img_size // patch_size) * (img_size // patch_size)\n\n    def forward(self, x):\n        if self.stem_conv:\n            x = self.conv(x)\n        x = self.proj(x)  # B, C, H, W\n        return x\n\n\nclass Downsample(nn.Module):\n    \"\"\"\n    Image to Patch Embedding, downsampling between stage1 and stage2\n    \"\"\"\n\n    def __init__(self, in_embed_dim, out_embed_dim, patch_size):\n        super().__init__()\n        self.proj = nn.Conv2d(\n            in_embed_dim, out_embed_dim, kernel_size=patch_size, stride=patch_size\n        )\n\n    def forward(self, x):\n        x = x.permute(0, 3, 1, 2)\n        x = self.proj(x)  # B, C, H, W\n        x = x.permute(0, 2, 3, 1)\n        return x\n\n\ndef outlooker_blocks(\n    block_fn,\n    index,\n    dim,\n    layers,\n    num_heads=1,\n    kernel_size=3,\n    padding=1,\n    stride=1,\n    mlp_ratio=3.0,\n    qkv_bias=False,\n    qk_scale=None,\n    attn_drop=0,\n    drop_path_rate=0.0,\n    **kwargs,\n):\n    \"\"\"\n    generate outlooker layer in stage1\n    return: outlooker layers\n    \"\"\"\n    blocks = []\n    for block_idx in range(layers[index]):\n        block_dpr = (\n            drop_path_rate * (block_idx + sum(layers[:index])) / (sum(layers) - 1)\n        )\n        blocks.append(\n            block_fn(\n                dim,\n                kernel_size=kernel_size,\n                padding=padding,\n                stride=stride,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                attn_drop=attn_drop,\n                drop_path=block_dpr,\n            )\n        )\n\n    blocks = nn.Sequential(*blocks)\n\n    return blocks\n\n\ndef transformer_blocks(\n    block_fn,\n    index,\n    dim,\n    layers,\n    num_heads,\n    mlp_ratio=3.0,\n    qkv_bias=False,\n    qk_scale=None,\n    attn_drop=0,\n    drop_path_rate=0.0,\n    **kwargs,\n):\n    \"\"\"\n    generate transformer layers in stage2\n    return: transformer layers\n    \"\"\"\n    blocks = []\n    for block_idx in range(layers[index]):\n        block_dpr = (\n            drop_path_rate * (block_idx + sum(layers[:index])) / (sum(layers) - 1)\n        )\n        blocks.append(\n            block_fn(\n                dim,\n                num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                attn_drop=attn_drop,\n                drop_path=block_dpr,\n            )\n        )\n\n    blocks = nn.Sequential(*blocks)\n\n    return blocks\n\n\nclass VOLO(nn.Module):\n    \"\"\"\n    Vision Outlooker, the main class of our model\n    --layers: [x,x,x,x], four blocks in two stages, the first block is outlooker, the\n              other three are transformer, we set four blocks, which are easily\n              applied to downstream tasks\n    --img_size, --in_chans, --num_classes: these three are very easy to understand\n    --patch_size: patch_size in outlook attention\n    --stem_hidden_dim: hidden dim of patch embedding, d1-d4 is 64, d5 is 128\n    --embed_dims, --num_heads: embedding dim, number of heads in each block\n    --downsamples: flags to apply downsampling or not\n    --outlook_attention: flags to apply outlook attention or not\n    --mlp_ratios, --qkv_bias, --qk_scale, --drop_rate: easy to undertand\n    --attn_drop_rate, --drop_path_rate, --norm_layer: easy to undertand\n    --post_layers: post layers like two class attention layers using [ca, ca],\n                  if yes, return_mean=False\n    --return_mean: use mean of all feature tokens for classification, if yes, no class token\n    --return_dense: use token labeling, details are here:\n                    https://github.com/zihangJiang/TokenLabeling\n    --mix_token: mixing tokens as token labeling, details are here:\n                    https://github.com/zihangJiang/TokenLabeling\n    --pooling_scale: pooling_scale=2 means we downsample 2x\n    --out_kernel, --out_stride, --out_padding: kerner size,\n                                               stride, and padding for outlook attention\n    \"\"\"\n\n    def __init__(\n        self,\n        layers,\n        img_size=224,\n        in_chans=3,\n        num_classes=1000,\n        patch_size=8,\n        stem_hidden_dim=64,\n        embed_dims=None,\n        num_heads=None,\n        downsamples=None,\n        outlook_attention=None,\n        mlp_ratios=None,\n        qkv_bias=False,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        norm_layer=nn.LayerNorm,\n        post_layers=None,\n        return_mean=False,\n        return_dense=True,\n        mix_token=False,\n        pooling_scale=2,\n        out_kernel=3,\n        out_stride=2,\n        out_padding=1,\n    ):\n\n        super().__init__()\n        self.num_classes = num_classes\n        self.num_features = embed_dims[-1]\n        self.patch_embed = PatchEmbed(\n            stem_conv=True,\n            stem_stride=2,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            hidden_dim=stem_hidden_dim,\n            embed_dim=embed_dims[0],\n        )\n\n        # inital positional encoding, we add positional encoding after outlooker blocks\n        self.pos_embed = nn.Parameter(\n            torch.zeros(\n                1,\n                img_size // patch_size // pooling_scale,\n                img_size // patch_size // pooling_scale,\n                embed_dims[-1],\n            )\n        )\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # set the main block in network\n        network = []\n        for i in range(len(layers)):\n            if outlook_attention[i]:\n                # stage 1\n                stage = outlooker_blocks(\n                    Outlooker,\n                    i,\n                    embed_dims[i],\n                    layers,\n                    downsample=downsamples[i],\n                    num_heads=num_heads[i],\n                    kernel_size=out_kernel,\n                    stride=out_stride,\n                    padding=out_padding,\n                    mlp_ratio=mlp_ratios[i],\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    attn_drop=attn_drop_rate,\n                    norm_layer=norm_layer,\n                )\n                network.append(stage)\n            else:\n                # stage 2\n                stage = transformer_blocks(\n                    Transformer,\n                    i,\n                    embed_dims[i],\n                    layers,\n                    num_heads[i],\n                    mlp_ratio=mlp_ratios[i],\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop_path_rate=drop_path_rate,\n                    attn_drop=attn_drop_rate,\n                    norm_layer=norm_layer,\n                )\n                network.append(stage)\n\n            if downsamples[i]:\n                # downsampling between two stages\n                network.append(Downsample(embed_dims[i], embed_dims[i + 1], 2))\n\n        self.network = nn.ModuleList(network)\n\n        # set post block, for example, class attention layers\n        self.post_network = None\n        if post_layers is not None:\n            self.post_network = nn.ModuleList(\n                [\n                    get_block(\n                        post_layers[i],\n                        dim=embed_dims[-1],\n                        num_heads=num_heads[-1],\n                        mlp_ratio=mlp_ratios[-1],\n                        qkv_bias=qkv_bias,\n                        qk_scale=qk_scale,\n                        attn_drop=attn_drop_rate,\n                        drop_path=0.0,\n                        norm_layer=norm_layer,\n                    )\n                    for i in range(len(post_layers))\n                ]\n            )\n            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dims[-1]))\n            trunc_normal_(self.cls_token, std=0.02)\n\n        # set output type\n        self.return_mean = return_mean  # if yes, return mean, not use class token\n        self.return_dense = (\n            return_dense  # if yes, return class token and all feature tokens\n        )\n        if return_dense:\n            assert not return_mean, \"cannot return both mean and dense\"\n        self.mix_token = mix_token\n        self.pooling_scale = pooling_scale\n        if mix_token:  # enable token mixing, see token labeling for details.\n            self.beta = 1.0\n            assert return_dense, \"return all tokens if mix_token is enabled\"\n        if return_dense:\n            self.aux_head = (\n                nn.Linear(embed_dims[-1], num_classes)\n                if num_classes > 0\n                else nn.Identity()\n            )\n        self.norm = norm_layer(embed_dims[-1])\n\n        # Classifier head\n        self.head = (\n            nn.Linear(embed_dims[-1], num_classes) if num_classes > 0 else nn.Identity()\n        )\n\n        trunc_normal_(self.pos_embed, std=0.02)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"pos_embed\", \"cls_token\"}\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes):\n        self.num_classes = num_classes\n        self.head = (\n            nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n        )\n\n    def forward_embeddings(self, x):\n        # patch embedding\n        x = self.patch_embed(x)\n        # B,C,H,W-> B,H,W,C\n        x = x.permute(0, 2, 3, 1)\n        return x\n\n    def forward_tokens(self, x):\n        for idx, block in enumerate(self.network):\n            if idx == 2:  # add positional encoding after outlooker blocks\n                x = x + self.pos_embed\n                x = self.pos_drop(x)\n            x = block(x)\n\n        B, H, W, C = x.shape\n        x = x.reshape(B, -1, C)\n        return x\n\n    def forward_cls(self, x):\n        B, N, C = x.shape\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        for block in self.post_network:\n            x = block(x)\n        return x\n\n    def forward(self, x):\n        # step1: patch embedding\n        x = self.forward_embeddings(x)\n\n        # mix token, see token labeling for details.\n        if self.mix_token and self.training:\n            lam = np.random.beta(self.beta, self.beta)\n            patch_h, patch_w = (\n                x.shape[1] // self.pooling_scale,\n                x.shape[2] // self.pooling_scale,\n            )\n            bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam, scale=self.pooling_scale)\n            temp_x = x.clone()\n            sbbx1, sbby1, sbbx2, sbby2 = (\n                self.pooling_scale * bbx1,\n                self.pooling_scale * bby1,\n                self.pooling_scale * bbx2,\n                self.pooling_scale * bby2,\n            )\n            temp_x[:, sbbx1:sbbx2, sbby1:sbby2, :] = x.flip(0)[\n                :, sbbx1:sbbx2, sbby1:sbby2, :\n            ]\n            x = temp_x\n        else:\n            bbx1, bby1, bbx2, bby2 = 0, 0, 0, 0\n\n        # step2: tokens learning in the two stages\n        x = self.forward_tokens(x)\n\n        # step3: post network, apply class attention or not\n        if self.post_network is not None:\n            x = self.forward_cls(x)\n        x = self.norm(x)\n\n        if self.return_mean:  # if no class token, return mean\n            return self.head(x.mean(1))\n\n        x_cls = self.head(x[:, 0])\n        if not self.return_dense:\n            return x_cls\n\n        x_aux = self.aux_head(\n            x[:, 1:]\n        )  # generate classes in all feature tokens, see token labeling\n\n        if not self.training:\n            return x_cls + 0.5 * x_aux.max(1)[0]\n\n        # if (\n        #     self.mix_token and self.training\n        # ):  # reverse \"mix token\", see token labeling for details.\n        #     x_aux = x_aux.reshape(x_aux.shape[0], patch_h, patch_w, x_aux.shape[-1])\n        #\n        #     temp_x = x_aux.clone()\n        #     temp_x[:, bbx1:bbx2, bby1:bby2, :] = x_aux.flip(0)[\n        #         :, bbx1:bbx2, bby1:bby2, :\n        #     ]\n        #     x_aux = temp_x\n        #\n        #     x_aux = x_aux.reshape(x_aux.shape[0], patch_h * patch_w, x_aux.shape[-1])\n        #\n        # # return these: 1. class token, 2. classes from all feature tokens, 3. bounding box\n        # return x_cls, x_aux, (bbx1, bby1, bbx2, bby2)\n\n        return x_cls + 0.5 * x_aux.max(1)[0]\n\n\n@register_model\ndef volo_d1(pretrained=False, **kwargs):\n    \"\"\"\n    VOLO-D1 model, Params: 27M\n    --layers: [x,x,x,x], four blocks in two stages, the first stage(block) is outlooker,\n            the other three blocks are transformer, we set four blocks, which are easily\n             applied to downstream tasks\n    --embed_dims, --num_heads,: embedding dim, number of heads in each block\n    --downsamples: flags to apply downsampling or not in four blocks\n    --outlook_attention: flags to apply outlook attention or not\n    --mlp_ratios: mlp ratio in four blocks\n    --post_layers: post layers like two class attention layers using [ca, ca]\n    See detail for all args in the class VOLO()\n    \"\"\"\n    layers = [4, 4, 8, 2]  # num of layers in the four blocks\n    embed_dims = [192, 384, 384, 384]\n    num_heads = [6, 12, 12, 12]\n    mlp_ratios = [3, 3, 3, 3]\n    downsamples = [True, False, False, False]  # do downsampling after first block\n    outlook_attention = [True, False, False, False]\n    # first block is outlooker (stage1), the other three are transformer (stage2)\n    model = VOLO(\n        layers,\n        embed_dims=embed_dims,\n        num_heads=num_heads,\n        mlp_ratios=mlp_ratios,\n        downsamples=downsamples,\n        outlook_attention=outlook_attention,\n        post_layers=[\"ca\", \"ca\"],\n        **kwargs,\n    )\n    model.default_cfg = default_cfgs[\"volo\"]\n    return model\n\n\n@register_model\ndef volo_d2(pretrained=False, **kwargs):\n    \"\"\"\n    VOLO-D2 model, Params: 59M\n    \"\"\"\n    layers = [6, 4, 10, 4]\n    embed_dims = [256, 512, 512, 512]\n    num_heads = [8, 16, 16, 16]\n    mlp_ratios = [3, 3, 3, 3]\n    downsamples = [True, False, False, False]\n    outlook_attention = [True, False, False, False]\n    model = VOLO(\n        layers,\n        embed_dims=embed_dims,\n        num_heads=num_heads,\n        mlp_ratios=mlp_ratios,\n        downsamples=downsamples,\n        outlook_attention=outlook_attention,\n        post_layers=[\"ca\", \"ca\"],\n        **kwargs,\n    )\n    model.default_cfg = default_cfgs[\"volo\"]\n    return model\n\n\n@register_model\ndef volo_d3(pretrained=False, **kwargs):\n    \"\"\"\n    VOLO-D3 model, Params: 86M\n    \"\"\"\n    layers = [8, 8, 16, 4]\n    embed_dims = [256, 512, 512, 512]\n    num_heads = [8, 16, 16, 16]\n    mlp_ratios = [3, 3, 3, 3]\n    downsamples = [True, False, False, False]\n    outlook_attention = [True, False, False, False]\n    model = VOLO(\n        layers,\n        embed_dims=embed_dims,\n        num_heads=num_heads,\n        mlp_ratios=mlp_ratios,\n        downsamples=downsamples,\n        outlook_attention=outlook_attention,\n        post_layers=[\"ca\", \"ca\"],\n        **kwargs,\n    )\n    model.default_cfg = default_cfgs[\"volo\"]\n    return model\n\n\n@register_model\ndef volo_d4(pretrained=False, **kwargs):\n    \"\"\"\n    VOLO-D4 model, Params: 193M\n    \"\"\"\n    layers = [8, 8, 16, 4]\n    embed_dims = [384, 768, 768, 768]\n    num_heads = [12, 16, 16, 16]\n    mlp_ratios = [3, 3, 3, 3]\n    downsamples = [True, False, False, False]\n    outlook_attention = [True, False, False, False]\n    model = VOLO(\n        layers,\n        embed_dims=embed_dims,\n        num_heads=num_heads,\n        mlp_ratios=mlp_ratios,\n        downsamples=downsamples,\n        outlook_attention=outlook_attention,\n        post_layers=[\"ca\", \"ca\"],\n        **kwargs,\n    )\n    model.default_cfg = default_cfgs[\"volo_large\"]\n    return model\n\n\n@register_model\ndef volo_d5(pretrained=False, **kwargs):\n    \"\"\"\n    VOLO-D5 model, Params: 296M\n    stem_hidden_dim=128, the dim in patch embedding is 128 for VOLO-D5\n    \"\"\"\n    layers = [12, 12, 20, 4]\n    embed_dims = [384, 768, 768, 768]\n    num_heads = [12, 16, 16, 16]\n    mlp_ratios = [4, 4, 4, 4]\n    downsamples = [True, False, False, False]\n    outlook_attention = [True, False, False, False]\n    model = VOLO(\n        layers,\n        embed_dims=embed_dims,\n        num_heads=num_heads,\n        mlp_ratios=mlp_ratios,\n        downsamples=downsamples,\n        outlook_attention=outlook_attention,\n        post_layers=[\"ca\", \"ca\"],\n        stem_hidden_dim=128,\n        **kwargs,\n    )\n    model.default_cfg = default_cfgs[\"volo_large\"]\n    return model\n\n\ndef volo_load_pretrained_weights(\n    model, checkpoint_path, use_ema=False, strict=True, num_classes=1000\n):\n    \"\"\"load pretrained weight for VOLO models\"\"\"\n    state_dict = load_state_dict(checkpoint_path, model, use_ema, num_classes)\n    model.load_state_dict(state_dict, strict=strict)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-04T06:59:35.214442Z","iopub.execute_input":"2022-01-04T06:59:35.214908Z","iopub.status.idle":"2022-01-04T06:59:35.341463Z","shell.execute_reply.started":"2022-01-04T06:59:35.214813Z","shell.execute_reply":"2022-01-04T06:59:35.340385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nfrom pytorch_lightning import LightningModule\nfrom timm import create_model\nfrom torch import nn\n\n\n\nclass Model(LightningModule):\n    def __init__(self, cfg, batch_size=32):\n        super().__init__()\n        self.cfg = cfg\n        self.batch_size = batch_size\n        self.__build_model()\n        self._criterion = eval(self.cfg.loss)()\n        self.transform = get_default_transforms()\n        self.strong_transform = get_strong_transforms(self.cfg)\n        self.save_hyperparameters(cfg)\n\n    def __build_model(self):\n        self.backbone = create_model(\n            self.cfg.model.name, pretrained=False, num_classes=0, in_chans=3\n        )\n        num_features = self.backbone.num_features\n        if self.cfg.model.name == \"CSWin_64_12211_tiny_224\":\n            self.backbone.load_state_dict(\n                torch.hub.load_state_dict_from_url(\n                    \"https://github.com/microsoft/CSWin-Transformer/releases/download/v0.1.0/cswin_tiny_224.pth\"\n                ),\n                strict=False,\n            )\n        elif self.cfg.model.name == \"volo_d1\":\n            self.backbone.load_state_dict(\n                torch.hub.load_state_dict_from_url(\n                    \"https://github.com/sail-sg/volo/releases/download/volo_1/d1_224_84.2.pth.tar\"\n                ),\n                strict=False,\n            )\n        elif self.cfg.model.name == \"volo_d5\":\n            self.backbone.load_state_dict(\n                torch.hub.load_state_dict_from_url(\n                    \"https://github.com/sail-sg/volo/releases/download/volo_1/d5_224_86.10.pth.tar\"\n                ),\n                strict=False,\n            )\n\n        self.fc = nn.Sequential(\n            nn.Dropout(0.5), nn.Linear(num_features, self.cfg.model.output_dim)\n        )\n\n    def forward(self, x):\n        f = self.backbone(x)\n        out = self.fc(f)\n        return out\n\n    def training_step(self, batch, batch_idx):\n        loss, pred, labels = self.__share_step(batch, \"train\")\n        return {\"loss\": loss, \"pred\": pred, \"labels\": labels}\n\n    def validation_step(self, batch, batch_idx):\n        loss, pred, labels = self.__share_step(batch, \"val\")\n        return {\"pred\": pred, \"labels\": labels}\n\n    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n        images, labels = batch\n        images = self.transform[\"val\"](images)\n        embed = self.backbone(images).detach().cpu()\n        out = self.forward(images).squeeze(1)\n        if self.cfg.model.output_dim == 1:\n            out = out.sigmoid().detach().cpu() * 100.0\n        else:\n            pred = out.sigmoid().detach().cpu().sum(axis=1)\n            out = (pred, out.sigmoid().detach().cpu())\n        return out, embed\n\n    def __share_step(self, batch, mode):\n        images, labels = batch\n        labels = labels.float()\n        if self.cfg.model.output_dim == 1:\n            labels /= 100.0\n\n        images = self.transform[mode](images)\n\n        if torch.rand(1)[0] < 0.5 and mode == \"train\":\n            mix_images, target_a, target_b, lam = self.strong_transform(\n                images, labels, **self.cfg.strong_transform.params\n            )\n            logits = self.forward(mix_images).squeeze(1)\n            loss = self._criterion(logits, target_a) * lam + (\n                1 - lam\n            ) * self._criterion(logits, target_b)\n        else:\n            logits = self.forward(images).squeeze(1)\n            loss = self._criterion(logits, labels)\n\n        if self.cfg.model.output_dim == 1:\n            pred = logits.sigmoid().detach().cpu() * 100.0\n            labels = labels.detach().cpu() * 100.0\n        else:\n            pred = logits.sigmoid().detach().cpu().sum(axis=1)\n            labels = labels.detach().cpu().sum(axis=1)\n        return loss, pred, labels\n\n    def training_epoch_end(self, outputs):\n        self.__share_epoch_end(outputs, \"train\")\n\n    def validation_epoch_end(self, outputs):\n        self.__share_epoch_end(outputs, \"val\")\n\n    def __share_epoch_end(self, outputs, mode):\n        preds = []\n        labels = []\n        for out in outputs:\n            pred, label = out[\"pred\"], out[\"labels\"]\n            preds.append(pred)\n            labels.append(label)\n        preds = torch.cat(preds)\n        labels = torch.cat(labels)\n        metrics = torch.sqrt(((labels - preds) ** 2).mean())\n        self.log(f\"{mode}_loss\", metrics)\n\n    def check_gradcam(\n        self, dataloader, target_layer, target_category, reshape_transform=None\n    ):\n        cam = GradCAMPlusPlus(\n            model=self,\n            target_layer=target_layer,\n            use_cuda=self.cfg.trainer.gpus,\n            reshape_transform=reshape_transform,\n        )\n\n        org_images, labels = iter(dataloader).next()\n        cam.batch_size = len(org_images)\n        images = self.transform[\"val\"](org_images)\n        images = images.to(self.device)\n        logits = self.forward(images).squeeze(1)\n        pred = logits.sigmoid().detach().cpu().numpy() * 100\n        labels = labels.cpu().numpy()\n\n        grayscale_cam = cam(\n            input_tensor=images, target_category=target_category, eigen_smooth=True\n        )\n        org_images = org_images.detach().cpu().numpy().transpose(0, 2, 3, 1) / 255.0\n        return org_images, grayscale_cam, pred, labels\n\n    def configure_optimizers(self):\n        optimizer = eval(self.cfg.optimizer.name)(\n            self.parameters(), **self.cfg.optimizer.params\n        )\n        scheduler = eval(self.cfg.scheduler.name)(\n            optimizer, **self.cfg.scheduler.params\n        )\n        return [optimizer], [scheduler]\n\n    def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):\n        # https://pytorch-lightning.readthedocs.io/en/latest/guides/speed.html#set-grads-to-none\n        optimizer.zero_grad(set_to_none=False)\n\n\nclass ClassificationModel(LightningModule):\n    def __init__(self, cfg, batch_size=32):\n        super().__init__()\n        self.cfg = cfg\n        self.batch_size = batch_size\n        self.__build_model()\n        self._criterion = eval(self.cfg.loss)()\n        self.transform = get_default_transforms()\n        self.strong_transform = get_strong_transforms(self.cfg)\n        self.save_hyperparameters(cfg)\n\n    def __build_model(self):\n        self.backbone = create_model(\n            self.cfg.model.name, pretrained=False, num_classes=0, in_chans=3\n        )\n        num_features = self.backbone.num_features\n        self.fc = nn.Sequential(\n            nn.Dropout(0.5), nn.Linear(num_features, self.cfg.model.output_dim)\n        )\n\n    def forward(self, x):\n        f = self.backbone(x)\n        out = self.fc(f)\n        return out\n\n    def training_step(self, batch, batch_idx):\n        loss, pred, labels = self.__share_step(batch, \"train\")\n        return {\"loss\": loss, \"pred\": pred, \"labels\": labels}\n\n    def validation_step(self, batch, batch_idx):\n        loss, pred, labels = self.__share_step(batch, \"val\")\n        return {\"pred\": pred, \"labels\": labels}\n\n    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n        images, labels = batch\n        images = self.transform[\"val\"](images)\n        out = self.forward(images).squeeze(1)\n        out = out.sigmoid().detach().cpu()\n        return out\n\n    def __share_step(self, batch, mode):\n        images, labels = batch\n        labels = labels.float()\n\n        images = self.transform[mode](images)\n\n        if torch.rand(1)[0] < 0.5 and mode == \"train\":\n            mix_images, target_a, target_b, lam = self.strong_transform(\n                images, labels, **self.cfg.strong_transform.params\n            )\n            logits = self.forward(mix_images).squeeze(1)\n            loss = self._criterion(logits, target_a) * lam + (\n                1 - lam\n            ) * self._criterion(logits, target_b)\n        else:\n            logits = self.forward(images).squeeze(1)\n            loss = self._criterion(logits, labels)\n\n        pred = logits.sigmoid().detach().cpu()\n        labels = labels.detach().cpu()\n        return loss, pred, labels\n\n    def training_epoch_end(self, outputs):\n        self.__share_epoch_end(outputs, \"train\")\n\n    def validation_epoch_end(self, outputs):\n        self.__share_epoch_end(outputs, \"val\")\n\n    def __share_epoch_end(self, outputs, mode):\n        preds = []\n        labels = []\n        for out in outputs:\n            pred, label = out[\"pred\"], out[\"labels\"]\n            preds.append(pred)\n            labels.append(label)\n        preds = torch.cat(preds)\n        labels = torch.cat(labels)\n        metrics = torch.nn.BCEWithLogitsLoss()(\n            preds.to(torch.float32), labels.to(torch.float32)\n        )\n        self.log(f\"{mode}_loss\", metrics)\n\n    def check_gradcam(\n        self, dataloader, target_layer, target_category, reshape_transform=None\n    ):\n        cam = GradCAMPlusPlus(\n            model=self,\n            target_layer=target_layer,\n            use_cuda=self.cfg.trainer.gpus,\n            reshape_transform=reshape_transform,\n        )\n\n        org_images, labels = iter(dataloader).next()\n        cam.batch_size = len(org_images)\n        images = self.transform[\"val\"](org_images)\n        images = images.to(self.device)\n        logits = self.forward(images).squeeze(1)\n        pred = logits.sigmoid().detach().cpu().numpy() * 100\n        labels = labels.cpu().numpy()\n\n        grayscale_cam = cam(\n            input_tensor=images, target_category=target_category, eigen_smooth=True\n        )\n        org_images = org_images.detach().cpu().numpy().transpose(0, 2, 3, 1) / 255.0\n        return org_images, grayscale_cam, pred, labels\n\n    def configure_optimizers(self):\n        optimizer = eval(self.cfg.optimizer.name)(\n            self.parameters(), **self.cfg.optimizer.params\n        )\n        scheduler = eval(self.cfg.scheduler.name)(\n            optimizer, **self.cfg.scheduler.params\n        )\n        return [optimizer], [scheduler]\n\n    def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):\n        # https://pytorch-lightning.readthedocs.io/en/latest/guides/speed.html#set-grads-to-none\n        optimizer.zero_grad(set_to_none=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:59:35.343066Z","iopub.execute_input":"2022-01-04T06:59:35.343545Z","iopub.status.idle":"2022-01-04T06:59:35.393153Z","shell.execute_reply.started":"2022-01-04T06:59:35.343507Z","shell.execute_reply":"2022-01-04T06:59:35.392315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"configs = [\n    paw_config,\n    bin_swin_config,\n    center_crop_swin_config,\n    bin_paw_config, \n    age_config, \n    breed_config, \n    adoption_speed_config, \n    gender_config,\n    maturity_size_config\n]","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:59:35.396163Z","iopub.execute_input":"2022-01-04T06:59:35.396405Z","iopub.status.idle":"2022-01-04T06:59:35.405028Z","shell.execute_reply.started":"2022-01-04T06:59:35.396379Z","shell.execute_reply":"2022-01-04T06:59:35.404293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paw_preds = []\nimg_embs = []\nbin_swin_paw_preds = []\ncenter_crop_swin_paw_preds = []\nbin_paw_preds = []\nbin_paw_preds_vecs = []\nage_preds = []\nbreed_preds = []\nadoption_speed_preds = []\nadoption_speed_preds_vecs = []\ngender_preds = []\nmaturity_size_preds = []\nmaturity_size_preds_vecs = []\nfor fold in range(max([config.n_splits for config in configs])):\n    print(\"*\"*20 + f\"fold: {fold}\" + \"*\"*20)\n    \n    # paw\n    if fold in paw_config.train_folds:\n        print(\"*\"*20 + f\"inference paw\" + \"*\"*20)\n        datamodule = PetfinderInferenceDataModule(test_df, paw_config)\n        model = Model(paw_config)\n        model.load_state_dict(\n            torch.load(f\"../input/petfinder-pawpularity-score-exp-{paw_config.exp_num}/fold_{fold}_best_loss.ckpt\")[\"state_dict\"] # FIX ME: pth か ckptか統一する\n        )\n        model = model.cuda().eval()\n\n        trainer = pl.Trainer(\n            logger=False,\n            **paw_config.trainer,\n        )\n        output = trainer.predict(\n            model=model,\n            dataloaders=datamodule.predict_dataloader(),\n            return_predictions=True,\n        )\n        oof_pred = []\n        oof_img_emb = []\n        for _output in output:\n            oof_pred.append(_output[0])\n            oof_img_emb.append(_output[1])\n        oof_pred = torch.cat(oof_pred).numpy()\n        oof_img_emb = torch.cat(oof_img_emb).numpy()\n        paw_preds.append(oof_pred)\n        img_embs.append(oof_img_emb)\n        \n        del datamodule, trainer, model, oof_pred, oof_img_emb, output\n        gc.collect()\n        torch.cuda.empty_cache()\n        pl.utilities.memory.garbage_collection_cuda()\n\n    # bin swin paw\n    if fold in bin_swin_config.train_folds:\n        print(\"*\"*20 + f\"inference bin swin paw\" + \"*\"*20)\n        datamodule = PetfinderInferenceDataModule(test_df, bin_swin_config)\n        model = Model(bin_swin_config)\n        model.load_state_dict(\n            torch.load(f\"../input/petfinder-pawpularity-score-exp-{bin_swin_config.exp_num}/fold_{fold}_best_loss.ckpt\")[\"state_dict\"]\n        )\n        model = model.cuda().eval()\n\n        trainer = pl.Trainer(\n            logger=False,\n            **bin_swin_config.trainer,\n        )\n        output = trainer.predict(\n            model=model,\n            dataloaders=datamodule.predict_dataloader(),\n            return_predictions=True,\n        )\n        oof_pred = []\n        for _output in output:\n            oof_pred.append(_output[0][0])\n        oof_pred = torch.cat(oof_pred).numpy()\n        bin_swin_paw_preds.append(oof_pred)\n        \n        del datamodule, trainer, model, oof_pred, output\n        gc.collect()\n        torch.cuda.empty_cache()\n        pl.utilities.memory.garbage_collection_cuda()\n        \n    # center_crop_swin paw\n    if fold in center_crop_swin_config.train_folds:\n        print(\"*\"*20 + f\"inference center_crop_swin paw\" + \"*\"*20)\n        datamodule = PetfinderCenterInferenceDataModule(test_df, center_crop_swin_config)\n        model = Model(center_crop_swin_config)\n        model.load_state_dict(\n            torch.load(f\"../input/petfinder-pawpularity-score-exp-{center_crop_swin_config.exp_num}/fold_{fold}_best_loss.ckpt\")[\"state_dict\"]\n        )\n        model = model.cuda().eval()\n\n        trainer = pl.Trainer(\n            logger=False,\n            **center_crop_swin_config.trainer,\n        )\n        output = trainer.predict(\n            model=model,\n            dataloaders=datamodule.predict_dataloader(),\n            return_predictions=True,\n        )\n        oof_pred = []\n        for _output in output:\n            oof_pred.append(_output[0])\n        oof_pred = torch.cat(oof_pred).numpy()\n        center_crop_swin_paw_preds.append(oof_pred)\n        \n        del datamodule, trainer, model, oof_pred, output\n        gc.collect()\n        torch.cuda.empty_cache()\n        pl.utilities.memory.garbage_collection_cuda()\n       \n    # bin_paw\n    if fold in bin_paw_config.train_folds:\n        print(\"*\"*20 + f\"inference bin_paw\" + \"*\"*20)\n\n        datamodule = PetfinderBinPawpularityInferenceDataModule(test_df, bin_paw_config)\n        model = Model(bin_paw_config)\n        model.load_state_dict(\n            torch.load(f\"../input/petfinder-pawpularity-score-exp-{bin_paw_config.exp_num}/fold_{fold}_best_loss.ckpt\")[\"state_dict\"]\n        )\n        model = model.cuda().eval()\n\n        trainer = pl.Trainer(\n            logger=False,\n            **bin_paw_config.trainer,\n        )\n\n        output = trainer.predict(\n            model=model,\n            dataloaders=datamodule.predict_dataloader(),\n            return_predictions=True,\n        )\n        oof_pred = []\n        oof_pred_vec = []\n        for _output in output:\n            oof_pred.append(_output[0][0])\n            oof_pred_vec.append(_output[0][1])\n        oof_pred = torch.cat(oof_pred).numpy()\n        bin_paw_preds.append(oof_pred)\n        oof_pred_vec = torch.cat(oof_pred_vec).numpy()\n        bin_paw_preds_vecs.append(oof_pred_vec)\n        \n        del datamodule, trainer, model, oof_pred, oof_pred_vec, output\n        gc.collect()\n        torch.cuda.empty_cache()\n        pl.utilities.memory.garbage_collection_cuda()\n        \n    # age\n    if fold in age_config.train_folds:\n        print(\"*\"*20 + f\"inference age\" + \"*\"*20)\n        datamodule = PetfinderAgeInferenceDataModule(test_df, age_config)\n        model = Model(age_config)\n        model.load_state_dict(\n            torch.load(f\"../input/petfinder-pawpularity-score-exp-{age_config.exp_num}/fold_{fold}_best_loss.pth\")\n        )\n        model = model.cuda().eval()\n\n        trainer = pl.Trainer(\n            logger=False,\n            **age_config.trainer,\n        )\n        output = trainer.predict(\n            model=model,\n            dataloaders=datamodule.predict_dataloader(),\n            return_predictions=True,\n        )\n        oof_pred = []\n        for _output in output:\n            oof_pred.append(_output[0])\n        oof_pred = torch.cat(oof_pred)\n        age_preds.append(oof_pred.numpy())\n        \n        del datamodule, trainer, model, oof_pred, output\n        gc.collect()\n        torch.cuda.empty_cache()\n        pl.utilities.memory.garbage_collection_cuda()\n\n    # breed\n    if fold in breed_config.train_folds:\n        print(\"*\"*20 + f\"inference breed\" + \"*\"*20)\n\n        datamodule = PetfinderBreedInferenceDataModule(test_df, breed_config)\n        model = ClassificationModel(breed_config)\n        model.load_state_dict(\n            torch.load(f\"../input/petfinder-pawpularity-score-exp-{breed_config.exp_num}/fold_{fold}_best_loss.pth\")\n        )\n        model = model.cuda().eval()\n\n        trainer = pl.Trainer(\n            logger=False,\n            **breed_config.trainer,\n        )\n\n        oof_pred = trainer.predict(\n            model=model,\n            dataloaders=datamodule.predict_dataloader(),\n            return_predictions=True,\n        )\n        breed_preds.append(torch.cat(oof_pred).numpy())\n        \n        del datamodule, trainer, model, oof_pred\n        gc.collect()\n        torch.cuda.empty_cache()\n        pl.utilities.memory.garbage_collection_cuda()\n\n    # adoption_speed\n    if fold in adoption_speed_config.train_folds:\n        print(\"*\"*20 + f\"inference adoption_speed\" + \"*\"*20)\n\n        datamodule = PetfinderAdoptionSpeedInferenceDataModule(test_df, adoption_speed_config)\n        model = Model(adoption_speed_config)\n        model.load_state_dict(\n            torch.load(f\"../input/petfinder-pawpularity-score-exp-{adoption_speed_config.exp_num}/fold_{fold}_best_loss.ckpt\")[\"state_dict\"] # FIX ME: pth か ckptか統一する\n        )\n        model = model.cuda().eval()\n\n        trainer = pl.Trainer(\n            logger=False,\n            **adoption_speed_config.trainer,\n        )\n\n        output = trainer.predict(\n            model=model,\n            dataloaders=datamodule.predict_dataloader(),\n            return_predictions=True,\n        )\n        oof_pred = []\n        oof_pred_vec = []\n        for _output in output:\n            oof_pred.append(_output[0][0])\n            oof_pred_vec.append(_output[0][1])\n        oof_pred = torch.cat(oof_pred).numpy()\n        adoption_speed_preds.append(oof_pred)\n        oof_pred_vec = torch.cat(oof_pred_vec).numpy()\n        adoption_speed_preds_vecs.append(oof_pred_vec)\n        \n        del datamodule, trainer, model, oof_pred, oof_pred_vec, output\n        gc.collect()\n        torch.cuda.empty_cache()\n        pl.utilities.memory.garbage_collection_cuda()\n\n    # gender\n    if fold in gender_config.train_folds:\n        print(\"*\"*20 + f\"inference gender\" + \"*\"*20)\n\n        datamodule = PetfinderGenderInferenceDataModule(test_df, gender_config)\n        model = ClassificationModel(gender_config)\n        model.load_state_dict(\n            torch.load(f\"../input/petfinder-pawpularity-score-exp-{gender_config.exp_num}/fold_{fold}_best_loss.pth\")\n        )\n        model = model.cuda().eval()\n\n        trainer = pl.Trainer(\n            logger=False,\n            **gender_config.trainer,\n        )\n\n        oof_pred = trainer.predict(\n            model=model,\n            dataloaders=datamodule.predict_dataloader(),\n            return_predictions=True,\n        )\n        gender_preds.append(torch.cat(oof_pred).numpy())\n        \n        del datamodule, trainer, model, oof_pred\n        gc.collect()\n        torch.cuda.empty_cache()\n        pl.utilities.memory.garbage_collection_cuda()\n\n\n    # Maturity Speed\n    if fold in maturity_size_config.train_folds:\n        print(\"*\"*20 + f\"inference MaturitySpeed\" + \"*\"*20)\n\n        datamodule = PetfinderMaturitySizeInferenceDataModule(test_df, maturity_size_config)\n        model = Model(maturity_size_config)\n        model.load_state_dict(\n            torch.load(f\"../input/petfinder-pawpularity-score-exp-{maturity_size_config.exp_num}/fold_{fold}_best_loss.ckpt\")[\"state_dict\"] \n        )\n        model = model.cuda().eval()\n\n        trainer = pl.Trainer(\n            logger=False,\n            **adoption_speed_config.trainer,\n        )\n\n        output = trainer.predict(\n            model=model,\n            dataloaders=datamodule.predict_dataloader(),\n            return_predictions=True,\n        )\n        oof_pred = []\n        oof_pred_vec = []\n        for _output in output:\n            oof_pred.append(_output[0][0])\n            oof_pred_vec.append(_output[0][1])\n        oof_pred = torch.cat(oof_pred).numpy()\n        maturity_size_preds.append(oof_pred)\n        oof_pred_vec = torch.cat(oof_pred_vec).numpy()\n        maturity_size_preds_vecs.append(oof_pred_vec)\n        \n        del datamodule, trainer, model, oof_pred, oof_pred_vec, output\n        gc.collect()\n        torch.cuda.empty_cache()\n        pl.utilities.memory.garbage_collection_cuda()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:59:35.40665Z","iopub.execute_input":"2022-01-04T06:59:35.406884Z","iopub.status.idle":"2022-01-04T07:12:56.246634Z","shell.execute_reply.started":"2022-01-04T06:59:35.406861Z","shell.execute_reply":"2022-01-04T07:12:56.245836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"oof_pred\"] = np.mean(paw_preds, axis=0)\n\nemb_cols = [ f\"emb_{i}\" for i in range(len(img_embs[0][0]))]\ntest_df.loc[:, emb_cols] = np.mean(img_embs, axis=0)\n\ntest_df[\"bin_swin_oof_pred\"] = np.mean(bin_swin_paw_preds, axis=0)\n\ntest_df[\"center_crop_swin_oof_pred\"] = np.mean(center_crop_swin_paw_preds, axis=0)\n\n\ntest_df[\"bin_paw_oof_pred\"] = np.mean(bin_paw_preds, axis=0)\nbin_paw_pred_vec_cols = [f\"bin_paw_pred_vec_{i}\" for i in range(bin_paw_preds_vecs[0].shape[1])]\ntest_df.loc[:, bin_paw_pred_vec_cols] = np.mean(bin_paw_preds_vecs, axis=0)\ntest_df[\"bin_paw_oof_pred_bin\"] = pd.cut(\n                test_df[\"bin_paw_oof_pred\"],\n                3,\n                labels=False,\n            )\n\ntest_df[\"Age\"] = np.mean(age_preds, axis=0)\ntest_df[\"Age_bin\"] = pd.cut(\n    test_df[\"Age\"], [-np.inf, 7, 13, 48, np.inf], labels=False\n)\ntest_df[\"Age_year\"] = test_df[\"Age\"] // 12\n\nbreed_cols = [f\"breed_pred_{i}\" for i in range(len(breed_preds[0][0]))]\ntest_df.loc[:, breed_cols] = np.mean(breed_preds, axis=0)\ntest_df[\"breed\"] = np.argmax(np.mean(breed_preds, axis=0), axis=1)\n\npred_vec_cols = [f\"adoption_speed_pred_vec_{i}\" for i in range(4)]\ntest_df[\"AdoptionSpeed\"] = np.mean(adoption_speed_preds, axis=0)\ntest_df.loc[:, pred_vec_cols] = np.mean(adoption_speed_preds_vecs, axis=0)\n\ngender_cols = [f\"gender_pred_{i}\" for i in range(len(gender_preds[0][0]))]\ntest_df.loc[:, gender_cols] = np.mean(gender_preds, axis=0)\ntest_df[\"gender\"] = np.argmax(np.mean(gender_preds, axis=0), axis=1)\n\npred_vec_cols = [f\"maturity_size_pred_vec_{i}\" for i in range(3)]\ntest_df[\"MaturitySize\"] = np.mean(maturity_size_preds, axis=0)\ntest_df.loc[:, pred_vec_cols] = np.mean(maturity_size_preds_vecs, axis=0)\ntest_df[\"MaturitySize_bin\"] = pd.cut(\n    test_df[\"MaturitySize\"], 4, labels=False\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:12:56.248405Z","iopub.execute_input":"2022-01-04T07:12:56.248673Z","iopub.status.idle":"2022-01-04T07:12:56.437675Z","shell.execute_reply.started":"2022-01-04T07:12:56.248642Z","shell.execute_reply":"2022-01-04T07:12:56.43697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head().T","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:12:56.438944Z","iopub.execute_input":"2022-01-04T07:12:56.43918Z","iopub.status.idle":"2022-01-04T07:12:56.457993Z","shell.execute_reply.started":"2022-01-04T07:12:56.439145Z","shell.execute_reply":"2022-01-04T07:12:56.457369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bbox_info = pd.read_csv(\"/kaggle/working/bbox_info.csv\")[\n                [\n                    \"Id\",\n                    \"height\",\n                    \"width\",\n                    \"aspect_ratio\",\n                    \"area\",\n                    \"num_dog\",\n                    \"num_cat\",\n                    \"num_teddy_bear\",\n                    \"num_person\",\n                    \"num_dog_cat\",\n                    \"num_dog_cat_teddy_bear\",\n                ]\n            ]\nbbox_info","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:12:56.459353Z","iopub.execute_input":"2022-01-04T07:12:56.459623Z","iopub.status.idle":"2022-01-04T07:12:56.480273Z","shell.execute_reply.started":"2022-01-04T07:12:56.459589Z","shell.execute_reply":"2022-01-04T07:12:56.479397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[[col for col in test_df.columns if \"Id\" in col]]","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:12:56.481495Z","iopub.execute_input":"2022-01-04T07:12:56.481809Z","iopub.status.idle":"2022-01-04T07:12:56.496663Z","shell.execute_reply.started":"2022-01-04T07:12:56.481767Z","shell.execute_reply":"2022-01-04T07:12:56.496019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.merge(test_df, bbox_info, how=\"left\", left_on=\"org_Id\", right_on=\"Id\")\ndel bbox_info\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:12:56.49783Z","iopub.execute_input":"2022-01-04T07:12:56.498155Z","iopub.status.idle":"2022-01-04T07:12:56.709775Z","shell.execute_reply.started":"2022-01-04T07:12:56.498116Z","shell.execute_reply":"2022-01-04T07:12:56.708936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = test_df[[col for col in test_df.columns if col != \"Id_y\"]].rename(columns={\"Id_x\": \"Id\"})","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:12:56.711063Z","iopub.execute_input":"2022-01-04T07:12:56.711416Z","iopub.status.idle":"2022-01-04T07:12:56.723469Z","shell.execute_reply.started":"2022-01-04T07:12:56.711379Z","shell.execute_reply":"2022-01-04T07:12:56.722762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[[col for col in test_df.columns if \"Id\" in col]]","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:12:56.724902Z","iopub.execute_input":"2022-01-04T07:12:56.725186Z","iopub.status.idle":"2022-01-04T07:12:56.738076Z","shell.execute_reply.started":"2022-01-04T07:12:56.725148Z","shell.execute_reply":"2022-01-04T07:12:56.737433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"types.\"\"\"\nfrom typing import Union\n\nimport numpy as np\nimport pandas as pd\n\n\ntry:\n    from cudf import DataFrame as CDataFrame\n    from cudf import Series as CSeries\n    from cupy import ndarray as CNDArray\nexcept ImportError:\n    CDataFrame = None\n    CSeries = None\n    CNDArray = None\n\nXDataFrame = Union[CDataFrame, pd.DataFrame]\nXSeries = Union[CSeries, np.ndarray]\nXSeriesB = Union[CSeries, pd.Series]\nXNDArray = Union[CNDArray, np.ndarray]\n\nimport importlib\nfrom typing import Any, List, Dict\n\nclass TransformerMixin:\n    \"\"\"Mixin class for `xfeat.encoder`.\"\"\"\n\n    def fit(self, input_df: XDataFrame) -> None:\n        \"\"\"Fit to data frame.\n\n        Args:\n            input_df (XDataFrame): Input data frame.\n        \"\"\"\n        raise NotImplementedError(\"Not implemented yet.\")\n\n    def transform(self, input_df: XDataFrame) -> XDataFrame:\n        \"\"\"Transform data frame.\n\n        Args:\n            input_df (XDataFrame): Input data frame.\n        Returns:\n            XDataFrame : Output data frame.\n        \"\"\"\n        raise NotImplementedError(\"Not implemented yet.\")\n\n    def fit_transform(self, input_df: XDataFrame) -> XDataFrame:\n        \"\"\"Fit to data frame, then transform it.\n\n        Args:\n            input_df (XDataFrame): Input data frame.\n        Returns:\n            XDataFrame : Output data frame.\n        \"\"\"\n        self.fit(input_df)\n        return self.transform(input_df)\n\n\nfrom itertools import combinations\nfrom typing import List, Optional\n\n\nclass ConcatCombination(TransformerMixin):\n    \"\"\"Generate combination of string columns.\n\n    Example:\n        ::\n\n            >>> import pandas as pd\n            >>> from xfeat import ConcatCombination\n            >>> df = pd.DataFrame({\n              \"col1\": [\"a\", \"b\"],\n              \"col2\": [\"@\", \"%\"],\n              \"col3\": [\"X\", \"Y\"]\n            })\n            >>> encoder = ConcatCombination()\n            >>> encoder.fit_transform(df)\n              col1 col2 col3 col1col2_combi col1col3_combi col2col3_combi\n            0    a    @    X             a@             aX             @X\n            1    b    %    Y             b%             bY             %Y\n\n            >>> encoder = ConcatCombination(output_suffix=\"\", drop_origin=True)\n            >>> encoder.fit_transform(df)\n              col1col2 col1col3 col2col3\n            0       a@       aX       @X\n            1       b%       bY       %Y\n\n            >>> encoder = ConcatCombination(output_suffix=\"\", drop_origin=True, r=3)\n            >>> encoder.fit_transform(df)\n              col1col2col3\n            0          a@X\n            1          b%Y\n\n    Args:\n        input_cols (Optional[List[str]]):\n            Input column names. The default uses all columns of the input data frame.\n\n        include_cols (Optional[List[str]]):\n            Columns of the input data frame that are passed on to the output data frame.\n            Defaults: None.\n\n        output_prefix (str):\n            Prefix of output column name. Defaults: `\"\"`.\n\n        output_suffix (str):\n            Suffix of output column name. Defaults: `\"_combi\"`.\n\n        drop_origin (bool):\n            Drop the original column names. Defaults: `False`.\n\n        fillna (str):\n            To concatenate the string columns, the missing values are replaced with the\n            string value `fillna`. Defaults: `\"_NaN_\"`.\n\n        r (int):\n            Length of combinations. Default: `2`.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_cols: Optional[List[str]] = None,\n        include_cols: Optional[List[str]] = None,\n        output_prefix: str = \"\",\n        output_suffix: str = \"_combi\",\n        drop_origin: bool = False,\n        fillna: str = \"_NaN_\",\n        r: int = 2,\n    ):\n        self._input_cols = input_cols or []\n        self._include_cols = include_cols or []\n        self._output_prefix = output_prefix\n        self._output_suffix = output_suffix\n        self._r = r\n        self._fillna = fillna\n        self._drop_origin = drop_origin\n\n    def fit_transform(self, input_df: XDataFrame) -> XDataFrame:\n        \"\"\"Fit to data frame, then transform it.\n\n        Args:\n            input_df (XDataFrame): Input data frame.\n\n        Returns:\n            XDataFrame : Output data frame.\n        \"\"\"\n        input_cols = self._input_cols\n\n        if not input_cols:\n            self._input_cols = [\n                col\n                for col in input_df.columns.tolist()\n                if col not in self._include_cols\n            ]\n\n        return self.transform(input_df)\n\n    def transform(self, input_df: XDataFrame) -> XDataFrame:\n        \"\"\"Transform data frame.\n\n        Args:\n            input_df (XDataFrame): Input data frame.\n\n        Returns:\n            XDataFrame : Output data frame.\n        \"\"\"\n        cols = []\n\n        n_fixed_cols = len(self._include_cols)\n        df = input_df.copy()\n\n        for cols_pairs in combinations(self._input_cols, r=self._r - n_fixed_cols):\n            fixed_cols_str = \"\".join(self._include_cols)\n            pairs_cols_str = \"\".join(cols_pairs)\n            new_col = (\n                self._output_prefix\n                + fixed_cols_str\n                + pairs_cols_str\n                + self._output_suffix\n            )\n            cols.append(new_col)\n\n            concat_cols = self._include_cols + list(cols_pairs)\n            new_ser = None\n            for col in concat_cols:\n                if new_ser is None:\n                    new_ser = df[col].fillna(self._fillna).copy()\n                else:\n                    new_ser = new_ser + df[col].fillna(self._fillna)\n\n            df[new_col] = new_ser\n\n        if self._drop_origin:\n            return df[cols]\n\n        return df\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-04T07:12:56.739604Z","iopub.execute_input":"2022-01-04T07:12:56.740096Z","iopub.status.idle":"2022-01-04T07:12:59.532529Z","shell.execute_reply.started":"2022-01-04T07:12:56.740058Z","shell.execute_reply":"2022-01-04T07:12:59.531558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ngroupby_keys = [\n    \"MaturitySize_bin\",\n    \"Age_bin\",\n    \"breed\",\n    \"gender\",\n]\nnew_cat_df = pd.concat(\n    [\n        ConcatCombination(drop_origin=True, r=r).fit_transform(\n            test_df[groupby_keys].astype(str).fillna(\"none\")\n        )\n        for r in [\n            2,\n            3,\n            4,\n        ]\n    ],\n    axis=\"columns\",\n)\n\nfor col in new_cat_df.columns:\n    le = LabelEncoder()\n    new_cat_df[col] = le.fit_transform(new_cat_df[col])","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:12:59.534026Z","iopub.execute_input":"2022-01-04T07:12:59.534295Z","iopub.status.idle":"2022-01-04T07:12:59.566659Z","shell.execute_reply.started":"2022-01-04T07:12:59.534246Z","shell.execute_reply":"2022-01-04T07:12:59.565879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_cat_df","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:12:59.568009Z","iopub.execute_input":"2022-01-04T07:12:59.568872Z","iopub.status.idle":"2022-01-04T07:12:59.584878Z","shell.execute_reply.started":"2022-01-04T07:12:59.568831Z","shell.execute_reply":"2022-01-04T07:12:59.584175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.concat(\n    [test_df, new_cat_df],\n    axis=\"columns\",\n)\ncombi_cat_cols = new_cat_df.columns.to_list()\ndel new_cat_df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:12:59.587457Z","iopub.execute_input":"2022-01-04T07:12:59.587668Z","iopub.status.idle":"2022-01-04T07:12:59.845034Z","shell.execute_reply.started":"2022-01-04T07:12:59.587644Z","shell.execute_reply":"2022-01-04T07:12:59.844199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"groupby_dict = []\nnum_var_list = [\n    \"Age\",\n    \"MaturitySize\",\n    \"AdoptionSpeed\",\n    \"adoption_speed_pred_vec_0\",\n    \"adoption_speed_pred_vec_1\",\n    \"adoption_speed_pred_vec_2\",\n    \"adoption_speed_pred_vec_3\",\n    \"gender_pred_0\",\n    \"gender_pred_1\",\n    \"gender_pred_2\",\n] + [col for col in test_df.keys() if \"breed_pred_\" in col]\nnum_stats_list = [\n    \"mean\",\n    \"var\",\n    \"std\",\n    \"min\",\n    \"max\",\n    \"sum\",\n]\nfor key in groupby_keys + combi_cat_cols + [\"bin_paw_oof_pred_bin\"]:\n    groupby_dict.append(\n        {\n            \"key\": [key],\n            \"var\": num_var_list,\n            \"agg\": num_stats_list,\n        }\n    )","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:12:59.846318Z","iopub.execute_input":"2022-01-04T07:12:59.846679Z","iopub.status.idle":"2022-01-04T07:12:59.854445Z","shell.execute_reply.started":"2022-01-04T07:12:59.846637Z","shell.execute_reply":"2022-01-04T07:12:59.85365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm_notebook as tqdm","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:12:59.855739Z","iopub.execute_input":"2022-01-04T07:12:59.856018Z","iopub.status.idle":"2022-01-04T07:12:59.863878Z","shell.execute_reply.started":"2022-01-04T07:12:59.855979Z","shell.execute_reply":"2022-01-04T07:12:59.863172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GroupbyTransformer:\n    def __init__(self, param_dict=None):\n        self.param_dict = param_dict\n\n    def _get_params(self, p_dict):\n        key = p_dict[\"key\"]\n        if \"var\" in p_dict.keys():\n            var = p_dict[\"var\"]\n        else:\n            var = self.var\n        if \"agg\" in p_dict.keys():\n            agg = p_dict[\"agg\"]\n        else:\n            agg = self.agg\n        if \"on\" in p_dict.keys():\n            on = p_dict[\"on\"]\n        else:\n            on = key\n        return key, var, agg, on\n\n    def _aggregate(self, dataframe):\n        self.features = []\n        for param_dict in tqdm(self.param_dict):\n            key, var, agg, on = self._get_params(param_dict)\n            all_features = list(set(key + var))\n            new_features = self._get_feature_names(key, var, agg)\n            features = (\n                dataframe[all_features].groupby(key)[var].agg(agg).reset_index()\n            )\n            features.columns = key + new_features\n            self.features.append(features)\n        return self\n\n    def _merge(self, dataframe, merge=True):\n        for param_dict, features in tqdm(\n            zip(self.param_dict, self.features), total=len(self.features)\n        ):\n            key, var, agg, on = self._get_params(param_dict)\n            if merge:\n                dataframe = dataframe.merge(features, how=\"left\", on=on)\n            else:\n                new_features = self._get_feature_names(key, var, agg)\n                dataframe = pd.concat([dataframe, features[new_features]], axis=1)\n        return dataframe\n\n    def transform(self, dataframe):\n        self._aggregate(dataframe)\n        return self._merge(dataframe, merge=True)\n\n    def _get_feature_names(self, key, var, agg):\n        _agg = []\n        for a in agg:\n            if not isinstance(a, str):\n                _agg.append(a.__name__)\n            else:\n                _agg.append(a)\n        return [\"_\".join([a, v, \"groupby\"] + key) for v in var for a in _agg]\n\n    def get_feature_names(self):\n        self.feature_names = []\n        for param_dict in self.param_dict:\n            key, var, agg, on = self._get_params(param_dict)\n            self.feature_names += self._get_feature_names(key, var, agg)\n        return self.feature_names\n\n    def get_numerical_features(self):\n        return self.get_feature_names()\n\n\nclass DiffGroupbyTransformer(GroupbyTransformer):\n    def _aggregate(self):\n        raise NotImplementedError\n\n    def _merge(self):\n        raise NotImplementedError\n\n    def transform(self, dataframe):\n        for param_dict in tqdm(self.param_dict):\n            key, var, agg, on = self._get_params(param_dict)\n            for a in agg:\n                for v in var:\n                    if not isinstance(a, str):\n                        new_feature = \"_\".join([\"diff\", a.__name__, v, \"groupby\"] + key)\n                        base_feature = \"_\".join([a.__name__, v, \"groupby\"] + key)\n                    else:\n                        new_feature = \"_\".join([\"diff\", a, v, \"groupby\"] + key)\n                        base_feature = \"_\".join([a, v, \"groupby\"] + key)\n#                     print(new_feature)\n                    if str(dataframe[v].dtype) == \"category\":\n                        dataframe[new_feature] = dataframe[base_feature] - dataframe[\n                            v\n                        ].astype(int)\n                    else:\n                        dataframe[new_feature] = dataframe[base_feature] - dataframe[v]\n\n        return dataframe\n\n    def _get_feature_names(self, key, var, agg):\n        _agg = []\n        for a in agg:\n            if not isinstance(a, str):\n                _agg.append(a.__name__)\n            else:\n                _agg.append(a)\n        return [\"_\".join([\"diff\", a, v, \"groupby\"] + key) for v in var for a in _agg]\n\n\nclass RatioGroupbyTransformer(GroupbyTransformer):\n    def _aggregate(self):\n        raise NotImplementedError\n\n    def _merge(self):\n        raise NotImplementedError\n\n    def transform(self, dataframe):\n        for param_dict in tqdm(self.param_dict):\n            key, var, agg, on = self._get_params(param_dict)\n            for a in agg:\n                for v in var:\n                    if not isinstance(a, str):\n                        new_feature = \"_\".join(\n                            [\"ratio\", a.__name__, v, \"groupby\"] + key\n                        )\n                        base_feature = \"_\".join([a.__name__, v, \"groupby\"] + key)\n                    else:\n                        new_feature = \"_\".join([\"ratio\", a, v, \"groupby\"] + key)\n                        base_feature = \"_\".join([a, v, \"groupby\"] + key)\n#                     print(new_feature)\n\n                    if str(dataframe[v].dtype) == \"category\":\n                        dataframe[new_feature] = dataframe[base_feature] / dataframe[\n                            v\n                        ].astype(int)\n                    else:\n                        dataframe[new_feature] = dataframe[base_feature] / dataframe[v]\n        return dataframe\n\n    def _get_feature_names(self, key, var, agg):\n        _agg = []\n        for a in agg:\n            if not isinstance(a, str):\n                _agg.append(a.__name__)\n            else:\n                _agg.append(a)\n        return [\"_\".join([\"ratio\", a, v, \"groupby\"] + key) for v in var for a in _agg]\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-04T07:12:59.865603Z","iopub.execute_input":"2022-01-04T07:12:59.865877Z","iopub.status.idle":"2022-01-04T07:12:59.894669Z","shell.execute_reply.started":"2022-01-04T07:12:59.865842Z","shell.execute_reply":"2022-01-04T07:12:59.893993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"groupby = GroupbyTransformer(groupby_dict)\ntest_df = groupby.transform(test_df)\n\ngroupby = DiffGroupbyTransformer(groupby_dict)\ntest_df = groupby.transform(test_df)\n\ngroupby = RatioGroupbyTransformer(groupby_dict)\ntest_df = groupby.transform(test_df)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-04T07:12:59.895973Z","iopub.execute_input":"2022-01-04T07:12:59.896459Z","iopub.status.idle":"2022-01-04T07:13:05.051411Z","shell.execute_reply.started":"2022-01-04T07:12:59.896425Z","shell.execute_reply":"2022-01-04T07:13:05.050621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"end\")","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:05.052757Z","iopub.execute_input":"2022-01-04T07:13:05.053107Z","iopub.status.idle":"2022-01-04T07:13:05.05819Z","shell.execute_reply.started":"2022-01-04T07:13:05.053066Z","shell.execute_reply":"2022-01-04T07:13:05.05723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_train = pd.read_csv(f\"../input/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}/oof_pred.csv\")\noof_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:05.059641Z","iopub.execute_input":"2022-01-04T07:13:05.059971Z","iopub.status.idle":"2022-01-04T07:13:05.133779Z","shell.execute_reply.started":"2022-01-04T07:13:05.059934Z","shell.execute_reply":"2022-01-04T07:13:05.132913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbdt_config.categorical_cols","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:05.135172Z","iopub.execute_input":"2022-01-04T07:13:05.135463Z","iopub.status.idle":"2022-01-04T07:13:05.145559Z","shell.execute_reply.started":"2022-01-04T07:13:05.135427Z","shell.execute_reply":"2022-01-04T07:13:05.144206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = pd.read_csv(\"../input/petfinder-pawpularity-score/train.csv\")[\"Pawpularity\"].values\ny_train","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:05.147342Z","iopub.execute_input":"2022-01-04T07:13:05.147616Z","iopub.status.idle":"2022-01-04T07:13:05.179507Z","shell.execute_reply.started":"2022-01-04T07:13:05.147583Z","shell.execute_reply":"2022-01-04T07:13:05.17876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if gbdt_config[\"pre_process\"][\"xentropy\"]:\n    y_train = y_train / 100\nelif gbdt_config[\"pre_process\"][\"tweedie\"]:\n    y_train = 100 - y_train","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:05.180732Z","iopub.execute_input":"2022-01-04T07:13:05.181201Z","iopub.status.idle":"2022-01-04T07:13:05.186107Z","shell.execute_reply.started":"2022-01-04T07:13:05.181164Z","shell.execute_reply":"2022-01-04T07:13:05.185265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:05.18751Z","iopub.execute_input":"2022-01-04T07:13:05.188175Z","iopub.status.idle":"2022-01-04T07:13:05.195807Z","shell.execute_reply.started":"2022-01-04T07:13:05.188114Z","shell.execute_reply":"2022-01-04T07:13:05.194937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = load_pickle(f\"../input/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}/cols.pkl\")","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:05.19733Z","iopub.execute_input":"2022-01-04T07:13:05.19803Z","iopub.status.idle":"2022-01-04T07:13:05.214955Z","shell.execute_reply.started":"2022-01-04T07:13:05.197992Z","shell.execute_reply":"2022-01-04T07:13:05.214298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for col in cols:\n#     print(col)","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-01-04T07:13:05.216122Z","iopub.execute_input":"2022-01-04T07:13:05.216406Z","iopub.status.idle":"2022-01-04T07:13:05.219802Z","shell.execute_reply.started":"2022-01-04T07:13:05.216345Z","shell.execute_reply":"2022-01-04T07:13:05.2189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = load_pickle(f\"../input/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}/model.pkl\")","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:05.221143Z","iopub.execute_input":"2022-01-04T07:13:05.221929Z","iopub.status.idle":"2022-01-04T07:13:06.207942Z","shell.execute_reply.started":"2022-01-04T07:13:05.221893Z","shell.execute_reply":"2022-01-04T07:13:06.207181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:06.21631Z","iopub.execute_input":"2022-01-04T07:13:06.216543Z","iopub.status.idle":"2022-01-04T07:13:06.248277Z","shell.execute_reply.started":"2022-01-04T07:13:06.216515Z","shell.execute_reply":"2022-01-04T07:13:06.247482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from cuml.preprocessing.TargetEncoder import TargetEncoder\n\nif gbdt_config[\"target_encoding\"]:\n    cat_cols = gbdt_config[\"categorical_cols\"]\n    for cat_col in tqdm(cat_cols):\n        encoder = TargetEncoder(n_folds=4, smooth=0.3)\n        encoder.fit(oof_train[cat_col], y_train)\n        test_df[cat_col + \"_TE\"] = encoder.transform(test_df[cat_col])","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:06.249575Z","iopub.execute_input":"2022-01-04T07:13:06.249902Z","iopub.status.idle":"2022-01-04T07:13:14.09824Z","shell.execute_reply.started":"2022-01-04T07:13:06.249864Z","shell.execute_reply":"2022-01-04T07:13:14.097419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set(cols) - set(test_df.columns)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:14.099602Z","iopub.execute_input":"2022-01-04T07:13:14.100098Z","iopub.status.idle":"2022-01-04T07:13:14.109225Z","shell.execute_reply.started":"2022-01-04T07:13:14.100057Z","shell.execute_reply":"2022-01-04T07:13:14.108556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set(test_df.columns) - set(cols)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:14.110584Z","iopub.execute_input":"2022-01-04T07:13:14.111037Z","iopub.status.idle":"2022-01-04T07:13:14.141551Z","shell.execute_reply.started":"2022-01-04T07:13:14.111001Z","shell.execute_reply":"2022-01-04T07:13:14.140899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\nxgb.__version__","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:14.142815Z","iopub.execute_input":"2022-01-04T07:13:14.143073Z","iopub.status.idle":"2022-01-04T07:13:14.149503Z","shell.execute_reply.started":"2022-01-04T07:13:14.143039Z","shell.execute_reply":"2022-01-04T07:13:14.148422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds = np.zeros(len(test_df))\nfor fold in range(len(models)):\n    model = models[fold]\n\n    test_preds += (\n        model.predict(test_df[cols].values, ntree_limit=model.best_ntree_limit) / len(models)\n    )","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:14.15106Z","iopub.execute_input":"2022-01-04T07:13:14.152264Z","iopub.status.idle":"2022-01-04T07:13:14.273841Z","shell.execute_reply.started":"2022-01-04T07:13:14.152235Z","shell.execute_reply":"2022-01-04T07:13:14.273096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"xgb_oof_pred_xentropy_065\"] = test_preds * 100","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:14.275226Z","iopub.execute_input":"2022-01-04T07:13:14.275505Z","iopub.status.idle":"2022-01-04T07:13:14.280547Z","shell.execute_reply.started":"2022-01-04T07:13:14.27547Z","shell.execute_reply":"2022-01-04T07:13:14.279512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GBDT_EXP_NUM = \"054\"","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:14.281864Z","iopub.execute_input":"2022-01-04T07:13:14.282874Z","iopub.status.idle":"2022-01-04T07:13:14.289201Z","shell.execute_reply.started":"2022-01-04T07:13:14.282763Z","shell.execute_reply":"2022-01-04T07:13:14.288454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbdt_config = OmegaConf.load(\n    f\"../input/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}/output.json\"\n)\n# pprint(gbdt_config)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:14.290509Z","iopub.execute_input":"2022-01-04T07:13:14.290818Z","iopub.status.idle":"2022-01-04T07:13:16.074285Z","shell.execute_reply.started":"2022-01-04T07:13:14.290771Z","shell.execute_reply":"2022-01-04T07:13:16.07352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_train = pd.read_csv(f\"../input/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}/oof_pred.csv\")\noof_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:16.075625Z","iopub.execute_input":"2022-01-04T07:13:16.075915Z","iopub.status.idle":"2022-01-04T07:13:16.139345Z","shell.execute_reply.started":"2022-01-04T07:13:16.075878Z","shell.execute_reply":"2022-01-04T07:13:16.138649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbdt_config.categorical_cols","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:16.140727Z","iopub.execute_input":"2022-01-04T07:13:16.141003Z","iopub.status.idle":"2022-01-04T07:13:16.149849Z","shell.execute_reply.started":"2022-01-04T07:13:16.140968Z","shell.execute_reply":"2022-01-04T07:13:16.148887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = pd.read_csv(\"../input/petfinder-pawpularity-score/train.csv\")[\"Pawpularity\"].values\ny_train","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:16.151677Z","iopub.execute_input":"2022-01-04T07:13:16.152067Z","iopub.status.idle":"2022-01-04T07:13:16.174445Z","shell.execute_reply.started":"2022-01-04T07:13:16.151993Z","shell.execute_reply":"2022-01-04T07:13:16.173623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if gbdt_config [\"pre_process\"][\"xentropy\"]:\n    y_train = y_train / 100\nelif gbdt_config[\"pre_process\"][\"tweedie\"]:\n    y_train = 100 - y_train","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:16.175777Z","iopub.execute_input":"2022-01-04T07:13:16.176051Z","iopub.status.idle":"2022-01-04T07:13:16.180889Z","shell.execute_reply.started":"2022-01-04T07:13:16.176014Z","shell.execute_reply":"2022-01-04T07:13:16.179999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:16.182241Z","iopub.execute_input":"2022-01-04T07:13:16.182581Z","iopub.status.idle":"2022-01-04T07:13:16.19174Z","shell.execute_reply.started":"2022-01-04T07:13:16.182544Z","shell.execute_reply":"2022-01-04T07:13:16.190855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = load_pickle(f\"../input/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}/cols.pkl\")","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:16.193457Z","iopub.execute_input":"2022-01-04T07:13:16.193754Z","iopub.status.idle":"2022-01-04T07:13:16.210626Z","shell.execute_reply.started":"2022-01-04T07:13:16.193707Z","shell.execute_reply":"2022-01-04T07:13:16.209977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for col in cols:\n#     print(col)","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-01-04T07:13:16.21188Z","iopub.execute_input":"2022-01-04T07:13:16.212135Z","iopub.status.idle":"2022-01-04T07:13:16.215786Z","shell.execute_reply.started":"2022-01-04T07:13:16.212101Z","shell.execute_reply":"2022-01-04T07:13:16.214834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = load_pickle(f\"../input/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}/model.pkl\")","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:16.217523Z","iopub.execute_input":"2022-01-04T07:13:16.218026Z","iopub.status.idle":"2022-01-04T07:13:17.17639Z","shell.execute_reply.started":"2022-01-04T07:13:16.217988Z","shell.execute_reply":"2022-01-04T07:13:17.175605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from cuml.preprocessing.TargetEncoder import TargetEncoder\n\nif gbdt_config[\"target_encoding\"]:\n    cat_cols = gbdt_config[\"categorical_cols\"]\n    for cat_col in tqdm(cat_cols):\n        encoder = TargetEncoder(n_folds=4, smooth=0.3)\n        encoder.fit(oof_train[cat_col], y_train)\n        test_df[cat_col + \"_TE\"] = encoder.transform(test_df[cat_col])","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:17.177821Z","iopub.execute_input":"2022-01-04T07:13:17.178075Z","iopub.status.idle":"2022-01-04T07:13:18.189815Z","shell.execute_reply.started":"2022-01-04T07:13:17.178041Z","shell.execute_reply":"2022-01-04T07:13:18.189118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set(cols) - set(test_df.columns)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:18.19371Z","iopub.execute_input":"2022-01-04T07:13:18.194182Z","iopub.status.idle":"2022-01-04T07:13:18.212313Z","shell.execute_reply.started":"2022-01-04T07:13:18.194144Z","shell.execute_reply":"2022-01-04T07:13:18.207311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set(test_df.columns) - set(cols)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:18.215298Z","iopub.execute_input":"2022-01-04T07:13:18.215641Z","iopub.status.idle":"2022-01-04T07:13:18.242042Z","shell.execute_reply.started":"2022-01-04T07:13:18.215604Z","shell.execute_reply":"2022-01-04T07:13:18.241155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds = np.zeros(len(test_df))\nfor fold in range(len(models)):\n    model = models[fold]\n\n    test_preds += (\n        model.predict(test_df[cols].values, ntree_limit=model.best_ntree_limit) / len(models)\n    )","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:18.243564Z","iopub.execute_input":"2022-01-04T07:13:18.243844Z","iopub.status.idle":"2022-01-04T07:13:18.318936Z","shell.execute_reply.started":"2022-01-04T07:13:18.243807Z","shell.execute_reply":"2022-01-04T07:13:18.31825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"xgb_oof_pred_xentropy_054\"] = test_preds * 100","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:18.320332Z","iopub.execute_input":"2022-01-04T07:13:18.320595Z","iopub.status.idle":"2022-01-04T07:13:18.325966Z","shell.execute_reply.started":"2022-01-04T07:13:18.320561Z","shell.execute_reply":"2022-01-04T07:13:18.325257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GBDT_EXP_NUM = \"064\"","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:18.327345Z","iopub.execute_input":"2022-01-04T07:13:18.327836Z","iopub.status.idle":"2022-01-04T07:13:18.334523Z","shell.execute_reply.started":"2022-01-04T07:13:18.327798Z","shell.execute_reply":"2022-01-04T07:13:18.333655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbdt_config = OmegaConf.load(\n    f\"../input/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}/output.json\"\n)\n# pprint(gbdt_config)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:18.335551Z","iopub.execute_input":"2022-01-04T07:13:18.335856Z","iopub.status.idle":"2022-01-04T07:13:20.384922Z","shell.execute_reply.started":"2022-01-04T07:13:18.335822Z","shell.execute_reply":"2022-01-04T07:13:20.384175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_train = pd.read_csv(f\"../input/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}/oof_pred.csv\")\noof_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:20.386095Z","iopub.execute_input":"2022-01-04T07:13:20.386375Z","iopub.status.idle":"2022-01-04T07:13:20.449331Z","shell.execute_reply.started":"2022-01-04T07:13:20.386323Z","shell.execute_reply":"2022-01-04T07:13:20.44864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbdt_config.categorical_cols","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:20.450385Z","iopub.execute_input":"2022-01-04T07:13:20.450644Z","iopub.status.idle":"2022-01-04T07:13:20.459407Z","shell.execute_reply.started":"2022-01-04T07:13:20.450608Z","shell.execute_reply":"2022-01-04T07:13:20.458626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = pd.read_csv(\"../input/petfinder-pawpularity-score/train.csv\")[\"Pawpularity\"].values\ny_train","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:20.460781Z","iopub.execute_input":"2022-01-04T07:13:20.461571Z","iopub.status.idle":"2022-01-04T07:13:20.483109Z","shell.execute_reply.started":"2022-01-04T07:13:20.461527Z","shell.execute_reply":"2022-01-04T07:13:20.482397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbdt_config[\"target\"]","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:20.484161Z","iopub.execute_input":"2022-01-04T07:13:20.484506Z","iopub.status.idle":"2022-01-04T07:13:20.489837Z","shell.execute_reply.started":"2022-01-04T07:13:20.484469Z","shell.execute_reply":"2022-01-04T07:13:20.488939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbdt_config[\"pre_process\"][\"xentropy\"]","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:20.491176Z","iopub.execute_input":"2022-01-04T07:13:20.492047Z","iopub.status.idle":"2022-01-04T07:13:20.498814Z","shell.execute_reply.started":"2022-01-04T07:13:20.492007Z","shell.execute_reply":"2022-01-04T07:13:20.498046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbdt_config[\"pre_process\"][\"tweedie\"]","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:20.500159Z","iopub.execute_input":"2022-01-04T07:13:20.500903Z","iopub.status.idle":"2022-01-04T07:13:20.507638Z","shell.execute_reply.started":"2022-01-04T07:13:20.500867Z","shell.execute_reply":"2022-01-04T07:13:20.506822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if gbdt_config[\"target\"] == \"residual\":\n    y_train = y_train - (oof_train[\"ensemble_pred\"] - oof_train[\"xgb_oof_pred\"])\n    \nif gbdt_config[\"pre_process\"][\"xentropy\"]:\n    y_train = y_train / 100\nelif gbdt_config[\"pre_process\"][\"tweedie\"]:\n    y_train = 100 - y_train","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:20.509063Z","iopub.execute_input":"2022-01-04T07:13:20.50977Z","iopub.status.idle":"2022-01-04T07:13:20.517989Z","shell.execute_reply.started":"2022-01-04T07:13:20.509735Z","shell.execute_reply":"2022-01-04T07:13:20.517233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:20.519158Z","iopub.execute_input":"2022-01-04T07:13:20.519551Z","iopub.status.idle":"2022-01-04T07:13:20.528947Z","shell.execute_reply.started":"2022-01-04T07:13:20.51951Z","shell.execute_reply":"2022-01-04T07:13:20.528074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = load_pickle(f\"../input/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}/cols.pkl\")","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:20.530234Z","iopub.execute_input":"2022-01-04T07:13:20.53097Z","iopub.status.idle":"2022-01-04T07:13:20.546649Z","shell.execute_reply.started":"2022-01-04T07:13:20.530929Z","shell.execute_reply":"2022-01-04T07:13:20.546006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for col in cols:\n#     print(col)","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-01-04T07:13:20.548321Z","iopub.execute_input":"2022-01-04T07:13:20.548805Z","iopub.status.idle":"2022-01-04T07:13:20.55218Z","shell.execute_reply.started":"2022-01-04T07:13:20.54876Z","shell.execute_reply":"2022-01-04T07:13:20.551226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = load_pickle(f\"../input/petfinder-pawpularity-score-xgb-exp-{GBDT_EXP_NUM}/model.pkl\")","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:20.553308Z","iopub.execute_input":"2022-01-04T07:13:20.553746Z","iopub.status.idle":"2022-01-04T07:13:20.993347Z","shell.execute_reply.started":"2022-01-04T07:13:20.553709Z","shell.execute_reply":"2022-01-04T07:13:20.992548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from cuml.preprocessing.TargetEncoder import TargetEncoder\n\nif gbdt_config[\"target_encoding\"]:\n    cat_cols = gbdt_config[\"categorical_cols\"]\n    for cat_col in tqdm(cat_cols):\n        encoder = TargetEncoder(n_folds=4, smooth=0.3)\n        encoder.fit(oof_train[cat_col], y_train)\n        test_df[cat_col + \"_TE\"] = encoder.transform(test_df[cat_col])","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:20.994697Z","iopub.execute_input":"2022-01-04T07:13:20.995029Z","iopub.status.idle":"2022-01-04T07:13:21.814594Z","shell.execute_reply.started":"2022-01-04T07:13:20.994984Z","shell.execute_reply":"2022-01-04T07:13:21.81377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set(cols) - set(test_df.columns)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:21.816125Z","iopub.execute_input":"2022-01-04T07:13:21.816421Z","iopub.status.idle":"2022-01-04T07:13:21.826601Z","shell.execute_reply.started":"2022-01-04T07:13:21.816382Z","shell.execute_reply":"2022-01-04T07:13:21.825666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set(test_df.columns) - set(cols)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:21.828244Z","iopub.execute_input":"2022-01-04T07:13:21.828661Z","iopub.status.idle":"2022-01-04T07:13:21.856599Z","shell.execute_reply.started":"2022-01-04T07:13:21.828623Z","shell.execute_reply":"2022-01-04T07:13:21.855942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds = np.zeros(len(test_df))\nfor fold in range(len(models)):\n    model = models[fold]\n\n    test_preds += (\n        model.predict(test_df[cols].values, ntree_limit=model.best_ntree_limit) / len(models)\n    )","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:21.859151Z","iopub.execute_input":"2022-01-04T07:13:21.859348Z","iopub.status.idle":"2022-01-04T07:13:21.934595Z","shell.execute_reply.started":"2022-01-04T07:13:21.859324Z","shell.execute_reply":"2022-01-04T07:13:21.933908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[[col for col in test_df.columns if \"oof\" in col]]","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:21.935855Z","iopub.execute_input":"2022-01-04T07:13:21.93609Z","iopub.status.idle":"2022-01-04T07:13:21.969167Z","shell.execute_reply.started":"2022-01-04T07:13:21.936057Z","shell.execute_reply":"2022-01-04T07:13:21.968345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.Series(test_preds).describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:21.970831Z","iopub.execute_input":"2022-01-04T07:13:21.971145Z","iopub.status.idle":"2022-01-04T07:13:21.983019Z","shell.execute_reply.started":"2022-01-04T07:13:21.971106Z","shell.execute_reply":"2022-01-04T07:13:21.982047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"xgb_oof_pred_residual_064\"] = test_preds + test_df[\"oof_pred\"]","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:21.984393Z","iopub.execute_input":"2022-01-04T07:13:21.984709Z","iopub.status.idle":"2022-01-04T07:13:21.991183Z","shell.execute_reply.started":"2022-01-04T07:13:21.984673Z","shell.execute_reply":"2022-01-04T07:13:21.9902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[[col for col in test_df.columns if (\"oof\" in col) & (\"groupby\" not in col) & (\"TE\" not in col)]]","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:21.992726Z","iopub.execute_input":"2022-01-04T07:13:21.99311Z","iopub.status.idle":"2022-01-04T07:13:22.01586Z","shell.execute_reply.started":"2022-01-04T07:13:21.993076Z","shell.execute_reply":"2022-01-04T07:13:22.015202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_df[[\"oof_pred\", \"xgb_oof_pred_xentropy\", \"tweedie_xgb_oof_pred_xentropy\"]]\ntest_df[[\"oof_pred\", \"bin_swin_oof_pred\", \"center_crop_swin_oof_pred\", \"xgb_oof_pred_xentropy_054\", \"xgb_oof_pred_residual_064\", \"xgb_oof_pred_xentropy_065\"]]","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:22.01713Z","iopub.execute_input":"2022-01-04T07:13:22.017433Z","iopub.status.idle":"2022-01-04T07:13:22.031094Z","shell.execute_reply.started":"2022-01-04T07:13:22.017396Z","shell.execute_reply":"2022-01-04T07:13:22.030276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights = np.array([-3.04675469,  1.37952536,  2.31896654,  0.26659276,  3.77603365,1.27495351])","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:22.032736Z","iopub.execute_input":"2022-01-04T07:13:22.033866Z","iopub.status.idle":"2022-01-04T07:13:22.038747Z","shell.execute_reply.started":"2022-01-04T07:13:22.033813Z","shell.execute_reply":"2022-01-04T07:13:22.037855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"Pawpularity\"] = np.dot(test_df[[\"oof_pred\", \"bin_swin_oof_pred\", \"center_crop_swin_oof_pred\", \"xgb_oof_pred_xentropy_054\", \"xgb_oof_pred_residual_064\", \"xgb_oof_pred_xentropy_065\"]].values / 6, weights)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:22.040521Z","iopub.execute_input":"2022-01-04T07:13:22.041739Z","iopub.status.idle":"2022-01-04T07:13:22.04931Z","shell.execute_reply.started":"2022-01-04T07:13:22.041701Z","shell.execute_reply":"2022-01-04T07:13:22.048485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_df[\"Pawpularity\"] = 0.5*test_df[\"oof_pred\"] + 0.5*test_df[\"xgb_oof_pred_xentropy\"]\n# test_df[\"Pawpularity\"] = test_df[\"oof_pred\"] ","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:22.050957Z","iopub.execute_input":"2022-01-04T07:13:22.051343Z","iopub.status.idle":"2022-01-04T07:13:22.055736Z","shell.execute_reply.started":"2022-01-04T07:13:22.051303Z","shell.execute_reply":"2022-01-04T07:13:22.054746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"karunru = test_df[[\"org_Id\", \"Pawpularity\"]].rename(columns={\"org_Id\": \"Id\"})","metadata":{"execution":{"iopub.status.busy":"2022-01-04T07:13:22.057543Z","iopub.execute_input":"2022-01-04T07:13:22.05795Z","iopub.status.idle":"2022-01-04T07:13:22.071573Z","shell.execute_reply.started":"2022-01-04T07:13:22.057915Z","shell.execute_reply":"2022-01-04T07:13:22.070789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# クーラビ part","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/timm-3monthsold/pytorch-image-models-master 2')\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm.notebook import tqdm\nimport os, gc\nimport random\nfrom PIL import Image\nimport tifffile as tiff\nimport cv2\nimport zipfile\nimport collections\nfrom PIL import Image\nfrom sklearn import preprocessing\nfrom random import randint\nfrom glob import glob\nimport shutil\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torchvision\nfrom torchvision import transforms\nimport albumentations as A\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed = 2020\nseed_everything(seed)\nsz1 = 224\nsz2 = 384\nNFOLDS = 5\npet_num = 4\n\n#ImageNet\nmean = np.array([[[0.485, 0.456, 0.406]]])\nstd = np.array([[[0.229, 0.224, 0.225]]])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/petfinder-pawpularity-score/sample_submission.csv')\n#test_df = pd.concat([pd.read_csv('/kaggle/input/petfinder-pawpularity-score/sample_submission.csv')]*900, ignore_index=True)\ntest_df['class'] = -1\ntest_df['conf'] = -1\ntest_ids = test_df.Id.to_list()\n#test_ids = ['0013fd999caf9a3efe1352ca1b0d937e', '0009c66b9439883ba2750fb825e1d7db', '0007de18844b0dbbb5e1f607da0606e0']\ntest_dir = \"/kaggle/input/petfinder-pawpularity-score/test/\"\n#test_dir = '/kaggle/input/petfinder2-sample-images/'\nshutil.copytree('/kaggle/input/yolov5-official-v31-dataset/yolov5', '/kaggle/working/yolov5')\nos.chdir('/kaggle/working/yolov5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = pd.DataFrame(columns=test_df.columns)\ntmp","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python detect.py\\\n--weights /kaggle/input/ultralyticsyolov5aweights/yolov5x.pt\\\n--class 15 16\\\n--img 512\\\n--conf 0.3\\\n--iou 0.5\\\n--source $test_dir\\\n--name inference\\\n--save-txt --save-conf --exist-ok","metadata":{"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir('/kaggle/working')\nsave_dir = f'/kaggle/working/crop_images/'\nos.makedirs(save_dir, exist_ok=True)\n\nfor n, image_id in tqdm(enumerate(test_ids)):\n    orig_image = cv2.imread(f'/kaggle/input/petfinder-pawpularity-score/test/{image_id}.jpg')\n    orig_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n    height = orig_image.shape[0]\n    width = orig_image.shape[1]\n    try:\n        file_path = f'/kaggle/working/yolov5/runs/detect/inference/labels/{image_id}.txt'\n        f = open(file_path, 'r')\n        data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)\n        data = data[:, [0, 5, 1, 2, 3, 4]]\n        data = data[np.argsort(data[:, 1])[::-1]]  #sort by conf\n        for i, d in enumerate(data):\n            xmin = int((d[2]-d[4]/2)*width)\n            ymin = int((d[3]-d[5]/2)*height)\n            xmax = int((d[2]+d[4]/2)*width)\n            ymax = int((d[3]+d[5]/2)*height)\n            width_half = (xmax - xmin) // 2\n            height_half = (ymax - ymin) // 2\n            r = np.maximum(width_half, height_half)\n            xc = (xmin + xmax) // 2\n            yc = (ymin + ymax) // 2\n            final_xmin = np.maximum(xc-r, 0)\n            final_ymin = np.maximum(yc-r, 0)\n            final_xmax = np.minimum(xc+r, width)\n            final_ymax = np.minimum(yc+r, height)\n            crop_img = orig_image[final_ymin:final_ymax, final_xmin:final_xmax, :]\n            np.save(save_dir + f'{image_id}-{i}', crop_img.astype(np.uint8))\n            df = pd.DataFrame(columns=test_df.columns)\n            df.loc[0, 'Id'] = f'{image_id}-{i}'\n            df.loc[0, 'class'] = d[0]\n            df.loc[0, 'conf'] = d[1]\n            tmp = tmp.append(df, ignore_index=True)\n            \n    except:\n        np.save(save_dir + f'{image_id}-0', orig_image.astype(np.uint8))\n        df = pd.DataFrame(columns=test_df.columns)\n        df.loc[0, 'Id'] = f'{image_id}-0'\n        df.loc[0, 'class'] = 'NA'\n        df.loc[0, 'conf'] = 'NA'\n        tmp = tmp.append(df, ignore_index=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = tmp.rename(columns={'Id': 'Id2'})\ntmp['img_idx'] = tmp['Id2'].apply(lambda x: x.split('-')[1])\ntmp['Id'] = tmp['Id2'].apply(lambda x: x.split('-')[0])\ntmp","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset(Dataset):\n    def __init__(self, df, size, transform=None):\n        self.df = df\n        self.size = size\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        image_id = self.df.loc[idx, 'Id2']\n        img = np.load(f'/kaggle/working/crop_images/{image_id}.npy')\n        img = cv2.resize(img, (self.size, self.size)).astype(np.float32)\n        \n        if self.transform:\n            sample = self.transform(image=img)\n            img = sample['image']\n        \n        img = (img/255.0 - mean) / std\n        img = np.transpose(img, (2, 0, 1))\n        img = torch.from_numpy(img)\n\n        return img\n    \n    \nclass Dataset2(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        image_id = self.df.loc[idx, 'Id']\n        img = np.load(f'/kaggle/working/crop_images/{image_id}.npy').astype(np.float32)\n        \n        if self.transform:\n            sample = self.transform(image=img)\n            img = sample['image']\n        \n        img = (img/255.0 - mean) / std\n        img = np.transpose(img, (2, 0, 1))\n        img = torch.from_numpy(img)\n\n        return img\n    \n\ndef inference_fn1(data_loader, model, device):\n    model.eval()    \n    val_preds = []\n    \n    for i, x in enumerate(data_loader):\n        img = x\n        img = img.to(device, dtype=torch.float)\n        \n        with torch.no_grad():\n            pred = model(img)\n            val_preds.append(nn.Softmax()(pred).detach().cpu().numpy())\n            \n    val_preds = np.concatenate(val_preds)\n                \n    return val_preds\n\n\ndef inference_fn2(data_loader, model, device):\n    model.eval()    \n    val_preds = []\n    \n    for i, x in enumerate(data_loader):\n        img = x\n        img = img.to(device, dtype=torch.float)\n        \n        with torch.no_grad():\n            pred = model(img)\n            val_preds.append(nn.Sigmoid()(pred).detach().cpu().numpy() * 100)\n            \n    val_preds = np.concatenate(val_preds)\n                \n    return val_preds\n\n\nclass Model(nn.Module):\n    def __init__(self, model_name=None, pretrained=False, num_classes=100):\n        super().__init__()\n        self.model = timm.create_model(\n            model_name=model_name, \n            in_chans=3, \n            pretrained=pretrained\n            )\n        n_features = self.model.head.in_features\n        self.model.head = nn.Linear(n_features, num_classes)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#224x224\n#5folds\n#cropped imgs\ntest_ds = Dataset(df=tmp, size=sz1, transform=None)\ntest_dl = DataLoader(dataset=test_ds, batch_size=256, shuffle=False, num_workers=2)\ntarget_cols = np.arange(1, 101)\npredictions1 = 0\npredictions2 = 0\n\nfor fold in range(NFOLDS):\n    print(f'======== 224 yolo FOLD:{fold} inference ========')\n    \n    #SwinT type1: 100 outputs\n    model1 = Model(model_name='swin_large_patch4_window7_224_in22k', num_classes=100)\n    model1.load_state_dict(torch.load(f'/kaggle/input/petfinder2-swint1-weight-016/swint1_fold_{fold}.pth'))\n    model1.to(device)\n    prediction1 = inference_fn1(test_dl, model1, device)\n    predictions1 += (target_cols.reshape(-1, 100) * prediction1.reshape(-1, 100)).sum(axis=1) / NFOLDS\n    \n    #SwinT type2: 1 output\n    model2 = Model(model_name='swin_large_patch4_window7_224_in22k', num_classes=1)\n    model2.model.head = nn.Sequential(\n        nn.Linear(model2.model.head.in_features, 128),\n        nn.Dropout(0.0),\n        nn.Linear(128, 64),\n        nn.Linear(64, 1)\n    )\n    model2.load_state_dict(torch.load(f'/kaggle/input/petfinder2-swint2-weight-011/swint2_fold_{fold}.pth'))\n    model2.to(device)\n    prediction2 = inference_fn2(test_dl, model2, device).flatten()\n    predictions2 += prediction2 / NFOLDS\n    \n    del model1, model2, prediction1, prediction2\n    gc.collect()\n    \ntmp['pred1'] = predictions1\ntmp['pred2'] = predictions2\n\ndel test_ds, test_dl, predictions1, predictions2\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#384x384\n#5folds\n#cropped imgs\ntest_ds = Dataset(df=tmp, size=sz2, transform=None)\ntest_dl = DataLoader(dataset=test_ds, batch_size=64, shuffle=False, num_workers=2)\ntarget_cols = np.arange(1, 101)\npredictions3 = 0\npredictions4 = 0\n\nfor fold in range(NFOLDS):\n    print(f'======== 384 yolo FOLD:{fold} inference ========')\n    \n    #SwinT type1: 100 outputs\n    model1 = Model(model_name='swin_large_patch4_window12_384_in22k', num_classes=100)\n    model1.load_state_dict(torch.load(f'/kaggle/input/petfinder2-swint1-weight-068/swint1_fold_{fold}.pth'))\n    model1.to(device)\n    prediction3 = inference_fn1(test_dl, model1, device)\n    predictions3 += (target_cols.reshape(-1, 100) * prediction3.reshape(-1, 100)).sum(axis=1) / NFOLDS\n    \n    #SwinT type2: 1 output\n    model2 = Model(model_name='swin_large_patch4_window12_384_in22k', num_classes=1)\n    model2.model.head = nn.Sequential(\n        nn.Linear(model2.model.head.in_features, 128),\n        nn.Dropout(0.0),\n        nn.Linear(128, 64),\n        nn.Linear(64, 1)\n    )\n    model2.load_state_dict(torch.load(f'/kaggle/input/petfinder2-swint2-weight-028/swint2_fold_{fold}.pth'))\n    model2.to(device)\n    prediction4 = inference_fn2(test_dl, model2, device).flatten()\n    predictions4 += prediction4 / NFOLDS\n    \n    del model1, model2, prediction3, prediction4\n    gc.collect()\n    \ntmp['pred3'] = predictions3\ntmp['pred4'] = predictions4\n\ndel test_ds, test_dl, predictions3, predictions4\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#postprocessing by multi-pets\ntest_df = tmp.groupby('Id').head(pet_num).groupby('Id').mean().reset_index()\ntest_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('../input/petfinder-pawpularity-score/sample_submission.csv')\ncoorabi = pd.concat(\n    [sub, \n     test_df[['pred1']], \n     test_df[['pred2']], \n     test_df[['pred3']], \n     test_df[['pred4']]\n    ], axis=1\n)\ncoorabi","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -r /kaggle/working/yolov5\n!rm -r /kaggle/working/crop_images","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fastai\n#224x224\n#5folds\n#original imgs\nimport fastai\nfrom fastai.vision.all import *\nfrom fastai.callback.all import *\nimport torchvision.models as torch_models\n\ndef petfinder_rmse(input, target):\n    return 100 * torch.sqrt(F.mse_loss(F.sigmoid(input.flatten()), target))\n\ntest_df = pd.read_csv('/kaggle/input/petfinder-pawpularity-score/sample_submission.csv')\ntest_df['Path'] = '../input/petfinder-pawpularity-score/test/' + test_df['Id'] + '.jpg'\npredictions5 = 0\n\nfor fold in range(NFOLDS):\n    print(f'======== fastai FOLD:{fold} inference ========')\n    learn = load_learner(fname = Path(f'/kaggle/input/petfinder2-swint3-weight-001/swint3_fold_{fold}.pkl'), cpu=False)\n    test_dl = learn.dls.test_dl(test_df)\n    preds, _ = learn.get_preds(dl=test_dl)\n    predictions5 += preds * 100 / NFOLDS\n    del learn, test_dl, preds\n    gc.collect()\n    \ncoorabi['pred5'] = predictions5\n\ndel predictions5\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#224x224\n#5folds\n#center crop\n#binary classifier\ntest_df = pd.read_csv('/kaggle/input/petfinder-pawpularity-score/sample_submission.csv')\ntest_ids = test_df.Id.to_list()\nos.chdir('/kaggle/working')\nsave_dir = f'/kaggle/working/crop_images/'\nos.makedirs(save_dir, exist_ok=True)\n\nfor n, image_id in tqdm(enumerate(test_ids)):\n    orig_image = cv2.imread(f'/kaggle/input/petfinder-pawpularity-score/test/{image_id}.jpg')\n    orig_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n    height = orig_image.shape[0]\n    width = orig_image.shape[1]\n    xc = width // 2\n    yc = height // 2\n    r = np.minimum(xc, yc)\n    xmin = np.maximum(xc-r, 0)\n    ymin = np.maximum(yc-r, 0)\n    xmax = np.minimum(xc+r, width)\n    ymax = np.minimum(yc+r, height)\n    crop_img = orig_image[ymin:ymax, xmin:xmax, :]\n    crop_img = cv2.resize(crop_img, (sz1, sz1)).astype(np.uint8)\n    np.save(save_dir + f'{image_id}', crop_img)\n\ntest_ds = Dataset2(df=coorabi, transform=None)\ntest_dl = DataLoader(dataset=test_ds, batch_size=256, shuffle=False, num_workers=2)\ntarget_cols = np.arange(1, 101)\npredictions6 = 0\npredictions7 = 0\nbinary = 0\n\nfor fold in range(NFOLDS):\n    print(f'======== 224 center FOLD:{fold} inference ========')\n    \n    #SwinT type1: 100 outputs\n    model1 = Model(model_name='swin_large_patch4_window7_224_in22k', num_classes=100)\n    model1.load_state_dict(torch.load(f'/kaggle/input/petfinder2-swint1-weight-076/swint1_fold_{fold}.pth'))\n    model1.to(device)\n    prediction6 = inference_fn1(test_dl, model1, device)\n    predictions6 += (target_cols.reshape(-1, 100) * prediction6.reshape(-1, 100)).sum(axis=1) / NFOLDS\n    \n    #SwinT type2: 1 output\n    model2 = Model(model_name='swin_large_patch4_window7_224_in22k', num_classes=1)\n    model2.model.head = nn.Sequential(\n        nn.Linear(model2.model.head.in_features, 128),\n        nn.Dropout(0.0),\n        nn.Linear(128, 64),\n        nn.Linear(64, 1)\n    )\n    model2.load_state_dict(torch.load(f'/kaggle/input/petfinder2-swint2-weight-036/swint2_fold_{fold}.pth'))\n    model2.to(device)\n    prediction7 = inference_fn2(test_dl, model2, device).flatten()\n    predictions7 += prediction7 / NFOLDS\n    \n    del model1, model2, prediction6, prediction7\n    gc.collect()\n    \n    print(f'======== 224 Binary FOLD:{fold} inference ========')\n    model = Model(model_name='swin_large_patch4_window7_224_in22k', num_classes=1)\n    model.model.head = nn.Sequential(\n        nn.Linear(model.model.head.in_features, 128),\n        nn.Dropout(0.0),\n        nn.Linear(128, 64),\n        nn.Linear(64, 1)\n    )\n    model.load_state_dict(torch.load(f'/kaggle/input/petfinder2-binary-classifier-weight/fold_{fold}.pth'))\n    model.to(device)\n    prediction = inference_fn2(test_dl, model, device).flatten()\n    binary += prediction / NFOLDS\n    \n    del model, prediction\n    gc.collect()\n\ncoorabi['pred6'] = predictions6\ncoorabi['pred7'] = predictions7\ncoorabi['binary1'] = binary\n\n!rm -r /kaggle/working/crop_images\ndel test_ds, test_dl, predictions6, predictions7, binary\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#384x384\n#5folds\n#center crop\n#binary classifier\ntest_df = pd.read_csv('/kaggle/input/petfinder-pawpularity-score/sample_submission.csv')\ntest_ids = test_df.Id.to_list()\nos.chdir('/kaggle/working')\nsave_dir = f'/kaggle/working/crop_images/'\nos.makedirs(save_dir, exist_ok=True)\n\nfor n, image_id in tqdm(enumerate(test_ids)):\n    orig_image = cv2.imread(f'/kaggle/input/petfinder-pawpularity-score/test/{image_id}.jpg')\n    orig_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n    height = orig_image.shape[0]\n    width = orig_image.shape[1]\n    xc = width // 2\n    yc = height // 2\n    r = np.minimum(xc, yc)\n    xmin = np.maximum(xc-r, 0)\n    ymin = np.maximum(yc-r, 0)\n    xmax = np.minimum(xc+r, width)\n    ymax = np.minimum(yc+r, height)\n    crop_img = orig_image[ymin:ymax, xmin:xmax, :]\n    crop_img = cv2.resize(crop_img, (sz2, sz2)).astype(np.uint8)\n    np.save(save_dir + f'{image_id}', crop_img)\n\ntest_ds = Dataset2(df=coorabi, transform=None)\ntest_dl = DataLoader(dataset=test_ds, batch_size=64, shuffle=False, num_workers=2)\ntarget_cols = np.arange(1, 101)\npredictions8 = 0\npredictions9 = 0\nbinary = 0\n\nfor fold in range(NFOLDS):\n    print(f'======== 384 center FOLD:{fold} inference ========')\n    \n    #SwinT type1: 100 outputs\n    model1 = Model(model_name='swin_large_patch4_window12_384_in22k', num_classes=100)\n    model1.load_state_dict(torch.load(f'/kaggle/input/petfinder2-swint1-weight-077/swint1_fold_{fold}.pth'))\n    model1.to(device)\n    prediction8 = inference_fn1(test_dl, model1, device)\n    predictions8 += (target_cols.reshape(-1, 100) * prediction8.reshape(-1, 100)).sum(axis=1) / NFOLDS\n    \n    #SwinT type2: 1 output\n    model2 = Model(model_name='swin_large_patch4_window12_384_in22k', num_classes=1)\n    model2.model.head = nn.Sequential(\n        nn.Linear(model2.model.head.in_features, 128),\n        nn.Dropout(0.0),\n        nn.Linear(128, 64),\n        nn.Linear(64, 1)\n    )\n    model2.load_state_dict(torch.load(f'/kaggle/input/petfinder2-swint2-weight-037/swint2_fold_{fold}.pth'))\n    model2.to(device)\n    prediction9 = inference_fn2(test_dl, model2, device).flatten()\n    predictions9 += prediction9 / NFOLDS\n    \n    del model1, model2, prediction8, prediction9\n    gc.collect()\n    \n    print(f'======== 384 Binary FOLD:{fold} inference ========')\n    model = Model(model_name='swin_large_patch4_window12_384_in22k', num_classes=1)\n    model.model.head = nn.Sequential(\n        nn.Linear(model.model.head.in_features, 128),\n        nn.Dropout(0.0),\n        nn.Linear(128, 64),\n        nn.Linear(64, 1)\n    )\n    model.load_state_dict(torch.load(f'/kaggle/input/petfinder2-binary-classifier-weight2/fold_{fold}.pth'))\n    model.to(device)\n    prediction = inference_fn2(test_dl, model, device).flatten()\n    binary += prediction / NFOLDS\n    \n    del model, prediction\n    gc.collect()\n    \ncoorabi['pred8'] = predictions8\ncoorabi['pred9'] = predictions9\ncoorabi['binary2'] = binary\n\n!rm -r /kaggle/working/crop_images\ndel test_ds, test_dl, predictions8, predictions9, binary\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coorabi['Pawpularity'] = ((coorabi['pred1'] * 0.275 + coorabi['pred2'] * 0.225 + coorabi['pred3'] * 0.275 + coorabi['pred4'] * 0.225) * 0.75 + coorabi['pred5'] * 0.25) * 0.6 + (coorabi['pred6'] * 0.22 + coorabi['pred7'] * 0.18 + coorabi['pred8'] * 0.33 + coorabi['pred9'] * 0.27) * 0.4\ncoorabi['binary'] = (coorabi['binary1'] + coorabi['binary2']) / 2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coorabi","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble","metadata":{}},{"cell_type":"code","source":"final = pd.read_csv('../input/petfinder-pawpularity-score/sample_submission.csv')\nfinal","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final['Pawpularity'] = karunru['Pawpularity'].values * 0.5 + coorabi['Pawpularity'].values * 0.5\nfinal['binary'] = coorabi['binary'].values\nfinal","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PostProcess","metadata":{}},{"cell_type":"code","source":"idx = final[(final['binary'] > 51) & (final['Pawpularity'] < 49)].index","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final.loc[idx, 'Pawpularity'] = final.loc[idx, 'Pawpularity'].values * 0.1 + final.loc[idx, 'binary'].values * 0.9","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final[['Id', 'Pawpularity']].to_csv('/kaggle/working/submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}