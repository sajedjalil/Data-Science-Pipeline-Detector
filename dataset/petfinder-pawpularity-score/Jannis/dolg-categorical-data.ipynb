{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PetFinder.my - Pawpularity Contest Solution using a DOLG model and additional categorical data","metadata":{}},{"cell_type":"markdown","source":"**Training code which was submitted to the Petfinder.my Competition**\n\nThis notebook deals with:\n* training code of a Deep Orthogonal Fusion of Local and Global Features (DOLG) model\n* adding an additional branch which uses categorical information\n* categorical data includes (i) image meta features like colorfulness, saturation, size ratio ... and (ii) image content feature like animal type, breed\n* training can be executed on TPU\n\n**CV score was around 17.5 and Private Leaderbord score ~17.8**","metadata":{}},{"cell_type":"code","source":"import joblib\nimport os\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow_addons as tfa\n\nfrom functools import partial\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import KFold\nfrom sklearn.utils import shuffle\nfrom kaggle_datasets import KaggleDatasets\n\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import applications, layers, Model, Input\nfrom tensorflow.keras import (layers, Sequential, activations, initializers)\nfrom tensorflow.keras.applications import EfficientNetB5","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:43:20.810328Z","iopub.execute_input":"2022-01-19T07:43:20.810673Z","iopub.status.idle":"2022-01-19T07:43:20.818382Z","shell.execute_reply.started":"2022-01-19T07:43:20.810642Z","shell.execute_reply":"2022-01-19T07:43:20.817484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DOLG implementation with EfficientNetB5 base\n**DOLG implementation origins from https://github.com/innat/DOLG-TensorFlow (see also the authors Kaggle Notebooks https://www.kaggle.com/ipythonx). Some adaptions were made to use an EfficientNet base**","metadata":{}},{"cell_type":"code","source":"class MultiAtrous(tf.keras.Model):\n    def __init__(self, dilation_rates = [6, 12, 18], upsampling = 1, kernel_size = 3, padding = \"same\", **kwargs):\n        super(MultiAtrous, self).__init__(name = 'MultiAtrous', **kwargs)\n        self.dilation_rates = dilation_rates\n        self.kernel_size = kernel_size\n        self.upsampling = upsampling\n        self.padding = padding\n        self.dilated_convs = [layers.Conv2D(filters = int(1024 / 4), kernel_size = self.kernel_size, padding = self.padding, dilation_rate = rate) for rate in self.dilation_rates]\n        self.gap_branch = Sequential([layers.Lambda(lambda t4d: K.mean(t4d, axis = (1, 2), keepdims = True), name = 'GlobalAverage2D'), layers.Conv2D(int(1024 / 2), kernel_size = 1), layers.Activation('relu'), layers.UpSampling2D(size = self.upsampling, interpolation = \"bilinear\")], name = 'gap_branch')\n\n    def call(self, inputs, training = None, **kwargs):\n        local_feature = []\n        for dilated_conv in self.dilated_convs:\n            x = dilated_conv(inputs)\n            x = self.gap_branch(x)\n            local_feature.append(x)\n        return tf.concat(local_feature, axis = -1)\n\n    def get_config(self):\n        config = {'dilation_rates': self.dilation_rates, 'kernel_size': self.kernel_size, 'padding': self.padding, 'upsampling': self.upsampling}\n        base_config = super(MultiAtrous, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\nclass DOLGLocalBranch(tf.keras.Model):\n    def __init__(self, img_size, **kwargs):\n        super(DOLGLocalBranch, self).__init__(name = 'LocalBranch', **kwargs)\n        self.multi_atrous = MultiAtrous(padding = 'same', upsampling = int(img_size / 32))\n        self.conv1 = layers.Conv2D(1024, kernel_size = 1)\n        self.conv2 = layers.Conv2D(1024, kernel_size = 1, use_bias = False)\n        self.conv3 = layers.Conv2D(1024, kernel_size = 1)\n        self.bn = layers.BatchNormalization()\n\n    def call(self, inputs, training = None, **kwargs):\n        local_feat = self.multi_atrous(inputs)\n        local_feat = self.conv1(local_feat)\n        local_feat = tf.nn.relu(local_feat)\n        local_feat = self.conv2(local_feat)\n        local_feat = self.bn(local_feat)\n        norm_local_feat = tf.math.l2_normalize(local_feat)\n        attn_map = tf.nn.relu(local_feat)\n        attn_map = self.conv3(attn_map)\n        attn_map = activations.softplus(attn_map)\n        return norm_local_feat * attn_map\n\nclass OrthogonalFusion(layers.Layer):\n    def __init__(self, **kwargs):\n        super().__init__(name = 'OrthogonalFusion', **kwargs)\n\n    def call(self, inputs):\n        local_feat, global_feat = inputs\n        height = local_feat.shape[1]\n        width = local_feat.shape[2]\n        depth = local_feat.shape[3]\n\n        local_feat = tf.reshape(local_feat, [-1, height * width, depth])\n        local_feat = tf.transpose(local_feat, perm = [0, 2, 1])\n\n        projection = tf.matmul(tf.expand_dims(global_feat, axis = 1), local_feat)\n        projection = tf.matmul(tf.expand_dims(global_feat, axis = 2), projection)\n        projection = tf.reshape(projection, [-1, height, width, depth])\n\n        global_feat_norm = tf.norm(global_feat, ord = 2, axis = 1)\n        projection = projection / tf.reshape(global_feat_norm * global_feat_norm, shape = [-1, 1, 1, 1])\n        local_feat = tf.transpose(local_feat, perm = [0, 1, 2])\n        local_feat = tf.reshape(local_feat, [-1, height, width, depth])\n\n        orthogonal_comp = local_feat - projection\n        global_feat = tf.expand_dims(tf.expand_dims(global_feat, axis = 1), axis = 1)\n        global_feat = tf.broadcast_to(global_feat, tf.shape(local_feat))\n        output = tf.concat([global_feat, orthogonal_comp], axis = -1)\n        return output\n\nclass GeneralizedMeanPooling2D(layers.Layer):\n    def __init__(self, init_norm = 3.0, normalize = False, epsilon = 1e-6, **kwargs):\n        self.init_norm = init_norm\n        self.normalize = normalize\n        self.epsilon = epsilon\n        super(GeneralizedMeanPooling2D, self).__init__(name = 'GeM', **kwargs)\n\n    def build(self, input_shape):\n        self.p = self.add_weight(name = \"norms\", shape = (input_shape[-1],), initializer = initializers.constant(self.init_norm), trainable = True)\n        super(GeneralizedMeanPooling2D, self).build(input_shape)\n\n    def call(self, inputs):\n        x = tf.reduce_mean(tf.abs(inputs ** self.p), axis = [1, 2], keepdims = False) + self.epsilon\n        x = x ** (1.0 / self.p)\n        if self.normalize:\n            x = tf.nn.l2_normalize(x, 1)\n        return x\n\n    def get_config(self):\n        config = {'init_norm': self.init_norm, 'normalize': self.normalize, 'epsilon': self.epsilon}\n        base_config = super(GeneralizedMeanPooling2D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n    \n\nclass DOLGNet(tf.keras.Model):\n    def __init__(self, img_size, **kwargs):\n        \n        self.img_size = img_size\n        super(DOLGNet, self).__init__(name = 'DOLGNet', **kwargs)\n        \n        self.orthogonal_fusion = OrthogonalFusion()\n        self.local_branch = DOLGLocalBranch(img_size)\n        self.glob_branch_pool = Sequential([layers.GlobalAveragePooling2D(), layers.Dense(1024, activation = None)], name = 'GlobalBranchPooling')\n        \n        base = applications.EfficientNetB5(\n            include_top = False,\n            weights = 'imagenet',\n            input_shape=(img_size,img_size,3),\n            input_tensor = Input((img_size, img_size, 3))\n        )\n        \n        # Batchlayers not to be trained\n        for layer in reversed(base.layers):\n            if isinstance(layer, tf.keras.layers.BatchNormalization):\n                layer.trainable = False\n            else:\n                layer.trainable = True\n        \n        self.new_base = Model([base.inputs], [base.get_layer('block5g_add').output,\n            base.get_layer('block7c_add').output\n        ], name = 'EfficientNet')\n        \n        \n        self.classifier = Sequential([layers.GlobalAveragePooling2D(name = 'HeadGAP')], name = 'Classifiers')\n        \n            \n    def call(self, inputs, training = None, **kwargs):\n        to_local, to_global = self.new_base(inputs)\n        local_feat = self.local_branch(to_local)\n        global_feat = self.glob_branch_pool(to_global)\n        \n        orthogonal_feat = self.orthogonal_fusion([local_feat, global_feat])\n        \n        return self.classifier(orthogonal_feat)\n\n\n    def build_graph(self):\n        x = tf.keras.layers.Input(shape = (self.img_size, self.img_size, 3), name=\"cnn_input\")\n        return Model(inputs = [x], outputs = self.call(x))","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:43:23.232372Z","iopub.execute_input":"2022-01-19T07:43:23.232644Z","iopub.status.idle":"2022-01-19T07:43:23.277284Z","shell.execute_reply.started":"2022-01-19T07:43:23.232619Z","shell.execute_reply":"2022-01-19T07:43:23.276481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initalize TPU","metadata":{}},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    DEVICE = \"TPU\"\n    batchsize_factor = 32\nexcept:\n    DEVICE = \"notTPU\"\n    strategy = tf.distribute.get_strategy()\n    batchsize_factor = 32\n\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\nREPLICAS =  strategy.num_replicas_in_sync\nBATCH_SIZE = batchsize_factor * strategy.num_replicas_in_sync\nGCS_PATH = KaggleDatasets().get_gcs_path(f'tfrecs-new')\nFILENAMES = tf.io.gfile.glob(GCS_PATH + '/*.tfrecords')","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:43:27.480173Z","iopub.execute_input":"2022-01-19T07:43:27.480512Z","iopub.status.idle":"2022-01-19T07:43:37.421668Z","shell.execute_reply.started":"2022-01-19T07:43:27.480483Z","shell.execute_reply":"2022-01-19T07:43:37.420743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inspect TFRecords\n\nThe dataset contains several features:\n* All features provided by the competition hosts (accessory, info, collage, action, face, near, human...)\n* An image quality assessment feature called \"brisque\" and calculated with the piq pytorch package (https://github.com/photosynthesis-team/piq)\n* An colorfulness score (calculated according to https://www.pyimagesearch.com/2017/06/05/computing-image-colorfulness-with-opencv-and-python/)\n* The image size and the size ration (both scaled between 0 and 1)\n* Image brightness and saturation\n* The type of the animal (0: dog, 1: cat). For the classification a pretrained ResNet50V2 was used\n* The breed of the dog (using an Inception architecture trained on the Kaggle dog breed dataset). The values were one-hot encoded afterwards.","metadata":{}},{"cell_type":"code","source":"raw_dataset = tf.data.TFRecordDataset(FILENAMES[0])\n\ndef _get_keys(raw_dataset):\n    for raw_record in raw_dataset.take(1):\n        example = tf.train.Example()\n        example.ParseFromString(raw_record.numpy())\n        return dict(example.features.feature).keys()\n\n# Get feature columns from tfrecords and exclude features which are not used for training\nCONSIDERED_COLS = [k for k in _get_keys(raw_dataset) if k not in [\"image\", \"image_height\", \"image_width\", \"score\"]]\nprint(f\"{len(CONSIDERED_COLS)} features e.g. {CONSIDERED_COLS[0:5]}\")\n\nCONSIDERED_COLS = list(map(lambda x: x.lower(), CONSIDERED_COLS))","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:43:52.519803Z","iopub.execute_input":"2022-01-19T07:43:52.520789Z","iopub.status.idle":"2022-01-19T07:43:53.394486Z","shell.execute_reply.started":"2022-01-19T07:43:52.520748Z","shell.execute_reply":"2022-01-19T07:43:53.393542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare TFRecords and apply light augmentations","metadata":{}},{"cell_type":"code","source":"DEFAULT_IMG_SIZE = (512,512)\n\ndef _parse_image(proto, train):\n    \n    image_feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'image_width': tf.io.FixedLenFeature([], tf.int64),\n        'image_height': tf.io.FixedLenFeature([], tf.int64)\n        \n    }\n    \n    features = dict(\n        image_feature_description, **{k: tf.io.FixedLenFeature([], tf.float32) for k in CONSIDERED_COLS}\n    )\n        \n    if train:\n        features[\"score\"] = tf.io.FixedLenFeature([], tf.float32)\n        \n    return tf.io.parse_single_example(proto, features)\n\n\ndef decode_image(image, img_size, normalize=False):\n    image = tf.image.decode_jpeg(image, channels=3) \n    shapes = tf.shape(image)\n    h, w = shapes[-3], shapes[-2]\n    small = tf.minimum(h, w)\n    image = tf.image.resize_with_crop_or_pad(image, small, small)\n    image = tf.image.resize(image, img_size)\n    \n    # EfficientNet shouldn't be normalized as this is done in a custom model layer\n    if normalize:\n        image = tf.cast(image, tf.float16)\n        image = image / 255.0\n    \n    return tf.reshape(image, [*img_size, 3])\n\ndef get_image_and_label(proto, train, img_size):\n    sample = _parse_image(proto, train=train)\n    \n    img = decode_image(sample[\"image\"], img_size)\n    \n    features = {\"cnn_input\": img,\n               \"dense_input\": tf.stack([sample[c] for c in CONSIDERED_COLS])}\n    \n    if train:\n        return features, tf.cast(sample[\"score\"], tf.float32) / 100.0  \n    \n    return features, None\n\ndef augmentation(img):\n    \n    img = tf.image.random_flip_left_right(img)\n    \n    img = tf.image.random_flip_up_down(img)\n    \n    if tf.random.uniform([], 0, 1.0, dtype = tf.float32) > 0.75:\n        img = tf.image.transpose(img)\n    \n    probablity_rotation = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n    if probablity_rotation > 0.75:\n        img = tf.image.rot90(img, k = 3)\n    elif probablity_rotation > 0.5:\n        img = tf.image.rot90(img, k = 2)\n    elif probablity_rotation > 0.25:\n        img = tf.image.rot90(img, k = 1)   \n        \n    return img\n\ndef augmentation_wrapper(x, y):\n    x.update({\"cnn_input\": augmentation(x[\"cnn_input\"])})\n    return x, y\n\ndef scaling_wrapper(x, y):\n    return x[\"cnn_input\"] / 255.0\n\ndef get_tfrecord_size(tfrecord):\n    return sum(1 for _ in tfrecord)\n\ndef get_training_dataset(tfr, batchsize, img_size=DEFAULT_IMG_SIZE):\n    return tfr.map(partial(get_image_and_label, train=True, img_size=img_size)).repeat().map(\n        lambda x,y: augmentation_wrapper(x,y)).shuffle(1000).batch(batchsize, drop_remainder=True).prefetch(AUTOTUNE)\n\ndef get_validation_dataset(tfr, batchsize, img_size=DEFAULT_IMG_SIZE):\n    return tfr.map(partial(get_image_and_label, train=True, img_size=img_size)).batch(batchsize, drop_remainder=True).prefetch(AUTOTUNE)\n\ndef get_normalization_batch(tfr, batchsize, img_size=DEFAULT_IMG_SIZE):\n    return tfr.map(partial(get_image_and_label, train=True, img_size=img_size)).map(\n        lambda x,y: scaling_wrapper(x,y)).shuffle(1000).batch(batchsize, drop_remainder=True).prefetch(AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:43:55.29585Z","iopub.execute_input":"2022-01-19T07:43:55.296148Z","iopub.status.idle":"2022-01-19T07:43:55.320522Z","shell.execute_reply.started":"2022-01-19T07:43:55.296115Z","shell.execute_reply":"2022-01-19T07:43:55.319643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building the complete model consisting of a DLOG branch and another branch for including the categorical features","metadata":{}},{"cell_type":"code","source":"def dolg():\n    tf.keras.backend.reset_uids()\n    model = DOLGNet(img_size = DEFAULT_IMG_SIZE[0]) \n    return model.build_graph()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:43:58.598734Z","iopub.execute_input":"2022-01-19T07:43:58.599041Z","iopub.status.idle":"2022-01-19T07:43:58.604219Z","shell.execute_reply.started":"2022-01-19T07:43:58.599011Z","shell.execute_reply":"2022-01-19T07:43:58.603188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dense_net():\n    inputs = tf.keras.layers.Input(shape=len(CONSIDERED_COLS), name=\"dense_input\")\n    embedding_layer = tf.keras.layers.Embedding(input_dim=len(CONSIDERED_COLS), output_dim=10, name=\"emb_2\")(inputs)\n    flatten_layer = tf.keras.layers.Flatten()(embedding_layer)\n    dense_layer = tf.keras.layers.Dense(10, activation=\"relu\")(flatten_layer)\n    model = tf.keras.Model(inputs=inputs, outputs=dense_layer)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:44:00.359646Z","iopub.execute_input":"2022-01-19T07:44:00.360169Z","iopub.status.idle":"2022-01-19T07:44:00.366715Z","shell.execute_reply.started":"2022-01-19T07:44:00.36012Z","shell.execute_reply":"2022-01-19T07:44:00.365664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stack_model(models):\n    combined_inputs = [model.input for model in models]\n    combined_outputs = [model.output for model in models]\n    concat_layer = tf.keras.layers.concatenate(combined_outputs)\n    dense_layer_ = tf.keras.layers.Dense(1024, activation=\"relu\")(concat_layer)\n    dropout_layer_0 = tf.keras.layers.Dropout(0.3)(dense_layer_)\n    dense_layer_0 = tf.keras.layers.Dense(512, activation=\"relu\")(dropout_layer_0)\n    dropout_layer = tf.keras.layers.Dropout(0.3)(dense_layer_0)\n    dense_layer_1 = tf.keras.layers.Dense(128, activation=\"relu\")(dropout_layer)\n    dense_layer_2 = tf.keras.layers.Dense(32, activation=\"relu\")(dense_layer_1)\n    dense_layer_3 = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dense_layer_2)\n    return tf.keras.Model(inputs=combined_inputs, outputs=[dense_layer_3])","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:44:02.38986Z","iopub.execute_input":"2022-01-19T07:44:02.390166Z","iopub.status.idle":"2022-01-19T07:44:02.399277Z","shell.execute_reply.started":"2022-01-19T07:44:02.390136Z","shell.execute_reply":"2022-01-19T07:44:02.3983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Learning rate sheduler","metadata":{}},{"cell_type":"code","source":"EPOCHS = 15\n\ndef lrfn(epoch, bs=BATCH_SIZE, epochs=EPOCHS):\n\n    LR_START = 1e-5\n    LR_MAX = 1e-4\n    LR_FINAL = 5e-5\n    LR_RAMPUP_EPOCHS = 3\n    LR_SUSTAIN_EPOCHS = 0\n    DECAY_EPOCHS = epochs  - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1\n    LR_EXP_DECAY = (LR_FINAL / LR_MAX) ** (1 / (EPOCHS - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1))\n\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = LR_START + (LR_MAX + LR_START) * (epoch / LR_RAMPUP_EPOCHS) ** 2.5\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        epoch_diff = epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS\n        decay_factor = (epoch_diff / DECAY_EPOCHS) * 3.141592653589793\n        decay_factor= (tf.math.cos(decay_factor).numpy() + 1) / 2        \n        lr = LR_FINAL + (LR_MAX - LR_FINAL) * decay_factor\n\n    return lr","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:44:04.369375Z","iopub.execute_input":"2022-01-19T07:44:04.370027Z","iopub.status.idle":"2022-01-19T07:44:04.379015Z","shell.execute_reply.started":"2022-01-19T07:44:04.369985Z","shell.execute_reply":"2022-01-19T07:44:04.37801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt  \nrng = [i for i in range(20)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y);","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:44:07.667023Z","iopub.execute_input":"2022-01-19T07:44:07.667289Z","iopub.status.idle":"2022-01-19T07:44:07.936074Z","shell.execute_reply.started":"2022-01-19T07:44:07.667261Z","shell.execute_reply":"2022-01-19T07:44:07.935436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Overall architecture\n\n**The model has two different inputs. An image for the cnn part and and feature vector containing the meta information**","metadata":{}},{"cell_type":"code","source":"_dolg = dolg()\n_dense = dense_net()\nstacked_model = stack_model([_dolg,_dense])\nstacked_model.compile()\ntf.keras.utils.plot_model(stacked_model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:44:10.247498Z","iopub.execute_input":"2022-01-19T07:44:10.248079Z","iopub.status.idle":"2022-01-19T07:44:20.784223Z","shell.execute_reply.started":"2022-01-19T07:44:10.24804Z","shell.execute_reply":"2022-01-19T07:44:20.783385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KFold Splitting and Training","metadata":{}},{"cell_type":"code","source":"kfold = KFold(n_splits=4, shuffle=True, random_state=0)\nfilenames = [f for f in [_ for _ in os.listdir(\"../input/tfrecs-new\") if len(_.split(\".\")) > 1] if f.split(\".\")[1] ==\"tfrecords\"]\nfolds = {}\nfor fold, (train_idx, val_idx) in enumerate(kfold.split(filenames)):\n  folds[fold] = {\"split\": (np.take(filenames, train_idx), np.take(filenames, val_idx))}","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:44:20.786691Z","iopub.execute_input":"2022-01-19T07:44:20.786972Z","iopub.status.idle":"2022-01-19T07:44:20.806536Z","shell.execute_reply.started":"2022-01-19T07:44:20.786937Z","shell.execute_reply":"2022-01-19T07:44:20.805474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ignore_order = tf.data.Options()\nignore_order.experimental_deterministic = False\n\nif DEVICE ==\"TPU\":\n    tf.tpu.experimental.initialize_tpu_system(tpu)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:44:20.808696Z","iopub.execute_input":"2022-01-19T07:44:20.809049Z","iopub.status.idle":"2022-01-19T07:44:28.571641Z","shell.execute_reply.started":"2022-01-19T07:44:20.809007Z","shell.execute_reply":"2022-01-19T07:44:28.570695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_history = []\n\n\nimg_size = DEFAULT_IMG_SIZE\n\nfor fold in [3]:\n\n    validation_records = tf.data.TFRecordDataset(\n      [f for f in FILENAMES if f.split(\"/\")[-1] in folds[fold][\"split\"][1]],\n      num_parallel_reads=AUTOTUNE)\n\n    train_records = tf.data.TFRecordDataset([f for f in FILENAMES if f.split(\"/\")[-1] in folds[fold][\"split\"][0]],\n                                          num_parallel_reads=AUTOTUNE)\n\n    train_records = train_records.with_options(ignore_order)\n\n    validation_data = get_validation_dataset(validation_records, BATCH_SIZE, img_size) \n    \n    print(f\"Used batchsize: {BATCH_SIZE}\")\n    \n    train_data = get_training_dataset(train_records, BATCH_SIZE, img_size)\n\n    train_size = get_tfrecord_size(train_records)\n    validation_size = get_tfrecord_size(validation_records)\n\n    with strategy.scope():\n\n        model1 = dolg()\n        model2 = dense_net()\n        model = stack_model([model1, model2])\n\n        # get data to set mean and std vor the normalization layers\n        model.get_layer('EfficientNet').get_layer('normalization').adapt(\n            get_normalization_batch(train_records, BATCH_SIZE, img_size)\n        )   \n\n        opt = tf.keras.optimizers.Adam(lr=0.001)\n        \n        loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.01)\n\n        model.compile(loss=loss, optimizer=opt, metrics=[\"mse\", tf.keras.metrics.RootMeanSquaredError()])\n\n        model.summary()\n\n        cb_lr = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=1)\n\n        cb_earlystop = tf.keras.callbacks.EarlyStopping( \n                           patience=3, restore_best_weights=True, verbose=1)\n\n        params = {\n            \"epochs\":15,\n            \"steps_per_epoch\":train_size//BATCH_SIZE,\n            \"validation_data\": validation_data,\n            \"callbacks\": [cb_earlystop, cb_lr]\n        } \n\n        print(f\"Fold: {fold}, {train_size} train images {validation_size} validation images\")\n\n        history = model.fit(train_data, **params)\n\n        cv_history.append(history.history)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:44:28.573913Z","iopub.execute_input":"2022-01-19T07:44:28.57427Z","iopub.status.idle":"2022-01-19T08:12:13.305544Z","shell.execute_reply.started":"2022-01-19T07:44:28.574229Z","shell.execute_reply":"2022-01-19T08:12:13.304473Z"},"trusted":true},"execution_count":null,"outputs":[]}]}