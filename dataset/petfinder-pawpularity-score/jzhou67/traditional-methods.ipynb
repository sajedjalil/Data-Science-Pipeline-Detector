{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport time\nimport cv2\nimport math\n\nimport tensorflow as tf\n\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import load_model\n\n\nfrom sklearn import *\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\n\n\nfrom tensorflow.keras.utils import Sequence\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport albumentations as A \n\nnp.random.seed(0)\ntf.random.set_seed(0)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Data","metadata":{}},{"cell_type":"code","source":"DATA_DIR = \"../input/petfinder-pawpularity-score/\"\nTRAIN_CSV = DATA_DIR + \"train.csv\"\nTEST_CSV = DATA_DIR + \"test.csv\"\nTRAIN_DIR = DATA_DIR + \"train/\"\nTEST_DIR = DATA_DIR + \"test/\"\n\nIMG_SIZE = 224\nBATCH_SIZE = 32\nTARGET = 'Pawpularity'\nFEATURES = ['Subject Focus', 'Eyes', 'Face', 'Near', \n        'Action', 'Accessory', 'Group', 'Collage', \n        'Human', 'Occlusion', 'Info', 'Blur']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(TRAIN_CSV)\ntest = pd.read_csv(TEST_CSV)\nprint('train dataset shape: ', train.shape)\nprint('test dataset shape: ', test.shape)\ntrain.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Part","metadata":{"execution":{"iopub.status.busy":"2021-12-01T05:26:35.560455Z","iopub.execute_input":"2021-12-01T05:26:35.560785Z","iopub.status.idle":"2021-12-01T05:26:35.579374Z","shell.execute_reply.started":"2021-12-01T05:26:35.560746Z","shell.execute_reply":"2021-12-01T05:26:35.578249Z"}}},{"cell_type":"code","source":"Xf = np.load('../input/preprocessedtrain/Xf4.npy')\n\nAtt = train.drop(['Id', 'Pawpularity'], axis=1)\nY = train['Pawpularity']\ntrainXf, valXf, trainAtt, valAtt, trainY, valY = train_test_split(Xf, Att, Y,\n                                                                  train_size=0.8,\n                                                                  test_size=0.2, \n                                                                  random_state=4487)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=500)\ntrainXn= pca.fit_transform(trainXf)\nvalXn= pca.transform(valXf)\n\ntrainAttXn = np.concatenate([trainXf, trainAtt],1)\nvalAttXn = np.concatenate([valXf, valAtt],1)\nprint(trainAttXn.shape)\nprint(valAttXn.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paramgrid = {'alpha': [2.154434690031882],\n             'gamma': [0.004641588833612777]}\nkrrcv = model_selection.GridSearchCV(\n    kernel_ridge.KernelRidge(kernel='rbf'), # estimator\n    paramgrid,\n    scoring='neg_mean_squared_error',\n    cv=5,\n    n_jobs=-1, verbose=True)\n\nkrrcv.fit(trainAttXn, trainY)\nprint(krrcv.best_params_)\nkrrcv_pred = krrcv.predict(valAttXn)\nrmse = metrics.mean_squared_error(valY, krrcv_pred, squared=False)\nprint(\"rmse =\", rmse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing Part","metadata":{}},{"cell_type":"code","source":"def get_bbox_merge(out_boxes, image_shape):\n    ## merge all bbox\n    h_min, w_min = 999, 999\n    h_max, w_max = 0, 0\n    for i in range(len(out_boxes)):\n        top, left, bottom, right = out_boxes[i]\n        h_min = min(h_min, top)\n        w_min = min(w_min, left)\n        h_max = max(h_max, bottom)\n        w_max = max(w_max, right)\n\n    h_min = max(0, np.floor(h_min + 0.5).astype('int32'))\n    w_min = max(0, np.floor(w_min + 0.5).astype('int32'))\n    h_max = min(image_shape[0]-1, np.floor(h_max + 0.5).astype('int32'))\n    w_max = min(image_shape[1]-1, np.floor(w_max + 0.5).astype('int32'))\n\n    return (h_min, h_max, w_min, w_max)\n\ndef get_bbox_max_area(out_boxes, image_shape):\n    ### keep one with max area\n    max_area = 0\n    for i in range(len(out_boxes)):\n        top, left, bottom, right = out_boxes[i]\n        area = (bottom-top)*(right-left)\n        if area > max_area:\n            max_area = area\n            h_min, w_min, h_max, w_max = out_boxes[i]\n\n    h_min = max(0, np.floor(h_min + 0.5).astype('int32'))\n    w_min = max(0, np.floor(w_min + 0.5).astype('int32'))\n    h_max = min(image_shape[0]-1, np.floor(h_max + 0.5).astype('int32'))\n    w_max = min(image_shape[1]-1, np.floor(w_max + 0.5).astype('int32'))\n\n    return (h_min, h_max, w_min, w_max)\n\ndef get_bbox_max_score(out_boxes, out_scores, image_shape):\n    ### keep one with the highest score\n    max_score = 0\n    for i in range(len(out_boxes)):\n        if out_scores[i] > max_score:\n            h_min, w_min, h_max, w_max = out_boxes[i]\n            max_score = out_scores[i]\n\n    h_min = max(0, np.floor(h_min + 0.5).astype('int32'))\n    w_min = max(0, np.floor(w_min + 0.5).astype('int32'))\n    h_max = min(image_shape[0]-1, np.floor(h_max + 0.5).astype('int32'))\n    w_max = min(image_shape[1]-1, np.floor(w_max + 0.5).astype('int32'))\n\n    return (h_min, h_max, w_min, w_max)\n\ndef cv2_letterbox_image(image, expected_size):\n    ih, iw = image.shape[0:2]\n    ew, eh = expected_size\n    scale = min(eh / ih, ew / iw)\n    nh = int(ih * scale)\n    nw = int(iw * scale)\n    image = cv2.resize(image, (nw, nh), interpolation=cv2.INTER_CUBIC)\n    top = (eh - nh) // 2\n    bottom = eh - nh - top\n    left = (ew - nw) // 2\n    right = ew - nw - left\n    new_img = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT)\n    return new_img\n\n\ndef yolo_head(feats, anchors, num_classes, input_shape):\n    \"\"\"Convert final layer features to bounding box parameters.\"\"\"\n    num_anchors = len(anchors)\n    # Reshape to batch, height, width, num_anchors, box_params.\n    anchors_tensor = K.reshape(K.constant(anchors), [1, 1, 1, num_anchors, 2])\n\n    grid_shape = K.shape(feats)[1:3]  # height, width\n    grid_y = K.tile(K.reshape(K.arange(0, stop=grid_shape[0]), [-1, 1, 1, 1]),\n                    [1, grid_shape[1], 1, 1])\n    grid_x = K.tile(K.reshape(K.arange(0, stop=grid_shape[1]), [1, -1, 1, 1]),\n                    [grid_shape[0], 1, 1, 1])\n    grid = K.concatenate([grid_x, grid_y])\n    grid = K.cast(grid, K.dtype(feats))\n\n    feats = K.reshape(\n        feats, [-1, grid_shape[0], grid_shape[1], num_anchors, num_classes + 5])\n\n    # Adjust preditions to each spatial grid point and anchor size.\n    box_xy = (K.sigmoid(feats[..., :2]) + grid) / K.cast(grid_shape[...,::-1], K.dtype(feats))\n    box_wh = K.exp(feats[..., 2:4]) * anchors_tensor / K.cast(input_shape[...,::-1], K.dtype(feats))\n    box_confidence = K.sigmoid(feats[..., 4:5])\n    box_class_probs = K.sigmoid(feats[..., 5:])\n\n    return box_xy, box_wh, box_confidence, box_class_probs\n\ndef yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape):\n    '''Get corrected boxes'''\n    box_yx = box_xy[..., ::-1]\n    box_hw = box_wh[..., ::-1]\n    input_shape = K.cast(input_shape, K.dtype(box_yx))\n    image_shape = K.cast(image_shape, K.dtype(box_yx))\n    new_shape = K.round(image_shape * K.min(input_shape / image_shape))\n    offset = (input_shape - new_shape) / 2. / input_shape\n    scale = input_shape / new_shape\n    box_yx = (box_yx - offset) * scale\n    box_hw *= scale\n\n    box_mins = box_yx - (box_hw / 2.)\n    box_maxes = box_yx + (box_hw / 2.)\n    boxes = K.concatenate([\n        box_mins[..., 0:1],  # y_min\n        box_mins[..., 1:2],  # x_min\n        box_maxes[..., 0:1],  # y_max\n        box_maxes[..., 1:2]  # x_max\n    ])\n\n    # Scale boxes back to original image shape.\n    boxes *= K.concatenate([image_shape, image_shape])\n    return boxes\n\ndef yolo_boxes_and_scores(feats, anchors, num_classes, input_shape, image_shape):\n    '''Process Conv layer output'''\n    box_xy, box_wh, box_confidence, box_class_probs = yolo_head(feats,\n                                                                anchors, num_classes, input_shape)\n    boxes = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape)\n    boxes = K.reshape(boxes, [-1, 4])\n    box_scores = box_confidence * box_class_probs\n    box_scores = K.reshape(box_scores, [-1, num_classes])\n    return boxes, box_scores\n\ndef yolo_eval(yolo_outputs,\n              anchors,\n              num_classes,\n              image_shape,\n              max_boxes=20,\n              score_threshold=.6,\n              iou_threshold=.5):\n    \"\"\"Evaluate YOLO model on given input and return filtered boxes.\"\"\"\n    num_layers = len(yolo_outputs)\n    anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]] if num_layers == 3 else [[3, 4, 5], [1, 2, 3]]  # default setting\n    input_shape = K.shape(yolo_outputs[0])[1:3] * 32\n    boxes = []\n    box_scores = []\n    for l in range(num_layers):\n        _boxes, _box_scores = yolo_boxes_and_scores(yolo_outputs[l],\n                                                    anchors[anchor_mask[l]], num_classes, input_shape, image_shape)\n        boxes.append(_boxes)\n        box_scores.append(_box_scores)\n    boxes = K.concatenate(boxes, axis=0)\n    box_scores = K.concatenate(box_scores, axis=0)\n\n    mask = box_scores >= score_threshold\n    max_boxes_tensor = K.constant(max_boxes, dtype='int32')\n    boxes_ = []\n    scores_ = []\n    for c in [15, 16]:\n        class_boxes = tf.boolean_mask(boxes, mask[:, c])\n        class_box_scores = tf.boolean_mask(box_scores[:, c], mask[:, c])\n        nms_index = tf.image.non_max_suppression(\n            class_boxes, class_box_scores, max_boxes_tensor, iou_threshold=iou_threshold)\n        class_boxes = K.gather(class_boxes, nms_index)\n        class_box_scores = K.gather(class_box_scores, nms_index)\n        boxes_.append(class_boxes)\n        scores_.append(class_box_scores)\n    boxes_ = K.concatenate(boxes_, axis=0)\n    scores_ = K.concatenate(scores_, axis=0)\n\n    return boxes_, scores_\n\nclass YOLO(object):\n    def __init__(self, config=None, train=False, **kwargs):\n        self.model_path = '../input/kerasyolov3/yolo.h5'\n        self.anchors_path = '../input/kerasyolov3/yolo_anchors.txt'\n        self.score = 0.3\n        self.iou = 0.45\n        self.model_image_size = (416, 416)\n        self.__dict__.update(kwargs)  # update with user overrides\n        self.anchors = self._get_anchors()\n        self.num_classes = 80\n        self.load_yolo_model()\n        self.ret_merge = True\n        self.ret_max_area = True\n        self.ret_max_score = True\n\n    def _get_anchors(self):\n        anchors_path = os.path.expanduser(self.anchors_path)\n        with open(anchors_path) as f:\n            anchors = f.readline()\n        anchors = [float(x) for x in anchors.split(',')]\n        return np.array(anchors).reshape(-1, 2)\n\n    def load_yolo_model(self):\n        model_path = os.path.expanduser(self.model_path)\n        assert model_path.endswith(\n            '.h5'), 'Keras model or weights must be a .h5 file.'\n\n        self.yolo_model = load_model(model_path, compile=False)\n        print('{} model, anchors, and classes loaded.'.format(model_path))\n\n    def compute_output(self, image_data, image_shape):\n        self.input_image_shape = tf.constant(image_shape)\n\n        boxes, scores = yolo_eval(self.yolo_model(image_data), self.anchors,\n                                           self.num_classes, self.input_image_shape,\n                                           score_threshold=self.score, iou_threshold=self.iou)\n        return boxes, scores\n\n    def detect_image(self, image):\n        h, w = image.shape[0],image.shape[1]\n        boxed_image = cv2_letterbox_image(image, tuple(reversed(self.model_image_size))).astype('float32')\n        image_data = np.expand_dims(boxed_image/255., 0)  # Add batch dimension.\n        out_boxes, out_scores = self.compute_output(image_data, [h, w])\n\n        res = {}\n        if len(out_boxes) == 0:\n            tmp =  (0, h-1, 0, w-1)\n            if self.ret_merge:\n                res['merge'] = tmp\n            if self.ret_max_area:\n                res['max_area'] = tmp\n            if self.ret_max_score:\n                res['max_score'] = tmp\n        else:\n            if self.ret_merge:\n                res['merge'] = get_bbox_merge(out_boxes, [h, w])\n            if self.ret_max_area:\n                res['max_area'] = get_bbox_max_area(out_boxes, [h, w])\n            if self.ret_max_score:  \n                res['max_score'] = get_bbox_max_score(out_boxes, out_scores, [h, w])\n        return res\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NOTE: for testing online\nyolo = YOLO()\ntest_bbox_info_merge = []\ntest_bbox_info_max_area = []\ntest_bbox_info_max_score = []\nstart = time.time()\nprint('Start to detect...')\ntest_names = test['Id']\nfor name in test_names:\n    fn = TEST_DIR + name + '.jpg'\n    img = cv2.imread(fn)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    res = yolo.detect_image(img)\n    test_bbox_info_merge.append(res['merge'])\n    test_bbox_info_max_area.append(res['max_area'])\n    test_bbox_info_max_score.append(res['max_score'])\nused_time = time.time() - start\nprint('The total process lasts', used_time, 's')\n\ntest['Bbox_merge'] = test_bbox_info_merge\ntest['Bbox_max_area'] = test_bbox_info_max_area\ntest['Bbox_max_score'] = test_bbox_info_max_score\n\ntest.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image(image_id, bbox, aug=None, image_folder=TRAIN_DIR):\n    img_path = os.path.join(image_folder, '{}.jpg'.format(image_id))\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    y_min, y_max, x_min, x_max = bbox\n    img = img[y_min:y_max, x_min:x_max,:]\n    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n    if aug:\n        img = aug(image=img)[\"image\"]\n    img = img[np.newaxis,...]\n    return img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataGenerator(Sequence):\n    def __init__(\n        self, \n        data,\n        target=None,\n        img_folder=TRAIN_DIR,\n        batch_size=BATCH_SIZE,\n        fine_tune=False,\n        extract_features=False,   \n        augment=False,\n        bbox_type='Bbox_max_score'\n    ):\n        self.data = data\n        self.target = target\n        self.batch_size = batch_size\n        self.fine_tune = fine_tune\n        self.augment = augment\n        self.extract_features = extract_features\n        self.img_folder = img_folder\n\n        if self.augment:\n            self.augmentation =A.Compose([A.CLAHE(), \n                                          A.HorizontalFlip(), \n                                          A.RandomRotate90(),\n                                          A.Transpose(),\n                                          A.ShiftScaleRotate(shift_limit=0.0425,\n                                                             scale_limit=0.40, \n                                                             rotate_limit=25), \n                                          A.HueSaturationValue()])\n        self.bbox_type = bbox_type\n           \n    \n    def __len__(self):\n        return math.ceil(len(self.data) / self.batch_size)\n    \n    def __getitem__(self, idx):\n        start_idx = idx * self.batch_size\n        end_idx = (idx + 1) * self.batch_size\n        if end_idx > len(self.data):\n            end_idx = len(self.data)\n        ids = self.data[start_idx : end_idx]['Id'].values\n        box = self.data[start_idx : end_idx][self.bbox_type].values\n\n        if self.augment:\n            images = np.concatenate([np.array(get_image(ids[i], box[i], aug=self.augmentation, \n                                                        image_folder=self.img_folder)) for i in range(end_idx-start_idx)], axis=0)\n        else:\n            images = np.concatenate([np.array(get_image(ids[i], box[i],image_folder=self.img_folder)) for i in range(end_idx-start_idx)], axis=0)\n        ###normalize as efficietnet\n        images = keras.applications.efficientnet.preprocess_input(images)\n        \n        if self.extract_features:\n            return images\n        \n        meta = np.array(self.data[start_idx : end_idx][FEATURES]).astype(np.float32)\n        if self.fine_tune:\n            return images, meta\n    \n        if self.target is None or not self.target.any():\n            return [images, meta]\n        \n        target = np.array(self.target[start_idx : end_idx]).astype(np.float32)\n        return [images, meta], target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_f = keras.applications.EfficientNetB0(weights='../input/efficientnetb0notop/efficientnetb0_notop.h5',\n                                            include_top=False, # remove the classification layer\n                                            pooling='avg') # apply GlobalAveragePooling\ntestX_ds = DataGenerator(test, img_folder=TEST_DIR, extract_features=True)\ntestXf = model_f.predict(testX_ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testAtt = test.drop(['Id', 'Bbox_merge', 'Bbox_max_area', 'Bbox_max_score'], axis=1)\ntestXn= pca.transform(testXf)\ntestAttXn = np.concatenate([testXf, testAtt],1)\nprint(testAttXn.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_submission = krrcv.predict(testAttXf)\nsubmission_df = pd.DataFrame({\n    'Id': test['Id'],\n    'Pawpularity': pred_submission,\n}).set_index('Id')\n\nsubmission_df.to_csv('submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}