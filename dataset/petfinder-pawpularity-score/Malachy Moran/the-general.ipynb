{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fast AI training\n\nAfter attempting the project in the PyTorch and PyTorch Lightning frameworks, I finally decided to learn fast AI.\n\nAfter running multiple controlled experiments, I used the following model structure as my \"backup\" submission. It was designed to be as generalizable as possible. It never scored the highest on the Public Leaderboard, but ended up being quite good once the other 75% of the data was released.\n\n### **THIS NOTEBOOK IS AN EXAMPLE ONLY**\n\nIn reality this notebook was split up into two parts, a training part (which had 12 models and took all of 9 hours) and a submission notebook. You will see in my notes below where this split occurs.","metadata":{}},{"cell_type":"code","source":"#First we need to import our models\nimport sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\nfrom timm import create_model","metadata":{"execution":{"iopub.status.busy":"2022-01-11T04:34:36.718037Z","iopub.execute_input":"2022-01-11T04:34:36.718417Z","iopub.status.idle":"2022-01-11T04:34:40.291668Z","shell.execute_reply.started":"2022-01-11T04:34:36.718326Z","shell.execute_reply":"2022-01-11T04:34:40.290873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport math\nimport pickle\nimport gc\n\nimport fastai\nfrom fastai.vision.all import *\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nimport torchvision\nfrom torchvision import transforms as T\nfrom torchvision.io import read_image\n\nimport sklearn\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-01-11T04:34:40.295117Z","iopub.execute_input":"2022-01-11T04:34:40.295642Z","iopub.status.idle":"2022-01-11T04:34:41.269016Z","shell.execute_reply.started":"2022-01-11T04:34:40.295611Z","shell.execute_reply":"2022-01-11T04:34:41.268289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class args:\n  folder_name = Path('../input/petfinder-pawpularity-score')\n  seed = 1212\n  num_splits = 3 #Normally I have this set to 12, because that is the most I can train in 9 hours\n  batch_size = 32\n  num_workers = 2\n  imagesize = 224\n  model_name = 'swin_large_patch4_window7_224'","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-01-11T04:34:41.27018Z","iopub.execute_input":"2022-01-11T04:34:41.271119Z","iopub.status.idle":"2022-01-11T04:34:41.275384Z","shell.execute_reply.started":"2022-01-11T04:34:41.271087Z","shell.execute_reply":"2022-01-11T04:34:41.274748Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#First we read in our training data and create a new column with the image file locations\ndf = pd.read_csv(args.folder_name/\"train.csv\")\ndf['filename'] = df['Id'].map(lambda x:str(args.folder_name/'train'/x)+'.jpg')\n\n#feature_cols = [col for col in df.columns if col not in ['Id', 'Pawpularity', 'filename']]","metadata":{"execution":{"iopub.status.busy":"2022-01-11T04:34:41.277972Z","iopub.execute_input":"2022-01-11T04:34:41.278528Z","iopub.status.idle":"2022-01-11T04:34:41.427856Z","shell.execute_reply.started":"2022-01-11T04:34:41.278492Z","shell.execute_reply":"2022-01-11T04:34:41.4271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets take a look and make sure the data is structured right\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-11T04:34:41.429037Z","iopub.execute_input":"2022-01-11T04:34:41.429293Z","iopub.status.idle":"2022-01-11T04:34:41.451259Z","shell.execute_reply.started":"2022-01-11T04:34:41.42926Z","shell.execute_reply":"2022-01-11T04:34:41.450613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Setting seeds for reproducibility\nseed = args.seed\n\ndevice = torch.device(\"cuda:0\")\n\nset_seed(seed, reproducible=True)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.use_deterministic_algorithms = True","metadata":{"execution":{"iopub.status.busy":"2022-01-11T04:34:41.452335Z","iopub.execute_input":"2022-01-11T04:34:41.452656Z","iopub.status.idle":"2022-01-11T04:34:41.457998Z","shell.execute_reply.started":"2022-01-11T04:34:41.452618Z","shell.execute_reply":"2022-01-11T04:34:41.457159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#BCE logits works best when we are working with numbers between 0 and 1, so we will rescale the Pawpularity\ndf = df.drop(columns=['Id'])\ndf = df.sample(frac=1).reset_index(drop=True)\ndf['norm_score'] = df['Pawpularity']/100\n\n#Rice rule\nnum_bins = int(np.ceil(2*((len(df))**(1./3))))\n\ndf['bins'] = pd.cut(df['norm_score'], bins=num_bins, labels=False)\n\n#Now we create our Folds. First we create a new column and assign it the value of -1\n#This allows us to tell if we have had an issue (because our process should change this number)\ndf['fold'] = -1\n\n#Now we assign a number representing the fold number(0-11) to the fold column based on a stratified random\n#sample\nstrat_kfold = StratifiedKFold(n_splits=args.num_splits, random_state=seed, shuffle=True)\nfor i, (_, train_index) in enumerate(strat_kfold.split(df.index, df['bins'])):\n    df.iloc[train_index, -1] = i","metadata":{"execution":{"iopub.status.busy":"2022-01-17T17:24:18.792403Z","iopub.execute_input":"2022-01-17T17:24:18.79277Z","iopub.status.idle":"2022-01-17T17:24:18.897879Z","shell.execute_reply.started":"2022-01-17T17:24:18.792675Z","shell.execute_reply":"2022-01-17T17:24:18.896475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This will be our training metric\ndef rmse(input,target):\n    return 100*torch.sqrt(F.mse_loss(torch.sigmoid(input.flatten()), target))","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-01-11T04:34:41.492646Z","iopub.execute_input":"2022-01-11T04:34:41.49305Z","iopub.status.idle":"2022-01-11T04:34:41.497639Z","shell.execute_reply.started":"2022-01-11T04:34:41.493016Z","shell.execute_reply":"2022-01-11T04:34:41.496895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_data(fold):\n    \n    #Here we create a copy of the data, and assign the fold whose number matches the current fold num to\n    #the validation set\n    df_f = df.copy()\n    df_f['is_valid'] = (df_f['fold'] == fold)\n    \n    dls = ImageDataLoaders.from_df(df_f, #pass in train DataFrame\n                               valid_pct=.2, #80-20 train-validation random split\n                               valid_col='is_valid', #this is the column we assigned\n                               seed=args.seed, #seed\n                               fn_col='filename', #filename is the column in the df that has the image paths\n                               label_col='norm_score', #our scaled labels\n                               y_block=RegressionBlock, #The type of target\n                               bs= args.batch_size, #pass in batch size\n                               num_workers= args.num_workers,\n                               item_tfms= Resize(args.imagesize), #We will only be resizing items\n                               batch_tfms=setup_aug_tfms( #All of these are to help with generalization\n                                                        [Brightness(), Contrast(), Hue(), \n                                                          Saturation(), \n                                                          RandomErasing(p=.3, sh =.1, max_count = 2)])\n                                  )\n    \n    return dls","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-01-11T04:34:41.500417Z","iopub.execute_input":"2022-01-11T04:34:41.500616Z","iopub.status.idle":"2022-01-11T04:34:41.509363Z","shell.execute_reply.started":"2022-01-11T04:34:41.500594Z","shell.execute_reply":"2022-01-11T04:34:41.50798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now we define a new function to get the data and create a new model\n\ndef get_learner(fold_num):\n    data = get_data(fold_num)\n    \n    #After a lot of trial and error, SWINL turned out to be the best fitting model\n    model = create_model(args.model_name, pretrained=True, num_classes=data.c)\n    \n    #We convert the learner to fp16 to speed training and allow a larger batch size\n    learn = Learner(data, model, loss_func=BCEWithLogitsLossFlat(), metrics=rmse).to_fp16()\n\n    return learn","metadata":{"execution":{"iopub.status.busy":"2022-01-11T04:35:33.486405Z","iopub.execute_input":"2022-01-11T04:35:33.486938Z","iopub.status.idle":"2022-01-11T04:35:33.49188Z","shell.execute_reply.started":"2022-01-11T04:35:33.486879Z","shell.execute_reply":"2022-01-11T04:35:33.490783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(args.num_splits):\n    \n    #Let us know where we are at\n    print(f'Fold {i} results')\n    \n    #Get our model\n    learn = get_learner(fold_num=i)\n    \n    #The original version of this notebook had the function outside the loop, by moving it inside the loop\n    #We allow each learner to have it's own LR that works best for that specific fold\n    lr = learn.lr_find(end_lr=3e-2)\n    \n    \n    #We will fit 3-12 models, with 1/3 Standard, 1/3 with mixup, and 1/3 with cutmix\n    if i % 3 == 0:\n        print('No Mixing')\n        #Learner is fit for a maximum of 5 epochs, but stops after 2 consecutive epochs with no improvement\n        #To RMSE\n        learn.fit_one_cycle(5, lr.valley, cbs=[SaveModelCallback(), \n                                           EarlyStoppingCallback(monitor='rmse', \n                                                                 comp=np.less, patience=2)])\n    if i % 3 == 1:\n        print('Mixup')\n        mixup = MixUp()\n        learn.fit_one_cycle(5, lr.valley, cbs=[mixup, SaveModelCallback(), \n                                       EarlyStoppingCallback(monitor='rmse', \n                                                             comp=np.less, patience=2)])\n    if i % 3 == 2:\n        print('CutMix')\n        cutmix = CutMix()\n        learn.fit_one_cycle(5, lr.valley, cbs=[cutmix, SaveModelCallback(), \n                                       EarlyStoppingCallback(monitor='rmse', \n                                                             comp=np.less, patience=2)])\n    \n    #Fastai does not allow us to export the model in fp 16, so we need to change it back to fp32\n    learn.to_fp32()\n\n    learn.export(f'{i}best_weights.pkl')\n\n    learn.recorder.plot_loss()\n    \n    #Memory Management\n    del learn, lr\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-11T04:35:38.409664Z","iopub.execute_input":"2022-01-11T04:35:38.409956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After all of this training is complete, the result will be a set of 3-12 model weights. A separate notebook was used to make predictions. This notebook would import each of the 3-12 models, make the predictions and average them. The code for this notebook is below, just for reference.\n\nThe notebook had the same imports as this notebook (sys, timm, the timm dataset, and all the packages)","metadata":{}},{"cell_type":"code","source":"#Creates a folder for our SWINL Model in the os path and copies the model from the timm data to this location\n#This prevents the notebook from looking on the internet for the model\nif not os.path.exists('/root/.cache/torch/hub/checkpoints/'):\n    os.makedirs('/root/.cache/torch/hub/checkpoints/')\n!cp '../input/swin-transformer/swin_large_patch4_window7_224_22kto1k.pth' '/root/.cache/torch/hub/checkpoints/swin_large_patch4_window7_224_22kto1k.pth'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load our test data\ntstdf = pd.read_csv(args.folder_name/'test.csv')\ntstdf['filename'] = tstdf['Id'].map(lambda x:str(args.folder_name/'test'/x)+'.jpg')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating a column of fake Pawpularity scores to overwright later\ntstdf['Pawpularity'] = [1]*len(tstdf)\ntstdf = tstdf.drop(columns=['Id'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_learner(fold_num):\n    data = get_data(fold_num)\n    \n    #Imports the learner based on the fold num\n    learn = load_learner(f'../input/fastai-swinl/{fold_num}best_weights.pkl', cpu = False).to_fp16()\n    \n    return learn","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We start by initializing the predictions list\nall_preds = []\n\nfor i in range(args.num_splits):\n    \n    #First we pull in our model from the checkpoint\n    learn = get_learner(fold_num=i)\n    \n    #We define our dataloader as the same as before\n    dls = ImageDataLoaders.from_df(df,#pass in train DataFrame\n                               valid_pct=.2, #80-20 train-validation random split\n                               seed=args.seed, #seed\n                               fn_col='filename', #filename is the column in the df that has the image paths\n                               label_col='norm_score', #our scaled labels\n                               y_block=RegressionBlock, #The type of target\n                               bs= args.batch_size, #pass in batch size\n                               num_workers= args.num_workers,\n                               item_tfms= Resize(args.imagesize), #We will only be resizing items\n                               batch_tfms=setup_aug_tfms( #All of these are to help with generalization\n                                                        [Brightness(), Contrast(), Hue(), \n                                                          Saturation(), \n                                                          RandomErasing(p=.3, sh =.1, max_count = 2)])\n                                  )\n    \n    #We assign our test set based on the data loader assigned above.\n    test_dl = dls.test_dl(tstdf)\n    \n    #After lots of experimentation, the default test time augmentation parameters were best\n    preds, _ = learn.tta(dl=test_dl)\n    \n    #add the predictions to our data frame\n    all_preds.append(preds)\n    \n    #Memory Management\n    del learn\n\n    torch.cuda.empty_cache()\n\n    gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#First we create the dataframe we will use to submit the sample\nsample_df = pd.read_csv(args.folder_name/'sample_submission.csv')\n\n#We take the simple mean of all the predictions.\npreds = np.mean(np.stack(all_preds), axis=0)\n\n#Scale them back up from the \"norm score\"\nsample_df['Pawpularity'] = preds*100\n\n#And save\nsample_df.to_csv('submission.csv',index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Just to check\npd.read_csv('submission.csv').head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Reference**\n###  [Petfinder Pawpularity EDA & fastai starter üê±üê∂](https://www.kaggle.com/tanlikesmath/petfinder-pawpularity-eda-fastai-starter)\n###  [Petfinder& fastai with DataAugmentation KFold 10](https://www.kaggle.com/bobber/petfinder-fastai-with-dataaugmentation-kfold-10)\n###  [Lovely Doggo with Bonky (fastai &timm)](https://www.kaggle.com/warotjanpinitrat/lovely-doggo-with-bonky-fastai-timm)","metadata":{}}]}