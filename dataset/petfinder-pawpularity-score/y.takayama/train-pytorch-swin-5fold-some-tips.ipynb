{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Overview\nThis notebook is based on the following notebook.\n\nhttps://www.kaggle.com/ytakayama/pytorch-baseline-try-to-reproduce-fastai-notebook\n\nI added some tips to improve scores.\n- evaluate at every N steps per epoch\n\nIn this notebook, model is evaluated every 62 steps after second epoch.\n- OneCycleLR\n- change how to resize: Fastai's strong point?\n> we resize so that the larger dimension is match and crop (randomly on the training set, center crop for the validation set)\nhttps://docs.fast.ai/vision.augment.html#Resize\n\nImages are cropped so that their aspect ratio is maintaining when resizing.\n\nMy experiments in the past had dared to exclude the good resize method of fastai most of public notebooks include.\n\nhttps://www.kaggle.com/ytakayama/fastai-very-simple-baseline\n\nCaution: Time and status viewed at progress bar of training includes validation time.","metadata":{}},{"cell_type":"markdown","source":"## libraries","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings(\"ignore\")\n#general\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import RobustScaler\nimport pickle\nfrom tqdm.auto import tqdm\nfrom collections import defaultdict\nimport os\nimport numpy as np\nimport pandas as pd\nimport random\nimport gc\nimport cv2\ngc.enable()\nimport glob\npd.set_option('display.max_columns', None) \nfrom sklearn.linear_model import RidgeCV\n\n# visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# augmentation\nfrom albumentations.pytorch import ToTensorV2\nimport albumentations as A\n\n# deep learning\nimport timm\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, OneCycleLR, CosineAnnealingLR, ReduceLROnPlateau, StepLR, LambdaLR\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport imageio\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\n\n# metrics\nfrom sklearn.metrics import mean_squared_error","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-03T02:17:12.311876Z","iopub.execute_input":"2022-01-03T02:17:12.312215Z","iopub.status.idle":"2022-01-03T02:17:18.988364Z","shell.execute_reply.started":"2022-01-03T02:17:12.312132Z","shell.execute_reply":"2022-01-03T02:17:18.987248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not os.path.exists('/root/.cache/torch/hub/checkpoints/'):\n    os.makedirs('/root/.cache/torch/hub/checkpoints/')\n!cp '../input/swin-transformer/swin_large_patch4_window7_224_22kto1k.pth' '/root/.cache/torch/hub/checkpoints/swin_large_patch4_window7_224_22kto1k.pth'","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-03T02:17:18.991028Z","iopub.execute_input":"2022-01-03T02:17:18.991343Z","iopub.status.idle":"2022-01-03T02:17:28.192796Z","shell.execute_reply.started":"2022-01-03T02:17:18.991297Z","shell.execute_reply":"2022-01-03T02:17:28.191675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"class Config:\n    model_name = \"swint_large224\"\n    base_dir = \"/content/drive/MyDrive/petfinder\"\n    data_dir = \"../input/petfinder-pawpularity-score/\"\n    meta_data_dir = \"../input/trainmeta/\"\n    model_dir = \".\"\n    output_dir = \".\"\n    img_train_dir = os.path.join(data_dir, \"train\")\n    img_test_dir = os.path.join(data_dir, \"test\")\n    random_seed = 555\n    n_epoch = 5\n    n_fold = 5\n    tta = True # calculate cv score in case TTA is executed\n    tta_times = 4\n    tta_beta = 1 / tta_times\n    model_path = \"swin_large_patch4_window7_224\"\n    pretrained = True\n    inp_channels = 3\n    im_size =  224\n    lr = 2e-5\n    opt_wd_non_norm_bias = 0.01\n    opt_wd_norm_bias = 0\n    opt_beta1 = 0.9\n    opt_beta2 = 0.99\n    opt_eps = 1e-5\n    batch_size = 16\n    epoch_step_valid = 3\n    steps_per_epoch = 62\n    num_workers = 8\n    out_features = 1\n    dropout = 0\n    aug_decay_epoch = 4\n    mixup = False\n    if mixup:\n        mixup_epoch = n_epoch\n    else:\n        mixup_epoch = 0\n    mixup_alpha =0.2\n    scheduler_name = \"OneCycleLR\" #OneCycleLR\n    reduce_lr_factor = 0.6\n    reduce_lr_patience = 1\n    T_0 = 4 \n    T_max =4\n    T_mult =1\n    min_lr = 1e-7\n    max_lr =2e-5\n    is_debug = False\n    if is_debug:\n        n_epoch = 1\n        aug_decay_epoch = 1\n        n_fold = 2\n        n_sample_debug = 500\n        tta_times = 2\n        tta_beta = 1 / tta_times","metadata":{"execution":{"iopub.status.busy":"2022-01-03T02:17:28.195396Z","iopub.execute_input":"2022-01-03T02:17:28.195802Z","iopub.status.idle":"2022-01-03T02:17:28.210181Z","shell.execute_reply.started":"2022-01-03T02:17:28.195734Z","shell.execute_reply":"2022-01-03T02:17:28.209117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## set up environments & prepare data\n\n- set_seed\nSet random seed for random, torch, and numpy\n\nhttps://docs.fast.ai/torch_core.html#set_seed\n\nif reproducible is True:\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed=Config.random_seed):\n    #os.environ['PYTHONSEED'] = str(seed)\n    np.random.seed(seed%(2**32-1))\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic =True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything()\n# device optimization\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n\nprint(f'Using device: {device}')","metadata":{"execution":{"iopub.status.busy":"2022-01-03T02:17:28.213321Z","iopub.execute_input":"2022-01-03T02:17:28.213686Z","iopub.status.idle":"2022-01-03T02:17:28.504042Z","shell.execute_reply.started":"2022-01-03T02:17:28.213642Z","shell.execute_reply":"2022-01-03T02:17:28.503108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_train_dir = os.path.join(Config.data_dir, 'train')\ndef return_imgfilepath(name, folder=img_train_dir):\n    path = os.path.join(folder, f'{name}.jpg')\n    return path\n\ntrain_file_path = os.path.join(Config.data_dir, 'train.csv')\ntrain_df = pd.read_csv(train_file_path)\n\n# set image filepath\ntrain_df['file_path'] = train_df['Id'].progress_apply(lambda x: return_imgfilepath(x))\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-03T02:17:28.506985Z","iopub.execute_input":"2022-01-03T02:17:28.507573Z","iopub.status.idle":"2022-01-03T02:17:28.668431Z","shell.execute_reply.started":"2022-01-03T02:17:28.507501Z","shell.execute_reply":"2022-01-03T02:17:28.667413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## StratifiedKFold","metadata":{}},{"cell_type":"code","source":"if Config.is_debug:\n    train_df = train_df.sample(500).reset_index(drop = True)\ntrain_df['norm_score'] = train_df['Pawpularity'] / 100\n#Sturges' rule\nnum_bins = int(np.floor(1+(3.3)*(np.log2(len(train_df)))))\ntrain_df['bins'] = pd.cut(train_df['norm_score'], bins=num_bins, labels=False)\ntrain_df['fold'] = -1\n\nskf = StratifiedKFold(n_splits = Config.n_fold, shuffle=True, random_state =Config.random_seed)\nfor i, (_, train_index) in enumerate(skf.split(train_df.index, train_df['bins'])):\n    train_df.iloc[train_index, -1] = i\n    \ntrain_df['fold'] = train_df['fold'].astype('int')\n\ntrain_df.fold.value_counts().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2022-01-03T02:17:28.670423Z","iopub.execute_input":"2022-01-03T02:17:28.670839Z","iopub.status.idle":"2022-01-03T02:17:29.097176Z","shell.execute_reply.started":"2022-01-03T02:17:28.670795Z","shell.execute_reply":"2022-01-03T02:17:29.096209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[train_df['fold']==0].head()","metadata":{"execution":{"iopub.status.busy":"2022-01-03T02:17:29.098934Z","iopub.execute_input":"2022-01-03T02:17:29.099413Z","iopub.status.idle":"2022-01-03T02:17:29.124505Z","shell.execute_reply.started":"2022-01-03T02:17:29.099369Z","shell.execute_reply":"2022-01-03T02:17:29.123543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset & augmentation\n\n### PetDataset\nchannels of image converted to 0-1\n\n### get_transforms\nOnly resizing is applied to both train and valid data ","metadata":{}},{"cell_type":"code","source":"class PetDataset(Dataset):\n    def __init__(self, image_filepaths, targets, transform=None):\n        self.image_filepaths = image_filepaths\n        self.targets = targets\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.image_filepaths)\n\n    def __getitem__(self, idx):\n        image_filepath = self.image_filepaths[idx]\n        with open(image_filepath, 'rb') as f:\n            image = Image.open(f)\n            image_rgb = image.convert('RGB')\n        image = np.array(image_rgb)\n\n        if self.transform is not None:\n            image = self.transform(image = image)[\"image\"]\n        \n        image = image / 255 # convert to 0-1\n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        target = self.targets[idx]\n\n        image = torch.tensor(image, dtype = torch.float)\n        target = torch.tensor(target, dtype = torch.float)\n        return image, target","metadata":{"execution":{"iopub.status.busy":"2022-01-03T02:17:29.126421Z","iopub.execute_input":"2022-01-03T02:17:29.12704Z","iopub.status.idle":"2022-01-03T02:17:29.138362Z","shell.execute_reply.started":"2022-01-03T02:17:29.126994Z","shell.execute_reply":"2022-01-03T02:17:29.13734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMAGENET_MEAN = [0.485, 0.456, 0.406]  # RGB\nIMAGENET_STD = [0.229, 0.224, 0.225]  # RGB\ndef get_train_transforms(epoch, dim = Config.im_size):\n    return A.Compose(\n        [             \n            # resize like Resize in fastai\n            A.SmallestMaxSize(max_size=dim, p=1.0),\n            A.RandomCrop(height=dim, width=dim, p=1.0),\n            A.VerticalFlip(p = 0.5),\n            A.HorizontalFlip(p = 0.5)\n            #A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n        ]\n  )\n\ndef get_inference_fixed_transforms(mode=0, dim = Config.im_size):\n    if mode == 0: # do not original aspects, colors and angles\n        return A.Compose([\n                A.SmallestMaxSize(max_size=dim, p=1.0),\n                A.CenterCrop(height=dim, width=dim, p=1.0),\n                #A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n            ], p=1.0)\n    elif mode == 1:\n        return A.Compose([\n                A.SmallestMaxSize(max_size=dim, p=1.0),\n                A.CenterCrop(height=dim, width=dim, p=1.0),\n                #A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),,\n                A.VerticalFlip(p = 1.0)\n            ], p=1.0)    \n    elif mode == 2:\n        return A.Compose([\n                A.SmallestMaxSize(max_size=dim, p=1.0),\n                A.CenterCrop(height=dim, width=dim, p=1.0),\n                #A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n                A.HorizontalFlip(p = 1.0)\n            ], p=1.0)\n    elif mode == 3:\n        return A.Compose([\n                A.SmallestMaxSize(max_size=dim, p=1.0),\n                A.CenterCrop(height=dim, width=dim, p=1.0),\n                #A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n                A.Transpose(p=1.0)\n            ], p=1.0)\n        \ndef get_inference_random_transforms(mode=0, dim = Config.im_size):\n    if mode == 0: # do not original aspects, colors and angles\n        return A.Compose([\n                A.SmallestMaxSize(max_size=dim, p=1.0),\n                A.CenterCrop(height=dim, width=dim, p=1.0),\n                #A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n            ], p=1.0)\n    else:\n        return A.Compose(\n            [            \n                A.SmallestMaxSize(max_size=dim, p=1.0),\n                A.CenterCrop(height=dim, width=dim, p=1.0),\n                A.VerticalFlip(p = 0.5),\n                A.HorizontalFlip(p = 0.5)\n                #A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n            ]\n      )","metadata":{"execution":{"iopub.status.busy":"2022-01-03T02:17:29.142159Z","iopub.execute_input":"2022-01-03T02:17:29.142454Z","iopub.status.idle":"2022-01-03T02:17:29.159952Z","shell.execute_reply.started":"2022-01-03T02:17:29.142422Z","shell.execute_reply":"2022-01-03T02:17:29.15904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## model","metadata":{}},{"cell_type":"code","source":"class PetNet(nn.Module):\n    def __init__(\n        self,\n        model_name = Config.model_path,\n        out_features = Config.out_features,\n        inp_channels=Config.inp_channels,\n        pretrained=Config.pretrained\n    ):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels, num_classes = out_features)\n    \n    def forward(self, image):\n        output = self.model(image)\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-01-03T02:17:29.165155Z","iopub.execute_input":"2022-01-03T02:17:29.165568Z","iopub.status.idle":"2022-01-03T02:17:29.174905Z","shell.execute_reply.started":"2022-01-03T02:17:29.165505Z","shell.execute_reply":"2022-01-03T02:17:29.173845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## helper function\n\n### divice_norm_bias\nfunction to divide parameters at BatchNorm layer and all bias or not.\n\n### usr_rmse_score\ncalculate competition metrics\n\nreference\n- https://docs.fast.ai/learner.html#Learner\n- https://docs.fast.ai/optimizer.html#Adam\n- https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html\n- https://docs.fast.ai/losses.html#BCEWithLogitsLossFlat\n","metadata":{}},{"cell_type":"code","source":"def divice_norm_bias(model): \n    norm_bias_params = []\n    non_norm_bias_params = []\n    except_wd_layers = ['norm', '.bias']\n    for n, p in model.model.named_parameters():\n        if any([nd in n for nd in except_wd_layers]):\n            norm_bias_params.append(p)\n        else:\n            non_norm_bias_params.append(p)\n    return norm_bias_params, non_norm_bias_params\n\ndef usr_rmse_score(output, target):\n    y_pred = torch.sigmoid(output).cpu()\n    y_pred = y_pred.detach().numpy()*100\n    target = target.cpu()*100\n    \n    return mean_squared_error(target, y_pred, squared=False)\n\ndef rmse_oof(_oof_df, fold=None):\n    oof_df = _oof_df.copy()\n    if fold is not None:\n        oof_df = oof_df[oof_df[\"fold\"] == fold]\n    target = oof_df['Pawpularity'].values\n    y_pred = oof_df['pred'].values\n    if fold is not None:\n        print(f'fold {fold}: {mean_squared_error(target, y_pred, squared=False)}')\n    else:\n        print(f'overall: {mean_squared_error(target, y_pred, squared=False)}')\n\nclass MetricMonitor:\n    def __init__(self, float_precision=3):\n        self.float_precision = float_precision\n        self.reset()\n\n    def reset(self):\n        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n\n    def update(self, metric_name, val):\n        metric = self.metrics[metric_name]\n\n        metric[\"val\"] += val\n        metric[\"count\"] += 1\n        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n\n    def __str__(self):\n        return \" | \".join(\n            [\n                \"{metric_name}: {avg:.{float_precision}f}\".format(\n                    metric_name=metric_name, avg=metric[\"avg\"],\n                    float_precision=self.float_precision\n                )\n                for (metric_name, metric) in self.metrics.items()\n            ]\n        )\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-01-03T02:17:29.17708Z","iopub.execute_input":"2022-01-03T02:17:29.177797Z","iopub.status.idle":"2022-01-03T02:17:29.195234Z","shell.execute_reply.started":"2022-01-03T02:17:29.177751Z","shell.execute_reply":"2022-01-03T02:17:29.193676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_scheduler(optimizer):\n    scheduler = None\n    if Config.scheduler_name == 'CosineAnnealingWarmRestarts':\n        scheduler = CosineAnnealingWarmRestarts(\n            optimizer,\n            T_0=Config.T_0,\n            eta_min=Config.min_lr,\n            last_epoch=-1\n        )\n    elif Config.scheduler_name == 'OneCycleLR':\n        # div=25\n        # initial_lr =max_lr/div\n        # default last_lr =initial lr / final_div_factor(10000) = max_lr \n        # in case fastai  default last_lr =max_lr / div_final(100000)\n        scheduler = OneCycleLR(\n            optimizer,\n            max_lr=Config.max_lr,\n            pct_start = 0.25, # same as fastai, defaut 0.3\n            steps_per_epoch=int(((Config.n_fold - 1) * train_df.shape[0]) / (Config.n_fold * Config.batch_size)) + 1,\n            epochs = Config.n_epoch\n        )\n\n    elif Config.scheduler_name == 'CosineAnnealingLR':\n        scheduler = CosineAnnealingLR(\n            optimizer,\n            T_max=Config.T_max,\n            eta_min=Config.min_lr,\n            last_epoch=-1\n        )\n    elif Config.scheduler_name == 'ReduceOnPlateauLR':\n        scheduler = ReduceLROnPlateau(\n            optimizer,\n            mode = 'min',\n            factor=Config.reduce_lr_factor,\n            patience=Config.reduce_lr_patience,\n            verbose = True\n        )\n    return scheduler","metadata":{"execution":{"iopub.status.busy":"2022-01-03T02:17:29.19727Z","iopub.execute_input":"2022-01-03T02:17:29.197769Z","iopub.status.idle":"2022-01-03T02:17:29.211656Z","shell.execute_reply.started":"2022-01-03T02:17:29.197689Z","shell.execute_reply":"2022-01-03T02:17:29.21045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def valid_fn(y_valid, X_valid_paths, model, criterion, epoch):\n    model.eval()\n    #model = layer_freeze(model)\n    tta_mode = 0\n    test_targets = []\n    test_preds = []\n    valid_dataset = PetDataset(\n      image_filepaths = X_valid_paths,\n      targets = y_valid,\n      transform = get_inference_fixed_transforms(tta_mode)\n    )\n    valid_loader = DataLoader(\n      valid_dataset,\n      batch_size = Config.batch_size,\n      shuffle = False,\n      num_workers = Config.num_workers,\n      pin_memory = True\n    )\n    metric_monitor = MetricMonitor()\n    stream = tqdm(valid_loader)\n    for i, (images, target) in enumerate(stream, start = 1):\n        images = images.to(device, non_blocking = True).float()\n        target = target.to(device, non_blocking = True).float().view(-1, 1)\n        with torch.no_grad():\n            output = model(images)\n        loss = criterion(output, target)\n        rmse_score = usr_rmse_score(output, target)\n        metric_monitor.update('Loss', loss.item())\n        metric_monitor.update('RMSE', rmse_score)\n        stream.set_description(f\"Epoch: {epoch:02}. Valid. {metric_monitor}\")\n\n        targets = (target.detach().cpu().numpy() * 100).ravel().tolist()\n        pred = (torch.sigmoid(output).detach().cpu().numpy() * 100).ravel().tolist()\n\n        test_preds.extend(pred)\n        test_targets.extend(targets)\n    test_preds = np.array(test_preds)\n    test_targets = np.array(test_targets)\n    del valid_loader, valid_dataset, target, output\n    gc.collect()\n    torch.cuda.empty_cache()\n    return test_targets, test_preds","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-03T02:17:29.21485Z","iopub.execute_input":"2022-01-03T02:17:29.216534Z","iopub.status.idle":"2022-01-03T02:17:29.231815Z","shell.execute_reply.started":"2022-01-03T02:17:29.216496Z","shell.execute_reply":"2022-01-03T02:17:29.230488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tta_fn(y_valid, X_valid_paths, model, criterion, epoch):\n    model.eval()\n    #model = layer_freeze(model)\n    test_targets = []\n    test_preds = []\n    for tta_mode in range(Config.tta_times):\n        print(f'tta mode:{tta_mode}')\n        valid_dataset = PetDataset(\n          image_filepaths = X_valid_paths,\n          targets = y_valid,\n          transform = get_inference_fixed_transforms(tta_mode)\n        )\n        valid_loader = DataLoader(\n          valid_dataset,\n          batch_size = Config.batch_size,\n          shuffle = False,\n          num_workers = Config.num_workers,\n          pin_memory = True\n        )\n        metric_monitor = MetricMonitor()\n        stream = tqdm(valid_loader)\n        tta_preds = []\n        for i, (images, target) in enumerate(stream, start = 1):\n            images = images.to(device, non_blocking = True).float()\n            target = target.to(device, non_blocking = True).float().view(-1, 1)\n            with torch.no_grad():\n                output = model(images)\n\n            targets = (target.detach().cpu().numpy() * 100).ravel().tolist()\n            pred = (torch.sigmoid(output).detach().cpu().numpy() * 100).ravel().tolist()\n            loss = criterion(output, target)\n            rmse_score = usr_rmse_score(output, target)\n            metric_monitor.update('Loss', loss.item())\n            metric_monitor.update('RMSE', rmse_score)\n            stream.set_description(f\"Epoch: {epoch:02}. Valid. {metric_monitor}\")\n\n            tta_preds.extend(pred)\n            if tta_mode == 0:\n                test_targets.extend(targets)\n        test_preds.append(tta_preds)\n    test_preds = np.array(test_preds)\n    # default preds * tta_beta + aug_preds mean * ( 1 - tta_beta)\n    #print(test_preds.shape)\n    final_preds = Config.tta_beta * test_preds[0] + ( 1 - Config.tta_beta) * np.mean(test_preds[1:], axis =0)\n    test_targets = np.array(test_targets)\n    del valid_loader, valid_dataset, target, output\n    gc.collect()\n    torch.cuda.empty_cache()\n    return test_targets, final_preds","metadata":{"execution":{"iopub.status.busy":"2022-01-03T02:17:29.233795Z","iopub.execute_input":"2022-01-03T02:17:29.234215Z","iopub.status.idle":"2022-01-03T02:17:29.252004Z","shell.execute_reply.started":"2022-01-03T02:17:29.234151Z","shell.execute_reply":"2022-01-03T02:17:29.25068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## training loop\n\n- scheduler: OneCycleLR\n- optimizer: AdamW\nhyper parameters are the same as Adam in Fastai","metadata":{}},{"cell_type":"code","source":"def training_loop(filepaths, targets):\n    skf = StratifiedKFold(n_splits = Config.n_fold, shuffle=True, random_state=Config.random_seed)\n    oof_df = pd.DataFrame()\n    for i_fold, (train_idx, valid_idx) in enumerate(skf.split(filepaths, target_bins)):\n        print(f'=== fold {i_fold}: training ===')\n        \"\"\"\n        separate train/valid data \n        \"\"\"\n        X_train_paths = filepaths[train_idx]\n        y_train = targets[train_idx]\n        X_valid_paths = filepaths[valid_idx]\n        y_valid = targets[valid_idx]\n        valid_ids = ids[valid_idx]\n        \"\"\"\n        prepare dataset\n        \"\"\"\n        train_dataset = PetDataset(\n          image_filepaths = X_train_paths,\n          targets = y_train,\n          transform = get_train_transforms(0)\n        )\n\n        \"\"\"\n        create dataloader\n        \"\"\"\n        train_loader = DataLoader(\n          train_dataset,\n          batch_size = Config.batch_size,\n          shuffle = True,\n          num_workers = Config.num_workers,\n          pin_memory = True\n        )\n\n        \"\"\"\n        instantiate model, cost function and optimizer\n        \"\"\"\n        model = PetNet()\n        model = model.to(device)\n        criterion = nn.BCEWithLogitsLoss()\n        norm_bias_params, non_norm_bias_params = divice_norm_bias(model)\n        #print(f\"norm bias params: {len(norm_bias_params)}, non norm bias params: {len(non_norm_bias_params)}\")\n        optimizer = torch.optim.AdamW(\n          [\n              {'params': norm_bias_params, 'weight_decay': Config.opt_wd_norm_bias},\n              {'params': non_norm_bias_params, 'weight_decay': Config.opt_wd_non_norm_bias},\n          ],\n          betas=(Config.opt_beta1, Config.opt_beta2),\n          eps=Config.opt_eps,\n          lr = Config.lr,\n          amsgrad = False\n        )\n        scheduler = get_scheduler(optimizer)\n        \"\"\"\n        train / valid loop\n        \"\"\"\n        best_rmse = np.inf\n        scaler = GradScaler()\n        for epoch in range(1, Config.n_epoch + 1):\n            print(f'=== fold:{i_fold} epoch: {epoch}: training ===')\n            \n            metric_monitor = MetricMonitor()\n            stream = tqdm(train_loader)\n\n            for batch_idx, (images, target) in enumerate(stream, start = 1):\n            #for batch_idx, (images, target) in enumerate(train_loader):\n                model.train()\n                #train_fn(train_loader, model, criterion, optimizer, epoch, params, scheduler)\n                if Config.mixup_epoch >= epoch:\n                    images, target_a, target_b, lam = mixup_data(images, target.view(-1 ,1))\n                    images = images.to(device, dtype = torch.float)\n                    target_a = target_a.to(device, dtype = torch.float)\n                    target_b = target_b.to(device, dtype = torch.float)\n                else:\n                    images = images.to(device, non_blocking = True).float()\n                    target = target.to(device, non_blocking = True).float().view(-1, 1)\n                optimizer.zero_grad()\n                with autocast(): # mixed precision\n                    output = model(images)\n\n                    if Config.mixup_epoch >= epoch:\n                        loss = mixup_criterion(criterion, output, target_a, target_b, lam)\n                    else:\n                        loss = criterion(output, target)\n                rmse_score = usr_rmse_score(output, target)\n                metric_monitor.update('Loss', loss.item())\n                metric_monitor.update('RMSE', rmse_score)\n                stream.set_description(f'Epoch: {epoch:02}. Train. {metric_monitor}')\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n\n                if (scheduler is not None) & (Config.scheduler_name != 'ReduceOnPlateauLR') :\n                    scheduler.step()\n            \n                if ( ( ( batch_idx % Config.steps_per_epoch == 0) & (epoch >= Config.epoch_step_valid) ) | ( batch_idx == len(train_loader) ) ):\n                    valid_targets, preds = valid_fn(y_valid, X_valid_paths, model, criterion, epoch)\n                    valid_rmse = round(mean_squared_error(valid_targets, preds, squared=False), 3)\n                    print(f'epoch: {epoch}, batch: {batch_idx}/{len(train_loader)}, valid rmse: {valid_rmse}')\n                    if Config.scheduler_name == 'ReduceOnPlateauLR':\n                        scheduler.step(valid_rmse)\n\n                    if valid_rmse < best_rmse:\n                        best_rmse = valid_rmse\n                        model_name = Config.model_path\n                        torch.save(model.state_dict(), f'{Config.model_dir}/{model_name}_fold{i_fold}.pth')\n                        print(\"saved model.\")\n                        _oof_df = pd.DataFrame(data={'Id': valid_ids, 'pred':preds, 'fold': i_fold, 'Pawpularity':valid_targets}, index=valid_idx)\n\n        del model, output, train_loader, train_dataset\n        gc.collect()\n        \n        torch.cuda.empty_cache()\n        oof_df = pd.concat([oof_df, _oof_df])\n    return oof_df.sort_values('Id')","metadata":{"execution":{"iopub.status.busy":"2022-01-03T02:17:29.254152Z","iopub.execute_input":"2022-01-03T02:17:29.254534Z","iopub.status.idle":"2022-01-03T02:17:29.283796Z","shell.execute_reply.started":"2022-01-03T02:17:29.254455Z","shell.execute_reply":"2022-01-03T02:17:29.282567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ids = train_df['Id'].values\nfilepaths = train_df['file_path'].values\ntargets = train_df['Pawpularity'].values /100\nnum_bins = int(np.floor(1+(3.3)*(np.log2(len(train_df)))))\ntarget_bins = pd.cut(targets, bins=num_bins, labels=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T02:17:29.285647Z","iopub.execute_input":"2022-01-03T02:17:29.286395Z","iopub.status.idle":"2022-01-03T02:17:29.300378Z","shell.execute_reply.started":"2022-01-03T02:17:29.286348Z","shell.execute_reply":"2022-01-03T02:17:29.299279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_df = training_loop(filepaths, targets)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T02:17:29.302155Z","iopub.execute_input":"2022-01-03T02:17:29.302601Z","iopub.status.idle":"2022-01-03T02:18:40.580422Z","shell.execute_reply.started":"2022-01-03T02:17:29.302557Z","shell.execute_reply":"2022-01-03T02:18:40.579389Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TTA","metadata":{}},{"cell_type":"code","source":"def tta_loop(filepaths, targets):\n    skf = StratifiedKFold(n_splits = Config.n_fold, shuffle=True, random_state=Config.random_seed)\n    oof_df = pd.DataFrame()\n    for i_fold, (train_idx, valid_idx) in enumerate(skf.split(filepaths, target_bins)):\n        print(f'=== fold {i_fold}: validation ===')\n        \"\"\"\n        separate valid data \n        \"\"\"\n        X_valid_paths = filepaths[valid_idx]\n        y_valid = targets[valid_idx]\n        valid_ids = ids[valid_idx]\n        \"\"\"\n        instantiate model, cost function and optimizer\n        \"\"\"\n        model = PetNet()\n        model_name = f'{Config.model_dir}/{Config.model_path}_fold{i_fold}.pth'\n        model.load_state_dict(torch.load(model_name))\n        model = model.to(device)\n        criterion = nn.BCEWithLogitsLoss()\n        epoch = 0\n        valid_targets, preds = tta_fn(y_valid, X_valid_paths, model, criterion, epoch)\n        valid_rmse = round(mean_squared_error(valid_targets, preds, squared=False), 3)\n        _oof_df = pd.DataFrame(data={'Id': valid_ids, 'pred':preds, 'fold': i_fold, 'Pawpularity':valid_targets}, index=valid_idx)\n        del model\n        gc.collect()\n        \n        torch.cuda.empty_cache()\n        oof_df = pd.concat([oof_df, _oof_df])\n    return oof_df.sort_values('Id')","metadata":{"execution":{"iopub.status.busy":"2022-01-03T02:18:40.5823Z","iopub.execute_input":"2022-01-03T02:18:40.58389Z","iopub.status.idle":"2022-01-03T02:18:40.595452Z","shell.execute_reply.started":"2022-01-03T02:18:40.583841Z","shell.execute_reply":"2022-01-03T02:18:40.594441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Config.tta:\n    oof_tta_df = tta_loop(filepaths, targets)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T02:18:40.597413Z","iopub.execute_input":"2022-01-03T02:18:40.597746Z","iopub.status.idle":"2022-01-03T02:19:21.296827Z","shell.execute_reply.started":"2022-01-03T02:18:40.597678Z","shell.execute_reply":"2022-01-03T02:19:21.295796Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## cv score","metadata":{}},{"cell_type":"markdown","source":"### without TTA","metadata":{}},{"cell_type":"code","source":"# no TTA\nfor i in range(Config.n_fold):\n    rmse_oof(oof_df, i)\nrmse_oof(oof_df)\noof_df.to_csv('oof.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T02:19:21.298854Z","iopub.execute_input":"2022-01-03T02:19:21.29921Z","iopub.status.idle":"2022-01-03T02:19:21.354191Z","shell.execute_reply.started":"2022-01-03T02:19:21.299162Z","shell.execute_reply":"2022-01-03T02:19:21.353063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(oof_df['Pawpularity'].values, alpha = 0.4, color = 'b', label = 'target', bins = 50)\npred_bins = int((np.max(oof_df['pred'].values) - np.min(oof_df['pred'].values)) //2)\nplt.hist(oof_df['pred'].values, alpha = 0.4, color = 'g', label = 'prediction', bins = pred_bins)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-03T02:19:21.355697Z","iopub.execute_input":"2022-01-03T02:19:21.356676Z","iopub.status.idle":"2022-01-03T02:19:21.768026Z","shell.execute_reply.started":"2022-01-03T02:19:21.356586Z","shell.execute_reply":"2022-01-03T02:19:21.767055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### with TTA","metadata":{}},{"cell_type":"code","source":"# with TTA\nfor i in range(Config.n_fold):\n    rmse_oof(oof_tta_df, i)\nrmse_oof(oof_tta_df)\noof_tta_df.to_csv('oof_tta.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T02:19:21.76951Z","iopub.execute_input":"2022-01-03T02:19:21.770896Z","iopub.status.idle":"2022-01-03T02:19:21.790305Z","shell.execute_reply.started":"2022-01-03T02:19:21.770834Z","shell.execute_reply":"2022-01-03T02:19:21.78928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(oof_tta_df['Pawpularity'].values, alpha = 0.4, color = 'b', label = 'target', bins = 50)\ntta_pred_bins = int((np.max(oof_tta_df['pred'].values) - np.min(oof_tta_df['pred'].values)) //2)\nplt.hist(oof_df['pred'].values, alpha = 0.4, color = 'g', label = 'prediction', bins = tta_pred_bins)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-03T02:19:21.792191Z","iopub.execute_input":"2022-01-03T02:19:21.79257Z","iopub.status.idle":"2022-01-03T02:19:22.21671Z","shell.execute_reply.started":"2022-01-03T02:19:21.792499Z","shell.execute_reply":"2022-01-03T02:19:22.215664Z"},"trusted":true},"execution_count":null,"outputs":[]}]}