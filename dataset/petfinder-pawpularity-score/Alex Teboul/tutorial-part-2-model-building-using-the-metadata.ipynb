{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tutorial Part 2: Model Building using the Metadata\n\nHello and welcome to Part 2 of this Pawpularity Contest Tutorial Series. In [Tutorial Part 1: EDA for Beginners ](https://www.kaggle.com/alexteboul/tutorial-part-1-eda-for-beginners/notebook), we covered the exploratory data analysis process from start to finish for the PetFinder.my Pawpularity Contest. If you'd like to learn more about how to explore and visualize both the metadata and image data for this competition, check out that notebook.\n\n**To recap some important findings from that exploration:**\n* The feature variables are all binary, 0s or 1s, while the target variable (Pawpularity) is 1-100.\n* The feature variables appear to have similar distributions of pawpularity scores, between themselves and their respective classes (0s vs 1s).\n* The images are hard to differentiate in terms of Pawpularity score just by looking at them.\n\nIn this notebook, I'm going to cover how to build some simple models using the metadata provided in the competition. By metadata, I'm referring to the .csv training data in this contest. According to the competition hosts, this data was created manually by people looking at the pet pictures and has features like Eyes, Group, Blur, Face, etc.\n\n**In this notebook you'll learn about:**\n* Decision Tree Classification\n* Decision Tree Regression\n* Ordinary Least Squares Regression\n* Ridge Regression\n* Bernoulli Naive Bayes Classification\n* Random Forest Regression\n* Histogram-based Gradient Boosting Regression (LightGBM)\n* How to evaluate your models\n* Plan for next steps\n\n**TLDR:**\n* All the models kind of suck and it's the metadata-Pawpularity relationship that is bad. Unlikely that any amount of parameter tuning or different models will make these too much better. Best bet is to use the images themselves. Which I will demonstrate in Tutorial Part 3.\n\n| Model                                            | RMSE   |\n|--------------------------------------------------|--------|\n| 4.1 Decision Tree Regression                     | 20.857 |\n| 4.2 Decision Tree Classification                 | 22.900 |\n| 5.1 Ordinary Least Square Regression             | 20.827 |\n| 5.2 Ridge Regression                             | 20.827 |\n| 6.1 Bernoulli Naive Bayes Classification         | 23.468 |\n| 7.1 Random Forest Regression                     | 20.838 |\n| 7.2 Histogram-based Gradient Boosting Regression | 20.924 |\n\n**Index:**\n1. Load in your packages\n2. Is this a Classification or Regression Problem?\n3. Get the Data\n4. Decision Trees  \n - 4.1 Decision Tree Regressor\n - 4.2 Decision Tree Classifier\n5. Linear Models\n - 5.1 Ordinary Least Squares Regression\n - 5.2 Ridge Regression\n6. Naive Bayes\n - 6.1 Bernoulli Naive Bayes \n7. Ensemble Methods\n - 7.1 Random Forest Regression\n - 7.2 Histogram-based Gradient Boosting Regression\n8. Conclusion\n\n\n**Tutorials so far:**\n\n1. **In [Tutorial Part 1: EDA for Beginners](https://www.kaggle.com/alexteboul/tutorial-part-1-eda-for-beginners)**, we covered the exploratory data analysis process from start to finish for the PetFinder.my Pawpularity Contest.\n2. **In [Tutorial Part 2: Model Building using the Metadata](https://www.kaggle.com/alexteboul/tutorial-part-2-model-building-using-the-metadata)**, we built models using the metadata (.csv data) provided by the competition hosts. Specifically, we tried Decision Tree Classification, Decision Tree Regression, Ordinary Least Squares Regression, Ridge Regression, Bernoulli Naive Bayes Classification, Random Forest Regression, and Histogram-based Gradient Boosting Regression (LightGBM). RMSE 20.54\n3. **In [Tutorial Part 3: CNN Image Models 1](https://www.kaggle.com/alexteboul/tutorial-part-3-cnn-image-modeling-1)**, we explored preprocessing the training images, explaining the data types necessary to model with images, a basic Convolutional Neural Network architecture, and submitting predictions. ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"-------","metadata":{}},{"cell_type":"markdown","source":"## 1: Load in your packages","metadata":{}},{"cell_type":"code","source":"#load in packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nimport time\nimport math\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\n#Models\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\n\n#use sklearn.metrics.mean_squared_error() AND math.sqrt() to get RMSE\nfrom sklearn.metrics import mean_squared_error\n\n#set a random_state to be used in the notebook\nrandom_state = 7","metadata":{"execution":{"iopub.status.busy":"2022-03-10T02:24:28.390192Z","iopub.execute_input":"2022-03-10T02:24:28.390448Z","iopub.status.idle":"2022-03-10T02:24:29.454922Z","shell.execute_reply.started":"2022-03-10T02:24:28.390367Z","shell.execute_reply":"2022-03-10T02:24:29.454213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"------","metadata":{}},{"cell_type":"markdown","source":"## 2: Is this a Classification or Regression Problem?\nOne of the first questions to ask in the model building process is to determine what type of machine learning (ML) problem you are trying to solve. Because we are given example data with both Pawpularity scores and feature data in the metadata .csv files, we know that we can at least treat this as a supervised learning problem. We have labels --> okay cool it's supervised learning. \n\nNext, we need to decide whether this is a classication (categorical variable prediction) or regression (continous variable prediction). As it turns out, we can do either method in this case. We can treat the Pawpularity score as a continous variable for our regression models to predict, or as a categorical variable between 1-100 for our classification models to predict. \n\nWhen our predictions are judged however, the Root Mean Square Error (RMSE) metric will be used to evaluate model success. RMSE is the square root of the variance of the residuals (your prediction errors). More plainly, residuals are a measure of how far off from the regression line data points are. The RMSE is a metric that evaluates how spread apart these residuals actually are. Basically, the smaller the RMSE you get, the better. Theoretically, if your model perfectly predicted everything, RMSE would be 0. RMSE is typically associated with regression evaluation.\n\n$$RMSE = \\sqrt{\\frac{1}{n}\\Sigma_{i=1}^{n}{\\Big(\\frac{d_i -f_i}{\\sigma_i}\\Big)^2}}$$","metadata":{}},{"cell_type":"code","source":"#Example of RMSE\nexample_y_actual    = [10,20,30,40,50,60,70,80,90,100]\nexample_y_predicted = [12,20,35,45,50,30,50,80,92,95]\nexample_MSE = mean_squared_error(example_y_actual, example_y_predicted)\nexample_RMSE = math.sqrt(example_MSE)\nprint(\"Example Mean Squared Error:\", round(example_MSE,1))\nprint(\"Example Root Mean Square Error:\", round(example_RMSE,1))","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:38:58.680576Z","iopub.execute_input":"2021-10-13T20:38:58.680887Z","iopub.status.idle":"2021-10-13T20:38:58.690139Z","shell.execute_reply.started":"2021-10-13T20:38:58.680858Z","shell.execute_reply":"2021-10-13T20:38:58.689324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-------","metadata":{}},{"cell_type":"markdown","source":"## 3: Get the data\nFirst, load the data in from the train.csv so we can start training some models. We're going to be training and testing multiple models to get a sense of what works. It helps to start with this train/test splitting so that you can reuse the groups for each model.\n\nLet's start with an 80-20 train-test split. So 80% of the train.csv will be used to train our models and 20% will be used to test how well the models work with new data. ","metadata":{}},{"cell_type":"code","source":"#load in the data:\n#source path (where the Pawpularity contest data resides)\npath = '../input/petfinder-pawpularity-score/'\n\n#Get the metadata (the .csv data) and put it into DataFrames\ntrain_df = pd.read_csv(path + 'train.csv')\n\n#show the dimensions of the train metadata.\nprint('train_df dimensions: ', train_df.shape)\ntrain_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:38:59.246333Z","iopub.execute_input":"2021-10-13T20:38:59.24694Z","iopub.status.idle":"2021-10-13T20:38:59.285227Z","shell.execute_reply.started":"2021-10-13T20:38:59.246901Z","shell.execute_reply":"2021-10-13T20:38:59.284543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have our data in a dataframe, so let's store our feature variables in X and target variable in y. Note that Id has to be removed from the feature variables. We just want the columns our models can use.","metadata":{}},{"cell_type":"code","source":"#select Pawpularity as target variable:\ny = train_df['Pawpularity']\n\n#select all the other columns minus Id and Pawpualarirty as the feature variables:\nX = train_df.drop(['Id','Pawpularity'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:38:59.492622Z","iopub.execute_input":"2021-10-13T20:38:59.493124Z","iopub.status.idle":"2021-10-13T20:38:59.499389Z","shell.execute_reply.started":"2021-10-13T20:38:59.493088Z","shell.execute_reply":"2021-10-13T20:38:59.49842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now make the train-test splits\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=random_state)\nprint('Dimensions: \\n x_train:{} \\n x_test{} \\n y_train{} \\n y_test{}'.format(x_train.shape, x_test.shape, y_train.shape, y_test.shape))","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:38:59.623338Z","iopub.execute_input":"2021-10-13T20:38:59.623679Z","iopub.status.idle":"2021-10-13T20:38:59.634433Z","shell.execute_reply.started":"2021-10-13T20:38:59.623645Z","shell.execute_reply":"2021-10-13T20:38:59.633814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#x_train and x_test are dataframes\nx_train.head(2)\n#x_test.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:38:59.852164Z","iopub.execute_input":"2021-10-13T20:38:59.854127Z","iopub.status.idle":"2021-10-13T20:38:59.866813Z","shell.execute_reply.started":"2021-10-13T20:38:59.854089Z","shell.execute_reply":"2021-10-13T20:38:59.86583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y_train and y_test are pandas series. .iloc[0:2] shows the first two elements of the series. In our case a pawpularity score of 26 and 11.\ny_train.iloc[0:2]\n#y_test.iloc[0]","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:38:59.952482Z","iopub.execute_input":"2021-10-13T20:38:59.952765Z","iopub.status.idle":"2021-10-13T20:38:59.960427Z","shell.execute_reply.started":"2021-10-13T20:38:59.952737Z","shell.execute_reply":"2021-10-13T20:38:59.95938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-------","metadata":{}},{"cell_type":"markdown","source":"## 4: Decision Trees\nDecision trees are one of the most basic machine learning algorithms and on relatively small datasets they can be a good place to start. One interesting feature of decision trees is that they can be used for both regression and classification supervised learning problems. Check out this great decision trees explanation on KDNuggets: [Link](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)","metadata":{}},{"cell_type":"markdown","source":"### 4.1 Decision Tree Regressor\n* Play around with the DecisionTreeRegressor parameters: [SciKit Documentation - Decision Tree Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)","metadata":{}},{"cell_type":"code","source":"#create the Regressor\ntree_reg = DecisionTreeRegressor(max_depth = 3, min_samples_split = 10)\n\n#train the model\nstart = time.time()\ntree_reg.fit(x_train, y_train)\nstop = time.time()\n\n#predict the response for the test data\ntree_reg_pred = tree_reg.predict(x_test)\n\n#print the RMSE\nprint(f'Training time: {round((stop - start),3)} seconds')\ntree_reg_RMSE = math.sqrt(mean_squared_error(y_test, tree_reg_pred))\nprint(f'tree_reg_RMSE: {round(tree_reg_RMSE,3)}')","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:39:00.607374Z","iopub.execute_input":"2021-10-13T20:39:00.607644Z","iopub.status.idle":"2021-10-13T20:39:00.62191Z","shell.execute_reply.started":"2021-10-13T20:39:00.607616Z","shell.execute_reply":"2021-10-13T20:39:00.620897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cool, we got a RMSE for our first model! Note that 20.857 is pretty bad, which suggests right off the bat that our initial assumptions about the models being unlikely to fit the data well were on track. ","metadata":{}},{"cell_type":"code","source":"#visualize the decision tree\nfig = plt.figure(figsize=(15,5))\nplot = tree.plot_tree(tree_reg, feature_names=x_train.columns.values.tolist(), filled=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:39:00.931162Z","iopub.execute_input":"2021-10-13T20:39:00.931508Z","iopub.status.idle":"2021-10-13T20:39:02.095881Z","shell.execute_reply.started":"2021-10-13T20:39:00.931474Z","shell.execute_reply":"2021-10-13T20:39:02.094848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's see what our predictions look like vs the actual \ndef ActualvPredictionsGraph(y_test,y_pred,title):\n    if max(y_test) >= max(y_pred):\n        my_range = int(max(y_test))\n    else:\n        my_range = int(max(y_pred))\n    plt.figure(figsize=(12,3))\n    plt.scatter(range(len(y_test)), y_test, color='blue')\n    plt.scatter(range(len(y_pred)), y_pred, color='red')\n    plt.xlabel('Index ')\n    plt.ylabel('Pawpularity ')\n    plt.title(title,fontdict = {'fontsize' : 15})\n    plt.legend(handles = [mpatches.Patch(color='red', label='prediction'),mpatches.Patch(color='blue', label='actual')])\n    plt.show()\n    return\n\n#plot it\nActualvPredictionsGraph(y_test[0:50], tree_reg_pred[0:50], \"First 50 Actual v. Predicted\")\nActualvPredictionsGraph(y_test, tree_reg_pred, \"All Actual v. Predicted\")\n\n#plot actual v predicted in histogram form\nplt.figure(figsize=(12,4))\nsns.histplot(tree_reg_pred,color='r',alpha=0.3,stat='probability', kde=True)\nsns.histplot(y_test,color='b',alpha=0.3,stat='probability', kde=True)\nplt.legend(labels=['prediction','actual'])\nplt.title('Actual v Predict Distribution')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:39:02.097939Z","iopub.execute_input":"2021-10-13T20:39:02.09827Z","iopub.status.idle":"2021-10-13T20:39:03.354796Z","shell.execute_reply.started":"2021-10-13T20:39:02.098236Z","shell.execute_reply":"2021-10-13T20:39:03.353948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When you're actually submitting to the competition, you'd make the predictions on the test data from the competition. This is different from the test data we created to help with our model evaluation just now.\n\n**How to submit to the competition:**\n* #gets the data\n    * test_df = pd.read_csv('../input/petfinder-pawpularity-score/test.csv')\n* #drops the Id column from the test_df dataframe (it already doesn't have Pawpularity so no need to remove that)\n    * x_test_submission = test_df.drop(['Id'],axis=1)\n* #predict with a model you've trained, in this case tree_reg, and add the predictions to the test_df dataframe\n    * test_df['Pawpularity'] = tree_reg.predict(x_test_submission)\n* #keep just the Id and Pawpularity score for the submission\n    * submission_df = test_df[['Id','Pawpularity']]\n* #save it to a .csv file called submission.csv\n    * submission_df.to_csv(\"submission.csv\", index=False)","metadata":{}},{"cell_type":"code","source":"#Now submit to the competition using the model:\ntest_df = pd.read_csv('../input/petfinder-pawpularity-score/test.csv') #gets the data\nx_test_submission = test_df.drop(['Id'],axis=1) #drops the Id column from the test_df dataframe (it already doesn't have Pawpularity so no need to remove that)\ntest_df['Pawpularity'] = tree_reg.predict(x_test_submission) #predict with a model you've trained, in this case tree_reg, and add the predictions to the test_df dataframe\nsubmission_df = test_df[['Id','Pawpularity']] #keep just the Id and Pawpularity score for the submission\nsubmission_df.to_csv(\"submission.csv\", index=False) #save it to a .csv file called submission.csv\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:39:03.35639Z","iopub.execute_input":"2021-10-13T20:39:03.35681Z","iopub.status.idle":"2021-10-13T20:39:03.379326Z","shell.execute_reply.started":"2021-10-13T20:39:03.356764Z","shell.execute_reply":"2021-10-13T20:39:03.378081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2 Decision Tree Classifier\n* Play around with the DecisionTreeClassifier parameters: [SciKit Documentation - Decision Tree Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)","metadata":{}},{"cell_type":"code","source":"#create the Classifier\ntree_clf = DecisionTreeClassifier(max_depth = 3, min_samples_split = 10)\n\n#train the model\nstart = time.time()\ntree_clf.fit(x_train, y_train)\nstop = time.time()\n\n#predict the response for the test data\ntree_clf_pred = tree_clf.predict(x_test)\n\n#print the RMSE\nprint(f'Training time: {round((stop - start),3)} seconds')\ntree_clf_RMSE = math.sqrt(mean_squared_error(y_test, tree_clf_pred))\nprint(f'tree_clf_RMSE: {round(tree_clf_RMSE,3)}')","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:39:03.381568Z","iopub.execute_input":"2021-10-13T20:39:03.381913Z","iopub.status.idle":"2021-10-13T20:39:03.401265Z","shell.execute_reply.started":"2021-10-13T20:39:03.381872Z","shell.execute_reply":"2021-10-13T20:39:03.400054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again pretty a pretty bad model, but just shows that you can use a decision tree for both classification and regression.","metadata":{}},{"cell_type":"code","source":"#plot it\nActualvPredictionsGraph(y_test[0:50], tree_clf_pred[0:50], \"First 50 Actual v. Predicted\")\nActualvPredictionsGraph(y_test, tree_clf_pred, \"All Actual v. Predicted\")\n\n#plot actual v predicted in histogram form\nplt.figure(figsize=(12,4))\nsns.histplot(tree_clf_pred,color='r',alpha=0.3,stat='probability', kde=True)\nsns.histplot(y_test,color='b',alpha=0.3,stat='probability', kde=True)\nplt.legend(labels=['prediction','actual'])\nplt.title('Actual v Predict Distribution')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:39:03.402495Z","iopub.execute_input":"2021-10-13T20:39:03.402734Z","iopub.status.idle":"2021-10-13T20:39:04.496353Z","shell.execute_reply.started":"2021-10-13T20:39:03.402707Z","shell.execute_reply":"2021-10-13T20:39:04.495557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Important:** Note that the classifier is really just predicting values are going to fall within a narrow range. It's doing a poor job of actually predicting based on how different pet features might be impacting the Pawpularity score. Instead, the model has learned that guessing near the mean Pawpularity scores is a good way to minimize the error. We do not want this!","metadata":{}},{"cell_type":"code","source":"#Now submit to the competition using the model:\ntest_df = pd.read_csv('../input/petfinder-pawpularity-score/test.csv')\nx_test_submission = test_df.drop(['Id'],axis=1) \ntest_df['Pawpularity'] = tree_clf.predict(x_test_submission) \nsubmission_df = test_df[['Id','Pawpularity']]\nsubmission_df.to_csv(\"submission.csv\", index=False)\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:39:04.497583Z","iopub.execute_input":"2021-10-13T20:39:04.497827Z","iopub.status.idle":"2021-10-13T20:39:04.517142Z","shell.execute_reply.started":"2021-10-13T20:39:04.497786Z","shell.execute_reply":"2021-10-13T20:39:04.516282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Decision Trees Summary:** Decision Trees produced pretty poor models. But it's always good to know what doesn't work as well as what does.\n\n* **tree_reg_RMSE:** 20.857\n* **tree_clf_RMSE:** 22.900","metadata":{}},{"cell_type":"markdown","source":"--------","metadata":{}},{"cell_type":"markdown","source":"## 5: Linear Models\n* Play around with the Linear Models and their parameters: [SciKit Documentation - Linear Models](https://scikit-learn.org/stable/modules/linear_model.html)","metadata":{}},{"cell_type":"markdown","source":"### 5.1 Ordinary Least Squares Regression\nMost basic Linear Model is just linear regression. Multicollinearity problems, or the features being overly similar, have a big impact on these models. Feature selection is an important part of using linear models, but we're going to skip this for the sake of simplicity.\n* Tune the OLS linear regression model and find more info: [SciKit Documentation - Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)","metadata":{}},{"cell_type":"code","source":"#create the Ordinary Least Squares Regression model\nols_reg = LinearRegression()\n\n#train the model\nstart = time.time()\nols_reg.fit(x_train, y_train)\nstop = time.time()\n\n#predict the response for the test data\nols_reg_pred = ols_reg.predict(x_test)\n\n#print the RMSE\nprint(f'Training time: {round((stop - start),3)} seconds')\nols_reg_RMSE = math.sqrt(mean_squared_error(y_test, ols_reg_pred))\nprint(f'ols_reg_RMSE: {round(ols_reg_RMSE,3)}')","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:39:04.519085Z","iopub.execute_input":"2021-10-13T20:39:04.519302Z","iopub.status.idle":"2021-10-13T20:39:04.536437Z","shell.execute_reply.started":"2021-10-13T20:39:04.519277Z","shell.execute_reply":"2021-10-13T20:39:04.535391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot it\nActualvPredictionsGraph(y_test[0:50], ols_reg_pred[0:50], \"First 50 Actual v. Predicted\")\nActualvPredictionsGraph(y_test, ols_reg_pred, \"All Actual v. Predicted\")\n\n#plot actual v predicted in histogram form\nplt.figure(figsize=(12,4))\nsns.histplot(ols_reg_pred,color='r',alpha=0.3,stat='probability', kde=True)\nsns.histplot(y_test,color='b',alpha=0.3,stat='probability', kde=True)\nplt.legend(labels=['prediction','actual'])\nplt.title('Actual v Predict Distribution')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:39:04.537897Z","iopub.execute_input":"2021-10-13T20:39:04.53831Z","iopub.status.idle":"2021-10-13T20:39:06.057821Z","shell.execute_reply.started":"2021-10-13T20:39:04.538257Z","shell.execute_reply":"2021-10-13T20:39:06.057164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So the ordinary least squares linear regression model is even more narrow in its predictions than the decision trees - not great.","metadata":{}},{"cell_type":"code","source":"#Now submit to the competition using the model:\ntest_df = pd.read_csv('../input/petfinder-pawpularity-score/test.csv')\nx_test_submission = test_df.drop(['Id'],axis=1) \ntest_df['Pawpularity'] = ols_reg.predict(x_test_submission) \nsubmission_df = test_df[['Id','Pawpularity']]\nsubmission_df.to_csv(\"submission.csv\", index=False)\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:39:06.058926Z","iopub.execute_input":"2021-10-13T20:39:06.059167Z","iopub.status.idle":"2021-10-13T20:39:06.080078Z","shell.execute_reply.started":"2021-10-13T20:39:06.059141Z","shell.execute_reply":"2021-10-13T20:39:06.079239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2 Ridge Regression\nAnother basic regression model is called ridge regression. Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients. The ridge coefficients minimize a penalized residual sum of squares. Basically, the larger the value of alpha the greater the amount of shrinkage and thus the coefficients become more robust to collinearity. Let's see if this improves the model at all.\n* For more on the documentation: [SciKit Learn Documentation - Ridge Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge)","metadata":{}},{"cell_type":"code","source":"#create the Ridge Regression model\nridge_reg = Ridge(alpha=2.0)\n\n#train the model\nstart = time.time()\nridge_reg.fit(x_train, y_train)\nstop = time.time()\n\n#predict the response for the test data\nridge_reg_pred = ridge_reg.predict(x_test)\n\n#print the RMSE\nprint(f'Training time: {round((stop - start),3)} seconds')\nridge_reg_RMSE = math.sqrt(mean_squared_error(y_test, ridge_reg_pred))\nprint(f'ridge_reg_RMSE: {round(ridge_reg_RMSE,3)}')","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:52:47.519277Z","iopub.execute_input":"2021-10-13T20:52:47.519594Z","iopub.status.idle":"2021-10-13T20:52:47.538215Z","shell.execute_reply.started":"2021-10-13T20:52:47.519565Z","shell.execute_reply":"2021-10-13T20:52:47.537073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot it\nActualvPredictionsGraph(y_test[0:50], ridge_reg_pred[0:50], \"First 50 Actual v. Predicted\")\nActualvPredictionsGraph(y_test, ridge_reg_pred, \"All Actual v. Predicted\")\n\n#plot actual v predicted in histogram form\nplt.figure(figsize=(12,4))\nsns.histplot(ridge_reg_pred,color='r',alpha=0.3,stat='probability', kde=True)\nsns.histplot(y_test,color='b',alpha=0.3,stat='probability',kde=True)\nplt.legend(labels=['prediction','actual'])\nplt.title('Actual v Predict Distribution')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:50:09.487614Z","iopub.execute_input":"2021-10-13T20:50:09.487917Z","iopub.status.idle":"2021-10-13T20:50:11.359383Z","shell.execute_reply.started":"2021-10-13T20:50:09.487889Z","shell.execute_reply":"2021-10-13T20:50:11.358459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interesting, it's exactly the same as the OLS model. Think on why this is.","metadata":{}},{"cell_type":"code","source":"#Now submit to the competition using the model:\ntest_df = pd.read_csv('../input/petfinder-pawpularity-score/test.csv')\nx_test_submission = test_df.drop(['Id'],axis=1) \ntest_df['Pawpularity'] = ridge_reg.predict(x_test_submission) \nsubmission_df = test_df[['Id','Pawpularity']]\nsubmission_df.to_csv(\"submission.csv\", index=False)\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:39:07.758399Z","iopub.execute_input":"2021-10-13T20:39:07.75915Z","iopub.status.idle":"2021-10-13T20:39:07.780874Z","shell.execute_reply.started":"2021-10-13T20:39:07.759114Z","shell.execute_reply":"2021-10-13T20:39:07.780132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Turns out the Ridge Regression is just as bad as the Ordinarly Least Squares Regression model. Let's try something that will better approximate and learn from the distribution of Pawpularity scores.\n\n**Linear Models Summary:**\n* **ols_reg_RMSE:** 20.827\n* **ridge_reg_RMSE:** 20.827","metadata":{}},{"cell_type":"markdown","source":"------------","metadata":{}},{"cell_type":"markdown","source":"## 6: Naive Bayes\nNaive Bayes methods are a set of supervised learning algorithms based on applying Bayes‚Äô theorem. They have the ‚Äúnaive‚Äù assumption of conditional independence between every pair of features given the value of the target variable. They also tend to be reasonably fast.\n* Learn more about Naive Bayes and play with the parameters: [SciKit Documentation - Naive Bayes Classifier](https://scikit-learn.org/stable/modules/naive_bayes.html)","metadata":{}},{"cell_type":"markdown","source":"### 6.1 Bernoulli Naive Bayes\n\nBernoulliNB implements the naive Bayes algorithm for data that is distributed according to multivariate Bernoulli distributions. This means that each feature is assumed to be a binary-valued (Bernoulli, boolean) variable. This is the structure of the metadata we are given in this contest so let's see how this works. You'd typically see this used in some Natural Language Processing tasks, but it could be useful in this unique case as well.\n* Learn more here: [SciKit Documentation - BernoulliNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB)","metadata":{}},{"cell_type":"code","source":"#create the Bernoulli Naive Bayes model\nBernoulliNB_clf = BernoulliNB()\n\n#train the model\nstart = time.time()\nBernoulliNB_clf.fit(x_train, y_train)\nstop = time.time()\n\n#predict the response for the test data\nBernoulliNB_clf_pred = BernoulliNB_clf.predict(x_test)\n\n#print the RMSE\nprint(f'Training time: {round((stop - start),3)} seconds')\nBernoulliNB_clf_RMSE = math.sqrt(mean_squared_error(y_test, BernoulliNB_clf_pred))\nprint(f'BernoulliNB_clf_RMSE: {round(BernoulliNB_clf_RMSE,3)}')","metadata":{"execution":{"iopub.status.busy":"2021-10-13T21:17:04.400583Z","iopub.execute_input":"2021-10-13T21:17:04.400909Z","iopub.status.idle":"2021-10-13T21:17:04.442028Z","shell.execute_reply.started":"2021-10-13T21:17:04.400879Z","shell.execute_reply":"2021-10-13T21:17:04.440608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot it\nActualvPredictionsGraph(y_test[0:50], BernoulliNB_clf_pred[0:50], \"First 50 Actual v. Predicted\")\nActualvPredictionsGraph(y_test, BernoulliNB_clf_pred, \"All Actual v. Predicted\")\n\n#plot actual v predicted in histogram form\nplt.figure(figsize=(12,4))\nsns.histplot(BernoulliNB_clf_pred,color='r',alpha=0.3,stat='probability',kde=True)\nsns.histplot(y_test,color='b',alpha=0.3,stat='probability', kde=True)\nplt.legend(labels=['prediction','actual'])\nplt.title('Actual v Predict Distribution')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T21:17:05.278742Z","iopub.execute_input":"2021-10-13T21:17:05.279052Z","iopub.status.idle":"2021-10-13T21:17:06.829441Z","shell.execute_reply.started":"2021-10-13T21:17:05.279Z","shell.execute_reply":"2021-10-13T21:17:06.828351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now submit to the competition using the model:\ntest_df = pd.read_csv('../input/petfinder-pawpularity-score/test.csv')\nx_test_submission = test_df.drop(['Id'],axis=1) \ntest_df['Pawpularity'] = BernoulliNB_clf.predict(x_test_submission) \nsubmission_df = test_df[['Id','Pawpularity']]\nsubmission_df.to_csv(\"submission.csv\", index=False)\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T21:17:06.83214Z","iopub.execute_input":"2021-10-13T21:17:06.832505Z","iopub.status.idle":"2021-10-13T21:17:06.861077Z","shell.execute_reply.started":"2021-10-13T21:17:06.83246Z","shell.execute_reply":"2021-10-13T21:17:06.85983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So far this is actually the worst RMSE result, but notice how the predictions from this model more closely match the actual distribution of Pawpularity scores. So this model is not just guessing values close to the mean to get a good evaluation score, but actually predicting Pawpularity values that more closely resembles the true human interaction with these pet pictures. For this reason, I like this model the most so far, even though it has the worst RMSE so far.\n\n**Naive Bayes Summary:**\n* **BernoulliNB_clf_RMSE:** 23.468","metadata":{}},{"cell_type":"markdown","source":"-------","metadata":{}},{"cell_type":"markdown","source":"## 7: Ensemble Methods\nEnsemble methods can be quite useful in a number of applications. If you want to learn more about ensemble methods, bagging, boosting, and stacking, check out this great resource online: [Link](https://analyticsindiamag.com/basics-of-ensemble-learning-in-classification-techniques-explained/). Some of them like Random Forests can get quite slow to train on big datasets, but we this is a small enough dataset for them. For now, I'll just show Random Forests and a version of Gradient Boosting called Histogram-based Gradient Boosting Classification Tree based on LightGBM.\n\n* If you want to learn more about Ensemble Methods or try out different ones: [SciKit Learn Documentation - Ensemble Methods](https://scikit-learn.org/stable/modules/ensemble.html#ensemble)","metadata":{}},{"cell_type":"markdown","source":"### 7.1 Random Forest\nThe random forest ensemble is pretty much just making a bunch of decision trees and coming to consesus about how to get a single prediction from the many trees.\n* For more on the Random Forest and parameter tuning: [SciKit Learn Documentation - Random Forests](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)","metadata":{}},{"cell_type":"code","source":"#create the Random Forest ensemble\nRF_reg = RandomForestRegressor(n_estimators=50, max_depth=3)\n\n#train the model\nstart = time.time()\nRF_reg.fit(x_train, y_train)\nstop = time.time()\n\n#predict the response for the test data\nRF_reg_pred = RF_reg.predict(x_test)\n\n#print the RMSE\nprint(f'Training time: {round((stop - start),3)} seconds')\nRF_reg_RMSE = math.sqrt(mean_squared_error(y_test, RF_reg_pred))\nprint(f'RF_reg_RMSE: {round(RF_reg_RMSE,3)}')","metadata":{"execution":{"iopub.status.busy":"2021-10-13T21:35:53.953316Z","iopub.execute_input":"2021-10-13T21:35:53.954658Z","iopub.status.idle":"2021-10-13T21:35:54.119662Z","shell.execute_reply.started":"2021-10-13T21:35:53.954587Z","shell.execute_reply":"2021-10-13T21:35:54.118828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot it\nActualvPredictionsGraph(y_test[0:50], RF_reg_pred[0:50], \"First 50 Actual v. Predicted\")\nActualvPredictionsGraph(y_test, RF_reg_pred, \"All Actual v. Predicted\")\n\n#plot actual v predicted in histogram form\nplt.figure(figsize=(12,4))\nsns.histplot(RF_reg_pred,color='r',alpha=0.3,stat='probability',kde=True)\nsns.histplot(y_test,color='b',alpha=0.3,stat='probability', kde=True)\nplt.legend(labels=['prediction','actual'])\nplt.title('Actual v Predict Distribution')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T21:36:09.721753Z","iopub.execute_input":"2021-10-13T21:36:09.722556Z","iopub.status.idle":"2021-10-13T21:36:12.001818Z","shell.execute_reply.started":"2021-10-13T21:36:09.722506Z","shell.execute_reply":"2021-10-13T21:36:12.000576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now submit to the competition using the model:\ntest_df = pd.read_csv('../input/petfinder-pawpularity-score/test.csv')\nx_test_submission = test_df.drop(['Id'],axis=1) \ntest_df['Pawpularity'] = RF_reg.predict(x_test_submission) \nsubmission_df = test_df[['Id','Pawpularity']]\nsubmission_df.to_csv(\"submission.csv\", index=False)\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T22:23:24.873412Z","iopub.execute_input":"2021-10-13T22:23:24.874333Z","iopub.status.idle":"2021-10-13T22:23:24.903566Z","shell.execute_reply.started":"2021-10-13T22:23:24.874268Z","shell.execute_reply":"2021-10-13T22:23:24.902546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Random Forest isn't really any better than a simple decision tree in our case.  It predicts in pretty much the same way by guessing everything has Pawpularity near the mean Pawpularity score.","metadata":{}},{"cell_type":"markdown","source":"### 7.2 Histogram-based Gradient Boosting Regression\nThis is pretty much **LightGBM** but you can use it in scikit learn! Because it's experimental, you have to call: from sklearn.experimental import enable_hist_gradient_boosting as well.\n* For more and parameter info: [SciKit Learn Documentation - Histogram-based Gradient Boosting Regression](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html)\n","metadata":{}},{"cell_type":"code","source":"#create the Histogram-based Gradient Boosting Regression model\nHistGB_reg = HistGradientBoostingRegressor()\n\n#train the model\nstart = time.time()\nHistGB_reg.fit(x_train, y_train)\nstop = time.time()\n\n#predict the response for the test data\nHistGB_reg_pred = HistGB_reg.predict(x_test)\n\n#print the RMSE\nprint(f'Training time: {round((stop - start),3)} seconds')\nHistGB_reg_RMSE = math.sqrt(mean_squared_error(y_test, HistGB_reg_pred))\nprint(f'HistGB_clf_RMSE: {round(HistGB_reg_RMSE,3)}')","metadata":{"execution":{"iopub.status.busy":"2021-10-13T21:33:40.957485Z","iopub.execute_input":"2021-10-13T21:33:40.958203Z","iopub.status.idle":"2021-10-13T21:33:41.59068Z","shell.execute_reply.started":"2021-10-13T21:33:40.958153Z","shell.execute_reply":"2021-10-13T21:33:41.589822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot it\nActualvPredictionsGraph(y_test[0:50], HistGB_reg_pred[0:50], \"First 50 Actual v. Predicted\")\nActualvPredictionsGraph(y_test, HistGB_reg_pred, \"All Actual v. Predicted\")\n\n#plot actual v predicted in histogram form\nplt.figure(figsize=(12,4))\nsns.histplot(HistGB_reg_pred,color='r',alpha=0.3,stat='probability',kde=True)\nsns.histplot(y_test,color='b',alpha=0.3,stat='probability', kde=True)\nplt.legend(labels=['prediction','actual'])\nplt.title('Actual v Predict Distribution')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T21:34:38.393379Z","iopub.execute_input":"2021-10-13T21:34:38.393706Z","iopub.status.idle":"2021-10-13T21:34:40.437377Z","shell.execute_reply.started":"2021-10-13T21:34:38.393672Z","shell.execute_reply":"2021-10-13T21:34:40.436257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now submit to the competition using the model:\ntest_df = pd.read_csv('../input/petfinder-pawpularity-score/test.csv')\nx_test_submission = test_df.drop(['Id'],axis=1) \ntest_df['Pawpularity'] = HistGB_reg.predict(x_test_submission) \nsubmission_df = test_df[['Id','Pawpularity']]\nsubmission_df.to_csv(\"submission.csv\", index=False)\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T22:27:56.115668Z","iopub.execute_input":"2021-10-13T22:27:56.116111Z","iopub.status.idle":"2021-10-13T22:27:56.147806Z","shell.execute_reply.started":"2021-10-13T22:27:56.116077Z","shell.execute_reply":"2021-10-13T22:27:56.146794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nice, so our ensemble methods worked, but not significantly better than simpler models. Also because this HistGB model is the last one in the doc that's what would be submitted, but better to have a separate notebook with only 1 model your actual submissions.\n\n**Ensemble Methods Summary:**\n* **RF_reg_RMSE:** 20.838\n* **HistGB_clf_RMSE:** 20.924","metadata":{}},{"cell_type":"markdown","source":"-------","metadata":{}},{"cell_type":"markdown","source":"## 8: Conclusion\n\nWe ran 7 different models, and none did particularly well! üòÖ\n\n**Summary of Models:**\n\n| Model                                            | RMSE   |\n|--------------------------------------------------|--------|\n| 4.1 Decision Tree Regression                     | 20.857 |\n| 4.2 Decision Tree Classification                 | 22.900 |\n| 5.1 Ordinary Least Square Regression             | 20.827 |\n| 5.2 Ridge Regression                             | 20.827 |\n| 6.1 Bernoulli Naive Bayes Classification         | 23.468 |\n| 7.1 Random Forest Regression                     | 20.838 |\n| 7.2 Histogram-based Gradient Boosting Regression | 20.924 |\n\n\nThese appear to suck because the metadata is just not really predictive of the target Pawpularity. The fact that all the models score pretty similarly even when parameters are modified supports this idea. All the models are pretty much just guessing the mean or close to it in an attempt to minimize errors. You could probably sit here all day trying to find the perfect parameter settings for these or other models, but it will never produce incredible results. In this case, the data and the target are the problem - not the models chosen. From this we can conclude that if there is in fact a way to build better models for pawpularity, it will involve using the images and not this metadata. \n\nI'll post a Tutorial Part 3 soon which will cover how to build models using the images and hopefully see some lower RMSE scores!\n\n### Hope this helped!","metadata":{}}]}