{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"../input/titlestyle/style2.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-21T14:05:25.640186Z","iopub.execute_input":"2021-07-21T14:05:25.640734Z","iopub.status.idle":"2021-07-21T14:05:25.670652Z","shell.execute_reply.started":"2021-07-21T14:05:25.640651Z","shell.execute_reply":"2021-07-21T14:05:25.669825Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"heading\">\n   <h1><span style=\"color: white\">Intro</span></h1>\n</div>\n<div class=\"content\">\n\n<u>ðŸ“” Public notebooks - 130+</u><br>\n\n<u>ðŸ¥‡ Gold medals - 6+</u><br>\n\n<u>ðŸ¥ˆ Silver medals - 18+</u><br>\n\n<u>ðŸ¥‰ Bronze medals - 41+</u><br>\n\n\nWordcloud made of notebook titles:\n<img src=\"https://i.imgur.com/hCvj3cP.png\" alt=\"img1\"/>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style = \"font-family: Arial;font-size:1.6em;color: #0a6121;background: #ace6bc;padding:5px;border-style: solid;border-color:#0a6121;\">\n<b>Summa summarum:</b> LightGBM, ANN, lag target features\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"heading\">\n   <h1><span style=\"color: white\">Goal</span></h1>\n</div>\n<div class='content'>\n    <h3><b>âš¾ Forecast supporters digital engagement (time-series forecasting) using data such as player performance, social media and team factors.</b></h3>\n    <h3><b>âš¾ The supporters engagement is provided through 4 different time-series. The real meaning of the series is not explained but it may be related to likes, retweets and replies on Twitter or something similar.</b></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"data_dir = '/kaggle/input/mlb-player-digital-engagement-forecasting'\n\ndf = pd.read_csv(f\"{data_dir}/train.csv\").dropna(axis=1,how='all')\ndaily_data_nested_df_names = df.drop('date', axis = 1).columns.values.tolist()\n\ndf_name = 'nextDayPlayerEngagement'\ndate_nested_table = df[['date', df_name]]\n\ndate_nested_table = (date_nested_table[\n  ~pd.isna(date_nested_table[df_name])].reset_index(drop = True))\n\ndaily_dfs_collection = []\n\nfor date_index, date_row in date_nested_table.iterrows():\n    daily_df = pd.read_json(date_row[df_name])\n\n    daily_df['dailyDataDate'] = date_row['date']\n\n    daily_dfs_collection = daily_dfs_collection + [daily_df]\n\nunnested_table = (pd.concat(daily_dfs_collection,ignore_index = True).\n                  set_index('dailyDataDate').reset_index())\n\nunnested_table.to_pickle(f\"train_{df_name}.pickle\")\n\ndel(date_nested_table, daily_dfs_collection, unnested_table)\n        \ntrain_target = pd.read_pickle('train_nextDayPlayerEngagement.pickle')\ntrain_target['engagementMetricsDate'] = pd.to_datetime(train_target['engagementMetricsDate'])\ntrain_target['dailyDataDate'] = train_target['dailyDataDate'].astype(str)\ntrain_target['dailyDataDate'] = pd.to_datetime(train_target['dailyDataDate'], format=\"%Y%m%d\")\n\nfig, axes = plt.subplots(4, 1, figsize=(12,15))\n\nfor i, ax in enumerate(axes):\n    train_target.groupby('engagementMetricsDate').mean()[f'target{i+1}'].plot(ax=ax)\n    ax.set_title(f'mean target{i+1}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-21T14:06:23.085176Z","iopub.execute_input":"2021-07-21T14:06:23.085556Z","iopub.status.idle":"2021-07-21T14:08:20.646744Z","shell.execute_reply.started":"2021-07-21T14:06:23.085522Z","shell.execute_reply":"2021-07-21T14:08:20.645975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"heading\">\n   <h1><span style=\"color: white\">Ideas</span></h1>\n</div>\n<div class='content'>\nThis topic won't go into the domain range of this competition but rather to summarize the technical ideas shared in the notebooks. Nevertheless, if you are interested in <b>applications of DS in baseball</b>, feel free to check this discussion <a href=\"https://www.kaggle.com/c/mlb-player-digital-engagement-forecasting/discussion/245334\">https://www.kaggle.com/c/mlb-player-digital-engagement-forecasting/discussion/245334</a> as well as other baseball data sets <a href=\"https://www.kaggle.com/c/mlb-player-digital-engagement-forecasting/discussion/245328\">https://www.kaggle.com/c/mlb-player-digital-engagement-forecasting/discussion/245328</a><br><br>\n\nCross-validation (CV) is a technique used in machine learning to determine how well the forecasts are generalizing. The goal of this process is to estimate the error rate of the model - how far off are our predictions from reality. It does this by splitting up historical data into different groups and using one group as a test set and then comparing it against an independent set of data that was not used for training.<br><br>\n\nIn opposite to k-fold CV, where the data set is divided into k-folds and each fold is treated as a test set in one iteration, <b>CV for time-series</b> has a slightly different logic. The main reason is that time-series are time depending data and it wouldn't be accurate to use data from the future as a training set to predict past data. While splitting the data, we have to follow the time order to simulate real-life data flow. This can be done using either a fixed or expanding training set.<br><br>\n\n<img src=\"https://miro.medium.com/max/1204/1*qvdnPF8ETV9mFdMT0Y_BBA.png\" alt=\"img1\"/>\n<center><i>Time-series CV, Source: <a href=\"https://medium.com/@soumyachess1496/cross-validation-in-time-series-566ae4981ce4\">https://medium.com/@soumyachess1496/cross-validation-in-time-series-566ae4981ce4</a></i></center>\n<a href=\"\"></a>\n\nThe neural network is a computational system modeled after the human brain, which uses a series of interconnected layers that loosely represent neurons in the brain. They work by trying to identify patterns in data, and then use that knowledge to make predictions about new data. The example of <b>ANN implemented in Keras</b> that predicts all four targets at once is presented here <a href=\"https://www.kaggle.com/ulrich07/mlb-ann-with-lags-tf-keras\">https://www.kaggle.com/ulrich07/mlb-ann-with-lags-tf-keras</a> <b>ANN implemented in PyTorch</b> is described here <a href=\"https://www.kaggle.com/nicohrubec/mlb-pytorch-dnn-loop-feature-engineering\">https://www.kaggle.com/nicohrubec/mlb-pytorch-dnn-loop-feature-engineering</a><br><br>\n\nLightGBM is a predictive analytics library for Python that can be used with both CPU and GPU. It is a scalable, unified model predictive framework that supports tree-based models. In comparison to XGBoost, the Light GBM is almost 7 times faster which is a much better approach when dealing with large datasets. This turns out to be a huge advantage when you are working on large datasets in limited-time competitions.<br><br>\n\nA Cat Boosting regressor is a regression algorithm that improves model performance by sequentially adding weaker models. Generally, it can be used for both classification and regression problems. The initial model is trained using all of the data and then subsequent models are trained using subsets of the data that are too noisy to predict well, but contain not enough data to train a good model without overtraining. Example of <b>LightGBM plus CatBoosting</b> can be found here <a href=\"https://www.kaggle.com/lhagiimn/lightgbm-catboost-ann-2505f2\">https://www.kaggle.com/lhagiimn/lightgbm-catboost-ann-2505f2</a>\nSimilarly, <b>LightGBM + ANN in Keras</b> is here <a href=\"https://www.kaggle.com/mlconsult/1-35-lightgbm-ann\">https://www.kaggle.com/mlconsult/1-35-lightgbm-ann</a>\n    \n<img src=\"https://image.slidesharecdn.com/xgboostandlightgbm-180201121028/95/xgboost-lightgbm-21-638.jpg?cb=1517487076\" alt=\"img3\"/>\n<center><i>Difference between XGBoost and LightGBM, <a href=\"https://www.slideshare.net/GabrielCyprianoSaca/xgboost-lightgbm\">https://www.slideshare.net/GabrielCyprianoSaca/xgboost-lightgbm</a></i></center><br>\n    \nSome classic time-series forecasting methods, such as <b>SARIMAX, VAR, and HWES</b> can recognize autoregression, moving average or seasonal parts of time series. In this competition, some of the econometrics methods have been tested here <a href=\"https://www.kaggle.com/adarshsng/time-series-forecasting-case-study\">https://www.kaggle.com/adarshsng/time-series-forecasting-case-study</a> but you can find a more <b>detailed tutorial about economentrics and time-series</b> here <a href=\"https://www.kaggle.com/eneszvo/time-series-forecasting-p1-es-arima-var\">https://www.kaggle.com/eneszvo/time-series-forecasting-p1-es-arima-var</a>.<br><br>\n    \nðŸ’¡ Even if lag targets are not provided in the private test, you can save your own predictions and use them as lag targets. This <b>approach with different lags</b> seemed to work for the current leader in the competition <a href=\"https://www.kaggle.com/c/mlb-player-digital-engagement-forecasting/discussion/250945#1383850\">https://www.kaggle.com/c/mlb-player-digital-engagement-forecasting/discussion/250945#1383850</a>\n\n</div>\n","metadata":{}}]}