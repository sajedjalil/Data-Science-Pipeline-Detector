{"nbformat_minor":1,"nbformat":4,"cells":[{"source":"There are some really cool Kernels sitting out there for this competition and they are far slicker and efficient than this. This is just a brief data dive and some features using TFIDF and Truncated SVD on them. ","outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_uuid":"51625e8132e1ee8c0499967fba497b4f0b578012","_cell_guid":"67c743ea-d656-4713-86bb-f55b4c2a178a"},"execution_count":null},{"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom nltk.corpus import stopwords\nimport re\nimport gc\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# from subprocess import check_output\n# print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":true,"trusted":true,"_uuid":"6235fb60c126731b0070df90e1e46350beaa948b","_cell_guid":"d3866280-7e93-49b7-b43d-27f9cabbaf66"},"execution_count":1},{"source":"import os\ncdir = os.getcwd()\ntVars = pd.read_csv('../input/training_variants')\nvVars = pd.read_csv('../input/test_variants')\ntText = pd.read_csv('../input/training_text',sep='\\|\\|',\n                    skiprows=1,engine='python',names=[\"ID\",\"text\"])\nvText = pd.read_csv('../input/test_text',sep='\\|\\|',\n                    skiprows=1,engine='python',names=[\"ID\",\"text\"])","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":true,"trusted":true,"_uuid":"5a1c2a3d070e806b3473e9e07afaa3367526a7ab","_cell_guid":"a16da2b1-fb1c-4c58-9527-c9ed4cebf056"},"execution_count":2},{"source":"tVars.head()","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"21bfe5db7daf46ab95d723a322fbe8021d1ca26a","_cell_guid":"199aa5e9-40d7-41e1-a443-a7816b61bb84"},"execution_count":3},{"source":"tText.head()","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"80a2ce71b57064b3a5b789eea03ce8a246f67776","_cell_guid":"c3003118-3ad6-4416-9d67-99d1fba8f7f2"},"execution_count":4},{"source":"print(tText['text'][0][:20], '   ', len(tText['text'][0]))","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"ba0dfdc524c1b0eb13fe7a56df33bc5da9346e7a","_cell_guid":"7141a2ab-def2-4c4d-be44-eacaba228589"},"execution_count":5},{"source":"It looks like each of these items in the text are going to have fairly lengthy descriptions and we can treat these as documents, or at least I am.","outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_uuid":"c1b6990db8a8916f51292e401188af5e87a63af7","_cell_guid":"37b98fd9-6c51-4c32-b15c-9649cee3b66a"},"execution_count":null},{"source":"print(len(tText), len(tVars), len(vText), len(vVars))","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"a105228acbed27ec7310a4f50d37a6d7577c35c1","_cell_guid":"6f1b4aa8-235d-456c-b80c-06bcfd959c54"},"execution_count":6},{"source":"from collections import Counter\nvarsGeneCount = Counter(tVars.Gene)\nprint(varsGeneCount, '\\n', len(varsGeneCount))","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"7be050fc4c30a229a7b6ddae914158a63d13305b","_cell_guid":"348c060b-4eb0-4095-b2b6-d3ad85da109b"},"execution_count":7},{"source":"That will be giving us 264 different categories if we decide to encode the labels. At this point I am not sure I know what they mean either.  What about the classes?","outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_uuid":"75382b73a3c5dec0e32258a213d5a26fe0424fc8","_cell_guid":"5729768a-fa8d-4ace-b34a-60bba896acf6"},"execution_count":null},{"source":"plt.figure(figsize=(12,8))\nax = sns.countplot(x=\"Class\", data=tVars)\nplt.ylabel('Frequency'); plt.xlabel('Class')\nplt.title('Freq. of Classes in Training Variants')\nplt.show()","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"37a8a2679a6ffad3cc2b48758deeea8d4ac9cc04","_cell_guid":"cf59f037-94be-4b8d-a02d-fdde9d16b9e7"},"execution_count":8},{"source":"subfile = pd.read_csv('../input/submissionFile')\nsubfile.head(3)","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"b583a462b9ad8faa695f4415c738571b4b109b01","_cell_guid":"69fd7bbc-56a4-43c8-977c-a8d9ea1cbb27"},"execution_count":9},{"source":"With so many \"7\" classes there could be an issue of predictions skewed that way which may be something to look at down the road.","outputs":[],"cell_type":"markdown","metadata":{"_uuid":"4b516c98beb06126aaa25df6c64e82dca6560b93","_cell_guid":"d3d81347-805f-4608-82c8-200db7d6efcc"},"execution_count":null},{"source":"varsVariationCount = Counter(tVars.Variation)\nprint('Number of unique varations in trainVars.Variation: \\n', len(varsVariationCount))","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"89b393740802929b0d93848033e0998f47cb817d","_cell_guid":"f80b224a-adbc-4548-a962-c8ccb42b4ba6"},"execution_count":10},{"source":"fig, ax=plt.subplots(1,1,figsize=(12,8))\nax = sns.distplot(pd.factorize(tVars['Variation'])[0]/len(tVars), bins=150, color='r')","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"91128dbb0a8dde5c38a397a0e800379c5af8a84f","_cell_guid":"53e47f43-5826-4b52-9302-252948d7e0ba"},"execution_count":11},{"source":"Once again, many, many different categories here, and it appears there are a lot of unique ones with only 1 count. With so many in \"7\" it could be a slight disadvantage.","outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_uuid":"30ef76b6258b1220575958e80cf9ae221b998ed4","_cell_guid":"4226fd28-58d8-4040-9550-c692a42dd245"},"execution_count":null},{"source":"Since there is not that much else to look at in the Training Variants, lets take a little closer look into the Training Text. _I kept getting errors importing the stop words so I just grabbed them._","outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_uuid":"0aa42c6dd04b662dad4c406dc74e5d892967e345","_cell_guid":"555f6535-8750-4f6e-b18e-200e0878a5fd"},"execution_count":null},{"source":"def textClean(text):\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = text.lower().split()\n    stops = {'so', 'his', 't', 'y', 'ours', 'herself', \n             'your', 'all', 'some', 'they', 'i', 'of', 'didn', \n             'them', 'when', 'will', 'that', 'its', 'because', \n             'while', 'those', 'my', 'don', 'again', 'her', 'if',\n             'further', 'now', 'does', 'against', 'won', 'same', \n             'a', 'during', 'who', 'here', 'have', 'in', 'being', \n             'it', 'other', 'once', 'itself', 'hers', 'after', 're',\n             'just', 'their', 'himself', 'theirs', 'whom', 'then', 'd', \n             'out', 'm', 'mustn', 'where', 'below', 'about', 'isn',\n             'shouldn', 'wouldn', 'these', 'me', 'to', 'doesn', 'into',\n             'the', 'until', 'she', 'am', 'under', 'how', 'yourself',\n             'couldn', 'ma', 'up', 'than', 'from', 'themselves', 'yourselves',\n             'off', 'above', 'yours', 'having', 'mightn', 'needn', 'on', \n             'too', 'there', 'an', 'and', 'down', 'ourselves', 'each',\n             'hadn', 'ain', 'such', 've', 'did', 'be', 'or', 'aren', 'he', \n             'should', 'for', 'both', 'doing', 'this', 'through', 'do', 'had',\n             'own', 'but', 'were', 'over', 'not', 'are', 'few', 'by', \n             'been', 'most', 'no', 'as', 'was', 'what', 's', 'is', 'you', \n             'shan', 'between', 'wasn', 'has', 'more', 'him', 'nor',\n             'can', 'why', 'any', 'at', 'myself', 'very', 'with', 'we', \n             'which', 'hasn', 'weren', 'haven', 'our', 'll', 'only',\n             'o', 'before'}\n    ## I ketp getting errors on importing the stopwords and I have no clue why\n    #stops = set(stopwords.words(\"English\"))\n    text = [w for w in text if not w in stops]    \n    text = \" \".join(text)\n    text = text.replace(\".\",\" \").replace(\",\",\" \")\n    return(text)","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":true,"trusted":true,"_uuid":"ac381ee79f94d36262f21caf20026f7ab044541e","_cell_guid":"0ad28724-92d4-4fe8-9d61-ebed4c9d392f"},"execution_count":12},{"source":"trainText = []\nfor it in tText['text']:\n    newT = textClean(it)\n    trainText.append(newT)\ntestText = []\nfor it in vText['text']:\n    newT = textClean(it)\n    testText.append(newT)","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":true,"trusted":true,"_uuid":"9e3b17fe413440ed8ac822e035af220f853a76c9","_cell_guid":"a58ca003-43c1-459f-9cb0-430cc0f1a561"},"execution_count":13},{"source":"After a little cleaning, I wonder how the text shapes up now? Are there any other additional words that we could cut out?","outputs":[],"cell_type":"markdown","metadata":{"_uuid":"f7db25f0b35da5eff59287769089018c19b29801","_cell_guid":"d62e18c8-c273-4ffa-94ee-d0e74d5e900d"},"execution_count":null},{"source":"trainText[0][:100]","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"61d755d57489a3cffe270da7d1b9ea28876bbab0","_cell_guid":"cbf31dd0-cbc4-4fbe-8504-ec6f7ac19932"},"execution_count":14},{"source":"So each of the portions of the doc are cleaned and we can check out the most common of a few of them...","outputs":[],"cell_type":"markdown","metadata":{"_uuid":"99d6f9bec5b19f21e91f18afffd761c375265341","_cell_guid":"e9511696-3f37-4376-8b25-2ba8303b19ab"},"execution_count":null},{"source":"for i in range(10):\n    print('\\n Doc ', str(i))\n    stopCheck = Counter(trainText[i].split())\n    print(stopCheck.most_common()[:10])","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"86d33da4e02be03414f72820f3c77ca178ce8b8b","_cell_guid":"e659f5b0-0caa-4d05-927c-b177acaa7754"},"execution_count":15},{"source":"There looks like a lot of similarity here just by a quick visual examination. ","outputs":[],"cell_type":"markdown","metadata":{"_uuid":"99902141d262e8be4f7b7a1428f36abd3f47bf31","_cell_guid":"4c565a4f-fc57-4f21-b977-33bf7bcf97f8"},"execution_count":null},{"source":"tops = Counter(str(trainText).split()).most_common()[:20]\nlabs, vals = zip(*tops)\nidx = np.arange(len(labs))\nwid=0.6\nfig, ax=plt.subplots(1,1,figsize=(14,8))\nax=plt.bar(idx, vals, wid, color='g')\nax=plt.xticks(idx - wid/8, labs, rotation=25, size=14)\nplt.title('Top Twenty Counts of Most-Common Words Among Text')","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"f82b23c6addca196f6319c986fd6b526d6ebdc10","_cell_guid":"c146ac32-6074-40f8-93b4-33666a61ce4b"},"execution_count":16},{"source":"gc.collect()","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"b68a70e110c502291d84680cab06e51198952662","_cell_guid":"13e4349c-6b67-4fda-887b-3ca51dfe495c"},"execution_count":17},{"source":"There is some weight on the far left of this plot. Lets throw these words into another stop word check and re examine. If we really worked this into the mix, we could probably use the max_df setting in the TFIDF and Count Vectorizers below. \n\nA few time through this and I think we should drop some more words than just the top 20. Lets try 30.","outputs":[],"cell_type":"markdown","metadata":{"_uuid":"067efeba2b4b065391eaf87d41bc260c3607cc6f","_cell_guid":"14dc7d5c-1f3e-4133-b2a1-3544f8a39c68"},"execution_count":null},{"source":"topsInc = Counter(str(trainText).split()).most_common()[:30]\nlabsInc, valsInc = zip(*topsInc)","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":true,"trusted":true,"_uuid":"390c14306d530eaa4bf129c4de48563ccf5c4861","_cell_guid":"676981fd-092b-43a2-a9e7-c7dc5f37478e"},"execution_count":18},{"source":"def stopCheck(text, stops):\n    text = text.split()\n#     stops = {'mutations', 'cancer'}\n    text = [w for w in text if not w in stops]    \n    text = \" \".join(text)\n    return text","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":true,"trusted":true,"_uuid":"d51ad6696f27546f0cd5e5806e876c7ac7aa1129","_cell_guid":"4d01b950-67e2-49be-93cb-415b93e7cd32"},"execution_count":19},{"source":"trainText2 = []\nfor it in trainText:\n    newT = stopCheck(it,labsInc)\n    trainText2.append(newT)\n    \ntestText2 = []\nfor it in testText:\n    newT = stopCheck(it,labsInc)\n    testText2.append(newT)","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":true,"trusted":true,"_uuid":"c822879f99e029e06ed63793fad4d3da23537d8e","_cell_guid":"5bc9ec9c-ef03-48b1-8137-4449afb74338"},"execution_count":20},{"source":"trainText2[2][:100]","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"c627ab9293df0a1d07c2e680b3fff6790c4b5c2e","_cell_guid":"e8909477-d878-48a0-8bcd-1ed49b3e0eaf"},"execution_count":21},{"source":"tops = Counter(str(trainText2).split()).most_common()[:20]\nlabs, vals = zip(*tops)\nidx = np.arange(len(labs))\nwid=0.6\nfig, ax=plt.subplots(1,1,figsize=(14,8))\nax=plt.bar(idx, vals, wid, color='b')\nax=plt.xticks(idx - wid/8, labs, rotation=25, size=14)\nplt.title('Top Twenty Counts of Most-Common Words Among Text')","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"27c7b8be033500591260b2702c5c3ec1294d07f6","_cell_guid":"f74d2b8d-7648-401c-8bfd-dc07db96abe9"},"execution_count":22},{"source":"gc.collect()","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"1e4836fd069992a92db8a27c22326b8ef1124874","_cell_guid":"b3a045e0-26e1-4191-9dd7-b3a73edab6f7"},"execution_count":23},{"source":"This looks a little more balanced so we will go with this set of texts.","outputs":[],"cell_type":"markdown","metadata":{"_uuid":"7b818a428426ff1771ae26a64981ab274d6e51e3","_cell_guid":"fda019a6-19d7-44e5-955a-54c0431cb807"},"execution_count":null},{"source":"maxFeats = 500","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":true,"trusted":true,"_uuid":"129d1d035127e5b731d9f3c67be39645fc29b465","_cell_guid":"785cdbef-901b-40db-a858-f18b588a1971"},"execution_count":24},{"source":"tfidf = TfidfVectorizer(min_df=5, max_features=maxFeats, ngram_range=(1,3),\n                        strip_accents='unicode',\n                        lowercase =True, analyzer='word', token_pattern=r'\\w+',\n                        use_idf=True, smooth_idf=True, sublinear_tf=True, \n                        stop_words = 'english')\ntfidf.fit(trainText2)","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"ed54ee4c7cbe5f54304cd98e26d38a8bf9c4431c","_cell_guid":"d9e1721b-ae8e-47b7-8b47-766b0dfb9543"},"execution_count":null},{"source":"cvec = CountVectorizer(min_df=5, ngram_range=(1,3), max_features=maxFeats, \n                       strip_accents='unicode',\n                       lowercase =True, analyzer='word', token_pattern=r'\\w+',\n                       stop_words = 'english')\ncvec.fit(trainText2)","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"cc6d24f7e6b14f9e68ef9c9ff0c7aee4072b39af","_cell_guid":"cb1ac754-b9a6-439b-a406-cddedcd5483f"},"execution_count":null},{"source":"_3321x8595819 sparse matrix of type 'class 'numpy.int64''\n\twith 38030516 stored elements in Compressed Sparse Row format_\n    \nThis is the print output of just using nGram features 1-3. Its kind of a large item to pass around even if its sparse so limiting the maximum features helps in here.","outputs":[],"cell_type":"markdown","metadata":{"_uuid":"3986e5f22baf5a9e9d114f700941194c736caa26","_cell_guid":"7849d775-0d8e-47b1-b51e-61fdb7964fcc"},"execution_count":null},{"source":"## I played around with the componenets and 360-390 seemed to work best for me...\nsvdT = TruncatedSVD(n_components=390)\nsvdTFit = svdT.fit_transform(tfidf.transform(trainText2))","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":true,"trusted":true,"_uuid":"1189bf0b16b02dd8b2467d4e936213e15faa8f33","_cell_guid":"1b36d618-0864-4623-8866-8637553e9eed"},"execution_count":null},{"source":"This is something I did in other comps that helped out some in feature building. ","outputs":[],"cell_type":"markdown","metadata":{"_uuid":"d2263106781cbd3fc6e075ad33a1c7be2f33f3fb","_cell_guid":"e6ad94ca-4dfc-445b-8861-6919bccaa59e"},"execution_count":null},{"source":"def buildFeats(texts, variations):\n    temp = variations.copy()\n    print('Encoding...')\n    temp['Gene'] = pd.factorize(variations['Gene'])[0]\n    temp['Variation'] = pd.factorize(variations['Variation'])[0]\n    temp['Gene_to_Variation_Ratio'] = temp['Gene']/temp['Variation']\n    \n    print('Lengths...')\n    temp['doc_len'] = [len(x) for x in texts]\n    temp['unique_words'] = [len(set(x))  for x in texts]\n    \n    print('TFIDF...')\n    temp_tfidf = tfidf.transform(texts)\n    temp['tfidf_sum'] = temp_tfidf.sum(axis=1)\n    temp['tfidf_mean'] = temp_tfidf.mean(axis=1)\n    temp['tfidf_len'] =  (temp_tfidf != 0).sum(axis = 1)\n    \n    print('Count Vecs...')\n    temp_cvec = cvec.transform(texts)\n    temp['cvec_sum'] = temp_cvec.sum(axis=1)\n    temp['cvec_mean'] = temp_cvec.mean(axis=1)\n    temp['cvec_len'] =  (temp_cvec != 0).sum(axis = 1)\n    \n    print('Latent Semantic Analysis Cols...')\n    tempc = list(temp.columns)\n    temp_lsa = svdT.transform(temp_tfidf)\n    \n    for i in range(np.shape(temp_lsa)[1]):\n        tempc.append('lsa'+str(i+1))\n    temp = pd.concat([temp, pd.DataFrame(temp_lsa, index=temp.index)], axis=1)\n    \n    return temp, tempc","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":true,"trusted":true,"_uuid":"b18a08198a8c3acadaebf685e1aaad952c74f1ad","_cell_guid":"bcb3a85c-9bb5-4d52-8db5-d7a9b8a079fc"},"execution_count":null},{"source":"trainDf, traincol = buildFeats(trainText2, tVars)\ntestDf, testcol = buildFeats(testText2, vVars)","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"6c2109406767c52a74ccb2ca3cd90c3e544758f5","_cell_guid":"1f2343ca-1dd5-448c-ae34-ae9b3121c0a8"},"execution_count":null},{"source":"trainDf.columns = traincol\ntestDf.columns = testcol","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":true,"trusted":true,"_uuid":"979736f82444e5b115b7af3211396f7da70723e4","_cell_guid":"81b4ca5c-353e-4b1f-ab2f-c4b2583e1daa"},"execution_count":null},{"source":"In order to get the right prediction we have to get the classes in the range of 0-8.","outputs":[],"cell_type":"markdown","metadata":{"_uuid":"79972ea7b927c35e90b9539cc25432f041895108","_cell_guid":"e5251e06-2590-4849-aef7-b6d1117bf288"},"execution_count":null},{"source":"classes = tVars.Class - 1\nprint('Original:', Counter(tVars.Class), '\\n ReHashed:', Counter(classes))","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"eac5408d27172ac77dba108dcc3035888a8123f4","_cell_guid":"ae9d17ec-0292-4e9b-bd97-f18ce2a48a62"},"execution_count":null},{"source":"dft, dfv, yt, yv = train_test_split(trainDf.drop(['ID','Class'],axis=1),\n                                    classes,\n                                    test_size = 0.1,\n                                    random_state=31415)\nprint(np.shape(dft))","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"1cb89434694f46129a8515fc3b6690662ed5cd08","_cell_guid":"ded543a0-48b3-424d-b939-1d6e7092c586"},"execution_count":null},{"source":"import gc\nprint('Format a Train and Validation Set for LGB')\nd_train = lgb.Dataset(dft, label=yt)\nd_val = lgb.Dataset(dfv, label=yv)\n               \ngc.collect()","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"f1775010186be23ef1aad07fff2f9725685df634","_cell_guid":"eb33094f-bce6-4961-b036-c9decd09688e"},"execution_count":null},{"source":"parms = {'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'multiclass',\n    'num_class': 9,\n    'metric': {'multi_logloss'},\n    'learning_rate': 0.05, \n    'max_depth': 5,\n    'num_iterations': 400, \n    'num_leaves': 95, \n    'min_data_in_leaf': 60, \n    'lambda_l1': 1.0,\n    'feature_fraction': 0.8, \n    'bagging_fraction': 0.8, \n    'bagging_freq': 5}\n\nrnds = 260\nmod = lgb.train(parms, train_set=d_train, num_boost_round=rnds,\n               valid_sets=[d_val], valid_names=['dval'], verbose_eval=20,\n               early_stopping_rounds=20)\n","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"1c1ff1e22fa950e570c22344e885ad1a58a6c993","_cell_guid":"92fd9fcf-54d4-46c2-b6ca-a7038de62dfe"},"execution_count":null},{"source":"import matplotlib.pyplot as plt\n%matplotlib inline\nlgb.plot_importance(mod, max_num_features=30, figsize=(14,10))","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"98f992fbcb0e0e676fcfb169b76ab631eef5e869","_cell_guid":"1ea3304f-a8d9-486f-bb00-e779b0e098a7"},"execution_count":null},{"source":"pred = mod.predict(testDf.drop(['ID'],axis=1))","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":true,"trusted":true,"_uuid":"657f3527ecc5ff90f7a996836c553a10dd9e4c00","_cell_guid":"47b5af88-a5fc-466d-84bb-8e3270d1c378"},"execution_count":null},{"source":"sub = pd.DataFrame(pred, index=testDf.index)\nsub.columns = subfile.columns[1:]\nsub.index_name = subfile.columns[0]\nsub['ID'] = testDf.index\nsub.head()","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"21ddd5c22c78e0ebcf6ab1609fbc82e64876f2d6","_cell_guid":"c51b9d74-5148-4b0e-8bcd-63d9aa1ddb6d"},"execution_count":null},{"source":"import datetime\nnow = datetime.datetime.now()\nsub.to_csv('lgb_'+str(now.strftime(\"%Y-%m-%d-%H-%M\"))+'.csv', index=False)","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":true,"_uuid":"0d0410e0b7ff3d8d4924ac56146052b0b2e0daf7","_cell_guid":"fe1f6fe5-d596-4ccf-848d-6088cf1a2914"},"execution_count":null},{"source":"","outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":true,"trusted":true,"_uuid":"cea89d1a1eaaf15857a420f8450a2dae79b4c783","_cell_guid":"9ae2564f-f0e2-4b6e-a4d6-3c4c0f4732db"},"execution_count":null}],"metadata":{"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.1","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","mimetype":"text/x-python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}}}