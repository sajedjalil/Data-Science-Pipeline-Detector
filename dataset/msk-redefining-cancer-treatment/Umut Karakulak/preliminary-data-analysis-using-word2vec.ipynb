{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","version":"3.6.1","file_extension":".py","name":"python","nbconvert_exporter":"python","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3}},"anaconda-cloud":{}},"nbformat_minor":1,"nbformat":4,"cells":[{"metadata":{"_cell_guid":"735da42a-f22f-4593-8064-e84aeed4ad8a","_uuid":"6db1914e33b0978c91f82ba1f39763c03e8b6bb2"},"cell_type":"markdown","source":"# A simple anaysis of the dataset using nltk and Word2Vec\nThis notebook goes over the dataset in the following order:\n- Read the data into a dataframe using pandas library.\n- Cleaning unnecessary data (unique or null columns).\n- Analyzing data distributions.\n- Analyzing text data via keywords and summarization.\n- Tokenizing (Lemmatization and stopwording) for further analysis.\n- Analyzing word distributions for any surface correlations.\n- Creating a word cloud of the whole text.\n- Using Word2Vec to check the correlation between text and the classes.\n  \n------  \n**Disclaimer:** I couldn't find a way to upload the trained word2vec weight vectors to kaggle kernel, so I just attached the results as markdown. More thorough version can be found on my [github](https://github.com/umutto/Kaggle-Personalized-Medicine/blob/master/data_analysis.ipynb).  \n  \n*This kernel has been tested with python 3.6 (x64) on Windows.*"},{"metadata":{"collapsed":true,"_cell_guid":"68bc6008-0c54-4242-88e8-ece657edafbf","_uuid":"2743be3dcb624cc8038a878530d95b96aadb74e9"},"cell_type":"code","execution_count":null,"outputs":[],"source":"%matplotlib inline\n\n# Data wrapper libraries\nimport pandas as pd\nimport numpy as np\n\n# Visualization Libraries\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch\nfrom matplotlib.markers import MarkerStyle\nimport seaborn as sns\n\n# Text analysis helper libraries\nfrom gensim.summarization import summarize\nfrom gensim.summarization import keywords\n\n# Text analysis helper libraries for word frequency etc..\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\n# Word cloud visualization libraries\nfrom scipy.misc import imresize\nfrom PIL import Image\nfrom wordcloud import WordCloud, ImageColorGenerator\nfrom collections import Counter\n\n# Word2Vec related libraries\nfrom gensim.models import KeyedVectors\n\n# Dimensionaly reduction libraries\nfrom sklearn.decomposition import PCA\n\n# Clustering library\nfrom sklearn.cluster import KMeans\n\n# Set figure size a bit bigger than default so everything is easily red\nplt.rcParams[\"figure.figsize\"] = (11, 7)"},{"metadata":{"_cell_guid":"9f1cac1b-973c-459e-b019-8f51942804e0","_uuid":"5639b952184820abd153255842cfbff9106d35b1"},"cell_type":"markdown","source":"Let's take a casual look at the *variants* data."},{"metadata":{"collapsed":true,"_cell_guid":"a10c73a9-ef65-439f-8633-bebe9fe3bd99","_uuid":"ee1e3624e86beaa3eeb142e56e82f95c2e348e1b"},"cell_type":"code","execution_count":null,"outputs":[],"source":"df_variants = pd.read_csv('../input/training_variants').set_index('ID')\ndf_variants.head()"},{"metadata":{"_cell_guid":"499507a5-bd67-40bd-9e8c-93b482959eed","_uuid":"95dd47fc041bdd22a01f38de37b414eb3ba8d8f2"},"cell_type":"markdown","source":"Let's take a look at the *text* data. Data is still small enough for memory so read to memory using pandas."},{"metadata":{"collapsed":true,"_cell_guid":"a7bb0422-5cc7-4c5b-91ec-f1d99d702670","_uuid":"88af716db0de30be05863325eca3e58cb135a1f9"},"cell_type":"code","execution_count":null,"outputs":[],"source":"df_text = pd.read_csv('../input/training_text', sep='\\|\\|', engine='python', \n                      skiprows=1, names=['ID', 'Text']).set_index('ID')\ndf_text.head()"},{"metadata":{"_cell_guid":"68ba24b9-33b2-413f-b0aa-970cda386534","_uuid":"cd7b9286af6f8cf58b80825654987084ebc1969e"},"cell_type":"markdown","source":"Join two dataframes on index"},{"metadata":{"collapsed":true,"_cell_guid":"6555e149-7319-491e-8ae3-a282c87386fa","_uuid":"374274aec5a4ff115c3e7fbf68d152c5fde634be"},"cell_type":"code","execution_count":null,"outputs":[],"source":"df = pd.concat([df_variants, df_text], axis=1)\ndf.head()"},{"metadata":{"_cell_guid":"106c6353-c822-4a2d-a4de-6cd95b63de12","_uuid":"b40c4a376c687694bcc0ea7b170b26760f9190c1"},"cell_type":"markdown","source":"*Variation* column is mostly consists of independant unique values. So its not very helpfull for our predictions. So we will drop it."},{"metadata":{"collapsed":true,"_cell_guid":"ab3dece5-13dd-4fc4-a039-cf39eabc15dd","_uuid":"5bcfcd99c12f59df3dfc52d389fd4ebc4eec1294"},"cell_type":"code","execution_count":null,"outputs":[],"source":"df['Variation'].describe()"},{"metadata":{"_cell_guid":"946b3375-7bdf-40c2-a75a-5a9e8388f551","_uuid":"1bf47b2c689ed94f6eb4635f56dfa2e24dfd9520"},"cell_type":"markdown","source":"*Gene* column is a bit more complicated, values seems to be heavly skewed.  \nData can still be valuable if normalized and balanced by weights.  "},{"metadata":{"collapsed":true,"_cell_guid":"33d8f4c4-37a1-41e0-a3d4-eab554f4ba33","_uuid":"ee676b737ba8b178b7e18db4c7814994012b73eb"},"cell_type":"code","execution_count":null,"outputs":[],"source":"plt.figure()\nax = df['Gene'].value_counts().plot(kind='area')\n\nax.get_xaxis().set_ticks([])\nax.set_title('Gene Frequency Plot')\nax.set_xlabel('Gene')\nax.set_ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()"},{"metadata":{"_cell_guid":"5a045f1c-0eb5-43f2-bd44-f0e17dc6bb22","_uuid":"1e942cbeaa4c5e63f6b71177596ab6d1b3fb2078"},"cell_type":"markdown","source":"Even with domination of some gene's, it still gives a nice insight from their class distributions.  \nBut not to over complicate things for this kernel, we'll skip that and drop it as well."},{"metadata":{"collapsed":true,"_cell_guid":"0e990288-e4f7-4ce4-bfac-e4a09fd0278b","_uuid":"5bdba79cf75f4a1d8552fb03e86dd75db88428f4"},"cell_type":"code","execution_count":null,"outputs":[],"source":"fig, axes = plt.subplots(nrows=3, ncols=3, sharey=True, figsize=(11,11))\n\n# Normalize value counts for better comparison\ndef normalize_group(x):\n    label, repetition = x.index, x\n    t = sum(repetition)\n    r = [n/t for n in repetition]\n    return label, r\n\nfor idx, g in enumerate(df.groupby('Class')):\n    label, val = normalize_group(g[1][\"Gene\"].value_counts())\n    ax = axes.flat[idx]\n    ax.bar(np.arange(5), val[:5],\n           tick_label=label[:5]) \n    ax.set_title(\"Class {}\".format(g[0]))\n    \nfig.text(0.5, 0.97, '(Top 5) Gene Frequency per Class', ha='center', fontsize=14, fontweight='bold')\nfig.text(0.5, 0, 'Gene', ha='center', fontweight='bold')\nfig.text(0, 0.5, 'Frequency', va='center', rotation='vertical', fontweight='bold')\nfig.tight_layout(rect=[0.03, 0.03, 0.95, 0.95])"},{"metadata":{"_cell_guid":"aea60649-f512-4717-8f68-3686001ea50a","_uuid":"2a3a67be45d9c7dfa98963f38668041a856ee0fc"},"cell_type":"markdown","source":"And finally lets look at the class distribution."},{"metadata":{"collapsed":true,"_cell_guid":"fa9fed90-0725-4a2c-9814-15ef946c90e5","_uuid":"a42e7661c3824cb73e3631fd4650656e232c4371"},"cell_type":"code","execution_count":null,"outputs":[],"source":"plt.figure()\nax = df['Class'].value_counts().plot(kind='bar')\n\nax.set_title('Class Distribution Over Entries')\nax.set_xlabel('Class')\nax.set_ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()"},{"metadata":{"_cell_guid":"c7cf14fb-cc6f-43a8-95d6-9c178810bee1","_uuid":"1c61e9d05a5beeeee35314782f1fa640f2af8f7a"},"cell_type":"markdown","source":"Distribution looks skewed towards some classes, there are not enough examples for classes 8 and 9. During training, this can be solved using bias weights, careful sampling in batches or simply removing some of the dominant data to equalize the field.  \n  \n  ----\nFinally, lets drop the columns we don't need and be done with the initial cleaning."},{"metadata":{"collapsed":true,"_cell_guid":"7945dde4-ea1c-4192-9211-1470c474b30b","_uuid":"98e3dc99311f96bd90e0aabd50b66c8026d07254"},"cell_type":"code","execution_count":null,"outputs":[],"source":"df.drop(['Gene', 'Variation'], axis=1, inplace=True)\n\n# Additionaly we will drop the null labeled texts too\ndf = df[df['Text'] != 'null']"},{"metadata":{"_cell_guid":"e184bf23-f789-495f-8901-453c27f173c3","_uuid":"569ff28d633782e0abfaa6b9e5c10ef33ebb309e"},"cell_type":"markdown","source":"Now let's look at the remaining data in more detail.  \nText is too long and detailed and technical, so I've decided to summarize it using gensim's TextRank algorithm.  \nStill didn't understand anything :/"},{"metadata":{"collapsed":true,"_cell_guid":"38580240-7231-4ea2-8248-a10efe88e074","_uuid":"37f88c2d93d9a5b8d4221ef243594af3d64fd73b"},"cell_type":"code","execution_count":null,"outputs":[],"source":"t_id = 0\ntext = df.loc[t_id, 'Text']\n\nword_scores = keywords(text, words=5, scores=True, split=True, lemmatize=True)\nword_scores = ', '.join(['{}-{:.2f}'.format(k, s[0]) for k, s in word_scores])\nsummary = summarize(text, word_count=100)\n\nprint('ID [{}]\\nKeywords: [{}]\\nSummary: [{}]'.format(t_id, word_scores, summary))"},{"metadata":{"_cell_guid":"fbdc05b8-1db5-4c8d-b4fb-b5435282be78","_uuid":"92bef461c126b63add7ac87a17860196b5b89003"},"cell_type":"markdown","source":"Text is tokenized, cleaned of stopwords and lemmatized for word frequency analysis.  \n\nTokenization obviously takes a lot of time on a corpus like this. So bear that in mind.  \nMay skip this, use a simpler tokenizer like `ToktokTokenizer` or just use `str.split()` instead."},{"metadata":{"collapsed":true,"_cell_guid":"477b75f6-5e86-4013-8d79-fa6e45481377","_uuid":"1959056b14f476f60d7e92494aa3917653d4140f"},"cell_type":"code","execution_count":null,"outputs":[],"source":"custom_words = [\"fig\", \"figure\", \"et\", \"al\", \"al.\", \"also\",\n                \"data\", \"analyze\", \"study\", \"table\", \"using\",\n                \"method\", \"result\", \"conclusion\", \"author\", \n                \"find\", \"found\", \"show\", '\"', \"’\", \"“\", \"”\"]\n\nstop_words = set(stopwords.words('english') + list(punctuation) + custom_words)\nwordnet_lemmatizer = WordNetLemmatizer()\n\nclass_corpus = df.groupby('Class').apply(lambda x: x['Text'].str.cat())\nclass_corpus = class_corpus.apply(lambda x: Counter(\n    [wordnet_lemmatizer.lemmatize(w) \n     for w in word_tokenize(x) \n     if w.lower() not in stop_words and not w.isdigit()]\n))"},{"metadata":{"_cell_guid":"77f8ebf2-d600-4232-92f7-11cc8f909d37","_uuid":"5c399a507dec0036c7d60d32cc25355b4b5380ea"},"cell_type":"markdown","source":"Lets look at the dominant words in classes. And see if we can find any correlation."},{"metadata":{"collapsed":true,"_cell_guid":"d173959f-1c85-40ec-8aa1-d6aa6a5ea24d","_uuid":"7fae6828866caa4766ab8a1e45b48bb9e8ac529d"},"cell_type":"code","execution_count":null,"outputs":[],"source":"class_freq = class_corpus.apply(lambda x: x.most_common(5))\nclass_freq = pd.DataFrame.from_records(class_freq.values.tolist()).set_index(class_freq.index)\n\ndef normalize_row(x):\n    label, repetition = zip(*x)\n    t = sum(repetition)\n    r = [n/t for n in repetition]\n    return list(zip(label,r))\n\nclass_freq = class_freq.apply(lambda x: normalize_row(x), axis=1)\n\n# set unique colors for each word so it's easier to read\nall_labels = [x for x in class_freq.sum().sum() if isinstance(x,str)]\nunique_labels = set(all_labels)\ncm = plt.get_cmap('Blues_r', len(all_labels))\ncolors = {k:cm(all_labels.index(k)/len(all_labels)) for k in all_labels}\n\nfig, ax = plt.subplots()\n\noffset = np.zeros(9)\nfor r in class_freq.iteritems():\n    label, repetition = zip(*r[1])\n    ax.barh(range(len(class_freq)), repetition, left=offset, color=[colors[l] for l in label])\n    offset += repetition\n    \nax.set_yticks(np.arange(len(class_freq)))\nax.set_yticklabels(class_freq.index)\nax.invert_yaxis()\n\n# annotate words\noffset_x = np.zeros(9) \nfor idx, a in enumerate(ax.patches):\n    fc = 'k' if sum(a.get_fc()) > 2.5 else 'w'\n    ax.text(offset_x[idx%9] + a.get_width()/2, a.get_y() + a.get_height()/2, \n            '{}\\n{:.2%}'.format(all_labels[idx], a.get_width()), \n            ha='center', va='center', color=fc, fontsize=14, family='monospace')\n    offset_x[idx%9] += a.get_width()\n    \nax.set_title('Most common words in each class')\nax.set_xlabel('Word Frequency')\nax.set_ylabel('Classes')\n\nplt.tight_layout()\nplt.show()"},{"metadata":{"_cell_guid":"4e716a9f-ee3f-4d2e-b392-31ee5a3d33c3","_uuid":"2befd23a2138413de89da78427ce703bfddda446"},"cell_type":"markdown","source":"**Mutation** and **cell** seems to be commonly dominating in all classes, not very informative. But the graph is still helpful. And would give more insight if we were to ignore most common words.  \nLet's plot how many times 25 most common words appear in the whole corpus."},{"metadata":{"collapsed":true,"_cell_guid":"ca31e217-7354-4138-9247-c7c9f75f75e2","_uuid":"6e9ccd0be0d8775cb9b6368b124f18d28293c943"},"cell_type":"code","execution_count":null,"outputs":[],"source":"whole_text_freq = class_corpus.sum()\n\nfig, ax = plt.subplots()\n\nlabel, repetition = zip(*whole_text_freq.most_common(25))\n\nax.barh(range(len(label)), repetition, align='center')\nax.set_yticks(np.arange(len(label)))\nax.set_yticklabels(label)\nax.invert_yaxis()\n\nax.set_title('Word Distribution Over Whole Text')\nax.set_xlabel('# of repetitions')\nax.set_ylabel('Word')\n\nplt.tight_layout()\nplt.show()"},{"metadata":{"_cell_guid":"0fddd5bc-953f-4107-9c97-f2a744f6389e","_uuid":"3a3a31d03094f2b5f4d8b6d08d36f44a1071a008"},"cell_type":"markdown","source":"-----\n## Sadly code below is copied and pasted as markdown (couldn't upload the files I need to run into Kaggle)  \n  \nMore thorough version can be found on my [github](https://github.com/umutto/Kaggle-Personalized-Medicine/blob/master/data_analysis.ipynb).    \n\n-----"},{"metadata":{"_cell_guid":"049e10ea-3ece-4f30-8b0e-c409da9dfb6c","_uuid":"a17cf8c79c1044fe53dcb523a0dd1c11be757f8f"},"cell_type":"markdown","source":"Words are plotted to a word cloud using the beautiful [word_cloud](https://github.com/amueller/word_cloud) library.  \nThis part is unnecessary for analysis but pretty =)."},{"metadata":{"collapsed":true,"_cell_guid":"d5a13ffb-5f01-42cb-89d0-d8c4e36d09aa","_uuid":"00c9cdc80160412a7daff2f4c5375d1971d114db"},"cell_type":"markdown","source":"\n```python\ndef resize_image(np_img, new_size):\n    old_size = np_img.shape\n    ratio = min(new_size[0]/old_size[0], new_size[1]/old_size[1])\n    \n    return imresize(np_img, (round(old_size[0]*ratio), round(old_size[1]*ratio)))\n\nmask_image = np.array(Image.open('tmp/dna_stencil.png').convert('L'))\nmask_image = resize_image(mask_image, (4000, 2000))\n\nwc = WordCloud(max_font_size=140,\n               min_font_size=8,\n               max_words=1000,\n               width=mask_image.shape[1], \n               height=mask_image.shape[0],\n               prefer_horizontal=.9,\n               relative_scaling=.52,\n               background_color=None,\n               mask=mask_image,\n               mode=\"RGBA\").generate_from_frequencies(freq)\n\nplt.figure()\nplt.axis(\"off\")\nplt.tight_layout()\nplt.imshow(wc, interpolation=\"bilinear\")\n```"},{"metadata":{"_cell_guid":"23bfe63a-04db-4a55-a8a2-466d15b61173","_uuid":"df09aba16cc624410608972716b0b7b198c1dc7f"},"cell_type":"markdown","source":"![](http://i.imgur.com/oRQptjx.png?1)\n  \nWe can also use the text data and visualize the relationships between words using Word2Vec. Even average the word vectors of a sentence and visualize the relationship between sentences.  \n(Doc2Vec could give much better results, for simplicity averaging word vectors are sufficient for this kernel)  \n  \nWe'll use gensim's word2vec algorithm with Google's (huge) pretrained word2vec tokens."},{"metadata":{"_cell_guid":"ea4cb51e-62cb-4a6f-a0f0-283d933dba02","_uuid":"d8b44273593359926562761e526a9e66ebc1d841"},"cell_type":"markdown","source":"```python\nvector_path = r\"word_vectors\\GoogleNews-vectors-negative300.bin\"\n\nmodel = KeyedVectors.load_word2vec_format (vector_path, binary=True)\nmodel.wv.similar_by_word('mutation')\n```"},{"metadata":{"_cell_guid":"c19f95a8-bcde-491b-8196-da0722525daa","_uuid":"cb775987ccc9d4648f7807b6d4f7db074456ce1d"},"cell_type":"markdown","source":"```\n[('mutations', 0.8541924953460693),  \n ('genetic_mutation', 0.8245046138763428),  \n ('mutated_gene', 0.7879971861839294),  \n ('gene_mutation', 0.7823827266693115),  \n ('genetic_mutations', 0.7393667697906494),  \n ('gene', 0.7343351244926453),  \n ('gene_mutations', 0.7275242209434509),  \n ('genetic_variant', 0.7182294726371765),  \n ('alleles', 0.7164379358291626),  \n ('mutant_gene', 0.7144376039505005)] \n ```\n\nThe results of word2vec looks really promising.  \n  \n----\nNow that we can somewhat understand the relationship between words, we'll use that to understand the relationship between sentences and documents. I'll be simply averaging the word vectors over a sentence, but better ways exist like using idf weighted averages or training a paragraph2vec model from scratch over the corpus."},{"metadata":{"_cell_guid":"5f22bd25-a44c-4897-82f4-b73ccc91f107","_uuid":"f6eadf5e8ac5f408aba6ef6422f7ff56f422f501"},"cell_type":"markdown","source":"```python\ndef get_average_vector(text):\n    tokens = [w.lower() for w in word_tokenize(text) if w.lower() not in stop_words]\n    return np.mean(np.array([model.wv[w] for w in tokens if w in model]), axis=0)\n\nmodel.wv.similar_by_vector(get_average_vector(df.loc[0, 'Text']))\n```"},{"metadata":{"_cell_guid":"2f043df4-f885-4a9d-b015-b4105b590df5","_uuid":"8105d55a5b99a39abd2fa224bb91bf315ba9c92b"},"cell_type":"markdown","source":"```\n[('cyclic_AMP_cAMP', 0.7930851578712463),\n ('mRNA_transcripts', 0.7838510274887085),\n ('oncogenic_transformation', 0.7836254239082336),\n ('MT1_MMP', 0.7755827307701111),\n ('microRNA_molecule', 0.773587703704834),\n ('tumorigenicity', 0.7722263932228088),\n ('coexpression', 0.7706621885299683),\n ('transgenic_mice_expressing', 0.7698256969451904),\n ('pleiotropic', 0.7698150873184204),\n ('cyclin_B1', 0.7696200013160706)]\n```\n  \nAnd finally we can visualize the relationships between sentences by averaging the vector representations of each word in a sentence and reducing the vector dimensions to 2D (Google's Word2Vec embeddings come as [,300] vectors).  \nI will use PCA for dimensionality reduction because it usually is faster (and/or uses less memory) but t-sne could give better results."},{"metadata":{"_cell_guid":"b76c710d-7e6c-483a-a588-e49afb45dbcc","_uuid":"fa21c1bfe9a8ca1f8a8a5e2aff7ad062f3a09f55"},"cell_type":"markdown","source":"```python\ntext_vecs = df.apply(lambda x: (x['Class'], get_average_vector(x['Text'])), axis=1)\nclasses, vecs = list(zip(*text_vecs.values))\n\npca = PCA(n_components=2)\nreduced_vecs = pca.fit_transform(vecs)\n\nfig, ax = plt.subplots()\n\ncm = plt.get_cmap('jet', 9)\ncolors = [cm(i/9) for i in range(9)]\nax.scatter(reduced_vecs[:,0], reduced_vecs[:,1], c=[colors[c-1] for c in classes], cmap='jet', s=8)\n\n\nplt.legend(handles=[Patch(color=colors[i], label='Class {}'.format(i+1)) for i in range(9)])\n\nplt.show()\n```"},{"metadata":{"_cell_guid":"9b685559-a110-4a2b-8f92-cf3315885fd3","_uuid":"18c11dc03496714f5b00370dbda502e83fdb0417"},"cell_type":"markdown","source":"![](http://i.imgur.com/hT6GIAK.png)\n  \nNo imminent correlation can be seen based on this analysis.  \nThis may be due to:\n- Dimensional Reduction (we may not be seeing the correlation in 2D).\n- Averaging word vectors are not effective solutions to infer sentence/paragraph vectors.\n- There is no obvious correlation between texts.\n  \nIn any case let's see the difference with a simple k-means clustering."},{"metadata":{"_cell_guid":"a13d5c53-5372-4d08-ba14-8da6f3b8513a","_uuid":"b3597d2537eb04ee864e804b5a8b34d077ba89eb"},"cell_type":"markdown","source":"```python\nkmeans = KMeans(n_clusters=9).fit(vecs)\nc_labels = kmeans.labels_\n\nfig, ax = plt.subplots()\n\ncm = plt.get_cmap('jet', 9)\ncolors = [cm(i/9) for i in range(9)]\nax.scatter(reduced_vecs[:,0], reduced_vecs[:,1], c=[colors[c-1] for c in c_labels], cmap='jet', s=8)\n\nplt.legend(handles=[Patch(color=colors[i], label='Class {}'.format(i+1)) for i in range(9)])\n\nplt.show()\n```"},{"metadata":{"_cell_guid":"972d9632-0d7e-48a0-a874-93e6acfb6227","_uuid":"959de5583cb023177f8a26c2622ae218bad76240"},"cell_type":"markdown","source":"![](http://i.imgur.com/IYjRzd0.png)"}]}