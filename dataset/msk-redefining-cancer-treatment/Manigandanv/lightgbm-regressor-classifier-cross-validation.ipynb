{"nbformat_minor":0,"cells":[{"cell_type":"markdown","outputs":[],"execution_count":null,"metadata":{"_execution_state":"idle","_cell_guid":"515cc8c8-e082-45e7-adf9-a661032d36de","collapsed":false,"_uuid":"cdd14503df107f852dc3b0814ca29f3212ec2463"},"source":"Lightgbm regressor.This lightgbm is even consuming less memory and even runs in my machine as well. Any new ideas are welcome!"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_execution_state":"idle","_cell_guid":"3f639d94-f52f-4d73-901a-99a370d16e34","trusted":false,"_uuid":"3fdc2ff65c042e0dcefcc7d0fc011e8a38ff0a48"},"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy.sparse as ssp\n\nfrom sklearn import metrics, model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\ntrain_variants_df = pd.read_csv(\"../input/training_variants\")\ntest_variants_df = pd.read_csv(\"../input/test_variants\")\ntrain_text_df = pd.read_csv(\"../input/training_text\", sep=\"\\|\\|\", engine=\"python\", skiprows=1, names=[\"ID\", \"Text\"])\ntest_text_df = pd.read_csv(\"../input/test_text\", sep=\"\\|\\|\", engine=\"python\", skiprows=1, names=[\"ID\", \"Text\"])\n\ntfidf = TfidfVectorizer(\n\tmin_df=5, max_features=500, strip_accents='unicode',lowercase =True,\n\tanalyzer='word', token_pattern=r'\\w+', use_idf=True, \n\tsmooth_idf=True, sublinear_tf=True, stop_words = 'english').fit(train_text_df[\"Text\"])\n\ntest_data = train_text_df.append(test_text_df)\nX_tfidf_text = tfidf.transform(test_data[\"Text\"])\n\n#Feature reduction. \nfrom sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(200)\nSVD_data = svd.fit_transform(X_tfidf_text)\n\nX_train_text = SVD_data [:train_text_df.shape[0]]\nX_test_text = SVD_data [train_text_df.shape[0]:]\n\nfeatures = tfidf.get_feature_names()\n\nID_train = train_variants_df.ID\nID_test = test_variants_df.ID\n\ny = train_variants_df.Class.values-1\n\ntrain_variants_df = train_variants_df.drop(['ID','Class'], axis=1)\ntest_variants_df = test_variants_df.drop(['ID'], axis=1)\n\ndata = train_variants_df.append(test_variants_df)\n\nX_data = pd.get_dummies(data).values\n\nX = X_data[:train_variants_df.shape[0]]\nX_test = X_data[train_variants_df.shape[0]:]\n\nX = ssp.hstack([pd.DataFrame(X_train_text), X], format='csr')\nX_test = ssp.hstack((pd.DataFrame(X_test_text), X_test), format='csr')\n\ny_test = np.zeros((X_test.shape[0], max(y)+1))\n\n#LightGBM Regressor\nimport lightgbm\nfrom lightgbm import LGBMRegressor\nfrom lightgbm import LGBMClassifier\nmodel = LGBMRegressor(boosting_type='gbdt', objective='multiclass',num_class=9,\n                      early_stopping = 50,num_iteration=10000,num_leaves=31,is_enable_sparse='true',\n                      tree_learner='data',min_data_in_leaf=600,max_depth=4, learning_rate=0.1, \n                      n_estimators=675, max_bin=255, subsample_for_bin=50000, min_split_gain=5, \n                      min_child_weight=5, min_child_samples=10, subsample=0.995, subsample_freq=1, \n                      colsample_bytree=1, reg_alpha=0, reg_lambda=0, seed=0, nthread=-1, silent=True)\n\n#Fit to training data\nmodel.fit(X, y)\n#Generate Predictions\ny_pred=model.predict(X_test)\nclasses = \"class1,class2,class3,class4,class5,class6,class7,class8,class9\".split(',')\nsubm = pd.DataFrame(y_pred, columns=classes)\nsubm['ID'] = ID_test\n#Save predictions to 'output.csv'\nsubm.to_csv('output.csv', index=False)"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_execution_state":"idle","_cell_guid":"ec5d2968-d208-4c3d-a739-c0fdc74674ea","trusted":false,"collapsed":false,"_uuid":"0d317e8f64ae5acdf2744b7f4e973c2efa90910b"},"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy.sparse as ssp\n\nfrom sklearn import metrics, model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\ntrain_variants_df = pd.read_csv(\"../input/training_variants\")\ntest_variants_df = pd.read_csv(\"../input/test_variants\")\ntrain_text_df = pd.read_csv(\"../input/training_text\", sep=\"\\|\\|\", engine=\"python\", skiprows=1, names=[\"ID\", \"Text\"])\ntest_text_df = pd.read_csv(\"../input/test_text\", sep=\"\\|\\|\", engine=\"python\", skiprows=1, names=[\"ID\", \"Text\"])\n\ntfidf = TfidfVectorizer(\n\tmin_df=5, max_features=500, strip_accents='unicode',lowercase =True,\n\tanalyzer='word', token_pattern=r'\\w+', use_idf=True, \n\tsmooth_idf=True, sublinear_tf=True, stop_words = 'english').fit(train_text_df[\"Text\"])\n\ntest_data = train_text_df.append(test_text_df)\nX_tfidf_text = tfidf.transform(test_data[\"Text\"])\n\n#Feature reduction. \nfrom sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(200)\nSVD_data = svd.fit_transform(X_tfidf_text)\n\nX_train_text = SVD_data [:train_text_df.shape[0]]\nX_test_text = SVD_data [train_text_df.shape[0]:]\n\nfeatures = tfidf.get_feature_names()\n\nID_train = train_variants_df.ID\nID_test = test_variants_df.ID\n\ny = train_variants_df.Class.values-1\n\ntrain_variants_df = train_variants_df.drop(['ID','Class'], axis=1)\ntest_variants_df = test_variants_df.drop(['ID'], axis=1)\n\ndata = train_variants_df.append(test_variants_df)\n\nX_data = pd.get_dummies(data).values\n\nX = X_data[:train_variants_df.shape[0]]\nX_test = X_data[train_variants_df.shape[0]:]\n\nX = ssp.hstack([pd.DataFrame(X_train_text), X], format='csr')\nX_test = ssp.hstack((pd.DataFrame(X_test_text), X_test), format='csr')\n\ny_test = np.zeros((X_test.shape[0], max(y)+1))\n\n#LightGBM classifier\nimport lightgbm\nfrom lightgbm import LGBMRegressor\nfrom lightgbm import LGBMClassifier\nmodel = LGBMClassifier(boosting_type='gbdt', objective='multiclass',\n                       num_class=9,early_stopping = 50,num_iteration=10000,num_leaves=31,\n                       is_enable_sparse='true',tree_learner='data',min_data_in_leaf=600,max_depth=4,\n                       learning_rate=0.01, n_estimators=675, max_bin=255, subsample_for_bin=50000, \n                       min_split_gain=5, min_child_weight=5, min_child_samples=10, subsample=0.995, \n                       subsample_freq=1, colsample_bytree=1, reg_alpha=0, \n                       reg_lambda=0, seed=0, nthread=-1, silent=True)\n\n#Fit to training data\nmodel.fit(X, y)\n#Generate Predictions\ny_pred=model.predict_proba(X_test)\nclasses = \"class1,class2,class3,class4,class5,class6,class7,class8,class9\".split(',')\nsubm = pd.DataFrame(y_pred, columns=classes)\nsubm['ID'] = ID_test\n#Save predictions to 'output.csv'\nsubm.to_csv('outputclassic1.csv', index=False)"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_execution_state":"idle","_cell_guid":"01119d40-6ee2-4708-a568-d03bb6fdf140","trusted":false,"collapsed":false,"_uuid":"a73496580e606e582b4f7e6809e0e5a45c0dc381"},"source":"from sklearn import *\nimport sklearn\nimport pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('../input/training_variants')\ntest = pd.read_csv('../input/test_variants')\ntrainx = pd.read_csv('../input/training_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\ntestx = pd.read_csv('../input/test_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n\ntrain = pd.merge(train, trainx, how='left', on='ID')\ny = train['Class'].values\ntrain = train.drop('Class', axis=1)\n\ntest = pd.merge(test, testx, how='left', on='ID')\npid = test['ID'].values\n\ndf_all = pd.concat((train, test), axis=0, ignore_index=True)\ndf_all['Gene_Share'] = df_all.apply(lambda r: sum([1 for w in r['Gene'].split(' ') if w in r['Text'].split(' ')]), axis=1)\ndf_all['Variation_Share'] = df_all.apply(lambda r: sum([1 for w in r['Variation'].split(' ') if w in r['Text'].split(' ')]), axis=1)\n\n#commented for Kaggle Limits\n'''for i in range(56):\n    df_all['Gene_'+str(i)] = df_all['Gene'].map(lambda x: str(x[i]) if len(x)>i else '')\n    df_all['Variation'+str(i)] = df_all['Variation'].map(lambda x: str(x[i]) if len(x)>i else '')'''\n\n\ngen_var_lst = sorted(list(train.Gene.unique()) + list(train.Variation.unique()))\nprint(len(gen_var_lst))\ngen_var_lst = [x for x in gen_var_lst if len(x.split(' '))==1]\nprint(len(gen_var_lst))\ni_ = 0\n#commented for Kaggle Limits\n'''for gen_var_lst_itm in gen_var_lst:\n    if i_ % 100 == 0: print(i_)\n    df_all['GV_'+str(gen_var_lst_itm)] = df_all['Text'].map(lambda x: str(x).count(str(gen_var_lst_itm)))\n    i_ += 1'''\n\nfor c in df_all.columns:\n    if df_all[c].dtype == 'object':\n        if c in ['Gene','Variation']:\n            lbl = preprocessing.LabelEncoder()\n            df_all[c+'_lbl_enc'] = lbl.fit_transform(df_all[c].values)  \n            df_all[c+'_len'] = df_all[c].map(lambda x: len(str(x)))\n            df_all[c+'_words'] = df_all[c].map(lambda x: len(str(x).split(' ')))\n        elif c != 'Text':\n            lbl = preprocessing.LabelEncoder()\n            df_all[c] = lbl.fit_transform(df_all[c].values)\n        if c=='Text': \n            df_all[c+'_len'] = df_all[c].map(lambda x: len(str(x)))\n            df_all[c+'_words'] = df_all[c].map(lambda x: len(str(x).split(' '))) \n\ntrain = df_all.iloc[:len(train)]\ntest = df_all.iloc[len(train):]\n\nclass cust_regression_vals(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n    def fit(self, x, y=None):\n        return self\n    def transform(self, x):\n        x = x.drop(['Gene', 'Variation','ID','Text'],axis=1).values\n        return x\n\nclass cust_txt_col(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n    def __init__(self, key):\n        self.key = key\n    def fit(self, x, y=None):\n        return self\n    def transform(self, x):\n        return x[self.key].apply(str)\n\nprint('Pipeline...')\nfp = pipeline.Pipeline([\n    ('union', pipeline.FeatureUnion(\n        n_jobs = -1,\n        transformer_list = [\n            ('standard', cust_regression_vals()),\n            ('pi1', pipeline.Pipeline([('Gene', cust_txt_col('Gene')), ('count_Gene', feature_extraction.text.CountVectorizer(analyzer=u'char', ngram_range=(1, 8))), ('tsvd1', decomposition.TruncatedSVD(n_components=20, n_iter=25, random_state=12))])),\n            ('pi2', pipeline.Pipeline([('Variation', cust_txt_col('Variation')), ('count_Variation', feature_extraction.text.CountVectorizer(analyzer=u'char', ngram_range=(1, 8))), ('tsvd2', decomposition.TruncatedSVD(n_components=20, n_iter=25, random_state=12))])),\n            #commented for Kaggle Limits\n            #('pi3', pipeline.Pipeline([('Text', cust_txt_col('Text')), ('tfidf_Text', feature_extraction.text.TfidfVectorizer(ngram_range=(1, 2))), ('tsvd3', decomposition.TruncatedSVD(n_components=50, n_iter=25, random_state=12))]))\n        ])\n    )])\n\ntrain = fp.fit_transform(train); print(train.shape)\ntest = fp.transform(test); print(test.shape)\n\ny = y - 1 #fix for zero bound array\n# LightGBM "},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_execution_state":"idle","_cell_guid":"06b67a9f-0777-4f92-b153-eb9dc2f49995","trusted":false,"collapsed":false,"_uuid":"82a7f7aca93e2bc9e7cf0aa43b5c3131e805b0e3"},"source":"import lightgbm as lgb\nimport matplotlib.pyplot as plt\nlgb_params = {\n    'learning_rate': 0.01,\n    'max_depth': 5,\n    'num_leaves': 40, \n    'objective': 'multiclass',\n    'num_class':9,\n    'tree_learner':'voting',\n    'metric':'multi_logloss',\n    'feature_fraction': 0.75,\n    'bagging_fraction': 0.75,\n    'max_bin': 100}\n# form LightGBM datasets\ndtrain_lgb = lgb.Dataset(train, label=y)\n# LightGBM, cross-validation\ncv_result_lgb = lgb.cv(lgb_params, \n                       dtrain_lgb, \n                       num_boost_round=1000, \n                       nfold=5, \n                       stratified=True, \n                       early_stopping_rounds=50, \n                       verbose_eval=100, \n                       show_stdv=True)\nnum_boost_rounds_lgb = len(cv_result_lgb['multi_logloss-mean'])\nprint('num_boost_rounds_lgb=' + str(num_boost_rounds_lgb))\n# train model\nmodel_lgb = lgb.train(lgb_params, dtrain_lgb, num_boost_round=num_boost_rounds_lgb)\ny_pred=model_lgb.predict(test)\nclasses = \"class1,class2,class3,class4,class5,class6,class7,class8,class9\".split(',')\nsubm = pd.DataFrame(y_pred, columns=classes)\nsubm['ID'] = pid\n\nsubm.to_csv('submission.csv', index=False)"}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","name":"python","nbconvert_exporter":"python","version":"3.6.1","mimetype":"text/x-python"}},"nbformat":4}