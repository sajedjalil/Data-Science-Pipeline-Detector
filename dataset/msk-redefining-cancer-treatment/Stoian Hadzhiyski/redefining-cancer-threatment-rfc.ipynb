{"cells":[{"cell_type":"markdown","source":"## Redefining Cancer Threatment - RFC\n\nA solution using Random Forest Classifier.","metadata":{"_uuid":"5e1508e3f269a8d036fc4da49306b67e729a4c74","_cell_guid":"a67020ca-ad54-42c6-8cd8-88e3917b4072"}},{"cell_type":"code","source":"import pandas as pd","outputs":[],"execution_count":null,"metadata":{"_uuid":"2e6de93228aa2a94ba2479fa923b596259eb8c37","_cell_guid":"7e8c0e58-91ee-46f4-affe-2cc9a978e57d","collapsed":true}},{"cell_type":"markdown","source":"### Load all data sets","metadata":{"_uuid":"832baf68fd3e9ac8699b7d5ff8459a099569538c","_cell_guid":"1bf3ba81-1fe8-4622-b590-251eda9b54a4"}},{"cell_type":"code","source":"def load_data(name, extension=None):\n    data = pd.read_csv('../input/{0}_variants{1}'.format(name, '.' + extension if extension is not None else ''))\n    text = pd.read_csv('../input/{0}_text{1}'.format(name, '.' + extension if extension is not None else ''), \n                             sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n    data = pd.merge(data, text, on='ID').fillna('')\n    \n    return data","outputs":[],"execution_count":null,"metadata":{"_uuid":"a8e2f01da978b314c28666d3dcf1b279f72b101b","_cell_guid":"4c90c679-3ae7-4551-bdb6-9a2d4e07029a","collapsed":true}},{"cell_type":"code","source":"# training data\ntrain_data = load_data('training')\nprint(train_data.shape)\ntrain_data.columns.values","outputs":[],"execution_count":null,"metadata":{"_uuid":"c62308ace15c2e59c0d1948beeb516d7d9747b41","_cell_guid":"eabd411a-a22b-4707-affc-e7903690e27c"}},{"cell_type":"code","source":"# test data\ntest_data = load_data('test')\nprint(test_data.shape)\ntest_data.columns.values","outputs":[],"execution_count":null,"metadata":{"_uuid":"a1a685942ea90fb55f1c160918be6bc058b7a8b9","_cell_guid":"8ea28d55-a81e-463e-a233-0fc4108ebd9b"}},{"cell_type":"code","source":"# test data for stage 2\nstage2_test_data = load_data('stage2_test', 'csv')\nprint(stage2_test_data.shape)\nstage2_test_data.columns.values","outputs":[],"execution_count":null,"metadata":{"_uuid":"f6b569237caef6752724971a50a8481d3c32d947","_cell_guid":"ed95b688-2a8f-4d45-806b-d1d3093a8ef3"}},{"cell_type":"code","source":"# released labels for stage 2\nsolution_data = pd.read_csv('../input/stage1_solution_filtered.csv')\nsolution_data.shape","outputs":[],"execution_count":null,"metadata":{"_uuid":"13c9674957d568c0e61b0ec55fd65738306309c6","_cell_guid":"57ce55eb-8e83-400c-bd9f-96cb554060a4"}},{"cell_type":"markdown","source":"### Extending the training data with the released test labels\n\nConcatenating the released test data with the training data.","metadata":{"_uuid":"bb4f655a8e965f15d29b5c054ae1d216f346902c","_cell_guid":"f890a22b-6d2b-4812-b17b-8dcaf5af6ee1"}},{"cell_type":"code","source":"# data normalization\nsolution_data_labels = solution_data[['class1', 'class2', 'class3', 'class4', 'class5', 'class6', 'class7', 'class8', 'class9']]\nsolution_data_labels.columns = [1, 2, 3, 4, 5, 6, 7, 8, 9]\nsolution_data['Class'] = solution_data_labels.idxmax(axis=1)\nsolution_data = solution_data[['ID', 'Class']]\nsolution_data.head(5)","outputs":[],"execution_count":null,"metadata":{"_uuid":"dd3cfa7bb8d7358038a18beac6e441be221e8770","_cell_guid":"b3a11d3f-0086-4fde-afe0-1a93ad8970b4"}},{"cell_type":"code","source":"# merging the released labels with the test data\nreleased_test_data = pd.merge(solution_data, test_data, \n                            left_on=['ID'],\n                            right_on=['ID'],\n                            how='inner')\nreleased_test_data.shape","outputs":[],"execution_count":null,"metadata":{"_uuid":"1b637521441e9d0b2c277b5a141ba6cbb1e6eac7","_cell_guid":"aaf10c6d-c16a-4d68-9991-50a3c8d8af7b"}},{"cell_type":"code","source":"released_test_data.head(5)","outputs":[],"execution_count":null,"metadata":{"_uuid":"d0d549638397d18361b8b75ed945b8725fc167de","_cell_guid":"87b74c20-f981-47f5-bcf8-59874bdbf7cc"}},{"cell_type":"code","source":"# extending the train data\nextended_train_data = pd.concat([train_data, released_test_data], ignore_index=True)\nextended_train_data.shape","outputs":[],"execution_count":null,"metadata":{"_uuid":"96087ffc34e65ed84888e189a616a7b804ca92f5","_cell_guid":"6193308b-7675-4ff1-a980-15c54b301287"}},{"cell_type":"markdown","source":"### Extracting features from the given data set\n\nExtracting more features from the Genes and Variations","metadata":{"_uuid":"9ee77e493b09d9c45c7dcb900ce846a501f75201","_cell_guid":"50122ae8-e473-46e4-bead-da14275ab176"}},{"cell_type":"code","source":"def extract_features_from_genes_and_variations(df):\n    gene_features = pd.get_dummies(df['Gene'])\n    variation_features = pd.get_dummies(df['Variation'])\n    features = gene_features.join(variation_features)\n\n    return features","outputs":[],"execution_count":null,"metadata":{"_uuid":"d7dbfbb9f46fd92a759ea929b4f1c9a25fa9e4cd","_cell_guid":"8a6a9641-298d-445a-b8a5-39a808115471","collapsed":true}},{"cell_type":"markdown","source":"Use the full data given from Gene and Variations.","metadata":{}},{"cell_type":"code","source":"train_and_test_data = pd.concat([train_data, test_data], ignore_index=True)\ntrain_and_test_data.shape","outputs":[],"execution_count":null,"metadata":{"_uuid":"a6d4994e6815821f5256f3fa548d2909f206a765","_cell_guid":"9bc2b56a-c192-4832-9b65-6c9539373edc"}},{"cell_type":"code","source":"train_features = extract_features_from_genes_and_variations(train_and_test_data)\ntest_features = extract_features_from_genes_and_variations(stage2_test_data)","outputs":[],"execution_count":null,"metadata":{"_uuid":"1a2c379f1b926d3732910aad951c420dc9c41c9b","_cell_guid":"17a1eb8d-c58c-4366-ab67-2a70695a9b41","collapsed":true}},{"cell_type":"markdown","source":"Finding common features between the train and test data.","metadata":{"_uuid":"74344f381c3e3fc3807971643d332b84c7c33321","_cell_guid":"63c8482f-df19-46a0-9fbf-eb2a0d7ac1ac"}},{"cell_type":"code","source":"common_features = list(set(train_features.columns.values) & set(test_features.columns.values))\ntrain_features = train_features[common_features]\ntest_features = test_features[common_features]\n\ntrain_features.shape, test_features.shape","outputs":[],"execution_count":null,"metadata":{"_uuid":"7eaa454fa252092cd249df6e4e191192d1f829b7","_cell_guid":"452e9d69-c19c-4c8c-a9d5-f19228857ca4"}},{"cell_type":"markdown","source":"### Performing predictions","metadata":{"_uuid":"39205ddc2adc0b0695bbffea466701d06217550c","_cell_guid":"e7a05953-31e2-44d8-8ab4-ac7ef4b31ca8"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.sparse import hstack\nimport numpy as np","outputs":[],"execution_count":null,"metadata":{"_uuid":"b8fd2b09854ef2c1d93a8e6597d4060e7b729517","_cell_guid":"78485248-38a1-4a0d-a72c-e997aab19de6","collapsed":true}},{"cell_type":"code","source":"def generate_features_and_labels(train_data, test_data, train_features, test_features):\n    '''\n    Generating a feature set by combining the two feature sets, \n    extracted from genes and variations and extracted from the texts.\n    '''\n    x_train = vectorizer.fit_transform(train_data['Text'])\n    x_train = hstack((x_train, train_data[['ID']].join(train_features).drop('ID', axis=1).values))\n    y_train = train_data['Class']\n    x_test = vectorizer.transform(test_data['Text'])\n    x_test = hstack((x_test, test_data[['ID']].join(test_features).drop('ID', axis=1).values))\n    \n    return x_train, y_train, x_test","outputs":[],"execution_count":null,"metadata":{"_uuid":"c06a35a5a4cdec72b56910777a5338c16b5cdc61","_cell_guid":"b330708a-1234-49ed-a140-4ab91588ff01","collapsed":true}},{"cell_type":"code","source":"def predict(x_train, y_train, x_test):\n    clf = RandomForestClassifier(n_jobs=3,\n                                n_estimators=100,\n                                criterion='entropy',\n                                random_state=300)\n\n    clf.fit(x_train, y_train)\n    return clf.predict(x_test)","outputs":[],"execution_count":null,"metadata":{"_uuid":"ec1e9078a0c078c8ea7c73027d7c9e5b81a014e0","_cell_guid":"166390a9-bc48-4fc2-bf75-4cf458fa1316","collapsed":true}},{"cell_type":"code","source":"def make_submission(test_data):\n    submission_data = pd.get_dummies(test_data['predicted_class'])\n    submission_data = test_data[['ID']].join(submission_data)\n    \n    labels = list(range(1, 10))\n    submission_data = submission_data[['ID'] + labels]\n    submission_data.columns = ['ID'] + ['class' + str(label) for label in labels]\n\n    submission_data.to_csv('submission.csv', index=False)","outputs":[],"execution_count":null,"metadata":{"_uuid":"4e57d0dcc1dcbbcd058f162394ed42ee2584dcc1","_cell_guid":"e4f43e6d-b34a-4690-a610-6a4ccdd817a9","collapsed":true}},{"cell_type":"markdown","source":"Validation testing","metadata":{"_uuid":"5c5df62c8e8ec205f45797d909c988922f5d55d4","_cell_guid":"9f8e942c-1083-494f-8ddd-85703755ee69"}},{"cell_type":"code","source":"train, test = train_test_split(extended_train_data, test_size=0.2)\nvectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n                                 stop_words='english')\n\nx_train, y_train, x_test = generate_features_and_labels(train, test, train_features, train_features)\ny_test = test['Class']","outputs":[],"execution_count":null,"metadata":{"_uuid":"78b5ebf8592a1152c827d749bad8d78f8de8e3ab","_cell_guid":"08cec382-813d-49b5-955a-7084656b6376","collapsed":true}},{"cell_type":"code","source":"predicted = predict(x_train, y_train, x_test)\nnp.mean(predicted == y_test)","outputs":[],"execution_count":null,"metadata":{"_uuid":"f7aaafb7d592a9ad70fc5012c7935f47f5180498","_cell_guid":"266e66b0-9d77-4583-9513-902d5a5413e6"}},{"cell_type":"markdown","source":"Prediction with the test data","metadata":{"_uuid":"09a2943d6d08aac7a4d07c232af00a27977811df","_cell_guid":"0975f69d-2517-4443-a974-5ed8be5d84c3"}},{"cell_type":"code","source":"x_train = vectorizer.fit_transform(extended_train_data['Text'])\nx_train = hstack((x_train, extended_train_data[['ID']].join(train_features).drop('ID', axis=1).values))\ny_train = train['Class']\nx_test = vectorizer.transform(stage2_test_data['Text'])\nx_test = hstack((x_test, stage2_test_data[['ID']].join(test_features).drop('ID', axis=1).values))\n\nx_train, y_train, x_test = generate_features_and_labels(extended_train_data, stage2_test_data, train_features, test_features)\n\npredicted = predict(x_train, y_train, x_test)\n\nstage2_test_data['predicted_class'] = predicted\nmake_submission(stage2_test_data)","outputs":[],"execution_count":null,"metadata":{"_uuid":"16361d2ceb96b8f73b477968e98248ae7f49887a","_cell_guid":"f8b3db94-4921-43e2-bb7d-4404c9b6075c","collapsed":true}}],"nbformat":4,"nbformat_minor":1,"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"name":"python","file_extension":".py","version":"3.6.1","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","pygments_lexer":"ipython3","nbconvert_exporter":"python"}}}