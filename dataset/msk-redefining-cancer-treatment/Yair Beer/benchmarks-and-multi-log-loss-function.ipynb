{"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"name":"python","version":"3.6.1","pygments_lexer":"ipython3","file_extension":".py"}},"nbformat":4,"cells":[{"cell_type":"markdown","metadata":{"_uuid":"0cdb254b1c06a16e96f56229d79857dcba03136d","_cell_guid":"bb2d269f-a72b-4d95-b5b9-aec2e48e20d3"},"source":"The purpose of this notebook is to create a benchmark code. It consists of creating the Multi Log Loss score function which would be used later for more complicated algorithms and checking it using simple benchmarks.\nI used an even 1/9 probability benchmark between all the classes and Bayesian probability.\nI've also checked how strong is the overfitting for using all the training data rather than using cross validation."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"39094815e8f3153d4a79facdc0f0c6b89798d017","_cell_guid":"f8d9b7c8-6fa8-4806-8da3-a41b1352077f"},"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"3581607e4645d42ae94e06462846f0a1504c5cc6","_cell_guid":"0144a905-ff0a-407c-a3f5-a32e9a031ca9"},"source":"# Reading train\ntrain_X = pd.read_csv(\n    '../input/training_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, \n    names=[\"ID\",\"Text\"])\ntrain_y = pd.DataFrame.from_csv(\"../input/training_variants\")\nprint('Train Text')\nprint(train_X.head())\nprint(\"train classes\")\nprint(train_y.head())\nprint(\"train classes probability\")\ntrain_y.Class.value_counts(normalize=True)\n"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"f73a312f32d4e4431b1c68a7f11213fd4af5a969","_cell_guid":"b53c17d6-2fd4-480b-8a88-897bbae4b8bb","collapsed":true},"source":"def multi_log_loss(y_true: np.array, y_pred: np.array):  # score function for CV\n    # Handle all zeroes\n    all_zeros = np.all(y_pred == 0, axis=1)\n    y_pred[all_zeros] = 1/9\n    # Normalise sum of row probabilities to one\n    row_sums = np.sum(y_pred, axis=1)\n    y_pred /= row_sums.reshape((-1, 1))\n    # Calculate score\n    n_rows = y_true.size\n    y_true = y_true - 1  # classes start from 1 where columns start from zero\n    score_sum = 0\n    for i in range(y_true.size):\n        score_sum -= np.log(y_pred[i, y_true[i]])\n    score = score_sum / n_rows\n    return score\n        "},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"296854e46b009b167620247a47179fe23ade68b7","_cell_guid":"3468499d-13e2-4fad-9dab-7c6b1db95096"},"source":"# Gives every class 1/9 probability\npredictions = np.repeat(1/9, train_y.size*9).reshape(train_y.size,9)\nbenchmark_blind = multi_log_loss(train_y.Class.values, predictions)\nprint(\"The score for equal probability per each class is:\")\nbenchmark_blind"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c54ccd3970537ee18525dc3c76833f109456cbec","_cell_guid":"d2987373-b98e-41b9-93f4-df1cfd6822d3","collapsed":true},"source":"def class_probability_list(train_y_series):\n    class_probability_series = train_y_series.value_counts(normalize=True)\n    probability_list = []\n    for i in range(1, 10):\n        probability_list.append(class_probability_series.at[i])\n    return probability_list"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"285e8f1bc766eb03bf93b3e27b160f45c4142506","_cell_guid":"0d92869c-7d86-4f2f-8cd8-dce6241fac0f"},"source":"# Gives every class its precentange - Overfitting full train (Bayesian)\npredictions = np.repeat(\n    [class_probability_list(train_y.Class)], train_y.size, axis=0)\nbenchmark_probability_blind = multi_log_loss(train_y.Class.values, predictions)\nprint(\"The score for Bayesian probabilities using the whole train is:\")\nprint(benchmark_probability_blind)"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"d4b3fc6663433dc8bcbf95fdaa1c885b83fbf610","_cell_guid":"b487f67f-c245-4ea0-a283-e12548be5f1f"},"source":"# Bayesian - without overfitting\n# Generate stratified cv\nfrom sklearn.model_selection import KFold\n\nbenchmark_probability_blind_no_overfit = []\n\nn_cv = 4\nskf = KFold(n_splits=n_cv, shuffle=True, random_state=1)\nfor indices_train, indices_test in skf.split(X=train_X.values):\n    predictions = class_probability_list(train_y.iloc[indices_train].Class)\n    predictions = np.repeat([predictions], indices_test.size, axis=0)\n    benchmark_probability_blind_no_overfit.append(\n        multi_log_loss(train_y.iloc[indices_test].Class.values, predictions))\nprint(\"The score for Bayesian probabilities with Kfold is:\")\nprint(np.mean(benchmark_probability_blind_no_overfit))"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true},"source":"print(\"Gene exploration\")\ntrain_y.Gene.value_counts(normalize=True)\n"}],"nbformat_minor":1}