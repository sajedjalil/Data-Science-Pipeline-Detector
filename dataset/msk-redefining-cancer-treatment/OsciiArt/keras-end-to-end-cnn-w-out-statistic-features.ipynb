{"nbformat_minor":1,"metadata":{"language_info":{"file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","mimetype":"text/x-python","version":"3.6.1","pygments_lexer":"ipython3","nbconvert_exporter":"python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat":4,"cells":[{"cell_type":"markdown","metadata":{"_uuid":"4ba984e341740e3083dad687525ed9c021c81fd3","_cell_guid":"1fe71a5f-5470-4c61-aded-2cc489012df2"},"source":"End-to-end neural network model without statistic  features like N-gram or TF-IDF.  \nGene and Variation info is processed by char-level CNN, and Text info is processed by word-level CNN,\nthen 3 features are combined and then processed by MLP.  \nMain nonlinearlity is GLU.   \nValidation logloss is about 1.09 (sorry, I didn't CV yet).  \nThis model is not so good and worse than MLP with statistic features."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"6d859f2ebaa1b8b00b63e3934f3fe22b7983ac5d","_cell_guid":"e32c9ade-246a-44d1-9fe1-0ed5090f0ead"},"source":"import numpy as np\nimport pandas as pd","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"1678cbd653e5ace6cd9272ebe8bc1324ecef2aad","_cell_guid":"e317cfaf-4b1b-4a4f-903c-f1b2a9b8ec72"},"source":"### Step 1: load data\ntrain = pd.read_csv('../input/training_variants')\ntest1 = pd.read_csv('../input/test_variants')\ntest2 = pd.read_csv('../input/stage2_test_variants.csv')\n\ntrainx = pd.read_csv('../input/training_text', sep=\"\\|\\|\", engine='python', \n                     header=None, skiprows=1, names=[\"ID\",\"Text\"])\ntest1x = pd.read_csv('../input/stage2_test_text.csv', sep=\"\\|\\|\", engine='python', \n                    header=None, skiprows=1, names=[\"ID\",\"Text\"])\ntestx2 = pd.read_csv('../input/stage2_test_text.csv', sep=\"\\|\\|\", engine='python', \n                     header=None, skiprows=1, names=[\"ID\",\"Text\"], encoding='utf-8')\n\ntest_solution = pd.read_csv('../input/stage1_solution_filtered.csv')","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"3061a7c00f0bc579e9d0e49dec5fcc746ee3f8d1","_cell_guid":"617db739-9d77-4158-ba48-015d82fab3b8"},"source":"# merge test 1 and test 1 solution\ntest_idx = sorted(list(test_solution['ID'].unique()))\ntest_filterd = test1.loc[test_idx]\n\ntest_y = test_solution.iloc[:,1:].as_matrix()\ntest_y = np.argmax(test_y, axis=1)\ntest_y = test_y+1\ntest_filterd['Class'] = test_y\n\ntestx_filterd = test1x.loc[test_idx]","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"7507362881b196309f8768e05051ec6c6cd13f8f","_cell_guid":"eb496d04-eed3-41ff-a450-7f1a16854de6"},"source":"# merge variants and text\ntrain = pd.merge(train, trainx, how='left', on='ID').fillna('')\ntest_filterd = pd.merge(test_filterd, testx_filterd, how='left', on='ID').fillna('')\ntest2 = pd.merge(test2, testx2, how='left', on='ID').fillna('')\n\npid = test2['ID'].values","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1f42ddacc8a4af7359b797cf1db60604ae00b9b3","_cell_guid":"2afbe393-f5e6-4a91-bc56-d7de0d1074a7"},"source":"# merge training and test data\ntrain_test1 = pd.concat((train, test_filterd), axis=0, ignore_index=True)\ny = train_test1['Class'].values # yを分離\ntrain_test1 = train_test1.drop(['Class'], axis=1)\n\ndf_all = pd.concat((train_test1, test2), axis=0, ignore_index=True)\ndf_all.shape # should be (4675, 4)","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"fb3b437829f51973635594f7566564ef6300ce7c","_cell_guid":"81e257d4-631f-4bb6-b796-32a28622aa0a"},"source":"### Step 2: Text Tokenize\nfrom keras.preprocessing.text import Tokenizer\n\n# tokenize gene in char level\ngene_tokenizer = Tokenizer(char_level=True)\nprint(\"tokenizer learning...\")\ngene_tokenizer.fit_on_texts(texts=df_all['Gene'])\nprint(\"word count\", len(gene_tokenizer.word_counts)) # 37","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b2141d7f42be3de5be35140d21cdeb17f7f39055","_cell_guid":"633080cd-ee91-41cd-a3f5-30c30ebf52c4"},"source":"gene_token_list = gene_tokenizer.texts_to_sequences(df_all['Gene'])\ngene_token = np.zeros([len(gene_token_list), 9], dtype=np.uint8)\nfor k, v in enumerate(gene_token_list):\n    gene_token[k,:len(v)] = np.array(v)\nfor i in range(5):\n    print(df_all['Gene'][i], gene_token[i])","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"d8ffd8d7e43ecdffc30b1c04f006a7fbd83663bd","_cell_guid":"34e41c8a-c29f-4d87-977d-7e975e070d5f"},"source":"# variation tokenize in char level\nvari_tokenizer = Tokenizer(char_level=True)\nprint(\"tokenizer learning...\")\nvari_tokenizer.fit_on_texts(texts=df_all['Variation'])\nprint(\"word count\", len(vari_tokenizer.word_counts)) # 65","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"d7a8d25a56eb12a6a2b27b90b55d6af09dd2ae39","_cell_guid":"786a02bd-afc9-4b58-8280-e8e6c86909aa"},"source":"vari_token_list = vari_tokenizer.texts_to_sequences(df_all['Variation'])\nvari_token = np.zeros([len(vari_token_list), 55], dtype=np.uint8)\nfor k, v in enumerate(vari_token_list):\n    vari_token[k,:len(v)] = np.array(v)\nfor i in range(5):\n    print(df_all['Variation'][i], vari_token[i])","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"af7ebfc16bc79c5ea48aae4f1d5ba9a44905f3e9","_cell_guid":"94423a19-eee9-4252-9d4a-63c1e8286480"},"source":"# text tokenize in word level. this process spends a few minutes.\ntext_tokenizer = Tokenizer()\nprint(\"tokenizer learning...\")\ntext_tokenizer.fit_on_texts(texts=df_all['Text'])\nprint(\"word count\", len(text_tokenizer.word_counts)) # 196704","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"4c57dc29f580ba1370d083e3272e3baffc058b08","_kg_hide-output":true,"_cell_guid":"05646e10-d703-4a74-b9f8-37d8ceb2d74f"},"source":"text_token_list = text_tokenizer.texts_to_sequences(df_all['Text']) #this process spends a few minutes.\nprint(text_token_list[0])","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"fbf8a29522ed2aa74e0292da9d685180350c8fb0","_cell_guid":"e0560aa5-9520-4427-a97e-0b84838160d8"},"source":"# split data into training and test\ngene_token_train = gene_token[:train_test1.shape[0]]\ngene_token_test = gene_token[train_test1.shape[0]:]\n\nvari_token_train = vari_token[:train_test1.shape[0]]\nvari_token_test = vari_token[train_test1.shape[0]:]\n\ntext_token_train = text_token_list[:train_test1.shape[0]]\ntext_token_test = text_token_list[train_test1.shape[0]:]","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"ad32ceb1dae10ed9645a683e5d49efc41a45901e","_cell_guid":"b62bb397-3cd4-4c74-b251-9fa3bef980a6"},"source":"# make y into one-hot\ny = y -1\nencoded_y = np.eye(9)[y]","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"8297b3a7cf3db8680e8c0dc349e6641b85b3492d","_cell_guid":"b5a9177d-8415-4f44-981a-ce788ca953da"},"source":"### Step 3: training\n# build model\nfrom keras.models import Model\nfrom keras.layers import Input, Embedding, Dense, Activation, Dropout, Reshape, Flatten\nfrom keras.layers import Conv1D, MaxPooling1D, BatchNormalization, Concatenate, Add, Multiply\nfrom keras.optimizers import Adam, SGD\n\ndef ConvGlu_block(input_tensor, nb_filter, kernel_size=7, strides=2):\n    x = Conv1D(nb_filter, kernel_size, padding='same', strides=strides)(input_tensor)\n    x = BatchNormalization()(x)\n    gate = Conv1D(nb_filter, kernel_size, padding='same', strides=strides)(input_tensor)\n    gate = BatchNormalization()(gate)\n    gate = Activation('sigmoid')(gate)\n    x = Multiply()([x, gate])\n    x = Dropout(0.5)(x)\n    shortcut = Conv1D(nb_filter, 1, padding='same', strides=strides)(input_tensor)\n    shortcut = BatchNormalization()(shortcut)\n\n    x = Add()([x, shortcut])\n\n    return x\n\ndef CNN(k=9,\n        embed_size=128,\n        length=[9,55, 2048],\n        boc=196704,\n        ):\n\n    input_gene = Input(shape=(length[0],))\n    x = Embedding(37+1, embed_size)(input_gene)\n    x = Reshape((length[0], embed_size))(x)\n    x = ConvGlu_block(input_tensor=x, nb_filter=128, kernel_size=7, strides=1)\n    x = MaxPooling1D(pool_size=9)(x)\n    feature_gene = Flatten()(x)\n\n    input_vari = Input(shape=(length[1],))\n    x = Embedding(65+1, embed_size)(input_vari)\n    x = Reshape((length[1], embed_size))(x)\n    x = ConvGlu_block(input_tensor=x, nb_filter=128, kernel_size=7, strides=1)\n    x = MaxPooling1D(pool_size=55)(x)\n    feature_vari = Flatten()(x)\n\n    input_text = Input(shape=(length[2],))\n    x = Embedding(boc, embed_size)(input_text)\n    x = Reshape((length[2], embed_size))(x)\n    x = ConvGlu_block(input_tensor=x, nb_filter=128, kernel_size=7, strides=2)\n    x = ConvGlu_block(input_tensor=x, nb_filter=256, kernel_size=7, strides=2)\n    x = ConvGlu_block(input_tensor=x, nb_filter=512, kernel_size=7, strides=2)\n    x = ConvGlu_block(input_tensor=x, nb_filter=512, kernel_size=7, strides=2)\n    x = ConvGlu_block(input_tensor=x, nb_filter=512, kernel_size=7, strides=2)\n\n    x = MaxPooling1D(pool_size=length[2]//2**5)(x)\n\n    feature_text = Flatten()(x)\n\n    gate = Dense(256)(feature_text)\n    gate = BatchNormalization()(gate)\n    gate = Activation('sigmoid')(gate)\n\n    linear = Concatenate()([feature_gene, feature_vari])\n    gated = Multiply()([linear, gate])\n    x = Dense(1024)(gated)\n    x = Activation('relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(1024)(x)\n    x = Activation('relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(1024)(x)\n    x = Activation('relu')(x)\n    x = Dropout(0.2)(x)\n    y = Dense(k, activation='softmax')(x)\n\n    model = Model(inputs=[input_gene,\n                          input_vari,\n                          input_text\n                          ],\n                  outputs=y)\n    opt = SGD(decay=1e-6, momentum=0.1, nesterov=False)\n    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n    return model\n\n#commented for Kaggle Limits\nlen_text = 256  #Change to 2048, 256 for Kaggle Limits. \nboc = 1000 # Change to 196704, 1000 for Kaggle Limits. this is number of words to use.\nmodel = CNN(length=[9,55,len_text], boc=boc)","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"a1092abf649e8ad39385faf2df5bce6a61783367","_cell_guid":"52bbdb50-f67f-4dd5-a90d-c088bf510581"},"source":"# define batch generator\ndef batch_generator(gene,vari, text, y, batch_size, shuffle=True, len_text=2048, boc=1000):\n    batch_index = 0\n    n = y.shape[0]\n    while 1:\n        if batch_index == 0:\n            index_array = np.arange(n)\n            if shuffle:\n                index_array = np.random.permutation(n)\n\n        current_index = (batch_index * batch_size) % n\n        if n >= current_index + batch_size:\n            current_batch_size = batch_size\n            batch_index += 1\n        else:\n            current_batch_size = n - current_index\n            batch_index = 0\n\n        batch_text = np.zeros([current_batch_size, len_text], np.uint32)\n        index_array_batch = index_array[current_index: current_index + current_batch_size]\n        for i in range(current_batch_size):\n            text_i = text[index_array_batch[i]]\n            text_i = np.array(text_i, dtype=np.uint32)\n            text_i = text_i[text_i<boc]\n            if text_i.shape[0] <= len_text:\n                batch_text[i,:text_i.shape[0]] = text_i\n            else:\n                if shuffle:\n                    start = np.random.randint(0, text_i.shape[0] - len_text)\n                else:\n                    start = 0\n                text_crop = text_i[start:start+len_text]\n                batch_text[i] = text_crop\n\n        batch_gene = gene[index_array[current_index: current_index + current_batch_size]]\n        batch_vari = vari[index_array[current_index: current_index + current_batch_size]]\n        batch_y = y[index_array[current_index: current_index + current_batch_size]]\n\n        yield [batch_gene, batch_vari, batch_text], batch_y","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"9cb4e147946d97252ac8369dcabb174c0ba74f18","_cell_guid":"c9552c76-e7fd-48ed-b076-6bf36065f06c"},"source":"import math\n# parameters\nnum_epoch = 5 #Change to 100, 5 for Kaggle Limits. \nbatch_size = 16\nlearning_rate = 0.001\nnb_val = 256\n\nnb_sample = y.shape[0]\nnb_train = nb_sample - nb_val\nnb_train_step = math.ceil(nb_train / batch_size)\nnb_val_step = math.ceil(nb_val / batch_size)","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"e1ccc6bca6b4b9bb980a1d70b571ca743cb3b0c4","_cell_guid":"46eb6d37-831e-4934-9cb6-51f73a6fe63d"},"source":"# split data into training and validation\nnp.random.seed(42)\nperm = np.arange(nb_sample)\nnp.random.shuffle(perm)\nidx_val, idx_train = perm[:nb_val], perm[nb_val:]\ntext_train = []\nfor i in idx_train:\n    text_train.append(text_token_train[i])\ntext_val = []\nfor i in idx_val:\n    text_val.append(text_token_train[i])","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"8f64b9295be4e72ce6d31ffdcf1b50cc97b313fa","_cell_guid":"9cdfd236-b102-40de-bd0c-235be637fec3"},"source":"# build batch generator\ngen = batch_generator(gene=gene_token_train[idx_train],\n                      vari=vari_token_train[idx_train,],\n                      text=text_train,\n                      y=encoded_y[idx_train],\n                      batch_size=batch_size,\n                      shuffle=True,\n                      len_text=len_text,\n                      boc=boc)\n\ngen_val = batch_generator(gene=gene_token_train[idx_val],\n                          vari=vari_token_train[idx_val],\n                          text=text_val,\n                          y=encoded_y[idx_val],\n                          batch_size=batch_size,\n                          shuffle=False,\n                          len_text=len_text,\n                          boc=boc)","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e082463695125224cbd9a0a09655dbf552e54f3f","_cell_guid":"ae44df23-e407-493d-a86b-0b42b2751641"},"source":"# training\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\nsave_checkpoint = ModelCheckpoint(filepath='best_weight.hdf5', monitor='val_loss', save_best_only=True)\nlerning_rate_schedular = ReduceLROnPlateau(patience=8, min_lr=learning_rate * 0.00001)\nearly_stopping = EarlyStopping(monitor='val_loss',patience=16, verbose=1, min_delta=1e-4, mode='min')\nCallbacks = [save_checkpoint, lerning_rate_schedular, early_stopping]\n\nmodel.fit_generator(gen,\n                    steps_per_epoch=nb_train_step,\n                    epochs=num_epoch,\n                    validation_data=gen_val,\n                    validation_steps=nb_val_step,\n                    callbacks=Callbacks)","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"647af4ca1ec5583b361957540533f8e2c2739082","_cell_guid":"8db625b6-6911-4f32-963c-ffb54abb9939"},"source":"### Step 4: prediction\n# build batch generator\ndef test_batch_generator(gene, vari, text, batch_size, shuffle=True, len_text=2048, boc=1000):\n    batch_index = 0\n    n = gene.shape[0]\n    while 1:\n        if batch_index == 0:\n            index_array = np.arange(n)\n            if shuffle:\n                index_array = np.random.permutation(n)\n\n        current_index = (batch_index * batch_size) % n\n        if n >= current_index + batch_size:\n            current_batch_size = batch_size\n            batch_index += 1\n        else:\n            current_batch_size = n - current_index\n            batch_index = 0\n\n        batch_text = np.zeros([current_batch_size, len_text], np.uint32)\n        index_array_batch = index_array[current_index: current_index + current_batch_size]\n        for i in range(current_batch_size):\n            text_i = text[index_array_batch[i]]\n            text_i = np.array(text_i, dtype=np.uint32)\n            text_i = text_i[text_i<boc]\n            if text_i.shape[0] <= len_text:\n                batch_text[i,:text_i.shape[0]] = text_i\n            else:\n                if shuffle:\n                    start = np.random.randint(0, text_i.shape[0] - len_text)\n                else:\n                    start = 0\n                text_crop = text_i[start:start+len_text]\n                batch_text[i] = text_crop\n\n        batch_gene = gene[index_array[current_index: current_index + current_batch_size]]\n        batch_vari = vari[index_array[current_index: current_index + current_batch_size]]\n\n        yield [batch_gene, batch_vari, batch_text]\n        \n        \ngen_test = test_batch_generator(gene=gene_token_test,\n                                vari=vari_token_test,\n                                text=text_token_test,\n                                batch_size=batch_size,\n                                shuffle=False,\n                                len_text=len_text,\n                                boc=boc,\n                                )","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"0194386671e0bc195a2cdb6f66622be4954e119f","_cell_guid":"16463d2f-54da-4466-a7d3-44a78861e22b"},"source":"# predict\nmodel.load_weights('best_weight.hdf5')\nnb_test = gene_token_test.shape[0]\nnb_test_step = math.ceil(nb_test / batch_size)\ny_pred = np.empty([nb_test, 9], dtype=np.float32)\nprint('predicting...')\nfor i in range(nb_test_step):\n    batch = next(gen_test)\n    predict = model.predict(batch)\n    if i != nb_test_step - 1:\n        y_pred[i * batch_size:(i + 1) * batch_size] = predict\n    else:\n        y_pred[i * batch_size:] = predict\nprint('done.')","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"4729832426f9316ab9b53e7ef46244b7b85d88a4","_cell_guid":"e4c80630-3c0a-43f1-903a-bf86a4d1ef02"},"source":"# make submission\nsubmission = pd.DataFrame(y_pred, columns=\n                          ['class1', 'class2', 'class3', 'class4', 'class5', 'class6', 'class7', 'class8', 'class9'])\nsubmission['ID'] = np.arange(y_pred.shape[0]) + 1\nsubmission.to_csv(\"submission_CNN.csv\", index=False)","outputs":[]}]}