{"cells":[{"source":"The purpose of this kernel is to demonstrate how to implement model selection methods, and feature engineering.","cell_type":"markdown","outputs":[],"metadata":{"_uuid":"d3387a8679da6f8726900841952243a57d381c5d","_execution_state":"idle","collapsed":false,"_cell_guid":"f9473971-8bb3-43c1-b883-88ef3e24c494"},"execution_count":null},{"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","cell_type":"code","outputs":[],"metadata":{"trusted":false,"_uuid":"41967b613760b1509255ea312fa21be17a2f0bbe","_execution_state":"idle","_cell_guid":"d6c93756-2ee2-41a5-a050-823d2e4b7ba5"},"execution_count":null},{"source":"train = pd.read_csv('../input/training_variants')\ntest = pd.read_csv('../input/test_variants')\ntrainx = pd.read_csv('../input/training_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\ntestx = pd.read_csv('../input/test_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n\ntrain = pd.merge(train, trainx, how='left', on='ID')\n\ntest = pd.merge(test, testx, how='left', on='ID')","cell_type":"code","outputs":[],"metadata":{"trusted":false,"_uuid":"e0034975e9682c82ad6bcb79b0130497f46f81d4","_execution_state":"idle","collapsed":false,"_cell_guid":"fc6741f4-bbbf-4c00-a48d-10d594e62a2d"},"execution_count":null},{"source":"First, lets add a few features regarding the length of the textual data.","cell_type":"markdown","outputs":[],"metadata":{"_uuid":"abfa0f1c542c35c59293f815cabf2bbdf2888d31","_execution_state":"idle","collapsed":false,"_cell_guid":"d04d5006-0ac5-4970-b110-bad2d0404a40"},"execution_count":null},{"source":"def catCount( train, test, col ):\n    \n    train.loc[:, col + '_count']  = train[col].apply(lambda x: len(x.split()))\n    test.loc[:, col + '_count'] = test[col].apply(lambda x: len(x.split()))\n\n    return train, test\n\ntrain, test = catCount(train, test, 'Text')","cell_type":"code","outputs":[],"metadata":{"trusted":false,"_uuid":"766f23ae3b7df2f27eb44331659bd49f9fccc862","_execution_state":"idle","collapsed":false,"_cell_guid":"c1cf9bcc-568a-4b48-8672-756bdb1cef05"},"execution_count":null},{"source":"def newCatFeatures( train_df, test_df, col, n_comp = 25 ):\n    \n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn.decomposition import TruncatedSVD\n    \n    print ( 'Features Before: ' + str(train_df.shape[1]) )\n    \n    wordSpace = train_df[col].append(test_df[col])\n    \n    wordCounts = [len(x.split()) for x in wordSpace]\n    \n    if np.max(wordCounts) > 20:\n        tfidf = TfidfVectorizer(strip_accents='unicode',lowercase =True, analyzer='char_wb', \n\t               ngram_range = (2,3), norm = 'l2', sublinear_tf = True, min_df = 1e-2,\n                                   stop_words = 'english').fit(wordSpace)\n    else:\n        tfidf = TfidfVectorizer(strip_accents='unicode',lowercase =True, analyzer='char', \n\t            ngram_range = (1,8), norm = 'l2', sublinear_tf = True, \n                                stop_words = 'english').fit(wordSpace)\n        \n    print ('Found term frequencies')\n    \n    svd = TruncatedSVD(n_components = n_comp, n_iter=25, random_state=12)\n    \n    Xtr = svd.fit_transform( tfidf.transform( train_df[col] ) )\n    Xtst = svd.transform( tfidf.transform( test_df[col] ) )\n\n    print ('Performed SVD')\n    \n    features_ = [ col + '_tfidf_svd_' + str(i+1) for i in range(Xtr.shape[1]) ]\n\n    train_df = pd.concat( [train_df, pd.DataFrame(Xtr, columns = features_) ], axis = 1)\n    test_df = pd.concat( [test_df, pd.DataFrame(Xtst, columns = features_) ], axis = 1)\n\n    print ( 'Features After: ' + str(train_df.shape[1]) + '\\n')\n    \n    train_df.drop(col, axis = 1, inplace = True)\n    test_df.drop(col, axis = 1, inplace = True)\n    \n    return train_df, test_df","cell_type":"code","outputs":[],"metadata":{"trusted":false,"_uuid":"6eb209860e1328098d52c446410f7d3ff1dacbbf","_execution_state":"idle","collapsed":false,"_cell_guid":"eeba9072-4ca6-40e1-8567-bbc0fd604522"},"execution_count":null},{"source":"Now lets find the tfidf of our textual features and then perform svd on the tfidf matrix.","cell_type":"markdown","outputs":[],"metadata":{"_uuid":"46a5d6840ccb094502e7c4883bc56aa1823225fe","_execution_state":"idle","collapsed":false,"_cell_guid":"857a8919-57f3-4c03-928e-fba0a5af16b3"},"execution_count":null},{"source":"train, test = newCatFeatures(train, test, 'Gene', n_comp = 20)\n\ntrain, test = newCatFeatures(train, test, 'Variation', n_comp = 20)\n\ntrain, test = newCatFeatures(train, test, 'Text', n_comp = 50)","cell_type":"code","outputs":[],"metadata":{"trusted":false,"_uuid":"67056e6c21ace9607c84ef467985ef453d145b12","_execution_state":"idle","collapsed":false,"_cell_guid":"df4c0822-2d6e-4cf0-b55e-994f3a6ce89b"},"execution_count":null},{"source":"train.drop('ID', axis = 1, inplace = True)\n\nclasses = ['Class' + str(i + 1) for i in range(9)]\n\ndf_sub = pd.DataFrame( columns = ['ID'] )\n\ndf_sub['ID'] = test.pop('ID')","cell_type":"code","outputs":[],"metadata":{"trusted":false,"_uuid":"10a11818593140b4c9837efb0971147d330353c8","_execution_state":"idle","collapsed":false,"_cell_guid":"29b71d6e-15a9-4489-9b4c-5d973f181353"},"execution_count":null},{"source":"To have an idea of the correlations, let us create a correlation plot.","cell_type":"markdown","outputs":[],"metadata":{"_uuid":"0e8f38ab245db37783ad5764c9b847d46bd6a318","_execution_state":"idle","collapsed":false,"_cell_guid":"e26fa032-fb10-4cb9-9b91-5b043468c3fa"},"execution_count":null},{"source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ncor_ = train.corr()\n\nsns.heatmap(cor_, vmax=.8, square=True)\n\nplt.show()\n\ntrain_labels = train.pop('Class') - 1","cell_type":"code","outputs":[],"metadata":{"trusted":false,"_uuid":"023a4c3dd1a43f02f7105cb5140be0e375502042","_execution_state":"idle","collapsed":false,"_cell_guid":"cd7807a0-a2cb-4fef-8429-78b479ff0d3d"},"execution_count":null},{"source":"Let use create a few models for our predictions.","cell_type":"markdown","outputs":[],"metadata":{"_uuid":"6940abbe46e48fa8a2bea260bf6996d54aa09c62","_execution_state":"idle","collapsed":false,"_cell_guid":"b95f6bd3-b80d-4894-881b-53c8c63bd315"},"execution_count":null},{"source":"#base learners\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\n\n#model selection\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n#aid in model selection\nfrom scipy.stats import expon\n\nparam_grid = {\"n_neighbors\": range(2,7)}\nparam_dist = {'C': expon(scale=100) }\n\nclf_list = [\n    GridSearchCV(KNeighborsClassifier(weights = 'uniform'), param_grid, cv = 5, scoring = 'neg_log_loss'),\n    GridSearchCV(KNeighborsClassifier(weights = 'distance'), param_grid, cv = 5, scoring = 'neg_log_loss'),\n    LogisticRegressionCV(cv = 5, solver = 'sag', multi_class = 'multinomial', n_jobs = -1),\n    MLPClassifier(activation = 'logistic', learning_rate = 'adaptive', warm_start = True),\n    MLPClassifier(activation = 'identity', learning_rate = 'adaptive', warm_start = True),\n    MLPClassifier(activation = 'tanh', learning_rate = 'adaptive', warm_start = True),\n    MLPClassifier(activation = 'relu', learning_rate = 'adaptive', warm_start = True)\n]\n\nn = len(clf_list)\n\npredictions = np.zeros( (test.shape[0], 9) )\n\nfor i in range(n):\n    print('At classifer: ' + str(i + 1) )\n    clf = clf_list[i]\n    \n    clf.fit(train, train_labels)\n    \n    predictions = predictions + clf.predict_proba( test )\n\npredictions = (1.0/n) * predictions\n\ndf_pred = pd.DataFrame(predictions, columns = classes)","cell_type":"code","outputs":[],"metadata":{"trusted":false,"_uuid":"19c06e09321e18038d453d35c6e458904aa9ac19","_execution_state":"idle","collapsed":false,"_cell_guid":"65c4ced0-6e5d-45c9-b355-f964f325af17"},"execution_count":null},{"source":"df_sub = pd.concat([df_sub, df_pred], axis = 1)\n\ndf_sub.to_csv('submission.csv', index=False)","cell_type":"code","outputs":[],"metadata":{"trusted":false,"_uuid":"a31e1e4533c32a000bbb6f256408caac7349a1cf","_execution_state":"idle","collapsed":false,"_cell_guid":"b0c4ee9b-ad30-4cde-8353-e3a31a658bc6"},"execution_count":null},{"source":"df_sub.head(10)","cell_type":"code","outputs":[],"metadata":{"trusted":false,"_uuid":"7e8cacee58615292e3abe01b2e078eb3a650911b","_execution_state":"idle","collapsed":false,"_cell_guid":"76166222-0794-4fb6-8a81-1e7ed91c7776"},"execution_count":null}],"nbformat_minor":0,"metadata":{"language_info":{"nbconvert_exporter":"python","version":"3.6.1","mimetype":"text/x-python","file_extension":".py","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat":4}