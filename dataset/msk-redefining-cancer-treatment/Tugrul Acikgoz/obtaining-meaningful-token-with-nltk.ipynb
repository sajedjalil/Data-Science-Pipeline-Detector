{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"file_extension":".py","nbconvert_exporter":"python","pygments_lexer":"ipython3","name":"python","version":"3.6.1","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python"}},"nbformat":4,"cells":[{"metadata":{"_uuid":"c57b02a2b846fd8dad444a9fb2f7c7dc9321eb65","_cell_guid":"15555e34-b5f4-450f-8824-a89e5ffbe898"},"cell_type":"markdown","source":"In this Kernel, Obtaining meaningful token with NLTK will be examined.\n\nNecessary libraries are uploaded like below."},{"metadata":{"_uuid":"7413700fec0a6068331ab3d871fef81c0b293b88","_cell_guid":"608a8975-1291-4b48-811d-d520b3700535","collapsed":true},"execution_count":null,"cell_type":"code","source":"# Import Pandas\nimport pandas as pd\n# Import Counter\nfrom collections import Counter\n# Import WordNetLemmatizer\nfrom nltk.stem import WordNetLemmatizer\n# Import word_tokenize\nfrom nltk.tokenize import word_tokenize\n# Import stopwords\nfrom nltk.corpus import stopwords\n# Import pyplot\nimport matplotlib.pyplot as plt\n# Import string\nimport string\n\n\n# Import randint just for test\nfrom random import randint\n\n# Main image size\nplt.rcParams[\"figure.figsize\"] = (18, 9)","outputs":[]},{"metadata":{"_uuid":"4f4dab9cef9799fbc2661831353c8a83ea346fe9","_cell_guid":"1a37c077-2cc4-48d3-9a78-45b44df4bab6"},"cell_type":"markdown","source":"Get text data by using pandas like below."},{"metadata":{"_uuid":"f137176c0e13e82a20d78792cb76403820745c66","_cell_guid":"933b776b-1778-4d41-bb5d-b7debc3e476e","collapsed":true},"execution_count":null,"cell_type":"code","source":"df_text = pd.read_csv(r'../input/training_text', sep='\\|\\|', engine='python', skiprows=1, names=['ID', 'Text']).set_index('ID')","outputs":[]},{"metadata":{"_uuid":"ecf989755eacb0bbed732b943eac4b1f14f296ff","_cell_guid":"bdd6c50c-dd0d-449c-8f48-ead341c79d7c"},"cell_type":"markdown","source":"For examining data, let's just get random input in the text field."},{"metadata":{"_uuid":"481d4f5fa4bc176f6a176cd1e124bd4bb5292651","_cell_guid":"f4889ed3-e275-4835-b50a-3580ebc7d6e2","collapsed":true},"execution_count":null,"cell_type":"code","source":"# Tokenize the article: tokens\ntokens = word_tokenize(str(df_text.iloc[randint(0, len(df_text.index))].values))\n# Take the 15 most common tokens\nonly_token_all=sorted(Counter(tokens).most_common(15), key=lambda w: w[1], reverse=True)","outputs":[]},{"metadata":{"_uuid":"257e794bfc2a31ff5cae466963c033631be6972e","_cell_guid":"2d9719bc-fb3b-4d6f-9f60-19f144a4885b"},"execution_count":null,"cell_type":"code","source":"fig1, ax = plt.subplots()\nax.bar(range(len(only_token_all)), [t[1] for t in only_token_all]  , align=\"center\")\nax.set_xticks(range(len(only_token_all)))\nax.set_xticklabels([t[0] for t in only_token_all])\nplt.xlabel('Tokens')\nplt.ylabel('Number of Usage')\nplt.title(r'$\\mathrm{Common\\ Tokens\\ in\\ TEXT:}\\ Applied\\ Tokenize$')\nplt.grid(True)\nplt.show()","outputs":[]},{"metadata":{"_uuid":"8cedd395bc77f5bf16129851c508693e5a8ecb58","_cell_guid":"c7972e07-2b05-43fc-8498-4946f0a14fe7"},"cell_type":"markdown","source":"As it can be seen from figure above, there are lots of non-alphabetic characters, so let's clear those like below."},{"metadata":{"_uuid":"d607a45d03887bc3494d21da7f79ed7927af6bd7","_cell_guid":"f412723e-7600-41cf-9647-ac5c60d35ce5","collapsed":true},"execution_count":null,"cell_type":"code","source":"# Convert the tokens into lowercase: lower_tokens\nlower_tokens = [t.lower() for t in tokens]\n# and delete punctuation\nlower_tokens = [''.join(c for c in s if c not in string.punctuation) for s in lower_tokens]\nlower_tokens = [s for s in lower_tokens if s]\n# Retain alphanumeric: alpha_only\nalpha_only = [t for t in lower_tokens if not t.isdigit()]\n# Again take the 15 most common tokens\nalpha_only_all=sorted(Counter(alpha_only).most_common(15), key=lambda w: w[1], reverse=True)","outputs":[]},{"metadata":{"_uuid":"31135964eee2407e87d92b24eea7187ab741cd59","_cell_guid":"f3bad5a1-cd29-4dc7-ab70-7ff7476156fa"},"execution_count":null,"cell_type":"code","source":"fig2, ax = plt.subplots()\nax.bar(range(len(alpha_only_all)), [t[1] for t in alpha_only_all]  , align=\"center\")\nax.set_xticks(range(len(alpha_only_all)))\nax.set_xticklabels([t[0] for t in alpha_only_all])\nplt.xlabel('Tokens')\nplt.ylabel('Number of Usage')\nplt.title(r'$\\mathrm{Common\\ Tokens\\ in\\ TEXT:}\\ Applied\\ Tokenize&Alpha$')\nplt.grid(True)\nplt.show()","outputs":[]},{"metadata":{"_uuid":"79d088d2beb46de855e8588f9246eff44482aff0","_cell_guid":"b3b77ad0-e4f1-4a1e-96c6-aba1e5aeade8"},"cell_type":"markdown","source":"Started to see some meaningful data, but there are still lots of tokens which we don't need. Let's proceed with clearing stop words."},{"metadata":{"_uuid":"900123f8fd37c67307c8e9b682c47bae3b7a84d8","_cell_guid":"a740d81f-1145-4bd2-ac2e-7cb419955108","collapsed":true},"execution_count":null,"cell_type":"code","source":"# Remove all stop words: no_stops\nstop = set(stopwords.words('english'))\nno_stops = [t for t in alpha_only if t not in stop]\n# Again take the 15 most common tokens\nno_stops_all=sorted(Counter(no_stops).most_common(15), key=lambda w: w[1], reverse=True)","outputs":[]},{"metadata":{"_uuid":"bf0eebb4fd52dab99e0a040b6835a1712bde8687","_cell_guid":"7db73db3-9414-4f57-bee4-65ff7878c9b1"},"execution_count":null,"cell_type":"code","source":"fig3, ax = plt.subplots()\nax.bar(range(len(no_stops_all)), [t[1] for t in no_stops_all]  , align=\"center\")\nax.set_xticks(range(len(no_stops_all)))\nax.set_xticklabels([t[0] for t in no_stops_all])\nplt.xlabel('Tokens')\nplt.ylabel('Number of Usage')\nplt.title(r'$\\mathrm{Common\\ Tokens\\ in\\ TEXT:}\\ Applied\\ Tokenize&Alpha&Stop$')\nplt.grid(True)\nplt.show()","outputs":[]},{"metadata":{"_uuid":"469c843118c99df83fc66451d807acd8db24fba3","_cell_guid":"da9c04e2-3bc3-47e0-aa4f-94e89576f5a1"},"cell_type":"markdown","source":"Now we are progressing! Now let's clear plural words."},{"metadata":{"_uuid":"b9b15d101c8b488de3f5daeba1c9b14d48269a75","_cell_guid":"c0c5fd72-12e6-40c4-873c-49dc8636065b","collapsed":true},"execution_count":null,"cell_type":"code","source":"# Instantiate the WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n# Lemmatize all tokens into a new list: lemmatized\nlemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n# Append the 15 most common tokens for lemmatizer\nlemmatized_all=sorted(Counter(lemmatized).most_common(15), key=lambda w: w[1], reverse=True)","outputs":[]},{"metadata":{"_uuid":"70bd8a29f54ddf04f241dfe5c3ec71ab772f33df","_cell_guid":"f381c3bf-447d-49a2-8825-cc8e0413368d"},"execution_count":null,"cell_type":"code","source":"fig4, ax = plt.subplots()\nax.bar(range(len(lemmatized_all)), [t[1] for t in lemmatized_all]  , align=\"center\")\nax.set_xticks(range(len(lemmatized_all)))\nax.set_xticklabels([t[0] for t in lemmatized_all])\nplt.xlabel('Tokens')\nplt.ylabel('Number of Usage')\nplt.title(r'$\\mathrm{Common\\ Tokens\\ in\\ TEXT:}\\ Applied\\ Tokenize&Alpha&Stop&Lemmatized$')\nplt.grid(True)\nplt.show()","outputs":[]},{"metadata":{"_uuid":"9aee9481c146e883639c5042bc9ceaaba073efdf","_cell_guid":"e2f578a5-502f-4685-83c8-4ce4f6808a35"},"cell_type":"markdown","source":"hmm... Not much progress. Let's clear this data by using custom stop words like below."},{"metadata":{"_uuid":"e0b1d6fc455db36bf5aaaf703a7d6f5014faa327","_cell_guid":"da944f4e-cac7-432e-aacc-89ac96c5da18","collapsed":true},"execution_count":null,"cell_type":"code","source":"# Remove all stop words with updated data\nstop.update(['study', 'table', 'method', 'conclusion', 'case', 'data', 'syndrome', 'analyze', 'author', 'show', 'control', 'expression', 'supplementary', 'result', 'figure','fig', 'level', 'deletion', 'mm', 'state', 'effect', 'stability', 'activity','change','structure', 'line', 'loss', 'expression', 'et', 'al'])\nno_stops_updated = [t for t in lemmatized if t not in stop]\n# Append the 15 most common tokens for no_stop\nno_stops_updated_all = sorted(Counter(no_stops_updated).most_common(15), key=lambda w: w[1], reverse=True)","outputs":[]},{"metadata":{"_uuid":"4791e554e857b58aa8fb352a20c589c6b868101e","_cell_guid":"a959f616-1ebc-4435-aac2-5a09c7f0f0c9"},"execution_count":null,"cell_type":"code","source":"fig5, ax = plt.subplots()\nax.bar(range(len(no_stops_updated_all)), [t[1] for t in no_stops_updated_all]  , align=\"center\")\nax.set_xticks(range(len(no_stops_updated_all)))\nax.set_xticklabels([t[0] for t in no_stops_updated_all])\nplt.xlabel('Tokens')\nplt.ylabel('Number of Usage')\nplt.title(r'$\\mathrm{Common\\ Tokens\\ in\\ TEXT:}\\ Applied\\ Tokenize&Alpha&Updated-Stop&Lemmatized$')\nplt.grid(True)\nplt.show()","outputs":[]},{"metadata":{"_uuid":"393b34164e869158b4a33c281f2042c305279e16","_cell_guid":"9178fcca-924c-4344-b500-bd3721fdf2ed"},"cell_type":"markdown","source":"Now we see quite good data to start our learning process.\nHowever; NLTK stopwords is not quite enough, so we had to update it. I suggest to use 'sklearn.feature_extraction.stop_words' or 'spacy.en.language_data' as these have data twice of stopwords. I wanted to use only NLTK for this experiment."}],"nbformat_minor":1}