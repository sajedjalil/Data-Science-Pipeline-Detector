{"cells":[{"source":"%matplotlib inline\nimport numpy as np\nimport pandas as pd \nimport nltk\nimport string\nimport operator\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.manifold import TSNE\nfrom gensim.models import word2vec\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom nltk.corpus import wordnet as wn\nfrom nltk.probability import FreqDist\nimport re","metadata":{"collapsed":true,"_cell_guid":"8a814e5d-7099-4e20-93f6-b37fa7041237","_uuid":"1f908514f7447e8786422a52ec9d07c828444f4c"},"cell_type":"code","outputs":[],"execution_count":36},{"source":"This class uses POS Tagging and the RAKE algorithm to compute either a summary of the most important sentences or to extract the keywords for every document. ","metadata":{"_cell_guid":"76ae240f-d53b-491b-979d-c402ffa6e998","_uuid":"b0ff80106120a56a832b5cb7bac695604b5c0f0d"},"cell_type":"markdown"},{"source":"class RAKE_tagged():\n    '''\n    This class applies the RAKE (Rapid Automatic Keyword Extraction) algorithm after filtering and stemming the possible candidates.\n    it can be used for any language by passing the taggers, tokenizers and stemmers to the initialisation\n    '''\n    def __init__(self, no_of_keywords, stopwords='auto', pos =['N'], tokenizer='auto', tagging='auto', stemming='auto'):\n        self.pos = pos\n        self.nkey = no_of_keywords\n        self.stopwords = stopwords\n        self.tokenizer = tokenizer\n        self.tagging = tagging\n        self.lemma = stemming\n        self.finaltext = []\n\n\n        pass\n    \n    def isPunct(self, word):\n        return len(word) == 1 and word in string.punctuation\n\n    def isNumeric(self, word):\n        try:\n            float(word) if '.' in word else int(word)\n            return True\n        except ValueError:\n            return False\n            \n    def manage_params(self):\n        if type(self.stopwords) == list:\n            pass\n        elif self.stopwords == \"auto\":\n            self.stopwords = set(nltk.corpus.stopwords.words())\n        else:\n            self.stopwords = []\n            \n        if self.tokenizer == 'auto':\n            self.tokenizer = nltk.word_tokenize\n        else:\n            pass\n        \n        if self.tagging == 'auto':\n            self.tagging = nltk.pos_tag\n        else:\n            pass\n        \n        if self.lemma == 'auto':\n            lemmatizer = WordNetLemmatizer()\n            self.lemma = lemmatizer.lemmatize\n        else:\n            pass\n\n    def candidate_keywords(self, document_sents):\n        '''\n        Finding the possible candidates for keywords, by removing stopwords and filtering by POS tag\n        '''\n        phrase_list = []\n        all_words = []\n\n        for sentence in document_sents:\n            words = map(lambda x: \"#\" if x in self.stopwords else x, self.tagging(self.tokenizer(sentence.lower())))\n            \n            # create lemmatized corpus for word2vec (lists-of-words in lists-of-sentences in list-of-corpus)\n            sent = []\n            for w in words:\n                all_words.append(w)\n                if w[0] != \"#\" or self.isPunct(w[0]) == False:\n                    try:\n                        sent.append(self.lemma(w[0], w[1][0].lower()))\n                    except:\n                        sent.append(self.lemma(w[0]))\n            self.finaltext.append(sent)\n        \n            \n        \n        # back to the RAKE Algorithm \n        phrase = []\n        candidates =[]\n\n        for (word, tag) in all_words:\n            if word == \"#\" or self.isPunct(word):\n                if len(phrase) > 0:\n                    phrase_list.append(phrase)\n                    phrase = []\n            else:\n                if self.pos != None:\n                    phrase.append(word)\n                    for t in self.pos:\n                        if tag.startswith(t):\n                            candidates.append(self.lemma(word, t[0].lower()))\n                                #print(self.lemma(word, t.lower()))\n                        else:\n                            pass\n                else:\n                    phrase.append(word)\n                    candidates.append(word)\n        return phrase_list, candidates\n    \n    def calculate_word_scores(self, phrase_list, candidates):\n        word_freq = nltk.FreqDist()\n        word_degree = nltk.FreqDist()\n        for phrase in phrase_list:\n          degree = len(list(filter(lambda x: not self.isNumeric(x), phrase))) - 1\n          for word in phrase:\n            word_freq[word] += 1\n            word_degree[word] += degree\n        for word in word_freq.keys():\n          word_degree[word] = word_degree[word] + word_freq[word]\n        word_scores = {}\n        for word in word_freq.keys():\n            if word in candidates:\n                word_scores[word] = word_degree[word] / word_freq[word]\n        return word_scores\n    \n    def calculate_phrase_scores(self, phrase_list, word_scores, candidates):\n        phrase_scores = {}\n        for phrase in phrase_list:\n          phrase_score = 0\n          for word in phrase:\n              if word in candidates:\n                phrase_score += word_scores[word]\n          phrase_scores[\" \".join(phrase)] = phrase_score\n        return phrase_scores\n    \n    def fit(self):\n        #might be useful for piping\n        pass\n\n    def transform(self, corpus, output_type=\"w\"):\n        keyword_results = []\n        self.manage_params()\n        for document in corpus:\n            sentences = nltk.sent_tokenize(document)\n            phrase_list, candidate_list = self.candidate_keywords(sentences)\n            word_scores = self.calculate_word_scores(phrase_list, candidate_list)\n            phrase_scores = self.calculate_phrase_scores(phrase_list, word_scores, candidate_list)\n            if output_type == \"s\":\n                sorted_scores = sorted(phrase_scores.items(),key=operator.itemgetter(1), reverse=True)\n            else:\n                sorted_scores = sorted(word_scores.items(),key=operator.itemgetter(1), reverse=True)           \n            keyword_results.append([k[0] for k in sorted_scores[0:self.nkey]])\n        return keyword_results","metadata":{"collapsed":true,"_cell_guid":"882dcf38-47ec-4a67-bb2a-501c7a1d1a5b","_uuid":"d2c0dd48aa8550c9733aa7ee74cf6f201897e844"},"cell_type":"code","outputs":[],"execution_count":37},{"source":"Computing the Vectors for the keywords with word2vec","metadata":{"_cell_guid":"7dac7d66-61d6-4953-b2be-82a7235fc342","_uuid":"1fa655f0d8b5ef68bfccad69bcfa7f2a81961302"},"cell_type":"markdown"},{"source":"class Computing_Vectors():\n    \n    def __init__(self):\n\n        pass\n    \n    def fit(self, corpus):\n        self.model = word2vec.Word2Vec(corpus, size=100, window=5, min_count=1, workers=4)\n        \n    \n    def transform(self, keywords):\n        self.arrays = []\n        \n        self.model = self.model.wv\n        found = 0\n        notfound = 0\n        for kwl in keywords:\n            for keyw in kwl:\n                try:\n                    self.arrays.append((self.model[keyw], keyw))\n                    found += 1\n                except:\n                    notfound += 1\n                    pass\n\n        return self.arrays\n    \n    def visualize(self):\n        x_arr =np.array([i[0] for i in self.arrays])\n        tsne = TSNE(n_components=2)\n        X_tsne = tsne.fit_transform(x_arr)\n        \n        plt.figure(figsize=(200, 200), dpi=100)\n        max_x = np.amax(X_tsne, axis=0)[0]\n        max_y = np.amax(X_tsne, axis=0)[1]\n        plt.xlim((-max_x,max_x))\n        plt.ylim((-max_y,max_y))\n\n        \n        for row_id in range(0, len(self.arrays)):\n            target_word = self.arrays[row_id][1]\n            x = X_tsne[row_id, 0]\n            y = X_tsne[row_id, 1]\n            plt.annotate(target_word, (x,y))\n\n        plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n        plt.savefig(\"word2vec.png\")\n        plt.show()\n ","metadata":{"collapsed":true,"_cell_guid":"9a85a356-0d5b-4cfe-9fe1-39654b250933","_uuid":"1c7847058e7ea6a1370429cf8c8a893081a3e98a"},"cell_type":"code","outputs":[],"execution_count":38},{"source":"Clustering the vectors to find concepts","metadata":{"_cell_guid":"d3508b4c-f200-4bde-a54f-d2fd163e0c5c","_uuid":"bbabe45284c43955fcffcffffedaf75f8704aa1f"},"cell_type":"markdown"},{"source":"class Clustering():\n    def __init__(self,  w2v, model):\n        self.model = model\n        self.w2v = w2v\n        self.w2vecarrays = np.array([i[0] for i in w2v])\n        pass\n    \n    def fit_transform(self, number_of_clusters):\n        cluster = KMeans(n_clusters=number_of_clusters, random_state=0).fit(self.w2vecarrays)\n        #cluster = DBSCAN(eps=0.01, min_samples=3).fit(self.w2vecarrays)\n        #cluster = SpectralClustering(n_clusters=number_of_clusters, eigen_solver='arpack',affinity=\"nearest_neighbors\")\n        #cluster = AgglomerativeClustering(n_clusters=number_of_clusters, linkage='ward')\n        self.labels = cluster.labels_\n        counts = np.bincount(self.labels[self.labels>=0])\n               \n        self.concepts = {}\n        \n        for row_id in range(0, len(self.labels)):\n            word = self.w2v[row_id][1]\n            label = self.labels[row_id]\n            if label in self.concepts:\n                self.concepts[label].append(word)\n            else:\n                self.concepts[label] = [word]\n                \n        return self.concepts\n    \n    def visualize(self):\n        tsne = TSNE(n_components=2)\n        X_tsne = tsne.fit_transform(self.w2vecarrays)\n        \n        for concept in sorted(self.concepts):\n            try:\n                txt = \" \".join(self.concepts[concept])\n                wordcloud = WordCloud(background_color=\"white\",max_font_size=40, relative_scaling=.5).generate(txt)\n                plt.figure()\n                plt.imshow(wordcloud)       \n                plt.title(concept)\n                plt.axis(\"off\")\n                plt.show()\n            except ValueError:\n                print(self.concepts[concept])","metadata":{"collapsed":true,"_cell_guid":"c41c6a23-1168-4b34-a420-e18e30848a70","_uuid":"a129124352d3ae3a4111a0a5164fe5ff6c540396"},"cell_type":"code","outputs":[],"execution_count":39},{"source":"Now we put everything together and see the result","metadata":{"_cell_guid":"e29e5223-6a98-47db-9c55-0c9365025b12","_uuid":"6a9fdf723a63a8957c4888d4ff43a2134c8a89f1"},"cell_type":"markdown"},{"source":"First we load the data","metadata":{"_cell_guid":"3483b44f-a2a1-426c-ab53-c301d6835d1c","_uuid":"e5ac193b8cb98bb70bc5ce3145d02896009ac443"},"cell_type":"markdown"},{"source":"\nfrom nltk.corpus import inaugural\n\nsample = [{'ID': fileid, 'Text': inaugural.raw(fileid)} for fileid in inaugural.fileids()]\ndf = pd.DataFrame(sample)\n\ndf= pd.read_csv('../input/stage2_test_text.csv', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n#df.head()\n\ndf_txt = df['Text'][:2000]","metadata":{"_cell_guid":"a94a2502-668c-47b2-a80b-8b25c41ca681","_uuid":"2e0beb2612115e058cfa0b29996b1765c570dca2"},"cell_type":"code","outputs":[],"execution_count":40},{"source":"In the next step we use two layers of the RAKE algorithm to compute Noun-only-keywords. The first layer computes a summary of  the most important sentences. The second layer computes the keywords. Since the corpus is relatively small, we use a summary of 100 sentences in the first layer and then compute 80 keywords each. ","metadata":{"_cell_guid":"e3d48a4d-da33-40aa-a5c4-2dae37d69b49","_uuid":"0fdc0eb16041ee2d45d814cc8870e1ff8411a39a"},"cell_type":"markdown"},{"source":"myRE1 = RAKE_tagged(10, stopwords='auto', pos=[\"N\", \"VBP\", \"R\"])\nmyRE = RAKE_tagged(10, stopwords='auto', pos=[\"N\"])\nsummary = myRE1.transform(df_txt, output_type=\"s\")\nsummaries =[\"; \".join(s) for s in summary]\nkeywords = myRE.transform(summaries, output_type=\"w\")","metadata":{"collapsed":true,"_cell_guid":"b8d60a9a-20a9-41ae-b9ce-3e7a04a58748","_uuid":"2280b743297176c405a91121f10e26bf5a935d63"},"cell_type":"code","outputs":[],"execution_count":41},{"source":"In the next step we use word2vec to compute a vector representation of the keywords. To visualize them, we can perform dimensonality reduction using tsne.","metadata":{"_cell_guid":"506fec7c-a9a6-4398-a2d0-437a850d57cd","_uuid":"328dc5cd4d3e6854f3363f46cc1db454c7d570de"},"cell_type":"markdown"},{"source":"CV = Computing_Vectors()\nCV.fit(myRE1.finaltext)\narr = CV.transform(keywords)\nCV.visualize()","metadata":{"_cell_guid":"7ec0644f-5303-43fe-9699-7fd296d91129","_uuid":"2cc4b5d120cb0bbf012dcaf2dd13a0c60b20456b","scrolled":true},"cell_type":"code","outputs":[],"execution_count":42},{"source":"In this step we cluster the result with KMeans. It would be interesting to try other clustering algorithms for this task. For example DBSCAN. We visualize the output as wordclouds.","metadata":{"_cell_guid":"5df076e5-5209-4399-b119-b45993298d52","_uuid":"493cfa282f0ce195a5df1c9b0c910e14a56a8c25"},"cell_type":"markdown"},{"source":"CL = Clustering(arr, CV.model)\nconcepts = CL.fit_transform(25)\nCL.visualize()","metadata":{"_cell_guid":"38b4a5e8-d8d8-437f-a090-de3848c960c6","_uuid":"d34d46f9547ec0a1826e03c3a5912993992f6fae"},"cell_type":"code","outputs":[],"execution_count":43},{"source":"Ok so now we can work with those concepts and see what we can do. First,let's write a class to extract some simple relationships between those concepts\n","metadata":{"_cell_guid":"0b5b26b2-492e-4fe2-a2eb-6ae0205708bd","_uuid":"6c7a856316389c9d7f3a0fff35c2e125f18942eb"},"cell_type":"markdown"}],"nbformat":4,"metadata":{"language_info":{"nbconvert_exporter":"python","version":"3.6.3","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"name":"python","file_extension":".py","mimetype":"text/x-python"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":1}