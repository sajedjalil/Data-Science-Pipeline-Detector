{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse import hstack\nfrom sklearn.preprocessing import normalize\nimport time, re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nimport sklearn.metrics as metrics\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"codestarttime = time.clock()\n# Read training variant csv.\ntrainingV = pd.read_csv(r'/kaggle/input/msk-redefining-cancer-treatment/training_variants',encoding = 'utf-8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"trainingV.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Read training text csv.\ntrainingT = pd.read_csv(r'/kaggle/input/msk-redefining-cancer-treatment/training_text',sep='\\|\\|', header = None, skiprows = 1, names = ['ID','Text'],encoding = 'utf-8')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"trainingT.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Merging both Data Frames\ntrainData = trainingV.merge(trainingT,how= 'inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"trainData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Re-ordering columns\ntrainData = trainData.reindex(columns=['ID','Gene','Variation','Text','Class'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"trainData.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check for null values"},{"metadata":{"trusted":false},"cell_type":"code","source":"trainData.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"trainData.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing Null values"},{"metadata":{"trusted":false},"cell_type":"code","source":"trainData = trainData[~trainData.Text.isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"trainData.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"trainData.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the distribution of data for each class"},{"metadata":{"trusted":false},"cell_type":"code","source":"df = trainData.groupby('Class').Gene.describe()\ndf = df.reset_index()\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.barplot(x = 'Class',y = 'count',data= df)\nplt.title('Count of Gene in Each Class')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We could see that count of Class 7 is high and Class 8 and 9 are very less in count"},{"metadata":{},"cell_type":"markdown","source":" Install WordCloud for plotting the most common words in each class"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Install WordCloud for plotting the most common words in each class\n!pip install wordcloud\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Function to remove any special character, any extra spaces in the Text column\ndef text_preprocessing(total_text):\n    if type(total_text) is not int:\n        string = \"\"\n        # replace every special char with space\n        total_text = re.sub('[^a-zA-Z0-9\\n]', ' ', str(total_text))\n        # replace multiple spaces with single space\n        total_text = re.sub('\\s+', ' ', total_text)\n        # converting all the chars into lower-case.\n        total_text = total_text.lower()\n\n        for word in total_text.split():\n            string += word + \" \"\n\n        return string","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apply Text_processing function to Text Column"},{"metadata":{"trusted":false},"cell_type":"code","source":"#text processing stage.\nstart_time = time.clock()\ntrainData.Text = trainData.Text.apply(text_preprocessing)\nprint('Time took for preprocessing the text :',time.clock() - start_time, \"seconds\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"trainData.Text.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Get top n words in the Text\ndef get_top_n_words(corpus, n=None):\n    vec = CountVectorizer(stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    string = ''\n    words = [string + x[0] for x in words_freq[:n]]\n    return ' '.join(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plot Word Cloud for the top N words in the class\ndef plot_wordCloud(df,Class):\n    df = df[df.Class == Class]\n    text = df.Text\n    common2500Words = get_top_n_words(text,2500)\n    wordcloud = WordCloud(background_color=\"white\").generate(common2500Words)\n    plt.figure(figsize= (15,5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Plot Words cloud for top 2500 words in each class__"},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_wordCloud(df = trainData,Class = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_wordCloud(df = trainData,Class = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_wordCloud(df = trainData,Class = 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_wordCloud(df = trainData,Class = 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_wordCloud(df = trainData,Class = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_wordCloud(df = trainData,Class = 6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_wordCloud(df = trainData,Class = 7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_wordCloud(df = trainData,Class = 8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_wordCloud(df = trainData,Class = 9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's see the top 100 words in all the class"},{"metadata":{"trusted":false},"cell_type":"code","source":"topWords = get_top_n_words(trainData.Text, n=100)\ntopWords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Mutations, Cells, Patients, protein, tumoir, variants, kinase are some of the words that occur in almost all the classes"},{"metadata":{},"cell_type":"markdown","source":"#### Lets do a barplot for the count of top 10 words in each class"},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_top_words(corpus, n=None):\n    vec = CountVectorizer(stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)         \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words = [x[0] for x in words_freq[:n]]\n    count = [x[1] for x in words_freq[:n]]\n    return words,count","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def plot_topwords(df,Class):\n    df = df[df.Class == Class]\n    text = df.Text\n    Words,Count = get_top_words(text,10)\n    plt.figure(figsize= (12,5))\n    sns.barplot(Count,Words)\n    plt.title('Class =' + str(Class))\n    plt.xlabel(\"Count of Words\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_topwords(df = trainData,Class = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_topwords(df = trainData,Class = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_topwords(df = trainData,Class = 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_topwords(df = trainData,Class = 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_topwords(df = trainData,Class = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_topwords(df = trainData,Class = 6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_topwords(df = trainData,Class = 7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_topwords(df = trainData,Class = 8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_topwords(df = trainData,Class = 9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Find the total number of words for each Text row"},{"metadata":{"trusted":false},"cell_type":"code","source":"trainData['Number of Words'] = trainData.Text.apply(lambda x: len(x.split()))\ntrainData.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of number of words "},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nsns.distplot(trainData['Number of Words'])\nplt.xlabel('Number of words in text', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.title(\"Frequency of number of words\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### We can see that the mean number of words in all the rows is 10000 "},{"metadata":{},"cell_type":"markdown","source":"#### Lets do a box plot for the number of words less than 30000"},{"metadata":{"trusted":false},"cell_type":"code","source":"df = trainData[trainData['Number of Words'] < 30000]\nplt.figure(figsize=(12,8))\nsns.boxplot(y= 'Number of Words' ,x='Class', data= df )\nplt.xlabel('Class', fontsize=12)\nplt.ylabel('Text - Number of words', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Split the data in to train and test"},{"metadata":{"trusted":false},"cell_type":"code","source":"X = trainData.drop(columns=['ID','Class'])\nY = trainData.Class\nx_train,x_test, y_train, y_test = train_test_split(X,Y,train_size = 0.7, random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Convert Gene column in to a Count Vectorizer matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"geneCV = CountVectorizer()\nxtrain_gene_feature = geneCV.fit_transform(x_train.Gene)\nxtest_gene_feature = geneCV.transform(x_test.Gene)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Convert Variation column in to a Count Vectorizer matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"variationCV = CountVectorizer()\nxtrain_variation_feature = variationCV.fit_transform(x_train.Variation)\nxtest_variation_feature = variationCV.transform(x_test.Variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Convert Text column in to a Count Vectorizer matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"textCV = CountVectorizer(stop_words= 'english',min_df= 5 )\nxtrain_text_feature = textCV.fit_transform(x_train.Text)\nxtest_text_feature = textCV.transform(x_test.Text)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Normalise the Text Column count vector matrix so that each is a unit vector\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"xtrain_text_feature =  normalize(xtrain_text_feature, axis=0)\nxtest_text_feature =  normalize(xtest_text_feature, axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Combine all the three count vector matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_gene_var_text = hstack((xtrain_gene_feature,xtrain_variation_feature,xtrain_text_feature)).tocsr()\ntest_gene_var_text = hstack((xtest_gene_feature,xtest_variation_feature,xtest_text_feature)).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_gene_var_text.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def predict_and_plot_confusion_matrix(train_x, train_y,test_x, test_y, clf):\n    clf.fit(train_x, train_y)\n#     sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n#     sig_clf.fit(train_x, train_y)\n    pred_y = clf.predict(test_x)\n\n    # for calculating log_loss we willl provide the array of probabilities belongs to each class\n    print(\"Log loss :\",log_loss(test_y, clf.predict_proba(test_x)))\n    # calculating the number of data points that are misclassified\n    print(\"Number of mis-classified points :\", np.count_nonzero((pred_y- test_y))/test_y.shape[0])\n    plot_confusion_matrix(test_y, pred_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def plot_confusion_matrix(y_test,pred_y):\n    plt.figure(figsize=(20,7))\n    labels = [1,2,3,4,5,6,7,8,9]\n    confuMatrix = confusion_matrix(y_test,pred_y)\n    sns.heatmap(confuMatrix,annot= True,cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lets' Build Machine Learning Models"},{"metadata":{},"cell_type":"markdown","source":"### Logistic regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"# We build a Logistic regression with default Parameters and check the perfomance\nLR = LogisticRegression()\nLR.fit(train_gene_var_text,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy and loss (cross-entropy) measure two different things. Cross-entropy loss awards lower loss to predictions which are closer to the class label. The accuracy, on the other hand, is a binary true/false for a particular sample. That is, Loss here is a continuous variable i.e. itâ€™s best when predictions are close to 1 (for true labels) and close to 0 (for false ones). While accuracy is kind of discrete."},{"metadata":{},"cell_type":"markdown","source":"Lets' calculate Log Loss for all the Machine Learning Models here"},{"metadata":{"trusted":false},"cell_type":"code","source":"#predict log-loss for train data\npredict_y = LR.predict_proba(train_gene_var_text)\nprint(\"The train log loss is:\",log_loss(y_train, predict_y, labels=LR.classes_, eps=1e-15))\n\n#predict log-loss for test data\npredict_y = LR.predict_proba(test_gene_var_text)\nprint(\"The test log loss is:\",log_loss(y_test, predict_y, labels=LR.classes_, eps=1e-15))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Lets do a Grid Search CV to find suitable parameters"},{"metadata":{"trusted":false},"cell_type":"code","source":"#We build logistic regression and find out best parameters(alpha and penalty) with Grid search and 10 fold CV on train data.\n\n#Make a dict of our parameters\nalpha = [10 ** x for x in range(-6, 3)]\nparams = {'C':alpha,'penalty':['l2']}\n\n#Build Logistic regression\nclfLR = LogisticRegression(class_weight='balanced',multi_class='multinomial',solver='newton-cg',n_jobs= -1)\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"#Grid Search with 10 fold CV\nrandom = GridSearchCV(clfLR,param_grid=params,n_jobs= -1,cv=10)\nrandom.fit(train_gene_var_text,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"best_alpha = random.best_params_['C']\nbest_penalty = random.best_params_['penalty']\nprint('The best value for Cost, C is ', best_alpha)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#build logistic regression with best hyper-parameters(alpha and penalty)\n#instead of using one vs rest we use multinomial which performs better compared to ovr in this case.\n#multinomial does not support linear solver so we use newton-cg as our optimization problem solver.\nclfLR = LogisticRegression(class_weight='balanced', C=best_alpha, penalty=best_penalty,multi_class='multinomial',solver='newton-cg')\nclfLR.fit(train_gene_var_text, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#predict log-loss for train data\npredict_y = clfLR.predict_proba(train_gene_var_text)\nprint('For values of best alpha = ', best_alpha,'penalty',best_penalty, \"The train log loss is:\",log_loss(y_train, predict_y, labels=LR.classes_, eps=1e-15))\n\n#predict log-loss for test data\npredict_y = clfLR.predict_proba(test_gene_var_text)\nprint('For values of best alpha = ', best_alpha,'penalty',best_penalty, \"The test log loss is:\",log_loss(y_test, predict_y, labels=LR.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To avoid rounding error while multiplying probabilites we use log-probability estimates.\nProbability calibration with sigmoid regression."},{"metadata":{"trusted":false},"cell_type":"code","source":"sig_clfLR = CalibratedClassifierCV(clfLR, method=\"sigmoid\")\nsig_clfLR.fit(train_gene_var_text, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#predict log-loss for train data\npredict_y = sig_clfLR.predict_proba(train_gene_var_text)\nprint('For values of best alpha = ', best_alpha,'penalty',best_penalty, \"The train log loss is:\",log_loss(y_train, predict_y, labels=clfLR.classes_, eps=1e-15))\n\n#predict log-loss for test data\npredict_y = sig_clfLR.predict_proba(test_gene_var_text)\nprint('For values of best alpha = ', best_alpha,'penalty',best_penalty, \"The test log loss is:\",log_loss(y_test, predict_y, labels=clfLR.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predict_and_plot_confusion_matrix(train_gene_var_text,y_train,test_gene_var_text,y_test,clf = sig_clfLR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Classifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Running the random forest with default parameters.\nrfc = RandomForestClassifier()\nrfc.fit(train_gene_var_text,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#predict log-loss for train data\npredict_y = rfc.predict_proba(train_gene_var_text)\nprint(\"The train log loss is:\",log_loss(y_train, predict_y, labels=rfc.classes_, eps=1e-15))\n\n#predict log-loss for test data\npredict_y = rfc.predict_proba(test_gene_var_text)\nprint(\"The test log loss is:\",log_loss(y_test, predict_y, labels=rfc.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [4,8,10],\n    'min_samples_leaf': range(100, 400, 200),\n    'min_samples_split': range(200, 500, 200),\n    'n_estimators': [100,200, 300], \n    'max_features': [5, 10]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1,verbose = 1)\n\ngrid_search.fit(train_gene_var_text,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Building the Random Forest Classifier with best parameters\nrfc = RandomForestClassifier(bootstrap=True,\n                             max_depth=4,\n                             min_samples_leaf=100, \n                             min_samples_split=200,\n                             max_features=5,\n                             n_estimators=100)\nrfc.fit(train_gene_var_text,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#predict log-loss for train data\npredict_y = rfc.predict_proba(train_gene_var_text)\nprint('For values of best alpha = ', best_alpha,'penalty',best_penalty, \"The train log loss is:\",log_loss(y_train, predict_y, labels=rfc.classes_, eps=1e-15))\n\n#predict log-loss for test data\npredict_y = rfc.predict_proba(test_gene_var_text)\nprint('For values of best alpha = ', best_alpha,'penalty',best_penalty, \"The test log loss is:\",log_loss(y_test, predict_y, labels=rfc.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# to avoid rounding error while multiplying probabilites we use log-probability estimates.\n#Probability calibration with sigmoid regression.\nsig_clfRFC = CalibratedClassifierCV(rfc, method=\"sigmoid\")\nsig_clfRFC.fit(train_gene_var_text, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#predict log-loss for train data\npredict_y = sig_clfRFC.predict_proba(train_gene_var_text)\nprint('For values of best alpha = ', best_alpha,'penalty',best_penalty, \"The train log loss is:\",log_loss(y_train, predict_y, labels=rfc.classes_, eps=1e-15))\n\n#predict log-loss for test data\npredict_y = sig_clfRFC.predict_proba(test_gene_var_text)\nprint('For values of best alpha = ', best_alpha,'penalty',best_penalty, \"The test log loss is:\",log_loss(y_test, predict_y, labels=rfc.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predict_and_plot_confusion_matrix(train_gene_var_text,y_train,test_gene_var_text,y_test,clf = sig_clfRFC)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Naive Bayes"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Build Naive Bayes with default parameters\nmnb = MultinomialNB()\n\nmnb.fit(train_gene_var_text,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#predict log-loss for train data\npredict_y = mnb.predict_proba(train_gene_var_text)\nprint( \"The train log loss is:\",log_loss(y_train, predict_y, labels=mnb.classes_, eps=1e-15))\n\n#predict log-loss for test data\npredict_y = mnb.predict_proba(test_gene_var_text)\nprint(\"The test log loss is:\",log_loss(y_test, predict_y, labels=mnb.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n# We build Multinomial NB and find out best parameters(alpha) with grid search and 10 fold CV on train data.\n\n#Make a dict of our parameters\nalpha = [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000]\nparams = {'alpha':alpha}\n\n#build multinomial NB\nclf = MultinomialNB()\n#Grid Search with 10 fold CV\nrandom = GridSearchCV(clf,param_grid=params,cv=10,return_train_score=True,n_jobs=2)\nrandom.fit(train_gene_var_text, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"best_alpha = random.best_params_['alpha']\nprint(\"The best value for aplha is \", best_alpha)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best value for alpha is the default value that is 1. "},{"metadata":{"trusted":false},"cell_type":"code","source":"# to avoid rounding error while multiplying probabilites we use log-probability estimates.\n#Probability calibration with sigmoid regression.\nsig_clfMNB = CalibratedClassifierCV(mnb, method=\"sigmoid\")\nsig_clfMNB.fit(train_gene_var_text, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#predict log-loss for train data\npredict_y = sig_clfMNB.predict_proba(train_gene_var_text)\nprint('For values of best alpha = ', best_alpha,'penalty',best_penalty, \"The train log loss is:\",log_loss(y_train, predict_y, labels=mnb.classes_, eps=1e-15))\n\n#predict log-loss for test data\npredict_y = sig_clfMNB.predict_proba(test_gene_var_text)\nprint('For values of best alpha = ', best_alpha,'penalty',best_penalty, \"The test log loss is:\",log_loss(y_test, predict_y, labels=mnb.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predict_and_plot_confusion_matrix(train_gene_var_text,y_train,test_gene_var_text,y_test,clf = sig_clfMNB)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear SVM Model"},{"metadata":{},"cell_type":"markdown","source":"#### Let's train a linear SVM model"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Build a Linear SVC model with default parameters\nfrom sklearn.svm import LinearSVC\nlinearsvc = LinearSVC()\nlinearsvc.fit(train_gene_var_text,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# specify range of parameters (C) as a list\nparams = {\"C\": [0.1, 1, 10, 100]}\n\nmodel = LinearSVC()\n\n# set up grid search scheme\n# note that we are still using the 5 fold CV scheme we set up earlier\nmodel_cv = GridSearchCV(estimator = model, param_grid = params,                          \n                        cv = 10, \n                        verbose = 1,\n                        n_jobs = -1,\n                       return_train_score=True)   \n# fit the model - it will fit 5 folds across all values of C\nmodel_cv.fit(train_gene_var_text,y_train)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"best_C = model_cv.best_params_['C']\nprint(\"The best value for C is \", best_C)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### To avoid rounding error while multiplying probabilites we use log-probability estimates. Probability calibration with sigmoid regression."},{"metadata":{"trusted":false},"cell_type":"code","source":"sig_clfLinearSVC = CalibratedClassifierCV(LinearSVC(C = 0.1), method=\"sigmoid\")\nsig_clfLinearSVC.fit(train_gene_var_text, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#predict log-loss for train data\npredict_y = sig_clfLinearSVC.predict_proba(train_gene_var_text)\nprint('For values of best alpha = ', best_alpha,'penalty',best_penalty, \"The train log loss is:\",log_loss(y_train, predict_y, labels=linearsvc.classes_, eps=1e-15))\n\n#predict log-loss for test data\npredict_y = sig_clfLinearSVC.predict_proba(test_gene_var_text)\nprint('For values of best alpha = ', best_alpha,'penalty',best_penalty, \"The test log loss is:\",log_loss(y_test, predict_y, labels=linearsvc.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predict_and_plot_confusion_matrix(train_gene_var_text,y_train,test_gene_var_text,y_test,clf = sig_clfLinearSVC)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conculsion"},{"metadata":{},"cell_type":"markdown","source":"Out of all above models, Calibrated Logistic Regression gives lesser Test Log Loss of __0.99__ followed by Calibrated SVM with Test Log Loss of __1.01__ and then Calibrated Naive Bayes with Loss of __1.26__ and then Calibrated Random Forest Classifier with a loss of __1.52__"},{"metadata":{},"cell_type":"markdown","source":"Our model logistic regression gives log-loss __0.99__ and error __34.0%__"},{"metadata":{},"cell_type":"markdown","source":"#### Lets test our test data with Calibrated Logistic Regression Model"},{"metadata":{},"cell_type":"markdown","source":"Load Test Data"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Read  Test Variant files\ntestV = pd.read_csv(r'/kaggle/input/msk-redefining-cancer-treatment/test_variants',encoding = 'utf-8')\ntestV.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Read  Test Text files\ntestT = pd.read_csv(r'/kaggle/input/msk-redefining-cancer-treatment/test_text',sep='\\|\\|', header = None, skiprows = 1, names = ['ID','Text'],encoding = 'utf-8')\ntestT.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"testData = pd.merge(testV,testT)\ntestData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#text processing stage.\nstart_time = time.clock()\ntestData.Text = testData.Text.apply(text_preprocessing)\nprint('Time took for preprocessing the text :',time.clock() - start_time, \"seconds\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Convert Gene column in to a Count Vectorizer matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"testData_gene_feature = geneCV.transform(testData.Gene)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Convert Variation column in to a Count Vectorizer matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"testData_variation_feature = variationCV.transform(testData.Variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Convert Text column in to a Count Vectorizer matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"testData_text_feature = textCV.transform(testData.Text.astype(str))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Normalise the Text Column count vector matrix so that each is a unit vector\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"testData_text_feature =  normalize(testData_text_feature, axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Combine all the three count vector matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"testData_gene_var_text = hstack((testData_gene_feature,testData_variation_feature,testData_text_feature)).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"testData_gene_var_text.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"final_pred = sig_clfLR.predict(testData_gene_var_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"final_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"testData['predicted_class'] = final_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"testData.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preparing submission data"},{"metadata":{"trusted":false},"cell_type":"code","source":"submission_df = pd.get_dummies(testData['predicted_class'],prefix= 'class',prefix_sep= ' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission_df.reset_index(inplace= True)\nsubmission_df.rename(columns={'index':'ID'},inplace= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"codeendtime = time.clock()\nprint('Code execution took: ', str((codeendtime - codestarttime)/60), 'mins')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}