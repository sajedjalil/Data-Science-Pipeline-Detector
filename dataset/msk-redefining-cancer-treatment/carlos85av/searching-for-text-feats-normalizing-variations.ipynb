{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"f3c7f112-b6a1-4931-a0b7-9cf276f2fdab","_uuid":"3f33e5ff847d76cddd3ae69a20863eaa192ca8a8"},"source":"# Searching for Unique Text Features and Normalizing Variations"},{"cell_type":"markdown","metadata":{"_cell_guid":"94dbc55f-e1d0-4cec-b3e5-ef5361e62016","_uuid":"5718b07c97102f47be18e349eb8081390959e384"},"source":"In this notebook I will explore the training dataset with the aim of finding relevant features to create our model.\n\nFirst of all, I have tried to make some sense of the variants and genes provided in the input files.\n\nThen, I have explored among the terms of the training texts taking into account their appearances in the different classes:\n    1- What are the terms that are unique in each class. How many documents in the training set can be classified only with this bag of words?\n    2- What are the terms that appears in the 9 classes. I will show them in a cloud tag for frequency.\n    3- How is the coverage of the terms that appears as much in one class, as much in two classes, and as much in three classes. \n\n"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"a517bc5f-15c0-4dd3-8feb-17c37200f3d9","collapsed":true,"_uuid":"31e5eb490d6ac54521dff7ae12fefd6da10f1456"},"source":"import pandas as pd\nimport numpy as np\nimport os\nimport json\nimport nltk, re, math, collections\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nimport matplotlib.pylab as plt\nimport operator\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb \nfrom datetime import datetime\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom os import path\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"138dbd3d-0093-4f2a-83b4-82ed72ae0df9","collapsed":true,"_uuid":"e0efab2f041a3c743d0b6dc0f612cb599a036898"},"source":"train_v = pd.read_csv('../input/training_variants')\ntest_v = pd.read_csv('../input/test_variants')\ntrain_t = pd.read_csv('../input/training_text',sep='\\|\\|',skiprows=1,engine='python',names=[\"ID\",\"Text\"])\ntest_t = pd.read_csv('../input/test_text',sep='\\|\\|',skiprows=1,engine='python',names=[\"ID\",\"Text\"])\n\ntrain = pd.merge(train_v, train_t, how='left', on='ID').fillna('')\ny_labels = train['Class'].values\n\ntest = pd.merge(test_v, test_t, how='left', on='ID').fillna('')\ntest_id = test['ID'].values"},{"cell_type":"markdown","metadata":{"_cell_guid":"502a3690-39d9-4a46-96f6-d8e9e578ac23","_uuid":"28af3b48a6b5d7bc5d69167645c74ccb5eb2e3c2"},"source":"# Let's explore Genes and Variations"},{"cell_type":"markdown","metadata":{"_cell_guid":"7960a527-72d3-49e7-aa16-bb03be84039f","_uuid":"f66809eee04cf05877e6fc691109f8e5bb744e71"},"source":"First of all, sorry for my english and my code, I'm a beginer with Python. ;)\n\nLet's go to create the function \"variationProc\". The aim is to get a group of features that apport more information than raw variations and genes.\n\nWithout any knowledge about genes I want to try to understand what is the information that they give us...\n\nTaking a view of variations, we can see that there are a lot of different forms in their presentation, so the information could have a lot of noise and probably we get unique variation values for the most of the dataset.\n\n"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"e1cac333-a0e3-4a59-9554-a1cd556c757b","collapsed":true,"_uuid":"a1905ad97f86f6e21598ca6a7b4b37659040aefe"},"source":"print(\"there are \",len(train[\"Variation\"]),\"rows for the training set\")\nprint(\"there are \",len(set(list(train[\"Variation\"]))), \" different values for variations\")\nprint(\"there are \",len(set(list(train[\"Gene\"]))), \" different values for genes\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"78ac5a11-ae11-41ac-8622-a4d68290e41b","_uuid":"8d4e1a9ada1c098299290ab9b549061472fe2755"},"source":"Using raw variations as a feature for training wont apport any information to the signal so if we use this, we will only get noise or overfitting if we overtrain the model."},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"a7cfe35e-687c-414c-9b70-11b320424d54","collapsed":true,"_uuid":"612397563b9202b586891517d9f892f4ee0ee925"},"source":"train[\"Variation\"][:50]"},{"cell_type":"markdown","metadata":{"_cell_guid":"849f0b97-7851-4c46-a555-99de13e1aa44","_uuid":"9ae8659230f6c3eed9c73abbaae783476d3d822f"},"source":"A lot of variations take the shape: \"letter\"\"number\"\"letter\". For example: V468G. \n"},{"cell_type":"markdown","metadata":{"_cell_guid":"b278b748-181c-442c-8959-71bb20e28bae","_uuid":"afe2fe6c80d17d11560ea58a3dc5d6d11a1d2b28"},"source":"Exploring the variables in deep we can see that there are others like \"truncating\", \"promoter\", \"amplification\", \"wildtype\", \"deletion\", \"insertion\", etc.\n\nMy approach is to create a dataset with the next columns:\n\n    1- Gene: the feature gene. Ex: runx1\n\n    2- Gene2: sometimes there are a second gene implicated in the operation (like in the operation fusion). For example in  the variation runx1\tevi1 fusion, evi1 woul be the second gene. \n\n    3- Operation: operation over feature gene (deletion, insertion, fusion, etc.)\n\n    4- Letter1: in V468G, letter1 would be V\n\n    5- Number1: in V468G, number1 would be 486\n\n    6- Letter2: sometimes in deletion/insertions there are 2 cases of letter-number. For example in the variaiton K745_A750del the letter2 would be A. \n\n    7- Number2: in K745_A750del, number2 would be 750\n\n    8- ObjLetter: letter-number-Objectiveletter. In the variation V468G the objletter would be G\n\nThe function variationProc try to get this dataset. Sorry for the code, I'm sure that it could be better with the use of regex."},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"78c16614-a57c-4f54-9d33-b659bf5e632d","collapsed":true,"_uuid":"cff8e6dae6d9011733ad84e2fbe6f832afd3a069"},"source":"def variationProc(variations, genes):\n    vari2=[]\n    for i in range(0, len(variations)):\n        esfusion=False\n        texto=variations[i].lower()\n        texto = texto.replace(\" \",\"\")\n        texto = texto.replace(\"_\",\"#\")        \n        texto = texto.replace(\"\\'\",\"\")\n        texto = texto.replace(\"-\",\"#\")\n        texto = texto.replace(\"Exon \",\"Exon\") \n        \n        if \"truncating\" in texto:\n            texto=\"trunc\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"\n        elif \"promotermut\" in texto:\n            texto=\"promotermut\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"\n        elif \"promoterhyper\" in texto:\n            texto=\"promoterhyper\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"\n        elif \"ampli\" in texto:\n            texto=\"ampli\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"\n        elif \"overex\" in texto:\n            texto=\"overex\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"\n        elif \"dnabinding\" in texto:\n            texto=\"dnabinding\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"\n        elif \"wildtype\" in texto:\n            texto=\"wildtype\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"\n        elif \"epigeneticsil\" in texto:\n            texto=\"epigeneticsil\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"\n        elif \"copynumberloss\" in texto:\n            texto=\"copynumberloss\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"\n        elif \"hypermethyl\" in texto:\n            texto=\"hypermethyl\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"\n        elif \"singlenucleotidepolymo\" in texto:\n            texto=\"singlenucleotidepolymo\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"  \n        elif \"exon\" in texto:\n            texto=texto+\"#null\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"\n        elif \"fs\" in texto:\n            texto=texto.replace(\"fs\",\"\")\n            if re.match(\"(\\D+)(\\d+)(\\D+)\", texto):\n                if texto[1:5].isnumeric():\n                    texto=\"fs\"+\"#\"+texto[0:1]+\"#\"+texto[1:5]+\"#null\"+\"#null\"+\"#null\"\n                elif texto[1:4].isnumeric():                       \n                    texto=\"fs\"+\"#\"+texto[0:1]+\"#\"+texto[1:4]+\"#null\"+\"#null\"+\"#null\"\n                else:                        \n                    texto=\"fs\"+\"#\"+texto[0:1]+\"#\"+texto[1:3]+\"#null\"+\"#null\"+\"#null\"\n            elif re.match(\"(\\D+)(\\d+)\", texto):\n                if \" \" not in texto:\n                    texto=\"fs\"+\"#\"+texto[0:1]+\"#\"+texto[1:]+\"#null\"+\"#null\"+\"#null\"\n            else:\n                texto=\"fs\"+\"#\"+texto\n        elif \"deletion/insertion\" in texto:\n            texto = texto.replace(\"deletion/insertion\",\"delins\")\n        elif \"delins\" in texto:\n            texto=texto.replace(\"delins\",\"\")\n            if \"#\" not in texto:\n                if re.match(\"(\\D+)(\\d+)(\\D+)\", texto):\n                    texto=\"delins\"+\"#\"+texto[0:1]+\"#\"+texto[1:4]+\"#\"+texto[4:4]+\"#null\"+\"#null\"+\"#null\"\n                elif re.match(\"(\\D+)(\\d+)\", texto):\n                    texto=\"delins\"+\"#\"+texto[0:1]+\"#\"+texto[1:]+\"#null\"+\"#null\"+\"#null\"\n                else:\n                    texto=\"delins\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"\n            else:\n                lista=texto.split(\"#\")\n                if re.match(\"(\\D+)(\\d+)\", lista[0]):\n                    texto=\"delins\"+\"#\"+lista[0][0:1]+\"#\"+lista[0][1:]+\"#\"\n                if re.match(\"(\\D+)(\\d+)\", lista[1]):\n                    texto=texto+lista[1][0:1]+\"#\"+lista[1][1:4]+\"#\"+lista[1][4:5]\n        elif \"fusion\" in texto:\n            esfusion=True\n            texto = texto.replace(\"fusion\",\"\")\n            lista=texto.split(\"#\")\n            if len(lista)==2:\n                if genes[i].lower() in lista[0].lower():\n                    texto = lista[1].lower()+\"#fusion\" +\"#null\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"\n                else:\n                    texto = lista[0].lower()+\"#fusion\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"\n            else:\n                texto = genes[i].lower()+\"#fusion\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"\n        elif \"deletion\" in texto:\n            texto = texto.replace(\"deletion\",\"del\")\n            texto = texto.replace(\"3del\",\"del\")\n        elif \"del\" in texto:\n            texto=texto.replace(\"del\",\"\")\n            if \"#\" not in texto:\n                if re.match(\"(\\D+)(\\d+)\", texto):\n                    texto=\"del\"+\"#\"+texto[0:1]+\"#\"+texto[1:]\n            else:\n                lista=texto.split(\"#\")\n                texto=\"del\"\n                if re.match(\"(\\D+)(\\d+)\", lista[0]):\n                    texto=texto+\"#\"+lista[0][0:1]+\"#\"+lista[0][1:]+\"#\"\n                if re.match(\"(\\D+)(\\d+)\", lista[1]):\n                    texto=texto+lista[1][0:1]+\"#\"+lista[1][1:]+\"#\"   \n                if re.match(\"(\\d+)(\\d+)\", lista[0]):\n                    texto=texto+\"#null#\"+lista[0] \n                if re.match(\"(\\d+)(\\d+)\", lista[1]):\n                    texto=texto+\"#null#\"+lista[1]\n        elif \"insertion\" in texto:\n            texto = texto.replace(\"insertion\",\"ins\")\n        elif \"ins\" in texto:\n            texto=texto.replace(\"ins\",\"\")\n            if \"#\" not in texto:\n                if re.match(\"(\\D+)(\\d+)(\\D+)\", texto):\n                    if \"#\" not in texto:\n                        texto=\"ins\"+\"#\"+texto[0:1]+\"#\"+texto[1:4]+\"#\"+texto[4:4]\n                elif re.match(\"(\\D+)(\\d+)\", texto):\n                    texto=\"ins\"+\"#\"+texto[0:1]+\"#\"+texto[1:]\n            else:\n                lista=texto.split(\"#\")\n                texto=\"ins\"\n                if re.match(\"(\\D+)(\\d+)\", lista[0]):\n                    texto=texto+\"#\"+lista[0][0:1]+\"#\"+lista[0][1:]+\"#\"\n                if re.match(\"(\\D+)(\\d+)\", lista[1]):\n                    texto=texto+lista[1][0:1]+\"#\"+lista[1][1:4]+\"#\"+lista[1][4:5]\n                if re.match(\"(\\d+)(\\d+)\", lista[0]):\n                    texto=texto+\"#null#\"+lista[0] \n                if re.match(\"(\\d+)(\\d+)\", lista[1]):\n                    texto=texto+\"#null#\"+lista[1]\n        elif \"dup\" in texto:\n            texto=texto.replace(\"dup\",\"\")\n            if \" \" not in texto:\n                if texto[1:].isnumeric():\n                    if re.match(\"(\\D+)(\\d+)(\\D+)\", texto):\n                        if \" \" not in texto:\n                            texto=\"dup\"+\"#\"+texto[0:1]+\"#\"+texto[1:4]+\"#\"+texto[4:4]\n                    elif re.match(\"(\\D+)(\\d+)\", texto):\n                        texto=\"dup\"+\"#\"+texto[0:1]+\"#\"+texto[1:]\n                else:\n                    texto=\"dup\"\n            else:\n                lista=texto.split(\"#\")\n                if re.match(\"(\\D+)(\\d+)\", lista[0]):\n                    texto=\"dup\"+\"#\"+lista[0][0:1]+\"#\"+lista[0][1:]+\"#\"\n                if re.match(\"(\\D+)(\\d+)\", lista[1]):\n                    texto=texto+lista[1][0:1]+\"#\"+lista[1][1:4]+\"#\"+lista[1][4:5]\n        elif \"splice\" in texto:\n            texto=texto.replace(\"splice\",\"\")\n            if \"#\" not in texto:\n                if texto[1:].isnumeric():\n                    if re.match(\"(\\d+)\", texto):\n                        texto=\"splice\"+\"#null#\"+texto+\"#null\"+\"#null\"+\"#null\"\n                    elif re.match(\"(\\D+)(\\d+)\", texto):\n                        texto=\"splice\"+\"#null#\"+texto[1:]+\"#null\"+\"#null\"+\"#null\"\n                else:\n                    texto=\"splice\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"\n            else:\n                lista=texto.split(\"#\")\n                if re.match(\"(\\D+)(\\d+)\", lista[0]):\n                    texto=\"splice\"+\"#null#\"+lista[0][1:]\n                else:\n                    texto=\"splice\"+\"#null#\"+lista[0]\n                    if re.match(\"(\\D+)(\\d+)\", lista[1]):\n                        texto=texto+\"#null#\"+lista[1][1:]+\"#null\"\n                    else:\n                        texto=texto+\"#null#\"+lista[1]\n        elif re.match(\"(\\D)(\\d+)(\\D+)\", texto):\n            if \" \" not in texto:\n                texto=\"sub\"+\"#\"+texto[0:1]+\"#\"+texto[1:len(texto)-1]+\"#null\"+\"#null\"+\"#\"+texto[len(texto)-1:]\n        elif re.match(\"(\\D)(\\d+)\", texto):\n            if \" \" not in texto:\n                texto=\"sub\"+\"#\"+texto[0:1]+\"#\"+texto[1:]+\"#null\"+\"#null\"+\"#null\"\n        else:\n            texto=\"others\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"+\"#null\"\n        \n        if esfusion:\n            vari2.append(genes[i].lower()+\"#\"+texto)\n        else:\n            vari2.append(genes[i].lower()+\"#null#\"+texto)\n            \n    mat=[]   \n    for linea in vari2:\n        linea.replace(\" \",\"\")\n        linea.replace(\"##\",\"#\")\n        lista=linea.split(\"#\")\n        lineaadd=[]\n        if len(lista)>4:\n            lista[4] = re.sub(\"\\D\", \"\", lista[4])\n            if lista[4]==\"\":\n                lista[4]=np.nan\n            else:\n                float(lista[4])\n        if len(lista)>6:\n            lista[6] = re.sub(\"\\D\", \"\", lista[6])\n            if lista[6]==\"\":\n                lista[6]=np.nan\n            else:\n                float(lista[6])\n        for te in range(0,len(lista)):\n            if lista[te] == \"null\" or lista[te] == \"\":\n                lista[te]=None\n                if te==4 or te==6:\n                    lista[te]=np.nan\n        if len(lista)<8:\n            for j in range(len(lista),9):\n                if j==4 or j==6:\n                    lista.append(np.nan)\n                else:\n                    lista.append(None)\n        \n        for j in range(0,8):\n            lineaadd.append(lista[j])\n        mat.append(lineaadd)\n        \n    print(\"Done...\")\n    return(mat)\n"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"23e0144b-bb98-4440-a80b-32e921e97cfe","collapsed":true,"_uuid":"0e7c647ef18e5ba9b735c7bff144ad4fb52e6660"},"source":"print(\"Processing gene and variation with VariationProc...\")\nvartra=variationProc(train[\"Variation\"], train[\"Gene\"])"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"41030d01-2417-4677-ab19-c9a50f1b3526","collapsed":true,"_uuid":"d6704ad6b419919b800a5356c9a867b6d2800783"},"source":"vardf = pd.DataFrame(vartra, columns=[\"gene1\",\"gene2\",\"operation\",\"letter1\",\"number1\",\"letter2\",\"number2\",\"objletter\"])\nvardf['Class'] = train[\"Class\"]"},{"cell_type":"markdown","metadata":{"_cell_guid":"ddda24aa-044a-47b1-9859-3ae203bc77f6","_uuid":"4ad86823117f290dc63aedd861828522ddf0f6d2"},"source":"We can see that there are a lot of None and NaN values in the gene2, letter2 and number2 columns"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"d7a99dba-46be-4e02-9057-d188b19d8dc5","collapsed":true,"_uuid":"8e593b354beabcd253e5dd0d83e6e0d28d520251"},"source":"vardf"},{"cell_type":"markdown","metadata":{"_cell_guid":"e7c55578-a811-4413-8420-0e92145a699d","_uuid":"93147c2029a9d17b91bd146880c5a55ba4553f80"},"source":"Let's explore the features with plotting..."},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"a4627bb1-fe82-45e9-a487-188ab1edf9dd","collapsed":true,"_uuid":"ab1cad2097a967d2412fe1d4cbb74ba1a99caab0"},"source":"plt.figure(figsize=(11,7))\nsns.countplot(x=\"Class\", data=train)\nplt.ylabel('Frequency', fontsize=13)\nplt.xlabel('Classes', fontsize=13)\nplt.title(\"Frequency of Classes\", fontsize=18)\nplt.show()\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"6e3e82d7-8ebd-4d0f-bc8b-86e0a2904480","_uuid":"15ad90f89ed9576d2a678cf8fd7e9447d4570aa3"},"source":"We have an unbalanced dataset..."},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"05d5ea21-197d-4d14-b051-ccec0842b198","collapsed":true,"_uuid":"04a5d40be93f9c0c1543ed41ff99d9fe5989fc23"},"source":"plt.figure(figsize=(11,57))\nsns.countplot(y=\"gene1\", data=vardf)\nplt.ylabel('Gene1', fontsize=13)\nplt.xlabel('Frequency', fontsize=13)\nplt.title(\"Frequency of Feature Gene1\", fontsize=18)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"a44ee6f1-62ae-4984-bc78-5e75858a679c","_uuid":"f437afa6e4f25dd6464fa13d2f66b7e2e9357152"},"source":"egfr, tp52, pten, brca1, brca2, braf and kit are the most common genes..."},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"fc3dfa5f-25a8-4919-817b-16ba29079d00","collapsed":true,"_uuid":"bbc60e35bbb947a6942c4d6478d65faa0ab7f597"},"source":"plt.figure(figsize=(10,50))\nsns.stripplot(x=\"Class\", y=\"gene1\", data=vardf, jitter=True);\nplt.show()"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"d57d9ac6-8af1-44f0-86fd-8b2b9526ebe6","collapsed":true,"_uuid":"bfe11f40ef67b2be071567645f3fe15256d5e7a6"},"source":"plt.figure(figsize=(10,30))\nsns.countplot(y=\"gene2\", data=vardf)\nplt.ylabel('Gene2', fontsize=13)\nplt.xlabel('Frequency', fontsize=13)\nplt.title(\"Frequency of Feature Gene2\", fontsize=18)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"1c3f5d19-95bb-4be1-8bd3-71432e417ef5","_uuid":"d6d6eeb33caf8c04780a1eea4c989032568e3fec"},"source":"There are a few rows that contains gene2. Should we ignore them?"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"da444ccc-e747-4a70-a09a-068174599bc2","collapsed":true,"_uuid":"21516d1ddcbde27d15a84260cff90901080f25f0"},"source":"plt.figure(figsize=(10,30))\nsns.stripplot(x=\"Class\", y=\"gene2\", data=vardf, jitter=True);\nplt.show()"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"ebf25233-3bed-4fc5-9c65-04083066930a","collapsed":true,"_uuid":"11459cd5d817aa9fe54b519497e9100e91be82bb"},"source":"plt.figure(figsize=(11,7))\nsns.countplot(y=\"operation\", data=vardf)\nplt.ylabel('Operation', fontsize=13)\nplt.xlabel('Frequency', fontsize=13)\nplt.title(\"Frequency of Feature Operation\", fontsize=18)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"cd1ec123-a25d-4ded-8b38-4f2286aafce1","_uuid":"3a91f65d2b041a5a7aac2b1496dc8090ae7f41b9"},"source":"The most frequent operation is sub: \"letter\"\"number\"\"letter\" form variations."},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"e4d0edfc-5080-4e89-b0f2-371471766bd0","collapsed":true,"_uuid":"a023735e9b67530186ba80ae0c9d58a5766f6f01"},"source":"plt.figure(figsize=(10,10))\nsns.stripplot(x=\"Class\", y=\"operation\", data=vardf, jitter=True);\nplt.show()"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"26609b1e-a247-460c-80bb-28b88978b808","collapsed":true,"_uuid":"11ccb05da4341609c957b68d1badcb8ee25a580d"},"source":"plt.figure(figsize=(11,7))\nsns.countplot(x=\"letter1\", data=vardf)\nplt.ylabel('Frequency', fontsize=13)\nplt.xlabel('Letter1', fontsize=13)\nplt.title(\"Frequency of Feature Letter1\", fontsize=18)\nplt.show()"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"cb55b5b4-e3e8-4707-90e0-77272600941c","collapsed":true,"_uuid":"d086dd50205dcd2513ac8bfefd1e96510d7c407f"},"source":"plt.figure(figsize=(10,10))\nsns.stripplot(x=\"Class\", y=\"letter1\", data=vardf, jitter=True);\nplt.show()"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"569fc236-04b9-4349-ad36-0eed96ced3a9","collapsed":true,"_uuid":"7da478a4c5d55df93d090bc2bbd2bde91822d851"},"source":"plt.figure(figsize=(10,10))\nsns.stripplot(x=\"Class\", y=\"number1\", data=vardf, jitter=True);\nplt.show()"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"6f7dd287-4a46-4007-92f0-ee86836b778d","collapsed":true,"_uuid":"cfc460af40ebda73fbfd991c7afb67ade2246440"},"source":"plt.figure(figsize=(11,7))\nsns.countplot(x=\"letter2\", data=vardf)\nplt.ylabel('Frequency', fontsize=13)\nplt.xlabel('Letter2', fontsize=13)\nplt.title(\"Frequency of Feature Letter2\", fontsize=18)\nplt.show()"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"8587aa1f-a84b-489d-85e1-74db23daeb26","collapsed":true,"_uuid":"a04c80af6cfc999761a971504379d321be836ee6"},"source":"plt.figure(figsize=(10,10))\nsns.stripplot(x=\"Class\", y=\"letter2\", data=vardf, jitter=True);\nplt.show()"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"6ca4912b-6e25-45ea-8cce-92f19dad3dd4","collapsed":true,"_uuid":"27f35630d979e7d3955273149f1aa624cc17ed94"},"source":"plt.figure(figsize=(10,10))\nsns.stripplot(x=\"Class\", y=\"number2\", data=vardf, jitter=True);\nplt.show()"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"c51d799d-d5fb-493f-bd43-98eb64a39603","collapsed":true,"_uuid":"f749d11dbf553c8763512abe86c1a0c89aa5ae51"},"source":"plt.figure(figsize=(11,7))\nsns.countplot(x=\"objletter\", data=vardf)\nplt.ylabel('Frequency', fontsize=13)\nplt.xlabel('ObjLetter', fontsize=13)\nplt.title(\"Frequency of Feature ObjLetter\", fontsize=18)\nplt.show()"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"f958cf93-f514-41b4-860c-affdd5d6f3b8","collapsed":true,"_uuid":"613d2eee822c70b3405bfa9af1844cf0cc80bad0"},"source":"plt.figure(figsize=(10,10))\nsns.stripplot(x=\"Class\", y=\"objletter\", data=vardf, jitter=True);\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"1efba4fc-6f9f-4a3b-bc49-b4fa1a932b29","collapsed":true,"_uuid":"c6e670ef03e5ea652c7c49acc908e7cf1a24906c"},"source":"# Some Text Mining Features"},{"cell_type":"markdown","metadata":{"_cell_guid":"d98f7df6-5826-47b6-9518-7cf6dc12d8b5","_uuid":"a8ce40b3c7c0a5c6fb5c2cf83980432f46f06bb9"},"source":"Let's start with searching the words that are unique in each class.\n\nFirst of all, I'm going to cluster the different texts in their respective classes."},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"c4cb64f6-1bd3-4081-9ef5-4d84e8018c6b","collapsed":true,"_uuid":"fd300ce327160a7e89b91bc76ef9e38d274bd22f"},"source":"c1, c2, c3, c4, c5, c6, c7, c8, c9 = \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n\nfor i in train[train[\"Class\"]==1][\"ID\"]:\n    c1+=train[\"Text\"][i]+\" \"\n\n\nfor i in train[train[\"Class\"]==2][\"ID\"]:\n    c2+=train[\"Text\"][i]+\" \"\n\nfor i in train[train[\"Class\"]==3][\"ID\"]:\n    c3+=train[\"Text\"][i]+\" \"\n    \nfor i in train[train[\"Class\"]==4][\"ID\"]:\n    c4+=train[\"Text\"][i]+\" \"\n    \nfor i in train[train[\"Class\"]==5][\"ID\"]:\n    c5+=train[\"Text\"][i]+\" \"\n    \n    \nfor i in train[train[\"Class\"]==6][\"ID\"]:\n    c6+=train[\"Text\"][i]+\" \"\n    \nfor i in train[train[\"Class\"]==7][\"ID\"]:\n    c7+=train[\"Text\"][i]+\" \"\n    \nfor i in train[train[\"Class\"]==8][\"ID\"]:\n    c8+=train[\"Text\"][i]+\" \"\n    \n    \nfor i in train[train[\"Class\"]==9][\"ID\"]:\n    c9+=train_t[\"Text\"][i]+\" \"\n \n"},{"cell_type":"markdown","metadata":{"_cell_guid":"6c28cae5-4565-4868-b197-ed72ea9b31dc","_uuid":"c554f1151986d7ff5ceb2ba4a47e70bbade1494a"},"source":"Tokenize function split a text in lemmatized tokens."},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"466a8d68-d531-49eb-b413-861737e57f84","collapsed":true,"_uuid":"d4ac235846b95c2a41a18799b3d5dc1988223186"},"source":"def tokenize(_str):\n    stops = set(stopwords.words(\"english\"))\n    tokens = collections.defaultdict(lambda: 0.)\n    wnl = nltk.WordNetLemmatizer()\n    for m in re.finditer(r\"(\\w+)\", _str, re.UNICODE):\n        m = m.group(1).lower()\n        if len(m) < 2: continue\n        if m in stops: continue\n        if m.isnumeric():continue\n        m = wnl.lemmatize(m)\n        tokens[m] += 1 \n    return tokens"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"48203778-eedb-4b69-a15f-bff1b85b8101","collapsed":true,"_uuid":"38cb352d1612689ba8dbae57e0a7f264ef86e7cb"},"source":"texts_for_training=[]\ntexts_for_test=[]\nnum_texts_train=len(train)\nnum_texts_test=len(test)\n\nprint(\"Tokenizing training texts\")\nfor i in range(0,num_texts_train):\n    if((i+1)%1000==0):\n        print(\"Text %d of %d\\n\"%((i+1), num_texts_train))\n    texts_for_training.append(tokenize(train[\"Text\"][i]))\n    \n\nprint(\"Tokenizing test texts\")\nfor i in range(0,num_texts_test):\n    if((i+1)%1000==0):\n        print(\"Text %d of %d\\n\"%((i+1), num_texts_test))\n    texts_for_test.append(tokenize(test[\"Text\"][i]))"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"9b3b1e1b-67b0-41fb-9af9-12718d547f17","collapsed":true,"_uuid":"b8c1ea3faa42a88c96dcd83497c565bdd8f8b388"},"source":""},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"35ea5140-05c1-43be-88ed-9c4d9c7ca1e1","collapsed":true,"_uuid":"97e04f1a57f805893633284d8f9d4398dd4529e3"},"source":"print(\"Tokenizing cluster 1\")\ncluster1=tokenize(c1)\n\nprint(\"Tokenizing cluster 2\")\ncluster2=tokenize(c2)\n\nprint(\"Tokenizing cluster 3\")\ncluster3=tokenize(c3)\n\nprint(\"Tokenizing cluster 4\")\ncluster4=tokenize(c4)\n\nprint(\"Tokenizing cluster 5\")\ncluster5=tokenize(c5)\n\nprint(\"Tokenizing cluster 6\")\ncluster6=tokenize(c6)\n\nprint(\"Tokenizing cluster 7\")\ncluster7=tokenize(c7)\n\nprint(\"Tokenizing cluster 8\")\ncluster8=tokenize(c8)\n\nprint(\"Tokenizing cluster 9\")\ncluster9=tokenize(c9)"},{"cell_type":"markdown","metadata":{"_cell_guid":"df383ab6-5eb8-4d7b-bd9b-709600552bce","_uuid":"09f6f444be6ef77afc72dc182a3a4352bee0b174"},"source":"uniqsPerClass is a function that returns the bag of words that appears in exactly n classes (if exact parameter is true) or as much in n classes (if exact parameter is false). The number of objective classes is determined by the objective parammeter, and clase is the parameter that contains the cluster to compare with. For example: if we want to find the terms that appears only in the class 1 we use uniqPerClass(cluster1, 1, True); if we want to find the terms of the class5 that appears as much in 3 classes we use uniqPerClass(cluster5,3,False); etc. "},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"c83c7f75-73bb-422e-b709-a97adc2a8051","collapsed":true,"_uuid":"a5dabf560ca292eafcfa71dfce4020666e11b600"},"source":"def uniqsPerClass(clase, objective, exact):\n\n    uniqs = collections.defaultdict(lambda: 0.)\n\n    for t, v in clase.items():\n        apears=0\n        if t in cluster1:\n            apears+=1\n        if t in cluster2:\n            apears+=1\n        if t in cluster3:\n            apears+=1\n        if t in cluster4:\n            apears+=1\n        if t in cluster5:\n            apears+=1\n        if t in cluster6:\n            apears+=1\n        if t in cluster7:\n            apears+=1  \n        if t in cluster8:\n            apears+=1\n        if t in cluster9:\n            apears+=1\n    \n        if exact:            \n            if apears==objective:\n                uniqs[t]=v\n        else:\n            if apears<(objective+1):\n                uniqs[t]=v\n    return uniqs\n"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"1f3044a9-84f7-4e60-a18b-b606db318e6a","collapsed":true,"_uuid":"5cf22cb73153398e29fb0b3b3b9e13cbf3260e2d"},"source":"uniC1=uniqsPerClass(cluster1,1,False)\nuniC2=uniqsPerClass(cluster2,1,False)\nuniC3=uniqsPerClass(cluster3,1,False)\nuniC4=uniqsPerClass(cluster4,1,False)\nuniC5=uniqsPerClass(cluster5,1,False)\nuniC6=uniqsPerClass(cluster6,1,False)\nuniC7=uniqsPerClass(cluster7,1,False)\nuniC8=uniqsPerClass(cluster8,1,False)\nuniC9=uniqsPerClass(cluster9,1,False)\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"8ff50f83-389f-49d5-8d3c-df358d4a7711","_uuid":"82d3db73a1556c5d311f1076295c2e7cca2e6315"},"source":"termsComps function takes a tokenized text and returns a list with the proportions of terms shared with the subgroups generated with the function uniqPerClas. It returns the degree of membership of a document to each class taking into account the terms selected by the cited function."},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"608841d2-5e4a-41bf-9363-bc86f51a5b57","collapsed":true,"_uuid":"3c37f18e2db30a6b25b9ace1e1b83ee8e02b35bb"},"source":"def termsComps(file):\n    c1,c2,c3,c4,c5,c6,c7,c8,c9=0.,0.,0.,0.,0.,0.,0.,0.,0.\n    for t, v in file.items():\n        if t in uniC1:\n            c1+=v\n        if t in uniC2:\n            c2+=v\n        if t in uniC3:\n            c3+=v\n        if t in uniC4:\n            c4+=v\n        if t in uniC5:\n            c5+=v\n        if t in uniC6:\n            c6+=v\n        if t in uniC7:\n            c7+=v\n        if t in uniC8:\n            c8+=v\n        if t in uniC9:\n            c9+=v\n        suma=c1+c2+c3+c4+c5+c6+c7+c8+c9\n        if suma==0:\n            suma=1\n            \n    return [c1/suma,c2/suma,c3/suma,c4/suma,c5/suma,c6/suma,c7/suma,c8/suma,c9/suma]"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"98405c3d-6450-497d-99c9-459493c3ad57","collapsed":true,"_uuid":"8f1db20caa98b5eefd41ff73ac08868b1fb13d2f"},"source":"uniqsTextMatr=[]\nfor file in texts_for_training:\n    uniqsTextMatr.append(termsComps(file))"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"b3d01327-950b-40dd-91e1-b8f53c7d5621","collapsed":true,"_uuid":"b8a9602c5139587c549b35885b567c99a528a197"},"source":"uniqText = pd.DataFrame(uniqsTextMatr, columns=['class'+str(c+1) for c in range(9)])\nuniqText['RealClass'] = train[\"Class\"]"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"89f72449-2afb-4147-8747-32d18f1e9f00","collapsed":true,"_uuid":"32dc88ccbcf6fa558aef2cf51af70cd07353aa58"},"source":"uniqText"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"9d59a01c-4bdb-4261-9855-d2d1d583809e","collapsed":true,"_uuid":"31cd312aa505e0380a8fcbd5eea8e6ff547c14ce"},"source":"def precisionT(subclas, realclas, takeNullConsider):\n    correct,total=0.,0.\n    for i in range(0, len(realclas)):\n        if not takeNullConsider:\n            if not vacuo(uniqTextList[i][0:9]):\n                total+=1\n                if uniqTextList[i][0:9].index(max(uniqTextList[i][0:9]))==realclas[i]-1:\n                    correct+=1\n        else:\n            total+=1\n            if uniqTextList[i][0:9].index(max(uniqTextList[i][0:9]))==realclas[i]-1:\n                correct+=1\n    return correct/total\n\ndef precisionCoverNull(subclas, realclas,classtocover):\n    correct,total=0.,0.\n    for i in range(0, len(realclas)):\n        if not vacuo(uniqTextList[i][0:9]):\n            total+=1\n            if uniqTextList[i][0:9].index(max(uniqTextList[i][0:9]))==realclas[i]-1:\n                correct+=1\n        else:\n            total+=1\n            if classtocover==realclas[i]:\n                correct+=1\n    return correct/total\n\n\ndef vacuo(row):\n    if row[0]==0.0 and row[1]==0.0 and row[2]==0.0 and row[3]==0.0 and row[4]==0.0 and row[5]==0.0 and row[6]==0.0 and row[7]==0.0 and row[8]==0.0:\n        return True\n    else:\n        return False\n    \n"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"6e41b8ef-a610-47f1-b990-7cae9ea4c424","collapsed":true,"_uuid":"de0fbb6adf9edd0b9a47ef3d47c3f37861b90080"},"source":"noinfo=0\nfor i in range(0,len(uniqText)):\n    row=[]\n    row.append(uniqText[\"class1\"][i])\n    row.append(uniqText[\"class2\"][i])\n    row.append(uniqText[\"class3\"][i])\n    row.append(uniqText[\"class4\"][i])\n    row.append(uniqText[\"class5\"][i])\n    row.append(uniqText[\"class6\"][i])\n    row.append(uniqText[\"class7\"][i])\n    row.append(uniqText[\"class8\"][i])\n    row.append(uniqText[\"class9\"][i])\n    if vacuo(row):\n        noinfo+=1\n    \n        \nprint(\"There are \",len(uniqText)-noinfo, \" texts of \",len(uniqText),\" in training set that can be classified in their correct class only with the \\\"unique words per class\\\" information\")\n\nuniqTextList=uniqText.values.tolist()  \n\nforcompare=[]\nfor i in range(0,len(uniqTextList)):\n    forcompare.append(uniqTextList[i][0:9])\n    "},{"cell_type":"markdown","metadata":{"_cell_guid":"33133c59-faa9-4ce6-83f7-8648578034a4","_uuid":"c16e5bafc9c508890af370c2ffbe15c01f77ec3b"},"source":"The precision without taking in consideration the null rows [0,0,0,...,0,0] is of 100%"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"7954f2e3-5ef5-4651-bed8-87791556f670","collapsed":true,"_uuid":"3b6df42c9fbfa1dcee628546e475ab8d4e24677e"},"source":"print(precisionT(forcompare,uniqText[\"RealClass\"],False))"},{"cell_type":"markdown","metadata":{"_cell_guid":"6fdb8bc4-c63e-4fca-b6b2-e917368fbbac","_uuid":"b2c956d3861491e977d2d88eb5fe57be723f77b1"},"source":"The precision without taking in consideration the null rows [0,0,0,...,0,0] is of aprox 60%"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"11b5e465-0599-40de-9b5d-c529a4dd7e42","collapsed":true,"_uuid":"0a5091d12d6c33e277f82a0c5630bd8f5d904e7c"},"source":"print(precisionT(forcompare,uniqText[\"RealClass\"],True))"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"6385e896-2a48-4a97-943a-a834bcdf76a7","collapsed":true,"_uuid":"a0501710df28e934867a1df9d76c38d98bad72ee"},"source":"uniqText.describe()"},{"cell_type":"markdown","metadata":{"_cell_guid":"8ddc7c1e-7f46-4947-a3a6-07a29857aa81","_uuid":"41bfc3500f5d85c09d7702095e2c573819d40a39"},"source":"Unique words for class 1"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"a2bd4fb1-b1c4-425c-8876-99182ac55c30","collapsed":true,"_uuid":"e76fdbf774dc4891420562e8e82f96f660e38191"},"source":"def dictotext(dic):\n    text=\"\"\n    for t,v in dic.items():\n        for i in range(0,int(v)):\n            text=text+t+\" \"\n    return text            "},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"6a118aff-11af-451f-a1c3-c87f284ab568","collapsed":true,"_uuid":"91d42923e3f10c36d523d719fb4ce6a51d915fc8"},"source":"print(\"there are \",len(uniC1),\"unique words in class1\")\ntext = dictotext(uniC1)\nwordcloud = WordCloud(width=800, height=400, max_font_size=80,collocations = False,).generate(text)\nplt.figure(figsize=(20,5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"dc04680f-f261-4271-87b7-af0eb52a0e1b","collapsed":true,"_uuid":"ff1cd393556097166f034613c96ac656ca75614f"},"source":"print(\"there are \",len(uniC2),\"unique words in class2\")\ntext = dictotext(uniC2)\nwordcloud = WordCloud(width=800, height=400, max_font_size=80,collocations = False).generate(text)\nplt.figure(figsize=(20,5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"c809a199-dfe4-443e-a743-a3c78c5e0529","collapsed":true,"_uuid":"bb388dc47c233da3494968a1b500f59855a969e6"},"source":"\nprint(\"there are \",len(uniC3),\"unique words in class3\")\ntext = dictotext(uniC3)\nwordcloud = WordCloud(width=800, height=400, max_font_size=80,collocations = False).generate(text)\nplt.figure(figsize=(20,5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"611412b5-275d-4f7a-9e1f-a1d473471f1c","collapsed":true,"_uuid":"7da85208bdc88edf509f29170e677ef574951170"},"source":"print(\"there are \",len(uniC4),\"unique words in class4\")\ntext = dictotext(uniC4)\nwordcloud = WordCloud(width=800, height=400, max_font_size=80,collocations = False).generate(text)\nplt.figure(figsize=(20,5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"240c65ab-436a-43e6-8477-2b22d5040dbb","collapsed":true,"_uuid":"c3b1090fb57de4fbe5c9cf719926fce3bb77409b"},"source":"print(\"there are \",len(uniC5),\"unique words in class5\")\ntext = dictotext(uniC5)\nwordcloud = WordCloud(width=800, height=400, max_font_size=80,collocations = False).generate(text)\nplt.figure(figsize=(20,5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"55e542a3-7f9f-4554-b110-489a1d045d86","collapsed":true,"_uuid":"6a568fc5ec5a066446698dc63964e07145cf501d"},"source":"print(\"there are \",len(uniC6),\"unique words in class6\")\ntext = dictotext(uniC6)\nwordcloud = WordCloud(width=800, height=400, max_font_size=80,collocations = False).generate(text)\nplt.figure(figsize=(20,5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"c65a5a10-652f-4c13-b91e-53084fba77ed","collapsed":true,"_uuid":"aed6855c4a799870b7316683779a00554bd40165"},"source":"print(\"there are \",len(uniC7),\"unique words in class7\")\ntext = dictotext(uniC7)\nwordcloud = WordCloud(width=800, height=400, max_font_size=80,collocations = False).generate(text)\nplt.figure(figsize=(20,5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"19c65964-c67d-439a-826b-491fe958cd17","collapsed":true,"_uuid":"47bf8475ea7da47688ba53befda9aaa447e1e89f"},"source":"print(\"there are \",len(uniC8),\"unique words in class8\")\ntext = dictotext(uniC8)\nwordcloud = WordCloud(width=800, height=400, max_font_size=80,collocations = False).generate(text)\nplt.figure(figsize=(20,5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"2427bcf1-48ee-4dc0-a8b0-56a9675c453a","collapsed":true,"_uuid":"7374822235e485dbb2328930d6986f376523d634"},"source":"print(\"there are \",len(uniC9),\"unique words in class9\")\ntext = dictotext(uniC9)\nwordcloud = WordCloud(width=800, height=400, max_font_size=80,collocations = False).generate(text)\nplt.figure(figsize=(20,5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"f932a96f-5b0f-40cc-8851-9aa8c6d3056b","collapsed":true,"_uuid":"f8e3cb64900f7455b52f0981cdfb07dec395b9cf"},"source":"Unique words could help us to improve the signal in some cases"},{"cell_type":"markdown","metadata":{"_cell_guid":"51bc5dcb-07fd-4bbc-b95e-1359648767f3","_uuid":"6a2818272b43525bc571c95e52a6357ba45885a6"},"source":"Now we are going to show the words shared by all the classes by frequency maybe they could add noise to the signal"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"7c74426f-5041-46b9-ba2e-ab1fe66d5e89","collapsed":true,"_uuid":"aff030dd1efd301cd80b74fe5993398a4f747ca3"},"source":"norel=uniqsPerClass(cluster8,9,True)"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"cbe1ac71-aec1-4307-b607-68fb362ab21a","collapsed":true,"_uuid":"41eb7a343ebf88c415c8afb469c498952f43fd77"},"source":"print(\"there are \",len(norel),\"words that appears in all classes\")\n\ntext = dictotext(norel)\nwordcloud = WordCloud(width=800, height=400, max_font_size=80,collocations = False).generate(text)\nplt.figure(figsize=(20,5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"4e81e006-1197-484e-8682-17a38fc85095","_uuid":"cafe05df11c956c87c623c48f832c254b4ad941b"},"source":"What about if we include all words that appears in 2 classes"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"2faaf041-ce85-445a-b3f4-b457d8f2bf4d","collapsed":true,"_uuid":"7dbc9bebd8eb4581ea2a98f5e6f70055c6899af7"},"source":"uniC1=uniqsPerClass(cluster1,2,False)\nuniC2=uniqsPerClass(cluster2,2,False)\nuniC3=uniqsPerClass(cluster3,2,False)\nuniC4=uniqsPerClass(cluster4,2,False)\nuniC5=uniqsPerClass(cluster5,2,False)\nuniC6=uniqsPerClass(cluster6,2,False)\nuniC7=uniqsPerClass(cluster7,2,False)\nuniC8=uniqsPerClass(cluster8,2,False)\nuniC9=uniqsPerClass(cluster9,2,False)\n\nuniqsTextMatr=[]\nfor file in texts_for_training:\n    uniqsTextMatr.append(termsComps(file))\n    \nuniqText = pd.DataFrame(uniqsTextMatr, columns=['class'+str(c+1) for c in range(9)])\nuniqText['RealClass'] = train[\"Class\"]"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"c3dfa44f-e475-497f-ab15-77c41b22ab5e","collapsed":true,"_uuid":"10302b5c924b87b9abf1f79adba822d0b999b5e8"},"source":"uniqText\n"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"3850188c-4ad8-4489-a612-a10c9f003824","collapsed":true,"_uuid":"70fe79809d04c35b9c13b7e66d9b9e9a72dbb57d"},"source":"noinfo=0\nfor i in range(0,len(uniqText)):\n    row=[]\n    row.append(uniqText[\"class1\"][i])\n    row.append(uniqText[\"class2\"][i])\n    row.append(uniqText[\"class3\"][i])\n    row.append(uniqText[\"class4\"][i])\n    row.append(uniqText[\"class5\"][i])\n    row.append(uniqText[\"class6\"][i])\n    row.append(uniqText[\"class7\"][i])\n    row.append(uniqText[\"class8\"][i])\n    row.append(uniqText[\"class9\"][i])\n    if vacuo(row):\n        noinfo+=1\n    \n        \nprint(\"There are \",len(uniqText)-noinfo, \" texts of \",len(uniqText),\" in training set that can be classified in their correct class only with the \\\"uniqsPerClass\\\" function information\")\n\nuniqTextList=uniqText.values.tolist()  \n\nforcompare=[]\nfor i in range(0,len(uniqTextList)):\n    forcompare.append(uniqTextList[i][0:9])\n    "},{"cell_type":"markdown","metadata":{"_cell_guid":"326acb9e-cce3-4cd8-af0b-23f7821909d5","collapsed":true,"_uuid":"c69dd1bdf895fae4d962f05f2fb12e5eec7d978a"},"source":"The precision without taking in consideration the null rows [0,0,0,...,0,0] is of aprox 82%"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"032542a5-fb92-434b-b356-e724e1607ed3","collapsed":true,"_uuid":"7d71e9b03a052ce466abaf739c494176ef1dbe40"},"source":"print(precisionT(forcompare,uniqText[\"RealClass\"],False))"},{"cell_type":"markdown","metadata":{"_cell_guid":"cd8f0c7f-46c1-4edf-874a-7d9edb41c9c2","collapsed":true,"_uuid":"0d4cc96620d1764030afd20dd63a922459ae1a5e"},"source":"The precision taking in consideration the null rows [0,0,0,...,0,0] is of 67%"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"4562696a-cd3c-48bb-921a-c4cb0f39f8c7","collapsed":true,"_uuid":"19eb0f12e01d70be0941b3331ec889f77ba0b15b"},"source":"print(precisionT(forcompare,uniqText[\"RealClass\"],True))"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"7347d5cf-0d17-4168-8ccb-e18084c152fb","collapsed":true,"_uuid":"357065645754d5814a762e288ea114629df44338"},"source":"uniC1=uniqsPerClass(cluster1,3,False)\nuniC2=uniqsPerClass(cluster2,3,False)\nuniC3=uniqsPerClass(cluster3,3,False)\nuniC4=uniqsPerClass(cluster4,3,False)\nuniC5=uniqsPerClass(cluster5,3,False)\nuniC6=uniqsPerClass(cluster6,3,False)\nuniC7=uniqsPerClass(cluster7,3,False)\nuniC8=uniqsPerClass(cluster8,3,False)\nuniC9=uniqsPerClass(cluster9,3,False)\n\nuniqsTextMatr=[]\nfor file in texts_for_training:\n    uniqsTextMatr.append(termsComps(file))\n    \nuniqText = pd.DataFrame(uniqsTextMatr, columns=['class'+str(c+1) for c in range(9)])\nuniqText['RealClass'] = train[\"Class\"]\nuniqText"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"fe2c7ae0-5201-487c-91e3-555fa07896e3","collapsed":true,"_uuid":"c9a4cbe46ae01801a5a50ad1062706ff56531bdf"},"source":"noinfo=0\nfor i in range(0,len(uniqText)):\n    row=[]\n    row.append(uniqText[\"class1\"][i])\n    row.append(uniqText[\"class2\"][i])\n    row.append(uniqText[\"class3\"][i])\n    row.append(uniqText[\"class4\"][i])\n    row.append(uniqText[\"class5\"][i])\n    row.append(uniqText[\"class6\"][i])\n    row.append(uniqText[\"class7\"][i])\n    row.append(uniqText[\"class8\"][i])\n    row.append(uniqText[\"class9\"][i])\n    if vacuo(row):\n        noinfo+=1\n    \n        \nprint(\"There are \",len(uniqText)-noinfo, \" texts of \",len(uniqText),\" in training set that can be classified in their correct class only with the \\\"uniqsPerClass\\\" function information\")\n\nuniqTextList=uniqText.values.tolist()  \n\nforcompare=[]\nfor i in range(0,len(uniqTextList)):\n    forcompare.append(uniqTextList[i][0:9])"},{"cell_type":"markdown","metadata":{"_cell_guid":"d19133f3-2b1d-4b81-b682-88217c56c3ca","_uuid":"91ea6fed6ee62aa2381cf8f1474a79ae17edd034"},"source":"The precision without taking in consideration the null rows [0,0,0,...,0,0] is of aprox 75%"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"2afa76fe-68c1-46cf-848d-ceb092abf008","collapsed":true,"_uuid":"ead29f837fe6e2f397bceb57942c273624743a34"},"source":"print(precisionT(forcompare,uniqText[\"RealClass\"],False))"},{"cell_type":"markdown","metadata":{"_cell_guid":"0e9aafd4-5dd8-459d-8212-9a369519c97b","_uuid":"a6e08fed88c4a3b0d01afb8f30a00db9c1fa8114"},"source":"The precision taking in consideration the null rows [0,0,0,...,0,0] is of 68%"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"dd1449b7-73f6-4bed-b432-33ff14813447","collapsed":true,"_uuid":"ce6d42ab7da99efa3b236353956db65ff0268841"},"source":"print(precisionT(forcompare,uniqText[\"RealClass\"],True))"},{"cell_type":"markdown","metadata":{"_cell_guid":"b6f3a0c5-d55a-4c0d-b295-96744fdccd26","_uuid":"1044d3f8ebf92a07e709fc08f292f492fcea275f"},"source":"So taking in acount only terms that appears in at maximum 3 classes, the signal could be improved"},{"cell_type":"markdown","metadata":{"_cell_guid":"deeb73c8-caca-463c-811e-b7973caf6273","_uuid":"6c658a0b5cfaa61794e237b7239471b60b7ea136"},"source":"# To be continued..."}],"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"mimetype":"text/x-python","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.1","file_extension":".py","pygments_lexer":"ipython3","nbconvert_exporter":"python"}}}