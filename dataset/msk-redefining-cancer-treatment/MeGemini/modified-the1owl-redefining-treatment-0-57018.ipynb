{"metadata":{"language_info":{"version":"3.6.1","nbconvert_exporter":"python","name":"python","file_extension":".py","mimetype":"text/x-python","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"}},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"a3076f31-7052-4e2c-ac9e-f6fd93d18a86","_uuid":"0d77c2144b6b5523aeb22d0ea999b216ebbe56fa"},"source":"# Donated to Cancer Treatment Too","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"217a645a-29bc-4f2d-8795-24fb4f55d94d","_uuid":"8abb336a1fb5b8c4e23b41efdaa9207c95b5974e"},"source":"* Note: As this is my first public Kernel and I'm still in learning NLP, please feel free to comment if you have any questions.\n* This Kernel is modified from [the1owl: Redefining Treatment](https://www.kaggle.com/the1owl/redefining-treatment-0-57456), really thanks!\n* I will highlight the main differences from the original.\n* Finnally, donated to cancer treatment too.","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"56f93ab8-fa0f-4bf4-9b30-625c2eda8d15","_uuid":"8c73ab9999e9d01e83d3a986e7bd1a13f2b800c2","trusted":false},"source":"from sklearn import preprocessing, pipeline, feature_extraction, decomposition, model_selection, metrics, cross_validation, svm\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.preprocessing import normalize, Imputer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\n\nimport sklearn\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\n\nimport datetime","outputs":[],"execution_count":1},{"cell_type":"code","metadata":{"_cell_guid":"c2340896-53c5-467b-8087-1c1136cf87a8","_uuid":"6c717a3ad79b2144092286757adeff5dd6f6945e","scrolled":false,"trusted":false},"source":"train = pd.read_csv('../input/training_variants')\ntest = pd.read_csv('../input/test_variants')\ntrainx = pd.read_csv('../input/training_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\ntestx = pd.read_csv('../input/test_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n\ntrain = pd.merge(train, trainx, how='left', on='ID').fillna('')\ny = train['Class'].values\ntrain = train.drop(['Class'], axis=1)\n\ntest = pd.merge(test, testx, how='left', on='ID').fillna('')\npid = test['ID'].values","outputs":[],"execution_count":2},{"cell_type":"code","metadata":{"_cell_guid":"a57c12f5-73c2-415a-9cd0-3adaed0ddd2c","_uuid":"22e5d25aa11c98d6f913d975cd51d436f8b64351","scrolled":true,"trusted":false},"source":"train.head()","outputs":[],"execution_count":3},{"cell_type":"code","metadata":{"_cell_guid":"fd7f7d96-f093-4321-8b60-e734c9d1794c","_uuid":"145564b8cf0753323450a93fd5202c458c49d1b6","trusted":false},"source":"y","outputs":[],"execution_count":4},{"cell_type":"code","metadata":{"_cell_guid":"e24024d6-c577-4099-8c4b-ed728bb3da82","_uuid":"42a876f902870ca8f1de3d0863dbc33921fac24b","trusted":false},"source":"test.head()","outputs":[],"execution_count":5},{"cell_type":"code","metadata":{"_cell_guid":"391d7e9a-96dd-454b-924f-0876bf6cefee","_uuid":"17c2a7ed868ae06ac1e15f8c90e19e4b17d321ad","trusted":false},"source":"pid","outputs":[],"execution_count":6},{"cell_type":"markdown","metadata":{"_cell_guid":"1edd1f68-0353-4f92-a739-db42d2adf265","_uuid":"4a26635cd88839c392aa1019e60b42b369ab3bd2"},"source":"### 1. Not use the codes below.\n***\nNot used in this Kernel.\n\n```python\n# commented for Kaggle Limits\nfor i in range(56):\n    df_all['Gene_'+str(i)] = df_all['Gene'].map(lambda x: str(x[i]) if len(x)>i else '')\n    df_all['Variation'+str(i)] = df_all['Variation'].map(lambda x: str(x[i]) if len(x)>i else '')\n```","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"90833673-de06-422c-8df2-13ffe5911a5c","_uuid":"86a1fbd09654d814e68622b05a327843290c7304","collapsed":true,"scrolled":true,"trusted":false},"source":"df_all = pd.concat((train, test), axis=0, ignore_index=True)\ndf_all['Gene_Share'] = df_all.apply(lambda r: sum([1 for w in r['Gene'].split(' ') if w in r['Text'].split(' ')]), axis=1)\ndf_all['Variation_Share'] = df_all.apply(lambda r: sum([1 for w in r['Variation'].split(' ') if w in r['Text'].split(' ')]), axis=1)","outputs":[],"execution_count":7},{"cell_type":"code","metadata":{"_cell_guid":"9527f162-c93d-433d-a834-9c0215c45069","_uuid":"4182af290004b8736d345f0e1a64f3e8969daf5d","scrolled":false,"trusted":false},"source":"df_all.head()","outputs":[],"execution_count":8},{"cell_type":"code","metadata":{"_cell_guid":"b923b200-2d21-4166-afc9-0f39f167d86e","_uuid":"158f5e7bfe484e1f006aefdfe681e4f2ac8db6cf","trusted":false},"source":"gen_var_lst = sorted(list(train.Gene.unique()) + list(train.Variation.unique()))\nprint(len(gen_var_lst))","outputs":[],"execution_count":9},{"cell_type":"code","metadata":{"_cell_guid":"0dc59167-7e16-43ac-bdc4-d0a48dcbd296","_uuid":"1a42b0fc3a8b361064c8f73fdfe5ed6525f85eba","scrolled":true,"trusted":false},"source":"gen_var_lst = [x for x in gen_var_lst if len(x.split(' '))==1]\nprint(len(gen_var_lst))\ni_ = 0\n\n#commented for Kaggle Limits\n# for gen_var_lst_itm in gen_var_lst:\n#     if i_ % 100 == 0: print(i_)\n#     df_all['GV_'+str(gen_var_lst_itm)] = df_all['Text'].map(lambda x: str(x).count(str(gen_var_lst_itm)))\n#     i_ += 1","outputs":[],"execution_count":10},{"cell_type":"code","metadata":{"_cell_guid":"38cca869-3d4d-44f6-8c55-b78bb2840609","_uuid":"199df6688fc7654251228cd85bb5dcd87fd5ebda","collapsed":true,"trusted":false},"source":"for c in df_all.columns:\n    if df_all[c].dtype == 'object':\n        if c in ['Gene','Variation']:\n            lbl = preprocessing.LabelEncoder()\n            df_all[c+'_lbl_enc'] = lbl.fit_transform(df_all[c].values)  \n            df_all[c+'_len'] = df_all[c].map(lambda x: len(str(x)))\n            df_all[c+'_words'] = df_all[c].map(lambda x: len(str(x).split(' ')))\n        elif c != 'Text':\n            lbl = preprocessing.LabelEncoder()\n            df_all[c] = lbl.fit_transform(df_all[c].values)\n        if c=='Text': \n            df_all[c+'_len'] = df_all[c].map(lambda x: len(str(x)))\n            df_all[c+'_words'] = df_all[c].map(lambda x: len(str(x).split(' '))) \n\ntrain = df_all.iloc[:len(train)]\ntest = df_all.iloc[len(train):]","outputs":[],"execution_count":11},{"cell_type":"code","metadata":{"_cell_guid":"ac511901-464b-4d08-aeba-1be18edb9661","_uuid":"570ce7b3ab8b9f46c51843ea9d5c103c24dbb06e","scrolled":true,"trusted":false},"source":"train.head()","outputs":[],"execution_count":12},{"cell_type":"code","metadata":{"_cell_guid":"3b870664-6bda-49f9-8f1d-9fc85072ef3d","_uuid":"3050f029fe7406337d4946b834c05124eaff2b76","scrolled":true,"trusted":false},"source":"test.head()","outputs":[],"execution_count":13},{"cell_type":"code","metadata":{"_cell_guid":"03a1bc89-a8dd-4e0b-9fb2-e4ddc8286fa6","_uuid":"a6406d11c0298511b4107abbbe0fa947de522791","trusted":false},"source":"train.shape","outputs":[],"execution_count":14},{"cell_type":"code","metadata":{"_cell_guid":"bba97217-b7ee-4747-b1bc-71e68d2d57fa","_uuid":"2d13c3b056df19eef7fac1416a9b27c01f2723fe","trusted":false},"source":"test.shape","outputs":[],"execution_count":15},{"cell_type":"code","metadata":{"_cell_guid":"ae2a7760-2706-4342-a0f5-03212584b9ae","_uuid":"7e2a2ec9fd7908f36000470699c19b4425fc6181","collapsed":true,"trusted":false},"source":"class cust_regression_vals(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n    def fit(self, x, y=None):\n        return self\n    def transform(self, x):\n        x = x.drop(['Gene', 'Variation','ID','Text'],axis=1).values\n        return x\n\nclass cust_txt_col(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n    def __init__(self, key):\n        self.key = key\n    def fit(self, x, y=None):\n        return self\n    def transform(self, x):\n        return x[self.key].apply(str)","outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"_cell_guid":"36663a09-da14-4df0-87ec-9fe8771abf6c","_uuid":"9b62f6d90b60d79c974b2f61a5d374e3412de7f2"},"source":"### 2. Main difference\n***\n#### 1. Pipeline Changed\n\nThe original Kernel uses the pipeline with these codes below for 'Text' feature extraction:\n```python\n#commented for Kaggle Limits\n('pi3', pipeline.Pipeline([('Text', cust_txt_col('Text')), \n                           ('tfidf_Text', feature_extraction.text.TfidfVectorizer(ngram_range=(1, 2))), \n                           ('tsvd3', decomposition.TruncatedSVD(n_components=50, n_iter=25, random_state=12))]))\n```\nUnfortunately, it can not fit my memory of 8GB + 2GB(swap). And without these features, I can only get nearly 0.7xxx on PL.\n\nSo, I try to use **HashingVectorizer + TfidfTransformer** instead of **TfidfVectorizer**. [Reference](http://scikit-learn.org/stable/modules/feature_extraction.html#vectorizing-a-large-text-corpus-with-the-hashing-trick)\n\n#### 2. Parameter Tuning\n\nFor HashingVectorizer saved my memory, I try to use **ngram_range=(1, 3)** with HashingVectorizer. \n\nAnd **n_components=300** with **TruncatedSVD**.\n\n#### 3. Batch Transform\n\nWith these codes, I can fit_transform the **train**, but still out of memory if transform **test**.\n\nSo, I try to use batch transform of test data step by step, and vstack all. \n\nAnd, it works!","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"6f895147-c745-4792-9a43-f6bd6ff7822c","_uuid":"5fcca871cc477a821fa964d87f46a314b168dfa5","scrolled":true,"trusted":false},"source":"print('Pipeline...')\nfp = pipeline.Pipeline([\n    ('union', pipeline.FeatureUnion(\n        n_jobs = -1,\n        transformer_list = [\n            ('standard', cust_regression_vals()),\n            ('pi1', pipeline.Pipeline([('Gene', cust_txt_col('Gene')), \n                                       ('count_Gene', feature_extraction.text.CountVectorizer(analyzer=u'char', ngram_range=(1, 8))), \n                                       ('tsvd1', decomposition.TruncatedSVD(n_components=20, n_iter=25, random_state=12))])),\n            ('pi2', pipeline.Pipeline([('Variation', cust_txt_col('Variation')), \n                                       ('count_Variation', feature_extraction.text.CountVectorizer(analyzer=u'char', ngram_range=(1, 8))), \n                                       ('tsvd2', decomposition.TruncatedSVD(n_components=20, n_iter=25, random_state=12))])),\n            #commented for Kaggle Limits\n#             ('pi3', pipeline.Pipeline([('Text', cust_txt_col('Text')), \n#                                        ('hv', feature_extraction.text.HashingVectorizer(decode_error='ignore', n_features=2 ** 16, non_negative=True, ngram_range=(1, 3))),\n#                                        ('tfidf_Text', feature_extraction.text.TfidfTransformer()), \n#                                        ('tsvd3', decomposition.TruncatedSVD(n_components=300, n_iter=25, random_state=12))]))\n\n        \n        ])\n    )])\n\n\ntrain = fp.fit_transform(train)\nprint (train.shape)\n\ntest_t = np.empty([0, train.shape[1]])\nstep = 200\nfor i in range(0, len(test), step):\n    step_end = i+step\n    step_end = step_end if step_end < len(test) else len(test)\n    _test = fp.transform(test.iloc[i:step_end])\n    test_t = np.vstack((test_t, _test))\ntest = test_t\nprint (test.shape)","outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"_cell_guid":"d373d666-f0a9-4f84-aa34-26c5674f4736","_uuid":"c85df64546e595beb036aa357b6135a063fc3d0b"},"source":"### 3. Xgboost Parameter Tuning\n***\n#### 1. eta 0.03333 -> 0.02 \n\nI like small learning rate.\n\n#### 2. max_depth 4 -> 6 \n\nBigger means higher risk of overfitting. But, since we got more features, maybe it could be better for this big data?! I'm not sure yet.\n\n#### 3. test_size 0.18 -> 0.15\n\nWith slightly more data to train.","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"7af95c64-27c7-4c86-9a72-775af3c3a9a2","_uuid":"2d1c0b506fb71abf8b6ac60f5f51d91504db209f","collapsed":true,"trusted":false},"source":"y = y - 1 #fix for zero bound array","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"_cell_guid":"2ba02c19-f3f5-4502-b976-3768453a2c1a","_uuid":"4e5ae29c6ac788f0d4d157d8ffde170e58023a82","scrolled":true,"trusted":false},"source":"file_pre = datetime.datetime.now().strftime('%m_%d_%H_%M_%S')\n\ndenom = 0\nfold = 1 #Change to 5, 1 for Kaggle Limits\nfor i in range(fold):\n    params = {\n#         'eta': 0.03333,\n        'eta': 0.02,\n#         'max_depth': 4,\n        'max_depth': 6,\n        'objective': 'multi:softprob',\n        'eval_metric': 'mlogloss',\n        'num_class': 9,\n        'seed': i,\n        'silent': True\n    }\n    x1, x2, y1, y2 = model_selection.train_test_split(train, y, test_size=0.15, random_state=i)\n    watchlist = [(xgb.DMatrix(x1, y1), 'train'), (xgb.DMatrix(x2, y2), 'valid')]\n    model = xgb.train(params, xgb.DMatrix(x1, y1), 1000,  watchlist, verbose_eval=50, early_stopping_rounds=100)\n    score1 = metrics.log_loss(y2, model.predict(xgb.DMatrix(x2), ntree_limit=model.best_ntree_limit), labels = list(range(9)))\n    print(score1)\n    #if score < 0.9:\n    if denom != 0:\n        pred = model.predict(xgb.DMatrix(test), ntree_limit=model.best_ntree_limit+80)\n        preds += pred\n    else:\n        pred = model.predict(xgb.DMatrix(test), ntree_limit=model.best_ntree_limit+80)\n        preds = pred.copy()\n    denom += 1\n#     submission = pd.DataFrame(pred, columns=['class'+str(c+1) for c in range(9)])\n#     submission['ID'] = pid\n#     submission.to_csv('./result/submission_xgb_fold_'  + str(i) + '_' + file_pre + '.csv', index=False)\npreds /= denom\nsubmission = pd.DataFrame(preds, columns=['class'+str(c+1) for c in range(9)])\nsubmission['ID'] = pid\nsubmission.to_csv('./result/submission_xgb_' + file_pre + '.csv', index=False)\n","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"_cell_guid":"e68d420c-618b-4936-9528-e21d9d993472","_uuid":"6c53a32d9150a8b3e531f978860eb59c54e3ea44","trusted":false},"source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nplt.rcParams['figure.figsize'] = (7.0, 7.0)\nxgb.plot_importance(booster=model,); plt.show()","outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"_cell_guid":"8c9d124c-2c43-433a-9876-9e9d4bd593b6","_uuid":"44fb8218b1775155c09a92ec9fbd3dd9e4fd751d"},"source":"### References\n* [Redefining Treatment](https://www.kaggle.com/the1owl/redefining-treatment-0-57456)\n* [Vectorizing a large text corpus with the hashing trick](http://scikit-learn.org/stable/modules/feature_extraction.html#vectorizing-a-large-text-corpus-with-the-hashing-trick)\n* [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"df9a7b7c-0f33-41c1-b188-92ec405ef626","_uuid":"0f62da96ffd443abab31de7de5c14fdb2844236b","collapsed":true,"trusted":false},"source":"","outputs":[],"execution_count":null}],"nbformat_minor":1,"nbformat":4}