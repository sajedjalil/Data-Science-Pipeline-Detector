{"metadata":{"hide_input":false,"language_info":{"name":"python","version":"3.6.2","nbconvert_exporter":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","file_extension":".py","codemirror_mode":{"name":"ipython","version":3}},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"cells":[{"metadata":{},"source":"# RNN + GRU + bidirectional + Attentional context\n\nThis kernes uses a recurrent neural network in keras that uses GRU cells with a bidirectional layer and an attention context layer. The model uses the begining of the text and the end of the text and them join both outputs along with an one hot encoded layer for the gene and another for the variation. The variation has been encoded using the first and the last letter.\n\nThis kernel is based in a kernel by [ReiiNakanoBasic](https://www.kaggle.com/reiinakano/basic-nlp-bag-of-words-tf-idf-word2vec-lstm).","cell_type":"markdown"},{"execution_count":null,"metadata":{"_cell_guid":"ef08df1c-3c94-4f63-bbc7-6cc93f46fb52","_execution_state":"idle","_uuid":"7b62b6d64cc54d675b18c29ee12eed7cd45a3154"},"source":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss, accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\nimport gensim\n\nimport scikitplot.plotters as skplt\n\nimport nltk\n\nimport os","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"0b39f26e-9c4c-46db-8fe5-86967734c0e5","_uuid":"58b74d086e6f6f067f337470219be9fd43211c08"},"source":"## Load data","cell_type":"markdown"},{"execution_count":null,"metadata":{"scrolled":true,"_cell_guid":"011dde04-0160-41cd-9149-5aeac26295fb","_execution_state":"idle","_uuid":"1b07b556871714b93d70806d58b5225be507e716"},"source":"df_train_txt = pd.read_csv('../input/training_text', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"])\ndf_train_var = pd.read_csv('../input/training_variants')\ndf_val_txt = pd.read_csv('../input/test_text', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"])\ndf_val_var = pd.read_csv('../input/test_variants')\n\ndf_test_txt = pd.read_csv('../input/stage2_test_text.csv', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"])\ndf_test_var = pd.read_csv('../input/stage2_test_variants.csv')","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{},"source":"df_val_txt = pd.read_csv('../input/test_text', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"])\ndf_val_var = pd.read_csv('../input/test_variants')\ndf_val_labels = pd.read_csv('../input/stage1_solution_filtered.csv')\ndf_val_labels['Class'] = pd.to_numeric(df_val_labels.drop('ID', axis=1).idxmax(axis=1).str[5:])\ndf_val_labels = df_val_labels[['ID', 'Class']]\ndf_val_txt = pd.merge(df_val_txt, df_val_labels, how='left', on='ID')","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{},"source":"df_train = pd.merge(df_train_var, df_train_txt, how='left', on='ID')\ndf_train.head()\ndf_test = pd.merge(df_test_var, df_test_txt, how='left', on='ID')\ndf_test.head()\ndf_val = pd.merge(df_val_var, df_val_txt, how='left', on='ID')\ndf_val = df_val[df_val_txt['Class'].notnull()]\ndf_val.head()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"f6abf33d-337d-4be8-8d50-34d2292a4c71","_uuid":"525e0ac244f14d7dbc0edfe5783c1439a3a10d3d"},"source":"## Word2Vec model","cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"609177df-1799-4f5e-b3e7-77c820561a7c","_execution_state":"busy","_uuid":"d631c05ece2e126a82481fa5a262d12ec7577e38"},"source":"class MySentences(object):\n    \"\"\"MySentences is a generator to produce a list of tokenized sentences \n    \n    Takes a list of numpy arrays containing documents.\n    \n    Args:\n        arrays: List of arrays, where each element in the array contains a document.\n    \"\"\"\n    def __init__(self, *arrays):\n        self.arrays = arrays\n \n    def __iter__(self):\n        for array in self.arrays:\n            for document in array:\n                for sent in nltk.sent_tokenize(document):\n                    yield nltk.word_tokenize(sent)\n\ndef get_word2vec(sentences, location):\n    \"\"\"Returns trained word2vec\n    \n    Args:\n        sentences: iterator for sentences\n        \n        location (str): Path to save/load word2vec\n    \"\"\"\n    if os.path.exists(location):\n        print('Found {}'.format(location))\n        model = gensim.models.Word2Vec.load(location)\n        return model\n    \n    print('{} not found. training model'.format(location))\n    model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=8)\n    print('Model done training. Saving to disk')\n    model.save(location)\n    return model","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"e62cf926-34d9-461a-bf92-21e38e9ff2d3","_execution_state":"busy","_uuid":"80d89a6184209c1630976aa5bdb9a93b1290b363"},"source":"w2vec = get_word2vec(\n    MySentences(\n        df_train['Text'].values, \n        df_val['Text'].values\n    ),\n    'w2vmodel'\n)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"591cc8cb-49c2-41d0-be0a-ff8ee7d4f230","_uuid":"b41f276a240b388d56fb5187a5b99ac0d3b76d59"},"source":"### Tokenizer\nWe'll define a transformer (with sklearn interface) to convert a document into its corresponding vector","cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"8be99804-9357-4522-a80b-aa328f5ef973","_execution_state":"busy","_uuid":"f188944da320461ad2e8f76a7a991454e2c0763c"},"source":"class MyTokenizer:\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        transformed_X = []\n        for document in X:\n            tokenized_doc = []\n            for sent in nltk.sent_tokenize(document):\n                tokenized_doc += nltk.word_tokenize(sent)\n            transformed_X.append(np.array(tokenized_doc))\n        return np.array(transformed_X)\n    \n    def fit_transform(self, X, y=None):\n        return self.transform(X)\n\nclass MeanEmbeddingVectorizer(object):\n    def __init__(self, word2vec):\n        self.word2vec = word2vec\n        # if a text is empty we should return a vector of zeros\n        # with the same dimensionality as all the other vectors\n        self.dim = len(word2vec.wv.syn0[0])\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = MyTokenizer().fit_transform(X)\n        \n        return np.array([\n            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n                    or [np.zeros(self.dim)], axis=0)\n            for words in X\n        ])\n    \n    def fit_transform(self, X, y=None):\n        return self.transform(X)\n","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"7db18b6e-d23f-412a-9f16-429082572649","_uuid":"934ddee5cd73ca17ee2bd73f647290697aca997c"},"source":"## RNN in Keras\nWe use a vocabulary of 10000 most used words and a sequence length of 3000 words (3000 for the beggining and 3000 for the ending)\n\nThis takes about few hours to run on GPU","cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"d1b0a192-ea18-419e-9ba4-bb52f12b9f78","_execution_state":"busy","_uuid":"8d4ee34e1e95faa8be9d93ecced116c324b61465"},"source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n# Use the Keras tokenizer\nVOCABULARY_SIZE = 10000\nSEQUENCE_LENGTH = 3000\ntokenizer = Tokenizer(num_words=VOCABULARY_SIZE)\ntokenizer.fit_on_texts(df_train['Text'].values)","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"0610df13-711b-4d28-810a-2f7361cea388","_execution_state":"busy","_uuid":"2fba830b1fd73426d1b3b2f22bac503d91d0a923"},"source":"# Train set\ntrain_set = df_train.sample(frac=1) # shuffle data first\ntrain_set_input = tokenizer.texts_to_sequences(train_set['Text'].values)\ntrain_set_input_reverse = [list(reversed(x)) for x in train_set_input]\ntrain_set_input_begin = pad_sequences(train_set_input, maxlen=SEQUENCE_LENGTH)\ntrain_set_input_end = pad_sequences(train_set_input_reverse, maxlen=SEQUENCE_LENGTH)\ntrain_set_output = pd.get_dummies(train_set['Class']).values\nprint(train_set_input_begin.shape, train_set_input_end.shape, train_set_output.shape)","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"0610df13-711b-4d28-810a-2f7361cea388","_execution_state":"busy","_uuid":"2fba830b1fd73426d1b3b2f22bac503d91d0a923"},"source":"# Validation set\nval_set_input = tokenizer.texts_to_sequences(df_val['Text'].values)\nval_set_input_reverse = [list(reversed(x)) for x in val_set_input]\nval_set_input_begin = pad_sequences(val_set_input, maxlen=SEQUENCE_LENGTH)\nval_set_input_end = pad_sequences(val_set_input_reverse, maxlen=SEQUENCE_LENGTH)\nval_set_output = pd.get_dummies(df_val['Class']).values\nprint(val_set_input_begin.shape, val_set_input_end.shape, val_set_output.shape)","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"0610df13-711b-4d28-810a-2f7361cea388","_execution_state":"busy","_uuid":"2fba830b1fd73426d1b3b2f22bac503d91d0a923"},"source":"# Test set\ntest_set_input = tokenizer.texts_to_sequences(df_test['Text'].values)\ntest_set_input_reverse = [list(reversed(x)) for x in test_set_input]\ntest_set_input_begin = pad_sequences(test_set_input, maxlen=SEQUENCE_LENGTH)\ntest_set_input_end = pad_sequences(test_set_input_reverse, maxlen=SEQUENCE_LENGTH)\nprint(test_set_input_begin.shape, test_set_input_end.shape)","cell_type":"code","outputs":[]},{"metadata":{},"source":"#### Add genes and variations as one hot encoding\nWe only transform the variations to use the first and last letter, otherwise it will be almos one variation per exmple and it will be useless.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_cell_guid":"0610df13-711b-4d28-810a-2f7361cea388","_execution_state":"busy","_uuid":"2fba830b1fd73426d1b3b2f22bac503d91d0a923"},"source":"# Add gene and variation to predictor\ngene_le = LabelEncoder()\nall_genes = np.concatenate([df_train['Gene'], df_val['Gene'], df_test['Gene']])\nall_variations = np.concatenate([df_train['Variation'], df_val['Variation'], df_test['Variation']])\nall_variations = np.asarray([v[0]+v[-1] for v in all_variations])\nprint (\"Unique genes: \", len(np.unique(all_genes)))\nprint (\"Unique variations:\", len(np.unique(all_variations)))\n\n# gene_encoded = gene_le.fit_transform(all_genes.ravel()).reshape(-1, 1)\n# gene_encoded = gene_encoded / np.max(gene_encoded.ravel())\n# variation_le = LabelEncoder()\n# variation_encoded = variation_le.fit_transform(all_variations).reshape(-1, 1)\n# variation_encoded = variation_encoded / np.max(variation_encoded)\n\ngene_encoded = pd.get_dummies(all_genes).values\nvariation_encoded = pd.get_dummies(all_variations).values\n\nlen_train_set = len(train_set_input)\nlen_val_set = len(val_set_input)\nlen_test_set = len(test_set_input)\ntrain_set_input_gene = gene_encoded[:len_train_set]\ntrain_set_input_variation = variation_encoded[:len_train_set]\nval_set_input_gene = gene_encoded[len_train_set:-len_test_set]\nval_set_input_variation = variation_encoded[len_train_set:-len_test_set]\ntest_set_input_gene = gene_encoded[-len_test_set:]\ntest_set_input_variation = variation_encoded[-len_test_set:]\n\nprint (len_train_set, len(train_set_input_gene))\nprint (len_val_set, len(val_set_input_gene))\nprint (len_test_set, len(test_set_input_gene))","cell_type":"code","outputs":[]},{"metadata":{},"source":"## Attention layer\n\nfrom: https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2","cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":true},"source":"from keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints\nimport numpy as np\n\ndef dot_product(x, kernel):\n    \"\"\"\n    Wrapper for dot product operation, in order to be compatible with both\n    Theano and Tensorflow\n    Args:\n        x (): input\n        kernel (): weights\n    Returns:\n    \"\"\"\n    if K.backend() == 'tensorflow':\n        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n    else:\n        return K.dot(x, kernel)\n    \n\nclass AttentionWithContext(Layer):\n    \"\"\"\n    Attention operation, with a context/query vector, for temporal data.\n    Supports Masking.\n    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n    \"Hierarchical Attention Networks for Document Classification\"\n    by using a context vector to assist the attention\n    # Input shape\n        3D tensor with shape: `(samples, steps, features)`.\n    # Output shape\n        2D tensor with shape: `(samples, features)`.\n    How to use:\n    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n    The dimensions are inferred based on the output shape of the RNN.\n    Note: The layer has been tested with Keras 2.0.6\n    Example:\n        model.add(LSTM(64, return_sequences=True))\n        model.add(AttentionWithContext())\n        # next add a Dense layer (for classification/regression) or whatever...\n    \"\"\"\n\n    def __init__(self,\n                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n                 W_constraint=None, u_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.u_regularizer = regularizers.get(u_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.u_constraint = constraints.get(u_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        super(AttentionWithContext, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        if self.bias:\n            self.b = self.add_weight((input_shape[-1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n\n        self.u = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_u'.format(self.name),\n                                 regularizer=self.u_regularizer,\n                                 constraint=self.u_constraint)\n\n        super(AttentionWithContext, self).build(input_shape)\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        uit = dot_product(x, self.W)\n\n        if self.bias:\n            uit += self.b\n\n        uit = K.tanh(uit)\n        ait = dot_product(uit, self.u)\n\n        a = K.exp(ait)\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n\n        # in some cases especially in the early stages of training the sum may be almost zero\n        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], input_shape[-1]","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"af64e666-69b4-4fe7-a8ce-c098e640cbf9","_execution_state":"busy","_uuid":"6aff6b56a17d5bc0650e1bb41e60fee7be6f52ac"},"source":"from keras.models import Sequential, Model\nfrom keras.layers import Dense, Embedding, LSTM, GRU, Bidirectional, Merge, Input, concatenate\nfrom keras.layers.merge import Concatenate\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\n# Build out our simple LSTM\nembed_dim = 128\nlstm_out = 196\n\n# Model saving callback\nckpt_callback = ModelCheckpoint('keras_model', \n                                 monitor='val_loss', \n                                 verbose=1, \n                                 save_best_only=True, \n                                 mode='auto')\n\n\ninput_sequence_begin = Input(shape=(train_set_input_begin.shape[1],))\ninput_sequence_end = Input(shape=(train_set_input_end.shape[1],))\ninput_gene = Input(shape=(train_set_input_gene.shape[1],))\ninput_variant = Input(shape=(train_set_input_variation.shape[1],))\n\nmerged = concatenate([input_gene, input_variant])\ndense = Dense(32, activation='sigmoid')(merged)\n\nembeds_begin = Embedding(VOCABULARY_SIZE, embed_dim, input_length = SEQUENCE_LENGTH)(input_sequence_begin)\nembeds_out_begin = Bidirectional(GRU(lstm_out, recurrent_dropout=0.2, dropout=0.2, return_sequences=True))(embeds_begin)\nattention_begin = AttentionWithContext()(embeds_out_begin)\n\nembeds_end = Embedding(VOCABULARY_SIZE, embed_dim, input_length = SEQUENCE_LENGTH)(input_sequence_end)\nembeds_out_end = Bidirectional(GRU(lstm_out, recurrent_dropout=0.2, dropout=0.2, return_sequences=True))(embeds_end)\nattention_end = AttentionWithContext()(embeds_out_end)\n\nmerged2 = concatenate([attention_begin, attention_end, dense])\ndense2 = Dense(9,activation='softmax')(merged2)\n\nmodel = Model(inputs=[input_sequence_begin, input_sequence_end, input_gene, input_variant], outputs=dense2)\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam')\nprint(model.summary())","cell_type":"code","outputs":[]},{"metadata":{},"source":"### Training","cell_type":"markdown"},{"execution_count":null,"metadata":{"scrolled":true,"_cell_guid":"d7eac40d-a8d6-47a7-9542-1a2b06f76ca8","_execution_state":"busy","_uuid":"a0da0f4a43684380e3dd1b6f53d03cc45384fbf9"},"source":"model.fit([train_set_input_begin, train_set_input_end, train_set_input_gene, train_set_input_variation], train_set_output, \n          epochs=6, batch_size=16, \n          validation_data=([val_set_input_begin,val_set_input_end,val_set_input_gene,val_set_input_variation], val_set_output), \n          callbacks=[ckpt_callback])","cell_type":"code","outputs":[]},{"metadata":{},"source":"### Validation","cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"72101f81-dd5a-4b27-b985-e4fa03a7d7fd","_execution_state":"busy","_uuid":"142200cc3855cd276bb3a9abf3526a61988fafee"},"source":"model = load_model('keras_model', custom_objects={'AttentionWithContext': AttentionWithContext})","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"da8194c1-bce5-4d3d-93a7-fbb6b832113e","_execution_state":"busy","_uuid":"fffe9f6d9907deabbf827f183c4cf33af917b131"},"source":"probas = model.predict([val_set_input_begin, val_set_input_end, val_set_input_gene, val_set_input_variation])\npred_indices = np.argmax(probas, axis=1)\nclasses = np.array(range(1, 10))\npreds = classes[pred_indices]\nprint('Log loss: {}'.format(log_loss(classes[np.argmax(val_set_output, axis=1)], probas)))\nprint('Accuracy: {}'.format(accuracy_score(classes[np.argmax(val_set_output, axis=1)], preds)))\nskplt.plot_confusion_matrix(classes[np.argmax(val_set_output, axis=1)], preds)\n","cell_type":"code","outputs":[]},{"metadata":{},"source":"## Train with validation set\n\nFor the final submission we can add the validation set to the training set and run the network for 4 epochs. After 4 epcohs it starts overfitting in the validation set. We only do this to add more training samples and try to get better results this way","cell_type":"markdown"},{"execution_count":null,"metadata":{},"source":"model.fit([\n    np.concatenate([train_set_input_begin, val_set_input_begin]), \n    np.concatenate([train_set_input_end,val_set_input_end]), \n    np.concatenate([train_set_input_gene, val_set_input_gene]), \n    np.concatenate([train_set_input_variation, val_set_input_variation])\n],  np.concatenate([train_set_output,val_set_output]), \n    epochs=4, batch_size=16, callbacks=[ckpt_callback])","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"e70c3e0c-3722-4ed0-943f-3909c2078f38","_uuid":"2eeb0f102a078b433d0c30b9d4e5fedebc58a236"},"source":"## Submission","cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"4d608be7-741c-462c-8a2a-39b81612000d","_execution_state":"busy","_uuid":"7b454ebc7996841cffcaba3a165d8b28b5c76466"},"source":"probas = model.predict([test_set_input_begin, test_set_input_end, test_set_input_gene, test_set_input_variation])","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"dbcb71f9-1854-44c3-b9cf-d060494887d8","_execution_state":"busy","_uuid":"81165d6e9870765c90fbb4624743ea49f9f6d1d8"},"source":"submission_df = pd.DataFrame(probas, columns=['class'+str(c+1) for c in range(9)])\nsubmission_df['ID'] = df_test['ID']\nsubmission_df.head()","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"9ce11af2-21b8-47e0-a83b-55ee14c1c53c","_execution_state":"busy","_uuid":"7264bb21f9c0028b6be3e637c8414d543266d113"},"source":"submission_df.to_csv('submission.csv', index=False)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"d1aa03cb-3faf-4dfe-97a7-97dbfff856ec","_uuid":"1ac56c47c3e55b3a6fde88332fe1e045e59fef9b"},"source":"# Public LB Score: 0.93662\n\nThe private leaderboard shows and score of 2.8. Everybody get much worse results in the private leader board, there has been a long discussion in the forums.","cell_type":"markdown"}],"nbformat_minor":1,"nbformat":4}