{"cells":[{"metadata":{"_cell_guid":"735da42a-f22f-4593-8064-e84aeed4ad8a","_uuid":"6db1914e33b0978c91f82ba1f39763c03e8b6bb2"},"cell_type":"markdown","source":"\n- Read the data into a dataframe using pandas library.\n- Cleaning unnecessary data (unique or null columns).\n- Analyzing data distributions.\n- Analyzing text data via keywords and summarization.\n- Tokenizing (Lemmatization and stopwording) for further analysis.\n- Analyzing word distributions for any surface correlations.\n- Creating a word cloud of the whole text.\n- Using Word2Vec to check the correlation between text and the classes.\n  \n------  "},{"metadata":{"trusted":true,"_uuid":"b4fc47c455b3a2507329b2500716168e1cea7b9e"},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"f5dc2f4696674b870ed50a23dda441dba48335f3"},"cell_type":"markdown","source":""},{"metadata":{"_cell_guid":"68bc6008-0c54-4242-88e8-ece657edafbf","_uuid":"2743be3dcb624cc8038a878530d95b96aadb74e9","trusted":true},"cell_type":"code","source":"%matplotlib inline\n\n# Data wrapper libraries\nimport pandas as pd\nimport numpy as np\n\n# Visualization Libraries\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch\nfrom matplotlib.markers import MarkerStyle\nimport seaborn as sns\n\n# Text analysis helper libraries\nfrom gensim.summarization import summarize\nfrom gensim.summarization import keywords\n\n# Text analysis helper libraries for word frequency etc..\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\n# Word cloud visualization libraries\nfrom scipy.misc import imresize\nfrom PIL import Image\n#from wordcloud import WordCloud, ImageColorGenerator\nfrom collections import Counter\n\n# Word2Vec related libraries\nfrom gensim.models import KeyedVectors\n\n# Dimensionaly reduction libraries\nfrom sklearn.decomposition import PCA\n\n# Clustering library\nfrom sklearn.cluster import KMeans\n\n# Set figure size a bit bigger than default so everything is easily red\nplt.rcParams[\"figure.figsize\"] = (11, 7)","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"9f1cac1b-973c-459e-b019-8f51942804e0","_uuid":"5639b952184820abd153255842cfbff9106d35b1"},"cell_type":"markdown","source":"Let's take a casual look at the *variants* data."},{"metadata":{"_cell_guid":"a10c73a9-ef65-439f-8633-bebe9fe3bd99","_uuid":"ee1e3624e86beaa3eeb142e56e82f95c2e348e1b","trusted":true},"cell_type":"code","source":"\n\ndf_variants = pd.read_csv(\"../input/training_variants\").set_index('ID').reset_index()\n#df_variants = pd.read_csv(source+\"/training_variants\")\ntest_variants_df = pd.read_csv('../input/test_variants')\ndf_text = pd.read_csv(\"../input/training_text\", sep=\"\\|\\|\", engine=\"python\", skiprows=1, names=[\"ID\", \"Text\"])\ntest_text_df = pd.read_csv(\"../input/test_text\", sep=\"\\|\\|\", engine=\"python\", skiprows=1, names=[\"ID\", \"Text\"])\n\nprint(\"Train Variant\".ljust(15), df_variants.shape)\nprint(\"Train Text\".ljust(15), df_text.shape)\nprint(\"Test Variant\".ljust(15), test_variants_df.shape)\nprint(\"Test Text\".ljust(15), test_text_df.shape)\n\n\n\ndf_variants.head()","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"499507a5-bd67-40bd-9e8c-93b482959eed","_uuid":"95dd47fc041bdd22a01f38de37b414eb3ba8d8f2"},"cell_type":"markdown","source":"Let's take a look at the *text* data. Data is still small enough for memory so read to memory using pandas."},{"metadata":{"trusted":true,"_uuid":"04b2d45d7d046fc00b1e4feb13f5137e7e9893e5"},"cell_type":"code","source":"print(\"For training data, there are a total of\", len(df_variants.ID.unique()), \"IDs,\", end='')\nprint(len(df_variants.Gene.unique()), \"unique genes,\", end='')\nprint(len(df_variants.Variation.unique()), \"unique variations and \", end='')\nprint(len(df_variants.Class.unique()),  \"classes\")","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"a7bb0422-5cc7-4c5b-91ec-f1d99d702670","_uuid":"88af716db0de30be05863325eca3e58cb135a1f9","trusted":true},"cell_type":"code","source":"df_text = pd.read_csv('../input/training_text', sep='\\|\\|', engine='python', \n                      skiprows=1, names=['ID', 'Text']).set_index('ID').reset_index()\ndf_text.head()","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5f048d75e5e56b3a5bcd8e662d59fccbad1e26d"},"cell_type":"code","source":"df_text.loc[:, 'Text_count']  = df_text[\"Text\"].apply(lambda x: len(x.split()))\ndf_text.head()","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"68ba24b9-33b2-413f-b0aa-970cda386534","_uuid":"cd7b9286af6f8cf58b80825654987084ebc1969e"},"cell_type":"markdown","source":"Join two dataframes on index"},{"metadata":{"_cell_guid":"6555e149-7319-491e-8ae3-a282c87386fa","_uuid":"374274aec5a4ff115c3e7fbf68d152c5fde634be","trusted":true},"cell_type":"code","source":"df = df_variants.merge(df_text, how=\"inner\", left_on=\"ID\", right_on=\"ID\")\ndf[df[\"Class\"]==1].head()","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"106c6353-c822-4a2d-a4de-6cd95b63de12","_uuid":"b40c4a376c687694bcc0ea7b170b26760f9190c1"},"cell_type":"markdown","source":"*Variation* column is mostly consists of independant unique values. So its not very helpfull for our predictions. So we will drop it."},{"metadata":{"trusted":true,"_uuid":"bb2a0cf57aee77322abe062c54da428f53221bed"},"cell_type":"code","source":"plt.figure(figsize=(12,8))\ngene_count_grp = df.groupby('Gene')[\"Text_count\"].sum().reset_index()\nsns.violinplot(x=\"Class\", y=\"Text_count\", data=df, inner=None)\nsns.swarmplot(x=\"Class\", y=\"Text_count\", data=df, color=\"w\", alpha=.5);\nplt.ylabel('Text Count', fontsize=14)\nplt.xlabel('Class', fontsize=14)\nplt.title(\"Text length distribution\", fontsize=18)\nplt.show()","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"217f2b39b2fa75172e9a4fae0d53e508da33a73e"},"cell_type":"markdown","source":"Distribution looks quite interesting and now I am in love with violin plots. All classes have most counts in between 0 to 20000. Just as expected. There should be some"},{"metadata":{"trusted":true,"_uuid":"4d4a3714ed7dd586e8941cc702643212d6c0e9bf"},"cell_type":"code","source":"fig, axs = plt.subplots(ncols=3, nrows=3, figsize=(15,15))\n\nfor i in range(3):\n    for j in range(3):\n        gene_count_grp = df[df[\"Class\"]==((i*3+j)+1)].groupby('Gene')[\"Text_count\"].mean().reset_index()\n        sorted_gene_group = gene_count_grp.sort_values('Text_count', ascending=False)\n        sorted_gene_group_top_7 = sorted_gene_group[:7]\n        sns.barplot(x=\"Gene\", y=\"Text_count\", data=sorted_gene_group_top_7, ax=axs[i][j])","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"b2712ee7902ff816691dcb443c657ca05c1e42d3"},"cell_type":"markdown","source":"Frequently occurring terms for each class"},{"metadata":{"_cell_guid":"ab3dece5-13dd-4fc4-a039-cf39eabc15dd","_uuid":"5bcfcd99c12f59df3dfc52d389fd4ebc4eec1294","trusted":true},"cell_type":"code","source":"df['Variation'].describe()","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"946b3375-7bdf-40c2-a75a-5a9e8388f551","_uuid":"1bf47b2c689ed94f6eb4635f56dfa2e24dfd9520"},"cell_type":"markdown","source":"*Gene* column is a bit more complicated, values seems to be heavly skewed.  \nData can still be valuable if normalized and balanced by weights.  "},{"metadata":{"_cell_guid":"33d8f4c4-37a1-41e0-a3d4-eab554f4ba33","_uuid":"ee676b737ba8b178b7e18db4c7814994012b73eb","trusted":false},"cell_type":"code","source":"plt.figure()\nax = df['Gene'].value_counts().plot(kind='area')\n\nax.get_xaxis().set_ticks([])\nax.set_title('Gene Frequency Plot')\nax.set_xlabel('Gene')\nax.set_ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"aea60649-f512-4717-8f68-3686001ea50a","_uuid":"2a3a67be45d9c7dfa98963f38668041a856ee0fc"},"cell_type":"markdown","source":"And finally lets look at the class distribution."},{"metadata":{"_cell_guid":"fa9fed90-0725-4a2c-9814-15ef946c90e5","_uuid":"a42e7661c3824cb73e3631fd4650656e232c4371","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.countplot(x=\"Class\", data=df_variants, palette=\"Blues_d\")\nplt.ylabel('Frequency', fontsize=14)\nplt.xlabel('Class', fontsize=14)\nplt.title(\"Distribution of genetic mutation classes\", fontsize=18)\nplt.show()","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"c7cf14fb-cc6f-43a8-95d6-9c178810bee1","_uuid":"1c61e9d05a5beeeee35314782f1fa640f2af8f7a"},"cell_type":"markdown","source":"Distribution looks skewed towards some classes, there are not enough examples for classes 8 and 9. During training, this can be solved using bias weights, careful sampling in batches or simply removing some of the dominant data to equalize the field.  \n  \n  ----\nFinally, lets drop the columns we don't need and be done with the initial cleaning."},{"metadata":{"trusted":true,"_uuid":"774acbf778ab9797a0466a7ea862c3a3b5f201c7"},"cell_type":"code","source":"gene_group = df_variants.groupby(\"Gene\")['Gene'].count()\nminimal_occ_genes = gene_group.sort_values(ascending=True)[:10]\nprint(\"Genes with maximal occurences\\n\", gene_group.sort_values(ascending=False)[:10])\nprint(\"\\nGenes with minimal occurences\\n\", minimal_occ_genes)","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"cd05cda8a228ffbf7b09b98778cde61b2aacc5af"},"cell_type":"markdown","source":"Lets have a look at some genes that has highest number of occurrences in each class."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"f789cd95c08764f1b0e8567a1a057306af16e1cb"},"cell_type":"code","source":"df_variants=df_variants.reset_index()","execution_count":21,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92040147266ed6133df07dd22c077ff7effaa065"},"cell_type":"code","source":"fig, axs = plt.subplots(ncols=3, nrows=3, figsize=(15,15))\n\nfor i in range(3):\n    for j in range(3):\n        gene_count_grp = df_variants[df_variants[\"Class\"]==((i*3+j)+1)].groupby('Gene')[\"ID\"].count().reset_index()\n        sorted_gene_group = gene_count_grp.sort_values('ID', ascending=False)\n        sorted_gene_group_top_7 = sorted_gene_group[:7]\n        sns.barplot(x=\"Gene\", y=\"ID\", data=sorted_gene_group_top_7, ax=axs[i][j])\n","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"23569a3710f923f3b2d8d01bf3253a67ef040bbc"},"cell_type":"markdown","source":"Some points we can conclude from these graphs:\nBRCA1 is highly dominating Class 5\nSF3B1 is highly dominating Class 9\nBRCA1 and BRCA2 are dominating Class 6"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"78563e1ca3ae0e386f6295b990eef0b6e470a591"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7945dde4-ea1c-4192-9211-1470c474b30b","_uuid":"98e3dc99311f96bd90e0aabd50b66c8026d07254","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e184bf23-f789-495f-8901-453c27f173c3","_uuid":"569ff28d633782e0abfaa6b9e5c10ef33ebb309e"},"cell_type":"markdown","source":"Now let's look at the remaining data in more detail.  \nText is too long and detailed and technical, so I've decided to summarize it using gensim's TextRank algorithm.  \nStill didn't understand anything :/"},{"metadata":{"_cell_guid":"38580240-7231-4ea2-8248-a10efe88e074","_uuid":"37f88c2d93d9a5b8d4221ef243594af3d64fd73b","trusted":true},"cell_type":"code","source":"t_id = 0\ntext = df.loc[t_id, 'Text']\n\nword_scores = keywords(text, words=5, scores=True, split=True, lemmatize=True)\n#word_scores = ', '.join(['{}-{:.2f}'.format(k, s[0]) for k, s in word_scores])\nsummary = summarize(text, word_count=100)\n\nprint('ID [{}]\\nKeywords: [{}]\\nSummary: [{}]'.format(t_id, word_scores, summary))","execution_count":24,"outputs":[]},{"metadata":{"_cell_guid":"fbdc05b8-1db5-4c8d-b4fb-b5435282be78","_uuid":"92bef461c126b63add7ac87a17860196b5b89003"},"cell_type":"markdown","source":"Text is tokenized, cleaned of stopwords and lemmatized for word frequency analysis.  \n\nTokenization obviously takes a lot of time on a corpus like this. So bear that in mind.  \nMay skip this, use a simpler tokenizer like `ToktokTokenizer` or just use `str.split()` instead."},{"metadata":{"_cell_guid":"477b75f6-5e86-4013-8d79-fa6e45481377","_uuid":"1959056b14f476f60d7e92494aa3917653d4140f","collapsed":true,"trusted":true},"cell_type":"code","source":"custom_words = [\"fig\", \"figure\", \"et\", \"al\", \"al.\", \"also\",\n                \"data\", \"analyze\", \"study\", \"table\", \"using\",\n                \"method\", \"result\", \"conclusion\", \"author\", \n                \"find\", \"found\", \"show\", '\"', \"’\", \"“\", \"”\"]\n\nstop_words = set(stopwords.words('english') + list(punctuation) + custom_words)\nwordnet_lemmatizer = WordNetLemmatizer()\n\nclass_corpus = df.groupby('Class').apply(lambda x: x['Text'].str.cat())\nclass_corpus = class_corpus.apply(lambda x: Counter(\n    [wordnet_lemmatizer.lemmatize(w) \n     for w in word_tokenize(x) \n     if w.lower() not in stop_words and not w.isdigit()]\n))","execution_count":25,"outputs":[]},{"metadata":{"_cell_guid":"77f8ebf2-d600-4232-92f7-11cc8f909d37","_uuid":"5c399a507dec0036c7d60d32cc25355b4b5380ea"},"cell_type":"markdown","source":"Lets look at the dominant words in classes. And see if we can find any correlation."},{"metadata":{"_cell_guid":"d173959f-1c85-40ec-8aa1-d6aa6a5ea24d","_uuid":"7fae6828866caa4766ab8a1e45b48bb9e8ac529d","trusted":true},"cell_type":"code","source":"class_freq = class_corpus.apply(lambda x: x.most_common(5))\nclass_freq = pd.DataFrame.from_records(class_freq.values.tolist()).set_index(class_freq.index)\n\ndef normalize_row(x):\n    label, repetition = zip(*x)\n    t = sum(repetition)\n    r = [n/t for n in repetition]\n    return list(zip(label,r))\n\nclass_freq = class_freq.apply(lambda x: normalize_row(x), axis=1)\n\n# set unique colors for each word so it's easier to read\nall_labels = [x for x in class_freq.sum().sum() if isinstance(x,str)]\nunique_labels = set(all_labels)\ncm = plt.get_cmap('Blues_r', len(all_labels))\ncolors = {k:cm(all_labels.index(k)/len(all_labels)) for k in all_labels}\n\nfig, ax = plt.subplots()\n\noffset = np.zeros(9)\nfor r in class_freq.iteritems():\n    label, repetition = zip(*r[1])\n    ax.barh(range(len(class_freq)), repetition, left=offset, color=[colors[l] for l in label])\n    offset += repetition\n    \nax.set_yticks(np.arange(len(class_freq)))\nax.set_yticklabels(class_freq.index)\nax.invert_yaxis()\n\n# annotate words\noffset_x = np.zeros(9) \nfor idx, a in enumerate(ax.patches):\n    fc = 'k' if sum(a.get_fc()) > 2.5 else 'w'\n    ax.text(offset_x[idx%9] + a.get_width()/2, a.get_y() + a.get_height()/2, \n            '{}\\n{:.2%}'.format(all_labels[idx], a.get_width()), \n            ha='center', va='center', color=fc, fontsize=14, family='monospace')\n    offset_x[idx%9] += a.get_width()\n    \nax.set_title('Most common words in each class')\nax.set_xlabel('Word Frequency')\nax.set_ylabel('Classes')\n\nplt.tight_layout()\nplt.show()","execution_count":26,"outputs":[]},{"metadata":{"_cell_guid":"4e716a9f-ee3f-4d2e-b392-31ee5a3d33c3","_uuid":"2befd23a2138413de89da78427ce703bfddda446"},"cell_type":"markdown","source":"**Mutation** and **cell** seems to be commonly dominating in all classes, not very informative. But the graph is still helpful. And would give more insight if we were to ignore most common words.  \nLet's plot how many times 25 most common words appear in the whole corpus."},{"metadata":{"_cell_guid":"ca31e217-7354-4138-9247-c7c9f75f75e2","_uuid":"6e9ccd0be0d8775cb9b6368b124f18d28293c943","trusted":true},"cell_type":"code","source":"whole_text_freq = class_corpus.sum()\n\nfig, ax = plt.subplots()\n\nlabel, repetition = zip(*whole_text_freq.most_common(25))\n\nax.barh(range(len(label)), repetition, align='center')\nax.set_yticks(np.arange(len(label)))\nax.set_yticklabels(label)\nax.invert_yaxis()\n\nax.set_title('Word Distribution Over Whole Text')\nax.set_xlabel('# of repetitions')\nax.set_ylabel('Word')\n\nplt.tight_layout()\nplt.show()","execution_count":27,"outputs":[]},{"metadata":{"_cell_guid":"049e10ea-3ece-4f30-8b0e-c409da9dfb6c","_uuid":"a17cf8c79c1044fe53dcb523a0dd1c11be757f8f"},"cell_type":"markdown","source":"Words are plotted to a word cloud using the beautiful [word_cloud](https://github.com/amueller/word_cloud) library.  \nThis part is unnecessary for analysis but pretty =)."},{"metadata":{"_cell_guid":"d5a13ffb-5f01-42cb-89d0-d8c4e36d09aa","_uuid":"00c9cdc80160412a7daff2f4c5375d1971d114db","collapsed":true},"cell_type":"markdown","source":"\n```python\ndef resize_image(np_img, new_size):\n    old_size = np_img.shape\n    ratio = min(new_size[0]/old_size[0], new_size[1]/old_size[1])\n    \n    return imresize(np_img, (round(old_size[0]*ratio), round(old_size[1]*ratio)))\n\nmask_image = np.array(Image.open('tmp/dna_stencil.png').convert('L'))\nmask_image = resize_image(mask_image, (4000, 2000))\n\nwc = WordCloud(max_font_size=140,\n               min_font_size=8,\n               max_words=1000,\n               width=mask_image.shape[1], \n               height=mask_image.shape[0],\n               prefer_horizontal=.9,\n               relative_scaling=.52,\n               background_color=None,\n               mask=mask_image,\n               mode=\"RGBA\").generate_from_frequencies(freq)\n\nplt.figure()\nplt.axis(\"off\")\nplt.tight_layout()\nplt.imshow(wc, interpolation=\"bilinear\")\n```"},{"metadata":{"_cell_guid":"23bfe63a-04db-4a55-a8a2-466d15b61173","_uuid":"df09aba16cc624410608972716b0b7b198c1dc7f"},"cell_type":"markdown","source":"![](http://i.imgur.com/oRQptjx.png?1)\n  \nWe can also use the text data and visualize the relationships between words using Word2Vec. Even average the word vectors of a sentence and visualize the relationship between sentences.  \n(Doc2Vec could give much better results, for simplicity averaging word vectors are sufficient for this kernel)  \n  \nWe'll use gensim's word2vec algorithm with Google's (huge) pretrained word2vec tokens."},{"metadata":{"_cell_guid":"ea4cb51e-62cb-4a6f-a0f0-283d933dba02","_uuid":"d8b44273593359926562761e526a9e66ebc1d841"},"cell_type":"markdown","source":"```python\nvector_path = r\"word_vectors\\GoogleNews-vectors-negative300.bin\"\n\nmodel = KeyedVectors.load_word2vec_format (vector_path, binary=True)\nmodel.wv.similar_by_word('mutation')\n```"},{"metadata":{"_cell_guid":"c19f95a8-bcde-491b-8196-da0722525daa","_uuid":"cb775987ccc9d4648f7807b6d4f7db074456ce1d"},"cell_type":"markdown","source":"```\n[('mutations', 0.8541924953460693),  \n ('genetic_mutation', 0.8245046138763428),  \n ('mutated_gene', 0.7879971861839294),  \n ('gene_mutation', 0.7823827266693115),  \n ('genetic_mutations', 0.7393667697906494),  \n ('gene', 0.7343351244926453),  \n ('gene_mutations', 0.7275242209434509),  \n ('genetic_variant', 0.7182294726371765),  \n ('alleles', 0.7164379358291626),  \n ('mutant_gene', 0.7144376039505005)] \n ```\n\nThe results of word2vec looks really promising.  \n  \n----\nNow that we can somewhat understand the relationship between words, we'll use that to understand the relationship between sentences and documents. I'll be simply averaging the word vectors over a sentence, but better ways exist like using idf weighted averages or training a paragraph2vec model from scratch over the corpus."},{"metadata":{"_cell_guid":"5f22bd25-a44c-4897-82f4-b73ccc91f107","_uuid":"f6eadf5e8ac5f408aba6ef6422f7ff56f422f501"},"cell_type":"markdown","source":"```python\ndef get_average_vector(text):\n    tokens = [w.lower() for w in word_tokenize(text) if w.lower() not in stop_words]\n    return np.mean(np.array([model.wv[w] for w in tokens if w in model]), axis=0)\n\nmodel.wv.similar_by_vector(get_average_vector(df.loc[0, 'Text']))\n```"},{"metadata":{"_cell_guid":"2f043df4-f885-4a9d-b015-b4105b590df5","_uuid":"8105d55a5b99a39abd2fa224bb91bf315ba9c92b"},"cell_type":"markdown","source":"```\n[('cyclic_AMP_cAMP', 0.7930851578712463),\n ('mRNA_transcripts', 0.7838510274887085),\n ('oncogenic_transformation', 0.7836254239082336),\n ('MT1_MMP', 0.7755827307701111),\n ('microRNA_molecule', 0.773587703704834),\n ('tumorigenicity', 0.7722263932228088),\n ('coexpression', 0.7706621885299683),\n ('transgenic_mice_expressing', 0.7698256969451904),\n ('pleiotropic', 0.7698150873184204),\n ('cyclin_B1', 0.7696200013160706)]\n```\n  \nAnd finally we can visualize the relationships between sentences by averaging the vector representations of each word in a sentence and reducing the vector dimensions to 2D (Google's Word2Vec embeddings come as [,300] vectors).  \nI will use PCA for dimensionality reduction because it usually is faster (and/or uses less memory) but t-sne could give better results."},{"metadata":{"_cell_guid":"b76c710d-7e6c-483a-a588-e49afb45dbcc","_uuid":"fa21c1bfe9a8ca1f8a8a5e2aff7ad062f3a09f55"},"cell_type":"markdown","source":"```python\ntext_vecs = df.apply(lambda x: (x['Class'], get_average_vector(x['Text'])), axis=1)\nclasses, vecs = list(zip(*text_vecs.values))\n\npca = PCA(n_components=2)\nreduced_vecs = pca.fit_transform(vecs)\n\nfig, ax = plt.subplots()\n\ncm = plt.get_cmap('jet', 9)\ncolors = [cm(i/9) for i in range(9)]\nax.scatter(reduced_vecs[:,0], reduced_vecs[:,1], c=[colors[c-1] for c in classes], cmap='jet', s=8)\n\n\nplt.legend(handles=[Patch(color=colors[i], label='Class {}'.format(i+1)) for i in range(9)])\n\nplt.show()\n```"},{"metadata":{"_cell_guid":"9b685559-a110-4a2b-8f92-cf3315885fd3","_uuid":"18c11dc03496714f5b00370dbda502e83fdb0417"},"cell_type":"markdown","source":"![](http://i.imgur.com/hT6GIAK.png)\n  \nNo imminent correlation can be seen based on this analysis.  \nThis may be due to:\n- Dimensional Reduction (we may not be seeing the correlation in 2D).\n- Averaging word vectors are not effective solutions to infer sentence/paragraph vectors.\n- There is no obvious correlation between texts.\n  \nIn any case let's see the difference with a simple k-means clustering."},{"metadata":{"_cell_guid":"a13d5c53-5372-4d08-ba14-8da6f3b8513a","_uuid":"b3597d2537eb04ee864e804b5a8b34d077ba89eb"},"cell_type":"markdown","source":"```python\nkmeans = KMeans(n_clusters=9).fit(vecs)\nc_labels = kmeans.labels_\n\nfig, ax = plt.subplots()\n\ncm = plt.get_cmap('jet', 9)\ncolors = [cm(i/9) for i in range(9)]\nax.scatter(reduced_vecs[:,0], reduced_vecs[:,1], c=[colors[c-1] for c in c_labels], cmap='jet', s=8)\n\nplt.legend(handles=[Patch(color=colors[i], label='Class {}'.format(i+1)) for i in range(9)])\n\nplt.show()\n```"},{"metadata":{"_cell_guid":"972d9632-0d7e-48a0-a874-93e6acfb6227","_uuid":"959de5583cb023177f8a26c2622ae218bad76240"},"cell_type":"markdown","source":"![](http://i.imgur.com/IYjRzd0.png)"}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":1}