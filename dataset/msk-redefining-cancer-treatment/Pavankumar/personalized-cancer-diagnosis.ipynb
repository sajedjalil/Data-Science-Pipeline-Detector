{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport time\nimport warnings\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nfrom scipy.sparse import hstack\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC \nfrom collections import Counter, defaultdict\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nimport math\nfrom sklearn.metrics import normalized_mutual_info_score\nfrom sklearn.ensemble import RandomForestClassifier\nwarnings.filterwarnings(\"ignore\")\n\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/msk-redefining-cancer-treatment/training_variants.zip')\nprint('Number of data points: ',data.shape[0])\nprint('Number of features: ',data.shape[1])\nprint('Features: ',data.columns.values)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_text = pd.read_csv('../input/msk-redefining-cancer-treatment/training_text.zip',sep='\\|\\|',\n                        engine='python',\n                       names=['ID','TEXT'],skiprows=1)\nprint('Number of data points: ',data_text.shape[0])\nprint('Number of features: ',data_text.shape[1])\nprint('Features: ',data_text.columns.values)\ndata_text.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\n\ndef nlp_preprocessing(total_text,index,column):\n    if type(total_text) is not int:\n        string =''\n        # replace every special char with space\n        total_text = re.sub('[^a-zA-Z0-9\\n]', ' ', total_text)\n        # replace multiple spaces with single space\n        total_text = re.sub('\\s+',' ',total_text)\n        # All text to lower case\n        total_text = total_text.lower()\n        \n        for word in total_text.split():\n            if not word in stop_words:\n                string += word + ' '\n        data_text[column][index] = string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.clock()\nfor index, row in data_text.iterrows():\n    if type(row['TEXT']) is str:\n        nlp_preprocessing(row['TEXT'],index,'TEXT')\n    else:\n        print('there is no text description for id: ',index)\nprint('Time took for preprocessing the text : ',time.clock()-start_time,'seconds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = pd.merge(data,data_text,on='ID',how='left')\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result[result.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.loc[result['TEXT'].isnull(),'TEXT'] = result['Gene'] +' '+ result['Variation']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result[result['ID']==1109]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train-Test and Cross Validate"},{"metadata":{},"cell_type":"markdown","source":"### splitting data into train test and cv (64:20:16)"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = result['Class'].values\nresult.Gene = result['Gene'].str.replace('\\s+','_')\nresult.Variation = result['Variation'].str.replace('\\s+','_')\n\nX_train,test_df,y_train,y_test = train_test_split(result,y_true,stratify=y_true,test_size=0.2)\ntrain_df,cv_df,y_train,y_cv = train_test_split(X_train,y_train,stratify=y_train,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of data points in train data:', train_df.shape[0])\nprint('Number of data points in test data:', test_df.shape[0])\nprint('Number of data points in cross validation data:', cv_df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distributions of y_i in Train,Test and CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_class_distribution = train_df['Class'].value_counts().sort_index()\ntest_class_distribution = test_df['Class'].value_counts().sort_index()\ncv_class_distribution = cv_df['Class'].value_counts().sort_index()\n\nmy_colors = 'rgbkymc'\ntrain_class_distribution.plot(kind='bar')\nplt.xlabel('Class')\nplt.ylabel('Data points per class')\nplt.title('Distribution of yi in train Data')\nplt.grid()\nplt.show()\n\nsorted_yi = np.argsort(-train_class_distribution.values)\nfor i in sorted_yi:\n    print('Number of data points in class', i+1, ':',train_class_distribution.values[i], '(', np.round((train_class_distribution.values[i]/train_df.shape[0]*100), 3), '%)')\n\n    \ntest_class_distribution.plot(kind='bar')\nplt.xlabel('Class')\nplt.ylabel('Data points per class')\nplt.title('Distribution of yi in test data')\nplt.grid()\nplt.show()\n\nsorted_yi =np.argsort(-test_class_distribution.values)\nfor i in sorted_yi:\n    print('Number of data points in class',i+1,':',test_class_distribution.values[i],'(',np.round((test_class_distribution.values[i]/test_df.shape[0] *100),3),'%)')\n    \ncv_class_distribution.plot(kind='bar')\nplt.xlabel('Class')\nplt.ylabel('Data points per class')\nplt.title('Distribution of yi in cv data')\nplt.grid()\nplt.show()\n\nsorted_yi = np.argsort(-cv_class_distribution.values)\nfor i in sorted_yi:\n    print('Number of data points in class',i+1,':',cv_class_distribution.values[i],'(',np.round((cv_class_distribution.values[i]/cv_df.shape[0] *100),3),'%)')    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix "},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(test_y,predict_y):\n    C = confusion_matrix(test_y,predict_y)\n    A = (((C.T)/(C.sum(axis=1))).T)\n    B = (C/C.sum(axis=0))\n    \n    labels =[1,2,3,4,5,6,7,8,9]\n    print('-'*20,'Confusion Matrix','-'*20)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(C,annot=True,cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    \n    print('-'*20,'Precision Matrix (column sum=1)','-'*20)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(A,annot=True,cmap='YlGnBu',fmt='.3f',xticklabels = labels,yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    \n    print('-'*20,'Recall Matrix (Row sum=1)','-'*20)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(B,annot=True,cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data_len =test_df.shape[0]\ncv_data_len = cv_df.shape[0]\n# cv set error\ncv_predicted_y = np.zeros((cv_data_len,9))\nfor i in range(cv_data_len):\n    rand_probs = np.random.rand(1,9)\n    cv_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\nprint('Log loss on Cross valibation data using random model: ', log_loss(y_cv,cv_predicted_y,eps=1e-15))\n\n# Test set error\ntest_predicted_y = np.zeros((test_data_len,9))\nfor i in range(test_data_len):\n    rand_probs = np.random.rand(1,9)\n    test_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\nprint('Log loss on test data using random model: ',log_loss(y_test,test_predicted_y,eps=1e-15))\n\npredicted_y = np.argmax(test_predicted_y,axis=1)\nplot_confusion_matrix(y_test,predicted_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Univariate Analysis"},{"metadata":{},"cell_type":"markdown","source":"### univariate Analysis on Gene feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_genes = train_df['Gene'].value_counts()\nprint('Number of Unique Genes: ',unique_genes.shape[0])\nprint(unique_genes.head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s = sum(unique_genes.values)\nh = unique_genes.values/s\nplt.plot(h,label='Histogram of Genes')\nplt.xlabel('Index of gene')\nplt.ylabel('Number of occurences')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c = np.cumsum(h)\nplt.plot(c)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# code for response coding with laplase smoothing:\n# alpha: used for laplase smoothing\n# feature: ['gene','Variation']\n# df: ['train_df','test_df','cv_df']\n# get gene variation feature Dict\ndef get_gv_fea_dict(alpha,feature,df):\n    value_count = train_df[feature].value_counts()\n    gv_dict =dict()\n    for i,denominator in value_count.items():\n        vec =[]\n        for k in range(1,10):\n            # print(train_df.loc[(train_df['Class']==1) & (train_df['Gene']=='BRCA1')])\n            cls_cnt = train_df.loc[(train_df['Class']==k) & (train_df[feature]==i)]\n            vec.append((cls_cnt.shape[0]+alpha*10)/(denominator + 90*alpha))\n            \n        gv_dict[i] = vec\n    return gv_dict\n\ndef get_gv_feature(alpha,feature,df):\n    gv_dict= get_gv_fea_dict(alpha,feature,df)\n    value_counts = train_df[feature].value_counts()\n    gv_fea =[]\n    \n    for index,row in df.iterrows():\n        if row[feature] in dict(value_counts).keys():\n            gv_fea.append(gv_dict[row[feature]])\n        else:\n            gv_fea.append([1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9])\n    return gv_fea","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### There are two ways we can featurize this Gene feature\n* One hot Encoding\n* Response Coding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# response coding for gene feature\nalpha =1\ntrain_gene_feature_responseCoding = np.array(get_gv_feature(alpha,'Gene',train_df))\ntest_gene_feature_responseCoding = np.array(get_gv_feature(alpha,'Gene',test_df))\ncv_gene_feature_responseCoding = np.array(get_gv_feature(alpha,'Gene',cv_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The shape of gene feature using response coding: ',train_gene_feature_responseCoding.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One hot encoding of Gene features\ngene_vectorizer = CountVectorizer()\ntrain_gene_feature_onehotCoding = gene_vectorizer.fit_transform(train_df['Gene'])\ntest_gene_feature_onehotCoding = gene_vectorizer.transform(test_df['Gene'])\ncv_gene_feature_onehotCoding = gene_vectorizer.transform(cv_df['Gene'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The shape of gene feature using one hot encoding: ',train_gene_feature_onehotCoding.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Gene'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gene_vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### How good is this Gene feature in predicting y_i??"},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-5,1)] # hyperparameter for SGD Classifier.\ncv_log_error_array =[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i,penalty='l2',loss='log',random_state=42)\n    clf.fit(train_gene_feature_onehotCoding,y_train)\n    sig_clf = CalibratedClassifierCV(clf,method='sigmoid')\n    sig_clf.fit(train_gene_feature_onehotCoding,y_train)\n    predict_y = sig_clf.predict_proba(cv_gene_feature_onehotCoding)\n    cv_log_error_array.append(log_loss(y_cv,predict_y,labels=clf.classes_,eps=1e-15))\n    print('For values of alpha :',i,'The log loss is:',log_loss(y_cv,predict_y,labels=clf.classes_,eps=1e-15))\n    \n    \nfig,ax = plt.subplots()\nax.plot(alpha,cv_log_error_array,c='g')\nfor i,txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title('Cross Validation Error for each alpha')\nplt.xlabel('alpha i')\nplt.ylabel('Error measure')\nplt.show()\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha],penalty='l2',loss='log',random_state=42)\nclf.fit(train_gene_feature_onehotCoding,y_train)\nsig_clf = CalibratedClassifierCV(clf,method='sigmoid')\nsig_clf.fit(train_gene_feature_onehotCoding,y_train)\n\npredict_y = sig_clf.predict_proba(train_gene_feature_onehotCoding)\nprint('For value of best alpha : ',alpha[best_alpha], 'The train log loss is: ',log_loss(y_train,predict_y,labels=clf.classes_,eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_gene_feature_onehotCoding)\nprint('For value of best alpha : ',alpha[best_alpha], 'The cv log loss is: ',log_loss(y_cv,predict_y,labels=clf.classes_,eps=1e-15))\npredict_y = sig_clf.predict_proba(test_gene_feature_onehotCoding)\nprint('For value of best alpha : ',alpha[best_alpha], 'The test log loss is: ',log_loss(y_test,predict_y,labels=clf.classes_,eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Is gene feature stable across all the data sets(train,test,cv)??"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_coverage =test_df[test_df['Gene'].isin(list(set(train_df['Gene'])))].shape[0]\ncv_coverage = cv_df[cv_df['Gene'].isin(list(set(train_df['Gene'])))].shape[0]\n\nprint('In test data: ',test_coverage,'out of: ',test_df.shape[0], ':',(test_coverage/test_df.shape[0])*100)\nprint('In cv data: ',cv_coverage,'out of: ',cv_df.shape[0],':',(cv_coverage/cv_df.shape[0])*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Univariate Analysis on Variation feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_variations = train_df['Variation'].value_counts()\nprint('Number of unique features: ',unique_variations.shape[0])\nprint(unique_variations.head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s = sum(unique_variations.values)\nh = unique_variations.values/s\nplt.plot(h,label='Histogram of Variations')\nplt.grid()\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c = np.cumsum(h)\nplt.plot(c,label='Cumulative distribution of variation')\nplt.grid()\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# alpha is used for laplace smoothing\nalpha = 1\n# train gene feature\ntrain_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", train_df))\n# test gene feature\ntest_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", test_df))\n# cross validation gene feature\ncv_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", cv_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train_variation_feature_responseCoding is a converted feature using the response coding method. The shape of Variation feature:\", train_variation_feature_responseCoding.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One hot encoding of variation feature\nvariation_vectorizer = CountVectorizer()\ntrain_variation_feature_onehotCoding = variation_vectorizer.fit_transform(train_df['Variation'])\ntest_variation_feature_onehotCoding = variation_vectorizer.transform(test_df['Variation'])\ncv_variation_feature_onehotCoding = variation_vectorizer.transform(cv_df['Variation'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train_variation_feature_onehotEncoded is converted feature using the one-hot encoding method. The shape of Variation feature:\", train_variation_feature_onehotCoding.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How good is the variation feature in predicting y_i??"},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-5,1)]\ncv_log_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i,penalty='l2',loss='log',random_state=42)\n    clf.fit(train_variation_feature_onehotCoding,y_train)\n    \n    sig_clf = CalibratedClassifierCV(clf,method='sigmoid')\n    sig_clf.fit(train_variation_feature_onehotCoding,y_train)\n    predict_y = sig_clf.predict_proba(cv_variation_feature_onehotCoding)\n    \n    cv_log_error_array.append(log_loss(y_cv,predict_y,labels=clf.classes_,eps=1e-15))\n    print('For values of alpha: ',i,'The log loss is: ',log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n    \nfig,ax = plt.subplots()\nax.plot(alpha,cv_log_error_array,c='g')\nfor i,txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)),(alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title('Cross validate error for each alpha')\nplt.xlabel('Alpha i')\nplt.ylabel('Error measure')\nplt.show()\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_variation_feature_onehotCoding, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_variation_feature_onehotCoding, y_train)\n\npredict_y = sig_clf.predict_proba(train_variation_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_variation_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_variation_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Is variation feature stable across all the data sets(train,test,cv)??"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_coverage = test_df[test_df['Variation'].isin(list(set(train_df['Variation'])))].shape[0]\ncv_coverage = cv_df[cv_df['Variation'].isin(list(set(train_df['Variation'])))].shape[0]\nprint('In test data',test_coverage, 'out of',test_df.shape[0], \":\",(test_coverage/test_df.shape[0])*100)\nprint('In cross validation data',cv_coverage, 'out of ',cv_df.shape[0],\":\" ,(cv_coverage/cv_df.shape[0])*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Univariate Analysis on Text feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"# cls_text is a data frame\n# for every row in data fram consider the 'TEXT'\n# split the words by space\n# make a dict with those words\n# increment its count whenever we see that word\n\ndef extract_dictionary_paddle(cls_text):\n    dictionary = defaultdict(int)\n    for index, row in cls_text.iterrows():\n        for word in row['TEXT'].split():\n            dictionary[word] +=1\n    return dictionary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n#https://stackoverflow.com/a/1602964\ndef get_text_responsecoding(df):\n    text_feature_responseCoding = np.zeros((df.shape[0],9))\n    for i in range(0,9):\n        row_index = 0\n        for index, row in df.iterrows():\n            sum_prob = 0\n            for word in row['TEXT'].split():\n                sum_prob += math.log(((dict_list[i].get(word,0)+10 )/(total_dict.get(word,0)+90)))\n            text_feature_responseCoding[row_index][i] = math.exp(sum_prob/len(row['TEXT'].split()))\n            row_index += 1\n    return text_feature_responseCoding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# building a CountVectorizer with all the words that occured minimum 3 times in train data\ntext_vectorizer = CountVectorizer(min_df=3)\ntrain_text_feature_onehotCoding = text_vectorizer.fit_transform(train_df['TEXT'])\n# getting all the feature names (words)\ntrain_text_features= text_vectorizer.get_feature_names()\n\n# train_text_feature_onehotCoding.sum(axis=0).A1 will sum every row and returns (1*number of features) vector\ntrain_text_fea_counts = train_text_feature_onehotCoding.sum(axis=0).A1\n\n# zip(list(text_features),text_fea_counts) will zip a word with its number of times it occured\ntext_fea_dict = dict(zip(list(train_text_features),train_text_fea_counts))\n\n\nprint(\"Total number of unique words in train data :\", len(train_text_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_list = []\n# dict_list =[] contains 9 dictoinaries each corresponds to a class\nfor i in range(1,10):\n    cls_text = train_df[train_df['Class']==i]\n    # build a word dict based on the words in that class\n    dict_list.append(extract_dictionary_paddle(cls_text))\n    # append it to dict_list\n\n# dict_list[i] is build on i'th  class text data\n# total_dict is buid on whole training text data\ntotal_dict = extract_dictionary_paddle(train_df)\n\n\nconfuse_array = []\nfor i in train_text_features:\n    ratios = []\n    max_val = -1\n    for j in range(0,9):\n        ratios.append((dict_list[j][i]+10 )/(total_dict[i]+90))\n    confuse_array.append(ratios)\nconfuse_array = np.array(confuse_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#response coding of text features\ntrain_text_feature_responseCoding  = get_text_responsecoding(train_df)\ntest_text_feature_responseCoding  = get_text_responsecoding(test_df)\ncv_text_feature_responseCoding  = get_text_responsecoding(cv_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/a/16202486\n# we convert each row values such that they sum to 1  \ntrain_text_feature_responseCoding = (train_text_feature_responseCoding.T/train_text_feature_responseCoding.sum(axis=1)).T\ntest_text_feature_responseCoding = (test_text_feature_responseCoding.T/test_text_feature_responseCoding.sum(axis=1)).T\ncv_text_feature_responseCoding = (cv_text_feature_responseCoding.T/cv_text_feature_responseCoding.sum(axis=1)).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# don't forget to normalize every feature\ntrain_text_feature_onehotCoding = normalize(train_text_feature_onehotCoding, axis=0)\n\n# we use the same vectorizer that was trained on train data\ntest_text_feature_onehotCoding = text_vectorizer.transform(test_df['TEXT'])\n# don't forget to normalize every feature\ntest_text_feature_onehotCoding = normalize(test_text_feature_onehotCoding, axis=0)\n\n# we use the same vectorizer that was trained on train data\ncv_text_feature_onehotCoding = text_vectorizer.transform(cv_df['TEXT'])\n# don't forget to normalize every feature\ncv_text_feature_onehotCoding = normalize(cv_text_feature_onehotCoding, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://stackoverflow.com/a/2258273/4084039\nsorted_text_fea_dict = dict(sorted(text_fea_dict.items(), key=lambda x: x[1] , reverse=True))\nsorted_text_occur = np.array(list(sorted_text_fea_dict.values()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of words for a given frequency.\nprint(Counter(sorted_text_occur))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train a Logistic regression+Calibration model using text features whicha re on-hot encoded\nalpha = [10 ** x for x in range(-5, 1)]\ncv_log_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_text_feature_onehotCoding, y_train)\n    \n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_text_feature_onehotCoding, y_train)\n    predict_y = sig_clf.predict_proba(cv_text_feature_onehotCoding)\n    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_text_feature_onehotCoding, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_text_feature_onehotCoding, y_train)\n\npredict_y = sig_clf.predict_proba(train_text_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_text_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_text_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_intersec_text(df):\n    df_text_vec = CountVectorizer(min_df=3)\n    df_text_fea = df_text_vec.fit_transform(df['TEXT'])\n    df_text_features = df_text_vec.get_feature_names()\n\n    df_text_fea_counts = df_text_fea.sum(axis=0).A1\n    df_text_fea_dict = dict(zip(list(df_text_features),df_text_fea_counts))\n    len1 = len(set(df_text_features))\n    len2 = len(set(train_text_features) & set(df_text_features))\n    return len1,len2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len1,len2 = get_intersec_text(test_df)\nprint(np.round((len2/len1)*100, 3), \"% of word of test data appeared in train data\")\nlen1,len2 = get_intersec_text(cv_df)\nprint(np.round((len2/len1)*100, 3), \"% of word of Cross Validation appeared in train data\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Machine learning Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data preparation for ML models.\n\n#Misc. functionns for ML models\n\n\ndef predict_and_plot_confusion_matrix(train_x, train_y,test_x, test_y, clf):\n    clf.fit(train_x, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x, train_y)\n    pred_y = sig_clf.predict(test_x)\n\n    # for calculating log_loss we willl provide the array of probabilities belongs to each class\n    print(\"Log loss :\",log_loss(test_y, sig_clf.predict_proba(test_x)))\n    # calculating the number of data points that are misclassified\n    print(\"Number of mis-classified points :\", np.count_nonzero((pred_y- test_y))/test_y.shape[0])\n    plot_confusion_matrix(test_y, pred_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def report_log_loss(train_x, train_y, test_x, test_y,  clf):\n    clf.fit(train_x, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x, train_y)\n    sig_clf_probs = sig_clf.predict_proba(test_x)\n    return log_loss(test_y, sig_clf_probs, eps=1e-15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function will be used just for naive bayes\n# for the given indices, we will print the name of the features\n# and we will check whether the feature present in the test point text or not\ndef get_impfeature_names(indices, text, gene, var, no_features):\n    gene_count_vec = CountVectorizer()\n    var_count_vec = CountVectorizer()\n    text_count_vec = CountVectorizer(min_df=3)\n    \n    gene_vec = gene_count_vec.fit(train_df['Gene'])\n    var_vec  = var_count_vec.fit(train_df['Variation'])\n    text_vec = text_count_vec.fit(train_df['TEXT'])\n    \n    fea1_len = len(gene_vec.get_feature_names())\n    fea2_len = len(var_count_vec.get_feature_names())\n    \n    word_present = 0\n    for i,v in enumerate(indices):\n        if (v < fea1_len):\n            word = gene_vec.get_feature_names()[v]\n            yes_no = True if word == gene else False\n            if yes_no:\n                word_present += 1\n                print(i, \"Gene feature [{}] present in test data point [{}]\".format(word,yes_no))\n        elif (v < fea1_len+fea2_len):\n            word = var_vec.get_feature_names()[v-(fea1_len)]\n            yes_no = True if word == var else False\n            if yes_no:\n                word_present += 1\n                print(i, \"variation feature [{}] present in test data point [{}]\".format(word,yes_no))\n        else:\n            word = text_vec.get_feature_names()[v-(fea1_len+fea2_len)]\n            yes_no = True if word in text.split() else False\n            if yes_no:\n                word_present += 1\n                print(i, \"Text feature [{}] present in test data point [{}]\".format(word,yes_no))\n\n    print(\"Out of the top \",no_features,\" features \", word_present, \"are present in query point\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### stacking the three types of features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# merging gene, variance and text features\n\n# building train, test and cross validation data sets\n# a = [[1, 2], \n#      [3, 4]]\n# b = [[4, 5], \n#      [6, 7]]\n# hstack(a, b) = [[1, 2, 4, 5],\n#                [ 3, 4, 6, 7]]\n\ntrain_gene_var_onehotCoding = hstack((train_gene_feature_onehotCoding,\n                                      train_variation_feature_onehotCoding))\ntest_gene_var_onehotCoding = hstack((test_gene_feature_onehotCoding,test_variation_feature_onehotCoding))\ncv_gene_var_onehotCoding = hstack((cv_gene_feature_onehotCoding,cv_variation_feature_onehotCoding))\n\ntrain_x_onehotCoding = hstack((train_gene_var_onehotCoding, train_text_feature_onehotCoding)).tocsr()\ntrain_y = np.array(list(train_df['Class']))\n\ntest_x_onehotCoding = hstack((test_gene_var_onehotCoding, test_text_feature_onehotCoding)).tocsr()\ntest_y = np.array(list(test_df['Class']))\n\ncv_x_onehotCoding = hstack((cv_gene_var_onehotCoding, cv_text_feature_onehotCoding)).tocsr()\ncv_y = np.array(list(cv_df['Class']))\n\n\ntrain_gene_var_responseCoding = np.hstack((train_gene_feature_responseCoding,train_variation_feature_responseCoding))\ntest_gene_var_responseCoding = np.hstack((test_gene_feature_responseCoding,test_variation_feature_responseCoding))\ncv_gene_var_responseCoding = np.hstack((cv_gene_feature_responseCoding,cv_variation_feature_responseCoding))\n\ntrain_x_responseCoding = np.hstack((train_gene_var_responseCoding, train_text_feature_responseCoding))\ntest_x_responseCoding = np.hstack((test_gene_var_responseCoding, test_text_feature_responseCoding))\ncv_x_responseCoding = np.hstack((cv_gene_var_responseCoding, cv_text_feature_responseCoding))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"One hot encoding features :\")\nprint(\"(number of data points * number of features) in train data = \", train_x_onehotCoding.shape)\nprint(\"(number of data points * number of features) in test data = \", test_x_onehotCoding.shape)\nprint(\"(number of data points * number of features) in cross validation data =\", cv_x_onehotCoding.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\" Response encoding features :\")\nprint(\"(number of data points * number of features) in train data = \", train_x_responseCoding.shape)\nprint(\"(number of data points * number of features) in test data = \", test_x_responseCoding.shape)\nprint(\"(number of data points * number of features) in cross validation data =\", cv_x_responseCoding.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Base Line Model\n### Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000]\ncv_log_error_array =[]\nfor i in alpha:\n    print('for alpha: ',i)\n    clf = MultinomialNB(alpha=i)\n    clf.fit(train_x_onehotCoding,train_y)\n    sig_clf = CalibratedClassifierCV(clf,method ='sigmoid')\n    sig_clf.fit(train_x_onehotCoding,train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n    cv_log_error_array.append(log_loss(cv_y,sig_clf_probs,labels = clf.classes_,eps=1e-15))\n    print('log loss: ',log_loss(cv_y,sig_clf_probs))\n    \nfig,ax = plt.subplots()\nax.plot(np.log10(alpha),cv_log_error_array,c='g')\nfor i ,txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)),(np.log10(alpha[i]),cv_log_error_array[i]))\nplt.grid()\nplt.xticks(np.log10(alpha))\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = MultinomialNB(alpha=alpha[best_alpha])\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\n\npredict_y = sig_clf.predict_proba(train_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\n# Variables that will be used in the end to make comparison table of all models\nnb_train = log_loss(y_train, sig_clf.predict_proba(train_x_onehotCoding), labels=clf.classes_, eps=1e-15)\nnb_cv = log_loss(y_cv, sig_clf.predict_proba(cv_x_onehotCoding), labels=clf.classes_, eps=1e-15)\nnb_test = log_loss(y_test, sig_clf.predict_proba(test_x_onehotCoding), labels=clf.classes_, eps=1e-15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing the model with best hyperparameter"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = MultinomialNB(alpha = alpha[best_alpha])\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\nsig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n# to avoid rounding error while multiplying probabilites we use log-probability estimates\nprint(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\nprint(\"Number of missclassified point :\", np.count_nonzero((sig_clf.predict(cv_x_onehotCoding)- cv_y))/cv_y.shape[0])\nplot_confusion_matrix(cv_y, sig_clf.predict(cv_x_onehotCoding.toarray()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Variables that will be used in the end to make comparison table of models\nnb_misclassified = (np.count_nonzero((sig_clf.predict(cv_x_onehotCoding)- cv_y))/cv_y.shape[0])*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importance, Correctly classified Point"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_point_index = 1\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices=np.argsort(-1*abs(clf.coef_))[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K Nearest Neighbour Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [x for x in range(1,100,4)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = KNeighborsClassifier(n_neighbors=i)\n    clf.fit(train_x_responseCoding, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_responseCoding, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_responseCoding)\n    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\nclf.fit(train_x_responseCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_responseCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_responseCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_responseCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_responseCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\n# Variables that will be used in the end to make comparison table of all models\nknn_train = log_loss(y_train, sig_clf.predict_proba(train_x_responseCoding), labels=clf.classes_, eps=1e-15)\nknn_cv = log_loss(y_cv, sig_clf.predict_proba(cv_x_responseCoding), labels=clf.classes_, eps=1e-15)\nknn_test = log_loss(y_test, sig_clf.predict_proba(test_x_responseCoding), labels=clf.classes_, eps=1e-15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\npredict_and_plot_confusion_matrix(train_x_responseCoding, train_y, cv_x_responseCoding, cv_y, clf)\n# Variables that will be used in the end to make comparison table of models\nknn_misclassified = (np.count_nonzero((sig_clf.predict(cv_x_responseCoding)- cv_y))/cv_y.shape[0])*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"## with Class Balancing\nalpha = [10 ** x for x in range(-6,3)]\ncv_log_error_array =[]\nfor i in alpha:\n    print('for alpha: ',i)\n    clf = SGDClassifier(class_weight='balanced',alpha=i,penalty='l2',loss='log',random_state=42)\n    clf.fit(train_x_onehotCoding,train_y)\n    sig_clf = CalibratedClassifierCV(clf,method='sigmoid')\n    sig_clf.fit(train_x_onehotCoding,train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n    cv_log_error_array.append(log_loss(cv_y,sig_clf_probs,labels=clf.classes_,eps=1e-15))\n    print('Log loss: ',log_loss(cv_y,sig_clf_probs))\n    \nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_onehotCoding)\nprint('For values of best alpha : ',alpha[best_alpha],'The train log loss is: ',log_loss(y_train,predict_y,labels=clf.classes_,eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\n# Variables that will be used in the end to make comparison table of all models\nlr_balance_train = log_loss(y_train, sig_clf.predict_proba(train_x_onehotCoding), labels=clf.classes_, eps=1e-15)\nlr_balance_cv = log_loss(y_cv, sig_clf.predict_proba(cv_x_onehotCoding), labels=clf.classes_, eps=1e-15)\nlr_balance_test = log_loss(y_test, sig_clf.predict_proba(test_x_onehotCoding), labels=clf.classes_, eps=1e-15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y, cv_x_onehotCoding, cv_y, clf)\n\n# Variables that will be used in the end to make comparison table of models\nlr_balance_misclassified = (np.count_nonzero((sig_clf.predict(cv_x_onehotCoding)- cv_y))/cv_y.shape[0])*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importance,Correctly Classified point"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from tabulate import tabulate\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], \n                    penalty='l2', loss='log', random_state=42)\nclf.fit(train_x_onehotCoding,train_y)\ntest_point_index = 1\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-1*abs(clf.coef_))[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### without class imbalancing"},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-6, 1)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_x_onehotCoding, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_onehotCoding, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\n# Variables that will be used in the end to make comparison table of all models\nlr_train = log_loss(y_train, sig_clf.predict_proba(train_x_onehotCoding), labels=clf.classes_, eps=1e-15)\nlr_cv = log_loss(y_cv, sig_clf.predict_proba(cv_x_onehotCoding), labels=clf.classes_, eps=1e-15)\nlr_test = log_loss(y_test, sig_clf.predict_proba(test_x_onehotCoding), labels=clf.classes_, eps=1e-15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y, cv_x_onehotCoding, cv_y, clf)\n\nlr_misclassified = (np.count_nonzero((sig_clf.predict(cv_x_onehotCoding)- cv_y))/cv_y.shape[0])*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_x_onehotCoding,train_y)\ntest_point_index = 1\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-1*abs(clf.coef_))[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-1*abs(clf.coef_))[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-5, 3)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for C =\", i)\n#     clf = SVC(C=i,kernel='linear',probability=True, class_weight='balanced')\n    clf = SGDClassifier( class_weight='balanced', alpha=i, penalty='l2', loss='hinge', random_state=42)\n    clf.fit(train_x_onehotCoding, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_onehotCoding, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\n# clf = SVC(C=i,kernel='linear',probability=True, class_weight='balanced')\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\n# Variables that will be used in the end to make comparison table of all models\nsvm_train = log_loss(y_train, sig_clf.predict_proba(train_x_onehotCoding), labels=clf.classes_, eps=1e-15)\nsvm_cv = log_loss(y_cv, sig_clf.predict_proba(cv_x_onehotCoding), labels=clf.classes_, eps=1e-15)\nsvm_test = log_loss(y_test, sig_clf.predict_proba(test_x_onehotCoding), labels=clf.classes_, eps=1e-15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42,class_weight='balanced')\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y,cv_x_onehotCoding,cv_y, clf)\n# Variables that will be used in the end to make comparison table of models\nsvm_misclassified = (np.count_nonzero((sig_clf.predict(cv_x_onehotCoding)- cv_y))/cv_y.shape[0])*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier\n### Hyper paramter tuning (With One hot Encoding)"},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha =[100,200,500,1000,2000]\nmax_depth =[5,10]\ncv_log_error_array=[]\nfor i in alpha:\n    for j in max_depth:\n        print('for n_estimators: ',i, ' and max depth: ',j)\n        clf = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42, n_jobs=-1)\n        clf.fit(train_x_onehotCoding, train_y)\n        sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n        sig_clf.fit(train_x_onehotCoding, train_y)\n        sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n        cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n        print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\n        \nbest_alpha = np.argmin(cv_log_error_array)\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_onehotCoding)\nprint('For values of best estimator = ', alpha[int(best_alpha/2)], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_onehotCoding)\nprint('For values of best estimator = ', alpha[int(best_alpha/2)], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_onehotCoding)\nprint('For values of best estimator = ', alpha[int(best_alpha/2)], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\n# Variables that will be used in the end to make comparison table of all models\nrf_train = log_loss(y_train, sig_clf.predict_proba(train_x_onehotCoding), labels=clf.classes_, eps=1e-15)\nrf_cv = log_loss(y_cv, sig_clf.predict_proba(cv_x_onehotCoding), labels=clf.classes_, eps=1e-15)\nrf_test = log_loss(y_test, sig_clf.predict_proba(test_x_onehotCoding), labels=clf.classes_, eps=1e-15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y,cv_x_onehotCoding,cv_y, clf)\nrf_misclassified = (np.count_nonzero((sig_clf.predict(cv_x_onehotCoding)- cv_y))/cv_y.shape[0])*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_point_index = 10\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\ntest_point_index = 1\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"-\"*50)\nget_impfeature_names(indices[:no_feature], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyper paramter tuning (With Response Coding)"},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [10,50,100,200,500,1000]\nmax_depth = [2,3,5,10]\ncv_log_error_array = []\nfor i in alpha:\n    for j in max_depth:\n        print(\"for n_estimators =\", i,\"and max depth = \", j)\n        clf = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42, n_jobs=-1)\n        clf.fit(train_x_responseCoding, train_y)\n        sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n        sig_clf.fit(train_x_responseCoding, train_y)\n        sig_clf_probs = sig_clf.predict_proba(cv_x_responseCoding)\n        cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n        print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\nbest_alpha = np.argmin(cv_log_error_array)\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/4)], criterion='gini', max_depth=max_depth[int(best_alpha%4)], random_state=42, n_jobs=-1)\nclf.fit(train_x_responseCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_responseCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_responseCoding)\nprint('For values of best alpha = ', alpha[int(best_alpha/4)], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_responseCoding)\nprint('For values of best alpha = ', alpha[int(best_alpha/4)], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_responseCoding)\nprint('For values of best alpha = ', alpha[int(best_alpha/4)], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\n# Variables that will be used in the end to make comparison table of all models\nrf_response_train = log_loss(y_train, sig_clf.predict_proba(train_x_responseCoding), labels=clf.classes_, eps=1e-15)\nrf_response_cv = log_loss(y_cv, sig_clf.predict_proba(cv_x_responseCoding), labels=clf.classes_, eps=1e-15)\nrf_response_test = log_loss(y_test, sig_clf.predict_proba(test_x_responseCoding), labels=clf.classes_, eps=1e-15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(max_depth=max_depth[int(best_alpha%4)], n_estimators=alpha[int(best_alpha/4)], criterion='gini', max_features='auto',random_state=42)\npredict_and_plot_confusion_matrix(train_x_responseCoding, train_y,cv_x_responseCoding,cv_y, clf)\n\n# Variables that will be used in the end to make comparison table of models\nrf_response_misclassified = (np.count_nonzero((sig_clf.predict(cv_x_responseCoding)- cv_y))/cv_y.shape[0])*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=hinge, penalty=l2, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=optimal, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, ])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n# read more about support vector machines with linear kernals here http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n# --------------------------------\n# default parameters \n# SVC(C=1.0, kernel=rbf, degree=3, gamma=auto, coef0=0.0, shrinking=True, probability=False, tol=0.001, \n# cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=ovr, random_state=None)\n\n# Some of methods of SVM()\n# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n# predict(X)\tPerform classification on samples in X.\n# --------------------------------\n# --------------------------------\n\n\n# read more about support vector machines with linear kernals here http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n# --------------------------------\n# default parameters \n# sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=gini, max_depth=None, min_samples_split=2, \n# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=auto, max_leaf_nodes=None, min_impurity_decrease=0.0, \n# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, \n# class_weight=None)\n\n# Some of methods of RandomForestClassifier()\n# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n# predict(X)\tPerform classification on samples in X.\n# predict_proba (X)\tPerform classification on samples in X.\n\n# some of attributes of  RandomForestClassifier()\n# feature_importances_ : array of shape = [n_features]\n# The feature importances (the higher, the more important the feature).\n# --------------------------------\n# --------------------------------\n\n\nclf1 = SGDClassifier(alpha=0.001, penalty='l2', loss='log', class_weight='balanced', random_state=0)\nclf1.fit(train_x_onehotCoding, train_y)\nsig_clf1 = CalibratedClassifierCV(clf1, method=\"sigmoid\")\n\nclf2 = SGDClassifier(alpha=1, penalty='l2', loss='hinge', class_weight='balanced', random_state=0)\nclf2.fit(train_x_onehotCoding, train_y)\nsig_clf2 = CalibratedClassifierCV(clf2, method=\"sigmoid\")\n\n\nclf3 = MultinomialNB(alpha=0.001)\nclf3.fit(train_x_onehotCoding, train_y)\nsig_clf3 = CalibratedClassifierCV(clf3, method=\"sigmoid\")\n\nsig_clf1.fit(train_x_onehotCoding, train_y)\nprint(\"Logistic Regression :  Log Loss: %0.2f\" % (log_loss(cv_y, sig_clf1.predict_proba(cv_x_onehotCoding))))\nsig_clf2.fit(train_x_onehotCoding, train_y)\nprint(\"Support vector machines : Log Loss: %0.2f\" % (log_loss(cv_y, sig_clf2.predict_proba(cv_x_onehotCoding))))\nsig_clf3.fit(train_x_onehotCoding, train_y)\nprint(\"Naive Bayes : Log Loss: %0.2f\" % (log_loss(cv_y, sig_clf3.predict_proba(cv_x_onehotCoding))))\nprint(\"-\"*50)\nalpha = [0.0001,0.001,0.01,0.1,1,10] \nbest_alpha = 999\nfor i in alpha:\n    lr = LogisticRegression(C=i)\n    sclf = StackingClassifier(classifiers=[sig_clf1, sig_clf2, sig_clf3], meta_classifier=lr, use_probas=True)\n    sclf.fit(train_x_onehotCoding, train_y)\n    print(\"Stacking Classifer : for the value of alpha: %f Log Loss: %0.3f\" % (i, log_loss(cv_y, sclf.predict_proba(cv_x_onehotCoding))))\n    log_error =log_loss(cv_y, sclf.predict_proba(cv_x_onehotCoding))\n    if best_alpha > log_error:\n        best_alpha = log_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(C=0.1)\nsclf = StackingClassifier(classifiers=[sig_clf1, sig_clf2, sig_clf3], meta_classifier=lr, use_probas=True)\nsclf.fit(train_x_onehotCoding, train_y)\n\nlog_error = log_loss(train_y, sclf.predict_proba(train_x_onehotCoding))\nprint(\"Log loss (train) on the stacking classifier :\",log_error)\n\nlog_error = log_loss(cv_y, sclf.predict_proba(cv_x_onehotCoding))\nprint(\"Log loss (CV) on the stacking classifier :\",log_error)\n\nlog_error = log_loss(test_y, sclf.predict_proba(test_x_onehotCoding))\nprint(\"Log loss (test) on the stacking classifier :\",log_error)\n\nprint(\"Number of missclassified point :\", np.count_nonzero((sclf.predict(test_x_onehotCoding)- test_y))/test_y.shape[0])\nplot_confusion_matrix(test_y=test_y, predict_y=sclf.predict(test_x_onehotCoding))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Variables that will be used in the end to make comparison table of all models\nstack_train = log_error\nstack_cv = log_error1\nstack_test = log_error2\nstack_misclassified = (np.count_nonzero((sclf.predict(test_x_onehotCoding)- test_y))/test_y.shape[0])*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from prettytable import PrettyTable\n\n# name of models\nnames = ['Naive Bayes','K-Nearest Neighbour','LR With Class Balancing',\\\n        'LR Without Class Balancing','Linear SVM',\\\n        'RF With One hot Encoding','RF With Response Coding']\n# Training loss\ntrain_loss = [nb_train,knn_train,lr_balance_train,lr_train,svm_train,rf_train,rf_response_train]\n\n# cv loss\ncv_loss = [nb_cv,knn_cv,lr_balance_cv,lr_cv,svm_cv,rf_cv,rf_response_cv]\n\n## Test loss\ntest_loss = [nb_test,knn_test,lr_balance_test,lr_test,svm_test,rf_test,rf_response_test]\n\n# Percentage Misclassified points\nmisclassified = [nb_misclassified,knn_misclassified,lr_balance_misclassified,lr_misclassified,svm_misclassified,\\\n                 rf_misclassified,rf_response_misclassified]\n\nnumbering = [1,2,3,4,5,6,7]\n\n# Initializing prettytable\nptable = PrettyTable()\nptable.add_column(\"S.NO.\",numbering)\nptable.add_column(\"MODEL\",names)\nptable.add_column(\"Train_loss\",train_loss)\nptable.add_column(\"CV_loss\",cv_loss)\nptable.add_column(\"Test_loss\",test_loss)\nptable.add_column(\"Misclassified(%)\",misclassified)\n\n# Printing the Table\nprint(ptable)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}