{"cells":[{"source":"## Description \n\nAluminum (Al), gallium (Ga), indium (In) sesquioxides are some of the most promising transparent conductors because of a combination of both large bandgap energies, which leads to optical transparency over the visible range, and high conductivities.\n\nThese alloys are described by the formula (AlxGayInz)2N O 3N; where x, y, and z can vary but are limited by the constraint x+y+z = 1. The total number of atoms in the unit cell, Ntotal = 2N+3N (where N is an integer), is typically between 5 and 100.\n\nHowever, the main limitation in the design of compounds is that identification and discovery of novel materials for targeted applications requires an examination of enormous compositional and configurational degrees of freedom (i.e., many combinations of x, y, and z).\n\nThe following information has been included:\n\n * Spacegroup (a label identifying the symmetry of the material)\n * Total number of Al, Ga, In and O atoms in the unit cell (Ntotal(Ntotal)\n * Relative compositions of Al, Ga, and In (x, y, z)\n * Lattice vectors and angles: lv1, lv2, lv3 (which are lengths given in units of angstroms (10^−10 meters) and \n * α, β, γ (which are angles in degrees between 0° and 360°)\n \nThe task for this competition is to predict two target properties:\n\n  1. Formation energy (an important indicator of the stability of a material)\n  2. Bandgap energy (an important property for optoelectronic applications)\n  \nSince they are continuous variables to be predicted that makes it a regression supervised problem.","cell_type":"markdown","metadata":{"collapsed":true,"_uuid":"cdbffcbb7f003c9dcdb14c27067dba7be8440e5a","_cell_guid":"294af655-1a6b-49ac-994e-72b0b03a599e"}},{"source":"# import libraries and Load data  \n\nWe will train and tune various regressors models through GridSearchCV to predict the best regressor.","cell_type":"markdown","metadata":{"_uuid":"901a5d4738c9aac9537ea74c380069c35eb1574b","_cell_guid":"0c80af16-4a96-4617-ac1c-c7d6b5dce160"}},{"source":"# import libraries and Load data  \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn import preprocessing\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_predict, train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error  #, mean_squared_log_error, mean_absolute_error\n\nfrom sklearn.linear_model import LinearRegression, Ridge,  RANSACRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor, BaggingRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost.sklearn import XGBRegressor\n\n\n# load data\ntrain_data = pd.read_csv('../input/nomad2018-predict-transparent-conductors/train.csv')\ntest_data = pd.read_csv('../input/nomad2018-predict-transparent-conductors/test.csv')\ntrain_data.head()\n","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_kg_hide-input":false,"_uuid":"3df7aa9a5da525c80acbd3a784bbfcc5a5f23af4","_cell_guid":"5f954fba-0cb2-4862-a333-7dafe94cd2db"}},{"source":"train_data.shape","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"f9c8a3aac7a50d35031023bf02ddaaae6e3ad34c","_cell_guid":"84df3c2c-1478-43fa-a62d-b260620824da"}},{"source":"test_data.head()","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"6f17d482afb8fe5c51641a3b68800c60a1be7ef4","_cell_guid":"68456206-4e87-4c85-bee9-4ebce2b96182"}},{"source":"test_data.shape","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"6897ed53c8715e1252211e4c2bb0a979251fb7e3","_cell_guid":"51987248-9ebf-49e8-b986-4ec8ce2983c5"}},{"source":"From the output of the head() and shape we can see that both the train_data and test_data have same features (columns) excepts the two target features  'formation_energy_ev_natom' and 'bandgap_energy_ev' missing in test_data. And thats what our goal is to predict these target columns for test_data.\n\nTrain_data and test_data has 2400 and 600 records respectively.","cell_type":"markdown","metadata":{"_uuid":"20b0e6bf90e2ede0cd844d9a320f7c34d081cbd7","_cell_guid":"5d3ff180-ee22-4cd3-ab14-46303752b289"}},{"source":"# Let's begin programing  ","cell_type":"markdown","metadata":{"_uuid":"dbb2f8c6a87ab4183626d974b57af54d57dbda81","_cell_guid":"43f3688c-60f1-42a4-8d8e-6abc95431fb4"}},{"source":"In brief : \n\n    As we observed there are not many columns or features to reduce or take out the irrelevant features. \n    As on initial glance we can say :\n           \n           1. All are numeric features.\n           2. target features are also continous numeric features so it is basically a regression problem.\n           3. the two features that can be removed before prediction are 'id' and  'number_of_total_atoms'.\n    \n    Some more observation exploratory data analysis can be done that I will do it in another post.\n    As mentioned in https://www.kaggle.com/c/nomad2018-predict-transparent-conductors many features need domain knowledge and are provided in the data set for purpose of data mining. \n    \n    With my understanding on the features, 'id' is simple series of number that identify the each rows. 'number_of_total_atoms' is combination of 'percent_atom_al', 'percent_atom_ga', and 'percent_atom_in' so that makes the feature 'number_of_total_atoms' less importand for prediction. I have less or negligible clarity on lattice vactors and lattice angle. I believe it depends on the contitutents the Al, Ga and In with oxygen and the process followed to mix these elements in required quantities that probably what different 'spacegroup' defines, such that the new transparent conductor acquired the specific lattice structure -vectors and angles.\n    \n    The code below gives some more information about each features. Like 'spacegroup' are basically of 6 types similarly 'number_of_total_atoms' can be grouped in to 6 categories. other features have quite high values which is not suggested to be categoried i think. \n     ","cell_type":"markdown","metadata":{"_uuid":"f208c51f4c62bedc086d7772fc264abc0ef9293a","_cell_guid":"4a25a7d9-0aee-48ea-970e-64f39b93cd49"}},{"source":"unique_values_distribution = []\ndef unique_col_values(df):\n    for column in df:\n        unique_values_distribution.append ((df[column].name, len(df[column].unique()), df[column].dtype ))\n        \nunique_col_values(train_data)\n\ncolumns_heading  = ['Header Name','Unique Count','Data Type']\n\ndata_distribution = pd.DataFrame.from_records(unique_values_distribution, columns=columns_heading)\ndata_distribution","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"fa61d232cfa6e39cf50c02f8d34730176a704ebc","_cell_guid":"9358aa5c-5386-47f1-a927-2737714057dc"}},{"source":"train_data[\"spacegroup\"].unique()","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"7427b9fffe7b27ecc42a296c1b96ad48473f9f6d","_cell_guid":"f4fe2be7-b956-4be9-a916-099a6ea6d768"}},{"source":"train_data[\"number_of_total_atoms\"].unique()","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"358f528deca5971d6b57a809c27fba971e56ee72","_cell_guid":"2ca186a0-a2d1-42a5-bc2c-56d447f0fc41"}},{"source":"\n### The basic overall logic :\n\n###### A. Identify and Select important features for modeling\n###### B. train and test various regressor on the selected features.\n\n  1. Split selected features into train and test data.\n  2. Apply GrideSearchCV and Pipeline as these fuctionality reduce coding and makes the code simple and more modular. Also it gives the freedom to execute various classifiers and regressors to execute at once along with cross-validation logic and many more functionality such as hyperparmeter, scoreing etc.... \n  3. Apply trained model on the above splitted test data.\n  4. Calculate and collect the 'Mean Square Error' (MSE), 'R2 Square' and 'Root Mean Square Log Error' (RMSLE) for all the trained models.\n  5. Compare the above error score results and the select the best model.\n  6. Using this selected model get the predicted output for formation and bandgap energy for the test data set.\n  7. Finally, create the submission.csv file with 'id', 'formation_energy_ev_natom'and 'bandgap_energy_ev'.\n","cell_type":"markdown","metadata":{"_uuid":"a5ae648f73ee70f9d5522f288c6effb5a4f6e2f4","_cell_guid":"3ddc476b-7d4c-40b0-92a7-e31597052a08"}},{"source":"### Data Cleanup\n\nAs mentioned lets remove 'id' and 'number_of_total_atoms' from train_data.","cell_type":"markdown","metadata":{"_uuid":"8047eccef1a4f35327dc277886c7f0f0a1f96102","_cell_guid":"d5cf7b45-ed19-4553-a852-e156e5c67729"}},{"source":"# 1. define the columns for train_data\n\ntrain_data = train_data[[ 'spacegroup', #'number_of_total_atoms', \n                         'percent_atom_al', 'percent_atom_ga',    'percent_atom_in', \n                         'lattice_vector_1_ang',     'lattice_vector_2_ang','lattice_vector_3_ang',\n                         'lattice_angle_alpha_degree','lattice_angle_beta_degree','lattice_angle_gamma_degree',\n                         'formation_energy_ev_natom','bandgap_energy_ev']]\n\ntrain_data.columns = [ 'spacegroup', #'number_of_total_atoms',                        \n                       'percent_atom_al', 'percent_atom_ga',     'percent_atom_in',  \n                       'lattice_vector_1_ang',     'lattice_vector_2_ang', 'lattice_vector_3_ang', \n                       'lattice_angle_alpha_degree', 'lattice_angle_beta_degree', 'lattice_angle_gamma_degree', \n                       'formation_energy_ev_natom','bandgap_energy_ev']\n\n","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"59e9b39c0a131977d6f1e2f7157a51aca25a4577","_cell_guid":"f0cf0a1f-a6f7-445e-bb28-3c53c6e9a44d"}},{"source":"### Data Slicing\n\nSplit the training data into train and data for formation energy and bandgap energy ","cell_type":"markdown","metadata":{"collapsed":true,"_uuid":"0528cc083bd2cbc16f72c60b3d5bb119643dae13","_cell_guid":"d631f8a7-5a52-45aa-a9f6-621202460711"}},{"source":"# 2. Separate the target from train_data and split the train_data into training and testing data.\nX_train = train_data.drop([ \"formation_energy_ev_natom\", \"bandgap_energy_ev\"], axis = 1)\n\nY_formation_energy = train_data['formation_energy_ev_natom']\nY_bandgap_energy   = train_data['bandgap_energy_ev']\n\n# \nfX_train_data, fX_test_data, fy_train_target, fy_test_target  = train_test_split(X_train, Y_formation_energy, \n                                                                                 test_size=0.25, random_state=42)\nbX_train_data, bX_test_data, by_train_target, by_test_target  = train_test_split(X_train, Y_bandgap_energy, \n                                                                                 test_size=0.25, random_state=42)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"a64c649887961839b5a86de99ffa6869a973bb36","_cell_guid":"21c7031f-ab4e-4b8d-9641-fae4965939d0"}},{"source":"### Data Training","cell_type":"markdown","metadata":{"_uuid":"818b2afd157983b707adc1e03747187ee7096a03","_cell_guid":"bd7993ea-6ac2-408e-a68d-10cf0e83181a"}},{"source":"","cell_type":"markdown","metadata":{"_uuid":"042726413e11284c11a3cda81c07437bf07724b8","_cell_guid":"eb8d5798-22b6-426e-864a-2497f027ef47"}},{"source":"lrg = LinearRegression()\nsvr = SVR()\ndtr = DecisionTreeRegressor()\nrfr = RandomForestRegressor()\ngbr = GradientBoostingRegressor()\nabr = AdaBoostRegressor()        \nbgr = BaggingRegressor()\netr = ExtraTreesRegressor()\nxgr = XGBRegressor(nthread=-1)\n\n\nregressors = {  'DTR' : dtr, 'SVR': svr,  'ABR' : abr,\n                'BGR' : bgr, 'RFR' : rfr, 'ETR': etr, 'GBR' : gbr, 'XGR' : xgr,\n                'DTRD' : dtr, 'SVRD': svr,  'ABRD' : abr,\n                'BGRD' : bgr, 'RFRD' : rfr, 'ETRD': etr, 'GBRD' : gbr, 'XGRD' : xgr} ","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"ac4d6c2b62e2473e9b2f95897e12802f57044e53","_cell_guid":"ea192583-bf51-483e-8121-bfe7745a61d4"}},{"source":"The function \"predict_evaluate\" below does the following :\n\nIt will evaluate various regression classifier using pipleline, hyperparameter and GridSearchCV\nIt will select the best performing classifier\nIt will train and predict on training data from \"train_test_split\" for both \"formation energy\" and \"bandgap energy\"\nIt will predict on training data from \"train_test_split\" for both \"formation energy\" and \"bandgap energy\"\nIt will get the error scores for all the classifers for MSE, R-Squared and Root Mean Square Log Error for comparision\nIt will return the prediction, error scores from training data and error scores from test data\n labels  = ['Clf','mean absolute error','mean square error','R2 squared', 'Mean Sq Log Error', 'Root Mean Sq Log Error']","cell_type":"markdown","metadata":{"_uuid":"ef4f89a8f0a7a3f48c34768728984ed96c15f953","_cell_guid":"3f5f3858-0429-442b-b46c-8e6579fcfd71"}},{"source":"param ={}\ndef hyper_parameters(var):\n    \n    if var == 'SVR':\n        param = { 'svr__gamma': ['auto'],                                  #[0.0001, 0.001, 0.005, 0.01, 0.1]    \n                  'svr__epsilon': [0.1],      \n                  'svr__tol': [0.001],       \n                  'svr__cache_size': [200,250,300,500] \n                }\n    elif var == 'RID':\n        param = { 'ridge__max_iter': [None],  \n                  'ridge__solver': ['auto'], \n                  'ridge__alpha': [1.0, 0.5,1.5],      \n                  'ridge__normalize': [False],       \n                  'ridge__tol': [1.0,0.001,0.01,0.1] \n                }\n    elif var == 'DTR':\n        param = { 'decisiontreeregressor__criterion': ['mse','mae'],\n                  'decisiontreeregressor__max_depth': [7],\n                  'decisiontreeregressor__max_features': ['auto', 'sqrt', 'log2'],   \n                  'decisiontreeregressor__max_leaf_nodes': [200] ,\n                  'decisiontreeregressor__min_samples_split':  [20],\n                  'decisiontreeregressor__min_samples_leaf': [7, 10,50 ]\n                } \n\n    elif var == 'RFR':\n        param = {'randomforestregressor__max_features' : ['auto'],\n                 'randomforestregressor__max_depth': [7],\n                 'randomforestregressor__n_estimators': [90,100],\n                 'randomforestregressor__min_samples_split':  [6,7,8,9]\n                }\n        \n    elif var == 'GBR':\n        param = {'gradientboostingregressor__n_estimators': [90],\n                 'gradientboostingregressor__learning_rate': [0.1],\n                 'gradientboostingregressor__max_depth': [3,7]\n                 #'gradientboostingregressor__loss': ['ls']\n                } \n        \n    elif var == 'ABR':\n        param = { #'adaboostregressor__random_state': [None],  \n                  #'adaboostregressor__base_estimator': [None],   \n                   'adaboostregressor__n_estimators': [160,170,180,190,200] , \n                   'adaboostregressor__loss': ['exponential'],   #['linear', 'square', 'exponential']  \n                   'adaboostregressor__learning_rate': [0.1] \n                    }\n    elif var == 'BGR':\n        param = { 'baggingregressor__n_estimators': [50,51,52], \n                  'baggingregressor__max_features': [9], #[7,8,9],\n                  #'baggingregressor__random_state': [None, 10,100],\n                  'baggingregressor__max_samples': [300]\n                 }\n    elif var == 'ETR':\n        param =  { #'extratreesregressor__random_state': [None,1,5],   #\n                   'extratreesregressor__criterion': ['mse'],\n                   'extratreesregressor__max_features': ['auto', 'sqrt', 'log2'], \n                   'extratreesregressor__n_estimators':[70,80,90]\n                     }    \n    elif var == 'XGR':     \n        param = { 'xgbregressor__max_depth': [3],\n                  'xgbregressor__learning_rate': [0.1],\n                  'xgbregressor__n_estimators': [80],\n                  #'xgbregressor__n_jobs': [dep]\n                  'xgbregressor__reg_lambda': [0.5],\n                  'xgbregressor__max_delta_step': [0.3]\n                  #'xgbregressor__min_child_weight': [1,2]\n                 }\n    # regressor with default parameters   \n    elif var == 'SVRD':\n            param = { }\n    elif var == 'RIDD':\n            param = { }\n    elif var == 'DTRD':\n            param = { }\n    elif var == 'RFRD':\n            param = { }\n    elif var == 'GBRD':\n            param = { }\n    elif var == 'ABRD':\n            param = { }\n    elif var == 'BGRD':\n            param = { }\n    elif var == 'ETRD':\n            param =  { }\n    elif var == 'XGRD':     \n            param = { }\n       \n    return param","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"c5321520aebb8ae83571a8bc59e10538d34f34cf","_cell_guid":"a4e7e1bd-19f8-47a0-ad14-35c181cd12e3"}},{"source":"import  time\ndef root_mean_squared_log_error(h, y): \n    \"\"\"\n    Compute the Root Mean Squared Log Error for hypthesis h and targets y\n    Args:\n        h - numpy array containing predictions with shape (n_samples, n_targets)\n        y - numpy array containing targets with shape (n_samples, n_targets)\n    \"\"\"\n    return np.sqrt(np.square(np.log(h + 1) - np.log(y + 1)).mean())\n\n\ndef collect_error_score(target, prediction):\n    meansquare_error = mean_squared_error (target, prediction)                 # Mean Squared Error  \n    r2square_error = r2_score(target, prediction)                              # R Squared  \n    rmslog_error = root_mean_squared_log_error(prediction, target)             # Root Mean Square Log Error  \n    #meanabsolute_error = mean_absolute_error (target, prediction)              # Absolute Mean Error \n    #msle = mean_squared_log_error(target, prediction)\n    \n    return ( meansquare_error, r2square_error, rmslog_error)\n    \n########    \ndef predict_evaluate(train_feature, train_target, test_feature, test_target):\n    \n    train_reg = []           # to collect the trained regressors\n    test_error_scores = []   # to collect the error scores\n    \n    print (\"==== Start training  Regressors ====\")\n    t = time.time()\n    for i, model in regressors.items():\n       \n        pipe = make_pipeline(preprocessing.StandardScaler(), model)\n        hyperparameters = hyper_parameters(i)\n        trainedmodel = GridSearchCV(pipe, hyperparameters, cv=15)\n    \n        # Fit and predict train data\n        #---------------------------\n        trainedmodel.fit(train_feature, train_target)\n        \n        print (i,' trained best score :: ',trainedmodel.best_score_)\n        print (\":::::::::::::::::::::::::::\")\n        \n        #print (i,' - ',trainedclfs.best_params_)\n        #print (trainedmodel.best_estimator_)\n        \n        # predict test data\n        pred_test = trainedmodel.predict(test_feature)\n        \n        # Get error scores on test data\n        mse, r2, rmsle = collect_error_score(test_target, pred_test)\n\n        test_error_scores.append ((i,  mse, r2, rmsle))\n        train_reg.append ((i, trainedmodel))\n        \n    print (\"==== Finished training  Regressors ====\")    \n    print (\" Total training time :  ({0:.3f} s)\\n\".format(time.time() - t) )\n    return ( test_error_scores, train_reg)\n    \ndef error_table (score, labels, sort_col ):\n    #labels  = ['Clf','mean absolute error','mean square error','R2 squared', 'Mean Sq Log Error', 'Root Mean Sq Log Error']\n    scored_df = pd.DataFrame.from_records(score, columns=labels, index = None)\n    sorted_scored = scored_df.sort_values(by = sort_col, ascending=True)\n    return sorted_scored\n    \n","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"20b9d3396ae5dfe07c46224661ca5118206b2da5","_cell_guid":"e531eb8b-29cb-4f5e-8848-3996e9c872de"}},{"source":"## Prediction and Evaluation for Formation Energy","cell_type":"markdown","metadata":{"_uuid":"0c3425d0001a3815975ec31d787178a8226e84a4","_cell_guid":"91b0f7c8-8812-4fb4-8ef6-1df37bd44282"}},{"source":"# Call \"predict_evaluate\" for Formation Energy\n# pass training and test data for Formation energy to \"predict_evaluate\"\n# \"predict_evaluate\" will return \n#      1. the classifier short initials ( like 'ETR' for ExtraTreesRegressor(), DTR for  DecisionTreeRegressor() etc...)\n#      2. training data error scores ( like mean square error  R2 squared  Root Mean Sq Log Error etc.. ) and\n#      3. test data error scores ( like mean square error  R2 squared  Root Mean Sq Log Error etc.. )    \n\nform_error_scores, trained_pred_form = predict_evaluate(fX_train_data, fy_train_target, fX_test_data, fy_test_target)   \n","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"cbc136b35ae7e3639c485f07b0112457aa9903af","_cell_guid":"bf3dfbc7-7183-4742-97bd-f1d263807b0f"}},{"source":"### Predicting error for Formation Energy","cell_type":"markdown","metadata":{"_uuid":"c0af79904dc81d480a834a40102f9de6e04befc9","_cell_guid":"9276a76b-6c0d-40b5-8bd0-ec43382ce6a3"}},{"source":"labels  = ['Regr','mean square error', 'R Squared', 'Root Mean Sq Log Error']\n#############\nprint(\"Formation Energy scores : on test data - ordered by Mean Square Error : \\n\")\nformation_energy_score = error_table (form_error_scores, labels,  'mean square error' )\nformation_energy_score\n","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"6d3aa0358bd37b37bc4fbe151e2313ee6fc1066c","_cell_guid":"888d1f7a-3311-4dcc-8327-a9f2975fe3bb"}},{"source":"It does not make any sense evaluate scores with poor values so lets extract or select top 10 best scores ","cell_type":"markdown","metadata":{"_uuid":"a3ce13973f02793ba038979d449181faed6183cf","_cell_guid":"df2f5d24-eea0-4baa-98ee-1e11fa44ea45"}},{"source":"# Select top 10 scores\nformation_energy_score_10 = formation_energy_score[0:10]\nformation_energy_score_10","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"587e2aef8a206b855bb28e5583666d706e428884","_cell_guid":"79c343e4-b112-4fbf-b138-15906f061e7d"}},{"source":"The table above gives the mean square error (MSE), R Squared and Root MEan Square Log Error (RMSLE)and ascending order of MSE, with model with lowest MSE value on the top. Which tells the model on top with lower MSE than the other models and also the adjusted R-squared is higher compared to others, it is probably a better model.  \n \nNote : A mean squared error (MSE) of zero indicates perfect skill, or no error. In our case the MSE of XGR regressor is lowest and close to zero. Similarly, R2 Square with value 1 or closest to one is better model. \n\nLet's visualise the above scores using bar graph.","cell_type":"markdown","metadata":{"_uuid":"b0b2289b8ed5157a55144727283c7a8df928c3d0","_cell_guid":"8e34d235-7f7f-4c66-8734-e0944fb18051"}},{"source":"#### Error Score distribution for Formation Energy for top 10 regressor","cell_type":"markdown","metadata":{"_uuid":"47c1b8654b0e51ad90a6efc3d575cb51c62775af","_cell_guid":"a27ec375-a8b9-4ef7-8e81-8d5917f98efc"}},{"source":"formation_energy_score_10.plot(kind='bar', ylim=(-0.20,1.0), figsize=(12,4), align='center', colormap=\"tab20\")\nplt.xticks(np.arange(10), formation_energy_score_10.Regr)\nplt.ylabel('Error Score')\nplt.title('Error Score Distribution by Regressor')\nplt.legend(bbox_to_anchor=(1.3, 0.9), loc=5, borderaxespad=0.5)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"08f45968297ecc340805f621145bc7568050ff06","_cell_guid":"eb514d78-bbd6-4be5-bd09-d0170cbd2807"}},{"source":"error_score_df = pd.DataFrame.from_records(formation_energy_score_10, columns=['Regr','mean square error','R Squared', 'Root Mean Sq Log Error' ], index = None)\n\nerror_score_df","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"83b187c4b709e7bfa105b20bcaf2d5f2a7524ea4","_cell_guid":"34a5bc29-7e07-4f4f-af04-db7920cf2cf1"}},{"source":"#### Root Mean Square Log Error for Formation Energy for top 10 regressor","cell_type":"markdown","metadata":{"_uuid":"c4fd3120d248a5ce215d45f7547cd3333a69b5a6","_cell_guid":"88d1c825-f83a-4cc8-87c9-91e11d7053b1"}},{"source":"RMSLEscore = error_score_df[['Regr','Root Mean Sq Log Error']]\n\nRMSLEscore.plot(kind='bar', ylim=None, figsize=(10,4), align='center', colormap=\"jet\") \nplt.xticks(np.arange(10), RMSLEscore.Regr) \nplt.ylabel('Error Score') \nplt.title('Root Mean Square Logistic Error - Distribution by Regressor') \nplt.legend(bbox_to_anchor=(1, 1), loc=2, borderaxespad=0.)\n","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"1455c6d5b8babaab9e8cee872740b196c60034b9","_cell_guid":"4dead70b-0341-4753-a5a9-880fba6729e8"}},{"source":"#### Mean Square Error for Formation Energy for top 10 regressor","cell_type":"markdown","metadata":{"_uuid":"952c5792460020f2652665b75f717007ecbdd455","_cell_guid":"b7ece99a-a191-4edd-a852-4ce32e41b175"}},{"source":"MSEscore = error_score_df[['Regr','mean square error']]\n\nMSEscore.plot(kind='bar', ylim=None, figsize=(10,4), align='center', colormap=\"rainbow\") \nplt.xticks(np.arange(10), MSEscore.Regr) \nplt.ylabel('Error Score') \nplt.title('Mean Square Error - Distribution by Regressor') \nplt.legend(bbox_to_anchor=(1, 1), loc=2, borderaxespad=0.)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"107b62628888d5de2dd5a648c6c5cff127cc9df9","_cell_guid":"8d1da06f-dea8-45ea-9e40-012f4ed69c7c"}},{"source":"#### R2 Square for Formation Energy for top 10 regressor","cell_type":"markdown","metadata":{"_uuid":"324936be52ec0cd7ddd123acb7b015264ef766c5","_cell_guid":"7a897cf0-7ae4-4f22-98f1-c794b050bcb6"}},{"source":"RSquarescore =error_score_df[['Regr','R Squared']]\n\nRSquarescore.plot(kind='bar', ylim=None, figsize=(10,4), align='center', colormap=\"Spectral\")\nplt.xticks(np.arange(20), RSquarescore.Regr)\nplt.ylabel('Error Score')\nplt.title('R Squared - Distribution by Regressor')\nplt.legend(bbox_to_anchor=(1, 1), loc=2, borderaxespad=0.)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"db2d3154a3179f82acf34b2989e91a2eead39872","_cell_guid":"f43bfc77-2b26-4c90-bffa-be8e2031b15c"}},{"source":"MSE_RMSLEscore =error_score_df[['Regr','mean square error','Root Mean Sq Log Error']]\n\nMSE_RMSLEscore.plot(kind='bar', ylim=None, figsize=(10,4), align='center', colormap=\"copper\")\nplt.xticks(np.arange(10), MSE_RMSLEscore.Regr)\nplt.ylabel('Error Score')\nplt.title('MSE and RMSLE - Distribution by Regressor')\nplt.legend(bbox_to_anchor=(1, 1), loc=2, borderaxespad=0.)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"29ee1e822a6666f3568cf35afad0e4268b3cf5da","_cell_guid":"a5339b07-351a-4aec-bcb6-b90134d71e78"}},{"source":"## Select Best Predictor for formation Energy","cell_type":"markdown","metadata":{"_uuid":"f0affb3c368acb9ee51ce536f1c2932f75fb308a","_cell_guid":"5597b152-afc8-45d4-9f69-83f05e0c7ee0"}},{"source":"Select the best trained regressor model which we will use to get the formation energy's predicted value for our original test data from test.csv.\n\n","cell_type":"markdown","metadata":{"_uuid":"7e4e0d8647925b91a86bbca4bf3968743dded4de","_cell_guid":"6ed2333b-8804-44e6-983c-7abe2a48686f"}},{"source":"# select the best regressor\ndef select_regressor(score_data, predictor) :\n    print (\" The Best prediction regressor with minimum MSE prediction \\n \")\n    print (\" --------------------------------------------------------- \\n \")\n    \"\"\"\n    # find the regressor initial such as 'GBR','XGR','BGRD' etc.,from \"formation_scored_tuned\" for which MSE is lowest.\n    # argmin() checks for the minimum value in the column 'mean square error' and returns the corresponing index value of the row\n    \"\"\"\n    #val = score_data.loc[score_data['mean square error'].argmin(), 'Regr']\n    val = score_data.loc[score_data['R Squared'].argmax(), 'Regr']\n    # iterate through the items in train_reg collection and compare with the minimum MSE regressor initials extracted above \n    # to select the best trained regressor\n    \n    for i in range(len(predictor)):\n        if predictor[i][0] == val:\n            print (predictor[i])\n            selected_reg = predictor[i][1]            \n    return selected_reg\n","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"f9bf5a2bebed40e887d690dfa268031d983dffd2","_cell_guid":"9294e7ba-edcc-4692-8f2c-3b8693e4e74c"}},{"source":"### Get Predicted Formation energy","cell_type":"markdown","metadata":{"_uuid":"7c610f8765091c7b5da4c7a14889ec34273ff302","_cell_guid":"f1f64e16-c08e-4ba6-8c35-4cbb07fe7321"}},{"source":"\nX_test_data = test_data.drop(['id', 'number_of_total_atoms'], axis = 1)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"cd9181fbb65d1bd51d495ee255c71f320a1c950e","_cell_guid":"3a0b1c34-3a9d-4e66-8d00-4d3e369273e4"}},{"source":"# Call the select_regressor function to get the regressor and\n# predict the formation energy for our original test_data (test.csv)\n\nselected_regressor_form = select_regressor(formation_energy_score, trained_pred_form)\n\ntest_pred_form = selected_regressor_form.predict(X_test_data)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"b106f0e32397156b117b6e29f5fcd02d25e822ea","_cell_guid":"d763030d-ec22-4436-8f2a-e8d065a962e7"}},{"source":"len (test_pred_form), test_pred_form.size ","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"a4116b3f98bca29a2fdc149a3af57ced7f3fe2fa","_cell_guid":"c9f98b82-d509-49fd-bc04-8fee6ff927d2"}},{"source":"# Evaluation for Bandgap Energy","cell_type":"markdown","metadata":{"_uuid":"44e841a79a3979828dd0e733683cd6351a01fff4","_cell_guid":"9dab1fee-ae9d-4b97-af0b-005d5450736f"}},{"source":"##############\n# Call \"predict_evaluate\" for Bandgap Energy\n# pass training and test data for Formation energy to \"predict_evaluate\"\n# \"predict_evaluate\" will return \n#      1. the classifier short initials\n#      2. training data error scores and\n#      3. test data error scores\n\ntest_error_scores, trained_pred_band   = predict_evaluate(bX_train_data, by_train_target, bX_test_data, by_test_target  )","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"2eafffb707b381d48b3fd7977917c0dd95f00db5","_cell_guid":"f797a9ac-009f-4e3b-9351-814a049f2c4d"}},{"source":"labels  = ['Regr','mean square error', 'R Squared', 'Root Mean Sq Log Error']\n\nprint(\"bandgap energy error scores on test data - ordered by mean square error : \\n\")\nbandgap_energy_score = error_table (test_error_scores, labels, 'mean square error')\nbandgap_energy_score","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"e66b22bd031e5448de91777614d3623335e7d3f3","_cell_guid":"0fb4e92f-3aad-4ce9-8b6a-9f00c2195b29"}},{"source":"# Select top 10 scores\nbandgap_energy_score_10 = bandgap_energy_score[0:10]\nbandgap_energy_score_10\n","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"41ed850258ac42dedb1c0d75048047a74953afec","_cell_guid":"c5d5974a-f177-4b89-a418-ab92df276d47"}},{"source":"\n#### Error Score distribution for Bandgap Energy for top 10 regressor","cell_type":"markdown","metadata":{"_uuid":"db6cc8930bbd85a5356307b534f9c0ce0b0e58ad","_cell_guid":"3c615d4d-cc06-4c46-95f1-a1156bfe9413"}},{"source":"bandgap_energy_score_10.plot(kind='bar', ylim=None, figsize=(12,4), align='center', colormap=\"tab20\")\nplt.xticks(np.arange(10), bandgap_energy_score_10.Regr)\nplt.ylabel('Error Score')\nplt.title('Error Score Distribution by Regressor')\nplt.legend(bbox_to_anchor=(1.3, 0.9), loc=5, borderaxespad=0.5)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"4b1504e9d8f6b26faff74bb44bd79c5da9d43064","_cell_guid":"7e41a4a4-4cf1-4ada-8a99-fa583a91f08d"}},{"source":"error_score_df = pd.DataFrame.from_records(bandgap_energy_score_10, columns=['Regr','mean square error','R Squared', 'Root Mean Sq Log Error' ], index = None)\n\nerror_score_df","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"7ec9afb95edfce234b9f381f04c5bc5f94fbbe5e","_cell_guid":"58b4a90d-c04e-4d40-990c-0e39648214ce"}},{"source":"#### Root Mean Square Log Error for Bandgap Energy for top 10 regressor","cell_type":"markdown","metadata":{"_uuid":"bf90eefaa632cb61582ba09801d2e2e3f1aac4ab","_cell_guid":"3aaa8f9a-a7fb-44d8-9930-d74cb09b97aa"}},{"source":"RMSLEscore = error_score_df[['Regr','Root Mean Sq Log Error']]\n\nRMSLEscore.plot(kind='bar', ylim=None, figsize=(10,3), align='center', colormap=\"jet\") \nplt.xticks(np.arange(10), RMSLEscore.Regr) \nplt.ylabel('Error Score') \nplt.title('Root Mean Square Logistic Error - Distribution by Regressor') \nplt.legend(bbox_to_anchor=(1, 1), loc=2, borderaxespad=0.)\n","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"66ac45437fa8b39df2c6a58689b428a82fe750f4","_cell_guid":"59423c19-2ee3-441a-adde-7879799ff4b7"}},{"source":"#### Mean Square Error for Bandgap Energy for top 10 regressor","cell_type":"markdown","metadata":{"_uuid":"1360bb4a39d97f493bf97d27f1e97808366b3877","_cell_guid":"d126a009-5d0f-45dc-a064-38d4288869d8"}},{"source":"print(\"bandgap energy error scores on test data - ordered by mean square error : \\n\")\nMSEscore = error_score_df[['Regr','mean square error']]\n\nMSEscore.plot(kind='bar', ylim=None, figsize=(10,4), align='center', colormap=\"rainbow\") \nplt.xticks(np.arange(10), MSEscore.Regr) \nplt.ylabel('Error Score') \nplt.title('Mean Square Error - Distribution by Regressor') \nplt.legend(bbox_to_anchor=(1, 1), loc=2, borderaxespad=0.)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"69d036e28a074940b76e8756262a9e988f84dced","_cell_guid":"1d2e77a2-664b-4f11-9c85-34b5dccaf587"}},{"source":"### Get Predicted Bandgap energy","cell_type":"markdown","metadata":{"_uuid":"540ae75f5f09b4a3e2a03eff1fcf9c54f9f94f2e","_cell_guid":"3d97eb22-4f3f-4835-abba-564216221518"}},{"source":"# Call the select_regressor function to get the regressor and\n# predict the bandgap energy for our original test_data (test.csv)\nselected_regressor_band = select_regressor(bandgap_energy_score, trained_pred_band)\n#\ntest_pred_band = selected_regressor_band.predict(X_test_data)\n","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"e4688943d9aa6d56bd868060ec2dcd5ea498aebb","_cell_guid":"46df0a08-1f1c-45d4-9f75-28a1aeb110f1"}},{"source":"len(test_pred_band)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"d77f598407eedc863427e5a13b84a1e65412f057","_cell_guid":"2851565c-eaa8-4ac4-810c-0a6f2698525b"}},{"source":"# The Submission file","cell_type":"markdown","metadata":{"_uuid":"d0283b6b914a02246f40ef41b01110a1b2de6209","_cell_guid":"90b2becd-8384-455f-b331-28451b328919"}},{"source":"\n","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"7c195c8ae60ead2ad9807464d66bbbcfe6f6ca79","_cell_guid":"78783507-7f75-4d21-9172-a40878f98180"}},{"source":"## Save the the output to \"submission.csv\" file formation_energy_ev_natom\tbandgap_energy_ev\n\nid=(test_data['id'])           # 'id' from test_data of test.csv\n\nsubmission_id = pd.DataFrame({ 'id' : id})\nsubmission_form = pd.DataFrame({ 'formation_energy_ev_natom': test_pred_form})  # dataframe for predict formation energy\nsubmission_band = pd.DataFrame({ 'bandgap_energy_ev': test_pred_band})          # dataframe for predict bandgap energy\nsubmission_df =  pd.concat([submission_form,submission_band],axis=1)            \nsubmission_df =  pd.concat([submission_id,submission_df],axis=1)                #dataframe with 'id', formation and bandgap energy \n# save into submission.csv\n#submission_df.to_csv('D:\\DataScienceCourse\\TensorFlow Bootcamp\\Kaggle_Predicting_Transparent_Conductors\\submission.csv', index=False)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"b12fbfa5968f3b24ca212a90b058c6e56613c0b6","_cell_guid":"6a56c8a6-bbff-4f3d-a602-c7237a4408e5"}},{"source":"","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"92f501926986ccccffe467857c9e83e1d0289744","_cell_guid":"774d4b6b-d9a7-4e76-bf9b-d271f0890670"}}],"nbformat":4,"nbformat_minor":1,"metadata":{"language_info":{"nbconvert_exporter":"python","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","version":"3.6.4","mimetype":"text/x-python","pygments_lexer":"ipython3"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}}}