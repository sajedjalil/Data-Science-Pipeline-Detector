{"nbformat":4,"cells":[{"cell_type":"markdown","metadata":{"collapsed":true},"source":"## Description \n\nAluminum (Al), gallium (Ga), indium (In) sesquioxides are some of the most promising transparent conductors because of a combination of both large bandgap energies, which leads to optical transparency over the visible range, and high conductivities.\n\nThese alloys are described by the formula (AlxGayInz)2N O 3N; where x, y, and z can vary but are limited by the constraint x+y+z = 1. The total number of atoms in the unit cell, Ntotal = 2N+3N (where N is an integer), is typically between 5 and 100.\n\nHowever, the main limitation in the design of compounds is that identification and discovery of novel materials for targeted applications requires an examination of enormous compositional and configurational degrees of freedom (i.e., many combinations of x, y, and z).\n\nThe following information has been included:\n\n * Spacegroup (a label identifying the symmetry of the material)\n * Total number of Al, Ga, In and O atoms in the unit cell (Ntotal(Ntotal)\n * Relative compositions of Al, Ga, and In (x, y, z)\n * Lattice vectors and angles: lv1, lv2, lv3 (which are lengths given in units of angstroms (10^−10 meters) and \n * α, β, γ (which are angles in degrees between 0° and 360°)\n \nThe task for this competition is to predict two target properties:\n\n  1. Formation energy (an important indicator of the stability of a material)\n  2. Bandgap energy (an important property for optoelectronic applications)\n  \nSince they are continuous variables to be predicted that makes it a regression supervised problem."},{"cell_type":"markdown","metadata":{},"source":"# import libraries and Load data  \n\nWe will train and tune various regressors models through GridSearchCV to predict the best regressor."},{"cell_type":"code","outputs":[],"metadata":{},"source":"# import libraries and Load data  \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.tree import export_graphviz\n%matplotlib inline\n\nfrom sklearn import preprocessing\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_predict, train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error  #, mean_squared_log_error, mean_absolute_error\n\n#from sklearn.linear_model import LinearRegression, Ridge,  RANSACRegressor\n#from sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor, BaggingRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost.sklearn import XGBRegressor\n#from scipy.stats import randint\n#import scipy.stats as st\n\n# load data\ntrain_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')\ntrain_data.head()\n","execution_count":1},{"cell_type":"code","outputs":[],"metadata":{},"source":"train_data.shape","execution_count":2},{"cell_type":"code","outputs":[],"metadata":{},"source":"test_data.head()","execution_count":3},{"cell_type":"code","outputs":[],"metadata":{},"source":"test_data.shape","execution_count":4},{"cell_type":"code","outputs":[],"metadata":{},"source":"train_data.columns[(train_data == 0).all()]","execution_count":5},{"cell_type":"markdown","metadata":{},"source":"From the output of the head() and shape we can see that both the train_data and test_data have same features (columns) excepts the two target features  'formation_energy_ev_natom' and 'bandgap_energy_ev' missing in test_data. And thats what our goal is to predict these target columns for test_data.\n\nTrain_data and test_data has 2400 and 600 records respectively."},{"cell_type":"markdown","metadata":{},"source":"# Let's begin programing  "},{"cell_type":"markdown","metadata":{},"source":"In brief : \n\n    As we observed there are not many columns or features to reduce or take out the irrelevant features. \n    As on initial glance we can say :\n           \n           1. All are numeric features.\n           2. target features are also continous numeric features so it is basically a regression problem.\n           3. the two features that can be removed before prediction are 'id' and  'number_of_total_atoms'.\n    \n    Some more observation exploratory data analysis can be done that I will do it in another post.\n    As mentioned in \"https://www.kaggle.com/c/nomad2018-predict-transparent-conductors\" many features need domain knowledge and are provided in the data set for purpose of data mining. \n    the description in \"https://www.kaggle.com/c/nomad2018-predict-transparent-conductors\" tells that the sum total of percentage of Al, Ga, In is 1.\n    \n    With my understanding on the features, 'id' is simple series of number that identify the each rows. 'number_of_total_atoms' is combination of 'percent_atom_al', 'percent_atom_ga', and 'percent_atom_in' so that makes the feature 'number_of_total_atoms' less importand for prediction. I have less or negligible clarity on lattice vactors and lattice angle. I believe it depends on the contitutents the Al, Ga and In with oxygen and the process followed to mix these elements in required quantities that probably what different 'spacegroup' defines, such that the new transparent conductor acquired the specific lattice structure -vectors and angles.\n    \n    The code below gives some more information about each features. Like 'spacegroup' are basically of 6 types similarly 'number_of_total_atoms' can be grouped in to 6 categories. other features have quite high values which is not suggested to be categoried i think. \n     "},{"cell_type":"code","outputs":[],"metadata":{},"source":"unique_values_distribution = []\ndef unique_col_values(df):\n    for column in df:\n        unique_values_distribution.append ((df[column].name, len(df[column].unique()), df[column].dtype ))\n        \nunique_col_values(train_data)\n\ncolumns_heading  = ['Header Name','Unique Count','Data Type']\n\ndata_distribution = pd.DataFrame.from_records(unique_values_distribution, columns=columns_heading)\ndata_distribution","execution_count":6},{"cell_type":"code","outputs":[],"metadata":{},"source":"train_data[\"spacegroup\"].unique()","execution_count":7},{"cell_type":"code","outputs":[],"metadata":{},"source":"train_data[\"number_of_total_atoms\"].unique()","execution_count":8},{"cell_type":"code","outputs":[],"metadata":{},"source":"#correlation matrix\ncorrmat = train_data.corr()\nplt.figure(figsize=(10, 10))\nsns.heatmap(corrmat, cmap='viridis');","execution_count":9},{"cell_type":"markdown","metadata":{},"source":"The correlation heat map shows almost all features are negatively correlated. Let's filter it further. "},{"cell_type":"code","outputs":[],"metadata":{},"source":"#correlation matrix\ncorrmat = train_data.corr()\n\nplt.figure(figsize=(12, 12))\n\nsns.heatmap(corrmat[(corrmat >= 0.4) | (corrmat <= -0.4)], \n            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,\n            annot=True, annot_kws={\"size\": 11}, square=True);","execution_count":10},{"cell_type":"markdown","metadata":{},"source":"From the correlation matrix we can observe that features are not strongly positively correlated except to some extent   'lattice_angle_beta_degree' with 'lattice_vector_1_ang' and 'percent_atom_al' with 'bandgap_energy_ev'"},{"cell_type":"markdown","metadata":{},"source":"\n### The basic overall logic :\n\n###### A. Identify and Select important features for modeling\n###### B. train and test various regressor on the selected features.\n\n  1. Split selected features into train and test data.\n  2. Apply GrideSearchCV and Pipeline as these fuctionality reduce coding and makes the code simple and more modular. Also it gives the freedom to execute various classifiers and regressors to execute at once along with cross-validation logic and many more functionality such as hyperparmeter, scoreing etc.... \n  3. Apply trained model on the above splitted test data.\n  4. Calculate and collect the 'Mean Square Error' (MSE), 'R2 Square' and 'Root Mean Square Log Error' (RMSLE) for all the trained models.\n  5. Compare the above error score results and the select the best model.\n  6. Using this selected model get the predicted output for formation and bandgap energy for the test data set.\n  7. Finally, create the submission.csv file with 'id', 'formation_energy_ev_natom'and 'bandgap_energy_ev'.\n"},{"cell_type":"markdown","metadata":{},"source":"### Data Cleanup\n\nAs mentioned lets remove 'id' and 'number_of_total_atoms' from train_data."},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"# 1. define the columns for train_data\n\ntrain_data = train_data[[ 'spacegroup',                 'number_of_total_atoms', \n                          'percent_atom_al',            'percent_atom_ga',            'percent_atom_in', \n                          'lattice_vector_1_ang',       'lattice_vector_2_ang',       'lattice_vector_3_ang', \n                          'lattice_angle_alpha_degree', 'lattice_angle_beta_degree',  'lattice_angle_gamma_degree',\n                          'formation_energy_ev_natom',  'bandgap_energy_ev'\n                        ]]\n\ntrain_data.columns = [    'spacegroup',                 'number_of_total_atoms', \n                          'percent_atom_al',            'percent_atom_ga',            'percent_atom_in', \n                          'lattice_vector_1_ang',       'lattice_vector_2_ang',       'lattice_vector_3_ang', \n                          'lattice_angle_alpha_degree', 'lattice_angle_beta_degree',  'lattice_angle_gamma_degree',\n                          'formation_energy_ev_natom',  'bandgap_energy_ev'\n                        ]\n\n# 2. define the columns for test_data\n\ntest_data = test_data[[   'spacegroup',                 'number_of_total_atoms', \n                          'percent_atom_al',            'percent_atom_ga',            'percent_atom_in', \n                          'lattice_vector_1_ang',       'lattice_vector_2_ang',       'lattice_vector_3_ang', \n                          'lattice_angle_alpha_degree', 'lattice_angle_beta_degree',  'lattice_angle_gamma_degree'\n                        ]]\n\ntest_data.columns = [     'spacegroup',                 'number_of_total_atoms', \n                          'percent_atom_al',            'percent_atom_ga',            'percent_atom_in', \n                          'lattice_vector_1_ang',       'lattice_vector_2_ang',       'lattice_vector_3_ang', \n                          'lattice_angle_alpha_degree', 'lattice_angle_beta_degree',  'lattice_angle_gamma_degree'\n                        ]","execution_count":11},{"cell_type":"markdown","metadata":{"collapsed":true},"source":"### Feature Importance and Selection Using XGBoost\n\nSplit the training data into train and data for formation energy and bandgap energy "},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"# 3. Separate the target from train_data and split the train_data into training and testing data.\nX_train = train_data.drop([ \"formation_energy_ev_natom\", \"bandgap_energy_ev\"], axis = 1)\n\nY_formation_energy = train_data['formation_energy_ev_natom']\nY_bandgap_energy   = train_data['bandgap_energy_ev']\n\n# \nfX_train_data, fX_test_data, fy_train_target, fy_test_target  = train_test_split(X_train, Y_formation_energy, \n                                                                                 test_size=0.25, random_state=101)\nbX_train_data, bX_test_data, by_train_target, by_test_target  = train_test_split(X_train, Y_bandgap_energy, \n                                                                                 test_size=0.25, random_state=101)","execution_count":12},{"cell_type":"code","outputs":[],"metadata":{},"source":"# 4. Feature Importance\n\nimport xgboost as xgb\n\nxgr = XGBRegressor()\nclassifier = xgb.sklearn.XGBRegressor(nthread=-1, seed=42)\n# Feature Importance : for Formation Energy\nxgr.fit(fX_train_data, fy_train_target)\nplt.figure(figsize=(20,15))\nxgb.plot_importance(xgr, ax=plt.gca())","execution_count":13},{"cell_type":"code","outputs":[],"metadata":{},"source":"# Feature Importance : for Bandgap Energy\nxgr.fit(bX_train_data, by_train_target)\nplt.figure(figsize=(20,15))\nxgb.plot_importance(xgr, ax=plt.gca())","execution_count":14},{"cell_type":"code","outputs":[],"metadata":{},"source":"\nplt.figure(figsize=(15,15))\nxgb.plot_tree(xgr, num_trees=7,ax=plt.gca())\n","execution_count":15},{"cell_type":"markdown","metadata":{},"source":"### Select relevant features for training the regressors"},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"#  Based on the important features for both Formation Energy and Bandgap Energy, we can remove 'spacegroup',  \n#  'number_of_total_atoms' and 'percent_atom_ga'.\n\n# And split the train data for Formation and Bandgap energy\n\n# 5. Re-define the columns for train_data\n\ntrain_data = train_data[[ 'percent_atom_al',            'percent_atom_in', \n                          'lattice_vector_1_ang',       'lattice_vector_2_ang',       'lattice_vector_3_ang', \n                          'lattice_angle_alpha_degree', 'lattice_angle_beta_degree',  'lattice_angle_gamma_degree',\n                          'formation_energy_ev_natom',  'bandgap_energy_ev'\n                        ]]\n\ntrain_data.columns = [    'percent_atom_al',            'percent_atom_in', \n                          'lattice_vector_1_ang',       'lattice_vector_2_ang',       'lattice_vector_3_ang', \n                          'lattice_angle_alpha_degree', 'lattice_angle_beta_degree',  'lattice_angle_gamma_degree',\n                          'formation_energy_ev_natom',  'bandgap_energy_ev'\n                        ]\n\n# 6. Re-define the columns for test_data\n\ntest_data = test_data[[   'percent_atom_al',            'percent_atom_in', \n                          'lattice_vector_1_ang',       'lattice_vector_2_ang',       'lattice_vector_3_ang', \n                          'lattice_angle_alpha_degree', 'lattice_angle_beta_degree',  'lattice_angle_gamma_degree'\n                        ]]\n\ntest_data.columns = [     'percent_atom_al',            'percent_atom_in', \n                          'lattice_vector_1_ang',       'lattice_vector_2_ang',       'lattice_vector_3_ang', \n                          'lattice_angle_alpha_degree', 'lattice_angle_beta_degree',  'lattice_angle_gamma_degree'\n                        ]\n\n# 7. Separate the target from train_data and split the train_data into training and testing data.\nX_train = train_data.drop([ \"formation_energy_ev_natom\", \"bandgap_energy_ev\"], axis = 1)\n\nY_formation_energy = train_data['formation_energy_ev_natom']\nY_bandgap_energy   = train_data['bandgap_energy_ev']\n\n# \nfX_train_data, fX_test_data, fy_train_target, fy_test_target  = train_test_split(X_train, Y_formation_energy, \n                                                                                 test_size=0.25, random_state=101)\nbX_train_data, bX_test_data, by_train_target, by_test_target  = train_test_split(X_train, Y_bandgap_energy, \n                                                                                 test_size=0.25, random_state=101)","execution_count":16},{"cell_type":"markdown","metadata":{},"source":"### Data Training"},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"\ndtr = DecisionTreeRegressor()\nrfr = RandomForestRegressor()\ngbr = GradientBoostingRegressor()\nabr = AdaBoostRegressor()        \nbgr = BaggingRegressor()\netr = ExtraTreesRegressor()\nxgr = XGBRegressor()  \n\n","execution_count":17},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"\nregressors = {  'ABR' : abr,  'BGR' : bgr, 'DTR' : dtr, 'ETR': etr, 'GBR' : gbr, 'XGR' : xgr  }  \n\nparam ={}\nrandom_state = 101\nreg = DecisionTreeRegressor(criterion='mse', max_depth=3, random_state = random_state, min_samples_leaf= 40)\n\n\ndef hyper_parameters(var):\n    \n    if  var == 'DTR':\n        param = { #'decisiontreeregressor__criterion': ['mse','mae'],\n                  'decisiontreeregressor__max_depth': [3],\n                  #'decisiontreeregressor__max_features': ['auto', 'sqrt', 'log2'],   \n                  #'decisiontreeregressor__max_leaf_nodes': [250] ,\n                  'decisiontreeregressor__min_samples_split':  [10],\n                  'decisiontreeregressor__min_samples_leaf': [40 ],\n                  #'decisiontreeregressor__splitter',\n                  #'decisiontreeregressor__min_impurity_decrease':[50],\n                  'decisiontreeregressor__random_state': [random_state]\n                }   \n\n    elif var == 'RFR':\n        param = {#'randomforestregressor__max_features' : ['auto'],\n                 'randomforestregressor__max_depth': [3],\n                 'randomforestregressor__n_estimators': [300],\n                 'randomforestregressor__min_samples_split':  [10],\n                 'randomforestregressor__min_samples_leaf': [40 ],\n                 #'randomforestregressor__min_impurity_decrease':[50],\n                 'randomforestregressor__random_state' :[random_state],\n                 'randomforestregressor__oob_score':[True]\n                }\n        \n    elif var == 'GBR':\n        param = {'gradientboostingregressor__n_estimators': [300],\n                 'gradientboostingregressor__learning_rate': [0.1],\n                 'gradientboostingregressor__max_depth': [3],\n                 #'gradientboostingregressor__loss': ['ls'],\n                 'gradientboostingregressor__min_samples_split':  [10],\n                 #'gradientboostingregressor__max_leaf_nodes'\n                 'gradientboostingregressor__min_samples_leaf': [40 ],\n                 'gradientboostingregressor__max_features': ['auto'],\n                 #'gradientboostingregressor__alpha'\n                 'gradientboostingregressor__random_state' :[random_state]\n                } \n        \n    elif var == 'ABR':\n        param = {  'adaboostregressor__random_state': [random_state],  \n                   'adaboostregressor__base_estimator': [reg],   \n                   'adaboostregressor__n_estimators': [300] , \n                   'adaboostregressor__loss': ['exponential'],   #['linear', 'square', 'exponential']  \n                   'adaboostregressor__learning_rate': [0.1] \n                    }\n    elif var == 'BGR':\n        param = { 'baggingregressor__n_estimators': [300], \n                  #'baggingregressor__max_features': [9], #[7,8,9],\n                  'baggingregressor__random_state': [random_state],\n                  'baggingregressor__n_jobs':[-1],\n                  'baggingregressor__bootstrap':[True],\n                  #'baggingregressor__base_estimator':[rfr],\n                  'baggingregressor__oob_score':[True],\n\n                  'baggingregressor__max_samples': [325]\n                 }\n    elif var == 'ETR':\n        param =  { 'extratreesregressor__random_state': [random_state],   \n                   #'extratreesregressor__criterion': ['mse','friedman_mse'],\n                   #'extratreesregressor__max_features': ['auto', 'sqrt', 'log2'], \n                   'extratreesregressor__max_depth':[3],\n                   'extratreesregressor__n_estimators':[300],\n                   'extratreesregressor__min_samples_split' :[10],\n                   'extratreesregressor__oob_score':[True],                 \n                    'extratreesregressor__bootstrap':[True],\n                    'extratreesregressor__min_samples_leaf':[40]\n                     }    \n    elif var == 'XGR':     \n        param = { 'xgbregressor__max_depth': [3],\n                  'xgbregressor__learning_rate': [0.1],\n                  'xgbregressor__n_estimators': [300],\n                  #'xgbregressor__n_jobs': [-1],\n                  'xgbregressor__reg_lambda': [0.5],\n                  'xgbregressor__max_delta_step': [0.3],\n                  #'xgbregressor__min_child_weight': [1,2],\n                  'xgbregressor__seed': [42],\n                  'xgbregressor__random_state':  [random_state]\n                 \n                 }\n       \n    return param","execution_count":18},{"cell_type":"markdown","metadata":{},"source":"### Some Function used:\n1. \"predict_evaluate\"\n2. \"root_mean_squared_log_error\"\n3. \"collect_error_score\"\n4. \"error_table\"\n\n\n1. The function \"predict_evaluate\" below does the following :\n    It will evaluate various regression classifier using pipleline, hyperparameter and GridSearchCV\n    It will select the best performing classifier\n    It will train and predict on training data from \"train_test_split\" for both \"formation energy\" and \"bandgap energy\"\n    It will predict on training data from \"train_test_split\" for both \"formation energy\" and \"bandgap energy\"\n    It will get the error scores for all the classifers for MSE, R-Squared and Root Mean Square Log Error for comparision\n    It will return the prediction, error scores from training data and error scores from test data\n     labels  = ['Clf','mean absolute error','mean square error','R2 squared', 'Mean Sq Log Error', 'Root Mean Sq Log Error']\n    I have only used MSE, R2 and RMSLE. With small modification in appropriate places in the code other error scores can also be used. \n2.  The function \"root_mean_squared_log_error\" below does the following :\n    It just calculate and returns RMSLE value.\n3. The function \"collect_error_score\" below does the following :\n    Basically, this function is used to collect the error scores that can be collected in an array for further observations, comparision and selection.  \n4. The function \"error_table\" below does the following :\n    It just uses the error scores for train data and test data and convert them into a dataframe."},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"import  time\nfrom sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\ndef root_mean_squared_log_error(h, y): \n    \"\"\"\n    Compute the Root Mean Squared Log Error for hypthesis h and targets y\n    Args:\n        h - numpy array containing predictions with shape (n_samples, n_targets)\n        y - numpy array containing targets with shape (n_samples, n_targets)\n    \"\"\"\n    return np.sqrt(np.square(np.log(h + 1) - np.log(y + 1)).mean())\n\n\ndef collect_error_score(target, prediction):\n    meansquare_error = mean_squared_error (target, prediction)                 # Mean Squared Error  \n    r2square_error = r2_score(target, prediction)                              # R Squared  \n    rmslog_error = root_mean_squared_log_error(prediction, target)             # Root Mean Square Log Error  \n    #meanabsolute_error = mean_absolute_error (target, prediction)              # Absolute Mean Error \n    #msle = mean_squared_log_error(target, prediction)\n    \n    return ( meansquare_error, r2square_error, rmslog_error)\n    \n########    \ndef predict_evaluate(train_feature, train_target, test_feature, test_target):\n    \n    train_reg = []           # to collect the trained regressors\n    test_error_scores = []   # to collect the error scores\n    train_error_scores = []   # to collect the error scores\n    \n    print (\"==== Start training  Regressors ====\")\n    t = time.time()\n    for i, model in regressors.items():\n        it = time.time()\n        pipe = make_pipeline(preprocessing.PolynomialFeatures(degree = 4), model)    #StandardScaler, MinMaxScaler\n        hyperparameters = hyper_parameters(i)\n        trainedmodel = GridSearchCV(pipe, hyperparameters, n_jobs = -1, verbose = 1, scoring = 'r2', cv=5)\n        # Fit and predict train data\n        #---------------------------\n        trainedmodel.fit(train_feature, train_target)\n        \n        print (i,' trained best score :: ',trainedmodel.best_score_)\n        print (\":::::::::::::::::::::::::::\")\n        \n        #print (i,' - ',trainedclfs.best_params_)\n        #print (trainedmodel.best_estimator_)\n        \n         # predict train data\n        pred_train = trainedmodel.predict(train_feature)\n        \n        # Get error scores on train data\n        tmse, tr2, trmsle = collect_error_score(train_target, pred_train)\n        train_error_scores.append ((i,  tmse, tr2, trmsle))\n        # predict test data\n        pred_test = trainedmodel.predict(test_feature)\n        \n        # Get error scores on test data\n        mse, r2, rmsle = collect_error_score(test_target, pred_test)\n\n        test_error_scores.append ((i,  mse, r2, rmsle))\n        train_reg.append ((i, trainedmodel))\n        print (i, \" :  Training time :  ({0:.3f} s)\\n\".format(time.time() - it) )\n    print (\"==== Finished training  Regressors ====\\n\")    \n    print (\" Total training time :  ({0:.3f} s)\\n\".format(time.time() - t) )\n    return ( train_error_scores,test_error_scores, train_reg)\n    \ndef error_table (score, labels, sort_col ):\n    #labels  = ['Clf','mean absolute error','mean square error','R2 squared', 'Mean Sq Log Error', 'Root Mean Sq Log Error']\n    scored_df = pd.DataFrame.from_records(score, columns=labels, index = None)\n    sorted_scored = scored_df.sort_values(by = sort_col, ascending=False)\n    return sorted_scored\n    \n","execution_count":19},{"cell_type":"markdown","metadata":{},"source":"## Prediction and Evaluation for Formation Energy"},{"cell_type":"code","outputs":[],"metadata":{},"source":"# Call \"predict_evaluate\" for Formation Energy\n# pass training and test data for Formation energy to \"predict_evaluate\"\n# \"predict_evaluate\" will return \n#      1. the classifier short initials ( like 'ETR' for ExtraTreesRegressor(), DTR for  DecisionTreeRegressor() etc...)\n#      2. training data error scores ( like mean square error  R2 squared  Root Mean Sq Log Error etc.. ) and\n#      3. test data error scores ( like mean square error  R2 squared  Root Mean Sq Log Error etc.. )    \n\ntrain_form_error_scores,form_error_scores, trained_pred_form = predict_evaluate(fX_train_data, fy_train_target, fX_test_data, fy_test_target)   \n","execution_count":20},{"cell_type":"markdown","metadata":{},"source":"## Formation Energy: Select scores for Train and Test Regressor Model"},{"cell_type":"markdown","metadata":{},"source":"### Error Scores for Formation Energy on training data"},{"cell_type":"code","outputs":[],"metadata":{},"source":"labels  = ['train Regr','train MSE', 'train R2', 'train RMSLE']\n\n#############\nprint(\"Formation Energy scores : on test data - ordered by train R Squared : \\n\")\ntrain_formation_energy_score = error_table (train_form_error_scores, labels,  'train R2' )\ntrain_formation_energy_score\n#train_error_score_df = pd.DataFrame.from_records(train_formation_energy_score, columns=['train Regr','train mean square error', 'train R Squared', 'train Root Mean Sq Log Error' ], index = None)\n#train_error_score_df","execution_count":21},{"cell_type":"markdown","metadata":{},"source":"### ### Error Scores for Formation Energy on testing data"},{"cell_type":"code","outputs":[],"metadata":{},"source":"labels  = ['test Regr','test MSE', 'test R2', 'test RMSLE']\n#############\nprint(\"Formation Energy scores : on test data - ordered by test R Squared : \\n\")\ntest_formation_energy_score = error_table (form_error_scores, labels,  'test R2' )\ntest_formation_energy_score\n","execution_count":22},{"cell_type":"markdown","metadata":{},"source":"The table above gives the mean square error (MSE), R Squared and Root MEan Square Log Error (RMSLE)and ascending order of MSE, with model with lowest MSE value on the top. Which tells the model on top with lower MSE than the other models and also the adjusted R-squared is higher compared to others, it is probably a better model.  \n \nNote : A mean squared error (MSE) of zero indicates perfect skill, or no error. Similarly, R2 Square with value 1 or closest to one is better model. \n\nLet's visualise the above scores using bar graph."},{"cell_type":"markdown","metadata":{},"source":"###  Compare training and test scores for overfitting,under fitting and good fitting for Formation Energy "},{"cell_type":"code","outputs":[],"metadata":{},"source":"score_df = pd.concat ([train_formation_energy_score, test_formation_energy_score], axis = 1)\nscore_df\n# diff is the difference between train R2 and Test R2\ndiff = score_df['train R2']- score_df['test R2']\nscore_df = pd.concat ([score_df, diff], axis = 1)\nscore_df = score_df.rename(columns={0:'R2 Diff'})\nscore_df = score_df.sort_values(by = 'test R2', ascending=False)\nscore_df","execution_count":23},{"cell_type":"markdown","metadata":{},"source":"#### R2 Score of Train and Test data for Formation Energy"},{"cell_type":"code","outputs":[],"metadata":{},"source":"R2score = score_df[['train Regr','train R2','test R2']]\n\nR2score.plot(kind='bar', ylim=None, figsize=(10,4), align='center', colormap=\"jet\") \nplt.xticks(np.arange(6), R2score['train Regr']) \nplt.ylabel('Error Score') \nplt.title('Formation Energy : R2 Score - Distribution by Regressor') \nplt.legend(bbox_to_anchor=(1, 1), loc=2, borderaxespad=0.)\n","execution_count":24},{"cell_type":"markdown","metadata":{},"source":"#### MSE Score of Train and Test data for Formation Energy"},{"cell_type":"code","outputs":[],"metadata":{},"source":"MSE_score =score_df[['train Regr','train MSE','test MSE']]\n\nMSE_score.plot(kind='bar', ylim=None, figsize=(10,4), align='center', colormap=\"copper\")\nplt.xticks(np.arange(6), MSE_score['train Regr'])\nplt.ylabel('Error Score')\nplt.title('Formation Energy : MSE - Distribution by Regressor')\nplt.legend(bbox_to_anchor=(1, 1), loc=2, borderaxespad=0.)","execution_count":25},{"cell_type":"markdown","metadata":{},"source":"#### RMSLE Error Score of Train and Test data for Formation Energy"},{"cell_type":"code","outputs":[],"metadata":{},"source":"\nRMSLE_score =score_df[['train Regr','train RMSLE','test RMSLE']]\n\nRMSLE_score.plot(kind='bar', ylim=None, figsize=(10,4), align='center', colormap=\"tab20\")\nplt.xticks(np.arange(6), RMSLE_score['train Regr'])\nplt.ylabel('RMSLE Error Score')\nplt.title('RMSLE - Distribution by Regressor')\nplt.legend(bbox_to_anchor=(1, 1), loc=2, borderaxespad=0.)","execution_count":26},{"cell_type":"markdown","metadata":{},"source":"# Evaluation for Bandgap Energy"},{"cell_type":"code","outputs":[],"metadata":{},"source":"##############\n# Call \"predict_evaluate\" for Bandgap Energy\n# pass training and test data for Formation energy to \"predict_evaluate\"\n# \"predict_evaluate\" will return \n#      1. the classifier short initials\n#      2. training data error scores and\n#      3. test data error scores\n\ntrain_band_error_scores,test_band_error_scores, trained_pred_band   = predict_evaluate(bX_train_data, by_train_target, bX_test_data, by_test_target  )","execution_count":27},{"cell_type":"markdown","metadata":{},"source":"## Bandgap Energy: Select scores for Train and Test Regressor Model"},{"cell_type":"code","outputs":[],"metadata":{},"source":"labels  = ['train Regr','train MSE', 'train R2', 'train RMSLE']\n\nprint(\"Bandgap Energy error scores on test data - ordered by Train R2 : \\n\")\ntrain_bandgap_energy_score = error_table (train_band_error_scores, labels, 'train R2')\ntrain_bandgap_energy_score\n","execution_count":28},{"cell_type":"code","outputs":[],"metadata":{},"source":"labels  = ['test Regr','test MSE', 'test R2', 'test RMSLE']\n#############\nprint(\"Bandgap Energy error scores on test data - ordered by test R Squared : \\n\")\ntest_bandgap_energy_score = error_table (test_band_error_scores, labels,  'test R2' )\ntest_bandgap_energy_score\n","execution_count":29},{"cell_type":"markdown","metadata":{},"source":"###  Compare training and test scores for overfitting,under fitting and good fitting for Bandgap Energy"},{"cell_type":"code","outputs":[],"metadata":{},"source":"score_df = pd.concat ([train_bandgap_energy_score, test_bandgap_energy_score], axis = 1)\nscore_df = score_df.sort_values(by = 'test R2', ascending=False)\nscore_df","execution_count":30},{"cell_type":"markdown","metadata":{},"source":"#### Root Mean Square Log Error for Bandgap Energy for top 10 regressor"},{"cell_type":"code","outputs":[],"metadata":{},"source":"R2score = score_df[['train Regr','train R2','test R2']]\n\nR2score.plot(kind='bar', ylim=None, figsize=(10,4), align='center', colormap=\"jet\") \nplt.xticks(np.arange(6), R2score['train Regr']) \nplt.ylabel('Error Score') \nplt.title('Bandgap Energy : R2 Score - Distribution by Regressor') \nplt.legend(bbox_to_anchor=(1, 1), loc=2, borderaxespad=0.)\n","execution_count":31},{"cell_type":"markdown","metadata":{},"source":"#### Mean Square Error for Bandgap Energy for top 10 regressor"},{"cell_type":"code","outputs":[],"metadata":{},"source":"MSE_score =score_df[['train Regr','train MSE','test MSE']]\n\nMSE_score.plot(kind='bar', ylim=None, figsize=(10,4), align='center', colormap=\"copper\")\nplt.xticks(np.arange(6), MSE_score['train Regr'])\nplt.ylabel('Error Score')\nplt.title('Bandgap Energy : MSE - Distribution by Regressor')\nplt.legend(bbox_to_anchor=(1, 1), loc=2, borderaxespad=0.)","execution_count":32},{"cell_type":"code","outputs":[],"metadata":{},"source":"\nRMSLE_score =score_df[['train Regr','train RMSLE','test RMSLE']]\n\nRMSLE_score.plot(kind='bar', ylim=None, figsize=(10,4), align='center', colormap=\"tab20\")\nplt.xticks(np.arange(6), RMSLE_score['train Regr'])\nplt.ylabel('RMSLE Error Score')\nplt.title('Bandgap Energy : RMSLE - Distribution by Regressor')\nplt.legend(bbox_to_anchor=(1, 1), loc=2, borderaxespad=0.)","execution_count":33},{"cell_type":"markdown","metadata":{},"source":"## Select Best Predictor for formation Energy"},{"cell_type":"markdown","metadata":{},"source":"Select the best trained regressor model which we will use to get the formation energy's predicted value for our original test data from test.csv.\n\n"},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"# select the best regressor\nX_test_data = test_data #.drop(['id', 'number_of_total_atoms'], axis = 1)\n\ndef select_regressor(score_data, predictor) :\n    print (\" The Best prediction regressor with minimum MSE prediction \\n \")\n    print (\" --------------------------------------------------------- \\n \")\n    \"\"\"\n    # find the regressor initial such as 'GBR','XGR','BGRD' etc.,from \"formation_scored_tuned\" for which MSE is lowest.\n    # argmin() checks for the minimum value in the column 'mean square error' and returns the corresponing index value of the row\n    \"\"\"\n    #val = score_data.loc[score_data['mean square error'].argmin(), 'Regr']\n    val = score_data.loc[score_data['test R2'].argmax(), 'test Regr']\n    # iterate through the items in train_reg collection and compare with the minimum MSE regressor initials extracted above \n    # to select the best trained regressor\n    #val ='ETR'\n    for i in range(len(predictor)):\n        if predictor[i][0] == val:\n            print (predictor[i])\n            selected_reg = predictor[i][1]            \n    return selected_reg\n","execution_count":34},{"cell_type":"markdown","metadata":{},"source":"### Get Predicted Formation energy"},{"cell_type":"code","outputs":[],"metadata":{},"source":"# Call the select_regressor function to get the regressor and\n# predict the formation energy for our original test_data (test.csv)\n\nselected_regressor_form = select_regressor(test_formation_energy_score, trained_pred_form)\n\ntest_pred_form = selected_regressor_form.predict(X_test_data)","execution_count":35},{"cell_type":"code","outputs":[],"metadata":{},"source":"len(test_pred_form)","execution_count":36},{"cell_type":"markdown","metadata":{},"source":"### Get Predicted Bandgap energy"},{"cell_type":"code","outputs":[],"metadata":{},"source":"# Call the select_regressor function to get the regressor and\n# predict the bandgap energy for our original test_data (test.csv)\nselected_regressor_band = select_regressor(test_bandgap_energy_score, trained_pred_band)\n#\ntest_pred_band = selected_regressor_band.predict(X_test_data)\n","execution_count":37},{"cell_type":"code","outputs":[],"metadata":{},"source":"len(test_pred_band)","execution_count":38},{"cell_type":"markdown","metadata":{},"source":"# The Submission file"},{"cell_type":"code","outputs":[],"metadata":{},"source":"\ntest_csv = pd.read_csv('../input/test.csv')","execution_count":40},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"## Save the the output to \"submission.csv\" file formation_energy_ev_natom\tbandgap_energy_ev\n\nid=(test_csv['id'])           # 'id' from test_data of test.csv\n\nsubmission_id = pd.DataFrame({ 'id' : id})\nsubmission_form = pd.DataFrame({ 'formation_energy_ev_natom': test_pred_form})  # dataframe for predict formation energy\nsubmission_band = pd.DataFrame({ 'bandgap_energy_ev': test_pred_band})          # dataframe for predict bandgap energy\nsubmission_df =  pd.concat([submission_form,submission_band],axis=1)            \nsubmission_df =  pd.concat([submission_id,submission_df],axis=1)                #dataframe with 'id', formation and bandgap energy \n# save into submission.csv\n# submission_df.to_csv('..\\submission.csv', index=False)","execution_count":42},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"","execution_count":null}],"metadata":{"language_info":{"name":"python","file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python","nbconvert_exporter":"python","version":"3.5.4","codemirror_mode":{"version":3,"name":"ipython"}},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"nbformat_minor":1}