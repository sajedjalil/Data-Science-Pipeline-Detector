{"metadata":{"language_info":{"pygments_lexer":"ipython3","file_extension":".py","name":"python","version":"3.6.3","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"execution_count":null,"cell_type":"code","outputs":[],"source":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nprint(train.shape)\ntrain.head(10)","metadata":{"_cell_guid":"e7bac91a-e9ef-4ecb-898c-4c54e171c0b9","_uuid":"fe7e1b8d6bfb4c0036409d1a5c81b913f575f78a"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"print(test.shape)\ntest.head(10)","metadata":{"_cell_guid":"365ba4a8-da01-4f2a-9e44-ec409d6fe4a0","_uuid":"a16cb07eee2ecbed439d255201526f4f36558823"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"target_fe = np.log1p(train.formation_energy_ev_natom)\ntarget_be = np.log1p(train.bandgap_energy_ev)\ndel train['formation_energy_ev_natom'], train['bandgap_energy_ev'], train['id'], test['id']","metadata":{"_cell_guid":"b30a8469-2f91-4347-85db-2c6ba92aee6d","collapsed":true,"_uuid":"ff945bd7d76902046d18587892675b2c8b7c31e9"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"sorted(train['spacegroup'].unique())","metadata":{"_cell_guid":"bd2e7bde-5e72-4c5e-b3ab-e072816e8497","_uuid":"bd3238a3b5d5ced3f5a5aae10159dbc0f33af2fc"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"sorted(test['spacegroup'].unique())","metadata":{"_cell_guid":"2473a1f6-8691-4aae-bc93-503c537fec62","_uuid":"a274c6fa14e9ba2d7886fd2c3a829987b3b73aad"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"train = pd.concat([train.drop(['spacegroup'], axis=1), \n                   pd.get_dummies(train['spacegroup'], prefix='SG')], axis=1)\ntest = pd.concat([test.drop(['spacegroup'], axis=1), \n                   pd.get_dummies(test['spacegroup'], prefix='SG')], axis=1)","metadata":{"_cell_guid":"642f90a4-a646-463b-a852-5b6f80e62f29","collapsed":true,"_uuid":"b630a91694589af7c989d776c8e56ae9e6132c27"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"import lightgbm as lgb\nimport multiprocessing\n\ndef cv_train_model(X, y, \n                   verbose_eval=None, \n                   early_stopping_rounds=None,\n                   params=None):\n    if type(y) is pd.core.frame.DataFrame:\n        y = y.values.ravel()\n    dstrain = lgb.Dataset(X, label=y)\n    max_boost_round = 4000\n    if params is None:\n        lgb_params = {\n            'objective': 'regression_l2',\n            'learning_rate': 0.008,\n            'num_threads': 4,#multiprocessing.cpu_count(),\n            'max_depth': 4,\n            'min_data_in_leaf': 23,\n            'feature_fraction': 0.93,\n            'bagging_fraction': 0.93,\n            'bagging_freq': 1,\n            'lambda_l2': 1e2,\n            'metric': ['mse']\n        }\n    print('lgb cv and training...')\n    if verbose_eval is None:\n        verbose_eval = int(max_boost_round/30)\n    if early_stopping_rounds is None:\n        early_stopping_rounds = int(max_boost_round/10)\n    cv_lgb = lgb.cv(lgb_params, dstrain,\n                    num_boost_round=max_boost_round,\n                    nfold=10,\n                    stratified=False,\n                    verbose_eval=verbose_eval,\n                    early_stopping_rounds=early_stopping_rounds,\n                    show_stdv=False)\n    best_round = np.argmin(cv_lgb['l2-mean'])\n    best_cv_mean = np.min(cv_lgb['l2-mean'])\n    print('best round', best_round)\n    print('best mse-mean', best_cv_mean)\n    model_lgb = lgb.train(lgb_params, dstrain, \n                          num_boost_round=best_round,\n                          valid_sets=dstrain,\n                          verbose_eval=verbose_eval)\n    print('lgb cv and training finished...')\n    return model_lgb, best_cv_mean\ndef get_feat_weight(model_lgb, feat_names, plot=True):\n    feat_weight = pd.DataFrame(model_lgb.feature_importance(),\n                               columns=['feature_importance'],\n                               index=feat_names)\n    if plot:\n        indices = np.argsort(feat_weight['feature_importance'])[::-1]\n        plt.figure(figsize=(12, 6))\n        plt.title('feature importance (lightgbm)')\n        plt.bar(range(len(feat_weight)), list(feat_weight.iloc[indices, 0]))\n        plt.xticks(range(len(feat_weight)), feat_weight.iloc[indices].index, \n                   rotation='vertical')\n        plt.xlim([-1, len(feat_weight)])\n        plt.show()\n    return feat_weight\ndef get_model_cv(df, y, plot=True, verbose_eval=False):\n    model_lgb, best_cv_mean = cv_train_model(df, y, verbose_eval=verbose_eval)\n    feat_weight = get_feat_weight(model_lgb, feat_names=df.columns, plot=plot)\n    print('best cv mean', best_cv_mean)\n    return best_cv_mean, feat_weight, model_lgb","metadata":{"_cell_guid":"a12034e6-3c2a-473f-bef3-2f5ca6e1c8c9","collapsed":true,"_uuid":"3908ab23833bf8b86b695b899537ac55f3026871"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"best_cv_mean_fe, feat_weight_fe, model_lgb_fe = get_model_cv(train, target_fe)\npred_fe = np.expm1(model_lgb_fe.predict(test))\nbest_cv_mean_be, feat_weight_be, model_lgb_be = get_model_cv(train, target_be)\npred_be = np.expm1(model_lgb_be.predict(test))\nscr_total = np.mean([np.sqrt(best_cv_mean_fe), np.sqrt(best_cv_mean_be)])\nprint(f'total cv score: {scr_total}')\nsub = pd.read_csv('../input/sample_submission.csv')\nsub['formation_energy_ev_natom'] = pred_fe\nsub['bandgap_energy_ev'] = pred_be\nsub.to_csv(f'sb_{scr_total}.csv', index=False) ### LB ~0.0570","metadata":{"_cell_guid":"f61bf3f7-bb08-413a-989e-4903d1f3f66c","_uuid":"bd99262964c190c7909baedc2a4145023e644a46"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"### uncomment to get cv pred\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nrg = RidgeCV(alphas=[0.003, 0.01, 0.3, 3, 10], cv=5)\n\ndef kfold_cv(X, y, test, n_splits=10, \n             train_lgb=True, \n             lgb_ratio=0.8,\n             cv_pred_test=False):\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=233)\n    cv_pred = np.zeros((len(test)))\n    scr_total = 0\n    for fold_id, (tr_idx, te_idx) in enumerate(kf.split(y)):\n        print(f'Starting No.{fold_id} fold CV out of {n_splits} ...')\n        X_tr, y_tr = X.iloc[tr_idx], y.iloc[tr_idx]\n        X_te, y_te = X.iloc[te_idx], y.iloc[te_idx]\n        rg.fit(StandardScaler().fit_transform(X_tr), y_tr)\n        pred_rg = rg.predict(StandardScaler().fit_transform(X_te))\n        mse = mean_squared_error(y_te, pred_rg)\n        print('=======rg mse:', mse)\n        if train_lgb==True:\n            _, _, model_lgb = get_model_cv(X_tr, y_tr, False, 0)\n            print('=======lgb mse:',mean_squared_error(y_te, model_lgb.predict(X_te)))\n            avg_mse = mean_squared_error(y_te, \n                                         pred_rg*(1-lgb_ratio)+\\\n                                         model_lgb.predict(X_te)*lgb_ratio)\n            print(f'=======avg mse: {avg_mse}')\n            scr_total += avg_mse / n_splits\n        else:\n            scr_total += mse / n_splits\n        if cv_pred_test == True:\n            if train_lgb == True: \n                cv_pred += (rg.predict(StandardScaler().fit_transform(\n                        test))*(1-lgb_ratio) + \\\n                            model_lgb.predict(test)*lgb_ratio)/n_splits\n            else:\n                cv_pred += rg.predict(StandardScaler().fit_transform(\n                        test))/n_splits\n    print(f'score total: {scr_total}')\n    if not cv_pred_test:\n        return scr_total\n    else:\n        return scr_total, np.expm1(cv_pred)\n#lgb_ratio = 0.95\n#cv_fe, cv_pred_fe = kfold_cv(train, target_fe, test, n_splits=10, \n#                             train_lgb=True, lgb_ratio=lgb_ratio, cv_pred_test=True)\n#cv_be, cv_pred_be = kfold_cv(train, target_be, test, n_splits=10, \n#                             train_lgb=True, lgb_ratio=lgb_ratio, cv_pred_test=True)\n#cv_total = np.mean([np.sqrt(cv_fe), np.sqrt(cv_be)])\n#print(f'fe: {cv_fe}; be: {cv_be}')\n#print(f'cv total rmsle:{cv_total}')\n#sub['formation_energy_ev_natom'] = cv_pred_fe\n#sub['bandgap_energy_ev'] = cv_pred_be\n#sub.to_csv(f'sb_lgb{lgb_ratio}_rg_{cv_total}.csv', index=False) ### LB ~0.0573","metadata":{"_cell_guid":"3bf249cf-ef29-485b-9624-9451486c8108","collapsed":true,"_uuid":"e6e6ff0e78d1a8a1b7897616dfb29768f2efc9e8"}}],"nbformat":4,"nbformat_minor":1}