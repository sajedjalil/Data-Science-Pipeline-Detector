{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\nThe fifth iteration of M Competitions. This competition contains two complimentary competitions, one involving point forecast and other one estimating the uncertainity of the forecast. \n\n### Data\n- The data provided in this competition is of hierarchical nature, starting at the item level and aggregating to that of departments, product categories, stores in three geographical areas of the US: California, Texas, and Wisconsin. The data is made available by Walmart Labs. \n- Besides the time series data, it also includes explanatory variables such as price, promotions, day of the week, and special events (e.g. Super Bowl, Valentineâ€™s Day, and Orthodox Easter) that affect sales which are used to improve forecasting accuracy.\n- The majority of the more than 42,840 time series display intermittency (sporadic sales including zeros).\n\nFollowing are the datasets available for this competition:-\n\n|S. no. |Dataset| Description |\n|-------|-------|-------------|\n|1| calender.csv|Contains information about the dates on which the products are sold |\n|2| sales_train_validation.csv | Contains the historical daily unit sales data per product and store (d_1 - d_1913) |\n|3| sample_submission.csv | submission file |\n|4| sell_prices.csv | Contains information about the price of the products sold per store and date |\n|5| sales_train_evaluation.csv | Available once month before competition deadline. Will include sales [d_1 - d_1941] |","metadata":{}},{"cell_type":"code","source":"# importing the libraries\n\nimport os\nimport math\nimport numpy as np                                 # linear algebra\nimport pandas as pd                                # dataframes\nimport matplotlib.pyplot as plt                    # visualizations\nimport seaborn as sns\nimport ipywidgets as widgets                       # interative jupyter\nfrom IPython.display import clear_output\n\nfrom scipy import stats                            # statistics\nfrom datetime import datetime, date, timedelta     # time\nfrom dateutil.relativedelta import relativedelta\nfrom statsmodels.tsa.seasonal import STL           # time-series decomposition\n\nfrom matplotlib.patches import Polygon","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting up the notebook parameters\n\nroot_dir = \"/kaggle/input/m5-forecasting-accuracy\"\nprint(\"root directory = {}\".format(root_dir))\n\nplt.rcParams[\"figure.figsize\"] = (16, 8)\nsns.set_style(\"darkgrid\")\npd.set_option(\"display.max_rows\", 20, \"display.max_columns\", None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"######################################\n######## Helper Functions ############\n######################################\n\n\ndef info_df(df):\n    \"\"\"\n    returns the dataframe describing nulls and unique counts\n    inp: dataframe\n    returns: dataframe with unique and null counts\n    \"\"\"\n    return pd.DataFrame({\n        \"uniques\": df.nunique(),\n        \"nulls\": df.isnull().sum(),\n        \"nulls (%)\": df.isnull().sum() / len(df)\n    }).T\n\n\ndef reduce_mem_usage(df, verbose=True):\n    \"\"\"\n    reduces the mem usage by performing certain coercion operations\n    inp: dataframe,\n         verbose (whether to print the info regarding mem reduction or Not)\n    returns: dataframe\n    \"\"\"\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n# 01. Importing the Datasets","metadata":{}},{"cell_type":"code","source":"# defining the datasets\npath_calendar_df = os.path.join(root_dir, \"calendar.csv\")\npath_sales_train_validation_df = os.path.join(root_dir, \"sales_train_validation.csv\")\npath_sell_prices_df = os.path.join(root_dir, \"sell_prices.csv\")\npath_sample_submission_df = os.path.join(root_dir, \"sample_submission.csv\")\n\n# importing the dataset\ncalendar_df = pd.read_csv(path_calendar_df, parse_dates = [\"date\"])\nsales_sample_df = pd.read_csv(path_sales_train_validation_df)\nsell_prices_df = pd.read_csv(path_sell_prices_df)\nsample_submission_df = pd.read_csv(path_sample_submission_df)\n\n# optimising the mem usage\ncalendar_df = reduce_mem_usage(calendar_df, verbose = True)\nsales_sample_df = reduce_mem_usage(sales_sample_df, verbose = True)\nsell_prices_df = reduce_mem_usage(sell_prices_df, verbose = True)\nsample_submission_df = reduce_mem_usage(sample_submission_df, verbose = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.1. Calendar\n\nThe calendar dataset contains the information about dates on which a products are sold. The information captured contins what event fell on that particular day or there were two events on the same day. They also captures SNAP days in all the three states. SNAP stood for *Suppliment Nutrition Assistance Program*. SNAP provides a monthly supplement for purchasing nutritious food. This information can be very useful as it might turn out to be driving factor for sales.","metadata":{}},{"cell_type":"code","source":"calendar_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info_df(calendar_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Column Descriptions\nThe following table lists down various attributes found in the dataset along with their descriptions:-\n\n|S.no.|Column Name| Descriptions|\n|-----|-----------|-------------|\n|01. | date | Date |\n|02. | wm_yr_wk | (not sure) looks like some sort of combination of year and week |\n|03. | weekday | Day of the week |\n|04. | wday| weekday encoded |\n|05. | month | month of the year |\n|06. | d | signifying which day it is in absolute term. All values are unique |\n|07. | event_name_1 | Name of the primary event, e.g. Super Bowl etc. 29 events and 1 null |\n|08. | event_type_1 | Type of event. Whether it is Sporting or Cultural or National or Religious |\n|09. | event_name_2 | Second event, if any. Only 5 values, rest is null|\n|10. | event_type_2 | Type of event |\n|11. | snap_CA | Whether SNAP food stamp is there or not for California state |\n|12. | snap_TX | Whether SNAP food stamp is there or not for Texas state |\n|13. | snap_WI | Whether SNAP food stamp is there or not for Wisconsin state |","metadata":{}},{"cell_type":"markdown","source":"## 1.2. Sales Train Validation\nThe sales train validation contains the historical unit sales data per product and per store. This dataset list down unit sales for 1913 days across various stores. The other level of product hierarchy provided are department, category and state. One more attribute `id` is provided which seems like combination of above said hierarchical attributes along with `item_id` along with a validation flag. The data provided to us in sales train validation is in wide format and should be converted to long form for further analysis.","metadata":{}},{"cell_type":"code","source":"sales_sample_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info_df(sales_sample_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Column Description\nThe following table lists down various attributes found in the dataset along with their descriptions:-\n\n| S.no. | Column Name | Description |\n|-------|-------------|-------------|\n| 01. | id | combination of below IDs and a validation flag|\n| 02. | item_id | Item ID |\n| 03. | dept_id | Department ID |\n| 04. | store_id | Store ID |\n| 05. | state_id | State ID |\n| 06. | d_1 to d_1969 | day wise units sold | ","metadata":{}},{"cell_type":"markdown","source":"## 1.3. Sell Prices\nThis dataset contains the information regarding the price of the product. We are also provided with `store_id` and `item_id` as hierarchy levels. The datetime column in this dataset is `wm_yr_wk`, which provide us with the week along with the year ","metadata":{}},{"cell_type":"code","source":"sell_prices_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info_df(sell_prices_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Column Description\nThe following table lists down various attributes found in the dataset along with their descriptions:-\n\n|S.no.| Column Name | Description|\n|-----|-------------|------------|\n| 01. | store_id | maps to store_id of Sales Train Validation table |\n| 02. | item_id | item id |\n| 03. | wm_yr_wk | described above |\n| 04. | sell prices | Price during that particular wm_yr_wk |","metadata":{}},{"cell_type":"markdown","source":"### 1.4. Sample Submission File","metadata":{}},{"cell_type":"code","source":"sample_submission_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n# 02. Exploratory Data Analysis - Univariate","metadata":{}},{"cell_type":"markdown","source":"As we wish to do the time series analysis on this data, a better representation would be to long form. Currently, the dataset is in wide format. To convert the dataset into long form from wide form, we can use `pd.melt()` funtion. Before proceeding with conversion, we need to divide given attributes into two sets of variables, identifiers and value variables. To convert the table from long to wide format, we can use `pd.pivot()` function.","metadata":{}},{"cell_type":"code","source":"# # create a smaller sample of sales_train_validation_df, to comply with memory demands\n# sales_sample_df = sales_sample_df.sample(100)    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Final len of the Dataset after unpivoting would be = {}\".format(sales_sample_df.shape[0] * 1913))\n\n# for unpivoting, we need to define the variables into two sets, id_vars and value_vars\nvalue_variables = [col for col in sales_sample_df.columns if col.startswith(\"d_\")]\nidentifier_variables = [col for col in sales_sample_df.columns if col not in value_variables] \n\n# converting the df from wide to long\nsales_sample_df = pd.melt(sales_sample_df, \n                          id_vars = identifier_variables, \n                          value_vars = value_variables)\n\nprint(\"Actual Shape after unpivoting = {}\".format(sales_sample_df.shape))\n\n# changing the variable name to apt names\nsales_sample_df = sales_sample_df.rename(columns = {\"variable\": \"day_number\", \"value\": \"units_sold\"})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a date column\nearliest_date = date(2011, 1, 29)\ndate_dict = {}                    # a dictionary to map the day_number values to real dates\nfor i in list(sales_sample_df[\"day_number\"].unique()):\n    dn_int = int(i[2:]) - 1                                   # indexing the string value to delete \"d_\" from the day_number and converting it to int\n                                                              # subtracting 1 because \"d_1\" would be our zeroth day. \n    date_ = earliest_date + timedelta(days = dn_int)\n    date_dict[i] = date_\n\n# mapping the dictionary to dataframe\nsales_sample_df[\"date\"] = sales_sample_df[\"day_number\"].map(date_dict)\nsales_sample_df[\"date\"] = pd.to_datetime(sales_sample_df[\"date\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sales_sample_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1. Item_ID level Time series aggregation\n\nIn this section, we will check the Item ID level time series aggregation of the series. This will help us to understand how the time series' trend on the item level. Below code snippet utilizes a dropdown widget to cleanly display the visualizations. ","metadata":{}},{"cell_type":"code","source":"ALL = \"ALL\"\ndef unique_sorted_value_fn(array):\n    \"\"\"\n    returns unique sorted values\n    inp: array\n    return array with unique values\n    \"\"\"\n    unique_arr = array.unique().tolist()\n    unique_arr.sort()\n    unique_arr.insert(0, ALL)   # if all values are to be selected\n    return unique_arr\n\n# initialize the dropdown\ndropdown_item_id = widgets.Dropdown(options = unique_sorted_value_fn(sales_sample_df[\"item_id\"]))\n\nitem_id_plot = widgets.Output()\n\ndef dropdown_item_id_eventhandler(change):\n    item_id_plot.clear_output()\n    with item_id_plot:\n        if (change.new == ALL):\n            display(sns.lineplot(x = \"date\", y = \"units_sold\", hue = \"item_id\", data = sales_sample_df))\n        else:\n            display(sns.lineplot(x = \"date\", y = \"units_sold\", hue = \"item_id\", \n                                 data = sales_sample_df[sales_sample_df[\"item_id\"] == change.new]))\n            plt.show()\n            \ndropdown_item_id.observe(dropdown_item_id_eventhandler, names='value')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(dropdown_item_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(item_id_plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insights:-\n1. The scale of most of the time series is small, with units sold being less than 10 on most of the days.\n2. Many time series are dominated by zeros sales.\n3. The data doesn't include time series that are uniform all along. For example, `item_id == HOBBIES_1_114` have zeros for whole of 2011 and start of 2012, which might signify that the product might have been launched in early part of 2012, hence no data before that exists.\n4. So, we have to deal with two challenges in this problem:-\n    - The data has alot of zeros and sudden spikes, hence we need a model that is robust to noise and can learn from intermittent data\n    - Deal with long gaps at the start of time series","metadata":{}},{"cell_type":"markdown","source":"### 2.2 Aggregated Data\n\nChecking the trends and patterns on the aggregated data. Because the data at `item_id` level doesn't give us much info, we might find some useful information and statistics at the aggregated level.","metadata":{}},{"cell_type":"code","source":"sns.lineplot(x = \"date\", y = \"units_sold\", data = sales_sample_df)\nplt.title(\"M5 - aggregated data\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insights:\n1. There is a clear upward trend in the aggregated sales.\n2. Year on Year, the time series goes through similar crests and troughs, highlighting some seasonal patterns.\n3. Every year, X-mas have sales as zero, which is due to the fact that stores are closed on that particular day.","metadata":{}},{"cell_type":"markdown","source":"### 2.3. Sales by State","metadata":{}},{"cell_type":"code","source":"sns.lineplot(x = \"date\", y = \"units_sold\", hue = \"state_id\", data = sales_sample_df)\nplt.title(\"State wise aggregated data\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above graph doesn't provide a very clear picture. It is better to view this data with bigger resolution.","metadata":{}},{"cell_type":"code","source":"# creating a new dataframe for statewise aggregation\nstatewise_df = sales_sample_df.groupby([\"state_id\", \"date\"]).agg({\n    \"units_sold\": \"sum\"\n}).reset_index()\n\n# extracting month and year from date for group by purposes\nstatewise_df[\"day\"] = statewise_df[\"date\"].dt.day\nstatewise_df[\"month\"] = statewise_df[\"date\"].dt.month\nstatewise_df[\"year\"] = statewise_df[\"date\"].dt.year\n\n# aggregating on month level for each state\nstatewise_df = statewise_df.groupby([\"month\", \"year\", \"state_id\"]).agg({\n    \"units_sold\": \"sum\", \n    \"day\": \"first\"\n}).reset_index()\n\nstatewise_df[\"date\"] = pd.to_datetime(statewise_df[\"year\"].astype(\"str\") + \"-\" + \\\n                                      statewise_df[\"month\"].astype(\"str\") + \"-\" + \\\n                                      statewise_df[\"day\"].astype(\"str\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lineplot(x = \"date\", y = \"units_sold\", hue = \"state_id\", data = statewise_df)\nplt.title(\"Statewise sales trend\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insights:\n1. The sales in California is clearly leading other two states, while Wisconsin is doing better than Texas in recent times\n2. Barring the peak in first half of 2014, the sales across CA remained more or less the same. ","metadata":{}},{"cell_type":"code","source":"del statewise_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4. Store-wise Aggregation\n\nAs stated below, there are total of 10 stores, 4 in CA, 3 in both TX and WI.","metadata":{}},{"cell_type":"code","source":"sales_sample_df.groupby(\"state_id\").agg({\"store_id\": \"nunique\"})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a new dataframe for storewise aggregation\nstorewise_df = sales_sample_df.groupby([\"state_id\", \"store_id\", \"date\"]).agg({\n    \"units_sold\": \"sum\"\n}).reset_index()\n\n# extracting month and year from date for group by purposes\nstorewise_df[\"day\"] = storewise_df[\"date\"].dt.day\nstorewise_df[\"month\"] = storewise_df[\"date\"].dt.month\nstorewise_df[\"year\"] = storewise_df[\"date\"].dt.year\n\n# aggregating on month level for each state and store\nstorewise_df = storewise_df.groupby([\"month\", \"year\", \"state_id\", \"store_id\"]).agg({\n    \"units_sold\": \"sum\", \n    \"day\": \"first\"\n}).reset_index()\n\nstorewise_df[\"date\"] = pd.to_datetime(storewise_df[\"year\"].astype(\"str\") + \"-\" + \\\n                                      storewise_df[\"month\"].astype(\"str\") + \"-\" + \\\n                                      storewise_df[\"day\"].astype(\"str\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state_list = list(storewise_df[\"state_id\"].unique())\nfor i in range(1, 4):\n    plt.subplot(3, 1, i)\n    sns.lineplot(x = \"date\", \n                 y = \"units_sold\", \n                 hue = \"store_id\", \n                 data = storewise_df[storewise_df[\"state_id\"] == state_list[i - 1]])\n    plt.title(\"Store wise trend in {}\".format(state_list[i - 1]))\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insights:-\n1. The sudden peak in first half of California sales was mostly contributed by CA_1. This could be due to some offer or discount going on.\n2. The TX_3 is clearly ahead of other stores in the state.\n3. The WI_2 peaks around 2013 and 2014 but level up with others by 2016.","metadata":{}},{"cell_type":"code","source":"del storewise_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.5. Category-wise Aggregation\n\nIn the dataset, we have 3 categories, Households, Foods and Hobbies. ","metadata":{}},{"cell_type":"code","source":"# creating a new dataframe for storewise aggregation\ncatwise_df = sales_sample_df.groupby([\"state_id\", \"cat_id\", \"date\"]).agg({\n    \"units_sold\": \"sum\"\n}).reset_index()\n\n# extracting month and year from date for group by purposes\ncatwise_df[\"day\"] = catwise_df[\"date\"].dt.day\ncatwise_df[\"month\"] = catwise_df[\"date\"].dt.month\ncatwise_df[\"year\"] = catwise_df[\"date\"].dt.year\n\n# aggregating on month level for each state and store\ncatwise_df = catwise_df.groupby([\"month\", \"year\", \"state_id\", \"cat_id\"]).agg({\n    \"units_sold\": \"sum\", \n    \"day\": \"first\"\n}).reset_index()\n\ncatwise_df[\"date\"] = pd.to_datetime(catwise_df[\"year\"].astype(\"str\") + \"-\" + \\\n                                    catwise_df[\"month\"].astype(\"str\") + \"-\" + \\\n                                    catwise_df[\"day\"].astype(\"str\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lineplot(x = \"date\", y = \"units_sold\", hue = \"cat_id\", data = catwise_df)\nplt.title(\"Catgory-wise Sales\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(1, 4):\n    plt.subplot(3, 1, i)\n    sns.lineplot(x = \"date\", \n                 y = \"units_sold\", \n                 hue = \"cat_id\", \n                 data = catwise_df[catwise_df[\"state_id\"] == state_list[i - 1]])\n    plt.title(\"Category wise trend in {}\".format(state_list[i - 1]))\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insights\n1. The \"Foods\", lead sales in every state. The difference is consderable in WI and TX but in CA, the sales of \"Foods\" and \"Households\" are quite close. CA is the only state where \"Households\" led sales, doing so before 2013. \n2. \"Hobbies\" have similar trend in almost every state.","metadata":{}},{"cell_type":"markdown","source":"### 2.6. Department-wise Aggregation\nIn the dataset, we have 7 different categories, with three belonging to \"Foods\" and \"Hibbies\" and \"Households\" having couple under their hood each.","metadata":{}},{"cell_type":"code","source":"sales_sample_df.groupby(\"cat_id\").agg({\"dept_id\": \"nunique\"})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a new dataframe for storewise aggregation\ndeptwise_df = sales_sample_df.groupby([\"cat_id\", \"dept_id\", \"date\"]).agg({\n    \"units_sold\": \"sum\"\n}).reset_index()\n\n# extracting month and year from date for group by purposes\ndeptwise_df[\"day\"] = deptwise_df[\"date\"].dt.day\ndeptwise_df[\"month\"] = deptwise_df[\"date\"].dt.month\ndeptwise_df[\"year\"] = deptwise_df[\"date\"].dt.year\n\n# aggregating on month level for each state and store\ndeptwise_df = deptwise_df.groupby([\"month\", \"year\", \"cat_id\", \"dept_id\"]).agg({\n    \"units_sold\": \"sum\", \n    \"day\": \"first\"\n}).reset_index()\n\ndeptwise_df[\"date\"] = pd.to_datetime(deptwise_df[\"year\"].astype(\"str\") + \"-\" + \\\n                                     deptwise_df[\"month\"].astype(\"str\") + \"-\" + \\\n                                     deptwise_df[\"day\"].astype(\"str\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_list = list(sales_sample_df[\"cat_id\"].unique())\nfor i in range(1, 4):\n    plt.subplot(3, 1, i)\n    sns.lineplot(x = \"date\", \n                 y = \"units_sold\", \n                 hue = \"dept_id\", \n                 data = deptwise_df[deptwise_df[\"cat_id\"] == cat_list[i - 1]])\n    plt.title(\"Category wise trend in {}\".format(cat_list[i - 1]))\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insights:-\n1. Foods_1 forms the major chunks of sales in Foods category. Similarly, Households_1 and Hobbies_1 dominate their respective categories.\n2. The Hobbies_2 is very close to zero, meaning, it might have days where sales is zero in majority. ","metadata":{}},{"cell_type":"markdown","source":"---\n# 03. Exploratory Data Analysis - Trend Series Decomposition\n\nThis section illustrates the use of `STL` to decompose a time series into three components: *trend*, *season(al)* and *residual*. STL uses **LOESS (locally estimated scatterplot smoothing)** to extract smooths estimates of the three components. The key inputs into STL are:\n- `season` - The length of the seasonal smoother. Must be odd.\n- `trend` - The length of the trend smoother, usually around 150% of season. Must be odd and larger than season.\n- `low_pass` - The length of the low-pass estimation window, usually the smallest odd number larger than the periodicity of the data.","metadata":{}},{"cell_type":"code","source":"# creating a new dataframe with aggregated sales.\nstl_df = sales_sample_df[[\"date\", \"units_sold\"]].set_index(\"date\")\nstl_df = stl_df.resample(\"D\").sum()\nstl_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stl = STL(stl_df, seasonal = 7)\nres = stl.fit()\nfig = res.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del stl_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finding more evidence for weekly seasonality\nAccording to our assumption, the seasonality factor that we have taken into consideration is 7, i.e. weekly. We can explore this seasonality factor again by checking how `day_of_week` are performing and whether their performance is consistent over time.","metadata":{}},{"cell_type":"code","source":"sales_sample_df[\"day_of_week\"] = sales_sample_df[\"date\"].dt.weekday\nsales_sample_df[\"month\"] = sales_sample_df[\"date\"].dt.month\nsales_sample_df[\"year\"] = sales_sample_df[\"date\"].dt.year","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"week_month_pivot = sales_sample_df.pivot_table(index = \"day_of_week\", \n                                               columns = \"month\", \n                                               values = \"units_sold\", \n                                               aggfunc = \"sum\")\n\nweek_month_pivot.columns = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\nweek_month_pivot.index = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n\nsns.heatmap(week_month_pivot, linewidth = 0.2, cmap=\"YlGnBu\")\nplt.title(\"Performance of sales for Day of Week aggregated on monthly basis\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:-\n1. The sales on weekends is considerable and consistently larger than that on weekdays.\n2. The months of Jan, May, and July shows a clear dip in sales for every day.","metadata":{}},{"cell_type":"code","source":"year_month_pivot = sales_sample_df.pivot_table(index = \"month\", \n                                               columns = \"year\", \n                                               values = \"units_sold\", \n                                               aggfunc = \"sum\")\nyear_month_pivot.index = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n\nsns.heatmap(year_month_pivot, linewidth = 0.2, cmap=\"YlGnBu\")\nplt.title(\"Performance of sales for Month aggregated on yearly basis\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:-\n1. The data we have is till Apr, 2016, hence the heatmap after that is empty.\n2. August and December delivers consistently high numbers. \n3. The sales usually peaks around the later months of the year. This could be attributes festivities and various promotion events falling in that part of the year.","metadata":{}},{"cell_type":"code","source":"del week_month_pivot, year_month_pivot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_list = [\"day_of_week\", \"month\"]\nfor i in range(0, 2):\n    plt.subplot(2, 1, i + 1)\n    tempdf = sales_sample_df.groupby([temp_list[i], \"state_id\"]).agg({\n        \"units_sold\": \"mean\"\n    }).reset_index()\n    sns.lineplot(x = temp_list[i], y = \"units_sold\", hue = \"state_id\", data = tempdf)\n    plt.title(\"units sold trend - {}\".format(temp_list[i]))\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n1. The pattern of sales for a week is similar for all the states\n2. The Texas usually shows a increasing trend in the first half of the year and decreasing trend in the second half of the year. For others, the sales remain more or less constant throughout the year\n---","metadata":{}},{"cell_type":"markdown","source":"# 04. Exploring Price and Calender Events\nThis section explores the additional datasets provided to us, i.e. calender and prices dataset. The prices dataset contains the information about the price of the products sold per store and date and the calender dataset contains information about the dates on which the products are sold.","metadata":{}},{"cell_type":"code","source":"calendar_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"event1_bool = []   # boolean list. Captures whether an event exist or not\nfor i in range(0, len(calendar_df)):\n    if calendar_df[\"event_name_1\"].iloc[i] == calendar_df[\"event_name_1\"].iloc[i]:\n        event1_bool.append(\"True\")\n    else:\n        event1_bool.append(\"False\")\n        \n# inserting the above list in calendar_df\ncalendar_df.insert(loc = 9, column = \"event_bool_1\", value = event1_bool)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot distribution of event days\nplt.subplot(1, 2, 1)\nsns.countplot(calendar_df[\"event_bool_1\"], palette = \"Set2\")\nplt.title(\"Frequency of events and non-events\")\nplt.xlabel(\"Whether event is there or Not\")\n\nplt.subplot(1, 2, 2)\nsns.countplot(y = calendar_df[\"event_type_1\"], palette = \"Set2\")\nplt.title(\"Frequency of the types of events\")\nplt.ylabel(\"Event Type\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot distribution of snap days across three states\nplt.subplot(1, 3, 1)\nsns.countplot(x = calendar_df[\"snap_CA\"], palette = \"RdBu\")\nplt.title(\"Frequency plot - Snap days across California\")\n\nplt.subplot(1, 3, 2)\nsns.countplot(x = calendar_df[\"snap_TX\"], palette = \"RdBu\")\nplt.title(\"Frequency plot - Snap days across Texas\")\n\nplt.subplot(1, 3, 3)\nsns.countplot(x = calendar_df[\"snap_WI\"], palette = \"RdBu\")\nplt.title(\"Frequency plot - Snap days across Wisconsin\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:-\n1. There are total of 162 event days in the dataset, which forms 8.2% of the dataset\n2. The distribution of snap days is uniform in each state","metadata":{}},{"cell_type":"code","source":"def generate_data(df, date_col, data_col):\n    \"\"\"\n    converts the pd dataframe into numpy arrays of data and respective dates\n    \n    inp: df (dataframe)\n         date_col (column which contains the dates)\n         data_col (data to be mapped)\n    returns: data_arr (array of data)\n             dates (array of dates)\n    \"\"\"\n    data_arr = np.array(df[data_col])\n    data_len = len(data_arr)\n    start_date = df[date_col].iloc[0]\n    dates = [start_date + timedelta(days = i) for i in range(data_len)]\n    return data_arr, dates\n\n\ndef calendar_array_fn(date_arr, data_arr):\n    \"\"\"\n    returns an array of shape (-1, 7)\n    \n    inp: date_arr (array of dates)\n         data_arr (array of data)\n    returns: i, j (indices)\n             calendar (array of data of shape (-1, 7))\n    \"\"\"\n    \n    # return the date as an ISO calendar (year, week, day)\n    i, j = zip(*[date.isocalendar()[1:] for date in date_arr])\n    i = np.array(i) - min(i) \n    j = np.array(j) - 1\n    max_i = max(i) + 1\n    calendar = np.nan * np.zeros((max_i, 7))  # creating empty arrays\n    calendar[i, j] = data_arr                 # creating a data matrix\n    \n    return i, j, calendar\n\n\ndef label_days(ax, date_arr, i, j, calendar):\n    \"\"\"\n    creates label for days\n    \n    inp: ax,\n         date_arr (array of dates),\n         i, j (indices),\n         calendar (calendar array, returned by calendar_arr_fn())\n    returns: nothing\n    \"\"\"\n    ni, nj = calendar.shape                          # len and width of the matrix\n    day_of_month_arr = np.nan * np.zeros((ni, 7))    # initializing day_of_month array\n    day_of_month_arr[i, j] = [date.day for date in date_arr]\n    \n    # ndenuerate - multi index iterator\n    for (i, j), day in np.ndenumerate(day_of_month_arr):\n        # following condition checks if the thing is not NaN\n        if np.isfinite(day):\n            ax.text(j, i, \n                    int(day), \n                    ha = \"center\", \n                    va = \"center\")\n    # defining x-axis labels\n    ax.set(xticks = np.arange(7), \n           xticklabels = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"])\n    ax.xaxis.tick_top()\n    \n    \ndef label_months(ax, date_arr, i, j, calendar):\n    \"\"\"\n    creates label for days\n    \n    inp: ax,\n         date_arr (array of dates),\n         i, j (indices),\n         calendar (calendar array, returned by calendar_arr_fn())\n    returns: nothing\n    \"\"\"\n    months_labels = np.array([\n        \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \n        \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n    ])                                                        # month names\n    months_arr = np.array([date.month for date in date_arr])     # extracting months from dates\n    unique_months = sorted(set(months_arr))                   # get unique months                  \n    yticks = [i[months_arr == m].mean() for m in unique_months]   \n    labels = [months_labels[m - 1] for m in unique_months]\n    ax.set(yticks = yticks)\n    ax.set_yticklabels(labels, rotation = 90)\n    \n    \ndef calendar_heatmap(ax, date_arr, data_arr):\n    i, j, calendar = calendar_array_fn(date_arr, data_arr)\n    im = ax.imshow(calendar, interpolation = \"none\", cmap = \"summer\")\n    label_days(ax, date_arr, i, j, calendar)\n    label_months(ax, date_arr, i, j, calendar)\n    # uncomment following line if you want colorbars\n    #ax.figure.colorbar(im)         \n    \ndef plot_calmap(ax, year, data_col):\n    \"\"\"\n    main function for ploting calendar heatmaps\n    \n    inp: year (which year to be plotted)\n         data_col (data column)\n    returns nothing\n    \"\"\"\n    data_arr, date_arr = generate_data(df = calendar_df[calendar_df[\"year\"] == year], \n                                       date_col = \"date\", \n                                       data_col = data_col)\n    calendar_heatmap(ax, date_arr, data_arr)\n    ax.set_title(\"{} distribution in the year {}\".format(data_col, year))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows = 1, ncols = 3, figsize = (19, 20))\nplot_calmap(ax[0], year = 2011, data_col = \"snap_CA\")\nplot_calmap(ax[1], year = 2011, data_col = \"snap_TX\")\nplot_calmap(ax[2], year = 2011, data_col = \"snap_WI\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:-\n1. The above plot conveys us that the snap days are usually occuring on same dates throughout the year. For California, the snap days occurs during first 10 days. For Texas, they occur on 1st, 3rd, 5th, 6th, 7th, 9th, 11th, 12th, 13th, and 15th of the month. Similarly, for Wisconsin, snap days are 2nd, 3rd, 5th, 6th, 8th, 9th, 11th, 12th, 14th and 15th of every month.\n2. The snap days happens before 15th of every month in every state.","metadata":{}},{"cell_type":"markdown","source":"## 4.2. sell_prices_df","metadata":{}},{"cell_type":"code","source":"sell_prices_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a few additional columns to aid in analysis below\n\nsell_prices_df[\"state\"] = sell_prices_df[\"store_id\"].str[:2]\nsell_prices_df[\"cat_id\"] = sell_prices_df[\"item_id\"].str[:-4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting the distribution of various stores in a state\n\nplt.figure(figsize = (20, 16))\nplt.subplots_adjust(hspace = 0.5)\n\nplt.subplot(4, 3, 1)\nfor i in [\"CA_1\", \"CA_2\", \"CA_3\", \"CA_4\"]:\n    sns.distplot(sell_prices_df[sell_prices_df[\"store_id\"] == i][\"sell_price\"], label = i)\n    plt.legend()\n    plt.xlabel(\"Sell Price\")\n    plt.title(\"Store-wise price distribution - California\")\n\nplt.subplot(4, 3, 2)\nfor j in [\"TX_1\", \"TX_2\", \"TX_3\"]:\n    sns.distplot(sell_prices_df[sell_prices_df[\"store_id\"] == j][\"sell_price\"], label = j)\n    plt.legend()\n    plt.xlabel(\"Sell Price\")\n    plt.title(\"Store-wise price distribution - Texas\")\n\nplt.subplot(4, 3, 3)\nfor k in [\"WI_1\", \"WI_2\", \"WI_3\"]:\n    sns.distplot(sell_prices_df[sell_prices_df[\"store_id\"] == k][\"sell_price\"], label = k)\n    plt.legend()\n    plt.xlabel(\"Sell Price\")\n    plt.title(\"Store-wise price distribution - Wisconsin\")\n    \nplt.subplot(4, 3, 4)\nfor i in [\"HOBBIES_1\", \"HOBBIES_2\"]:\n    sns.distplot(sell_prices_df[(sell_prices_df[\"cat_id\"] == i) & (sell_prices_df[\"state\"] == \"CA\")][\"sell_price\"], label = i)\n    plt.legend()\n    plt.xlabel(\"Sell Price\")\n    plt.title(\"Category-wise price distribution in California- Hobbies\")\n    \nplt.subplot(4, 3, 5)\nfor i in [\"HOBBIES_1\", \"HOBBIES_2\"]:\n    sns.distplot(sell_prices_df[(sell_prices_df[\"cat_id\"] == i) & (sell_prices_df[\"state\"] == \"TX\")][\"sell_price\"], label = i)\n    plt.legend()\n    plt.xlabel(\"Sell Price\")\n    plt.title(\"Category-wise price distribution in Texas- Hobbies\")\n    \nplt.subplot(4, 3, 6)\nfor i in [\"HOBBIES_1\", \"HOBBIES_2\"]:\n    sns.distplot(sell_prices_df[(sell_prices_df[\"cat_id\"] == i) & (sell_prices_df[\"state\"] == \"WI\")][\"sell_price\"], label = i)\n    plt.legend()\n    plt.xlabel(\"Sell Price\")\n    plt.title(\"Category-wise price distribution in Wisconsin- Hobbies\")\n\nplt.subplot(4, 3, 7)\nfor j in [\"HOUSEHOLD_1\", \"HOUSEHOLD_2\"]:\n    sns.distplot(sell_prices_df[(sell_prices_df[\"cat_id\"] == j) & (sell_prices_df[\"state\"] == \"CA\")][\"sell_price\"], label = j)\n    plt.legend()\n    plt.xlabel(\"Sell Price\")\n    plt.title(\"Category-wise price distribution in California - Households\")\n\nplt.subplot(4, 3, 8)\nfor j in [\"HOUSEHOLD_1\", \"HOUSEHOLD_2\"]:\n    sns.distplot(sell_prices_df[(sell_prices_df[\"cat_id\"] == j) & (sell_prices_df[\"state\"] == \"TX\")][\"sell_price\"], label = j)\n    plt.legend()\n    plt.xlabel(\"Sell Price\")\n    plt.title(\"Category-wise price distribution in Texas - Households\")  \n    \nplt.subplot(4, 3, 9)\nfor j in [\"HOUSEHOLD_1\", \"HOUSEHOLD_2\"]:\n    sns.distplot(sell_prices_df[(sell_prices_df[\"cat_id\"] == j) & (sell_prices_df[\"state\"] == \"WI\")][\"sell_price\"], label = j)\n    plt.legend()\n    plt.xlabel(\"Sell Price\")\n    plt.title(\"Category-wise price distribution in Wisconsin - Households\")\n\nplt.subplot(4, 3, 10)\nfor k in [\"FOODS_1\", \"FOODS_2\", \"FOODS_3\"]:\n    sns.distplot(sell_prices_df[(sell_prices_df[\"cat_id\"] == k) & (sell_prices_df[\"state\"] == \"CA\")][\"sell_price\"], label = k)\n    plt.legend()\n    plt.xlabel(\"Sell Price\")\n    plt.title(\"Category-wise price distribution in California - Foods\")\n    \nplt.subplot(4, 3, 11)\nfor k in [\"FOODS_1\", \"FOODS_2\", \"FOODS_3\"]:\n    sns.distplot(sell_prices_df[(sell_prices_df[\"cat_id\"] == k) & (sell_prices_df[\"state\"] == \"TX\")][\"sell_price\"], label = k)\n    plt.legend()\n    plt.xlabel(\"Sell Price\")\n    plt.title(\"Category-wise price distribution in Texas - Foods\")\n    \nplt.subplot(4, 3, 12)\nfor k in [\"FOODS_1\", \"FOODS_2\", \"FOODS_3\"]:\n    sns.distplot(sell_prices_df[(sell_prices_df[\"cat_id\"] == k) & (sell_prices_df[\"state\"] == \"WI\")][\"sell_price\"], label = k)\n    plt.legend()\n    plt.xlabel(\"Sell Price\")\n    plt.title(\"Category-wise price distribution in Wisconsin - Foods\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:-\n1. The probability distribution of the `sell_price` is almost identical in all the three states. The difference being is tail of distribution is long in case of Wisconsin. This might be due to the retailing strategy of selling many unique items with relatively small quantities sold of each, usually in addition to selling fewer popular items in large quantities. This [wired article](https://www.wired.com/2004/10/tail/) is a good introduction to long tail phenomenon\n2. The distribution of food prices have strikingly similar price distribution, with both range and peaks occuring at the same places. This might be due to the fact that food prices are generally similar across states and somewhat regulated.\n3. The Households have largest variations in the prices. They might be the ones which are contributing to long tails in the PDFs of prices.","metadata":{}}]}