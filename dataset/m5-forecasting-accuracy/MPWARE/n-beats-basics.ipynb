{"cells":[{"metadata":{},"cell_type":"markdown","source":"# N-Beats basics (PyTorch)\nhttps://github.com/philipperemy/n-beats\n\nhttps://arxiv.org/pdf/1905.10437.pdf (180 models ensembled in this paper with different backcast, loss and seed to beat M4 winner model)\n\nUnivariate Time Series (sales) predicted with N-Beats (forecast=28 days, backcast=3x28 days). Result is far away from simple LGB. Some model and hyperparameters tuning needs to be done. Feel free to fork and improve it.\n\n- v1.0: 3 N-BEATS models with different backcast\n- v1.1: Add first sales and cleaning\n- v1.2: Adam/0.001, **LB=1.791 **,\n  stack_types=[NBeatsNet.TREND_BLOCK, NBeatsNet.SEASONALITY_BLOCK, NBeatsNet.GENERIC_BLOCK],\n  thetas_dims=[2, 8, 3],\n  nb_blocks_per_stack=3,\n  hidden_layer_units=1024,\n  share_weights_in_stack=False\n- v1.3: Adam/0.001, **LB=1.076 **,\n  stack_types=[NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK],\n  thetas_dims=[4, 4],\n  nb_blocks_per_stack=2,\n  hidden_layer_units=1024,\n  share_weights_in_stack=True\n- v1.4: Adam/0.001, **LB=1.119 **,\n  stack_types=[NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK],\n  thetas_dims=[4, 4, 4],\n  nb_blocks_per_stack=3,\n  hidden_layer_units=1024,\n  share_weights_in_stack=True  \n- v1.5: Adam/0.001, **LB= **,\n  stack_types=[NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK],\n  thetas_dims=[8, 8],\n  nb_blocks_per_stack=3,\n  hidden_layer_units=1024,\n  share_weights_in_stack=True    \n  \n  ","execution_count":null},{"metadata":{"id":"0ywGleXmderl","trusted":true},"cell_type":"code","source":"import os, sys, random, gc, math, glob, time\nimport numpy as np\nimport pandas as pd\nimport io, timeit, os, gc, pickle, psutil\nimport joblib\nfrom matplotlib import cm\nfrom datetime import datetime, timedelta\nimport warnings\nfrom tqdm.notebook import tqdm\nfrom functools import partial\nfrom collections import OrderedDict\n\n# warnings.filterwarnings('ignore')\npd.set_option('display.max_colwidth', None)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 4000)","execution_count":null,"outputs":[]},{"metadata":{"id":"MQW2mh7Jdern","trusted":true},"cell_type":"code","source":"seed = 2020\nrandom.seed(seed)\nnp.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":6560,"status":"ok","timestamp":1590960960430,"user":{"displayName":"LYSE SOFT","photoUrl":"","userId":"09063443611678499226"},"user_tz":-120},"id":"LJuktKAwderr","outputId":"ad9076d1-fd97-44b5-fef4-7014769897aa","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nDEFAULT_FIG_WIDTH = 20\nsns.set_context(\"paper\", font_scale=1.2) ","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":6528,"status":"ok","timestamp":1590960960432,"user":{"displayName":"LYSE SOFT","photoUrl":"","userId":"09063443611678499226"},"user_tz":-120},"id":"KrM_e3D3derw","outputId":"26446b6c-f5a0-4d4f-f1ad-f02721d9ded0","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, TimeSeriesSplit, KFold, GroupKFold, ShuffleSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error, mean_absolute_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import defaultdict\n\n\nprint('Python    : ' + sys.version.split('\\n')[0])\nprint('Numpy     : ' + np.__version__)\nprint('Pandas    : ' + pd.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nfrom torch.autograd import Variable\nfrom torch import optim\nfrom torch.optim import Adam, SGD\nprint('PyTorch        : ' + torch.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_numpy_and_pytorch(s, sc):\n    random.seed(s)\n    os.environ['PYTHONHASHSEED'] = str(s)\n    np.random.seed(s)\n    # Torch\n    torch.manual_seed(sc)\n    torch.cuda.manual_seed(sc)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(sc)\n\nseed_numpy_and_pytorch(seed, seed)","execution_count":null,"outputs":[]},{"metadata":{"id":"ngnRMr2oder3","trusted":true},"cell_type":"code","source":"HOME =  \"./\"\nDATA_HOME = \"/kaggle/input/m5-forecasting-accuracy/\"\nTRAIN_DATA_HOME = DATA_HOME\n\nCALENDAR = DATA_HOME + \"calendar.csv\"\nSALES = DATA_HOME + \"sales_train_validation.csv\"\nPRICES = DATA_HOME + \"sell_prices.csv\"\nSAMPLE_SUBMISSION = DATA_HOME + \"sample_submission.csv\"\n\nMODELS_DIR = \"models\"\nif not os.path.exists(MODELS_DIR):\n    os.makedirs(MODELS_DIR)\n           ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load data and remove days with no sales at the beginning.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales_pd = pd.read_csv(SALES)\nd_cols = [c for c in train_sales_pd.columns if 'd_' in c]\nindex_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n\ngrid_df = pd.melt(frame = train_sales_pd, \n                 id_vars = index_columns,\n                 var_name = 'd',\n                 value_vars = d_cols,\n                 value_name = \"sales\")\n\ngrid_df[\"sales\"] = grid_df[\"sales\"].astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_prices_pd = pd.read_csv(PRICES)\n# No price = Nothing to sell, find initial release date. It allows removing real leading zeros.\nrelease_df = train_prices_pd.groupby(['store_id','item_id'])['wm_yr_wk'].agg(['min']).reset_index()\nrelease_df.columns = ['store_id','item_id','release']\n# Now we can merge release_df\ngrid_df = pd.merge(left=grid_df, right=release_df, on=['store_id','item_id'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_calendar_pd = pd.read_csv(CALENDAR)\ngrid_df = pd.merge(left=grid_df, right=train_calendar_pd[['wm_yr_wk','d']], on=['d'], how=\"left\")\n# Now we can cutoff some rows and free memory \ngrid_df = grid_df[grid_df['wm_yr_wk']>=grid_df['release']]\ngrid_df = grid_df.reset_index(drop=True)\ngrid_df.drop(columns=[\"release\", \"wm_yr_wk\"], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Go back to column per-day format.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_df[\"d\"] = grid_df[\"d\"].apply(lambda x: x[2:])\ngrid_df[\"d\"] = grid_df[\"d\"].astype(np.int16)\ngrid_df = grid_df.sort_values([\"id\", \"d\"])\nfull_train_pd = grid_df.pivot(index='id', columns='d', values='sales')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_train_pd.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check how much missing data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"d = full_train_pd.isnull().sum().reset_index().plot(x=\"d\")\nd = plt.title(\"Nan by day\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del grid_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"id":"RnQn3W3aKN2m","trusted":true},"cell_type":"code","source":"# Clean data: remove leading zeros and outliers\n# def clean_data(df_train, day_cols, indx):\n#     t = df_train.loc[indx].copy()\n#     t.loc[day_cols[((t.loc[day_cols]>0).cumsum()==0).values]] = np.nan\n#     q1 = t.loc[day_cols].quantile(0.25)\n#     q3 = t.loc[day_cols].quantile(0.75)\n#     iqr = q3-q1\n#     qm = (q3+1.5*iqr)\n#     t.loc[day_cols][t.loc[day_cols]>qm] = qm\n#     return t","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For each time series, extract one day (backcast/forecast) randomly within full history. Fill in leading missing data (if any) with median. If no data available at all then assume it's zero.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_m5_data(backcast_length, forecast_length, df, is_training=True):\n    x = np.array([]).reshape(0, backcast_length)\n    y = np.array([]).reshape(0, forecast_length)\n    x_tl_tl = df.values\n    for i in range(x_tl_tl.shape[0]):\n        if len(x_tl_tl[i]) < backcast_length + forecast_length:\n            continue\n        time_series = np.array(x_tl_tl[i])\n        time_series_cleaned = time_series[~np.isnan(time_series)]\n        if len(time_series_cleaned) < backcast_length + forecast_length:\n            # Not enough data, fill missing value with median        \n            median = np.nanmedian(time_series_cleaned) if len(time_series_cleaned) > 0 else 0.0\n            missing_data = median * np.ones(((backcast_length + forecast_length)-time_series_cleaned.shape[0]))\n            time_series_cleaned = np.concatenate([missing_data, time_series_cleaned])\n        if is_training:\n            time_series_cleaned_forlearning_x = np.zeros((1, backcast_length))\n            time_series_cleaned_forlearning_y = np.zeros((1, forecast_length))\n            # Random day within [140, 1886] i.e. 831 (if backcast = 140)\n            j = np.random.randint(backcast_length, time_series_cleaned.shape[0] + 1 - forecast_length)\n            # 140 backcast before random day\n            time_series_cleaned_forlearning_x[0, :] = time_series_cleaned[j - backcast_length: j]\n            # 28 forecast days after random day 691 <--> 831 <--> 859\n            time_series_cleaned_forlearning_y[0, :] = time_series_cleaned[j:j + forecast_length]\n        else:\n            time_series_cleaned_forlearning_x = np.zeros(\n                (time_series_cleaned.shape[0] + 1 - (backcast_length + forecast_length), backcast_length))\n            time_series_cleaned_forlearning_y = np.zeros(\n                (time_series_cleaned.shape[0] + 1 - (backcast_length + forecast_length), forecast_length))\n            for j in range(backcast_length, time_series_cleaned.shape[0] + 1 - forecast_length):\n                time_series_cleaned_forlearning_x[j - backcast_length, :] = time_series_cleaned[j - backcast_length:j]\n                time_series_cleaned_forlearning_y[j - backcast_length, :] = time_series_cleaned[j: j + forecast_length]\n        x = np.vstack((x, time_series_cleaned_forlearning_x))\n        y = np.vstack((y, time_series_cleaned_forlearning_y))\n\n    return x, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NBeatsNet(nn.Module):\n    SEASONALITY_BLOCK = 'seasonality'\n    TREND_BLOCK = 'trend'\n    GENERIC_BLOCK = 'generic'\n\n    def __init__(self,\n                 device,\n                 stack_types=(TREND_BLOCK, SEASONALITY_BLOCK),\n                 nb_blocks_per_stack=3,\n                 forecast_length=5,\n                 backcast_length=10,\n                 thetas_dims=(4, 8),\n                 share_weights_in_stack=False,\n                 hidden_layer_units=256,\n                 nb_harmonics=None):\n        super(NBeatsNet, self).__init__()\n        self.forecast_length = forecast_length\n        self.backcast_length = backcast_length\n        self.hidden_layer_units = hidden_layer_units\n        self.nb_blocks_per_stack = nb_blocks_per_stack\n        self.share_weights_in_stack = share_weights_in_stack\n        self.nb_harmonics = nb_harmonics\n        self.stack_types = stack_types\n        self.stacks = []\n        self.thetas_dim = thetas_dims\n        self.parameters = []\n        self.device = device\n        print(f'| N-Beats')\n        for stack_id in range(len(self.stack_types)):\n            self.stacks.append(self.create_stack(stack_id))\n        self.parameters = nn.ParameterList(self.parameters)\n        self.to(self.device)\n\n    def create_stack(self, stack_id):\n        stack_type = self.stack_types[stack_id]\n        print(f'| --  Stack {stack_type.title()} (#{stack_id}) (share_weights_in_stack={self.share_weights_in_stack})')\n        blocks = []\n        for block_id in range(self.nb_blocks_per_stack):\n            block_init = NBeatsNet.select_block(stack_type)\n            if self.share_weights_in_stack and block_id != 0:\n                block = blocks[-1]  # pick up the last one when we share weights.\n            else:\n                block = block_init(self.hidden_layer_units, self.thetas_dim[stack_id],\n                                   self.device, self.backcast_length, self.forecast_length, self.nb_harmonics)\n                self.parameters.extend(block.parameters())\n            print(f'     | -- {block}')\n            blocks.append(block)\n        return blocks\n\n    @staticmethod\n    def select_block(block_type):\n        if block_type == NBeatsNet.SEASONALITY_BLOCK:\n            return SeasonalityBlock\n        elif block_type == NBeatsNet.TREND_BLOCK:\n            return TrendBlock\n        else:\n            return GenericBlock\n\n    def forward(self, backcast):\n        forecast = torch.zeros(size=(backcast.size()[0], self.forecast_length,))  # maybe batch size here.\n        for stack_id in range(len(self.stacks)):\n            for block_id in range(len(self.stacks[stack_id])):\n                b, f = self.stacks[stack_id][block_id](backcast)\n                backcast = backcast.to(self.device) - b\n                forecast = forecast.to(self.device) + f\n        return backcast, forecast\n\n\ndef seasonality_model(thetas, t, device):\n    p = thetas.size()[-1]\n    assert p <= thetas.shape[1], 'thetas_dim is too big.'\n    p1, p2 = (p // 2, p // 2) if p % 2 == 0 else (p // 2, p // 2 + 1)\n    s1 = torch.tensor([np.cos(2 * np.pi * i * t) for i in range(p1)]).float()  # H/2-1\n    s2 = torch.tensor([np.sin(2 * np.pi * i * t) for i in range(p2)]).float()\n    S = torch.cat([s1, s2])\n    return thetas.mm(S.to(device))\n\n\ndef trend_model(thetas, t, device):\n    p = thetas.size()[-1]\n    assert p <= 4, 'thetas_dim is too big.'\n    T = torch.tensor([t ** i for i in range(p)]).float()\n    return thetas.mm(T.to(device))\n\n\ndef linspace(backcast_length, forecast_length):\n    lin_space = np.linspace(-backcast_length, forecast_length, backcast_length + forecast_length)\n    b_ls = lin_space[:backcast_length]\n    f_ls = lin_space[backcast_length:]\n    return b_ls, f_ls\n\n\nclass Block(nn.Module):\n\n    def __init__(self, units, thetas_dim, device, backcast_length=10, forecast_length=5, share_thetas=False,\n                 nb_harmonics=None):\n        super(Block, self).__init__()\n        self.units = units\n        self.thetas_dim = thetas_dim\n        self.backcast_length = backcast_length\n        self.forecast_length = forecast_length\n        self.share_thetas = share_thetas\n        self.fc1 = nn.Linear(backcast_length, units)\n        self.fc2 = nn.Linear(units, units)\n        self.fc3 = nn.Linear(units, units)\n        self.fc4 = nn.Linear(units, units)\n        self.device = device\n        self.backcast_linspace, self.forecast_linspace = linspace(backcast_length, forecast_length)\n        if share_thetas:\n            self.theta_f_fc = self.theta_b_fc = nn.Linear(units, thetas_dim, bias=False)\n        else:\n            self.theta_b_fc = nn.Linear(units, thetas_dim, bias=False)\n            self.theta_f_fc = nn.Linear(units, thetas_dim, bias=False)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x.to(self.device)))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.relu(self.fc4(x))\n        return x\n\n    def __str__(self):\n        block_type = type(self).__name__\n        return f'{block_type}(units={self.units}, thetas_dim={self.thetas_dim}, ' \\\n               f'backcast_length={self.backcast_length}, forecast_length={self.forecast_length}, ' \\\n               f'share_thetas={self.share_thetas}) at @{id(self)}'\n\n\nclass SeasonalityBlock(Block):\n\n    def __init__(self, units, thetas_dim, device, backcast_length=10, forecast_length=5, nb_harmonics=None):\n        if nb_harmonics:\n            super(SeasonalityBlock, self).__init__(units, nb_harmonics, device, backcast_length,\n                                                   forecast_length, share_thetas=True)\n        else:\n            super(SeasonalityBlock, self).__init__(units, forecast_length, device, backcast_length,\n                                                   forecast_length, share_thetas=True)\n\n    def forward(self, x):\n        x = super(SeasonalityBlock, self).forward(x)\n        backcast = seasonality_model(self.theta_b_fc(x), self.backcast_linspace, self.device)\n        forecast = seasonality_model(self.theta_f_fc(x), self.forecast_linspace, self.device)\n        return backcast, forecast\n\n\nclass TrendBlock(Block):\n\n    def __init__(self, units, thetas_dim, device, backcast_length=10, forecast_length=5, nb_harmonics=None):\n        super(TrendBlock, self).__init__(units, thetas_dim, device, backcast_length,\n                                         forecast_length, share_thetas=True)\n\n    def forward(self, x):\n        x = super(TrendBlock, self).forward(x)\n        backcast = trend_model(self.theta_b_fc(x), self.backcast_linspace, self.device)\n        forecast = trend_model(self.theta_f_fc(x), self.forecast_linspace, self.device)\n        return backcast, forecast\n\n\nclass GenericBlock(Block):\n\n    def __init__(self, units, thetas_dim, device, backcast_length=10, forecast_length=5, nb_harmonics=None):\n        super(GenericBlock, self).__init__(units, thetas_dim, device, backcast_length, forecast_length)\n\n        self.backcast_fc = nn.Linear(thetas_dim, backcast_length)\n        self.forecast_fc = nn.Linear(thetas_dim, forecast_length)\n\n    def forward(self, x):\n        # no constraint for generic arch.\n        x = super(GenericBlock, self).forward(x)\n\n        theta_b = F.relu(self.theta_b_fc(x))\n        theta_f = F.relu(self.theta_f_fc(x))\n\n        backcast = self.backcast_fc(theta_b)  # generic. 3.3.\n        forecast = self.forecast_fc(theta_f)  # generic. 3.3.\n\n        return backcast, forecast","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split(arr, size):\n    arrays = []\n    while len(arr) > size:\n        slice_ = arr[:size]\n        arrays.append(slice_)\n        arr = arr[size:]\n    arrays.append(arr)\n    return arrays\n\n\ndef batcher(dataset, batch_size, infinite=False):\n    while True:\n        x, y = dataset\n        for x_, y_ in zip(split(x, batch_size), split(y, batch_size)):\n            yield x_, y_\n        if not infinite:\n            break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_fit(net, optimiser, data_generator, on_save_callback, device, max_grad_steps=10000):\n    train_loss = 0.0\n    count = 0\n    for grad_step, (x, target) in enumerate(data_generator):\n        count = count + 1\n        optimiser.zero_grad()\n        net.train()\n        backcast, forecast = net(torch.tensor(x, dtype=torch.float).to(device))\n        loss = F.mse_loss(forecast, torch.tensor(target, dtype=torch.float).to(device))\n        loss.backward()\n        optimiser.step()\n        train_loss = train_loss + loss.item()\n        if grad_step > max_grad_steps:\n            break\n    return train_loss/count\n\n\ndef save(path, model, optimiser, grad_step):\n    torch.save({\n        #'grad_step': grad_step,\n        'model_state_dict': model.state_dict(),\n        #'optimizer_state_dict': optimiser.state_dict(),\n    }, path)\n\n\ndef load(path, model, optimiser):\n    if os.path.exists(path):\n        grad_step = 0\n        checkpoint = torch.load(path)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        #optimiser.load_state_dict(checkpoint['optimizer_state_dict'])\n        #grad_step = checkpoint['grad_step']\n        print(f'Restored checkpoint from {path}.')\n        return grad_step\n    return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nEPOCHS = 24\nbatch_size = 3049 # 15245 # 3049 # 10 # 6098 # 15245","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast_length = 28\n\nMODEL_NAMES= {\n    #\"NNv5_cv1_tss_Adam_2011-2016-N-BEATS_Hx2\": forecast_length*2,\n    \"NNv5_cv1_tss_Adam_2011-2016-N-BEATS_Hx3\": forecast_length*3,\n    #\"NNv5_cv1_tss_Adam_2011-2016-N-BEATS_Hx4\": forecast_length*4,\n    #\"NNv5_cv1_tss_Adam_2011-2016-N-BEATS_Hx5\": forecast_length*5,\n}\n            \nfor name, backcast_length in MODEL_NAMES.items():\n    \n    MODEL_PATH = HOME + \"models/\" + name\n    if not os.path.exists(MODEL_PATH):\n        os.makedirs(MODEL_PATH)\n    \n    BEST_NAME = MODEL_PATH + \"/best.pth\"\n    \n    print(\"Training:\", MODEL_PATH)\n    val_cols = [c for c in range(1914-(backcast_length+forecast_length), 1914)]\n    val_pd = full_train_pd[[c for c in val_cols]]\n\n    test_cols = [c for c in range(1914-(backcast_length), 1914)]\n    test_pd = full_train_pd[[c for c in test_cols]]\n\n    train_cols = [c for c in range(1, 1914-(backcast_length+forecast_length))]\n    train_pd = full_train_pd[[c for c in train_cols]]     \n    \n    # Validation data\n    X_val, y_val = get_m5_data(backcast_length, forecast_length, val_pd, is_training=False)\n\n    net = NBeatsNet(device=device,\n                    #stack_types=[NBeatsNet.TREND_BLOCK, NBeatsNet.SEASONALITY_BLOCK, NBeatsNet.GENERIC_BLOCK],\n                    stack_types=[NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK],\n                    forecast_length=forecast_length,\n                    thetas_dims=[8, 8], # [2, 8, 3],\n                    nb_blocks_per_stack=3, #3,\n                    backcast_length=backcast_length,\n                    hidden_layer_units=1024,\n                    share_weights_in_stack=True, # False,\n                    nb_harmonics=None)\n\n    optimiser = optim.Adam(net.parameters(), lr=0.001)\n    \n    best_score = 99999\n    total_steps = 0\n    loops = 3\n    history = []\n    for epoch in range(1, EPOCHS + 1):\n        print(\"Epoch:\", epoch)\n        # Pick random range for each time series within [1-1745] days available\n        X_train, y_train = get_m5_data(backcast_length, forecast_length, train_pd, is_training=True)\n        max_grad_steps = int(X_train.shape[0]/batch_size) * loops\n        print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape, \"batches:\", max_grad_steps)\n        #print(\"NaN X_train:\", len(X_train[np.isnan(X_train)]), \"NaN y_train:\", len(y_train[np.isnan(y_train)]))\n\n        # Train all batches\n        data_gen = batcher((X_train, y_train), batch_size=batch_size, infinite=True)    \n        train_loss = model_fit(net, optimiser, data_gen, None, device, max_grad_steps)\n        total_steps = total_steps + max_grad_steps\n\n        # Evaluation\n        net.eval()\n        with torch.no_grad():\n            _, f = net(torch.tensor(X_val, dtype=torch.float))\n            predictions = f.cpu().numpy()\n            if len(predictions[np.isnan(predictions)]) == 0:\n                score = mean_squared_error(y_val, predictions) # np.sqrt\n                print(\"Training loss: %.5f, Evaluation loss: %.5f\" % (train_loss, score))\n                history.append((epoch, train_loss, score))\n                if score < best_score:\n                    print(\"Saving model, score improvement from %.5f to %.5f\" % (best_score, score))\n                    best_score = score\n                    save(BEST_NAME, net, optimiser, total_steps)\n            else:\n                print(\"Some NaN predictions:\", len(predictions[np.isnan(predictions)]), \"train_loss:\", train_loss)\n                \n    history_pd = pd.DataFrame(history[1:], columns=[\"epoch\", \"train_loss\", \"val_loss\"])\n    fig, ax = plt.subplots(figsize=(24, 5))\n    d = history_pd.plot(kind=\"line\", x=\"epoch\", ax=ax) \n    plt.show()\n    \n    # Load best and predict\n    _ = load(BEST_NAME, net, optimiser)\n    net.eval()\n    with torch.no_grad():\n        _, f = net(torch.tensor(test_pd.values, dtype=torch.float))\n        predictions = f.cpu().numpy()\n        \n    # Clean and save\n    predictions_pd = pd.DataFrame(predictions)\n    predictions_pd.index = test_pd.index\n    predictions_pd.columns = [\"F%d\" % c for c in range(1,29)]\n    for c in predictions_pd.columns:\n        predictions_pd[c] = np.where(predictions_pd[c] < 0, 0.0, predictions_pd[c])\n    predictions_pd = predictions_pd.reset_index()\n    # predictions_pd[\"id\"] = predictions_pd[\"id\"].astype(str) + \"_validation\"\n    # display(predictions_pd.head())\n    # print(\"NaN:\", predictions_pd.isnull().sum().sum())\n    predictions_pd.fillna(0, inplace=True)\n    submission = pd.read_csv(SAMPLE_SUBMISSION)[['id']]\n    submission = submission.merge(predictions_pd, on=['id'], how='left').fillna(0) # evaluation filled in with 0\n    submission.to_csv(MODEL_PATH + \"/submission.csv.gz\", index=False, compression=\"gzip\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}