{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport random\n\nimport warnings\n\nimport category_encoders as ce\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import SGDRegressor, LinearRegression\nfrom sklearn.preprocessing import StandardScaler\n\nimport lightgbm as lgb\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"warnings.filterwarnings(action='once')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv', parse_dates=['date'], index_col='date')\ncalendar.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.fillna('Regular', inplace=True)\n\nlabel_encoder = LabelEncoder()\nlabel_cols = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n\n# Apply label encoder \nfor col in label_cols:\n    calendar[col] = label_encoder.fit_transform(calendar[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar['is_weekend'] = calendar['wday'].apply(lambda x: 1 if x == 1 or x == 2 else 0)\nseasons = {1: 1, 2: 1, 12: 1, 3: 2, 4: 2, 5: 2, 6: 3, 7: 3, 8: 3, 9: 4, 10: 4, 11: 4 }\ncalendar['season'] = calendar['month'].apply(lambda x: seasons[x])\ncalendar.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv')\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prices = pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv')\nprices['wm_yr_wk'] = prices['wm_yr_wk'].astype(np.int16)\nprices['sell_price'] = prices['sell_price'].astype(np.float32)\nprices['wm_yr_wk'] = prices['wm_yr_wk'].astype(np.int16)\nprices.set_index(['store_id', 'item_id', 'wm_yr_wk'], inplace=True)\nprices = prices.sort_index()\nprices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_dataset_simple(df, label_column):\n    test_df = df.loc['d_1914':,:].copy()\n    \n    valid_models_df = df.loc['d_1914': 'd_1941',:].copy()\n    \n    _df = df.loc[df.index[0]: 'd_1941',:].copy()\n    samples_idx = random.sample(range(_df.shape[0]), int(0.2 * _df.shape[0]))\n    \n    valid_df = _df.iloc[samples_idx]\n    \n    idx_train = list(set(list(range(0, item_df.shape[0] ))) - set(samples_idx))\n    train_df = df.iloc[idx_train].copy()\n    #train_df.loc[:,label_column] = train_df.loc[:,label_column]\n    \n    return train_df, valid_df, test_df, valid_models_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation_data = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv')\n\nevaluation_data.loc[:,'id'] = evaluation_data['id'].apply(lambda x: x[:-10] + 'validation')\nevaluation_data.set_index('id', inplace=True)\nevaluation_data = evaluation_data.loc[:,'d_1914': 'd_1941'].copy()\n\ndef get_rmse(submission_validation_boost):\n    sub = submission_validation_boost.set_index('id')\n    error = mean_squared_error(sub.values, evaluation_data.loc[sub.index,:].values)\n    return error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def apply_target_encoder(item_df, cat_features_to_encoding):\n    # TargetEncoder CatBoostEncoder\n    target_enc = ce.TargetEncoder(cols=cat_features_to_encoding)\n    target_enc.fit(item_df.loc[:'d_1913'][cat_features_to_encoding], item_df.loc[:'d_1913']['sales'])\n\n    return item_df.join(target_enc.transform(item_df[cat_features_to_encoding]).add_suffix('_target'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_sales_features(item_df, horizon=7):\n    lag = horizon // 7\n    item_df[f'sales_shift_{horizon}'] = item_df['sales'].shift(horizon)\n\n    item_df[f'sales_shift_{horizon}_shift_{horizon}'] = item_df['sales'].shift(horizon * 2)\n    \n    item_df['sales_mean_rolling_4_wday'] = item_df.groupby(['wday'])['sales'].transform(lambda x: x.rolling(4).mean())\n    item_df[f'sales_mean_rolling_4_wday_shift_{lag}'] = item_df.groupby(['wday'])['sales_mean_rolling_4_wday'].transform(lambda x: x.shift(lag))\n    \n    item_df[f'sales_mean_rolling_shift_{lag}_4_wday_shift_{lag}'] = item_df.groupby(['wday'])['sales_mean_rolling_4_wday'].transform(lambda x: x.shift(lag * 2))\n    item_df[f'sales_diff_rolling_shift_{lag}_4_wday_shift_{lag}'] = item_df[f'sales_mean_rolling_4_wday_shift_{lag}'] - item_df[f'sales_mean_rolling_shift_{lag}_4_wday_shift_{lag}']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_price_features(item_df, horizon=7):\n    lag = horizon // 7\n    item_df['sell_price_diff_shift_1'] = item_df['sell_price'] - item_df['sell_price'].shift(1)\n    item_df[f'sell_price_diff_shift_{horizon}'] = item_df['sell_price'] - item_df['sell_price'].shift(horizon)\n    item_df['sell_price_diff_rolling_7'] = item_df['sell_price'] - item_df['sell_price'].rolling(7).mean()\n    \n    item_df[f'sell_price_diff_shift_{horizon}_shift_1'] = item_df['sell_price'].shift(horizon) - item_df['sell_price'].shift(horizon + 1)\n    item_df[f'sell_price_diff_shift_{horizon}_shift_{horizon}'] = item_df['sell_price'].shift(horizon) - item_df['sell_price'].shift(horizon * 2)\n    item_df[f'sell_price_diff_shift_{horizon}_rolling_7'] = item_df['sell_price'].shift(28) - item_df['sell_price'].shift(horizon).rolling(7).mean()\n    \n    item_df[f'sell_price_diff_rolling_7_diff_rolling_7_shift{horizon}'] = item_df['sell_price_diff_rolling_7'] - item_df['sell_price_diff_rolling_7'].shift(horizon)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_categorical_features(item_df, horizon=7):\n    item_df[f'month_target_diff_{horizon}'] = item_df['month_target'] - item_df['month_target'].shift(horizon)\n    item_df[f'season_target_diff_{horizon}'] = item_df['season_target'] - item_df['season_target'].shift(horizon)\n    item_df[f'event_name_1_target_diff_{horizon}'] = item_df['event_name_1_target'] - item_df['event_name_1_target'].shift(horizon)\n    item_df[f'event_type_1_target_diff_{horizon}'] = item_df['event_type_1_target'] - item_df['event_type_1_target'].shift(horizon)\n    item_df[f'event_name_2_target_diff_{horizon}'] = item_df['event_name_2_target'] - item_df['event_name_2_target'].shift(horizon)\n    item_df[f'event_type_2_target_diff_{horizon}'] = item_df['event_type_2_target'] - item_df['event_type_2_target'].shift(horizon)\n    item_df[f'snap_CA_target_diff_{horizon}'] = item_df['snap_CA_target'] - item_df['snap_CA_target'].shift(horizon)\n    item_df[f'snap_TX_target_diff_{horizon}'] = item_df['snap_TX_target'] - item_df['snap_TX_target'].shift(horizon)\n    item_df[f'snap_WI_target_diff_{horizon}'] = item_df['snap_WI_target'] - item_df['snap_WI_target'].shift(horizon)\n    \n    item_df[f'season_target_diff_{horizon}_shift{horizon}'] = item_df['season_target'].shift(horizon) - item_df['season_target'].shift(horizon)\n    item_df[f'event_name_1_target_diff_{horizon}_shift{horizon}'] = item_df['event_name_1_target'].shift(horizon) - item_df['event_name_1_target'].shift(horizon * 2)\n    item_df[f'event_type_1_target_diff_{horizon}_shift{horizon}'] = item_df['event_type_1_target'].shift(horizon) - item_df['event_type_1_target'].shift(horizon * 2)\n    item_df[f'event_name_2_target_diff_{horizon}_shift{horizon}'] = item_df['event_name_2_target'].shift(horizon) - item_df['event_name_2_target'].shift(horizon * 2)\n    item_df[f'event_type_2_target_diff_{horizon}_shift{horizon}'] = item_df['event_type_2_target'].shift(horizon) - item_df['event_type_2_target'].shift(horizon * 2)\n    item_df[f'snap_CA_target_diff_{horizon}_shift{horizon}'] = item_df['snap_CA_target'].shift(horizon) - item_df['snap_CA_target'].shift(horizon * 2)\n    item_df[f'snap_TX_target_diff_{horizon}_shift{horizon}'] = item_df['snap_TX_target'].shift(horizon) - item_df['snap_TX_target'].shift(horizon * 2)\n    item_df[f'snap_WI_target_diff_{horizon}_shift{horizon}'] = item_df['snap_WI_target'].shift(horizon) - item_df['snap_WI_target'].shift(horizon * 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_created_features(horizon=7):\n    lag = horizon // 7\n    \n    linear_feature_columns = [\n        #f'sales_shift_{horizon}',\n        f'sales_mean_rolling_4_wday_shift_{lag}',\n        f'sell_price_diff_rolling_7_diff_rolling_7_shift{horizon}',\n        \n        f'month_target_diff_{horizon}',\n        f'season_target_diff_{horizon}',\n        f'event_name_1_target_diff_{horizon}',\n        f'event_type_1_target_diff_{horizon}',\n        f'event_name_2_target_diff_{horizon}',\n        f'event_type_2_target_diff_{horizon}',\n        f'snap_CA_target_diff_{horizon}',\n        f'snap_TX_target_diff_{horizon}',\n        f'snap_WI_target_diff_{horizon}',\n    ]\n    \n    feature_columns = [\n        #f'sales_shift_{horizon}',\n        #f'sales_shift_{horizon}_shift_{horizon}',\n        f'sales_mean_rolling_4_wday_shift_{lag}',\n        f'sales_mean_rolling_shift_{lag}_4_wday_shift_{lag}',\n        f'sales_diff_rolling_shift_{lag}_4_wday_shift_{lag}',\n        \n        'sell_price_diff_shift_1',\n        'sell_price_diff_rolling_7',\n        f'sell_price_diff_shift_{horizon}',\n        f'sell_price_diff_shift_{horizon}_shift_1',\n        f'sell_price_diff_shift_{horizon}_shift_{horizon}',\n        f'sell_price_diff_shift_{horizon}_rolling_7', # ?\n        f'sell_price_diff_rolling_7_diff_rolling_7_shift{horizon}',\n        \n        f'month_target_diff_{horizon}',\n        f'season_target_diff_{horizon}',\n        f'event_name_1_target_diff_{horizon}',\n        f'event_type_1_target_diff_{horizon}',\n        f'event_name_2_target_diff_{horizon}',\n        f'event_type_2_target_diff_{horizon}',\n        f'snap_CA_target_diff_{horizon}',\n        f'snap_TX_target_diff_{horizon}',\n        f'snap_WI_target_diff_{horizon}',\n        \n        f'season_target_diff_{horizon}_shift{horizon}',\n        f'event_name_1_target_diff_{horizon}_shift{horizon}',\n        f'event_type_1_target_diff_{horizon}_shift{horizon}',\n        f'event_name_2_target_diff_{horizon}_shift{horizon}',\n        f'event_type_2_target_diff_{horizon}_shift{horizon}',\n        f'snap_CA_target_diff_{horizon}_shift{horizon}',\n        f'snap_TX_target_diff_{horizon}_shift{horizon}',\n        f'snap_WI_target_diff_{horizon}_shift{horizon}',\n    ]\n    return feature_columns, linear_feature_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nidx_feature = ['id']\ncategorical_feature = [\n    'wday',\n    'month',\n    'year',\n    'event_name_1',\n    'event_type_1',\n    'snap_CA',\n    'snap_TX',\n    'snap_WI',\n    'event_name_2',\n    'event_type_2',\n    'is_weekend',\n    'season',\n]\ncat_features_to_encoding = [\n    'wday',\n    'month',\n    'year',\n    'event_name_1',\n    'event_type_1',\n    'snap_CA',\n    'snap_TX',\n    'snap_WI',\n    'event_name_2',\n    'event_type_2',\n    'is_weekend',\n    'season',\n]\nencoded_cat_features = [i + '_target' for i in cat_features_to_encoding]\n#train_generated_features = ['day_min', 'day_mean', 'day_max']\n\nlabel_column = 'sales'\n\nsub_columns = ['id'] + ['F%s' % i for i in range(1, 29)]\nsubmission_validation = pd.DataFrame(columns=sub_columns)\nsubmission_evaluation = pd.DataFrame(columns=sub_columns)\nsubmission_validation_lgb = pd.DataFrame(columns=sub_columns)\nsubmission_evaluation_lgb = pd.DataFrame(columns=sub_columns)\nsubmission_validation_sgd = pd.DataFrame(columns=sub_columns)\nsubmission_evaluation_sgd = pd.DataFrame(columns=sub_columns)\nsubmission_validation_linear = pd.DataFrame(columns=sub_columns)\nsubmission_evaluation_linear = pd.DataFrame(columns=sub_columns)\nsubmission_validation_mean = pd.DataFrame(columns=sub_columns)\nsubmission_evaluation_mean = pd.DataFrame(columns=sub_columns)\n\nrandom.seed(2)\nsamples_idx = random.sample(range(train_data.shape[0]), 300)\n\nmodels = {'linear': 0, 'lgb': 0, 'mean': 0, 'linearSGD': 0}\n\nfeature_importance_init =False\nfeature_importance = None\nfeature_importance_n = 0\n\n#for iteration, i in enumerate(samples_idx):\nfor iteration, i in enumerate(range(train_data.shape[0])):  \n    if iteration % 1000 == 1:\n        print(iteration, train_data.shape[0])\n        print('mean', get_rmse(submission_validation_mean))\n        print('linear', get_rmse(submission_validation_linear))\n        print('sgd', get_rmse(submission_validation_sgd))\n        print('lgb', get_rmse(submission_validation_lgb))\n        print('pred', get_rmse(submission_validation))\n    \n    row = train_data.loc[i]\n    all_id = row.id\n    item_id = row.item_id\n    dept_id = row.dept_id\n    cat_id = row.cat_id\n    store_id = row.store_id\n    state_id = row.state_id\n    sales = row['d_1':]\n    item_df = calendar.join(sales.to_frame('sales'), on='d')\n    \n    # add prices\n    item_df = item_df.join(prices.loc[store_id, item_id], on=['wm_yr_wk'])\n    item_df.sales.fillna(0, inplace=True)\n    item_df.loc[:,'sales'] = item_df.sales.astype('int64')\n    # drop early zeros rows\n    item_df = item_df.set_index('d')\n    \n    first_sale = item_df[item_df.sales!=0].index[0]\n    first_sale_int = int(first_sale[2:]) # 'd_1914' -> 1914\n    if first_sale_int > 1914 - 90:\n        first_sale = 'd_{}'.format(1914  - 90) # garanted 90 days history \n    item_df = item_df.loc[first_sale:,:]\n    \n    # apply target encoding\n    item_df = apply_target_encoder(item_df, cat_features_to_encoding)\n    \n    feature_columns = []\n    linear_feature_columns = []\n    # create features\n    for horizon in [28,]:\n        lag = horizon // 7 \n        # add price features\n        create_price_features(item_df, horizon=horizon)\n        # add categorical features\n        create_categorical_features(item_df, horizon=horizon)\n        # add sales features\n        create_sales_features(item_df, horizon=horizon)\n        features, linear_features = get_created_features(horizon=horizon)\n        \n        feature_columns += features\n        linear_feature_columns = linear_features\n\n    feature_columns = feature_columns + encoded_cat_features + categorical_feature\n    \n    # drop rows with na\n    item_df.dropna(inplace=True)\n    \n    predictions_list = []\n        \n    try:\n        train_df, valid_df, test_df, valid_models_df = get_dataset_simple(item_df[feature_columns + [label_column]], label_column=label_column)\n        \n        sc_X = StandardScaler()\n        sc_y = StandardScaler()\n        X_train = sc_X.fit_transform(train_df[linear_feature_columns])\n        y_train = sc_y.fit_transform(train_df[label_column].values.reshape(-1,1))\n\n        X_test = sc_X.fit_transform(test_df[linear_feature_columns])\n\n        X_valid = sc_X.fit_transform(valid_df[linear_feature_columns])\n        y_valid = sc_y.fit_transform(valid_df[label_column].values.reshape(-1,1))\n\n        model_sgd = SGDRegressor(max_iter=3000)\n        model_sgd.fit(X_train, y_train.reshape(-1))\n        prediction_sgd = sc_y.inverse_transform(model_sgd.predict(X_test).reshape(-1,1)).reshape(-1).tolist()\n        predictions_list.append(['linearSGD', prediction_sgd, mean_squared_error(valid_models_df.sales.values, prediction_sgd[0:28])])\n        \n        model = LinearRegression()\n        model.fit(X_train, y_train.reshape(-1))\n        prediction_linear = sc_y.inverse_transform(model.predict(X_test).reshape(-1,1)).reshape(-1).tolist()\n        predictions_list.append(['linear', prediction_linear, mean_squared_error(valid_models_df.sales.values, prediction_linear[0:28])])\n        \n        #lgb\n        dtrain = lgb.Dataset(train_df[feature_columns], label=train_df[label_column], categorical_feature=categorical_feature)\n        dvalid = lgb.Dataset(valid_df[feature_columns], label=valid_df[label_column], categorical_feature=categorical_feature)\n\n        param = {\n            'boosting_type': 'gbdt',\n            'objective': 'tweedie',\n            #'tweedie_variance_power': 1.1,\n            'metric': 'rmse',\n            'subsample': 0.5,\n            'subsample_freq': 1,\n            'learning_rate': 0.03,\n            'num_leaves': 64,\n            #'max_depth': 7,\n            'min_data_in_leaf': min(1024, test_df.shape[0] // 4),\n            'feature_fraction': 0.1,\n            #'max_bin': 10,\n            'boost_from_average': False,\n            'verbose': -1,\n            'lambda_l1': 0.8,\n            #'lambda_l2': 0,\n            #'min_gain_to_split': 1.,\n            #'min_sum_hessian_in_leaf': 1e-3,\n        }\n        # https://lightgbm.readthedocs.io/en/latest/index.html\n        bst = lgb.train(param, dtrain, valid_sets=[dvalid], num_boost_round = 2400, early_stopping_rounds=500, verbose_eval=False, categorical_feature=categorical_feature)\n        if not feature_importance_init:\n            feature_importance_init = True\n            feature_importance = bst.feature_importance()  \n        else:\n            feature_importance += bst.feature_importance()\n        feature_importance_n += 1\n\n        prediction_lgb = bst.predict(test_df[feature_columns]).tolist()\n        predictions_list.append(['lgb', prediction_lgb, mean_squared_error(valid_models_df.sales.values, prediction_lgb[0:28])])\n    except Exception as e:\n        print(i, all_id, item_df.shape)\n        print(e)\n        prediction_lgb = test_df[f'sales_mean_rolling_4_wday_shift_{lag}'].values.tolist()\n        \n    prediction_mean = (test_df[f'sales_mean_rolling_4_wday_shift_{lag}']).values.tolist()\n    predictions_list.append(['mean', prediction_mean, mean_squared_error(valid_models_df.sales.values, prediction_mean[0:28])])\n    \n    \n    best_predictions = sorted(predictions_list, key=lambda x: x[2])[0]\n    models[best_predictions[0]] += 1\n\n    key = all_id[:-10] + 'validation'\n    submission_validation.loc[len(submission_validation)] = [key] + best_predictions[1][0:28]\n    submission_evaluation.loc[len(submission_evaluation)] = [all_id] + best_predictions[1][28:56]\n    \n    submission_validation_lgb.loc[len(submission_validation_lgb)] = [key] + prediction_lgb[0:28]\n    submission_evaluation_lgb.loc[len(submission_evaluation_lgb)] = [all_id] + prediction_lgb[28:56]\n    \n    submission_validation_sgd.loc[len(submission_validation_sgd)] = [key] + prediction_sgd[0:28]\n    submission_evaluation_sgd.loc[len(submission_evaluation_sgd)] = [all_id] + prediction_sgd[28:56]\n    \n    submission_validation_linear.loc[len(submission_validation_linear)] = [key] + prediction_linear[0:28]\n    submission_evaluation_linear.loc[len(submission_evaluation_linear)] = [all_id] + prediction_linear[28:56]\n    \n    submission_validation_mean.loc[len(submission_validation_mean)] = [key] + prediction_mean[0:28]\n    submission_evaluation_mean.loc[len(submission_evaluation_mean)] = [all_id] + prediction_mean[28:56]\n\nprint('Final')\nprint('mean', get_rmse(submission_validation_mean))\nprint('linear', get_rmse(submission_validation_linear))\nprint('sgd', get_rmse(submission_validation_sgd))\nprint('lgb', get_rmse(submission_validation_lgb))\nprint('pred', get_rmse(submission_validation))\nprint(models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = sorted(list(zip(feature_columns, (feature_importance / feature_importance_n).tolist())), key=lambda x: x[1], reverse=True)\nfor i, j in feature_imp:\n    print(i, j)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Final\nmean 5.935171130952381\nlinear 8.818988455178928\nsgd 8.815847165302436\nlgb 4.544536117878011\npred 4.426746593113699\n{'linear': 23, 'lgb': 240, 'mean': 24, 'linearSGD': 13}\nCPU times: user 12min 49s, sys: 9.53 s, total: 12min 58s\nWall time: 4min 29s'''\npass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Final\nmean 5.935171130952381\nlinear 4.912764838532745\nsgd 4.916746732656525\nlgb 4.590960128500298\npred 4.201384342558303\n{'linear': 60, 'lgb': 137, 'mean': 25, 'linearSGD': 78}\nCPU times: user 8min 22s, sys: 7.44 s, total: 8min 30s\nWall time: 3min 56s'''\npass # only categorical_feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Final\nmean 5.935171130952381\nlinear 4.912764838532745\nsgd 4.91254679162634\nlgb 4.952590789885752\npred 4.577687654284275\n{'linear': 71, 'lgb': 147, 'mean': 21, 'linearSGD': 61}\nCPU times: user 12min 3s, sys: 9.76 s, total: 12min 13s\nWall time: 4min 52s'''\npass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Final\n\nmean 6.42752571100595\n\nlinear 3.660612029319008e+22\n\nsgd 3.1828151612797164\n\nlgb 4.006446010547885\n\npred 2.465040438042305\n\n{'linear': 2524, 'lgb': 23657, 'mean': 584, 'linearSGD': 3725}\n\nCPU times: user 14h 27min 17s, sys: 12min 41s, total: 14h 39min 59s\n\nWall time: 5h 40min 12s","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = submission_validation.append(submission_evaluation)\nsubmission.to_csv('/kaggle/working/my_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_mean = submission_validation_mean.append(submission_evaluation_mean)\nsubmission_mean.to_csv('/kaggle/working/my_submission_mean.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_lgb = submission_validation_lgb.append(submission_evaluation_lgb)\nsubmission_lgb.to_csv('/kaggle/working/my_submission_lgb.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_linear = submission_validation_linear.append(submission_evaluation_linear)\nsubmission_linear.to_csv('/kaggle/working/my_submission_linear.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_sgd = submission_validation_sgd.append(submission_evaluation_sgd)\nsubmission_sgd.to_csv('/kaggle/working/my_submission_sgd.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}