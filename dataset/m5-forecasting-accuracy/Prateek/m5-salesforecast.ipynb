{"cells":[{"metadata":{},"cell_type":"markdown","source":"!pip install -q git+https://github.com/tensorflow/docs","execution_count":null},{"metadata":{"_uuid":"61b754db-67f1-416b-b69e-e95bd8aab18e","_cell_guid":"3cc61d2d-19ad-441d-b8b0-16f8577b12c7","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestRegressor \nfrom sklearn.linear_model import LinearRegression\nimport pathlib\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_docs as tfdocs\nimport tensorflow_docs.plots\nimport tensorflow_docs.modeling\n\n# Read files and creating dataframes\nsell_prices = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sell_prices.csv\")\nsell_prices['wm_yr_wk']=sell_prices['wm_yr_wk'].astype(np.int32)\n\navg_Coming_Event_1=pd.read_csv(\"../input/average-unit-sold/avg_Coming_Event_1.csv\").rename(columns={'Units_sold':'avg_Coming_Event_1'})\navg_Coming_Event_2=pd.read_csv(\"../input/average-unit-sold/avg_Coming_Event_2.csv\").rename(columns={'Units_sold':'avg_Coming_Event_2'})\navg_event_name_1=pd.read_csv(\"../input/average-unit-sold/avg_event_name_1.csv\").rename(columns={'Units_sold':'avg_event_name_1'})\navg_event_name_2=pd.read_csv(\"../input/average-unit-sold/avg_event_name_2.csv\").rename(columns={'Units_sold':'avg_event_name_2'})\navg_wday=pd.read_csv(\"../input/average-unit-sold/avg_wday.csv\").rename(columns={'Units_sold':'avg_wday'})\navg_month=pd.read_csv(\"../input/average-unit-sold/avg_month.csv\").rename(columns={'Units_sold':'avg_month'})\navg_week=pd.read_csv(\"../input/average-unit-sold/avg_week.csv\").rename(columns={'Units_sold':'avg_week'})\navg_dept_id=pd.read_csv(\"../input/average-unit-sold/avg_dept_id.csv\").rename(columns={'Units_sold':'avg_dept_id'})\navg_cat_id=pd.read_csv(\"../input/average-unit-sold/avg_cat_id.csv\").rename(columns={'Units_sold':'avg_cat_id'})\navg_store_id=pd.read_csv(\"../input/average-unit-sold/avg_store_id.csv\").rename(columns={'Units_sold':'avg_store_id'})\navg_state_id=pd.read_csv(\"../input/average-unit-sold/avg_state_id.csv\").rename(columns={'Units_sold':'avg_state_id'})\n\nelasticity=pd.read_csv(\"../input/elasticity/elasticity.csv\", usecols=['id','elasticity_id_wk'])\nelasticity_cat_lvl=pd.read_csv(\"../input/elasticity-cat-lvl/elasticity_cat_lvl.csv\", usecols=['cat_id','elasticity_id_wk_cat_lvl'])\n\ncalendar = pd.read_csv(\"../input/calendar/calendar_v1.csv\")\ncalendar['date']=pd.to_datetime(calendar['date'],format=\"%d/%m/%y\")\n#usecols = ['date','wm_yr_wk','wday','month','d',\n#'event_name_1', 'event_type_1', 'event_name_2',\n#'event_type_2'])\ntrain = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv\")\n\n\n#selecting fraction of data while keeping same composition of catg ids\nf=0.2\nsales_train_validation=train[train['cat_id']=='FOODS'].sample(frac=f)\nsales_train_validation=sales_train_validation.append(train[train['cat_id']=='HOUSEHOLD'].sample(frac=f))\nsales_train_validation=sales_train_validation.append(train[train['cat_id']=='HOBBIES'].sample(frac=f))\ndel train\n#train=pd.DataFrame()\nsales_train_validation=sales_train_validation.reset_index()\nsales_train_validation=sales_train_validation.drop(columns=['index'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting Sales train data day columns into day variable\nsales_train_validation=sales_train_validation.melt(id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'], \n                                                   var_name='day_id',value_name='Units_sold')\nsales_train_validation['Units_sold']=sales_train_validation['Units_sold'].astype(np.int16)\nsales_train_validation['day_id']=sales_train_validation['day_id'].str[2:].astype(np.int64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"markdown","source":"#Merge Calendar data with sales train validation v1\n#calendar.head()\ncalendar['wday']=calendar['wday'].astype(np.int8)\ncalendar['month']=calendar['month'].astype(np.int8)\ncalendar['date']=pd.to_datetime(calendar['date'], infer_datetime_format=True)\n\ncalendar=calendar.rename(columns={'d':'day_id'})\ncalendar['day_id']=calendar['day_id'].str[2:]\n\ncalendar_hol_period_v1=calendar[['day_id','event_name_1','event_type_1']][calendar['event_type_1'].notnull()]\ncalendar_hol_period_v1=calendar_hol_period_v1.reset_index().drop(columns=['index'])\ncalendar_hol_period_v1['day_id_hol-5']=calendar_hol_period_v1['day_id'].astype(np.int16)-7\ncalendar_hol_period_v1['day_id_hol-1']=calendar_hol_period_v1['day_id'].astype(np.int16)-1\ncalendar_hol_period_v1['key']=0\ncalendar_hol_period_v1['key']=calendar_hol_period_v1['key'].astype(np.int8)\n\ncalendar_hol_period_v2=calendar[['day_id','event_name_2','event_type_2']][calendar['event_type_2'].notnull()]\ncalendar_hol_period_v2=calendar_hol_period_v2.reset_index().drop(columns=['index'])\ncalendar_hol_period_v2['day_id_hol-5']=calendar_hol_period_v2['day_id'].astype(np.int16)-7\ncalendar_hol_period_v2['day_id_hol-1']=calendar_hol_period_v2['day_id'].astype(np.int16)-1\ncalendar_hol_period_v2['key']=0\ncalendar_hol_period_v2['key']=calendar_hol_period_v2['key'].astype(np.int8)\n\n\ncalendar_temp_v1=calendar.copy()\ncalendar_temp_v1['key']=0\ncalendar_temp_v1['key']=calendar_temp_v1['key'].astype(np.int8)\ncalendar_temp_v1=calendar_temp_v1.merge(calendar_hol_period_v1, on='key', how='outer')\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"conditions_subset_hol_day_and_5_v1=((calendar_temp_v1['day_id_x'].astype(int)<=calendar_temp_v1['day_id_hol-1'])\n                  &(calendar_temp_v1['day_id_x'].astype(int)>=calendar_temp_v1['day_id_hol-5']))\n                                 \ncalendar_temp_v1=calendar_temp_v1[conditions_subset_hol_day_and_5_v1]\ncalendar_temp_v1=calendar_temp_v1.reset_index().drop(columns=['index','key'])\ncalendar_temp_v1=calendar_temp_v1[['day_id_x','event_name_1_y','event_type_1_y']]\n#calendar_temp['hol_period']=1\n\ncalendar_temp_v1=calendar_temp_v1.rename(columns={'day_id_x':'day_id'})\n\n\ncalendar_temp_v2=calendar.copy()\ncalendar_temp_v2['key']=0\ncalendar_temp_v2['key']=calendar_temp_v2['key'].astype(np.int8)\ncalendar_temp_v2=calendar_temp_v2.merge(calendar_hol_period_v2, on='key', how='outer')\nconditions_subset_hol_day_and_5_v1=((calendar_temp_v2['day_id_x'].astype(int)<=calendar_temp_v2['day_id_hol-1'])\n                  &(calendar_temp_v2['day_id_x'].astype(int)>=calendar_temp_v2['day_id_hol-5']))\n                                 \ncalendar_temp_v2=calendar_temp_v2[conditions_subset_hol_day_and_5_v1]\ncalendar_temp_v2=calendar_temp_v2.reset_index().drop(columns=['index','key'])\ncalendar_temp_v2=calendar_temp_v2[['day_id_x','event_name_2_y','event_type_2_y']]\n#calendar_temp['hol_period']=1\n\ncalendar_temp_v2=calendar_temp_v2.rename(columns={'day_id_x':'day_id'})\n\n\n\ncalendar_v1=calendar.copy()\ncalendar_v2=calendar.copy()\ncalendar_v1=calendar_v1.merge(calendar_temp_v1, how='left', on='day_id')\ncalendar_v1=calendar_v1.rename(columns={'event_name_1_y':'Coming_Event_1','event_type_1_y':'Coming_Event_Type_1'})\ncalendar_v1=calendar_v1.drop_duplicates(subset=['day_id'], keep='first')[['day_id','Coming_Event_1','Coming_Event_Type_1']]\n\ncalendar_v2=calendar_v2.merge(calendar_temp_v2, how='left', on='day_id')\ncalendar_v2=calendar_v2.rename(columns={'event_name_2_y':'Coming_Event_2','event_type_2_y':'Coming_Event_Type_2'})\ncalendar_v2=calendar_v2.drop_duplicates(subset=['day_id'], keep='first')[['day_id','Coming_Event_2','Coming_Event_Type_2']]\n\ncalendar=calendar.merge(calendar_v1, how='left', on='day_id')\ncalendar=calendar.merge(calendar_v2, how='left', on='day_id')\ncalendar=calendar.drop_duplicates(subset=['wm_yr_wk', 'wday', 'month', 'day_id', \n                                          'event_name_1', 'event_type_1','event_name_2', 'event_type_2'], keep='first')\n\ndel calendar_v1\ndel calendar_v2\ndel calendar_temp_v1\ndel calendar_temp_v2\n#calendar.to_csv(\"/kaggle/working/calendar_v1.csv\")","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_validation=sales_train_validation.merge(calendar, left_on=['day_id'], right_on=['day_id'])\nsales_train_validation=sales_train_validation.fillna(0)\nsales_train_validation['week']=sales_train_validation['wm_yr_wk'].astype(str).str[3:].astype(np.int16)\n#column=['Coming_Event_1','Coming_Event_2'\n#     ,'event_name_1','event_name_2','wday','month','week','dept_id','cat_id','store_id', 'state_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create lag variables on units sold for 1 week\nsales_train_validation=sales_train_validation[['date','id', 'item_id', 'dept_id', 'cat_id','week',\n                                               'store_id', 'state_id', 'day_id', 'wm_yr_wk', 'wday',\n                                               'month', 'event_name_1','event_type_1', 'event_name_2',\n                                               'event_type_2', 'Coming_Event_1','Coming_Event_Type_1',\n                                               'Coming_Event_2','Coming_Event_Type_2','Units_sold']].sort_values(by=['id','date']).reset_index().drop(columns='index')\n\nfor i in [7]:\n    sales_train_validation_v1=sales_train_validation[['id','date','Units_sold']]\n    sales_train_validation_v1=sales_train_validation_v1.set_index(['date','id']\n                                                             ).unstack().shift(i).stack(dropna=False\n                                                                                        ).reset_index().sort_values(by=['id','date']).rename(columns={'Units_sold':'Units_sold_d-'+str(i)})\n    sales_train_validation['Units_sold_d-'+str(i)]=sales_train_validation_v1['Units_sold_d-'+str(i)].reset_index().drop(columns='index')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Rolling mean for Units Sold\n\nfor i in [3,5,8]:\n    sales_train_validation_v1=sales_train_validation[['id','date','Units_sold']]\n    sales_train_validation_v1=sales_train_validation_v1.set_index(['date','id']\n                                                             ).unstack().rolling(i).mean().stack(dropna=False\n                                                                                        ).reset_index().sort_values(by=['id','date']).rename(columns={'Units_sold':'Units_sold_mean_'+str(i)})\n    sales_train_validation['Units_sold_mean_'+str(i)]=sales_train_validation_v1['Units_sold_mean_'+str(i)].reset_index().drop(columns='index')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Elasticity Calculation\n# calculated at Id, wm_yr_wk level becuase prices only change after a week\n## Elasticity=(dQ/Q)/(dS/S)\nsell_prices['id']=sell_prices['item_id']+'_'+sell_prices['store_id']+'_validation'\nsell_prices=sell_prices.drop(columns=['item_id','store_id'])\nsell_prices=sell_prices[['id','wm_yr_wk','sell_price']]\nsell_prices=sell_prices.sort_values(by=['id','wm_yr_wk']).reset_index().drop(columns=['index'])\n\nsell_prices=sell_prices.merge(sell_prices.set_index(['wm_yr_wk','id']).unstack().shift(1).stack(dropna=False).reset_index().rename(columns={'sell_price':'sell_price_w-1'}),\n                              how='left',on=['wm_yr_wk','id'],)\n\n\nsell_prices['unit_delta_price']=(sell_prices['sell_price_w-1']-sell_prices['sell_price'])/sell_prices['sell_price_w-1']\n\n\nweekly_sales=pd.pivot_table(sales_train_validation, values='Units_sold', index=['wm_yr_wk','id'], aggfunc='sum').reset_index()\n#weekly_sales=weekly_sales.merge(weekly_sales.set_index(['wm_yr_wk','id']).unstack().shift(1).stack(dropna=False).reset_index().rename(columns={'Units_sold':'Units_sold_w-1test'}),\n#                             how='left', on=['wm_yr_wk','id'])\nweekly_sales['Units_sold_w-1']=weekly_sales.set_index(['wm_yr_wk','id']).unstack().shift(1).stack(dropna=False).reset_index().rename(columns={'Units_sold':'Units_sold_w-1'})['Units_sold_w-1']\n\nweekly_sales['unit_delta_Units_sold']=(weekly_sales['Units_sold_w-1']-weekly_sales['Units_sold'])/weekly_sales['Units_sold_w-1']\nsell_prices=sell_prices.merge(weekly_sales,how='left',on=['wm_yr_wk','id'])\n\nsell_prices['elasticity_id_wk']=sell_prices['unit_delta_Units_sold']/sell_prices['unit_delta_price']\n\nelasticity=pd.pivot_table(sell_prices[np.isfinite(sell_prices['elasticity_id_wk'])], \n               values='elasticity_id_wk', index=['id'], aggfunc='mean').reset_index()\nelasticity_cat_lvl=elasticity.copy()\nelasticity_cat_lvl['cat_id']=elasticity_cat_lvl['id'].str.split('_',n = 1, expand = True)[0]\nelasticity_cat_lvl=pd.pivot_table(elasticity_cat_lvl, values='elasticity_id_wk', index=['cat_id'], aggfunc='mean').rename(columns={'elasticity_id_wk':'elasticity_id_wk_cat_lvl'})\n\nsell_prices=sell_prices[['wm_yr_wk','sell_price','id']]\nsales_train_validation=sales_train_validation.merge(elasticity, how='left', on='id')\nsales_train_validation=sales_train_validation.merge(elasticity_cat_lvl, how='left', on='cat_id')\nelasticity_cat_lvl.to_csv('/kaggle/working/elasticity_cat_lvl.csv')\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_validation=sales_train_validation.merge(elasticity, how='left', on='id')\nsales_train_validation=sales_train_validation.merge(elasticity_cat_lvl, how='left', on='cat_id')\nsales_train_validation['elasticity_id_wk'][sales_train_validation['elasticity_id_wk'].isnull()]=sales_train_validation['elasticity_id_wk_cat_lvl']\nsales_train_validation=sales_train_validation.drop(columns=['elasticity_id_wk_cat_lvl'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Discount Variable\nsell_prices['id']=sell_prices['item_id']+'_'+sell_prices['store_id']+'_validation' \nsell_prices=sell_prices.drop(columns=['item_id','store_id'])\n\nsell_prices['sell_price_max']=sell_prices.groupby(['id'])['sell_price'].transform(np.max)\nsell_prices['Disc']=((sell_prices['sell_price_max']-sell_prices['sell_price'])/sell_prices['sell_price_max'])\n\nsales_train_validation=sales_train_validation.merge(sell_prices,how='left' ,left_on=['id','wm_yr_wk'],\n                                                    right_on=['id','wm_yr_wk'])\nsales_train_validation['Disc'][sales_train_validation['Disc'].isnull()]=np.mean(sell_prices['Disc'][sell_prices['Disc']>0])\n\n#sales_train_validation=sales_train_validation[sales_train_validation['sell_price'].notnull()].reset_index()\n#sales_train_validation=sales_train_validation.drop(columns=['index'])\n\nsales_train_validation['sell_price_max']=sales_train_validation.groupby(['id'])['sell_price'].transform(np.max)\nsales_train_validation['Disc']=(sales_train_validation['sell_price_max'].sub(sales_train_validation['sell_price']))/sales_train_validation['sell_price_max']\n\n#Discount Variable lag for 1 week\nfor i in [3,5,7]:\n    sales_train_validation_v1=sales_train_validation[['id','date','Disc']]\n    sales_train_validation_v1=sales_train_validation_v1.set_index(['date','id']\n                                                             ).unstack().shift(i).stack(dropna=False\n                                                                                        ).reset_index().sort_values(by=['id','date']).rename(columns={'Disc':'Disc_d-'+str(i)})\n    sales_train_validation['Disc_d-'+str(i)]=sales_train_validation_v1['Disc_d-'+str(i)].reset_index().drop(columns='index')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsales_train_validation=sales_train_validation.merge(avg_Coming_Event_1, how='left', on=['Coming_Event_1'])\nsales_train_validation=sales_train_validation.merge(avg_Coming_Event_2, how='left', on=['Coming_Event_2'])\nsales_train_validation=sales_train_validation.merge(avg_event_name_1, how='left', on=['event_name_1'])\nsales_train_validation=sales_train_validation.merge(avg_event_name_2, how='left', on=['event_name_2'])\nsales_train_validation=sales_train_validation.merge(avg_wday, how='left', on=['wday'])\nsales_train_validation=sales_train_validation.merge(avg_month, how='left', on=['month'])\nsales_train_validation=sales_train_validation.merge(avg_week, how='left', on=['week'])\nsales_train_validation=sales_train_validation.merge(avg_dept_id, how='left', on=['dept_id'])\nsales_train_validation=sales_train_validation.merge(avg_cat_id, how='left', on=['cat_id'])\nsales_train_validation=sales_train_validation.merge(avg_store_id, how='left', on=['store_id'])\nsales_train_validation=sales_train_validation.merge(avg_state_id, how='left', on=['state_id'])\n\nsales_train_validation=sales_train_validation.drop(columns= ['item_id', 'dept_id', 'cat_id', 'week', 'store_id',\n       'state_id', 'wm_yr_wk', 'wday', 'month', 'event_name_1',\n       'event_type_1', 'event_name_2', 'event_type_2', 'Coming_Event_1',\n       'Coming_Event_Type_1', 'Coming_Event_2', 'Coming_Event_Type_2'])\n#sales_train_validation=sales_train_validation.fillna(sales_train_validation.mean())\nsales_train_validation=sales_train_validation.fillna(0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"\n\n##average sale of product in festival_1\nlst=['Coming_Event_1','Coming_Event_2'\n     ,'event_name_1','event_name_2','wday','month','week']\nfor i in lst:\n    sales_train_validation['avg_'+i]=sales_train_validation.groupby(['id',i])['Units_sold'].transform(np.mean)\n\n\nsales_train_validation=sales_train_validation.drop(columns=\n                                                   ['Coming_Event_1','Coming_Event_Type_1',\n                                                    'Coming_Event_2','Coming_Event_Type_2'])\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"\nsales_train_validation.columns\n#list=['dept_id', 'cat_id', 'store_id', 'state_id', 'wm_yr_wk', 'wday', 'month', 'year', 'event_name_1',\n#       'event_type_1', 'event_name_2', 'event_type_2', 'sell_price']\nlist=['Chanukah End', 'Christmas', 'Cinco De Mayo',\n       'ColumbusDay', 'Easter', 'Eid al-Fitr', 'EidAlAdha', \"Father's day\",\n       'Halloween', 'IndependenceDay', 'LaborDay', 'LentStart', 'LentWeek2',\n       'MartinLutherKingDay', 'MemorialDay', \"Mother's day\", 'NBAFinalsEnd',\n       'NBAFinalsStart', 'NewYear', 'OrthodoxChristmas', 'OrthodoxEaster',\n       'Pesach End', 'PresidentsDay', 'Purim End', 'Ramadan starts',\n       'StPatricksDay', 'SuperBowl', 'Thanksgiving', 'ValentinesDay',\n       'VeteransDay', 'Cultural', 'National', 'Religious', 'Sporting']\ndf=pd.DataFrame()\ndf['overall_avg']=[0,np.mean(sales_train_validation['Units_sold'])]\nfor i in list :\n    temp=pd.pivot_table(sales_train_validation,values='Units_sold', index=[i], aggfunc='mean').reset_index()\n    temp_1=pd.pivot_table(sales_train_validation,values='Units_sold', index=[i], aggfunc='count').reset_index()\n    temp_1=temp_1.rename(columns={'Units_sold':'Count'})\n    df=pd.concat([df,temp,temp_1[['Count']]],axis=1)\n\n#print(df)\ndf.to_csv('/kaggle/working/df.csv')\n\n#sales_train_validation[['event_name_1','event_type_1']].drop_duplicates(['event_name_1',\n#                                                                         'event_type_1'],keep='first').to_csv('/kaggle/working/event_name_type.csv')","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"lst_1=['item_id','cat_id','dept_id','store_id']\nfor j in lst_1:\n    sales_train_validation['avg_'+j]=sales_train_validation.groupby(j)['Units_sold'].transform(np.mean)\n\n##average sale of a state\nsales_train_validation['avg_state_sales']=sales_train_validation.groupby(['item_id','state_id'])['Units_sold'].transform(np.mean)\nsales_train_validation=sales_train_validation.drop(columns=['state_id'])","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"sales_train_validation['day_id']=sales_train_validation['day_id'].astype(int)\n\nsales_train_validation['date'] = pd.to_datetime(sales_train_validation['date'])\ny=pd.pivot_table(sales_train_validation[sales_train_validation['cat_id']=='FOODS'], values='Units_sold', index=['date'], aggfunc='sum')\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(y[y['Units_sold']>0], model = \"multiplicative\", period=1)\ndecomposition.plot()\n\ndecomposition.resid\ndecomposition.seasonal\ndecomposition.trend\ndecomposition.observed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_9=['Units_sold_d-1', 'Units_sold_d-7', 'Units_sold_mean_3']\nsales_train_validation.columns\ncolumns_11=['Units_sold_d-1', 'Units_sold_d-7', \n            'Units_sold_mean_3', 'Units_sold_mean_5_d-1', 'sell_price', 'Disc',\n            'Disc_d-1', 'Disc_d-7',\n            'avg_Coming_Event_1','avg_Coming_Event_2',\n            'avg_event_name_1', 'avg_event_name_2', 'avg_wday', \n            'avg_month', 'avg_week','elasticity_id_wk']\n\ncolumns_11=['Units_sold_d-7', \n            'Units_sold_mean_8', 'sell_price', 'Disc',\n            'Disc_d-7',\n            'avg_Coming_Event_1','avg_Coming_Event_2',\n            'avg_event_name_1', 'avg_event_name_2', 'avg_wday', \n            'avg_month', 'avg_week','elasticity_id_wk']\ncolumns_11=['Units_sold_d-7', 'Units_sold_mean_8', 'elasticity_id_wk',\n           'sell_price', 'Disc', 'Disc_d-7',\n           'avg_Coming_Event_1', 'avg_Coming_Event_2', 'avg_event_name_1',\n           'avg_event_name_2', 'avg_wday', 'avg_month', 'avg_week', 'avg_dept_id',\n           'avg_cat_id', 'avg_store_id', 'avg_state_id']\ncolumns_11=['Units_sold_d-7','Units_sold_mean_3','Units_sold_mean_5', 'Units_sold_mean_8', 'elasticity_id_wk',\n           'sell_price', 'Disc', 'Disc_d-7','Disc_d-5','Disc_d-3',\n           'avg_Coming_Event_1', 'avg_Coming_Event_2', 'avg_event_name_1',\n           'avg_event_name_2', 'avg_wday', 'avg_month', 'avg_week', 'avg_dept_id',\n           'avg_cat_id', 'avg_store_id', 'avg_state_id']\n#sales_train_validation = sales_train_validation.dropna(how='any',axis=0) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX=sales_train_validation[columns_11] \ny=sales_train_validation['Units_sold']\n\ndef build_model():\n  model = keras.Sequential([\n    layers.Dense(64, activation='relu', input_shape=[len(X.keys())]),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1)\n  ])\n\n  optimizer = tf.keras.optimizers.RMSprop(0.001)\n\n  model.compile(loss='mse',\n                optimizer=optimizer,\n                metrics=['mae', 'mse'])\n  return model\nmodel = build_model()\n\n\n\nEPOCHS = 4\n\n#history = \nmodel.fit(X, y,epochs=EPOCHS, validation_split = 0.2, verbose=0,\n          callbacks=[tfdocs.modeling.EpochDots()])\n\n#hist = pd.DataFrame(history.history)\n#hist['epoch'] = history.epoch\n#hist.tail()\n\ny_predict=model.predict(X)\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y, y_predict))\nprint('Mean Squared Error:', metrics.mean_squared_error(y, y_predict))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y, y_predict)))\nprint('R^2:', metrics.r2_score(y, y_predict))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#LGBM Model\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import GridSearchCV\n#gbm = lgb.LGBMRegressor(num_leaves=30,\n#                        learning_rate=0.05,\n#                        n_estimators=5)\n\nX=sales_train_validation[columns_11] \ny=sales_train_validation['Units_sold']\n\n#gbm.fit(X, y)\n\n#y_predict=gbm.predict(X)\n\nestimator = lgb.LGBMRegressor(num_leaves=50)\n\nparam_grid = {\n    'learning_rate': [0.1,0.2,0.5],\n    'n_estimators': [30,50,100]\n}\n\ngbm = GridSearchCV(estimator, param_grid, cv=3)\ngbm.fit(X, y)\ny_predict=gbm.predict(X)\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y, y_predict))\nprint('Mean Squared Error:', metrics.mean_squared_error(y, y_predict))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y, y_predict)))\nprint('R^2:', metrics.r2_score(y, y_predict))\nprint('best parameters for gbm:',gbm.best_params_)\n#gbm.feature_importances_","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"X=sales_train_validation[columns_9]\ny=sales_train_validation['Units_sold']\nreg = LinearRegression().fit(X, y)\n\ny_predict=reg.predict(X)\n#y_predict=np.maximum(y_predict, 0.)\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y, y_predict))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y, y_predict))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y, y_predict)))\nprint('R^2:', metrics.r2_score(y, y_predict))\nprint(pd.DataFrame({\"variables\":X.columns.tolist(),\"Coefficients\":reg.coef_}))\n#plt.plot(y,y_predict,'bo')","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#Fitting Random Forest Regression to the dataset\nX=sales_train_validation[columns_11] \ny=sales_train_validation['Units_sold']\n\n#create regressor object\nregressor = RandomForestRegressor(n_estimators = 5, random_state = 0, criterion='mse', max_depth=30)\n\n#fit the regressor with x and y data ****\nregressor.fit(X, y)\n\ny_predict=regressor.predict(X)\n\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y, y_predict))\nprint('Mean Squared Error:', metrics.mean_squared_error(y, y_predict))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y, y_predict)))\nprint('R^2:', metrics.r2_score(y, y_predict))\n\n#plt.plot(y,y_predict,'go')\n\n#del sales_train_validation\n#sales_train_validation.to_csv('/kaggle/working/model_data.csv')\n\n#sales_train_validation=sales_train_validation.drop(columns=['item_id','dept_id','cat_id',\n#                                                            'store_id','state_id','day_id','wm_yr_wk','wday','month'])\n#sales_train_validation=sales_train_validation.drop(columns=['event_name_1','event_type_1','event_name_2','event_type_2'])\n#sales_train_validation.info()","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction on validation 1914 to 1941","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_val_set=pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv\")\npredict_val_set=pd.concat([predict_val_set.iloc[:,:predict_val_set.columns.get_loc(\"state_id\")+1],\n                          predict_val_set.iloc[:,predict_val_set.columns.get_loc(\"d_1906\"):]], axis=1)\nfor r in np.arange(1914,1942):\n    predict_val_set_v1=predict_val_set.copy()\n    predict_val_set_v1['decile']=np.arange(1,len(predict_val_set_v1)+1)\n    predict_val_set_v1['decile']=np.ceil((predict_val_set_v1['decile']/(len(predict_val_set_v1)+1))*10)\n    \n    temp=pd.DataFrame()\n    for d in np.arange(1,11):\n        predict_validation=predict_val_set_v1[predict_val_set_v1['decile']==d].reset_index().drop(columns=['decile','index'])\n        predict_validation['d_'+str(r)]=np.nan\n\n        #melting dataframe\n        predict_validation=predict_validation.melt(id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'], \n                                                       var_name='day_id',value_name='Units_sold')\n           \n        \n        #predict_validation['Units_sold']=predict_validation['Units_sold'].astype(np.int16)\n        predict_validation['day_id']=predict_validation['day_id'].str[2:].astype(np.int64)\n        predict_validation=predict_validation[predict_validation['day_id']>(r-9)]\n        \n        \n        predict_validation=predict_validation.merge(calendar, left_on=['day_id'], right_on=['day_id'])\n        predict_validation=predict_validation.fillna(0)\n        predict_validation['week']=predict_validation['wm_yr_wk'].astype(str).str[3:].astype(np.int16)\n        \n        \n        #Create lag variables on units sold for 1 day, 1 month, 1 week, and 1 year\n        predict_validation=predict_validation[['date','id', 'item_id', 'dept_id', 'cat_id','week',\n                                                   'store_id', 'state_id', 'day_id', 'wm_yr_wk', 'wday',\n                                                   'month', 'event_name_1','event_type_1', 'event_name_2',\n                                                   'event_type_2', 'Coming_Event_1','Coming_Event_Type_1',\n                                                   'Coming_Event_2','Coming_Event_Type_2','Units_sold']].sort_values(by=['date','id']).reset_index().drop(columns=['index'])\n        for i in [7]:\n            predict_validation_v1=predict_validation[['id','date','Units_sold']]\n            predict_validation_v1=predict_validation_v1.set_index(['date','id']\n                                                                     ).unstack().shift(i).stack(dropna=False\n                                                                                                ).reset_index().rename(columns={'Units_sold':'Units_sold_d-'+str(i)})\n            \n            predict_validation['Units_sold_d-'+str(i)]=predict_validation_v1['Units_sold_d-'+str(i)].reset_index().drop(columns='index')\n            print(str(i)+' : run')\n\n\n        #Rolling mean for Units Sold\n        for i in [3,5,8]:\n            predict_validation_v1=predict_validation[['id','date','Units_sold']]\n            predict_validation_v1=predict_validation_v1.set_index(['date','id']\n                                                                     ).unstack().rolling(window=i,min_periods=1).mean().stack(dropna=False\n                                                                                                ).reset_index().rename(columns={'Units_sold':'Units_sold_mean_'+str(i)})\n            predict_validation['Units_sold_mean_'+str(i)]=predict_validation_v1['Units_sold_mean_'+str(i)].reset_index().drop(columns='index')\n            \n            print(str(i)+' : run')\n        \n        ##Discount Variable\n        sell_prices['id']=sell_prices['id'].str[:17]+'validation'\n        predict_validation=predict_validation.merge(sell_prices,how='left' ,left_on=['id','wm_yr_wk'],\n                                                            right_on=['id','wm_yr_wk'])\n        predict_validation['Disc'][predict_validation['Disc'].isnull()]=np.mean(sell_prices['Disc'][sell_prices['Disc']>0])\n\n        #predict_validation=predict_validation[predict_validation['sell_price'].notnull()].reset_index()\n        #predict_validation=predict_validation.drop(columns=['index'])\n        predict_validation=predict_validation.drop(columns=['sell_price_max'])\n       \n        #Discount Variable lag for 1 week\n        for i in [3,5,7]:\n            predict_validation_v1=predict_validation[['id','date','Disc']]\n            predict_validation_v1=predict_validation_v1.set_index(['date','id']\n                                                                     ).unstack().shift(i).stack(dropna=False\n                                                                                                ).reset_index().rename(columns={'Disc':'Disc_d-'+str(i)})\n            predict_validation['Disc_d-'+str(i)]=predict_validation_v1['Disc_d-'+str(i)].reset_index().drop(columns='index')\n            print(str(i)+' : run')\n\n        #filter only for days we need static variables for --\n        condition_v1=(predict_validation['day_id']>=r)\n                      #&(predict_validation['day_id']<=r+6))\n        predict_validation=predict_validation[condition_v1].reset_index().drop(columns=['index'])\n            \n        ##average sale of product in festival_1\n        predict_validation['Coming_Event_1']=predict_validation['Coming_Event_1'].astype(str)\n        predict_validation=predict_validation.merge(avg_Coming_Event_1, how='left', on=['Coming_Event_1'])\n        predict_validation['Coming_Event_2']=predict_validation['Coming_Event_2'].astype(str)\n        predict_validation=predict_validation.merge(avg_Coming_Event_2, how='left', on=['Coming_Event_2'])\n        predict_validation['event_name_1']=predict_validation['event_name_1'].astype(str)\n        predict_validation=predict_validation.merge(avg_event_name_1, how='left', on=['event_name_1'])\n        predict_validation['event_name_2']=predict_validation['event_name_2'].astype(str)\n        predict_validation=predict_validation.merge(avg_event_name_2, how='left', on=['event_name_2'])\n        predict_validation=predict_validation.merge(avg_wday, how='left', on=['wday'])\n        predict_validation=predict_validation.merge(avg_month, how='left', on=['month'])\n        predict_validation=predict_validation.merge(avg_week, how='left', on=['week'])\n        predict_validation=predict_validation.merge(avg_dept_id, how='left', on=['dept_id'])\n        predict_validation=predict_validation.merge(avg_cat_id, how='left', on=['cat_id'])\n        predict_validation=predict_validation.merge(avg_store_id, how='left', on=['store_id'])\n        predict_validation=predict_validation.merge(avg_state_id, how='left', on=['state_id'])\n        \n        predict_validation=predict_validation.merge(elasticity, how='left', on='id')\n        predict_validation=predict_validation.merge(elasticity_cat_lvl, how='left', on='cat_id')\n        predict_validation['elasticity_id_wk'][predict_validation['elasticity_id_wk'].isnull()]=predict_validation['elasticity_id_wk_cat_lvl']\n        predict_validation=predict_validation.drop(columns=['elasticity_id_wk_cat_lvl'])\n\n        \n        print(\"merge done !\")\n        predict_validation=predict_validation.drop(columns= ['item_id', 'dept_id', 'cat_id', 'week', 'store_id',\n       'state_id', 'wm_yr_wk', 'wday', 'month', 'event_name_1',\n       'event_type_1', 'event_name_2', 'event_type_2', 'Coming_Event_1',\n       'Coming_Event_Type_1', 'Coming_Event_2', 'Coming_Event_Type_2'])\n\n#        predict_validation=predict_validation.fillna(predict_validation.mean())\n        predict_validation=predict_validation.fillna(0)\n                \n        predict_validation['Units_sold']=np.ceil(model.predict(predict_validation[columns_11]))\n        predict_validation=predict_validation[['id','day_id','Units_sold']]\n        predict_validation['day_id']='d_'+predict_validation['day_id'].astype(str)\n        temp=temp.append(predict_validation)\n\n    temp=temp.pivot(index='id', columns='day_id', values='Units_sold').reset_index()\n    predict_val_set=predict_val_set.merge(temp, how='left', on='id')\n#   predict_val_set=predict_val_set.rename(columns={str(r):'d_'+str(r)})\n#   predict_val_set['d_'+str(r)]=np.ceil(predict_val_set['d_'+str(r)])\n    print(str(r)+' : run')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"predict_val_set=predict_val_set[['id',d_1914,d_1915,d_1916,d_1917,d_1918,d_1919,d_1920,d_1921,d_1922,d_1923,d_1924,d_1925,d_1926,d_1927,d_1928,d_1929,d_1930,d_1931,d_1932,d_1933,d_1934,d_1935,d_1936,d_1937,d_1938,d_1939,d_1940,d_1941]]\npredict_val_set[[1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941]]=\nnp.ceil(predict_val_set[[1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941]])\npredict_val_set.to_csv('/kaggle/working/validatio_1.csv')","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_val_set_v1=predict_val_set[['id','d_1914','d_1915','d_1916','d_1917','d_1918','d_1919','d_1920','d_1921','d_1922','d_1923','d_1924','d_1925','d_1926','d_1927','d_1928','d_1929','d_1930','d_1931','d_1932','d_1933','d_1934','d_1935','d_1936','d_1937','d_1938','d_1939','d_1940','d_1941']]\npredict_val_set_v1.to_csv(\"/kaggle/working/validation_8.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_eva_set=pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv\")\npredict_eva_set=pd.concat([predict_eva_set.iloc[:,:predict_eva_set.columns.get_loc(\"state_id\")+1],\n                          predict_eva_set.iloc[:,predict_eva_set.columns.get_loc(\"d_1934\"):]], axis=1)\nfor r in np.arange(1942,1970):\n    predict_eva_set_v1=predict_eva_set.copy()\n    predict_eva_set_v1['decile']=np.arange(1,len(predict_eva_set_v1)+1)\n    predict_eva_set_v1['decile']=np.ceil((predict_eva_set_v1['decile']/(len(predict_eva_set_v1)+1))*10)\n    \n    temp=pd.DataFrame()\n    for d in np.arange(1,11):\n        predict_evaluation=predict_eva_set_v1[predict_eva_set_v1['decile']==d].reset_index().drop(columns=['decile','index'])\n        predict_evaluation['d_'+str(r)]=np.nan\n\n        #melting dataframe\n        predict_evaluation=predict_evaluation.melt(id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'], \n                                                       var_name='day_id',value_name='Units_sold')\n           \n        \n        #predict_evaluation['Units_sold']=predict_evaluation['Units_sold'].astype(np.int16)\n        predict_evaluation['day_id']=predict_evaluation['day_id'].str[2:].astype(np.int64)\n        predict_evaluation=predict_evaluation[predict_evaluation['day_id']>(r-9)]\n        \n        \n        predict_evaluation=predict_evaluation.merge(calendar, left_on=['day_id'], right_on=['day_id'])\n        predict_evaluation=predict_evaluation.fillna(0)\n        predict_evaluation['week']=predict_evaluation['wm_yr_wk'].astype(str).str[3:].astype(np.int16)\n        \n        \n        #Create lag variables on units sold for 1 day, 1 month, 1 week, and 1 year\n        predict_evaluation=predict_evaluation[['date','id', 'item_id', 'dept_id', 'cat_id','week',\n                                                   'store_id', 'state_id', 'day_id', 'wm_yr_wk', 'wday',\n                                                   'month', 'event_name_1','event_type_1', 'event_name_2',\n                                                   'event_type_2', 'Coming_Event_1','Coming_Event_Type_1',\n                                                   'Coming_Event_2','Coming_Event_Type_2','Units_sold']].sort_values(by=['date','id']).reset_index().drop(columns=['index'])\n        for i in [7]:\n            predict_evaluation_v1=predict_evaluation[['id','date','Units_sold']]\n            predict_evaluation_v1=predict_evaluation_v1.set_index(['date','id']\n                                                                     ).unstack().shift(i).stack(dropna=False\n                                                                                                ).reset_index().rename(columns={'Units_sold':'Units_sold_d-'+str(i)})\n            \n            predict_evaluation['Units_sold_d-'+str(i)]=predict_evaluation_v1['Units_sold_d-'+str(i)].reset_index().drop(columns='index')\n            print(str(i)+' : run')\n\n\n        #Rolling mean for Units Sold\n        for i in [3,5,8]:\n            predict_evaluation_v1=predict_evaluation[['id','date','Units_sold']]\n            predict_evaluation_v1=predict_evaluation_v1.set_index(['date','id']\n                                                                     ).unstack().rolling(window=i,min_periods=1).mean().stack(dropna=False\n                                                                                                ).reset_index().rename(columns={'Units_sold':'Units_sold_mean_'+str(i)})\n            predict_evaluation['Units_sold_mean_'+str(i)]=predict_evaluation_v1['Units_sold_mean_'+str(i)].reset_index().drop(columns='index')\n            \n            print(str(i)+' : run')\n        \n        ##Discount Variable\n        sell_prices['id']=sell_prices['id'].str[:17]+'evaluation'\n        predict_evaluation=predict_evaluation.merge(sell_prices,how='left' ,left_on=['id','wm_yr_wk'],\n                                                            right_on=['id','wm_yr_wk'])\n        predict_evaluation['Disc'][predict_evaluation['Disc'].isnull()]=np.mean(sell_prices['Disc'][sell_prices['Disc']>0])\n\n        #predict_evaluation=predict_evaluation[predict_evaluation['sell_price'].notnull()].reset_index()\n        #predict_evaluation=predict_evaluation.drop(columns=['index'])\n        predict_evaluation=predict_evaluation.drop(columns=['sell_price_max'])\n       \n        #Discount Variable lag for 1 week\n        for i in [3,5,7]:\n            predict_evaluation_v1=predict_evaluation[['id','date','Disc']]\n            predict_evaluation_v1=predict_evaluation_v1.set_index(['date','id']\n                                                                     ).unstack().shift(i).stack(dropna=False\n                                                                                                ).reset_index().rename(columns={'Disc':'Disc_d-'+str(i)})\n            predict_evaluation['Disc_d-'+str(i)]=predict_evaluation_v1['Disc_d-'+str(i)].reset_index().drop(columns='index')\n            print(str(i)+' : run')\n\n        #filter only for days we need static variables for --\n        condition_v1=(predict_evaluation['day_id']>=r)\n                      #&(predict_evaluation['day_id']<=r+6))\n        predict_evaluation=predict_evaluation[condition_v1].reset_index().drop(columns=['index'])\n            \n        ##average sale of product in festival_1\n        predict_evaluation['Coming_Event_1']=predict_evaluation['Coming_Event_1'].astype(str)\n        predict_evaluation=predict_evaluation.merge(avg_Coming_Event_1, how='left', on=['Coming_Event_1'])\n        predict_evaluation['Coming_Event_2']=predict_evaluation['Coming_Event_2'].astype(str)\n        predict_evaluation=predict_evaluation.merge(avg_Coming_Event_2, how='left', on=['Coming_Event_2'])\n        predict_evaluation['event_name_1']=predict_evaluation['event_name_1'].astype(str)\n        predict_evaluation=predict_evaluation.merge(avg_event_name_1, how='left', on=['event_name_1'])\n        predict_evaluation['event_name_2']=predict_evaluation['event_name_2'].astype(str)\n        predict_evaluation=predict_evaluation.merge(avg_event_name_2, how='left', on=['event_name_2'])\n        predict_evaluation=predict_evaluation.merge(avg_wday, how='left', on=['wday'])\n        predict_evaluation=predict_evaluation.merge(avg_month, how='left', on=['month'])\n        predict_evaluation=predict_evaluation.merge(avg_week, how='left', on=['week'])\n        predict_evaluation=predict_evaluation.merge(avg_dept_id, how='left', on=['dept_id'])\n        predict_evaluation=predict_evaluation.merge(avg_cat_id, how='left', on=['cat_id'])\n        predict_evaluation=predict_evaluation.merge(avg_store_id, how='left', on=['store_id'])\n        predict_evaluation=predict_evaluation.merge(avg_state_id, how='left', on=['state_id'])\n        \n        predict_evaluation=predict_evaluation.merge(elasticity, how='left', on='id')\n        predict_evaluation=predict_evaluation.merge(elasticity_cat_lvl, how='left', on='cat_id')\n        predict_evaluation['elasticity_id_wk'][predict_evaluation['elasticity_id_wk'].isnull()]=predict_evaluation['elasticity_id_wk_cat_lvl']\n        predict_evaluation=predict_evaluation.drop(columns=['elasticity_id_wk_cat_lvl'])\n\n        \n        print(\"merge done !\")\n        predict_evaluation=predict_evaluation.drop(columns= ['item_id', 'dept_id', 'cat_id', 'week', 'store_id',\n       'state_id', 'wm_yr_wk', 'wday', 'month', 'event_name_1',\n       'event_type_1', 'event_name_2', 'event_type_2', 'Coming_Event_1',\n       'Coming_Event_Type_1', 'Coming_Event_2', 'Coming_Event_Type_2'])\n\n#        predict_evaluation=predict_evaluation.fillna(predict_evaluation.mean())\n        predict_evaluation=predict_evaluation.fillna(0)\n                \n        predict_evaluation['Units_sold']=np.ceil(model.predict(predict_evaluation[columns_11]))\n        predict_evaluation=predict_evaluation[['id','day_id','Units_sold']]\n        predict_evaluation['day_id']='d_'+predict_evaluation['day_id'].astype(str)\n        temp=temp.append(predict_evaluation)\n\n    temp=temp.pivot(index='id', columns='day_id', values='Units_sold').reset_index()\n    predict_eva_set=predict_eva_set.merge(temp, how='left', on='id')\n#   predict_eva_set=predict_eva_set.rename(columns={str(r):'d_'+str(r)})\n#   predict_eva_set['d_'+str(r)]=np.ceil(predict_eva_set['d_'+str(r)])\n    print(str(r)+' : run')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_eva_set_v1=predict_eva_set[['id','d_1942','d_1943','d_1944','d_1945','d_1946','d_1947','d_1948','d_1949','d_1950','d_1951','d_1952','d_1953','d_1954','d_1955','d_1956','d_1957','d_1958','d_1959','d_1960','d_1961','d_1962','d_1963','d_1964','d_1965','d_1966','d_1967','d_1968','d_1969']]\npredict_eva_set_v1.to_csv(\"/kaggle/working/evaluation_8.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}