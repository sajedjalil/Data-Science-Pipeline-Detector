{"cells":[{"metadata":{},"cell_type":"markdown","source":"#  Introduction\n\n## Problem Statement\nThe Makridakis Open Forecasting Center (MOFC) at the University of Nicosia conducts cutting-edge forecasting research and provides business forecast training. It helps companies achieve accurate predictions, estimate the levels of uncertainty, avoiding costly mistakes, and apply best forecasting practices. The MOFC is well known for its Makridakis Competitions, the first of which ran in the 1980s.\n\nIn this competition, the fifth iteration, hierarchical sales data from Walmart, the worldâ€™s largest company by revenue is given,** to forecast daily sales for the next 28 days.** The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy.\n\n![Walmart](https://entrackr.com/wp-content/uploads/2020/01/Walmart-cash-and-carry.jpg)\n\n## Data Provided\nIn the challenge, you are predicting item sales at stores in various locations for two 28-day time periods. Information about the data is found in the M5 Participants Guide.\nFiles\n*      **calendar.csv** - Contains information about the dates on which the products are sold.\n*     **sales_train_validation.csv** - Contains the historical daily unit sales data per product and store (d_1 - d_1913)\n*     **sample_submission.csv** - The correct format for submissions. Reference the Evaluation tab for more info.\n*     **sell_prices.csv** - Contains information about the price of the products sold per store and date.\n*     **sales_train_evaluation.csv** - Available once month before competition deadline. Will include sales (d_1 - d_1941)\n\nWe will have a sneak peak into the dataset below \n\n## Evaluation Metric\nThis competition uses a Weighted Root Mean Squared Scaled Error (RMSSE). The RMSSE metric is a variant of the original MASE (mean absolute scaled error) metric. The scaling was introduced to provide a scale-free error regardless of the data. An example for this competition would be a product that sells hundreds of units per day vs. a product that only sells a few times a week or month (the intermittent demand we are seeing in there series).\nKeeping that in mind our measurement is now scale free which allows us to compare many different types of time series. Another reason we are using RMSSE is because it does not suffer from actual values being zero like the MAPE metric."},{"metadata":{},"cell_type":"markdown","source":"# Content:\n1. Loading necessary libraries\n2. Reading the dataset\n3. Summary Statistics\n4. Time Series Views\n5. Impact of Events and SNAP days on sales\n6. Analysis on prices changes"},{"metadata":{},"cell_type":"markdown","source":"# 1. Loading necessary libraries"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"!pip install cufflink","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os #using operating system dependent functionality\nimport datetime #datetime module supplies classes for manipulating dates and times.\nimport math # provides access to the mathematical functions\nfrom IPython.display import display, HTML\n\n#For Plotting\n# Using plotly + cufflinks in offline mode\nimport plotly as py\nimport plotly.graph_objs as go\nimport plotly\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport cufflinks as cf\ncf.set_config_file(offline=True)\ninit_notebook_mode(connected=True)\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#For time series decomposition\nfrom matplotlib import pyplot\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n#Pandas option\npd.options.display.float_format = '{:.2f}'.format","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# 2. Reading the dataset"},{"metadata":{},"cell_type":"markdown","source":"Listing the available files"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# Listing the available files \nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sales_data = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv\")\nprice_data = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sell_prices.csv\")\ncalender_data = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/calendar.csv\")\nsubmission_data = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Summary Statistics"},{"metadata":{},"cell_type":"markdown","source":"### 1.1 Sales Data"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"The sales data has '{}' rows and '{}' columns\".format(sales_data.shape[0],sales_data.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Let's have a look at the data\nsales_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The unique identifier seems to be a concatenation of item_id and store_id. There are 3049 unique items and 10 unique stores (Total number of rows = 30490)\n\nThe columns d_1 to d_1913 gives the sales of the given item in that store on the nth day for 1913 days"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Let's make a list of date columns date_col = [d1,d2,d3,d4...]\ndate_col = [col for col in sales_data if col.startswith('d_')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the unique states in the sales dataset"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Let's look at the unique states in the sales dataset\nsales_data.state_id.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have information about 3 states of U.S. (California, Texas and Wisconsin)"},{"metadata":{},"cell_type":"markdown","source":"Lets look at the number of rows for each state"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Lets look at the number of rows for each state. Value_counts give you that\nsales_data.state_id.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Let's have a look at the ratio of the number of rows. Normalize = True gives you the ratio\nsales_data.state_id.value_counts(normalize =True) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"40% of the rows are about California, 30% rows are about Wisconsin and Texas"},{"metadata":{},"cell_type":"markdown","source":"Total sales from each of the state"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Calcuating total sales for each row/ id by adding the sales of each of the 1913 days\nsales_data['total_sales'] = sales_data[date_col].sum(axis=1)\n#Adding all the sales for each state\nsales_data.groupby('state_id').agg({\"total_sales\":\"sum\"}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting Sales Ratio across the 3 states"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Calculating the sales ratio\nstate_wise_sales_data = sales_data.groupby('state_id').agg({\"total_sales\":\"sum\"})/sales_data.total_sales.sum() * 100\nstate_wise_sales_data = state_wise_sales_data.reset_index()\n#Plotting the sales ratio\nfig1, ax1 = plt.subplots()\nax1.pie(state_wise_sales_data['total_sales'],labels= state_wise_sales_data['state_id'] , autopct='%1.1f%%',\n        shadow=True, startangle=90)# Equal aspect ratio ensures that pie is drawn as a circle\nax1.axis('equal')  \nplt.tight_layout()\nplt.title(\"State Wise total sales percentage\",fontweight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that 43.6% of sales come from California while Texas and Winscoin have comparable sales 27.6% and 28.8%"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Let's have a look at the unique stores\nprint(\"Let's have a look at the unique stores - \",sales_data.store_id.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have information from 10 stores : 4 stores of California, 3 stores of Texas and 3 stores of Winscoin"},{"metadata":{},"cell_type":"markdown","source":"### Plotting Sales Ratio across the 10 stores"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Caculating the sales ratio for the 10 stores\nstore_wise_sales_data=sales_data.groupby('store_id').agg({\"total_sales\":\"sum\"})/sales_data.total_sales.sum() * 100\n#Plotting the sales ratio for the 10 stores\nstore_wise_sales_data = store_wise_sales_data.reset_index()\nfig1, ax1 = plt.subplots()\nax1.pie(store_wise_sales_data['total_sales'],labels= store_wise_sales_data['store_id'] , autopct='%1.1f%%',\n        shadow=True, startangle=90)# Equal aspect ratio ensures that pie is drawn as a circle\nax1.axis('equal')  \nplt.tight_layout()\nplt.title(\"Store Wise total sales percentage\",fontweight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have the store CA_3 which has the highest sales ratio ~17% which has almost double the sales of any other store. While CA_4 has the lowest sales ratio ~6.2%"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Let's have a look at the unique categories \nprint(\"Let's have a look at the unique categories -\",sales_data.cat_id.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Let's have a look at the total sales from each of the 3 categries\nprint(\"Total Sales from each category\")\nsales_data.groupby('cat_id').agg({\"total_sales\":\"sum\"}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting Sales Ratio across the 3 categories"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Caculating the sales ratio for the 3 categories\ncat_wise_sales_data = sales_data.groupby('cat_id').agg({\"total_sales\":\"sum\"})/sales_data.total_sales.sum() * 100\ncat_wise_sales_data = cat_wise_sales_data.reset_index()\n#Plotting the sales ratio for the 3 categories\nfig1, ax1 = plt.subplots()\nax1.pie(cat_wise_sales_data['total_sales'],labels= cat_wise_sales_data['cat_id'] , autopct='%1.1f%%',\n        shadow=True, startangle=90)# Equal aspect ratio ensures that pie is drawn as a circle\nax1.axis('equal')  \nplt.tight_layout()\nplt.title(\"Category Wise total sales percentage\",fontweight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have almost 70% sales coming only from FOODS category.20% from HOUSEHOLD categories and a minor 10 % sales from HOBBIES "},{"metadata":{},"cell_type":"markdown","source":"### Plotting Sales of each category across the 3 states"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cat_state_sales =sales_data.groupby(['cat_id','state_id']).agg({\"total_sales\":\"sum\"}).groupby(level=0).apply(lambda x: 100 * x / float(x.sum())).unstack()\ncat_state_sales.columns = [f'{i}_{j}' if j != '' else f'{i}' for i,j in cat_state_sales.columns]\ncat_state_sales.plot(kind='bar', stacked=True)\nplt.title(\"Sales Distrubution for each category across states\",fontweight = \"bold\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see:\n* California contributes to almost 40% sales of foods and household categories but contributes to about 50% sales of hobbies category\n*  Winscoin has about 25% contribution in both hobbies and household categories but contributes to about 30% sales of food category\n* Texas contributes to almost 30% sales of foods and household categories but contributes to about 25% sales of hobbies category"},{"metadata":{},"cell_type":"markdown","source":"### Plotting sales ditribution for each state across categories"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Calculating sales distribution for each state \nstate_cat_sales = sales_data.groupby(['state_id','cat_id']).agg({\"total_sales\":\"sum\"}).groupby(level=0).apply(lambda x: 100 * x / float(x.sum())).unstack()\n#Plotting the sales distribution for each state\nstate_cat_sales.columns = [f'{i}_{j}' if j != '' else f'{i}' for i,j in state_cat_sales.columns]\nstate_cat_sales.plot(kind='bar', stacked=True)\nplt.title(\"Sales Distrubution for each state across categories\",fontweight = \"bold\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see:\n* Winscoin spends the highest in food 72%, 8% in hobbies and 20% in household\n* California spend 67% on food, 11% on hobbies and 22% on household\n* Texas spends 69% on food, 8% in hobbies and 23% in household\nSo we can see that all the 3 states have a bit of differences in spent"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Let's look at the unique departments\nprint(\"Let's look at the unique departments - \",sales_data.dept_id.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 7 departments in total (2 hobbies ,2 household and 3 food departments)"},{"metadata":{},"cell_type":"markdown","source":"### Plotting sales ditribution across departments"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Calculating sales distribution across departments\ndept_sales = sales_data.groupby('dept_id').agg({\"total_sales\":\"sum\"})/sales_data.total_sales.sum() * 100\n#Plotting\ndept_sales = dept_sales.reset_index()\nfig1, ax1 = plt.subplots()\nax1.pie(dept_sales['total_sales'],labels= dept_sales['dept_id'] , autopct='%1.1f%%',\n        shadow=True, startangle=90)# Equal aspect ratio ensures that pie is drawn as a circle\nax1.axis('equal')  \nplt.tight_layout()\nplt.title(\"Department Wise total sales percentage\",fontweight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see:\n* We have almost 50% sales coming from FOODS-3 department which is the highest (out of the total ~70% sales coming from foods)\n* In household, houshold_1 contributes to 17.5% to the total sles (out of the 22 % sales coming from household)\n* Of the 9.3% sales coming from hobbies 8.5% comes from hobbies_1"},{"metadata":{},"cell_type":"markdown","source":"### Plotting sales distribution of stores across departments"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Calculating the sales distribution of stores\nstore_dept_sales = sales_data.groupby(['store_id','dept_id']).agg({\"total_sales\":\"sum\"}).groupby(level=0).apply(lambda x: 100 * x / float(x.sum())).unstack()\nstore_dept_sales.columns = [f'{i}_{j}' if j != '' else f'{i}' for i,j in store_dept_sales.columns]\n#Plotting the sales distribution\nstore_dept_sales.plot(kind='bar', stacked=True)\nplt.title(\"Sales Distrubution for each store across departments\",fontweight = \"bold\")\nplt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), shadow=True, ncol=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"Sales distribution(in %) in each store accross different departments\")\nsales_data.groupby(['store_id','dept_id']).agg({\"total_sales\":\"sum\"}).groupby(level=0).apply(lambda x: 100 * x / float(x.sum())).unstack()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see:\n* Store WI_3 has the most skewed distribution of sales where 54% of sales come from food-3, ~7% sales in hobbies\n* Store CA_2 has lowest sales in FOODS_3 when compared to other stores(42%) and the highest in FOODS_1 and HOUSEHOLD_2 when compared to other stores(12% and 8%) where the averages are quite low"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dept_store_sales = sales_data.groupby(['dept_id','store_id']).agg({\"total_sales\":\"sum\"}).groupby(level=0).apply(lambda x: 100 * x / float(x.sum())).unstack()\ndept_store_sales.columns = [f'{i}_{j}' if j != '' else f'{i}' for i,j in dept_store_sales.columns]\ndept_store_sales.plot(kind='bar', stacked=True)\nplt.title(\"Sales Distrubution for each state across categories\",fontweight = \"bold\")\nplt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.5), shadow=True, ncol=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2 Price Data "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Let's have a look at the price data\nprice_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's first look at the distribution of prices across Categories"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"price_data[\"Category\"] = price_data[\"item_id\"].str.split(\"_\",expand = True)[0]\nplt.figure(figsize=(12,6))\np1=sns.kdeplot(price_data[price_data['Category']=='HOBBIES']['sell_price'], shade=True, color=\"b\")\np2=sns.kdeplot(price_data[price_data['Category']=='FOODS']['sell_price'], shade=True, color=\"r\")\np3=sns.kdeplot(price_data[price_data['Category']=='HOUSEHOLD']['sell_price'], shade=True, color=\"g\")\nplt.legend(labels=['HOBBIES','FOODS',\"HOUSEHOLD\"])\nplt.xscale(\"log\")\nplt.xlabel(\"Log of Prices\")\nplt.ylabel(\"Density\")\nplt.title(\"Density plot of log of prices accross Categories\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see:\n* Most of the prices for food products lie between 1 dollars and 10 dollars. As we can see the high peak between 10^0 and 10^1\n* Hobbies show a pretty wide range of prices\n* Households are costlier than Food"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Let's look at items with the maximum price change and minimum price change over the years\nitem_store_prices = price_data.groupby([\"item_id\",\"store_id\"]).agg({\"sell_price\":[\"max\",\"min\"]})\nitem_store_prices.columns = [f'{i}_{j}' if j != '' else f'{i}' for i,j in item_store_prices.columns]                                               \nitem_store_prices[\"price_change\"] = item_store_prices[\"sell_price_max\"] - item_store_prices[\"sell_price_min\"]\nitem_store_prices_sorted = item_store_prices.sort_values([\"price_change\",\"item_id\"],ascending=False).reset_index()\nitem_store_prices_sorted[\"category\"] = item_store_prices_sorted[\"item_id\"].str.split(\"_\",expand = True)[0]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"Items sorted by maximum price change over the years (top 10)\")\nitem_store_prices_sorted.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we get:\n* We see that household items specially HOUSEHOLD_2 department has shown the maximum price changes specially in Wisconsin.\n* The price changed the most for HOUSEHOLD_2_406 item in WI_3 store where the min price was just  \\$3.26 and had rised 32 times to  \\$107"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"Items sorted by least price changes over the years (top 10)\")\nitem_store_prices_sorted.tail(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we get:\n* FOODS_1_014 item hasnt changed prices over the years. Also, the price is fixed in all stores"},{"metadata":{},"cell_type":"markdown","source":"### Plotting boxplot for price changes"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Plotting boxplot\nsns.boxplot(x=\"price_change\", y=\"category\", data=item_store_prices_sorted)\ntitle = plt.title(\"Boxplot for maximum price change for each item over the years across all categories\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we get:\n* Most products dont change prices at all and most changes are restricted to 10-15\\\\$\n* Household items have the highest price changes over the years "},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Calender"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Let's look at the calender data\ncalender_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"The calender dataset has {} rows and {} columns\".format(calender_data.shape[0],calender_data.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see:\n* The calender data is given for all the 1913 days in the sales data (actually we have 1969 days)\n* We have at max 2 events in a day for which the event names and the event types are given\n* We also SNAP days flags for each state separately i.e. all states have different SNAP days.\n\nKnowing a bit about SNAP won't harm :- [SNAP](https://www.feedingamerica.org/take-action/advocate/federal-hunger-relief-programs/snap)\n\n**What is SNAP?**\n\nSNAP stands for the Supplemental Nutrition Assistance Program. SNAP is a federal program that helps millions of low-income Americans put food on the table. Across the United States there are 9.5 million families with children on SNAP. It is the largest program working to fight hunger in America.\n\n**What kinds of groceries can be purchased with SNAP?**\n\nHouseholds can use SNAP to buy nutritious foods such as breads and cereals, fruits and vegetables, meat and fish and dairy products. SNAP benefits cannot be used to buy any kind of alcohol or tobacco products or any nonfood items like household supplies and vitamins and medicines.\nN.B. So we can expect SNAP can help sales in food items "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Event names for each event type\nevents1 = calender_data[['event_type_1','event_name_1',]]\nevents2 = calender_data[['event_type_2','event_name_2',]]\nevents2.columns = [\"event_type_1\",\"event_name_1\"]\nevents = pd.concat([events1,events2],ignore_index = True)\nevents = events.dropna().drop_duplicates()\nevents\nevents_dict = {k: g[\"event_name_1\"].tolist() for k,g in events.groupby(\"event_type_1\")}\nprint(\"Event Names across different Event Types\")\npd.DataFrame(dict([ (k,pd.Series(v)) for k,v in events_dict.items()]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 10 National and Religious events, 6 Cultural Events and 3 Sporting events (30 events in total) in a year"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"snap_days = calender_data.groupby(['year','month'])['snap_CA','snap_TX','snap_WI'].sum().reset_index()\nprint(\"SNAP days for each month across the years for all the states\")\nsnap_days.pivot(index=\"month\",columns = \"year\",values = [\"snap_CA\",\"snap_TX\",\"snap_WI\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So every month we have 10 SNAP days for all the 3 states and it has been consistent througout the years which might fall on different days in different states"},{"metadata":{},"cell_type":"markdown","source":"Let's look at the unique days at which the 10 SNAP days of a month exists over the years"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Setting the start date\nbase = datetime.datetime(2011,1,29)\n#Calculating the total sales in a day\nsales_sum = pd.DataFrame(sales_data[date_col].sum(axis =0),columns = [\"sales\"])\n#Adding the date column\nsales_sum['datum'] = [base + datetime.timedelta(days=x) for x in range(1913)]\nsales_sum.set_index('datum', drop=True, inplace=True)\nsales_sum.sort_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Joining the calender data with the sales data to see the impact of events\ncalender_data['date'] = pd.to_datetime(calender_data['date'])\noverall_sales_special = pd.merge(calender_data,sales_sum, left_on = \"date\", right_on = \"datum\",how = \"right\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For CA:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"overall_sales_special.loc[overall_sales_special.snap_CA==1,\"date\"].dt.day.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that we 10 unique values. So, all the years SNAP days fall on the sam 10 days i.e. 1st of every month to 10th of every month in CA"},{"metadata":{},"cell_type":"markdown","source":"For TX:"},{"metadata":{"trusted":true},"cell_type":"code","source":"overall_sales_special.loc[overall_sales_special.snap_TX==1,\"date\"].dt.day.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also see 10 unique values dates that means in TX also SNAP falls on the same dates over the months. These dates are different from CA"},{"metadata":{},"cell_type":"markdown","source":"For WI:"},{"metadata":{"trusted":true},"cell_type":"code","source":"overall_sales_special.loc[overall_sales_special.snap_WI==1,\"date\"].dt.day.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also see 10 unique values dates that means in WI also SNAP falls on the same dates over the months. These dates are different from CA and TX"},{"metadata":{},"cell_type":"markdown","source":"# 4. Time Series Views"},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Across Years"},{"metadata":{},"cell_type":"markdown","source":"### Plotting daily sales time series"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Plotting daily states\nsales_sum.iplot(title = \"Daily Overall Sales\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see:\n* We see an increasing overall trend\n* We also see a monthly seasonality peaking at August.\n\nNow these things will be even clearer when we look at the time series decomposition.\n\nBefore that a little context:\n\nA given time series is thought to consist of three systematic components including level, trend, seasonality, and one non-systematic component called noise.\n\nThese components are defined as follows:\n\n    Level: The average value in the series.\n    Trend: The increasing or decreasing value in the series.\n    Seasonality: The repeating short-term cycle in the series.\n    Noise: The random variation in the series."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"result = seasonal_decompose(sales_sum, model='additive')\nresult.plot()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see:\n* The first graph shows the actual time series\n* The second graph shows the trend. The overall trend seems to be increasing over the years\n* The third graph shows the seasonality. We see a strong weekly seasonality\n* We also see lots of residuals in the last graph"},{"metadata":{},"cell_type":"markdown","source":"### Plotting monthly sales time series across the 3 states"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"state_level = sales_data.groupby(\"state_id\")[date_col].sum().reset_index().set_index('state_id').T\nstate_level['datum'] = [base + datetime.timedelta(days=x) for x in range(1913)]\nstate_level.set_index('datum', drop=True, inplace=True)\nstate_level.sort_index(inplace=True)\nstate_level.head()\nstate_month_level = state_level.groupby(pd.Grouper(freq='1M')).sum()\nstate_month_level.iplot(title = \"Monthly Sales accross States\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see:\n* CA sales has always been the highest. The peaks in August are the most evident at CA in comparison to other states. Seasonality impacts CA sales the most\n* WI have shown the highest increase in sales over the years.The sales were lower than TX before 2013. TX and WI sales were similar in 2013 to 2015 August. It has shown higher sales after 2015 August than TX. It will be interesting to look at what has caused WI to increase significantly over the years."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Plotting the sales time series decomposition for each state\nres1 = seasonal_decompose(state_month_level[\"CA\"], model='additive')\nres2 = seasonal_decompose(state_month_level[\"TX\"], model='additive')\nres3 = seasonal_decompose(state_month_level[\"WI\"], model='additive')\ndef plotseasonal(res, axes ):\n    res.observed.plot(ax=axes[0], legend=False)\n    axes[0].set_ylabel('Observed')\n    res.trend.plot(ax=axes[1], legend=False)\n    axes[1].set_ylabel('Trend')\n    res.seasonal.plot(ax=axes[2], legend=False)\n    axes[2].set_ylabel('Seasonal')\n    res.resid.plot(ax=axes[3], legend=False)\n    axes[3].set_ylabel('Residual')\n\nfig, axes = plt.subplots(ncols=3, nrows=4, sharex=True, figsize=(12,5))\n\nplotseasonal(res1, axes[:,0])\naxes[0,0].set_title(\"CA\")\nplotseasonal(res2, axes[:,1])\naxes[0,1].set_title(\"TX\")\nplotseasonal(res3, axes[:,2])\naxes[0,2].set_title(\"WI\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see:\n* CA and WI show a gradual increasing trend while TX grew up quite fast and then became a bit stagnant\n* CA and TX show a similar seasonality peaking at July, August and dipping at Dec ,Jan\n* WI shows a different seasonality peaking at March and dipping at April and then peaking again at August"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"store_level = sales_data.groupby(\"store_id\")[date_col].sum().reset_index().set_index('store_id').T\nstore_level['datum'] = [base + datetime.timedelta(days=x) for x in range(1913)]\nstore_level.set_index('datum', drop=True, inplace=True)\nstore_level.sort_index(inplace=True)\nstore_level.head()\nstore_month_level = store_level.groupby(pd.Grouper(freq='1M')).sum()\nstore_month_level.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting monthly sales time series across different stores"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cf.Figure(cf.subplots([store_month_level[['CA_1','CA_2','CA_3','CA_4']].figure(),store_month_level[['TX_1','TX_2','TX_3']].figure(),store_month_level[['WI_1','WI_2','WI_3']].figure()],shape=(1,3),subplot_titles=('CA', 'TX', 'WI'))).iplot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see:\n* In CA, CA_1 and CA_3 shows the highest seasonality in sales and have an increasing trend overall.CA_2 shows a stark decrease in 2015 but has then peaked up in 2016. CA_4 shows a similar increasing trend. Overall, CA stores show the same trend\n* In TX, TX_2 was really performing well till 2014 but has dipped after that drastically which is bad.TX_1 shows a moderate increasing trend. The good news in TX is TX_3 is consistently showing an increasing trend\n* In WI, WI_1 and WI_2 shows drastic increase in 2012 and 2013 from 50k sales to 100k sales. WI_3 which was the best store in 2012 dipped a lot during  2014 and 2015 but has increased a bit in 2016 "},{"metadata":{},"cell_type":"markdown","source":"### Plotting sales time series accross categories"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cat_level = sales_data.groupby(\"cat_id\")[date_col].sum().reset_index().set_index('cat_id').T\ncat_level['datum'] = [base + datetime.timedelta(days=x) for x in range(1913)]\ncat_level.set_index('datum', drop=True, inplace=True)\ncat_level.sort_index(inplace=True)\ncat_level.head()\ncat_level_level = cat_level.groupby(pd.Grouper(freq='1M')).sum()\ncat_level_level.iplot(title = \"Monthly Sales accross Categories\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see:\n* Food sales have always remained much higher than household and hobbies\n* Hobbies show a preety much flat trend and less seasonality\n* Food show the highest seasonality and household tend the show the highest increase in sales overall"},{"metadata":{},"cell_type":"markdown","source":"Let's see the time series decomposition for each of the categories as well to be more clear"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Plotting the sales time series decomposition for each state\nres1 = seasonal_decompose(cat_level_level[\"FOODS\"], model='additive')\nres2 = seasonal_decompose(cat_level_level[\"HOBBIES\"], model='additive')\nres3 = seasonal_decompose(cat_level_level[\"HOUSEHOLD\"], model='additive')\ndef plotseasonal(res, axes ):\n    res.observed.plot(ax=axes[0], legend=False)\n    axes[0].set_ylabel('Observed')\n    res.trend.plot(ax=axes[1], legend=False)\n    axes[1].set_ylabel('Trend')\n    res.seasonal.plot(ax=axes[2], legend=False)\n    axes[2].set_ylabel('Seasonal')\n    res.resid.plot(ax=axes[3], legend=False)\n    axes[3].set_ylabel('Residual')\n\nfig, axes = plt.subplots(ncols=3, nrows=4, sharex=True, figsize=(12,5))\n\nplotseasonal(res1, axes[:,0])\naxes[0,0].set_title(\"FOODS\")\nplotseasonal(res2, axes[:,1])\naxes[0,1].set_title(\"HOBBIES\")\nplotseasonal(res3, axes[:,2])\naxes[0,2].set_title(\"HOUSEHOLD\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see:\n* FOOD sales have increased quite much till 2012 and then is somewhat stagnant frim 2012 to 2016. And show a bit of spike in March and a huge spike in Auguts\n* HOBBIES sales have increased in each alternate years from 2012 Aug to 2013 Aug and then 2014 Aug to 2015 Aug. In has remained pretty stagnant in the rest two years. Here, the spikes in March is much more evident.\n* HOUSEHOLD sales have a better increasing trend and clean seasonality in March and August"},{"metadata":{},"cell_type":"markdown","source":"### Plotting monthly sales accross departments"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dept_level = sales_data.groupby(\"dept_id\")[date_col].sum().reset_index().set_index('dept_id').T\ndept_level['datum'] = [base + datetime.timedelta(days=x) for x in range(1913)]\ndept_level.set_index('datum', drop=True, inplace=True)\ndept_level.sort_index(inplace=True)\ndept_level.head()\ndept_monthly_level = dept_level.groupby(pd.Grouper(freq='1M')).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cf.Figure(cf.subplots([dept_monthly_level[['FOODS_1','FOODS_2','FOODS_3']].figure(),dept_monthly_level[['HOBBIES_1','HOBBIES_2']].figure(),dept_monthly_level[['HOUSEHOLD_1','HOUSEHOLD_2']].figure()],shape=(1,3),subplot_titles=('FOODS', 'HOBBIES', 'HOUSEHOLD'))).iplot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see:\n* In foods, FOODS_3 have the highest sales but also have a high seasonality ranging from 500k to 600k back and forth in the same year. FOODS_2 has also increased over the year.FOODS_1 has pretty much remained stagnant over the years\n* HOUSEHOLD_1 has shown the best increase in sales over the years as compared to other departmenrs"},{"metadata":{},"cell_type":"markdown","source":"### Plotting sales for each category in each of the state"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dept_cat_level = sales_data.groupby([\"state_id\",\"cat_id\"])[date_col].sum().reset_index().set_index([\"state_id\",\"cat_id\"]).T\ndept_cat_level['datum'] = [base + datetime.timedelta(days=x) for x in range(1913)]\ndept_cat_level.set_index('datum', drop=True, inplace=True)\ndept_cat_level.sort_index(inplace=True)\ndept_cat_level.columns = [f'{i}_{j}' if j != '' else f'{i}' for i,j in dept_cat_level.columns]\ndept_cat_monthly_level = dept_cat_level.groupby(pd.Grouper(freq='1M')).sum()\ncf.Figure(cf.subplots([dept_cat_monthly_level[['CA_FOODS','TX_FOODS','WI_FOODS']].figure(),dept_cat_monthly_level[['CA_HOBBIES','TX_HOBBIES','WI_HOBBIES']].figure(),dept_cat_monthly_level[['CA_HOUSEHOLD','TX_HOUSEHOLD','WI_HOUSEHOLD']].figure()],shape=(1,3),subplot_titles=('FOODS', 'HOBBIES', 'HOUSEHOLD'))).iplot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see:\n* Sales of Food items in CA have shown very less of an increase over the years although it has a high monthly seasonality peaking at August each year and dipping towards Year end and start (Nov, Dec, Jand and Feb)\n* Sales of Food items in TX also shows a similar trend although at a lower scale (High monthly seasonality peaking at August and dipping at Jan, Feb and overall trend remaining somewhat same)\n* Sales of Food items in WI has quite surpassed that of WI in 2015 and 2016. It's the only state showing an increase in Food items over the years. Also we dont see an obvious monthly seasonality in Food items at WI where peaks are at the month of March or July\n* Sales of Hobbies items doesnt show any monthly seasonalities as such\n* Rather in case of CA, sales of Hobbies items tends to increase every alternate year and dip at alternate year (aka cyclicity) with the dip in Nov,2012 as the worst but increase back quite well. Sales of Hobbies items at CA also shows an increasing trend\n* The sales of Hobbies items in TX and WI both show an increasing trend and have quite similar sales with a bit better sales increase in TX\n* Household sales shows the best increasing trend in all the states.\n* CA shows the best increase in Household items over the years and also has a monthly seasonality again peaking at August and dipping at December and January\n* TX shows a bit better sales than WI in Household throughout the years but both show similar increasing trend. They show an increase in sales of Household in August but it's not that evident enough to show a monthly seasonality. Infact, both also show a peak in Mar, 2013 "},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Across Week"},{"metadata":{},"cell_type":"markdown","source":"### Plotting sales over the week"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"days = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday', 'Sunday']\nsales_sum_weekday = sales_sum.groupby(sales_sum.index.weekday_name).mean().reindex(days)\nsns.set(rc={'figure.figsize':(15,5)})\nsns.barplot(x= sales_sum_weekday.index, y='sales', data=sales_sum_weekday)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Saturday sees the highest overall sales probably because of the first day of weekend and people rushing to buy groceries followed by Sunday also being a weekend"},{"metadata":{},"cell_type":"markdown","source":"### Plotting sales over the week for the 3 categories"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cat_level = sales_data.groupby(\"cat_id\")[date_col].sum().reset_index().set_index('cat_id').T\ncat_level['datum'] = [base + datetime.timedelta(days=x) for x in range(1913)]\ncat_level.set_index('datum', drop=True, inplace=True)\ncat_level.sort_index(inplace=True)\ncat_level.head()\ndays = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday', 'Sunday']\nsales_cat_weekday = cat_level.groupby([cat_level.index.weekday_name]).mean().reindex(days)\nsales_cat_weekday.iplot( kind=\"bar\",title = \"Avg. Sales across day of week\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see:\n* Sunday (~28.3K units) followed by Saturday (~28K units) are the dominant days for sales of Food Items \n* Saturday has more sales than Sunday in the case of Household and Hobbies although the differences are quite small\n* We do see a sense of weekly seasonality with highest sales in Saturday and Sunday and lowest sales in Wednesday and Thursday"},{"metadata":{},"cell_type":"markdown","source":"### 4.3 Across Months"},{"metadata":{},"cell_type":"markdown","source":"### Plotting sales over the months"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul','Aug', 'Sep', 'Oct', 'Nov', 'Dec'] \nmonthly_sales = sales_sum.groupby(sales_sum.index.strftime('%b')).mean().reindex(months)\nmonthly_sales.iplot( kind=\"bar\",title = \"Avg. Sales across months\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"August sees the highest overall sales in a year (~ 36K units sold in a month)"},{"metadata":{},"cell_type":"markdown","source":"### Plotting sales over the months across the 3 categories"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cat_level = sales_data.groupby(\"cat_id\")[date_col].sum().reset_index().set_index('cat_id').T\ncat_level['datum'] = [base + datetime.timedelta(days=x) for x in range(1913)]\ncat_level.set_index('datum', drop=True, inplace=True)\ncat_level.sort_index(inplace=True)\ncat_level.head()\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul','Aug', 'Sep', 'Oct', 'Nov', 'Dec'] \nmonthly_sales = cat_level.groupby(cat_level.index.strftime('%b')).mean().reindex(months)\nmonthly_sales.iplot( kind=\"bar\",title = \"Avg. Sales across months\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see:\n* August sees the highest sales in Foods which might be because of holidays or other climate conditions like high temperature and high precipitation\n* August also sees the highest sales in Household items\n* April and June sees the highest sales in Hobbies"},{"metadata":{},"cell_type":"markdown","source":"### 4.4 Across Days of Month"},{"metadata":{},"cell_type":"markdown","source":"### Plotting sales over the days of month"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"monthly_sales = sales_sum.groupby(cat_level.index.strftime('%d')).mean()\nsales_list = np.array(monthly_sales.values.tolist())\nsales_list = np.append(sales_list, np.repeat(np.nan, 4)).reshape(5,7)\nlabels = range(1,32)\nlabels = np.append(labels, np.repeat(np.nan, 4)).reshape(5,7)\nheat_map= sns.heatmap(sales_list,cmap = \"YlGnBu\",annot = labels, yticklabels = (\"Week 1\",\"Week 2\",\"Week 3\",\"Week 4\",\"Week 5\"))\nplt.title(\"Avg. Sales on day of month over the years\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Overall sales are primarily higher in the first two weeks of a month"},{"metadata":{},"cell_type":"markdown","source":"### Plotting sales over the month for the 3 categories"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cat_monthly_sales = cat_level.groupby(cat_level.index.strftime('%d')).mean()\nfoods_list = np.array(cat_monthly_sales['FOODS'].tolist())\nfoods_list = np.append(foods_list, np.repeat(np.nan, 4)).reshape(5,7)\nhobbies_list = np.array(cat_monthly_sales['HOBBIES'].tolist())\nhobbies_list = np.append(hobbies_list, np.repeat(np.nan, 4)).reshape(5,7)\nhousehold_list = np.array(cat_monthly_sales['HOUSEHOLD'].tolist())\nhousehold_list = np.append(household_list, np.repeat(np.nan, 4)).reshape(5,7)\nlabels = range(1,32)\nlabels = np.append(labels, np.repeat(np.nan, 4)).reshape(5,7)\n\n\nfig, (ax1, ax2 , ax3) = plt.subplots(1,3)\nfoods_map= sns.heatmap(foods_list,cmap = \"YlGnBu\",annot = labels, yticklabels = (\"Week 1\",\"Week 2\",\"Week 3\",\"Week 4\",\"Week 5\"), ax =ax1)\nhobbies_map= sns.heatmap(hobbies_list,cmap = \"YlGnBu\",annot = labels, yticklabels = (\"Week 1\",\"Week 2\",\"Week 3\",\"Week 4\",\"Week 5\"), ax =ax2)\nhousehold_map= sns.heatmap(household_list,cmap = \"YlGnBu\",annot = labels, yticklabels = (\"Week 1\",\"Week 2\",\"Week 3\",\"Week 4\",\"Week 5\"), ax =ax3)\nax1.set_title('FOODS')\nax2.set_title('HOBBIES')\nax3.set_title('HOUSEHOLD')\nplt.suptitle(\"Avg. Sales on day of month over all years across different categories \")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we get:\n* Food Items are bought in both the first and second week of a month\n* Hobbies and Household are only primarily bought in the first 3 days of the months."},{"metadata":{},"cell_type":"markdown","source":"# 5. Impact of Events and SNAP days on sales"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"overall_sales_special.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gauging impact of events"},{"metadata":{},"cell_type":"markdown","source":"### Plotting daily sales for the year 2012 to see the pattern of impact of events"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"overall_sales_special[overall_sales_special.year == 2012].groupby(\"date\")[\"sales\"].sum().iplot(title = \"Daily Overall Sales\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"Event days in 2012\")\noverall_sales_special[(overall_sales_special.year == 2012) & ((overall_sales_special.event_name_1.notnull()) | (overall_sales_special.event_name_2.notnull()))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we closely look at the sales for 2012 we see that people** prefer buying on the weekends(Saturdays preferably) *before the Events rather on the days of the events***. So we don't see a increase in the sales on the days of the events. But the increase in sales on the weekends before that can be attributed to that event. We see some exception though like Labour Day which is a Monday, we still see a peak on that day. Another one is Thanksgiving which is on a Thurday but we see a peak on the day before which is a wednesday"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Function for tagging events to the preceding weekend \nevent_days_sales = overall_sales_special[((overall_sales_special.event_name_1.notnull()) | (overall_sales_special.event_name_2.notnull()))]\noverall_sales_special[\"weekend_precede_event\"] = np.nan\n\ndef update_weekend_precede_event(week_e,wday,e1,e2):\n    e2 = '_' + e2 if type(e2) == str else ''\n    drift = e1 + e2\n    if wday == 1:\n        overall_sales_special.loc[(overall_sales_special['wm_yr_wk']==week_e)&(overall_sales_special['wday']==1),\"weekend_precede_event\"] = drift\n    else:\n        overall_sales_special.loc[(overall_sales_special['wm_yr_wk']==week_e)&((overall_sales_special['wday']==1)|(overall_sales_special['wday']==2)),\"weekend_precede_event\"] = drift\n        \n_ = event_days_sales.apply(lambda row : update_weekend_precede_event(row['wm_yr_wk'],row['wday'],row['event_name_1'], row['event_name_2']),axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"Events data with added weekend_prece_event column which marks the weekend before each of the event along with the event name\")\noverall_sales_special.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that Super Bowl event was on Sunday and we have mapped it to the previous Saturday and Sunday via the column weekend_precede_event"},{"metadata":{},"cell_type":"markdown","source":"### First let's look at impact of different types of events and then we will look at specific events"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#adding event type column\nevents.columns = [\"weekend_precede_event_type\",\"weekend_precede_event\"]\noverall_sales_special = pd.merge(overall_sales_special,events,how= \"left\",on=\"weekend_precede_event\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting sales of weekends before different event types"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Calculating sales impact of each event on preceding weekend\nevent_type_impact = overall_sales_special.groupby(['weekend_precede_event_type'])['sales'].mean().reset_index()\nevent_type_impact = event_type_impact.sort_values(\"sales\",ascending = False)\nevent_type_impact.columns = [\"event_type\",\"avg_sales_preceding_weekend\"]\n#Plotting a bar graph of avg. sales on the weekend days before the event to see the impact\nchart = sns.barplot(y= \"event_type\", x='avg_sales_preceding_weekend', data=event_type_impact)\nchart.axvline(sales_sum.sales.mean(),label = \"Avg. sales in a day\",c='red', linestyle='dashed')\nplt.title(\"Avg. Sales on preceding event of each type of event\", fontweight =\"bold\")\nleg = plt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see:\n* Weekends before all the event types have much higher sales than the avg. sales per day\n* Religious events have the highest impact to sales of the preceding weekend \n* National events have the lowest impact to sales of the preceding weekend"},{"metadata":{},"cell_type":"markdown","source":"### Plotting sales of weekends preceding each event"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Calculating sales impact of each event on preceding weekend\nevent_impact = overall_sales_special.groupby(['weekend_precede_event'])['sales'].mean().reset_index()\nevent_impact = event_impact.sort_values(\"sales\",ascending = False)\nevent_impact.columns = [\"events\",\"avg_sales_preceding_weekend\"]\n# Plotting a bar graph of avg. sales on the weekend days before the event to see the impact\nsns.set(rc={'figure.figsize':(15,3)})\nchart = sns.barplot(x= \"events\", y='avg_sales_preceding_weekend', data=event_impact)\nchart.axhline(sales_sum.sales.mean(),label = \"Avg. sales in a day\",c='red', linestyle='dashed')\nvar = chart.set_xticklabels(chart.get_xticklabels(), rotation=90)\nplt.title(\"Avg. sales on preceding weekend of each event\",fontweight = \"bold\")\nleg = plt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see:\n* Almost all weekends have higher sales than the average sales i.e 34k so events do impact sales\n* The highest sales is in the weekend before Easter as expected about 44k each day followed by EidAlAdha which are both Religious events (which follows our event type finding of religious event having the highest impact)\n* The lowest sales is in the weekend before New Year "},{"metadata":{},"cell_type":"markdown","source":"## Impact of SNAP days"},{"metadata":{},"cell_type":"markdown","source":"Different states have different SNAP dates so we will have to view them separetly"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Joining the state wise sales with the events table\noverall_sales_special = pd.merge(overall_sales_special,state_level.reset_index(),how = \"left\",left_on=\"date\",right_on=\"datum\")\noverall_sales_special.drop(\"datum\",axis = 1,inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Comparing the days with and w/o SNAP for all 3 states\nca_snap = overall_sales_special.groupby(\"snap_CA\")[\"CA\"].mean().reset_index()\ntx_snap = overall_sales_special.groupby(\"snap_TX\")[\"TX\"].mean().reset_index()\nwi_snap = overall_sales_special.groupby(\"snap_WI\")[\"WI\"].mean().reset_index()\nca_snap.columns = [\"Snap\",\"CA\"]\ntx_snap.columns = [\"Snap\",\"TX\"]\nwi_snap.columns = [\"Snap\",\"WI\"]\nsnap_impact = pd.merge(ca_snap,tx_snap,on = \"Snap\")\nsnap_impact = pd.merge(snap_impact,wi_snap,on = \"Snap\")\nsnap_impact = pd.melt(snap_impact, id_vars=['Snap'], value_vars=['CA','TX','WI'],var_name='State', value_name='Avg Sales')\n#Plotting bar plots for sales comparison\nsns.set(rc={'figure.figsize':(10,7)})\nchart=sns.barplot(x= \"State\", y='Avg Sales',hue = 'Snap' ,data=snap_impact)\nchart.axhline(overall_sales_special.CA.mean(),label = \"Avg. sales CA\",c='red', linestyle='dashed')\nchart.axhline(overall_sales_special.TX.mean(),label = \"Avg. sales TX\",c='blue', linestyle='dashed')\nchart.axhline(overall_sales_special.WI.mean(),label = \"Avg. sales WI\",c='black', linestyle='dashed')\nvar = chart.set_xticklabels(chart.get_xticklabels(), rotation=90)\nplt.title(\"Avg. Sales in each state on SNAP days(1) and on other days(0)\",fontweight=\"bold\")\nleg = plt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see:\n* All the states have higher sales on SNAP days. We can see the max increase in WI (~2k more sales on each day) while CA and TX have ~1k more sales on each day of SNAP\n* We have seen higher sales during the first 10-15 days of a month.We have also seen that SNAP days fall in the first 15 days. So, there might be a combined effect."},{"metadata":{},"cell_type":"markdown","source":"# 6. Analysis on prices changes"},{"metadata":{},"cell_type":"markdown","source":"Sales data of each item at a weekly level"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"item_sales = sales_data.groupby(\"item_id\")[date_col].sum().reset_index().set_index('item_id').T\n#Setting the start date\nbase = datetime.datetime(2011,1,29)\n#Adding the date column\nitem_sales['date'] = [base + datetime.timedelta(days=x) for x in range(1913)]\nitem_sales = pd.merge(item_sales,overall_sales_special[[\"date\",\"wm_yr_wk\"]],on = \"date\")\nitem_sales=item_sales.groupby([\"wm_yr_wk\"]).sum().reset_index()\nitem_sales['date'] = [base + datetime.timedelta(days=x) for x in range(0,1913,7)]\nitem_sales = item_sales.melt(id_vars=['wm_yr_wk',\"date\"], value_vars=item_sales.columns.drop([\"wm_yr_wk\",\"date\"]), var_name='item_id', value_name='sales')\nitem_sales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distribution of median prices of items over the years"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"item_mean_prices = price_data.groupby(\"item_id\")[\"sell_price\"].median().reset_index()\nitem_mean_prices.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's use a threshold of 3.42 to as the median to bucket each item as Cheap or Costly"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"labels = [\"Cheap\",\"Costly\"]\n#Bucketing each item as cheap or costly\nitem_mean_prices[\"item_price_bucket\"] =  pd.cut(item_mean_prices.sell_price, [0,3.42,np.inf], include_lowest=True,labels = labels)\nitem_mean_prices.head()\n#Joining with the actual table\nprice_data_bucketed = pd.merge(price_data,item_mean_prices[[\"item_id\",\"item_price_bucket\"]], on = \"item_id\",how = \"left\")\n#Joining with Sales data\nprice_data_bucketed = pd.merge(price_data_bucketed,item_sales,on = [\"wm_yr_wk\",\"item_id\"],how = \"left\")\n\nbase = datetime.datetime(2011,1,29)\n#Adding the date column\n\nprice_data_bucketed.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the changes in prices over the weeks for each Category and Price Bucket level"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Creating table at category, price bucket level\nmean_table = price_data_bucketed.groupby([\"date\",\"Category\",\"item_price_bucket\"]).agg({\"sell_price\":\"mean\",\"sales\":\"sum\"}).reset_index()\nmean_table[\"cat-bucket\"] = mean_table[\"Category\"].astype(str) + '-'+mean_table[\"item_price_bucket\"].astype(str)\n\n\n#PLotting the graph\nsns.set(rc={'figure.figsize':(20,5)})\nfig, (ax1, ax2) = plt.subplots(1,2)\nflatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\nsns.set_palette(flatui)\nprices_plot = sns.lineplot(x = \"date\",y = \"sell_price\",hue = \"cat-bucket\",data = mean_table,ax =ax1)\nsales_plot = sns.lineplot(x = \"date\",y = \"sales\",hue = \"cat-bucket\",data = mean_table,ax=ax2)\nax1.title.set_text(\"Change in avg. prices over the years for cheap and costly items of each category\")\nax2.title.set_text(\"Change in total sales over the years for cheap and costly items of each category\")\nfig.suptitle('Price and sales changes over the years',fontweight=\"bold\")\nax2.set_yscale('log')\nleg = ax2.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), shadow=True, ncol=3)\nleg = ax1.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), shadow=True, ncol=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see:\n* The prices for products in the cheaper products have increased very less over the years for all the 3 Categories\n* In the costlier bucket, we could see increase in avg.prices specially in HOBBIES in 2012 and 2013. FOODS also show some increase in prices \n* The increase in prices in Costly Hobbies product didnt impact the sales in 2012 and 2013, it actually grew quite well in those years.What we actually see is a dip in sales of Cheaper Hobbies products between 2012 and 2013\n* The gap between the selling price of cheaper products and costlier products of Food products is lowest, but the same gap is the highest when compared in terms of sales (i..e Betwee Cheaper Food and and Costlier Food).\n* The gap between the selling price of cheaper products and costlier products of Hobbies products is highest, but gap is quite low when compared in terms of sales (i..e Betwee Cheaper Hobbies products and and Costlier Hobbies products)."},{"metadata":{},"cell_type":"markdown","source":"Highly Influenced by :\nhttps://www.kaggle.com/headsortails/back-to-predict-the-future-interactive-m5-eda"},{"metadata":{},"cell_type":"markdown","source":"**Please Upvote my notebook if you like it** ðŸ™"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}