{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport tensorflow as tf\n\nfrom typing import Union\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading data\npath_folder = \"../input/m5-forecasting-accuracy/\"\n\ndf_train_full = pd.read_csv(path_folder + \"sales_train_evaluation.csv\")\ndf_calendar = pd.read_csv(path_folder + \"calendar.csv\")\ndf_prices = pd.read_csv(path_folder + \"sell_prices.csv\")\ndf_sample_submission_original = pd.read_csv(path_folder + \"sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Code to verify score and public ranking (used to validate model during testing)\nSource: https://www.kaggle.com/rohanrao/m5-how-to-get-your-public-lb-score-rank","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## evaluation metric\n## from https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834 and edited to get scores at all levels\nclass WRMSSEEvaluator(object):\n\n    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame):\n        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n        train_target_columns = train_y.columns.tolist()\n        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n\n        train_df['all_id'] = 0  # for lv1 aggregation\n\n        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n\n        if not all([c in valid_df.columns for c in id_columns]):\n            valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.calendar = calendar\n        self.prices = prices\n\n        self.weight_columns = weight_columns\n        self.id_columns = id_columns\n        self.valid_target_columns = valid_target_columns\n\n        weight_df = self.get_weight_df()\n\n        self.group_ids = (\n            'all_id',\n            'cat_id',\n            'state_id',\n            'dept_id',\n            'store_id',\n            'item_id',\n            ['state_id', 'cat_id'],\n            ['state_id', 'dept_id'],\n            ['store_id', 'cat_id'],\n            ['store_id', 'dept_id'],\n            ['item_id', 'state_id'],\n            ['item_id', 'store_id']\n        )\n\n        for i, group_id in enumerate(tqdm(self.group_ids)):\n            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n            scale = []\n            for _, row in train_y.iterrows():\n                series = row.values[np.argmax(row.values != 0):]\n                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n            setattr(self, f'lv{i + 1}_train_df', train_y)\n            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())\n\n            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n            setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())\n\n    def get_weight_df(self) -> pd.DataFrame:\n        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n\n        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n        return weight_df\n\n    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n        valid_y = getattr(self, f'lv{lv}_valid_df')\n        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n        scale = getattr(self, f'lv{lv}_scale')\n        return (score / scale).map(np.sqrt)\n\n    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n\n        group_ids = []\n        all_scores = []\n        for i, group_id in enumerate(self.group_ids):\n            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n            weight = getattr(self, f'lv{i + 1}_weight')\n            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n            group_ids.append(group_id)\n            all_scores.append(lv_scores.sum())\n\n        return group_ids, all_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## public LB rank\ndef get_lb_rank(score):\n    \"\"\"\n    Get rank on public LB as of 2020-05-31 23:59:59\n    \"\"\"\n    df_lb = pd.read_csv(\"../input/m5-accuracy-final-public-lb/m5-forecasting-accuracy-publicleaderboard-rank.csv\")\n\n    return (df_lb.Score <= score).sum() + 1\n\ndef get_ranking_from_submission(submission_original, evaluator, df_sample_submission_original=df_sample_submission_original):\n    submission = submission_original.copy()\n    submission = submission[submission.id.str.contains(\"validation\")]\n    \n    df_sample_submission = df_sample_submission_original.copy()\n    df_sample_submission[\"order\"] = range(df_sample_submission.shape[0])\n\n    submission = submission.merge(df_sample_submission[[\"id\", \"order\"]], on = \"id\").sort_values(\"order\").drop([\"id\", \"order\"], axis = 1).reset_index(drop = True)\n    submission.rename(columns = {\n        \"F1\": \"d_1914\", \"F2\": \"d_1915\", \"F3\": \"d_1916\", \"F4\": \"d_1917\", \"F5\": \"d_1918\", \"F6\": \"d_1919\", \"F7\": \"d_1920\",\n        \"F8\": \"d_1921\", \"F9\": \"d_1922\", \"F10\": \"d_1923\", \"F11\": \"d_1924\", \"F12\": \"d_1925\", \"F13\": \"d_1926\", \"F14\": \"d_1927\",\n        \"F15\": \"d_1928\", \"F16\": \"d_1929\", \"F17\": \"d_1930\", \"F18\": \"d_1931\", \"F19\": \"d_1932\", \"F20\": \"d_1933\", \"F21\": \"d_1934\",\n        \"F22\": \"d_1935\", \"F23\": \"d_1936\", \"F24\": \"d_1937\", \"F25\": \"d_1938\", \"F26\": \"d_1939\", \"F27\": \"d_1940\", \"F28\": \"d_1941\"\n    }, inplace = True)\n\n    groups, scores = evaluator.score(submission)\n\n    score_public_lb = np.mean(scores)\n    score_public_rank = get_lb_rank(score_public_lb)\n\n    for i in range(len(groups)):\n        print(\"Score for group {}: {}\".format(groups[i],round(scores[i], 5)))\n        \n    print(\"\\nPublic LB Score: {}\".format(round(score_public_lb, 5)))\n    print(\"Public LB Rank: {}\".format(score_public_rank))\n    \n    return None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Approach: 'GROUPED' modelling per store and department data day-by-day approach\n- 70 groups of store + department -> 70 models to build (between 149 and 532 items for each)\n- Inputs (for each store-department combination): previous sales, day + price data (only for the day to predict)\n\nReasons for such division:\n- kaggle notebook doesn't have enough RAM to handle the whole dataset and a correspondingly complex model at once\n- our hypothesis here is that the sale of an item in a store in one of the departments has an impact on a different item in that same department and store (e.g. a customer picks up an item and it reminds him of buying another one close to that item)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Feature engineering\nwe will transpose the day data (calendar and prices) and then stack it to our current input X (or X_reshaped)\n\nOther possible ideas:\n- scale the input data before modelling (only X should be scaled, and Y needs to stay as is) \n- start training when the item is first sold (different timeframes for each items / store)\n- average price accross all items for each day to add to the calendar data (to account for nationwide promotions)\n- temperature and weather on each day (e.g. if raining, probably not much visits to the store -> fewer sales)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#global variables\nT_predict = 28\nT_train = 30 #each sample will have 30 days of history (sales and daily data)\ntimeframe = 1*365 + 1 #we consider data from more than 1 year old is too outdated and useless in our modelling\nnodes_initial = 1000\ninitial_learning_param = 0.0004\nepoch_max = 50\ndrop_out = 0.01\n# batch_size = 1 #adding a batch doesn't seem helpful improving the model performance\n\ntest = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_features_per_day(df_calendar, timeframe, T_predict, df_prices, dept, store, test=test):\n    '''\n    Create features based on the day, events and each item's price (for the ones in the dept and store)\n    '''\n    df_calendar_edited = calendar_data(df_calendar, timeframe, T_predict, test)\n    df_prices_limited = prices_data(df_prices, df_calendar_edited, dept, store)\n    df_daily_data = pd.merge(df_calendar_edited, df_prices_limited, left_on='wm_yr_wk', right_index=True)\n    df_daily_data = df_daily_data.drop('wm_yr_wk', axis=1)\n    \n    return df_daily_data\n\n\ndef calendar_data(df_calendar, timeframe, T_predict, test):\n    '''\n    Builds the calendar data\n    '''\n    if test:\n        df_calendar_edited = df_calendar.iloc[-timeframe-2*T_predict:-T_predict].copy() ###FOR TESTING COMPARED TO VALIDATION SET\n    else:\n        df_calendar_edited = df_calendar.iloc[-timeframe-T_predict:].copy() ###FOR FINAL EVALUATION\n\n    #one hot encoding on the weekday and event_type_1 (because few values but these should be interesting for high spending days)\n    list_col_one_hot = ['weekday', 'event_type_1']\n    list_drop_or_not = [True, False] #removes one of the weekday to avoid multicollinearity later on\n    for col_one_hot,drop_or_not in zip(list_col_one_hot,list_drop_or_not):\n        df_calendar_edited = pd.concat([df_calendar_edited, pd.get_dummies(df_calendar_edited[col_one_hot], prefix=col_one_hot, drop_first=drop_or_not)], axis=1)\n\n    df_calendar_edited = df_calendar_edited.set_index('d')\n\n    list_col_calendar_keep = ['wm_yr_wk']\n    for col_to_add in list_col_one_hot:\n        list_col_keep_encoded = [col for col in df_calendar_edited.columns if col_to_add in col and col!=col_to_add]\n        list_col_calendar_keep += list_col_keep_encoded\n\n    df_calendar_edited = df_calendar_edited[list_col_calendar_keep]\n    \n    return df_calendar_edited\n\n\ndef prices_data(df_prices, df_calendar_edited, dept, store):\n    '''\n    Builds the pricing data\n    '''\n    #define the prices for each of the weeks\n    df_prices_limited = df_prices[df_prices['wm_yr_wk'].isin(df_calendar_edited['wm_yr_wk'].unique())].copy()\n    df_prices_limited['dept_id'] = df_prices_limited['item_id'].astype(str).str[:-4]\n    df_prices_limited = df_prices_limited[(df_prices_limited['dept_id'] == dept) & (df_prices_limited['store_id'] == store)]\n\n    #get the price for each item on each week in the separated one-hot encoded columns\n    df_prices_limited = pd.concat([df_prices_limited, pd.get_dummies(df_prices_limited[\"item_id\"], prefix=\"\")], axis=1)\n\n    list_col_items_encoded = [col for col in df_prices_limited.columns if col.startswith('_')]\n    for col_item_encoded in list_col_items_encoded:\n        df_prices_limited[col_item_encoded] *= df_prices_limited[\"sell_price\"]\n\n    #remove duplicate weeks\n    df_prices_limited = df_prices_limited[list_col_items_encoded + [\"wm_yr_wk\"]].groupby(\"wm_yr_wk\").sum()\n    \n    return df_prices_limited","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling\n\nSimple FFN (Feed-Forward Network) ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def approach3(df_train_full, df_sample_submission_original=df_sample_submission_original, df_calendar=df_calendar, \\\n              df_prices=df_prices, T_predict=T_predict, T_train=T_train, timeframe=timeframe, \\\n              nodes_initial=nodes_initial, initial_learning_param=initial_learning_param, epoch_max=epoch_max, drop_out=drop_out, \\\n             test=test):\n    '''\n    For each department+store combination (70 groups), get the data input, train a model and edit the submission file for this group\n    Note: we predict T_predict (default=28) days based on samples of T_train (default=365) days from all items\n    '''\n    \n    list_unique_dept_id = list(df_train_full['dept_id'].unique())\n    list_unique_store_id = list(df_train_full['store_id'].unique())\n\n    submission_grouped = df_sample_submission_original.set_index('id')\n    \n    counter = 1\n    for dept in list_unique_dept_id:\n        for store in list_unique_store_id:\n#     for dept, store in zip([list_unique_dept_id[3]], [list_unique_store_id[2]]):\n            print(\"counter:\", counter, \" - DEPARTMENT, STORE:\", dept, store)\n            df_daily_data = build_features_per_day(df_calendar, timeframe, T_predict, df_prices, dept, store)\n            df_train_limited = df_train_full[(df_train_full['dept_id'] == dept) & (df_train_full['store_id'] == store)]\n            \n            submission_grouped = approach_per_store(df_train_limited, submission_grouped, df_daily_data, \\\n                                                    T_predict, T_train, timeframe, nodes_initial, initial_learning_param, \\\n                                                    epoch_max, drop_out, test)\n            counter += 1\n    \n    if test:\n        ###FOR TESTING COMPARED TO VALIDATION SET        \n        get_evaluation(submission_grouped, df_train_full, df_calendar, df_prices)\n    else:\n        #add the numbers for the validation part (in case the evaluation is also based on these values)\n        submission_grouped.iloc[:30490,:] = df_train_full.iloc[:,-T_predict:].values\n        \n    return submission_grouped\n\n\ndef approach_per_store(df_train_limited, submission_grouped, df_daily_data, T_predict, T_train, timeframe, \\\n                       nodes_initial, initial_learning_param, epoch_max, drop_out, test):\n    \n    '''\n    Get the department+store input data, train the model, then predict for the next 28 days and edit the submission\n    '''\n    \n    if test:\n        df_train_grouped = df_train_limited.set_index(\"id\").iloc[:, 5:-28] ###FOR TESTING COMPARED TO VALIDATION SET\n    else:\n        df_train_grouped = df_train_limited.set_index(\"id\").iloc[:, 5:] ###FOR FINAL EVALUATION\n    \n    samples = timeframe - T_train\n    \n    X_reshaped, Y, in_dim, out_dim = make_input_data(df_train_grouped, df_daily_data, samples, timeframe)\n    history, model = build_train_model(X_reshaped, Y, in_dim, out_dim, nodes_initial, initial_learning_param,epoch_max,drop_out)\n    \n    #fill in the evaluation part in the submission file\n    submission_grouped = build_validation_predictions_per_dept_store(model, df_train_grouped, df_daily_data, \\\n                                                                     timeframe, T_train, T_predict, submission_grouped)\n    \n    return submission_grouped\n\n\ndef make_input_data(df_train_grouped, df_daily_data, samples, timeframe):\n    '''\n    Get model input data for the store+department group\n    Sales and days data are combined together the model input\n    '''\n    \n    X_reshaped = []\n    X_part1_sales = []\n    X_part2_days = []\n    Y = []\n\n    for col_index in range(samples):\n        \n        #first type of data: item sales\n        sales_array = df_train_grouped.iloc[:, -(timeframe - col_index):-(samples - col_index)].to_numpy()\n        output_sales_array = df_train_grouped.iloc[:, -(samples - col_index)].to_numpy()\n\n        #second type of data: daily data\n        day_data_array = df_daily_data.iloc[col_index + 1 : col_index + 1 + T_train + 1].transpose().to_numpy()\n        \n        #concatenate both inputs and reshape so that the ML model can handle it\n        concatenation_input_array = np.concatenate([sales_array.reshape(-1),day_data_array.reshape(-1)])\n\n        X_reshaped.append(concatenation_input_array)\n        Y.append(output_sales_array)\n\n    X_reshaped = np.stack(X_reshaped, axis=0)\n    Y = np.stack(Y, axis=0)\n    in_dim = X_reshaped.shape[1]\n    out_dim = Y.shape[1]\n    \n    return X_reshaped, Y, in_dim, out_dim\n\n\ndef build_train_model(X_reshaped, Y, in_dim, out_dim, nodes_initial, initial_learning_param, epoch_max, drop_out):\n    '''\n    Build a model based on the department+store group of data\n    Simple FFN, with a minor dropout to prevent overfitting (the model doesn't overfit much as its capacity is relatively low)\n    '''\n    \n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Dense(nodes_initial, activation='relu', input_dim=in_dim),\n        tf.keras.layers.Dropout(drop_out),\n        tf.keras.layers.Dense(nodes_initial, activation='relu'),\n        tf.keras.layers.Dense(out_dim)\n    ])\n\n    optimizer_model = tf.keras.optimizers.Adam(initial_learning_param)\n    model.compile(loss = 'mse', optimizer = optimizer_model)\n\n    #train model\n    history = model.fit(X_reshaped, Y,epochs=epoch_max)\n    \n    return history, model\n\n\ndef get_evaluation(submission_grouped, df_train_full, df_calendar, df_prices):\n    '''\n    When testing the model, edit the submission file so that the validation rows are the same as the evaluation rows\n    We then use the existing public leaderboard (as it was before the validation data was released) to get the predicted ranking\n    '''\n    \n    #NEEDS TO EDIT submission_grouped (now we edit the \"evaluation\" index and the \"validation\" one needs to be edited to get the evaluation)\n    submission_grouped_full = submission_grouped.copy()\n    submission_grouped_full.iloc[:30490,:] = submission_grouped_full.iloc[30490:,:].values\n    submission_grouped_full.reset_index(inplace=True)\n    \n    #get current ranking (to change when building the final submission)\n    evaluator = WRMSSEEvaluator(df_train_full.iloc[:, :-28], df_train_full.iloc[:, -28:], df_calendar, df_prices)\n    get_ranking_from_submission(submission_grouped_full, evaluator)\n    \n    return None\n\n\ndef build_validation_predictions_per_dept_store(model, df_train_grouped, df_daily_data, timeframe, T_train, T_predict, submission_grouped):\n    '''\n    Edit the submission file based on the predictions for the department and store group\n    '''\n    \n    validation_predictions = predictions(model, df_train_grouped, df_daily_data, timeframe, T_train, T_predict)\n    \n    #update submission (i.e. output file)\n    submission_grouped.loc[df_train_grouped.index,:] = validation_predictions.transpose()\n    \n    return submission_grouped\n\n\ndef predictions(model, df_train_grouped, df_daily_data, timeframe, T_train, T_predict):\n    '''\n    Build predictions arrays based on the model (day by day)\n    Note: we need to roll the T_predict (default=30 days) predicted arrays for both the sales data and the daily data\n    '''\n    \n    validation_predictions = []\n\n    #initialize sales data\n    rolling_sales_data = df_train_grouped.iloc[:, -T_train:].to_numpy().astype(float)\n\n    for prediction_day in range(1,T_predict+1):\n        # prediction_day = 1\n        #prediction_day = 1 is for d_1914, all the way to prediction_day = T_predict (default = 28) for d_1941\n\n\n        #daily data: simple index change\n        rolling_day_data = df_daily_data.iloc[timeframe - T_train + 1 + prediction_day - 2: \\\n                                              timeframe + 1 + prediction_day -1].transpose().to_numpy()\n\n        X_input_for_prediction = np.concatenate([rolling_sales_data.reshape(-1),rolling_day_data.reshape(-1)]).reshape(1,-1)\n\n        p = model.predict(X_input_for_prediction)[0]\n        p[p<0] = 0\n\n        # update the predictions list\n        validation_predictions.append(p)\n\n        #sales data: need to roll using the predicted next day's sales data\n        rolling_sales_data = np.roll(rolling_sales_data.transpose(), -1, axis=0).transpose()\n        rolling_sales_data.transpose()[-1] = p\n\n    validation_predictions = np.stack(validation_predictions, axis=0)\n\n    return validation_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_final = approach3(df_train_full)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission_final.to_csv(\"submission_M5_20200620_v2.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Other approaches tried:\n\n- item by item: too time-consuming without even using daily data (about 9h to compute with a simple FFN)\n- all items together (not grouping by store and department) with a simple FFN: too memory-heavy and lower performance\n- custom LSTM model (please see below): too time-consuming and lower performance\n    - Past sales -> go through an initial cell: simple RNN or LSTM\n    - The output of 1. and the array of next day data together with the array of price data -> go through a FFN\n    - The output of 2. -> goes through a final Dense layer to predict the next day sales for all items in the group\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_input_data(df_train_grouped, df_daily_data, samples, timeframe):\n    '''\n    We have 2 inputs to our model, one X_main going through a LSTM cell, the other X_day going through a standard Dense layer\n    The model output to predict is Y\n    '''\n    \n    #get model input data\n    X_main = []\n    X_day = []\n    Y = []\n\n    for col_index in range(samples):\n        # col_index = 335 ###THIS IS JUST FOR TESTING (within the make_input_data function)\n\n        #first type of data: item sales\n        sales_array = df_train_grouped.iloc[:, -(timeframe - col_index):-(samples - col_index)].to_numpy()\n        output_sales_array = df_train_grouped.iloc[:, -(samples - col_index)].to_numpy()\n\n        #second type of data: daily data\n        day_data_array = df_daily_data.iloc[col_index,:].to_numpy()\n        \n        #concatenate both inputs and reshape so that the ML model can handle it\n#         concatenation_input_array = np.concatenate([sales_array.reshape(-1),day_data_array.reshape(-1)])\n        \n        #reshape input sales array\n#         reshape_input_array = sales_array.reshape(-1)\n\n        X_main.append(sales_array)\n        X_day.append(day_data_array)\n        Y.append(output_sales_array)\n\n    X_main = np.stack(X_main, axis=0)\n    X_day = np.stack(X_day, axis=0)\n    Y = np.stack(Y, axis=0)\n    in_dim_main = (X_main.shape[1], X_main.shape[2])\n    in_dim_day = X_day.shape[1]\n    out_dim = Y.shape[1]\n    \n    return X_main, X_day, Y, in_dim_main, in_dim_day, out_dim\n\n\ndef custom_recurrent_model(in_dim_main, in_dim_day, out_dim, nodes_initial):\n    '''\n    Custom LSTM model\n    X_main goes through an LSTM cell\n    X_day goes through a Dense layer\n    Then join together through another Dense layer\n    Then output array\n    '''\n    \n    X_main = Input(in_dim_main)\n    X_day = Input((in_dim_day,))\n    \n    #use recursive model on X_main\n    X_main_edited = LSTM(nodes_initial, dropout=drop_out, recurrent_dropout=drop_out)(X_main)\n    \n    #use a simple Dense layer on the daily data\n    X_day_edited = Dense(nodes_initial, activation = 'relu')(X_day)\n    \n    #combine the two X\n    X = Add()([X_main_edited, X_day_edited])\n    X = Activation('relu')(X)\n    \n    #add another FFN\n    X = Dense(nodes_initial, activation = 'relu')(X)\n    \n    #add one final layer for the output (regression)\n    X = Dense(out_dim)(X)\n    \n    model = Model(inputs=[X_main, X_day], outputs=X, name=\"Custom Reccurent Model\")\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Final Ranking:\n- Private LB Score: 0.72949\n- Private LB Rank: 729 / 5558 (top 14%)\n\n\n# Evolution of public rankings (for reference)\n\n\n## 1. All items together\n\ndrop_out = 0.2\n- Public LB Score: 1.72354\n- Public LB Rank: 4209\n\ndrop_out = 0.1\n- Public LB Score: 1.35338\n- Public LB Rank: 4103\n\n## 2. Grouped items by store and department\n\ndrop_out = 0.01, learning_rate = 0.0004, per group, 200 nodes\n- Public LB Score: 0.9927\n- Public LB Rank: 3673\n\nadding daily data and increasing capacity (1000 nodes, 50 epochs, no batch size)\n- Public LB Score: 0.85322\n- Public LB Rank: 3546","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}