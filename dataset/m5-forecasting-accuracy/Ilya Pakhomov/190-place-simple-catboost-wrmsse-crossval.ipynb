{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <font color='red'>WARNING!</font> \n### <font color='red'>This notebook was tested on the 24 Gb RAM machine with swap. It won't properly work on 16 Gb RAM machine (including Kaggle notebooks).</font>    \n### <font color='red'>Possibly without swap memory you will need more than 24 Gb of RAM to train this model.</font>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Top 4% Solution [190 Place] - Simple Lag Features + CatBoost  \n### (Bonus - WRMSSE Cross Validation!)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Surprisingly simple and straighforward solution**  \n\n**Key aspects**:  \n\n1) Uses **only 500 last dates from the dataset** (including 28 forecasting dates)  \n\n2) **Features of the dataset**:  \n\n- **Lag features on** \"sells\": lag 28, 35, and 42 (without any special aggregations)  \n    - 'f_value_lag*' columns in the dataset\n\n\n- **Rolling mean window features** for every \"sells\" lag feature: rolling mean with windows 7, 14, 21  \n    - 'f_rolling_mean_\\*_by_value_lag\\*' columns in the dataset\n\n\n- **Label encoded categorical features** - plain \"event\", \"snap*\" and product selling characteristics (department id, category id, store id etc.) \n    - 'f_item_id'\n    - 'f_dept_id'\n    - 'f_cat_id'\n    - 'f_store_id'\n    - 'f_state_id'\n    - 'f_event_name_1'\n    - 'f_event_type_1'\n    - 'f_event_name_2'\n    - 'f_event_type_2'\n    - 'f_snap_CA'\n    - 'f_snap_TX'\n    - 'f_snap_WI'\n    \n\n- **Sell price** without any modification\n\n\n- **Calendar** features:\n    - 'f_year'\n    - 'f_month'\n    - 'f_week'\n    - 'f_day'\n    - 'f_dayofweek'\n\n\n3) **Model**  \n- `Catboost` with `eval_function=\"RMSE\"` and default hyperparameters (only differences - using \"Tweedie\" as `loss_function`)\n- **Non-recursive prediction method**\n\n\nAlso in the end of the notebook you will find **WRMSSE cross-validation** code that we have used to validate our model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Special thanks to people whos discussions and notebook were used to make this solution:**  \n- @timetraveller98 for the discussion topic: [Why tweedie works?](https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/150614)\n- @sibmike for his [Fast & Clear WRMSSE 18ms](https://www.kaggle.com/sibmike/fast-clear-wrmsse-18ms)  \n\n**Thank you for your code and insights!**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport sys\nfrom datetime import datetime, timedelta\nimport gc\nimport joblib\nfrom typing import Iterable, Union, Tuple\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn import preprocessing, metrics\nfrom sklearn.metrics import make_scorer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score, TimeSeriesSplit, train_test_split, BaseCrossValidator\nfrom sklearn.base import BaseEstimator\nfrom sklearn.linear_model import LinearRegression\nfrom scipy.sparse import csr_matrix\nfrom catboost import Pool, CatBoostRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Functions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### For constructing features ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_lag_feature(data: pd.DataFrame, feature_name: str, shift: int, lag: int\n                    ) -> Tuple[pd.DataFrame, str]:\n    f_name = f'f_{feature_name}_lag{shift + lag}'\n    data[f_name] = data.groupby(['f_id'])[feature_name].transform(lambda x: x.shift(shift + lag))\n    return data, f_name\n\n\ndef add_rolling_window_mean_feature(\n        data: pd.DataFrame, shift: int, lag_feature_name: str, window: int\n    ) -> Tuple[pd.DataFrame, str]:\n\n    lag_feature_wo_f = lag_feature_name[2:] if lag_feature_name.startswith('f_') else lag_feature_name\n    f_name = f'f_rolling_mean_{window}_by_{lag_feature_wo_f}'\n\n    data[f_name] = data.groupby(['f_id'])[lag_feature_name].transform(\n        lambda x: x.shift(shift).rolling(window).mean())\n    return data, f_name\n\n\ndef add_date_features(data: pd.DataFrame, date_feature_column_name: str) -> pd.DataFrame:\n    data['f_year'] = data[date_feature_column_name].dt.year - data[date_feature_column_name].dt.year.min()\n    data['f_month'] = data[date_feature_column_name].dt.month\n    data['f_week'] = data[date_feature_column_name].dt.week\n    data['f_day'] = data[date_feature_column_name].dt.day\n    data['f_dayofweek'] = data[date_feature_column_name].dt.dayofweek\n    return data\n\ndef load_data(input_dir: str) -> tuple:\n    cal = pd.read_csv(f'{input_dir}/calendar.csv')\n    stv = pd.read_csv(f'{input_dir}/sales_train_evaluation.csv')\n    ss = pd.read_csv(f'{input_dir}/sample_submission.csv')\n    sellp = pd.read_csv(f'{input_dir}/sell_prices.csv')\n\n    cal = reduce_mem_usage(cal)\n    stv = reduce_mem_usage(stv)\n    ss = reduce_mem_usage(ss)\n    sellp = reduce_mem_usage(sellp)\n\n    return cal, stv, ss, sellp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### General \"reduce_mem_usage\"\n_(to reduce memory usage of the initial data)_ ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading initial data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nINPUT_DIR = '/kaggle/input/m5-forecasting-accuracy'\ncal, stv, ss, sellp = load_data(INPUT_DIR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Joining and denormalizing data \n_(for every \"date\" to be represented as a row instead of a column)_","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"d_cols = [c for c in stv.columns if 'd_' in c] # sales data columns\nsmall_cal = cal[['date', 'd', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', \n                 'snap_CA', 'snap_TX', 'snap_WI', 'wm_yr_wk']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We've left only 500 history dates from the initial data** (instead of all 1941)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many history days to leave in dataframe\nHISTORY_DAYS_TO_LEAVE = 500","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# Leave only train data\ndf = stv[stv['id'].str.endswith('_evaluation')]\n\n# Fill with days to predict\nlast_day = int(df.columns[-1].replace('d_', ''))\n\n# Drop days that are earlier than out CUT_DATE date\ncols_to_remove = [f'd_{i}' for i in range(1, last_day - HISTORY_DAYS_TO_LEAVE+1)]\ndf = df.drop(cols_to_remove, axis=1)\n\nfor day in range(last_day + 1, last_day + 28 + 1):\n    df[f'd_{day}'] = np.nan\n\ndf = df.melt(id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n             var_name=\"d\",\n             value_name=\"value\")\n# Join with calendar\ndf = df.join(small_cal.set_index('d'), how='left', on='d')\n# Join with prices, inner for deleting the days where there was no price for item\ndf = df.merge(sellp, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'inner')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cleaning initial data to save memory","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del cal, stv, sellp, small_cal\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features construction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- Give all features prefix 'f_' for easier distinction between initial data and constructed features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SHIFT_DAYS = 28","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LAGS = [0, 7, 14]\nWINDOWS = [7, 14, 21]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Categorical data preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id',\n                    'snap_CA', 'snap_TX', 'snap_WI',\n                    'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n\ndf[categorical_cols] = df[categorical_cols].astype('category')\n\n# Filling NaNs in categorical features\nfor i in categorical_cols:\n    df[i] = df[i].cat.add_categories('unknown')\n    df[i] = df[i].fillna('unknown')\n\n# these features will not be preprocessed\ndf = df.rename(columns={i: f'f_{i}' for i in categorical_cols})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Add lag features for \"sales\" data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nlag_features = []\nfor lag in tqdm(LAGS):\n    df, lag_f = add_lag_feature(df, 'value', SHIFT_DAYS, lag)\n    \n    lag_features.append(lag_f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Add window features for \"sales\" data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for lag_f in tqdm(lag_features):\n    for window in WINDOWS:\n        df, _ = add_rolling_window_mean_feature(df, 0, lag_f, window)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Add \"date\" features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['date'] = pd.to_datetime(df['date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndf = add_date_features(df, 'date')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Leave \"price\" as a feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.rename(columns={'sell_price': 'f_sell_price'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning the dataset after feature engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Leave only df with features and target","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = [i for i in df.columns if i.startswith('f_')]\n\ndf = df[['date', 'value'] + feature_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Call garbage collector \n... to free some memory","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Store the resulted dataframe to the filesystem\nThis way we can get the resulted dataframe from disk without doing feature engineering again","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# joblib.dump(df, 'df_for_training.joblib', protocol=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training & submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# df = joblib.load('df_for_training.joblib')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = [i for i in df.columns if i.startswith('f_')]\nfeature_cols.remove('f_id')\n\ncategorical_cols = list(df.select_dtypes(include=['int8', 'category']).columns)\ncategorical_cols.remove('f_id')\n\nnum_cols = list(set(feature_cols) - set(categorical_cols))\n\nfor feature in categorical_cols:\n    encoder = preprocessing.LabelEncoder()\n    df[feature] = encoder.fit_transform(df[feature])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['value'] = df['value'].astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, train_data, features):\n    train_data = data[data['date'] <= '2016-05-22']\n    train_data = train_data.dropna()`\n    \n    model.fit(train_data[features], train_data['value'])\n\ndef predict_model(model, data, features):\n    test = data[data['date'] > '2016-05-22']\n    \n    y_pred = model.predict(data[features])\n    \n    return y_pred\n\ndef form_submission(data, submission, filename):\n    predictions = data[(data['date'] > '2016-05-22')][['id', 'date', 'value']]\n    \n    validation_rows = [row for row in submission['id'] if 'validation' in row] \n    validation = submission[submission['id'].isin(validation_rows)]\n    \n    predictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'value').reset_index()\n    predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n    evaluation = submission[['id']].merge(predictions, on = 'id')\n    \n    final = pd.concat([validation, evaluation])\n    final.to_csv(filename, index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fitting Catboost with most of the default parameters, but:**\n- using `eval_metric='RMSE'` as the most approxiate one to WRMSSE\n- using `loss_function='Tweedie:variance_power=1.5'` because target variable (sales) are appropriate for \"Tweedie\" distribution  \n_(most of the values of \"sales\" is around zero, none of them are less than zero and distribution has a large tail of greater than 0 values_  \n_more about Tweedie and why it is appropriate here is in this great discussion topic: [Why tweedie works?](https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/150614) )","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"model = CatBoostRegressor(\n    eval_metric='RMSE',\n    cat_features=categorical_cols,\n    verbose=1,\n    loss_function='Tweedie:variance_power=1.5',\n    # you can tweak \"used_ram_limit\" param to reduce memory usage when training\n    # but the same 190 place result is not guaranteed\n    # used_ram_limit='10gb'\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model(model, df, feature_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# predict and insert oredictions to df\npred = predict_model(model, df, feature_cols)\ndf['value'] = df['value'].astype(pred.dtype)\n\ndf.loc[df['date'] > '2016-05-22', 'value'] = pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Read in the data\nINPUT_DIR = '../m5-forecasting-accuracy'\nss = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"form_submission(df.rename({'f_id': 'id'}, axis=1), ss, 'submission_final_catboost_500days.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BONUS\n#### WRMSSE Cross Validation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It is generally adviced to perform cross validation to validate machine learning models (to avoid overfitting by only analyzing metric for only one subset of training data)\n\nMost of the notebooks have been focusing on only computing WRMSSE for the last 28 days of the training dataset.\n\nWe've decided to refactor WRMSSE code from the great public notebook [Fast & Clear WRMSSE 18ms](https://www.kaggle.com/sibmike/fast-clear-wrmsse-18ms) into a function that:\n- performs all the nessesary calculations (\"W\" and \"S\" matrixes) only based on the input data to this function \n- works with the \"denormalized\" data (row with features for every date of every item - the same shape it is need to fit the machine learning models)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def wrmsse(x, y_true, y_pred, feature_cols, forecast_horizon=28):\n    sales = pd.concat([x.reset_index(drop=True), \n                       y_true.reset_index(drop=True)], axis=1)\n    sales = sales.pivot_table(index=['f_id', \n                               'f_state_id', \n                               'f_store_id', \n                               'f_cat_id',\n                               'f_dept_id',\n                               'f_item_id'], \n                              columns='date', \n                              values='value',\n                              aggfunc='first',\n                              fill_value=0).reset_index()\n    \n    sales_pred = pd.concat([x.reset_index(drop=True), \n                            y_pred.reset_index(drop=True)], axis=1)\n    sales_pred = sales_pred.pivot_table(index=['f_id', \n                               'f_state_id', \n                               'f_store_id', \n                               'f_cat_id',\n                               'f_dept_id',\n                               'f_item_id'], \n                              columns='date', \n                              values='value',\n                              aggfunc='first',\n                              fill_value=0).reset_index()\n    \n    # List of categories combinations for aggregations as defined in docs:\n    dummies_list = [sales.f_state_id.astype(str), \n                    sales.f_store_id.astype(str), \n                    sales.f_cat_id.astype(str), \n                    sales.f_dept_id.astype(str), \n                    sales.f_state_id.astype(str) +'_'+ sales.f_cat_id.astype(str), \n                    sales.f_state_id.astype(str) +'_'+ sales.f_dept_id.astype(str),\n                    sales.f_store_id.astype(str) +'_'+ sales.f_cat_id.astype(str), \n                    sales.f_store_id.astype(str) +'_'+ sales.f_dept_id.astype(str), \n                    sales.f_item_id.astype(str), \n                    sales.f_state_id.astype(str) +'_'+ sales.f_item_id.astype(str), \n                    sales.f_id.astype(str)]\n\n    ## First element Level_0 aggregation 'all_sales':\n    dummies_df_list =[pd.DataFrame(np.ones(sales.shape[0]).astype(np.int8), \n                                   index=sales.index, \n                                   columns=['all']).T]\n\n    # List of dummy dataframes:\n    for i, cats in enumerate(dummies_list):\n        dummies_df_list += [pd.get_dummies(cats, \n                                           drop_first=False, \n                                           dtype=np.int8).T]\n\n    # Concat dummy dataframes in one go:\n    roll_mat_df = pd.concat(dummies_df_list, \n                            keys=list(range(12)), \n                            names=['level','id'])#.astype(np.int8, copy=False)\n\n    roll_index = roll_mat_df.index\n    roll_mat_csr = csr_matrix(roll_mat_df.values)\n    \n    # Rollup sales:\n    d_cols = [i for i in sales.columns if isinstance(i, pd.Timestamp)]\n    sales_train_val = roll_mat_csr * sales[d_cols].values\n\n    no_sales = np.cumsum(sales_train_val, axis=1) == 0\n    sales_train_val = np.where(no_sales, np.nan, sales_train_val)\n\n    # Denominator of RMSSE / WRMSSE\n    S = np.nanmean(np.diff(sales_train_val,axis=1)**2,axis=1)\n    \n    # Calculate the total sales in USD for each item id:\n    df_for_w = x[['f_id', 'date', 'f_sale_usd']]\n    \n    d_cols.sort()\n    cols_for_w = d_cols[-forecast_horizon:]\n    \n    df_for_w = df_for_w[df_for_w['date'].isin(cols_for_w)]\n    \n    total_sales_usd = df_for_w.groupby(\n        ['f_id'], sort=False)['f_sale_usd'].apply(np.sum).values\n    \n    # Roll up total sales by ids to higher levels:\n    weight2 = roll_mat_csr * total_sales_usd\n    \n    numerator = 12*weight2\n    denominator = np.sum(weight2)\n    # Safe divide to replace divide by 0 infinity results with \"0\" values\n    W = np.divide(numerator, \n                  denominator, \n                  out=np.zeros_like(numerator), \n                  where=denominator!=0)\n    \n    denominator = np.sqrt(S)\n    # Safe divide to replace divide by 0 infinity results with \"0\" values\n    SW = np.divide(W, \n                   denominator, \n                   out=np.zeros_like(W), \n                   where=denominator!=0)\n    \n    return np.nansum(\n                np.sqrt(\n                    np.mean(\n                        np.square(roll_mat_csr*(sales_pred[d_cols[-forecast_horizon:]].values - \n                                                sales[d_cols[-forecast_horizon:]].values))\n                            ,axis=1)) * SW)/12","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Function that makes time series specific cross validation by WRMSSE metric\n- Takes an unfitted model object, whole training \"data\" to perform cross validation on, feature columns to use for training and categorical columns (to use \"LabelEncoding on\")\n- Computes dates that split dataset in TimeSeriesSplit manner but leave 28 validation days and every step reduces training set by 28 days\n- Performs fit-predict on model object\n- Calculating WRMSSE on the predictions and storing them into the list\n- Returning the resulted list with scores","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def validate_model(\n        model: BaseEstimator,\n        data: pd.DataFrame,\n        feature_col_names: Iterable[str],\n        cat_feature_col_names: Iterable[str]) -> np.array:\n    data = data.reset_index(drop=True)\n    \n    # Splitting dataset\n    chunks = []\n    for i in range(int((data['date'].max() - data['date'].min()).days / 28)):\n        test_date_end = data['date'].max() - timedelta(days=i*28)\n        train_date_end = data['date'].max() - timedelta(days=(i+1)*28)\n        train_date_start = data['date'].min()\n\n        chunks.append((train_date_start, train_date_end, test_date_end))\n\n    for feature in cat_feature_col_names:\n        encoder = preprocessing.LabelEncoder()\n        data[feature] = encoder.fit_transform(data[feature])\n\n    estimators = [model]\n\n    scores = []\n    for train_date_start, train_date_end, test_date_end in chunks:\n        gc.collect()\n        \n        x_train = data[data['date'] <= train_date_end].drop('value', axis=1)\n        y_train = data[data['date'] <= train_date_end]['value']\n\n        x_val = data[(data['date'] > train_date_end) & \n                     (data['date'] <= test_date_end)].drop('value', axis=1)\n        y_val = data[(data['date'] > train_date_end) & \n                     (data['date'] <= test_date_end)]['value']\n        \n        print('train_date_start = ' + str(train_date_start))\n        print('train_date_end = ' + str(train_date_end))\n        print('test_date_end = ' + str(test_date_end))\n        \n        pipe = make_pipeline(*estimators)\n        \n        pipe.fit(x_train[feature_col_names], y_train)\n        y_pred = pipe.predict(x_val[feature_col_names])\n\n        wmrsse_score = wrmsse(pd.concat([x_train, x_val]), \n                              pd.concat([y_train, y_val]), \n                              pd.concat([y_train, pd.Series(y_pred, name='value')]), \n                              feature_col_names, \n                              forecast_horizon=28)\n        \n        scores.append(wmrsse_score)\n    \n    return scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = joblib.load('df_for_training.joblib')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['f_sale_usd'] = df['value'] * df['f_sell_price']\n\nfeature_cols = [i for i in df.columns if i.startswith('f_')]\nfeature_cols.remove('f_sale_usd')\n\ncategorical_cols = list(df.select_dtypes(include=['int8', 'category']).columns)\nnum_cols = list(set(feature_cols) - set(categorical_cols))\n\ndf = df.sort_values(by=categorical_cols)\n\nfor feature in categorical_cols:\n    encoder = preprocessing.LabelEncoder()\n    df[feature] = encoder.fit_transform(df[feature])\n\ndf['value'] = df['value'].astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_to_train = df[df['date'] <= '2016-04-24']\ndf_to_train = df_to_train.dropna(subset=num_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"regr_trans = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"scores = validate_model(regr_trans, \n                        df_to_train, \n                        feature_cols, \n                        categorical_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"scores","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}