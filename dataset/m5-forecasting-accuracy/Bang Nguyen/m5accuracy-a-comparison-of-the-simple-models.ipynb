{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook is created by Bang Nguyen for beginner of the M5 competition to easily understand and start making your first notebook to solve the problem.\n\nI started to explorer the M5 dataset here\n\nhttps://www.kaggle.com/nlebang/m5-forecasting-data-explanation\n\nStarting from the average 30 days simplest solution. We got the base-line score of 1.07118\n\nIn this notebooks, I compares the results of other simple solutions together.\n\nI do collect some codes from other notebooks.\n\nGive me an UPVOTE, if it is useful!\n\nThank you!","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this below notebook, we have the simplest solution (1.07118) by averaging 30 latest days of sales and use that for all predicted 28 days.\n\nhttps://www.kaggle.com/robikscube/m5-forecasting-starter-data-exploration \n\nI digged all the notebooks of M5 compitetion and select other solutions\n\n# 1. THE SIMPLEST LSTM MODEL\n\n1. https://www.kaggle.com/graymant/baseline-lstm-example -> This notebook having 2.07228 score employed the simple long-short-term-memory (LSTM) model.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import gc\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport os\n\nfrom tqdm import trange, tqdm_notebook\n\nfrom keras import backend as K\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Dropout\nfrom keras.layers import Reshape\nfrom keras.layers import LSTM\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM \nfrom keras.layers import Conv1D\nfrom keras.utils import to_categorical\nfrom keras.layers import MaxPooling1D\nfrom keras.layers import  GlobalAveragePooling1D\nfrom keras.utils import to_categorical\n\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_path = \"../input/m5-forecasting-accuracy\"\n\ndef get_salesval_coltypes():\n    keys = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'] + \\\n        [f\"d_{i}\" for i in range(1, 1914)]\n    values = ['object', 'category', 'category', 'category', 'category', 'category'] +\\\n        [\"uint16\" for i in range(1, 1914)]\n    return dict(zip(keys, values))\n\nsubmission = pd.read_csv(os.path.join(input_path, 'sample_submission.csv'))\nsales_train_val = pd.read_csv(os.path.join(input_path, 'sales_train_validation.csv'), \n                              dtype=get_salesval_coltypes())\n\n#calendar = pd.read_csv(os.path.join(input_path, 'calendar.csv'))\n#sell_prices = pd.read_csv(os.path.join(input_path, 'sell_prices.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take the timeseries data is in columns d_1 to d_1913 in \"sales_train_val\"\ndata = sales_train_val.iloc[:, 6:]\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalize the data\n#Or we can perform this step later using MinMaxScaler() or StandardScaler()\n#data = (data-data.min())/(data.max()-data.min())\n#data.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For later - test train split, for now just get shapes right\n#Now lets reshape the data into the 3D inputs required by the LSTM. \n#As a starting point we'll use input sequences of 100 timesteps to predict 28 steps ahead.\n# For LSTM, X needs to be a stack of shape (samples, timesteps, features)\n# So aiming at a shape of  = (~order of 30490 * timesteps, 28, 1)\nbase = []\npredictions = []\n\ntimesteps = 100\nprediction_steps = 28\n\n# Well just iterate through slicing timesteps until we get somewhat near the end. With a\n# proper train test split, we could be more precise\nfor i in range(1, 12):\n    #Take out the data in the period that we need\n    samples = data.iloc[:, i*timesteps:i*timesteps+timesteps]\n    preds = data.iloc[:, i*timesteps+timesteps:i*timesteps+timesteps+prediction_steps]\n    #Add to a new data set\n    base.extend(samples.to_numpy())\n    predictions.extend(preds.to_numpy())\n    print(f\"Samples {samples.shape}, preds {preds.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalize or Standarlize the data\ninput_scaler = MinMaxScaler()\noutput_scaler = StandardScaler()\n\n# Scale and reshape our input\nX_train = np.array(base)\ninput_scaler.fit(X_train)\nX_train = input_scaler.transform(X_train)\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n\n# Scale our prediction labels\nY_train_orig = np.array(predictions)\noutput_scaler.fit(Y_train_orig)\nY_train = output_scaler.transform(Y_train_orig)\nprint(X_train.shape)\nprint(Y_train.shape)\n\n# Note this could be horrible on memory. Later, need to look at generating this in batches\ndel predictions\ndel base\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets create the required LSTM model based on the input shapes\ndef root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n\nsteps = X_train.shape[1]\nn_features = X_train.shape[2]\nn_steps_out = Y_train.shape[1]\n\nmodel = tf.keras.Sequential()\nmodel.add(CuDNNLSTM(100, return_sequences=True, input_shape=(steps, n_features)))\nmodel.add(CuDNNLSTM(50))\n#model.add(LSTM(100, activation='relu'))\nmodel.add(tf.keras.layers.Dense(n_steps_out))\n# this loss needs changing to competition loss.\nmodel.compile(optimizer='adam', loss=root_mean_squared_error) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train the model\n#%%time\n\n# 0.6345 200 56\n# 0.5633 200 56 4m 14s\n\nmodel.fit(X_train, Y_train, epochs=100, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we have a trained model, we need to take the last set of timesteps \n#from the input data and get our final predictions.\n# Take a slice of n{timesteps} from the input data\nx_pred = data.iloc[:,-timesteps:].to_numpy()\n\n# Reshape to fit the format for input scalar\nx_pred = x_pred.reshape((len(sales_train_val), x_pred.shape[1]))\n# Normalize the input\nx_pred = input_scaler.transform(x_pred)\n# Reshape to fit the format for LSTM model\nx_pred = x_pred.reshape((len(sales_train_val), x_pred.shape[1], 1))\n\n# Get our predictions\nraw_pred = model.predict(x_pred)\n\n# Inverse to transform to get the predictions at the right scale\nall_pred = output_scaler.inverse_transform(raw_pred)\n# Round the predictions back to integers\nall_pred = np.round(np.abs(all_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To finish, we just need to stack our predictions into the format required for the submission file.\n\nAs we only predicted one set of 28 days, lets just stack them twice into the results file. This wouldn't be satisfactory for a final attempt on the private leaderboard, but for now while developing a model, it will do.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stack our predictions into a dataframe\nvalidation = pd.concat([pd.DataFrame(all_pred[:,0:prediction_steps]), pd.DataFrame(all_pred[:,-prediction_steps:])])\nvalidation = validation.astype(int)\n\n# Reset index to match the submission dataframe\nvalidation.reset_index(inplace=True, drop=True)\n\n# Add the id column from the submission dataframe to our results\nvalidation['id'] = submission.id\nvalidation = validation.reindex(\n        columns=['id'] + [c for c in validation.columns if c != 'id'], copy=False)\n\n# Add the correct colummn names for the submission file format\nvalidation.columns = ['id'] + [f\"F{i}\" for i in range(1, 29)]\n\nvalidation.to_csv('submission_simplestLSTM.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}