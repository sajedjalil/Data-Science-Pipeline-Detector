{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\t1. Calculate weight for the level 12 series\n\t2. Use the naive logic to make forecasts for each of the level 12 series\n\t3. Infer forecast, ground truth values, and weights for all the higher level series by aggregating\n\t4. Calculalte RMSSE for all series using the equation\n\t5. Multiply weight by respective RMSSE and add all these products"},{"metadata":{},"cell_type":"markdown","source":"## 0. Import libraries and read in data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"price_df = pd.read_csv(\"../input/m5-forecasting-accuracy/sell_prices.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"price_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cal_df = pd.read_csv(\"../input/m5-forecasting-accuracy/calendar.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cal_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cal_df[\"d\"]=cal_df[\"d\"].apply(lambda x: int(x.split(\"_\")[1]))\nprice_df[\"id\"] = price_df[\"item_id\"] + \"_\" + price_df[\"store_id\"] + \"_validation\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Calculate weight for the level 12 series"},{"metadata":{"trusted":true},"cell_type":"code","source":"for day in tqdm(range(1858, 1886)):\n    wk_id = list(cal_df[cal_df[\"d\"]==day][\"wm_yr_wk\"])[0]\n    wk_price_df = price_df[price_df[\"wm_yr_wk\"]==wk_id]\n    df = df.merge(wk_price_df[[\"sell_price\", \"id\"]], on=[\"id\"], how='inner')\n    df[\"unit_sales_\" + str(day)] = df[\"sell_price\"] * df[\"d_\" + str(day)]\n    df.drop(columns=[\"sell_price\"], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"dollar_sales\"] = df[[c for c in df.columns if c.find(\"unit_sales\")==0]].sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=[c for c in df.columns if c.find(\"unit_sales\")==0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"weight\"] = df[\"dollar_sales\"] / df[\"dollar_sales\"].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=[\"dollar_sales\"], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"weight\"] /= 12","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Infer round truth values, and weights for all the higher level series by aggregating"},{"metadata":{"trusted":true},"cell_type":"code","source":"agg_df = pd.DataFrame(df[[c for c in df.columns if c.find(\"d_\") == 0]].sum()).transpose()\nid_cols = [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\nfor col in id_cols:\n    agg_df[col] = 'all'\nagg_df[\"level\"] = 1\nagg_df[\"weight\"] = 1/12\ncolumn_order = agg_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"level_groupings = {2: [\"state_id\"], 3: [\"store_id\"], 4: [\"cat_id\"], 5: [\"dept_id\"], \n              6: [\"state_id\", \"cat_id\"], 7: [\"state_id\", \"dept_id\"], 8: [\"store_id\", \"cat_id\"], 9: [\"store_id\", \"dept_id\"],\n              10: [\"item_id\"], 11: [\"item_id\", \"state_id\"]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for level in tqdm(level_groupings):\n    temp_df = df.groupby(by=level_groupings[level]).sum().reset_index()\n    temp_df[\"level\"] = level\n    for c in column_order:\n        if c not in temp_df.columns:\n            temp_df[c] = 'all'\n    agg_df = agg_df.append(temp_df[column_order])\n\ndel temp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.shape[0], agg_df.shape[0], df.shape[0] + agg_df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agg_df[\"weight\"].sum() + df[\"weight\"].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agg_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Top down forecasts from different levels"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"last28_mean\"] = df[[c for c in df.columns if c.find(\"d_\")==0 and\\\n        int(c.split(\"_\")[1]) in range(1858, 1886)] +\\\n       [\"id\"]].set_index(\"id\").transpose().mean().reset_index()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for l in tqdm(range(1, 10)):\n    # 1. make forecast for this level\n    this_level_df = agg_df[agg_df[\"level\"]==l].copy()\n    for d in range(1, 29):\n        this_level_df[\"F_\" + str(l) + \"_\" + str(1885 + d)] = this_level_df[\"d_\" + str(1885 + d - 28)]\n    \n    # from this level, distribute forecast to all level 12 series\n    # find columns with non 'all' entries\n    important_column_ids = list(this_level_df[id_cols].columns[this_level_df[id_cols].nunique()!=1])\n    this_level_df.reset_index(drop=True, inplace=True)\n    for i, row in this_level_df.iterrows():\n        if len(important_column_ids) == 0:\n            level_mean_with_cond = this_level_df[[c for c in df.columns if c.find(\"d_\")==0 and\\\n                                   int(c.split(\"_\")[1]) in range(1858, 1886)]].transpose().mean()[0]\n            proportion = df[\"last28_mean\"] / level_mean_with_cond \n            for d in range(1, 29):\n                df[\"F_\" + str(l) + \"_\" + str(1885 + d)] = list(this_level_df[\"F_\" + str(l) + \"_\" + str(1885 + d)])[0] * proportion\n        else:\n            cond = True\n            for col in important_column_ids:\n                cond = cond & (df[col] == row[col])\n                \n            level_mean_with_cond = this_level_df[[c for c in df.columns if c.find(\"d_\")==0 and\\\n                                   int(c.split(\"_\")[1]) in range(1858, 1886)]].transpose().mean()[i]\n            proportion = df[\"last28_mean\"] / level_mean_with_cond \n            for d in range(1, 29):\n                df.loc[cond, \"F_\" + str(l) + \"_\" + str(1885 + d)] = row[\"F_\" + str(l) + \"_\" + str(1885 + d)] * proportion\n    \n# remake agg_df\nnew_agg_df = pd.DataFrame(df[[c for c in df.columns if c.find(\"d_\") == 0 or c.find(\"F_\") == 0]].sum()).transpose()\nid_cols = [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\nfor col in id_cols:\n    new_agg_df[col] = 'all'\nnew_agg_df[\"level\"] = 1\nnew_agg_df[\"weight\"] = 1/12\ncolumn_order = new_agg_df.columns\n\nfor level in level_groupings:\n    temp_df = df.groupby(by=level_groupings[level]).sum().reset_index()\n    temp_df[\"level\"] = level\n    for c in column_order:\n        if c not in temp_df.columns:\n            temp_df[c] = 'all'\n    new_agg_df = new_agg_df.append(temp_df[column_order])\ndel temp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agg_df = new_agg_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Calculalte RMSSE for all series using the equation"},{"metadata":{"trusted":true},"cell_type":"code","source":"h = 28\nn = 1885\ndef rmsse(ground_truth, forecast, train_series, axis=1):\n    # assuming input are numpy array or matrices\n    assert axis == 0 or axis == 1\n    assert type(ground_truth) == np.ndarray and type(forecast) == np.ndarray and type(train_series) == np.ndarray\n    \n    if axis == 1:\n        # using axis == 1 we must guarantee these are matrices and not arrays\n        assert ground_truth.shape[1] > 1 and forecast.shape[1] > 1 and train_series.shape[1] > 1\n    \n    numerator = ((ground_truth - forecast)**2).sum(axis=axis)\n    if axis == 1:\n        denominator = 1/(n-1) * ((train_series[:, 1:] - train_series[:, :-1]) ** 2).sum(axis=axis)\n    else:\n        denominator = 1/(n-1) * ((train_series[1:] - train_series[:-1]) ** 2).sum(axis=axis)\n    return (1/h * numerator/denominator) ** 0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_series_cols = [c for c in df.columns if c.find(\"d_\") == 0][:-28]\nground_truth_cols = [c for c in df.columns if c.find(\"d_\") == 0][-28:]\n\nforecast_cols_dict = {}\nfor i in range(1, 10):\n    forecast_cols_dict[i] = [c for c in df.columns if c.find(\"F_\"+str(i)+\"_\") == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1, 10):\n    df[\"rmsse_\" + str(i)] = rmsse(np.array(df[ground_truth_cols]), \n        np.array(df[forecast_cols_dict[i]]), np.array(df[train_series_cols]))\n    agg_df[\"rmsse_\" + str(i)] = rmsse(np.array(agg_df[ground_truth_cols]), \n        np.array(agg_df[forecast_cols_dict[i]]), np.array(agg_df[train_series_cols]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1, 10):\n    df[\"wrmsse_\" + str(i)] = df[\"weight\"] * df[\"rmsse_\" + str(i)]\n    agg_df[\"wrmsse_\" + str(i)] = agg_df[\"weight\"] * agg_df[\"rmsse_\" + str(i)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1, 10):\n    print(\"Aggregation by level\", str(i) + \":\")\n    print(df[\"wrmsse_\" + str(i)].sum() + agg_df[\"wrmsse_\" + str(i)].sum())\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Make submission file"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"last28_mean\"] = df[[c for c in df.columns if c.find(\"d_\")==0 and\\\n        int(c.split(\"_\")[1]) in range(1886, 1914)] +\\\n       [\"id\"]].set_index(\"id\").transpose().mean().reset_index()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"level_1_mean = agg_df[[c for c in df.columns if c.find(\"d_\")==0 and\\\n        int(c.split(\"_\")[1]) in range(1886, 1914)]].transpose().mean().reset_index()[0][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_df = df[[\"id\"]]\nfor i in range(1, 29):\n    proportion = df[\"last28_mean\"] / level_1_mean\n    submit_df[\"F\" + str(i)] = agg_df[agg_df[\"level\"]==1][\"d_\" + str(1885 + i)][0] * proportion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_df2 = submit_df.copy()\nsubmit_df2[\"id\"] = submit_df2[\"id\"].apply(lambda x: x.replace('validation',\n                                                              'evaluation'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_df = submit_df.append(submit_df2).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}