{"cells":[{"metadata":{},"cell_type":"markdown","source":"# M5 - calculating 'out_of_stock' feature 640x faster\n\nThe is notebook builds 'out_of_stock' feature 80x faster thanks to [math guys here](https://math.stackexchange.com/questions/364038/expected-number-of-coin-tosses-to-get-five-consecutive-heads/1030482#1030482)\n\nIn the [previous notebook](https://www.kaggle.com/sibmike/m5-out-of-stock-feature/), I layed out how **we can distinguish between zero demand and zero supply**. Unfortunately, it would take roughly 90 hours to calulate the feature with the help of simulations. This notebook calculates the feature in less then ~~an hour~~ 8 minutes because instead of using simulations we can use a formula to calculate **expected number of days to randomly get a gap of length N** (in other words N consecutive days with zero sales). \n\nOur gap problem is similar to **\"Expected Number of Coin Tosses to Get Five Consecutive Heads\"** problem which is discussed and solved [here](https://math.stackexchange.com/a/1834771). The remaining logic is the same: \n1. assume probability of sale on a day, p0 = sale_days/total_days\n2. for gap of length N find expected number of days to get it randomly E0 = E(N,p0)\n3. mark it as supply_gap if E0/365 > 100 (i.e. it takes **more than 100 years to observe the gap ranomly**)\n4. cut out supply_gaps from probability calculations: p1 = (sale_days-supply_gap_days)/(total_days-supply_gap_days)\n5. run recursively till p1 stops decreasing\n\nOnce in 100 years might be a little conservative threshold, so you can easily change this parameter to your taste. Formula for p1 can also be adjusted. Instead of constructing a boolean feature I have mapped gap days to E/total_days value and saved p value in a separate column. Have fun!\n\nPS: As you will see in the end even with the 100 years threshold, **non random gaps account for over 23% of the dataset**. (That is if I have not messed up somewhere;)\n\n**_PPS: Special thanks to @nadare for outstanding 8x booster that cut time from 1 hr to under 8 mins!_**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os, sys, gc, time, warnings, pickle, psutil, random\nfrom tqdm import tqdm\n\nimport seaborn as sns # data visualization library  \nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load data\ngrid_df = pd.read_pickle('/kaggle/input/m5-simple-fe/grid_part_1.pkl')\ngrid_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gap_finder(ts):\n    \n    # this function finds gaps and calculates their length:\n    # note ts: 0 = day with sales, 1 = days with 0 sales\n    \n    for i, gap in enumerate(ts):\n        if gap == 0: \n            continue\n        elif i!=0: \n            ts[i] += ts[i-1]\n            if ts[i-1]!=0: ts[i-1] = -1\n    return ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note: in 'gap' column: 1 is a day without sales:\ngrid_df['gaps'] = (~(grid_df['sales'] > 0)).astype(int)\ntotal_days = 1941\n\nprods = list(grid_df.id.unique())\ns_list = [] #list to hold gaps in days\ne_list = [] #list to hold expected values of gaps\np_list = [] #list to hold avg probability of no sales\n\n# original 1 hour version\n#for prod_id in tqdm(prods):\n#    \n#    # extract gap_series for a prod_id\n#    m = grid_df.id==prod_id\n#    sales_gaps = grid_df.loc[m,'gaps']\n#    \n#    # calculate initial probability\n#    zero_days = sum(sales_gaps)\n#    p = zero_days/total_days\n#    \n#    # find and mark gaps\n#    sales_gaps[:] = gap_finder(sales_gaps.values.copy())\n#    sales_gaps = sales_gaps.astype(int).replace(-1,np.nan).fillna(method='backfill').fillna(method='ffill')\n#    s_list += [sales_gaps]\n\n# magic x8 speed booster thanks to @nadare\nfor prod_id, df in tqdm(grid_df.groupby(\"id\")):   \n    # extract gap_series for a prod_id\n    sales_gaps = df.loc[:,'gaps']\n\n    # calculate initial probability\n    zero_days = sum(sales_gaps)\n    p = zero_days/total_days\n\n    # find and mark gaps\n    accum_add_prod = np.frompyfunc(lambda x, y: int((x+y)*y), 2, 1)\n    sales_gaps[:] = accum_add_prod.accumulate(df[\"gaps\"], dtype=np.object).astype(int)\n    sales_gaps[sales_gaps < sales_gaps.shift(-1)] = np.NaN\n    sales_gaps = sales_gaps.fillna(method=\"bfill\").fillna(method='ffill')\n    s_list += [sales_gaps]\n    \n    # calculate E/total_days for all possible gap lengths:\n    gap_length = sales_gaps.unique()\n    \n    d = {length: ((1-p**length)/(p**length*(1-p)))/365 for length in gap_length}\n    sales_E_years = sales_gaps.map(d)\n    \n    # cut out supply_gap days and run recursively\n    p1 = 0\n    while p1 < p:\n        \n        if p1!=0:\n            p=p1\n        \n        # once in 100 years event; change to your taste here\n        gap_days = sum(sales_E_years>100)\n            \n        p1 = (zero_days-gap_days+0.0001)/(total_days-gap_days)\n        \n        d = {length: ((1-p1**length)/(p1**length*(1-p1)))/365 for length in gap_length}\n        sales_E_years = sales_gaps.map(d)\n        \n    # add results to list it turns out masked replacemnt is a very expensive operation in pandas, so better do it in one go\n    e_list += [sales_E_years]\n    p_list += [pd.Series(p,index=sales_gaps.index)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add it to grid_df in one go fast!:\ngrid_df['gap_days'] = pd.concat(s_list)\ngrid_df['gap_e'] = pd.concat(e_list)\ngrid_df['sale_prob'] = pd.concat(p_list)\n##45664\n# Dump to pickle:\ngrid_df.to_pickle('grid_part_1_gaps.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# becuase we have some really extreme values lets take a log:\ngrid_df['gap_e_log10'] = np.log10((grid_df['gap_e'].values+1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# e over 100 years does not make much sense\nm = grid_df['gap_e_log10']>2\ngrid_df.loc[m,'gap_e_log10']=2\n\n# take a subsample to vizualise:\nnp.random.seed(19)\ndepts = list(grid_df.dept_id.unique())\n\nprod_list = []\nfor d in depts:\n    prod_by_dept=grid_df['item_id'][grid_df.dept_id == d].unique()\n    prod_list += list(np.random.choice(prod_by_dept,5))\n    \nm = grid_df.item_id.isin(prod_list)\nviz_df = grid_df[m]\nviz_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### mini EDA\nAs you can see there is a variety of non-random gaps. Large patches when a product disappears from all stores. Gaps when several products disappear. And lots of Store specific supply failures."},{"metadata":{"trusted":true},"cell_type":"code","source":"v_df = viz_df.pivot(index='d', columns='id', values='gap_e_log10')\nv_df = v_df.reindex(sorted(v_df.columns), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"v_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(15, 20))\ntemp = sns.heatmap(v_df, cmap='Reds')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finally lets calculate the proportion of non random gaps in original dataset.\n# as mentioned by @Amphi2 we should have dropped last 28 days, so lets substract them:\n\n(sum(grid_df['gap_e_log10'] >= 2) - 3049*10*28)/grid_df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" [Discussion here.](https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/138085#790628)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}