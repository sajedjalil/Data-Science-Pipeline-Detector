{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, we will use Walmart's M5 competition database and forecast sales for each product category using the MAPA - Multiple Aggregation Forecast Algorithm technique. Temporal aggregation can help to identify the characteristics of the series as they are improved\nthrough different frequencies (daily, weekly, monthly, bimonthly, for example). The objective is to show that the use of various levels of temporal aggregation can lead to substantial improvements in terms of performance forecasting."},{"metadata":{},"cell_type":"markdown","source":"## MAPA - Multiple Aggregation Prediction Algorithm\n\nAs the resources of the time series change with the frequency of the data (or the level of aggregation), different methods will be identified as ideal. This will produce different predictions, which will lead to different decisions. In essence, we have to deal with the \"true\" uncertainty of the model, which is the adequacy or incorrect specification of the model identified as ideal at a specific level of data aggregation. [1]\nThe idea of MAPA technique is to reduce the risk of overffiting and select an incorrect model. According to [1], the MAPA technique provides better forecasting performance when compared to traditional approaches.\n\n<img src=\"https://www.researchgate.net/profile/Nikolaos_Kourentzes/publication/265069129/figure/fig1/AS:295915979067408@1447563275667/The-standard-versus-the-MAPA-forecasting-approach.png\" alt=\"some text\">\n\nThere are three steps:\n<ul>\n  <li><b>Aggregation</b>.  the MAPA uses multiple\ninstances of the same data, which correspond to the different frequencies or aggregation levels. Here we use the levels daily, weekly, monthly and bimonthly of aggregation.</li>\n  <li><b>Forecasting</b>. Each one of the series calculated in step 1 should be forecasted separately (using SARIMAX). Seasonality and various high frequency components are expected to be modelled better at lower aggregation levels (such as monthly data), while the long-term trend will be better captured as the frequency is decreased (bimonthly data)</li>\n  <li><b>Combination</b>. The final step in the proposed MAPA approach concerns the appropriate combination of different forecasts derived from alternative frequencies. Before we can combine the predictions produced at the various frequencies, we need to turn them back to the original frequency. Then, we make the combination using functions such as mean, median, maximum or minimum of time series aggregations.</li>\n</ul>\n [1] PETROPOULOS, Fotios; KOURENTZES, Nikolaos. Improving forecasting via multiple temporal aggregation. Foresight: The International Journal of Applied Forecasting, v. 34, p. 12-17, 2014."},{"metadata":{},"cell_type":"markdown","source":"# Contents\n\n* [<font size=4>The dataset</font>](#1)\n\n\n* [<font size=4>Pre-processing and Exploratory Data Analysis</font>](#2)\n    * [Load the data](#2.1)\n    * [Resampling of time series](#2.2)\n    * [Decomposition](#2.3)\n    * [Autocorrelation Test](#2.4)\n    * [Stationarity test](#2.5)\n\n    \n* [<font size=4>SARIMAX Forecasting</font>](#3)\n    * [Foods Category forecasting](#3.1)\n    * [Hobbies Category forecasting](#3.2)\n    * [Household Category forecasting](#3.3)\n\n* [<font size=4>MAPA farecasting (Weekly, monthly and Bimonthly frequencies)</font>](#4)\n    * [Foods Category forecasting](#4.1)\n    * [Hobbies Category forecasting](#4.2)\n    * [Household Category forecasting](#4.3)\n\n* [<font size=4>Daily forecasting with exogenous variables</font>](#5)\n* [<font size=4>MAPA farecasting (Daily, Weekly, and monthly frequencies)</font>](#6)"},{"metadata":{},"cell_type":"markdown","source":"# The dataset <a id=\"1\"></a>\n\nWe will use two dataset .csv files:\n\n* <code>calendar.csv</code> - Contains the dates on which products are sold. The dates are in a <code>yyyy/dd/mm</code> format.\n\n* <code>sales_train_validation.csv</code> - Contains the historical daily unit sales data per product and store <code>[d_1 - d_1913]</code>.\n"},{"metadata":{},"cell_type":"markdown","source":"### Import libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport plotly.graph_objs as go #visualization library\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf #autocorrelation test\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.stattools import adfuller #stationarity test\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX \nfrom datetime import datetime, timedelta\nimport seaborn as sns\nfrom sklearn import metrics\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-processing and Exploratory Data Analysis <a id=\"2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"### Load the data <a id=\"2.1\"></a>\n> \nFirst, we will group the database into the three product categories (Foods, Hobbies and Household). We will use the base 'calendar.csv' to export the dates corresponding to the days of the column of the base 'sales_train_validation.csv'."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv')\n\ndata_dept = data.groupby(['dept_id']).sum() #group sales by department\ndata_item = data.groupby(['item_id']).sum() #group sales by item_id\ndata_cat = data.groupby(['cat_id']).sum().T #group sales by category\ndata_cat['day'] = data_cat.index\n\ndata_store = data.groupby(['store_id']).sum()\ndata_state_id = data.groupby(['state_id']).sum()\n\ncalendar = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')\ndata_calendar = calendar.iloc[:, [0, 2,3,4,5,6,7]]\n\n#Merge data_calendar columns related to commemorative data, days of the week, month and year.\ndata_cat = pd.merge(data_calendar, data_cat, how = 'inner', left_on='d', right_on='day')\ndata_cat_final = data_cat.iloc[:,[7,8,9]]\ndata_cat_final.index = data_cat['date']\ndata_cat_final.index = pd.to_datetime(data_cat_final.index , format = '%Y-%m-%d')\ndata_cat_final.parse_dates=data_cat_final.index\ndata_cat_final.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below is an interactive graph of sales by category from January 2011 to April 2016."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(\n    data=[go.Scatter(y=data_cat_final['2011-01':'2016-04'].FOODS, x=data_cat_final.index, name= 'Foods'), \n          go.Scatter(y=data_cat_final['2011-01':'2016-04'].HOBBIES, x=data_cat_final.index, name = 'Hobbies'),\n          go.Scatter(y=data_cat_final['2011-01':'2016-04'].HOUSEHOLD, x=data_cat_final.index, name = 'HouseHold')],\n    layout=go.Layout(\n        xaxis=dict(showgrid=False),\n        yaxis=dict(showgrid=False),\n    )\n)\nfig.update_layout(title_text=\"Sales by Category\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there is a moderate correlation between the three time series."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(data_cat_final[['FOODS','HOBBIES','HOUSEHOLD']].corr(), annot = True,  cbar=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Resampling of time series <a id=\"2.2\"></a>\nAs we can see, the original database contains daily sales data. We will do a resampling here for weekly, monthly and bimonthly frequencies. I going to ignore the first and last lines, otherwise we would be adding lines in an incomplete period"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cat_final_monthly = data_cat_final.iloc[:,[0,1,2]].resample('M').sum()[2:-1] #mensal resampling\ndata_cat_final_weekly = data_cat_final.iloc[:,[0,1,2]].resample('W').sum()[8:-1] #weekly resampling\ndata_cat_final_bimonthly = data_cat_final.iloc[:,[0,1,2]].resample('2M').sum()[1:-1] #bimonthy resamply","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot with monthly frequencies."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(\n    data=[go.Scatter(y=data_cat_final_monthly.FOODS, x=data_cat_final_monthly.FOODS.index, name= 'Foods'), \n          go.Scatter(y=data_cat_final_monthly.HOBBIES, x=data_cat_final_monthly.HOBBIES.index, name = 'Hobbies'),\n          go.Scatter(y=data_cat_final_monthly.HOUSEHOLD, x=data_cat_final_monthly.HOUSEHOLD.index, name = 'HouseHold')],\n    layout=go.Layout(\n        xaxis=dict(showgrid=False),\n        yaxis=dict(showgrid=False),\n    )\n)\nfig.update_layout(title_text=\"Sales by Category - Monthly\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decomposition <a id=\"2.3\"></a>\n\nThe decomposition of time series is a statistical task that deconstructs a time series into several components, each representing one of the underlying categories of patterns. Analyzing the decomposition of the time series of the category in monthly frequency, we can verify a semiannual seasonality with an approximately linear trend"},{"metadata":{},"cell_type":"markdown","source":"#### Foods Category - Decomposition"},{"metadata":{"trusted":true},"cell_type":"code","source":"decomposed = sm.tsa.seasonal_decompose(np.array(data_cat_final_monthly.FOODS),period=6) # The frequency is semestral\nfigure = decomposed.plot()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Hobbies Category - Decomposition"},{"metadata":{"trusted":true},"cell_type":"code","source":"decomposed = sm.tsa.seasonal_decompose(np.array(data_cat_final_monthly.HOBBIES),period=6) # The frequency is semestral\nfigure = decomposed.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Household Category - Decomposition","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decomposed = sm.tsa.seasonal_decompose(np.array(data_cat_final_monthly.HOUSEHOLD),period=6) # The frequency is semestral\nfigure = decomposed.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Autocorrelation Test <a id=\"2.4\"></a>\n\nA plot of the autocorrelation of a time series by lag is called the AutoCorrelation Function, ACF. PACF - partial autocorrelation is the resulting correlation after removing the effect of any correlation due to the terms in shorter intervals. Confidence intervals are drawn as a cone. This is defined as a 95% confidence interval, suggesting that the correlation values outside this code are likely to be a correlation and not a statistical fluke.\n\n\nAll results below have autocorrelation of 4 to 6 lags and autocorrelation of 2 lags that are statistically significant"},{"metadata":{},"cell_type":"markdown","source":"#### ACF and PACF - Foods Category"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.show()\nplot_acf(data_cat_final_monthly.FOODS,lags=12,title=\"ACF Foods\")\nplt.show()\nplot_pacf(data_cat_final_monthly.FOODS,lags=6,title=\"PACF Foods\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ACF and PACF - Hobbies Category"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_acf(data_cat_final_monthly.HOBBIES,lags=12,title=\"ACF HObbies\")\nplt.show()\nplot_pacf(data_cat_final_monthly.HOBBIES,lags=12,title=\"PACF Hobbies\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ACF and PACF - Household Category"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_acf(data_cat_final_monthly.HOUSEHOLD,lags=12,title=\"ACF Household\")\nplt.show()\nplot_pacf(data_cat_final_monthly.HOUSEHOLD,lags=12,title=\"PACF Household\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stationarity test <a id=\"2.5\"></a>\nTo check stationarity we use the Augmented Dickey-Fuller test, a type of statistical test called a unit root test.\n\nIf it is not possible to reject the null hypothesis ($p-value > 0.05$), this suggests that the time series has a unit root, which means that it is not stationary. It has some time-dependent structure.\n\nIf the null hypothesis is rejected ($p-value < 0.05$); it suggests the time series does not have a unit root, meaning it is stationary. It does not have time-dependent structure.\n\nAccording to the results below, all three series are non-stationary."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Augmented Dickey-Fuller test\nadf1 = adfuller(data_cat_final.FOODS, autolag='AIC')\nprint(\"p-value of Foods serie is: {}\".format(float(adf1[1])))\n#The test statistic is negative, we don't reject the null hypothesis (it looks non-stationary).\nadf2 = adfuller(data_cat_final.HOBBIES, autolag='AIC')\nprint(\"p-value of Hobbies serie is: {}\".format(float(adf2[1]))) #It isn't a random walk if p-value is less than 5%\n#The test statistic is negative, we don't reject the null hypothesis (it looks non-stationary).\nadf3 = adfuller(data_cat_final.HOUSEHOLD, autolag='AIC')\nprint(\"p-value of Household serie is: {}\".format(float(adf3[1]))) #It isn't a random walk if p-value is less than 5%\n#The test statistic is negative, we don't reject the null hypothesis (it looks non-stationary).","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SARIMAX Forecasting <a id=\"3\"></a>"},{"metadata":{},"cell_type":"markdown","source":"<b>ARIMA</b> (Autoregressive Integrated Moving Average) is a forecasting method for univariate time series data supports both an autoregressive (p) and moving average elements  (q). The integrated element  (d) refers to differencing allowing the method to support time series data with a trend.\n<img src=\"https://miro.medium.com/max/576/0*Ql_BphTqarSBmgrZ\" alt=\"some text\">\n\n<b>SARIMA</b> (Seasonal Autoregressive Integrated Moving Average)  is an extension of ARIMA that explicitly supports univariate time series data with a seasonal component.\nTrend Elements:\n<ul>\n  <li>p: Trend autoregression order.</li>\n  <li>d: Trend difference order</li>\n  <li>q: Trend moving average order.</li>\n</ul>\nSeasonal Elements:\n<ul>\n  <li>P: Seasonal autoregressive order </li>\n  <li>D: Seasonal difference order</li>\n  <li>Q: Seasonal moving average order</li>\n  <li>m: The number of time steps for a single seasonal period.</li>\n</ul>\n\n<b>SARIMAX</b> (Seasonal Auto-regressive Integrated Moving Average with exogenous variables) is an extension of SARIMA that supports the use of exogenous variables that can contribute to forecasting performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"#I created a function that should return a forecast and a summary with the main statistics.\n#actual - Time series that we will predict\n#order - [p, d, q] terms\n#seasonal order - [P,S,Q,m] sazonal terms\n#t- lag use for the test base and future prediction\ndef sarimax_predictor(actual, order, seasonal_order, t , start, title):\n    mdl = sm.tsa.statespace.SARIMAX(actual[start:-t],\n                                            order=order, seasonal_order=seasonal_order,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n    results = mdl.fit()\n    results.plot_diagnostics()\n    print(results.summary())\n    predict = results.predict(start=start, end=len(actual)+t)\n\n    fig = go.Figure(\n        data=[go.Scatter(y=actual[0:-t],x=actual[0:-t].index, name= 'Actual'),\n          go.Scatter(y=actual[-t-1::],x=actual[-t-1::].index, name= 'Test'),\n          go.Scatter(y=predict, x=predict.index, name= 'Predict')],\n        layout=go.Layout(\n        xaxis=dict(showgrid=False),\n        yaxis=dict(showgrid=False),\n        )\n    )\n    fig.update_layout(title_text= title)\n    fig.show()\n    return predict\n#defined function when we need to use exogenous variables\ndef sarimax_predictor_exog(actual, order, seasonal_order, t, title, exog):\n    mdl = sm.tsa.statespace.SARIMAX(actual[0:-t],\n                                            order=order, seasonal_order=seasonal_order, exog = exog[0:-t],\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False, time_varying_regression = False,\n                                            mle_regression = True)\n    results = mdl.fit()\n    results.plot_diagnostics()\n    print(results.summary())\n    #use only exogenous to forecasting (test set)\n    predict = results.predict(start=0, end=len(actual), exog=exog[-t-1::]) \n\n    fig = go.Figure(\n        data=[go.Scatter(y=actual[0:-t],x=actual[0:-t].index, name= 'Actual'),\n          go.Scatter(y=actual[-t-1::],x=actual[-t-1::].index, name= 'Test'),\n          go.Scatter(y=predict, x=predict.index, name= 'Predict')],\n        layout=go.Layout(\n        xaxis=dict(showgrid=False),\n        yaxis=dict(showgrid=False),\n        )\n    )\n    fig.update_layout(title_text= title)\n    fig.show()\n    return predict\n\n#evaluation test\ndef rmse(actual, predict, title):\n    from sklearn import metrics\n    rmse = np.sqrt(metrics.mean_squared_error(actual, predict))\n    print('The RMSE of ' + title + ' is:', rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nHere we will use a 6 month test base. The model should perform a 1 year forecasting (6 months beyond the test basis). Seasonality is 6 months (28 weeks and 3 months)"},{"metadata":{},"cell_type":"markdown","source":"### Foods Category forecasting <a id=\"3.1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"start = 0\npredicted_result_foods_weekly = sarimax_predictor(data_cat_final_weekly.FOODS, [1,1,0], [1,1,0,24], 7*4, start,\n                                                  'Weekly forecast - Foods')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_result_foods_monthly = sarimax_predictor(data_cat_final_monthly.FOODS, [5,1,1], [1,1,0,6], 6, start,\n                                                  'Monthly forecast- Foods')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_result_foods_bimonthly = sarimax_predictor(data_cat_final_bimonthly.FOODS, [2,1,0], [1,0,0,6], 3, start,\n                                                     'Bimonthly forecast')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RMSE in test fold\nrmse_foods_weekly= rmse(data_cat_final_weekly.FOODS[-28::], predicted_result_foods_weekly[-56-1:-28-1], 'weekly Foods - Test')\nrmse_foods_monthly= rmse(data_cat_final_monthly.FOODS[-6::], predicted_result_foods_monthly[-12-1:-6-1], 'monthy Foods - Test')\nrmse_foods_bimonthly= rmse(data_cat_final_bimonthly.FOODS[-3::], predicted_result_foods_bimonthly[-6-1:-3-1], 'bimonthy Foods - Test')\n\n#RMSE in train fold\nrmse_foods_weekly= rmse(data_cat_final_weekly.FOODS[start:-28], predicted_result_foods_weekly[0:-28*2-1], 'weekly Foods - Train')\nrmse_foods_monthly= rmse(data_cat_final_monthly.FOODS[start:-6], predicted_result_foods_monthly[0:-6*2-1], 'monthy Foods - Train')\nrmse_foods_bimonthly= rmse(data_cat_final_bimonthly.FOODS[start:-3], predicted_result_foods_bimonthly[0:-3*2-1], 'bimonthy Foods - Train')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hobbies Category forecasting <a id=\"3.2\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_result_hobbies_weekly = sarimax_predictor(data_cat_final_weekly.HOBBIES, [2,1,0], [1,1,0,24], 28, start,\n                                                  'Weekly forecast - Hobbies')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_result_hobbies_monthly = sarimax_predictor(data_cat_final_monthly.HOBBIES, [2,1,0], [2,0,0,12], 6, start,\n                                                  'Monthly forecast- Hobbies')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_result_hobbies_bimonthly = sarimax_predictor(data_cat_final_bimonthly.HOBBIES, [2,1,0], [1,0,0,3], 3, start,\n                                                     'Bimonthly forecast - Hobbies')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RMSE in test fold\nrmse_hobbies_weekly= rmse(data_cat_final_weekly.HOBBIES[-28::], predicted_result_hobbies_weekly[-56-1:-28-1], 'weekly Hobbies - Test')\nrmse_hobbies_monthly= rmse(data_cat_final_monthly.HOBBIES[-6::], predicted_result_hobbies_monthly[-12-1:-6-1], 'monthy Hobbies - Test')\nrmse_hobbies_bimonthly= rmse(data_cat_final_bimonthly.HOBBIES[-3::], predicted_result_hobbies_bimonthly[-6-1:-3-1], 'bimonthy Hobbies - Test')\n\n#RMSE in train fold\nrmse_hobbies_weekly= rmse(data_cat_final_weekly.HOBBIES[start:-28], predicted_result_hobbies_weekly[0:-28*2-1], 'weekly Hobbies - Train')\nrmse_hobbies_monthly= rmse(data_cat_final_monthly.HOBBIES[start:-6], predicted_result_hobbies_monthly[0:-6*2-1], 'monthy Hobbies - Train')\nrmse_hobbies_bimonthly= rmse(data_cat_final_bimonthly.HOBBIES[start:-3], predicted_result_hobbies_bimonthly[0:-3*2-1], 'bimonthy Hobbies - Train')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Household Category forecasting <a id=\"3.3\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_result_household_weekly = sarimax_predictor(data_cat_final_weekly.HOUSEHOLD, [7,1,0], [1,1,0,24], 28, start,\n                                                  'Weekly forecast - Household')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_result_household_monthly = sarimax_predictor(data_cat_final_monthly.HOUSEHOLD, [2,1,0], [2,0,0,6], 6, start,\n                                                  'Monthly forecast- Household')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_result_household_bimonthly = sarimax_predictor(data_cat_final_bimonthly.HOUSEHOLD, [2,0,0], [1,1,0,3], 3, start,\n                                                     'Bimonthly forecast - Household')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RMSE in test fold\nrmse_household_weekly= rmse(data_cat_final_weekly.HOUSEHOLD[-28::], predicted_result_household_weekly[-56-1:-28-1], 'weekly Household - Test')\nrmse_household_monthly= rmse(data_cat_final_monthly.HOUSEHOLD[-6::], predicted_result_household_monthly[-12-1:-6-1], 'monthy Household - Test')\nrmse_household_bimonthly= rmse(data_cat_final_bimonthly.HOUSEHOLD[-3::], predicted_result_household_bimonthly[-6-1:-3-1], 'bimonthy Household - Test')\n\n#RMSE in train fold\nrmse_household_weekly= rmse(data_cat_final_weekly.HOUSEHOLD[start:-28], predicted_result_household_weekly[0:-28*2-1], 'weekly Household - Train')\nrmse_household_monthly= rmse(data_cat_final_monthly.HOUSEHOLD[start:-6], predicted_result_household_monthly[0:-6*2-1], 'monthy Household - Train')\nrmse_household_bimonthly= rmse(data_cat_final_bimonthly.HOUSEHOLD[start:-3], predicted_result_household_bimonthly[0:-3*2-1], 'bimonthy Household - Train')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MAPA farecasting (Weekly, monthly and Bimonthly frequencies) <a id=\"4\"></a>"},{"metadata":{},"cell_type":"markdown","source":"The time series of MAPA smetimes is out of X axis of original time series. The objective of the alpha function is to find a constanst which when multiplied by the predicted time series, adjusts the x-axis minimizing the RMSE"},{"metadata":{"trusted":true},"cell_type":"code","source":"def alpha(actual, predict):\n    RMSE =[]\n    for i in np.arange(0.5,15, 0.01):\n        RMSE.append([i,np.sqrt(metrics.mean_squared_error(actual, predict*i))])\n    return np.array(RMSE)[np.argmin(np.array(RMSE)[:,1]),0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Foods Cateory forecasting <a id=\"4.1\"></a>\n\nIn the combining step, we need to disaggregate the data (from low to high frequency), which is more complicated than aggregation. Here, we will calculate the average we will apply to all periods equally. For example. If in a two-month period we have forecasted 100. The monthly breakdown will be 50 in the two months in the two-month period"},{"metadata":{"trusted":true},"cell_type":"code","source":"#step of combination\npredicted_result_foods_bimonthly = predicted_result_foods_bimonthly.resample('W').mean()\npredicted_result_foods_monthly = predicted_result_foods_monthly.resample('W').mean()\n#equally assigns the mean value of the low frequency time series\npredictions_foods = pd.DataFrame({'bimonthly': predicted_result_foods_bimonthly.groupby(predicted_result_foods_bimonthly.notnull().cumsum()).transform(lambda x : x.sum()/len(x))[0:-4],\n                         'monthly': predicted_result_foods_monthly.groupby(predicted_result_foods_monthly.notnull().cumsum()).transform(lambda x : x.sum()/len(x)),\n                         'weekly': predicted_result_foods_weekly[1::]})\nprediction_foods_mean = pd.DataFrame.mean(predictions_foods, axis = 1)\nprediction_foods_median = pd.DataFrame.median(predictions_foods, axis = 1)\nprediction_foods_min = pd.DataFrame.min(predictions_foods, axis = 1)\nprediction_foods_max = pd.DataFrame.max(predictions_foods, axis = 1)\nalpha_foods_max = alpha(data_cat_final_weekly.FOODS[start+1:-28], prediction_foods_max[start:-28*2-1])\nalpha_foods_min = alpha(data_cat_final_weekly.FOODS[start+1:-28], prediction_foods_min[start:-28*2-1])\nalpha_foods_mean = alpha(data_cat_final_weekly.FOODS[start+1:-28], prediction_foods_mean[start:-28*2-1])\nalpha_median = alpha(data_cat_final_weekly.FOODS[start+1:-28], prediction_foods_median[start:-28*2-1])\n\nfig = go.Figure(\n    data=[go.Scatter(y=data_cat_final_weekly.FOODS, x = data_cat_final_weekly.FOODS.index, name= 'Actual'), \n          go.Scatter(y=prediction_foods_min[0:-1]*alpha_foods_min, x= data_cat_final_weekly.FOODS.index, name= 'Predict Min'),\n          go.Scatter(y=prediction_foods_max[0:-1]*alpha_foods_max, x= data_cat_final_weekly.FOODS.index, name= 'Predict Max'),\n          go.Scatter(y=prediction_foods_mean[0:-1]*alpha_foods_mean, x= data_cat_final_weekly.FOODS.index, name= 'Predict Mean'),\n          go.Scatter(y=prediction_foods_median[0:-1]*alpha_median, x= data_cat_final_weekly.FOODS.index, name= 'Predict median')],\n    layout=go.Layout(\n        xaxis=dict(showgrid=False),\n        yaxis=dict(showgrid=False),\n    )\n)\nfig.update_layout(title_text=\"Foods Category - MAPA SARIMA forecast\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nThe best result was to use a maximum combination function. The result on the test base was closer, but it did not exceed the individual SARIMA model."},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse(data_cat_final_weekly.FOODS[-28::], predicted_result_foods_weekly[-56-1:-28-1], 'weekly Foods - Test')\nrmse(data_cat_final_weekly.FOODS[start+1:-28], predicted_result_foods_weekly[start+1:-28*2-1], 'weekly Foods - Train')\n#test Fold\nrmse_foods_max = rmse(data_cat_final_weekly.FOODS[-28::], prediction_foods_max[-57:-29]*alpha_foods_max, 'Foods Test - max')\nrmse_foods_min = rmse(data_cat_final_weekly.FOODS[-28::], prediction_foods_min[-57:-29]*alpha_foods_min, 'Foods Test - min')\nrmse_foods_mean = rmse(data_cat_final_weekly.FOODS[-28::], prediction_foods_mean[-57:-29]*alpha_foods_mean, 'Foods Test - mean')\nrmse_foods_median = rmse(data_cat_final_weekly.FOODS[-28::], prediction_foods_median[-57:-29]*alpha_median, 'Foods Test - median')\n#train Fold\nrmse_foods_max = rmse(data_cat_final_weekly.FOODS[start+1:-28], prediction_foods_max[start:-28*2-1]*alpha_foods_max, 'Foods Train - max')\nrmse_foods_min = rmse(data_cat_final_weekly.FOODS[start+1:-28], prediction_foods_min[start:-28*2-1]*alpha_foods_min, 'Foods Train - min')\nrmse_foods_mean = rmse(data_cat_final_weekly.FOODS[start+1:-28], prediction_foods_mean[start:-28*2-1]*alpha_foods_mean, 'Foods Train - mean')\nrmse_foods_median = rmse(data_cat_final_weekly.FOODS[start+1:-28], prediction_foods_median[start:-28*2-1]*alpha_median, 'Foods Train - median' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hobbies Category forecasting <a id=\"4.2\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_result_hobbies_bimonthly = predicted_result_hobbies_bimonthly.resample('W').mean()\npredicted_result_hobbies_monthly = predicted_result_hobbies_monthly.resample('W').mean()\npredictions_hobbies = pd.DataFrame({'bimestral': predicted_result_hobbies_bimonthly.groupby(predicted_result_hobbies_bimonthly.notnull().cumsum()).transform(lambda x : x.sum()/len(x))[0:-4],\n                         'mensal': predicted_result_hobbies_monthly.groupby(predicted_result_hobbies_monthly.notnull().cumsum()).transform(lambda x : x.sum()/len(x)),\n                         'weekly': predicted_result_hobbies_weekly[1::]})\nprediction_hobbies_mean = pd.DataFrame.mean(predictions_hobbies, axis = 1)\nprediction_hobbies_median = pd.DataFrame.median(predictions_hobbies, axis = 1)\nprediction_hobbies_min = pd.DataFrame.min(predictions_hobbies, axis = 1)\nprediction_hobbies_max = pd.DataFrame.max(predictions_hobbies, axis = 1)\n\nalpha_hobbies_max = alpha(data_cat_final_weekly.HOBBIES[start+1:-28], prediction_hobbies_max[start:-28*2-1])\nalpha_hobbies_min = alpha(data_cat_final_weekly.HOBBIES[start+1:-28], prediction_hobbies_min[start:-28*2-1])\nalpha_hobbies_mean = alpha(data_cat_final_weekly.HOBBIES[start+1:-28], prediction_hobbies_mean[start:-28*2-1])\nalpha_hobbies_median = alpha(data_cat_final_weekly.HOBBIES[start+1:-28], prediction_hobbies_median[start:-28*2-1])\n\nfig = go.Figure(\n    data=[go.Scatter(y=data_cat_final_weekly.HOBBIES, x = data_cat_final_weekly.HOBBIES.index, name= 'Actual'), \n          go.Scatter(y=prediction_hobbies_min[0:-1]*alpha_hobbies_min, x = data_cat_final_weekly.HOBBIES.index,name= 'Predict Min'),\n          go.Scatter(y=prediction_hobbies_max[0:-1]*alpha_hobbies_max, x = data_cat_final_weekly.HOBBIES.index, name= 'Predict Max'),\n          go.Scatter(y=prediction_hobbies_mean[0:-1]*alpha_hobbies_mean, x = data_cat_final_weekly.HOBBIES.index, name= 'Predict Mean'),\n          go.Scatter(y=prediction_hobbies_median[0:-1]*alpha_hobbies_median, x = data_cat_final_weekly.HOBBIES.index, name= 'Predict median')],\n    layout=go.Layout(\n        xaxis=dict(showgrid=False),\n        yaxis=dict(showgrid=False),\n    )\n)\nfig.update_layout(title_text=\"Hobbies Category - MAPA SARIMA forecast\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nAs for the Hobbies category, we have an improvement using the MAPA technique with the max function, beating the result of individual SARIMA."},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_hobbies_weekly= rmse(data_cat_final_weekly.HOBBIES[-28::], predicted_result_hobbies_weekly[-56-1:-28-1], 'weekly Hobbies - Test')\nrmse_hobbies_weekly= rmse(data_cat_final_weekly.HOBBIES[start+1:-28], predicted_result_hobbies_weekly[start+1:-28*2-1], 'weekly Hobbies - Train')\n\n#test Fold\nrmse_hobbies_max = rmse(data_cat_final_weekly.HOBBIES[-28::], prediction_hobbies_max[-57:-29]*alpha_hobbies_max, 'Hobbies Test - max')\nrmse_hobbies_min = rmse(data_cat_final_weekly.HOBBIES[-28::], prediction_hobbies_min[-57:-29]*alpha_hobbies_min, 'Hobbies Test - min')\nrmse_hobbies_mean = rmse(data_cat_final_weekly.HOBBIES[-28::], prediction_hobbies_mean[-57:-29]*alpha_hobbies_mean, 'Hobbies Test - mean')\nrmse_hobbies_median = rmse(data_cat_final_weekly.HOBBIES[-28::], prediction_hobbies_median[-57:-29]*alpha_hobbies_median, 'Hobbies Test - median')\n#train Fold\nrmse_hobbies_max = rmse(data_cat_final_weekly.HOBBIES[start+1:-28], prediction_hobbies_max[start:-28*2-1]*alpha_hobbies_max, 'Hobbies Train - max')\nrmse_hobbies_min = rmse(data_cat_final_weekly.HOBBIES[start+1:-28], prediction_hobbies_min[start:-28*2-1]*alpha_hobbies_min, 'Hobbies Train - min')\nrmse_hobbies_mean = rmse(data_cat_final_weekly.HOBBIES[start+1:-28], prediction_hobbies_mean[start:-28*2-1]*alpha_hobbies_mean, 'Hobbies Train - mean')\nrmse_hobbies_median = rmse(data_cat_final_weekly.HOBBIES[start+1:-28], prediction_hobbies_median[start:-28*2-1]*alpha_hobbies_median, 'Hobbies Train - median' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Household Category Forecasting <a id=\"4.3\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_result_household_bimonthly = predicted_result_household_bimonthly.resample('W').mean()\npredicted_result_household_monthly = predicted_result_household_monthly.resample('W').mean()\npredictions_household = pd.DataFrame({'bimestral': predicted_result_household_bimonthly.groupby(predicted_result_household_bimonthly.notnull().cumsum()).transform(lambda x : x.sum()/len(x))[0:-4],\n                         'mensal': predicted_result_household_monthly.groupby(predicted_result_household_monthly.notnull().cumsum()).transform(lambda x : x.sum()/len(x)),\n                         'weekly': predicted_result_household_weekly[1::]})\nprediction_household_mean = pd.DataFrame.mean(predictions_household, axis = 1)\nprediction_household_median = pd.DataFrame.median(predictions_household, axis = 1)\nprediction_household_min = pd.DataFrame.min(predictions_household, axis = 1)\nprediction_household_max = pd.DataFrame.max(predictions_household, axis = 1)\n\nalpha_household_max = alpha(data_cat_final_weekly.HOUSEHOLD[start+1:-28], prediction_household_max[start:-28*2-1])\nalpha_household_min = alpha(data_cat_final_weekly.HOUSEHOLD[start+1:-28], prediction_household_min[start:-28*2-1])\nalpha_household_mean = alpha(data_cat_final_weekly.HOUSEHOLD[start+1:-28], prediction_household_mean[start:-28*2-1])\nalpha_household_median = alpha(data_cat_final_weekly.HOUSEHOLD[start+1:-28], prediction_household_median[start:-28*2-1])\nfig = go.Figure(\n    data=[go.Scatter(y=data_cat_final_weekly.HOUSEHOLD, x=data_cat_final_weekly.HOUSEHOLD.index, name= 'Actual'), \n          go.Scatter(y=prediction_household_min[0:-1]*alpha_household_min, x=data_cat_final_weekly.HOUSEHOLD.index, name= 'Predict Min'),\n          go.Scatter(y=prediction_household_max[0:-1]*alpha_household_max, x=data_cat_final_weekly.HOUSEHOLD.index, name= 'Predict Max'),\n          go.Scatter(y=prediction_household_mean[0:-1]*alpha_household_mean, x=data_cat_final_weekly.HOUSEHOLD.index, name= 'Predict Mean'),\n          go.Scatter(y=prediction_household_median[0:-1]*alpha_household_median, x=data_cat_final_weekly.HOUSEHOLD.index, name= 'Predict median')],\n    layout=go.Layout(\n        xaxis=dict(showgrid=False),\n        yaxis=dict(showgrid=False),\n    )\n)\nfig.update_layout(title_text=\"Household Category - MAPA SARIMA forecast\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nThe result here is not the biggest on the training base, but the MAPA model was better on the test base."},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_household_weekly= rmse(data_cat_final_weekly.HOUSEHOLD[-28::], predicted_result_household_weekly[-56-1:-28-1], 'weekly Household - Test')\nrmse_household_weekly= rmse(data_cat_final_weekly.HOUSEHOLD[start:-28], predicted_result_household_weekly[start:-28*2-1], 'weekly Household - Train')\n\n#test Fold\nrmse_household_max = rmse(data_cat_final_weekly.HOUSEHOLD[-28::], prediction_household_max[-57:-29]*alpha_household_max, 'Household Test - max')\nrmse_household_min = rmse(data_cat_final_weekly.HOUSEHOLD[-28::], prediction_household_min[-57:-29]*alpha_household_min, 'Household Test - min')\nrmse_household_mean = rmse(data_cat_final_weekly.HOUSEHOLD[-28::], prediction_household_mean[-57:-29]*alpha_household_mean, 'Household Test - mean')\nrmse_household_median = rmse(data_cat_final_weekly.HOUSEHOLD[-28::], prediction_household_median[-57:-29]*alpha_household_median, 'Household Test - median')\n#train Fold\nrmse_household_max = rmse(data_cat_final_weekly.HOUSEHOLD[start+1:-28], prediction_household_max[start:-28*2-1]*alpha_household_max, 'Household Train - max')\nrmse_household_min = rmse(data_cat_final_weekly.HOUSEHOLD[start+1:-28], prediction_household_min[start:-28*2-1]*alpha_household_min, 'Household Train - min')\nrmse_household_mean = rmse(data_cat_final_weekly.HOUSEHOLD[start+1:-28], prediction_household_mean[start:-28*2-1]*alpha_household_mean, 'Household Train - mean')\nrmse_household_median = rmse(data_cat_final_weekly.HOUSEHOLD[start+1:-28], prediction_household_median[start:-28*2-1]*alpha_household_median, 'Household Train - median' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Daily forecasting with exogenous variables <a id=\"5\"></a>"},{"metadata":{},"cell_type":"markdown","source":"\nFor daily forecast, we will make use of exogenous variables (commemorative dates and days of the week)"},{"metadata":{"trusted":true},"cell_type":"code","source":"holidays = pd.get_dummies(data_cat['event_name_1'], dummy_na=True)\nweekdays = pd.get_dummies(data_cat['wday'])\nexog = pd.concat([holidays, weekdays], axis = 1)\nexog.index = pd.to_datetime(data_cat_final.index , format = '%Y-%m-%d')\nexog.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the daily time series, the seasonality is weekly."},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_result_foods_daily = sarimax_predictor_exog(data_cat_final.FOODS, [2,1,0], [2,1,0,7], 28*7,\n                        'Daily forecast - Foods', exog)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_result_hobbies_daily = sarimax_predictor_exog(data_cat_final.HOBBIES, [6,1,1], [1,1,0,7], 28*7,\n                        'Daily forecast - Hobbies', exog)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_result_household_daily = sarimax_predictor_exog(data_cat_final.HOUSEHOLD, [3,0,0], [1,1,0,7], 28*7,\n                        'Daily forecast - Household', exog)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MAPA farecasting (Daily, Weekly, and monthly frequencies) <a id=\"6\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_result_foods_monthly = predicted_result_foods_monthly.resample('W').mean()\npredictions_foods = pd.DataFrame({'mensal': predicted_result_foods_monthly.groupby(predicted_result_foods_monthly.notnull().cumsum()).transform(lambda x : x.sum()/len(x))['2011-04-03':'2016-05-01'],\n                         'weekly': predicted_result_foods_weekly[1::]['2011-04-03':'2016-05-01'],\n                          'daily': predicted_result_hobbies_daily.resample('W').sum()['2011-04-03':'2016-05-01']})\nprediction_foods_mean = pd.DataFrame.mean(predictions_foods, axis = 1)\nprediction_foods_median = pd.DataFrame.median(predictions_foods, axis = 1)\nprediction_foods_min = pd.DataFrame.min(predictions_foods, axis = 1)\nprediction_foods_max = pd.DataFrame.max(predictions_foods, axis = 1)\nalpha_foods_max = alpha(data_cat_final_weekly.FOODS['2011-04-03':'2015-03-29 '], prediction_foods_max['2011-04-03':'2015-03-29 '])\nalpha_foods_min = alpha(data_cat_final_weekly.FOODS['2011-04-03':'2015-03-29 '], prediction_foods_min['2011-04-03':'2015-03-29 '])\nalpha_foods_mean = alpha(data_cat_final_weekly.FOODS['2011-04-03':'2015-03-29 '], prediction_foods_mean['2011-04-03':'2015-03-29 '])\nalpha_median = alpha(data_cat_final_weekly.FOODS['2011-04-03':'2015-03-29 '], prediction_foods_median['2011-04-03':'2015-03-29 '])\n\nfig = go.Figure(\n    data=[go.Scatter(y=data_cat_final_weekly.FOODS, x= data_cat_final_weekly.FOODS.index, name= 'Actual'), \n          go.Scatter(y=prediction_foods_min[0:-1]*alpha_foods_min, x= data_cat_final_weekly.FOODS.index, name= 'Predict Min'),\n          go.Scatter(y=prediction_foods_max[0:-1]*alpha_foods_max, x= data_cat_final_weekly.FOODS.index, name= 'Predict Max'),\n          go.Scatter(y=prediction_foods_mean[0:-1]*alpha_foods_mean, x= data_cat_final_weekly.FOODS.index, name= 'Predict Mean'),\n          go.Scatter(y=prediction_foods_median[0:-1]*alpha_median, x= data_cat_final_weekly.FOODS.index, name= 'Predict median')],\n    layout=go.Layout(\n        xaxis=dict(showgrid=False),\n        yaxis=dict(showgrid=False),\n    )\n)\nfig.update_layout(title_text=\"Foods Category - MAPA SARIMA forecast\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The result here is also not better using the MAPA technique."},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse(data_cat_final_weekly.FOODS[-28::], predicted_result_foods_weekly[-56-1:-28-1], 'weekly Foods - Test')\nrmse(data_cat_final_weekly.FOODS[start+1:-28], predicted_result_foods_weekly[start+1:-28*2-1], 'weekly Foods - Train')\n#test Fold\nrmse_foods_max = rmse(data_cat_final_weekly.FOODS[-28::], prediction_foods_max[-57:-29]*alpha_foods_max, 'Foods Test - max')\nrmse_foods_min = rmse(data_cat_final_weekly.FOODS[-28::], prediction_foods_min[-57:-29]*alpha_foods_min, 'Foods Test - min')\nrmse_foods_mean = rmse(data_cat_final_weekly.FOODS[-28::], prediction_foods_mean[-57:-29]*alpha_foods_mean, 'Foods Test - mean')\nrmse_foods_median = rmse(data_cat_final_weekly.FOODS[-28::], prediction_foods_median[-57:-29]*alpha_median, 'Foods Test - median')\n#train Fold\nrmse_foods_max = rmse(data_cat_final_weekly.FOODS['2011-04-03':'2015-03-29 '], prediction_foods_max['2011-04-03':'2015-03-29 ']*alpha_foods_max, 'Foods Train - max')\nrmse_foods_min = rmse(data_cat_final_weekly.FOODS['2011-04-03':'2015-03-29 '], prediction_foods_min['2011-04-03':'2015-03-29 ']*alpha_foods_min, 'Foods Train - min')\nrmse_foods_mean = rmse(data_cat_final_weekly.FOODS['2011-04-03':'2015-03-29 '], prediction_foods_mean['2011-04-03':'2015-03-29 ']*alpha_foods_mean, 'Foods Train - mean')\nrmse_foods_median = rmse(data_cat_final_weekly.FOODS['2011-04-03':'2015-03-29 '], prediction_foods_median['2011-04-03':'2015-03-29 ']*alpha_median, 'Foods Train - median' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_result_hobbies_monthly = predicted_result_hobbies_monthly.resample('W').mean()\npredictions_hobbies = pd.DataFrame({'mensal': predicted_result_hobbies_monthly.groupby(predicted_result_hobbies_monthly.notnull().cumsum()).transform(lambda x : x.sum()/len(x))['2011-04-03':'2016-05-01'],\n                         'weekly': predicted_result_hobbies_weekly['2011-04-03':'2016-05-01'],\n                         'daily': predicted_result_hobbies_daily.resample('W').sum()['2011-04-03':'2016-05-01']})\nprediction_hobbies_mean = pd.DataFrame.mean(predictions_hobbies, axis = 1)\nprediction_hobbies_median = pd.DataFrame.median(predictions_hobbies, axis = 1)\nprediction_hobbies_min = pd.DataFrame.min(predictions_hobbies, axis = 1)\nprediction_hobbies_max = pd.DataFrame.max(predictions_hobbies, axis = 1)\n\nalpha_hobbies_max = alpha(data_cat_final_weekly.HOBBIES['2011-04-03':'2015-03-29 '], prediction_hobbies_max['2011-04-03':'2015-03-29 '])\nalpha_hobbies_min = alpha(data_cat_final_weekly.HOBBIES['2011-04-03':'2015-03-29 '], prediction_hobbies_min['2011-04-03':'2015-03-29 '])\nalpha_hobbies_mean = alpha(data_cat_final_weekly.HOBBIES['2011-04-03':'2015-03-29 '], prediction_hobbies_mean['2011-04-03':'2015-03-29 '])\nalpha_hobbies_median = alpha(data_cat_final_weekly.HOBBIES['2011-04-03':'2015-03-29 '], prediction_hobbies_median['2011-04-03':'2015-03-29 '])\n\nfig = go.Figure(\n    data=[go.Scatter(y=data_cat_final_weekly.HOBBIES, x= data_cat_final_weekly.HOBBIES.index, name= 'Actual'), \n          go.Scatter(y=prediction_hobbies_min[0:-1]*alpha_hobbies_min, x= data_cat_final_weekly.HOBBIES.index, name= 'Predict Min'),\n          go.Scatter(y=prediction_hobbies_max[0:-1]*alpha_hobbies_max, x= data_cat_final_weekly.HOBBIES.index, name= 'Predict Max'),\n          go.Scatter(y=prediction_hobbies_mean[0:-1]*alpha_hobbies_mean, x= data_cat_final_weekly.HOBBIES.index, name= 'Predict Mean'),\n          go.Scatter(y=prediction_hobbies_median[0:-1]*alpha_hobbies_median, x= data_cat_final_weekly.HOBBIES.index, name= 'Predict median')],\n    layout=go.Layout(\n        xaxis=dict(showgrid=False),\n        yaxis=dict(showgrid=False),\n    )\n)\nfig.update_layout(title_text=\"Hobbies Category - MAPA SARIMA forecast\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nThe result here was better with the MAPA technique using the median and mean for combination."},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_hobbies_weekly= rmse(data_cat_final_weekly.HOBBIES[-28::], predicted_result_hobbies_weekly[-56-1:-28-1], 'weekly Hobbies - Test')\nrmse_hobbies_weekly= rmse(data_cat_final_weekly.HOBBIES[start+1:-28], predicted_result_hobbies_weekly[start+1:-28*2-1], 'weekly Hobbies - Train')\n\n#test Fold\nrmse_hobbies_max = rmse(data_cat_final_weekly.HOBBIES[-28::], prediction_hobbies_max[-57:-29]*alpha_hobbies_max, 'Hobbies Test - max')\nrmse_hobbies_min = rmse(data_cat_final_weekly.HOBBIES[-28::], prediction_hobbies_min[-57:-29]*alpha_hobbies_min, 'Hobbies Test - min')\nrmse_hobbies_mean = rmse(data_cat_final_weekly.HOBBIES[-28::], prediction_hobbies_mean[-57:-29]*alpha_hobbies_mean, 'Hobbies Test - mean')\nrmse_hobbies_median = rmse(data_cat_final_weekly.HOBBIES[-28::], prediction_hobbies_median[-57:-29]*alpha_hobbies_median, 'Hobbies Test - median')\n#train Fold\nrmse_hobbies_max = rmse(data_cat_final_weekly.HOBBIES['2011-04-03':'2015-03-29 '], prediction_hobbies_max['2011-04-03':'2015-03-29 ']*alpha_hobbies_max, 'Hobbies Train - max')\nrmse_hobbies_min = rmse(data_cat_final_weekly.HOBBIES['2011-04-03':'2015-03-29 '], prediction_hobbies_min['2011-04-03':'2015-03-29 ']*alpha_hobbies_min, 'Hobbies Train - min')\nrmse_hobbies_mean = rmse(data_cat_final_weekly.HOBBIES['2011-04-03':'2015-03-29 '], prediction_hobbies_mean['2011-04-03':'2015-03-29 ']*alpha_hobbies_mean, 'Hobbies Train - mean')\nrmse_hobbies_median = rmse(data_cat_final_weekly.HOBBIES['2011-04-03':'2015-03-29 '], prediction_hobbies_median['2011-04-03':'2015-03-29 ']*alpha_hobbies_median, 'Hobbies Train - median' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_result_household_bimonthly = predicted_result_household_bimonthly.resample('W').mean()\npredicted_result_household_monthly = predicted_result_household_monthly.resample('W').mean()\npredictions_household = pd.DataFrame({'mensal': predicted_result_household_monthly.groupby(predicted_result_household_monthly.notnull().cumsum()).transform(lambda x : x.sum()/len(x))['2011-04-03':'2016-05-01'],\n                         'weekly': predicted_result_household_weekly['2011-04-03':'2016-05-01'],\n                          'daily': predicted_result_household_daily.resample('W').sum()['2011-04-03':'2016-05-01']})\nprediction_household_mean = pd.DataFrame.mean(predictions_household, axis = 1)\nprediction_household_median = pd.DataFrame.median(predictions_household, axis = 1)\nprediction_household_min = pd.DataFrame.min(predictions_household, axis = 1)\nprediction_household_max = pd.DataFrame.max(predictions_household, axis = 1)\n\nalpha_household_max = alpha(data_cat_final_weekly.HOUSEHOLD['2011-04-03':'2015-03-29 '], prediction_household_max['2011-04-03':'2015-03-29 '])\nalpha_household_min = alpha(data_cat_final_weekly.HOUSEHOLD['2011-04-03':'2015-03-29 '], prediction_household_min['2011-04-03':'2015-03-29 '])\nalpha_household_mean = alpha(data_cat_final_weekly.HOUSEHOLD['2011-04-03':'2015-03-29 '], prediction_household_mean['2011-04-03':'2015-03-29 '])\nalpha_household_median = alpha(data_cat_final_weekly.HOUSEHOLD['2011-04-03':'2015-03-29 '], prediction_household_median['2011-04-03':'2015-03-29 '])\nfig = go.Figure(\n    data=[go.Scatter(y=data_cat_final_weekly.HOUSEHOLD, x= data_cat_final_weekly.HOUSEHOLD.index, name= 'Actual'), \n          go.Scatter(y=prediction_household_min[0:-1]*alpha_household_min, x= data_cat_final_weekly.HOUSEHOLD.index, name= 'Predict Min'),\n          go.Scatter(y=prediction_household_max[0:-1]*alpha_household_max, x= data_cat_final_weekly.HOUSEHOLD.index, name= 'Predict Max'),\n          go.Scatter(y=prediction_household_mean[0:-1]*alpha_household_mean, x= data_cat_final_weekly.HOUSEHOLD.index, name= 'Predict Mean'),\n          go.Scatter(y=prediction_household_median[0:-1]*alpha_household_median, x= data_cat_final_weekly.HOUSEHOLD.index, name= 'Predict median')],\n    layout=go.Layout(\n        xaxis=dict(showgrid=False),\n        yaxis=dict(showgrid=False),\n    )\n)\nfig.update_layout(title_text=\"Household Category - MAPA SARIMA forecast\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nThe result here was excellent, better than the MAPA technique using aggregation with weekly, monthly and bimonthly frequencies"},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_household_weekly= rmse(data_cat_final_weekly.HOUSEHOLD[-28::], predicted_result_household_weekly[-56-1:-28-1], 'weekly Household - Test')\nrmse_household_weekly= rmse(data_cat_final_weekly.HOUSEHOLD[start:-28], predicted_result_household_weekly[start:-28*2-1], 'weekly Household - Train')\n\n#test Fold\nrmse_household_max = rmse(data_cat_final_weekly.HOUSEHOLD[-28::], prediction_household_max[-57:-29]*alpha_household_max, 'Household Test - max')\nrmse_household_min = rmse(data_cat_final_weekly.HOUSEHOLD[-28::], prediction_household_min[-57:-29]*alpha_household_min, 'Household Test - min')\nrmse_household_mean = rmse(data_cat_final_weekly.HOUSEHOLD[-28::], prediction_household_mean[-57:-29]*alpha_household_mean, 'Household Test - mean')\nrmse_household_median = rmse(data_cat_final_weekly.HOUSEHOLD[-28::], prediction_household_median[-57:-29]*alpha_household_median, 'Household Test - median')\n#train Fold\nrmse_household_max = rmse(data_cat_final_weekly.HOUSEHOLD['2011-04-03':'2015-03-29 '], prediction_household_max['2011-04-03':'2015-03-29 ']*alpha_household_max, 'Household Train - max')\nrmse_household_min = rmse(data_cat_final_weekly.HOUSEHOLD['2011-04-03':'2015-03-29 '], prediction_household_min['2011-04-03':'2015-03-29 ']*alpha_household_min, 'Household Train - min')\nrmse_household_mean = rmse(data_cat_final_weekly.HOUSEHOLD['2011-04-03':'2015-03-29 '], prediction_household_mean['2011-04-03':'2015-03-29 ']*alpha_household_mean, 'Household Train - mean')\nrmse_household_median = rmse(data_cat_final_weekly.HOUSEHOLD['2011-04-03':'2015-03-29 '], prediction_household_median['2011-04-03':'2015-03-29 ']*alpha_household_median, 'Household Train - median' )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}