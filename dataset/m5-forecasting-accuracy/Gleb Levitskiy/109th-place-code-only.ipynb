{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is a slightly altered version of the solution","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 1\nnp.random.seed(SEED)\nrandom.seed(SEED)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"calendar = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')\n\n\ndef set_calendar_data_types(df: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n    print('Setting calendat data types...')\n    \n    df['events_names'] = df[['event_name_1', 'event_name_2']].fillna('').sum(axis=1).replace('', 'None')\n    df['events_types'] = df[['event_type_1', 'event_type_2']].fillna('').sum(axis=1).replace('', 'None')\n    df['next_events_names'] = df[['event_name_1', 'event_name_2']].fillna('').sum(axis=1).replace('', np.nan).fillna(method='bfill')\n    df['next_events_types'] = df[['event_type_1', 'event_type_2']].fillna('').sum(axis=1).replace('', np.nan).fillna(method='bfill')\n    df['event_name_1'] = pd.Categorical(df['event_name_1'].fillna('None'))\n    df['event_type_1'] = pd.Categorical(df['event_type_1'].fillna('None'))\n    df['event_name_2'] = pd.Categorical(df['event_name_2'].fillna('None'))\n    df['event_type_2'] = pd.Categorical(df['event_type_2'].fillna('None'))\n    df['d'] = pd.Categorical(df['d'])\n    df['wm_yr_wk'] = pd.Categorical(df['wm_yr_wk'],\n                                    categories=df.wm_yr_wk.unique(), \n                                    ordered=True)\n    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n    df['wday'] = df['wday'].astype(np.int8)\n    df['snap_CA'] = df['snap_CA'].astype(bool)\n    df['snap_TX'] = df['snap_TX'].astype(bool)\n    df['snap_WI'] = df['snap_WI'].astype(bool)\n    df.drop(['year', 'weekday', 'month', 'wday'], axis=1, inplace=True)\n    \n    print('Done!')\n    return df\n\ndef get_calendar_time_features(df: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n    print('Get simple datetime features...')\n    \n    df['day_of_month'] = df['date'].dt.day.astype(np.int8)\n    df['day_of_week'] = df['date'].dt.dayofweek.astype(np.int8)\n    \n    print('Done!')\n    return df\n\ndef get_calendar_event_features(df: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n    print('Getting event features...')\n    \n    events_all_dates = df[(df['event_name_1'] != 'None') | (df['event_name_2'] != 'None')]\n    prev = 0\n    \n    for event in events_all_dates.iterrows():\n        df.loc[prev:event[0], 'days_to_event'] = (df.loc[event[0], 'date'] - df.loc[prev:event[0], 'date']).dt.days\n        prev = event[0]\n    \n    df['days_to_event'] = df['days_to_event'].replace(0, 25).astype(np.int8)\n    \n    events_names_dict = df['events_names'].value_counts().rank(method='first').to_dict()\n    events_types_dict = df['events_types'].value_counts().rank(method='first').to_dict()\n    df['events_names'] = df['events_names'].map(events_names_dict).astype(np.int8)\n    df['events_types'] = df['events_types'].map(events_types_dict).astype(np.int8)\n    df['next_events_names'] = df['next_events_names'].map(events_names_dict).astype(np.int8)\n    df['next_events_types'] = df['next_events_types'].map(events_types_dict).astype(np.int8)\n    \n    df.drop(['event_name_1', 'event_name_2', 'event_type_1', 'event_type_2'], axis=1, inplace=True)\n    \n    print('Done!')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ncalendar = (calendar\n            .pipe(set_calendar_data_types)\n            .pipe(get_calendar_time_features)\n            .pipe(get_calendar_event_features))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"sell_prices = pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv')\n\ndef set_sell_prices_cat_data_types(df: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n    print('Setting data types...')\n    \n    df['store_id'] = pd.Categorical(df['store_id'])\n    df['item_id'] = pd.Categorical(df['item_id'])\n    df['wm_yr_wk'] = pd.Categorical(df['wm_yr_wk'],\n                                    categories=df.wm_yr_wk.unique(), \n                                    ordered=True)\n    \n    print('Done!')\n    return df\n\ndef get_sell_prices_features(df: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n    print('Getting sell prices features...')\n    \n    df['dept_id'] = pd.Categorical(df['item_id'].str.slice(stop=-4))\n    df['cat_id'] = pd.Categorical(df['item_id'].str.slice(stop=-6))\n    df['state_id'] = pd.Categorical(df['store_id'].str.slice(stop=-2))\n    df['sell_price'] = df['sell_price'].replace((np.inf, -np.inf, np.nan), 0).astype(np.float16)\n    \n    print('Done!')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nsell_prices = (sell_prices\n               .pipe(set_sell_prices_cat_data_types)\n               .pipe(get_sell_prices_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import category_encoders\nfrom scipy import stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VAL_CUTOFF = '2015-05-01'\nTRAIN_CUTOFF = '2012-01-01'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_evaluation.csv')\ncpi = pd.read_csv('../input/cpi-m5/cpi.csv')\n\n\ndef merge_df(df: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n    print('Merging dataframes together...')\n    \n    df = df.merge(calendar, on=['wm_yr_wk'], how='left')\n    \n    df.drop(['wm_yr_wk'], axis=1, inplace=True)\n    \n    df = df.merge(pd.melt(sales, \n                          id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                          var_name='d', \n                          value_name='demand')[['item_id', 'store_id', 'd', 'demand']], \n                  on=['item_id', 'store_id', 'd'],\n                  how='left')\n    \n    cpi['CPI_2m_ago'] = cpi.groupby('state_id')['cpi'].transform(lambda x: x.shift(2)).astype(np.float16)\n    cpi.drop(['cpi', 'gasoline_price', 'employees', 'population', 'us_gasoline_price'], axis=1, inplace=True)\n    df['month'] = df['date'].dt.month.astype(np.int8)\n    df['year'] = df['date'].dt.year.astype(np.int16)\n    df = df.merge(cpi, on=['state_id', 'month', 'year'], how='left')\n    df.drop(['year', 'month'], axis=1, inplace=True)\n    \n    gc.collect()\n    print('Done!')\n    return df\n\n\ndef set_df_data_types(df: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n    print('Setting data types...')\n    \n    df['demand'] = df['demand'].astype(np.float16)\n    df['store_id'] = pd.Categorical(df['store_id'])\n    df['item_id'] = pd.Categorical(df['item_id'])\n    df['id'] = pd.Categorical(df['item_id'].str.cat(df['store_id'], sep='_'))\n    df['d'] = df['d'].str.slice(start=2).astype(np.int16)\n    df['CPI_2m_ago'] = df['CPI_2m_ago'].astype(np.float16)\n    \n    for state in df.state_id.unique():\n        \n        mask = df['state_id'] == state\n        df.loc[mask, 'is_snap_avaliable'] = df.loc[mask, f'snap_{state}']\n    \n    df['is_snap_avaliable'] = df['is_snap_avaliable'].astype(bool)\n    \n    df.drop(['snap_TX', 'snap_CA', 'snap_WI'], axis=1, inplace=True)\n    \n    gc.collect()\n    print('Done!')\n    return df\n\n\ndef get_demand_feature(df: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n    print('Getting demand lag values...')\n    \n    SHIFT = 29\n    \n    df[f'cumulative_mean_demand_{SHIFT}d_ago'] = (df.groupby(['store_id', 'item_id'])['demand']\n                                                  .transform(lambda x: x.shift(SHIFT).expanding().mean())\n                                                  .replace((np.inf, -np.inf, np.nan), 0)\n                                                  .astype(np.float16))\n    \n    df[f'cumulative_md_low_demand_{SHIFT}d_ago'] = (df.groupby(['store_id', 'item_id'])['demand']\n                                                    .transform(lambda x: x.shift(SHIFT).expanding().median().apply(np.floor))\n                                                    .replace((np.inf, -np.inf, np.nan), 0)\n                                                    .astype(np.int16))\n    \n    for i in [1, 4, 8]:\n        \n        df[f'ewm_mean_{i}w_demand_{SHIFT}d_ago'] = (df.groupby(['store_id', 'item_id'])['demand']\n                                                    .transform(lambda x: x.shift(SHIFT).ewm(span=7*i).mean())\n                                                    .replace((np.inf, -np.inf, np.nan), 0)\n                                                    .astype(np.float16))\n        \n        df[f'rolling_mean_{i}w_demand_{SHIFT}d_ago'] = (df.groupby(['store_id', 'item_id'])['demand']\n                                                        .transform(lambda x: x.shift(SHIFT).rolling(7*i).mean())\n                                                        .replace((np.inf, -np.inf, np.nan), 0)\n                                                        .astype(np.float16))\n        \n    gc.collect()\n    print('Done!')\n    return df\n\n\ndef encode_categorical_features(df: pd.core.frame.DataFrame, val_cutoff: str = VAL_CUTOFF) -> pd.core.frame.DataFrame:\n    print('Encoding categorical features...')\n    \n    for feat in ['store_id', 'item_id', 'dept_id']:\n        \n        encoder = category_encoders.CatBoostEncoder(handle_unknown='value', handle_missing='value', a=1e-9)\n        encoder.fit(df.loc[df.date < val_cutoff, feat], df.loc[df.date < val_cutoff, 'demand'])\n        df[feat] = encoder.transform(df[feat]).astype(np.float16)\n        del encoder\n        \n        if (feat == 'store_id') or (feat == 'dept_id'):\n            \n            temp_dict = {k: v for k, v in zip(df[feat].unique(), stats.rankdata(df[feat].unique()))}\n            df[feat] = df[feat].map(temp_dict).astype(np.int8)\n            del temp_dict\n    \n    df['expected_item_revenue'] = (df['item_id'] * df['sell_price']).replace((np.inf, -np.inf, np.nan), 0).astype(np.float16)\n    \n    df['is_in_CA'] = df['state_id'] == 'CA'\n    df['is_in_TX'] = df['state_id'] == 'TX'\n    df['is_in_WI'] = df['state_id'] == 'WI'\n    \n    df.drop(['state_id'], axis=1, inplace=True)\n    \n    df['is_foods'] = df['cat_id'] == 'FOODS'\n    df['is_household'] = df['cat_id'] == 'HOUSEHOLD'\n    df['is_hobbies'] = df['cat_id'] == 'HOBBIES'\n    \n    df.drop(['cat_id'], axis=1, inplace=True)\n    \n    gc.collect()\n    print('Done!')\n    return df\n\n\ndef cut_df(df: pd.core.frame.DataFrame, train_cutoff: str = TRAIN_CUTOFF) -> pd.core.frame.DataFrame:\n    print('Cutting off dataframe...')\n    \n    df = df.loc[df.date >= train_cutoff]\n    \n    gc.collect()\n    print('Done')\n    return df.reset_index(drop=True)\n\n\ndef get_dev_val_test(df: pd.core.frame.DataFrame) -> list:\n    print('Splitting dataset into Train-Validation-Test...')\n    \n    X_train = df[(df.date < VAL_CUTOFF)].drop(['demand', 'date', 'id', 'd'], axis=1)\n    X_valid = df[(df['d'] < 1942) & (df.date >= VAL_CUTOFF)].drop(['demand', 'date', 'id', 'd'], axis=1)\n    X_test = df[df['d'] >= 1914].drop(['demand', 'd'], axis=1)\n    y_train = df[(df.date < VAL_CUTOFF)]['demand'].astype(np.int16)\n    y_valid = df[(df['d'] < 1942) & (df.date >= VAL_CUTOFF)]['demand'].astype(np.int16)\n    \n    gc.collect()\n    print('Done!')\n    return [X_train, X_valid, X_test, y_train, y_valid]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nX_train, X_valid, X_test, y_train, y_valid = (sell_prices\n                                              .pipe(merge_df)\n                                              .pipe(set_df_data_types)\n                                              .pipe(get_demand_feature)\n                                              .pipe(encode_categorical_features)\n                                              .pipe(cut_df)\n                                              .pipe(get_dev_val_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del sell_prices, calendar, sales, cpi\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import PassiveAggressiveRegressor\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.compose import ColumnTransformer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = X_train.columns[(X_train.astype(np.float16).corrwith(y_train).abs() > 0.01)]\n\nnon_bool_cols = X_train[cols].select_dtypes(exclude='bool').columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_tr = ColumnTransformer([('scale', RobustScaler(quantile_range=(5, 95)), non_bool_cols)], remainder='passthrough')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = PassiveAggressiveRegressor(C=1e-5, \n                                fit_intercept=True, \n                                max_iter=1000, \n                                tol=1e-3, \n                                shuffle=True, \n                                verbose=1,\n                                loss='squared_epsilon_insensitive', \n                                epsilon=1e-5,\n                                random_state=SEED, \n                                warm_start=True, \n                                average=True)\n\nlr.fit(col_tr.fit_transform(X_train[cols]), y_train);\nlr.fit(col_tr.transform(X_valid[cols]), y_valid);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import catboost as cb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dev_pool = cb.Pool(X_train, \n                   y_train.astype(int))\nval_pool = cb.Pool(X_valid, \n                   y_valid.astype(int))\n\ntr_val_ratio = X_valid.shape[0] / X_train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_train, X_valid, y_train, y_valid\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbt_regressor = cb.CatBoostRegressor(iterations=5000,\n                                     learning_rate=0.1,\n                                     depth=7,\n                                     l2_leaf_reg=3.5,\n                                     loss_function='RMSE',\n                                     boosting_type='Plain',\n                                     eval_metric='RMSE',\n                                     feature_border_type='UniformAndQuantiles',\n                                     thread_count=4,\n                                     random_seed=SEED,\n                                     has_time=True,\n                                     random_strength=1,\n                                     bootstrap_type='MVS',\n                                     subsample=0.8,\n                                     max_bin=254,\n                                     score_function='L2',\n                                     model_shrink_rate=1e-5,\n                                     boost_from_average=False,\n                                     sampling_frequency='PerTreeLevel',\n                                     fold_permutation_block=1,\n                                     leaf_estimation_method='Newton',\n                                     leaf_estimation_iterations=1,\n                                     fold_len_multiplier=2,\n                                     model_shrink_mode='Constant',\n                                     task_type='CPU',\n                                     langevin=True,\n                                     diffusion_temperature=1e2,\n                                     used_ram_limit='15gb',\n                                     model_size_reg=0,\n                                     allow_writing_files=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbt_regressor.fit(dev_pool, eval_set=val_pool, early_stopping_rounds=300, verbose=100);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nplt.figure(figsize=(24,10));\nplt.barh(gbt_regressor.feature_names_, gbt_regressor.feature_importances_);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del dev_pool\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cbr = cb.CatBoostRegressor(iterations=int(gbt_regressor.get_best_iteration() * tr_val_ratio),\n                           learning_rate=0.1,\n                           depth=7,\n                           l2_leaf_reg=3.5,\n                           loss_function='RMSE',\n                           boosting_type='Plain',\n                           eval_metric='RMSE',\n                           feature_border_type='UniformAndQuantiles',\n                           thread_count=4,\n                           random_seed=SEED,\n                           has_time=True,\n                           random_strength=1,\n                           bootstrap_type='MVS',\n                           subsample=0.8,\n                           max_bin=254,\n                           score_function='L2',\n                           model_shrink_rate=0,\n                           boost_from_average=False,\n                           sampling_frequency='PerTreeLevel',\n                           fold_permutation_block=1,\n                           leaf_estimation_method='Newton',\n                           leaf_estimation_iterations=1,\n                           fold_len_multiplier=2,\n                           model_shrink_mode='Constant',\n                           task_type='CPU',\n                           langevin=True,\n                           diffusion_temperature=1e2,\n                           used_ram_limit='15gb',\n                           model_size_reg=0,\n                           allow_writing_files=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cbr.fit(val_pool, verbose=10, init_model=gbt_regressor);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del val_pool, gbt_regressor\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_gbt = np.maximum(cbr.predict(X_test.drop(['date', 'id'], axis=1)), 0)\npreds_lr = np.maximum(lr.predict(col_tr.transform(X_test[cols])), 0)\nX_test['demand'] = 0.9 * preds_gbt + 0.1 * preds_lr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del cbr, lr\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = X_test.pivot_table(index='id', columns='date', values='demand').reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_sub(predictions, submission):\n    \n    new_cols = [f'F{i}' for i in range(1, 29)]\n    validation = predictions.copy()\n    validation = validation[validation.columns[1:29]]\n    validation = validation.rename({k: v for k, v in zip(validation.columns, new_cols)}, axis=1)\n    validation['id'] = predictions['id'].apply(lambda x: x + '_validation')\n    \n    evaluation = predictions.copy()\n    evaluation = evaluation[evaluation.columns[29:]]\n    evaluation = evaluation.rename({k: v for k, v in zip(evaluation.columns, new_cols)}, axis=1)\n    evaluation['id'] = predictions['id'].apply(lambda x: x + '_evaluation')\n    \n    sub_1 = pd.merge(submission.loc[:30489, 'id'], validation, how='left')\n    sub_2 = pd.merge(submission.loc[30490:, 'id'], evaluation, how='left')\n    \n    return pd.concat([sub_1, sub_2], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = get_sub(predictions, submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}