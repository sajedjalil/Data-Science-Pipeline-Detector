{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Inequality regarding WRMSSE\nIn this notebook, I would like to show the inequality of WRMSSE.\n\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Interpretion on WRMSSE\nWeighted Root Mean Squared Scaled Error (WRMSSE) is defined as\n\n\\begin{equation}\n\\mathrm{WRMSSE} = \\sum_{i} w_i \\sqrt{ \\sum_{t=n+1}^{n+h} \\frac{1}{h} \\frac{(y_{it} - \\hat{y}_{it})^2}{S_i}},\n\\end{equation}\n\nwhere $S_i$ means random walk, \n\n\\begin{equation}\nS_i = \\frac{1}{n-1} \\sum^n_{t=2} (y_{it} - \\hat{y}_{it})^2.\n\\end{equation}\n\nWith some calculation, \n\n\\begin{equation}\n\\mathrm{WRMSSE} = \\sum_{i} \\frac{w_i}{\\sqrt{S_i}} \\sqrt{ \\sum_{t=n+1}^{n+h} \\frac{1}{h} (y_{it} - \\hat{y}_{it})^2}.\n\\end{equation}\n\nSo, we can regard $ w_i / \\sqrt{S_i}$ as the net weight.\nThis factor can be interpreted as price times randomness.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Inequality regarging WRMSSE\nUsing triangle inequality $\\sqrt{\\sum_i a_i^2} \\leq \\sum_i |a_i|$\n\\begin{equation}\n\\sqrt{ \\sum_t (y_{it} - \\hat{y}_{it})^2 } \\leq \\sum_t |y_{it} - \\hat{y}_{it}|.\n\\end{equation}\n\nThus, \n\\begin{equation}\n\\mathrm{WRMSSE} \\leq \\sum_{i, t} \\frac{w_i}{\\sqrt{h S_i}} |y_{it} - \\hat{y}_{it}|.\n\\end{equation}\n\nDefined as \n\\begin{equation}\nz_{it} := w_i y_{it} / \\sqrt{h S_i}, \\\\\nN := \\sum_i 1, \n\\end{equation}\n\nWRMSSE can be evaluated as\n\\begin{equation}\n\\mathrm{WRMSSE} \\leq hN \\times \\frac{1}{hN} \\sum_{i,t} |z_{it} - \\hat{z}_{it} |.\n\\end{equation}\n\nThe right side of this inequality can be interpreted as Mean Absolute Error (MAE).\nSimilar procedures are found in [this paper](https://robjhyndman.com/papers/mase.pdf), so I would like to call this one psuedo-Weighted Mean Absolute Scaled Error (psuedo-WMASE).\n\nSince WRMSSE is bounded above by psuedo-WMASE, WRMSSE is always lower than psuedo-WMASE.\n\nIn this expression, we don't have to care weights and scaling factors when predictions are evaluated \nbecause sum about indices i and t can be exchanged.\n\nIt is difficult to adopt WRMSSE directly, but after these transformations, we can use psuedo-WMASE as metric easily. \nWhen you take some transformation, true validation score for WRMSSE in training is bounded above by psuedo-WMASE.\n\n(TRUE Weighted Mean Absolute Scaled Error (WMASE) may be defined as\n\\begin{equation}\n\\mathrm{WMASE} = \\sum_{i} w_i \\sum_{t=n+1}^{n+h} \\frac{1}{h} \\frac{|y_{it} - \\hat{y}_{it}|}{S'_i},\n\\end{equation}\nwhere\n\\begin{equation}\nS'_i = \\frac{1}{n-1} \\sum^n_{t=2} |y_{it} - \\hat{y}_{it}|.\n\\end{equation}\n)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Difficulty in this competition\nIn this competition, we must predict not only one aggregated level but other aggregated or disaggregated levels.\n$y_{it}$ depends on other levels. \n\nIf we take bottom-up approach, all of upper level values are determined automatically.\nSo are if you take top-down or middle-out approach.\n(Of course, this is too naive. \nThere would be many approaches that overcomes this difficulty coming from this level-dependent metric).\n\nHowever, at least, we can know if the predictions at some level are good or not while training with validation sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## How to use this idea (example)\nThanks to the help of some great notebooks ([M5 - Simple FE](https://www.kaggle.com/kyakovlev/m5-simple-fe) [@kyakovlev](https://www.kaggle.com/kyakovlev) and [Fast & Clear WRMSSE 18ms](https://www.kaggle.com/sibmike/fast-clear-wrmsse-18ms) [@sibmike](https://www.kaggle.com/sibmike)), I would show the idea above with the simple bottom-up approach.\n\n* train: d_730 - d_1885\n* validation: d_1886 - d_1913\n* test: d_1914 - d_1941\n\nThe steps to apply the idea above are:\n1. Make $z_{it}$ with 'sales', weights, scaling factors.\n2. Train and predict (Target = $z_{it}$).\n3. Make 'sales' prediction at the bottom level from $z_{it}$","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport gc\nimport warnings\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## file path ##\nBASE = '../input/m5-simple-fe/grid_part_1.pkl'\nCALENDAR = '../input/m5-simple-fe/grid_part_3.pkl'\nLAGS = '../input/m5-lags-features/lags_df_28.pkl'\n\nSW = '../input/fast-clear-wrmsse-18ms/sw_df.pkl'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## basic features ##\ndf = pd.concat([pd.read_pickle(BASE),\n                    pd.read_pickle(CALENDAR).iloc[:,9:]],\n                    axis=1)\n\n#df = pd.read_pickle(BASE)\n\n## lag features ##\nlag_df = pd.read_pickle(LAGS)\nlag_df = lag_df.iloc[:, 3:11]\n\n## input data ##\ngrid_df = pd.concat([df, lag_df], axis=1)\n\ndel lag_df, df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## weights and scaling factors ##\n# s, w, sw in sw_df are scaling factor, weight, and the product of them respectively.\n# Since we use only the product sw, other columns are dropped.\n\nsw_df = pd.read_pickle(SW)\n\nsw_df.reset_index(inplace=True)\nsw_df = sw_df[sw_df.level==11]\nsw_df.drop(['level', 's', 'w'], axis=1, inplace=True)\n\nsw_df['id'] = sw_df['id'].astype('category')\ngrid_df = grid_df.merge(sw_df, on='id', how='left')\n\n# The product of sales and sw corresponds to z_it (different by a factor).\n# This one is the main target.\ngrid_df['sw_sales'] = grid_df['sales'] * grid_df['sw']\n\ndel sw_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## training model (LGBM) ##\n\n# train, validation and test set\nSTART_TRAIN = 730\nEND_TRAIN = 1913\nP_HORIZON = 28\n\ngrid_df = grid_df[grid_df.d>=START_TRAIN]\ngrid_df.to_pickle('grid_df_ex.pkl')\n\ntest_idx = grid_df.d > END_TRAIN\nvalid_idx = (grid_df.d <= END_TRAIN) & (grid_df.d > END_TRAIN - P_HORIZON)\ntrain_idx = (grid_df.d <= END_TRAIN- P_HORIZON) & (grid_df.d >= START_TRAIN)\n\n\n# hyper parameters\nlgb_params = {\n                    'boosting_type': 'gbdt',\n                    'objective': 'tweedie',\n                    'tweedie_variance_power': 1.1,\n                    'learning_rate': 0.1,\n                    'num_leaves': 2**5-1,\n                    'min_data_in_leaf': 2**6-1,\n                    'n_estimators': 100,\n                    'boost_from_average': False,\n                    'verbose': -1,\n                } \n\n# Set the metric in training Mean Absolute Error (MAE).\n# Using MAE with target 'sw_sales', validation values in training show psurdo-WMASE.\nlgb_params['metric'] = 'mae'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm train_data.bin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Indirect Prediction ##\n\n# features and target\nremove_fe = ['id', 'd', 'sales', 'sw', 'sw_sales']\nfeatures = [fe for fe in list(grid_df) if fe not in remove_fe]\n\nTARGET = 'sw_sales'\n\n# dataset\ntrain_data = lgb.Dataset(grid_df[train_idx][features], \n                        label=grid_df[train_idx][TARGET])\ntrain_data.save_binary('train_data.bin')\ntrain_data = lgb.Dataset('train_data.bin')\n\nvalid_data = lgb.Dataset(grid_df[valid_idx][features],\n                        label=grid_df[valid_idx][TARGET])\n\ndel grid_df\ngc.collect()\n\n# model training\nestimator = lgb.train(lgb_params,\n                      train_data,\n                      valid_sets = [train_data, valid_data],\n                      verbose_eval = 10,\n                      early_stopping_rounds = 5,\n                      )\n\n# Validaiton result means psuedo-WMASE at the bottom level (level 12).\n# Calculated psuedo-WMASE is different by a constant factor.\n# The validation score means ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The validation score in training is $\\frac{1}{hN} \\sum_i |z_{it} - \\hat{z}_{it}|$. To get psuedo-WMASE, we must the factor $hN$, but this is just a constant value ($h=28, N=30490$). \nSo I ignore the difference.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## prediction for test set ##\ngrid_df = pd.read_pickle('grid_df_ex.pkl')\n\ntest_data = grid_df[test_idx][features]\ngrid_df['sw_sales'][test_idx] = estimator.predict(test_data)\n\ndel test_data\ngc.collect()\n\n# sw_sales -> sales\ngrid_df['sales'][test_idx] = grid_df['sw_sales'][test_idx] / grid_df['sw'][test_idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The final output 'sales' accuracy is bound above at the bottom level.\n# However, there are some items with weight=0.\n# Some item has sales values of infinity......\ngrid_df['sales'][test_idx].max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At the bottom level we don't have to care this infinity, but for upper aggregated level, we must predict these zero-weight items sales if we take bottom-up approach.\n\nIn that case, we need another model which predicts or complements these zero-weight items sales.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}