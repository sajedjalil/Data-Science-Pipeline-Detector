{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"importing libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"calendar = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/calendar.csv\")\nsell_prices = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sell_prices.csv\")\nsales_train = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv\")\nsample_sub = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sample_submission.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def memory_reduction(dataset):\n    column_types = dataset.dtypes\n    temp = None\n    for x in range(len(column_types)):\n        column_types[x] = str(column_types[x])\n    for x in range(len(column_types)):\n        temp = dataset.columns[x]\n        if dataset.columns[x] == \"date\":\n            dataset[temp] = dataset[temp].astype(\"datetime64\")\n        if column_types[x] == \"int64\" and dataset.columns[x] != \"date\":\n            dataset[temp] = dataset[temp].astype(\"int16\")\n        if column_types[x] == \"object\" and dataset.columns[x] != \"date\":\n            dataset[temp] = dataset[temp].astype(\"category\")\n        if column_types[x] == \"float64\" and dataset.columns[x] != \"date\":\n            dataset[temp] = dataset[temp].astype(\"float16\")\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Frist priority is to reduce the memory of the data sets ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar = memory_reduction(calendar)\nsell_prices = memory_reduction(sell_prices)\nsales_train = memory_reduction(sales_train)\nsample_sub = memory_reduction(sample_sub)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA WRANGLING","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"lets start with CALENDAR","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#we arent removing data for the moment like we did previously \n# Next we need to transform our dates to usable or model efficient formats\n# For that we will be creating a day column and week num columns and reducing there memory\ncalendar[\"day\"] = pd.DatetimeIndex(calendar[\"date\"]).day\ncalendar[\"day\"] = calendar[\"day\"].astype(\"int8\")\ncalendar[\"week_num\"] = (calendar[\"day\"] - 1) // 7 + 1\ncalendar[\"week_num\"] = calendar[\"week_num\"].astype(\"int8\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now lets see it \ncalendar.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we have to add a category named missing in order to omit the NaN values as we have described object data type to category data type\ncalendar[\"event_name_1\"] = calendar[\"event_name_1\"].cat.add_categories('missing')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n# we are droping date\n# we are stripping d_ values to int\n# changing Nan values to mising in events \n# and also integer encoding the values of catagorical variable\ndef prep_calendar(df):\n    df = df.drop([\"date\", \"weekday\"], axis=1)\n    df = df.assign(d = df.d.str[2:].astype(int))\n    df[\"event_type_1\"] = df[\"event_type_1\"].cat.add_categories('missing')\n    df[\"event_name_2\"] = df[\"event_name_2\"].cat.add_categories('missing')\n    df[\"event_type_2\"] = df[\"event_type_2\"].cat.add_categories('missing')\n    \n    df = df.fillna(\"missing\")\n    cols = list(set(df.columns) - {\"wm_yr_wk\", \"d\"})\n    df[cols] = OrdinalEncoder(dtype=\"int\").fit_transform(df[cols])\n    df = memory_reduction(df)\n    return df\n\ncalendar = prep_calendar(calendar)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will be configuring SALES data ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#this is basically changing sales data from wide to long format \n\n\ndef reshape_sales(df, drop_d = None):\n    if drop_d is not None:\n        df = df.drop([\"d_\" + str(i + 1) for i in range(drop_d)], axis=1)\n    df = df.assign(id=df.id.str.replace(\"_validation\", \"\"))\n    df = df.reindex(columns=df.columns.tolist() + [\"d_\" + str(1913 + i + 1) for i in range(2 * 28)])\n    df = df.melt(id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n                 var_name='d', value_name='demand')\n    df = df.assign(d=df.d.str[2:].astype(\"int16\"))\n    return df\n\nsales_train = reshape_sales(sales_train, 488)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are creating embeddings for catagorcal variables rather than using dummies **please change it if you want too **","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"for that we are using ordinal encoder from sklearn package","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we are calculating rolling mean and standard deviation\ndef prep_sales(df):\n    #this is shifting the data by 28\n    df['lag_t28'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28))\n    #rolling mean window 7\n    df['rolling_mean_t7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).mean())\n    #rolling mean window 15\n    df['rolling_mean_t15'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(15).mean())\n    #rolling mean window 30\n    df['rolling_mean_t30'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).mean())\n    #rolling std window 7\n    df['rolling_std_t7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).std())\n    #rolling mean window 30\n    df['rolling_std_t30'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).std())\n\n    # Remove rows with NAs except for submission rows. rolling_mean_t180 was selected as it produces most missings\n    df = df[(df.d >= 1914) | (pd.notna(df.rolling_mean_t30))]\n    df = memory_reduction(df)\n\n    return df\n\nsales_train = prep_sales(sales_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NOW lets configure our sellin price ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we have only calculated rolling mean, std , cummulative relation btween them \n\ndef prep_selling_prices(df):\n    \n    gr = df.groupby([\"store_id\", \"item_id\"])[\"sell_price\"]\n    df[\"sell_price_rel_diff\"] = gr.pct_change()\n    df[\"sell_price_rel_diff\"] = df[\"sell_price_rel_diff\"].astype(\"float16\")\n    df[\"sell_price_rel_diff\"] = df[\"sell_price_rel_diff\"].fillna(0) \n    df[\"sell_price_roll_sd7\"] = gr.transform(lambda x: x.rolling(7).std())\n    df[\"sell_price_roll_sd7\"] = df[\"sell_price_roll_sd7\"].astype(\"float16\")\n    df[\"sell_price_roll_sd7\"] = df[\"sell_price_roll_sd7\"].fillna(0)\n    df[\"sell_price_cumrel\"] = (gr.shift(0) - gr.cummin()) / (1 + gr.cummax() - gr.cummin())\n    df[\"sell_price_cumrel\"] = df[\"sell_price_cumrel\"].astype(\"float16\")\n    df[\"sell_price_cumrel\"] = df[\"sell_price_cumrel\"].fillna(0)\n    df = memory_reduction(df)\n    return df\n\nsell_prices = prep_selling_prices(sell_prices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"COMBINING DATA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#merging with calendar\nsales_train = sales_train.merge(calendar, how=\"left\", on=\"d\")\ngc.collect()\nsales_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train = sales_train.merge(sell_prices, how=\"left\", on=[\"wm_yr_wk\", \"store_id\", \"item_id\"])\nsales_train.drop([\"wm_yr_wk\"], axis=1, inplace=True)\ngc.collect()\nsales_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del sell_prices\ngc.collect()\nsales_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# After this is Data Transformation for NN purpose if you need Other wise skip to Heading ML Model Trainng","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ncat_id_cols = [\"item_id\", \"dept_id\", \"store_id\", \"cat_id\", \"state_id\"]\ncat_cols = cat_id_cols + [\"wday\", \"month\", \"year\", \"event_name_1\", \n                          \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n# if you want to check the progress of it use \n# from tqdm import tqdm\n# for i, v in tqdm(enumerate(cat_id_cols)):\n# In loop to minimize memory use\nfor i, v in enumerate(cat_id_cols):\n    sales_train[v] = OrdinalEncoder(dtype=\"int\").fit_transform(sales_train[[v]])\n\nsales_train = memory_reduction(sales_train)\ngc.collect()\nsales_train.head()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nnum_cols = [\"sell_price\", \"sell_price_rel_diff\", \"sell_price_roll_sd7\", \"sell_price_cumrel\",\n            \"lag_t28\", \"rolling_mean_t7\", \"rolling_mean_t15\", \"rolling_mean_t30\", \n            \"rolling_std_t7\", \"rolling_std_t30\"]\nbool_cols = [\"snap_CA\", \"snap_TX\", \"snap_WI\"]\ndense_cols = num_cols + bool_cols\n\n# Need to do column by column due to memory constraints\nfor i, v in enumerate(num_cols):\n    sales_train[v] = sales_train[v].fillna(sales_train[v].median())\n    \nsales_train.head()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ntest = sales_train[sales_train.d >= 1914]\ntest[\"id\"] = test[\"id\"].astype(\"str\")\ntest = test.assign(id=test.id + \"_\" + np.where(test.d <= 1941, \"validation\", \"evaluation\"),\n                   F=\"F\" + (test.d - 1913 - 28 * (test.d > 1941)).astype(\"str\"))\ntest.head()\ngc.collect()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n# Input dict for training with a dense array and separate inputs for each embedding input\ndef make_X(df):\n    X = {\"dense1\": df[dense_cols].to_numpy()}\n    for i, v in enumerate(cat_cols):\n        X[v] = df[[v]].to_numpy()\n    return X\n\n# Submission data\nX_test = make_X(test)\n\n# One month of validation data\nflag = (sales_train.d < 1914) & (sales_train.d >= 1914 - 28)\nvalid = (make_X(sales_train[flag]),\n         sales_train[\"demand\"][flag])\n\n# Rest is used for training\nflag = sales_train.d < 1914 - 28\nX_train = make_X(sales_train[flag])\ny_train = sales_train[\"demand\"][flag]\n                             \ndel sales_train, flag\ngc.collect()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ngc.collect()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ny_train.head()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ML MODEL BUILDING","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## DATA SPLIT AND PREP FOR ML MODEL","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets first redue the data to d 1913 for train and rest for the purpose of test \n\ntrain = sales_train[sales_train['d'] <= 1913]\ntest = sales_train[sales_train['d'] >= 1913]\ntest = sales_train[sales_train['d'] >= 1941]\ndel sales_train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#new memory reduction\ndef memory_reduction(dataset):\n    column_types = dataset.dtypes\n    temp = None\n    for x in range(len(column_types)):\n        column_types[x] = str(column_types[x])\n    for x in range(len(column_types)):\n        temp = dataset.columns[x]\n        if dataset.columns[x] == \"date\":\n            dataset[temp] = dataset[temp].astype(\"datetime64\")\n        if column_types[x] == \"int16\" and dataset.columns[x] != \"date\":\n            dataset[temp] = dataset[temp].astype(\"int8\")\n        if column_types[x] == \"object\" and dataset.columns[x] != \"date\":\n            dataset[temp] = dataset[temp].astype(\"category\")\n        if column_types[x] == \"float16\" and dataset.columns[x] != \"date\":\n            dataset[temp] = dataset[temp].astype(\"float16\")\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"train = memory_reduction(train)\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categories to countinous columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving ids for future \ntrain_ids = train['id']\ntest_ids = test['id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#droping id for training purpose\ntrain = train.drop('id', axis = 1)\ntest = test.drop('id', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['item_id' ,'dept_id', 'cat_id', 'store_id', 'state_id']\node = OrdinalEncoder()\ntrain = ode.fit_transform(train)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating X and y columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## First a regression model to check that every thing works fine and a model can be created\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lreg = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lreg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}