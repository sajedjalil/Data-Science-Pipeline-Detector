{"cells":[{"metadata":{},"cell_type":"markdown","source":"# M5 Forecasting Challenge\n<img src=\"\" width=\"500\" height=\"300\" />\n\nThe Makridakis Competitions (also known as the M Competitions or M-Competitions) are a series of open competitions organized by teams led by forecasting researcher Spyros Makridakis and intended to evaluate and compare the accuracy of different forecasting methods.\n\nM5, the latest of the M Competitions, will run from March 2, to June 30, 2020. It will use real-life data from Walmart and will be run on Kaggle’s Platform. It will offer substantial prizes totalling $100,000 to the winners. The data is provided by Walmart and consist of around 100,000 hierarchical daily time series, starting at the level of SKUs and ending with the total demand of some large geographical area. In addition to the sales data, there is also be information about prices, advertising/promotional activity and inventory levels as well as the day of the week the data refers to.\n\nThis notebook is a quick overview of the 2020 M5 competition. \n\n- This competition has two components: **The Accuracy Competition** and ** The Uncertainty Competition**\n    - The accuracy competition will use the metric: **Weighted Root Mean Squared Scaled Error** (WRMSSE) weighted across all 42840 hierarchical time series\n    - The uncertainty competition will use the metric: **Weighted Scaled Pinball Loss** (WSPL)\n    \n** The Aim :** identify the most appropriate method(s) for different types of situations requiring predictions and making uncertainty estimates. Its ultimate purpose is to advance the theory of forecasting and improve its utilization by business and non-profit organizations. Its other goal is to compare the accuracy/uncertainty of ML and DL methods vis-à-vis those of standard statistical ones, and assess possible improvements versus the extra complexity and higher costs of using the various methods.\n\n**The goal :**predict sales data provided by Walmart for 28 days into the future. \n\n**The data :**  We are working with 42,840 hierarchical time series. The data were obtained in the 3 US states of California (CA), Texas (TX), and Wisconsin (WI). “Hierarchical” here means that data can be aggregated on different levels: item level, department level, product category level, and state level. The sales information reaches back from Jan 2011 to June 2016. In addition to the sales numbers, we are also given corresponding data on prices, promotions, and holidays. Note, that we have been warned that most of the time series contain zero values.\n\n\n**Evaluation**\nThe number of forecasts required, is h=28 days (4 weeks ahead). The performance measures are first computed for each series separately by averaging their values across the forecasting horizon and then averaged again across the series in a weighted fashion to obtain the final scores.\n\n**Weighting**\nIn contrast to the previous M competition, M5 involves the unit sales of various products of different selling volumes and prices that are organized in a hierarchical fashion. This means that, businesswise, in order for a method to perform well, it must provide accurate forecasts across all hierarchical levels. The forecasting error (WRMSSE) will be weighted across the M5 series based on their ** cumulative actual dollar sales **"},{"metadata":{},"cell_type":"markdown","source":"### All time series\n\n| Level id |       Aggregation Level        | Number of series |\n| --- | --- | --- |\n| 1 | Unit sales of all products, aggregated for all stores/states | 1 |\n| 2 | Unit sales of all products, aggregated for each State | 3 |\n| 3 | Unit sales of all products, aggregated for each store | 10 |\n| 4 | Unit sales of all products, aggregated for each category | 3 |\n| 5 | Unit sales of all products, aggregated for each department | 7 |\n| 6 | Unit sales of all products, aggregated for each State and category | 9 |\n| 7 | Unit sales of all products, aggregated for each State and department | 21 |\n| 8 | Unit sales of all products, aggregated for each store and category | 30 |\n| 9 | Unit sales of all products, aggregated for each store and department | 70 |\n| 10 | Unit sales of product x, aggregated for all stores/states | 3049 |\n| 11 |Unit sales of product x, aggregated for each State | 9147 |\n| 12 | Unit sales of product x, aggregated for each store | 30490 |\n|  Total |  | 42840 |"},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"from pandas.plotting import scatter_matrix, andrews_curves, autocorrelation_plot, lag_plot\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.ar_model import AutoReg, ar_select_order\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.api import acf, pacf, graphics\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.robust import mad\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\nimport plotly.offline as py\nimport plotly.express as px\nimport plotly.tools as tls\nimport cufflinks as cf\nimport plotly\nfrom IPython.display import HTML\nimport matplotlib.pyplot as plt\nfrom scipy.signal import butter\nfrom itertools import cycle\nfrom scipy import signal\nimport seaborn as sns\nimport pandas as pd \nimport numpy as np\nimport warnings\nimport os\nimport gc\n\npd.set_option('max_columns', 100)\npd.set_option('max_rows', 50)\n\ncf.go_offline()\npy.init_notebook_mode()\ncf.getThemes()\n\ncf.set_config_file(theme='space')\nwarnings.simplefilter('ignore')\npd.plotting.register_matplotlib_converters()\nsns.mpl.rc('figure',figsize=(16, 6))\nsns.set_style('whitegrid')\nsns.set_context('poster')\npalette = sns.color_palette('mako_r', 6)\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (16,6)\nplt.rcParams['axes.titlesize'] = 16","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Files\n- `calendar.csv` - Contains information about the dates on which the products are sold.\n- `sales_train_validation.csv` - Contains the historical daily unit sales data per product and store [d_1 - d_1913]\n- `sample_submission.csv` - The correct format for submissions. Reference the Evaluation tab for more info.\n- `sell_prices.csv` - Contains information about the price of the products sold per store and date.\n- `sales_train_evaluation.csv` - Available one month before competition deadline. Will include sales [d_1 - d_1941]"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"base = os.path.abspath('/kaggle/input/m5-forecasting-accuracy/')\nsell_prices = pd.read_csv(os.path.join(base + '/sell_prices.csv'))\ncalendar  = pd.read_csv(os.path.join(base + '/calendar.csv'))\nsales_train_validation  = pd.read_csv(os.path.join(base + '/sales_train_validation.csv'))\nsubmission_file = pd.read_csv(os.path.join(base + '/sample_submission.csv'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(f'Shape of Data Files: \\n\\n calendar        = {calendar.shape} \\n sales_train     = {sales_train_validation.shape} \\n sell_prices     = {sell_prices.shape} \\n submission_file = {submission_file.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** `sell_prices.csv` ** Contains information about the price of the products sold per store and date.\n- `store_id` : The id of the store where the product is sold.\n- `item_id` : The id of the product.\n- `wm_yr_wk` : The id of the week.\n- `sell_price`: The price of the product for the given week/store. The price is provided per week (average across seven days). If not available, this means that the product was not sold during the examined week.\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"sell_prices.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sell_prices.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sell_prices.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Price Change for six random Item')\nitems_sample = sell_prices.item_id.sample(6, random_state=2020)\nfor df, s in sell_prices[sell_prices.item_id.isin(items_sample)].groupby(['item_id']):\n    s.reset_index()['sell_price'].pct_change().iplot(theme='space', margin=(10, 10, 10, 30) ,dimensions=(800,150), xTitle='percent', yTitle='Days', title = f'price change of an item', mode='lines+markers', size=0.5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Descriptive Statistics for Item prices')\nsell_prices.groupby(['item_id'])['sell_price'].agg(['min', 'max', 'mean', 'count']).head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Descriptive Statistics for Item prices by Store')\nsell_prices.groupby(['store_id','item_id'])['sell_price'].agg(['min', 'max', 'mean', 'count']).head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`calendar.csv` : Contains information about the dates the products are sold.\n\n- `date` : The date in a “y-m-d” format.\n- `wm_yr_wk` : The id of the week the date belongs to.\n- `weekday` : The type of the day (Saturday, Sunday, …, Friday).\n- `wday`: The id of the weekday, starting from Saturday..\n- `month` : The month of the date.\n- `year` : The year of the date.\n- `event_name_1` : If the date includes an event, the name of this event.\n- `event_type_1` : If the date includes an event, the type of this event.\n- `event_name_2` : If the date includes a second event, the name of this event.\n- `event_type_2` : If the date includes a second event, the type of this event.  \n- `snap_CA, snap_TX, and snap_WI` : A binary variable (0 or 1) indicating whether the stores of CA, TX or WI allow SNAP2 purchases on the examined date. 1 indicates that SNAP purchases are allowed.\n "},{"metadata":{"trusted":false},"cell_type":"code","source":"calendar.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"calendar.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"calendar.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`sales_train_validation.csv` : Contains the historical daily unit sales data per product and store.\n\n- `item_id` : The id of the product.\n- `dept_id` : The id of the department the product belongs to.\n- `cat_id` : The id of the category the product belongs to.\n- `store_id`: The id of the store where the product is sold.\n- `state_id` : The State where the store is located.\n- `d_1, d_2, …, d_i, … d_1941` : The number of units sold at day i, starting from 2011-01-29.\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"sales_train_validation.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sales_train_validation.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sales_train_validation.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(f'Number of unique States      : {sales_train_validation.state_id.nunique()}')\nprint(f'Number of unique Stores      : {sales_train_validation.store_id.nunique()}')\nprint(f'Number of unique Categories  : {sales_train_validation.cat_id.nunique()  }')\nprint(f'Number of unique Items       : {sales_train_validation.item_id.nunique()}')\nprint(f'Number of unique Sale Prices : {sales_train_validation.id.nunique()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(f'Unique States      : {sales_train_validation.state_id.unique()}')\nprint(f'Unique Stores      : {sales_train_validation.store_id.unique()}')\nprint(f'Unique Categories  : {sales_train_validation.cat_id.unique()  }')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(f'Number of unique Items per Store :')\nsales_train_validation.groupby('store_id')['item_id'].agg(['count'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"days = [c for c in sales_train_validation.columns if 'd_' in c]\nprint(f'Number of Days in Validation Data : {len(days)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(f'Minimum of Max sales in ONE Day : {sales_train_validation.loc[:,days].T.max().min()} \\nMaximum of Max sales in ONE Day : {sales_train_validation.loc[:,days].T.max().max()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(f'Maximum of Min sales in ONE Day : {sales_train_validation.loc[:,days].T.min().max()} \\nMinimum of Min sales in ONE Day : {sales_train_validation.loc[:,days].T.min().min()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x, y = np.unique(sales_train_validation.loc[:,days].values.ravel(), return_counts=True)\ncounts = pd.DataFrame(y, index=x, columns=['Items Sold']).reset_index()\ncounts.columns = ['Items Sold', 'Day Count']\nprint('Number of Days with Count of each Items sold aggregated by Items')\ncounts.T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Let's look at Different Sale amounts agregated by Different Variables*"},{"metadata":{"trusted":false},"cell_type":"code","source":"sales_sum_by_store = sales_train_validation.groupby(['store_id']).sum().T.reset_index(drop = True)\nsales_mean_by_store = sales_train_validation.groupby(['store_id']).mean().T.reset_index(drop = True) \n\nprint('Sales aggregated by different Stores')\nsales_sum_by_store.iplot(kind='box',  margin=(10, 10, 10, 40) ,dimensions=(900,500), title = 'Total Sales by Store ID', xTitle = 'Store ID', yTitle = 'Total Sales')\nsales_mean_by_store.iplot(kind='box',  margin=(10, 10, 10, 40) ,dimensions=(900,500), title = 'Average Sales by Store ID', xTitle = 'Store ID', yTitle = 'Average Sales')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sales_sum_by_state = sales_train_validation.groupby(['state_id']).sum().T.reset_index(drop = True)\nsales_mean_by_state = sales_train_validation.groupby(['state_id']).mean().T.reset_index(drop = True) \n\nprint('Sales aggregated by different States')\nsales_sum_by_state.iplot(kind='box', margin=(10, 10, 10, 40) ,dimensions=(900,500), title = 'Total Sales by State', xTitle = 'State', yTitle = 'Total Sales')\nsales_mean_by_state.iplot(kind='box',  margin=(10, 10, 10, 40) ,dimensions=(900,500), title = 'Average Sales by State', xTitle = 'State', yTitle = 'Average Sales')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sales_sum_by_cat = sales_train_validation.groupby(['cat_id']).sum().T.reset_index(drop = True)\nsales_mean_by_cat = sales_train_validation.groupby(['cat_id']).mean().T.reset_index(drop = True) \n\nprint('Sales aggregated by different Categories')\nsales_sum_by_cat.iplot(kind='box', margin=(10, 10, 10, 40) ,dimensions=(900,500), title = 'Total Sales by Category', xTitle = 'Category', yTitle = 'Total Sales')\nsales_mean_by_cat.iplot(kind='box',  margin=(10, 10, 10, 40) ,dimensions=(900,500), title = 'Average Sales by Category', xTitle = 'Category', yTitle = 'Average Sales')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"department_mean_sale = pd.DataFrame(sales_train_validation.groupby(['dept_id']).mean().mean()).reset_index(drop=True).reset_index()\ndepartment_mean_sale.columns = ['Days', 'Sale']\nfig = px.scatter(department_mean_sale, x = 'Days', y = 'Sale',color='Sale', trendline='lowess', template='plotly_dark',\n                title='Average Total Sales of Item Per Day Over Time')\nfig.update_layout(\n    margin=dict(l=10, r=10, t=40, b=20),\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"days_dates = calendar.set_index('d')\ndays_item = sales_train_validation.loc[sales_train_validation['id'] == 'FOODS_3_825_WI_3_validation'].set_index('id')[days].T\nitem = days_item.merge(days_dates, left_index=True, right_index=True).set_index('date')\nitem.rename(columns = ({'FOODS_3_825_WI_3_validation' : 'item_'}), inplace=True)\nitem.iloc[:, 0].iplot(theme='space', margin=(10, 10, 10, 40) ,dimensions=(800,300), title = 'Sales of an item', xTitle='Year')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"samples = sales_train_validation.loc[:, days].sample(6, random_state=2020).T\nsamples = days_dates.merge(samples, left_index=True, right_index=True).set_index('date')\nsamples.iloc[:, -6:].iplot(theme='space', margin=(10, 10, 10, 40) ,dimensions=(800,500), title = 'Sales of different items', xTitle='Year')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"samples = sales_train_validation.loc[:, days].sample(6, random_state=2020).T.rolling(28).mean().fillna(0)\nsamples = days_dates.merge(samples, left_index=True, right_index=True).set_index('date')\nsamples.iloc[:, -6:].iplot(theme='space', margin=(10, 10, 10, 40) ,dimensions=(800,500), title = 'Rolling mean of Sales Window : 28', xTitle='Year')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"samples = sales_train_validation.loc[:, days].sample(6, random_state=2020).T.rolling(28).std().fillna(0)\nsamples = days_dates.merge(samples, left_index=True, right_index=True).set_index('date')\nsamples.iloc[:, -6:].iplot(theme='space', margin=(10, 10, 10, 40) ,dimensions=(800,500), title = 'Rolling std of Sales Window : 28', xTitle='Year')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"samples = sales_train_validation.loc[:, days].sample(6, random_state=2020).T.rolling(28).median().fillna(0)\nsamples = days_dates.merge(samples, left_index=True, right_index=True).set_index('date')\nsamples.iloc[:, -6:].iplot(theme='space', margin=(10, 10, 10, 40) ,dimensions=(800,500), title = 'Rolling median of Sales Window : 28', xTitle='Year')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"items = sales_train_validation['id'].sample(6, random_state=2020)\ndays_item = sales_train_validation.loc[sales_train_validation['id'].isin(items)].set_index('id')[days].T\nitems = days_item.merge(days_dates, left_index=True, right_index=True).set_index('date')\nitems.rename(columns = ({'HOUSEHOLD_2_026_CA_2_validation' : 'item_0',\n                        'FOODS_3_135_TX_1_validation' :      'item_1',\n                        'HOUSEHOLD_1_301_WI_1_validation' :  'item_2',\n                        'HOBBIES_2_001_WI_3_validation' :    'item_3',\n                        'HOUSEHOLD_1_371_WI_3_validation' :  'item_4',\n                        'HOUSEHOLD_2_238_WI_3_validation' :  'item_5',\n                       }), inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"items.groupby('wday').mean().iloc[:, :6].iplot(theme='space', margin=(10, 10, 10, 40) ,dimensions=(800,400), title = 'Average Sales by week day', xTitle='WeekDays', mode='lines+markers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"items.groupby('month').mean().iloc[:, :6].iplot(theme='space', margin=(10, 10, 10, 40) ,dimensions=(800,500), title = 'Average Sales by month', xTitle='Month', mode='lines+markers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"items.groupby('year').mean().iloc[:, :6].iplot(theme='space', margin=(10, 10, 10, 40) ,dimensions=(800,500), title = 'Average Sales by year', xTitle='Year', mode='lines+markers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sales_train_validation['sum'] = sales_train_validation.sum(axis=1, numeric_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sales_train_validation.groupby('dept_id')['sum'].mean().iplot(theme='space', color='gray', margin=(10, 10, 10, 40) ,dimensions=(800,300), title = 'Average Sales by Department', xTitle='Department', mode='lines+markers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sales_train_validation.groupby('store_id')['sum'].mean().iplot(theme='space', color='gray', margin=(10, 10, 10, 40) ,dimensions=(800,300), title = 'Average Sales by Store', xTitle='Store', mode='lines+markers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sales_train_validation.groupby('item_id')['sum'].mean().sample(frac=0.01, random_state=2020).iplot(theme='space', color='gray', margin=(10, 10, 10, 40) ,dimensions=(800,300), title='Average Sales By Item 0.01 of Data', xTitle='Item', mode='lines+markers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sales_train_validation.groupby('cat_id')['sum'].mean().iplot(theme='space', color='gray', margin=(10, 10, 10, 40) ,dimensions=(800,300), title='Average Sales By Category', xTitle='Category', mode='lines+markers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sales_train_validation.groupby('state_id')['sum'].mean().iplot(theme='space', color='gray',  kind='bar', margin=(10, 10, 10, 40) ,dimensions=(500,300), xTitle='Average By State', mode='lines+markers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"day_sums = pd.DataFrame(sales_train_validation.sum(axis=0, numeric_only=True), columns=['sum'])\ndays_ = days_dates.merge(day_sums, left_index=True, right_index=True).set_index('date')\ndays_['sum'].iplot(theme='space', color='gray', margin=(10, 10, 10, 40) ,dimensions=(800,500), title='Sum of Sales', xTitle='Year',  mode='lines')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"days_.groupby('wday')['sum'].agg(['mean']).iplot(theme='space', color='gray', margin=(10, 10, 10, 40) ,dimensions=(800,300), title='Sum of Sales By week day', xTitle='WeekDay', mode='lines+markers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"days_.groupby('month')['sum'].agg(['mean']).iplot(theme='space', color='gray', margin=(10, 10, 10, 40) ,dimensions=(800,500), title='Sum of Sales By month', xTitle='Month', mode='lines+markers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"days_[days_.event_name_1.notna()].groupby('event_name_1')['sum'].agg(['mean']).iplot(theme='space', color='gray', margin=(10, 10, 10, 40) ,dimensions=(800,500), title='Sum of Sales By Event Days', xTitle='Event', mode='lines+markers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"days_[days_.event_name_1.notna()].groupby('event_type_1')['sum'].agg(['mean']).iplot(theme='space', color='gray', kind = 'bar', margin=(10, 10, 10, 40) ,dimensions=(800,300),  title='Sum of Sales By Event Type', xTitle='Event Type', mode='lines+markers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"days_[days_.event_name_1.notna()].groupby('event_name_2')['sum'].agg(['mean']).iplot(theme='space', color='gray', kind = 'bar', margin=(10, 10, 10, 40) ,dimensions=(600,300),  title='Sum of Sales By Event Days', xTitle='Event', mode='lines+markers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"days_[days_.event_name_1.notna()].groupby('event_type_2')['sum'].agg(['mean']).iplot(theme='space', color='gray', kind = 'bar', margin=(10, 10, 10, 40) ,dimensions=(300,300), title='Sum of Sales By Event Type', xTitle='Event Type', mode='lines+markers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"days_.groupby('snap_CA')['sum'].agg(['mean']).iplot(theme='space', color='gray', kind = 'bar', margin=(10, 10, 10, 40) ,dimensions=(300,300), title='Sales By snap_CA', mode='lines+markers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"days_.groupby('snap_TX')['sum'].agg(['mean']).iplot(theme='space', color='gray', kind = 'bar', margin=(10, 10, 10, 40) ,dimensions=(300,300), title='Sales By snap_TX', mode='lines+markers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"days_.groupby('snap_WI')['sum'].agg(['mean']).iplot(theme='space', color='gray', kind = 'bar', margin=(10, 10, 10, 40) ,dimensions=(300,300), title='Sales By snap_WI', mode='lines+markers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_sample = sales_train_validation.loc[:,days].T\ndist = (data_sample==0).sum()\nhist_data = [dist]\ngroup_labels = ['Zero Sale distribution']\nfig = ff.create_distplot(hist_data, group_labels)\nfig.update_layout(autosize=False, width=800, height=500, margin=dict(l=10, r=10, b=10, t=40))\nfig.update_layout(template='plotly_dark', title_text='Zero Sale Days distribution')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_sample = sales_train_validation.loc[:,days].T\ndist = (data_sample==1).sum()\nhist_data = [dist]\ngroup_labels = ['One Sale distribution']\nfig = ff.create_distplot(hist_data, group_labels)\nfig.update_layout(autosize=False, width=800, height=500, margin=dict(l=10, r=10, b=10, t=40))\nfig.update_layout(template='plotly_dark', title_text='One Sale Days distribution')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_sample = sales_train_validation.loc[:,days].T\ndist = (data_sample==2).sum()\nhist_data = [dist]\ngroup_labels = ['Two Sale distribution']\nfig = ff.create_distplot(hist_data, group_labels)\nfig.update_layout(autosize=False, width=800, height=500, margin=dict(l=10, r=10, b=10, t=40))\nfig.update_layout(template='plotly_dark', title_text='Two Sale Days distribution')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_sample = sales_train_validation.loc[:,days].T\ndist = (data_sample==3).sum()\nhist_data = [dist]\ngroup_labels = ['Three Sale distribution']\nfig = ff.create_distplot(hist_data, group_labels)\nfig.update_layout(autosize=False, width=800, height=500, margin=dict(l=10, r=10, b=10, t=40))\nfig.update_layout(template='plotly_dark', title_text='Three Sale Days distribution')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### andrews curves for six random Items"},{"metadata":{"trusted":false},"cell_type":"code","source":"itx = items.T.iloc[:5].reset_index()\nplt.figure()\nax = andrews_curves(itx, class_column='index', colormap='cubehelix')\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### autocorrelation plot for a single random item"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure()\nax = autocorrelation_plot(items.T.iloc[1])\nax.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### lag plot for a single random item"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure()\nax = lag_plot(items.T.iloc[1])\nax.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"weeks_per_year = 260 #five year period\nplt.rcParams['figure.figsize'] = (16,12)\ntime_series = sales_sum_by_store[\"CA_1\"]\nsdc = seasonal_decompose(time_series, period = weeks_per_year)\nsdcs = pd.DataFrame(sdc.seasonal).reset_index()\nsdcs.columns = ['Days', 'Seasonal']\npx.scatter(sdcs, x='Days', y='Seasonal', width=800, height=400, color='Seasonal', trendline='lowess', template='plotly_dark',   title='Seasonal Component')\nfig.update_layout(\n    margin=dict(l=10, r=10, t=40, b=20),\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****"},{"metadata":{},"cell_type":"markdown","source":"**Croston Model :**\n\n Croston is a technique to forecast products with intermittent demand. The idea behind Croston method can be summarized in three simple steps:\n- Evaluate the average demand level when there is a demand occurrence.\n- Evaluate the average time between two demand occurrences.\n- Forecast the demand as the demand level (when there is an occurrence) multiplied by the probability to have an occurrence."},{"metadata":{"trusted":false},"cell_type":"code","source":"#https://medium.com/analytics-vidhya/croston-forecast-model-for-intermittent-demand-360287a17f5f\ndef Croston(ts,extra_periods=1,alpha=0.4):\n    d = np.array(ts) # Transform the input into a numpy array\n    cols = len(d) # Historical period length\n    d = np.append(d,[np.nan]*extra_periods) # Append np.nan into the demand array to cover future periods\n    \n    #level (a), periodicity(p) and forecast (f)\n    a,p,f = np.full((3,cols+extra_periods),np.nan)\n    q = 1 #periods since last demand observation\n    \n    # Initialization\n    first_occurence = np.argmax(d[:cols]>0)\n    a[0] = d[first_occurence]\n    p[0] = 1 + first_occurence\n    f[0] = a[0]/p[0]\n    # Create all the t+1 forecasts\n    for t in range(0,cols):        \n        if d[t] > 0:\n            a[t+1] = alpha*d[t] + (1-alpha)*a[t] \n            p[t+1] = alpha*q + (1-alpha)*p[t]\n            f[t+1] = a[t+1]/p[t+1]\n            q = 1           \n        else:\n            a[t+1] = a[t]\n            p[t+1] = p[t]\n            f[t+1] = f[t]\n            q += 1\n       \n    # Future Forecast \n    a[cols+1:cols+extra_periods] = a[cols]\n    p[cols+1:cols+extra_periods] = p[cols]\n    f[cols+1:cols+extra_periods] = f[cols]\n                      \n    df = pd.DataFrame.from_dict({\"Demand\":d,\"Forecast\":f,\"Period\":p,\"Level\":a,\"Error\":d-f})\n    return df\n\n\ndef Croston_TSB(ts,extra_periods=1,alpha=0.4,beta=0.4):\n    d = np.array(ts) # Transform the input into a numpy array\n    cols = len(d) # Historical period length\n    d = np.append(d,[np.nan]*extra_periods) # Append np.nan into the demand array to cover future periods\n    \n    #level (a), probability(p) and forecast (f)\n    a,p,f = np.full((3,cols+extra_periods),np.nan)\n    # Initialization\n    first_occurence = np.argmax(d[:cols]>0)\n    a[0] = d[first_occurence]\n    p[0] = 1/(1 + first_occurence)\n    f[0] = p[0]*a[0]\n                 \n    # Create all the t+1 forecasts\n    for t in range(0,cols): \n        if d[t] > 0:\n            a[t+1] = alpha*d[t] + (1-alpha)*a[t] \n            p[t+1] = beta*(1) + (1-beta)*p[t]  \n        else:\n            a[t+1] = a[t]\n            p[t+1] = (1-beta)*p[t]       \n        f[t+1] = p[t+1]*a[t+1]\n        \n    # Future Forecast\n    a[cols+1:cols+extra_periods] = a[cols]\n    p[cols+1:cols+extra_periods] = p[cols]\n    f[cols+1:cols+extra_periods] = f[cols]\n                      \n    df = pd.DataFrame.from_dict({\"Demand\":d,\"Forecast\":f,\"Period\":p,\"Level\":a,\"Error\":d-f})\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"days = range(1, 1913 + 1)\ntime_series_columns = [f'd_{i}' for i in days]\ntime_series_data = sales_train_validation[time_series_columns]\n\nforecast_ = time_series_data.apply(lambda x : Croston_TSB(x, extra_periods=28, alpha=0.05,beta=0.31)['Forecast'].tail(28), axis=1)\n\ncols = ['F'+str(i+1) for i in range(28)]\nforecast_.columns = cols\n\nvalidation_ids = sales_train_validation['id'].values\nevaluation_ids = [i.replace('validation', 'evaluation') for i in validation_ids]\nids = np.concatenate([validation_ids, evaluation_ids])\npredictions = pd.DataFrame(ids, columns=['id'])\nforecast = pd.concat([forecast_] * 2).reset_index(drop=True)\npredictions = pd.concat([predictions, forecast], axis=1)\npredictions.to_csv('submission.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}