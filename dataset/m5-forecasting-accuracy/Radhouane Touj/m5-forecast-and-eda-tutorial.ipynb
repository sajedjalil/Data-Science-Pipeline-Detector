{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport numpy as np\nfrom matplotlib.gridspec import GridSpec\nfrom fbprophet import Prophet\nfrom fbprophet.plot import plot_plotly, plot_components_plotly, add_changepoints_to_plot\nfrom sklearn.metrics import mean_squared_error\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport statsmodels.api as sm\nimport calendar\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.io as pio\npio.templates[\"draft\"] = go.layout.Template(\n    layout_annotations=[\n        dict(\n            textangle=-30,\n            opacity=0.1,\n            font=dict(color=\"black\", size=100),\n            xref=\"paper\",\n            yref=\"paper\",\n            x=0.5,\n            y=0.5,\n            showarrow=False,\n        )\n    ]\n)\npio.templates.default = \"draft\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import plotly.offline as py\n# py.init_notebook_mode(connected=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', 2000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- `calendar.csv` - Contains information about the dates on which the products are sold.\n- `sales_train_validation.csv` - Contains the historical daily unit sales data per product and store [d_1 - d_1913]\n- `sample_submission.csv` - The correct format for submissions. Reference the Evaluation tab for more info.\n- `sell_prices.csv` - Contains information about the price of the products sold per store and date.\n- `sales_train_evaluation.csv` - Includes sales [d_1 - d_1941] (labels used for the Public leaderboard)"},{"metadata":{},"cell_type":"markdown","source":"# 1. Read Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar_data = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\ncalendar_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_validation = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv')\nsales_train_validation.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_evaluation = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv')\nsales_train_evaluation.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"<i>`sales_train_validation` Dataset is our train data set: [D1 - D1913].</i> <br>\n<i>`sales_train_evalutaion` Dataset is data used to evaluate our models, it contains [D1914 - D1941].</i>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = sales_train_validation.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1 General Information"},{"metadata":{"trusted":true},"cell_type":"code","source":"states = train_data.state_id.unique().tolist()\nprint(f\"States Present in The Dataset: {states} ({len(states)})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stores = train_data.store_id.unique().tolist()\nprint(f\"Stores Present in The Dataset: {stores} ({len(stores)})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories = train_data.cat_id.unique().tolist()\nprint(f\"Categories Present in The Dataset: {categories} ({len(categories)})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items = train_data.dept_id.unique().tolist()\nprint(f\"Items Present in The Dataset: {items} ({len(items)})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"There are {len(train_data.item_id.unique())} Items in The Dataset\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Total Numner of Time Series: {len(train_data.id.unique())} !\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Example of Time Series"},{"metadata":{"trusted":true},"cell_type":"code","source":"d_cols = train_data.columns.tolist()[6:] # Sales columns\nnon_d_cols = list(reversed(train_data.columns.tolist()[:6])) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.loc[train_data.d_100 == train_data.d_100.max()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take the example of item <b>FOODS_3_586</b> sales in California store <b>TX_3</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def merge_with_calendar(data, calendar_data):\n    # data should have a date column \"d\"\n    assert 'd' in data.columns, 'DataFrame should have a column \"d\" !'\n    # Merge With Calendar\n    cal = calendar_data[['d', 'date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year',\n                    'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']]\n    d = pd.merge(cal, data, on=\"d\")\n    # Fill Missing Event Values with None\n    for col in ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']:\n        d[col].fillna('None', inplace=True)\n    return d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_ts_example(data, d_cols, calendar_data, item_id, store_id, idx=None):\n    try:\n        if idx is None:\n            ts = data.loc[(data['item_id'] == item_id) & (data['store_id'] == store_id)]\n            ts = ts[d_cols].T.reset_index()\n            ts.columns = ['d', 'sales']\n        else:\n            ts = data.loc[idx][d_cols].reset_index()\n            ts.columns = ['d', 'sales']\n        # Make sure that sales column's type is int\n        ts[\"sales\"] = ts[\"sales\"].astype(\"int\")\n        return merge_with_calendar(ts, calendar_data)\n    except Exception as e:\n        print(f'Can not extract time series: {e}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = get_ts_example(train_data, d_cols, calendar_data, item_id='FOODS_3_586', store_id='TX_3')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Length of the time series: {len(ts)} days.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Years in the TS: {ts.year.unique()}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ts['event_type_2'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<i>From 1913 days there are only 4 days with type 2 event! As a result we'll ignore that event type.</i>"},{"metadata":{},"cell_type":"markdown","source":"<b> Plots </b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"events_1_data = ts['event_type_1'].value_counts().iloc[1:]\nfig = plt.figure(figsize=(11, 5))\nax = fig.add_subplot()\nax.pie(x=events_1_data.values,\n       labels=events_1_data.index,\n       shadow=True,\n       radius=1,\n       autopct='%1.1f%%')\nax.set_title('Distribution of Type 1 Events')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20, 6))\nax = fig.add_subplot()\nax.plot(ts.date, ts.sales)\nax.set_xticks(ts.date.values[::90])\nax.set_xlabel('Date')\nax.set_ylabel('Sales')\nax.grid()\nax.set_title(f'Sales of FOODS_3_586 in TX_3 Store')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Area Plot\nfig = px.area(ts, \n              x='date',\n              y='sales',\n              title='Time Series Yearly Area Plot',\n              facet_row='year',\n              facet_row_spacing=0.05)\nfig.update_layout(width=900,\n                 height=900)\n             \nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for year in ts.year.unique():\n    t = ts[ts.year == year]\n    fig = plt.figure(figsize=(20, 6))\n    ax = fig.add_subplot()\n    t_event_1 = t.loc[t.event_type_1 != 'None']\n    ax.plot(t.date, t.sales)\n    ax.scatter(t_event_1.date,\n               t_event_1.sales,\n               color='red',\n               label='Type 1 Event')\n    ax.set_xticks(t.date.values[::30])\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.grid()\n    ax.set_title(f'Sales of FOODS_3_586 in TX_3 store for {year}')\n    ax.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20, 15))\nax1 = fig.add_subplot(311)\nsns.boxplot(data=ts, x='month', y='sales', ax=ax1)\nax2 = fig.add_subplot(312)\nsns.boxplot(data=ts, x='weekday', y='sales', ax=ax2)\nax3 = fig.add_subplot(313)\nsns.boxplot(data=ts, x='event_type_1', y='sales', ax=ax3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- A simple observation: from the first plot we can see that the biggest part of sales of this item (<i>FOODS_3_586</i>) in this store (<i>TX_3</i>) is during Month 8 (August).<br>\n- This very detailed level (<i>Item level</i>) won't generate many insights, aggregated levels will do such as <i>State</i>, <i>Store</i>, <i>Category</i> and <i>Department</i> levels."},{"metadata":{},"cell_type":"markdown","source":"Plotly offers some interesting plots and visualizations ! Let's try some of them at our time series."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(ts,\n                   x='sales',\n                   marginal='box',\n                   title='Sales Distribution for FOODS_3_586 at TX_3 store')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(data_frame=ts, \n              x='date', \n              y='sales', \n              color='year', \n              title='Sales of FOODS_3_586 at TX_3 Store',)\nfig.update_layout(legend=dict(x=1,\n                              y=1,\n                              title_font_family=\"Times New Roman\",\n                              bgcolor=\"snow\",\n                              bordercolor=\"Black\",\n                              borderwidth=1),\n                  font=dict(size=11)\n                 )\n# plot and legend are Interactive !\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"An interesting pattern to observe is that, for this item example, sales decrease to the lowest level at the end of each year !"},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in ['wday', 'month', 'year', 'event_name_1']: #wday = weekday\n    data_feature = ts.groupby(feature).mean()['sales'].reset_index().sort_values(by='sales')\n    fig = px.bar(data_frame=data_feature,\n          x=feature,\n          y='sales',\n          title=f'Average Sales by {feature}')\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can easily notice that the item's Average sale is at its maximum at Father's Day."},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Aggregated Level Analysis"},{"metadata":{},"cell_type":"markdown","source":"- Now we'll go through more aggregated analysis."},{"metadata":{},"cell_type":"markdown","source":" - The Data hierarchy is:<br>\n <b>State</b> ==>  <b>Store</b> ==>  <b>Category</b> ==>  <b>Department</b> ==>  <b>Item</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"d = train_data.groupby(['state_id','store_id']).count().reset_index()\nd['state_id'].value_counts().plot(kind=\"bar\", grid=True, title=\"(A) Number of Stores by State\", yticks=[0,1,2,3,4])\nplt.show()\nd = train_data.groupby(['state_id','id']).count().reset_index()\nd['state_id'].value_counts().plot(kind=\"bar\", grid=True, title=\"(B) Number of Items by State\")\nplt.show()\nd = train_data.groupby(['store_id','item_id']).count().reset_index()\nd['store_id'].value_counts().plot(kind=\"bar\", grid=True, title=\" (C) Number of Items by Store\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We will use <b>pandas.DataFrame.stack</b> function to transform our data.\n- The result data should have as columns <b>['d', 'state_id','store_id','cat_id','dept_id','item_id', 'id', 'sales']</b> where \"d\" column has values in <br>[d_1,..., d_1913].\n- Initially we have data with 10 stores and 3049 items per store so as a result we have 30490 time series !\n- Each time series contains sales data of 1913 days. Final output data will have then 8 columns and 30490*1913 = 58327370 rows !"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform Data Structure\ndata = train_data.set_index(non_d_cols)\n# the following will make one column for sales and one columns for \"d\" values (d_1 ... d_1913)\ndata = data.stack()\ndata = data.to_frame() \ndata.columns = [\"sales\"]\ndata.reset_index(inplace=True)\ndata.columns = non_d_cols + [\"d\", \"sales\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<i>The followoing are function that will be used for different analysis.</i>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_daily_data(df,level):\n    levels_dict = {'cat':'Category', 'dept':'Department', 'store':'Store', 'state':'State'}\n    fig = px.line(data_frame=df, \n                  x='date', \n                  y='sales', \n                  color=f'{level}_id', \n                  title=f'Sales by {levels_dict[level]}')\n    fig.update_layout(legend=dict(x=1,\n                                  y=1,\n                                  title_font_family=\"Times New Roman\",\n                                  bgcolor=\"snow\",\n                                  bordercolor=\"Black\",\n                                  borderwidth=1),\n                      font=dict(size=11)\n                     )\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_yearly_data(df,level):\n    levels_dict = {'cat':'Category', 'dept':'Department', 'store':'Store', 'state':'State'}\n    n_years = df.year.nunique()\n    years = df.year.unique()\n    level_elements = df[f'{level}_id'].unique().tolist()  \n    all_colors= ['red','green','blue','purple','cyan','orange','pink','yellow','black','magenta']\n    colors = all_colors[:len(level_elements)]\n    fig = plt.figure(figsize=(15,15))\n    gs = GridSpec(n_years, len(level_elements))\n    c_idx = 0\n    for l_e, color in zip(level_elements,colors):\n        r_idx = 0\n        for year in years:\n            ax = fig.add_subplot(gs[r_idx,c_idx], xticks=[], yticks=[])\n            df1 = df.loc[(df.year == year) & (df[f'{level}_id'] == l_e)]\n            ax.plot(df1.date, df1['sales'], color=color, linewidth=0.9)\n            ax.set_title(f'{l_e}: {year}')\n            r_idx += 1\n        c_idx+=1\n    fig.suptitle(f'Yearly Sales by {levels_dict[level]}')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_average_sales(df,level):\n    for feature in ['weekday', 'month', 'year', 'event_name_1']:\n        data_feature = df.groupby([f'{level}_id', feature]).mean()['sales'].reset_index()\n        fig = px.bar(data_frame=data_feature,\n              x=feature,\n              y='sales',\n              color=f'{level}_id',\n              title=f'Average Sales by {feature}')\n        fig .update_layout(legend=dict(x=1,\n                                       y=1,\n                                       title_font_family=\"Times New Roman\",\n                                       bgcolor=\"mintcream\",\n                                       bordercolor=\"black\",\n                                       borderwidth=1))\n        fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>For plotly plots, you can double click on legend to visualize data parts separately. You can also zoom in, zoom out and autoscale plots.</b>"},{"metadata":{},"cell_type":"markdown","source":"### 2.3.1 State Level Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing state-level data\nstate_data = data.groupby([\"state_id\",\"d\"]).sum()[\"sales\"]\nstate_data = state_data.reset_index()\nstate_data = merge_with_calendar(state_data, calendar_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_daily_data(state_data,'state')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(state_data,\n                   x='sales',\n                   color='state_id',\n                   marginal='box',\n                   title='Sales Distribution By State')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- California has the highest number of sales.\n- We observe again the sales decrease to their lowest level (previously observed with a single item time series) at the end of every year, let's try to get more details about this pattern !"},{"metadata":{"trusted":true},"cell_type":"code","source":"dlow = state_data.loc[(state_data['sales']<20) & (state_data['month']==12)]\ndlow.style.applymap(lambda x:\"background-color:yellow\", subset=['event_name_1'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> It was Christmas effect !</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_yearly_data(state_data,'state')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_average_sales(state_data,'state')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Sales average is increasing over years.\n- Weekends correspond to the highest sales average.\n- Sales average is almost the same over different months."},{"metadata":{},"cell_type":"markdown","source":"### 2.3.2 Store Level Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing store-level data\nstore_data = data.groupby([\"store_id\",\"d\"]).sum()[\"sales\"]\nstore_data = store_data.reset_index()\nstore_data = merge_with_calendar(store_data, calendar_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"store_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_daily_data(store_data,'store')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(store_data,\n                   x='sales',\n                   color='store_id',\n                   marginal='box',\n                   title='Sales Distribution By Store')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- CA_3 is the store having the highest number of sales."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_yearly_data(store_data,'store')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_average_sales(store_data,'store')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Let's see Sales' correlations between different Stores."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_data = pd.pivot_table(data=store_data,\n                           index='date',\n                           values='sales',\n                           columns='store_id')\ncorr_data.sort_values(by=\"date\", inplace=True)\nplt.figure(figsize=(12,5))\nheatmap = sns.heatmap(corr_data.corr(), annot=True, fmt='.2f')\nheatmap.set_yticklabels(heatmap.yaxis.get_ticklabels(), rotation=0)\nheatmap.set_title('Correlation Between Stores Sales')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The highest correlation is between CA_1 and CA_3 stores, in the same state.\n- The lowest correlation is between WI_1 and WI_3 stores, in the same state!"},{"metadata":{},"cell_type":"markdown","source":"### 2.3.3 Catgeory Level Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.groupby('cat_id').count()['id'].reset_index().plot(x='cat_id', \n                                                              kind='bar', \n                                                              figsize=(15,5),\n                                                              grid=True,\n                                                              title='Number of Items by Category')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing category-level data\ncat_data = data.groupby([\"cat_id\",\"d\"]).sum()[\"sales\"]\ncat_data = cat_data.reset_index()\ncat_data = merge_with_calendar(cat_data, calendar_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_daily_data(cat_data,'cat')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- FOODS is the category having the highest number of sales, HOBBIES having the lowest one."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(cat_data,\n                   x='sales',\n                   color='cat_id',\n                   marginal='box',\n                   title='Sales Distribution By Category')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_yearly_data(cat_data,'cat')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_average_sales(cat_data,'cat')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot a Sales Calendar Heatmap of the first and last years (by Categroy)."},{"metadata":{"trusted":true},"cell_type":"code","source":"years = cat_data.year.unique()\nfor year in [years[0],years[-1]]:\n    for cat in cat_data.cat_id.unique():\n        dyear = cat_data.loc[(cat_data.year == year) & (cat_data.cat_id == cat)] # & (cat_data.cat_id == cat)\n        fig = go.Figure(data=go.Heatmap(\n                z=dyear.sales, #z,\n                x=dyear.date, #dates,\n                y=dyear.cat_id, #programmers,\n                colorscale=px.colors.sequential.Plasma_r))\n\n        fig.update_layout(\n            title=f'{cat} Sales {year}',\n            xaxis_nticks=36)\n\n        fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.4 Department Level Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.groupby('dept_id').count()['id'].reset_index().plot(x='dept_id', \n                                                              kind='bar', \n                                                              figsize=(15,5),\n                                                              grid=True,\n                                                              title='Number of Items by Department')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing department-level data\ndept_data = data.groupby([\"dept_id\",\"d\"]).sum()[\"sales\"]\ndept_data = dept_data.reset_index()\ndept_data = merge_with_calendar(dept_data, calendar_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dept_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_daily_data(dept_data,'dept')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- FOODS_3 and HOBBIES_2 have respectively the highest and lowest number of sales among all departments."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(dept_data,\n                   x='sales',\n                   color='dept_id',\n                   marginal='box',\n                   title='Sales Distribution By Department')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_yearly_data(dept_data,'dept')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_average_sales(dept_data,'dept')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_data = pd.pivot_table(data=dept_data,\n                           index='date',\n                           values='sales',\n                           columns='dept_id')\ncorr_data.sort_values(by=\"date\", inplace=True)\nplt.figure(figsize=(12,5))\nheatmap = sns.heatmap(corr_data.corr(), annot=True, fmt='.2f')\nheatmap.set_yticklabels(heatmap.yaxis.get_ticklabels(), rotation=0)\nheatmap.set_title('Correlation Between Department Sales')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After the analyzing and visualizing part comes the forecast part  !"},{"metadata":{},"cell_type":"markdown","source":"# 3. Forecast"},{"metadata":{},"cell_type":"markdown","source":"- In this part, we'll choose 6 Time Series from the 30490 ones we have and use them to test different forecasting approaches and evaluate them."},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast_horizon = 28 # from d_1914 to d_1941\nd_fcst_columns = sales_train_evaluation.columns[-forecast_horizon:].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_ground_truth(idx, df, d_fcst_columns):\n    return df.loc[idx, d_fcst_columns].values  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_results(fcst, y_eval, rmse, algo, item):\n    fig = plt.figure(figsize=(11, 5))\n    ax = fig.add_subplot()\n    ax.plot(fcst, color='red', label='Forecast')\n    ax.plot(y_eval, color='blue', label='Ground Truth')\n    ax.set_title(f' {algo} for {item}, RMSE: {rmse}')\n    ax.grid()\n    ax.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Time Series Selection"},{"metadata":{},"cell_type":"markdown","source":"We will choose:\n- 3 Time Series with enough data and few zeros.\n- 3 Time Series with many zeros."},{"metadata":{"trusted":true},"cell_type":"code","source":"choice_data = train_data.copy()\nchoice_data['d_val'] = choice_data[d_cols].mean(axis=1)\nchoice_data.drop(columns=d_cols,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Time Series Averages: Min {choice_data.d_val.min()} Max:{choice_data.d_val.max()} Median: {choice_data.d_val.median()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx1 = choice_data.loc[choice_data['d_val'] >= 50].sample(n=3, random_state=1).index\nidx2 = choice_data.loc[(choice_data['d_val'] <= 5) & (choice_data['d_val'] > 1)].sample(n=3, random_state=1).index\nidx = idx1.tolist() + idx2.tolist()\nts_test = train_data.iloc[idx]#.reset_index(drop=True)\ntest_items = ts_test.id.unique().tolist()\ntest_items = [x[:-11] for x in test_items]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_items","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataframe for RMSE\nrmse_summary = pd.DataFrame({\"items\":test_items}, index=idx)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Statistical Method: ARIMA"},{"metadata":{},"cell_type":"markdown","source":"<b>ARIMA</b> stands for <b>A</b>uto<b>R</b>egressive <b>I</b>ntegrated <b>M</b>oving <b>A</b>verage."},{"metadata":{},"cell_type":"markdown","source":"Types:\n- <b>ARIMA</b>: Non-Seasonal. \n- <b>SARIMA</b>: Seasonal ARIMA.\n- <b>SARIMAX</b>: Seasonal ARIMA with eXogenous variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"def choose_sarimax_order_and_forecast(ps,ds,qs, y_train, y_eval):\n    best_model, best_rmse, best_order, best_fcst = None, None, None, None\n    for p in ps:\n        for d in ds:\n            for q in qs:\n                order = (p,d,q)\n                model = sm.tsa.SARIMAX(y_train, \n                               order=order, \n                               trend='c',\n                               enforce_invertibility=False,\n                               enforce_stationarity=False).fit(disp=False, warn_convergence=False)\n                fcst = model.predict(start=len(y_train), end=len(y_train) - 1 + len(y_eval))\n                try:\n                    fcst = [round(x) for x in fcst]\n                    rmse = round(np.sqrt(mean_squared_error(fcst, y_eval)), 3)\n                    if (best_rmse is None) or (rmse < best_rmse):\n                        best_model, best_rmse, best_order, best_forecast= model, rmse, order, fcst\n                except Exception as e:\n                    print(f'For order={order}, model results are invalid: {e}')\n    print(f\"Best Order: {best_order}\")\n    return best_rmse, best_forecast","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df0 = get_ts_example(ts_test, d_cols, calendar_data, item_id=None, store_id=None, idx=idx[0])\ndf0['date'] = df0['date'].apply(lambda x : pd.to_datetime(x))\ndf0 = df0[['date','sales']]\ny_train = df0[\"sales\"].values\ndf0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = seasonal_decompose(y_train, model='additive', period=365)\nfig = result.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2,1,figsize=(20,7))\nsm.tsa.graphics.plot_acf(y_train, lags=30, ax=ax[0])\nax[0].set_title('Autocorreation Function: lags=30')\nsm.tsa.graphics.plot_pacf(y_train, lags=30, ax=ax[1])\nax[1].set_title('Partial Autocorreation Function: lags=30')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"order = (p,d,q)\n- p: AutoRegression (AR) order.\n- d: Trend Differncing order.\n- q: Moving Average (MA) order. <br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting Autocorrelation with pandas\nfig, ax = plt.subplots(1,1,figsize=(20,7))\npd.plotting.autocorrelation_plot(y_train, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sarimax_model = sm.tsa.SARIMAX(y_train, \n                               order=(7,1,7), \n                               trend='c',\n                               enforce_invertibility=False,\n                               enforce_stationarity=False).fit(disp=False, warn_convergence=False)\nsarimax_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_eval = get_ground_truth(idx[0], sales_train_evaluation, d_fcst_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fcst = sarimax_model.predict(start=len(y_train), end=len(y_train) - 1 + len(y_eval))\nrmse = round(np.sqrt(mean_squared_error(fcst, y_eval)), 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_results(fcst, y_eval, rmse, \"SARIMAX\", test_items[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SARIMAX Parameters Grid\nps = range(1,8)\nds = range(0,2)\nqs = range(0,8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"rmse_sarimax = []\nfor i,ix in enumerate(idx):\n    print(f\"Processing {test_items[i]}...\")\n    # Get Time Series (Train)\n    df0 = get_ts_example(ts_test, d_cols, calendar_data, item_id=None, store_id=None, idx=ix)\n    df0['date'] = df0['date'].apply(lambda x : pd.to_datetime(x))\n    df0 = df0[['date','sales']]\n    y_train = df0[\"sales\"].values\n    y_eval = get_ground_truth(ix, sales_train_evaluation, d_fcst_columns)\n    # Train SARIMAX model\n    rmse, fcst = choose_sarimax_order_and_forecast(ps,ds,qs, y_train, y_eval)\n    # Plot\n    plot_results(fcst, y_eval, rmse, \"SARIMAX\", test_items[i])\n    rmse_sarimax.append(rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataframe for RMSE\nrmse_summary = pd.DataFrame({\"items\":test_items}, index=idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_summary[\"RMSE_SARIMAX\"] = rmse_sarimax\nrmse_summary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.3 FB Prophet"},{"metadata":{},"cell_type":"markdown","source":"FB Prophet requires a column 'ds' (for dates) and a columns 'y' (target variable)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_holidays(calendar_data, dates):\n    holidays = calendar_data.loc[calendar_data['d'].isin(dates), ['date','event_name_1']].dropna()\n    holidays['ds'] = holidays['date'].apply(lambda x : pd.to_datetime(x))\n    holidays['upper_window'] = 0\n    holidays['lower_window'] = 0\n    holidays.rename(columns={\"event_name_1\":\"holiday\"}, inplace=True)\n    holidays.drop(columns='date', inplace=True)\n    holidays.reset_index(drop=True, inplace=True)\n    return holidays","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# holidays for FB prophet model\nholidays = generate_holidays(calendar_data, d_cols+d_fcst_columns)\nholidays","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = get_ts_example(ts_test, d_cols, calendar_data, item_id=None, store_id=None, idx=idx[0])\ndf['ds'] = df['date'].apply(lambda x : pd.to_datetime(x))\ndf = df[['ds','sales']]\ndf = df.rename(columns={'sales':'y'})\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['cap'] = df['y'].max()\ndf['floor'] = df['y'].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating Model\nmodel = Prophet(daily_seasonality=True, \n                holidays=holidays,\n                holidays_prior_scale=0.2,\n                growth='logistic', # possible value: 'logistic', 'linear' or 'flat'\n                changepoint_range=0.8, # default value\n                changepoint_prior_scale=0.2) # default value\nmodel.add_seasonality(name='weekly', period=7, fourier_order=3)\nmodel.add_seasonality(name='yearly', period=364, fourier_order=10)\nmodel.fit(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Forecasting\nfuture = model.make_future_dataframe(periods=forecast_horizon)\nfuture['cap'] = df['y'].max()\nfuture['floor'] = df['y'].min()\nforecast = model.predict(future)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1 = model.plot(forecast)\na = add_changepoints_to_plot(fig1.gca(), model, forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Interactive plots\nplot_plotly(model, forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Components\nplot_components_plotly(model, forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fcst = forecast['yhat'].values[-forecast_horizon:]\nfcst = [int(x) for x in fcst]\nground_truth = get_ground_truth(idx[0], sales_train_evaluation, d_fcst_columns)\nrmse = round(np.sqrt(mean_squared_error(ground_truth, fcst)), 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(11, 5))\nax = fig.add_subplot()\nax.plot(fcst, color='red', label='Forecast')\nax.plot(ground_truth, color='blue', label='Ground Truth')\nax.set_title(f'FB Prophet for {test_items[i]}, RMSE: {rmse}')\nax.grid()\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FB Prophet Parameters Grid\nchangepoint_prior_scale = [0.0001, 0.001, 0.1, 0.5]\nseasonality_prior_scale = [0.01, 0.1, 1, 10]\nholiday_prior_scale = [0.1, 0.2, 0.5, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tune_fbprophet_and_forecast(y_eval, df, forecast_horizon, chngp, sps, hps):\n    best_rmse, best_fcst = None, None\n    # Choosing the Best Parameters\n    for p1 in chngp:\n        for p2 in sps:\n            for p3 in hps:\n                # Training Model\n                model = Prophet(daily_seasonality=True, \n                                holidays=holidays,\n                                holidays_prior_scale=p3,\n                                seasonality_prior_scale=p2,\n                                growth='logistic', \n                                changepoint_range=0.8, \n                                changepoint_prior_scale=p1)\n                model.add_seasonality(name='weekly', period=7, fourier_order=3)\n                model.add_seasonality(name='yearly', period=364, fourier_order=10)\n                model.fit(df)\n                # Forecasting\n                future = model.make_future_dataframe(periods=forecast_horizon)\n                future['cap'] = df['y'].max()\n                future['floor'] = df['y'].min()\n                forecast = model.predict(future)\n                # Evaluating\n                fcst = forecast['yhat'].values[-forecast_horizon:]\n                fcst = [int(x) for x in fcst]\n                rmse = round(np.sqrt(mean_squared_error(y_eval, fcst)), 3)\n                if (best_rmse is None) or (rmse < best_rmse):\n                    best_rmse, best_fcst = rmse, fcst\n    return best_rmse, best_fcst","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_fbprophet = []\nfor i,ix in enumerate(idx):\n    print(f\"Processing {test_items[i]}...\")\n    # Get Time Series (Train)\n    df = get_ts_example(ts_test, d_cols, calendar_data, item_id=None, store_id=None, idx=ix)\n    df['ds'] = df['date'].apply(lambda x : pd.to_datetime(x))\n    df = df[['ds','sales']]\n    df = df.rename(columns={'sales':'y'})\n    # Add Cap and Floor Columns\n    df['cap'] = df['y'].max()\n    df['floor'] = df['y'].min()\n    # Get Ground Truth\n    y_eval = get_ground_truth(ix, sales_train_evaluation, d_fcst_columns)\n    # Train FBProphet model\n    rmse, fcst = tune_fbprophet_and_forecast(y_eval, df, forecast_horizon, changepoint_prior_scale, seasonality_prior_scale, holiday_prior_scale)\n    # Plot\n    plot_results(fcst, y_eval, rmse, \"FB Prophet\", test_items[i])\n    rmse_fbprophet.append(rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_summary[\"RMSE_FBPROPHET\"] = rmse_fbprophet\nrmse_summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r1 = round(rmse_summary.RMSE_FBPROPHET.mean(), 3)\nr2 = round(rmse_summary.RMSE_SARIMAX.mean(), 3)\nprint(f\"MEAN RMSE SARIMAX: {r2}, FBProphet: {r1}\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}