{"cells":[{"metadata":{},"cell_type":"markdown","source":"> # M5 Statistical Benchmarks with Python classes"},{"metadata":{},"cell_type":"markdown","source":"This notebook provides some of the **statistical benchmark models** proposed by **M5 organizers** (for more details about these models and for more general information on the M5 competition, please refer to the [M5 Competitors Guide](https://mk0mcompetitiont8ake.kinstacdn.com/wp-content/uploads/2020/02/M5-Competitors-Guide_Final-1.pdf)).\nBonus part: a benchmark using facebook prophet has also been provided.\n\nAlthaugh more efficient packages already exists for some the following models, the aim of this notebook is to present how we can easily **implement these benchmark models from scratch** so as to better **understand how they work**.\nMoreover, I decided to use simple Python classes for each one of the model for making the code more modular.\n\nA final submission file is created by averaging the predictions of the top 2 (with respect to WRMSSE on the validation set) benchmark models.\n\nIf you found the notebook useful, please upvote it ;-)\n\nIf you have any remarks/questions, do not hesitate to comment, I'll be more than happy to discuss with you."},{"metadata":{},"cell_type":"markdown","source":"## LOAD LIBRARIES"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom itertools import cycle\nfrom scipy.stats import hmean\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.offline as pyo\nfrom scipy.optimize import minimize_scalar\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing\nimport itertools\nfrom functools import partial\nfrom multiprocessing import Pool\nimport statsmodels.api as sm\nimport warnings\nfrom statsmodels.tsa.api import SimpleExpSmoothing\nfrom scipy.ndimage.interpolation import shift\n\npyo.init_notebook_mode(connected=True)\nimport math\n\nfrom typing import Union\nfrom tqdm.auto import tqdm as tqdm\n\nimport m5_constants as cnt\n\npd.set_option('max_columns', 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    \"\"\"\n    from M5 Forecast: Keras with Categorical Embeddings V2\n    https://www.kaggle.com/mayer79/m5-forecast-keras-with-categorical-embeddings-v2\n    \"\"\"\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'\\\n                      .format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\n\n\ndef load_raw_data():\n    \"\"\"\n    Load raw input data. Paths are in the constant.py file\n    \n    Return: \n     - df_train_val : sales train val data-frame\n     - df_calendar : calendar data-frame\n     - df_price : price data-frame\n     - df_sample_sub : sample submission data-frame\n    \"\"\"\n    df_train_val = pd.read_csv(cnt.SALES_TRAIN_VAL_PATH)\n    df_calendar = pd.read_csv(cnt.CALENDAR_PATH)\n    df_price = pd.read_csv(cnt.SELL_PRICE_PATH)\n    df_sample_sub = pd.read_csv(cnt.SAMPLE_SUBMISSION)\n    \n    df_train_val = reduce_mem_usage(df_train_val)\n    df_calendar = reduce_mem_usage(df_calendar)\n    df_price = reduce_mem_usage(df_price)\n    df_sample_sub = reduce_mem_usage(df_sample_sub)\n    \n    print(\"df_train_val shape: \", df_train_val.shape)\n    print(\"df_calendar shape: \", df_calendar.shape)\n    print(\"df_price shape: \", df_price.shape)\n    print(\"df_sample_sub shape: \", df_sample_sub.shape)\n    \n    return df_train_val, df_calendar, df_price, df_sample_sub\n\ndef split_train_val_sales(df_sales, horizon):\n    \"\"\"\n    train-val split of sales data according to the horizon parameter\n    \"\"\"\n    \n    df_sales_train = df_sales.iloc[:,:-horizon]\n    df_val_item = df_sales[['id','item_id','dept_id','cat_id','store_id','state_id']]\n    df_val_qty = df_sales.iloc[:,-cnt.HORIZON:]\n    df_sales_val = pd.concat([df_val_item, df_val_qty], axis=1)\n    \n    print(\"df_sales_train shape: \", df_sales_train.shape)\n    print(\"df_sales_val shape: \", df_sales_val.shape)\n    \n    return df_sales_train, df_sales_val","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Raw input data\ndf_train_val, df_calendar, df_price, df_sample_sub = load_raw_data()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split Train-Val"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Split train val sales\ndf_train, df_val = split_train_val_sales(df_sales=df_train_val, horizon=cnt.HORIZON)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_val.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_calendar.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_price.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sample_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sample_sub.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_time_series(index, df_train, calendar, df_eval=None, preds=None):\n    \n    df_eval = df_val.copy()\n    id_columns = [i for i in df_train_val.columns if not i.startswith('d_')]\n    d_columns_train =  [i for i in df_train.columns if i.startswith('d_')]\n    if df_eval is not None:\n        d_columns_eval =  [i for i in df_eval.columns if i.startswith('d_')]\n    \n    calendar = calendar[['d', 'date']]\n\n    # Train\n    train_serie = df_train.iloc[[index],:]\n    train_serie = pd.melt(train_serie, id_vars=id_columns,\n                          value_vars=d_columns_train)\n    train_serie.columns = id_columns + ['d', 'sales']\n    train_serie = train_serie.merge(calendar, on='d', how='left')\n    # Eval\n    if df_eval is not None:\n        eval_serie = df_eval.iloc[[index],:]\n        eval_serie = pd.melt(eval_serie, id_vars=id_columns,\n                             value_vars=d_columns_eval)\n        eval_serie.columns = id_columns + ['d', 'sales']\n        eval_serie = eval_serie.merge(calendar, on='d', how='left')\n    # Pred\n    if preds is not None:\n        pred_serie = pd.concat([eval_serie[['date']],\n                                pd.DataFrame(preds[index, :].ravel(),\n                                             columns=['sales'])],\n                               axis=1)\n\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=train_serie.date,\n                             y=train_serie['sales'],\n                             name=\"train\",\n                             line_color='deepskyblue'))\n    if df_eval is not None:\n        fig.add_trace(go.Scatter(x=eval_serie.date,\n                                 y=eval_serie['sales'],\n                                 name=\"eval\",\n                                 line_color='dimgray'))\n    if preds is not None:\n        fig.add_trace(go.Scatter(x=pred_serie.date,\n                                 y=pred_serie['sales'],\n                                 name=\"pred\",\n                                 line_color='darkmagenta'))\n\n    fig.update_layout(title_text='Time Series: '+ df_train.iloc[index,0],\n                      xaxis_rangeslider_visible=True)\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## WRMSSEEvaluator"},{"metadata":{"trusted":true},"cell_type":"code","source":"class WRMSSEEvaluator(object):\n    \n    \"\"\"\n    From WRMSSE Evaluator with extra feature\n    https://www.kaggle.com/dhananjay3/wrmsse-evaluator-with-extra-features\n    \n    \"\"\"\n    \n    group_ids = ( 'all_id', 'state_id', 'store_id', 'cat_id', 'dept_id', 'item_id',\n        ['state_id', 'cat_id'],  ['state_id', 'dept_id'], ['store_id', 'cat_id'],\n        ['store_id', 'dept_id'], ['item_id', 'state_id'], ['item_id', 'store_id'])\n\n    def __init__(self, \n                 train_df: pd.DataFrame, \n                 valid_df: pd.DataFrame, \n                 calendar: pd.DataFrame, \n                 prices: pd.DataFrame):\n        '''\n        intialize and calculate weights\n        '''\n        self.calendar = calendar\n        self.prices = prices\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.train_target_columns = [i for i in self.train_df.columns if i.startswith('d_')]\n        self.weight_columns = self.train_df.iloc[:, -28:].columns.tolist()\n\n        self.train_df['all_id'] = \"all\"\n\n        self.id_columns = [i for i in self.train_df.columns if not i.startswith('d_')]\n        self.valid_target_columns = [i for i in self.valid_df.columns if i.startswith('d_')]\n\n        if not all([c in self.valid_df.columns for c in self.id_columns]):\n            self.valid_df = pd.concat([self.train_df[self.id_columns], self.valid_df],\n                                      axis=1, \n                                      sort=False)\n        self.train_series = self.trans_30490_to_42840(self.train_df, \n                                                      self.train_target_columns, \n                                                      self.group_ids)\n        self.valid_series = self.trans_30490_to_42840(self.valid_df, \n                                                      self.valid_target_columns, \n                                                      self.group_ids)\n        self.weights = self.get_weight_df()\n        self.scale = self.get_scale()\n        self.train_series = None\n        self.train_df = None\n        self.prices = None\n        self.calendar = None\n\n    def get_scale(self):\n        '''\n        scaling factor for each series ignoring starting zeros\n        '''\n        scales = []\n        for i in tqdm(range(len(self.train_series))):\n            series = self.train_series.iloc[i].values\n            series = series[np.argmax(series!=0):]\n            scale = ((series[1:] - series[:-1]) ** 2).mean()\n            scales.append(scale)\n        return np.array(scales)\n    \n    def get_name(self, i):\n        '''\n        convert a str or list of strings to unique string \n        used for naming each of 42840 series\n        '''\n        if type(i) == str or type(i) == int:\n            return str(i)\n        else:\n            return \"--\".join(i)\n    \n    def get_weight_df(self) -> pd.DataFrame:\n        \"\"\"\n        returns weights for each of 42840 series in a dataFrame\n        \"\"\"\n        day_to_week = self.calendar.set_index(\"d\")[\"wm_yr_wk\"].to_dict()\n        weight_df = self.train_df[[\"item_id\", \"store_id\"] + self.weight_columns].set_index(\n            [\"item_id\", \"store_id\"]\n        )\n        weight_df = (\n            weight_df.stack().reset_index().rename(columns={\"level_2\": \"d\", 0: \"value\"})\n        )\n        weight_df[\"wm_yr_wk\"] = weight_df[\"d\"].map(day_to_week)\n        weight_df = weight_df.merge(\n            self.prices, how=\"left\", on=[\"item_id\", \"store_id\", \"wm_yr_wk\"]\n        )\n        weight_df[\"value\"] = weight_df[\"value\"] * weight_df[\"sell_price\"]\n        weight_df = weight_df.set_index([\"item_id\", \"store_id\", \"d\"]).unstack(level=2)[\n            \"value\"\n        ]\n        weight_df = weight_df.loc[\n            zip(self.train_df.item_id, self.train_df.store_id), :\n        ].reset_index(drop=True)\n        weight_df = pd.concat(\n            [self.train_df[self.id_columns], weight_df], axis=1, sort=False\n        )\n        weights_map = {}\n        for i, group_id in enumerate(tqdm(self.group_ids, leave=False)):\n            lv_weight = weight_df.groupby(group_id)[self.weight_columns].sum().sum(axis=1)\n            lv_weight = lv_weight / lv_weight.sum()\n            for i in range(len(lv_weight)):\n                weights_map[self.get_name(lv_weight.index[i])] = np.array(\n                    [lv_weight.iloc[i]]\n                )\n        weights = pd.DataFrame(weights_map).T / len(self.group_ids)\n\n        return weights\n\n    def trans_30490_to_42840(self, df, cols, group_ids, dis=False):\n        '''\n        transform 30490 sries to all 42840 series\n        '''\n        series_map = {}\n        for i, group_id in enumerate(tqdm(self.group_ids, leave=False, disable=dis)):\n            tr = df.groupby(group_id)[cols].sum()\n            for i in range(len(tr)):\n                series_map[self.get_name(tr.index[i])] = tr.iloc[i].values\n        return pd.DataFrame(series_map).T\n    \n    def get_rmsse(self, valid_preds) -> pd.Series:\n        '''\n        returns rmsse scores for all 42840 series\n        '''\n        score = ((self.valid_series - valid_preds) ** 2).mean(axis=1)\n        rmsse = (score / self.scale).map(np.sqrt)\n        return rmsse\n\n    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds],\n                                axis=1, \n                                sort=False)\n        valid_preds = self.trans_30490_to_42840(valid_preds, \n                                                self.valid_target_columns, \n                                                self.group_ids, \n                                                True)\n        self.rmsse = self.get_rmsse(valid_preds)\n        self.contributors = pd.concat([self.weights, self.rmsse], \n                                      axis=1, \n                                      sort=False).prod(axis=1)\n        return np.sum(self.contributors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_columns = [i for i in df_train_val.columns if not i.startswith('d_')]\nd_columns_train =  [i for i in df_train.columns if i.startswith('d_')]\nd_columns_val =  [i for i in df_val.columns if i.startswith('d_')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_fold_df = df_train.copy()\nvalid_fold_df = df_val.copy()\nerror_eval = WRMSSEEvaluator(train_fold_df, valid_fold_df[d_columns_val], df_calendar, df_price)\n\nl = list([train_fold_df, valid_fold_df])\ndel l","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Statistical Benchmarks"},{"metadata":{},"cell_type":"markdown","source":"## Generic Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class M5model(object):\n    \"\"\"\n    Generic class for representing M5 Benchmark statistical models \n    \"\"\"\n    \n    def __init__(self, horizon):\n        \"\"\"\n        horizon : integer, horizon of prediction.\n        \"\"\"\n        self.horizon = horizon\n        \n    def _remove_starting_zeros(self, serie):\n        \"\"\"\n        Remove starting zeros from serie\n        \n        \"\"\"\n        start_index = np.argmax(serie!=0)\n        return serie[start_index:]\n        \n    def predict(self, serie):\n        pass\n    \n        \n    def predict_all(self, df_train):\n        \"\"\"\n        Predict using the Naive (persistence) method on a DataFrame of time series.\n        \n        Parameters\n        ----------\n        df_train : pd.DataFrame, shape (nb_series, ids+d_) d_{i} columns contains sales\n        \n        Returns\n        -------\n        preds : array, shape (nb_series, horizon)\n            Returns predicted values.\n        \"\"\"\n        nb_series = df_train.shape[0]\n        preds = np.zeros((nb_series,self.horizon))\n        d_columns =  [i for i in df_train.columns if i.startswith('d_')]\n\n        for index, row in enumerate(tqdm(df_train[d_columns].itertuples(index=False), total=len(df_train))):\n            row = np.array(row) \n            series = self._remove_starting_zeros(row)\n            preds[index,:] = self.predict(series)\n        return preds\n    \n    def create_submission_file(self, df, file_name):\n        \"\"\"\n        Create submission file with the predictions\n        NB: We double the horizon to take into accoint validation & evaluation forcasts as requested in the submission file\n        \"\"\"\n        \n        single_horizon = self.horizon\n        # double horizon to take into accoint validation & evaluation forcast in the submission file\n        self.horizon = 2 * single_horizon\n        \n        preds = self.predict_all(df)\n        \n        sample_submission = pd.read_csv(cnt.SAMPLE_SUBMISSION)\n        sample_submission.iloc[0:preds.shape[0],1:] = preds[:,0:single_horizon]\n        sample_submission.iloc[-preds.shape[0]:,1:] = preds[:,-single_horizon:]\n\n        sample_submission.to_csv(file_name, index=False, compression='gzip')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive"},{"metadata":{"trusted":true},"cell_type":"code","source":"class M5Naive(M5model):\n    \"\"\"\n    Naive (persistence) method for time series forecasting.\n    Last known value will be persisted.\n    \n    \"\"\"\n\n    def predict(self, serie):\n        \"\"\"\n        Predict using the Naive (persistence) method.\n        \n        Returns\n        -------\n        predictions : array, shape (horizon,)\n            Returns predicted values.\n        \"\"\"\n        last_value = serie[-1]\n        predictions = np.ones(self.horizon) * last_value\n        return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"naive_preds_val = M5Naive(horizon=cnt.HORIZON).predict_all(df_train)\nnaive_error = error_eval.score(naive_preds_val)\nnaive_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"naive_preds_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"naive_preds_val[:,0:cnt.HORIZON].shape, naive_preds_val[:,-cnt.HORIZON:].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_time_series(index=24, df_train=df_train, df_eval=df_val,\n                 preds=naive_preds_val, calendar=df_calendar)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Seasonal Naive"},{"metadata":{"trusted":true},"cell_type":"code","source":"class M5SeasonalNaive(M5model):\n    \"\"\"\n    Seasinal Naive (persistence) method for time series forecasting.\n    Last known values in the given seasonal perdiod (expressed in number of days) will be persisted.\n    \n    \"\"\"    \n    \n    def __init__(self, horizon, seasonal_days):\n        \"\"\"\n        Initialization\n        \n        Parameters\n        ----------\n        horizon : integer, horizon of prediction.\n        seasonal_days: int, number of day determining the series seasonality (ex: 7 for weekly)\n        \"\"\"\n        self.horizon = horizon\n        self.seasonal_days = seasonal_days\n        \n\n    def predict(self, sequence):\n        \n        \"\"\"\n        Predict using the Seasonal Naive method.\n        \n        Returns\n        -------\n        predictions : array, shape (horizon,)\n            Returns predicted values.\n        \"\"\"\n        last_seasonal_values = sequence[-self.seasonal_days:]\n        predictions = np.tile(last_seasonal_values,\n                              math.ceil(self.horizon/self.seasonal_days))[:self.horizon]\n        \n        return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"snaive_preds_val = M5SeasonalNaive(horizon=cnt.HORIZON,\n                                   seasonal_days=7).predict_all(df_train)\nsnaive_error = error_eval.score(snaive_preds_val)\nsnaive_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_time_series(index=24, df_train=df_train, df_eval=df_val,\n                 preds=snaive_preds_val, calendar=df_calendar)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Simple Exponential Smoothing"},{"metadata":{"trusted":true},"cell_type":"code","source":"class M5SimpleExponentialSmoothing(M5model):\n    \"\"\"\n    Simple Exponential Smooting method for time series forecasting.\n    \n    \"\"\"\n    \n    def __init__(self, horizon=1, alpha=.1,\n                 optimized=False, bounds=(0,1),\n                 maxiter=3):\n        \"\"\"\n        Params:\n        ----------\n        \n        horizon : integer, horizon of prediction.\n        alpha : float,Exponential smoothing parameter, range(0,1)\n        optimized: boolean, if True alpha is calculated and optimized automatically\n        bounds: 2D-tuple, (lower_bound, upper_bound) for alpha param\n        maxiter: int, max number of iteration for finding the optimal alpha (the higher the more accurate, but also the slower)\n        \"\"\"\n        self.horizon = horizon\n        self.alpha = alpha\n        self.optimized = optimized\n        self.bounds = bounds\n        self.maxiter = maxiter\n        \n    def _fit(self, ts, alpha):\n        \"\"\"\n        Fit Simple Exponential Smoothing \n        \"\"\"\n        len_ts = len(ts)\n        es = np.zeros(len_ts) # exponential-smoothing array\n        \n        # init\n        es[0] = ts[0]\n        \n        for i in range(1, len_ts):\n            es[i] = alpha * ts[i-1] + (1-alpha)*es[i-1]\n            \n        return es\n    \n    def _mse(self, ts, alpha):\n        \n        es = self._fit(ts, alpha)\n        \n        mse = np.mean(np.square(ts - es))\n        \n        return mse\n    \n    def _best_alpha(self, ts):\n        \"\"\"\n        Calculate best alpha parameter based on MSE\n        \"\"\"\n        \n        res = minimize_scalar(lambda alpha: self._mse(ts, alpha),\n                              bounds=self.bounds,\n                              method='bounded',\n                              options={'xatol': 1e-05,\n                                       'maxiter': self.maxiter}\n                             )\n        \n        return res.x\n    \n    def predict(self, ts):\n        \"\"\"\n        Predict with Simple Exponential Smoothing method\n        \n        Parameters\n        ----------\n        ts : array, time series array\n        \n        Returns\n        -------\n        preds : array, shape (horizon,)\n            Returns predicted values.\n        \"\"\"\n        if self.optimized:\n            alpha = self._best_alpha(ts)\n            self.alpha = alpha\n        \n        len_ts = len(ts)\n        es = np.zeros(len_ts) # exponential-smoothing array\n        \n        # init\n        es[0] = ts[0]\n        \n        for i in range(1, len_ts):\n            es[i] = self.alpha * ts[i] + (1-self.alpha)*es[i-1]\n        \n        preds = np.repeat(es[-1], self.horizon)\n        \n        return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ses_preds_val = M5SimpleExponentialSmoothing(horizon=cnt.HORIZON, alpha=.1,\n                                             optimized=False, bounds=(0.1,.3),\n                                             maxiter = 1).predict_all(df_train)\nses_error = error_eval.score(ses_preds_val)\nses_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_time_series(index=24, df_train=df_train, df_eval=df_val,\n                 preds=ses_preds_val, calendar=df_calendar)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Moving Average"},{"metadata":{"trusted":true},"cell_type":"code","source":"class M5MovingAverage(M5model):\n    \n    \"\"\"\n    Moving Average method for time series forecasting.\n    \n    \"\"\"\n    \n    def __init__(self, horizon, k, optimized=False,\n                 k_lb=2, k_ub=5, last_n_values=None):\n        \"\"\"\n        horizon : integer, horizon of prediction.\n        k : integer, moving average is calculated from the moving last k elements \n        optimized boolean, if True parameter k is calculated and optimized automatically\n        k_lb :  integer, lower bound of k paramter\n        k_ub : integer, upper bound of k paramter\n        last_n_values : int, default None, take last n values of the serie to calculate best k param (to speed up)\n\n        \"\"\"\n        self.horizon = horizon\n        self.k = k\n        self.optimized = optimized\n        self.k_lb = k_lb\n        self.k_ub = k_ub\n        self.last_n_values = last_n_values\n        \n    \n    \n    def calculate_best_k_parameter(self, serie):\n        \"\"\"\n        Calulate the optimal (in terms of mse) paramter k for moving average.\n        Paramter k determines the last k elements of the serie that have to be taken to calculate the (moving) average\n\n        Parameters\n        ----------\n        serie : array, vector containing the serie's values\n        \n        Returns\n        -------\n        best_k : int\n               Returns the best k value selected from the range [k_lb, k_ub] by minimizing the insample MSE.\n        \"\"\"\n        serie = self._remove_starting_zeros(serie)\n        \n        if self.last_n_values is not None:\n            serie = serie[-self.last_n_values:] #reduce serie to its last_n_values\n            \n        serie_len = len(serie)\n        mse = np.zeros(self.k_ub - self.k_lb +1)\n        all_k = list(range(self.k_lb, self.k_ub + 1))\n        \n\n        for ind, k in enumerate(all_k):\n            moving_average_values = np.zeros((serie_len - k))\n            for i in range(k, serie_len):\n                sliding_window = serie[i-k:i]\n                moving_average_values[i-k] = np.average(sliding_window)\n\n            mse[ind] = np.average(np.square(serie[k:] - moving_average_values))\n\n        best_k = all_k[np.argmin(mse)]\n    \n        return best_k\n    \n    def predict(self, serie):\n        \"\"\"\n        Predict using the Moving Average method.\n        \n        Parameters\n        ----------\n        serie : array, vector of serie values\n       \n        Returns\n        -------\n        predictions : array, shape (horizon,)\n            Returns predicted values.\n        \"\"\"\n        \n        if self.optimized:\n            \n            serie_optimized = serie\n            \n            if self.last_n_values is not None:\n                # To speed up calcuation, best k param is calculated from last_n_values\n                serie_optimized = serie_optimized[-self.last_n_values:] \n            best_k = self.calculate_best_k_parameter(serie_optimized)\n            self.k = best_k\n    \n        working_serie = np.concatenate((serie[-self.k:], np.zeros(self.horizon)))\n        for i in range(self.horizon):\n            working_serie[self.k+i] = np.average(working_serie[i:self.k+i])\n        return working_serie[self.k:]\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nma_preds_val = M5MovingAverage(k=3, horizon=cnt.HORIZON, optimized=True,\n                               k_lb=3, k_ub=5, last_n_values=28).predict_all(df_train)\nma_error = error_eval.score(ma_preds_val)\nma_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_time_series(index=24, df_train=df_train, df_eval=df_val,\n                 preds=ma_preds_val, calendar=df_calendar)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Croston"},{"metadata":{"trusted":true},"cell_type":"code","source":"class M5Croston(M5model):\n    \n    def __init__(self, horizon):\n        self.horizon = horizon\n        self.smoothing_level = .1\n        self.optimized = False\n        self.maxiter = 3\n        self.debiasing = 1\n    \n    \n    def _inter_demand_intervals(self, ts):\n        \"\"\"\n        Calculate inter-demand intervals of serie\n        \"\"\"\n        \n        demand_times = np.argwhere(ts>0).ravel() +1\n        a = demand_times - shift(demand_times, 1, cval=0)\n        return a\n    \n    def _positive_demand(self, ts):\n        \n        \"\"\"\n        Calculates non-zero demand (values) of a serie\n        \"\"\"\n        return ts[ts>0]\n    \n    def predict(self, ts):\n        \n        \"\"\"\n        Predict using the Croston method.\n        \n        Parameters\n        ----------\n        ts : array, vector of time-series values\n        horizon : integer, horizon of prediction.\n        \n        Returns\n        -------\n        predictions : array, shape (horizon,)\n            Returns predicted values.\n        \"\"\"\n        \n        ts = np.array(ts)\n        p = self._inter_demand_intervals(ts)\n        a = self._positive_demand(ts)\n        \n        p_est = M5SimpleExponentialSmoothing(horizon=self.horizon,\n                                             alpha=.1,\n                                             optimized=self.optimized,\n                                             bounds=(0.1,.3)).predict(p)\n        a_est = M5SimpleExponentialSmoothing(horizon=self.horizon,\n                                             alpha=.1,\n                                             optimized=self.optimized,\n                                             bounds=(0.1,.3)).predict(a)\n      \n        # Future Forecast\n        future_forecasts = self.debiasing * np.divide(a_est, p_est)\n        \n        return future_forecasts\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncro_preds_val = M5Croston(horizon=cnt.HORIZON).predict_all(df_train)\ncro_error = error_eval.score(cro_preds_val)\ncro_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_time_series(index=24, df_train=df_train, df_eval=df_val,\n                 preds=cro_preds_val, calendar=df_calendar)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Optimized Croston"},{"metadata":{"trusted":true},"cell_type":"code","source":"class M5OptCroston(M5Croston):\n    \"\"\"\n    Optimized Croston model\n    \"\"\"\n    \n    def __init__(self, horizon, maxiter):\n        super().__init__(horizon)\n        self.optimized = True\n        self.maxiter = maxiter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\noptcro_preds_val = M5OptCroston(horizon=cnt.HORIZON,\n                                maxiter=3).predict_all(df_train)\noptcro_error = error_eval.score(optcro_preds_val)\noptcro_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_time_series(index=24, df_train=df_train, df_eval=df_val,\n                 preds=optcro_preds_val, calendar=df_calendar)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Syntetos-Boylan Approximation (SBA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class M5SBA(M5Croston):\n    \"\"\"\n    Syntetos-Boylan Approximation (SBA) model\n    \"\"\"\n    \n    def __init__(self, horizon):\n        super().__init__(horizon)\n        self.debiasing = .95","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nsba_preds_val = M5SBA(horizon=cnt.HORIZON).predict_all(df_train)\nsba_error = error_eval.score(sba_preds_val)\nsba_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_time_series(index=24, df_train=df_train, df_eval=df_val,\n                 preds=sba_preds_val, calendar=df_calendar)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Teunter-Syntetos-Babai method (TSB)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class M5TSB(M5model):\n    \n    \"\"\"\n    Teunter-Syntetos-Babai method (TSB)\n    \n    Inspired by https://medium.com/analytics-vidhya/croston-forecast-model-for-intermittent-demand-360287a17f5f\n    \"\"\"\n    \n    def __init__(self, horizon, alpha, beta):\n        self.horizon = horizon\n        self.alpha = alpha\n        self.beta = beta\n        \n    def predict(self, ts):\n        \n        ts = np.array(ts) # Transform the input into a numpy array\n        len_ts = len(ts) # Historical period length\n    \n        #level (a), probability(p) and forecast (f)\n        a = np.zeros(len_ts+1)\n        p = np.zeros(len_ts+1)\n        f = np.zeros(len_ts+1)\n        \n        first_occurence = np.argmax(ts>0)\n        a[0] = ts[first_occurence]\n        p[0] = 1/(1 + first_occurence)\n        f[0] = p[0]*a[0]\n\n        # Create all the t+1 forecasts\n        for t in range(0,len_ts): \n            if ts[t] > 0:\n                a[t+1] = self.alpha*ts[t] + (1-self.alpha)*a[t] \n                p[t+1] = self.beta*(1) + (1-self.beta)*p[t]  \n            else:\n                a[t+1] = a[t]\n                p[t+1] = (1-self.beta)*p[t]       \n            f[t+1] = p[t+1]*a[t+1]\n\n        # Future Forecast\n        \n        preds = np.repeat(f[-1], self.horizon)\n        \n        return preds\n                      \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntsb_preds_val = M5TSB(horizon=cnt.HORIZON,\n                      alpha=.1, beta=.1).predict_all(df_train)\ntsb_error = error_eval.score(tsb_preds_val)\ntsb_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_time_series(index=24, df_train=df_train, df_eval=df_val,\n                 preds=tsb_preds_val, calendar=df_calendar)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exponential Smoothing"},{"metadata":{"trusted":true},"cell_type":"code","source":"class M5TopDown(object):\n    \"\"\"\n    Base class to model Top-DOwn approach: \n        - predicting top-level time-series and \n        - disaggregating predictions proportional to bottom time-series values\n    \"\"\"\n    \n    def __init__(self, horizon, df_train, df_calendar):\n        self.horizon = horizon\n        self.df_train = df_train\n        self.df_calendar = df_calendar\n        self.top_level = self._get_top_level_timeserie()\n    \n    def _get_top_level_timeserie(self):\n        \"\"\"\n        Calculate top level time series by aggregating low level time-series\n        \"\"\"\n        \n        data = np.sum(self.df_train.iloc[:,6:].values, axis=0)\n        index= pd.date_range(start=self.df_calendar['date'][0],\n                             end=self.df_calendar['date'][len(data)-1],\n                             freq='D')\n        top_level = pd.Series(data, index)\n        return top_level\n    \n    def _get_weights(self):\n        \"\"\"\n        Claculate weights based on the last 28 days for each time series;\n        These weights will be used to disaggregate the top level time-series\n        \"\"\"\n        w = np.sum(self.df_train.iloc[:,-28:].values, axis=1) / sum(self.top_level[-28:])\n        w = w.reshape(len(w),1)\n        return w\n    \n    def _parameters_tuning(self):\n        \"\"\"\n        Tuning hyper-parameters of the model used for predicting the top-level time-series future horizon.\n        The implenetation will depend on the method used (ex: Expontial Smoothign, ARIMA, etc)\n        \"\"\"\n        pass\n    \n    def predict_top_level(self):\n        \"\"\"\n        Predict the future horizon of top level time-series\n        \"\"\"\n        pass\n    \n    def predict_bottom_levels(self):\n        \"\"\"\n        Predict the future horizon of the bottom level time-series by disaggregating the the top level predictions\n        \"\"\"\n        \n        w = self._get_weights()\n        \n        top_level_preds = self.predict_top_level().values\n        top_level_preds = top_level_preds.reshape(1,len(top_level_preds))\n        \n        preds = np.multiply(top_level_preds, w)\n        \n        return preds\n    \n    \n    def create_submission_file(self, file_name):\n        \"\"\"\n        Create submission file with the predictions\n        NB: We double the horizon to take into account validation & evaluation forcasts as requested in the submission file\n        \"\"\"\n        \n        single_horizon = self.horizon\n        # double horizon to take into accoint validation & evaluation forcast in the submission file\n        self.horizon = 2 * single_horizon\n        \n        preds = self.predict_bottom_levels()\n        \n        sample_submission = pd.read_csv(cnt.SAMPLE_SUBMISSION)\n        sample_submission.iloc[0:preds.shape[0],1:] = preds[:,0:single_horizon]\n        sample_submission.iloc[-preds.shape[0]:,1:] = preds[:,-single_horizon:]\n\n        sample_submission.to_csv(file_name, index=False, compression='gzip')\n        \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class M5ExponentialSmoothing(M5TopDown):\n    \"\"\"\n    An algorithm is used to select the most appropriate exponential smoothing model\n    for predicting the top level of the hierarchy (level 1 of Table 1), indicated through information criteria (AIC).\n    \n    The top-down method will be used for obtaining reconciled forecasts\n    at the rest of the hierarchical levels (based on historical proportions, estimated for the last 28 days).\n    \"\"\"\n    \n    def __init__(self, horizon, df_train, df_calendar):\n        self.horizon = horizon\n        self.df_train = df_train\n        self.df_calendar = df_calendar\n        self.top_level = self._get_top_level_timeserie()\n    \n    def _parameters_tuning(self):\n        \n        # prepare param grid\n        trend_param = ['add', 'mul', None]\n        seasonal_param = ['add', 'mul', None]\n        damped_param=[True, False]\n        \n        params = [trend_param, seasonal_param, damped_param]\n        grid_param = list(itertools.product(*params))\n        grid_param = [(trend, seasonal, damped) for trend, seasonal, damped in grid_param if not(trend==None and damped==True)]\n        \n        aic = np.ones(len(grid_param))*np.nan\n        \n        # grid-search\n        for i, (trend, seasonal, damped) in enumerate(grid_param):\n            \n            ES = ExponentialSmoothing(self.top_level, trend=trend,\n                                      seasonal=seasonal, damped=damped, \n                                      seasonal_periods=7,freq='D').fit(optimized=True, use_brute=True)\n            aic[i] = ES.aic\n        \n        # best parameters & AIC\n        best_index = np.nanargmin(aic)\n        best_params = grid_param[best_index]\n        best_aic = aic[best_index]\n        \n        return best_params, best_aic\n    \n    def predict_top_level(self):\n        \n        \n        (best_trend, best_seas, best_dumped), best_aic = self._parameters_tuning()\n        \n        ES = ExponentialSmoothing(self.top_level, trend=best_trend,\n                                  damped=best_dumped, seasonal=best_seas,\n                                  seasonal_periods=7, freq='D').fit(optimized=True, use_brute=True)\n        top_level_preds = ES.forecast(self.horizon)\n        \n        return top_level_preds\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nes_preds_val = M5ExponentialSmoothing(horizon=cnt.HORIZON, df_train=df_train, df_calendar=df_calendar).predict_bottom_levels()\nes_error = error_eval.score(es_preds_val)\nes_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_time_series(index=24, df_train=df_train, df_eval=df_val, preds=es_preds_val, calendar=df_calendar)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ARIMA"},{"metadata":{"trusted":true},"cell_type":"code","source":"class M5ARIMA(M5TopDown):\n    \n    def __init__(self, horizon, df_train, df_calendar):\n        self.horizon = horizon\n        self.df_train = df_train\n        self.df_calendar = df_calendar\n        self.top_level = self._get_top_level_timeserie()\n        self.exog_fit = None\n        self.exog_pred = None\n    \n    def _get_aic(self, order):\n        \"\"\"\n        Because some parameter combinations may lead to numerical misspecifications,\n        we explicitly disabled warning messages in order to avoid an overload of warning messages.\n        These misspecifications can also lead to errors and throw an exception,\n        so we make sure to catch these exceptions and ignore the parameter combinations that cause these issues.\n        \"\"\"\n        aic =np.nan\n        try:\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\"ignore\",\n                                        message=\"Maximum Likelihood optimization failed to converge. Check mle_retvals\")\n\n                arima_mod=sm.tsa.statespace.SARIMAX(endog=self.top_level, exog=self.exog_fit,\n                                                    order=order,\n                                                    enforce_stationarity=False, \n                                                    enforce_invertibility=False).fit()\n                aic=arima_mod.aic\n        except:\n            pass\n        return aic\n\n    def _parameters_tuning(self,n_jobs=7):\n        d = range(3)\n        p = range(12)\n        q = range(12)\n\n        params = [p, d, q]\n        pdq_params = list(itertools.product(*params))\n\n        get_aic_partial=partial(self._get_aic)\n        p = Pool(n_jobs)\n        res_aic = p.map(get_aic_partial, pdq_params)  \n        p.close()\n        \n        best_aic_index = np.nanargmin(res_aic)\n        best_aic = res_aic[best_aic_index]\n        best_pdq = pdq_params[best_aic_index]\n        \n        return best_pdq, best_aic\n    \n    def predict_top_level(self):\n        \n        \n        best_pdq, best_aic = self._parameters_tuning()\n        \n        arima_mod = sm.tsa.statespace.SARIMAX(endog=self.top_level, exog=self.exog_fit,\n                                              order=best_pdq,\n                                              enforce_stationarity=False, \n                                              enforce_invertibility=False).fit()\n        top_level_preds = arima_mod.forecast(self.horizon, exog=self.exog_pred)\n        \n        return top_level_preds\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\narima_preds_val = M5ARIMA(horizon=cnt.HORIZON, df_train=df_train, df_calendar=df_calendar).predict_bottom_levels()\narima_error = error_eval.score(arima_preds_val)\narima_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_time_series(index=24, df_train=df_train, df_eval=df_val, preds=arima_preds_val, calendar=df_calendar)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ARIMAX"},{"metadata":{"trusted":true},"cell_type":"code","source":"class M5ARIMAX(M5ARIMA):\n    \n    def __init__(self, horizon, df_train, df_calendar):\n        self.horizon = horizon\n        self.df_train = df_train\n        self.df_calendar = df_calendar\n        self.train_len = len([x for x in self.df_train.columns if x.startswith('d_')])\n        self.top_level = self._get_top_level_timeserie()\n        self.exog_fit, self.exog_pred = self._get_exploratory_variables()\n        \n        \n        \n    def _get_exploratory_variables(self):\n        calendar = self.df_calendar[['date', 'snap_CA','snap_TX','snap_WI', 'event_name_1', 'event_name_2']].copy()\n        calendar['snap_count'] = calendar[['snap_CA','snap_TX','snap_WI']].apply(np.sum,axis=1)\n\n        calendar['is_event_1'] = [isinstance(x , str)*1 for x in calendar['event_name_1']]\n        calendar['is_event_2'] = [isinstance(x , str)*1 for x in calendar['event_name_2']]\n        calendar['is_event'] = calendar[['is_event_1', 'is_event_2']].apply(np.sum, axis=1)\n        calendar['is_event'] = np.where(calendar['is_event']>0,1,0)\n\n        exog_fit = calendar[['snap_count', 'is_event']].iloc[:self.train_len,:].values\n        exog_pred = calendar[['snap_count', 'is_event']].iloc[self.train_len:self.train_len+self.horizon,:].values\n\n        return exog_fit, exog_pred\n    \n    def create_submission_file(self, file_name):\n        \"\"\"\n        Create submission file with the predictions\n        NB: We double the horizon to take into account validation & evaluation forcasts as requested in the submission file\n        \"\"\"\n        \n        single_horizon = self.horizon\n        # double horizon to take into accoint validation & evaluation forcast in the submission file\n        #self.horizon = 2 * single_horizon\n        \n        # Ri-Calculate Ex Variables to take into account 2*horizon\n        self.exog_fit, self.exog_pred = self._get_exploratory_variables()\n        \n        preds = self.predict_bottom_levels()\n        \n        sample_submission = pd.read_csv(cnt.SAMPLE_SUBMISSION)\n        sample_submission.iloc[0:preds.shape[0],1:] = preds[:,0:single_horizon]\n        sample_submission.iloc[-preds.shape[0]:,1:] = preds[:,-single_horizon:]\n\n        sample_submission.to_csv(file_name, index=False, compression='gzip')\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\narimax_preds_val = M5ARIMAX(horizon=cnt.HORIZON, df_train=df_train, df_calendar=df_calendar).predict_bottom_levels()\narimax_error = error_eval.score(arimax_preds_val)\narimax_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_time_series(index=24, df_train=df_train, df_eval=df_val, preds=arimax_preds_val, calendar=df_calendar)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bonus: Facebook Prophet"},{"metadata":{},"cell_type":"markdown","source":"Eventhough Facebook Prophet method is not an official benchmark provided by the M5 organizers, I decided to give it a try.\nAs for ARIMAX, I added the exploratory variables as well as the built it holidays."},{"metadata":{"trusted":true},"cell_type":"code","source":"from fbprophet import Prophet\n\nclass M5Prophet(M5ARIMAX):\n    \n    def __init__(self, horizon, df_train, df_calendar):\n        self.horizon = horizon\n        self.df_train = df_train\n        self.df_calendar = df_calendar\n        self.train_len = len([x for x in self.df_train.columns if x.startswith('d_')])\n        self.top_level = self._get_top_level_timeserie()\n        self.exog_fit, self.exog_pred = self._get_exploratory_variables()\n        \n    \n    def predict_top_level(self):\n        \n        df = pd.DataFrame({'ds':self.top_level.index, 'y':self.top_level.values})\n        df['snap_count'] = self.exog_fit[:,0]\n        df['is_event'] = self.exog_fit[:,1]\n        \n        m = Prophet()\n        m.add_regressor('snap_count')\n        m.add_regressor('is_event')\n        \n        m.add_country_holidays(country_name='US')\n        m.fit(df)\n        \n        future = m.make_future_dataframe(periods=self.horizon)\n        future['snap_count'] = np.concatenate((self.exog_fit[:,0],self.exog_pred[:,0]))\n        future['is_event'] = np.concatenate((self.exog_fit[:,1], self.exog_pred[:,1]))\n        \n        preds = m.predict(future)\n        \n        top_level_preds = pd.Series(preds['yhat'].values[-self.horizon:],\n                                    index = preds['ds'].values[-self.horizon:])\n        \n        return top_level_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfp_preds_val = M5Prophet(horizon=cnt.HORIZON, df_train=df_train, df_calendar=df_calendar).predict_bottom_levels()\nfp_error = error_eval.score(fp_preds_val)\nfp_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_time_series(index=24, df_train=df_train, df_eval=df_val, preds=fp_preds_val, calendar=df_calendar)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation Scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"method = ['Naive', 'sNaive', 'SES', 'MA', 'CRO', 'optCRO', 'SBA','TSB', 'ES', 'ARIMA', 'ARIMAX', 'prophet']\nerror = [naive_error, snaive_error, ses_error, ma_error, cro_error, optcro_error,\n         sba_error, tsb_error, es_error, arima_error, arimax_error, fp_error]\nvalidation_errors = pd.DataFrame({'method':method, 'WRMSSE':error}).sort_values('WRMSSE').reset_index(drop=True)\n\nvalidation_errors","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission files for all methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nM5Naive(horizon=cnt.HORIZON).create_submission_file(df_train_val,\n                                                    file_name='submission_naive.csv.gz')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nM5SeasonalNaive(horizon=cnt.HORIZON, seasonal_days=7).create_submission_file(df_train_val,\n                                                                             file_name='submission_snaive.csv.gz')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nM5SimpleExponentialSmoothing(horizon=cnt.HORIZON, alpha=.1,\n                             optimized=True, bounds=(0.1,.3),\n                             maxiter = 10).create_submission_file(df_train_val,\n                                                                  file_name='submission_ses.csv.gz')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nM5MovingAverage(k=3, horizon=cnt.HORIZON, optimized=True,\n                k_lb=3, k_ub=5, last_n_values=100).create_submission_file(df_train_val,\n                                                                          file_name='submission_ma.csv.gz')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nM5Croston(horizon=cnt.HORIZON).create_submission_file(df_train_val,\n                                                      file_name='submission_cro.csv.gz')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nM5OptCroston(horizon=cnt.HORIZON, maxiter=10).create_submission_file(df_train_val,\n                                                                     file_name='submission_optcro.csv.gz')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nM5SBA(horizon=cnt.HORIZON).create_submission_file(df_train_val,\n                                                  file_name='submission_sba.csv.gz')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nM5TSB(horizon=cnt.HORIZON, alpha=.1, beta=.1).create_submission_file(df_train_val,\n                                                                     file_name='submission_tsb.csv.gz')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nM5ExponentialSmoothing(horizon=cnt.HORIZON,\n                       df_train=df_train_val,\n                       df_calendar=df_calendar).create_submission_file(file_name='submission_es.csv.gz')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nM5ARIMA(horizon=cnt.HORIZON,\n        df_train=df_train_val,\n        df_calendar=df_calendar).create_submission_file(\"submission_arima.csv.gz\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nM5ARIMAX(horizon=cnt.HORIZON,\n        df_train=df_train_val,\n        df_calendar=df_calendar).create_submission_file(\"submission_arimax.csv.gz\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"M5Prophet(horizon=cnt.HORIZON,\n          df_train=df_train,\n          df_calendar=df_calendar).create_submission_file(\"submission_prophet.csv.gz\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final Submission"},{"metadata":{},"cell_type":"markdown","source":"Final submission is calculated by averaging the best 3 methods with respct to WRMSSE on the validaation set."},{"metadata":{"trusted":true},"cell_type":"code","source":"top_methods = validation_errors['method'].head(3).values\nsubmission_files = ['submission_'+method.lower()+'.csv.gz' for method in top_methods]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_preds = np.zeros((60980, cnt.HORIZON, len(submission_files)))\n\nfor i, file in tqdm(enumerate(submission_files)):\n    sub_df = pd.read_csv(file)\n    all_preds[:,:,i] = sub_df.iloc[:,1:].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_pred = np.mean(all_preds, axis=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_pred.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Write final submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_submission = pd.read_csv(cnt.SAMPLE_SUBMISSION)\nfinal_submission.iloc[0:final_pred.shape[0],1:] = final_pred\n\nfinal_submission.to_csv(\"submission.csv.gz\", index=False, compression='gzip')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}