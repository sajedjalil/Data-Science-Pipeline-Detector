{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThe Makridakis Competitions (also known as the M-Competitions) are a series of open competitions organized by teams led by forecasting researcher Spyros Makridakis and intended to evaluate and compare the accuracy of different forecasting methods - source(Wikipedia)\n\nAttempting to to approach the forecasting with a number of models. Used Tarun's Kernel to begin with.\nAnd it will be evolved over with my own intuitions etc. We have been challenged to forecast the data 28 days in to the future ie from day 1914 to day 1941. We would be attempting to due the forecast as well as the uncertainty distribution.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"### The DataSet\n\n* calendar.csv - Contains the dates on which products are sold. The dates are in a yyyy/dd/mm format.\n\n* sales_train_validation.csv - Contains the historical daily unit sales data per product and store [d_1 - d_1913].\n\n* submission.csv - Demonstrates the correct format for submission to the competition.\n\n* sell_prices.csv - Contains information about the price of the products sold per store and date.\n\n* sales_train_evaluation.csv - Available one month before the competition deadline. It will include sales for [d_1 - d_1941].\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets try to visualize the data and get some insights","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Import Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport time\nimport math\nimport datetime\nfrom math import log, floor\nfrom sklearn.neighbors import KDTree\n\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.utils import shuffle\nfrom tqdm.notebook import tqdm as tqdm\nfrom random import randint\n\nimport seaborn as sns\nfrom matplotlib import colors\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\nimport calendar\nimport datetime\nfrom matplotlib.colors import ColorConverter, ListedColormap\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport pywt\nfrom statsmodels.robust import mad\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nimport scipy\nfrom scipy.fftpack import fft\nimport statsmodels\nfrom scipy import signal\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom fbprophet import Prophet\nfrom scipy.signal import butter, deconvolve\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_dir = '/kaggle/input/m5-forecasting-accuracy'\ncalender = pd.read_csv('%s/calendar.csv' % input_dir)\nselling_prices = pd.read_csv('%s/sell_prices.csv' % input_dir)\nsample_submission = pd.read_csv('%s/sample_submission.csv' % input_dir)\nsales_train_val = pd.read_csv('%s/sales_train_validation.csv' % input_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_val.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe there is an id for each of departments, category, item and a general id which is a combination of all these. Many of the data points ahs got zero values. The sales date are denoted by 'd_'.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = np.unique(sales_train_val.id.values)\ndate_cols = [c for c in sales_train_val.columns if 'd_' in c]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Understanding the sales data\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"By plotting the sales data for a shorter time duration, we can get a better picture or better visualization\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_1 = sales_train_val.loc[sales_train_val['id'] == ids[12]].set_index('id')[date_cols].values[0][:120]\nx_2 = sales_train_val.loc[sales_train_val['id'] == ids[4]].set_index('id')[date_cols].values[0][240:360]\nx_3 = sales_train_val.loc[sales_train_val['id'] == ids[47]].set_index('id')[date_cols].values[0][600:720]\n\nfig = make_subplots(rows=3, cols=1)\nfig.add_trace(go.Scatter(x=np.arange(len(x_1)), y=x_1, showlegend=False,\n                    mode='lines+markers', name=\"First sample\", marker=dict(color=\"limegreen\")), row=1, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_2)), y=x_2, showlegend=False,\n                    mode='lines+markers', name=\"Second sample\", marker=dict(color=\"hotpink\")), row=2, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_3)), y=x_3, showlegend=False,\n                    mode='lines+markers', name=\"Third sample\", marker=dict(color=\"lightsteelblue\")), row=3, col=1)\n\nfig.update_layout(height=1200, width=800, title_text=\"Sample Sales Snippets\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see that the since the data changes sharply between the days.  We can apply some smoothening to visualize any underlying trends. At firt lets start with some time averaging using **Savitzky golay** filters. \nBy controlling the window length and polynomila order we we can get a smoothened curve from these. The window length chosen here just bu intuition, if its bigger you will get a more smoothened curve.\n\nAlso another way  we can go for is wavelet denoising","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_1 = sales_train_val.loc[sales_train_val['id'] == ids[5]].set_index('id')[date_cols].values[0][:120]\nx_2 = sales_train_val.loc[sales_train_val['id'] == ids[7]].set_index('id')[date_cols].values[0][240:360]\nx_3 = sales_train_val.loc[sales_train_val['id'] == ids[13]].set_index('id')[date_cols].values[0][600:720]\n\nsmoothened_signal_1 = signal.savgol_filter(x_1, polyorder = 3, window_length=15)\nsmoothened_signal_2 = signal.savgol_filter(x_2, polyorder = 3, window_length=15)\nsmoothened_signal_3 = signal.savgol_filter(x_3, polyorder = 3, window_length=15)\n\n\nfig = make_subplots(rows=3, cols=1)\nfig.add_trace(go.Scatter(x=np.arange(len(x_1)), y=x_1, showlegend=False,\n                    mode='lines+markers', name=\"original signal\", marker=dict(color=\"limegreen\")), row=1, col=1)\nfig.add_trace(go.Scatter(x=np.arange(len(x_1)), y=smoothened_signal_1, showlegend=False,\n                    mode='lines', name=\"smoothened signal\", marker=dict(color=\"darkgreen\")), row=1, col=1)\n\n\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_2)), y=x_2, showlegend=False,\n                    mode='lines+markers', name=\"original signal\", marker=dict(color=\"hotpink\")), row=2, col=1)\nfig.add_trace(go.Scatter(x=np.arange(len(x_2)), y=smoothened_signal_2, showlegend=False,\n                    mode='lines', name=\"smoothened signal\", marker=dict(color=\"darkred\")), row=2, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_3)), y=x_3, showlegend=False,\n                    mode='lines+markers', name=\"original signal\", marker=dict(color=\"lightsteelblue\")), row=3, col=1)\nfig.add_trace(go.Scatter(x=np.arange(len(x_3)), y=smoothened_signal_3, showlegend=False,\n                    mode='lines', name=\"smoothened signal\", marker=dict(color=\"darkslateblue\")), row=3, col=1)\n\nfig.update_layout(height=1200, width=800, title_text=\"Sample Sales Snippets\")\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aggregate Sales vs Time\n\nWe can start doing the same for different aggregation levels. Since we haver got three states, That can be an aggregation parameter, then storewise, department wise etc.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"aggr_array = []\nfor d in date_cols:\n    aggr_array.append(sales_train_val[d].values.sum())\n    \ndate_range = calender['date']\nd_st = date_range.values[0]\nd_end = date_range.values[1912]\n    \ndate_range = pd.date_range(start=d_st, end=d_end)\n\n# Doing a monthly time averaging we can plot them along side\n\ndaily_time_series_df = pd.DataFrame(data=aggr_array, columns=['Sales'], index=date_range)\nmonthly_time_seies_df = daily_time_series_df.resample('1M').sum()\n\nfig = make_subplots(rows=2, cols=1, subplot_titles=(\"Daily Aggregate Sales\", \"Monthly Aggregate Sales\"))\n\nfig.add_trace(go.Scatter(x=date_range, y=aggr_array, showlegend=False,\n                    mode='lines', name=\"Aggregate Sales\", marker=dict(color=\"darkgreen\")), row = 1, col=1)\nfig.add_trace(go.Scatter(x=monthly_time_seies_df.index[1:-1], y=monthly_time_seies_df['Sales'].values[1:-1],\n                         marker=dict(color=\"darkgreen\")), row = 2, col=1)\n\nfig.update_layout(title=\"Aggregate Sales\",height=1200, width=1000)\nfig.update_xaxes(title_text=\"Date\", row=1, col=1)\nfig.update_yaxes(title_text=\"Sales in Units\", row=1, col=1)\nfig.update_yaxes(title_text=\"Sales in Units\", row=2, col=1)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Ahh nice! plotly is a savior!. Good thing is that you can zoom in over the graphs.\n\nsome observations we can take away\n*  Sales are increasing over time\n*  There are seasonal behaviors in sales trends such as monthly/weekly trends. Out of this, the weekly trends are very evident.\n* End of every year there is a sharp dip in sales may be due to the vacation/holiday season.\n\nMay be we can do some time based normalisation and apply a Fourier/Wavelet transform to get more idea about the trend frequencies. But lets keep it for later(sorry, just being a fan of Fourier transforms !)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Now lets plot the sepearte data statewise","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"store_ids = sales_train_val['store_id'].values\nstate_id = []\n\nfor id in store_ids:\n    state_id.append(id[:2])\nstate_id = np.array(state_id)\nsales_train_val['state_id'] = state_id\n\ndf_grouped_statewise = sales_train_val.groupby('state_id')\n\nfig = go.Figure()\n\nfor name, group in df_grouped_statewise:\n    aggr_array = []\n    for d in date_cols:\n        aggr_array.append(group[d].values.sum())\n    daily_time_series_df = pd.DataFrame(data=aggr_array, columns=['Sales'], index=date_range)\n    monthly_time_seies_df = daily_time_series_df.resample('1M').sum()\n    fig.add_trace(go.Scatter(x=monthly_time_seies_df.index[1:-1], y=monthly_time_seies_df['Sales'].values[1:-1],\n                             showlegend=True, mode='lines', name=\"%s Sales\" % name))\nfig.update_layout(\n    title=\"State Wise Aggregate Sales\", xaxis_title=\"Date\", yaxis_title=\"Unit Sales\", height=600, width=1000)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Plotting Aggregate Sales per Stores\n\nThere are 3 categories and 10 stores in total. We can try plotting for each of the categories and for stores in each state. Using matplotlib this time. Here I am using both plotly and Matplotlib throughout the notebook. Users can have a good feel how these libraries can be used for dynamoic plotting of data.\n","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, 3, figsize=(16, 4), sharey=True, constrained_layout=True)\nfig.suptitle('Storewise Sales Data')\ni = 0\nfor state_name, state_group in df_grouped_statewise:\n    df_store_wise_grouped = state_group.groupby('store_id')\n    axs[i].set_title('%s store-wise sales' % state_name)\n    axs[i].set(xlabel='Date', ylabel='Sales Units')\n    axs[i].grid(True)\n    for store_name, store_group in df_store_wise_grouped:\n        aggr_array = []\n        for d in date_cols:\n            aggr_array.append(store_group[d].values.sum())\n        daily_time_series_df = pd.DataFrame(data=aggr_array, columns=['Sales'], index=date_range)\n        monthly_time_seies_df = daily_time_series_df.resample('1M').sum()\n        axs[i].plot(monthly_time_seies_df.index[1:-1], monthly_time_seies_df['Sales'].values[1:-1], label='%s' % store_name)\n        axs[i].legend()\n    i += 1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quick Observation:\n    \nThe sales in stores in California varies by a lot. It could be due local tech Industry booms, leading to higher purchasing power/higher migrant population etc.\n\nSales in Texas stores are fairly Equally placed. We can see a general dip in sales around 2014. That could be due to Oil price collapse during 2014. Since Texas is highly dependant on oil revenue, that could have dragged local income and hence their purchasing power as well\n\nThw WI_1 & WI_2 stores shows a surprising jump towards the beginning of 2013. This could be due to say opening up of a major Industrial facility providing lots of jobs simulataneously(By the way these are just guesses!)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Sales per Category/Department\n\nFor time being lets plot only for CA stores","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ca_stores_df = df_grouped_statewise.get_group('CA')\n\ndepartments = np.unique(ca_stores_df['dept_id'].values)\n\nfig, axs = plt.subplots(1, len(departments), figsize=(16, 5), sharey=True)\nfig.suptitle('Dept-Wies Sales Data for CA stores')\n\nfor i, d in enumerate(departments):\n    axs[i].set_title('%s' % d)\n    axs[i].set(xlabel='Date')\n    if i == 0:\n        axs[i].set(ylabel='Sales Units')\n    axs[i].grid(True)\n    dept_sales_data = ca_stores_df[ca_stores_df['dept_id'] == d]\n    aggr_array = []\n    for d in date_cols:\n        aggr_array.append(dept_sales_data[d].values.sum())\n    daily_time_series_df = pd.DataFrame(data=aggr_array, columns=['Sales'], index=date_range)\n    monthly_time_seies_df = daily_time_series_df.resample('1M').sum()\n    axs[i].plot(monthly_time_seies_df.index[1:-1], monthly_time_seies_df['Sales'].values[1:-1],\n                label='%s' % store_name)\nfig.autofmt_xdate()\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decompose Time Series Data into Trend and Seasonality\nGot these excellent explations from the website:\nhttps://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/\n\nTime series decomposition involves thinking of a series as a combination of level, trend, seasonality, and noise components.\n\nDecomposition provides a useful abstract model for thinking about time series generally and for better understanding problems during time series analysis and forecasting\n\nTime Series Components\nA useful abstraction for selecting forecasting methods is to break a time series down into systematic and unsystematic components.\n\n**Systematic**: Components of the time series that have consistency or recurrence and can be described and modeled.\n**Non-Systematic**: Components of the time series that cannot be directly modeled.\nA given time series is thought to consist of three systematic components including level, trend, seasonality, and one non-systematic component called noise.\n\nThese components are defined as follows:\n\n**Level**: The average value in the series.\n\n**Trend**: The increasing or decreasing value in the series.\n\n**Seasonality**: The repeating short-term cycle in the series.\n\n**Noise**: The random variation in the series.\n\n**Additive Model**\nAn additive model suggests that the components are added together as follows:\n\ny(t) = Level + Trend + Seasonality + Noise\nAn additive model is linear where changes over time are consistently made by the same amount.\n\n**Multiplicative Model**\nA multiplicative model suggests that the components are multiplied together as follows:\n\ny(t) = Level x Trend x  Seasonality x  Noise\n\nAs a first step we can analyse the series assuming it as a Additive data.\nHere I am trying with naive decomposition to begin with.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"aggr_array = []\nfor d in date_cols:\n    aggr_array.append(sales_train_val[d].values.sum())\n\ndaily_time_series_df = pd.DataFrame(data=aggr_array, columns=['Sales'], index=date_range)\n\n\nseries = daily_time_series_df['Sales']\nresult = seasonal_decompose(series, model='additive')\nresult_list = [result.trend, result.seasonal, result.resid, result.observed]\n\nfig = make_subplots(rows=len(result_list), cols=1, subplot_titles=(\"Trend\", \"Seasonal\", \"Residual\", \"Observed\"))\n\nfor i, res in enumerate(result_list):\n    fig.add_trace(go.Scatter(x=date_range, y=res, showlegend=True, mode='lines'), row=i + 1, col=1)\n\n                  \nfig.update_layout(title=\"Seasonal Decomposition\", height=1000, width=1000)\nfig.update_xaxes(title_text=\"Date\", row=4, col=1)\nfig.update_yaxes(title_text=\"Value\", row=1, col=1)\nfig.update_yaxes(title_text=\"Value\", row=2, col=1)\nfig.update_yaxes(title_text=\"Value\", row=3, col=1)\nfig.update_yaxes(title_text=\"Value\", row=4, col=1)                \nfig.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observations\nClearly we can see the different components at play. At this moment we can not be so sure if this is the exact decomposition we want. But honestly it looks nice to me!\nI has clearly captured the weekly seasonalities, The Monthly seasonality where the sales peaks shortly after the pay date(assuming monthly pay).\nLets have a closer look at the Sesonal component, before even having a look I would guess its the weekly variations captured in there. But anyways lets have a look!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=date_range[:140], y=result_list[1][:140], showlegend=True, mode='lines'))\nfig.update_layout(title=\"Seasonal Component\", xaxis_title=\"Date\", yaxis_title=\"Units/Value\", height=500, width=800)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations continued\nOkay if we can have an even closer look, its clear that it shows the weekly fluctuations in the sales data where the sales peaks at weekends(Saturday and Sunday).\n\nMoving to the noise component(noise), The dip at year end is captured there rather being in seasonal trend. But its okay we know whats going on right.\n\nIt would be a good comparison between the Trend part of of the decomposition and the overall trend for Observed data. It can bring in an idea how effective our decomposition algorithm is here.Lets bring in polynomial regressor do make a trendline","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"aggr_array = []\nfor d in date_cols:\n    aggr_array.append(sales_train_val[d].values.sum())\n\ndaily_time_series_df = pd.DataFrame(data=aggr_array, columns=['Sales'], index=date_range)\n\nX_values = range(len(daily_time_series_df))\ncoeffs = np.polyfit(X_values, daily_time_series_df['Sales'].values, 5)\npoly_eqn = np.poly1d(coeffs)\ny_hat = poly_eqn(X_values)\n\ndaily_time_series_df['sales_trend'] = y_hat\n\n# since there are some nan values at the beginning and end lets take those values out.\ndecomp_trend = result.trend[3:-3]\ncoeffs = np.polyfit(X_values[3:-3], decomp_trend, 5)\npoly_eqn = np.poly1d(coeffs)\ny_hat = poly_eqn(X_values)\n\ndaily_time_series_df['sales_trend_decomp'] = y_hat\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=daily_time_series_df.index, y=daily_time_series_df['Sales'].values,\n                             showlegend=True, mode='lines'))\nfig.add_trace(go.Scatter(x=daily_time_series_df.index, y=daily_time_series_df['sales_trend'].values, mode='lines',\n                        showlegend=True, marker=dict(color=\"black\"), name='observed_trend'))\nfig.add_trace(go.Scatter(x=daily_time_series_df.index, y=daily_time_series_df['sales_trend_decomp'].values, mode='lines',\n                        showlegend=True, marker=dict(color=\"orange\"), name = 'decomposed trend'))\nfig.update_layout(\n    title=\"Observed Sales and trend-line\", xaxis_title=\"Date\", yaxis_title=\"Unit Sales\", height=500, width=1000)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Its clear from the plots that, the trendline from seasonal decomposition is quite aligned with the observed trendline. Hence the naive decomposition does a very good job in getting here the trend right.\n\nAnother thing we can perform is doing a multiplicative decomposition and analyse the results. But its usually meant for non-linear scenario. But well, there isn't any harm in giving a try right!\n\n## Multiplicative Decomposition\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"series = daily_time_series_df['Sales']\nresult = seasonal_decompose(series, model='multiplicative')\nresult_list = [result.trend, result.seasonal, result.resid, result.observed]\n\nfig = make_subplots(rows=len(result_list), cols=1, subplot_titles=(\"Trend\", \"Seasonal\", \"Residual\", \"Observed\"))\n\nfor i, res in enumerate(result_list):\n    fig.add_trace(go.Scatter(x=date_range, y=res, showlegend=True, mode='lines'), row=i + 1, col=1)\n\n                  \nfig.update_layout(title=\"Seasonal Decomposition\", height=1000, width=1000)\nfig.update_xaxes(title_text=\"Date\", row=4, col=1)\nfig.update_yaxes(title_text=\"Value\", row=1, col=1)\nfig.update_yaxes(title_text=\"Value\", row=2, col=1)\nfig.update_yaxes(title_text=\"Value\", row=3, col=1)\nfig.update_yaxes(title_text=\"Value\", row=4, col=1)                \nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\nlike the additive decomposition, the multiplicative decomposition also churns out similar data set. So here the linearity/non-linearity ddnt matter at all!. The sesonal component clearly identifies what to be multiplied with the data.\n\nNow lets attempt to plot the data Statewise and further we can dig in storewise as well.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"state_ids = np.unique(sales_train_val['state_id'].values)\nfig = make_subplots(rows=1, cols=3, subplot_titles = state_ids)\n\nfor i, st_id in enumerate(state_ids):\n    state_wise_df = sales_train_val[sales_train_val['state_id'] == st_id]\n\n    aggr_array = []\n    for d in date_cols:\n        aggr_array.append(state_wise_df[d].values.sum())\n    daily_time_series_df = pd.DataFrame(data=aggr_array, columns=['Sales'], index=date_range)\n    \n    X_values = range(len(daily_time_series_df))\n    coeffs = np.polyfit(X_values, daily_time_series_df['Sales'].values, 5)\n    poly_eqn = np.poly1d(coeffs)\n    y_hat = poly_eqn(X_values)\n\n    daily_time_series_df['sales_trend'] = y_hat\n    \n    fig.add_trace(go.Scatter(x=daily_time_series_df.index, y=daily_time_series_df['Sales'].values,\n                             showlegend=True, mode='lines', name=\"%s Sales\" % name), row=1, col=i + 1)\n    fig.add_trace(go.Scatter(x=daily_time_series_df.index, y=daily_time_series_df['sales_trend'].values,\n                             showlegend=True,marker=dict(color=\"black\"), mode='lines', name=\"%s Sales\" % name), row=1, col=i + 1)\nfig.update_layout(\n    title=\"State Wise Aggregate Sales & Trend\", xaxis_title=\"Date\", yaxis_title=\"Unit Sales\", height=400, width=1100)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe similar trends statewise as well in the above graph. Let's go another level deep, Say plotting it Departmentwise. I think It would better if can plot the the trend values along with the daily fluctuating values to get a good feel.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dept_ids = sales_train_val['dept_id'].values\ndept_category = []\nfor d_id in dept_ids:\n    dept_category.append(d_id[:-2])\ndept_category = np.array(dept_category)\n\nsales_train_val['dept_cat'] = dept_category\ndept_cat_ids = np.unique(dept_category)\n\n# using matplotlib\nfig, axs = plt.subplots(1, 3, figsize=(16, 6), sharey=True, constrained_layout=True)\nfig.suptitle('Storewise Sales Data')\n\nfor i, st_id in enumerate(state_ids):\n    state_wise_df = sales_train_val[sales_train_val['state_id'] == st_id]\n    axs[i].set_title('%s - Department Wise sales' % st_id)\n    axs[i].set(xlabel='Date', ylabel='Sales Units')\n    for j, dp_cat_id in enumerate(dept_cat_ids):\n        dept_wise_df = state_wise_df[state_wise_df['dept_cat'] == dp_cat_id]\n        aggr_array = []\n        for d in date_cols:\n            aggr_array.append(dept_wise_df[d].values.sum())\n        daily_time_series_df = pd.DataFrame(data=aggr_array, columns=['Sales'], index=date_range)\n        \n        X_values = range(len(daily_time_series_df))\n        coeffs = np.polyfit(X_values, daily_time_series_df['Sales'].values, 5)\n        poly_eqn = np.poly1d(coeffs)\n        y_hat = poly_eqn(X_values)\n\n        daily_time_series_df['sales_trend'] = y_hat\n        axs[i].plot(daily_time_series_df.index, daily_time_series_df['Sales'].values, label='%s' % dp_cat_id)\n        axs[i].plot(daily_time_series_df.index, daily_time_series_df['sales_trend'].values, color = 'black')\n        axs[i].legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay! So food is the wild card here with maximum fluctuations fluctuations observed. We can go on an on ploting subcategories etc. But may be keep it for a while later.\nI think its also important to capture the weekly and monthly seasonality of Sales. So lets try plotting that.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, len(state_ids), figsize=(14, 6), sharey=True, constrained_layout=True)\nfig.suptitle('Day wise Sales Data')\nweekdays = calender['weekday'][:1913]\nweekday_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\nfor i, st_id in enumerate(state_ids):\n    state_wise_df = sales_train_val[sales_train_val['state_id'] == st_id]\n    for dp_cat_id in dept_cat_ids:\n        dept_wise_df = state_wise_df[state_wise_df['dept_cat'] == dp_cat_id]\n        aggr_array = []\n        for d in date_cols:\n            aggr_array.append(dept_wise_df[d].values.sum())\n        daily_time_series_df = pd.DataFrame(data=aggr_array, columns=['Sales'], index=date_range)\n        daily_time_series_df['weekday'] = weekdays.values\n        weekly_data = []\n        for w in weekday_names:\n            day_wise_sum = (daily_time_series_df[daily_time_series_df['weekday'] == w])['Sales'].values.sum()\n            weekly_data.append(day_wise_sum)\n\n        axs[i].plot(weekday_names, weekly_data, label='%s' % dp_cat_id, linewidth=4)\n        axs[i].legend()\n    axs[i].set_title('%s - Department Wise sales on Weekdays' % st_id)\n    axs[i].set(xlabel='Date', ylabel='Sales Units')\nfig.autofmt_xdate(rotation=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay, we can clearly observe the the sales rises in the weekend and Its even high on Monday. Trend for Hobbies category is not that visible with the above scale, but I am expecting a similar behaviour for the same.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Calender data\n\nLets look at the events going on after this from the calendar data. The are specific days at which events are going on, which could boost or affect the sales. We dont know! lets have a look","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.unique(calender.event_name_1.astype(str), return_counts=True))\nprint(np.unique(calender.event_type_1.astype(str), return_counts=True))\nprint(np.unique(calender.event_name_2.astype(str), return_counts=True))\nprint(np.unique(calender.event_type_2.astype(str), return_counts=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay this gives a quick numerical look @ the data. And another inpection , looking in to the parameters event_name_2 and event_type_2, they are only a few and most likely a spinoff from the first set. I am ignoring those. \n\nLets plot in year wise how many days we have got events happening, type of event etc.\n\nSince I couldn't install the package, I just copied the revelant fuction to plot the calendar events.\nIf anyone knows letme know how to permenantly install a new package in kaggle!. otherwise I face quite a bit of trouble installing it everytime","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n \ndef yearplot(data, year=None, how='sum', vmin=None, vmax=None, cmap='Reds',\n             fillcolor='whitesmoke', linewidth=1, linecolor=None,\n             daylabels=calendar.day_abbr[:], dayticks=True,\n             monthlabels=calendar.month_abbr[1:], monthticks=True, ax=None,\n             **kwargs):\n    \"\"\"\n    Plot one year from a timeseries as a calendar heatmap.\n \n    Parameters\n    ----------\n    data : Series\n        Data for the plot. Must be indexed by a DatetimeIndex.\n    year : integer\n        Only data indexed by this year will be plotted. If `None`, the first\n        year for which there is data will be plotted.\n    how : string\n        Method for resampling data by day. If `None`, assume data is already\n        sampled by day and don't resample. Otherwise, this is passed to Pandas\n        `Series.resample`.\n    vmin, vmax : floats\n        Values to anchor the colormap. If `None`, min and max are used after\n        resampling data by day.\n    cmap : matplotlib colormap name or object\n        The mapping from data values to color space.\n    fillcolor : matplotlib color\n        Color to use for days without data.\n    linewidth : float\n        Width of the lines that will divide each day.\n    linecolor : color\n        Color of the lines that will divide each day. If `None`, the axes\n        background color is used, or 'white' if it is transparent.\n    daylabels : list\n        Strings to use as labels for days, must be of length 7.\n    dayticks : list or int or bool\n        If `True`, label all days. If `False`, don't label days. If a list,\n        only label days with these indices. If an integer, label every n day.\n    monthlabels : list\n        Strings to use as labels for months, must be of length 12.\n    monthticks : list or int or bool\n        If `True`, label all months. If `False`, don't label months. If a\n        list, only label months with these indices. If an integer, label every\n        n month.\n    ax : matplotlib Axes\n        Axes in which to draw the plot, otherwise use the currently-active\n        Axes.\n    kwargs : other keyword arguments\n        All other keyword arguments are passed to matplotlib `ax.pcolormesh`.\n \n    Returns\n    -------\n    ax : matplotlib Axes\n        Axes object with the calendar heatmap.\n    \"\"\"\n    if year is None:\n        year = data.index.sort_values()[0].year\n \n    if how is None:\n        \n        by_day = data\n    else:\n        \n        by_day = data.resample('D', how=how)\n \n    # Min and max per day.\n    if vmin is None:\n        vmin = by_day.min()\n    if vmax is None:\n        vmax = by_day.max()\n \n    if ax is None:\n        ax = plt.gca()\n \n    if linecolor is None:\n        linecolor = ax.get_facecolor()\n        if ColorConverter().to_rgba(linecolor)[-1] == 0:\n            linecolor = 'white'\n \n    # Filter on year.\n    by_day = by_day[str(year)]\n \n    # Add missing days.\n    by_day = by_day.reindex(\n        pd.date_range(start=str(year), end=str(year + 1), freq='D')[:-1])\n \n    # Create data frame we can pivot later.\n    by_day = pd.DataFrame({'data': by_day,\n                           'fill': 1,\n                           'day': by_day.index.dayofweek,\n                           'week': by_day.index.week})\n \n    # There may be some days assigned to previous year's last week or\n    # next year's first week. We create new week numbers for them so\n    # the ordering stays intact and week/day pairs unique.\n    by_day.loc[(by_day.index.month == 1) & (by_day.week > 50), 'week'] = 0\n    by_day.loc[(by_day.index.month == 12) & (by_day.week < 10), 'week'] \\\n        = by_day.week.max() + 1\n \n    # Pivot data on day and week and mask NaN days.\n    plot_data = by_day.pivot('day', 'week', 'data').values[::-1]\n    plot_data = np.ma.masked_where(np.isnan(plot_data), plot_data)\n \n    # Do the same for all days of the year, not just those we have data for.\n    fill_data = by_day.pivot('day', 'week', 'fill').values[::-1]\n    fill_data = np.ma.masked_where(np.isnan(fill_data), fill_data)\n \n    # Draw heatmap for all days of the year with fill color.\n    ax.pcolormesh(fill_data, vmin=0, vmax=1, cmap=ListedColormap([fillcolor]))\n \n    # Draw heatmap.\n    kwargs['linewidth'] = linewidth\n    kwargs['edgecolors'] = linecolor\n    ax.pcolormesh(plot_data, vmin=vmin, vmax=vmax, cmap=cmap, **kwargs)\n \n    # Limit heatmap to our data.\n    ax.set(xlim=(0, plot_data.shape[1]), ylim=(0, plot_data.shape[0]))\n \n    # Square cells.\n    ax.set_aspect('equal')\n \n    # Remove spines and ticks.\n    for side in ('top', 'right', 'left', 'bottom'):\n        ax.spines[side].set_visible(False)\n    ax.xaxis.set_tick_params(which='both', length=0)\n    ax.yaxis.set_tick_params(which='both', length=0)\n \n    # Get indices for monthlabels.\n    if monthticks is True:\n        monthticks = range(len(monthlabels))\n    elif monthticks is False:\n        monthticks = []\n    elif isinstance(monthticks, int):\n        monthticks = range(len(monthlabels))[monthticks // 2::monthticks]\n \n    # Get indices for daylabels.\n    if dayticks is True:\n        dayticks = range(len(daylabels))\n    elif dayticks is False:\n        dayticks = []\n    elif isinstance(dayticks, int):\n        dayticks = range(len(daylabels))[dayticks // 2::dayticks]\n \n    ax.set_xlabel('')\n    ax.set_xticks([by_day.ix[datetime.date(year, i + 1, 15)].week\n                   for i in monthticks])\n    ax.set_xticklabels([monthlabels[i] for i in monthticks], ha='center')\n \n    ax.set_ylabel('')\n    ax.yaxis.set_ticks_position('right')\n    ax.set_yticks([6 - i + 0.5 for i in dayticks])\n    ax.set_yticklabels([daylabels[i] for i in dayticks], rotation='horizontal',\n                       va='center')\n    return ax\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below we use the information from the calender file plot days with events alongside days of the year. He I have just shown it for the days in 2012.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"boolean_event_index = calender['event_name_1'].notnull()[:len(date_range)].values\nevent_days = pd.Series(np.ones(len(date_range)) * boolean_event_index, index=date_range)\n\nfig, axs = plt.subplots(1, 1, figsize=(16, 4))\naxs = yearplot(event_days, year=2012, cmap=\"YlGn\")\naxs.set_title('Event Days - 2012')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SNAP - Supplemental Nutrition Assistance Program (SNAP)\nThe info is obtained from their website:\n\nSNAP provides nutrition benefits to supplement the food budget of needy families so they can purchase healthy food and move towards self-sufficiency.\nSNAP can only buy food for the household, so any analysis for this set of data should be attributed to food in general.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Another interesting analysis I can think about is to predict the effect of events on sales volumes. We might have to do it quite carefully of course, after seggregated by state and department-wise.\nI would prefer to compare it with the trend data we isolated and see how much difference it makes with the day before that or after.\nAlso it would be good check if we can kind-of find an overlapping tendency between the noise and holidays as well. Here we also make some assumptions that\n1. weekly fluctuations are taken care by the seasonal behavior\n2. Trend is a good enough metric to compare \n\nLets begin!\nLike as I mentioned before analysing the data by statewise/dept wise makes more sense than putting it all together. Lets start with the Texan stores.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Days with SNAP purchase\n\nA similar calender plot can be made for SNAP days, by that way we can get the effect of the same on purchases of food items(most likely)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting SNAP days\nsnap_tx = pd.Series(calender.snap_TX[:len(date_range)].values, index=date_range)\nsnap_ca = pd.Series(calender.snap_CA[:len(date_range)].values, index=date_range)\nsnap_wi = pd.Series(calender.snap_WI[:len(date_range)].values, index=date_range)\n\nfig1, axs1 = plt.subplots(1, 1, figsize=(16, 12))\naxs1 = yearplot(snap_tx, year=2012, cmap=\"YlGn\")\naxs1.set_title('SNAP days TX - 2012')\nfig1.show()\n\nfig2, axs2 = plt.subplots(1, 1, figsize=(16, 12))\naxs2 = yearplot(snap_ca, year=2012, cmap=\"YlGn\")\naxs2.set_title('SNAP days CA - 2012')\nfig2.show()\n\nfig3, axs3 = plt.subplots(1, 1, figsize=(16, 12))\naxs3 = yearplot(snap_wi, year=2012, cmap=\"YlGn\")\naxs3.set_title('SNAP days CA - 2012')\nfig3.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texas_df = sales_train_val[sales_train_val['state_id'] == 'TX']\nfig = make_subplots(rows=4, cols=3, subplot_titles = dept_cat_ids)\nfor i, dp_cat_id in enumerate(dept_cat_ids):\n    dept_wise_df = texas_df[texas_df['dept_cat'] == dp_cat_id]\n    aggr_array = []\n    for d in date_cols:\n        aggr_array.append(dept_wise_df[d].values.sum())\n    daily_time_series_df = pd.DataFrame(data=aggr_array, columns=['Sales'], index=date_range)\n    series = daily_time_series_df['Sales']\n    result = seasonal_decompose(series, model='additive')\n    result_list = [result.trend, result.seasonal, result.resid, result.observed]\n    fig.add_trace(go.Scatter(x=daily_time_series_df.index, y=result.trend,\n                             showlegend=True, mode='lines', name=\"%s Trend\" % dp_cat_id), row=1, col=i + 1)\n    fig.add_trace(go.Scatter(x=daily_time_series_df.index, y=result.seasonal,\n                             showlegend=True, mode='lines', name=\"%s Seas\" % dp_cat_id), row=2, col=i + 1)\n    fig.add_trace(go.Scatter(x=daily_time_series_df.index, y=result.resid,\n                             showlegend=True, mode='lines', name=\"%s Resid\" % dp_cat_id), row=3, col=i + 1)\n    fig.add_trace(go.Scatter(x=daily_time_series_df.index, y=result.observed,\n                             showlegend=True, mode='lines', name=\"%s Obser\" % dp_cat_id), row=4, col=i + 1)\n    \nfig.update_layout(title=\"TX Seasonal Department-wise Decomposition\", height=800, width=1200)\nfig.update_xaxes(title_text=\"Date\", row=4, col=1)\nfig.update_yaxes(title_text=\"Sales Units\", row=1, col=1)\nfig.update_yaxes(title_text=\"Sales Units\", row=2, col=1)\nfig.update_yaxes(title_text=\"Sales Units\", row=3, col=1)\nfig.update_yaxes(title_text=\"Sales Units\", row=4, col=1)                \nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the obove visualisations we can get a clear idea regarding the seasonal effects involved in each of the categories.\n\nThe next task would be to analyse and quanity the effect of Event days and SNAP days. That can be tried out may be getting the trend lines for SNAP days and Event days seperate. I am not sure at this moment at what exact level of details we need comparison, but lets start with the effect on foods category.\n\nlets plot and compare the trend lines for TX for event and snap days seperately","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig, axs = plt.subplots(3, 3, figsize=(20, 16))\n\nfor j, st_id in enumerate(state_ids):\n    snap_state_id = 'snap_%s' % st_id\n    snap_state = pd.Series(calender[snap_state_id][:len(date_range)].values, index=date_range)\n    snap_state_df = snap_state.reset_index()\n    snap_on_state_df = snap_state_df[snap_state_df[0] == 1]\n    snap_off_state_df = snap_state_df[snap_state_df[0] == 0]\n    \n    state_df = sales_train_val[sales_train_val['state_id'] == st_id]\n    for i, dp_cat_id in enumerate(dept_cat_ids):\n        dept_wise_df = state_df[state_df['dept_cat'] == dp_cat_id]\n        aggr_array = []\n        for d in date_cols:\n            aggr_array.append(dept_wise_df[d].values.sum())\n        daily_time_series_df = pd.DataFrame(data=aggr_array, columns=['Sales'], index=date_range)\n        series = daily_time_series_df['Sales']\n\n        X_values = range(len(snap_off_state_df.index.values))\n        coeffs = np.polyfit(X_values, daily_time_series_df['Sales'].values[snap_off_state_df.index.values], 5)\n        poly_eqn = np.poly1d(coeffs)\n        y_hat_snap_off = poly_eqn(X_values)\n\n        X_values = range(len(snap_on_state_df.index.values))\n        coeffs = np.polyfit(X_values, daily_time_series_df['Sales'].values[snap_on_state_df.index.values], 5)\n        poly_eqn = np.poly1d(coeffs)\n        y_hat_snap_on = poly_eqn(X_values)\n\n        axs[j, i].plot(snap_off_state_df['index'], y_hat_snap_off, label= 'non snap', linewidth=2, color = 'green')\n        axs[j, i].plot(snap_on_state_df['index'], y_hat_snap_on, label= 'with snap', linewidth=2, color = 'red')\n        axs[j, i].plot(snap_off_state_df['index'], daily_time_series_df['Sales'].values[snap_off_state_df.index.values],\n                    linewidth=2, color = 'green', alpha = 0.2)\n        axs[j, i].plot(snap_on_state_df['index'], daily_time_series_df['Sales'].values[snap_on_state_df.index.values],\n                    linewidth=2, color = 'red', alpha = 0.1)\n        axs[j, i].legend()\n        axs[j, i].set_title('%s - sales - %s' % (dp_cat_id, st_id))\n        axs[j, i].set(xlabel='Date', ylabel='Sales Units')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay, great!. Things are getting more clear. Food sales are considerably higher in all the states compared to other categories.. There is also a marginal effect on the other items as well. These are all useful inputs which can go in to forecasting. But there is also an evident trend seen in Texas, that SNAP effect is thinning out toward the end, whereas for Wisconsin its still prominent towards 2016 may be at around the same level.\n\nThe next one may be we can think about trying out is with respect to days with events. But since they are considerably sparse, the analysis accuracy could be considerably lower. But there is no harm in trying it out right.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(3, 3, figsize=(20, 16))\n\nboolean_event_index = calender['event_name_1'].notnull()[:len(date_range)].values\nevent_days = pd.Series(np.ones(len(date_range)) * boolean_event_index, index=date_range)\nevent_days_df = event_days.reset_index()\nevent_on_df = event_days_df[event_days_df[0] == 1]\nevent_off_df = event_days_df[event_days_df[0] == 0]\n\nfor j, st_id in enumerate(state_ids):\n    state_df = sales_train_val[sales_train_val['state_id'] == st_id]\n    for i, dp_cat_id in enumerate(dept_cat_ids):\n        dept_wise_df = state_df[state_df['dept_cat'] == dp_cat_id]\n        aggr_array = []\n        for d in date_cols:\n            aggr_array.append(dept_wise_df[d].values.sum())\n        daily_time_series_df = pd.DataFrame(data=aggr_array, columns=['Sales'], index=date_range)\n        series = daily_time_series_df['Sales']\n\n        X_values = range(len(event_off_df.index.values))\n        coeffs = np.polyfit(X_values, daily_time_series_df['Sales'].values[event_off_df.index.values], 5)\n        poly_eqn = np.poly1d(coeffs)\n        y_hat_snap_off = poly_eqn(X_values)\n\n        X_values = range(len(event_on_df.index.values))\n        coeffs = np.polyfit(X_values, daily_time_series_df['Sales'].values[event_on_df.index.values], 4)\n        poly_eqn = np.poly1d(coeffs)\n        y_hat_snap_on = poly_eqn(X_values)\n\n        axs[j, i].plot(event_off_df['index'], y_hat_snap_off, label= 'Non Event days', linewidth=2, color = 'green')\n        axs[j, i].plot(event_on_df['index'], y_hat_snap_on, label= 'Event days', linewidth=2, color = 'red')\n        axs[j, i].plot(event_off_df['index'], daily_time_series_df['Sales'].values[event_off_df.index.values],\n                    linewidth=2, color = 'green', alpha = 0.2)\n        axs[j, i].plot(event_on_df['index'], daily_time_series_df['Sales'].values[event_on_df.index.values],\n                    linewidth=2, color = 'red', alpha = 0.2)\n        axs[j, i].legend()\n        axs[j, i].set_title('%s - sales - %s' % (dp_cat_id, st_id))\n        axs[j, i].set(xlabel='Date', ylabel='Sales Units')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay, We can derive a general conclusion out of this like the sales are usually lower in Events day, since people would like to be at home rather than going to a super market on those days. But there is one clear observation that, at the year end there is a dip, which could be attributed to christmas. Hence it would be meaningfull to have another plot to compare without the specific dip observed every year.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Prices of Items\n\nWe have got the prices of every items and can analyse the effects of those on the demand. One analysis that we could do is how the prices varies over time. We can estimate this may be departmentwise or even one or tow random samples to start with. I think this could be reflecting on the slight inflationary trend.\n\nLets randomly select some of the items from the list and plot over time","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"items = np.unique(selling_prices.item_id.values)\n\ndate_id_df = calender.iloc[:, :2].drop_duplicates(subset='wm_yr_wk', keep='first')\nsp_new_df = pd.merge(selling_prices, date_id_df, on='wm_yr_wk')\n\n# randomly choosing some products out of the list\n\nfig, axs = plt.subplots(1, 3, figsize=(16, 5))\ncolor = ['green', 'blue', 'violet']\nfor i in range(3):\n    id_ = randint(0, len(items))\n    item_price_df = sp_new_df[sp_new_df.item_id == items[id_]]\n    item_price_df = item_price_df[item_price_df.store_id == 'CA_1']\n    item_price_df['date'] = pd.to_datetime(item_price_df.date)\n    item_price_df = item_price_df.sort_values(by='date')\n    ymin = (item_price_df.sell_price.values.min())/1.25\n    ymax = (item_price_df.sell_price.values.max())*1.2\n    axs[i].plot(item_price_df.date, item_price_df.sell_price, label= '%s' % items[id_], linewidth=2, color = color[i])\n    axs[i].set_ylim([ymin,ymax])\n    axs[i].legend()\nfig.autofmt_xdate(rotation=30)\nplt.show()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To be continued....","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Alright, one thing is clear. There isn't any notable increase in prices over the time. Its practically constant. At this moment may be we should rule out any possibility that future prices can impact the consumption behaviour.\n\nAt last I think its time we roll in to the forecasting side....hang tight!\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.to_datetime(item_price_df.date.values[0])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}