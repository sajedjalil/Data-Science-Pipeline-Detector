{"cells":[{"metadata":{},"cell_type":"markdown","source":"# M5 Forecasting - Accuracy TimeSeriesSplit CV- Poisson Loss\n\n<style>\ncode, kbd, pre, samp {\n    font-family:'consolas', Lucida Console, SimSun, Fira Code, Monaco !important;\n    font-size: 11pt !important;\n}\n\ndiv.output_area pre {\n    font-family: 'consolas', Lucida Console, SimSun, Fira Code, Monaco !important;\n    font-size:  10pt !important;\n}\n\ndiv.output_area img, div.output_area svg {\n    background-color: #FFFFFF !important;\n}\n</style>\n\nXiao Song\n\n<https://xsong.ltd/en>     \n[Kaggle profile](https://www.kaggle.com/rikdifos/)\n\nThe competition webpage: <https://www.kaggle.com/c/m5-forecasting-accuracy>\n\n\nGithub repo for [M5 Forecasting – Accuracy](https://www.kaggle.com/c/m5-forecasting-accuracy) : Top 103rd rank 2% (solo silver medal) solution: https://github.com/songxxiao/m5_compete  \n\nThis notebook is base on: [Simple LGBM GroupKFold CV](https://www.kaggle.com/ragnar123/simple-lgbm-groupkfold-cv/)\n\n<!---\n\n`sales_train_validation` 包含的是d_1-d_1913的数据(2011-01-29 ~ 2011-04-24)，共30490行   \n`sales_train_evaluation` 包含的是d_1-d_1941的数据(2011-01-29 ~ 2011-05-22)，共30490行   \n`submission` 共60980行，Public LB评价的是`sales_train_validation`(d_1914-d_1941)  \nPrivate LB评价的是sales_train_evaluation(d_1942-d_1968)\n--->","execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"%config InlineBackend.figure_format = 'svg'\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nfrom datetime import datetime, timedelta\nfrom sklearn import preprocessing, metrics\n\nfrom sklearn.model_selection import TimeSeriesSplit\n\nimport gc\nimport os\nimport time\n\nplt.style.use('seaborn')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    '''reduce RAM usage\n    '''\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024 ** 2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\n\n# function to read the data and merge it (ignoring some columns, this is a very fst model)\ndef read_data():\n    '''data input\n    '''\n    print('Reading files...')\n    calendar = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\n    calendar = reduce_mem_usage(calendar)\n    print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n    sell_prices = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\n    sell_prices = reduce_mem_usage(sell_prices)\n    print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n    sales_train_evaluation = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv')\n    print('Sales train evaluation has {} rows and {} columns'.format(sales_train_evaluation.shape[0], sales_train_evaluation.shape[1]))\n    submission = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')\n    return calendar, sell_prices, sales_train_evaluation, submission\n\n\ndef melt_and_merge(calendar, sell_prices, sales_train_evaluation, submission, nrows = 55000000, merge = False):\n    \n    # melt sales data, get it ready for training\n    sales_train_evaluation = pd.melt(sales_train_evaluation, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n    print('Melted sales train validation has {} rows and {} columns'.format(sales_train_evaluation.shape[0], sales_train_evaluation.shape[1]))\n    sales_train_evaluation = reduce_mem_usage(sales_train_evaluation)\n    \n    # seperate test dataframes\n    test1_rows = [row for row in submission['id'] if 'validation' in row]\n    test2_rows = [row for row in submission['id'] if 'evaluation' in row]\n    test1 = submission[submission['id'].isin(test1_rows)]\n    test2 = submission[submission['id'].isin(test2_rows)]\n    \n    # change column names\n    test1.columns = ['id', 'd_1914', 'd_1915', 'd_1916', 'd_1917', 'd_1918', 'd_1919', 'd_1920', 'd_1921', 'd_1922', 'd_1923', 'd_1924', 'd_1925', 'd_1926', 'd_1927', 'd_1928', 'd_1929', 'd_1930', 'd_1931', \n                      'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937', 'd_1938', 'd_1939', 'd_1940', 'd_1941']\n    test2.columns = ['id', 'd_1942', 'd_1943', 'd_1944', 'd_1945', 'd_1946', 'd_1947', 'd_1948', 'd_1949', 'd_1950', 'd_1951', 'd_1952', 'd_1953', 'd_1954', 'd_1955', 'd_1956', 'd_1957', 'd_1958', 'd_1959', \n                      'd_1960', 'd_1961', 'd_1962', 'd_1963', 'd_1964', 'd_1965', 'd_1966', 'd_1967', 'd_1968', 'd_1969']\n    \n    # get product table\n    product = sales_train_evaluation[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n    \n    # merge with product table\n    product['id'] = product['id'].str.replace('_evaluation','_validation')\n    test1 = test1.merge(product, how = 'left', on = 'id') # validation\n    product['id'] = product['id'].str.replace('_validation','_evaluation')\n    test2 = test2.merge(product, how = 'left', on = 'id') # evaluation\n    \n    # \n    test1 = pd.melt(test1, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n    test2 = pd.melt(test2, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n    \n    sales_train_evaluation['part'] = 'train'\n    test1['part'] = 'validation'\n    test2['part'] = 'evaluation'\n    \n    data = pd.concat([sales_train_evaluation, test1, test2], axis = 0)\n    \n    del sales_train_evaluation, test1, test2\n    \n    # get only a sample for fst training\n    data = data.loc[nrows:]\n    \n    # drop some calendar features\n    # calendar.drop(['weekday', 'wday', 'month', 'year','snap_CA','snap_TX','snap_WI'], inplace = True, axis = 1)\n    \n    # delete test2 for now\n    data = data[data['part'] != 'validation']\n    \n    if merge:\n        # notebook crash with the entire dataset (maybee use tensorflow, dask, pyspark xD)\n        data = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\n        data.drop(['weekday', 'wday', 'month', 'year','snap_CA','snap_TX','snap_WI'], inplace = True, axis = 1)\n        # get the sell price data (this feature should be very important)\n        data = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n        print('Our final dataset to train has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\n    else: \n        pass\n    \n    data.to_pickle('data_clean.pkl')\n    gc.collect()\n    \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n\ndef transform(data):\n    '''data transformation\n    '''\n    start = time.time()\n    nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n    for feature in nan_features:\n        data[feature].fillna('unknown', inplace = True)\n    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n    for feature in cat:\n        encoder = preprocessing.LabelEncoder()\n        data[feature] = encoder.fit_transform(data[feature])\n    print('Data transformation costs %7.2f seconds'%(time.time()-start))\n    return data\n\ndef simple_fe(data):\n    '''do some feature engineering\n    '''\n    start = time.time()\n    # rolling demand features\n    data_fe = data[['id', 'demand']]\n    \n    window = 28\n    periods = [7, 15, 30, 90]\n    group = data_fe.groupby('id')['demand']\n    \n    # most recent lag data\n    for period in periods:\n        data_fe['demand_rolling_mean_t' + str(period)] = group.transform(lambda x: x.shift(window).rolling(period).mean())\n\n    periods = [7, 90]\n    for period in periods:\n        data_fe['demand_rolling_std_t' + str(period)] = group.transform(lambda x: x.shift(window).rolling(period).std())\n        \n    # reduce memory\n    data_fe = reduce_mem_usage(data_fe)\n    \n    # get time features\n    data['date'] = pd.to_datetime(data['date'])\n    time_features = ['year', 'month', 'week', 'day', 'dayofweek', 'dayofyear']\n    dtype = np.int16\n    for time_feature in time_features:\n        data[time_feature] = getattr(data['date'].dt, time_feature).astype(dtype)\n        \n    # concat lag and rolling features with main table\n    lag_rolling_features = [col for col in data_fe.columns if col not in ['id', 'demand']]\n    data = pd.concat([data, data_fe[lag_rolling_features]], axis = 1)\n    \n    #data['weekends'] = 0\n    #data.loc[(data['dayofweek'] == 5) | (data['dayofweek'] == 6),'weekends'] = 1\n    data['weekends'] = np.where((data['date'].dt.dayofweek) < 5, 0, 1)\n\n    del data_fe\n    gc.collect()\n    \n    print('Simple feature engineering costs %7.2f seconds'%(time.time()-start))\n    return data\n\ndef run_lgb(data):\n    '''cross validation\n    '''\n    start = time.time()\n    \n    data = data.sort_values('date')\n    \n    x_train = data[data['date'] <= '2016-05-22']\n    y_train = x_train['demand']\n    \n    test = data[(data['date'] > '2016-05-22')]\n    \n\n    del data\n    gc.collect()\n\n    params = {\n        'boosting_type': 'gbdt',\n        'metric': 'rmse',\n        'objective': 'poisson', # loss function\n        'seed': 225,\n        'learning_rate': 0.02,\n        'lambda': 0.4, # l2 regularization\n        'reg_alpha': 0.4, # l1 regularization\n        'max_depth': 5, # max depth of decision trees\n        'num_leaves': 64, #  number of leaves\n        'bagging_fraction': 0.7, # bootstrap sampling\n        'bagging_freq' : 1,\n        'colsample_bytree': 0.7 # feature sampling\n    }\n    \n      \n    oof = np.zeros(len(x_train))\n    preds = np.zeros(len(test))\n    \n    n_fold = 3 #3 for timely purpose of the kernel\n    folds = TimeSeriesSplit(n_splits=n_fold) # use TimeSeriesSplit cv\n    splits = folds.split(x_train, y_train)\n\n    feature_importance_df = pd.DataFrame()\n    \n    for fold, (trn_idx, val_idx) in enumerate(splits):\n        print(f'Training fold {fold + 1}')\n        \n        train_set = lgb.Dataset(x_train.iloc[trn_idx][features], y_train.iloc[trn_idx], categorical_feature = cat)\n        \n        val_set = lgb.Dataset(x_train.iloc[val_idx][features], y_train.iloc[val_idx], categorical_feature = cat)\n\n        model = lgb.train(params, train_set, num_boost_round = 2400, early_stopping_rounds = 50, \n                          valid_sets = [val_set], verbose_eval = 50)\n        \n        \n        lgb.plot_importance(model, importance_type = 'gain', precision = 0,\n                            height = 0.5, figsize = (6, 10), \n                            title = f'fold {fold} feature importance', ignore_zero = True) \n        \n        fold_importance_df = pd.DataFrame()\n        fold_importance_df['feature'] = features\n        fold_importance_df['importance'] = model.feature_importance(importance_type = 'gain')\n        fold_importance_df['fold'] = fold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n        oof[val_idx] = model.predict(x_train.iloc[val_idx][features]) # prediction\n        preds += model.predict(test[features]) / 3 # calculate mean prediction value of 3 models\n        print('-' * 50)\n        print('\\n')\n    model.save_model('model.lgb') # save model\n    del x_train\n        \n    print('3 folds cross-validation costs %7.2f seconds'%(time.time() - start))\n\n    oof_rmse = np.sqrt(metrics.mean_squared_error(y_train, oof))\n    print(f'Our out of folds rmse is {oof_rmse}')\n    del y_train\n        \n    test = test[['id', 'date', 'demand']]\n    test['demand'] = preds\n    gc.collect()\n    return test, feature_importance_df\n\n\ndef predict(test, submission):\n    '''predict test and validation data label\n    '''\n    start = time.time()\n    predictions = test[['id', 'date', 'demand']]\n    predictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'demand').reset_index()\n    predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n    predictions.to_csv('predictions.csv', index = False)\n\n    prediction_val = predictions.copy()\n    prediction_val['id'] = prediction_val['id'].str.replace('_evaluation','_validation') # change id to validation\n    prediction_val.to_csv('prediction_val.csv', index = False)\n    \n    concated = pd.concat([predictions, prediction_val])\n    del predictions, prediction_val,\n    #final = submission[['id']].merge(concated, on = 'id', how='left')\n    #del concated\n    print('final dataset to train has {} rows and {} columns'.format(concated.shape[0], concated.shape[1]))\n    concated.to_csv('submission.csv', index = False)\n    print('Data prediction costs %7.2f seconds'%(time.time() - start))\n    \n\n# define list of features\n\nfeatures = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'sell_price', 'year', \n                'month', 'week', 'day', 'dayofweek', 'dayofyear', 'demand_rolling_mean_t7', 'demand_rolling_mean_t15', 'demand_rolling_mean_t30', 'demand_rolling_mean_t90',\n                'demand_rolling_std_t7', 'demand_rolling_std_t90','weekends']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def train_and_evaluate(): \n    '''套娃\n    '''\n    calendar, sell_prices, sales_train_evaluation, submission = read_data()\n    data = melt_and_merge(calendar, sell_prices, sales_train_evaluation, submission, nrows = 27500000, merge = True)\n    data = transform(data)\n    data['date'] = pd.to_datetime(data['date'])\n    days = abs((data['date'].min() - data['date'].max()).days)\n    # how many training data do we need to train with at least 2 years and consider lags\n    need = 365 + 365 + 90 + 28\n    print(f'We have {(days - 28)} days of training history')\n    print(f'we have {(days - 28 - need)} days left')\n    if (days - 28 - need) > 0:\n        print('We have enought training data, lets continue')\n    else:\n        print('Get more training data, training can fail')\n        \n    data = simple_fe(data)\n    \n    data = reduce_mem_usage(data)\n\n    print('Removing first 118 days')\n    # eliminate the first 118 days of our train data because of lags\n    min_date = data['date'].min() + timedelta(days = 118)\n    data = data[data['date'] > min_date]\n    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n\n    test, feature_importance_df = run_lgb(data)\n    del data\n    fi = (feature_importance_df[['feature', 'importance']]\n        .groupby('feature')\n        .mean()\n        .sort_values(by='importance', ascending=False))\n    fi['feature'] = fi.index\n    plt.figure(figsize=(6,7))\n    sns.barplot(x='importance', y='feature', data=fi[:40])\n    plt.title('LightGBM Features (averaged over folds)')\n    predict(test, submission) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\ntrain_and_evaluate() ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}