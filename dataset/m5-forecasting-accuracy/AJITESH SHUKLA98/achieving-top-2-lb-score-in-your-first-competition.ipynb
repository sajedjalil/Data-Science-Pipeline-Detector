{"cells":[{"metadata":{},"cell_type":"markdown","source":"**<font size=\"4\">Here is a glimpse of the approach I followed in the M-5 Forecasting Accuracy Competition to achieve a Leaderboard position of 2% (silver medal) in my first Kaggle competition:</font>**"},{"metadata":{},"cell_type":"markdown","source":"<font size=\"3\">I started the competition just a week before the final submission deadline, so I didn't had the time to write everything from scratch, which I was going to do before starting. Instead, I looked for information others have already shared, followed the discussions and used the knowledge gained to create my base model. Then I added my own features to that base model, and with some parameter tuning, it turned out to be a great one. Through this approach, I actually gained much more knowledge and applied learning than I could get working fully from scratch.     \n    To those who want a short answer, the key to achieve good results is feature extraction. To those intrested in actual workflow, please follow along. Go through M5-Forecasting Accuracy competition from Walmart before that https://www.kaggle.com/c/m5-forecasting-accuracy/overview/description. Since I used information from many notebooks and discussions, it wouldn't be feasible to reference everyone, but I am thankful to the community, and therefore, am adding to it.\n\nThe steps I followed are shown in subsequent sections:</font>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport plotly_express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom lightgbm import LGBMRegressor\nimport joblib\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"4\">1. Extract the data</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv')\nsales.name = 'sales'\ncalendar = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\ncalendar.name = 'calendar'\nprices = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\nprices.name = 'prices'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"3\">Add new prediction rows, i.e. 1942-1969, which is what we are predicting</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"for d in range(1942,1970):\n    col = 'd_' + str(d)\n    sales[col] = 0\n    sales[col] = sales[col].astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"4\">2. Save the kernel from memory overflow risk</font>"},{"metadata":{},"cell_type":"markdown","source":"<font size=\"3\">If you use the default datatypes assigned to the kernels by dataframe, you will most likely run into memory allocation error, considering such huge dataset. This is what I think is a must for people with less computing power in their PC. I also changed datatypes of all my added features based on convenience/precision required such that minimum possible memory is allocated.</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast(df):\n    cols = df.dtypes.index.tolist()\n    types = df.dtypes.values.tolist()\n    for i,t in enumerate(types):\n        if 'int' in str(t):\n            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n                df[cols[i]] = df[cols[i]].astype(np.int8)\n            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n                df[cols[i]] = df[cols[i]].astype(np.int16)\n            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n                df[cols[i]] = df[cols[i]].astype(np.int32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.int64)\n        elif 'float' in str(t):\n            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n                df[cols[i]] = df[cols[i]].astype(np.float16)\n            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n                df[cols[i]] = df[cols[i]].astype(np.float32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.float64)\n        elif t == np.object:\n            if cols[i] == 'date':\n                df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n            else:\n                df[cols[i]] = df[cols[i]].astype('category')\n    return df  \n\nsales = downcast(sales)\nprices = downcast(prices)\ncalendar = downcast(calendar)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"4\">3. Convert the data from wide to long format (Melting)</font>"},{"metadata":{},"cell_type":"markdown","source":"<font size=\"3\">The conversion to long format enables easier transformation operations to columns, and we can easily monitor the columns for their feature importances/ other factors due to their accessability. </font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.melt(sales, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='d', value_name='sold').dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combine the three datasets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.merge(df, calendar, on='d', how='left')\ndf = pd.merge(df, prices, on=['store_id','item_id','wm_yr_wk'], how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before moving on towards the best part, i.e. feature extraction, let's store the category codes of the category columns we added:"},{"metadata":{"trusted":true},"cell_type":"code","source":"d_id = dict(zip(df.id.cat.codes, df.id))\nd_item_id = dict(zip(df.item_id.cat.codes, df.item_id))\nd_dept_id = dict(zip(df.dept_id.cat.codes, df.dept_id))\nd_cat_id = dict(zip(df.cat_id.cat.codes, df.cat_id))\nd_store_id = dict(zip(df.store_id.cat.codes, df.store_id))\nd_state_id = dict(zip(df.state_id.cat.codes, df.state_id))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"4\">4. Feature Extraction</font>"},{"metadata":{},"cell_type":"markdown","source":"<font size=\"3\">This is the part I liked the most, the part that I mostly did on my own, and the part that helped me achieve the tier. Adding new meaningful features or removing others based on their respective feature importances is the approach I used for this competition.\n\nThe following features are added/converted/deleted:</font>\n"},{"metadata":{},"cell_type":"markdown","source":"<font size=\"3\">a) Transform events with a label encoder and convert to category datatype, use only the number present in the day feature as an integer to get an insight on the time dependence, convert categorical features to respective codes and use day-of-month from the dates feature as others are already there.</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"list1=['event_name_1','event_type_1','event_name_2','event_type_2']\nfrom sklearn.preprocessing import LabelEncoder\nfor i in list1:\n    df[i] = df[i].cat.add_categories(\"nan\").fillna(\"nan\")\n    df[i]=LabelEncoder().fit_transform(df[i]).astype(np.int8)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.d = df['d'].apply(lambda x: x.split('_')[1]).astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = df.dtypes.index.tolist()\ntypes = df.dtypes.values.tolist()\nfor i,type in enumerate(types):\n    if type.name == 'category':\n        df[cols[i]] = df[cols[i]].cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extracting day_of_week has shown some memory errors, therefore I have dropped the date, \n# it is a good feature though, try to incorporate it and let me know the reason for error \n# df['date'] = df['date'].apply(lambda x: x.strftime('%d')).astype(np.int8)\ndf.drop(['date'],axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#make datatype of event as category\nfor i in list1:\n    df[i]=df[i].astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"3\">b) Introduce lags for number of items sold </font>"},{"metadata":{},"cell_type":"markdown","source":"Lags help us in a time series dataset by enabling to use previous values of a feature as aother feature.           \n**Note**: Use only **lags starting from 28 days** as any less than that is going to give poor test results since some of the 28 prediction columns will remain 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"# I have used 4 lags here in intervals of 7, all showed a good value of feature importance\nlags = [28,35,42,49]\nfor lag in lags:\n    df['sold_lag_'+str(lag)] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],as_index=False)['sold'].shift(lag).astype(np.float16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"3\">c) Introduce lags for events</font>"},{"metadata":{},"cell_type":"markdown","source":"My reasoning for this is that people usually do shopping a day before, or weekend before an important event and not on the event itself. Also the sales may dip immediately after the day of event (it was just a guess but gave a good feature to use)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# I have added 4 days surrounding the event as features\nlags2 = [-2,-1,1,2]\nfor lag in lags2:\n    df['event1_lag_'+str(lag)] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],as_index=False)['event_name_1'].shift(lag).astype(np.float16)\n    df['event1_lag_'+str(lag)].fillna(100, inplace=True)\n    df['event1_lag_'+str(lag)]=df['event1_lag_'+str(lag)].astype(np.int8)\n    df['event1_lag_'+str(lag)]=df['event1_lag_'+str(lag)].astype('category')\n# event type didn't showed a good feature importance, opposite to event itself\n# for lag in lags2:\n#     df['eventtype1_lag_'+str(lag)] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],as_index=False)['event_type_1'].shift(lag).astype(np.float16).fillna(100, inplace=True)\n#     df['eventtype1_lag_'+str(lag)].fillna(100, inplace=True)\n#     df['eventtype1_lag_'+str(lag)]=df['eventtype1_lag_'+str(lag)].astype(np.int8)\n#     df['eventtype1_lag_'+str(lag)]=df['eventtype1_lag_'+str(lag)].astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"3\">d) Apply mean encodings, atleast of the 12 aggregations they have taken in their WRMSE document</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['item_sold_avg'] = df.groupby('item_id')['sold'].transform('mean').astype(np.float16)    \ndf['state_sold_avg'] = df.groupby('state_id')['sold'].transform('mean').astype(np.float16)    #total 3 unique values, 1 for each state\ndf['store_sold_avg'] = df.groupby('store_id')['sold'].transform('mean').astype(np.float16)  #10 unique values\ndf['cat_sold_avg'] = df.groupby('cat_id')['sold'].transform('mean').astype(np.float16)\ndf['dept_sold_avg'] = df.groupby('dept_id')['sold'].transform('mean').astype(np.float16)\ndf['cat_dept_sold_avg'] = df.groupby(['cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)\ndf['store_item_sold_avg'] = df.groupby(['store_id','item_id'])['sold'].transform('mean').astype(np.float16)\ndf['cat_item_sold_avg'] = df.groupby(['cat_id','item_id'])['sold'].transform('mean').astype(np.float16)\ndf['dept_item_sold_avg'] = df.groupby(['dept_id','item_id'])['sold'].transform('mean').astype(np.float16)\ndf['state_store_sold_avg'] = df.groupby(['state_id','store_id'])['sold'].transform('mean').astype(np.float16)\ndf['state_store_cat_sold_avg'] = df.groupby(['state_id','store_id','cat_id'])['sold'].transform('mean').astype(np.float16)\ndf['store_cat_dept_sold_avg'] = df.groupby(['store_id','cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"3\">e) Currently week id is not encoded and has large values, taking higher space. Reduce the space by encoding with integers (int64->int8).</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['wm_yr_wk_linear']=LabelEncoder().fit_transform(df['wm_yr_wk'].values).astype(np.int16)\n\ndf.drop(['wm_yr_wk'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"3\">f) Add a price-diff feature which gives the increase/decrease in price of an item at a store from it's previous week value. Its reasoning is pretty obvious.</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['price_lag'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],as_index=False)['sell_price'].shift(7).astype(np.float16)\ndf['price-diff']=df['price_lag']-df['sell_price']\ndf.drop(['price_lag'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"3\">g) Decimal feature which shows the decimal part of the price. I just used it to gain more insights from price feature as I think that's the most important feature here (it may show some user interaction with prices, discrepancies with change, etc)</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sell_price'].fillna(-1,inplace=True)\ndf['decimal']=df['sell_price'].apply(lambda x: 100*(x-int(x))).astype(np.int16)\ndf['sell_price'].replace(-1,np.nan,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"3\">h) Difference of price of an item at a store from it's current trend/tragectory </font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['expanding_price_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sell_price'].transform(lambda x: x.expanding(2).mean()).astype(np.float16)\ndf['diff_moving_mean']=df['expanding_price_mean']-df['sell_price']\ndf.drop(['expanding_price_mean'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some more memory reduction to save the kernel:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['price-diff']=df['price-diff'].astype(np.float16)\ndf.drop(['wday'], axis=1, inplace=True)\ndf['decimal']=df['decimal'].astype(np.int8)\ndf['year']=LabelEncoder().fit_transform(df['year']).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"3\">h) A trend feature for price which gives difference from overall mean, instead of a expending mean as used previously.</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['daily_avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id','d'])['sell_price'].transform('mean').astype(np.float16)\ndf['avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sell_price'].transform('mean').astype(np.float16)\ndf['selling_trend'] = (df['daily_avg_sold'] - df['avg_sold']).astype(np.float16)\ndf.drop(['daily_avg_sold','avg_sold'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop features with about zero feature importance to save computation"},{"metadata":{"trusted":true},"cell_type":"code","source":"list3=['cat_id','state_id']\nfor i in list3:\n    df.drop([i], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save Dataframe to output. Remove some start days as they are not as much important and lags there are empty."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[df['d']>=49]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf.to_pickle('data.pkl')\ndel df, sales, prices, calendar\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"4\">5. Model preperation and evaluation</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_pickle('data.pkl')\nvalid = data[(data['d']>=1599) & (data['d']<1942)][['id','d','sold']]  \nvalid_csv=data[(data['d']>=1914) & (data['d']<1942)][['id','d','sold']]\ntest = data[data['d']>=1942][['id','d','sold']]\neval_preds = test['sold']\nvalid_preds = valid['sold']\nvalid_preds_csv=valid_csv['sold']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Store categorical columns to manually provide as input to LGBM Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_column=[]\nfor i in data.columns:\n    if(str(data.dtypes[i])=='category'):\n        cat_column.append(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"3\">LGBM model training.      \nNote: I have tried train test split of 1 year to do some tunning of parameters and then trained the model on all the problem set, i.e. day-1 to day-1914 to recieve output. \nThe best approach here will be to do a time series CV, and you may try it if you can compute that.</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"for store in d_store_id:\n    df = data[data['store_id']==store]\n    \n    #Split the data\n    X_train, y_train = df[df['d']<1914].drop('sold',axis=1), df[df['d']<1914]['sold']\n    # Uncover if you want to use last year for train\n    #X_valid, y_valid = df[(df['d']>=1599) & (df['d']<1942)].drop('sold',axis=1), df[(df['d']>=1599) & (df['d']<1942)]['sold']\n    X_valid_csv, y_valid_csv = df[(df['d']>=1914) & (df['d']<1942)].drop('sold',axis=1), df[(df['d']>=1914) & (df['d']<1942)]['sold']\n    X_test = df[df['d']>=1942].drop('sold',axis=1)\n    \n    #Train and validate\n    model = LGBMRegressor(\n        learning_rate= 0.05,\n        subsample=0.6,\n        feature_fraction=0.6,\n        num_iterations = 1200,\n        max_bin=350,\n        num_leaves= 100,\n        lambda_l2=0.003,\n        max_depth=200,\n        min_data_in_leaf= 80,\n        force_row_wise= True,\n    )\n    print('*****Prediction for Store: {}*****'.format(d_store_id[store]))\n    model.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_valid_csv,y_valid_csv)],\n             eval_metric='rmse',  verbose=100, early_stopping_rounds=20,categorical_feature=cat_column)\n    valid_preds_csv[X_valid_csv.index] = model.predict(X_valid_csv)\n    eval_preds[X_test.index] = model.predict(X_test)\n    filename = 'model'+str(d_store_id[store])+'.pkl'\n    # save model\n    joblib.dump(model, filename)\n    del model, X_train, y_train, X_valid_csv, y_valid_csv\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance_df = pd.DataFrame()\nfeatures = [f for f in data.columns if f != 'sold']\nfor filename in os.listdir('/kaggle/working/'):\n    if 'model' in filename:\n        # load model\n        model = joblib.load(filename)\n        store_importance_df = pd.DataFrame()\n        store_importance_df[\"feature\"] = features\n        store_importance_df[\"importance\"] = model.feature_importances_\n        store_importance_df[\"store\"] = filename[5:9]\n        feature_importance_df = pd.concat([feature_importance_df, store_importance_df], axis=0)\n    \ndef display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:].index\n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    plt.figure(figsize=(8, 10))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (averaged over store predictions)')\n    plt.tight_layout()\n    \ndisplay_importances(feature_importance_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"4\">6. Submission</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get the actual validation results\nvalid_csv['sold'] = valid_preds_csv\nvalidation = valid_csv[['id','d','sold']]\nvalidation = pd.pivot(validation, index='id', columns='d', values='sold').reset_index()\nvalidation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]\nvalidation.id = validation.id.map(d_id).str.replace('evaluation','validation')\n\n#Get the evaluation results\ntest['sold'] = eval_preds\nevaluation = test[['id','d','sold']]\nevaluation = pd.pivot(evaluation, index='id', columns='d', values='sold').reset_index()\nevaluation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]\n#Remap the category id to their respective categories\nevaluation.id = evaluation.id.map(d_id)\n\n#Prepare the submission\nsubmit = pd.concat([validation,evaluation]).reset_index(drop=True)\n# submit.memory_usage().sum()\nsubmit.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do some post processing in the sub file also, like replacing the predictions below zero with zero as they are just increasing your error. Had I seen this before, I'd have been on the gold tier instead of silver. Analyzing the excel/result file is important."},{"metadata":{},"cell_type":"markdown","source":"<font size=\"3\">Thank you all for being patient. I wrote this notebook in relation with the M-5 competition but I wanted to provide people, mostly newcomers a general approach to have a good start in Kaggle. The notebooks/discussions are very valuable for general practical learning in Machine Learning and I would recommend every newcomer/member to involve in them to gain more knowledge.</font>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}