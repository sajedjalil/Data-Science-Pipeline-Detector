{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Reshape the data, basic EDA & a baseline model\n* Naive baseline - mean on that day of week per store, or the last sale value\n* reshape from columnar (wide format) to long\n\n* Some eda code borrowed from here: https://www.kaggle.com/rdizzl3/eda-and-baseline-model\n\n\n* reshaping note: some items may only start to be sold after a certain date - would be best to cutoff them off before that to avoid noise in the model. e.g. take first index/col >0 as start ? \n\n* Issue: currently, notebook crashes when reshaping/pivoting :|. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_sales = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv')\nprint(f\"train shape {train_sales.shape}\")\nsubmission_file = pd.read_csv('../input/m5-forecasting-accuracy/sample_submission.csv')\nprint(f\"submission_file shape {submission_file.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####### We are given previous data days sales in the sales_train_validation dataset.\n\n* d_1914 - d_1941 represents the validation row\n* d_1942 - d_1969 represents the evaluation rows.\n    * WE could drop them from the pivoting data (leave ot for testing prediction rows) , or leave them in then split later for easy creation of test set data in smae format"},{"metadata":{"trusted":true},"cell_type":"code","source":"days = range(1, 1913 + 1)\ntime_series_columns = [f'd_{i}' for i in days]\n\ntime_series_data = train_sales[time_series_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_sales.columns)\nid_df_columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales[id_df_columns].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales[id_df_columns + time_series_columns].head(2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##opt - drop out test set rows\n# train_sales = train_sales[id_df_columns + time_series_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        #else: df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## reduce memory usage. There's an imporved version of this function that also saves data as categoircals type, but that can affect joins if not handled explicitly\n# train_sales = reduce_mem_usage(train_sales)\n\n### we know the max range of the sales cols, let's just set them all to int 16 (some are int8 , but that doesn't matter if we 'll cast it)\ndisplay(train_sales.info())\ntrain_sales[time_series_columns] = train_sales[time_series_columns].astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_sales[id_df_columns] = train_sales[id_df_columns].astype('category')\ndisplay(train_sales.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_series_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metadata"},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar = pd.read_csv(\"../input/m5-forecasting-accuracy/calendar.csv\",parse_dates=[\"date\"])\nprint(calendar.shape)\nprices = pd.read_csv(\"../input/m5-forecasting-accuracy/sell_prices.csv\")\nprint(prices.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.tail(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#no need to keep the textual weekday name, we have it from wday + data. Saturday = 1,Sunday\t2, Friday\t7\ncalendar.drop(\"weekday\",axis=1,inplace=True)\n\n## we  drop the prefix from the calendar date/d column for easy merging with sales data. .\ncalendar[\"d\"] = calendar[\"d\"].replace(\"d_\",\"\",regex=True).astype(int)\ncalendar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prices","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reshape sales data to long format\n* Also join with calendar data\n\n* The IDs being set to categorical type slows this down immensely\n\n* due to memory - we may wish to split this into sub frames, them concat them. e.g. split by state or store_id?\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"After reshaping to 1 row per id per day/date, we would have: {train_sales.shape[0]*time_series_data.shape[1]} rows\")\n## 58 million rows. many sparse likely","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npd.wide_to_long(train_sales.head(3),\"d_\",i=id_df_columns,j=\"sales\").reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stores_list = list(set(train_sales[\"store_id\"]))\nstores_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n### reshape incrementally - hopefully this will help with memory errors\ndfs= []\nfor st in stores_list:  \n    df = train_sales.loc[train_sales[\"store_id\"]==st]#.head()\n    dfs.append(pd.wide_to_long(df,\"d_\",i=id_df_columns,j=\"day\").reset_index())\n    \ndf = pd.concat(dfs)\ndf.rename(columns={\"d_\":\"sales\"})\ndel(dfs)\nprint(df.shape)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# train_sales = pd.wide_to_long(train_sales,\"d_\",i=id_df_columns,j=\"sales\").reset_index()\n# print(train_sales.shape)\n# train_sales","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv(\"sales_basic_v1_all.csv.gz\",index=False,compression=\"gzip\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Predictions\n* We need to provide predictions for the next 28 days for each of the series. For the validation series that is days 1914 - 1941 and for the evaluation that is days 1942 - 1969.\n* https://www.kaggle.com/rdizzl3/eda-and-baseline-model"},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_ids = train_sales['id'].values\nevaluation_ids = [i.replace('validation', 'evaluation') for i in validation_ids]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = np.concatenate([validation_ids, evaluation_ids])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = pd.DataFrame(ids, columns=['id'])\nforecast = pd.concat([forecast] * 2).reset_index(drop=True)\npredictions = pd.concat([predictions, forecast], axis=1)\npredictions.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}