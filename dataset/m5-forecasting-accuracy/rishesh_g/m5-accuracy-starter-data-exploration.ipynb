{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# M5 Forecasting Challenge","metadata":{}},{"cell_type":"markdown","source":"Hey Guys!! This is my first Notebook I am submitting here on this platform. There may be many mistakes here but kindly bear with me. Any comments would be highly appreciated.\n  In this notebook i have tried to give a visual overview of the data presented in the competition by means of various graphs in order to gain a better understanding of our objective in this competition.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('bmh')\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nmin_scaler = MinMaxScaler()\nscaler = StandardScaler()\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install jovian","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jovian","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_DIR = '../input/m5-forecasting-accuracy'\nclndr = pd.read_csv(f'{INPUT_DIR}/calendar.csv')\ndf_val = pd.read_csv(f'{INPUT_DIR}/sales_train_validation.csv')\nsubmsn = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv')\nprc = pd.read_csv(f'{INPUT_DIR}/sell_prices.csv')\ndf = pd.read_csv(f'{INPUT_DIR}/sales_train_evaluation.csv')","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sizeof_fmt(num, suffix='B'):\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f%s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f%s%s\" % (num, 'Yi', suffix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2 \n    print ('Initial Usage = {:5.2f} Mb'.format(start_mem))\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                       df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduce_mem_usage(df)\nreduce_mem_usage(clndr)\nreduce_mem_usage(prc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clndr['date'] = pd.to_datetime(clndr.date)\nclndr['days'] = clndr['date'].dt.day","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cln = clndr[:1941]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dates = cln['date']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### *Data Pre-processing*","metadata":{}},{"cell_type":"code","source":"import time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#da = clndr[0:1913]\nda = clndr.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"da['event_name_1'] = da['event_name_1'].apply(lambda x: np.where(pd.isnull(x),0,1))\nda['event_type_1'] = da['event_type_1'].apply(lambda x: np.where(pd.isnull(x),0,1))\nda['event_name_2'] = da['event_name_2'].apply(lambda x: np.where(pd.isnull(x),0,1))\nda['event_type_2'] = da['event_type_2'].apply(lambda x: np.where(pd.isnull(x),0,1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"da = da.iloc[:,:14]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"da['date'] = pd.to_datetime(da.date)\nda.set_index('date',inplace=True)\nda.drop(['wm_yr_wk','weekday','d'],axis = 1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l1 = ['wday','month','year']\nfor col in l1:\n    da[col] = da[col].astype('category')\nda = pd.get_dummies(da,columns=l1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def SMA(data):\n    return data.mean()  \n\ndef std(data):\n    return data.std()\n\ndef EMA(data):\n    dats = data.astype(float)\n    data['EMA'] = dats.ewm(span = 20).mean()\n    return data['EMA']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = [i for i in df.columns if 'd_' in i ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"date_tr = clndr['date']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_sale(item):\n    tmp = df.set_index(df_val['id']).loc[item,cols]\n    tmp = tmp.reset_index().drop('index',axis = 1).rename(columns = {0:item})\n    return pd.merge(date_tr,tmp,left_index = True,right_index=True).set_index('date')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dframe(item):\n    tmp = da.copy()\n    item_sale = get_sale(item)\n    \n    #tmp.replace('nan', np.nan).fillna(0)\n    tmp = pd.merge(tmp,item_sale, left_index=True, right_index=True, how = 'left')\n    tmp.rename(columns = {item:'item'},inplace =True)\n    \n    for i in (1,7,14,28,365):\n        tmp['lag_'+str(i)] = tmp['item'].transform(lambda x: x.shift(i))\n    \n    \n    for i in [7,14,28,60,180,365]:\n        tmp['rolling_mean_'+str(i)] = tmp['item'].transform(lambda x: x.shift(28).rolling(i).mean())\n        tmp['rolling_std_'+str(i)]  = tmp['item'].transform(lambda x: x.shift(28).rolling(i).std())\n    \n    \n    tmp = tmp.replace('nan', np.nan).fillna(0)\n    \n    return tmp.to_numpy()\n    \n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### *Trying CNNs* ","metadata":{}},{"cell_type":"markdown","source":"Data Preparation","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset,DataLoader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\ny_scaler = MinMaxScaler()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''dx = dframe(item)\nxdata = dx.copy()'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''trains = xdata[:1500]\ntests = xdata[1500:1913]\nval = xdata[1885:1941]\nevl = xdata[1913:]'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sliding_windows_mutli_features(data, seq_length):\n    x = []\n    y = []\n    data_x = scaler.fit_transform(data)\n    data_y = data[:,32].reshape(-1,1)\n\n    for i in range((data.shape[0])-seq_length-seq_length+1):\n        #print (data.shape[0])\n        #print (len(data)-seq_length-1)\n        #print (i,(i+seq_length))\n        _x = data_x[i:(i+seq_length),:] ## 16 columns for features  \n        _y = data_y[i+seq_length:i+seq_length+seq_length] ## column 0 contains the labbel\n        #print ('x - ',_x)\n        #print ('y - ',_y)\n        x.append(_x)\n        y.append(_y)\n\n    return np.array(x),np.array(y).reshape(-1,28)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''train_x,train_y = sliding_windows_mutli_features(trains, seq_length=28)\ntest_x,test_y = sliding_windows_mutli_features(tests, seq_length=28)\nval_x,val_y = sliding_windows_mutli_features(val, seq_length=28)\nevl_x,evl_y = sliding_windows_mutli_features(evl, seq_length=28)\n\nprint(train_x.shape)\nprint (train_y.shape)\nprint (test_x.shape)\nprint (test_y.shape)\nprint (val_x.shape)\nprint (val_y.shape)\nprint (evl_x.shape)\nprint (evl_y.shape)\n\ntrain_set = FeatureDataset(train_x,train_y)\ntest_set = FeatureDataset(test_x,test_y)\nval_set = FeatureDataset(val_x,val_y)\nevl_set = FeatureDataset(evl_x,evl_y)\n\ntrain_loader = DataLoader(dataset = train_set,\n                         batch_size = 10\n                         )\ntest_loader = DataLoader(dataset = test_set,\n                         batch_size = 10\n                         )\nval_loader = DataLoader(dataset = val_set,\n                         batch_size = 1\n                         )\nevl_loader = DataLoader(dataset = evl_set,\n                         batch_size = 1\n                         )'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeatureDataset(Dataset):\n    def __init__(self,feature,target):\n        self.feature = feature\n        self.target = target\n    \n    def __len__(self):\n        return len(self.feature)\n    \n    def __getitem__(self,idx):\n        item = self.feature.reshape(self.feature.shape[0],self.feature.shape[2],self.feature.shape[1])[idx]\n        label = self.target[idx]\n    \n        \n        return item,label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pre_process(xdata,split,seq_length):\n    trains = xdata[:split]\n    tests = xdata[split:1913]\n    val = xdata[1885:1941]\n    evl = xdata[1913:]\n    \n    \n    train_x,train_y = sliding_windows_mutli_features(trains, seq_length)\n    test_x,test_y = sliding_windows_mutli_features(tests, seq_length)\n    val_x,val_y = sliding_windows_mutli_features(val, seq_length=28)\n    evl_x,evl_y = sliding_windows_mutli_features(evl, seq_length=28)\n    \n    train_set = FeatureDataset(train_x,train_y)\n    test_set = FeatureDataset(test_x,test_y)\n    val_set = FeatureDataset(val_x,val_y)\n    evl_set = FeatureDataset(evl_x,evl_y)\n    \n    train_loader = DataLoader(dataset = train_set,\n                         batch_size = 500\n                         )\n    test_loader = DataLoader(dataset = test_set,\n                         batch_size = 300\n                         )\n    val_loader = DataLoader(dataset = val_set,\n                         batch_size = 1\n                         )\n    evl_loader = DataLoader(dataset = evl_set,\n                         batch_size = 1\n                         )\n    \n    return train_loader,test_loader,val_loader,evl_loader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''pre_process(xdata,1500,28)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Building Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm_notebook as tqdm\nfrom torch.nn.utils import weight_norm\nimport gc\nimport collections\nimport random","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNN_Forecast(nn.Module):\n    def __init__(self,c_in,c_bw,c_out,ks=3,d=2,s=1):\n        super().__init__()\n        self.conv1 = weight_norm(nn.Conv1d(c_in,c_bw,kernel_size=ks,dilation = d,padding = int((d*(ks-1))/2),stride = s))\n        self.relu = nn.ReLU(inplace = True)\n        self.conv2 = weight_norm(nn.Conv1d(c_bw,c_out,kernel_size=ks,dilation = d,padding = int((d*(ks-1))/2),stride = s))\n        self.dp = nn.Dropout(0.2)\n        self.shortcut = lambda x: x\n        if c_in != c_out:\n            self.shortcut = nn.Conv1d(c_in, c_out,kernel_size = 1,stride=1)\n        \n        y = torch.randn(c_in,28).view(-1,c_in,28)\n        self.to_linear = None\n        self.convs(y)\n        \n        self.fc1 = nn.Linear(self.to_linear,512)\n        self.fc2 = nn.Linear(512,256)\n        self.fc3 = nn.Linear(256,28)\n        \n    \n    def convs(self,x):\n        r = self.shortcut(x)\n        x = self.relu(self.conv1(x))\n        x = self.dp(x)\n        x = F.leaky_relu(self.conv2(x),0.1)\n        x = self.dp(x)\n        #print ('r - ',r.shape)\n        #print ('x - ',x.shape)\n        r = r.view(x.shape)\n        \n        if self.to_linear is None:\n            self.to_linear = x[0].shape[0]*x[0].shape[1]\n        return x.add_(r)\n        \n    def forward(self,x):\n        x = self.convs(x)\n        x = x.view(-1,self.to_linear)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        \n        return x\n\nmodel = CNN_Forecast(50,64,32)\nprint (model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def conv1d(c_in,c_out,stride= 1,dilation = 1,ks = 3):\n    pad = int((dilation*(ks-1))/2)\n    return nn.Conv1d(c_in,c_out,kernel_size = ks, stride = stride, dilation = dilation, padding = pad)\n\ndef reg(max_ks):\n    return nn.Sequential(nn.ReLU(inplace = True),\n                         nn.Dropout(0.2),\n                         nn.MaxPool1d(max_ks))\n\n\ndef size(s_len,ks,d,c_out,pool,max_ks=1,s=1):\n    pad = pad = int((d*(ks-1))/2)\n    l_out = (s_len+2*pad-d*(ks-1)-1)/s + 1\n    if not pool:\n        size = c_out*l_out\n    else:\n        l_out = (l_out-(max_ks-1)-1)/max_ks + 1\n        size = c_out*l_out\n    \n    return int(size)\n\n    \nclass ConvNet(nn.Module):\n    def __init__(self,n_start,c_out,ks,d,max_ks):\n        super().__init__()\n        self.conv0 = conv1d(n_start,c_out[0],ks = ks[0],dilation = d[0])\n        self.conv1 = conv1d(n_start,c_out[1],ks = ks[1],dilation = d[1])\n        self.conv2 = conv1d(n_start,c_out[2],ks = ks[2],dilation = d[2])\n        \n        self.reg0 = reg(max_ks)\n        self.reg1 = reg(max_ks)\n        self.reg2 = reg(max_ks)\n        \n        self.size0 = size(28,ks[0],d[0],c_out[0],max_ks = max_ks,pool = True)\n        self.size1 = size(28,ks[1],d[1],c_out[1],max_ks = max_ks,pool = True)\n        self.size2 = size(28,ks[2],d[2],c_out[2],max_ks = max_ks,pool = True)\n        \n        self.fc1 = nn.Linear((self.size0+self.size1+self.size2),100)\n        self.fc2 = nn.Linear(100,1)\n\n    def forward(self,x):\n        x0 = self.reg0(self.conv0(x))\n        x1 = self.reg1(self.conv1(x))\n        x2 = self.reg2(self.conv2(x))\n        \n        x0 = x0.view(-1,self.size0)\n        x1 = x1.view(-1,self.size1)\n        x2 = x2.view(-1,self.size2)\n        \n        x = torch.cat([x0,x1,x2],1)\n        \n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        \n        return x\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = ConvNet(50,c_out = [50,64,78],ks = [5,7,11],d = [2,2,2],max_ks = 2).double().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  patience=500,factor =0.5 ,min_lr=1e-7, eps=1e-08)\ncriterion = nn.MSELoss()'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''train_losses = []\nvalid_losses = []\ndef Train():\n    \n    running_loss = .0\n    \n    model.train()\n    \n    for idx, (inputs,labels) in enumerate(train_loader):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        preds = model(inputs)\n        loss = criterion(preds,labels)\n        loss.backward()\n        scheduler.step(loss)\n        optimizer.step()\n        running_loss += loss\n        \n    train_loss = running_loss/len(train_loader)\n    train_losses.append(train_loss.cpu().detach().numpy())\n    \n    print(f'train_loss {train_loss}')\n    \ndef Valid():\n    running_loss = .0\n    \n    model.eval()\n    \n    with torch.no_grad():\n        for idx, (inputs, labels) in enumerate(test_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            preds = model(inputs)\n            loss = criterion(preds,labels)\n            scheduler.step(loss)\n            running_loss += loss\n            \n        valid_loss = running_loss/len(test_loader)\n        valid_losses.append(valid_loss.cpu().detach().numpy())\n        print(f'valid_loss {valid_loss}')\n        \nepochs = 50\nfor epoch in range(epochs):\n    print('epochs {}/{}'.format(epoch+1,epochs))\n    Train()\n    Valid()\n    gc.collect()'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import matplotlib.pyplot as plt\nplt.plot(train_losses,label='train_loss')\nplt.plot(valid_losses,label='valid_loss')\nplt.title('MSE Loss')\nplt.xlim(0,epochs)\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''prediction = [0]*385\nrunning_loss = 0\nmodel.eval()\nwith torch.no_grad():\n        for idx, (inputs, labels) in enumerate(test_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            preds = model(inputs)\n            prediction[idx] = preds.cpu().detach().numpy()\n            loss = criterion(preds,labels)\n            running_loss += loss\n        test_loss = running_loss/len(test_loader)\n        print(f'test_loss {test_loss}')'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#res = y_scaler.inverse_transform(prediction)\n'''res = pd.DataFrame(prediction)\nres.index = test_ydates\nres.rename({0:'item'},axis =1,inplace=True)\nres.reset_index(inplace=True)\n\n#res1 = y_scaler.inverse_transform(prediction)\nres1 = pd.DataFrame(prediction)\nres1.index = test_ydates\nres1.rename({0:'item'},axis =1,inplace=True)\nres1.reset_index(inplace=True)\n\ny_tests = pd.DataFrame(test_y)\ny_tests.index = test_ydates\ny_tests.rename(columns = {0:'item'},inplace = True)\n\nactual = y_tests.reset_index()'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''prediction = [0]*28\nrunning_loss = 0\nmodel.eval()\nwith torch.no_grad():\n        for idx, (inputs, labels) in enumerate(val_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            preds = model(inputs)\n            prediction[idx] = preds.cpu().detach().numpy()\n            loss = criterion(preds,labels)\n            running_loss += loss\n        test_loss = running_loss/len(val_loader)\n        print(f'test_loss {test_loss}')'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"it = ['HOBBIES_1_002_CA_1_validation','HOBBIES_1_004_CA_1_validation','HOBBIES_1_003_CA_1_validation','HOBBIES_1_001_CA_1_validation']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Trainings1(product):\n    running_loss = .0\n    model.train()\n    \n    for idx, (inputs,labels) in enumerate(train_loader):\n        inputs = inputs.to(device)\n        labels = labels.to(device,non_blocking = True)\n        optimizer.zero_grad()\n        preds = model(inputs)\n        loss = criterion(preds,labels)\n        loss.backward()\n        #scheduler.step(loss)\n        optimizer.step()\n        running_loss += loss\n        \n        #print (f\"epoch {epoch}, datapoints {idx*10}\")\n    \n    train_loss = running_loss/len(train_loader)\n    train_losses[str(product)].append(train_loss.cpu().detach().numpy().item())\n        \n        \ndef Validations1(product):\n    running_loss = .0\n    model.eval()\n    \n    with torch.no_grad():\n        for idx, (inputs, labels) in enumerate(test_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device,non_blocking = True)\n            optimizer.zero_grad()\n            preds = model(inputs)\n            loss = criterion(preds,labels)\n            #scheduler.step(loss)\n            running_loss += loss\n            #print (f\"epoch {epoch}, datapoints {idx*10}\")\n            \n        valid_loss = running_loss/len(test_loader)\n        valid_losses[str(product)].append(valid_loss.cpu().detach().numpy().item())\n            \ndef Predicts1(product):\n    with torch.no_grad():\n            for idx, (inputs,_) in enumerate(val_loader):\n                inputs = inputs.to(device)\n                optimizer.zero_grad()\n                preds = model(inputs)\n                pred_dict.update({product:preds.cpu().detach().numpy().reshape(28,).tolist()})\n\n            \n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_cv_loss = pd.DataFrame()\nvalid_cv_loss = pd.DataFrame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''start1 = time.time()\n#start = time.time()\nseq_length = 28\nspilt = 1500\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ncriterion = nn.MSELoss()\nepochs = 20\n\ntrain_losses = collections.defaultdict(list)\nvalid_losses = collections.defaultdict(list)\nprediction = [0]*28\npred_dict = {}\n#print ('Time Taken for Initialisation = {}s'.format((time.time() - start))*100)\n\nfirst = True\n\nmodel = CNN_Forecast(50,64,32,ks = 11,d=3).double().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  patience=500,factor =0.5 ,min_lr=1e-7, eps=1e-08)\n\nfor product in tqdm(df_val['id']):\n    \n    \n    #print (product)\n    #start = time.time()\n    dset = dframe(product)\n    #dset = np.load('.'.join([product,'npy']))\n    #print ('Time Taken for Creating Dataset = {}s'.format((time.time() - start))*100)\n    #print ('-----Dateset Created-----')\n    #start = time.time()\n    train_loader,test_loader,val_loader,_ = pre_process(dset,split,seq_length)\n    #print ('Time Taken for Creating Loaders = {}s'.format((time.time() - start))*100)\n    #print ('Loaders Ready !!!')\n\n    #print ('Training')\n    #start = time.time()\n    for epoch in range(epochs):\n        Trainings1(product)\n        Validations1(product)\n    #print ('Time Taken for Validating Model = {}s'.format((time.time() - start))*100)\n    Predicts1(product)\ntrain_losses = pd.DataFrame(train_losses)\nvalid_losses = pd.DataFrame(valid_losses)\npred_d = pd.DataFrame(pred_dict)\ntrain_cv_loss = pd.concat([train_cv_loss,train_losses],axis = 1)\nvalid_cv_loss = pd.concat([valid_cv_loss,valid_losses],axis = 1)\ntrain_losses = collections.defaultdict(list)\nvalid_losses = collections.defaultdict(list)\n    \n    \nend = time.time()\nprint('Time taken is {}s'.format(end-start1))'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Trainings1(product):\n    model.train()\n    \n    for idx, (inputs,labels) in enumerate(train_loader):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        preds = model(inputs)\n        loss = criterion(preds,labels)\n        loss.backward()\n        #scheduler.step(loss)\n        optimizer.step()\n        \n        \ndef Validations1(product):\n    model.eval()\n    \n    with torch.no_grad():\n        for idx, (inputs, labels) in enumerate(test_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            preds = model(inputs)\n            loss = criterion(preds,labels)\n            #scheduler.step(loss)\n\n            \ndef Predicts1(product):\n    with torch.no_grad():\n            for idx, (inputs,_) in enumerate(val_loader):\n                inputs = inputs.to(device)\n                optimizer.zero_grad()\n                preds = model(inputs)\n                pred_dict.update({product:preds.cpu().detach().numpy().reshape(28,).tolist()})\n\n            \n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start1 = time.time()\n#start = time.time()\nseq_length = 28\nsplit = 1500\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ncriterion = nn.MSELoss()\nepochs = 20\n\nprediction = [0]*28\npred_dict = {}\n\nfirst = True\n\nmodel = CNN_Forecast(50,64,32,ks = 11,d=3).double()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  patience=500,factor =0.5 ,min_lr=1e-7, eps=1e-08)\n\nidir_torch2 = '../input/torch-model-new'\nloaded = torch.load(f'{idir_torch2}/cpoints_new.pth')\n\nmodel.load_state_dict(loaded['model_state'])\noptimizer.load_state_dict(loaded['optim_state'])\nmodel.to(device)\nfor state in optimizer.state.values():\n    for k, v in state.items():\n         if isinstance(v, torch.Tensor):\n            state[k] = v.cuda()\n            \nfor product in tqdm(df_val['id']):\n    \n    \n    dset = dframe(product)\n    #dset = np.load('.'.join([product,'npy']))\n    train_loader,test_loader,val_loader,evl_loader = pre_process(dset,split,seq_length)\n    \n    '''for epoch in range(epochs):\n        Trainings1(product)\n        Validations1(product)'''\n   \n    Predicts1(product)\n\npred_d = pd.DataFrame(pred_dict).T\npred_d.to_csv('predictions1.csv')    \n    \nend = time.time()\n\nprint('Time taken is {}s'.format(end-start1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(pred_dict).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_dset1.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = {\n        \"model_state\":model.state_dict(),\n        \"optim_state\":optimizer.state_dict()\n    }\n    \ntorch.save(checkpoint,'cpoints_new.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_d.to_csv('predictions_val.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_d","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#64,32 bs = 500\n#28,28 bs = 500\n#100,200 bs = 500\n#64,32 ks = 20 bs = 500\n#50,64,32,ks = 11,d=4 bs = 1913\n#bs = 200 and 100\n#bs = 500 and 300 64,32 11,3 e = 20\n#e = 15\n#e = 30\n#e = 5\n#e = 15 t = 8hrs for saved else 11 hrs\n#e = 20 t = 13hrs for not saved else 11.25\n#e = 20 t = 10hr no scheduler saved model\n#e= 20 t = 12hrs with scheduler saved model\n#e= 20 t = 10.5hrs with no scheduler non saved model\n\n#ConvNet(50,c_out = [50,64,78],ks = [5,7,11],d = [2,2,2],max_ks = 2)\n#CNN_Forecast(50,100,150)\n#ConvNet(50,c_out = [50,64,78],ks = [5,7,11],d = [2,3,4],max_ks = 2)\n#ConvNet(50,c_out = [50,64,78],ks = [7,11,13],d = [2,5,7],max_ks = 2)\n#ConvNet(50,c_out = [50,64,78],ks = [7,11,13],d = [2,5,7],max_ks = 2) without batch normalization\n#CNN_Forecast(50,64,32,ks = 11,d=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idir_dset = '../input/predictions'\nloaded_dset1 = pd.read_csv(f'{idir_dset}/predictions1.csv')\n\nidir_dset2 = '../input/predictions2'\nloaded_dset2 = pd.read_csv(f'{idir_dset2}/predictions2.csv')\n\nidir_dset3 = '../input/predictions-evl'\nloaded_dset3 = pd.read_csv(f'{idir_dset3}/predictions_evl.csv')\n\nidir_dset4 = '../input/predictions-val'\nloaded_dset4 = pd.read_csv(f'{idir_dset4}/predictions_val.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = pd.concat([loaded_dset4,loaded_dset3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds.columns = submsn.columns\npreds.index = submsn.index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submsn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds.set_index('id').to_csv('submission4.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idir_sb = '../input/m5-first-public-notebook-under-0-50'\nsubmt_dset2 = pd.read_csv(f'{idir_sb}/submission_1.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submt_dset2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds[preds['F25']<0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}