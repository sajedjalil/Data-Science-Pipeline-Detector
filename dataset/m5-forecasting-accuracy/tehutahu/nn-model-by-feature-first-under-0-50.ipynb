{"cells":[{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"from  datetime import datetime, timedelta\nimport gc\nimport numpy as np, pandas as pd\nimport lightgbm as lgb\n\nfrom typing import Union\nfrom tqdm.notebook import tqdm_notebook as tqdm\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# import optuna.integration.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> This notebook aims to push the public LB under 0.50. Certainly, the competition is not yet at its peak and there clearly remains room for improvement.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Credits\n\n* [First R notebook](https://www.kaggle.com/kailex/m5-forecaster-v2)\n* [Python translation](https://www.kaggle.com/kneroma/m5-forecast-v2-python)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Changes\n* v5 : try to optimise the LGBM params (go below in lgbm params section to see changes)\n* v4 : add df, X_train deletion before training step --> increasing train sample without memeroy issues","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns: #columns毎に処理\n        col_type = df[col].dtypes\n        if col_type in numerics: #numericsのデータ型の範囲内のときに処理を実行. データの最大最小値を元にデータ型を効率的なものに変更\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"red\" size=\"5\">If you appreciate the effort We're putting in, please upvote us :) </font>","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n         \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\nPRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"pd.options.display.max_columns = 50","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"h = 28 \nmax_lags = 57\ntr_last = 1913\nfday = datetime(2016,4, 25)\nfday","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def create_dt(is_train = True, nrows = None, first_day = 1200):\n    prices = pd.read_csv(\"../input/m5-forecasting-accuracy/sell_prices.csv\", dtype = PRICE_DTYPES)\n    for col, col_dtype in PRICE_DTYPES.items():\n        if col_dtype == \"category\":\n            prices[col] = prices[col].cat.codes.astype(\"int16\")\n            prices[col] -= prices[col].min()\n            \n    cal = pd.read_csv(\"../input/m5-forecasting-accuracy/calendar.csv\", dtype = CAL_DTYPES)\n    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n    for col, col_dtype in CAL_DTYPES.items():\n        if col_dtype == \"category\":\n            cal[col] = cal[col].cat.codes.astype(\"int16\")\n            cal[col] -= cal[col].min()\n    \n    start_day = max(1 if is_train  else tr_last-max_lags, first_day)\n    numcols = [f\"d_{day}\" for day in range(start_day,tr_last+1)]\n    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n    dtype = {numcol:\"float32\" for numcol in numcols} \n    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n    dt = pd.read_csv(\"../input/m5-forecasting-accuracy/sales_train_validation.csv\", \n                     nrows = nrows, usecols = catcols + numcols, dtype = dtype)\n    \n    for col in catcols:\n        if col != \"id\":\n            dt[col] = dt[col].cat.codes.astype(\"int16\")\n            dt[col] -= dt[col].min()\n    \n    if not is_train:\n        for day in range(tr_last+1, tr_last+ 28 +1):\n            dt[f\"d_{day}\"] = np.nan\n    \n    dt = pd.melt(dt,\n                  id_vars = catcols,\n                  value_vars = [col for col in dt.columns if col.startswith(\"d_\")],\n                  var_name = \"d\",\n                  value_name = \"sales\")\n    \n    dt = dt.merge(cal, on= \"d\", copy = False)\n    dt = dt.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n    \n    return reduce_mem_usage(dt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def create_fea(dt):\n    lags = [7, 28]\n    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n    for lag, lag_col in zip(lags, lag_cols):\n        dt[lag_col] = dt[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n\n    wins = [7, 28]\n    for win in wins :\n        for lag,lag_col in zip(lags, lag_cols):\n            dt[f\"rmean_{lag}_{win}\"] = dt[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean())\n\n    \n    \n    date_features = {\n        \n        \"wday\": \"weekday\",\n        \"week\": \"weekofyear\",\n        \"month\": \"month\",\n        \"quarter\": \"quarter\",\n        \"year\": \"year\",\n        \"mday\": \"day\",\n#         \"ime\": \"is_month_end\",\n#         \"ims\": \"is_month_start\",\n    }\n    \n#     dt.drop([\"d\", \"wm_yr_wk\", \"weekday\"], axis=1, inplace = True)\n    \n    for date_feat_name, date_feat_func in date_features.items():\n        if date_feat_name in dt.columns:\n            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n        else:\n            dt[date_feat_name] = getattr(dt[\"date\"].dt, date_feat_func).astype(\"int16\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"FIRST_DAY = 700 # If you want to load all the data set it to '1' -->  Great  memory overflow  risk !","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"%%time\n\ndf = create_dt(is_train=True, first_day= FIRST_DAY)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"%%time\n\ncreate_fea(df)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"df.dropna(inplace = True)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## NN","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Dropout, Dense, Embedding, concatenate, BatchNormalization, Flatten\nfrom keras import backend as K\nfrom keras.losses import mean_squared_error as mse_loss\nfrom keras.optimizers import RMSprop, Adam\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\ndef rmse(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=0))\n\ndef build_model(numericals):\n\n    #Inputs\n    nums = []\n    for col in numericals:\n        nums.append(Input(shape=[1], name=col))\n    \n    item_id = Input(shape=[1], name=\"item_id\")\n    dept_id = Input(shape=[1], name=\"dept_id\")\n    store_id = Input(shape=[1], name=\"store_id\")\n    cat_id = Input(shape=[1], name=\"cat_id\")\n    state_id = Input(shape=[1], name=\"state_id\")\n    event_name_1 = Input(shape=[1], name=\"event_name_1\")\n    event_name_2 = Input(shape=[1], name=\"event_name_2\")\n    event_type_1 = Input(shape=[1], name=\"event_type_1\")\n    event_type_2 = Input(shape=[1], name=\"event_type_2\")\n#     mday = Input(shape=[1], name=\"mday\")\n#     month = Input(shape=[1], name=\"month\")\n#     quarter = Input(shape=[1], name=\"quarter\")\n#     snap_CA = Input(shape=[1], name=\"snap_CA\")\n#     snap_TX = Input(shape=[1], name=\"snap_TX\")\n#     snap_WI = Input(shape=[1], name=\"snap_WI\")\n#     wday = Input(shape=[1], name=\"wday\")\n#     week = Input(shape=[1], name=\"week\")\n#     year = Input(shape=[1],name=\"year\")\n    \n    #Embeddings layers\n    emb_item_id = Embedding(3049, 10)(item_id)\n    emb_dept_id = Embedding(7, 2)(dept_id)\n    emb_store_id = Embedding(10, 2)(store_id)\n    emb_cat_id = Embedding(3, 2)(cat_id)\n    emb_state_id = Embedding(3, 2)(state_id)\n    emb_event_name_1 = Embedding(31, 4)(event_name_1)\n    emb_event_name_2 = Embedding(4, 2)(event_name_2)\n    emb_event_type_1 = Embedding(5, 2)(event_type_1)\n    emb_event_type_2 = Embedding(3, 2)(event_type_2)\n\n    concat_emb = concatenate([\n           Flatten() (emb_item_id)\n         , Flatten() (emb_dept_id)\n         , Flatten() (emb_store_id)\n         , Flatten() (emb_cat_id)\n         , Flatten() (emb_state_id)\n         , Flatten() (emb_event_name_1)\n         , Flatten() (emb_event_name_2)\n         , Flatten() (emb_event_type_1)\n         , Flatten() (emb_event_type_2)\n    ])\n    categ = Dense(128, activation='relu') (concat_emb)\n    categ = Dropout(0.2686)(categ)\n    categ = BatchNormalization()(categ)\n    dateg = Dense(128, activation='relu') (categ)\n    categ = Dropout(0.0051)(categ)\n    \n    #main layer\n    main_l = concatenate([categ, *nums])\n    main_l = Dense(32,activation='relu') (main_l)\n    main_l = Dropout(0.0517)(main_l)\n    main_l = BatchNormalization()(main_l)\n    main_l = Dense(16,activation='relu') (main_l)\n    main_l = Dropout(0.0216)(main_l)\n    \n    #output\n    output = Dense(1) (main_l)\n\n    model = Model(\n        [\n        item_id,\n        dept_id,\n        store_id, \n        cat_id, \n        state_id,\n        event_name_1,\n        event_name_2,\n        event_type_1,\n        event_type_2,\n        *nums\n        ], \n        output\n    )\n\n    model.compile(optimizer=Adam(lr=3e-4),\n                  loss=mse_loss,\n                  metrics=[rmse])\n    model.summary()\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(keras_model, X_t, y_train, batch_size, epochs, X_v, y_valid,):\n    early_stopping = EarlyStopping(monitor='val_rmse', mode='min', patience=5, verbose=1, restore_best_weights=True)\n    reduce_lr = ReduceLROnPlateau(monitor='val_rmse', factor=0.1, patience=3, verbose=1, mode='min')\n    model_checkpoint = ModelCheckpoint(f\"model.h5\", save_best_only=True, verbose=1, monitor='val_rmse', mode='min')\n\n    hist = keras_model.fit(X_t, y_train, batch_size=batch_size, epochs=epochs,\n                            validation_data=(X_v, y_valid), verbose=1,\n                            callbacks=[early_stopping, reduce_lr, model_checkpoint])\n    \n    return keras_model, hist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_keras_data(df, feat_cols):\n    X = {col: np.array(df[col]) for col in feat_cols}\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"useless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\"]\nuse_cols = df.columns[~df.columns.isin(useless_cols)]\ncat_feats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\nnum_cols = list(np.setdiff1d(use_cols, cat_feats))\n\nnp.random.seed(777)\n\nfake_valid_inds = np.random.choice(df.index.values, 2_000_000, replace = False)\ntrain_inds = np.setdiff1d(df.index.values, fake_valid_inds)\ndf_train = df.loc[train_inds]\ndf_val = df.loc[fake_valid_inds]\n\n# valid_day = df[\"date\"].max() - timedelta(days=28)\n# df_train = df[df[\"date\"] <= valid_day]\n# df_val = df[df[\"date\"] > valid_day]\n\nX_train = df_train[use_cols]\ny_train = df_train[\"sales\"]\nX_val = df_val[use_cols]\ny_val = df_val[\"sales\"]\nX_train = get_keras_data(X_train, use_cols)\nX_val = get_keras_data(X_val, use_cols)\ndel df_train, df_val, df; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"batch_size = 1024\nepochs = 32\n\nmodel = build_model(num_cols)\nmodel, hist = train_model(model, X_train, y_train, batch_size, epochs, X_val, y_val, )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# https://keras.io/visualization/\ndef plot_history(history, filename='rmse.png'):\n    # Plot training & validation accuracy values\n    plt.plot(history.history['rmse'])\n    plt.plot(history.history['val_rmse'])\n    plt.title('Model RMSE')\n    plt.ylabel('RMSE')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Val'], loc='upper left')\n    plt.savefig(filename)\n    plt.show()\n    plt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history(hist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# %%time\n\n# np.random.seed(777)\n\n# fake_valid_inds = np.random.choice(X_train.index.values, 2_000_000, replace = False)\n# train_inds = np.setdiff1d(X_train.index.values, fake_valid_inds)\n# train_data = lgb.Dataset(X_train.loc[train_inds] , label = y_train.loc[train_inds], \n#                          categorical_feature=cat_feats, free_raw_data=False)\n# fake_valid_data = lgb.Dataset(X_train.loc[fake_valid_inds], label = y_train.loc[fake_valid_inds],\n#                               categorical_feature=cat_feats,\n#                  free_raw_data=False)# This is a random sample, we're not gonna apply any time series train-test-split tricks here!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# del df, X_train, y_train, fake_valid_inds,train_inds ; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# params = {\n#         \"objective\" : \"poisson\",\n#         \"metric\" :\"rmse\",\n#         \"force_row_wise\" : True,\n#         \"learning_rate\" : 0.075,\n# #         \"sub_feature\" : 0.8,\n#         \"sub_row\" : 0.75,\n#         \"bagging_freq\" : 1,\n#         \"lambda_l2\" : 0.1,\n# #         \"nthread\" : 4\n# #         \"metric\": [\"rmse\"],\n#     'verbosity': 1,\n#     'num_iterations' : 1200,\n#     'num_leaves': 128,\n#     \"min_data_in_leaf\": 100,\n# }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# %%time\n\n# m_lgb = lgb.train(params, train_data, valid_sets = [valid_data], verbose_eval=20) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# m_lgb.save_model(\"model.lgb\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# %%time\n\n# alphas = [1.028, 1.023, 1.018]\n# weights = [1/len(alphas)]*len(alphas)\n# sub = 0.\n\n# for icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n\n#     te = create_dt(False)\n#     cols = [f\"F{i}\" for i in range(1,29)]\n\n#     for tdelta in range(0, 28):\n#         day = fday + timedelta(days=tdelta)\n#         print(tdelta, day)\n#         tst = te[(te.date >= day - timedelta(days=max_lags)) & (te.date <= day)].copy()\n#         create_fea(tst)\n#         tst = tst.loc[tst.date == day , use_cols]\n#         te.loc[te.date == day, \"sales\"] = alpha*m_lgb.predict(tst) # magic multiplier by kyakovlev\n\n\n\n#     te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n# #     te_sub.loc[te.date >= fday+ timedelta(days=h), \"id\"] = te_sub.loc[te.date >= fday+timedelta(days=h), \n# #                                                                           \"id\"].str.replace(\"validation$\", \"evaluation\")\n#     te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n#     te_sub = te_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][cols].reset_index()\n#     te_sub.fillna(0., inplace = True)\n#     te_sub.sort_values(\"id\", inplace = True)\n#     te_sub.reset_index(drop=True, inplace = True)\n#     te_sub.to_csv(f\"submission_{icount}.csv\",index=False)\n#     if icount == 0 :\n#         sub = te_sub\n#         sub[cols] *= weight\n#     else:\n#         sub[cols] += te_sub[cols]*weight\n#     print(icount, alpha, weight)\n\n\n# sub2 = sub.copy()\n# sub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\n# sub = pd.concat([sub, sub2], axis=0, sort=False)\n# sub.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# cat_feats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\n# useless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\"]\n# use_cols = df.columns[~df.columns.isin(useless_cols)]\n\n# m_lgb = lgb.Booster(model_file=\"../input/m5-first-public-notebook-under-0-50/model.lgb\")\n# m_lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_train, y_train, X_val, y_val ; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"%%time\n\nsub = 0.\n\n# fday = valid_day\nte = create_dt(False)\ncols = [f\"F{i}\" for i in range(1,29)]\n\nfor tdelta in range(0, 28):\n    day = fday + timedelta(days=tdelta)\n    print(tdelta, day)\n    tst = te[(te.date >= day - timedelta(days=max_lags)) & (te.date <= day)].copy()\n    create_fea(tst)\n    tst = tst.loc[tst.date == day , use_cols]\n    tst = get_keras_data(tst, use_cols)\n    te.loc[te.date == day, \"sales\"] = model.predict(tst)\n\n\nte_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n#     te_sub.loc[te.date >= fday+ timedelta(days=h), \"id\"] = te_sub.loc[te.date >= fday+timedelta(days=h), \n#                                                                           \"id\"].str.replace(\"validation$\", \"evaluation\")\nte_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\"id\"].cumcount()+1]\nte_sub = te_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][cols].reset_index()\nte_sub.fillna(0., inplace = True)\nte_sub.sort_values(\"id\", inplace = True)\nte_sub.reset_index(drop=True, inplace = True)\nte_sub.to_csv(f\"submission_.csv\",index=False)\nsub = te_sub\n\nsub2 = sub.copy()\nsub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\nsub = pd.concat([sub, sub2], axis=0, sort=False)\nsub.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# sub = te_sub\n\n# sub2 = sub.copy()\n# sub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\n# sub = pd.concat([sub, sub2], axis=0, sort=False)\n# sub.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"sub.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# sub.id.nunique(), sub[\"id\"].str.contains(\"validation$\").sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"sub.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PB Lank by WRMSSEE","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"## evaluation metric\n## from https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834 and edited to get scores at all levels\nclass WRMSSEEvaluator(object):\n\n    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame):\n        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n        train_target_columns = train_y.columns.tolist()\n        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n\n        train_df['all_id'] = 0  # for lv1 aggregation\n\n        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n\n        if not all([c in valid_df.columns for c in id_columns]):\n            valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.calendar = calendar\n        self.prices = prices\n\n        self.weight_columns = weight_columns\n        self.id_columns = id_columns\n        self.valid_target_columns = valid_target_columns\n\n        weight_df = self.get_weight_df()\n\n        self.group_ids = (\n            'all_id',\n            'state_id',\n            'store_id',\n            'cat_id',\n            'dept_id',\n            ['state_id', 'cat_id'],\n            ['state_id', 'dept_id'],\n            ['store_id', 'cat_id'],\n            ['store_id', 'dept_id'],\n            'item_id',\n            ['item_id', 'state_id'],\n            ['item_id', 'store_id']\n        )\n\n        for i, group_id in enumerate(tqdm(self.group_ids)):\n            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n            scale = []\n            for _, row in train_y.iterrows():\n                series = row.values[np.argmax(row.values != 0):]\n                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n            setattr(self, f'lv{i + 1}_train_df', train_y)\n            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())\n\n            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n            setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())\n\n    def get_weight_df(self) -> pd.DataFrame:\n        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n\n        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n        return weight_df\n\n    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n        valid_y = getattr(self, f'lv{lv}_valid_df')\n        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n        scale = getattr(self, f'lv{lv}_scale')\n        return (score / scale).map(np.sqrt)\n\n    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n\n        group_ids = []\n        all_scores = []\n        for i, group_id in enumerate(self.group_ids):\n            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n            weight = getattr(self, f'lv{i + 1}_weight')\n            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n            group_ids.append(group_id)\n            all_scores.append(lv_scores.sum())\n\n        return group_ids, all_scores\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"## public LB rank\ndef get_lb_rank(score):\n    \"\"\"\n    Get rank on public LB as of 2020-05-31 23:59:59\n    \"\"\"\n    df_lb = pd.read_csv(\"../input/m5-accuracy-final-public-lb/m5-forecasting-accuracy-publicleaderboard-rank.csv\")\n\n    return (df_lb.Score <= score).sum() + 1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"## reading data\ndef make_evaluator():\n    df_train_full = pd.read_csv(\"../input/m5-forecasting-accuracy/sales_train_evaluation.csv\")\n    df_calendar = pd.read_csv(\"../input/m5-forecasting-accuracy/calendar.csv\")\n    df_prices = pd.read_csv(\"../input/m5-forecasting-accuracy/sell_prices.csv\")\n#     df_train = df_train_full.drop(df_train_full.columns[df_train_full.columns.str.startswith('d_')][:first_day-1], axis=1)[:-28]\n    df_train = df_train_full.iloc[:, :-28]\n    df_valid = df_train_full.iloc[:, -28:]\n\n    df_train, df_valid, df_calendar, df_prices = [reduce_mem_usage(df) for df in [df_train, df_valid, df_calendar, df_prices]]\n    evaluator = WRMSSEEvaluator(df_train, df_valid, df_calendar, df_prices)\n    return evaluator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def evaluate_WRMSSEE(preds_valid):\n    df_sample_submission = pd.read_csv(\"../input/m5-forecasting-accuracy/sample_submission.csv\")\n    df_sample_submission[\"order\"] = range(df_sample_submission.shape[0])\n    \n    preds_valid = preds_valid[preds_valid.id.str.contains(\"validation\")]\n    preds_valid = preds_valid.merge(df_sample_submission[[\"id\", \"order\"]], on = \"id\").sort_values(\"order\").drop([\"id\", \"order\"], axis = 1)\n    preds_valid.rename(columns = {\n        \"F1\": \"d_1914\", \"F2\": \"d_1915\", \"F3\": \"d_1916\", \"F4\": \"d_1917\", \"F5\": \"d_1918\", \"F6\": \"d_1919\", \"F7\": \"d_1920\",\n        \"F8\": \"d_1921\", \"F9\": \"d_1922\", \"F10\": \"d_1923\", \"F11\": \"d_1924\", \"F12\": \"d_1925\", \"F13\": \"d_1926\", \"F14\": \"d_1927\",\n        \"F15\": \"d_1928\", \"F16\": \"d_1929\", \"F17\": \"d_1930\", \"F18\": \"d_1931\", \"F19\": \"d_1932\", \"F20\": \"d_1933\", \"F21\": \"d_1934\",\n        \"F22\": \"d_1935\", \"F23\": \"d_1936\", \"F24\": \"d_1937\", \"F25\": \"d_1938\", \"F26\": \"d_1939\", \"F27\": \"d_1940\", \"F28\": \"d_1941\"\n    }, inplace = True)\n    \n    evaluator = make_evaluator()\n    groups, scores = evaluator.score(preds_valid)\n\n    score_public_lb = np.mean(scores)\n    score_public_rank = get_lb_rank(score_public_lb)\n\n    for i in range(len(groups)):\n        print(f\"Score for group {groups[i]}: {round(scores[i], 5)}\")\n\n    print(f\"\\nPublic LB Score: {round(score_public_lb, 5)}\")\n    print(f\"Public LB Rank: {score_public_rank}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"evaluate_WRMSSEE(sub)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}