{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Introduction**\n\nWelcome to our notebook. The main goal of this notebook is to find the best solution for the “M5 Competition - Accuracy”. Specifically, we will try to predict the future sales for Walmart based on hierarchical sales data, generously made available by Walmart, starting at the item level and aggregating to that of departments, product categories and stores in three geographical areas of the US: California, Texas, and Wisconsin.\n\nIn this Kernel, we will explain each stage of analysis, such as cleaning and editing the existing data. Then, we will apply 3 different machine learning algorithms in order to achieve the best results in terms of accuracy. More specific, we will use Facebook Prophet, Random Forest Regression and XGBoost.\n\n**What is Facebook Prophet?** Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.\n\n**What is Random Forest Regression** A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n\n**What is XGBoost** XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.\n\n**Dataset** The dataset that we will apply the machine learning models constitutes sales data that they were provided by Walmart. More specific, the M5 dataset contains sales data for 3,049 different products, classified in 3 product categories (Hobbies, Foods and Household).\n\n**Data Sources Explanation** In order to proceed in the analysis, 3 different files are provided:\n\nsell_prices.csv: Contains information about the price of the products sold per store and date. sales_train.csv: Contains the historical daily unit sales data per product and store. calendar: Contains information about the dates the products are sold."},{"metadata":{},"cell_type":"markdown","source":"Firstly, we need to import all the necessary libraries and files"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from __future__ import print_function\nfrom sklearn.model_selection import train_test_split\nfrom ipywidgets import interact, interactive, fixed, interact_manual, Dropdown\nfrom ipywidgets import Layout\nimport ipywidgets as widgets\nfrom IPython.display import display\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom fbprophet import Prophet\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to understand the problem we need to read the data from each data source. Thus, we will have a better view of the data that will help us to schedule our actions properly."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#DATASET WITH DATES\ncalendar = pd.read_csv(\"../input/m5-forecasting-accuracy/calendar.csv\")\n\n#DATASET WITH SALES\nsales_eval = pd.read_csv(\"../input/m5-forecasting-accuracy/sales_train_evaluation.csv\")\n\n#DATASET WITH THE NEXT 28 DAILY SALES\nsales = pd.read_csv(\"../input/m5-forecasting-accuracy/sales_train_validation.csv\")\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CALENDAR HEAD\nprint(calendar.head())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SALES HEAD\nprint(sales_eval.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is important to eliminate duplicate values. So we need to aggregate all the products in order to create a file with unique products."},{"metadata":{"trusted":true},"cell_type":"code","source":"#CREATE FILE WITH ALL PRODUCTS REGARDLESS OF THE ANNEX SO THAT WE DONT HAVE DUPLICATE PRODUCTS\nsales_final=sales.groupby('item_id',as_index=False).sum()\nsales_final['total']=sales_final.iloc[:,1:].sum(axis=1)\n\n#SORT THE FINAL FILE IN DESCENDING ORDER TO APPEAR IN THE DROP DOWN MENU BASED ON SALES \nsales_total_sort=sales_final.iloc[:,[0,-1]].sort_values(by='total',ascending=False)\n\n#GROUP BY THE FILE WITH THE REAL 28 NEXT DAILY SALES\nsales_eval_final=sales_eval.groupby('item_id',as_index=False).sum()\n\nprint(sales_eval_final.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In addition, we will split the file that we created based on the product categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"#SPLIT  DATASET BASED ON THE CATEGORIES OF THE PRODUCTS &\n#SORT THEM IN DESCENDING ORDER TO APPEAR IN THE DROP DOWN MENU BASED ON SALES\n\n#HOBBIES\nhobbies = sales_final[sales_final['item_id'].str.split('_').str[0]=='HOBBIES']\nhobbies.shape\n\nhobbies_sort=hobbies.iloc[:,[0,-1]].sort_values(by='total',ascending=False)\nprint(hobbies_sort.head())\n\n#HOUSEHOLD\nhousehold = sales_final[sales_final['item_id'].str.split('_').str[0]=='HOUSEHOLD']\nhousehold.shape\n\nhousehold_sort=household.iloc[:,[0,-1]].sort_values(by='total',ascending=False)\nprint(household_sort.head())\n\n#FOODS\nfoods = sales_final[sales_final['item_id'].str.split('_').str[0]=='FOODS']\nfoods.shape\n\nfoods_sort=foods.iloc[:,[0,-1]].sort_values(by='total',ascending=False)\nprint(foods_sort.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We created a few functions in order to better manage the data. Thus, it will be easier to build the models properly without any mistakes. So, as we can easily understand, this procedure is really vital."},{"metadata":{},"cell_type":"markdown","source":"First we created a function for the date features, such as date, month, year. Thus, it is easier to call each of them when it is neccessary. Actually, we created a variable X which containts the dataframe with the date features."},{"metadata":{"trusted":true},"cell_type":"code","source":"#CREATE A FUNCTION TO ADD FEATURES ON TRAIN SET BASED ON DATES\ndef date_features(df, label=None,r=None):\n    df = df.copy()\n\n    df['date'] = df.Date\n    df['month'] = df['date'].dt.month\n    df['year'] = df['date'].dt.year\n    df['dayofweek'] = [i for i in calendar['wday'][:r]]\n    df['quarter'] = df['date'].dt.quarter\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['dayofmonth'] = df['date'].dt.day\n    df['weekofyear'] = df['date'].dt.isocalendar().week.astype('int64')\n    df['Event_1']=np.where(calendar['event_type_1'][:r].isna(),0,1)\n    df['Event_2']=np.where(calendar['event_type_2'][:r].isna(),0,1)\n    #SATURDAY OR SUNDAY\n    df['S&S']=[1 if calendar.iloc[i,3]==1 else 1 if calendar.iloc[i,3]==2 else 0 for i in range(len(calendar[:r]))]\n    \n    X = df[['dayofweek','quarter','month','year',\n           'dayofyear','dayofmonth','weekofyear','Event_1','Event_2','S&S']]\n    if label:\n        y = df[label]\n        return X, y\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CREATE A FUNCTION TO ADD FEATURES ON TEST SET BASED ON DATES\ndef date_features_next28(df, label=None,r1=None,r2=None):\n    df = df.copy()\n\n    df['date'] = df.Date\n    df['month'] = df['date'].dt.month\n    df['year'] = df['date'].dt.year\n    df['dayofweek'] = [i for i in calendar['wday'][r1:r2]] \n    df['quarter'] = df['date'].dt.quarter\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['dayofmonth'] = df['date'].dt.day\n    df['weekofyear'] = df['date'].dt.isocalendar().week.astype('int64')\n    df['Event_1']=np.where(calendar['event_type_1'][r1:r2].isna(),0,1)\n    df['Event_2']=np.where(calendar['event_type_2'][r1:r2].isna(),0,1)\n    #SATURDAY OR SUNDAY\n    df['S&S']=[1 if calendar.iloc[i,3]==1 else 1 if calendar.iloc[i,3]==2 else 0 for i in range(len(calendar[r1:r2]))]\n    \n    X = df[['dayofweek','quarter','month','year',\n           'dayofyear','dayofmonth','weekofyear','Event_1','Event_2','S&S']]\n    if label:\n        y = df[label]\n        return X, y\n    return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to mention that this Kernel is dynamic. Specifically, we created a drop down menu in order to give users the capability to choose between various reports/results. To be more specific, the choices are: XGBoost, FB Prophet, RandomForest, Plot Times Series, See Trends, XGBoost2"},{"metadata":{"trusted":true},"cell_type":"code","source":"#CREATE THE DROP DOWN WITH THE PRODUCTS CATEGORIES, RANGE OF SALES, THE PRODUCT, VISUALIZATIONS AND PREDICTIONS ALGORITHMS\n\n#CHOICES OF DROP DOWN MENU\n\ncat={'HOBBIES':hobbies_sort,'HOUSEHOLD':household_sort,'FOODS':foods_sort,'TOTAL':sales_total_sort}\nopt=sales_total_sort.sort_values('total')['total'][:]\nAlgoChoices={3:\"Select\",4:\"Plot Time Series\",5:\"See Trends\",6:\"Plot Total Sales\",0:\"FB Prophet\",1:\"XGBoost\",2:\"XGBoost2\",7:\"RandomForest\"}\n\ncatW=widgets.Dropdown(options = cat.keys(),description='Category:')\n\n#RANGE OF SALES\nrange_slider=widgets.SelectionRangeSlider(options=opt,index=(200,2000),layout=Layout(width='50%'),step=100,description='Sales Number',disabled=False, feature_weights=True)\n\nidW=widgets.Dropdown(description='Product:')\n\nalgoChoice=widgets.Dropdown(options=AlgoChoices.values(),description='Algorithm:')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below you will find the function which is important in order to call the option \"Plot Time Series\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"#DAILY SALES FOR EACH PRODUCT\ndef Plot_TimeSeries():\n    \n    df_sales_temp = sales_final[sales_final['item_id']==idW.value].iloc[:,1:-1].sum(axis=0)\n    df_sales_temp.index = range(1913)\n    \n    Sales_dict ={'Sales' : df_sales_temp,'Date' : pd.to_datetime(calendar['date'][:-56])}\n   \n    product_sales_per_date = pd.DataFrame(Sales_dict)\n    \n    #DAILY SALES PLOT\n    sns.set(font_scale=2)  # big\n    fig, ax = plt.subplots(figsize=(40,14))\n    a = sns.lineplot(x=\"Date\", y=\"Sales\", data=product_sales_per_date)\n    a.set_title(\"Daily Sales Data\",fontsize=30)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below you will find the function which is important in order to call the option \"See Trends\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"#TRENDS\ndef See_Trends():\n    \n    df_sales_temp = sales_final[sales_final['item_id']==idW.value].iloc[:,1:-1].sum(axis=0)\n    df_sales_temp.index = range(1913)   \n    \n    Sales_dict ={'Sales' : df_sales_temp,'Date' : pd.to_datetime(calendar['date'][:-56])}\n    \n    product_sales_per_date = pd.DataFrame(Sales_dict)\n    \n    #Call the Function\n    X, y = date_features(product_sales_per_date, label='Sales')\n    df_new = pd.concat([X, y], axis=1)\n    df_new.head()\n\n    #Plotting the Features to see trends (SALES PER MONTH)\n    sns.set(font_scale=2)  # big\n    fig, ax = plt.subplots(figsize=(28,10))\n    palette = sns.color_palette(\"mako_r\", 4)\n    a = sns.barplot(x=\"month\", y=\"Sales\",hue = 'year',data=df_new)\n    a.set_title(\"Store Sales Data\",fontsize=15)\n    plt.legend(loc='best')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below you will find the function which is important in order to call \"Plot Total Sales\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"#PLOT FOR TOTAL SALES\ndef Plot_Total_Sales():\n    \n    df_sales_temp = sales_final[sales_final['item_id']==idW.value].iloc[:,1:-1].sum(axis=0)\n    df_sales_temp.index = range(1913)   \n    \n    Sales_dict ={'Sales' : df_sales_temp,'Date' : pd.to_datetime(calendar['date'][:-56])}\n    \n    product_sales_per_date = pd.DataFrame(Sales_dict)   \n    \n    #Call the Function\n    X, y = date_features(product_sales_per_date, label='Sales')\n    df_new = pd.concat([X, y], axis=1)\n    df_new.head()\n    \n    fig,(ax1,ax2,ax3,ax4)= plt.subplots(nrows=4)\n    fig.set_size_inches(30,45)\n    \n    monthAggregated = pd.DataFrame(df_new.groupby(\"month\")[\"Sales\"].sum()).reset_index().sort_values('Sales')\n    sns.barplot(data=monthAggregated,x=\"month\",y=\"Sales\",ax=ax1)\n    ax1.set(xlabel='Month', ylabel='Total Sales received')\n    ax1.set_title(\"Total Sales received By Month\",fontsize=15)\n\n    monthAggregated = pd.DataFrame(df_new.groupby(\"dayofweek\")[\"Sales\"].sum()).reset_index().sort_values('Sales')\n    sns.barplot(data=monthAggregated,x=\"dayofweek\",y=\"Sales\",ax=ax2)\n    ax2.set(xlabel='dayofweek', ylabel='Total Sales received')\n    ax2.set_title(\"Total Sales received By Weekday\",fontsize=15)\n\n    monthAggregated = pd.DataFrame(df_new.groupby(\"quarter\")[\"Sales\"].sum()).reset_index().sort_values('Sales')\n    sns.barplot(data=monthAggregated,x=\"quarter\",y=\"Sales\",ax=ax3)\n    ax3.set(xlabel='Quarter', ylabel='Total Sales received')\n    ax3.set_title(\"Total Sales received By Quarter\",fontsize=15)\n\n    monthAggregated = pd.DataFrame(df_new.groupby(\"year\")[\"Sales\"].sum()).reset_index().sort_values('Sales')\n    sns.barplot(data=monthAggregated,x=\"year\",y=\"Sales\",ax=ax4)\n    ax4.set(xlabel='year', ylabel='Total Sales received')\n    ax4.set_title(\"Total Sales received By year\",fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below you will find the function which is vital for FB Prophet's implementation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#FUNCTION FOR FBPROPHET \ndef my_Prophet():\n    \n    df_sales_temp = sales_final[sales_final['item_id']==idW.value].iloc[:,1:-1].sum(axis=0)\n    df_sales_temp.index = range(1913)       \n    \n    Sales_dict ={'y' : df_sales_temp,'ds' : pd.to_datetime(calendar['date'][:-56])}\n    \n    product_sales_per_date = pd.DataFrame(Sales_dict)\n    \n    #SPLIT THE DATA SET\n    split_date = '2016-02-01'\n    subset1 = (product_sales_per_date['ds'] <= split_date)\n    subset2 = (product_sales_per_date['ds'] > split_date)\n\n\n    X_tr = product_sales_per_date.loc[subset1]\n    X_tst = product_sales_per_date.loc[subset2]\n    print(\"train shape\",X_tr.shape)\n    print(\"test shape\",X_tst.shape)\n\n    pd.plotting.register_matplotlib_converters()\n    f, ax = plt.subplots(figsize=(42,15))\n    X_tr.plot(kind='line', x='ds', y='y', color='blue', label='Train', ax=ax)\n    X_tst.plot(kind='line', x='ds', y='y', color='red', label='Test', ax=ax)\n    plt.title('Sales Amount Traning and Test data')\n    plt.show()\n    model =Prophet()\n    model.add_country_holidays(country_name='US')\n    model.add_seasonality(name='custom_seasonality', period=28,fourier_order=5)\n    model.fit(X_tr)\n\n    \n    future_dates = pd.DataFrame({'ds':pd.date_range('2016-04-24', periods=28)})\n    forecast = model.predict(future_dates)\n    forecast[['ds', 'yhat']]\n    \n    # Plot the components of the model\n    fig = model.plot_components(forecast,figsize=(45,15))\n\n    # Plot the forecast\n    f, ax = plt.subplots(1)\n    f.set_figheight(15)\n    f.set_figwidth(45)\n    fig = model.plot(forecast,ax=ax)\n    plt.show()\n\n\n    # Plot the forecast with the actuals\n    f, ax = plt.subplots(1)\n    f.set_figheight(15)\n    f.set_figwidth(45)\n    ax.scatter(X_tst.ds, X_tst['y'], color='r')\n    fig = model.plot(forecast, ax=ax)\n\n    f, ax = plt.subplots(figsize=(14,5))\n    f.set_figheight(15)\n    f.set_figwidth(45)\n    X_tst.plot(kind='line',x='ds', y='y', color='red', label='Test', ax=ax)\n    forecast.plot(kind='line',x='ds',y='yhat', color='green',label='Forecast', ax=ax)\n    plt.title('Forecast vs Actuals')\n    plt.show()\n    \n    real_sales=sales_eval_final[sales_eval_final['item_id']==idW.value].iloc[:,1914:].sum(axis=0).values\n    rmse=np.sqrt(mean_squared_error(real_sales, forecast['yhat']))\n\n    print(forecast[['ds', 'yhat']])\n    print(\"RMSE\",rmse)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The differentiate of the dataset between train and test its a very common step in order to apply the ML models properly."},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_test_split(data, n_test):\n    \n    return data[:-n_test, :], data[-n_test:, :]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will apply XGBoost in order to forecast the next 28 days for a certain products based only on \nprevious historical sales. In order to apply this method properly is needed to tranform time series data to supervised. Then, we will use a function called \"walk_forward_validation\" which is responsible for the estimation of RMSE and MAE."},{"metadata":{"trusted":true},"cell_type":"code","source":"# forecast monthly sales with xgboost\n\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[0]\n    df = pd.DataFrame(data)\n    cols = list()\n    \n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n    \n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        # put it all together\n    agg = pd.concat(cols, axis=1)\n    \n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n        \n    return agg.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def xgboost_forecast(train, testX):\n    \n    # transform list into array\n    train = np.asarray(train)\n    \n    # split into input and output columns\n    trainX, trainy = train[:, :-1], train[:, -1]\n    \n    # fit model\n    model= XGBRegressor(n_estimators=125,learning_rate=0.01,gamma=0.01,reg_lambda=0.01,max_depth=7)\n    model.fit(trainX, trainy)\n    # make a one-step prediction\n    yhat = model.predict(np.asarray([testX]))\n    \n    return yhat[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# walk-forward validation for univariate data\ndef walk_forward_validation(data, n_test):\n    \n    predictions = list()\n    \n    # split dataset\n    train, test = train_test_split(data, n_test)\n    \n    # seed history with training dataset\n    history = [x for x in train]\n    \n    # step over each time-step in the test set\n    for i in range(len(test)):\n        \n        # split test row into input and output columns\n        testX, testy = test[i, :-1], test[i, -1]\n   \n        # fit model on history and make a prediction\n        yhat = xgboost_forecast(history, testX)\n         # store forecast in list of predictions\n        predictions.append(yhat)\n        \n        # add actual observation to history for the next loop\n        history.append(test[i])\n        \n        # summarize progress\n        print('>expected=%.1f, predicted=%.1f' % (testy, yhat))\n    \n    # estimate prediction error\n    mae=mean_absolute_error(test[:, -1], predictions)\n    rmse=np.sqrt(mean_squared_error(test[:, -1], predictions))\n    \n    return mae,rmse, test[:, -1], predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def XGBoost():\n    \n    df_sales_temp = sales_final[sales_final['item_id']==idW.value].iloc[:,1:-1].sum(axis=0)\n    df_sales_temp.index = range(1913)\n    \n    Sales_dict ={'Sales' : df_sales_temp,'Date' : pd.to_datetime(calendar['date'][:-28])}\n    \n    product_sales_per_date = pd.DataFrame(Sales_dict)\n    \n    X,y=date_features(product_sales_per_date, label='Sales',r=1941)\n    \n    X=X.set_index(product_sales_per_date['Date'])\n    series=X\n    values=y\n    data = series_to_supervised(values, n_in=6, n_out=1)\n    mae,rmse ,y, yhat = walk_forward_validation(data,28)\n    \n    return (\"MAE\",round(mae,4)), (\"RMSE\",rmse)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will apply XGBoost (XGBoost2) for a second time. However, we will use some extra features such as Year or Dayofweek."},{"metadata":{"trusted":true},"cell_type":"code","source":"def XGBoost2():\n    \n    df_sales_temp = sales_final[sales_final['item_id']==idW.value].iloc[:,1:-1].sum(axis=0)\n    df_sales_temp.index = range(1913)\n \n    Sales_dict ={'Sales' : df_sales_temp,'Date' : pd.to_datetime(calendar['date'][:-56])}\n    \n    product_sales_per_date = pd.DataFrame(Sales_dict)\n    X,y=date_features(product_sales_per_date, label='Sales',r=1913)\n\n    #split the dataset at 70-30 \n    df_train=X.iloc[:-574,:]\n    df_test=X.iloc[-574:,:] \n\n    X_train=df_train\n    y_train=y[:-574]\n    x_test=df_test\n    y_test=y[-574:]\n\n    # Initialize XGB and GridSearch\n    xgb_reg = xgb.XGBRegressor(colsample_bytree= 0.7,subsample=0.4,reg_alpha=0.7,min_child_weight=30,n_estimators=50,learning_rate=0.1,gamma=0.7,reg_lambda=1.5,max_depth=3)\n    xgb_reg.fit(X_train,y_train)\n    pred_xgb=xgb_reg.predict(x_test)\n    \n    #NEW DATASET(NEXT 28 DAYS)\n    df_test28next_xgb=pd.DataFrame()\n    df_test28next_xgb['Date']=pd.to_datetime(calendar[calendar['date']>\"2016-04-24\"]['date'][:28])\n    X_new=date_features_next28(df_test28next_xgb,r1=1913,r2=1941)\n    \n    #PREDICTIONS FOR NEXT 28 DAYS\n    pred2=xgb_reg.predict(X_new)\n    \n    #ADD THE PREDICTIONS AND THE REAL SALES AT TEST DATA SET\n    X_new['Predicted Sales']=abs(pred2.round(0))\n    X_new['Real Sales']=sales_eval_final[sales_eval_final['item_id']==idW.value].iloc[:,1914:].sum(axis=0).values\n   \n    #THE TEST DATA SET WITH PREDICTIONS AND REAL SALES\n    print(X_new)\n    \n    #RMSE\n    rmse=np.sqrt(mean_squared_error(y_test, pred_xgb))\n    print(\"RMSE\",rmse)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to predict the sales for the next 28 days, we will use RandomForest, too"},{"metadata":{"trusted":true},"cell_type":"code","source":"def RandomForest():\n    \n    from sklearn.model_selection import train_test_split\n    df_sales_temp = sales_final[sales_final['item_id']==idW.value].iloc[:,1:-1].sum(axis=0)\n    df_sales_temp.index = range(1913)\n\n    Sales_dict ={'Sales' : df_sales_temp,'Date' : pd.to_datetime(calendar['date'][:-56])}\n    product_sales_per_date = pd.DataFrame(Sales_dict)\n    \n    X, y= date_features(product_sales_per_date, label='Sales',r=1913)\n    \n    Dataframe_RandomForest = pd.concat([X, y], axis=1)\n    predictors=Dataframe_RandomForest.drop(['Sales'],axis=1)\n    target=Dataframe_RandomForest['Sales']\n    X_train,X_test_cv,y_train,y_test_cv=train_test_split(predictors, target, test_size=0.25,random_state=42)\n    \n    #Hypertuned Model\n    model = RandomForestRegressor(n_estimators=100,oob_score=True,n_jobs =1,random_state=42,max_features='auto',min_samples_leaf=4)\n    model.fit(X_train,y_train)\n    \n    #PREDICTION TO TRAIN SER\n    pred=model.predict(X_test_cv)\n    \n    #NEW DATASET(NEXT 28 DAYS)\n    df_test28next=pd.DataFrame()\n    df_test28next['Date']=pd.to_datetime(calendar[calendar['date']>\"2016-04-24\"]['date'][:28])\n    X_test_28=date_features_next28(df_test28next,r1=1913,r2=1941)\n    \n    #PREDICTIONS FOR NEXT 28 DAYS\n    pred2=model.predict(X_test_28)\n    \n    #ADD THE PREDICTIONS AND THE REAL SALES AT TEST DATA SET\n    X_test_28['Predicted Sales']=pred2.round(0)\n    X_test_28['Real Sales']=sales_eval_final[sales_eval_final['item_id']==idW.value].iloc[:,1914:].sum(axis=0).values\n    \n    #TEST DATA SET\n    print(X_test_28)\n    \n    #RMSE\n    rmse=np.sqrt(mean_squared_error(y_test_cv, pred))\n    print(\"RMSE\",rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We created a drop down menu in order to give the user some choices regarding ML models & plots. Below, you can see the available choices."},{"metadata":{"trusted":true},"cell_type":"code","source":"#MENU\ndef Choice(A):\n    if A==3:\n        return (\"Select\")\n    elif A==6:\n        return Plot_Total_Sales()\n    elif A==5:\n        return See_Trends()\n    elif A==4:\n        return Plot_TimeSeries()\n    elif A==0:    \n        return my_Prophet()\n    elif A==1:\n        return XGBoost()\n    elif A==2:\n        return XGBoost2()\n    elif A==7:\n        return RandomForest()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interact is used for the dropdown creation. Specifically, the user should first choose a product category, the sales range using a slidebar, the product that he/she wants to make the prediction and finally the ML model that he/she wants to apply or the plot that he/she wants to create."},{"metadata":{"trusted":true},"cell_type":"code","source":"@interact(Category=catW,RS=range_slider,Product=idW, AC=algoChoice)\ndef print_idW(Category,RS,Product,AC):\n    idW.options=cat[Category][(cat[Category]['total']>=RS[0]) & (cat[Category]['total']<=RS[1])]['item_id']\n    return Choice(list(AlgoChoices.keys())[list(AlgoChoices.values()).index(AC)])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}