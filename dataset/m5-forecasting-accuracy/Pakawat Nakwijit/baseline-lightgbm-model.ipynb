{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from  datetime import datetime, timedelta\nimport gc\nimport numpy as np, pandas as pd\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns: #columns毎に処理\n        col_type = df[col].dtypes\n        if col_type in numerics: \n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_columns = 50","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = \"../input/m5-forecasting-accuracy/\"\ncalendar = pd.read_csv(data_dir+'calendar.csv')\ncalendar = reduce_mem_usage(calendar)\nprint('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n\nsell_prices = pd.read_csv(data_dir+'sell_prices.csv')\nsell_prices = reduce_mem_usage(sell_prices)\nprint('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n\nsales = pd.read_csv(data_dir+'sales_train_validation.csv')\nprint('Sales train validation has {} rows and {} columns'.format(sales.shape[0], sales.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idCols = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\nproduct = sales[idCols].drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nsubmission = pd.read_csv(data_dir+'sample_submission.csv')\nvalidate_submission = submission[submission.id.str.endswith('validation')]\neval_submission = submission[submission.id.str.endswith('evaluation')]\n\n# change column name\nnewcolumns = [\"id\"] + [\"d_{}\".format(i) for i in range(1914, 1914+28)]\nvalidate_submission.columns = newcolumns\nvalidate_submission = validate_submission.merge(product, how = 'left', on = 'id')\n\n# newcolumns = [\"id\"] + [\"d_{}\".format(i) for i in range(1942, 1942+28)]\n# eval_submission.columns = newcolumns\n# eval_submission['id'] = eval_submission['id'].str.replace('_evaluation','_validation')\n# eval_submission = eval_submission.merge(product, how = 'left', on = 'id')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idCols = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n\n# Use only the last three years\nDAYS = 365*2; LAST_DAY=1913\ndayCols = [\"d_{}\".format(i) for i in range(LAST_DAY-DAYS+1, LAST_DAY+1)]\nprint(len(dayCols), dayCols[0])\nsales = sales[idCols+dayCols]\nprint(sales.shape)\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def melted(df, name=\"\"):\n    df = pd.melt(df, id_vars = idCols, var_name = 'day', value_name = 'demand')\n    print('{}: {} rows and {} columns'.format(name, df.shape[0], df.shape[1]))\n    df = reduce_mem_usage(df)\n    # df.to_csv(name+\".csv\")\n    return df\n\nmelted_sales = melted(sales)\nmelted_sales[\"part\"] = \"train\"\nmelted_validate = melted(validate_submission)\nmelted_validate[\"part\"] = \"validate\"\n# melted_eval = melted(eval_submission)\n# melted_eval[\"part\"] = \"evaluate\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data = melted_sales\n# data = pd.concat([melted_sales, melted_validate, melted_eval], axis = 0)\ndata = pd.concat([melted_sales, melted_validate], axis = 0)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ndel melted_sales, melted_validate\ndel submission, validate_submission, eval_submission, product\ndel sales,\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df.dropna(inplace = True)\n# df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge with calendar, sell_prices\n\ncalendar.drop(['weekday', 'wday', 'month', 'year'], inplace = True, axis = 1)\ndata = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\ndata.drop(['d', 'day'], inplace = True, axis = 1)\n\nprint('Our dataset to train has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\n\n# get the sell price data (this feature should be very important)\ndata = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'])\n\nprint('Our final dataset to train has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del calendar, sell_prices\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\ndef encode_categorical(dt, cols):\n    for col in cols:\n        # Leave NaN as it is.\n#         le = preprocessing.LabelEncoder()\n#         not_null = df[col][df[col].notnull()]\n#         df[col] = pd.Series(le.fit_transform(not_null), index=not_null.index)\n        \n#         np.save(f'label_class_{col}.npy', le.classes_)\n        dt[col] = dt[col].astype(\"category\").cat.codes.astype(\"int16\")\n        dt[col] -= dt[col].min()\n    return dt\n\n\ndata = encode_categorical(data, [\"cat_id\", \"dept_id\", \"item_id\", \"state_id\", \"store_id\"]).pipe(reduce_mem_usage)\nvalues = {'event_name_1': \"normal\", 'event_type_1': \"normal\", \"event_name_2\": \"normal\", 'event_type_2': \"normal\"}\ndata.fillna(value=values, inplace = True);\n\ndata = encode_categorical(data, [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]).pipe(reduce_mem_usage)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def datetime_features(df):\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    attrs = [\n        \"year\", \n        \"quarter\", \n        \"month\", \n        \"week\", \n        \"day\", \n        \"dayofweek\", \n        \"weekday\",\n        \"weekofyear\",\n#         \"is_year_end\", \n#         \"is_year_start\", \n#         \"is_quarter_end\", \n#         \"is_quarter_start\", \n#         \"is_month_end\",\n#         \"is_month_start\",\n    ]\n\n    for attr in attrs:\n        dtype = np.int16 if attr == \"year\" else np.int8\n        df[attr] = getattr(df['date'].dt, attr).astype(dtype)\n    df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n    \n    return df\n\ndata = datetime_features(data)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sort_values(by=['id', \"date\"], inplace=True)\n\nX_train = data[data[\"part\"]==\"train\"]\nX_val = data[data[\"part\"]==\"validate\"]\nX_eval = data[data[\"part\"]==\"evaluate\"]\n\nprint(len(X_train), len(X_val), len(X_eval))\ndel data; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle \ndbfile = open('X_train.pkl', 'wb') \npickle.dump(X_train, dbfile)\ndbfile.close() #Dont forget this \n\ndbfile = open('X_val.pkl', 'wb') \npickle.dump(X_val, dbfile)\ndbfile.close() #Dont forget this ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n\ndef numerical_feature(df):\n    for i in [7, 28]:\n        df[f\"shifted_t{i}\"] = df[[\"id\",\"demand\"]].groupby('id')[\"demand\"].shift(i)\n\n    for win, col in [(7, \"shifted_t7\"), (7, \"shifted_t28\"), (28, \"shifted_t7\"), (28, \"shifted_t28\")]:\n        df[f\"rolling_mean_{col}_w{win}\"] = df[[\"id\", col]].groupby('id')[col].shift(1).rolling(win, min_periods=1).mean()\n    return df\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nX_train = numerical_feature(X_train)\nX_train.dropna(inplace = True)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\nuseless_cols = [\"id\", \"part\", \"date\", \"demand\",\"d\", \"wm_yr_wk\", \"weekday\"]\ntrain_cols = X_train.columns[~X_train.columns.isin(useless_cols)]\ntrain_cols \n\ny_train = X_train[\"demand\"]\nX_train = X_train[train_cols]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import train_test_split\nnp.random.seed(777)\n\nX, x_test, Y, y_test = train_test_split(X_train, y_train, test_size=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_train, y_train; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n        \"objective\" : \"poisson\",\n        \"metric\" :\"rmse\",\n        \"force_row_wise\" : True,\n        \"learning_rate\" : 0.075,\n#         \"sub_feature\" : 0.8,\n        \"sub_row\" : 0.75,\n        \"bagging_freq\" : 1,\n        \"lambda_l2\" : 0.1,\n#         \"nthread\" : 4\n        \"metric\": [\"rmse\"],\n    'verbosity': 1,\n    'num_iterations' : 3000,\n    'num_leaves': 128,\n    \"min_data_in_leaf\": 100,\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del X_eval, X_val\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_set = lgb.Dataset(X, Y)\ntest_set = lgb.Dataset(x_test, y_test)\ndel X, Y, x_test, y_test; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle \ndbfile = open('train_set.pkl', 'wb') \npickle.dump(train_set, dbfile)\ndbfile.close() #Dont forget this \n\ndbfile = open('test_set.pkl', 'wb') \npickle.dump(test_set, dbfile)\ndbfile.close() #Dont forget this ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from IPython.display import FileLink\n# FileLink(r'train_set.pkl')\n# del FileLink, dbfile, encode_categorical, math, np, pickle, preprocessing, reduce_mem_usage, train_test_split, timedelta\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# model = lgb.train(params, train_set, num_boost_round = 3000, early_stopping_rounds = 500, valid_sets = [train_set, test_set], verbose_eval = 100)\nmodel = lgb.train(params, train_set, valid_sets = [test_set], verbose_eval = 100)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"YEAH\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_model(\"model.lgb\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle \ndbfile = open('LightGBM.pkl', 'wb') \npickle.dump(model, dbfile)\ndbfile.close() #Dont forget this ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nwith open(\"X_train.pkl\", 'rb') as fin:\n    X_train = pickle.load(fin)\n\nwith open(\"X_val.pkl\", 'rb') as fin:\n    X_val = pickle.load(fin)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlastdate = X_train[\"date\"].max() - pd.DateOffset(days=86)\nX_train = X_train[X_train['date'] >= lastdate]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\nimport datetime\nimport dateutil.relativedelta\n\ndef predict(model, X_train, X_test, factor=1):\n    DATES = X_test[\"date\"].unique()\n    NDATE = len(DATES)\n    print(\"NDATE\", NDATE)\n    \n    col = [\"id\"] + [\"F{}\".format(i) for i in range(1, NDATE+1)]\n    itemId = X_train[\"dept_id\"].unique()\n    print(\"#CHUNK\", len(itemId))\n    \n    acc_o = []\n    itemId = sorted(itemId)\n    for iid in itemId:\n        test = X_test[X_test[\"dept_id\"]==iid]\n        \n        ids = test[\"id\"].unique()\n        oarr = np.zeros((len(ids), NDATE+1))\n        o = pd.DataFrame(oarr, columns=col)\n        \n        o[\"id\"] = test[test[\"date\"]==DATES[0]][\"id\"].values\n        \n        train = X_train[X_train[\"dept_id\"]==iid]\n        \n        ## XX=test, X=train\n        lastmonth = pd.to_datetime(train.head(1)[\"date\"])\n        for idx, date in enumerate(DATES):\n            \n            newrow = test[test[\"date\"]==date]\n            train = train.append(newrow)\n            \n            train.sort_values(by=['id', \"date\"], inplace=True)\n# #             print(\"num feats START\")\n            feat = numerical_feature(train)\n            \n# #             print(\"num feats DONE\")\n\n#             print(f\"============== {idx} ==========\")\n#             p = feat[feat[\"id\"]==\"FOODS_1_001_CA_1_validation\"]\n# #             print(p)\n#             print(p.tail(15)[[\"date\", \"demand\", \"shifted_t7\"]])\n#             if idx==10:\n#                 return None\n            \n            x = feat.loc[feat[\"date\"] == date , train_cols]\n            val_pred = model.predict(x, num_iteration=model.best_iteration)\n\n            \n            o[f\"F{idx+1}\"] = val_pred*factor\n            \n            \n            train.loc[train[\"date\"]==date, \"demand\"] = val_pred*factor\n            \n            \n            lastmonth = lastmonth + pd.DateOffset(days=1)\n            train = train[train['date'] >= str(lastmonth.values[0])]\n            \n        acc_o.append(o)\n        acc_o = [pd.concat(acc_o)]\n        print(iid)\n#         break\n    \n    acc_o = pd.concat(acc_o)\n    return acc_o","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pp = []\nweights = [1, 1.028, 1.023, 1.018]\nfor w in weights:\n    print(\"======== w\",w,\"==========\")\n    pred = predict(model, X_train, X_val, factor=w)\n    pp.append(pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avgpred = pd.DataFrame([])\navgpred[\"id\"] = pp[0][\"id\"]\nfor i in range(1, 29):\n    avgpred[f\"F{i}\"] = (pp[1][f\"F{i}\"]+pp[2][f\"F{i}\"]+pp[3][f\"F{i}\"])/3\n    \n#     print(sum(avgpred[f\"F{i}\"]), sum(pp[0][f\"F{i}\"]))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(data_dir+'sample_submission.csv')\ndfeval = submission[submission.id.str.endswith('evaluation')]\nassert len(dfeval)==len(avgpred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_csv(i, d):\n    df = pd.concat([d, dfeval]) \n\n    df.sort_values(\"id\", inplace = True)\n    df.reset_index(drop=True, inplace = True)\n    if i==\"\":\n        df.to_csv(f\"submission.csv\")\n    else:\n        df.to_csv(f\"submission.v9.{i}.csv\")\n    \nfor i in range(len(pp)):\n    save_csv(i, pp[i])\n\nsave_csv(\"\", avgpred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}