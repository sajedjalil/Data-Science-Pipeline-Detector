{"cells":[{"metadata":{},"cell_type":"markdown","source":"# TLDR: I have not found a way to use weights or a custom evaluation function to make and lgbm model optimize for the WRMSSE. All adjustments made the baseline model perform worse. If anyone has any words to guide me in the right direction, it would be greatly appreciated. \n\n\nThis notebook is meant to test using weights and scales to improve the WRMSSE score. I used the same data and model hyper parameters in each model. I assembled the training data, including the weights and scaling factors in another kernal. The features came from [@kyakovlev](https://www.kaggle.com/kyakovlev). I used [lags fe](https://www.kaggle.com/kyakovlev/m5-lags-features) and [simple fe](https://www.kaggle.com/kyakovlev/m5-simple-fe). I got the weights from the m5methods. I got the scales manually using the data in simple fe and lags fe notebooks. I grouped by id, took difference between sales and a lag 1 shift, squared it, took the mean, and took the squareroot. \nTraining data: one year of data leading up to day 1886\nValidation data: 1886 to 1913\nAlthough the weights werent meant for this validation period, I had still thought that including the weights would help. \nI tried two weighting schemes and two custom objective functions. \n5 models: \n1. Baseline:  normal hyper parameters taken from public notebooks. Look below for specifics. \n2. Weighted: took the weights/scales and input that into the training lgbm Dataset object before training\n3. Weighted squared: took (weights/scales)^2 and input that in as before. This was done since I thought it represented the derivative of the square of the WRMSSE function, and I thought I could minimize the WRMSSE by trying to minimize its square. \n4. Custom WMSSE: Used custom objective function with weights as in 2\n5. Cusom WMSSE with squared weights: Same as 4, but with the weights from 3. \n\nTrained all models for 1500 iterations and took the score for the validation period from every 25 booster(iteration) of each model \n\nResults: 1 Basline best was around .53, followed by 4, and closely by 2, with about a .57 score. 3 came in around 1.39 and number 5 was terrible with something like 5.0, \n\nConclustion: I have not found a way to use weights or a custom evaluation function to make and lgbm model optimize for the WRMSSE. \n\nNext steps: Next, I will look into ways to improve my weighting or loss function. Some options include: \n- Change the target to sales * price so that the products are naturally weighted to favor expensive items. \n- Combining the last step with using scaling factors \n- Looking into pytorch autograd to try to get the exact gradient for the total WRMSSE \n- Trying to take the derivative of the entire WRMSSE function, maybe even as if we were just predicting a one day horizon, which might make the calculations easier. \n- Training models to predict different levels of aggregation then \"averaging\" all the predictions so that they come to an optimal \"agreement\""},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For this experiment will be using the \n# last 28 days of data as our validation set\n# and 1 year of data to train on \nTRAIN_START = 1913 - 28 - 365 # last day - val period -length of train period\nTRAIN_END = 1913 - 28 # final day subtract days of validation\nNUM_ITERATIONS = 1500","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We start by reading in the data and looking at the features \n# we have to use. All features based on 28 day lag so \n# we can make one model for all 28 days of prediction\ndf = pd.read_pickle('/kaggle/input/df-basic-scale-weight-ready-pickle/df_basic_weight_scale.pkl')\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the two columns to test \n# with our custom objective function\ndf['scaled_weight'] = df.weight/df.scale\n# Also a squared version, which reflects \n# the derivative of the squared WRMSSE\ndf['scaled_weight_squared'] = (df.weight/df.scale)**2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List category features for lgbm model\ncat_feats = [ 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', \n             'event_type_1', 'event_name_2', 'event_type_2']\n# List the columns to drop from our training \ndrop_cols = ['id', 'd', 'sales', 'scale', 'weight', 'scaled_weight', 'scaled_weight_squared']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make the training data, our df for one year before testing period\nX_train = df[(df.d >= TRAIN_START) & (df.d <= TRAIN_END)].drop(drop_cols, axis=1)\ny_train = df[(df.d >= TRAIN_START) & (df.d <= TRAIN_END)]['sales']\n\n# Also get the weight scale columns before we delete df\nweight_train = df[(df.d >= TRAIN_START) & (df.d <= TRAIN_END)]['scaled_weight']\nweight_squared_train = df[(df.d >= TRAIN_START) & (df.d <= TRAIN_END)]['scaled_weight_squared']\n\n\nX_test = df[(df.d > TRAIN_END) & (df.d <= TRAIN_END + 28)].drop(drop_cols, axis=1)\n# y_test = OUR PREDICTIONS, TO BE SCORED AGAINST THE GROUND TRUTH\n\n# We will also need the id column of the test set to join with predictions\nX_test_id = df[(df.d > TRAIN_END) & (df.d <= TRAIN_END + 28)][['id', 'd']]\n\ndel df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# We will take a 5% sample for 'fake' (in train sample) validation, but \n# we are most concerned with how the models score on the \n# true validation set \nnp.random.seed(777)\n\nfake_valid_inds = np.random.choice(X_train.index.values, 500_000, replace = False)\ntrain_inds = np.setdiff1d(X_train.index.values, fake_valid_inds)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make the first training set with no weights for a baseline\nimport lightgbm as lgb\ntrain_data = lgb.Dataset(X_train.loc[train_inds] , label = y_train.loc[train_inds],\n                         categorical_feature=cat_feats, free_raw_data=False)\nfake_valid_data = lgb.Dataset(X_train.loc[fake_valid_inds], label = y_train.loc[fake_valid_inds],\n                              categorical_feature=cat_feats,\n                 free_raw_data=False)\n# This is a random sample, we're not gonna apply any time series train-test-split tricks here","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start with some reasonable params, not \n# optimal, but good enought to measure effect \n# of weigts and loss functions\nparams = {\n        \"objective\" : \"regression\",\n        \"metric\" :\"rmse\",\n        \"force_row_wise\" : True,\n        \"learning_rate\" : 0.075,\n#         \"sub_feature\" : 0.8,\n        \"sub_row\" : 0.75,\n        \"bagging_freq\" : 1,\n        \"lambda_l2\" : 0.1,\n#         \"nthread\" : 4\n        \"metric\": [\"rmse\"],\n    'verbosity': 1,\n    'num_iterations' : NUM_ITERATIONS,\n    'num_leaves': 128,\n    \"min_data_in_leaf\": 50,\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nbaseline_lgb = lgb.train(params, train_data, valid_sets = [fake_valid_data], verbose_eval=100) \n# Delete data to save memory\ndel train_data, fake_valid_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets train a model with weights\ntrain_data = lgb.Dataset(X_train.loc[train_inds] , \n                         label = y_train.loc[train_inds],\n                         weight = weight_train[train_inds], # putting in weights\n                         categorical_feature=cat_feats, \n                         free_raw_data=False)\nfake_valid_data = lgb.Dataset(X_train.loc[fake_valid_inds], \n                              label = y_train.loc[fake_valid_inds],\n                              weight = weight_train[fake_valid_inds], # putting in weights\n                              categorical_feature=cat_feats,\n                 free_raw_data=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nweight_lgb = lgb.train(params, train_data, valid_sets = [fake_valid_data], verbose_eval=100) \n# Delete data to save memory\ndel train_data, fake_valid_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets make another model with the squared scaled weights\ntrain_data = lgb.Dataset(X_train.loc[train_inds] , \n                         label = y_train.loc[train_inds],\n                         weight = weight_squared_train[train_inds], # putting in weights\n                         categorical_feature=cat_feats, \n                         free_raw_data=False)\nfake_valid_data = lgb.Dataset(X_train.loc[fake_valid_inds], \n                              label = y_train.loc[fake_valid_inds],\n                              weight = weight_squared_train[fake_valid_inds], # putting in weights\n                              categorical_feature=cat_feats,\n                 free_raw_data=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nweight_squared_lgb = lgb.train(params, train_data, valid_sets = [fake_valid_data], verbose_eval=100) \n# Delete data to save memory\ndel train_data, fake_valid_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now lets create a custom loss function \n# Custom loss function \ndef WMSSE(preds, train_data):\n    labels = train_data.get_label()\n    weight = train_data.get_weight()\n    loss = weight*((preds - labels)**2)\n    grad = 2 * weight * (preds - labels)\n    hess = 2 * weight\n    return grad, hess","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets train a model with weights and the \n# WMSSE obj function \ntrain_data = lgb.Dataset(X_train.loc[train_inds] , \n                         label = y_train.loc[train_inds],\n                         weight = weight_train[train_inds], # putting in weights\n                         categorical_feature=cat_feats, \n                         free_raw_data=False)\nfake_valid_data = lgb.Dataset(X_train.loc[fake_valid_inds], \n                              label = y_train.loc[fake_valid_inds],\n                              weight = weight_train[fake_valid_inds], # putting in weights\n                              categorical_feature=cat_feats,\n                 free_raw_data=False)\n\nWMSSE_weight_lgb = lgb.train(params, train_data, valid_sets = [fake_valid_data], verbose_eval=100, fobj=WMSSE) \n# Delete data to save memory\ndel train_data, fake_valid_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets train a model with squared weights and the \n# WMSSE obj function \ntrain_data = lgb.Dataset(X_train.loc[train_inds] , \n                         label = y_train.loc[train_inds],\n                         weight = weight_squared_train[train_inds], # putting in weights\n                         categorical_feature=cat_feats, \n                         free_raw_data=False)\nfake_valid_data = lgb.Dataset(X_train.loc[fake_valid_inds], \n                              label = y_train.loc[fake_valid_inds],\n                              weight = weight_squared_train[fake_valid_inds], # putting in weights\n                              categorical_feature=cat_feats,\n                 free_raw_data=False)\n\nWMSSE_weight_squared_lgb = lgb.train(params, train_data, valid_sets = [fake_valid_data], verbose_eval=100, fobj=WMSSE) \n# Delete data to save memory\ndel train_data, fake_valid_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up an evaluator to test our predictions \nfrom typing import Union\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm as tqdm\n\n                                    \nclass WRMSSEEvaluator(object):\n\n    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame):\n        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n        train_target_columns = train_y.columns.tolist()\n        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n\n        train_df['all_id'] = 0  # for lv1 aggregation\n\n        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n\n        if not all([c in valid_df.columns for c in id_columns]):\n            valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.calendar = calendar\n        self.prices = prices\n\n        self.weight_columns = weight_columns\n        self.id_columns = id_columns\n        self.valid_target_columns = valid_target_columns\n\n        weight_df = self.get_weight_df()\n\n        self.group_ids = (\n            'all_id',\n            'state_id',\n            'store_id',\n            'cat_id',\n            'dept_id',\n            ['state_id', 'cat_id'],\n            ['state_id', 'dept_id'],\n            ['store_id', 'cat_id'],\n            ['store_id', 'dept_id'],\n            'item_id',\n            ['item_id', 'state_id'],\n            ['item_id', 'store_id']\n        )\n\n        for i, group_id in enumerate(tqdm(self.group_ids)):\n            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n            scale = []\n            for _, row in train_y.iterrows():\n                series = row.values[np.argmax(row.values != 0):]\n                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n            setattr(self, f'lv{i + 1}_train_df', train_y)\n            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())\n\n            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n            setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())\n\n    def get_weight_df(self) -> pd.DataFrame:\n        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n\n        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n        return weight_df\n\n    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n        valid_y = getattr(self, f'lv{lv}_valid_df')\n        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n        scale = getattr(self, f'lv{lv}_scale')\n        return (score / scale).map(np.sqrt)\n\n    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n\n        all_scores = []\n        for i, group_id in enumerate(self.group_ids):\n            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n            weight = getattr(self, f'lv{i + 1}_weight')\n            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n            all_scores.append(lv_scores.sum())\n\n        return np.mean(all_scores)\n    \n#######################################################################################################################\n# Reading in data \nPATH = '../input/m5-forecasting-accuracy/'\nstv = pd.read_csv(f'{PATH}sales_train_validation.csv')\ncal = pd.read_csv(f'{PATH}calendar.csv')\nss = pd.read_csv(f'{PATH}sample_submission.csv')\nsp = pd.read_csv(f'{PATH}sell_prices.csv')\n\n\nDAYS_BACK=0 # number of days before 1913 that is the end\n# of the validation period\n\n# Creating our train_fold and valid_fold, according to how many days back \ntrain_fold_df = stv.iloc[:, : -(28 + DAYS_BACK)]\nvalid_fold_df = stv.iloc[:, -(28 + DAYS_BACK): 1919-DAYS_BACK].copy()\n\n# Instantiating evaluators \n# ee = WRMSSEEvaluator_extra(train_fold_df, valid_fold_df, cal, sp)\ne = WRMSSEEvaluator(train_fold_df, valid_fold_df, cal, sp)\ndel stv, cal ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will make a function to get the \n# predictions for every 25th booster of a \n# model\ndef score_model(model, evaluator, test_x, test_id, boosters):\n    scores = []\n    X_test_id = test_id.copy()\n    print('scoring...')\n    for booster in boosters: \n        \n        preds = model.predict(test_x, num_iteration=booster)\n        X_test_id['preds'] = preds\n        preds = X_test_id.pivot('id', 'd', 'preds').reset_index()\n        preds = pd.merge(ss.iloc[:30490]['id'], preds, on='id', how='left').iloc[:, 1:].values\n        scores.append(evaluator.score(preds))\n        \n    return scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = baseline_lgb.predict(X_test)\nX_test_id_ = X_test_id.copy()\nX_test_id_['preds'] = preds\npreds = X_test_id_.pivot('id', 'd', 'preds').reset_index()\npreds = pd.merge(ss.iloc[:30490]['id'], preds, on='id', how='left').iloc[:, 1:].values\n# scores.append(evaluator.score(preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"e.score(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BOOSTERS = [25*i for i in range(1, NUM_ITERATIONS//25 + 1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_dict = {'baseline_lgb': score_model(baseline_lgb, e, X_test, X_test_id, BOOSTERS),\n              'weight_lgb': score_model(weight_lgb, e, X_test, X_test_id, BOOSTERS),\n              'weight_squared_lgb': score_model(weight_squared_lgb, e, X_test, X_test_id, BOOSTERS),\n              'WMSSE_weight_lgb': score_model(WMSSE_weight_lgb, e, X_test, X_test_id, BOOSTERS),\n              'WMSSE_weight_squared_lgb': score_model(WMSSE_weight_squared_lgb, e, X_test, X_test_id, BOOSTERS),}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_df = pd.DataFrame(scores_dict, index=BOOSTERS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_df.plot(figsize=(16,6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_df.to_csv('scores_df.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}