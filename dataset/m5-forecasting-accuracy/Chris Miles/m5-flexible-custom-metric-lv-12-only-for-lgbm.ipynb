{"cells":[{"metadata":{},"cell_type":"markdown","source":"Version 8: Thanks again to [JHkerwin](https://www.kaggle.com/jhkerwin), who spotted an error in my previous notebook, weights and scales for the past 2 years. I fixed it, and reran this kernal, so now the weights match perfectly! \n\nVersion 7: Fixing error in my instansiation of WRMSSEEvaluatordashboard. I had a valid_df input that was for a period ending 28 days before it should have been. Thank you to [JHkerwin](https://www.kaggle.com/jhkerwin) for spotting this error. The result is that our custom metric now scores within .003 of the evaluator dashboard, although the weights are still slightly different and I don't know why exactly. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Need it right now? \n1. Put [this weights and scales notebook](https://www.kaggle.com/chrisrichardmiles/weights-and-scales-for-the-past-2-years) in your data input. \n2. Copy and paste the next hidden cell into your notebook. It has the helper functions. \n3. Copy the next next hidden cell and paste it right below your dataset, which should be in the proper \"format\" (see below).","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# import numpy as np \n# import pandas as pd \n# import random\n\n# ########## Getting the scales and weights #############\n# path = '/kaggle/input/weights-and-scales-for-the-past-2-years/'\n\n# def get_weights_scales_level_12(df, end_test, path):\n#     \"\"\"Gets the scale, weight, and scaled weight in a dataframe, \n#     aligned with the 'id' column of df. \n#     ::path: input path where weight_scale_x files are\n#     \"\"\"\n    \n#     # Get the weights and scales for all the levels \n#     wdf = pd.read_csv(f'{path}weight_scale_{end_test-27}.csv')\n#     # Get the sqrt of the scale, because I didn't do \n#     # that part in the previous notebook. Then divide the weight by the \n#     # sqrt(scale) to get the proper scaled_weight\n#     wdf['scaled_weight'] = wdf.weight/np.sqrt(wdf.scale)\n\n#     # For this function, we just want level 12 weights and scales\n#     wdf = wdf[wdf.Level_id == 'Level12']\n\n#     # We make an 'id' column for easy merging, df must have 'id' column\n#     wdf['id'] = wdf['Agg_Level_1'] + '_' +  wdf['Agg_Level_2'] + '_validation'\n\n#     # Taking just he columns we want to use in the merge \n#     wdf = wdf[['id', 'scale', 'weight', 'scaled_weight']]\n\n#     # Merge with 'id' column of the df\n#     wdf = pd.merge(df[['id']], wdf, on='id', how='left')\n    \n#     return wdf\n\n\n# ############ Calculations function #############\n# # We will define a function outside of the custom \n# # metric. This is because the custom function only \n# # has two inputs: preds and train_data, and we need \n# # to incorporate more than that to make it flexible. \n\n# def L12_WRMSSE(preds, actuals, p_horizon, num_products, scale, weight): \n    \n#     actuals = actuals[-(p_horizon * num_products):]\n#     preds = preds[-(p_horizon * num_products):]\n#     diff = actuals - preds\n\n#     # The first step in calculating the wrmsse is \n#     # squareing the daily error.\n#     res = diff ** 2\n\n#     # Now divide the result by the appropriate scale\n#     # take values of scale to avoid converting res \n#     # to a pandas series\n#     res = res/scale.values\n\n#     # The next step is summing accross the horizon\n#     # We must reshape our data to get the products\n#     # in line. \n#     res = res\n#     res = res.reshape(p_horizon, num_products)\n\n#     # Now we take the mean accross the prediction horizon\n#     # and take the square root of the result.\n#     res = res.mean(axis=0)\n#     res = np.sqrt(res)\n\n#     # Now we multiply each result with the appropriate\n#     # scaled_weight. We just need the first 30490 entries \n#     # of the scaled_weight column\n#     res = res * weight\n#     res = res.sum()\n#     return res","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# ## THIS WILL BE THE TRAINING DATA IN YOUR NOTEBOOK\n# ## DONT COPY THIS PART. YOU NEED A DATA SET, grid_df, LIKE \n# ## THIS ONE, WITH AN 'id' COLUMN.\n\n\n# # grid_df = grid_df[grid_df['d']<=END_TEST].reset_index(drop=True)\n# # END_TEST = 1913 # last day of the validation set.\n\n# ############### \"Fit\" custom metric #################\n# #####################################################\n\n# ################## Variables ######################\n# # Variables needed for the metric. For other training\n# # sets, replace grid_df with the training set, but \n# # make sure that the data is ordered by day and everything \n# # is in correct alignment with values for every product and \n# # every day. \n\n# wdf = get_weights_scales_level_12(grid_df, END_TEST, path)\n\n# P_HORIZON = 28                       # Prediction horizon \n# NUM_PRODUCTS = grid_df.id.nunique()  # Number of products \n\n# scale = wdf[-P_HORIZON * NUM_PRODUCTS:].scale\n# weight = wdf[-P_HORIZON * NUM_PRODUCTS:].weight[:NUM_PRODUCTS]\n# weight = weight/weight.sum()\n# ################### Custom metric #####################\n# def custom_metric(preds, train_data):\n#     actuals = train_data.get_label()\n#     res = L12_WRMSSE(preds, actuals, P_HORIZON, NUM_PRODUCTS, scale, weight)\n#     return 'L12_WRMSSE', res, False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Level 12 WRMSSE\n$$\nWRMSSE = \\sum_{i=1}^{30490} \\left(W_i \\times \\sqrt{\\frac{\\sum_{j=1}^{28}{(D_j)^2}}{S_i}}\\right)\n$$\n* W_i: the weight of the ith series \n* S_i: the scaling factor of the ith series \n* D_j: The difference between sales and predicted sales for the ith series on day j\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Why custom metric?\nDuring training, we would like to measure our model's improvement with respect to the WRMSSE, especially to notice when our model is getting worse (early stopping). I don't know how well other metrics are tracking WRMSSE improvement, but I know they are not explicitly designed with the WRMSSE in mind. \n\n# Why only level 12?\nIts probably faster than adding more levels with rollups. There may be other advantages as well, if you are predicting levels separately and \"reconciling\" them. I think there are other reasons. What do you think? Please tell me pros and cons for single level metrics. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Steps to create a level 12 custom metric that will work with any training data, even if you are using a subset of product ids: \n1. Prepare training data: Data must be in correct order and have values for all products and all days in the validation period, including last 28 days of training \n2. Weights and scales: Make a dataframe with the level 12 weights and scales for all items in your data, aligned with your data for easy use. \n3. Create a wrmsse_lv_12 calculation function and a custom metric for Lightgbm, utilizing the function. ","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"################# Obective ###################\n# Create a flexible metric to measure the WRMSSE\n# for level 12 only. \n\n# Flexible means we can use \n# data with less than all the products and still \n# get a meaningful metric. For instance, trianing \n# by store_id will give you only 3049 of 10490 products,\n# so the full WRMSSE level is not possible, but \n# with weights scaled proportionatly, we can have \n# a meaningful metric. \n\n# The metric will depend on \n# the fact that there is a value for every product \n# for every day of the prediction horizon, because \n# we rely on reshaping of the (sales - preds) column\n# for horizon aggregation. If there is missing values, \n# our data will not be aligned correctly. \n\n# This metric is unique in that it requires specific \n# structure of the training and validation data, because \n# it performs aggregations based on specific products. \n# The metric relies on data ordered by day, then product\n# id. \n\n# Everything will be made clear below. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport random\n\n\nTARGET = 'sales'      # Our Target\nEND_TEST = 1913      # Last day of the 28 day test period\n\nNUM_ITERATIONS = 200\nDIVISOR = 10 # verbosity will be NUM_ITERATIONS//DIVISOR\n\n# ID_COLS = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Preparing training data and baseline model\nWe will need to train a model to explicitly show how we are calculating the level 12 WRMSSE. We will use the basic features created in [simple fe](https://www.kaggle.com/kyakovlev/m5-simple-fe), along with some code to make lightgbm models built on the code in [custom features](https://www.kaggle.com/kyakovlev/m5-custom-features), both authored by \n[@kyakovlev](https://www.kaggle.com/kyakovlev). \n","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"########################### Load data\n########################### Basic features were created here:\n########################### https://www.kaggle.com/kyakovlev/m5-simple-fe\n#################################################################################\n\n# Read data\ngrid_df = pd.concat([pd.read_pickle('../input/m5-simple-fe/grid_part_1.pkl'),\n                     pd.read_pickle('../input/m5-simple-fe/grid_part_2.pkl').iloc[:,2:],\n                     pd.read_pickle('../input/m5-simple-fe/grid_part_3.pkl').iloc[:,2:]],\n                     axis=1)\n\n############ Truncating data set for fast testing #############\n\n############ 90 days training\n############ 28 day test period, \n############ 118 total days in dataset\n############ Validation same as test set for testing purposes\n##############################################################\n# We reset the index so that the weights and scales df, created\n# later in this kernal, will be in alignment.\ngrid_df = grid_df[(grid_df.d > (END_TEST - 118)) & (grid_df.d <= END_TEST)].reset_index(drop=True)\n\n\n\n####################### Masks for data #######################\n##############################################################\n\ntrain_mask = grid_df['d']<=(END_TEST-28)\n\n# Test mask, also used here as validation set in lgbm.\ntest_mask = grid_df['d']>(END_TEST-28)\n\n# Also need a mask for the last 28 days in the training \n# set in order for our custom metric to work. \ntrain_valid_mask = train_mask & (grid_df['d']>(END_TEST-56))\n\n\n#################### Feature columns ########################\n\nremove_features = ['id','d',TARGET, 'weight', 'scale', 'sw'] \nfeatures_columns = [col for col in list(grid_df) if col not in remove_features]\n# Get the test set for models to predict. This is the same \n# as the validation set in training. We use it to see \n# the full WRMSSE.\ntest_x = grid_df[test_mask][features_columns]\n\n# We also need the test_id for the get_preds function \ntest_id = grid_df[test_mask][['id','d']]\n\n########################### Baseline model\n#################################################################################\n\n# We will need some global VARS for future\n\nSEED = 42             # Our random seed for everything\nrandom.seed(42)     # to make all tests \"deterministic\"\nnp.random.seed(SEED)\n\n\n# Features that we want to exclude from training\nremove_features = ['id','d',TARGET, 'weight', 'scale', 'sw'] \n\n# Our baseline model serves\n# to do fast checks of\n# changes to our model weights and objective\n\n# We will use LightGBM for our tests\nimport lightgbm as lgb\n\n\nlgb_params = {\n                    'boosting_type': 'gbdt',         # Standart boosting type\n                    'objective': 'regression',       # Standart loss for RMSE\n                    'metric': ['rmse'],              # as we will use rmse as metric \"proxy\"\n                    'subsample': 0.8,                # Lets really speed things up\n                    'subsample_freq': 1,\n                    'learning_rate': 0.05,           # 0.5 is \"fast enough\" for us\n                    'num_leaves': 2**7-1,            # We will need model only for fast check\n                    'min_data_in_leaf': 2**8-1,      # So we want it to train faster even with drop in generalization \n                    'feature_fraction': 0.8,\n                    'n_estimators': NUM_ITERATIONS,            # We don't want to limit training (you can change 5000 to any big enough number)\n#                     'early_stopping_rounds': 30,     # We will stop training almost immediately (if it stops improving) \n                    'seed': SEED,\n                    'verbose': -1\n                } \n\n                \n# Small function to test different weights, \n# objectives, and metrics\n# estimator = make_fast_test(grid_df)\n# it will return lgb booster for future analisys\ndef make_fast_test(df, lgb_params, weight_column=None, fobj=None, objective=None, lr=None, metric=None):\n\n    features_columns = [col for col in list(df) if col not in remove_features]\n\n    tr_x, tr_y = df[train_mask][features_columns], df[train_mask][TARGET]\n        \n    # Valid set is the same as the \"test\" set for comparison of \n    # custom metric compared to the level 12 of the WRMSSE evaluator\n    vl_x, vl_y = df[test_mask][features_columns], df[test_mask][TARGET]\n    \n    if weight_column: \n            tr_weight = df[train_mask][weight_column]\n            vl_weight = df[test_mask][weight_column]\n            train_data = lgb.Dataset(tr_x, label=tr_y, weight=tr_weight)\n            valid_data = lgb.Dataset(vl_x, label=vl_y, weight=vl_weight)\n    else: \n        train_data = lgb.Dataset(tr_x, label=tr_y)\n        valid_data = lgb.Dataset(vl_x, label=vl_y)\n        \n    lgb_params = lgb_params.copy()\n    \n    if objective: \n        if metric: \n            lgb_params.update({'objective': objective,\n                               'metric': metric})\n        else: \n            lgb_params.update({'objective': objective,\n                               'metric': objective})\n    if lr: \n        lgb_params.update({'learning_rate': lr})\n    \n    if fobj: \n        if metric: \n            estimator = lgb.train(\n                                lgb_params,\n                                train_data,\n                                valid_sets = [train_data, valid_data],\n                                verbose_eval = NUM_ITERATIONS//DIVISOR,\n                                fobj=fobj, \n                                feval=metric\n                            )\n        else:\n            estimator = lgb.train(\n                                lgb_params,\n                                train_data,\n                                valid_sets = [train_data,valid_data],\n                                verbose_eval = NUM_ITERATIONS//DIVISOR,\n                                fobj=fobj\n                            )\n    else: \n        if metric:\n            estimator = lgb.train(\n                                    lgb_params,\n                                    train_data,\n                                    valid_sets = [train_data, valid_data],\n                                    verbose_eval = NUM_ITERATIONS//DIVISOR,\n                                    feval=metric\n                                )\n        else: \n            estimator = lgb.train(\n                                    lgb_params,\n                                    train_data,\n                                    valid_sets = [train_data,valid_data],\n                                    verbose_eval = NUM_ITERATIONS//DIVISOR,\n                                )\n    \n    \n    return estimator\n\n# Make baseline model\nbaseline_model = make_fast_test(grid_df, lgb_params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Function to get proper weights and scales: \nThis utilizes [my notebook](https://www.kaggle.com/chrisrichardmiles/weights-and-scales-for-the-past-2-years), which has already generated weights and scales for the last 2 years, using the [evaluator](https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834) from [sakami](https://www.kaggle.com/sakami) but you could make your own function.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"########## Getting the scales and weights #############\npath = '/kaggle/input/weights-and-scales-for-the-past-2-years/'\n\ndef get_weights_scales_level_12(df, end_test, path):\n    \"\"\"Gets the scale, weight, and scaled weight in a dataframe, \n    aligned with the 'id' column of df. \n    ::path: input path where weight_scale_x files are\n    \"\"\"\n    \n    # Get the weights and scales for all the levels \n    wdf = pd.read_csv(f'{path}weight_scale_{end_test-27}.csv')\n    # Get the sqrt of the scale, because I didn't do \n    # that part in the previous notebook. Then divide the weight by the \n    # sqrt(scale) to get the proper scaled_weight\n    wdf['scaled_weight'] = wdf.weight/np.sqrt(wdf.scale)\n\n    # For this function, we just want level 12 weights and scales\n    wdf = wdf[wdf.Level_id == 'Level12']\n\n    # We make an 'id' column for easy merging, df must have 'id' column\n    wdf['id'] = wdf['Agg_Level_1'] + '_' +  wdf['Agg_Level_2'] + '_validation'\n\n    # Taking just he columns we want to use in the merge \n    wdf = wdf[['id', 'scale', 'weight', 'scaled_weight']]\n\n    # Merge with 'id' column of the df\n    wdf = pd.merge(df[['id']], wdf, on='id', how='left')\n    \n    return wdf\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation metric for comparison\nIn order to verify our result to the level 12 WRMSSE, we will use [WRMSSE evaluator dashboard](https://www.kaggle.com/tnmasui/m5-wrmsse-evaluation-dashboard), authored by [@tnmasui](https://www.kaggle.com/tnmasui). I made a slight code adjustment to limit the graphing to the 12 levels only. ","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns; sns.set()\nimport gc\n\nfrom sklearn import preprocessing\nimport lightgbm as lgb\n\nfrom typing import Union\nfrom tqdm.notebook import tqdm_notebook as tqdm\n\nDATA_DIR = '/kaggle/input/m5-forecasting-accuracy/'\n\nclass WRMSSEEvaluator_dashboard(object):\n\n    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, \n                 calendar: pd.DataFrame, prices: pd.DataFrame):\n        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n        train_target_columns = train_y.columns.tolist()\n        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n\n        train_df['all_id'] = 'all'  # for lv1 aggregation\n\n        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')]\\\n                     .columns.tolist()\n        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')]\\\n                               .columns.tolist()\n\n        if not all([c in valid_df.columns for c in id_columns]):\n            valid_df = pd.concat([train_df[id_columns], valid_df], \n                                 axis=1, sort=False)\n\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.calendar = calendar\n        self.prices = prices\n\n        self.weight_columns = weight_columns\n        self.id_columns = id_columns\n        self.valid_target_columns = valid_target_columns\n\n        weight_df = self.get_weight_df()\n\n        self.group_ids = (\n            'all_id',\n            'state_id',\n            'store_id',\n            'cat_id',\n            'dept_id',\n            ['state_id', 'cat_id'],\n            ['state_id', 'dept_id'],\n            ['store_id', 'cat_id'],\n            ['store_id', 'dept_id'],\n            'item_id',\n            ['item_id', 'state_id'],\n            ['item_id', 'store_id']\n        )\n\n        for i, group_id in enumerate(tqdm(self.group_ids)):\n            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n            scale = []\n            for _, row in train_y.iterrows():\n                series = row.values[np.argmax(row.values != 0):]\n                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n            setattr(self, f'lv{i + 1}_train_df', train_y)\n            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)\\\n                    [valid_target_columns].sum())\n\n            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n            setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())\n\n    def get_weight_df(self) -> pd.DataFrame:\n        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns]\\\n                    .set_index(['item_id', 'store_id'])\n        weight_df = weight_df.stack().reset_index()\\\n                   .rename(columns={'level_2': 'd', 0: 'value'})\n        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n\n        weight_df = weight_df.merge(self.prices, how='left',\n                                    on=['item_id', 'store_id', 'wm_yr_wk'])\n        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n        weight_df = weight_df.set_index(['item_id', 'store_id', 'd'])\\\n                    .unstack(level=2)['value']\\\n                    .loc[zip(self.train_df.item_id, self.train_df.store_id), :]\\\n                    .reset_index(drop=True)\n        weight_df = pd.concat([self.train_df[self.id_columns],\n                               weight_df], axis=1, sort=False)\n        return weight_df\n\n    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n        valid_y = getattr(self, f'lv{lv}_valid_df')\n        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n        scale = getattr(self, f'lv{lv}_scale')\n        return (score / scale).map(np.sqrt) \n\n    def score(self, valid_preds: Union[pd.DataFrame, \n                                       np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape \\\n               == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, \n                                       columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], \n                                 valid_preds], axis=1, sort=False)\n\n        all_scores = []\n        for i, group_id in enumerate(self.group_ids):\n\n            valid_preds_grp = valid_preds.groupby(group_id)[self.valid_target_columns].sum()\n            setattr(self, f'lv{i + 1}_valid_preds', valid_preds_grp)\n            \n            lv_scores = self.rmsse(valid_preds_grp, i + 1)\n            setattr(self, f'lv{i + 1}_scores', lv_scores)\n            \n            weight = getattr(self, f'lv{i + 1}_weight')\n            lv_scores = pd.concat([weight, lv_scores], axis=1, \n                                  sort=False).prod(axis=1)\n            \n            all_scores.append(lv_scores.sum())\n            \n        self.all_scores = all_scores\n\n        return np.mean(all_scores)\n    \n\n    \ndef create_viz_df(df,lv):\n    \n    df = df.T.reset_index()\n    if lv in [6,7,8,9,11,12]:\n        df.columns = [i[0] + '_' + i[1] if i != ('index','') \\\n                      else i[0] for i in df.columns]\n    df = df.merge(calendar.loc[:, ['d','date']], how='left', \n                  left_on='index', right_on='d')\n    df['date'] = pd.to_datetime(df.date)\n    df = df.set_index('date')\n    df = df.drop(['index', 'd'], axis=1)\n    \n    return df\n\ndef create_dashboard(evaluator, by_level_only=False, model_name=None):\n    \n    wrmsses = [np.mean(evaluator.all_scores)] + evaluator.all_scores\n    labels = ['Overall'] + [f'Level {i}' for i in range(1, 13)]\n\n    ## WRMSSE by Level\n    plt.figure(figsize=(12,5))\n    ax = sns.barplot(x=labels, y=wrmsses)\n    ax.set(xlabel='', ylabel='WRMSSE')\n    \n    #######################ALTERATION##########################\n    title = 'WRMSSE by Level'\n    if model_name: \n        title = f'WRMSSE by Level for {model_name}'\n    plt.title(title, fontsize=20, fontweight='bold')\n    #######################ALTERATION-COMPLETE##########################\n\n  \n    for index, val in enumerate(wrmsses):\n        ax.text(index*1, val+.01, round(val,4), color='black', \n                ha=\"center\")\n        \n    #######################ALTERATION##########################\n    if by_level_only:       # stops function early for quick plotting of \n        plt.show()          # for quick plotting of levels\n        return\n    #######################ALTERATION-COMPLETE##########################\n\n    # configuration array for the charts\n    n_rows = [1, 1, 4, 1, 3, 3, 3, 3, 3, 3, 3, 3]\n    n_cols = [1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n    width = [7, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n    height = [4, 3, 12, 3, 9, 9, 9, 9, 9, 9, 9, 9]\n    \n    for i in range(1,13):\n        \n        scores = getattr(evaluator, f'lv{i}_scores')\n        weights = getattr(evaluator, f'lv{i}_weight')\n        \n        if i > 1 and i < 9:\n            if i < 7:\n                fig, axs = plt.subplots(1, 2, figsize=(12, 3))\n            else:\n                fig, axs = plt.subplots(2, 1, figsize=(12, 8))\n                \n            ## RMSSE plot\n            scores.plot.bar(width=.8, ax=axs[0], color='g')\n            axs[0].set_title(f\"RMSSE\", size=14)\n            axs[0].set(xlabel='', ylabel='RMSSE')\n            if i >= 4:\n                axs[0].tick_params(labelsize=8)\n            for index, val in enumerate(scores):\n                axs[0].text(index*1, val+.01, round(val,4), color='black', \n                            ha=\"center\", fontsize=10 if i == 2 else 8)\n            \n            ## Weight plot\n            weights.plot.bar(width=.8, ax=axs[1])\n            axs[1].set_title(f\"Weight\", size=14)\n            axs[1].set(xlabel='', ylabel='Weight')\n            if i >= 4:\n                axs[1].tick_params(labelsize=8)\n            for index, val in enumerate(weights):\n                axs[1].text(index*1, val+.01, round(val,2), color='black', \n                            ha=\"center\", fontsize=10 if i == 2 else 8)\n                    \n            fig.suptitle(f'Level {i}: {evaluator.group_ids[i-1]}', size=24 ,\n                         y=1.1, fontweight='bold')\n            plt.tight_layout()\n            plt.show()\n\n        trn = create_viz_df(getattr(evaluator, f'lv{i}_train_df')\\\n                            .iloc[:, -28*3:], i)\n        val = create_viz_df(getattr(evaluator, f'lv{i}_valid_df'), i)\n        pred = create_viz_df(getattr(evaluator, f'lv{i}_valid_preds'), i)\n\n        n_cate = trn.shape[1] if i < 7 else 9\n\n        fig, axs = plt.subplots(n_rows[i-1], n_cols[i-1], \n                                figsize=(width[i-1],height[i-1]))\n        if i > 1:\n            axs = axs.flatten()\n\n        ## Time series plot\n        for k in range(0, n_cate):\n\n            ax = axs[k] if i > 1 else axs\n\n            trn.iloc[:, k].plot(ax=ax, label='train')\n            val.iloc[:, k].plot(ax=ax, label='valid')\n            pred.iloc[:, k].plot(ax=ax, label='pred')\n            ax.set_title(f\"{trn.columns[k]}  RMSSE:{scores[k]:.4f}\", size=14)\n            ax.set(xlabel='', ylabel='sales')\n            ax.tick_params(labelsize=8)\n            ax.legend(loc='upper left', prop={'size': 10})\n\n        if i == 1 or i >= 9:\n            fig.suptitle(f'Level {i}: {evaluator.group_ids[i-1]}', size=24 , \n                         y=1.1, fontweight='bold')\n        plt.tight_layout()\n        plt.show()\n        \ntrain_df = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv')\ncalendar = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')\nsell_prices = pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv')\ndata_df = train_df.loc[:, :'d_' + str(END_TEST)]\n\ntrain_fold_df = data_df.iloc[:, :-28]\nvalid_fold_df = data_df.iloc[:, -28:].copy()\n# Instantiate an evaluator for scoring validation periodstarting day 1886\ne = WRMSSEEvaluator_dashboard(train_fold_df, valid_fold_df, calendar, sell_prices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Helper function\n# We only need to for comparing our predictions with an evaluator\n\n\n################## Prediction handlers ###################\n\n# Lets get the sample submission file also for our functions \nss = pd.read_csv('../input/m5-forecasting-accuracy/sample_submission.csv')\n\n\ndef get_preds(model,test_x, test_id, ss, booster=None):\n    X_test_id = test_id.copy()\n    if booster: \n        preds = model.predict(test_x, num_iteration=booster)\n    else:\n        preds = model.predict(test_x)\n    X_test_id['preds'] = preds\n    preds = X_test_id.pivot('id', 'd', 'preds').reset_index()\n    preds = pd.merge(ss.iloc[:30490]['id'], preds, on='id', how='left').iloc[:, 1:].values\n    return preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Example calculations","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"############ Level 12 WRMSSE #############\n\n# We want calculate the WRMSSE for level 12. \n# We will use the same 28 day horizon as the \n# competition metric, but this could be changed \n# to any horizon as long there is a value for every \n# product on every day of the prediction horizon. \n# This is because we must reshape the data to sum\n# accross the prediction horizon. \n\n########## Get scales and weights #############\nwdf = get_weights_scales_level_12(grid_df, END_TEST, path)\nwdf.head()\n\n########## Veryfying the data will work ############\n# According to our parameters in our make_fast_test, \n# our validation set has 28 days of data. In order\n# for our custom metric to work, \n# we can only consider the last 28 days of training\n# data, and we must verify there are values for all \n# days.\n\n\n# Checking that data has value for every day. \n# Both train and test shape[0]/28 should \n# be 30490. train_valid is to get only the\n# last 28 days of data of the training set.\ntrain_valid = grid_df[train_valid_mask]\nprint(train_valid.shape[0]/28) \nprint(grid_df[test_mask].shape[0]/28)\n# GOOD they are correct\n\n########## Calculating the WRMSSE ############\n\n# We start with a column which shows the \n# difference between actuals and predictions.\n# We will simulate this with predictions from \n# the baseline model for example purposes. \npreds = baseline_model.predict(test_x)\nactuals = grid_df[test_mask].sales\ndiff = actuals - preds\n\n# The first step in calculating the wrmsse is \n# squaring the daily error.\nres = diff ** 2\n\n# Now divide the result by the appropriate scale.\n# wdf is aligned with grid_df so this is easy.\n# This will result in a pandas series.\nscale = wdf[test_mask].scale\nres = res/scale\n\n# The next step is summing accross the horizon\n# We must reshape our data to get the products\n# in line. We must turn the pandas series into \n# an array to reshape.  \nres = res.values\nres = res.reshape(28, 30490)\n\n# Now we take the mean accross the prediction horizon\n# and take the square root of the result.\nres = res.mean(axis=0)\nres = np.sqrt(res)\n\n# Now we multiply each result with the appropriate\n# scaled_weight. We just need the first 30490 entries \n# of the weight column.\nweight = wdf[test_mask].weight[:30490]\nres = res * weight\nres = res.sum()\n\nprint(f\"We calculate the WRMSSE of level 12 to be {res}\")\n\n############## Compare to WRMSSE ###############\n_ = e.score(get_preds(baseline_model, test_x, test_id, ss))\ncreate_dashboard(e, by_level_only=True)\n\n#################### No discrepency in results ######################\n# Edit: After fixing my errors the wieghts match perefectly \ndf = e.lv12_weight.reset_index()\ndf['id'] = df.item_id + '_' + df.store_id\nprint('############# Examining weights ##################\\n\\n')\nprint('Weights for the evaluator')\nprint(df.sort_values('id')[['id', 0]].head())\nprint('\\n\\nWeights that I am using')\nprint(wdf[:30490][['id', 'weight']].sort_values('id').head())\nprint('\\nAll is good')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Creating the custom metric","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"############ Calculations function #############\n# We will define a function outside of the custom \n# metric. This is because the custom function only \n# has two inputs: preds and train_data, and we need \n# to incorporate more than that to make it flexible. \n\ndef L12_WRMSSE(preds, actuals, p_horizon, num_products, scale, weight): \n    \n    actuals = actuals[-(p_horizon * num_products):]\n    preds = preds[-(p_horizon * num_products):]\n    diff = actuals - preds\n\n    # The first step in calculating the wrmsse is \n    # squareing the daily error.\n    res = diff ** 2\n\n    # Now divide the result by the appropriate scale\n    # take values of scale to avoid converting res \n    # to a pandas series\n    res = res/scale.values\n\n    # The next step is summing accross the horizon\n    # We must reshape our data to get the products\n    # in line. \n    res = res\n    res = res.reshape(p_horizon, num_products)\n\n    # Now we take the mean accross the prediction horizon\n    # and take the square root of the result.\n    res = res.mean(axis=0)\n    res = np.sqrt(res)\n\n    # Now we multiply each result with the appropriate\n    # scaled_weight. We just need the first 30490 entries \n    # of the scaled_weight column\n    res = res * weight\n    res = res.sum()\n    return res\n\n\n################## Variables ######################\n# Variables needed for the metric. For other training\n# sets, replace grid_df with the training set, but \n# make sure that the data is ordered by day and everything \n# is in correct alignment with values for every product and \n# every day. \n\nP_HORIZON = 28         # Prediction horizon \nNUM_PRODUCTS = 30490   # Number of products \n\nscale = wdf[train_valid_mask].scale\nweight = wdf[train_valid_mask].weight[:NUM_PRODUCTS]\n\n################### Custom metric #####################\ndef custom_metric(preds, train_data):\n    actuals = train_data.get_label()\n    res = L12_WRMSSE(preds, actuals, P_HORIZON, NUM_PRODUCTS, scale, weight)\n    return 'L12_WRMSSE', res, False\n\n################## Custom metric in action #####################\n# Im leaving the RMSE metric in for now\nbaseline_model = make_fast_test(grid_df, lgb_params, metric=custom_metric)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making it flexible","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"################# Making if flexible ######################\n# Lets say we want to train a model, using a subset of \n# the total products, say training by store_id. \nstore_mask = grid_df.store_id == 'CA_1'\ngrid_df_ca = grid_df[store_mask] \n\n# The problem is that the level 12 weights do not add \n# up to 1 now. All  we need to do is scale up the weights\n# and adjust the total products. \nP_HORIZON = 28         # Prediction horizon \nNUM_PRODUCTS = grid_df_ca.id.nunique()   # Number of products \n\nscale = wdf[train_valid_mask][store_mask].scale\nweight = wdf[train_valid_mask][store_mask].weight[:NUM_PRODUCTS]\n\n# Normalize weights so that the sum to 1.\nweight = weight/weight.sum()\n\n################### Custom metric #####################\ndef custom_metric(preds, train_data):\n    actuals = train_data.get_label()\n    res = L12_WRMSSE(preds, actuals, P_HORIZON, NUM_PRODUCTS, scale, weight)\n    return 'L12_WRMSSE', res, False\n\n################## Custom metric in action #####################\n# Im leaving the RMSE metric in for now\nbaseline_model = make_fast_test(grid_df_ca, lgb_params, metric=custom_metric)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion: \nWe can now generate a custom metric for level 12, regardless of how we subset our training data. To use this method, these conditions must be met: \n* Training data must be ordered by day, then product id for correct alignment\n* There is a value for every product for every day in the validation period, as well as the last (length(validation)) days of the training set\n* You have a dataframe (wdf for me here) that has the correct scales and weights, aligned with your data correctly. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Next steps: \n\n## Test custom objectives and scaling methods, using this custom metric to evaluate the usefulness of the methods. \n\n## Trying using this method for early stopping. ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}