{"cells":[{"metadata":{},"cell_type":"markdown","source":"# The search for the loss function \n## TLDR: Failed so far to produce very good results. \n## Usefulness: Shows how to create different custom loss functions, and demonstrates how they can use local variables within them. This also shows how one could calculate the gradient (of some sort) with sparce matrices during training. \n## The main reason for sharing is to show how I am trying to get the gradients for each level 1-12. This is towards the bottom of the notebook. Please let my know if you have any hints for how to make a custom objective function properly. \nThis notebook is copied from, and uses the features generated by kyakovlev in [this notebook](https://www.kaggle.com/kyakovlev/m5-simple-fe). They are used to make simple models and compare different objective functions. I have just started testing different objective functions and this notebook could serve as a rough template, although it is unpolished for now. \n\nI also use ragnars asymetric loss function from [this kernal](https://www.kaggle.com/ragnar123/simple-lgbm-groupkfold-cv) as a baseline\n\nI also use [Tomonori's](https://www.kaggle.com/tnmasui) [this amazing dashboard](https://www.kaggle.com/tnmasui/m5-wrmsse-evaluation-dashboard) with a slight modefication, well noted in the code, to display only the 12 level breakdown of the the wrmsse scores for fast display. This dashboard is amazing and everyone should use it to get more info on how models are doing. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os, sys, gc, warnings, psutil, random\nfrom scipy.sparse import csr_matrix\n\nwarnings.filterwarnings('ignore')\n\nTARGET = 'sales'      # Our Target\nEND_TEST = 1913      # Last day of the 28 day test period\nNUM_ITERATIONS = 1000   ### SET THIS HIGHER IF YOU WANT TO SEE BETTER TESTING\nID_COLS = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns; sns.set()\nimport gc\n\nfrom sklearn import preprocessing\nimport lightgbm as lgb\n\nfrom typing import Union\nfrom tqdm.notebook import tqdm_notebook as tqdm\n\nDATA_DIR = '/kaggle/input/m5-forecasting-accuracy/'\n\nclass WRMSSEEvaluator_dashboard(object):\n\n    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, \n                 calendar: pd.DataFrame, prices: pd.DataFrame):\n        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n        train_target_columns = train_y.columns.tolist()\n        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n\n        train_df['all_id'] = 'all'  # for lv1 aggregation\n\n        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')]\\\n                     .columns.tolist()\n        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')]\\\n                               .columns.tolist()\n\n        if not all([c in valid_df.columns for c in id_columns]):\n            valid_df = pd.concat([train_df[id_columns], valid_df], \n                                 axis=1, sort=False)\n\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.calendar = calendar\n        self.prices = prices\n\n        self.weight_columns = weight_columns\n        self.id_columns = id_columns\n        self.valid_target_columns = valid_target_columns\n\n        weight_df = self.get_weight_df()\n\n        self.group_ids = (\n            'all_id',\n            'state_id',\n            'store_id',\n            'cat_id',\n            'dept_id',\n            ['state_id', 'cat_id'],\n            ['state_id', 'dept_id'],\n            ['store_id', 'cat_id'],\n            ['store_id', 'dept_id'],\n            'item_id',\n            ['item_id', 'state_id'],\n            ['item_id', 'store_id']\n        )\n\n        for i, group_id in enumerate(tqdm(self.group_ids)):\n            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n            scale = []\n            for _, row in train_y.iterrows():\n                series = row.values[np.argmax(row.values != 0):]\n                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n            setattr(self, f'lv{i + 1}_train_df', train_y)\n            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)\\\n                    [valid_target_columns].sum())\n\n            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n            setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())\n\n    def get_weight_df(self) -> pd.DataFrame:\n        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns]\\\n                    .set_index(['item_id', 'store_id'])\n        weight_df = weight_df.stack().reset_index()\\\n                   .rename(columns={'level_2': 'd', 0: 'value'})\n        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n\n        weight_df = weight_df.merge(self.prices, how='left',\n                                    on=['item_id', 'store_id', 'wm_yr_wk'])\n        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n        weight_df = weight_df.set_index(['item_id', 'store_id', 'd'])\\\n                    .unstack(level=2)['value']\\\n                    .loc[zip(self.train_df.item_id, self.train_df.store_id), :]\\\n                    .reset_index(drop=True)\n        weight_df = pd.concat([self.train_df[self.id_columns],\n                               weight_df], axis=1, sort=False)\n        return weight_df\n\n    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n        valid_y = getattr(self, f'lv{lv}_valid_df')\n        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n        scale = getattr(self, f'lv{lv}_scale')\n        return (score / scale).map(np.sqrt) \n\n    def score(self, valid_preds: Union[pd.DataFrame, \n                                       np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape \\\n               == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, \n                                       columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], \n                                 valid_preds], axis=1, sort=False)\n\n        all_scores = []\n        for i, group_id in enumerate(self.group_ids):\n\n            valid_preds_grp = valid_preds.groupby(group_id)[self.valid_target_columns].sum()\n            setattr(self, f'lv{i + 1}_valid_preds', valid_preds_grp)\n            \n            lv_scores = self.rmsse(valid_preds_grp, i + 1)\n            setattr(self, f'lv{i + 1}_scores', lv_scores)\n            \n            weight = getattr(self, f'lv{i + 1}_weight')\n            lv_scores = pd.concat([weight, lv_scores], axis=1, \n                                  sort=False).prod(axis=1)\n            \n            all_scores.append(lv_scores.sum())\n            \n        self.all_scores = all_scores\n\n        return np.mean(all_scores)\n    \n\n    \ndef create_viz_df(df,lv):\n    \n    df = df.T.reset_index()\n    if lv in [6,7,8,9,11,12]:\n        df.columns = [i[0] + '_' + i[1] if i != ('index','') \\\n                      else i[0] for i in df.columns]\n    df = df.merge(calendar.loc[:, ['d','date']], how='left', \n                  left_on='index', right_on='d')\n    df['date'] = pd.to_datetime(df.date)\n    df = df.set_index('date')\n    df = df.drop(['index', 'd'], axis=1)\n    \n    return df\n\ndef create_dashboard(evaluator, by_level_only=False, model_name=None):\n    \n    wrmsses = [np.mean(evaluator.all_scores)] + evaluator.all_scores\n    labels = ['Overall'] + [f'Level {i}' for i in range(1, 13)]\n\n    ## WRMSSE by Level\n    plt.figure(figsize=(12,5))\n    ax = sns.barplot(x=labels, y=wrmsses)\n    ax.set(xlabel='', ylabel='WRMSSE')\n    \n    #######################ALTERATION##########################\n    title = 'WRMSSE by Level'\n    if model_name: \n        title = f'WRMSSE by Level for {model_name}'\n    plt.title(title, fontsize=20, fontweight='bold')\n    #######################ALTERATION-COMPLETE##########################\n\n  \n    for index, val in enumerate(wrmsses):\n        ax.text(index*1, val+.01, round(val,4), color='black', \n                ha=\"center\")\n        \n    #######################ALTERATION##########################\n    if by_level_only:       # stops function early for quick plotting of \n        plt.show()          # for quick plotting of levels\n        return\n    #######################ALTERATION-COMPLETE##########################\n\n    # configuration array for the charts\n    n_rows = [1, 1, 4, 1, 3, 3, 3, 3, 3, 3, 3, 3]\n    n_cols = [1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n    width = [7, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n    height = [4, 3, 12, 3, 9, 9, 9, 9, 9, 9, 9, 9]\n    \n    for i in range(1,13):\n        \n        scores = getattr(evaluator, f'lv{i}_scores')\n        weights = getattr(evaluator, f'lv{i}_weight')\n        \n        if i > 1 and i < 9:\n            if i < 7:\n                fig, axs = plt.subplots(1, 2, figsize=(12, 3))\n            else:\n                fig, axs = plt.subplots(2, 1, figsize=(12, 8))\n                \n            ## RMSSE plot\n            scores.plot.bar(width=.8, ax=axs[0], color='g')\n            axs[0].set_title(f\"RMSSE\", size=14)\n            axs[0].set(xlabel='', ylabel='RMSSE')\n            if i >= 4:\n                axs[0].tick_params(labelsize=8)\n            for index, val in enumerate(scores):\n                axs[0].text(index*1, val+.01, round(val,4), color='black', \n                            ha=\"center\", fontsize=10 if i == 2 else 8)\n            \n            ## Weight plot\n            weights.plot.bar(width=.8, ax=axs[1])\n            axs[1].set_title(f\"Weight\", size=14)\n            axs[1].set(xlabel='', ylabel='Weight')\n            if i >= 4:\n                axs[1].tick_params(labelsize=8)\n            for index, val in enumerate(weights):\n                axs[1].text(index*1, val+.01, round(val,2), color='black', \n                            ha=\"center\", fontsize=10 if i == 2 else 8)\n                    \n            fig.suptitle(f'Level {i}: {evaluator.group_ids[i-1]}', size=24 ,\n                         y=1.1, fontweight='bold')\n            plt.tight_layout()\n            plt.show()\n\n        trn = create_viz_df(getattr(evaluator, f'lv{i}_train_df')\\\n                            .iloc[:, -28*3:], i)\n        val = create_viz_df(getattr(evaluator, f'lv{i}_valid_df'), i)\n        pred = create_viz_df(getattr(evaluator, f'lv{i}_valid_preds'), i)\n\n        n_cate = trn.shape[1] if i < 7 else 9\n\n        fig, axs = plt.subplots(n_rows[i-1], n_cols[i-1], \n                                figsize=(width[i-1],height[i-1]))\n        if i > 1:\n            axs = axs.flatten()\n\n        ## Time series plot\n        for k in range(0, n_cate):\n\n            ax = axs[k] if i > 1 else axs\n\n            trn.iloc[:, k].plot(ax=ax, label='train')\n            val.iloc[:, k].plot(ax=ax, label='valid')\n            pred.iloc[:, k].plot(ax=ax, label='pred')\n            ax.set_title(f\"{trn.columns[k]}  RMSSE:{scores[k]:.4f}\", size=14)\n            ax.set(xlabel='', ylabel='sales')\n            ax.tick_params(labelsize=8)\n            ax.legend(loc='upper left', prop={'size': 10})\n\n        if i == 1 or i >= 9:\n            fig.suptitle(f'Level {i}: {evaluator.group_ids[i-1]}', size=24 , \n                         y=1.1, fontweight='bold')\n        plt.tight_layout()\n        plt.show()\n        \ntrain_df = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv')\ncalendar = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')\nsell_prices = pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv')\ntrain_df = train_df.loc[:, :'d_' + str(END_TEST)]\n\ntrain_fold_df = train_df.iloc[:, :-28]\nvalid_fold_df = train_fold_df.iloc[:, -28:].copy()\n# Instantiate an evaluator for scoring validation periodstarting day 1886\ne = WRMSSEEvaluator_dashboard(train_fold_df, valid_fold_df, calendar, sell_prices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper functions \n# Lets get the sample submission file also for our functions \nss = pd.read_csv('../input/m5-forecasting-accuracy/sample_submission.csv')\n\ndef score_model(evaluator, model, test_x, test_id, boosters, ss):\n    \"\"\"\n    Returns a list of scores for each booster in boosters, \n    which is a list of numbers, denoting the iteration of \n    the model.\n    \n    boosters: list of numbers\n    ss: sample_submission file, as to align the data, \n    needed for most public evaluators\n    \"\"\"\n    scores = []\n    X_test_id = test_id.copy()\n    print('scoring...')\n    for booster in boosters: \n        \n        preds = model.predict(test_x, num_iteration=booster)\n        X_test_id['preds'] = preds\n        preds = X_test_id.pivot('id', 'd', 'preds').reset_index()\n        preds = pd.merge(ss.iloc[:30490]['id'], preds, on='id', how='left').iloc[:, 1:].values\n        scores.append(evaluator.score(preds))\n        \n    return scores\n\n\ndef get_preds(model,test_x, test_id, ss, booster=None):\n    X_test_id = test_id.copy()\n    if booster: \n        preds = model.predict(test_x, num_iteration=booster)\n    else:\n        preds = model.predict(test_x)\n    X_test_id['preds'] = preds\n    preds = X_test_id.pivot('id', 'd', 'preds').reset_index()\n    preds = pd.merge(ss.iloc[:30490]['id'], preds, on='id', how='left').iloc[:, 1:].values\n    return preds","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"########################### Load data\n########################### Basic features were created here:\n########################### https://www.kaggle.com/kyakovlev/m5-simple-fe\n#################################################################################\n\n# Read data\ngrid_df = pd.concat([pd.read_pickle('../input/m5-simple-fe/grid_part_1.pkl'),\n                     pd.read_pickle('../input/m5-simple-fe/grid_part_2.pkl').iloc[:,2:],\n                     pd.read_pickle('../input/m5-simple-fe/grid_part_3.pkl').iloc[:,2:]],\n                     axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training d"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Subsampling\n# to make all calculations faster.\n# Keep only data after day 1885-90 (90 days of training data)\ngrid_df = grid_df[(grid_df.d > 1885-90) & (grid_df.d <= END_TEST)]\n# Set up test set (day 1886-1913) \nremove_features = ['id','d',TARGET, 'weight', 'scale'] \nfeatures_columns = [col for col in list(grid_df) if col not in remove_features]\ntest_x = grid_df[grid_df['d']>(END_TEST-28)][features_columns]\n# We also need the id column to merge with sample submission so it is\n# in the proper order to score with our evaluator\ntest_id = grid_df[grid_df['d']>(END_TEST-28)][['id', 'd']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Baseline model\n#################################################################################\n\n# We will need some global VARS for future\n\nSEED = 42             # Our random seed for everything\nrandom.seed(42)     # to make all tests \"deterministic\"\nnp.random.seed(SEED)\n\n\n# Drop some items from \"TEST\" set part (1914...)\ngrid_df = grid_df[grid_df['d']<=END_TEST].reset_index(drop=True)\n\n# Features that we want to exclude from training\nremove_features = ['id','d',TARGET, 'weight', 'scale'] \n\n# Our baseline model serves\n# to do fast checks of\n# changes to our model weights and objective\n\n# We will use LightGBM for our tests\nimport lightgbm as lgb\n\n\nlgb_params = {\n                    'boosting_type': 'gbdt',         # Standart boosting type\n                    'objective': 'regression',       # Standart loss for RMSE\n                    'metric': ['rmse'],              # as we will use rmse as metric \"proxy\"\n                    'subsample': 0.8,                # Lets really speed things up\n                    'subsample_freq': 1,\n                    'learning_rate': 0.05,           # 0.5 is \"fast enough\" for us\n                    'num_leaves': 2**7-1,            # We will need model only for fast check\n                    'min_data_in_leaf': 2**8-1,      # So we want it to train faster even with drop in generalization \n                    'feature_fraction': 0.8,\n                    'n_estimators': NUM_ITERATIONS,            # We don't want to limit training (you can change 5000 to any big enough number)\n#                     'early_stopping_rounds': 30,     # We will stop training almost immediately (if it stops improving) \n                    'seed': SEED,\n                    'verbose': -1\n                } \n\n                \n# Small function to make fast features tests\n# estimator = make_fast_test(grid_df)\n# it will return lgb booster for future analisys\ndef make_fast_test(df, lgb_params, weight_column=None,weight_valid=None, fobj=None):\n\n    features_columns = [col for col in list(df) if col not in remove_features]\n\n    tr_x, tr_y = df[df['d']<=(END_TEST-28)][features_columns], df[df['d']<=(END_TEST-28)][TARGET]\n    if weight_column: \n#         tr_weight = df[df['d']<=(END_TEST-28)][weight_column]\n        tr_weight = weight_column # now you must put int the column yourself, not just a 'col' name\n\n    vl_x, v_y = df[df['d']>(END_TEST-28)][features_columns], df[df['d']>(END_TEST-28)][TARGET]\n    if weight_valid: \n        vl_weight = df[df['d']>(END_TEST-28)][weight_column]\n        \n    if weight_column: \n        if weight_valid: \n            train_data = lgb.Dataset(tr_x, label=tr_y, weight=tr_weight)\n            valid_data = lgb.Dataset(vl_x, label=v_y, weight=vl_weight)\n        else: \n            train_data = lgb.Dataset(tr_x, label=tr_y, weight=tr_weight)\n            valid_data = lgb.Dataset(vl_x, label=v_y)\n    else: \n        train_data = lgb.Dataset(tr_x, label=tr_y)\n        valid_data = lgb.Dataset(vl_x, label=v_y)\n    \n    \n    if fobj: \n        estimator = lgb.train(\n                            lgb_params,\n                            train_data,\n                            valid_sets = [train_data,valid_data],\n                            verbose_eval = NUM_ITERATIONS//10,\n                            fobj=fobj\n                        )\n    else: \n        estimator = lgb.train(\n                                lgb_params,\n                                train_data,\n                                valid_sets = [train_data,valid_data],\n                                verbose_eval = NUM_ITERATIONS//10,\n                            )\n    \n    \n    return estimator\n\n# Make baseline model\n# baseline_model = make_fast_test(grid_df, lgb_params)\n# baseline_model_poisson = make_fast_test(grid_df, lgb_params_poisson)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# BOOSTERS = [(NUM_ITERATIONS//10)*i for i in range(1, 11)]\n# BOOSTERS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# res_df = pd.DataFrame({'baseline_model': score_model(e, baseline_model, test_x, test_id, BOOSTERS, ss)}, \n#                      index=BOOSTERS)\n\n# res_df.plot(figsize=(15,6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Custom loss function"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Custom loss function \ndef WMSSE(preds, train_data):\n    labels = train_data.get_label()\n    weight = train_data.get_weight()\n    loss = weight*((preds - labels)**2)\n    grad = 2 * weight * (preds - labels)\n    hess = 2 * weight\n    return grad, hess","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Custom objective function: getting the gradients "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get a copy of the training data that will go into \n# the lgbm dataset. We will make scaled_weight\n# columns for each level, containing the scaled weight of\n# the series to which each row belongs to. Initialize \n# the weith matrix with just the id columns\ntr_x = grid_df[grid_df['d']<=(END_TEST-28)]\ntr_w = tr_x[ID_COLS]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the weights and scales for all the levels \nwdf = pd.read_csv('../input/weights-and-scales-for-the-past-2-years/weight_scale_1886.csv')\n# Get the sqrt of the scale, because I didn't do \n# that part in the previous notebook. Then divide the weight by the \n# sqrt(scale) to get the proper scaled_weight\nwdf['scaled_weight'] = wdf.weight/np.sqrt(wdf.scale)\nwdf = wdf[[ 'Level_id', 'Agg_Level_1', 'Agg_Level_2','scaled_weight']]\n\n######################### level 1 #######################################\ntr_w['level_1_sw'] = wdf.loc[wdf['Level_id'] == 'Level1', 'scaled_weight'][0]\n\n######################### level 2 #######################################\nlevel = wdf[wdf.Level_id == 'Level2']\n# Set the name of the aggregation column to match \n# the column in our id columns\nlevel.columns = ['Level_id', 'state_id', 'Agg_Level_2', 'scaled_weight']\nlevel_scaled_weight = pd.merge(tr_w[['state_id']], level[['state_id', 'scaled_weight']],\n                               on='state_id', how='left')[['scaled_weight']]\ntr_w['level_2_sw'] = level_scaled_weight['scaled_weight'].values\n\ndel level, level_scaled_weight\n\n######################### level 3 #######################################\nlevel = wdf[wdf.Level_id == 'Level3']\n# Set the name of the aggregation column to match \n# the column in our id columns\nlevel.columns = ['Level_id', 'store_id', 'Agg_Level_2', 'scaled_weight']\nlevel_scaled_weight = pd.merge(tr_w[['store_id']], level[['store_id', 'scaled_weight']],\n                               on='store_id', how='left')[['scaled_weight']]\ntr_w['level_3_sw'] = level_scaled_weight['scaled_weight'].values\n\ndel level, level_scaled_weight\n\n######################### level 4 #######################################\nlevel = wdf[wdf.Level_id == 'Level4']\n# Set the name of the aggregation column to match \n# the column in our id columns\nlevel.columns = ['Level_id', 'cat_id', 'Agg_Level_2', 'scaled_weight']\nlevel_scaled_weight = pd.merge(tr_w[['cat_id']], level[['cat_id', 'scaled_weight']],\n                               on='cat_id', how='left')[['scaled_weight']]\ntr_w['level_4_sw'] = level_scaled_weight['scaled_weight'].values\n\ndel level, level_scaled_weight\n\n######################### level 5 #######################################\nlevel = wdf[wdf.Level_id == 'Level5']\n# Set the name of the aggregation column to match \n# the column in our id columns\nlevel.columns = ['Level_id', 'dept_id', 'Agg_Level_2', 'scaled_weight']\nlevel_scaled_weight = pd.merge(tr_w[['dept_id']], level[['dept_id', 'scaled_weight']],\n                               on='dept_id', how='left')[['scaled_weight']]\ntr_w['level_5_sw'] = level_scaled_weight['scaled_weight'].values\n\ndel level, level_scaled_weight\n\n######################### level 6 #######################################\nlevel = wdf[wdf.Level_id == 'Level6']\n# Set the name of the aggregation column to match \n# the column in our id columns\nlevel.columns = ['Level_id', 'state_id', 'cat_id', 'scaled_weight']\nlevel_scaled_weight = pd.merge(tr_w[['state_id', 'cat_id']], level[['state_id', 'cat_id', 'scaled_weight']],\n                               on=['state_id', 'cat_id'], how='left')[['scaled_weight']]\ntr_w['level_6_sw'] = level_scaled_weight['scaled_weight'].values\n\ndel level, level_scaled_weight\n\n######################### level 7 #######################################\nlevel = wdf[wdf.Level_id == 'Level7']\n# Set the name of the aggregation column to match \n# the column in our id columns\nlevel.columns = ['Level_id', 'state_id', 'dept_id', 'scaled_weight']\nlevel_scaled_weight = pd.merge(tr_w[['state_id', 'dept_id']], level[['state_id', 'dept_id', 'scaled_weight']],\n                               on=['state_id', 'dept_id'], how='left')[['scaled_weight']]\ntr_w['level_7_sw'] = level_scaled_weight['scaled_weight'].values\n\ndel level, level_scaled_weight\n\n######################### level 8 #######################################\nlevel = wdf[wdf.Level_id == 'Level8']\n# Set the name of the aggregation column to match \n# the column in our id columns\nlevel.columns = ['Level_id', 'store_id', 'cat_id', 'scaled_weight']\nlevel_scaled_weight = pd.merge(tr_w[['store_id', 'cat_id']], level[['store_id', 'cat_id', 'scaled_weight']],\n                               on=['store_id', 'cat_id'], how='left')[['scaled_weight']]\ntr_w['level_8_sw'] = level_scaled_weight['scaled_weight'].values\n\ndel level, level_scaled_weight\n\n######################### level 9 #######################################\nlevel = wdf[wdf.Level_id == 'Level9']\n# Set the name of the aggregation column to match \n# the column in our id columns\nlevel.columns = ['Level_id', 'store_id', 'dept_id', 'scaled_weight']\nlevel_scaled_weight = pd.merge(tr_w[['store_id', 'dept_id']], level[['store_id', 'dept_id', 'scaled_weight']],\n                               on=['store_id', 'dept_id'], how='left')[['scaled_weight']]\ntr_w['level_9_sw'] = level_scaled_weight['scaled_weight'].values\n\ndel level, level_scaled_weight\n\n######################### level 10 #######################################\nlevel = wdf[wdf.Level_id == 'Level10']\n# Set the name of the aggregation column to match \n# the column in our id columns\nlevel.columns = ['Level_id', 'item_id', 'dept_id', 'scaled_weight']\nlevel_scaled_weight = pd.merge(tr_w[['item_id']], level[['item_id', 'scaled_weight']],\n                               on=['item_id'], how='left')[['scaled_weight']]\ntr_w['level_10_sw'] = level_scaled_weight['scaled_weight'].values\n\ndel level, level_scaled_weight\n\n######################### level 11 #######################################\nlevel = wdf[wdf.Level_id == 'Level11']\n# Set the name of the aggregation column to match \n# the column in our id columns\nlevel.columns = ['Level_id', 'state_id', 'item_id', 'scaled_weight']\nlevel_scaled_weight = pd.merge(tr_w[['state_id', 'item_id']], level[['state_id', 'item_id', 'scaled_weight']],\n                               on=['state_id', 'item_id'], how='left')[['scaled_weight']]\ntr_w['level_11_sw'] = level_scaled_weight['scaled_weight'].values\n\ndel level, level_scaled_weight\n\n######################### level 12 #######################################\nlevel = wdf[wdf.Level_id == 'Level12']\n# Set the name of the aggregation column to match \n# the column in our id columns\nlevel.columns = ['Level_id', 'item_id', 'store_id', 'scaled_weight']\nlevel_scaled_weight = pd.merge(tr_w[['item_id', 'store_id']], level[['item_id', 'store_id', 'scaled_weight']],\n                               on=['item_id', 'store_id'], how='left')[['scaled_weight']]\ntr_w['level_12_sw'] = level_scaled_weight['scaled_weight'].values\n\ndel level, level_scaled_weight, wdf\ngc.collect()\n\ncols = [col for col in tr_w.columns if 'level' in col]\ntr_w = tr_w[cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Roll up matrices for gradients\nThey are commented out because I can't utilize all of them in a custom loss function so far"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get a \"dummy matix\" to roll up level 1 for each day\n# roll_1 = csr_matrix(pd.get_dummies(tr_x.d, drop_first=False).values)\n# roll_2 = csr_matrix(pd.get_dummies(tr_x.d.astype('str') + tr_x.state_id.astype('str'),\n#                                    drop_first=False).values)\n\n# roll_3 = csr_matrix(pd.get_dummies(tr_x.d.astype('str') + tr_x.store_id.astype('str'),\n#                                    drop_first=False).values)\n\n# roll_4 = csr_matrix(pd.get_dummies(tr_x.d.astype('str') + tr_x.cat_id.astype('str'),\n#                                    drop_first=False).values)\n\n# roll_5 = csr_matrix(pd.get_dummies(tr_x.d.astype('str') + tr_x.dept_id.astype('str'),\n#                                    drop_first=False).values)\n\n# roll_6 = csr_matrix(pd.get_dummies(tr_x.d.astype('str') + tr_x.state_id.astype('str') + tr_x.cat_id.astype('str') ,\n#                                    drop_first=False).values)\n\n# roll_7 = csr_matrix(pd.get_dummies(tr_x.d.astype('str') + tr_x.state_id.astype('str') + tr_x.dept_id.astype('str'),\n#                                    drop_first=False).values)\n\n# roll_8 = csr_matrix(pd.get_dummies(tr_x.d.astype('str') + tr_x.store_id.astype('str') + tr_x.cat_id.astype('str'),\n#                                    drop_first=False).values)\n\n# roll_9 = csr_matrix(pd.get_dummies(tr_x.d.astype('str') + tr_x.store_id.astype('str') + tr_x.dept_id.astype('str'),\n#                                    drop_first=False).values)\n\n# roll_10 = csr_matrix(pd.get_dummies(tr_x.d.astype('str') + tr_x.item_id.astype('str') + tr_x.dept_id.astype('str'),\n#                                    drop_first=False).values)\n\n# roll_11 = csr_matrix(pd.get_dummies(tr_x.d.astype('str') + tr_x.state_id.astype('str') + tr_x.item_id.astype('str'),\n#                                    drop_first=False).values)\n\n# roll_12 = csr_matrix(pd.get_dummies(tr_x.d.astype('str') + tr_x.item_id.astype('str') + tr_x.store_id.astype('str'),\n#                                    drop_first=False).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nroll_1 = csr_matrix(pd.get_dummies(tr_x.d, drop_first=False).values)\nroll_2 = csr_matrix(pd.get_dummies(tr_x.d.astype('str') + tr_x.state_id.astype('str'),\n                                   drop_first=False).values)\nroll_4 = csr_matrix(pd.get_dummies(tr_x.d.astype('str') + tr_x.cat_id.astype('str'),\n                                   drop_first=False).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef get_level_grad(level_roll, level_sw, actuals, preds):\n    \"\"\"Returns the partial gradient. l1 is a sparse matix \n    made from get_dummies with levels \"\"\"\n    # We only need the differences to \"roll up\",\n    # meaning aggregate and sum (i think)\n    diff = actuals - preds\n\n    # Reshape the difference vector so that we \n    # can do matix multiplication l1 * diff\n    diff = np.reshape(diff,(level_roll.T.shape[1],-1))\n\n    # Execute the matrix multiplication. The result is \n    # the total difference of sales - predicted for \n    # each day. \n    res = level_roll.T * diff\n\n    # For each day, all we need is the sign of \n    # daily difference, so we \"squish\" positive \n    # and negative values to 1 and -1. Then we \n    # flip the sign, because the partial derivative\n    # with respect to preds is negative when \n    # actuals - preds is positive, and visa versa.\n    res = np.where(res < 0, 1, -1)\n    \n    # reshape the result to be contained in one axis\n    res = res.reshape(res.shape[0])\n    \n    # Expand the result into a matrix that will match\n    # the size of level_roll for element-wise multiplication\n    res = np.tile(res, (level_roll.shape[0],1))\n    \n    # Element-wise multiplication \n    res = level_roll.multiply(res)\n    \n    # Squish sparse matrix into one column\n    res = res.sum(axis=1)\n    \n    # Reshape into single row\n    res = np.array(res).reshape(1,-1)[0]\n\n    # Res is now a column with 1 or -1 for each row, \n    # indicating the sign of the derivative of the \n    # level_series to which that row belongs. \n    # This is our gradient with respect to this level. \n    level_grad = level_sw * res\n    return level_grad","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing: \nEverything after this are my first attempts at making custom functions. The only one that seems promissing right now is obj_10, which is the only time I was able to see slight improvement (must change NUM_ITERATIONS at the top to 200 or more to see it improve) over the asymetric loss function from ragnar in [this kernal](https://www.kaggle.com/ragnar123/simple-lgbm-groupkfold-cv)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def obj(preds, train_data): \n    actuals = train_data.get_label()\n    diff = actuals - preds\n    \n#     g1 = get_level_grad(roll_1, tr_w.level_1_sw, actuals, preds)\n#     g4 = get_level_grad(roll_4, tr_w.level_4_sw, actuals, preds)\n\n    \n    sign = np.where(diff<0, 1, -1)\n    g12 = tr_w.level_12_sw * sign\n    \n    grad = g12\\\n#     + g4\\\n#     + g1\n    grad = grad/np.abs(grad).mean()\n    hess = np.where(diff>0, 1, 1)\n\n    return grad, hess","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def obj_6(preds, train_data): \n    actuals = train_data.get_label()\n    diff = actuals - preds\n    \n    g1 = get_level_grad(roll_1, tr_w.level_1_sw, actuals, preds)\n\n\n    grad = np.where(diff < 0, np.where(g1>0,-1 * diff, -1/2 * diff), np.where(g1<0,-1 * diff, -1/2 * diff))\n    hess = np.where(diff < 0, np.where(g1>0,1 , 1/2), np.where(g1<0,1 , 1/2))\n    return grad, hess","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def obj_7(preds, train_data): \n    actuals = train_data.get_label()\n    diff = actuals - preds\n    \n    g1 = get_level_grad(roll_1, tr_w.level_1_sw, actuals, preds)\n\n\n    grad = np.where(diff < 0, np.where(g1>0,-1 * diff, -1/5 * diff), np.where(g1<0,-1 * diff, -1/5 * diff))\n    hess = np.where(diff < 0, np.where(g1>0,1 , 1/5), np.where(g1<0,1 , 1/5))\n    return grad, hess","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def obj_8(preds, train_data): \n    actuals = train_data.get_label()\n    diff = actuals - preds\n    \n    g1 = get_level_grad(roll_1, tr_w.level_1_sw, actuals, preds)\n\n\n\n    grad = np.where(diff < 0, np.where(g1>0,-1 * diff, -4/5 * diff), np.where(g1<0,-1 * diff, -4/5 * diff))\n    hess = np.where(diff < 0, np.where(g1>0,1 , 4/5), np.where(g1<0,1 , 4/5))\n    return grad, hess","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def obj_9(preds, train_data): \n    actuals = train_data.get_label()\n    diff = actuals - preds\n    \n    g1 = get_level_grad(roll_1, tr_w.level_1_sw, actuals, preds)\n    g2 = get_level_grad(roll_2, tr_w.level_2_sw, actuals, preds)\n    \n    g1 += g2\n\n    grad = np.where(diff < 0, np.where(g1>0,-1 * diff, -4/5 * diff), np.where(g1<0,-1 * diff, -4/5 * diff))\n    hess = np.where(diff < 0, np.where(g1>0,1 , 4/5), np.where(g1<0,1 , 4/5))\n    return grad, hess","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def obj_10(preds, train_data): \n    actuals = train_data.get_label()\n    diff = actuals - preds\n    \n    g1 = get_level_grad(roll_1, tr_w.level_1_sw, actuals, preds)\n    g2 = get_level_grad(roll_2, tr_w.level_2_sw, actuals, preds)\n    g4 = get_level_grad(roll_4, tr_w.level_4_sw, actuals, preds)\n\n    g1 += g2 + g4\n\n\n    grad = np.where(diff < 0, np.where(g1>0,-1 * diff, -4/5 * diff), np.where(g1<0,-1 * diff, -4/5 * diff))\n    hess = np.where(diff < 0, np.where(g1>0,1 , 4/5), np.where(g1<0,1 , 4/5))\n    return grad, hess","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom_asymmetric_train(y_pred, y_true):\n    y_true = y_true.get_label()\n    residual = (y_true - y_pred).astype(\"float\")\n    grad = np.where(residual < 0, -2 * residual, -2 * residual * 1.15)\n    hess = np.where(residual < 0, 2, 2 * 1.15)\n\n    return grad, hess\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom_asymmetric_train_2(y_pred, y_true):\n    y_true = y_true.get_label()\n    residual = (y_true - y_pred).astype(\"float\")\n    grad = np.where(residual < 0, -2 * residual, -2 * residual )\n    hess = np.where(residual < 0, 2, 2 )\n\n    return grad, hess","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_cat = make_fast_test(grid_df, lgb_params, fobj=custom_asymmetric_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_cat_2 = make_fast_test(grid_df, lgb_params, fobj=custom_asymmetric_train_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_obj_6 = make_fast_test(grid_df, lgb_params, fobj=obj_6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_obj_7 = make_fast_test(grid_df, lgb_params, fobj=obj_7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_obj_8 = make_fast_test(grid_df, lgb_params, fobj=obj_8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_obj_9 = make_fast_test(grid_df, lgb_params, fobj=obj_9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_obj_10 = make_fast_test(grid_df, lgb_params, fobj=obj_10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing results "},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = e.score(get_preds(m_cat, test_x, test_id, ss))\ncreate_dashboard(e, by_level_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = e.score(get_preds(m_cat_2, test_x, test_id, ss))\ncreate_dashboard(e, by_level_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = e.score(get_preds(m_obj_6, test_x, test_id, ss))\ncreate_dashboard(e, by_level_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = e.score(get_preds(m_obj_7, test_x, test_id, ss))\ncreate_dashboard(e, by_level_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = e.score(get_preds(m_obj_8, test_x, test_id, ss))\ncreate_dashboard(e, by_level_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = e.score(get_preds(m_obj_9, test_x, test_id, ss))\ncreate_dashboard(e, by_level_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = e.score(get_preds(m_obj_10, test_x, test_id, ss))\ncreate_dashboard(e, by_level_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}