{"cells":[{"metadata":{},"cell_type":"markdown","source":"# The point of this notebook: \nCreate a starting module of functions that we can easily customize to make many features. \nStart with this basic lags feature engineering framework, customize it with new functions (we will have to do it in batches though due to memory issues). You could also alter the functions to create features for different aggregation levels (must aggregate sales before processing). \n\n# What you can get out of this notebook\n\n1. Know how to make lag features from the horizontal \"rectangle\" data representation, which is how the data starts.\n2. A flexible, copy-pastable, customizable, pipeline-insertionable, mini module of functions at the end of the notebook. \n3. Knoweldge of how to utilize numpy to do quick rolling window aggregations.\n\n\n# RAM issues \nMy notebook must have crashed 100 times while I was trying to finish this and make it nice. \n#### Things to mind when doing these kinds of computations: \n* Datatypes matter: We use this info by setting features to float16. We should be careful if there will be many caculations that demand finer details that float64 provides. EXAMPLE: feature.astype(np.float16). Objects and float64 seems to eat up memory and cause \"allocating too much memory\" crashes.\n* Numpy functions on rolling windows: I do a technique to make the rolling window mean and std calculations fast. I think it calculates all the windows at once in parallel or something. But this std will use too much ram very easily. Therefore I had to do batches of size 10. Even with this, the RAM almost maxes out when calculating the standard deviation of the 180 sized window. Keep this in mind when using other numpy functions, custom functions, or window sizes for calculations. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom time import time \nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################## Load data ####################\ntrain_df = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"################## Create Grid ##################\n#\n# We want our data in a \n# \"grid\" form, where we have a row for every \n# product id on every day. This is the proper \n# data representation for an lgbm (at least that \n# I know). \ns = time()\nstart_time = time()\nDROP_COLS = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\ngrid_df = train_df.drop(DROP_COLS, axis=1).melt(id_vars='id', var_name='d', value_name='sales')\nprint(f\"Total time for melt: {(time() - start_time)/60} min\")\n\n# Saving space\nstart_time = time()\ngrid_df['d'] = grid_df.d.str[2:].astype(np.int16)\nprint(f\"Total time for day col change: {(time() - start_time)/60} min\")\n\n\nstart_time = time()\ngrid_df['id'] = grid_df.id.astype('category')\nprint(f\"Total time for category: {(time() - start_time)/60} min\")\n\nprint(f'Total time: {(time() - s)/60}')\ngrid_df\n\ndel s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n################# Faster grid ceation #####################\n# BE CAREFUL ABOUT DTYPES. I don't set sales and d columns \n# dtypes but I have to in order to conserve memory and \n# prevent my notebooks from crashing. My final functions \n# do have the adjustments. \ndays = 1913\nd_cols = [f'd_{i}' for i in range(1, days + 1)]\nindex = train_df.id.astype('category')\nsales = train_df[d_cols].values.T.reshape(-1,)\n\n\ng = pd.DataFrame({'id': np.tile(index, days), \n                  'd': np.concatenate([[i] * 30490 for i in range(1, days + 1)]), \n                  'sales': np.float64(sales)})\n\ndisplay((g == grid_df).all().all())\ndel index, sales, days","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################### Rectangle ###################\n#\n# I will take the sales values as they are to \n# form my base \"rectangle\" of sales. \n# I think I can take this recatangle and \n# quickly reshape it so that it lines up \n# with grid_df. If I am correct we can use this \n# to create any lags we want super fast. \n\nd_cols = [f'd_{i}' for i in range(1,1914)]\nrec = train_df[d_cols].values\n\n\n################## Test ########################\n\n# I will test my idea by reshaping the basic \n# rectangle so that it matches sales.\ntest_sales = rec.T.reshape(-1)\nprint('test_sales matches sales?? ', (test_sales == grid_df['sales']).all())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############# Make lag_1 feature ###############\nlag_day = 1\n\n# We need to take off the last (lag_day) columns\n# from our rectangle. Then we can reshape the \n# sales to long format.\nlag = rec[:, :-lag_day].T.reshape(-1,)\n\n# The new column must be prepended with np.nans\n# to make up for the data we have cut off \n# our rectangle. Therefore, all the d_1 products \n# in grid_df will have np.nan for lag_1. In \n# fact, as we carry out this process for all \n# lag days, rows with sales on d_x will have \n# np.nan values for all lags lag_y where y >= x.\ngrid_df[f'lag_{lag_day}'] = np.append(np.zeros(30490 * lag_day) + np.nan, lag).astype(np.float16)\n\n\n###### Checking work\n# Lets check our work. Looking at day 1912\n# of train_df.tail() should be the same as \n# grid_df''lag_1'].tail() \nprint('Checking our work')\ndisplay(train_df[['d_1912']].tail(10))\ndisplay(grid_df[['lag_1']].tail(10))\nprint('They are the same. Fantastic!')\n\ndel lag_day, lag","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################ Make lag function #################\ndef make_lag_col(rec, lag_day=1):\n    \"\"\"rec is just train_df[d_cols].values\"\"\"\n    \n    # We need to take off the last lag_day columns\n    lag = rec[:, :-lag_day].T.reshape(-1,)\n\n    # The new column must be prepended with np.nans\n    return np.append(np.zeros(30490 * lag_day) + np.nan, lag).astype(np.float16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n############### Make lags for 14 days ###############\nfor i in range(1, 16): \n    grid_df[f'lag_{i}'] = make_lag_col(rec=rec, lag_day=i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n############ Pandas shift ##############\n# I realized later that we could also \n# just use pandas shift.  easier to implement. \n# Here we will do it for g, which was the\n# same as grid_df before adding lags. \n# So I think this is the better way of adding \n# lags. Our time was not wasted though, \n# because we learned skills that we will \n# need for making rolling windows. \nfor i in range(1,16):\n    g[f'lag_{i}'] = g['sales'].shift(30490 * i).astype(np.float16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del g\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################# Rolling features #################\n####################################################\n\n######## rolling window ##############\n#\n# Lets again utilize our sales rectangel rec, and \n# do some fast rolling calculations. \n# \n########### rolling window function ############\n# Please check\n# out this article: \n## https://rigtorp.se/2011/01/01/rolling-statistics-numpy.html\n# it shows how to create rolling windows that you can\n# use to do really fast numpy calculations with. \ndef rolling_window(a, window):\n    \"\"\"Reference: https://rigtorp.se/2011/01/01/rolling-statistics-numpy.html\n    A super fast way of getting rolling windows on a numpy array. \"\"\"\n    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n    strides = a.strides + (a.strides[-1],)\n    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n\n###### Example to see it works  #######\nx = np.array([[1,2,3,4,5] for i in range(3)])\nprint(\"Here is our array x\")\ndisplay(x)\nrw = rolling_window(x, 3)\n\nprint(f\"Here is our rw array, with shape {rw.shape} made from x\")\ndisplay(rw)\n\nprint(\"Here is our rolling mean with window 3\")\ndisplay(np.mean(rw, axis=-1))\n\nprint(\"Here is our rolling std with window 3\")\nnp.std(rw, axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del x, rw","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############# Rolling features funciton ################\n\n####### Walk through ##########\n# Lets make rolling_mean_3\n## Variables for function \nwindow = 3\nrw = rolling_window(rec, 3)\nprint(f'shape of rw is {rw.shape}')\nfunction = np.mean\n\n# We need to take off the last columns so \n# get the rolling feature shifted one day. \ncol = function(rw, -1)[:, :-1].T.reshape(-1,)\n\n# The new column must be prepended with np.nans\ncol = np.append(np.zeros(30490 * window) + np.nan, col).astype(np.float16)\n\n# Make sure the shape matches grid_df\ndisplay(grid_df.shape[0])\ndisplay(col.shape[0])\ndisplay(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del rw, window, function, col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################ make rolling col helper ###############\n\n#### version 1 ######\n\n# def make_rolling_col_v1(rw, window, function): \n#     # We need to take off the last columns to\n#     # get the rolling feature shifted one day. \n\n#     col = function(rw, -1)[:, :-1].T.reshape(-1,)\n\n#     # The new column must be prepended with np.nans \n#     # to account for missing gaps\n\n#     return np.append(np.zeros(30490 * window) + np.nan, col).astype(np.float16)\n\n# This version is commented out because it breaks my \n# notebook session. I get a message saying I have tried \n# to allocate too much memory. I discovered that the \n# problem was with np.std when the window was 30 or \n# above. I believe the problem was np was trying to \n# calculate std for all windows, and that was just \n# too much. But I experimented with np.split(rw), and \n# found that there was no problem calculating std in \n# 10 batches, even for window 180. I have set splits \n# to 10. If you have a function or window that still \n# causes a crash, you can increase splits to 3049, the \n# next factor of 30490. \n# I have noticed a slight slow down \n# when doing this, so I will leave it at 10 for now. \n\n##### experiment code to show problem #####\n## This will break \n# rw = rolling_window(rec, 180)\n# np.std(rw, -1) \n\n## This will not break\n# rw = rolling_window(rec, 180)\n# x= np.split(rw, 10, axis=0)\n# x = [np.std(rw, -1) for rw in x]\n\n\n#### Final version #####\ndef make_rolling_col(rw, window, function): \n    # We need to take off the last columns to\n    # get the rolling feature shifted one day.\n    \n    split_rw = np.split(rw, 10, axis=0)\n    split_col = [function(rw, -1) for rw in split_rw]\n    col = np.concatenate(split_col)\n    col = col[:, :-1].T.reshape(-1,)\n\n    # The new column must be prepended with np.nans \n    # to account for missing gaps\n    return np.append(np.zeros(30490 * window) + np.nan, col).astype(np.float16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_rolling_cols(df: pd.DataFrame, rec: np.array, windows: list, functions: list, function_names: list): \n    \"\"\"Adds rolling features to df.\"\"\"\n    \n    print( 72 * '#', '\\nAdding rolling columns\\n',  )\n    start_time = time()\n    f = list(zip(functions, function_names))\n    \n    for window in windows: \n        rw = rolling_window(rec, window)\n        for function in f: \n            s_time = time()\n            df[f'shift_1_rolling_{function[1]}_{str(window)}'] = make_rolling_col(rw, window, function[0])\n            print(f'{function[1]} with window {window} time: {(time() - s_time):.2f} seconds')\n            \n    print(f'Total time for rolling cols: {(time() - start_time)/60:.2f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################ Adding rolling features ###############\nadd_rolling_cols(grid_df, \n                 rec, \n                 windows=[7, 14, 30, 60, 180], \n                 functions=[np.mean, np.std], \n                 function_names=['mean', 'std'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n################ Shifted lag rolling features ###################\n#\n# Perhaps I want to also want to know 7 day rolling \n# mean, but from 7 seven days ago. This could go \n# directly into a model, or we could create a weekly\n# momentum feature = shift_1_rolling_mean_7/shift_8_rolling_mean_7. \n# I propose we have already calculated these features, \n# we just need to shift the columns by 30490 * (shift_days - 1).\n# We subtract 1 from shift_days because the column shift_1_rolling_mean_7\n# is already shifted 1 day. \n######## prototype #########\n### Objective ###\n# create col shift_8_rolling_mean_7: shift 7, rolling mean with window 7.\n\n### Features check ###\n# shift_8_rolling_mean_7[-30490:] == grid_df[grid_df.d == 1913 - 7]['rolling_mean_7']\n\n### x ###\nshift_8_rolling_mean_7 = grid_df['shift_1_rolling_mean_7'].shift((8-1) * 30490)\n\n### test ###\n(shift_8_rolling_mean_7[-30490:] == grid_df[grid_df.d == 1913 - 7]['shift_1_rolling_mean_7'].values).all()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############ Shifting function ###############\ndef add_shift_cols(grid_df, shifts, cols, num_series=30490): \n    for shift in shifts: \n        for col in cols: \n            grid_df[f\"{col.replace('shift_1', f'shift_{shift}')}\"] = grid_df[col].shift((shift - 1) * num_series)\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############## Adding shifted rolling mean ###############\nshifts = [7, 14, 21, 28]\ncols = [f'shift_1_rolling_mean_{i}' for i in [7, 14]]\nadd_shift_cols(grid_df, shifts, cols, num_series=30490)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(grid_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del grid_df, shift_8_rolling_mean_7, shifts, cols\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Module of functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"################## Helper functions ########################\n############################################################\n\n################## Load data ####################\n# train_df = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv')\n\n############################################################\n######################### Imports ##########################\nimport numpy as np \nimport pandas as pd\nfrom time import time\nimport gc\n\n############################################################\n#################### Making grid_df ########################\n\ndef nan_leading_zeros(rec):\n    rec = rec.astype(np.float64)\n    zero_mask = rec.cumsum(axis=1) == 0\n    rec[zero_mask] = np.nan\n    return rec\n\ndef make_grid_df(train_df, pred_horizon=True): \n    \"\"\"Returns a grid \"\"\"\n    \n    start_time = time()\n    print(\"#\" * 72, \"\\nMaking grid_df\")\n    # Add 28 days for the predicton horizon \n    \n    last_day = int(train_df.columns[-1][2:])\n    if pred_horizon: \n        for i in range(last_day + 1, last_day + 29): \n            train_df[f'd_{i}'] = np.nan\n            \n            \n    d_cols = [col for col in train_df.columns if 'd_' in col]\n    index = train_df.id\n    index = pd.Series(np.tile(index, last_day + 28)).astype('category')\n    \n    # Turn leading zeros into np.nan\n    rec = nan_leading_zeros(train_df[d_cols].values)\n    sales = rec.T.reshape(-1,)\n\n    \n    grid_df = pd.DataFrame({'id': index, \n                      'd': np.concatenate([[i] * 30490 for i in range(1, last_day + 28 + 1)]).astype(np.int16), \n                      'sales': sales})\n    print(f'Time: {(time() - start_time):.2f} seconds')\n    return grid_df, rec\n\n############################################################\n#####################@ Basic lags ##########################\n\ndef add_lags(grid_df, lags = range(1,16)):\n    \n    start_time = time()\n    print( 72 * '#', '\\nAdding lag columns')\n    for i in lags:\n        grid_df[f'lag_{i}'] = grid_df['sales'].shift(30490 * i).astype(np.float16)\n    \n    print(f'Time: {(time() - start_time):.2f} seconds')\n        \n        \n############################################################       \n################# Rolling window columns ###################\n\ndef rolling_window(a, window):\n    \"\"\"Reference: https://rigtorp.se/2011/01/01/rolling-statistics-numpy.html\n    A super fast way of getting rolling windows on a numpy array. \"\"\"\n    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n    strides = a.strides + (a.strides[-1],)\n    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n\ndef make_rolling_col(rw, window, function): \n    # We need to take off the last columns to\n    # get the rolling feature shifted one day.\n    \n    split_rw = np.split(rw, 10, axis=0)\n    split_col = [function(rw, -1) for rw in split_rw]\n    col = np.concatenate(split_col)\n    col = col[:, :-1].T.reshape(-1,)\n\n    # The new column must be prepended with np.nans \n    # to account for missing gaps\n    return np.append(np.zeros(30490 * window) + np.nan, col).astype(np.float16)\n\n\ndef add_rolling_cols(df: pd.DataFrame, rec: np.array, windows: list, functions: list, function_names: list): \n    \"\"\"Adds rolling features to df.\"\"\"\n    \n    print( 72 * '#', '\\nAdding rolling columns\\n',  )\n    start_time = time()\n    f = list(zip(functions, function_names))\n    \n    for window in windows: \n        rw = rolling_window(rec, window)\n        for function in f: \n            s_time = time()\n            df[f'shift_1_rolling_{function[1]}_{str(window)}'] = make_rolling_col(rw, window, function[0])\n            print(f'{function[1]} with window {window} time: {(time() - s_time):.2f} seconds')\n            \n    print(f'Total time for rolling cols: {(time() - start_time)/60:.2f}')\n    \n    \n    \n############################################################       \n################# Shifting function ########################\ndef add_shift_cols(grid_df, shifts, cols, num_series=30490): \n    \n    print( 72 * '#', '\\nAdding shift columns',  )\n    start_time = time()\n    for shift in shifts: \n        for col in cols: \n            grid_df[f\"{col.replace('shift_1', f'shift_{shift}')}\"] = grid_df[col].shift((shift - 1) * num_series)\n    print(f'Time: {(time() - start_time):.2f} seconds')\n\n\n            \n            \n            \n############################################################       \n################# Create lags df ###########################\ndef make_lags_df(train_df): \n    \n    start_time = time()\n    grid_df, rec = make_grid_df(train_df)\n    add_lags(grid_df)\n    add_rolling_cols(grid_df, \n                     rec, \n                     windows=[7, 14, 30, 60, 180], \n                     functions=[np.mean, np.std], \n                     function_names=['mean', 'std'])\n    \n    \n    shifts = [8, 15]\n    cols = [f'shift_1_rolling_mean_{i}' for i in [7, 14, 30, 60]]\n    add_shift_cols(grid_df, shifts, cols, num_series=30490)\n    \n    print(72 * '#', f'Total time: {(time() - start_time)//60:} : {(time() - start_time)%60:.2f}')\n    return grid_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_df = make_lags_df(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_df.to_pickle('lags.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_df.info()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}