{"cells":[{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# More helper functions\nIn this notebook, we will develop some helper functions that will generate series for all 12 aggregation levels, as well as basic statistics for each series. We will use this info for EDA, and possibly for feature engineering. In particular, the get_stats_df function could be used again to create imbedded features for ML algorithms as well as baseline models. \n\n## get_series_df(train_df, rollup_matrix_csr, rollup_index): \nThis will take the data as given in sales_train_validaiton.csv, and return the series for all 12 levels of aggregation. \n\n## get_stats_df(series_df, cal_df): \nTakes series_df df as returned by the previous function and returns a dataframe with all the stats for each series. \n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"############### Imports ######################\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Local module imports \nfrom m5_helpers import get_rollup, get_w_df\n\n############################### Load data ###########################\nprices_df = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\nss = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')\ncal_df = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\ntrain_df = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv')\n\n################## Series for all 12 levels ##################\n# We will need an aggregating matrix \"fit\" on train_df. \n# Good thing we previously made a function to get that. \nrollup_matrix_csr, rollup_index  = get_rollup(train_df)\n\n# We want a dataframe with all the aggregated series. \nseries_df = pd.DataFrame(data=rollup_matrix_csr * train_df.iloc[:, 6:].values,\n                         index=rollup_index, \n                         columns=train_df.iloc[:, 6:].columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################# stats ######################\n# Lets if we can use describe and transpose \n# to get some statistical features of different columns\nseries_df.T.loc[:, (1, slice(None))].describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n############ Create stats_df ################\nstats_df = series_df.T.describe().T\nstats_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is good, but I'd like to do a bit better. For one thing, count is \n# not really helpful as levels 10-12 because leading zeros indicate that \n# an item is not for sale yet, and therefore should not be included in \n# count. The min column is also not useful because christams will give \n# close to zero sales for all series since Walmart is closed on christmas.\n# Finally, I would like to add the relevant percentiles that will be \n# used in the uncertainty competition. \n\n################### Leading zeros ######################\n\n# We would like to set all leading zeros to np.nan so \n# they won't be counted in by the .describe() method. \n# To do this we need a mask for series_df that only shows\n# the leading zeros. If we compare series_df \n# values to cumulative sum values multiplied by any \n# number != 1 (2 chosen here), the values are only equal\n# if they are at a location of a leading zero. \nzero_mask = series_df.cumsum(axis=1) * 2 == series_df\n\n# Now set the leading zeros to np.nan\nseries_df[zero_mask] = np.nan\n\n\n################## Christmas closure ####################\n# First find all x where 'd_x' represents christmas. \nxmas_days = cal_df[cal_df.date.str[-5:] == '12-25'].d.str[2:].astype('int16')\n\n# I will choose to replace sales for every christmas with \n# the average of the day before and the day after. \nfor x in xmas_days: \n    series_df[f'd_{x}'] = (series_df[f'd_{x-1}'] + series_df[f'd_{x+1}']) / 2\n    \n    \n################ Percentiles ######################\n# These will be especially useful in the uncertainty competition. \npercentiles = [.005, .025, .165, .25, .5, .75, .835, .975, .995]\n\n############### Recreate stats_df #################\nstats_df = series_df.T.describe(percentiles).T\n\n################## fraction 0 #######################\n# We want to know what fraction of sales are zero \nstats_df['fraction_0'] = ((series_df == 0).sum(axis = 1) / stats_df['count'])\n\n############### Add weights ###################\nw_df = get_w_df(train_df, cal_df, prices_df, rollup_index, rollup_matrix_csr, start_test=1914)\nstats_df = pd.concat([stats_df, w_df], axis=1)\n\n############### Pickle files ##################\nstats_df.to_pickle('stats_df.pkl')\nseries_df.to_pickle('series_df.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"series_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"series_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## THIS WILL BE PUT IN THE HELPERS.PY FILE AND WILL BE UPDATED THERE, NOT HERE.\n\n####################################### Module ##########################################\n#########################################################################################\nimport pandas as pd\n################### series_df function #####################\ndef get_series_df(train_df, rollup_matrix_csr, rollup_index, cal_df):\n    \"\"\"Returns a dataframe with series for all 12 levels of aggregation. We also \n    replace leading zeros with np.nan and replace christmas sales with average \n    of the day before and day after christmas\"\"\"\n    \n    series_df = pd.DataFrame(data=rollup_matrix_csr * train_df.iloc[:, 6:].values,\n                         index=rollup_index, \n                         columns=train_df.iloc[:, 6:].columns)\n    \n    zero_mask = series_df.cumsum(axis=1) * 2 == series_df\n\n    # Now set the leading zeros to np.nan\n    series_df[zero_mask] = np.nan\n\n    ################## Christmas closure ####################\n    # First find all x where 'd_x' represents christmas. \n    xmas_days = cal_df[cal_df.date.str[-5:] == '12-25'].d.str[2:].astype('int16')\n\n    # I will choose to replace sales for every christmas with \n    # the average of the day before and the day after. \n    for x in xmas_days: \n        series_df[f'd_{x}'] = (series_df[f'd_{x-1}'] + series_df[f'd_{x+1}']) / 2\n    \n    return series_df \n\n\n\n################## stats_df function #######################\ndef get_stats_df(series_df, cal_df):\n    \"\"\"Returns a dataframe that shows basic stats for all \n    series in sereis_df.\"\"\"\n    \n    ################ Percentiles ######################\n    # These will be especially useful in the uncertainty competition. \n    percentiles = [.005, .025, .165, .25, .5, .75, .835, .975, .995]\n\n\n    ############# Create stats_df ########################\n    stats_df = series_df.T.describe(percentiles).T\n\n    ################## fraction 0 #######################\n    # We want to know what fraction of sales are zero \n    stats_df['fraction_0'] = ((series_df == 0).sum(axis = 1) / stats_df['count'])\n    \n    return stats_df","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}