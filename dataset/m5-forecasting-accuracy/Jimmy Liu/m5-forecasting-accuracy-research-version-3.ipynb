{"cells":[{"metadata":{},"cell_type":"markdown","source":"# M5 Forecasting Accuracy Research\n\nThis is a continuation of my work on analyzing the sales data of Walmart's TX_1 store (Version 1 found here:https://www.kaggle.com/jimmyliuu/m5-forecast-accuracy-research-version-1). This week, I add a couple exogenous variables into my SARIMA model in hopes of improving forecast accuracy. I chose to force weekly seasonality and event_name_1 into the model. \n\nI followed Jason Brownlee's \"How to Decompose Time Series Data into Trend and Seasonality\" to build the SARIMAX model with forced weekly seasonality (found here:https://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/)\n\nI followed Oscar Arzamendia's \"Time Series Forecasting - A Getting Started Guide\" as a guide to feature engineering. I used one-hot-encoding on event_name_1 to feed it into the SARIMAX model (found here:https://towardsdatascience.com/time-series-forecasting-a-getting-started-guide-c435f9fa2216#:~:text=An%20exogenous%20variable%20is%20one,without%20being%20affected%20by%20it.)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load in relevant datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"CalendarDF=pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/calendar.csv\", header=0)\nSalesDF=pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv\", header=0) #June 1st Dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os, psutil\n\npid = os.getpid()\npy = psutil.Process(pid)\nmemory_use = py.memory_info()[0] / 2. ** 30\nprint ('memory GB:' + str(np.round(memory_use, 2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"CalendarDF['date'] = pd.to_datetime(CalendarDF.date)\n\nTX_1_Sales = SalesDF[['TX_1' in x for x in SalesDF['store_id'].values]]\nTX_1_Sales = TX_1_Sales.reset_index(drop = True)\nTX_1_Sales.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate MultiIndex for easier aggregration.\nTX_1_Indexed = pd.DataFrame(TX_1_Sales.groupby(by = ['cat_id','dept_id','item_id']).sum())\nTX_1_Indexed.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aggregate total sales per day for each sales category\nFood = pd.DataFrame(TX_1_Indexed.xs('FOODS').sum(axis = 0))\nHobbies = pd.DataFrame(TX_1_Indexed.xs('HOBBIES').sum(axis = 0))\nHousehold = pd.DataFrame(TX_1_Indexed.xs('HOUSEHOLD').sum(axis = 0))\nFood.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge the aggregated sales data to the calendar dataframe based on date\nCalendarDF = CalendarDF.merge(Food, how = 'left', left_on = 'd', right_on = Food.index)\nCalendarDF = CalendarDF.rename(columns = {0:'Food'})\nCalendarDF = CalendarDF.merge(Hobbies, how = 'left', left_on = 'd', right_on = Hobbies.index)\nCalendarDF = CalendarDF.rename(columns = {0:'Hobbies'})\nCalendarDF = CalendarDF.merge(Household, how = 'left', left_on = 'd', right_on = Household.index)\nCalendarDF = CalendarDF.rename(columns = {0:'Household'})\nCalendarDF.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop dates with null sales data\nCalendarDF = CalendarDF.drop(CalendarDF.index[1941:])\nCalendarDF.reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, I perform a couple of correlation tests between each of the sales categories (food, hobbies, and household).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Collect sales data from each category into one dataframe\ncategoriesDF = CalendarDF[['Food','Hobbies','Household']]\ncategoriesDF.corr(method = 'pearson')\ncategoriesDF.corr(method = 'spearman')\ncategoriesDF.corr(method = 'kendall')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building the SARIMAX model with forced weekly seasonality\n\nReference: https://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/ Section 15","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\n\nFood.index = CalendarDF['date']\n\n# Split food sales data into train and test \nfoodTrain = Food['20110129':'20160410']\nfoodTest = Food['20160411':'20160522']\n\n# Drop 0 sales values to prepare data for multiplicative seasonal decomposition\nfoodTrain = foodTrain[foodTrain[foodTrain.columns[0]] !=0]\n\n# Seasonal decomposition\nresult = seasonal_decompose(foodTrain, model = 'multiplicative', extrapolate_trend = 'freq', freq = 7) # frequency set to weekly\n\n# Store seasonality component of decomposition\nseasonal = result.seasonal.to_frame()\nseasonal_index = result.seasonal[-7:].to_frame()\n\n# Merge the train data and the seasonality \nfoodTrain = foodTrain.merge(seasonal, how = 'left', on = foodTrain.index , left_index = True, right_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building the SARIMAX model\n# I use the Pyramid Arima package to perform an auto-SARIMAX forecast\n\n!pip install pmdarima\nimport pmdarima as pm\n\n#SARIMAX Model setting the exogenous variable to weekly seasonality \nsxmodel = pm.auto_arima(foodTrain[foodTrain.columns[0]], exogenous= foodTrain[['seasonal']],\n                           start_p=1, start_q=1,\n                           test='adf',\n                           max_p=3, max_q=3, m=7,\n                           start_P=0, seasonal=True,\n                           d=None, D=1, trace=True,\n                           error_action='ignore',  \n                           suppress_warnings=True, \n                           stepwise=True)\n\nsxmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Forecasting using the SARIMAX model\nimport matplotlib.pyplot as plt\n\nn_periods = 42\nfitted, confint = sxmodel.predict(n_periods = n_periods,  exogenous= np.tile(seasonal_index['seasonal'], 6).reshape(-1,1),  return_conf_int = True)\n\nindex_of_fc = pd.date_range(foodTest.index[0], periods = n_periods, freq = 'D')\n\n# make series for plotting purpose\nfitted_series = pd.Series(fitted, index=index_of_fc)\nlower_series = pd.Series(confint[:, 0], index=index_of_fc)\nupper_series = pd.Series(confint[:, 1], index=index_of_fc)\n\n# Plot\nplt.plot(foodTest)\nplt.plot(fitted_series, color='darkgreen')\nplt.fill_between(lower_series.index, \n                 lower_series, \n                 upper_series, \n                 color='k', alpha=.15)\n\nplt.title(\"SARIMA - Total Sales of TX_1\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building the SARIMAX model using event_name_1 as an exogenous variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# data engineering for event_name_1\nCalendarDF['isweekday'] = [1 if wday >= 3 else 0 for wday in CalendarDF.wday.values]\nCalendarDF['isweekend'] = [0 if wday > 2 else 1 for wday in CalendarDF.wday.values]\nCalendarDF['holiday_weekend'] = [1 if (we == 1 and h not in [np.nan]) else 0 for we,h in CalendarDF[['isweekend','event_name_1']].values]\nCalendarDF['holiday_weekday'] = [1 if (wd == 1 and h not in [np.nan]) else 0 for wd,h in CalendarDF[['isweekday','event_name_1']].values]\n\n# one-hot-encoding event_name_1\nCalendarDF = pd.get_dummies(CalendarDF, columns=['event_name_1'], prefix=['holiday'], dummy_na=True)\n\nFood = CalendarDF['Food']\nFood.index = CalendarDF['date']\n\n# Section out the columns created by encoding and concat with Food dataframe\ntemp = CalendarDF.iloc[:,16:50]\ntemp.index = CalendarDF['date']\nFood = pd.concat([Food, temp], axis = 1)\n\nfoodTrain = Food['20110129':'20160410']\nfoodTest = Food['20160411':'20160522']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the SARIMAX model\nsxmodel_event = pm.auto_arima(foodTrain[foodTrain.columns[0]], exogenous= foodTrain.iloc[:,1:],\n                           start_p=1, start_q=1,\n                           test='adf',\n                           max_p=3, max_q=3, m=7,\n                           start_P=0, seasonal=True,\n                           d=None, D=1, trace=True,\n                           error_action='ignore',  \n                           suppress_warnings=True, \n                           stepwise=True)\n\nsxmodel_event.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Forecast\nn_periods = 42\nevent_predict, confint = sxmodel_event.predict(n_periods = n_periods,  exogenous= foodTest.iloc[:,1:],  return_conf_int = True)\n\nindex_of_fc = pd.date_range(foodTest.index[0], periods = n_periods, freq = 'D')\n\n# make series for plotting purpose\nfitted_series = pd.Series(event_predict, index=index_of_fc)\nlower_series = pd.Series(confint[:, 0], index=index_of_fc)\nupper_series = pd.Series(confint[:, 1], index=index_of_fc)\n\n# Plot\n#plt.plot(foodTrain)\nplt.plot(foodTest)\nplt.plot(fitted_series, color='darkgreen')\nplt.fill_between(lower_series.index, \n                 lower_series, \n                 upper_series, \n                 color='k', alpha=.15)\n\nplt.title(\"SARIMA - Total Sales of TX_1\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparing Results\n\nI will now compare the forecasting results from the ARIMA model, the SARIMA model, and the two SARIMAX models using the sMAPE and MASE functions. (reference: https://gist.github.com/bshishov/5dc237f59f019b26145648e2124ca1c9)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Accuracy metrics\ndef symmetric_mean_absolute_percentage_error(actual,forecast):\n    return 1/len(actual) * np.sum(2 * np.abs(forecast-actual)/(np.abs(actual)+np.abs(forecast)))\n\ndef mean_absolute_error(actual, forecast):\n    return np.mean(np.abs(actual - forecast))\n\ndef naive_forecasting(actual, seasonality):\n    return actual[:-seasonality]\n\ndef mean_absolute_scaled_error(actual, forecast, seasonality):\n    return mean_absolute_error(actual, forecast) / mean_absolute_error(actual[seasonality:], naive_forecasting(actual, seasonality))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"symmetric_mean_absolute_percentage_error(foodTest[foodTest.columns[0]], fitted) #sMAPE of SARIMAX with forced seasonality","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"symmetric_mean_absolute_percentage_error(foodTest[foodTest.columns[0]], event_predict) #sMAPE of SARIMAX with event_name_1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both SARIMAX models are slightly more accurate than the ARIMA model based on sMAPE.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}