{"cells":[{"metadata":{},"cell_type":"markdown","source":"# This kernel is a summary of the descriptions on the M5 Guidelines paper.\n- All information is contained in the link below.\n- Link : https://mofc.unic.ac.cy/m5-competition/"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport plotnine \n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n## Dataset"},{"metadata":{},"cell_type":"markdown","source":"- calendar.csv - Contains information about the dates on which the products are sold.\n- sales_train_validation.csv - Contains the historical daily unit sales data per product and store [d_1 - d_1913]\n- sample_submission.csv - The correct format for submissions. Reference the Evaluation tab for more info.\n- sell_prices.csv - Contains information about the price of the products sold per store and date.\n- sales_train_evaluation.csv - **Available once month before competition deadline**. Will include sales [d_1 - d_1941]"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv\")\nsell = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sell_prices.csv\")\ncalendar = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/calendar.csv\")\nsub = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![image.png](https://github.com/choco9966/Kaggle/blob/master/M5%20Forecasting/image/overview.PNG?raw=true)\n\n![image.png](https://github.com/choco9966/Kaggle/blob/master/M5%20Forecasting/image/aggtable.PNG?raw=true)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Unit sales of all products, aggregated for each state\", train['state_id'].nunique())\nprint(\"Unit sales of all products, aggregated for each store\", train['store_id'].nunique())\nprint(\"Unit sales of all products, aggregated for each category\", train['cat_id'].nunique())\nprint(\"Unit sales of all products, aggregated for each department\", train['dept_id'].nunique())\nprint(\"Unit sales of all products, aggregated for each State and category\", train['state_id'].nunique() * train['cat_id'].nunique())\nprint(\"Unit sales of all products, aggregated for each State and department\", train['state_id'].nunique() * train['dept_id'].nunique())\nprint(\"Unit sales of all products, aggregated for each store and category\", train['store_id'].nunique() * train['cat_id'].nunique())\nprint(\"Unit sales of all products, aggregated for each store and department\", train['store_id'].nunique() * train['dept_id'].nunique())\nprint(\"Unit sales of all products, aggregated for each  and category\", train['dept_id'].nunique() * train['cat_id'].nunique())\nprint(\"Unit sales of product x, aggregated for all stores/states\", train['item_id'].nunique())\nprint(\"Unit sales of product x, aggregated for all states\", train['item_id'].nunique() * train['state_id'].nunique())\nprint(\"Unit sales of product x, aggregated for all stores\", train['item_id'].nunique() * train['store_id'].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### File 1: \"calendar.csv\"\nContains information about the dates the products are sold.\n- date: The date in a ‚Äúy-m-d‚Äù format.\n- wm_yr_wk: The id of the week the date belongs to.\n- weekday: The type of the day (Saturday, Sunday, ‚Ä¶, Friday).\n- wday: The id of the weekday, starting from Saturday.\n- month: The month of the date.\n- year: The year of the date.\n- event_name_1: If the date includes an event, the name of this event.\n- event_type_1: If the date includes an event, the type of this event.\n- event_name_2: If the date includes a second event, the name of this event.\n- event_type_2: If the date includes a second event, the type of this event.\n- snap_CA, snap_TX, and snap_WI: A binary variable (0 or 1) indicating whether the stores of CA, TX or WI allow SNAP2 purchases on the examined date. 1 indicates that SNAP purchases are allowed."},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.head(8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Event information "},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar[calendar['event_name_1'].notnull()].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from plotnine import *\nimport plotnine","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agg = calendar.groupby('event_name_1')['event_name_1'].agg({'count'}).reset_index()\n(ggplot(data = agg) \n  + geom_bar(aes(x='event_name_1', y='count'), fill='#49beb7', color='black', stat='identity')\n  + scale_color_hue(l=0.45)\n  + theme_light() \n  + theme(\n         axis_text_x = element_text(angle=80),\n         figure_size=(12,8),\n         legend_position=\"none\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agg = calendar.groupby('event_type_1')['event_type_1'].agg({'count'}).reset_index()\n(ggplot(data = agg) \n  + geom_bar(aes(x='event_type_1', y='count'), fill='#49beb7', color='black', stat='identity')\n  + scale_color_hue(l=0.45)\n  + theme_light() \n  + theme(\n         axis_text_x = element_text(angle=80),\n         figure_size=(12,8),\n         legend_position=\"none\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar[calendar['event_name_2'].notnull()].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"event_name_2 notnull shape : \", calendar[calendar['event_name_2'].notnull()].shape)\nprint(\"event_name_1 and 2 notnull shape : \", calendar[(calendar['event_name_2'].notnull()) & (calendar['event_name_1'].notnull())].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agg = calendar.groupby('event_name_2')['event_name_2'].agg({'count'}).reset_index()\n(ggplot(data = agg) \n  + geom_bar(aes(x='event_name_2', y='count'), fill='#49beb7', color='black', stat='identity')\n  + scale_color_hue(l=0.45)\n  + theme_light() \n  + theme(\n         axis_text_x = element_text(angle=80),\n         figure_size=(12,8),\n         legend_position=\"none\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agg = calendar.groupby('event_type_2')['event_type_2'].agg({'count'}).reset_index()\n(ggplot(data = agg) \n  + geom_bar(aes(x='event_type_2', y='count'), fill='#49beb7', color='black', stat='identity')\n  + scale_color_hue(l=0.45)\n  + theme_light() \n  + theme(\n         axis_text_x = element_text(angle=80),\n         figure_size=(12,8),\n         legend_position=\"none\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### File 2: \"sell_prices.csv\"\nContains information about the price of the products sold per store and date.\n- store_id: The id of the store where the product is sold.\n- item_id: The id of the product.\n- wm_yr_wk: The id of the week.\n- sell_price: The price of the product for the given week/store. The price is provided per week (average across seven days). If not available, this means that the product was not sold during the examined week. Note that although prices are constant at weekly basis, they may change through time (both training and test set). "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sell.shape)\nsell.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### File 3: ‚Äúsales_train.csv‚Äù\n\nContains the historical daily unit sales data per product and store.\n- item_id: The id of the product.\n- dept_id: The id of the department the product belongs to.\n- cat_id: The id of the category the product belongs to.\n- store_id: The id of the store where the product is sold.\n- state_id: The State where the store is located.\n- d_1, d_2, ‚Ä¶, d_i, ‚Ä¶ d_1941: The number of units sold at day i, starting from 2011-01-29."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submission File\n\nEach row contains an id that is a concatenation of an item_id, a store_id, and the prediction interval, which is either validation (corresponding to the Public leaderboard), or evaluation (corresponding to the Private leaderboard). You are predicting 28 forecast days (F1-F28) of items sold for each row. For the validation rows, this corresponds to d_1914 - d_1941, and for the evaluation rows, this corresponds to d_1942 - d_1969. (Note: a month before the competition close, the ground truth for the validation rows will be provided.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation\nThis competition uses a Weighted Root Mean Squared Scaled Error (RMSSE). Extensive details about the metric, scaling, and weighting can be found in the [M5 Participants Guide.](https://mofc.unic.ac.cy/m5-competition/)\n\n1. Forecasting horizon\n\nThe number of forecasts required, both for point and probabilistic forecasts, is h=28 days(4 weeks ahead).\nThe performance measures are first computed for each series separately by averaging their values across\nthe forecasting horizon and then averaged again across the series in a weighted fashion (see below) to\nobtain the final scores. \n\n2. Point forecasts\n\nThe accuracy of the point forecasts will be evaluated using the Root Mean Squared Scaled Error (RMSSE),\nwhich is a variant of the well-known Mean Absolute Scaled Error (MASE) proposed by Hyndman and\nKoehler (2006)\n. The measure is calculated for each series as follows:\n\n![image.png](https://github.com/choco9966/Kaggle/blob/master/M5%20Forecasting/image/rmsse.PNG?raw=true)\n\nwhere ùëåùë° is the actual future value of the examined time series at point t, ùëåùë°_hat the generated forecast, n the length of the training sample (number of historical observations), and h the forecasting horizon. \n\nAfter estimating the RMSSE for all the 42,840 time series of the competition, the participating methods will be ranked using the Weighted RMSSE (WRMSSE), as described latter in this Guide, using the following\n\n![image.png](https://github.com/choco9966/Kaggle/blob/master/M5%20Forecasting/image/wrmsse.PNG?raw=true)\n\nwhere ùë§ùëñ is the weight of the ùëñùë°‚Ñé series of the competition. A lower WRMSSE score is better\n\nNote that the weight of each series will be computed based on the last 28 observations of the training sample of the dataset, i.e., the cumulative actual dollar sales that each series displayed in that particular period (sum of units sold multiplied by their respective price). An indicative example for computing the WRMSSE will be available on the [GitHub repository](https://github.com/Mcompetitions) of the competition"},{"metadata":{},"cell_type":"markdown","source":"## Before competitions\n- M4 : https://github.com/Mcompetitions/M4-methods\n- Discussion by RDizzl3 : https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133469"},{"metadata":{},"cell_type":"markdown","source":"## Benchmarks models\ncode : https://github.com/Mcompetitions/M4-methods\n\n- Naive\n- Seaonal Naive\n- Simple Exponential Smoothing \n- Moving Averages \n- Croston‚Äôs method\n- Optimized Croston‚Äôs method\n- Syntetos-Boylan Approximation\n- Teunter-Syntetos-Babai method\n- Aggregate-Disaggregate Intermittent Demand Approach\n- Intermittent Multiple Aggregation Prediction Algorithm\n- Exponential Smoothing\n- Exponential Smoothing with eXplanatory variables \n- AutoRegressive Integrated Moving Average\n- AutoRegressive Integrated Moving Average with eXplanatory variables \n- Multi-Layer Perceptron\n- Random Forest\n- Global Multi-Layer Perceptron\n- Global Random Forest (GRF)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}