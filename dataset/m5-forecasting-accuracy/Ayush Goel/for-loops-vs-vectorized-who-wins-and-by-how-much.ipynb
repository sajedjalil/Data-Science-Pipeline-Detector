{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nSo we all have heard of matrices and vectors, and we use numpy all the time. But what is the importance of it. Why can't we just use a for-loop instead of using the numpy syntax, np.(function_name here). The difference is in performance.\n\nIn this notebook, I will be running only dot products and element wise multiplication. Feel free to fork the notebook and try different input sizes, or different commonly used operations.\n\nVectorized implementations (numpy) are much faster and more efficient as compared to for-loops. To really see HOW large the difference is, let's try some simple operations used in most machine learnign algorithms (especially deep learning).","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        pass\n        \nprint(\"Finished Importing Libraries\")\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, first let's create 2 new vectors each with 1 million rows, all with random numbers.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"v1 = np.random.rand(1000000, 1)\nv2 = np.random.rand(1000000, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multiplication by a Scalar","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Scaling a vector by a constant is important in processes like normalization. Quite often, there are for-loop implementations used, which take unnecessary amounts of time.\n\nLet's look at the difference.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling Vector - For loop\nstart = time.process_time()\nv1_scaled = np.zeros((1000000, 1))\n\nfor i in range(len(v1)):\n    v1_scaled[i] = 2 * v1[i]\n\nend = time.process_time()\n    \nprint(\"Scaling vector Answer = \" + str(v1_scaled))\nprint(\"Time taken = \" + str(1000*(end - start)) + \" ms\")  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scaling Vector - Vectorized\nstart = time.process_time()\nv1_scaled = np.zeros((1000000, 1))\n\nv1_scaled = 2 * v1\n\nend = time.process_time()\n    \nprint(\"Scaling vector Answer = \" + str(v1_scaled))\nprint(\"Time taken = \" + str(1000*(end - start)) + \" ms\")  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, vectorization is almost 300 times faster than for-loops. That is impressive.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Dot Products","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we are going to perform the dot product of the two vectors. This is the formula used to multiply the different values of the features of a training example and the weights that the model has learned. In deep learning, this process is repeated lots and lots of times. Even simple algorithms like Linear Regression make use of this a lot.\n\nIf you are not familiar with the formula, it is basically multiplying the two vectors element wise and then summing all of the elements. Here is the for loop implementation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dot product For loop\nstart = time.process_time()\nproduct = 0\n\nfor i in range(len(v1)):\n    product += v1[i] * v2[i]\n\nend = time.process_time()\n\nprint(\"Dot product Answer = \" + str(product))\nprint(\"Time taken = \" + str(1000*(end - start)) + \" ms\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the vectorized implementation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dot product Vectorized\nstart = time.process_time()\nproduct = 0\n\nproduct = np.dot(v1.T, v2)\n\nend = time.process_time()\n\nprint(\"Dot product Answer = \" + str(product))\nprint(\"Time taken = \" + str(1000*(end - start)) + \" ms\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, vectorized implementation is roughtly 600 times faster! (It may change slightly, but overall it should roughly be the same).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Element Wise multiplication","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Another important operation is just multiplying the two vectors element wise. Let's see the difference between the for-loop and vectorized version for this.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Element wise mutliplication For loop\nstart = time.process_time()\n\nanswer = np.zeros((1000000, 1))\n\nfor i in range(len(v1)):\n    answer[i] = v1[i] * v2[i]\n    \nend = time.process_time()\n\nprint(\"Element Wise answer = \" + str(answer))\nprint(\"Time Taken = \" + str(1000*(end - start)) + \" ms\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Element wise multiplication Vectorized\nstart = time.process_time()\n\nanswer = np.zeros((1000000, 1))\n\nanswer = v1 * v2\n\nend = time.process_time()\n\nprint(\"Element Wise answer = \" + str(answer))\nprint(\"Time Taken = \" + str(1000*(end - start)) + \" ms\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, vectorized implementation is almost 500 times faster! \nHuge boost in performance.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Element Wise Matrix Multiplication\n\nNow let's investigate element wise matrix multiplication. Since this will have a complexity of O(n2), I will use smaller matrix sizes (just enough to see a good difference, but not too large).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Element wise matrix multiplication For loop\n\nm1 = np.random.rand(1000, 1000)\nm2 = np.random.rand(1000, 1000)\nanswer = np.zeros((1000, 1000))\n\nstart = time.process_time()\n\nfor i in range(m1.shape[0]):\n    for j in range(m1.shape[1]):\n        answer[i, j] = m1[i, j] * m2[i, j]\n    \nend = time.process_time()\n\nprint(\"Element Wise Matrix answer = \" + str(answer))\nprint(\"Time Taken = \" + str(1000*(end - start)) + \" ms\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Element wise matrix multiplication Vectorized\nanswer = np.zeros((1000, 1000))\n\nstart = time.process_time()\n\nanswer = np.multiply(m1, m2)\n\nend = time.process_time()\n\nprint(\"Element Wise Matrix answer = \" + str(answer))\nprint(\"Time Taken = \" + str(1000*(end - start)) + \" ms\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, Numpy is almost 370 times faster than for-loops. This is going to save lots of time. Imagine having hundreds of features and millions of rows. Using a for-loop would unnecessariy kill your computer.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Time-complexity Plot\n\nNow let's try and plot the performance over different sizes of input. This way, we can actually compare how each method's time complexity grows.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sizes = [10, 100, 1000, 10000, 100000, 1000000, 10000000]\ncomplexity = pd.DataFrame(columns=['sizes', 'for_loop', 'numpy'])\ncomplexity['sizes'] = sizes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will be using a for-loop to iterate through all of the input sizes (excuse me).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for_loops = []\nnumpy = []\n\nfor size in sizes:\n    v1 = np.random.rand(size, 1)\n    v2 = np.random.rand(size, 1)\n    \n    #For loop implementation\n    start = time.process_time()\n    product = 0\n\n    for i in range(len(v1)):\n        product += v1[i] * v2[i]\n\n    end = time.process_time()\n    \n    for_loops.append(1000*(end-start))\n    \n    #Vectorized implementation\n    \n    start = time.process_time()\n    product = 0\n\n    product = np.dot(v1.T, v2)\n\n    end = time.process_time()\n    numpy.append(1000*(end - start))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"complexity['for_loops'] = for_loops\ncomplexity['numpy'] = numpy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(complexity['sizes'], complexity['for_loops'])\nplt.plot(complexity['sizes'], complexity['numpy'])\n\nplt.xscale(value='log')\nplt.xlabel(\"Size of input\")\nplt.ylabel(\"Time taken in ms\")\nplt.legend(['for loop', 'numpy'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My God!! Look at the way the for-loop implementation grows and compare that with numpy. Numpy is almost flat, but for loop grows drastically (by the way, the graph actually grows linearly, I have just made the x-axis logarithmic as it makes reading the plot a lot easier.\n\n\n# Summary\n\nIf you are curious, for-loop looks like it is following the time-complexity of O(N). Numpy however, is basically following a complexity of O(1). \n\nMost real-world datasets used for ML/AI use millions or billions of rows. As you can see from the plot, using vectorized implementations can save a lot of time.\n\n\n\nSo I just have 1 final request: Please use vectorized implementations using numpy or pandas whenever you can. Try to avoid using for-loops, it will save you a lot of time (and money if you use servers).\n\n\nI hoped you like this simple analysis of why you should try using numpy (make friends with the library. It will reward you).","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}