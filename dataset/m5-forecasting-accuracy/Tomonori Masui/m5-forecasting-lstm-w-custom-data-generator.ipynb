{"cells":[{"metadata":{},"cell_type":"markdown","source":"# M5 Forecasting - LSTM w/ Custom Generator\n\nThis notebook shows LSTM training/prediction with a custom data generator for Keras LSTM model. The model uses sequences of sales and prices of {w_size} days with categorical features being used with embeddings to predict next one day sales on each item.\nFor the submission, it makes prediction with 28 days loop where each one day prediction is used for an input for the next days' prediction in the loop. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns; sns.set()\nimport gc\n\nfrom sklearn.model_selection import GroupKFold, KFold, TimeSeriesSplit\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM, GRU\nfrom tensorflow.keras.layers import Input, Flatten, Concatenate, BatchNormalization, Embedding\nfrom tensorflow.keras.losses import mse\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Configuration"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"DATA_DIR = '/kaggle/input/m5-forecasting-accuracy/'\n\nDEBUG = False # turning on/off degugging mode\nCV = False # turning on/off cross validation\n\nif DEBUG:\n    rows = 100\n    w_size = 15\n    batch_size=32\n    epochs = 3\n    span_lst = [7]\nelse:\n    rows = None\n    w_size = 30 # LSTM window size\n    batch_size=512\n    epochs = 35\n    span_lst = [7, 30, 90] # moving avarage time wiondows","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training Dataset\n\n### Sales Dataset\n* Transposing (items, days) dimension into (days, items)\n* Only last 360 days are used for now to save run time"},{"metadata":{"trusted":true},"cell_type":"code","source":"d_dtypes = {}\nfor i in range(1914):\n    d_dtypes[f'd_{i}'] = np.int32\n    \nsales = pd.read_csv(DATA_DIR + 'sales_train_validation.csv',\n                    dtype=d_dtypes, nrows=rows)\n\n# categories are used for categorical model input\ncategories = sales[['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']]\nsales['id'] = sales['id'].apply(lambda x: x[:-11])\nids = sales['id'].values\n\nif DEBUG:\n    sales = sales.iloc[:, -150:].T.reset_index()\nelse:\n    # Only last 360 days are used to save run time\n    sales = sales.iloc[:, -360:].T.reset_index()\n    \nsales.columns = ['d'] + list(ids)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calendar Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar = pd.read_csv(DATA_DIR + 'calendar.csv',\n                       dtype={'wm_yr_wk': np.int32, 'wday': np.int32, \n                              'month': np.int32, 'year': np.int32, \n                              'snap_CA': np.int32, 'snap_TX': np.int32,\n                              'snap_WI': np.int32})\n\n# subsetting by starting date in sales\ncalendar = calendar[calendar.d.apply(lambda x: int(x[2:])) >= int(sales.d[0][2:])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Price Dataset\n* Transposing the long format into (weeks, items) dimension\n* Then it is merged to calendar data to make it daily format"},{"metadata":{"trusted":true},"cell_type":"code","source":"prices = pd.read_csv(DATA_DIR + 'sell_prices.csv',\n                          dtype={'wm_yr_wk': np.int32, \n                                 'sell_price': np.float32})\nprices = prices.loc[prices.wm_yr_wk >= \\\n                    calendar[calendar.d == sales.d[0]]['wm_yr_wk'].values[0]]\n\nprices['id'] = prices.apply(lambda x: x.item_id + '_' + x.store_id, axis=1)\nprices = prices.pivot(index='wm_yr_wk', columns='id', values='sell_price')\n\nprices = calendar[['d','wm_yr_wk']].merge(prices, how='inner', on=['wm_yr_wk'])\nprices.drop('wm_yr_wk', axis=1, inplace=True)\nprices = prices.loc[:, list(sales.columns)]\n\ncalendar.drop(['date','wm_yr_wk', 'weekday', 'd'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing\n\nBoth sales and prices are log scaled and then standardized by global average and std."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_log = np.log(sales.iloc[:, 1:].values + 1)\nsales_mean = np.mean(sales_log)\nsales_std = np.std(sales_log)\nsales.iloc[:, 1:] = (sales_log - sales_mean) / sales_std\n\nprices_log = np.log(prices.iloc[:, 1:].values)\nprices_mean = np.mean(prices_log)\nprices_std = np.std(prices_log)\nprices.iloc[:, 1:] = (prices_log - prices_mean) / prices_std\n\nsales.fillna(0, inplace=True)\nprices.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Categorical features are label encoded to be used for embeddings."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_ft1 = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\ncat_ft2 = ['wday','month', 'year', 'event_name_1', 'event_type_1',\n           'event_name_2', 'event_type_2']\n\ncategory_counts = {}\nstate_le = None\n\ndef LabelEncoding(df, cat_ft):\n    \n    for col in cat_ft:\n        le = LabelEncoder()\n        df.loc[:, col] = df[col].astype(str)\n        df.loc[:, col] = le.fit_transform(df[col])\n        category_counts[col] = len(list(le.classes_))\n\n    return df\n\ncategories = LabelEncoding(categories, cat_ft1)\ncalendar = LabelEncoding(calendar, cat_ft2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining SequenceGenerator\n\nCustom data generator is used to create input sequences and scalers during the model training.\n\n* Sales:\n  \n  Raw sales values along with its moving averages (7days, 30days, 90days) are used for model input sequences.\n  \n  input shape: (days, items) -> output shape: (items, days, features)\n  \n* Prices:\n  \n  Raw price sequence values are used.\n  \n  input shape: (days, items) -> output shape: (items, days, feature)\n  \n* Categories:\n  \n  item_id, dept_id, cat_id, store_id, state_id are used as a single values for each row. These are fed into embedding layer.\n  \n  input shape: (items, features) -> output shape: (items, 1) * features\n  \n* Calendar:\n\n  wday, month, year, event_name_1, event_type_1, event_name_2, event_type_2 are used as a single values for each row. These are going to embedding layer. Snap indicator corresponding to the item's state are also fetched and it will be directly put into dense layer.\n  \n  input shape: (days, features) -> output shape: (items, 1) * features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def moving_average(a, n):\n    \n    if a.shape[0] >= n:\n        ret = np.cumsum(a, axis=0)\n        ret[n:, :] = ret[n:, :] - ret[:-n, :]\n        ret[:n-1, :] = np.zeros((n-1, ret.shape[1]))\n        return ret / n\n    else:\n        return np.zeros((a.shape[0], a.shape[1]))\n    \nclass SequenceGenerator:\n    \n    def __init__(self, inputs, spans=[7], window=30, batch_size=32, infer=False):\n        self.sales = inputs[0]\n        self.prices = inputs[1]\n        self.categories = inputs[2]\n        self.calendar = inputs[3]\n        self.spans = spans\n        self.window = window\n        self.infer = infer\n        self.num_items = self.sales.shape[1]\n        \n        if self.infer:\n            self.batch_size = self.num_items\n            self.num_days = self.sales.shape[0] - self.window + 1\n            self.steps_per_day = 1 \n            self.steps = 1\n        else:\n            self.batch_size = batch_size\n            self.num_days = self.sales.shape[0] - self.window\n            self.steps_per_day = self.num_items // self.batch_size + 1\n            self.steps = self.steps_per_day * self.num_days\n\n    def generate(self):\n        \n        ## for inference, it starts from the the last starting date (no slides in days)\n        ## for training/validation, it starts from day 0\n        start_day = self.num_days - 1 if self.infer else 0\n            \n        while True:            \n            \n            for day in range(start_day, self.num_days):\n                    \n                s = self.sales[day:day+self.window, :].reshape(1, self.window, -1)\n                p = self.prices[day:day+self.window, :]\\\n                    .reshape(1, self.window, -1)\n\n                X = np.concatenate((s,p),axis=0)\n\n                for span in self.spans:\n                    \n                    span_ = day if day < span else span\n\n                    ma = moving_average(self.sales[day-span_:day+self.window, :]\n                                        , n=span)[span_:, :]\\\n                        .reshape(1, self.window, -1)\n\n                    X = np.concatenate((X, ma),axis=0)\n\n                ## transposing (features, days, items) into (items, days, features)\n                X = np.transpose(X, (2,1,0)) \n\n                if not self.infer:\n                    y = self.sales[day+self.window, :].reshape(-1,1) \n                    \n                for i in range(self.steps_per_day):\n                    \n                    ## if the batch go over the maxium item number, \n                    ## the batch_size will be truncated\n                    if (i+1)*self.batch_size > self.num_items:\n                        end = self.num_items\n                    else:\n                        end = (i+1)*self.batch_size\n                    \n                    ## categories has (items, features) shape\n                    ## only relevant item rows are fetched\n                    cat = self.categories[i*self.batch_size:end, :]\n                    state_id = cat[:, -1]\n                    # reshaping into (features, items, 1)\n                    cat = cat.T.reshape(cat.shape[1], cat.shape[0], 1)\n                    \n                    ## calender values are taken at prediction target date\n                    calen = self.calendar[day+self.window,:7].reshape(1,-1)\n                    calen = np.repeat(calen, end-i*self.batch_size, axis=0)\n                    calen = calen.T.reshape(calen.shape[1], calen.shape[0], 1)\n                    \n                    ## snap values are taken at prediction target date\n                    snap = self.calendar[day+self.window,7:].reshape(1,-1)\n                    snap = np.repeat(snap, end-i*self.batch_size, axis=0)\n                    # taking only relevant state's snap values for each row\n                    snap = snap[np.arange(len(snap)), state_id].reshape(-1,1)\n                    \n                    if self.infer:\n                        yield [X[i*self.batch_size: end]] + [j for j in cat]\\\n                                + [j for j in calen] + [snap]\n                    else:\n                        yield [X[i*self.batch_size: end]] + [j for j in cat] \\\n                              + [j for j in calen] + [snap],\\\n                              y[i*self.batch_size: end]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining Model\n\n* Sales and price sequences are put into LSTM\n* Categorical features are going into Embedding layer\n* Snap indicator is put directly into Dense layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"def define_model(lstm_w_size, lstm_n_fts):\n    \n    ## Categorical embedding\n    cat_inputs = []\n    for cat in cat_ft1+cat_ft2:\n        cat_inputs.append(Input(shape=[1], name=cat))\n        \n    cat_embeddings = []\n    for i, cat in enumerate(cat_ft1+cat_ft2):\n        cat_embeddings.append(Embedding(category_counts[cat], \n                                        min(50, int(category_counts[cat]+1/ 2)), \n                                        name = cat + \"_embed\")(cat_inputs[i]))\n\n    cat_output = Concatenate()([Flatten()(cat_emb) \\\n                                          for cat_emb in cat_embeddings])\n    cat_output = Dropout(.7)(cat_output)\n    \n    # snap input\n    snap_input = Input(shape=[1])\n\n    ## LSTM\n    lstm_input = Input(shape=(lstm_w_size, lstm_n_fts))\n    lstm_output = CuDNNLSTM(32)(lstm_input)\n    \n    concat = Concatenate()([\n        lstm_output,\n        cat_output,\n        snap_input\n    ])\n        \n    dense_output = Dense(10, activation='relu')(concat)\n    out = Dense(1)(dense_output)\n    model = Model(inputs=[lstm_input] + cat_inputs + [snap_input],\n                  outputs=out)\n\n    model.compile(optimizer='adam', loss='mse')\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Training - Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_training(inputs, cv, w_size=30, batch_size=32, epochs=10,\n                   early_stopping=10, plt_iter=True):\n\n    val_scores=[]\n    train_evals=[]\n    valid_evals=[]\n    best_epoch=[]\n\n    for idx, (train_index, val_index) in enumerate(cv.split(inputs[0])):\n        \n        if idx >= 2: # skipping the first 2 fold to save run time\n\n            #print(\"###### fold %d ######\" % (idx+1))\n            sales_train, sales_val = inputs[0][train_index, :],\\\n                                     inputs[0][val_index, :]\n            prices_train, prices_val = inputs[1][train_index, :],\\\n                                       inputs[1][val_index, :]\n            calendar_train, calendar_val = inputs[3][train_index, :],\\\n                                           inputs[3][val_index, :]\n            inputs_train = [sales_train, prices_train, inputs[2], calendar_train]\n            inputs_val = [sales_val, prices_val, inputs[2], calendar_val]\n\n            train_gen = SequenceGenerator(inputs_train, spans=span_lst,\n                                          window=w_size, batch_size=batch_size)\n            val_gen = SequenceGenerator(inputs_val, spans=span_lst, window=w_size,\n                                        batch_size=batch_size)\n\n            model = define_model(w_size, 2+len(span_lst))\n            early_stop = EarlyStopping(patience=early_stopping,\n                                       verbose=True,\n                                       restore_best_weights=True)\n\n            hist = model.fit_generator(train_gen.generate(),\n                      validation_data=val_gen.generate(),\n                      epochs=epochs,\n                      steps_per_epoch=train_gen.steps, \n                      validation_steps=val_gen.steps, \n                      callbacks=[early_stop],\n                      verbose=0)\n\n            val_scores.append(np.min(hist.history['val_loss']))\n            train_evals.append(hist.history['loss'])\n            valid_evals.append(hist.history['val_loss'])\n\n            best_epoch.append(np.argmin(hist.history['val_loss']) + 1)\n    \n    print('### CV scores by fold ###')\n    for i in range(2, cv.get_n_splits(sales)):\n        print(f'fold {i+1}: {val_scores[i-2]:.4f} at epoch {best_epoch[i-2]}')\n    print('CV mean score: {0:.4f}, std: {1:.4f}'\\\n          .format(np.mean(val_scores), np.std(val_scores)))\n    \n    if plt_iter:\n        \n        fig, axs = plt.subplots(1, 2, figsize=(11,4))\n        \n        for i, ax in enumerate(axs.flatten()):\n            if i < cv.get_n_splits(sales):\n                ax.plot(train_evals[i], label='training')\n                ax.plot(valid_evals[i], label='validation')\n                ax.set(xlabel='epoch', ylabel='loss')\n                ax.set_title(f'fold {i+1+2}', fontsize=12)\n                ax.legend(loc='upper right', prop={'size': 9})\n                          \n        fig.tight_layout()\n        plt.show()\n\n    return best_epoch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n\nsales = sales.iloc[:,1:].values\nprices = prices.iloc[:,1:].values\ncategories = categories.values\ncalendar = calendar.values\ninputs = [sales, prices, categories, calendar]\n\nif CV:\n    cv = TimeSeriesSplit(n_splits=4)\n    best_epoch = model_training(inputs, cv, w_size=w_size, \n                                batch_size=batch_size, \n                                epochs=epochs, early_stopping=5, \n                                plt_iter=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Training without CV split\nThis model will be used to make predictions for the submission file"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntrain_gen = SequenceGenerator(inputs, spans=span_lst, window=w_size,\n                              batch_size=batch_size)\nmodel = define_model(w_size, 2+len(span_lst))\nhist = model.fit_generator(train_gen.generate(),\n                           epochs=best_epoch[-1] if CV else epochs,\n                           steps_per_epoch=train_gen.steps,\n                           verbose=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions for Submission\n* Predicting day by day by looping through 28 days\n* Each prediction will be used for the model input for the next day prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# subsetting len-w_size-90: as we need the first 90 days \n# prior to the LSTM window to calculate 90 days moving avarage\nsales_test = sales[sales.shape[0]-w_size-90:, :]\nprices_test = prices[sales.shape[0]-w_size-90:, :]\ncalendar_test = calendar[sales.shape[0]-w_size-90:, :]\ntest_inputs = [sales_test, prices_test, categories, calendar_test]\n\nfor i in range(28):\n    \n    test_gen = SequenceGenerator(test_inputs, spans=span_lst, \n                                 window=w_size, infer=True)\n    test_iter = test_gen.generate()\n    X = next(test_iter)\n    y_pred = model.predict(X)\n\n    # appending predicted sales to the input and shifting it by 1\n    sales_test = np.append(sales_test, y_pred.reshape(1,-1), axis=0)[1:, :]\n    prices_test = prices_test[1:, :]\n    calendar_test = calendar_test[1:, :]\n    test_inputs = [sales_test, prices_test, categories, calendar_test]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"scaling predicted values back into original values"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_test = np.exp((sales_test * sales_std) + sales_mean) - 1\nsales_test = np.maximum(sales_test, np.zeros(sales_test.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making Submission\nWe are only creating \"validation\" (corresponding to the Public leaderboard) submission. The submission for \"evaluation\" (corresponding to the Private leaderboard) is out-of-scope for now."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(DATA_DIR + 'sample_submission.csv', nrows=rows)\n\nif DEBUG:\n    submission.iloc[:100, 1:] = sales_test[-28:, :].T\nelse:\n    submission.iloc[:len(submission)//2, 1:] = sales_test[-28:, :].T\n    \nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}