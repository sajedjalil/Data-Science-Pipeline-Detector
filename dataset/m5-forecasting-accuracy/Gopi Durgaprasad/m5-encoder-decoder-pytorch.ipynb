{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\nimport pandas as pd\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport joblib\nimport glob\nfrom tqdm import tqdm_notebook as tqdm\n\nimport torch\nimport cv2\nfrom PIL import Image\n\nimport torchvision.transforms as transforms\nimport torchvision.models as pretrained_models\nimport os\n#import pretrainedmodels\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport torchvision\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nfrom tqdm import trange\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics\nfrom sklearn import preprocessing\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## utils\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef find_start_end(data: np.ndarray):\n    \"\"\"\n    Calculates start and end of real demand data. Start is an index of first non-zero and\n    end index of non-zero\n\n    :param data: Time series, shape [n_items, n_days]\n    :return : start_idx, end_idx\n    \"\"\"\n\n    n_items = data.shape[0]\n    n_days = data.shape[1]\n    \n    start_idx = np.full(n_items, -1, dtype=np.int32)\n    end_idx = np.full(n_items, -1, dtype=np.int32)\n\n    for item in range(n_items):\n        # scan from start to the end\n        for day in range(n_days):\n            if not np.isnan(data[item, day]) and data[item, day] > 0:\n                start_idx[item] = day\n                break\n        # reverse scan, from end to start\n        for day in range(n_days - 1, -1, -1):\n            if not np.isnan(data[item, day]) and data[item, day] > 0:\n                end_idx[item] = day\n                break\n\n    return start_idx, end_idx\n\n\ndef read_x(df, start, end) -> pd.DataFrame:\n    \"\"\"\n    Gets source data from start to end data, Any data can be None\n    :param df -> dataframe\n    :param start -> start day\n    :param end -> end day\n\n    :returns -> df \n    \"\"\"\n    if start and end:\n        return df.loc[:, start:end]\n    elif end:\n        return df.loc[:, end]\n    else:\n        return df\n\n\ndef prepare_data(df , start, end, valid_threshold):\n    \"\"\"\n    Reads source data, calculates start and end of each series, drops bad series, calculates log1p(demand)\n    :param df\n    :param start: start date of effective time interval, can be None to start from beginning\n    :param end: end dae of effective time interval, can be None to return all data\n    :param valid_threshold: minimal ratio of series real length to entire (end-start) interval.Series dropped if\n    ratio is less then threshold\n    :return: tuple(log1p(series), series start, series end)\n    \"\"\"\n\n    df = read_x(df, start, end)\n    starts, ends = find_start_end(df.values)\n\n    # boolean mask for bad (too short) series\n    page_mask = (ends - starts) / df.shape[1] < valid_threshold\n\n    print(\"Masked %d pages from %d\" % (page_mask.sum(), len(df)))\n    inv_mask = ~page_mask\n    df = df[inv_mask]\n    \n    #return np.log1p(df), starts[inv_mask], ends[inv_mask]\n    return df, starts[inv_mask], ends[inv_mask]\n\ndef encode_id_features(df):\n    \"\"\"\n    Applies one-hot encoding to id features and normalises result\n    :param df: id features DataFrame (one column per features)\n    :return: dictionary feature_name: encoded_values.Encoded values is [n_ids, n_values] array\n    \"\"\"\n\n    df = df.set_index(\"id\")\n\n    def encode(column) -> pd.DataFrame:\n        one_hot = pd.get_dummies(df[column], drop_first=False)\n        return (one_hot - one_hot.mean()) / one_hot.std()\n    \n    return {str(column): encode(column) for column in df.columns}\n    \n\ndef encode_id_features(id_features):\n\n    \n    df = id_features.set_index(\"id\")\n    df[\"id\"] = df.index.values\n\n    #id\n    #id = pd.get_dummies(df[\"id\"], drop_first=False, prefix=\"id\")\n    #id = (id - id.mean()) / id.std()\n\n    #final_df = id\n\n    #item_id\n    item_id = pd.get_dummies(df[\"item_id\"], drop_first=False, prefix=\"item_id\")\n    item_id = (item_id - item_id.mean()) / item_id.std()\n\n    # dept_id\n    dept_id = pd.get_dummies(df[\"dept_id\"], drop_first=False, prefix=\"dept_id\")\n    dept_id = (dept_id - dept_id.mean()) / dept_id.std()\n\n    # cat_id\n    cat_id = pd.get_dummies(df[\"cat_id\"], drop_first=False, prefix=\"cat_id\")\n    cat_id = (cat_id - cat_id.mean()) / cat_id.std()\n\n    # store_id\n    store_id = pd.get_dummies(df[\"store_id\"], drop_first=False, prefix=\"store_id\")\n    store_id = (store_id - store_id.mean()) / store_id.std()\n\n    # state_id\n    state_id = pd.get_dummies(df[\"state_id\"], drop_first=False, prefix=\"state_id\")\n    state_id = (state_id - state_id.mean()) / state_id.std()\n\n\n    # encoded_id_df\n    final_df = pd.merge(item_id,dept_id, how=\"left\", left_index=True, right_index=True)\n    final_df = pd.merge(final_df,cat_id, how=\"left\", left_index=True, right_index=True)\n    final_df = pd.merge(final_df,store_id, how=\"left\", left_index=True, right_index=True)\n    final_df = pd.merge(final_df,state_id, how=\"left\", left_index=True, right_index=True)\n\n\n\n    print(f\"item_id : {item_id.shape} , dept_id : {dept_id.shape} , cat_id : {cat_id.shape} , store_id : {store_id.shape}, state_id : {state_id.shape}\")\n\n    print(f\"encoded_id_df : {final_df.shape}\")\n\n    return final_df\n\ndef normalize(values: np.ndarray):\n    return (values - values.mean()) / np.std(values)\n\ndef lag_indexes(begin, end):\n    \"\"\"\n    Calculates indexes for 3, 6, 9, 12 months backward lag for the given date range\n    :param begin: start of date range\n    :param end: end of date range\n    :return : List of 4 Series, one for each lag. For each Series, index is date in range(begin, end),value is an index\n    of target (lagged) date in a same Series. If target date is out of (begin, end) range, index is -1\n    \"\"\"\n\n    dr = pd.date_range(begin, end)\n\n    #key is date, value is day index\n    base_index = pd.Series(np.arange(0, len(dr)), index=dr)\n\n    def lag(offset):\n        dates = dr - offset\n\n        date_lag = []\n        for d in dates:\n            if d in base_index.index:\n                date_lag.append(base_index.loc[d])\n            else:\n                date_lag.append(-1)\n        return pd.Series(data=np.array(date_lag).astype(np.int16), index=dr)\n    \n    return [lag(pd.DateOffset(months=m)) for m in (3, 6, 9, 12)]\n\ndef single_autocorr(series, lag):\n    \"\"\"\n    Autocorrelation for single data series\n    :param series: traffic series\n    :param lag: lag, days\n    :return:\n    \"\"\"\n    s1 = series[lag:]\n    s2 = series[:-lag]\n    ms1 = np.mean(s1)\n    ms2 = np.mean(s2)\n    ds1 = s1 - ms1\n    ds2 = s2 - ms2\n    divider = np.sqrt(np.sum(ds1 * ds1)) * np.sqrt(np.sum(ds2 * ds2))\n    return np.sum(ds1 * ds2) / divider if divider != 0 else 0\n\ndef batch_autocorr(data, lag, starts, ends, threshold, backoffset=0):\n    \"\"\"\n    Calculate autocorrelation for batch (many time series at once)\n    :param data: Time series, shape [n_pages, n_days]\n    :param lag: Autocorrelation lag\n    :param starts: Start index for each series\n    :param ends: End index for each series\n    :param threshold: Minimum support (ratio of time series length to lag) to calculate meaningful autocorrelation.\n    :param backoffset: Offset from the series end, days.\n    :return: autocorrelation, shape [n_series]. If series is too short (support less than threshold),\n    autocorrelation value is NaN\n    \"\"\"\n    n_series = data.shape[0]\n    n_days = data.shape[1]\n    max_end = n_days - backoffset\n    corr = np.empty(n_series, dtype=np.float64)\n    support = np.empty(n_series, dtype=np.float64)\n    for i in range(n_series):\n        series = data[i]\n        end = min(ends[i], max_end)\n        real_len = end - starts[i]\n        support[i] = real_len/lag\n        if support[i] > threshold:\n            series = series[starts[i]:end]\n            c_365 = single_autocorr(series, lag)\n            c_364 = single_autocorr(series, lag-1)\n            c_366 = single_autocorr(series, lag+1)\n            # Average value between exact lag and two nearest neighborhs for smoothness\n            corr[i] = 0.5 * c_365 + 0.25 * c_364 + 0.25 * c_366\n        else:\n            corr[i] = np.NaN\n    return corr #, support\n\ndef create_seles_features(sales_df):\n\n    id_features = sales_df[[\"id\",\"item_id\", \"dept_id\", \"cat_id\", \"store_id\",\"state_id\"]]\n\n    sales_df.drop(columns=[\"item_id\", \"dept_id\", \"cat_id\", \"store_id\",\"state_id\"],inplace=True)\n\n    df = sales_df.set_index(\"id\")\n\n    df, starts, ends = prepare_data(df, start=None, end=None, valid_threshold=0.0)\n\n    train_days_range = pd.date_range('29-01-2011',periods=1913)\n    valid_days_range = pd.date_range('25-04-2016',periods=28)\n    test_days_range = pd.date_range('23-05-2016',periods=28)\n\n    df.columns = df.columns.to_series().apply(lambda x: int(x.split(\"_\")[-1]))\n\n    date_start, date_end = train_days_range[0] , train_days_range[-1]\n    features_end = valid_days_range[-1]\n\n    print(f\"date_satart : {date_start} , date_end : {date_end}, features_end : {features_end}\")\n    \n    encoded_id_features = encode_id_features(id_features)\n\n    item_popularity = df.median(axis=1)\n    item_popularity = (item_popularity - item_popularity.mean()) / item_popularity.std()\n\n    # Yearly(annual) autocorrelation\n    raw_year_autocorr = batch_autocorr(df.values, 365, starts, ends, 1.5, 0)\n    year_unknown_pct = np.sum(np.isnan(raw_year_autocorr))/len(raw_year_autocorr)\n    # Normalise all the things\n    year_autocorr = normalize(np.nan_to_num(raw_year_autocorr))\n\n    # Quarterly autocorrelation\n    raw_quarter_autocorr = batch_autocorr(df.values, int(round(365.25/4)), starts, ends, 2, 0)\n    quarter_unknown_pct = np.sum(np.isnan(raw_quarter_autocorr)) / len(raw_quarter_autocorr)  # type: float\n    # Normalise all the things\n    quarter_autocorr = normalize(np.nan_to_num(raw_quarter_autocorr))\n\n    print(\"Percent of undefined autocorr = yearly:%.3f, quarterly:%.3f\" % (year_unknown_pct, quarter_unknown_pct))\n\n    final_df = pd.DataFrame({\n        \"item_popularity\" : item_popularity,\n        \"year_autocorr\" : year_autocorr,\n        \"quarter_autocorr\": quarter_autocorr\n    })\n\n    final_df.index = df.index.values\n\n    final_df = pd.merge(final_df,encoded_id_features, how=\"left\", left_index=True, right_index=True)\n\n    print(\"id_features : \", final_df.shape)\n\n    extra_features = {\n        \"train_days_range\": train_days_range,\n        \"valid_days_range\": valid_days_range,\n        \"test_days_range\": test_days_range,\n        \"starts\" : starts,\n        \"ends\" : ends\n    }\n    \n    return final_df, df, extra_features\n\n    \ndef create_date_features(df):\n    \n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n\n    #df.set_index(\"date\", inplace=True)\n\n    # week day\n    week_period = 7 / (2 * np.pi)\n    dow_norm = df[\"wday\"].values / week_period\n    wday_cos = np.cos(dow_norm)\n    wday_sin = np.sin(dow_norm)\n    \n    \"\"\"\n    # month\n    month_period = 12 / (2 * np.pi)\n    dow_norm = df[\"month\"].values / month_period\n    month_cos = np.cos(dow_norm)\n    month_sin = np.sin(dow_norm)\n\n    #print(df[\"date\"])\n\n    # day\n    day_period = 31 / (2 * np.pi)\n    dow_norm = df[\"date\"].dt.day / day_period\n    day_cos = np.cos(dow_norm)\n    day_sin = np.sin(dow_norm)\n    \"\"\"\n\n    # month\n    month = pd.get_dummies(df[\"month\"], drop_first=False, prefix=\"month\")\n    month = (month - month.mean()) / month.std()\n\n    # day\n    day = pd.get_dummies(df[\"date\"].dt.day, drop_first=False, prefix=\"date\")\n    day = (day - day.mean()) / day.std()\n\n\n    # event_name_1\n    event_name_1 = pd.get_dummies(df[\"event_name_1\"], drop_first=False, dummy_na=True, prefix=\"event_name_1\")\n    event_name_1 = (event_name_1 - event_name_1.mean()) / event_name_1.std()\n\n    # event_type_1\n    event_type_1 = pd.get_dummies(df[\"event_type_1\"], drop_first=False, dummy_na=True, prefix=\"event_type_1\")\n    event_type_1 = (event_type_1 - event_type_1.mean()) / event_type_1.std()\n\n    # event_name_2\n    event_name_2 = pd.get_dummies(df[\"event_name_2\"], drop_first=False, dummy_na=True, prefix=\"event_name_2\")\n    event_name_2 = (event_name_2 - event_name_2.mean()) / event_name_2.std()\n\n    # event_type_2\n    event_type_2 = pd.get_dummies(df[\"event_type_2\"], drop_first=False, dummy_na=True, prefix=\"event_type_2\")\n    event_type_2 = (event_type_2 - event_type_2.mean()) / event_type_2.std()\n\n    snap_CA = df[\"snap_CA\"].values\n    snap_TX = df[\"snap_TX\"].values\n    snap_WI = df[\"snap_WI\"].values\n\n    final_df = pd.DataFrame({\n        \"date\":df[\"date\"],\n        \"wday_cos\": wday_cos,\n        \"wday_sin\": wday_sin,\n        \"snap_CA\": snap_CA,\n        \"snap_TX\": snap_TX,\n        \"snap_WI\": snap_WI\n    })\n\n    final_df = pd.merge(final_df,month, how=\"left\", left_index=True, right_index=True)\n    final_df = pd.merge(final_df,day, how=\"left\", left_index=True, right_index=True)\n    final_df = pd.merge(final_df,event_name_1, how=\"left\", left_index=True, right_index=True)\n    final_df = pd.merge(final_df,event_type_1, how=\"left\", left_index=True, right_index=True)\n    final_df = pd.merge(final_df,event_name_2, how=\"left\", left_index=True, right_index=True)\n    final_df = pd.merge(final_df,event_type_2, how=\"left\", left_index=True, right_index=True)\n    \n    final_df.set_index(\"date\", inplace=True)\n\n    return final_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Id Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_validation = pd.read_csv(\"../input/m5-forecasting-accuracy/sales_train_validation.csv\")\nsales_train_validation = reduce_mem_usage(sales_train_validation)\nid_features_df, demand_df, extra_features_dict = create_seles_features(sales_train_validation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lag Indexs"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lag_indexes(begin, end):\n    \"\"\"\n    Calculates indexes for 3, 6, 9, 12 months backward lag for the given date range\n    :param begin: start of date range\n    :param end: end of date range\n    :return : List of 4 Series, one for each lag. For each Series, index is date in range(begin, end),value is an index\n    of target (lagged) date in a same Series. If target date is out of (begin, end) range, index is -1\n    \"\"\"\n\n    dr = pd.date_range(begin, end)\n\n    #key is date, value is day index\n    base_index = pd.Series(np.arange(0, len(dr)), index=dr)\n\n    def lag(offset):\n        dates = dr - offset\n\n        date_lag = []\n        for d in dates:\n            if d in base_index.index:\n                date_lag.append(base_index.loc[d])\n            else:\n                date_lag.append(-1)\n        return pd.Series(data=np.array(date_lag).astype(np.int16), index=dr)\n    \n    return [lag(pd.DateOffset(months=m)) for m in (1, 3, 6, 9, 12)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lag_idx = lag_indexes('2011-01-29', '2016-05-22')\n\nlag_idx_df = pd.DataFrame({\n    \"month_1\": lag_idx[0],\n    \"month_3\": lag_idx[1],\n    \"month_6\": lag_idx[2],\n    \"month_9\": lag_idx[3],\n    \"month_12\": lag_idx[4]\n\n})\n\nlag_index = lag_idx_df.values\n\nlag_index.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking NaN values\nid_features_df.isna().sum().sum(), demand_df.isna().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calendar Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncalendar = pd.read_csv(\"../input/m5-forecasting-accuracy/calendar.csv\")\ncalendar = reduce_mem_usage(calendar)\ncalendar_features_df = create_date_features(calendar)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check NaN values\ncalendar_features_df.isna().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_features_df.shape, demand_df.shape, calendar_features_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convert to Pytorch tensors"},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_pytorch_tensors(df):\n    df_tensor = torch.tensor(df.values)\n    df_indexs = df.index.values\n    return df_tensor, df_indexs\n\nid_tensor, id_idx = convert_pytorch_tensors(id_features_df)\ndemand_tensor, demand_idx = convert_pytorch_tensors(demand_df)\ncalender_tensor, calender_idx = convert_pytorch_tensors(calendar_features_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_tensor.shape, demand_tensor.shape,calender_tensor.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pytorch Data Loader "},{"metadata":{"trusted":true},"cell_type":"code","source":"# make train data set for pytorch data loaders\n\ntrain_df = pd.DataFrame(demand_df.values, columns = np.arange(1, 1914))\ntrain_df[\"id\"] = id_idx\ntrain_df[\"id_index\"] = np.arange(len(id_idx))\ntrain_df = pd.melt(train_df, id_vars=[\"id\", \"id_index\"], value_name=\"demand\", var_name=\"day\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train & Valid & Test df's"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_days_df = train_df[(train_df.day > 1500) & (train_df.day < 1885)]\nvalid_days_df = train_df[train_df.day == 1885]\ntest_days_df = train_df[train_df.day == 1913]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_days_df.shape, valid_days_df.shape, test_days_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For traing take some days in between"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_days = np.arange(1500,1913,28)\ntrain_days_df = train_days_df[train_days_df.day.isin(train_days)]\ntrain_days_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DataLoader Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TimeDataset:\n    def __init__(self, df, hparams):\n        \n        self.id = df.id.values\n        self.id_index = df.id_index.values\n        self.day = df.day.values\n        \n        self.train_window = hparams[\"train_window\"]\n        self.predict_window = hparams[\"predict_window\"]\n        \n    def __len__(self):\n        return len(self.id)\n    \n    def __getitem__(self, item):\n        \n        id = self.id[item]\n        id_index = self.id_index[item]\n        day = self.day[item]-1 # because day index starts from 1\n        \n        id_features = id_tensor[id_index].unsqueeze(0)\n        \n        # train_x / traing\n        calendar_train = calender_tensor[day-self.train_window:day, :]\n        id_train = torch.repeat_interleave(id_features, repeats=self.train_window, dim=0)\n        \n        demand_train = demand_tensor[id_index, day-self.train_window:day]\n        lag_index_train = lag_index[day-self.train_window:day]\n        lag_train = demand_tensor[id_index, lag_index_train]\n        \n        # train_y / prediction\n        calendar_predict = calender_tensor[day:day+self.predict_window, :]\n        id_predict = torch.repeat_interleave(id_features, repeats=self.predict_window, dim=0)\n        \n        demand_predict = demand_tensor[id_index, day:day+self.predict_window]\n        \n        \n        train_x = torch.cat([id_train, calendar_train, demand_train.unsqueeze(-1).double() ,lag_train.double()], dim=1)\n        train_y = torch.cat([id_predict, calendar_predict], dim=1)\n        \n        return {\n            \"encoder_input\":train_x,\n            \"train_demand\":demand_train,\n            \"decoder_input\":train_y,\n            \"prediction_demand\" : demand_predict\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example\n\nhparams = {\n    \"train_window\": 100,\n    \"predict_window\": 28\n}\n\nexample = TimeDataset(train_days_df, hparams)\noutput = example.__getitem__(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output[\"encoder_input\"].shape, output[\"train_demand\"].shape, output[\"decoder_input\"].shape, output[\"prediction_demand\"].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, hparams):\n        super(Encoder, self).__init__()\n\n        print(hparams[\"encoder_input_size\"])\n\n        self.RNN = nn.RNN(\n                    input_size=hparams[\"encoder_input_size\"],\n                    hidden_size=hparams[\"encoder_hidden_size\"], \n                    num_layers=hparams[\"encoder_num_layers\"],\n                    batch_first=True\n                )\n\n        \n    def forward(self, X):\n        \n        rnn_out, rnn_state = self.RNN(X)\n\n        encoder_state = rnn_state.squeeze(0)\n\n        return rnn_out, encoder_state\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, hparams):\n        super(Decoder, self).__init__()\n\n        self.hparams = hparams\n        \n        if hparams[\"decoder_rnn_layers\"] > 1:\n            pass\n        else:\n            self.cell = nn.GRUCell(input_size=hparams[\"decoder_input_size\"],\n                              hidden_size=hparams[\"decoder_hidden_size\"]\n                              )\n            \n        self.projected_output = nn.Linear(in_features=hparams[\"decoder_fc_in_features\"], out_features=1)\n\n    \n\n    def forward(self, encoder_state, decoder_inputs, previous_y):\n        \n        predict_days = self.hparams[\"predict_window\"]\n\n        # [batch_size, time, input_depth] -> [time, batch_size, input_depth]\n        inputs_by_time = decoder_inputs.permute(1,0,2)\n\n        # Return raw outputs for RNN loss calcula\n        targets_ta = []\n\n        #print(previous_y.shape)\n        prev_output = previous_y.unsqueeze(-1)\n\n        prev_state = encoder_state\n\n        for time in range(inputs_by_time.shape[0]):\n            \n            # RNN inputs for current step\n            features = inputs_by_time[time]\n\n            # [batch, predict_window, readout_depth * n_heads] -> [batch, readout_depth * n_heads]\n            \n            # append previous predicted values to input features\n            next_input = torch.cat([prev_output, features], dim=1)\n        \n            # Run RNN cell\n            #print(next_input.shape)\n            output = self.cell(next_input, prev_state)\n\n            # Make prediction from RNN outputs\n            projected_output = self.projected_output(output)\n\n            targets_ta.append(projected_output)\n\n            prev_output = projected_output\n            prev_state = output\n\n    \n        # Get final tensors from buffer list\n        targets = torch.stack(targets_ta)\n\n        # [time, batch_size, 1] -> [time, batch_size]\n        targets = targets.squeeze(-1)\n        \n        # [time, batch_size] -> [batch_size, time]\n        targets = targets.transpose(1,0)\n\n        return targets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loss Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def criterion1(pred1, targets):\n    l1 = nn.MSELoss()(pred1, targets)\n    return torch.sqrt(l1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train & Eval Loops"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(encoder, decoder,train_loader, epoch , encoder_optimizer, decoder_optimizer, scheduler, history, DEVICE):\n    encoder.train()\n    decoder.train()\n    total_loss = 0\n    loss = 0\n    \n    RMSE_list = []\n    \n    t = tqdm(train_loader)\n    for i, d in enumerate(t):\n        \n        encoder_input = d[\"encoder_input\"].float().to(DEVICE)\n        train_demand = d[\"train_demand\"].float().to(DEVICE)\n        decoder_input = d[\"decoder_input\"].float().to(DEVICE)\n        prediction_demand = d[\"prediction_demand\"].float().to(DEVICE)\n        \n        encoder_optimizer.zero_grad()\n        decoder_optimizer.zero_grad()\n        \n        previous_y = train_demand[:, -1]\n        \n        encoder_readout, encoder_state = encoder(encoder_input)\n        output = decoder(encoder_state, decoder_input, previous_y)\n\n        prediction_demand = prediction_demand.squeeze(-1)\n\n        loss = criterion1(output, prediction_demand)\n\n        loss.backward()\n\n        total_loss += loss\n        \n        encoder_optimizer.step()\n        decoder_optimizer.step()\n\n        \n        t.set_description(f'Epoch {epoch+1} : Loss: %.4f'%(total_loss/(i+1)))\n\n        if history is not None:\n            history.loc[epoch + i / len(X), 'train_loss'] = loss.data.cpu().numpy()\n            history.loc[epoch + i / len(X), 'lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n\n            \ndef eval_model(encoder, decoder,valid_loader, epoch , scheduler, history, DEVICE):\n    encoder.eval()\n    decoder.eval()\n    total_loss = 0\n    loss = 0\n    \n    t = tqdm(valid_loader)\n    RMSE_list = []\n\n    with torch.no_grad():\n        for i, d in enumerate(t):\n        \n            encoder_input = d[\"encoder_input\"].float().to(DEVICE)\n            train_demand = d[\"train_demand\"].float().to(DEVICE)\n            decoder_input = d[\"decoder_input\"].float().to(DEVICE)\n            prediction_demand = d[\"prediction_demand\"].float().to(DEVICE)\n\n            encoder_optimizer.zero_grad()\n            decoder_optimizer.zero_grad()\n\n            previous_y = train_demand[:, -1]\n\n            encoder_readout, encoder_state = encoder(encoder_input)\n            output = decoder(encoder_state, decoder_input, previous_y)\n\n            prediction_demand = prediction_demand.squeeze(-1)\n\n            l1 = criterion1(output, prediction_demand)\n\n            loss += l1\n\n            output = output.cpu().numpy()\n            predict = prediction_demand.cpu().numpy()\n\n            for pred, real in zip(output, predict):\n                rmse = sklearn.metrics.mean_squared_error(real, pred, squared=False)\n                RMSE_list.append(rmse)\n\n    loss /= len(valid_loader)\n\n    print(f'Valid loss: %.4f RMSE : %.4f'%(loss, np.mean(RMSE_list)))\n\n    return loss, np.mean(RMSE_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hparams"},{"metadata":{"trusted":true},"cell_type":"code","source":"hparams = {\n    \"encoder_input_size\" : 3173,\n    \"encoder_hidden_size\" : 128,\n    \"encoder_num_layers\" : 1,\n    \"decoder_rnn_layers\" : 1,\n    \"decoder_input_size\" : 3168,\n    \"decoder_hidden_size\" : 128,\n    \"decoder_fc_in_features\" : 128,\n    \"predict_window\":28,\n    \"train_window\":100\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pytorch DataLoaders"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = TimeDataset(train_days_df, hparams)\n    \ntrain_loader = torch.utils.data.DataLoader(\n        dataset=train_dataset,\n        batch_size= 128,\n        shuffle=False,\n        num_workers=4,\n        drop_last=False\n    )\n\nvalid_dataset = TimeDataset(valid_days_df, hparams)\n    \nvalid_loader = torch.utils.data.DataLoader(\n        dataset=valid_dataset,\n        batch_size= 128,\n        shuffle=False,\n        num_workers=4,\n        drop_last=False\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = pd.DataFrame()\nhistory2 = pd.DataFrame()\n\nDEVICE = \"cuda\"\nTRAIN_BATCH_SIZE = 128\nTEST_BATCH_SIZE = 128\nEPOCHS = 1\nstart_e = 0\n\nencoder = Encoder(hparams)\ndecoder = Decoder(hparams)\n\nencoder.to(DEVICE)\ndecoder.to(DEVICE)\n\n\n#optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\nencoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.0001)\ndecoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.0001)\n\n#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, mode='min', factor=0.7, verbose=True, min_lr=1e-5)\n    \nprint(\"FOLD : \", )\n\nfor epoch in range(start_e, EPOCHS):\n    #(encoder, decoder,train_loader, epoch , encoder_optimizer, decoder_optimizer, scheduler=None, history=None)\n    train_model(encoder, decoder, train_loader, epoch, encoder_optimizer, decoder_optimizer, scheduler=None, history=None, DEVICE=DEVICE)\n    loss, rmse = eval_model(encoder, decoder,valid_loader, epoch , scheduler=None, history=None, DEVICE=DEVICE)\n    torch.save(encoder.state_dict(), f\"encoder_{epoch}_loss_{loss}_rmse_{rmse}.pth\")\n    torch.save(decoder.state_dict(), f\"decoder_{epoch}_loss_{loss}_rmse_{rmse}.pth\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"color:blue;\"> Please upvote if you like it. It motivates me. Thank you ☺️ .</h3>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}