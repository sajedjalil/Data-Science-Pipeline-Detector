{"cells":[{"metadata":{},"cell_type":"markdown","source":"# State Space Modelling for M5 Forecasting Accuracy","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This notebook was inspired by this discussion post: [Any \"time series\" model < 0.7?](https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/147961). In this notebook I attempt to break 0.7 using only [state space models](http://www.scholarpedia.org/article/State_space_model), rather than with neural networks.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"There are various notebooks that have used LightGBM with categorical variables to model seasonality and trend in sales, e.g.:\n* [for_Japanese_beginner(with WRMSSE in LGBM))](https://www.kaggle.com/girmdshinsei/for-japanese-beginner-with-wrmsse-in-lgbm)\n* [Very fst Model](https://www.kaggle.com/ragnar123/very-fst-model)\n* [m5-baseline](https://www.kaggle.com/harupy/m5-baseline)\n* [M5 - Simple FE](https://www.kaggle.com/kyakovlev/m5-simple-fe)\n\nThese use sklearn LabelEncoder variously for year, quarter-in-year, month-in-year, week-in-year, week-in-month, day-in-month, and day-of-week. (Interestingly, all these notebooks treat the resultant labels as numeric rather than categorical in LightGBM, but that is a matter for another day.)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"My own investigation suggests there are at least three seasonalities within the sales data: month-in-year, day-in-month, and day-of-week. However, adding week-in-year improves LightGBM performance; week-in-year and month-in-year could perhaps be better modelled together as day-in-year.\n\nThere are several ways to deal with multiple seasonality in time series forecasting (whether we are using state space models or neural networks) each with their drawbacks:\n* Exponential smoothing with multiple seasonalities incorporated directly (i.e., Holt-Winters method). This approach was taken by the winner of the M4 competition as a precursor step to modelling and forecasting with a neural network, see [here](https://www.sciencedirect.com/science/article/pii/S0169207019301153). That work was done in R; Python's Holt-Winters only allows a single seasonality.\n* [TBATS](https://medium.com/intive-developers/forecasting-time-series-with-multiple-seasonalities-using-tbats-in-python-398a00ac0e8a): this is too computationally expensive for automatically modelling and forecasting 30490 series (even offline on a faster machine).\n* SARIMA (seasonal auto-regressive integrated moving average) allows modelling a single seasonality only, but one can encode the other seasonalities using Fourier terms as exogenous variables and employ SARIMAX, see the TBATS link above for a comparison of the two methods. I have found that trying to do all the computation (including the other exogenous variables that we have) in one SARIMAX takes far too long (~72 hours for all 30490 series on dual-threaded quad core @ 2.80 GHz).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this notebook I instead take a step-wise approach to trend and seasonality modelling and forecasting, using [differencing](https://machinelearningmastery.com/remove-trends-seasonality-difference-transform-python/), seasonal decomposition, ARMA, SARMA, ARMAX, and [Holt-Winters](https://machinelearningmastery.com/exponential-smoothing-for-time-series-forecasting-in-python/) for variety. Note that [ARMA](https://en.wikipedia.org/wiki/Autoregressive%E2%80%93moving-average_model), SARMA, and ARMAX are all special cases of SARIMAX and the same function is used to estimate all of them. ARMA assumes the series is weakly stationary, i.e., the moments (mean, variance, skewness, kurtosis, etc.) are constant over time and the error process can be modelled as an **A**uto-**R**egressive (AR) **M**oving **A**verage (MA) process. SARMA adds a single **S**easonality term, ARMAX adds e**X**ogenous variables (in our case dummies for `event_name_1` and the binary SNAP features).","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from tqdm import tqdm_notebook as tqdm\nimport gc\nimport pickle\nfrom datetime import datetime, timedelta\nfrom scipy.sparse import csr_matrix\nfrom joblib import Parallel, delayed\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 10)\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_path = '/kaggle/input/m5-forecasting-accuracy/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.\n                      format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read data files\ncalendar = pd.read_csv(input_path + '/calendar.csv')\ncalendar = reduce_mem_usage(calendar)\nsell_prices = pd.read_csv(input_path + '/sell_prices.csv')\nsell_prices = reduce_mem_usage(sell_prices)\nsales_train_val = pd.read_csv(input_path + '/sales_train_validation.csv')\nsales_train_val = reduce_mem_usage(sales_train_val)\nsample_submission = pd.read_csv(input_path + '/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_ITEMS = sales_train_val.shape[0]  # 30490\nDAYS_PRED = sample_submission.shape[1] - 1  # 28","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# keep a multiple of DAYS_PRED days (columns) for convenience\nncols = sales_train_val.shape[1] - sales_train_val.shape[1] % DAYS_PRED\nsales_train_val = sales_train_val.iloc[:, -ncols:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take the transpose so that we have one day for each row, and 30490 items' sales as columns\nsales_train_val = sales_train_val.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot total sales\nplt.style.use('classic')\nsalesTotal = np.sum(sales_train_val, axis=1)\nplt.plot(salesTotal)\nplt.xticks(salesTotal.index[np.arange(1, len(salesTotal), step=280)])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# difference the series to remove trend\nlag_sales_train_val = sales_train_val.iloc[:-DAYS_PRED, :]\nlag_sales_train_val_test = sales_train_val.iloc[-DAYS_PRED:, :]\nsales_train_val = sales_train_val.iloc[DAYS_PRED:, :]\nsales_train_val = sales_train_val - lag_sales_train_val.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot total differenced sales\nplt.style.use('classic')\nsalesTotalI = np.sum(sales_train_val, axis=1)\nplt.plot(salesTotalI)\nplt.xticks(salesTotalI.index[np.arange(1, len(salesTotalI), step=280)])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Seasonal Decomposition and ARMA Forecasting\nstatsmodels help pages:\n* [seasonal_decompose](https://www.statsmodels.org/stable/generated/statsmodels.tsa.seasonal.seasonal_decompose.html)\n* [SARIMAX](https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html)\n\nThe idea is to use `seasonal_decompose` to get the mean (which may be fluctuating) and seasonality (which is periodic) of the training data. The mean is then forecast by an ARMA(3,2) model using `SARIMAX` and the seasonality is forecast by simple repetition. This is quite slow and gives poor dynamic forecasts if we enforce a day-in-year seasonality (`period=365`) so we down-sample to 28-day blocks and make a one-step-ahead forecast to cover the whole test period. The downside of this is that we forecast a constant mean for the test period, whereas it may be trending. Hopefully the subsequent steps will compensate for this.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove yearly seasonality by down-sampling, decomposing, and forecasting with ARMA\ndef yearly_season(ts):\n    ts = pd.Series(ts)\n    ts.index = pd.to_datetime(ts.index)\n    ts.index.freq = 'N'\n    ts28 = ts.resample('28N').sum()\n    decomp = seasonal_decompose(ts28.values, 'additive', period=13, extrapolate_trend='freq')\n    try:\n        # the order here was chosen offline using autoarima for the total sales, \n        # we could use autoarima for each series but that would take a lot longer\n        mod = SARIMAX(decomp.trend, order=(3, 0, 2))\n        # increase maxiter and use 'powell' here as it is gradient-free and the default method often doesn't converge for these data\n        res = mod.fit(disp=False, cov_type='robust', maxiter=200, method='powell')\n        predict = res.get_prediction(start=0, end=len(ts28))\n        predicted_mean = np.repeat(predict.predicted_mean, 28) / 28\n    except:\n        predicted_mean = np.repeat(np.mean(ts28), len(ts28)+1)\n        predicted_mean = np.repeat(predicted_mean, 28) / 28\n    # extend seasonal component by one period\n    seasonal = decomp.seasonal\n    seasonal = np.append(seasonal, seasonal[-13])\n    seasonal = np.repeat(seasonal, 28) / 28\n    predicted_mean += seasonal\n    return predicted_mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run the parallel job for yearly seasonal - uncomment this if you edit yearly_seasonal()\n#fit_forecast1 = Parallel(n_jobs=-1)(delayed(yearly_season)(\n#    sales_train_val.iloc[:, i].values\n#) for i in tqdm(range(NUM_ITEMS)))\n#file = open('yearly_seasonal2.pkl', 'wb')\n#pickle.dump(fit_forecast1, file)\n#file.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load pre-calculate yearly seasonal fit & forecast - comment out if you are editing yearly_seasonal()\nfile = open('/kaggle/input/seasonalities2/seasonality2/yearly_seasonal2.pkl', 'rb')\nfit_forecast1 = pickle.load(file)\nfile.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get yearly seasonal in same format as sales_train_val\nfit_forecast1 = np.concatenate(fit_forecast1)\nfit_forecast1 = fit_forecast1.reshape(-1, NUM_ITEMS, order='F')\nfit1 = fit_forecast1[:-DAYS_PRED, :]\nforecast1 = fit_forecast1[-DAYS_PRED:, :]\ndel fit_forecast1 ; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot total sales and total fit & forecast\nplt.plot(salesTotalI, color='blue')\nfitTotal = pd.Series(np.sum(fit1, axis=1))\nfitTotal.index = salesTotalI.index\nplt.plot(fitTotal, color='orange')\nforecastTotal = pd.Series(np.sum(forecast1, axis=1))\nforecastTotal.index = ['d_' + str(i) for i in range(1914,1914+28)]\nplt.plot(forecastTotal, color='red')\nplt.xticks(salesTotalI.index[np.arange(1, len(salesTotalI), step=280)])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove trend and first seasonal component from sales\nsales_train_val = sales_train_val - fit1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove first 28-day period as we have no fit\nsales_train_val = sales_train_val.iloc[28:, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot transformed series\nsales1Total = np.sum(sales_train_val, axis=1)\nplt.plot(sales1Total)\nplt.xticks(sales1Total.index[np.arange(1, len(sales1Total), step=280)])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SARMA for Day-of-Week Seasonality","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# SARMA with day-of-week seasonality\ndef dow_season(ts):\n    try:\n        mod = SARIMAX(ts, order=(1, 0, 1), seasonal_order=(1, 1, 1, 7))\n        res = mod.fit(disp=False, cov_type='robust', maxiter=200, method='powell')\n        predict = res.get_prediction(start=0, end=len(ts) + 27)\n        predicted_mean = predict.predicted_mean\n    except:\n        predicted_mean = np.repeat(np.mean(ts), len(ts) + 28)\n    return predicted_mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run the parallel job - uncomment this if you edit dow_season()\n#fit_forecast2 = Parallel(n_jobs=-1)(delayed(dow_season)(\n#    sales_train_val.iloc[:, i].values\n#) for i in tqdm(range(NUM_ITEMS)))\n#file = open('dow_seasonal2.pkl', 'wb')\n#pickle.dump(fit_forecast2, file)\n#file.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load pre-calculated dow seasonality - comment this out if you are editing dow_season()\nfile = open('/kaggle/input/seasonalities2/seasonality2/dow_seasonal2.pkl', 'rb')\nfit_forecast2 = pickle.load(file)\nfile.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get day-of-week seasonal in same format as sales_train_val \nfit_forecast2 = np.concatenate(fit_forecast2)\nfit_forecast2 = fit_forecast2.reshape(-1, NUM_ITEMS, order='F')\nfit2 = fit_forecast2[:-DAYS_PRED, :]\nforecast2 = fit_forecast2[-DAYS_PRED:, :]\ndel fit_forecast2 ; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot transformed and fit\nsales1Total = np.sum(sales_train_val, axis=1)\nplt.plot(sales1Total, color='blue')\nfit2Total = pd.Series(np.sum(fit2, axis=1))\nfit2Total.index = sales1Total.index\nplt.plot(fit2Total, color='orange')\nforecast2Total = pd.Series(np.sum(forecast2, axis=1))\nforecast2Total.index = ['d_' + str(i) for i in range(1914,1914+28)]\nplt.plot(forecast2Total, color='red')\nplt.xticks(sales1Total.index[np.arange(1, len(sales1Total), step=280)])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove second seasonal component from sales\nsales_train_val = sales_train_val - fit2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot transformed series\nsales2Total = np.sum(sales_train_val, axis=1)\nplt.plot(sales2Total)\nplt.xticks(sales2Total.index[np.arange(1, len(sales2Total), step=280)])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# it is not obvious from the plot, but because of construction, we have no fit for the first day\nsales_train_val = sales_train_val.iloc[1:, :]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ARMAX for Events and SNAP","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# there are obvious spikes for Thanksgiving and 25th December, and presumably other events, so we model these with dummies and ARMAX\n# calendar: keep needed rows & columns\ncalendar.drop(['wm_yr_wk', 'weekday', 'd', 'event_type_1', 'event_type_2'], axis=1, inplace=True)\ncalendar = calendar.iloc[-(sales_train_val.shape[0] + DAYS_PRED * 2):, :]\ncalendar['date'] = pd.to_datetime(calendar['date'])\n\n# one-hot-encode event_name_1\nevent_name_1_ohe = pd.get_dummies(calendar['event_name_1'], dummy_na=True, dtype=np.int8)\ncalendar.drop(['event_name_1'], axis=1, inplace=True)\ncalendar = pd.concat([calendar, event_name_1_ohe], axis=1)\n\n# one-hot-encode event_name_2\n#event_name_2_ohe = pd.get_dummies(calendar['event_name_2'], dummy_na=True, dtype=np.int8)\n#calendar.drop(['event_name_2'], axis=1, inplace=True)\n#calendar = pd.concat([calendar, event_name_2_ohe], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can also add Fourier terms (first and second order) to model the day-in-month seasonality\n# but this takes too long to compute the ARMAX\n#fourier = pd.DataFrame({'date': calendar['date'].unique()})\n#fourier = fourier.set_index(pd.PeriodIndex(fourier['date'], freq='D'))\n#fourier['sin30'] = np.sin(2 * np.pi * fourier.index.day / 30.4375)\n#fourier['cos30'] = np.cos(2 * np.pi * fourier.index.day / 30.4375)\n#fourier['sin30_2'] = np.sin(4 * np.pi * fourier.index.day / 30.4375)\n#fourier['cos30_2'] = np.cos(4 * np.pi * fourier.index.day / 30.4375)\n#fourier.drop(columns=['date'], inplace=True)\ncalendar.drop(columns=['date'], inplace=True)\n\n# merge by concat the calendar features with fourier\n#fourier.index = calendar.index\n#calendar = pd.concat([calendar, fourier], axis=1)\n#del fourier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set calendar index to align with sales_train_val\ncalendar.index = ['d_' + str(i) for i in range(67, 1914 + DAYS_PRED * 2)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ARMAX(1,1) - exogenous variables are one-hot-encoded event_name_1 and binary SNAP features\ndef events_snap(ts):\n    try:\n        mod = SARIMAX(ts, order=(1, 0, 1), \n                      exog=calendar.drop(['wday', 'month', 'year', 'event_name_2'], axis=1).iloc[:-(DAYS_PRED * 2), :].values)\n        res = mod.fit(disp=False, cov_type='robust', maxiter=200, method='powell')\n        predict = res.get_prediction(start=0, end=len(ts) + 27, \n                                     exog=calendar.drop(['wday', 'month', 'year', 'event_name_2'], axis=1).\\\n                                                   iloc[-(DAYS_PRED * 2):-DAYS_PRED, :].values)\n        predicted_mean = predict.predicted_mean\n    except:\n        predicted_mean = np.repeat(np.mean(ts), len(ts) + 28)\n    return predicted_mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run the parallel job - this has ETA of ~70 hours on Kaggle so I have precomputed it\n#fit_forecast3 = Parallel(n_jobs=-1)(delayed(events_snap)(\n#    sales_train_val.iloc[:, i].values\n#) for i in tqdm(range(NUM_ITEMS)))\n#file = open('events_snap2.pkl', 'wb')\n#pickle.dump(fit_forecast3, file)\n#file.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load pre-calculated events, snap fit & forecast\nfile = open('/kaggle/input/seasonalities2/seasonality2/events_snap2.pkl', 'rb')\nfit_forecast3 = pickle.load(file)\nfile.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get events, snap fit & forecast in same format as sales_train_val \nfit_forecast3 = np.concatenate(fit_forecast3)\nfit_forecast3 = fit_forecast3.reshape(-1, NUM_ITEMS, order='F')\nfit3 = fit_forecast3[:-DAYS_PRED, :]\nforecast3 = fit_forecast3[-DAYS_PRED:, :]\ndel fit_forecast3 ; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot transformed\nplt.plot(sales2Total, color='blue')\nfit3Total = pd.Series(np.sum(fit3, axis=1))\nfit3Total.index = sales2Total.index[1:]\nplt.plot(fit3Total, color='orange')\nforecast3Total = pd.Series(np.sum(forecast3, axis=1))\nforecast3Total.index = ['d_' + str(i) for i in range(1914,1914+28)]\nplt.plot(forecast3Total, color='red')\nplt.xticks(sales2Total.index[np.arange(1, len(sales2Total), step=280)])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove trend and first seasonal component from sales\nsales_train_val = sales_train_val - fit3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot transformed series\nsales3Total = np.sum(sales_train_val, axis=1)\nplt.plot(sales3Total)\nplt.xticks(sales3Total.index[np.arange(1, len(sales3Total), step=280)])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Holt-Winters for Day-in-Month Seasonality","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# model day-in-month seasonality with Holt-Winters\ndef dayinmonth(ts):\n    mod = ExponentialSmoothing(ts, trend='add', seasonal='add', seasonal_periods=30)\n    res = mod.fit()\n    fit = res.fittedvalues\n    forecast = res.forecast(DAYS_PRED)\n    fit_forecast = np.append(fit, forecast)\n    return fit_forecast","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run the parallel job \n#fit_forecast4 = Parallel(n_jobs=-1)(delayed(dayinmonth)(\n#    sales_train_val.iloc[:, i].values\n#) for i in tqdm(range(NUM_ITEMS)))\n#file = open('dayinmonth.pkl', 'wb')\n#pickle.dump(fit_forecast4, file)\n#file.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load pre-calculated day-in-month fit & forecast\nfile = open('/kaggle/input/seasonalities2/seasonality2/dayinmonth2.pkl', 'rb')\nfit_forecast4 = pickle.load(file)\nfile.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get day-in-month in same format as sales_train_val \nfit_forecast4 = np.concatenate(fit_forecast4)\nfit_forecast4 = fit_forecast4.reshape(-1, NUM_ITEMS, order='F')\nfit4 = fit_forecast4[:-DAYS_PRED, :]\nforecast4 = fit_forecast4[-DAYS_PRED:, :]\ndel fit_forecast4 ; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot transformed\nplt.plot(sales3Total, color='blue')\nfit4Total = pd.Series(np.sum(fit4, axis=1))\nfit4Total.index = sales3Total.index\nplt.plot(fit4Total, color='orange')\nforecast4Total = pd.Series(np.sum(forecast4, axis=1))\nforecast4Total.index = ['d_' + str(i) for i in range(1914,1914+28)]\nplt.plot(forecast4Total, color='red')\nplt.xticks(sales2Total.index[np.arange(1, len(sales2Total), step=280)])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Forecast & Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine forecasts\npreds = forecast1 + forecast2 + forecast3 + forecast4 + lag_sales_train_val_test\npreds = pd.DataFrame(data=preds)\npreds.index =  ['d_' + str(i) for i in range(1914,1914+28)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sanity check\npredsTotal = preds.sum(axis=1)\npredsTotal.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission\nsubmission = preds.T\nsubmission = pd.concat((submission, submission), ignore_index=True)\n\nidColumn = sample_submission[[\"id\"]]\nsubmission[[\"id\"]] = idColumn  \n\ncols = list(submission.columns)\ncols = cols[-1:] + cols[:-1]\nsubmission = submission[cols]\ncolsdeneme = [\"id\"] + [f\"F{i}\" for i in range (1,29)]\nsubmission.columns = colsdeneme\n\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Notes\n* SARIMAX may display a warning: `UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.` However, the default starting parameters are zeros anyway, so I don't know what effect, if any, this has. I believe convergence is more important.\n* SARIMAX may also display a warning: `UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.` Ditto.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}