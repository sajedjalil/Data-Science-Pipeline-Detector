{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os, sys, gc, time, warnings, pickle, psutil, random\n\nimport time\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## Simple \"Memory profilers\" to see memory usage\ndef get_memory_usage():\n    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n        \ndef sizeof_fmt(num, suffix='B'):\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f%s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f%s%s\" % (num, 'Yi', suffix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Vars\n#################################################################################\nTARGET = 'sales'         # Our main target\nEND_TRAIN = 1913         # Last day in train set\nMAIN_INDEX = ['id','d']  # We can identify item by these columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Load Data\n#################################################################################\nprint('Load Main Data')\n\n# We will need only train dataset\n# to show lags concept\ntrain_df = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv')\n\n# To make all calculations faster\n# we will limit dataset by 'CA' state\ntrain_df = train_df[train_df['state_id']=='CA']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Data Representation\n#################################################################################\n\n# Let's check our shape\nprint('Shape', train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Horizontal representation\n\n# If we feed directly this data to model\n# our label will be values in column 'd_1913'\n# all other columns will be our \"features\"\n\n# In lag terminology all d_1->d_1912 columns\n# are our lag features \n# (target values in previous time period)\n\n# Good thing that we have a lot of features here\n# Bad thing is that we have just 12196 \"train rows\"\n# Note: here and after all numbers are limited to 'CA' state\ntrain_df.iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Vertical representation\n\n# In other hand we can think of d_ columns\n# as additional labels and can significantly \n# scale up our training set to 23330948 rows\n\n# Good thing that our model will have \n# greater input for training\n# Bad thing that we are losing lags that we had\n# in horizontal representation and\n# also new data set consumes much more memory\n\nindex_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\ntrain_df = pd.melt(train_df, \n                  id_vars = index_columns, \n                  var_name = 'd', \n                  value_name = TARGET)\n\ntrain_df[train_df['id']=='HOBBIES_1_001_CA_1_validation'].iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"## Some minification\ntrain_df['d'] = train_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n\nicols = ['id','item_id','dept_id','cat_id','store_id','state_id']\nfor col in icols:\n    train_df[col] = train_df[col].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Lags creation\n#################################################################################\n\n# We have several \"code\" solutions here\n# As our dataset is allready sorted by d values\n# we can simply shift() values\n# also we have to keep in mind that \n# we need to aggregate values on 'id' level\n\n# group and shift in loop\ntemp_df = train_df[['id','d',TARGET]]\n\nstart_time = time.time()\nfor i in range(1,8):\n    print('Shifting:', i)\n    temp_df['lag_'+str(i)] = temp_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(i))\n    \nprint('%0.2f min: Time for loops' % ((time.time() - start_time) / 60))\n\n\n# Or same in \"compact\" manner\nLAG_DAYS = [col for col in range(1,8)]\ntemp_df = train_df[['id','d',TARGET]]\n\nstart_time = time.time()\ntemp_df = temp_df.assign(**{\n        '{}_lag_{}'.format(col, l): temp_df.groupby(['id'])[col].transform(lambda x: x.shift(l))\n        for l in LAG_DAYS\n        for col in [TARGET]\n    })\n\nprint('%0.2f min: Time for bulk shift' % ((time.time() - start_time) / 60))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The result\ntemp_df[temp_df['id']=='HOBBIES_1_001_CA_1_validation'].iloc[:10]\n\n# You can notice many NaNs values - it's normal\n# because there is no data for day 0,-1,-2\n# (out of dataset time periods)\n\n# Same works for test set\n# be careful to make lag features:\n# for day 1920 there is no data about day 1919 (until 1913)\n# So if you want to predict day 1915 your \n# lag features have to start from 2 \n# (1915(predicting day) - 1913(last day with label in dataset))\n# and so on.\n\n# There are few options to work \n# with NaNs in train set\n## 1. drop it train_df[train_df['d']>MAX_LAG_DAY] \n## 1.1 in our case we already dropped some lines by release date\n##     so you have find d.min() for each id\n##     and drop train_df[train_df['d']>(train_df['d_min']+MAX_LAG_DAY)] \n## 2. If you want to keep it you can \n##    fill with '-1' to be able to convert to int\n## 3. Leave as it is\n## 4. Fill with mean -> not recommended","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Rolling lags\n#################################################################################\n\n# We restored some day sales values from horizontal representation\n# as lag features but just few of them (last 7 days or less)\n# because of memory limits we can't have many lag features\n# How we can get additional information from other days?\n\n## Rolling aggragations\n\ntemp_df = train_df[['id','d','sales']]\n\nstart_time = time.time()\n\nfor i in [14,30,60]:\n    print('Rolling period:', i)\n    temp_df['rolling_mean_'+str(i)] = temp_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).mean())\n    temp_df['rolling_std_'+str(i)]  = temp_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).std())\n\n# lambda x: x.shift(1)\n# 1 day shift will serve only to predict day 1914\n# for other days you have to shift PREDICT_DAY-1913\n\n# Such aggregations will help us to restore\n# at least part of the information for our model\n# and out of 14+30+60->104 columns we can have just 6\n# with valuable information (hope it is sufficient)\n# you can also aggregate by max/skew/median etc \n# also you can try other rolling periods 180,365 etc\nprint('%0.2f min: Time for loop' % ((time.time() - start_time) / 60))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The result\ntemp_df[temp_df['id']=='HOBBIES_1_002_CA_1_validation'].iloc[:20]\n\n# Same for NaNs values - it's normal\n# because there is no data for \n# 0*(rolling_period),-1*(rolling_period),-2*(rolling_period)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Memory ussage\n#################################################################################\n# Let's check our memory usage\nprint(\"{:>20}: {:>8}\".format('Original rolling df',sizeof_fmt(temp_df.memory_usage(index=True).sum())))\n\n# can we minify it?\n# 1. if our dataset are aligned by index \n#    you don't need 'id' 'd' 'sales' columns\ntemp_df = temp_df.iloc[:,3:]\nprint(\"{:>20}: {:>8}\".format('Values rolling df',sizeof_fmt(temp_df.memory_usage(index=True).sum())))\n\n# can we make it even smaller?\n# carefully change dtype and/or\n# use sparce matrix to minify 0s\n# Also note that lgbm accepts matrixes as input\n# that is good for memory reducion \nfrom scipy import sparse \ntemp_matrix = sparse.csr_matrix(temp_df)\n\n# restore to df\ntemp_matrix_restored = pd.DataFrame(temp_matrix.todense())\nrestored_cols = ['roll_' + str(i) for i in list(temp_matrix_restored)]\ntemp_matrix_restored.columns = restored_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Remove old objects\n#################################################################################\ndel temp_df, train_df, temp_matrix, temp_matrix_restored","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Apply on grid_df\n#################################################################################\n# lets read grid from \n# https://www.kaggle.com/kyakovlev/m5-simple-fe\n# to be sure that our grids are aligned by index\ngrid_df = pd.read_pickle('../input/m5-simple-fe/grid_part_1.pkl')\n\n# We need only 'id','d','sales'\n# to make lags and rollings\ngrid_df = grid_df[['id','d','sales']]\nSHIFT_DAY = 28\n\n# Lags\n# with 28 day shift\nstart_time = time.time()\nprint('Create lags')\n\nLAG_DAYS = [col for col in range(SHIFT_DAY,SHIFT_DAY+15)]\ngrid_df = grid_df.assign(**{\n        '{}_lag_{}'.format(col, l): grid_df.groupby(['id'])[col].transform(lambda x: x.shift(l))\n        for l in LAG_DAYS\n        for col in [TARGET]\n    })\n\n# Minify lag columns\nfor col in list(grid_df):\n    if 'lag' in col:\n        grid_df[col] = grid_df[col].astype(np.float16)\n\nprint('%0.2f min: Lags' % ((time.time() - start_time) / 60))\n\n# Rollings\n# with 28 day shift\nstart_time = time.time()\nprint('Create rolling aggs')\n\nfor i in [7,14,30,60,180]:\n    print('Rolling period:', i)\n    grid_df['rolling_mean_'+str(i)] = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).mean()).astype(np.float16)\n    grid_df['rolling_std_'+str(i)]  = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).std()).astype(np.float16)\n\n# Rollings\n# with sliding shift\nfor d_shift in [1,7,14]: \n    print('Shifting period:', d_shift)\n    for d_window in [7,14,30,60]:\n        col_name = 'rolling_mean_tmp_'+str(d_shift)+'_'+str(d_window)\n        grid_df[col_name] = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(d_shift).rolling(d_window).mean()).astype(np.float16)\n    \n    \nprint('%0.2f min: Lags' % ((time.time() - start_time) / 60))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Export\n#################################################################################\nprint('Save lags and rollings')\ngrid_df.to_pickle('lags_df_'+str(SHIFT_DAY)+'.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Final list of new features\n#################################################################################\ngrid_df.info()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}