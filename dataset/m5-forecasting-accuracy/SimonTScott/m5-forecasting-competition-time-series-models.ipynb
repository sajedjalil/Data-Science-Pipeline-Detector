{"cells":[{"metadata":{"_uuid":"2fa6459a-1759-4df7-8006-563a6d6b9d7c","_cell_guid":"61c28ff8-edfc-475e-b9b9-1c5133d0918a","trusted":true},"cell_type":"markdown","source":"# M5 Forecasting Competition: Time Series Models\n\nThe M5 competition was a compeition held from April - June 2020 where competitors aimed to predict the next 28 days of Walmart sales.  The data included the previous 5 years of sales for over 3,000 distinct items spread across 10 different Walmart stores.  Further information was included such as promotional events that occured each week.  The challenge is in predicting such low count data for more than 30,000 time series.\n\nThe score is calculated so that a low score is better and a perfect score is 0.  High priced items are typically weighted more.  Hence, the best scores will have \"lower forecasting errors for the series that are more valuable for the company\".\n\nFurther details of the competition are found at: https://mofc.unic.ac.cy/m5-competition/.\n\nThis notebook will show models submitted in the M5 competition.  An improvement not finished in time for submission is also developed below and shows some significant improvements.\n\nThe best model in this notebook was an ARIMA model with a hierarchical distribution method and got a public score of 0.78380.  This tuned parameters via a mixture of the Akaike information criterion and cross-validation.  The best submitted technique in time for the competition deadline was similar but only used cross-validation for tuning and got a score of 0.85226.\n\nClick the \"Code\" button on the right to view code written.  This notebook is written in Python and uses Pandas, statsmodels and matplotlib for data analysis.\n\n1. [The Data: Import and Clean](#section-one)\n2. [EDA](#section-two)\n3. [The Naive and Mean Methods](#section-three)\n4. [Seasonal Holt-Winters](#section-four)\n5. [ARIMA](#section-five)\n7. [Conclusion](#section-six)\n\n<a id=\"section-one\"></a>\n## The Data: Import and Clean\n\nLots of data is given by the competition including:\n\n* Number of sales by item and store for each day.\n* Sale prices for each item in a store.\n* Dates for promotional events, such as Easter or the Super Bowl.\n* Breakdown for each item by: state, store, category (Food, Household and Hobbies) and department within each category.\n\nThe competition was structured so that in the last month of the competition the next 28 days of data was released.  The final competition scores, kept hidden till after the deadline, were calculated only on how well entrants could predict the next 28 days after that newly released data.  This notebook will use all the available data up to the end of the competition as if it had not yet ended."},{"metadata":{"_uuid":"682ba4fe-1941-4aca-bcdf-6fd4dc620dd4","_cell_guid":"0ee8b982-cd4f-4cf8-98fe-bbd5291869e0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import math as math\nimport random as random\nimport itertools as it\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport warnings\nfrom statsmodels.tsa import holtwinters as hw\nfrom statsmodels.tsa.arima.model import ARIMA as ARIMA\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\nfrom numpy.linalg import LinAlgError\n\n# Import\ncalendar = pd.read_csv(\"../input/m5-forecasting-accuracy/calendar.csv\")\nprices = pd.read_csv(\"../input/m5-forecasting-accuracy/sell_prices.csv\")\nsales = pd.read_csv(\"../input/m5-forecasting-accuracy/sales_train_evaluation.csv\")\n\n# Clean so day is number only\nsales.rename({\"d_{}\".format(i) : i for i in range(1,1942)}, axis = 1, inplace = True)\ncalendar.loc[:,\"d\"] = calendar.loc[:,\"d\"].str.extract(\"d_(\\d+)\").iloc[:,0].astype(int)\n# Set prices in sales format\n## Note not all prices available, some na\nprices.loc[:,\"id\"] = prices.loc[:,\"item_id\"]+\"_\"+prices.loc[:,\"store_id\"]+\"_evaluation\"\nprices.drop([\"store_id\",\"item_id\"],axis=1,inplace=True)\nprices = prices.merge(calendar.loc[:,[\"wm_yr_wk\",\"d\"]], on=\"wm_yr_wk\",how=\"left\").drop(\"wm_yr_wk\",axis=1)\nprices = prices.pivot(\"id\",\"d\",\"sell_price\").rename_axis(columns=None)\nprices = prices.merge(sales.loc[:,[\"id\",\"store_id\",\"state_id\"]],left_index=True,right_on=\"id\")\n\n# Highest day number in sales evaluation data\nmax_pred = sales.select_dtypes(\"number\").columns.max()\n\n#\nwarnings.simplefilter('error', ConvergenceWarning)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33277ac3-85fb-4602-a939-47d641dd6fbb","_cell_guid":"e4b7e6d2-5f6e-4f39-8427-d43f3110c02d","trusted":true},"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n## EDA\n\nFirst, let's look at total sales by the hierarchical levels over all days.  Note the day to day sales fluctuate rapidly so the rolling average is plotted."},{"metadata":{"_uuid":"73d46994-3fe7-44e6-8ccc-b097658f7c53","_cell_guid":"4fd258ba-b429-47e6-8b24-4c16eae82cae","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"begin = 1\nstart = 1942\n\n# Total sales over time\nsales_total = sales.loc[:,range(begin,start)].sum(axis=0).to_frame(\"total\")\nsales_total[\"mean\"] = sales_total[\"total\"].rolling(7).mean()\n\nfig = plt.figure(figsize = (8,4), dpi = 100)\nax = fig.add_subplot(1,1,1)\n\nax.plot(range(begin,start), sales_total[\"total\"], linewidth = 0.5)\nax.plot(range(begin,start), sales_total[\"mean\"])\n\ndef labels(axis, title=\"Total Sales\") :\n    axis.set_title(title)\n    axis.set_xlabel(\"Day\")\n    axis.set_ylabel(\"Sales\")\nlabels(ax)\nax.legend((\"Total\",\"Rolling 7-day average\"))\n\nplt.show()\n\n# Total sales over time by state and store\nsales_total = sales.loc[:,[\"store_id\"]+list(range(begin,start))]\nsales_total = sales_total.groupby(\"store_id\").sum()\n\nfig = plt.figure(figsize = (8,4), dpi = 100)\nax = fig.add_subplot(1,1,1)\n\ncolours_state = {\"CA\" : \"green\", \"TX\" : \"red\", \"WI\" : \"blue\"}\nlines = []\nlines_names = []\nlinestyles = [\"-\",\"--\",\":\"]\nlinestyles = linestyles + [\"-.\"] + linestyles*2\nnum = 0\nfor store_id in sales_total.index :\n    line = ax.plot(range(begin,start),\n                   sales_total.loc[store_id,:].rolling(7).mean(),\n                   color = colours_state[store_id[0:2]],\n                   linestyle = linestyles[num],\n                   linewidth = 0.5)\n    lines.append(line)\n    lines_names.append(store_id)\n    \n    num += 1\n    \nlabels(ax, title=\"Store Total Sales 7-day Rolling Average\")\ndef legend(ax, names = None) : ##? lines,\n    ax.legend(names, loc = \"center left\", bbox_to_anchor = [1.01,0.5])\nlegend(ax, names=lines_names)\n\nplt.show()\n\n# Total sales over time by category and department\nsales_total = sales.loc[:,[\"state_id\",\"dept_id\"]+list(range(begin,start))]\nsales_total = sales_total.groupby([\"state_id\",\"dept_id\"]).sum().reset_index()\n\nfig = plt.figure(figsize = (10,3), dpi = 100)\nfig.suptitle(\"Department Total Sales 7-day Rolling Average\",y=1)\n\ncolours_dept = {\"HOBBIES\" : \"green\", \"HOUSEHOLD\" : \"red\", \"FOODS\" : \"blue\"}\nlines = []\nlines_names = []\nlinestyles = [\"-\",\"--\"]\nlinestyles = linestyles + [\":\"] + linestyles*2\ncount=1\nfor state_id in sales.loc[:,\"state_id\"].unique() :\n    ax = fig.add_subplot(1,3,count)\n    ax.set_title(state_id)\n    ax.set_xlabel(\"Day\")\n    ax.set_ylabel(\"Sales\")\n    \n    num = 0\n    for dept_id in sales_total.loc[sales_total.loc[:,\"state_id\"]==state_id,\"dept_id\"].unique() :\n        line = ax.plot(range(begin,start), \n                       sales_total.loc[(sales_total.loc[:,\"dept_id\"]==dept_id)\n                                       & (sales_total.loc[:,\"state_id\"]==state_id),\n                                       range(begin,start)]\n                                  .iloc[0,:]\n                                  .rolling(7)\n                                  .mean(), \n                       color = colours_dept[dept_id.split(\"_\")[0]],\n                       linestyle = linestyles[num],\n                       linewidth = 0.5)\n        lines.append(line)\n        lines_names.append(dept_id)\n\n        num += 1\n    \n    count += 1\n\nax.legend(lines_names, \n          loc = \"center\", \n          bbox_to_anchor = [-0.7,-0.25], \n          ncol=len(lines_names),\n          prop={\"size\" : 7})\n\nplt.show()\n\ndel sales_total","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8218928e-7915-49a5-9e61-b6ca5beca083","_cell_guid":"595d53cd-1416-4ca7-8e67-21c7e4330e24","trusted":true},"cell_type":"markdown","source":"There is some clear month-to-month and year-to-year seasonality but not for all stores or departments.  There is some outlier behaviour such as jumps for WI_1 and WI_2.\n\nRolling averages over 7 days do not reveal week-to-week seasonality so the last 28 days are plotted below."},{"metadata":{"_uuid":"842ce0ff-e89d-413f-9994-5926f67f7b06","_cell_guid":"add3e0cb-07d8-4753-96b9-4de73d3791df","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"begin = 1942-28\nstart = 1942\n\n# Total sales over time\nsales_total = sales.loc[:,range(begin,start)].sum(axis=0).to_frame(\"total\")\n\nfig = plt.figure(figsize = (8,4), dpi = 100)\nax = fig.add_subplot(1,1,1)\n\nax.plot(range(begin,start), sales_total, linewidth = 2)\n\ndef labels(axis, title=\"Total Sales\") :\n    axis.set_title(title)\n    axis.set_xlabel(\"Day\")\n    axis.set_ylabel(\"Sales\")\nlabels(ax)\n\nplt.show()\n\n# Total sales over time by state and store\nsales_total = sales.loc[:,[\"store_id\"]+list(range(begin,start))]\nsales_total = sales_total.groupby(\"store_id\").sum()\n\nfig = plt.figure(figsize = (8,4), dpi = 100)\nax = fig.add_subplot(1,1,1)\n\ncolours_state = {\"CA\" : \"green\", \"TX\" : \"red\", \"WI\" : \"blue\"}\nlines = []\nlines_names = []\nlinestyles = [\"-\",\"--\",\":\"]\nlinestyles = linestyles + [\"-.\"] + linestyles*2\nnum = 0\nfor store_id in sales_total.index :\n    line = ax.plot(range(begin,start),\n                   sales_total.loc[store_id,range(begin,start)],\n                   color = colours_state[store_id[0:2]],\n                   linestyle = linestyles[num],\n                   linewidth = 0.5)\n    lines.append(line)\n    lines_names.append(store_id)\n    \n    num += 1\n    \nlabels(ax, title=\"Store Total Sales\")\ndef legend(ax, names = None) : ##? lines,\n    ax.legend(names, loc = \"center left\", bbox_to_anchor = [1.01,0.5])\nlegend(ax, names=lines_names)\n\nplt.show()\n\n# Total sales over time by category and department\nsales_total = sales.loc[:,[\"state_id\",\"dept_id\"]+list(range(begin,start))]\nsales_total = sales_total.groupby([\"state_id\",\"dept_id\"]).sum().reset_index()\n\nfig = plt.figure(figsize = (10,3), dpi = 100)\nfig.suptitle(\"Department Total Sales\", y=1)\n\ncolours_dept = {\"HOBBIES\" : \"green\", \"HOUSEHOLD\" : \"red\", \"FOODS\" : \"blue\"}\nlines = []\nlines_names = []\nlinestyles = [\"-\",\"--\"]\nlinestyles = linestyles + [\":\"] + linestyles*2\ncount=1\nfor state_id in sales.loc[:,\"state_id\"].unique() :\n    ax = fig.add_subplot(1,3,count)\n    ax.set_title(state_id)\n    ax.set_xlabel(\"Day\")\n    ax.set_ylabel(\"Sales\")\n    \n    num = 0\n    for dept_id in sales_total.loc[sales_total.loc[:,\"state_id\"]==state_id,\"dept_id\"].unique() :\n        line = ax.plot(range(begin,start), \n                       sales_total.loc[(sales_total.loc[:,\"dept_id\"]==dept_id)\n                                       & (sales_total.loc[:,\"state_id\"]==state_id),\n                                       range(begin,start)]\n                                  .iloc[0,:], \n                       color = colours_dept[dept_id.split(\"_\")[0]],\n                       linestyle = linestyles[num],\n                       linewidth = 0.5)\n        lines.append(line)\n        lines_names.append(dept_id)\n\n        num += 1\n    \n    count += 1\n\nax.legend(lines_names, \n          loc = \"center\", \n          bbox_to_anchor = [-0.7,-0.25], \n          ncol=len(lines_names),\n          prop={\"size\" : 7})\n\nplt.show()\n\ndel sales_total","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f8da06f-526c-4a63-a00a-0380ad7e8cb4","_cell_guid":"a1443a66-fcde-4b08-a027-48ab09ec561e","trusted":true},"cell_type":"markdown","source":"Clearly, there is weekly seasonality.  It is strongest at the highest level and store level, except store WI_2.  Department total sales are not as strongly seasonal for many departments, especially those of TX and WI."},{"metadata":{"_uuid":"f5753f04-ebbe-456a-b417-c4f59532b24f","_cell_guid":"d7033dcd-423e-46a2-b238-308f4daba24a","trusted":true},"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n## The Naive and Mean Models\n\nThe simplest possible model is one which assumes the next day will be the same as the last.  This is the naive model.  The mean model for each time series predicts future values as the mean of the last few days.\n\nThese are a useful starting point as a comparison to more advanced methods.  In rare cases these simple models will the best statistical methods can do, hence they can be treated as benchmarks which other models must improve upon.\n\nThe mean is calculated from the previous 28 days of sales, this was decided based on the analysis below."},{"metadata":{"_uuid":"4789ad56-ddc9-49a9-a1d2-cfee8af59f71","_cell_guid":"dfd704f3-6794-449b-bc99-beb0f136d366","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Naive method\n## array: 1d pandas array\n## begin, start, end: model input [begin,start), prediction [start,end], begin used for the naive day ##?\n## return: 1d pandas array, only predictions [start,end], same format as array\ndef predict_naive(predictor, begin=None, start=None, end=None) :    \n    if begin==None : begin = predictor.index.max()\n    if start==None : start = begin+1\n    if end==None : end = start+28-1\n        \n    return pd.Series([predictor.loc[begin]]*(end-start+1),\n                     range(start,end+1))\n\n# Apply naive to sales\nsales_naive = sales.loc[:,[max_pred]].apply(\n    lambda x : predict_naive(x, begin=max_pred, start=max_pred+1, end=max_pred+28),\n    axis=1\n)\nsales_naive = pd.merge(sales.loc[:,[\"id\"]],\n                       sales_naive,\n                       how=\"right\",\n                       left_index=True,\n                       right_index=True)\n\n# Mean method\ndef predict_mean(predictor, begin=None, start=None, end=None) :    \n    if begin==None : begin = predictor.index.min()\n    if start==None : start = predictor.index.max()+1\n    if end==None : end = start+28-1\n    \n    mean = predictor.loc[range(begin,start)].mean()\n    mean = round(mean)\n    \n    return pd.Series([mean]*(end-start+1),\n                     range(start,end+1)).astype(int)\n\n# Apply to mean sales\nstart = max_pred+1\nbegin = start-28\nend = start+28-1\n\nsales_mean = sales.apply(\n    lambda x : predict_mean(x, begin, start, end),\n    axis=1\n)\nsales_mean = pd.merge(sales.loc[:,[\"id\"]],\n                      sales_mean,\n                      left_index=True,\n                      right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3aae0c51-be08-4e97-acf3-0b0c2dc5cd64","_cell_guid":"9d8b4656-3ec5-41d7-b954-9defe198ecdf","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Plot example prediction\nfig = plt.figure(figsize = (8,2))\nax = fig.add_subplot(1,1,1)\n\nax.plot(range(begin,start),\n        sales.loc[3,range(begin,start)])\nax.plot(range(start,end+1),\n        sales_naive.loc[3,range(start,end+1)],\n        linestyle = \"--\")\nax.plot(range(start,end+1),\n        sales_mean.loc[3,range(start,end+1)],\n        linestyle = \"--\")\n\nax.set_title(\"Example: {}\".format(sales.loc[3,\"id\"]))\nax.legend([\"Observed\", \"Naive\", \"Mean\"], loc = \"center left\", bbox_to_anchor = [1.01,0.5])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5cb35e98-ee5b-47f7-a148-5d655098a79c","_cell_guid":"70a80e6c-f655-4f76-910f-7a59d57dfcd6","trusted":true},"cell_type":"markdown","source":"### Time series cross-validation\n\nTime series cross-validation uses the model on lots of different historical data to gauge the errors that might be expected on actual future applications of the model.  Performing rolling time-series cross-validation ten times with the mean model produces the following residuals."},{"metadata":{"_uuid":"53831096-3f8d-4f9a-be2e-9cb8912a0876","_cell_guid":"2af3afb3-0703-41f7-ab17-e834f4c20391","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Time series rolling cross-validation\n## return: pandas DataFrame with columns of\n##         averaged mean error, MAE, MSE if output=\"aggregate\"\n##       : pandas DataFrame with rows kth fold\n##         and columns of k, start, residual days if output=\"residuals\"\n### Note ts cv done on all or some of predictors\ndef cross_validation(\n        function = None, # function: kwargs predictor, begin, start, end.  MUST return df\n        predictor = None, # pandas dataframe, series x days, MUST HAVE UNIQUE INDEX\n        window_prior = None, # Days as input to function, i.e.start-begin\n        window_post = None, # Days as output of function, i.e. end-start+1\n        k_folds = 10) :\n    \n    #\n    out_array = pd.DataFrame()\n\n    # Interval between each cv fold\n    if k_folds == \"all\" :\n        k_folds = predictor.shape[-1]-window_prior-window_post+1\n        interval = 1\n    else :\n        interval = int((predictor.shape[-1]-window_prior-window_post+1)\n                       / k_folds)\n    if predictor.shape[-1] < window_prior+window_post :\n        raise Exception(\"Not enough days with given window sizes.\")\n    \n    # For each k_fold,\n    for k in range(1, k_folds+1) :\n        # Set begin, start, end\n        begin = predictor.columns[0] + interval*(k-1)\n        start = begin + window_prior\n        end = start + window_post - 1\n\n        # Prediction\n        prediction = function(predictor = predictor,\n                              begin = begin,\n                              start = start,\n                              end = end)\n\n        # Residuals\n        residuals = (prediction - predictor.loc[:,range(start,end+1)]).T.reset_index(drop=True).T\n        \n        # Append residuals to out_array\n        residuals = pd.merge(pd.DataFrame({\"k\" : [k]*residuals.shape[0],\n                                           \"start\" : [start]*residuals.shape[0]}, \n                                          index = residuals.index),\n                             residuals,\n                             left_index=True,\n                             right_index=True)\n\n        out_array = out_array.append(residuals)\n\n    out_array.loc[:,\"k\"] = out_array.loc[:,\"k\"].astype(int)\n\n    return out_array\n\n# Calculate cross-validated residuals. \n## DataFrame columns of fold, start, day. Index is id.\ncv_mean = cross_validation(\n    lambda predictor,begin,start,end : \n        predictor.apply(lambda x : predict_mean(x,begin,start,end),\n                        axis=1),\n    sales.set_index(\"id\").loc[:,range(max_pred-28*10,max_pred)],\n    window_prior = 28,\n    window_post = 28,\n    k_folds=10\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"415730d0-6d2b-4bd9-891b-5662b184a489","_cell_guid":"cdf2cb46-dae1-41b0-b9e4-bad14b76b764","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Statistics\narray = cv_mean.drop([\"k\",\"start\"],axis=1).stack()\nprint(\"Mean error: {}\\nMAE: {}\\nRMSE: {}\".format(\n    array.mean(),\n    array.abs().mean(),\n    np.sqrt((array**2).mean())\n))\n\n# Plot\nfig = plt.figure(figsize = (12,4), dpi = 100)\nax = fig.add_subplot(1,2,1)\n\n##\nax.hist(array,\n        bins = range(int(array.min()),\n                     int(array.max())+1,\n                     max(int((array.max()-array.min())/100),1)), \n        log=True)\n\nax.set_title(\"Residuals Histogram\")\nax.set_xlabel(\"Residual\")\nax.set_ylabel(\"Number of predictions\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4ce0c94-aa52-45dc-bd81-b03da50dd2a0","_cell_guid":"860eb76f-bd6f-407e-9c69-43ad4b1d7051","trusted":true},"cell_type":"markdown","source":"### Hyper-parameter tuning\n\nThe problem with the mean method is that all prior days are weighted equally so outlier behaviour in the past may have a significant effect.  The number of prior days used to calculate the mean is a hyper-parameter which can be tuned to give the optimal result.\n\nThe number of prior days' data points are varied and time series cross-validation is performed each time to chose the best value for forecasting."},{"metadata":{"_uuid":"5d743336-2db8-4d3a-82ca-614b699cc9b0","_cell_guid":"c098a05a-122f-4ccc-aad5-35a9814af320","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#\ndef hp_tuner(\n        params_list, # list of parameters for which t-s c-v each run on function\n        function = None, # function: kwargs predictor, begin, start, end, parameters\n        predictor = None, # 2d pandas array series x days\n        window_prior = None, # List of days, each as input to function, i.e.start-begin\n        window_post = None, # List of days to predict, each as output of function, i.e. end-start+1\n        k_folds = 10\n    ) :\n    \n    #\n    array = pd.DataFrame()\n    \n    #\n    for parameter in params_list :\n        #\n        out = cross_validation(lambda predictor,begin,start,end : \n                                   function(predictor=predictor, \n                                            begin=begin, \n                                            start=start, \n                                            end=end, \n                                            parameter=parameter),\n                               predictor=predictor,\n                               window_prior=window_prior,\n                               window_post=window_post,\n                               k_folds=k_folds)\n        \n        #\n        out = out.T.append(pd.DataFrame([[parameter for i in range(0,out.index.shape[0])]], \n                                        columns=out.index, \n                                        index=[\"parameter\"])).T\n        \n        array = array.append(out)\n\n    return array\n\n#\nparams_list = [7*i for i in range(0,10)]\n#\nhp = hp_tuner(\n    params_list,\n    lambda predictor,begin,start,end,parameter :\n        predictor.apply(lambda x : predict_mean(x,begin+parameter,start,end),\n                        axis=1),\n    sales.set_index(\"id\").loc[:,range(max_pred-28*10,max_pred)],\n    window_prior = max(params_list)+7,\n    window_post = 28,\n    k_folds=10\n)\nhp_mean = (hp.drop([\"start\"],axis=1)\n             .groupby([\"parameter\",\"k\"])\n             .apply(lambda x : x.drop([\"parameter\",\"k\"],axis=1)\n                                .abs()\n                                .stack()\n                                .mean())\n             .reset_index()\n             .drop(\"k\",axis=1)\n             .groupby(\"parameter\",as_index=False)\n             .mean()\n             .rename({0:\"MAE\"},axis=1)\n)\n\n# Plot\nfig = plt.figure(figsize = (6,2), dpi = 100)\n\nax = fig.add_subplot(1,1,1)\nax.plot(max(params_list)+7-hp_mean.loc[:,\"parameter\"],\n        hp_mean.loc[:,\"MAE\"])\nax.set_xlabel(\"Hyper-parameter\")\nax.set_ylabel(\"Mean Absolute Error\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7753eb8-fb85-462f-9e60-01d5995ce74f","_cell_guid":"0f90a5c6-92fa-474b-917a-fe36c4f082c8","trusted":true},"cell_type":"markdown","source":"Cleary, 28 prior days is the best choice.\n\n### Results\n\nThe validation submission scores were 1.46378 for naive and 1.16115 for mean.  As one would expect, the mean does better."},{"metadata":{"_uuid":"042fee52-b723-49f5-a21c-2883515a47de","_cell_guid":"7c035702-347b-4e8b-a744-8596450b9c1b","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def submission(array, file_name_add=\"\") :\n    sales_prediction = (array.loc[:,[\"id\"]+list(range(1914,1914+28))]\n                             .rename({1914+i-1 : \"F\"+str(i) for i in range(1,29)},\n                                     axis=1)\n                       )\n    sales_prediction.loc[:,\"id\"] = sales_prediction.loc[:,\"id\"].str.replace(\"evaluation\", \"validation\")\n    \n    sales_prediction.append(\n        array.loc[:,[\"id\"]+list(range(1914+28,1914+28*2))]\n             .rename({1914+28+i-1 : \"F\"+str(i) for i in range(1,29)},\n                     axis=1)\n    ).to_csv(path_or_buf = \"M5_accuracy_submission_{}.csv\".format(file_name_add),\n             index=False)\n\n# Validation days\n## Apply to naive sales\nsales_naive_val = sales.apply(\n    lambda x : predict_naive(x, max_pred-28, max_pred-28+1, max_pred-28+28),\n    axis=1\n)\nsales_naive_val = pd.merge(sales.loc[:,[\"id\"]],\n                           sales_naive_val,\n                           left_index=True,\n                           right_index=True)\n## Apply to mean sales\nsales_mean_val = sales.apply(\n    lambda x : predict_mean(x, begin-28, start-28, end-28),\n    axis=1\n)\nsales_mean_val = pd.merge(sales.loc[:,[\"id\"]],\n                          sales_mean_val,\n                          left_index=True,\n                          right_index=True)\n\n# Merge\nsales_naive = sales_naive_val.merge(sales_naive, on=\"id\", how=\"outer\")\nsales_mean = sales_mean_val.merge(sales_mean, on=\"id\", how=\"outer\")\n\n# Save submission\nsubmission(sales_naive, file_name_add = \"naive\")\nsubmission(sales_mean, file_name_add = \"mean\")\n\n## Description:\n## The naive forecasting model.\n## The mean forecasting model.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b41f3035-ba90-4776-ab0b-28881a8826d5","_cell_guid":"fc9b948e-8ebf-4f47-9746-54123ce510c8","trusted":true},"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n## Seasonal Holt-Winters\n\nThe seasonal Holt-Winters model is a relatively simple model for time series.  However, it doesn't handle data with lots of low values well so the hierarchical method is used too.  This means the seasonal Holt-Winters model is applied to the total store sales then the individual item sales are calculated by multiplying this total by the fraction of the item's historical sales.\n\nBelow the store level time series are plotted.  Predictions are in red."},{"metadata":{"_uuid":"79e394b5-46fd-461e-9e1b-a27bd95c365c","_cell_guid":"08aefe28-ba25-4603-b061-b7f88341a489","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#\nstart = max_pred+1\nbegin= start-28*4#d 1\nend = start+28-1\n\n# Smoothing\nsales_total = sales.loc[:,[\"store_id\"]+list(range(1,start))].groupby(\"store_id\").sum()\n\nwarnings_SHW = []\n\n# Apply seasonal Holt-Winters\n## array: 2d pandas DataFrame 1 x days\n## begin, start, end: model input [begin,start), prediction [start,end]\n## return: pandas Series, only predictions [start,end], same format as array\ndef predict_SHW(predictor, begin=None, start=None, end=None, warnings=None) :\n    if begin==None : begin = predictor.columns.min()\n    if start==None : start = predictor.columns.max()+1\n    if end==None : end = start+28-1\n    \n    # Parameters, begin at 0\n    hw_train = hw.ExponentialSmoothing(predictor.iloc[0].loc[range(begin,start)].tolist(),\n                                       trend = \"add\",\n                                       seasonal = \"add\",\n                                       seasonal_periods = 7)\n    try : \n        # Fit\n        hw_train = hw_train.fit()\n\n        # Prediction - first time at zero\n        hw_prediction = hw_train.predict(start = start-begin, \n                                         end = end-begin)\n        # shift time to first time at 1 (i.e. back to start)\n        hw_prediction = pd.Series(list(hw_prediction),\n                                  range(start,end+1))\n    except ConvergenceWarning :\n        hw_prediction = pd.Series([np.nan]*(end-start+1), index=range(start,end+1))\n        if warnings is not None :\n            warnings.append(predictor.index[0])\n        else : \n            print(\"ConvergenceWarning\")\n    \n    return hw_prediction\n\nshw_prediction = sales_total.apply(lambda x : predict_SHW(pd.DataFrame(x).T, \n                                                          warnings=warnings_SHW), \n                                   axis=1)\nprint(\"Did not converge: {}\".format(warnings_SHW))\n\n# Plot\n## observed, prediction: 2d arrays, must have same indices, indices used as plot titles\n## begin, start, end: observed plotted [begin,start), prediction plotted [start,end]\n## fig_title: whole figure title (suptitle)\ndef plot_forecasts(observed, \n                   prediction=None,\n                   fig_title=None) :    \n    fig = plt.figure(figsize = (8,1*len(observed.index)), dpi = 100)\n    fig.suptitle(fig_title, y=1.015)\n    \n    count = 1\n    for index in observed.index :\n        ax = fig.add_subplot(math.ceil(len(observed.index)/2),\n                             min(observed.shape[0],2),\n                             count)\n        \n        ax.plot(observed.columns, \n                observed.loc[index,:],\n                linewidth = 0.5,\n                color = \"gray\")\n        if prediction is not None :\n            ax.plot(prediction.columns, \n                    prediction.loc[index,:], \n                    linewidth = 0.5,\n                    color = \"red\")\n            maximum = max(observed.loc[index,:].max(),\n                          prediction.loc[index,:].max())\n            minimum = min(observed.loc[index,:].min(),\n                          prediction.loc[index,:].min())\n            ax.vlines(start, ymin = minimum, ymax = maximum, linewidth = 0.5, linestyle = \"--\")\n        \n        ax.set_title(index)\n        ax.set_xlabel(\"Day\")\n        ax.set_ylabel(\"Total Sales\")\n        \n        count += 1\n    \n    if len(observed.index) >= 2 : fig.tight_layout()\n\n    plt.show()\n\nplot_forecasts(sales_total.drop(warnings_SHW).loc[:,range(start-28*4,start)], \n               shw_prediction.dropna())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2a571cf-fb71-4df5-b490-e2578fb8cf29","_cell_guid":"ac022da0-a7b6-4f35-b6fe-37000d1d5bb7","trusted":true},"cell_type":"markdown","source":"### Time-series cross-validation\n\nTime series cross-validation is performed to gauge errors and the errors averaged to give the mean absolute error for each day.  The forecasts switch from over-predicting to under-predicting around day 12.  The averaged errors over all days are also given."},{"metadata":{"_uuid":"a03d817f-eaaa-4e7c-a556-a505ab38dc58","_cell_guid":"3dc76986-7cf1-4d1d-9577-16df3f6724de","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Cross-validation\ncv_shw = cross_validation(\n    lambda predictor,begin,start,end : \n        predictor.apply(lambda x : predict_SHW(pd.DataFrame(x).T,begin,start,end),\n                        axis=1),\n    sales_total.loc[:,range(1,max_pred+1)],\n    window_prior = 1600,\n    window_post = 28,\n    k_folds=10\n)\n\n# Statistics\nprint(\"Mean error: {}\\nMAE: {}\\nRMSE: {}\".format(\n    cv_shw.drop([\"k\",\"start\"],axis=1).stack().mean(),\n    cv_shw.drop([\"k\",\"start\"],axis=1).stack().abs().mean(),\n    np.sqrt((cv_shw.drop([\"k\",\"start\"],axis=1).stack()**2).mean())\n))\n\n# Plot errors\n# Plot\n## Output: Prints histogram or line/point plots of residuals\ndef plot_residuals(\n        array, # Pandas DataFrame containing residuals of series x days\n        group=None, # group for plotting lines if graph=\"line\"\n        title=None, # Plot title\n        quantile=0.95\n    ) :\n    \n    #? outliers + non_outliers = array ???\n    outliers = (array.reset_index(drop=True)\n                     .apply(lambda x : \n                                x.loc[x.abs().nlargest(int((1-quantile)*x.shape[0])).index]\n                                 .reset_index(drop=True)\n                           )\n               )\n    wedge = array.quantile([1-quantile,quantile],interpolation=\"nearest\")\n    \n    fig = plt.figure(figsize = (8,4), dpi = 100)\n    ax = fig.add_subplot(1,1,1)\n    \n    ax.fill_between(x = wedge.columns.to_list(),\n                    y1 = wedge.loc[1-quantile,:].to_list(),\n                    y2 = wedge.loc[quantile,:].to_list())\n    ax.scatter(outliers.stack().reset_index(1).loc[:,\"level_1\"], \n               outliers.stack().reset_index(drop=True),\n               s=2**2)\n\n    MAE = array.stack().reset_index(1).sort_values(\"level_1\").groupby(\"level_1\",as_index=False).mean()\n    ax.plot(MAE.loc[:,\"level_1\"], \n            MAE.iloc[:,1],\n            c=\"#76a834\",\n            linewidth=3)\n    ax.legend([\"Mean absolute error\",\"95th percentile range\", \"Outlier residual\"], \n              loc = \"center left\", \n              bbox_to_anchor = [1.01,0.5])\n    #print(MAE.loc[:,\"level_1\"], \n    #      MAE.iloc[:,1])\n    \n    ax.set_title(title)\n    ax.set_xlabel(\"Forecast Days Ahead\")\n    ax.set_ylabel(\"Residual\")\n    \n    plt.show()\nplot_residuals(cv_shw.drop([\"k\",\"start\"], axis=1),quantile=0.95)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c72e81e-a2a1-49bf-a754-fbcd57505116","_cell_guid":"32bb5fbe-612c-45d7-a050-0ec91c344f31","trusted":true},"cell_type":"markdown","source":"### Hierarchical distribution\n\nTo get individual sales for a specific item, the mean of the previous 28 sales are calculated for that item.  This is divided by the sum of all other items' means.  This fraction multiplies the total store sales from the seasonal Holt-Winters model to get the indidual item's sales.  This is done for each item in each store separately."},{"metadata":{"_uuid":"41841a27-5583-4b04-9142-9ac68356023e","_cell_guid":"1005271b-08e8-4406-a1ea-396821ff1680","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Calculate hierarchical distribution 1 level down using mean method\n## Returns: prediction under hierarchical mean model with group column\ndef hierarchical_mean(level0, # pandas DataFrame, prediction of higher aggregate sales,\n                              # only columns of index and predictions\n                      observed, # pandas DataFrame, prior observations, only columns\n                                # of index, group and observed values\n                      group=\"store_id\", # column in observed containing group that is in the index of level0\n                      begin=None, \n                      start=None, \n                      end=None) :    \n    if begin==None : begin = observed.select_dtypes('number').columns.column.min()\n    if start==None : start = level0.select_dtypes('number').column.min()\n    if end==None : end = level0.select_dtypes('number').column.max()\n    \n    # Mean prediction in \"p\"\n    sales_prediction = observed.loc[:,[group]+list(range(begin,start))] ##d [index,group]\n    sales_prediction.loc[:,\"p\"] = (\n        sales_prediction.loc[:,range(begin,start)].apply(lambda x : x.mean(),\n                                                         axis=1)\n    )\n    sales_prediction.drop(range(begin,start), axis=1, inplace=True)\n    # Calculate p from mean\n    sums = sales_prediction.loc[:,[group,\"p\"]].groupby(group).sum().loc[:,\"p\"]\n    sales_prediction.loc[:,\"p\"] = sales_prediction.apply(lambda x : x[\"p\"] / sums[x[group]],\n                                                         axis=1)\n    # Merge with level0 and *p\n    sales_prediction = pd.merge(sales_prediction, level0, on = group, how = \"left\")\n    sales_prediction.loc[:,range(start,end+1)] = (\n        sales_prediction.apply(lambda x : x.loc[range(start,end+1)] * x.loc[\"p\"],\n                               axis=1)\n    ).applymap(lambda x : np.nan if np.isnan(x)\n                          else round(x))\n\n    return sales_prediction.drop(\"p\",axis=1)\n\n# Replace non-converged with mean\nsales_total_mean = sales_mean.merge(sales.loc[:,[\"id\",\"store_id\"]],on=\"id\",how=\"left\")\nsales_total_mean = sales_total_mean.drop(\"id\",axis=1).groupby(\"store_id\").sum()\nshw_prediction = shw_prediction.fillna(sales_total_mean)\n\n# Caluclate sales from shw_prediction\nsales_SHW = hierarchical_mean(shw_prediction.reset_index()\n                                            .rename({\"index\" : \"store_id\"},axis=1)\n                                            .fillna(sales_mean), \n                              sales,\n                              group=\"store_id\",\n                              begin=start-28,\n                              start=start,\n                              end=end)\nsales_SHW = sales.loc[:,[\"id\"]].merge(sales_SHW, left_index=True, right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd2f03cd-1b7a-4859-9046-f01c3f9b5100","_cell_guid":"f84c722e-1257-4d9a-9e8e-5c12f7d493aa","trusted":true},"cell_type":"markdown","source":"### Results\n\nThe score was 0.78909.  Where the calculation did not converge, the series was replaced with the mean method results.  A significant improvement on the naive and mean methods."},{"metadata":{"_uuid":"9191b3da-6585-4d24-b33a-46923f527210","_cell_guid":"9be88a5c-a22e-42c3-81f0-af8e712fa83f","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Validation\n# Apply to mean sales\nwarnings_SHW_val = []\nshw_prediction_val = sales_total.apply(lambda x : predict_SHW(pd.DataFrame(x).T,\n                                                              begin=begin,\n                                                              start=start-28,\n                                                              end=end-28,\n                                                              warnings=warnings_SHW_val),\n                                       axis=1)\nprint(\"Did not converge: {}\".format(warnings_SHW_val))\n# Replace non-converged with mean\nshw_prediction_val = shw_prediction_val.fillna(sales_total_mean)\n\n# Hierarchical for sales\nsales_SHW_val = hierarchical_mean(shw_prediction_val.reset_index()\n                                                    .rename({\"index\" : \"store_id\"},axis=1),\n                                  sales,\n                                  group=\"store_id\",\n                                  begin=max_pred+1-28-28,\n                                  start=start-28,\n                                  end=max_pred)\nsales_SHW_val = sales.loc[:,[\"id\"]].merge(sales_SHW_val, left_index=True, right_index=True)\n\n# Merge with evaluation prediction\nsales_SHW = sales_SHW_val.drop(\"store_id\",axis=1).merge(sales_SHW.drop(\"store_id\",axis=1),\n                                                        on=\"id\",\n                                                        how=\"outer\")\n\n# Submission - validation\nsubmission(sales_SHW, file_name_add=\"SHW\")\n\n# Description:\n# Seasonal Holt-Winters of store total sales with hierarchical distribution factor given by mean of last 112 days in training data.\n\nwarnings.simplefilter('default')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a7cf3e7-7a17-4857-8e69-9d4ee521c500","_cell_guid":"fc5be7c7-7715-487b-9f84-89ff47025425","trusted":true},"cell_type":"markdown","source":"<a id=\"section-five\"></a>\n## ARIMA\n\nThe seasonal ARIMA model is used with the hierarchical method described earlier.  The model requires 7 hyper-parameters, notationally (p,d,q)x(P,D,Q,m), to be set which is a major challenge and can not always be done without trial and error.\n\nThe number of differences \"d\" and number of seasonal differences \"D\" can be determined by taking the difference and seasonal difference up to the order that the new differenced time series looks stationary.  The seasonal period is given by \"m\".  The variance and difference transformations give:"},{"metadata":{"_uuid":"584f75a9-cdd3-4ca4-a9ef-ad20d443ea70","_cell_guid":"11234bf3-3964-4814-ae1a-7cf5c4b546b7","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from statsmodels.tsa import stattools as stats\n\n#\nstart = max_pred+1\nbegin= start-28*4\nend = start+28-1\n\n#\nsales_total = sales.loc[:,[\"store_id\"]+list(range(1,max_pred+1))].groupby(\"store_id\").sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f495ea0e-e22c-4fb6-8f30-09bb12399c65","_cell_guid":"5fa49efe-c1f9-4d47-852d-f02d00a21a95","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Difference array entries d times and/or with difference m (x_m-x_1)\n## Returns: pandas DataFrame, differenced with ALL na columns removed\n### Note ALL columns containing na removed.\ndef diff(array, d, D, m=0) :\n    new_array = array.copy()\n    # differenced d\n    for i in range(0, d) :\n        new_array = new_array - new_array.shift(1, axis=1)\n    # seasonal m\n    if m > 0 :\n        # differenced D\n        for i in range(0, D) :\n            new_array = new_array - new_array.shift(m, axis=1)\n    return new_array.dropna(axis=1)\n\n#\n# plot_forecasts(sales_total.loc[:,range(begin,start)], fig_title=\"No Differencing\")\n#\nplot_forecasts(diff(sales_total,d=0,D=1,m=7).loc[:,range(begin,start)], fig_title=\"d=0, D=1, m=7\")\n# plot_forecasts(diff(sales_total,d=1,D=1,m=7).loc[:,range(begin,start)], fig_title=\"d=1, D=1, m=7\")\n#\nplot_forecasts(diff(sales_total.loc[[\"WI_2\",\"WI_3\"],:], d=0, D=1,m=28).loc[:,range(begin,start)], \n               fig_title=\"d=0, D=1, m=28\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb72dc1d-903e-457f-818c-f959e6847a64","_cell_guid":"c9782287-219e-4da8-9c01-a09d67443c1d","trusted":true},"cell_type":"markdown","source":"For the most part, D = 1 removes any signs of seasonality and the graphs look approximately stationary.  This is true when seasonal difference m = 7 days for all stores except for WI_2 and WI_3.  They require m = 28 to appear stationary.  Further differencing (not shown) causes the graphs to oscillate rapidly above and below the horizontal, a sign of over differencing.\n\nACF and PACF plots can reveal the appropriate p,q,P and Q parameters.  Where D = 1 and m = 7, the low decay and low values at the start indicate low p and q values of zero or one.  As before, WI_2 and WI_3 only have this property when m = 28."},{"metadata":{"_uuid":"a0dc53fa-bca3-41b5-acdb-1a6526c45f08","_cell_guid":"fe240ff5-f577-490c-887a-1b0e0089d555","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# ACF and PACF graphs\n## array: \ndef corr_graphs(array, fig_title = None) :\n    if len(array.shape) < 2 :\n        new_array = pd.DataFrame([array.copy()])\n    else :\n        new_array = array.copy()\n        \n    #\n    fig = plt.figure(figsize = (12,2*new_array.shape[0]), dpi = 125)\n    fig.suptitle(fig_title, y=1.015, size=18)\n    \n    nrows = new_array.shape[0]\n    count = 1\n    for index in new_array.index :        \n        row = new_array.loc[index,:]\n        #\n        n_lags = 20\n        # ACF\n        acf, acf_conf, q_stat, q_stat_p = stats.acf(row, nlags = n_lags, alpha = 0.05, qstat=True, fft=True)\n        # PACF\n        pacf, pacf_conf= stats.pacf(row, nlags = n_lags, alpha = 0.05)\n        \n        # Plot ACF\n        ax1 = fig.add_subplot(nrows,2,1+(count-1)*2)\n        ax1.bar(range(0,n_lags+1), acf, width = 0.25, color = \"blue\")\n        ax1.fill_between(range(0,n_lags+1), acf_conf[:,0], acf_conf[:,1], linewidth = 0.5, color = \"gray\", alpha = 0.25)\n        ax1.hlines(y = [1.96/math.sqrt(new_array.shape[1]),\n                        -1.96/math.sqrt(new_array.shape[1])], \n                   xmin = 0, xmax = n_lags, \n                   linestyles=\"dashed\")\n        \n        ax1.set_ylim(-1,1)\n        ax1.set_title(\"Store {store}\".format(store=index))\n        ax1.set_xlabel(\"Lag\")\n        ax1.set_ylabel(\"ACF\")\n        plt.grid(True)    \n        \n        # Plot PACF\n        ax2 = fig.add_subplot(nrows,2,2+(count-1)*2)\n        ax2.bar(range(0,n_lags+1), pacf, width = 0.25, color = \"blue\")\n        ax2.fill_between(range(0,n_lags+1), pacf_conf[:,0], pacf_conf[:,1], linewidth = 0.5, color = \"gray\", alpha = 0.25)\n        ax2.hlines(y = [1.96/math.sqrt(new_array.shape[1]),\n                        -1.96/math.sqrt(new_array.shape[1])], \n                   xmin = 0, xmax = n_lags, \n                   linestyles=\"dashed\")\n        \n        ax2.set_ylim(-1,1)\n        ax2.set_title(\"Store {store}\".format(store=index))\n        ax2.set_xlabel(\"Lag\")\n        ax2.set_ylabel(\"PACF\")\n        plt.grid(True)\n        \n        count += 1\n    \n    fig.tight_layout()\n    \n    plt.show()\n    \ncorr_graphs(diff(sales_total.loc[:,range(begin,start)],d=0,D=1,m=7), \n            fig_title = \"d=0, D=1, m=7\")\ncorr_graphs(diff(sales_total.loc[[\"WI_2\",\"WI_3\"],range(begin,start)],d=0,D=1,m=28), \n            fig_title = \"d=0, D=1, m=28\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fad2ee2c-2e8e-473a-8d72-52d365c70b56","_cell_guid":"84fd6307-e946-4c6f-95c6-c3f168e90f9d","trusted":true},"cell_type":"markdown","source":"### Hyper-parameter tuning and cross-validation\n\nUnfortunately it's not clear which is best so an exhaustive grid search approach is run on all possible values.  For each value, cross-validation is used and the errors averaged over all days.\n\nThe best parameters were chosen by first picking those with the lowest AICc for a given (d,D) pair.  Then the parameter with the lowest MAE was chosen out of these.  This was done for each store separately."},{"metadata":{"_uuid":"30d24e9f-1f75-40db-8fdc-ac52e4e0f9f5","_cell_guid":"4b250fc0-e8c3-40ee-96b8-140996b8868f","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Predict with SARIMA model\n## predictors: pandas DataFrame 1 x days\n## order, seasonal_order: see statsmodels\n## return: pandas Series containing predictions and index of days\ndef predict_SARIMA(predictor, \n                   order, \n                   seasonal_order, \n                   begin=None, \n                   start=None, \n                   end=None, \n                   AICc=None,\n                   warnings=None) :\n    if begin==None : begin = predictor.columns.min()\n    if start==None : start = predictor.columns.max()+1\n    if end==None : end = start+28-1\n        \n    try : \n        # Fit SARIMA\n        model = ARIMA(predictor.iloc[0].loc[range(begin,start)].tolist(), \n                      order = order, \n                      seasonal_order = seasonal_order)\n        fit = model.fit()\n\n        # Predict\n        SARIMA_prediction = pd.Series(fit.predict(start=start-begin,\n                                      end=end-begin)\n                                     )\n        if AICc is not None :\n            AICc.append([predictor.index[0],\n                         str([order,seasonal_order]),\n                         str((order[1],seasonal_order[1])),\n                         fit.aicc])\n    except (ConvergenceWarning, LinAlgError) as e :\n        SARIMA_prediction = pd.Series([np.nan]*(end-start+1))\n        if warnings is not None :\n            warnings.append([predictor.index[0], str([order, seasonal_order])])\n        else :\n            print(e)\n    \n    # Re-index\n    SARIMA_prediction.rename({i : start+i for i in range(0,end-start+1)},\n                             inplace=True)#, axis=1\n    SARIMA_prediction.name = predictor.index[0]\n\n    return SARIMA_prediction\n\n# List of parameters to try\n# Index of stores (unique) with list of many lists [order, seasonal_order]\n## With 7 day period\nparams_list_7 = it.product(it.product([0,1],repeat=3),\n                           it.product([0,1],repeat=3))\nparams_list_7 = pd.Series(params_list_7)\nparams_list_7 = params_list_7.apply(\n    lambda x : [x[0],tuple(list(x[1])+[7])]\n)\nparams_list_7 = params_list_7.reset_index().rename({\"index\":\"param_code\",\n                                                    0:\"parameter\"},\n                                                   axis=1)\n## With 28 day period\nparams_list_28 = it.product(it.product([0,1],repeat=3),\n                            it.product([0,1],repeat=3))\nparams_list_28 = pd.Series(params_list_28)\nparams_list_28 = params_list_28.apply(\n    lambda x : [x[0],tuple(list(x[1])+[28])]\n)\nparams_list_28 = params_list_28.reset_index().rename({\"index\":\"param_code\",\n                                                      0:\"parameter\"},\n                                                     axis=1)\n# All params\nparams_list = (pd.concat([params_list_7,params_list_28])\n                 .drop(\"param_code\",axis=1)\n                 .reset_index(drop=True)\n                 .reset_index()\n                 .rename({\"index\":\"param_code\"},axis=1))\n\n#\nAICc=[]\nwarnings_SARIMA = []\nhp_7 = hp_tuner(\n    params_list_7.loc[:,\"parameter\"],\n    lambda predictor,begin,start,end,parameter : \n        predictor.apply(lambda x : predict_SARIMA(pd.DataFrame(x).T,\n                                                  parameter[0],\n                                                  parameter[1],\n                                                  begin,\n                                                  start,\n                                                  end,\n                                                  AICc=AICc,\n                                                  warnings=warnings_SARIMA),\n                        axis=1),\n    sales_total.loc[:,range(max_pred-28*10,max_pred)],\n    window_prior = 28*4,\n    window_post = 28,\n    k_folds=10\n)\n# With 28 days to WI_2 and WI_3\nhp_28 = hp_tuner(\n    params_list_28.loc[:,\"parameter\"],\n    lambda predictor,begin,start,end,parameter : \n        predictor.apply(lambda x : predict_SARIMA(pd.DataFrame(x).T,\n                                                  parameter[0],\n                                                  parameter[1],\n                                                  begin,\n                                                  start,\n                                                  end,\n                                                  AICc=AICc,\n                                                  warnings=warnings_SARIMA),\n                        axis=1),\n    sales_total.loc[[\"WI_2\",\"WI_3\"],range(max_pred-28*10,max_pred)],\n    window_prior = 28*4,\n    window_post = 28,\n    k_folds=10\n)\n# Join\nhp = pd.concat([hp_7.reset_index(),hp_28.reset_index()])\ndel hp_28\n\n# Include param_code\nhp.loc[:,\"parameter_key\"] = hp.loc[:,\"parameter\"].astype(str)\nparams_list.loc[:,\"parameter_key\"] = params_list.loc[:,\"parameter\"].astype(str)\nhp = hp.merge(params_list.drop(\"parameter\",axis=1),\n              on=\"parameter_key\", \n              how=\"left\")\nhp.drop(\"parameter_key\", axis=1, inplace=True)\nparams_list.drop(\"parameter_key\", axis=1, inplace=True)\n\n# Print number of warnings for each store_id and parameter\ndef count_warnings(warnings_list) :\n    # Count warnings\n    warnings_count = (\n        pd.DataFrame(warnings_list, columns=[\"store_id\",\"parameter\"])\n          .groupby(\"store_id\")\n          .apply(lambda x : x.loc[:,\"parameter\"]\n                             .value_counts())\n          .reset_index()\n    )\n    if warnings_count.shape[1]==2 : \n        warnings_count.loc[:,\"parameter\"] = warnings_count.columns[1]\n        warnings_count.columns = [\"store_id\",\"count\", \"parameter\"]\n        warnings_count = warnings_count.loc[:,[\"store_id\",\"parameter\",\"count\"]]\n    else :\n        warnings_count = warnings_count.rename({\"level_1\":\"parameter\",\"parameter\":\"count\"},\n                                               axis=1)\n    \n    # Include param_code\n    params_list.loc[:,\"parameter_key\"] = params_list.loc[:,\"parameter\"].astype(str)\n    warnings_count = (warnings_count.merge(params_list.drop(\"parameter\",axis=1), \n                                           left_on=\"parameter\",\n                                           right_on=\"parameter_key\",\n                                           how=\"left\")\n                                    .drop(\"parameter_key\",axis=1)\n                     )\n    params_list.drop(\"parameter_key\", axis=1, inplace=True)\n    \n    warnings_count = warnings_count.sort_values([\"store_id\",\"count\",\"param_code\"])\n    \n    return warnings_count\n\nif len(warnings_SARIMA) > 0 :     \n    # Count warnings\n    warnings_count_SARIMA = count_warnings(warnings_SARIMA)\n    print(\"Did not converge/complete:\\n{}\\n\".format(warnings_count_SARIMA))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b61d8df7-3ef8-41e7-a743-8c44df6830e9","_cell_guid":"690e3a85-e28b-4130-8fce-6d90643b53df","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# For MAE, AICc averaged over folds\nhp_stats = hp.copy()\n\n# MAE\n## Mean over days\nhp_stats.loc[:,\"MAE\"] = (hp_stats.drop([\"store_id\",\"parameter\",\"param_code\",\"start\",\"k\"],\n                                       axis=1)\n                                 .abs()\n                                 .mean(axis=1))\nhp_stats = hp_stats.loc[:,[\"store_id\",\"param_code\",\"MAE\"]]\n## Mean over folds\nhp_stats = (hp_stats.groupby([\"store_id\",\"param_code\"])\n                    .mean()\n                    .reset_index())\nhp_stats = hp_stats.merge(params_list, on=\"param_code\", how=\"left\")\n\n# AICc\n## Store AICc\nparams_list.loc[:,\"parameter_key\"] = params_list.loc[:,\"parameter\"].astype(str)\nhp_AICc = (pd.DataFrame(AICc,columns=[\"store_id\",\"parameter\",\"diff\",\"AICc\"])\n             .merge(params_list.drop(\"parameter\",axis=1), \n                     left_on=\"parameter\",\n                     right_on=\"parameter_key\",\n                     how=\"left\")\n              .drop([\"parameter_key\",\"parameter\"],axis=1))\nparams_list.drop(\"parameter_key\", axis=1, inplace=True)\n## Mean over folds\nhp_AICc = hp_AICc.groupby([\"store_id\",\"diff\",\"param_code\"]).mean().reset_index()\n## Merge with hp_stats\nhp_stats = hp_stats.merge(hp_AICc, on=[\"store_id\",\"param_code\"],how=\"left\")\nhp_stats = hp_stats.loc[:,[\"store_id\",\"parameter\",\"diff\",\"MAE\",\"AICc\",\"param_code\"]]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d413507-d82b-4d06-a465-b784facf1c72","_cell_guid":"a4fc03f5-a843-4206-a738-e24adf22fd6a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# For comparison of approach only.\n# # Top MAE\n# hp_top_MAE = (hp_stats.sort_values(\"MAE\")\n#                       .groupby(\"store_id\")\n#                       .head(1)\n#                       .sort_values(\"store_id\")\n#                       .reset_index(drop=True))\n# # Top AICc\n# hp_top_AICc = (hp_stats.loc[hp_stats.loc[:,\"diff\"]==\"(0, 1)\",:]\n#                        .sort_values(\"AICc\")\n#                        .groupby([\"store_id\"])\n#                        .head(1)\n#                        .sort_values(\"store_id\")\n#                        .reset_index(drop=True))\n\n# Top MAE of top AICc for each diff\nhp_top_MAE_AICc = (hp_stats.sort_values(\"AICc\")\n                           .groupby([\"store_id\",\"diff\"])\n                           .head(1)\n                           .sort_values(\"MAE\")\n                           .groupby(\"store_id\")\n                           .head(1)\n                           .sort_values(\"store_id\")\n                           .reset_index(drop=True))\n\n#\nfor top in [hp_top_MAE_AICc] :\n    print(\"Top parameters:\\n{}\\n\".format(\n        top.drop(\"diff\",axis=1)\n    ))\n\n    # Plot residuals\n    top_residuals = top.drop(\"parameter\",axis=1).merge(\n        hp,\n        on=[\"store_id\",\"param_code\"],\n        how=\"left\"\n    ).drop(list(top.columns)+[\"k\",\"start\"],axis=1)\n    print(\"Mean error: {}\\nMAE: {}\\nRMSE: {}\\n\".format(\n        top_residuals.stack().mean(),\n        top_residuals.stack().abs().mean(),\n        math.sqrt((top_residuals.stack()**2).mean())\n    ))\n    plot_residuals(top_residuals.astype(float),\n                   quantile=0.95)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Further checks (not shown) ensure the ACF of residuals are approximately white noise further demonstrating appropriateness of these parameters."},{"metadata":{"_uuid":"c94a614a-8de2-4f52-bf51-f593337be195","_cell_guid":"5e6bf55b-b549-43d3-8c44-0c0d7f1e3d92","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Choose hp_top\nhp_top = hp_top_MAE_AICc.set_index(\"store_id\")\n\n# Apply final parameters\nparams_list = pd.DataFrame()\nfor store_id in hp_top.index :\n    params_list = params_list.append(\n        pd.Series(\n            {\"order\" : hp_top.loc[store_id, \"parameter\"][0],\n             \"seasonal_order\" : hp_top.loc[store_id, \"parameter\"][1]},\n            name = store_id\n        )\n    )\n\n# Final predictions on evaluation data\nstart = max_pred+1\nbegin= start-28*4\nend = start+28-1\nwarnings_SARIMA = []\nSARIMA_prediction = sales_total.apply(\n    lambda x : predict_SARIMA(pd.DataFrame(x.loc[range(begin,start)]).T,\n                              order=params_list.loc[x.name,\"order\"],\n                              seasonal_order=params_list.loc[x.name,\"seasonal_order\"],\n                              begin=begin,\n                              start=start,\n                              end=end,\n                              warnings=warnings_SARIMA),\n    axis=1\n)\n\n# Print number of warnings for each store_id and parameter\nif len(warnings_SARIMA) > 0 :     \n    # Count warnings\n    warnings_count_SARIMA = count_warnings(warnings_SARIMA)\n    print(\"Did not converge/complete:\\n{}\\n\".format(warnings_count_SARIMA))\n\n# Replace non-converged with mean\nSARIMA_prediction = SARIMA_prediction.fillna(sales_total_mean)\n\n#\nsales_SARIMA = hierarchical_mean(SARIMA_prediction.reset_index().rename({\"index\" : \"store_id\"},axis=1),\n                                 sales,\n                                 group=\"store_id\",\n                                 begin=start-28,\n                                 start=start,\n                                 end=end)\nsales_SARIMA = sales.loc[:,[\"id\"]].merge(sales_SARIMA, left_index=True,right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab509da0-2b92-4853-9aca-467d1a09565f","_cell_guid":"7f7866a3-45cd-4a93-a67b-1d24fa01cde3","trusted":true},"cell_type":"markdown","source":"### Results\n\nThe score is 0.78380, a slight improvement on the seasonal Holt-Winters model.  Unfortunately, the cross-validated errors are slightly higher for the ARIMA model indicating its reliabilty is similar or worse.\n\nThe model submitted in time for the final was slightly different.  It was an ARIMA model with parameters that had the lowest MAE and did not consider the AICc.  It got a notably lower score of 0.85226."},{"metadata":{"_uuid":"914cf8dd-0141-4f36-9334-ca090bb4adb8","_cell_guid":"86f58176-1f8d-4c77-aea7-a6a975543238","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Validation\nwarnings_SARIMA = []\nSARIMA_prediction_val = sales_total.apply(\n    lambda x : predict_SARIMA(pd.DataFrame(x).T,\n                              order=params_list.loc[x.name,\"order\"],\n                              seasonal_order=params_list.loc[x.name,\"seasonal_order\"],\n                              begin=begin-28,\n                              start=start-28,\n                              end=end-28, \n                              warnings=warnings_SARIMA),\n    axis=1\n)\n\n# Print number of warnings for each store_id and parameter\nif len(warnings_SARIMA) > 0 :     \n    # Count warnings\n    warnings_count_SARIMA = count_warnings(warnings_SARIMA)\n    print(\"Did not converge/complete:\\n{}\\n\".format(warnings_count_SARIMA))\n    \n# Replace non-converged with mean\nSARIMA_prediction_val = SARIMA_prediction_val.fillna(sales_total_mean)\n\nsales_SARIMA_val = hierarchical_mean(SARIMA_prediction_val.reset_index().rename({\"index\" : \"store_id\"},axis=1),\n                                     sales,\n                                     begin=start-28-28,\n                                     start=start-28,\n                                     end=end-28).drop(\"store_id\",axis=1)\nsales_SARIMA_val = sales.loc[:,[\"id\"]].merge(sales_SARIMA_val, left_index=True,right_index=True)\n\n# Merge\nsales_SARIMA = sales_SARIMA_val.merge(sales_SARIMA, on=\"id\", how=\"outer\")\n\n# Submission - validation\nsubmission(sales_SARIMA, file_name_add=\"SARIMA\")\n\n# Description:\n# SARIMA model of store total sales with hierarchical distribution factor given by mean of last 112 days in training data.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68ca6573-c1eb-4e48-b69a-6db7df36b7cf","_cell_guid":"f98a063b-cf1b-467f-b755-2ee0877a4432","trusted":true},"cell_type":"markdown","source":"<a id=\"section-six\"></a>\n## Conclusion\n\nThe ARIMA model may have gotten the highest score here but was only narrowly better than the seasonal Holt-Winters model.  Cross-validation further indicates slightly better reliability in the latter.  When selecting ARIMA paramters, AICc proved an important consideration in model selection.\n\nUltimately, the seasonal Holt-Winters model is the narrowly the best choice based purely on reliability.\n\nIncluding other data available such as prices and promotional events may substantially improve the score.\n\nThe winner of the competition used a decision tree method via the LightGBM package, hence beating traditional statistical models like the ones explored here.  Nevertheless, both approaches prove useful in predicting the many tens of thousands of time series and future competitions or research should hopefully help in clarifying the differences and strength of each."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}